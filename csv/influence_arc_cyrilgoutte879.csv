2005.jeptalnrecital-long.24,J93-2003,0,0.0126525,"Missing"
2005.jeptalnrecital-long.24,P01-1030,1,0.892376,"Missing"
2005.jeptalnrecital-long.24,P04-1064,1,0.882028,"Missing"
2005.jeptalnrecital-long.24,P99-1041,0,0.0877922,"Missing"
2005.jeptalnrecital-long.24,W02-1018,0,0.0264918,"Missing"
2005.jeptalnrecital-long.24,P00-1056,0,0.327309,"Missing"
2005.jeptalnrecital-long.24,J04-4002,0,0.0723566,"Missing"
2005.jeptalnrecital-long.24,W99-0604,0,0.111557,"Missing"
2005.jeptalnrecital-long.24,P03-1021,0,0.0362064,"Missing"
2005.jeptalnrecital-long.24,P02-1040,0,0.072645,"Missing"
2005.jeptalnrecital-long.24,N03-2036,0,0.0275543,"Missing"
2007.mtsummit-papers.34,J93-2003,0,0.0170215,"Missing"
2007.mtsummit-papers.34,2006.eamt-1.27,0,0.103424,"it documents, see which errors crop up over and over (these will be different for any given system/domain pair), and begin to emulate what the human is doing.” (p. 779) Their paper does not explore that possibility in any detail, though. (Allen and Hogan, 2000) propose the development of a “processing engine that could automatically fix up machine translation raw output before such texts are even given to a human posteditor”. They are not very explicit about how such an engine would operate but their discussion suggests that it would be fed with manually developed sets of post-editing rules. (Elming, 2006) presents the first published results on the use of an APE module to correct MT output. He uses transformation-based learning to correct the output of the Patrans RBMT system and reports a 4.6 point increase in BLEU score. In (Simard et al., 2007a) we presented a set of experiments on using standard phrase-based SMT technology to build an APE module for a classical rule-based MT system. The APE task is then viewed as a machine translation task in which our SMT system “translates” from the language of RBMT outputs into the language of their manually post-edited counterparts. In the experiments"
2007.mtsummit-papers.34,W06-1607,0,0.0266641,"Missing"
2007.mtsummit-papers.34,N03-1017,0,0.0661419,"Missing"
2007.mtsummit-papers.34,koen-2004-pharaoh,0,0.0341227,"steps: preprocessing of raw data into tokens; decoding to produce one or more translation hypotheses; and error-driven re-scoring to choose the best hypothesis. For languages such as French and English, the first of these steps (tokenization) is mostly a straightforward process and we do not describe it any further here. Decoding is the central task in SMT, involving a search for the hypotheses that have highest probabilities of being translations of the current source sentence according to a model of . PORTAGE implements a dynamic programming beam search decoding algorithm similar to that of Koehn (2004), in which translation hypotheses are constructed by combining in various ways the target-language part of phrase pairs whose source-language part matches the input. These phrase pairs come from large phrase tables constructed by collecting matching pairs of contiguous text segments from word-aligned bilingual corpora. PORTAGE’s model for is a log-linear combination of four main components: one or more -gram targetlanguage models, one or more phrase translation models, a distortion (word-reordering) model, and a sentence-length feature. The phrase-based translation model is similar to that of"
2007.mtsummit-papers.34,W02-1018,0,0.0269009,"osteditor under a range of different MT conditions: SMT versus RBMT, non-adapted versus adapted RBMT and presence or absence of an APE module. Our main finding is that in combination with the SMT-based APE module, the vanilla RBMT system performs almost as well as its manually adapted version. In other words, the APE has succeeded in capturing whatever benefits were brought by manual system adaptation. In section 5 we discuss that result before concluding the paper. 2 SMT and Automatic Post-Editing The work reported here is based on the paradigm of phrasebased statistical machine translation (Marcu and Wong, 2002; Koehn et al., 2003). Our PORTAGE SMT system (Sadat et al., 2005) is a typical exemplification of that paradigm. In recent months, we tested it in the role of an APE module for a commercial RBMT system (Simard et al., 2007a), and in this paper we show that APE constitutes a good way to adapt the RBMT system to a new domain. 2.2 Previous results on SMT-based APE 2.1 The PORTAGE SMT system PORTAGE is a phrase-based, statistical machine translation system, developed at the National Research Council of Canada (NRC) (Sadat et al., 2005).1 Like other SMT systems, it learns to translate from existin"
2007.mtsummit-papers.34,P03-1021,0,0.0269477,"-linear combination of four main components: one or more -gram targetlanguage models, one or more phrase translation models, a distortion (word-reordering) model, and a sentence-length feature. The phrase-based translation model is similar to that of Koehn, with the exception that phrase probability estimates are smoothed using the Good-Turing technique (Foster et al., 2006). The distortion model is also very similar to Koehn’s, with the exception of a final cost to account for sentence endings. Feature function weights in the log-linear model are set using Och’s minimum error rate algorithm (Och, 2003). This is essentially an iterative two-step process: for a given set of source sentences, generate -best translation hypotheses, that are representative of the entire decoding            1 search space; then, apply a variant of Powell’s algorithm to find weights that optimize the BLEU score over these hypotheses, compared to reference translations. This process is repeated until the set of translations stabilizes, i.e. no new translations are produced at the decoding step. To improve raw output from decoding, PORTAGE relies on a re-scoring strategy: given a list of -best"
2007.mtsummit-papers.34,P02-1040,0,0.0898536,"Missing"
2007.mtsummit-papers.34,W05-0822,0,0.035014,", non-adapted versus adapted RBMT and presence or absence of an APE module. Our main finding is that in combination with the SMT-based APE module, the vanilla RBMT system performs almost as well as its manually adapted version. In other words, the APE has succeeded in capturing whatever benefits were brought by manual system adaptation. In section 5 we discuss that result before concluding the paper. 2 SMT and Automatic Post-Editing The work reported here is based on the paradigm of phrasebased statistical machine translation (Marcu and Wong, 2002; Koehn et al., 2003). Our PORTAGE SMT system (Sadat et al., 2005) is a typical exemplification of that paradigm. In recent months, we tested it in the role of an APE module for a commercial RBMT system (Simard et al., 2007a), and in this paper we show that APE constitutes a good way to adapt the RBMT system to a new domain. 2.2 Previous results on SMT-based APE 2.1 The PORTAGE SMT system PORTAGE is a phrase-based, statistical machine translation system, developed at the National Research Council of Canada (NRC) (Sadat et al., 2005).1 Like other SMT systems, it learns to translate from existing parallel corpora. The system translates text in three main steps"
2007.mtsummit-papers.34,W07-0728,1,0.876832,"llen and Hogan, 2000) propose the development of a “processing engine that could automatically fix up machine translation raw output before such texts are even given to a human posteditor”. They are not very explicit about how such an engine would operate but their discussion suggests that it would be fed with manually developed sets of post-editing rules. (Elming, 2006) presents the first published results on the use of an APE module to correct MT output. He uses transformation-based learning to correct the output of the Patrans RBMT system and reports a 4.6 point increase in BLEU score. In (Simard et al., 2007a) we presented a set of experiments on using standard phrase-based SMT technology to build an APE module for a classical rule-based MT system. The APE task is then viewed as a machine translation task in which our SMT system “translates” from the language of RBMT outputs into the language of their manually post-edited counterparts. In the experiments reported in the paper, the addition of such an APE module resulted in very substantial gains in translation quality (figures will be provided below). Once we realize that APE is feasible, we are led to ask to what extent it could be used as a mea"
2007.mtsummit-papers.34,2006.amta-papers.25,0,0.104635,"Missing"
2009.mtsummit-papers.9,P91-1023,0,0.0263454,"usable English or French sentences to be aligned with blank lines. It is uncertain whether other problems exist and what, if any, effect such problems would have on the results. Finally, and perhaps less of an issue and more of an idiosyncrasy of the corpus, is the imbalance between the number of English original and French original sentences ( 4:1 ratio). 3.1 Preprocessing As an initial pre-processing step, we first lowercased, tokenized and sentence-aligned the corpus using NRC’s tools. The tokenizer is standard, and the sentence aligner implements the well-known Gale and Church algorithm (Church and Gale, 1991). As an additional step, alignments with null sentences on either side were removed. We considered the data at two levels of granularity. For the simpler, sentence level variant, each line of the aligned corpus, is considered a basic textual unit, also called a fragment. We also used larger units containing consecutive sequences of sentences with the same original language, henceforth called blocks. This yields our block level results. Note that by definition, blocks are of very different length, containing 3 to several thousand words each. In addition, as one block of French-original (fo) tex"
2009.mtsummit-papers.9,P02-1040,0,0.0940905,"Missing"
2009.mtsummit-papers.9,C08-1118,0,0.189671,"Missing"
2012.amta-papers.7,W05-0909,0,0.0140376,"kin et al., 2010). We use the following feature functions The parameters of the log-linear model are tuned by optimizing BLEU on the development set using MIRA (Chiang et al., 2008).4 Phrase extraction is done by aligning the corpus at the word level using both HMM and IBM2 models, using the union of phrases extracted from these separate alignments for the phrase table, with a maximum phrase length of 7 tokens. Phrase pairs were filtered so that the top 30 translations for each source phrase were retained. The translation performance was measured using BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005). 4 Results We show how SMT output degrades with increasing alignment noise. We see that, surprisingly, even relatively high levels of noise have little impact on translation performance. We then compare the robustness of PBMT systems to that of Translation Memories, a commom computer-aided translation tool. 4.1 Impact on translation performance Figure 3 shows how translation performance, as estimated by BLEU (circles), degrades when the number of misaligned sentence pairs increases. Not surprisingly, increasing the noise level produces a general decrease in performance. Although there are var"
2012.amta-papers.7,D08-1024,0,0.0199038,"Missing"
2012.amta-papers.7,W10-1717,1,0.881245,"Missing"
2012.amta-papers.7,moore-2002-fast,0,0.356444,"Missing"
2012.amta-papers.7,P02-1040,0,0.0867124,"ecent NIST and WMT evaluations (Larkin et al., 2010). We use the following feature functions The parameters of the log-linear model are tuned by optimizing BLEU on the development set using MIRA (Chiang et al., 2008).4 Phrase extraction is done by aligning the corpus at the word level using both HMM and IBM2 models, using the union of phrases extracted from these separate alignments for the phrase table, with a maximum phrase length of 7 tokens. Phrase pairs were filtered so that the top 30 translations for each source phrase were retained. The translation performance was measured using BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005). 4 Results We show how SMT output degrades with increasing alignment noise. We see that, surprisingly, even relatively high levels of noise have little impact on translation performance. We then compare the robustness of PBMT systems to that of Translation Memories, a commom computer-aided translation tool. 4.1 Impact on translation performance Figure 3 shows how translation performance, as estimated by BLEU (circles), degrades when the number of misaligned sentence pairs increases. Not surprisingly, increasing the noise level produces a general decrease"
2012.eamt-1.65,P10-1088,0,0.407355,"Missing"
2012.eamt-1.65,D07-1090,0,0.144182,"all training set and perplexity as evaluation metric. (Koehn et al., 2003) and (Suresh, 2010) show that BLEU score has a log-linear dependency with training corpus size, but this is limited to 350k training sentence pairs. Learning curves were also presented in order to motivate the use of active learning for MT (Bloodgood and Callison-Burch, 2010; Haffari et al., 2011). They attempt to address the challenge of “diminishing returns” in learning MT, although this is again done with small training corpora (<90k sentence pairs), and, on a log-scale, performance seems again to increase linearly. (Brants et al., 2007) produced a large-scale study, but focused on the language model training only, with billions of (monolingual) tokens. The first complete and systematic analysis of PBSMT learning curves was obtained by (Turchi et al., 2008) using the Spanish-English Europarl, and recently extended to larger training data and more systems by (Turchi et al., 2011). In their work, accurate learning curves obtained over a large range of data sizes confirm that performance grows linearly in the log domain. The reason why relatively few systematic studies have been reported may be that producing accurate learning c"
2012.eamt-1.65,P03-1021,0,0.159324,"le for academic purposes, while Portage is a similar package, available to partners of the National Research Council Canada. Given a parallel training corpus, both perform basic preprocessing (tokenization, lowercasing, etc.) if necessary, and build the various components of the model. Both use standard external tools for training the language model, such as SRILM (Stolcke, 2002). Moses uses GIZA++ (Och and Ney, 2003) for word alignments, while Portage uses an in-house IBM model and HMM implementation. The parameters of the log-linear models are tuned using minimum error rate training (MERT, (Och, 2003)). Earlier experiments performed on the Europarl corpus with both systems showed (Turchi et al., 2011) that despite small differences in observed performance, both systems produce very similar learning curves. 3 3.1 Experimental Setting Corpora We experiment with large corpora in two language pairs: French-English and Chinese-English. For French-English, we use the Giga corpus (Callison-Burch et al., 2009) to provide the training, development and one in-domain test set. As out-of-domain test set, we use two different samples from the EMEA corpus (Tiedemann, 2009), which contains parallel docum"
2012.eamt-1.65,J03-1002,0,0.00345257,"ystems capable of learning translation tables, language models and decoding parameters from one or several parallel corpora. Moses is a complete open-source phrase-based translation toolkit available for academic purposes, while Portage is a similar package, available to partners of the National Research Council Canada. Given a parallel training corpus, both perform basic preprocessing (tokenization, lowercasing, etc.) if necessary, and build the various components of the model. Both use standard external tools for training the language model, such as SRILM (Stolcke, 2002). Moses uses GIZA++ (Och and Ney, 2003) for word alignments, while Portage uses an in-house IBM model and HMM implementation. The parameters of the log-linear models are tuned using minimum error rate training (MERT, (Och, 2003)). Earlier experiments performed on the Europarl corpus with both systems showed (Turchi et al., 2011) that despite small differences in observed performance, both systems produce very similar learning curves. 3 3.1 Experimental Setting Corpora We experiment with large corpora in two language pairs: French-English and Chinese-English. For French-English, we use the Giga corpus (Callison-Burch et al., 2009) t"
2012.eamt-1.65,P02-1038,0,0.197904,"Missing"
2012.eamt-1.65,P02-1040,0,0.0832072,"Missing"
2012.eamt-1.65,steinberger-etal-2006-jrc,0,0.0300033,"Missing"
2012.eamt-1.65,W08-0305,1,0.867301,"earning curves were also presented in order to motivate the use of active learning for MT (Bloodgood and Callison-Burch, 2010; Haffari et al., 2011). They attempt to address the challenge of “diminishing returns” in learning MT, although this is again done with small training corpora (<90k sentence pairs), and, on a log-scale, performance seems again to increase linearly. (Brants et al., 2007) produced a large-scale study, but focused on the language model training only, with billions of (monolingual) tokens. The first complete and systematic analysis of PBSMT learning curves was obtained by (Turchi et al., 2008) using the Spanish-English Europarl, and recently extended to larger training data and more systems by (Turchi et al., 2011). In their work, accurate learning curves obtained over a large range of data sizes confirm that performance grows linearly in the log domain. The reason why relatively few systematic studies have been reported may be that producing accurate learning curves up to large data sizes with state-of-the-art systems requires the use of high performance computing in a carefully set up environment. This may seem dispensable when typical SMT research is usually focused on maximizin"
2012.eamt-1.65,J07-2003,0,0.0532757,"wj are target words. Parameter estimation is crucial for both the translation and language model features. Conditional probabilities are estimated from a large training corpus using empirical counts and various smoothing strategies. In addition, the weights λi are also estimated from a (usually disjoint) corpus of source and target sentence pairs. The size and composition of the training data will therefore have an influence on the quality of the predictions b e through the estimation of both the log-linear parameters and the feature functions. Note that alternate models such as hierarchical (Chiang, 2007) or syntax based (Zollman and Venugopal, 2006) have been developed and could also be studied. However their use on the large scale necessary for creating accurate learning curves would require solving a number of practical issues and we focus instead on the straight PBSMT approach, which has been shown in recent MT evaluations (Callison-Burch et al., 2009; Callison-Burch et al., 2011) to offer competitive performance. 2.1 PBSMT Software Several software packages are available for training PBSMT systems. In this work, we use Moses (Koehn et al., 2007) and Portage (Ueffing et al., 2007), two sta"
2012.eamt-1.65,N09-1047,0,0.150989,"Missing"
2012.eamt-1.65,2005.mtsummit-papers.11,0,0.074031,"Missing"
2012.eamt-1.65,P07-2045,0,0.0170712,"Note that alternate models such as hierarchical (Chiang, 2007) or syntax based (Zollman and Venugopal, 2006) have been developed and could also be studied. However their use on the large scale necessary for creating accurate learning curves would require solving a number of practical issues and we focus instead on the straight PBSMT approach, which has been shown in recent MT evaluations (Callison-Burch et al., 2009; Callison-Burch et al., 2011) to offer competitive performance. 2.1 PBSMT Software Several software packages are available for training PBSMT systems. In this work, we use Moses (Koehn et al., 2007) and Portage (Ueffing et al., 2007), two state-of-the-art systems capable of learning translation tables, language models and decoding parameters from one or several parallel corpora. Moses is a complete open-source phrase-based translation toolkit available for academic purposes, while Portage is a similar package, available to partners of the National Research Council Canada. Given a parallel training corpus, both perform basic preprocessing (tokenization, lowercasing, etc.) if necessary, and build the various components of the model. Both use standard external tools for training the languag"
2012.eamt-1.65,N03-1017,0,0.0407541,"Missing"
2012.eamt-1.65,2005.mtsummit-tutorials.1,0,0.180665,"ion model is more important than to the language model (αT M > αLM ). The values of αLM and αT M vary across the test sets, and correspond to an increase of 1 to 1.3 BLEU point per doubling of the training data for the LM and 1.2 to 1.8 BLEU point per doubling for the TM. However, the ratio is rather stable, indicating that the relative importance of the TM w.r.t. the LM is stable across domains. Not surprisingly, the more similar the test set is to the training data, the larger is the BLEU point growth. Our results are qualitatively compatible with the observations reported in a tutorial by (Och, 2005), although the increments in BLEU with each doubling of the training data size are reported 0.5 and 2.5 points for the language and translation models, respectively, in the context of Arabic-English translation. The ratio we observed in our experiments is lot more favourable to the language model. In order to validate this finding, we performed 310 Test Set Giga Emea News 2009 αLM 0.0133 0.0134 0.0097 αT M 0.0182 0.0168 0.0122 αT M /αLM 1.368 1.2563 1.2532 Table 2: Empirical estimation of the contributions αLM and αT M of the LM and TM, respectively, ( is smaller than 1×10−4 ), in BLEU per lo"
2012.eamt-1.65,C10-1124,0,0.0193111,"ifferent conditions: very different language pairs, in-domain and outof-domain data, differing level of corpus homogeneity. etc. We emphasize that obtaining systematic and accurate learning curves requires a significant effort, even with an high performance computing architecture (Figure 2 requires translating more than 3 million test sentences with 91 models). The learning curves obtained here suggest that, on an absolute (linear) scale, performance gains per fixed amount of additional data decrease. The diminishing improvements in performance after an early fast growth was also reported by (Uszkoreit et al., 2010) who mined the Web to extract very large sets of parallel documents. Starting with two corpora (French/Spanish to English) similar in dimensions to the Giga training set and using the News 2009 test sets, they report that adding more than 4,800 M words from a different domain resulted in relative small performance gains (< 2 BLEU points). On a log-scale, on the other hand, there is no sign that performance gains decrease as we keep doubling the training corpus size, at least up to 20M sentence pairs. Note that although usual 311 MT metrics have natural bounds (0 for error-based metrics such as"
2012.eamt-1.65,W06-3119,0,0.0276421,"estimation is crucial for both the translation and language model features. Conditional probabilities are estimated from a large training corpus using empirical counts and various smoothing strategies. In addition, the weights λi are also estimated from a (usually disjoint) corpus of source and target sentence pairs. The size and composition of the training data will therefore have an influence on the quality of the predictions b e through the estimation of both the log-linear parameters and the feature functions. Note that alternate models such as hierarchical (Chiang, 2007) or syntax based (Zollman and Venugopal, 2006) have been developed and could also be studied. However their use on the large scale necessary for creating accurate learning curves would require solving a number of practical issues and we focus instead on the straight PBSMT approach, which has been shown in recent MT evaluations (Callison-Burch et al., 2009; Callison-Burch et al., 2011) to offer competitive performance. 2.1 PBSMT Software Several software packages are available for training PBSMT systems. In this work, we use Moses (Koehn et al., 2007) and Portage (Ueffing et al., 2007), two state-of-the-art systems capable of learning tran"
2012.eamt-1.65,2002.tmi-tutorials.2,0,0.078084,"Missing"
2012.eamt-1.65,W09-0401,0,\N,Missing
2012.eamt-1.65,W11-2103,0,\N,Missing
2020.coling-main.576,P14-2048,0,0.04731,"Missing"
2020.coling-main.576,P13-1157,0,0.0293958,"be seen in most examples of Figure 2. This suggests that alignment features in the bilingual task could be useful. T- MOP explicitly captures alignment information, but does not seem to make good use of it. We were otherwise impressed 6560 Figure 1: Cumulative accuracy in the bilingual task calculated over the number of target sentences produced by the XLM engine, sorted by their number of tokens. by the overall quality of the MT, and rapidly realized how difficult it would be for human annotators to achieve a decent level of performance on this task. This is in line with the observations of Arase and Zhou (2013), who report lower performances for humans than for machines at detecting translations produced by statistical phrase-based MT. To better understand the type of information our classifiers base their decisions on, we inspected cases where our classifiers predominantly classified the human translations as such20 , and the machine translation counterpart is predominantly recognized as a machine translation. For 32 such cases randomly selected, we manually produced minimal pairs (3 on average), that is, as small as possible variants of the automatic translation, to see at which point the classifi"
2020.coling-main.576,Q19-1038,0,0.0462084,"Missing"
2020.coling-main.576,W15-5202,0,0.0189346,"e-translated material of our training corpus: two left-to-right models, and two right-to-left ones. We computed 18 features: ratios of min and max logprob over the (target) sentence per model (four features), the number of tokens with a logprob less than {mean, max, −6} (three features per model), as well as the logprob of the full sentence given by the left-to-right models (two features). T- MOP T- MOP (Jalili Sabet et al., 2016) is a translation memory cleaning tool which computes 27 features for detecting spurious sentence pairs, including broad features (such as length ratio) adapted from Barbu (2015), some based on IBM models computed by MGIZA++ (Gao and Vogel, 2008), as well We used the fairseq-interactive module of the fairseq-py toolkit10 . Very specific rules such as replace(’ ;’,’;’) or replace(’https :’,’https:’). 13 Larger vocabularies do not yield notable performance differences. 11 12 6556 as some features based on multilingual word embeddings, using the method proposed by Søgaard et al. (2015). While in T- MOP, those features are aggregated in an unsupervised way (that is, with rules), we instead pass them to a random forest classifier trained specifically to distinguish human f"
2020.coling-main.576,W08-0509,0,0.012046,"ght models, and two right-to-left ones. We computed 18 features: ratios of min and max logprob over the (target) sentence per model (four features), the number of tokens with a logprob less than {mean, max, −6} (three features per model), as well as the logprob of the full sentence given by the left-to-right models (two features). T- MOP T- MOP (Jalili Sabet et al., 2016) is a translation memory cleaning tool which computes 27 features for detecting spurious sentence pairs, including broad features (such as length ratio) adapted from Barbu (2015), some based on IBM models computed by MGIZA++ (Gao and Vogel, 2008), as well We used the fairseq-interactive module of the fairseq-py toolkit10 . Very specific rules such as replace(’ ;’,’;’) or replace(’https :’,’https:’). 13 Larger vocabularies do not yield notable performance differences. 11 12 6556 as some features based on multilingual word embeddings, using the method proposed by Søgaard et al. (2015). While in T- MOP, those features are aggregated in an unsupervised way (that is, with rules), we instead pass them to a random forest classifier trained specifically to distinguish human from machine translations. Because of the nature of the feature set,"
2020.coling-main.576,C18-1122,0,0.036047,"Missing"
2020.coling-main.576,P13-2121,0,0.0150216,"k most frequent character n-grams in the MT output of our training material, with n ranging from 2 to 7.13 Each sentence is then encoded by the frequency of the terms in this vocabulary, thus leading to a large sparse representation which is passed to a classifier. In the bilingual task, we also consider the top 30k n-grams of the source-language version of the training corpus, leading to representations of 60k dimensions. KEN LM As a point of comparison, in the monolingual task, we experimented with features extracted from four {3, 4}-gram word language models trained with the kenLM package (Heafield et al., 2013) on the machine-translated material of our training corpus: two left-to-right models, and two right-to-left ones. We computed 18 features: ratios of min and max logprob over the (target) sentence per model (four features), the number of tokens with a logprob less than {mean, max, −6} (three features per model), as well as the logprob of the full sentence given by the left-to-right models (two features). T- MOP T- MOP (Jalili Sabet et al., 2016) is a translation memory cleaning tool which computes 27 features for detecting spurious sentence pairs, including broad features (such as length ratio)"
2020.coling-main.576,P16-4009,0,0.025357,"Missing"
2020.coling-main.576,P07-2045,0,0.00571854,"orpus OSCAR (Ortiz Su´arez et al., 2019). Unlike RoBERTa, CamemBERT uses sentence piece tokenization (Kudo and Richardson, 2018) and performs whole word masking, which has been shown to be preferable (Joshi et al., 2019). The architecture of the base model is a multi-layer bidirectional transformer (Devlin et al., 2018b; Vaswani et al., 2017) with 12 transformer blocks of hidden size 768 and 12 self attention heads. FlauBERT (Le et al., 2019) The base model we used is trained on 71GB of publicly available French data and the data was pre-processed and tokenized using a basic French tokenizer (Koehn et al., 2007). The model was trained with the MLM training objective. XLM-RO BERTA (Ruder et al., 2019) is a multilingual language model, trained on 100 different languages. It is an extended version of XLM (see Section 4.1). mBERT (Devlin et al., 2018b) is very similar to the original BERT model with 12 layers of bidirectional transformers, but released as a single language model trained on 104 separate languages from Wikipedia pages, with a shared word piece vocabulary. The model does not use any marker for input language and the pre-trained model is not made to extract translation pairs to have similar"
2020.coling-main.576,D18-2012,0,0.0244325,"l classification task, on all test sets. X, F, D, and GT refer to the XLM, FairSeq, DeepL, and Google translation engines, respectively. Underlined scores are produced by classifiers trained with XLM material; FairSeq material has been used otherwise. CamemBERT (Martin et al., 2019) is based on the RoBERTa (Liu et al., 2019) architecture (which is basically a BERT model with improved hyper-parameters for robust performance) and is trained on 138GB of plain French text taken from multilingual corpus OSCAR (Ortiz Su´arez et al., 2019). Unlike RoBERTa, CamemBERT uses sentence piece tokenization (Kudo and Richardson, 2018) and performs whole word masking, which has been shown to be preferable (Joshi et al., 2019). The architecture of the base model is a multi-layer bidirectional transformer (Devlin et al., 2018b; Vaswani et al., 2017) with 12 transformer blocks of hidden size 768 and 12 self attention heads. FlauBERT (Le et al., 2019) The base model we used is trained on 71GB of publicly available French data and the data was pre-processed and tokenized using a basic French tokenizer (Koehn et al., 2007). The model was trained with the MLM training objective. XLM-RO BERTA (Ruder et al., 2019) is a multilingual"
2020.coling-main.576,Y15-2041,0,0.0614647,"Missing"
2020.coling-main.576,W18-6301,0,0.0139195,"he TM are into French, we focus on this language direction. Our goal is to build classifiers that determine if a translation is human or machine-made. For this, we need training data that contains both types of translations. We create such data by machine translating a subset of 530k sentence pairs, randomly sampled from the TM. These machine translations are performed using two different neural MT systems, themselves trained using a distinct subset of 5.8M sentence pairs, also randomly sampled from the TM.2 These two MT systems, one based on XLM (Lample and Conneau, 2019) and one on FairSeq (Ott et al., 2018), are detailed in Section 4. Thus, two distinct classifier training sets are created, one from each MT system: each contains 530k human translations and 530k machine translations, totalling 1.06M examples. 1 2 http://www.statmt.org/europarl All sampling in the TM was done in such a way as to ensure comparable representations of each domain. 6554 We proceed similarly to produce test sets to evaluate the performance of our classifiers: we randomly sample 10k sentence pairs from the TM, machine translate the English versions into French using our XLM and FairSeq MT systems, thus creating two test"
2020.coling-main.576,P19-4007,0,0.0374539,"Missing"
2020.coling-main.576,P16-1162,0,0.0382988,"NMT systems using English-French texts from the TM. We provide the details of this process here. 4.1 Cross-lingual Language Model (XLM) In Lample and Conneau (2019), the authors propose three models: two unsupervised ones that do not use sentence pairs in translation relation, and a supervised one that does. We focus on the third model, called the Translation Language Modeling (TLM) which tackles cross-lingual pre-training in a way similar to the BERT model (Devlin et al., 2018a) with notable differences. First, XLM is based on a shared source-target vocabulary using Byte Pair Encoding (BPE) (Sennrich et al., 2016). We used the 60k BPE vocabulary which comes with the pre-trained language model.7 Second, XLM is trained to predict both source and target masked words, leveraging both source and target contexts, encouraging the model to align the source and target representations. Third, XLM stores the ID for the language and the token order (i.e., positional encoding) in both languages which builds a relationship between related tokens in the two languages. During training and when translating, we use a beam search of width 6 and a length penalty of 1. XLM is implemented in PyTorch8 and supports distribute"
2020.coling-main.576,P15-1165,0,0.0361127,"Missing"
2020.vardial-1.26,W19-1402,1,0.523782,"Missing"
2020.vardial-1.26,N19-1423,0,0.409378,"ant languages from a large set of non-relevant languages and discriminating between these languages. To solve this task, we used an approach similar to the one we developed for the cuneiform language identification shared task at VarDial 2019 (Bernier-Colborne et al., 2019). This approach was ranked first in that competition, which was one of the first times a language identification shared task was won by a neural network (Zampieri et al., 2019). It is a deep learning approach based on character embeddings and a transformer network (Vaswani et al., 2017) trained in a similar fashion to BERT (Devlin et al., 2019). On this year’s ULI shared task, we did not have the same level of success using this neural approach to language identification, as our accuracy scores on the held-out test set were significantly lower than the baseline scores. In this paper, we explore the challenges presented by the ULI task and explain the reasons for the low accuracy scores achieved by our model in the official evaluation. We then show that neural language identification can produce much better results if trained and tuned correctly. 2 Task Definition and Data The goal of this language identification task is simply to id"
2020.vardial-1.26,goldhahn-etal-2012-building,0,0.0987902,"Missing"
2020.vardial-1.26,2020.vardial-1.1,0,0.340746,"Missing"
2020.vardial-1.26,W17-0221,0,0.192112,"Missing"
2021.vardial-1.15,W19-1402,1,0.693559,"tasks (Goutte and Léger, 2016) . For more details on this classification algorithm, refer to Goutte et al. (2014, Sec. 2.2). For reference, running inference on the official test, which contains over 1.5 million examples, takes about four and a half hours using four Intel Xeon CPUs @ 2.6 GHz, and could be reduced by using more CPUs. Training takes about five and a half hours. Memory requirement is below 32 GB. 3.2 Transformer The second approach is a deep learning approach, which the NRC team has previously applied, with very different levels of success, to Cuneiform language identification (Bernier-Colborne et al., 2019) and Uralic language identification (BernierColborne and Goutte, 2020). In the former case, it produced a winning submission, whereas in the latter, it was well under the baseline, because of a flaw in the methods used to sample training and evaluation data. The model is a deep neural network which takes sequences of characters as input. Characters are embedded and fed through a stack of bidirectional transformers (Vaswani et al., 2017), which encodes the sequence. The output of this encoder is a sequence of hidden state vectors (one per input character), which is then fed to various output he"
2021.vardial-1.15,N19-1423,0,0.21554,"ate 29 Uralic language varieties, among a total of 178 language varieties from various families, given a short text (typically a sentence). This was a re-run of the ULI task at VarDial 2020 (G˘aman et al., 2020). We experimented with two different approaches to the ULI task. The first is a probabilistic classifier that exploits only character 5-grams as features. The second is a deep learning approach based on character embeddings and a transformer network (Vaswani et al., 2017), which is first pretrained through self-supervision, then fine-tuned on the ULI task, in a similar fashion to BERT (Devlin et al., 2019). This second approach is essentially an improved version of the one developed by the NRC team for the first run of the ULI task (Bernier-Colborne and Goutte, 2020). By improving the sampling functions used to sample training and evaluation data, and making a few other small changes to the Data and Task Definition The ULI data and task are described by Jauhiainen et al. (2020). The goal of the task is to identify the language of a short text, typically a single sentence. If more than one language is used (e.g. code switching), the main language must be identified. This was a closed task, so th"
2021.vardial-1.15,W16-4823,1,0.813178,"ment set, which, while a valid design choice for this competition, added a degree of complexity. 3 Models We evaluated two approaches to this task, which we describe in this section. 3.1 Probcat The first approach employs a probabilistic classifier (Gaussier et al., 2002), that we call Probcat, which we trained using character 5-grams as features. The classifier is similar to multinomial Naive Bayes except that it does not assume that all n-grams in a given text are generated from a single class. It has been used in the past to obtain state-of-the-art results on language identification tasks (Goutte and Léger, 2016) . For more details on this classification algorithm, refer to Goutte et al. (2014, Sec. 2.2). For reference, running inference on the official test, which contains over 1.5 million examples, takes about four and a half hours using four Intel Xeon CPUs @ 2.6 GHz, and could be reduced by using more CPUs. Training takes about five and a half hours. Memory requirement is below 32 GB. 3.2 Transformer The second approach is a deep learning approach, which the NRC team has previously applied, with very different levels of success, to Cuneiform language identification (Bernier-Colborne et al., 2019)"
2021.vardial-1.15,W14-5316,1,0.873409,"omplexity. 3 Models We evaluated two approaches to this task, which we describe in this section. 3.1 Probcat The first approach employs a probabilistic classifier (Gaussier et al., 2002), that we call Probcat, which we trained using character 5-grams as features. The classifier is similar to multinomial Naive Bayes except that it does not assume that all n-grams in a given text are generated from a single class. It has been used in the past to obtain state-of-the-art results on language identification tasks (Goutte and Léger, 2016) . For more details on this classification algorithm, refer to Goutte et al. (2014, Sec. 2.2). For reference, running inference on the official test, which contains over 1.5 million examples, takes about four and a half hours using four Intel Xeon CPUs @ 2.6 GHz, and could be reduced by using more CPUs. Training takes about five and a half hours. Memory requirement is below 32 GB. 3.2 Transformer The second approach is a deep learning approach, which the NRC team has previously applied, with very different levels of success, to Cuneiform language identification (Bernier-Colborne et al., 2019) and Uralic language identification (BernierColborne and Goutte, 2020). In the form"
2021.vardial-1.15,2020.vardial-1.1,0,0.480508,"Missing"
2021.vardial-1.15,W17-1212,0,0.0269418,"sampling function and early stopping) to track 1, and two of which were tuned to track 3: • Track 1, run 1: frequency-based sampling (with α = 0.75, γ = 0.5), early stopped for track 1. • Track 1, run 2: accuracy-based sampling (with prank ), early stopped for track 1. • Track 3, run 1: frequency-based sampling (with α = 0.75, γ = 0.5), early stopped for track 3. • Track 3, run 2: accuracy-based sampling (with pinv ), early stopped for track 3. 5 Official Results The scores of our best runs on the official test set are shown in Table 3. The baseline scores were computed using the HeLI method (Jauhiainen et al., 2017). Probcat was the only system to beat the baseline on track 2, and one of two systems that beat the baseline on track 1 (along with an ensemble of SVM and naive Bayes models exploiting 3-, 4-, and 5-grams, which scored 0.809 on track 1, but only 0.593 on track 2 according to the leaderboard1 • Accuracy-based sampling, using either pinv or prank to convert class-wise dev-scores to 1 http://urn.fi/urn:nbn:fi: lb-2020102201 131 at the time of writing). The baseline on track 3 has yet to be beaten. Our results using the transformer model are competitive on tracks 2 and 3, but less so on track 1, w"
2021.vardial-1.15,D19-1410,0,0.0125134,"e languages used for training, and must therefore have learned some of the specific surface regularities of each. The output head for this task is a softmax over the vocabulary (or alphabet), which takes as input the encoding (i.e. final hidden state) of a masked character, and the loss is cross-entropy. After pre-training, we fine-tune for language identification by keeping the pre-trained encoder, discarding the MLM head, and replacing it with a head containing: an average pooling layer, that averages the final hidden states of the encoder to produce a fixed-length encoding of the sentence (Reimers and Gurevych, 2019); followed by a relu activation; and finally a softmax over the 178 languages. Again, the loss is cross-entropy. The vocabulary (or alphabet) contains every character that appears more than once in the training portion of the train/dev/test split we created (see Section 4.1). Characters that are not in this vocab are replaced with a special symbol reserved for unknown characters, before the encoding stage. The hyperparameter settings we used largely follow the recommendations of Devlin et al. (2019), except that we use fewer layers than their base architecture (to reduce training time and memo"
C04-1046,2003.mtsummit-papers.52,1,\N,Missing
C04-1046,J93-2003,0,\N,Missing
C04-1046,N03-2021,0,\N,Missing
C04-1046,J96-1002,0,\N,Missing
C04-1046,P02-1040,0,\N,Missing
C04-1046,J04-4002,0,\N,Missing
C04-1046,P02-1038,0,\N,Missing
C04-1046,P00-1016,0,\N,Missing
C04-1046,W03-0413,1,\N,Missing
C16-1089,P14-1113,0,0.0280547,"Middle: Pick the middle N keyphrases, These may strike the right balance: some similarity with the rest, i.e. not noisy, but not too similar, i.e. specific to a document. This separation is somewhat crude, but further investigation in Section 4.2 show that further refinements do not yield better performance. 2.3 Hierarchy-based Discrimintive Keyphrase Extraction In order to use hierarchical semantic information to extract discriminative keyphrases, we first need to model the hierarchical information between keyphrases. We generalize the linear projection for hierarchical relations proposed by Fu et al. (2014), by using a Deep Belief Network (DBN) to model this relationship on keyphrase embeddings. A d-dimensional vector for each keyphrase is obtained using again word2vec, as in Section 2.1. The hierarchical information between two keyphrases p and q is then modeled as a binary classification problem: From a 2 × d dimensional input containing the embeddings p and q, the model predicts whether q is a child of p (positive class) or not (negative class). DBNs are deep learning models consisting of multiple layers of hidden variables, often used to obtain abstract representations (e.g., features) for r"
C16-1089,W03-1028,0,0.561144,"ses are a static property of documents, that is, a given document would always produce a fixed set of keyphrases. Many approaches were developped for that purpose. For example, the Keyphrase Extraction Algorithm, or KEA (Witten et al., 1998), uses a supervised learning method (Na¨ıve Bayes) to predict keyphrases based on their lexical features. Turney (2000) developed a genetic algorithm (GenEx) to extract keyphrases, and showed that this outperformed the well-known C4.5 algorithm. More recent work on supervised keyphrase extraction used, e.g., a combination of lexical and syntactic features (Hulth, 2003) or other statistical classifiers such as support vector machine (SVM) (Zhang et al., 2006) or conditional random fields (CRF) (Zhang et al., 2008). Unsupervised methods were also proposed, based on a graph-based ranking model (Mihalcea and Tarau, 2004), or using co-occurrences (Matsuo and Ishizuka, 2004), enriched with WordNet (Martinez-Romo et al., 2016). Unsupervised keyphrase extraction was also applied to shorter texts from twitter, using multiple random walks to topic context (Zhao et al., 2011) or unsupervised feature extraction (Marujo et al., 2015). The use of hierarchical information"
C16-1089,P15-2105,0,0.0246097,"Missing"
C16-1089,P08-1028,0,0.0420522,"|p |is the number of words in p. For example, the embedding for Machine Learning is the average of the vector 1 https://code.google.com/archive/p/word2vec/ 933 representations for Machine and for Learning. The similarity between two phrases p and q is: P p i qi hp, qi cosine(p, q) = p =qP i P hp, pihq, qi ( i p2i )( i qi2 ) (2) where i runs over the dimensions of the chosen embedding space, and h·, ·i is the scalar product notation. Note that despite its simplicity, arithmetic average has been found to be very effective among many alternatives when combining word vectors to represent phrases (Mitchell and Lapata, 2008). 2.2 Similarity-based Discriminative Keyphrase Extraction Now equipped with a similarity between keyphrases, we turn to extracting discriminative keyphrases for a document. In the similarity-based approach, we consider every candidate keyphrase p, and compare it to all other keyphrases from the group of document by computing the average similarity score between p and all other keyphrases q from all documents in the group. In our example, this would be all expertise keyphrases extracted for all researchers in the group considered: sScore(p) = 1 X cosine(p, q), C (3) q∈K where K is the set of k"
C16-1089,P11-1039,0,0.0242735,"ork on supervised keyphrase extraction used, e.g., a combination of lexical and syntactic features (Hulth, 2003) or other statistical classifiers such as support vector machine (SVM) (Zhang et al., 2006) or conditional random fields (CRF) (Zhang et al., 2008). Unsupervised methods were also proposed, based on a graph-based ranking model (Mihalcea and Tarau, 2004), or using co-occurrences (Matsuo and Ishizuka, 2004), enriched with WordNet (Martinez-Romo et al., 2016). Unsupervised keyphrase extraction was also applied to shorter texts from twitter, using multiple random walks to topic context (Zhao et al., 2011) or unsupervised feature extraction (Marujo et al., 2015). The use of hierarchical information to extract keyphrases was explored in (Smatana and Butka, 2016; Berend, 2016). Although these supervised and unsupervised methods achieve improved performance, little work has been done to generate discriminative keyphrases based on other documents in the group. This work is licenced under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 932 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics"
C16-1089,W04-3252,0,\N,Missing
C18-1212,W16-5702,0,0.165887,"Pozdnoukhov and Kaiser, 2011; Ertl et al., 2012; Vavliakis et al., 2012; Lau et al., 2012; Zhou and Chen, 2014), summarization, or finding influential users in social media. However, only few studies used LDA to detect topic changes over time (Lau et al., 2012; Zhou and Chen, 2014). Once a main event is identified, detecting key sub-events is an essential task. For example, during a public health epidemic crisis, detecting turning points in the spread of the disease is extremely important. Relatively little attention has been paid on detecting these change points during an event. For example, Bruggermann et al. (2016) used the dynamic topic model, aka DTM (Blei and Lafferty, 2006), to detect and track stories in news articles. In addition, although many studies use topic models for event detection, very few focus on real-time or sub-event detection. In statistics, change point detection (CPD) is the task of finding locations where the underlying stochastic process governing time series changes. Change-point detection algorithms can be split into two categories: Real-time (or on-line) detection and retrospective (or off-line) detection, depending on how data is used. Most CPD algorithms are retrospective (B"
C18-1212,L18-1277,1,0.779733,"te the performance of the DTM model on topic identification, or whether the turning points from the word-topic distribution discover actual sub-events. Wang and Goutte (2017) evaluated the performance of change point detection algorithms using the temporal profiles of hashtags and frequency of tweets on two twitter data sets. Their work is an early attempt to use CPD algorithms on the content of document streams. Previous studies (Guralnik and Srivastava, 1999; James et al., 2016) detected significant changes from sensor signals but do not use the textual content of message streams. Recently, Goutte et al. (2018) used a similar on-line CPD approach, but applied it on time series recording the sentiment polarity in streams of tweets. 3 Methods In this section, we describe on-line topic modeling, several competing ways to produce time series from the topic models, and several change point detection algorithms. 3.1 On-line Topic Model We use both standard LDA and an on-line version of LDA with infinite vocabulary, oLDA∞ . The basic LDA generates documents from a distribution over topics, each topic having a distribution over words. For D documents over K topics, the generative process is as follows (Blei"
C18-1212,C12-1093,0,0.132311,"ams such as twitter messages has received a lot of attention. The approaches used range from topic modeling, incremental clustering, the concept of interestingness, and others (Hasan et al., 2017). For example, methods relying on topic models detect latent topics from tweets and use that semantic structure to guide the event detection task. In particular, Latent Dirichlet Allocation, aka LDA (Blei et al., 2003), and extensions have been widely used to model topics from large corpora. It has been used for event detection (Pozdnoukhov and Kaiser, 2011; Ertl et al., 2012; Vavliakis et al., 2012; Lau et al., 2012; Zhou and Chen, 2014), summarization, or finding influential users in social media. However, only few studies used LDA to detect topic changes over time (Lau et al., 2012; Zhou and Chen, 2014). Once a main event is identified, detecting key sub-events is an essential task. For example, during a public health epidemic crisis, detecting turning points in the spread of the disease is extremely important. Relatively little attention has been paid on detecting these change points during an event. For example, Bruggermann et al. (2016) used the dynamic topic model, aka DTM (Blei and Lafferty, 2006)"
C18-1212,W17-2702,1,0.70683,"me and location of messages as additional variables, it outperformed oLDA on the tested datasets. In addition to using spatial information on top of LDA, none of the previous studies consider real-time event detection. Other recent work attempt to build storylines from either news articles or social media data. Bruggermann et al. (2016) use the word-topic distribution from the DTM model to represent the changes during events, but did not evaluate the performance of the DTM model on topic identification, or whether the turning points from the word-topic distribution discover actual sub-events. Wang and Goutte (2017) evaluated the performance of change point detection algorithms using the temporal profiles of hashtags and frequency of tweets on two twitter data sets. Their work is an early attempt to use CPD algorithms on the content of document streams. Previous studies (Guralnik and Srivastava, 1999; James et al., 2016) detected significant changes from sensor signals but do not use the textual content of message streams. Recently, Goutte et al. (2018) used a similar on-line CPD approach, but applied it on time series recording the sentiment polarity in streams of tweets. 3 Methods In this section, we d"
D10-1044,N04-4006,0,0.0192611,"s led previous workers to adopt ad hoc linear weighting schemes (Finch and Sumita, 2008; Foster and Kuhn, 2007; L¨u et al., 2007). However, we note that the final conditional estimates p(s|t) from a given phrase table maximize the likelihood of joint empirical phrase pair counts over a word-aligned corpus. This suggests a direct parallel to (1): ! ! αi pi (s|t), (2) α ˆ = argmax p˜(s, t) log α s,t i where p˜(s, t) is a joint empirical distribution extracted from the IN dev set using the standard procedure.2 An alternative form of linear combination is a maximum a posteriori (MAP) combination (Bacchiani et al., 2004). For the TM, this is: p(s|t) = cI (s, t) + β po (s|t) , cI (t) + β (3) where cI (s, t) is the count in the IN phrase table of pair (s, t), po (s|t)""is its probability under the OUT TM, and cI (t) = s! cI (s! , t). This is motivated by taking β po (s|t) to be the parameters of a Dirichlet prior on phrase probabilities, then maximizing posterior estimates p(s|t) given the IN corpus. Intuitively, it places more weight on OUT when less evidence from IN is available. To set β, we used the same criterion as for α, over a dev corpus: βˆ = argmax β ! s,t p˜(s, t) log cI (s, t) + β po (s|t) . cI (t) +"
D10-1044,W09-0432,0,0.316476,"ombining split features. Recent work by Finkel and Manning (2009) which re-casts Daum´e’s approach in a hierarchical MAP framework may be applicable to this problem. Moving beyond directly related work, major themes in SMT adaptation include the IR (Hildebrand et al., 2005; L¨u et al., 2007; Zhao et al., 2004) and mixture (Finch and Sumita, 2008; Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L¨u et al., 2007) approaches for LMs and TMs described above, as well as methods for exploiting monolingual in-domain text, typically by translating it automatically and then performing self training (Bertoldi and Federico, 2009; Ueffing et al., 2007; Schwenk and Senellart, 2009). There has also been some work on adapting the word alignment model prior to phrase extraction (Civera and Juan, 2007; Wu et al., 2005), and on dynamically choosing a dev set (Xu et al., 2007). Other work includes transferring latent topic distributions from source to target language for LM adaptation, (Tam et al., 2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita, 2008). 6 Conclusion In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus"
D10-1044,W07-0722,0,0.0718684,"ond directly related work, major themes in SMT adaptation include the IR (Hildebrand et al., 2005; L¨u et al., 2007; Zhao et al., 2004) and mixture (Finch and Sumita, 2008; Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L¨u et al., 2007) approaches for LMs and TMs described above, as well as methods for exploiting monolingual in-domain text, typically by translating it automatically and then performing self training (Bertoldi and Federico, 2009; Ueffing et al., 2007; Schwenk and Senellart, 2009). There has also been some work on adapting the word alignment model prior to phrase extraction (Civera and Juan, 2007; Wu et al., 2005), and on dynamically choosing a dev set (Xu et al., 2007). Other work includes transferring latent topic distributions from source to target language for LM adaptation, (Tam et al., 2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita, 2008). 6 Conclusion In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance. Each out-of-domain phrase pair is characterized by a set of simple features intended to reflect how useful it will be. The fea"
D10-1044,P07-1033,0,0.119667,"Missing"
D10-1044,W08-0334,0,0.206659,"tive weights are set as follows: ! ! p˜(w, h) log αi pi (w|h), (1) α ˆ = argmax α i w,h where α is a weight vector containing an element αi for each domain (just IN and OUT in our case), pi are the corresponding domain-specific models, and p˜(w, h) is an empirical distribution from a targetlanguage training corpus—we used the IN dev set for this. It is not immediately obvious how to formulate an equivalent to equation (1) for an adapted TM, because there is no well-defined objective for learning TMs from parallel corpora. This has led previous workers to adopt ad hoc linear weighting schemes (Finch and Sumita, 2008; Foster and Kuhn, 2007; L¨u et al., 2007). However, we note that the final conditional estimates p(s|t) from a given phrase table maximize the likelihood of joint empirical phrase pair counts over a word-aligned corpus. This suggests a direct parallel to (1): ! ! αi pi (s|t), (2) α ˆ = argmax p˜(s, t) log α s,t i where p˜(s, t) is a joint empirical distribution extracted from the IN dev set using the standard procedure.2 An alternative form of linear combination is a maximum a posteriori (MAP) combination (Bacchiani et al., 2004). For the TM, this is: p(s|t) = cI (s, t) + β po (s|t) , cI (t)"
D10-1044,N09-1068,0,0.0403739,"t splits each feature into domain-specific and general copies. At first glance, this seems only peripherally related to our work, since the specific/general distinction is made for features rather than instances. However, for multinomial models like our LMs and TMs, there is a one to one correspondence between instances and features, eg the correspondence between a phrase pair (s, t) and its conditional multinomial probability p(s|t). As mentioned above, it is not obvious how to apply Daum´e’s approach to multinomials, which do not have a mechanism for combining split features. Recent work by Finkel and Manning (2009) which re-casts Daum´e’s approach in a hierarchical MAP framework may be applicable to this problem. Moving beyond directly related work, major themes in SMT adaptation include the IR (Hildebrand et al., 2005; L¨u et al., 2007; Zhao et al., 2004) and mixture (Finch and Sumita, 2008; Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L¨u et al., 2007) approaches for LMs and TMs described above, as well as methods for exploiting monolingual in-domain text, typically by translating it automatically and then performing self training (Bertoldi and Federico, 2009; Ueffing et al., 2007; Schwenk and Se"
D10-1044,W07-0717,1,0.846999,"reported in. Phrase-level granularity distinguishes our work from previous work by Matsoukas et al (2009), who weight sentences according to sub-corpus and genre membership. Finally, we make some improvements to baseline approaches. We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set. This is a simple and effective alternative to setting weights discriminatively to maximize a metric such as BLEU. A similar maximumlikelihood approach was used by Foster and Kuhn (2007), but for language models only. For comparison to information-retrieval inspired baselines, eg (L¨u et al., 2007), we select sentences from OUT using language model perplexities from IN. This is a straightforward technique that is arguably better suited to the adaptation task than the standard method of treating representative IN sentences as queries, then pooling the match results. The paper is structured as follows. Section 2 describes our baseline techniques for SMT adaptation, and section 3 describes the instance-weighting approach. Experiments are presented in section 4. Section 5 covers"
D10-1044,W09-0439,1,0.776725,"baseline approach is to concatenate data from IN and OUT. Its success depends on the two domains being relatively close, and on the OUT corpus not being so large as to overwhelm the contribution of IN. When OUT is large and distinct, its contribution can be controlled by training separate IN and OUT models, and weighting their combination. An easy way to achieve this is to put the domain-specific LMs and TMs into the top-level log-linear model and learn optimal weights with MERT (Och, 2003). This has the potential drawback of increasing the number of features, which can make MERT less stable (Foster and Kuhn, 2009). 2.2 Linear Combinations Apart from MERT difficulties, a conceptual problem with log-linear combination is that it multiplies feature probabilities, essentially forcing different features to agree on high-scoring candidates. This is appropriate in cases where it is sanctioned by Bayes’ law, such as multiplying LM and TM probabilities, but for adaptation a more suitable framework is often a mixture model in which each event may be generated from some domain. This leads to a linear combination of domain-specific probabilities, with weights in [0, 1], normalized to sum to 1. Linear weights are d"
D10-1044,2005.eamt-1.19,0,0.676063,"rom IN is available. To set β, we used the same criterion as for α, over a dev corpus: βˆ = argmax β ! s,t p˜(s, t) log cI (s, t) + β po (s|t) . cI (t) + β 2 Using non-adapted IBM models trained on all available IN and OUT data. 453 The MAP combination was used for TM probabilities only, in part due to a technical difficulty in formulating coherent counts when using standard LM smoothing techniques (Kneser and Ney, 1995).3 2.3 Sentence Selection Motivated by information retrieval, a number of approaches choose “relevant” sentence pairs from OUT by matching individual source sentences from IN (Hildebrand et al., 2005; L¨u et al., 2007), or individual target hypotheses (Zhao et al., 2004). The matching sentence pairs are then added to the IN corpus, and the system is re-trained. Although matching is done at the sentence level, this information is subsequently discarded when all matches are pooled. To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model. The number of top-ranked pairs to retain is chosen to optimize dev-set BLEU score. 3 Instance We"
D10-1044,P07-1034,0,0.585228,"ss effective in our setting, where IN and OUT are disparate. The idea of distinguishing between general and domain-specific examples is due to Daum´e and Marcu (2006), who used a maximum-entropy model with latent variables to capture the degree of specificity. Daum´e (2007) applies a related idea in a simpler way, by splitting features into general and domain-specific versions. This highly effective approach is not directly applicable to the multinomial models used for core SMT components, which have no natural method for combining split features, so we rely on an instance-weighting approach (Jiang and Zhai, 2007) to downweight domain-specific examples in OUT. Within this framework, we use features intended to capture degree of generality, including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples. Our second contribution is to apply instance 451 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 451–459, c MIT, Massachusetts, USA, 9-11 October 2010. !2010 Crown in Right of Canada. weighting at the level of phrase pairs. Sentence pairs are the natural instances for SMT, but sentences often contain a mix of do"
D10-1044,W07-0733,0,0.744354,"es, eg the correspondence between a phrase pair (s, t) and its conditional multinomial probability p(s|t). As mentioned above, it is not obvious how to apply Daum´e’s approach to multinomials, which do not have a mechanism for combining split features. Recent work by Finkel and Manning (2009) which re-casts Daum´e’s approach in a hierarchical MAP framework may be applicable to this problem. Moving beyond directly related work, major themes in SMT adaptation include the IR (Hildebrand et al., 2005; L¨u et al., 2007; Zhao et al., 2004) and mixture (Finch and Sumita, 2008; Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L¨u et al., 2007) approaches for LMs and TMs described above, as well as methods for exploiting monolingual in-domain text, typically by translating it automatically and then performing self training (Bertoldi and Federico, 2009; Ueffing et al., 2007; Schwenk and Senellart, 2009). There has also been some work on adapting the word alignment model prior to phrase extraction (Civera and Juan, 2007; Wu et al., 2005), and on dynamically choosing a dev set (Xu et al., 2007). Other work includes transferring latent topic distributions from source to target language for LM adaptation, (Tam et al.,"
D10-1044,N03-1017,0,0.0138788,"27 1,894 1,664 1,357 Table 1: Corpora 8 www.itl.nist.gov/iad/mig//tests/mt/2009 456 The reference medicine for Silapo is EPREX/ERYPO, which contains epoetin alfa. Le m´edicament de r´ef´erence de Silapo est EPREX/ERYPO, qui contient de l’´epo´etine alfa. — I would also like to point out to commissioner Liikanen that it is not easy to take a matter to a national court. Je voudrais pr´eciser, a` l’adresse du commissaire Liikanen, qu’il n’est pas ais´e de recourir aux tribunaux nationaux. Figure 1: Sentence pairs from EMEA (top) and Europarl text. We used a standard one-pass phrase-based system (Koehn et al., 2003), with the following features: relative-frequency TM probabilities in both directions; a 4-gram LM with Kneser-Ney smoothing; word-displacement distortion model; and word count. Feature weights were set using Och’s MERT algorithm (Och, 2003). The corpus was wordaligned using both HMM and IBM2 models, and the phrase table was the union of phrases extracted from these separate alignments, with a length limit of 7. It was filtered to retain the top 30 translations for each source phrase using the TM part of the current log-linear model. 4.2 Results Table 2 shows results for both settings and all"
D10-1044,D07-1036,0,0.765764,"Missing"
D10-1044,D09-1074,0,0.764711,"s in Natural Language Processing, pages 451–459, c MIT, Massachusetts, USA, 9-11 October 2010. !2010 Crown in Right of Canada. weighting at the level of phrase pairs. Sentence pairs are the natural instances for SMT, but sentences often contain a mix of domain-specific and general language. For instance, the sentence Similar improvements in haemoglobin levels were reported in the scientific literature for other epoetins would likely be considered domain-specific despite the presence of general phrases like were reported in. Phrase-level granularity distinguishes our work from previous work by Matsoukas et al (2009), who weight sentences according to sub-corpus and genre membership. Finally, we make some improvements to baseline approaches. We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set. This is a simple and effective alternative to setting weights discriminatively to maximize a metric such as BLEU. A similar maximumlikelihood approach was used by Foster and Kuhn (2007), but for language models only. For comparison to information-retrieval inspired bas"
D10-1044,P03-1021,0,0.0597961,"dure for generating the phrase table from which the TM distributions are derived. 2.1 Simple Baselines The natural baseline approach is to concatenate data from IN and OUT. Its success depends on the two domains being relatively close, and on the OUT corpus not being so large as to overwhelm the contribution of IN. When OUT is large and distinct, its contribution can be controlled by training separate IN and OUT models, and weighting their combination. An easy way to achieve this is to put the domain-specific LMs and TMs into the top-level log-linear model and learn optimal weights with MERT (Och, 2003). This has the potential drawback of increasing the number of features, which can make MERT less stable (Foster and Kuhn, 2009). 2.2 Linear Combinations Apart from MERT difficulties, a conceptual problem with log-linear combination is that it multiplies feature probabilities, essentially forcing different features to agree on high-scoring candidates. This is appropriate in cases where it is sanctioned by Bayes’ law, such as multiplying LM and TM probabilities, but for adaptation a more suitable framework is often a mixture model in which each event may be generated from some domain. This leads"
D10-1044,2009.mtsummit-posters.17,0,0.0173592,"Manning (2009) which re-casts Daum´e’s approach in a hierarchical MAP framework may be applicable to this problem. Moving beyond directly related work, major themes in SMT adaptation include the IR (Hildebrand et al., 2005; L¨u et al., 2007; Zhao et al., 2004) and mixture (Finch and Sumita, 2008; Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L¨u et al., 2007) approaches for LMs and TMs described above, as well as methods for exploiting monolingual in-domain text, typically by translating it automatically and then performing self training (Bertoldi and Federico, 2009; Ueffing et al., 2007; Schwenk and Senellart, 2009). There has also been some work on adapting the word alignment model prior to phrase extraction (Civera and Juan, 2007; Wu et al., 2005), and on dynamically choosing a dev set (Xu et al., 2007). Other work includes transferring latent topic distributions from source to target language for LM adaptation, (Tam et al., 2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita, 2008). 6 Conclusion In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance. Each out"
D10-1044,P07-1066,0,0.266726,"in (IN) for which a smaller amount of parallel There is a fairly large body of work on SMT adaptation. We introduce several new ideas. First, we aim to explicitly characterize examples from OUT as belonging to general language or not. Previous approaches have tried to find examples that are similar to the target domain. This is less effective in our setting, where IN and OUT are disparate. The idea of distinguishing between general and domain-specific examples is due to Daum´e and Marcu (2006), who used a maximum-entropy model with latent variables to capture the degree of specificity. Daum´e (2007) applies a related idea in a simpler way, by splitting features into general and domain-specific versions. This highly effective approach is not directly applicable to the multinomial models used for core SMT components, which have no natural method for combining split features, so we rely on an instance-weighting approach (Jiang and Zhai, 2007) to downweight domain-specific examples in OUT. Within this framework, we use features intended to capture degree of generality, including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples. Our second c"
D10-1044,P07-1004,0,0.160253,"nt work by Finkel and Manning (2009) which re-casts Daum´e’s approach in a hierarchical MAP framework may be applicable to this problem. Moving beyond directly related work, major themes in SMT adaptation include the IR (Hildebrand et al., 2005; L¨u et al., 2007; Zhao et al., 2004) and mixture (Finch and Sumita, 2008; Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L¨u et al., 2007) approaches for LMs and TMs described above, as well as methods for exploiting monolingual in-domain text, typically by translating it automatically and then performing self training (Bertoldi and Federico, 2009; Ueffing et al., 2007; Schwenk and Senellart, 2009). There has also been some work on adapting the word alignment model prior to phrase extraction (Civera and Juan, 2007; Wu et al., 2005), and on dynamically choosing a dev set (Xu et al., 2007). Other work includes transferring latent topic distributions from source to target language for LM adaptation, (Tam et al., 2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita, 2008). 6 Conclusion In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve i"
D10-1044,P05-1058,0,0.0263712,"rk, major themes in SMT adaptation include the IR (Hildebrand et al., 2005; L¨u et al., 2007; Zhao et al., 2004) and mixture (Finch and Sumita, 2008; Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L¨u et al., 2007) approaches for LMs and TMs described above, as well as methods for exploiting monolingual in-domain text, typically by translating it automatically and then performing self training (Bertoldi and Federico, 2009; Ueffing et al., 2007; Schwenk and Senellart, 2009). There has also been some work on adapting the word alignment model prior to phrase extraction (Civera and Juan, 2007; Wu et al., 2005), and on dynamically choosing a dev set (Xu et al., 2007). Other work includes transferring latent topic distributions from source to target language for LM adaptation, (Tam et al., 2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita, 2008). 6 Conclusion In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance. Each out-of-domain phrase pair is characterized by a set of simple features intended to reflect how useful it will be. The features are weighted"
D10-1044,2007.mtsummit-papers.68,0,0.0959361,"rand et al., 2005; L¨u et al., 2007; Zhao et al., 2004) and mixture (Finch and Sumita, 2008; Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L¨u et al., 2007) approaches for LMs and TMs described above, as well as methods for exploiting monolingual in-domain text, typically by translating it automatically and then performing self training (Bertoldi and Federico, 2009; Ueffing et al., 2007; Schwenk and Senellart, 2009). There has also been some work on adapting the word alignment model prior to phrase extraction (Civera and Juan, 2007; Wu et al., 2005), and on dynamically choosing a dev set (Xu et al., 2007). Other work includes transferring latent topic distributions from source to target language for LM adaptation, (Tam et al., 2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita, 2008). 6 Conclusion In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance. Each out-of-domain phrase pair is characterized by a set of simple features intended to reflect how useful it will be. The features are weighted within a logistic model to give an overall weight that i"
D10-1044,C04-1059,0,0.614506,"Missing"
D10-1044,D08-1076,0,\N,Missing
H05-1095,J93-2003,0,0.00752012,"asured with the NIST evaluation metric. Translations are produced by means of a beam-search decoder. Experimental results are presented, that demonstrate how the proposed method allows to better generalize from the training data. 1 Arne Mauser RWTH Aachen University arne.mauser@rwth-aachen.de 2 Introduction Possibly the most remarkable evolution of recent years in statistical machine translation is the step from word-based models to phrase-based models (Och et al., 1999; Marcu and Wong, 2002; Yamada and Knight, 2002; Tillmann and Xia, 2003). While in traditional word-based statistical models (Brown et al., 1993) the atomic unit that translation operates on is the word, phrase-based methods acknowledge the significant role played in language by multiword expressions, thus incorporating in a statistical framework the insight behind Example-Based Machine Translation (Somers, 1999). However, Phrase-based models proposed so far only deal with multi-word units that are sequences Non-contiguous phrases Why should it be a good thing to use phrases composed of possibly non-contiguous sequences of words? In doing so we expect to improve translation quality by better accounting for additional linguistic phenome"
H05-1095,P05-1033,0,0.0570595,"ned training corpus. Our approach is based on a direct approximation of the posterior probability Pr (tI1 |sJ1 ), using a loglinear model: Pr (tI1 |sJ1 ) M X Our model currently relies on seven feature functions, which we describe here. (1) 1 1 = exp ZsJ • The compositional bi-phrase feature function hcomp : this is introduced to compensate for ! λm hm (tI1 , sJ1 ) m=1 1 In such a model, the contribution of each feature function hm is determined by the corresponding model parameter λm ; ZsJ denotes a normalization 1 constant. This type of model is now quite widely 757 Recent work from Chiang (Chiang, 2005) addresses similar concerns to those motivating our work by introducing a Synchronous CFG for bi-phrases. If on one hand SCFGs allow to better control the order of the material inserted in the gaps, on the other gap size does not seem to be taken into account, and phrase dovetailing such as the one involving “do want” and “not anymore” in Fig. 2 is disallowed. hbp ’s strong tendency to overestimate the probability of rare bi-phrases; it is computed as in equation (2), except that bi-phrase probabilities are computed based on individual word translation probabilities, somewhat as in IBM mod"
H05-1095,P01-1030,1,0.79189,"uent source phrases, as most phrases have less than 20 translations. 6.3 Experiments The parameters of the model were optimized independantly for each bi-phrase library. In all cases, we performed only 2 iterations of the training procedure, then measured the performance of the system on the test set in terms of the NIST and BLEU scores against one reference translation. As a point of comparison, we also trained an IBM-4 translation model with the GIZA++ toolkit (Och and Ney, 2000), using the combined bi-phrase building and training sets, and translated the test set using the ReWrite decoder (Germann et al., 2001)5 . Table 2 describes the various libraries that were used for our experiments, and the results obtained for each. System/library ReWrite A1 A2 -g0 A2 -g3 B 1 -g0 B1 B 2 -g0 B 2 -g3 B 1 -g1 B 1 -g2 B 1 -g3 B 1 -g4 bi-phrases 238 K 642 K 4.1 M 193 K 267 K 499 K 3.3 M 206 K 213 K 218 K 222 K NIST 6.6838 6.6695 6.7675 6.7068 6.7898 6.9172 6.7290 6.9707 6.8979 6.9406 6.9546 6.9527 BLEU 0.3324 0.3310 0.3363 0.3283 0.3369 0.3407 0.3391 0.3552 0.3441 0.3454 0.3518 0.3423 Table 2: Bi-phrase libraries and results The top part of the table presents the results for the A libraries. As can be seen, librar"
H05-1095,P04-1064,1,0.936967,"iterbi alignments, forward and reverse, enriched with alignments from the union) and then generate bi-phrases by combining together individual alignments that co-occur in the same pair of sentences. This is the strategy that is usually adopted in other phrase-based MT approaches (Zens and Ney, 2003; Och and Ney, 2004). Here, the difference is that we are not restricted to combinations that produce strictly contiguous bi-phrases. The second strategy is to rely on a wordalignment method that naturally produces many-tomany alignments between non-contiguous words, such as the method described in (Goutte et al., 2004). By means of a matrix factorization, this method produces a parallel partition of the two texts, seen as sets of word tokens. Each token therefore belongs to one, and only one, subset within this partition, and corresponding subsets in the source and target make up what are called cepts. For example, in Figure 1, these cepts are represented by the circles numbered 1, 2 and 3; each cept thus connects word tokens in the source and the target, regardless of position or contiguity. These cepts naturally constitute bi-phrases, and can be used directly to produce a biphrase library. Obviously, the"
H05-1095,W02-1018,0,0.0165656,"lso presented that deals such phrases, as well as a training method based on the maximization of translation accuracy, as measured with the NIST evaluation metric. Translations are produced by means of a beam-search decoder. Experimental results are presented, that demonstrate how the proposed method allows to better generalize from the training data. 1 Arne Mauser RWTH Aachen University arne.mauser@rwth-aachen.de 2 Introduction Possibly the most remarkable evolution of recent years in statistical machine translation is the step from word-based models to phrase-based models (Och et al., 1999; Marcu and Wong, 2002; Yamada and Knight, 2002; Tillmann and Xia, 2003). While in traditional word-based statistical models (Brown et al., 1993) the atomic unit that translation operates on is the word, phrase-based methods acknowledge the significant role played in language by multiword expressions, thus incorporating in a statistical framework the insight behind Example-Based Machine Translation (Somers, 1999). However, Phrase-based models proposed so far only deal with multi-word units that are sequences Non-contiguous phrases Why should it be a good thing to use phrases composed of possibly non-contiguous sequ"
H05-1095,P00-1056,0,0.0383638,"equivalents. While the first of these filters typically eliminates a large number of entries, the second only affects the most frequent source phrases, as most phrases have less than 20 translations. 6.3 Experiments The parameters of the model were optimized independantly for each bi-phrase library. In all cases, we performed only 2 iterations of the training procedure, then measured the performance of the system on the test set in terms of the NIST and BLEU scores against one reference translation. As a point of comparison, we also trained an IBM-4 translation model with the GIZA++ toolkit (Och and Ney, 2000), using the combined bi-phrase building and training sets, and translated the test set using the ReWrite decoder (Germann et al., 2001)5 . Table 2 describes the various libraries that were used for our experiments, and the results obtained for each. System/library ReWrite A1 A2 -g0 A2 -g3 B 1 -g0 B1 B 2 -g0 B 2 -g3 B 1 -g1 B 1 -g2 B 1 -g3 B 1 -g4 bi-phrases 238 K 642 K 4.1 M 193 K 267 K 499 K 3.3 M 206 K 213 K 218 K 222 K NIST 6.6838 6.6695 6.7675 6.7068 6.7898 6.9172 6.7290 6.9707 6.8979 6.9406 6.9546 6.9527 BLEU 0.3324 0.3310 0.3363 0.3283 0.3369 0.3407 0.3391 0.3552 0.3441 0.3454 0.3518 0.3"
H05-1095,J03-1002,0,0.00889681,"d on the first “free” position in the target language sentence, i.e. either the leftmost gap, or the right end of the sequence. Figure 2 illustrates this process with an example. To produce translations, our approach therefore relies on a collection of bi-phrases, what we call a bi-phrase library. Such a library is constructed from a corpus of existing translations, aligned at the word level. Two strategies come to mind to produce noncontiguous bi-phrases for these libraries. The first is to align the words using a “standard” word alignement technique, such as the Refined Method described in (Och and Ney, 2003) (the intersection of two IBM Viterbi alignments, forward and reverse, enriched with alignments from the union) and then generate bi-phrases by combining together individual alignments that co-occur in the same pair of sentences. This is the strategy that is usually adopted in other phrase-based MT approaches (Zens and Ney, 2003; Och and Ney, 2004). Here, the difference is that we are not restricted to combinations that produce strictly contiguous bi-phrases. The second strategy is to rely on a wordalignment method that naturally produces many-tomany alignments between non-contiguous words, su"
H05-1095,J04-4002,0,0.0387242,"ting translations, aligned at the word level. Two strategies come to mind to produce noncontiguous bi-phrases for these libraries. The first is to align the words using a “standard” word alignement technique, such as the Refined Method described in (Och and Ney, 2003) (the intersection of two IBM Viterbi alignments, forward and reverse, enriched with alignments from the union) and then generate bi-phrases by combining together individual alignments that co-occur in the same pair of sentences. This is the strategy that is usually adopted in other phrase-based MT approaches (Zens and Ney, 2003; Och and Ney, 2004). Here, the difference is that we are not restricted to combinations that produce strictly contiguous bi-phrases. The second strategy is to rely on a wordalignment method that naturally produces many-tomany alignments between non-contiguous words, such as the method described in (Goutte et al., 2004). By means of a matrix factorization, this method produces a parallel partition of the two texts, seen as sets of word tokens. Each token therefore belongs to one, and only one, subset within this partition, and corresponding subsets in the source and target make up what are called cepts. For examp"
H05-1095,W99-0604,0,0.0809757,"slation model is also presented that deals such phrases, as well as a training method based on the maximization of translation accuracy, as measured with the NIST evaluation metric. Translations are produced by means of a beam-search decoder. Experimental results are presented, that demonstrate how the proposed method allows to better generalize from the training data. 1 Arne Mauser RWTH Aachen University arne.mauser@rwth-aachen.de 2 Introduction Possibly the most remarkable evolution of recent years in statistical machine translation is the step from word-based models to phrase-based models (Och et al., 1999; Marcu and Wong, 2002; Yamada and Knight, 2002; Tillmann and Xia, 2003). While in traditional word-based statistical models (Brown et al., 1993) the atomic unit that translation operates on is the word, phrase-based methods acknowledge the significant role played in language by multiword expressions, thus incorporating in a statistical framework the insight behind Example-Based Machine Translation (Somers, 1999). However, Phrase-based models proposed so far only deal with multi-word units that are sequences Non-contiguous phrases Why should it be a good thing to use phrases composed of possib"
H05-1095,P03-1021,0,0.0338002,"guities they contain. 4 Say we have a set of source-language sentences S. For a given value of λ, we can compute the set of corresponding target-language translations T . Given a set of reference (“gold-standard”) translations R for S and a function E(T, R) which measures the “error” in T relative to R, then we can formulate the parameter estimation problem as2 : Parameter Estimation The values of the λ parameters of the log-linear model can be set so as to optimize a given criterion. For instance, one can maximize the likelyhood of some set of training sentences. Instead, and as suggested by Och (2003), we chose to maximize directly the quality of the translations produced by the system, as measured with a machine translation evaluation metric. 758 As pointed out by Och, one notable difficulty with this approach is that, because the computation of T is based on an argmax operation (see eq. 1), it is not continuous with regard to λ, and standard gradientdescent methods cannot be used to solve the optimization. Och proposes two workarounds to this problem: the first one relies on a direct optimization method derived from Powell’s algorithm; the second introduces a smoothed (continuous) versio"
H05-1095,P02-1040,0,0.111177,"to λ, and standard gradientdescent methods cannot be used to solve the optimization. Och proposes two workarounds to this problem: the first one relies on a direct optimization method derived from Powell’s algorithm; the second introduces a smoothed (continuous) version of the error function E(T, R) and then relies on a gradient-based optimization method. We have opted for this last approach. Och shows how to implement it when the error function can be computed as the sum of errors on individual sentences. Unfortunately, this is not the case for such widely used MT evaluation metrics as BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). We show here how it can be done for NIST; a similar derivation is possible for BLEU. The NIST evaluation metric computes a weighted n-gram precision between T and R, multiplied by a factor B(S, T, R) that penalizes short translations. It can be formulated as: B(S, T, R) × N X n=1 P s∈S In (ts , rs ) P s∈S Cn (ts ) (3) where N is the largest n-gram considered (usually N = 4), In (ts , rs ) is a weighted count of common n-grams between the target (ts ) and reference (rs ) translations of sentence s, and Cn (ts ) is the total number of n-grams in ts . To derive a ver"
H05-1095,N03-2036,0,0.095765,"as a training method based on the maximization of translation accuracy, as measured with the NIST evaluation metric. Translations are produced by means of a beam-search decoder. Experimental results are presented, that demonstrate how the proposed method allows to better generalize from the training data. 1 Arne Mauser RWTH Aachen University arne.mauser@rwth-aachen.de 2 Introduction Possibly the most remarkable evolution of recent years in statistical machine translation is the step from word-based models to phrase-based models (Och et al., 1999; Marcu and Wong, 2002; Yamada and Knight, 2002; Tillmann and Xia, 2003). While in traditional word-based statistical models (Brown et al., 1993) the atomic unit that translation operates on is the word, phrase-based methods acknowledge the significant role played in language by multiword expressions, thus incorporating in a statistical framework the insight behind Example-Based Machine Translation (Somers, 1999). However, Phrase-based models proposed so far only deal with multi-word units that are sequences Non-contiguous phrases Why should it be a good thing to use phrases composed of possibly non-contiguous sequences of words? In doing so we expect to improve t"
H05-1095,P02-1039,1,0.503169,"ls such phrases, as well as a training method based on the maximization of translation accuracy, as measured with the NIST evaluation metric. Translations are produced by means of a beam-search decoder. Experimental results are presented, that demonstrate how the proposed method allows to better generalize from the training data. 1 Arne Mauser RWTH Aachen University arne.mauser@rwth-aachen.de 2 Introduction Possibly the most remarkable evolution of recent years in statistical machine translation is the step from word-based models to phrase-based models (Och et al., 1999; Marcu and Wong, 2002; Yamada and Knight, 2002; Tillmann and Xia, 2003). While in traditional word-based statistical models (Brown et al., 1993) the atomic unit that translation operates on is the word, phrase-based methods acknowledge the significant role played in language by multiword expressions, thus incorporating in a statistical framework the insight behind Example-Based Machine Translation (Somers, 1999). However, Phrase-based models proposed so far only deal with multi-word units that are sequences Non-contiguous phrases Why should it be a good thing to use phrases composed of possibly non-contiguous sequences of words? In doing"
H05-1095,N03-1017,0,0.183385,"rom a corpus of existing translations, aligned at the word level. Two strategies come to mind to produce noncontiguous bi-phrases for these libraries. The first is to align the words using a “standard” word alignement technique, such as the Refined Method described in (Och and Ney, 2003) (the intersection of two IBM Viterbi alignments, forward and reverse, enriched with alignments from the union) and then generate bi-phrases by combining together individual alignments that co-occur in the same pair of sentences. This is the strategy that is usually adopted in other phrase-based MT approaches (Zens and Ney, 2003; Och and Ney, 2004). Here, the difference is that we are not restricted to combinations that produce strictly contiguous bi-phrases. The second strategy is to rely on a wordalignment method that naturally produces many-tomany alignments between non-contiguous words, such as the method described in (Goutte et al., 2004). By means of a matrix factorization, this method produces a parallel partition of the two texts, seen as sets of word tokens. Each token therefore belongs to one, and only one, subset within this partition, and corresponding subsets in the source and target make up what are cal"
H05-1095,N04-1033,0,\N,Missing
L16-1284,W15-5412,0,0.0342684,"Missing"
L16-1284,W15-5410,0,0.477289,"e were many cases of republication (e.g. British texts republished by an American newspaper and tagged as American by the original sources) that made the task for this language group unfeasible.(Zampieri et al., 2014) Closed Open MAC MMS NRC SUKI BOBICEV BRUNIBP PRHLT INRIA NLEL OSEVAL 95.54 95.24 95.24 94.67 94.14 93.66 92.74 83.91 64.04 - 95.65 91.84 76.17 MAC SUKI NRC MMS BOBICEV PRHLT NLEL OSEVAL 94.01 93.02 93.01 92.78 92.22 90.80 62.78 - 93.41 89.56 75.30 System Description Test Set A (Malmasi and Dras, 2015b) (Zampieri et al., 2015a) (Goutte and L´eger, 2015) (Jauhiainen et al., 2015) (Bobicev, 2015) ´ et al., 2015) (Acs (Franco-Salvador et al., 2015) (Fabra-Boluda et al., 2015) Test Set B (Malmasi and Dras, 2015b) (Jauhiainen et al., 2015) (Goutte and L´eger, 2015) (Zampieri et al., 2015a) (Bobicev, 2015) (Franco-Salvador et al., 2015) (Fabra-Boluda et al., 2015) - Table 2: DSL Shared Task 2015 - Accuracy results for open and closed submissions using test sets A and B. Teams ranked by their results in the closed submission. We observe that for all systems, performance dropped from test set A (complete texts) to test set B (name entities removed). This was the expected outcome. However, p"
L16-1284,D14-1069,0,0.0549297,"Zampieri et al., 2015b). The analysis and results of this paper complement the information presented in the shared task reports and provide novel and important information for researchers and developers interested in language identification and particularly in the problem of discriminating between similar languages. 2. Related Work Language identification in written texts is a wellestablished research topic in computational linguistics. Interest in the task is evidenced by early n-gram-based approaches (Dunning, 1994; Grefenstette, 1995) to more recent studies (Brown, 2013; Lui et al., 2014a; Brown, 2014; Sim˜oes et al., 2014). The interest in the discrimination of similar languages, language varieties, and dialects is more recent but it has been growing in the past few years. Examples of studies include the cases of Malay and Indonesian (Ranaivo-Malanc¸on, 2006), Chinese varieties (Huang and Lee, 2008), South Slavic languages (Ljubeˇsi´c et al., 2007; Ljubeˇsi´c and Kranjˇci´c, 2015), Portuguese varieties (Zampieri and Gebre, 2012), Spanish varieties (Zampieri et al., 2013; Maier and G´omez-Rodrıguez, 2014), English varieties (Lui and Cook, 2013), Persian and Dari (Malmasi and Dras, 2015a),"
L16-1284,L16-1522,0,0.0379652,"014). The interest in the discrimination of similar languages, language varieties, and dialects is more recent but it has been growing in the past few years. Examples of studies include the cases of Malay and Indonesian (Ranaivo-Malanc¸on, 2006), Chinese varieties (Huang and Lee, 2008), South Slavic languages (Ljubeˇsi´c et al., 2007; Ljubeˇsi´c and Kranjˇci´c, 2015), Portuguese varieties (Zampieri and Gebre, 2012), Spanish varieties (Zampieri et al., 2013; Maier and G´omez-Rodrıguez, 2014), English varieties (Lui and Cook, 2013), Persian and Dari (Malmasi and Dras, 2015a), Romanian dialects (Ciobanu and Dinu, 2016), and a number of studies on Arabic dialects (Elfardy and Diab, 2014; Zaidan and Callison-Burch, 2014; Tillmann et al., 2014; Malmasi et al., 2015a) A number of shared tasks on language identification have been organized in the recent years ranging from generalpurpose language identification (Baldwin and Lui, 2010) to more specific challenges such as the TweetLID shared task which focused on Twitter data (Zubiaga et al., 2014; Zubiaga et al., 2015), the shared task on Language Identification in Code-Switched Data (Solorio et al., 2014), and the two editions of the discriminating between simila"
L16-1284,W15-5409,0,0.249914,"y an American newspaper and tagged as American by the original sources) that made the task for this language group unfeasible.(Zampieri et al., 2014) Closed Open MAC MMS NRC SUKI BOBICEV BRUNIBP PRHLT INRIA NLEL OSEVAL 95.54 95.24 95.24 94.67 94.14 93.66 92.74 83.91 64.04 - 95.65 91.84 76.17 MAC SUKI NRC MMS BOBICEV PRHLT NLEL OSEVAL 94.01 93.02 93.01 92.78 92.22 90.80 62.78 - 93.41 89.56 75.30 System Description Test Set A (Malmasi and Dras, 2015b) (Zampieri et al., 2015a) (Goutte and L´eger, 2015) (Jauhiainen et al., 2015) (Bobicev, 2015) ´ et al., 2015) (Acs (Franco-Salvador et al., 2015) (Fabra-Boluda et al., 2015) Test Set B (Malmasi and Dras, 2015b) (Jauhiainen et al., 2015) (Goutte and L´eger, 2015) (Zampieri et al., 2015a) (Bobicev, 2015) (Franco-Salvador et al., 2015) (Fabra-Boluda et al., 2015) - Table 2: DSL Shared Task 2015 - Accuracy results for open and closed submissions using test sets A and B. Teams ranked by their results in the closed submission. We observe that for all systems, performance dropped from test set A (complete texts) to test set B (name entities removed). This was the expected outcome. However, performance is, in most cases, only 1 or 2 percentage points lower which is not a"
L16-1284,W15-5403,0,0.727991,".g. British texts republished by an American newspaper and tagged as American by the original sources) that made the task for this language group unfeasible.(Zampieri et al., 2014) Closed Open MAC MMS NRC SUKI BOBICEV BRUNIBP PRHLT INRIA NLEL OSEVAL 95.54 95.24 95.24 94.67 94.14 93.66 92.74 83.91 64.04 - 95.65 91.84 76.17 MAC SUKI NRC MMS BOBICEV PRHLT NLEL OSEVAL 94.01 93.02 93.01 92.78 92.22 90.80 62.78 - 93.41 89.56 75.30 System Description Test Set A (Malmasi and Dras, 2015b) (Zampieri et al., 2015a) (Goutte and L´eger, 2015) (Jauhiainen et al., 2015) (Bobicev, 2015) ´ et al., 2015) (Acs (Franco-Salvador et al., 2015) (Fabra-Boluda et al., 2015) Test Set B (Malmasi and Dras, 2015b) (Jauhiainen et al., 2015) (Goutte and L´eger, 2015) (Zampieri et al., 2015a) (Bobicev, 2015) (Franco-Salvador et al., 2015) (Fabra-Boluda et al., 2015) - Table 2: DSL Shared Task 2015 - Accuracy results for open and closed submissions using test sets A and B. Teams ranked by their results in the closed submission. We observe that for all systems, performance dropped from test set A (complete texts) to test set B (name entities removed). This was the expected outcome. However, performance is, in most cases, only 1 or 2 percentage"
L16-1284,W13-1728,1,0.85986,"A and 4 In the 2015 edition, organizers did not use language group names as in the 2014 edition. We use them for both editions in this paper for the sake of clarity and consistency. 1801 B was MAC (Malmasi and Dras, 2015b) which proposed an ensemble of SVM classifiers for this task. Two other SVMbased approaches were tied in 2nd for test set A, one by the NRC team (Goutte and L´eger, 2015) and MMS (Zampieri et al., 2015a), which experimented with three different approaches and obtained the best results combining TF-IDF and an SVM classifier previously used for native language identification (Gebre et al., 2013). The NRC team included members of NRC-CNRC, winners of the DSL closed submission track in 2014. Both in 2014 and in 2015 they used a two-stage classification approach to predict first the language group and then the language within the predicted group. Two other teams used two-stage classification approaches: NLEL (Fabra-Boluda et al., 2015) and BRUniBP ´ et al., 2015). (Acs A number of computational techniques have been explored in the DSL 2015 including token-based backoff by SUKI team (Jauhiainen et al., 2015), prediction by partial matching (PPM) by BOBICEV (Bobicev, 2015), and word and s"
L16-1284,W15-5413,1,0.877349,"Missing"
L16-1284,W14-5316,1,0.633192,"Missing"
L16-1284,Y08-1042,0,0.049537,"en similar languages. 2. Related Work Language identification in written texts is a wellestablished research topic in computational linguistics. Interest in the task is evidenced by early n-gram-based approaches (Dunning, 1994; Grefenstette, 1995) to more recent studies (Brown, 2013; Lui et al., 2014a; Brown, 2014; Sim˜oes et al., 2014). The interest in the discrimination of similar languages, language varieties, and dialects is more recent but it has been growing in the past few years. Examples of studies include the cases of Malay and Indonesian (Ranaivo-Malanc¸on, 2006), Chinese varieties (Huang and Lee, 2008), South Slavic languages (Ljubeˇsi´c et al., 2007; Ljubeˇsi´c and Kranjˇci´c, 2015), Portuguese varieties (Zampieri and Gebre, 2012), Spanish varieties (Zampieri et al., 2013; Maier and G´omez-Rodrıguez, 2014), English varieties (Lui and Cook, 2013), Persian and Dari (Malmasi and Dras, 2015a), Romanian dialects (Ciobanu and Dinu, 2016), and a number of studies on Arabic dialects (Elfardy and Diab, 2014; Zaidan and Callison-Burch, 2014; Tillmann et al., 2014; Malmasi et al., 2015a) A number of shared tasks on language identification have been organized in the recent years ranging from generalpu"
L16-1284,W15-5408,0,0.33486,"Missing"
L16-1284,W14-5317,0,0.112771,"Missing"
L16-1284,U13-1003,0,0.421039,"more recent studies (Brown, 2013; Lui et al., 2014a; Brown, 2014; Sim˜oes et al., 2014). The interest in the discrimination of similar languages, language varieties, and dialects is more recent but it has been growing in the past few years. Examples of studies include the cases of Malay and Indonesian (Ranaivo-Malanc¸on, 2006), Chinese varieties (Huang and Lee, 2008), South Slavic languages (Ljubeˇsi´c et al., 2007; Ljubeˇsi´c and Kranjˇci´c, 2015), Portuguese varieties (Zampieri and Gebre, 2012), Spanish varieties (Zampieri et al., 2013; Maier and G´omez-Rodrıguez, 2014), English varieties (Lui and Cook, 2013), Persian and Dari (Malmasi and Dras, 2015a), Romanian dialects (Ciobanu and Dinu, 2016), and a number of studies on Arabic dialects (Elfardy and Diab, 2014; Zaidan and Callison-Burch, 2014; Tillmann et al., 2014; Malmasi et al., 2015a) A number of shared tasks on language identification have been organized in the recent years ranging from generalpurpose language identification (Baldwin and Lui, 2010) to more specific challenges such as the TweetLID shared task which focused on Twitter data (Zubiaga et al., 2014; Zubiaga et al., 2015), the shared task on Language Identification in Code-Switche"
L16-1284,Q14-1003,0,0.14435,"ieri et al., 2014; Zampieri et al., 2015b). The analysis and results of this paper complement the information presented in the shared task reports and provide novel and important information for researchers and developers interested in language identification and particularly in the problem of discriminating between similar languages. 2. Related Work Language identification in written texts is a wellestablished research topic in computational linguistics. Interest in the task is evidenced by early n-gram-based approaches (Dunning, 1994; Grefenstette, 1995) to more recent studies (Brown, 2013; Lui et al., 2014a; Brown, 2014; Sim˜oes et al., 2014). The interest in the discrimination of similar languages, language varieties, and dialects is more recent but it has been growing in the past few years. Examples of studies include the cases of Malay and Indonesian (Ranaivo-Malanc¸on, 2006), Chinese varieties (Huang and Lee, 2008), South Slavic languages (Ljubeˇsi´c et al., 2007; Ljubeˇsi´c and Kranjˇci´c, 2015), Portuguese varieties (Zampieri and Gebre, 2012), Spanish varieties (Zampieri et al., 2013; Maier and G´omez-Rodrıguez, 2014), English varieties (Lui and Cook, 2013), Persian and Dari (Malmasi and"
L16-1284,W14-5315,0,0.117061,"ieri et al., 2014; Zampieri et al., 2015b). The analysis and results of this paper complement the information presented in the shared task reports and provide novel and important information for researchers and developers interested in language identification and particularly in the problem of discriminating between similar languages. 2. Related Work Language identification in written texts is a wellestablished research topic in computational linguistics. Interest in the task is evidenced by early n-gram-based approaches (Dunning, 1994; Grefenstette, 1995) to more recent studies (Brown, 2013; Lui et al., 2014a; Brown, 2014; Sim˜oes et al., 2014). The interest in the discrimination of similar languages, language varieties, and dialects is more recent but it has been growing in the past few years. Examples of studies include the cases of Malay and Indonesian (Ranaivo-Malanc¸on, 2006), Chinese varieties (Huang and Lee, 2008), South Slavic languages (Ljubeˇsi´c et al., 2007; Ljubeˇsi´c and Kranjˇci´c, 2015), Portuguese varieties (Zampieri and Gebre, 2012), Spanish varieties (Zampieri et al., 2013; Maier and G´omez-Rodrıguez, 2014), English varieties (Lui and Cook, 2013), Persian and Dari (Malmasi and"
L16-1284,W14-4204,0,0.229199,"Missing"
L16-1284,D14-1144,1,0.843128,"al., 2014). The dataset is entitled DSL Corpus Collection, or DSLCC, and it includes short excerpts from journalistic texts from previously released corpora and repository.2 Texts in the DSLCC v. 1.0 were written in thirteen languages or language varieties and divided into the following six groups: Group A (Bosnian, Croatian, Serbian), Group B (Indonesian, Malay), Group C (Czech, Slovak), Group D (Brazilian Portuguese, European Portuguese), Group E (Peninsu1 This task focuses on identifying the mother tongue of a learner writer based on stylistic cues; all the texts are in the same language (Malmasi and Dras, 2014; Malmasi and Dras, 2015c). 2 See Tan et al. (2014) for a complete list of sources. 1800 lar Spanish, Argentine Spanish), and Group F3 (American English, British English). In the 2014 edition, eight teams participated and submitted results to the DSL shared task (eight teams in the closed and two teams in the open submission). Five of these teams wrote system description papers. The complete shared task report is available in Zampieri et al. (2014). We summarize the results in Table 1 in terms of accuracy (best performing entries displayed in bold). Team NRC-CNRC RAE UMich UniMelb-NLP QMUL LIR"
L16-1284,W15-5407,1,0.445859,"anguage identification systems (Tiedemann and Ljubeˇsi´c, 2012). Closely-related languages such as Indonesian and Malay or Croatian and Serbian are very similar both at their spoken and at their written forms making it difficult for systems to discriminate between them. Varieties of the same language, e.g. Spanish from South America or Spain, are even more difficult to detect than similar languages. Nevertheless, in both cases, recent work has shown that it is possible to train algorithms to discriminate between similar languages and language varieties with high accuracy (Goutte et al., 2014; Malmasi and Dras, 2015b). This study looks in more detail into the features that help algorithms discriminating between similar languages, taking into account the results of two recent editions of the Discriminating between Similar Languages (DSL) shared task (Zampieri et al., 2014; Zampieri et al., 2015b). The analysis and results of this paper complement the information presented in the shared task reports and provide novel and important information for researchers and developers interested in language identification and particularly in the problem of discriminating between similar languages. 2. Related Work Lang"
L16-1284,W15-0620,1,0.852128,"w years. Examples of studies include the cases of Malay and Indonesian (Ranaivo-Malanc¸on, 2006), Chinese varieties (Huang and Lee, 2008), South Slavic languages (Ljubeˇsi´c et al., 2007; Ljubeˇsi´c and Kranjˇci´c, 2015), Portuguese varieties (Zampieri and Gebre, 2012), Spanish varieties (Zampieri et al., 2013; Maier and G´omez-Rodrıguez, 2014), English varieties (Lui and Cook, 2013), Persian and Dari (Malmasi and Dras, 2015a), Romanian dialects (Ciobanu and Dinu, 2016), and a number of studies on Arabic dialects (Elfardy and Diab, 2014; Zaidan and Callison-Burch, 2014; Tillmann et al., 2014; Malmasi et al., 2015a) A number of shared tasks on language identification have been organized in the recent years ranging from generalpurpose language identification (Baldwin and Lui, 2010) to more specific challenges such as the TweetLID shared task which focused on Twitter data (Zubiaga et al., 2014; Zubiaga et al., 2015), the shared task on Language Identification in Code-Switched Data (Solorio et al., 2014), and the two editions of the discriminating between similar languages (DSL) shared task. To our knowledge, however, no comprehensive analysis of the kind we are proposing in this paper has been carried ou"
L16-1284,W14-5314,0,0.212335,"Missing"
L16-1284,W14-3907,0,0.0354693,"n and Dari (Malmasi and Dras, 2015a), Romanian dialects (Ciobanu and Dinu, 2016), and a number of studies on Arabic dialects (Elfardy and Diab, 2014; Zaidan and Callison-Burch, 2014; Tillmann et al., 2014; Malmasi et al., 2015a) A number of shared tasks on language identification have been organized in the recent years ranging from generalpurpose language identification (Baldwin and Lui, 2010) to more specific challenges such as the TweetLID shared task which focused on Twitter data (Zubiaga et al., 2014; Zubiaga et al., 2015), the shared task on Language Identification in Code-Switched Data (Solorio et al., 2014), and the two editions of the discriminating between similar languages (DSL) shared task. To our knowledge, however, no comprehensive analysis of the kind we are proposing in this paper has been carried out on the results obtained in a language identification shared task and our work fills this gap. The most similar analysis was applied to Native Language Identification (NLI)1 using the 2013 NLI shared task dataset (Malmasi et al., 2015b). In the next sections we present the systems that participated in the two editions of the DSL shared task. 2.1. DSL Shared Task 2014 The first edition of the"
L16-1284,C12-1160,0,0.332039,"Missing"
L16-1284,W14-5313,0,0.149574,"growing in the past few years. Examples of studies include the cases of Malay and Indonesian (Ranaivo-Malanc¸on, 2006), Chinese varieties (Huang and Lee, 2008), South Slavic languages (Ljubeˇsi´c et al., 2007; Ljubeˇsi´c and Kranjˇci´c, 2015), Portuguese varieties (Zampieri and Gebre, 2012), Spanish varieties (Zampieri et al., 2013; Maier and G´omez-Rodrıguez, 2014), English varieties (Lui and Cook, 2013), Persian and Dari (Malmasi and Dras, 2015a), Romanian dialects (Ciobanu and Dinu, 2016), and a number of studies on Arabic dialects (Elfardy and Diab, 2014; Zaidan and Callison-Burch, 2014; Tillmann et al., 2014; Malmasi et al., 2015a) A number of shared tasks on language identification have been organized in the recent years ranging from generalpurpose language identification (Baldwin and Lui, 2010) to more specific challenges such as the TweetLID shared task which focused on Twitter data (Zubiaga et al., 2014; Zubiaga et al., 2015), the shared task on Language Identification in Code-Switched Data (Solorio et al., 2014), and the two editions of the discriminating between similar languages (DSL) shared task. To our knowledge, however, no comprehensive analysis of the kind we are proposing in this pap"
L16-1284,W14-5307,1,0.851072,"Missing"
L16-1284,W15-5411,1,0.885723,"Missing"
L16-1284,W15-5401,1,0.771913,"Missing"
L18-1277,W17-2709,0,0.0192949,"ng on a mixture of natural language processing and statistical modeling. Social media analysis typically operates from two types of data: raw stream data and filtered stream data. Event detection detects either emerging events or specific events from raw stream data. In particular, the large amount of work done on Topic Detection and Tracking (TDT) attempts to detect emerging events. As a recent example, Laban and Hearst (2017) collected 4 million news articles, generated topics from the news, merged them into stories, and visualized the stories along a timeline. For specific event detection, Atkinson et al. (2017) created a corpus of security-related events extracted from news data by learning lexico-semantic patterns, and classified events into security-related categories. Filtered stream data is usually generated by filtering social media using keywords related to public security, natural disaster etc. The TREC 2014 temporal summarization track focused on monitoring events by detecting sub-events, extracting relevant sentences and summarizing them, from a sequence of stream data (Aslam et al., 2014). Zhao et al. (2014) retrieved relevant documents, calculated the text similarity of sentences within t"
L18-1277,W16-5702,0,0.064893,"arget the detection of significant changes, typically within an existing topic or event. We do not summarize changes like in the TREC 2014 temporal summarization task, but summarization can be used as a post† ‡ from University of Toronto, ON, Canada from Carleton University, ON, Canada processing step. Earlier work used change point detection techniques to detect significant changes from sensor signals (Guralnik and Srivastava, 1999; James et al., 2014), but these do not use the textual content of message streams. Some recent studies focus on detecting changes within a storyline. For example, Bruggermann et al. (2016) used the dynamic topic model (Blei and Lafferty, 2006) to identify topics from news and the changes in the word distributions from the topic model were used to represent changes within the storyline. Also, Wang and Goutte (2017) detected changes within events from the temporal profile of hashtags in tweets and evaluated the resulting performance on two twitter datasets. Our approach relates to some of this prior work, with clear distinctions. First, we target the detection of significant changes within an existing event or storyline, rather than detect and track events as in the TDT setup. Se"
L18-1277,W17-2701,0,0.0146791,"ple, a significant action in a sport event will result in a flurry of positive or negative posts on twitter. Monitoring message streams to detect these changes requires automatic methods relying on a mixture of natural language processing and statistical modeling. Social media analysis typically operates from two types of data: raw stream data and filtered stream data. Event detection detects either emerging events or specific events from raw stream data. In particular, the large amount of work done on Topic Detection and Tracking (TDT) attempts to detect emerging events. As a recent example, Laban and Hearst (2017) collected 4 million news articles, generated topics from the news, merged them into stories, and visualized the stories along a timeline. For specific event detection, Atkinson et al. (2017) created a corpus of security-related events extracted from news data by learning lexico-semantic patterns, and classified events into security-related categories. Filtered stream data is usually generated by filtering social media using keywords related to public security, natural disaster etc. The TREC 2014 temporal summarization track focused on monitoring events by detecting sub-events, extracting rele"
L18-1277,W17-2702,1,0.376372,"ronto, ON, Canada from Carleton University, ON, Canada processing step. Earlier work used change point detection techniques to detect significant changes from sensor signals (Guralnik and Srivastava, 1999; James et al., 2014), but these do not use the textual content of message streams. Some recent studies focus on detecting changes within a storyline. For example, Bruggermann et al. (2016) used the dynamic topic model (Blei and Lafferty, 2006) to identify topics from news and the changes in the word distributions from the topic model were used to represent changes within the storyline. Also, Wang and Goutte (2017) detected changes within events from the temporal profile of hashtags in tweets and evaluated the resulting performance on two twitter datasets. Our approach relates to some of this prior work, with clear distinctions. First, we target the detection of significant changes within an existing event or storyline, rather than detect and track events as in the TDT setup. Second, we focus on detecting the locations of significant changes from the message stream, rather than extract descriptive phrases from the text. Also, instead of detecting changes from external signals obtained from sensors or si"
N07-1064,allen-2004-case,0,0.0455551,"Missing"
N07-1064,J93-2003,0,0.0126252,"algorithm to find weights that optimize the BLEU score over these hypotheses, compared to reference translations. This process is repeated until the set of translations stabilizes, i.e. no new translations are produced at the decoding step. 511 To improve raw output from decoding, Portage relies on a rescoring strategy: given a list of n-best translations from the decoder, the system reorders this list, this time using a more elaborate loglinear model, incorporating more feature functions, in addition to those of the decoding model: these typically include IBM-1 and IBM-2 model probabilities (Brown et al., 1993) and an IBM-1-based feature function designed to detect whether any word in one language appears to have been left without satisfactory translation in the other language; all of these feature functions can be used in both language directions, i.e. source-to-target and target-to-source. In the experiments reported in the next section, the Portage system is used both as a translation and as an APE system. While we can think of a number of modifications to such a system to better adapt it to the post-editing task (some of which are discussed later on), we have done no such modifications to the sy"
N07-1064,2006.eamt-1.27,0,0.223315,"Missing"
N07-1064,W06-1607,0,0.0214531,"e sourcelanguage part matches the input. These phrase pairs come from large phrase tables constructed by collecting matching pairs of contiguous text segments from word-aligned bilingual corpora. Portage’s model for P (t|s) is a log-linear combination of four main components: one or more ngram target-language models, one or more phrase translation models, a distortion (word-reordering) model, and a sentence-length feature. The phrasebased translation model is similar to that of Koehn, with the exception that phrase probability estimates P (˜ s|t˜) are smoothed using the Good-Turing technique (Foster et al., 2006). The distortion model is also very similar to Koehn’s, with the exception of a final cost to account for sentence endings. Feature function weights in the loglinear model are set using Och’s minium error rate algorithm (Och, 2003). This is essentially an iterative two-step process: for a given set of source sentences, generate n-best translation hypotheses, that are representative of the entire decoding search space; then, apply a variant of Powell’s algorithm to find weights that optimize the BLEU score over these hypotheses, compared to reference translations. This process is repeated until"
N07-1064,N03-1017,0,0.0249678,"Missing"
N07-1064,koen-2004-pharaoh,0,0.0105662,"sing of raw data into tokens; decoding to produce one or more translation hypotheses; and error-driven rescoring to choose the best final hypothesis. For languages such as French and English, the first of these phases (tokenization) is mostly a straightforward process; we do not describe it any further here. Decoding is the central phase in SMT, involving a search for the hypotheses t that have highest probabilities of being translations of the current source sentence s according to a model for P (t|s). Portage implements a dynamic programming beam search decoding algorithm similar to that of Koehn (2004), in which translation hypotheses are constructed by combining in various ways the target-language part of phrase pairs whose sourcelanguage part matches the input. These phrase pairs come from large phrase tables constructed by collecting matching pairs of contiguous text segments from word-aligned bilingual corpora. Portage’s model for P (t|s) is a log-linear combination of four main components: one or more ngram target-language models, one or more phrase translation models, a distortion (word-reordering) model, and a sentence-length feature. The phrasebased translation model is similar to t"
N07-1064,W02-1018,0,0.021527,"ime-consuming and expensive, it can only fix a subset of the MT system’s problems. The quality of machine translation (MT) is generally considered insufficient for use in the field without a significant amount of human correction. In the translation world, the term post-editing is often used to refer to the process of manually correcting MT output. While the conventional wisdom is that postediting MT is usually not cost-efficient compared to full human translation, there appear to be situations The advent of Statistical Machine Translation, and most recently phrase-based approaches (PBMT, see Marcu and Wong (2002), Koehn et al. (2003)) into the commercial arena seems to hold the promise of a solution to this problem: because the MT system learns directly from existing translations, it can be automatically customized to new domains and tasks. However, the success of this operation cru508 Proceedings of NAACL HLT 2007, pages 508–515, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics cially depends on the amount of training data available. Moreover, the current state of the technology is still insufficient for consistently producing human readable translations. This state of affa"
N07-1064,P03-1021,0,0.0247436,"ombination of four main components: one or more ngram target-language models, one or more phrase translation models, a distortion (word-reordering) model, and a sentence-length feature. The phrasebased translation model is similar to that of Koehn, with the exception that phrase probability estimates P (˜ s|t˜) are smoothed using the Good-Turing technique (Foster et al., 2006). The distortion model is also very similar to Koehn’s, with the exception of a final cost to account for sentence endings. Feature function weights in the loglinear model are set using Och’s minium error rate algorithm (Och, 2003). This is essentially an iterative two-step process: for a given set of source sentences, generate n-best translation hypotheses, that are representative of the entire decoding search space; then, apply a variant of Powell’s algorithm to find weights that optimize the BLEU score over these hypotheses, compared to reference translations. This process is repeated until the set of translations stabilizes, i.e. no new translations are produced at the decoding step. 511 To improve raw output from decoding, Portage relies on a rescoring strategy: given a list of n-best translations from the decoder,"
N07-1064,W05-0822,0,0.0179818,"hile the automation of this process can be envisaged in many different ways, the task is not conceptually very different from the translation task itself. Therefore, there doesn’t seem to be any good reason why a machine translation system could not handle the post-editing task. In particular, given such data as described in Section 2.2, the idea of using a statistical MT system for post-editing is appealing. Portage is precisely such a system, which we describe here. Portage is a phrase-based, statistical machine translation system, developed at the National Research Council of Canada (NRC) (Sadat et al., 2005). A version of the Portage system is made available by the NRC to Canadian universities for research and education purposes. Like other SMT systems, it learns to translate from existing parallel corpora. The system translates text in three main phases: preprocessing of raw data into tokens; decoding to produce one or more translation hypotheses; and error-driven rescoring to choose the best final hypothesis. For languages such as French and English, the first of these phases (tokenization) is mostly a straightforward process; we do not describe it any further here. Decoding is the central phas"
N07-1064,2006.amta-papers.25,0,0.19614,"Missing"
P04-1064,J93-2003,0,0.106841,"lustrated on a French-English alignment task from the Hansard. 1 Introduction Aligning words from mutually translated sentences in two different languages is an important and difficult problem. It is important because a wordaligned corpus is typically used as a first step in order to identify phrases or templates in phrase-based Machine Translation (Och et al., 1999), (Tillmann and Xia, 2003), (Koehn et al., 2003, sec. 3), or for projecting linguistic annotation across languages (Yarowsky et al., 2001). Obtaining a word-aligned corpus usually involves training a word-based translation models (Brown et al., 1993) in each directions and combining the resulting alignments. Besides processing time, important issues are completeness and propriety of the resulting alignment, and the ability to reliably identify general Nto-M alignments. In the following section, we introduce the problem of aligning words from a corpus that is already aligned at the sentence level. We show how this problem may be phrased in terms of matrix factorisation. We then identify a number of constraints on word alignment, show that these constraints entail that word alignment is equivalent to orthogonal non-negative matrix factorisa"
P04-1064,W03-0305,1,0.785885,"to post-edit the reference alignment to ensure coverage and proper alignments. Another possibility would be to use the probabilistic model to mimic the reference data and generate both “Sure” and “Probable” alignments using eg thresholds on the estimated alignment probabilities. This approach may lead to better performance according to our metrics, but it is not obvious that the produced alignments will be more reasonable or even useful in a practical application. We also tested our approach on the RomanianEnglish task of the same workshop, cf. table 4. The ’HLT-03 best’ is our earlier work (Dejean et al., 2003), simply based on IBM4 alignment using an additional lexicon extracted from the corpus. Slightly better results have been published since (Barbu, 2004), using additional linguistic processing, but those were not presented at the workshop. Note that the reference alignments for RomanianEnglish contain only “Sure” alignments, and therefore we only report the performance on those. In addition, AER = 1 − FS in this setting. Table 4 shows that the matrix factorisation approach does not offer any quantitative improvements over these results. A gain of up to 10 points in recall does not offset a larg"
P04-1064,N03-1017,0,0.024138,"nts such as orthogonality. We then propose an algorithm for orthogonal non-negative matrix factorisation, based on a probabilistic model of the alignment data, and apply it to word alignment. This is illustrated on a French-English alignment task from the Hansard. 1 Introduction Aligning words from mutually translated sentences in two different languages is an important and difficult problem. It is important because a wordaligned corpus is typically used as a first step in order to identify phrases or templates in phrase-based Machine Translation (Och et al., 1999), (Tillmann and Xia, 2003), (Koehn et al., 2003, sec. 3), or for projecting linguistic annotation across languages (Yarowsky et al., 2001). Obtaining a word-aligned corpus usually involves training a word-based translation models (Brown et al., 1993) in each directions and combining the resulting alignments. Besides processing time, important issues are completeness and propriety of the resulting alignment, and the ability to reliably identify general Nto-M alignments. In the following section, we introduce the problem of aligning words from a corpus that is already aligned at the sentence level. We show how this problem may be phrased in"
P04-1064,W03-0301,0,0.0903773,"em of aligning words from a corpus that is already aligned at the sentence level. We show how this problem may be phrased in terms of matrix factorisation. We then identify a number of constraints on word alignment, show that these constraints entail that word alignment is equivalent to orthogonal non-negative matrix factorisation, and we give a novel algorithm that solves this problem. This is illustrated using data from the shared tasks of the 2003 HLT-NAACL Workshop on Building le droit increase de permis ne augmente pas Figure 1: 1-1, M-1, 1-N and M-N alignments. and Using Parallel Texts (Mihalcea and Pedersen, 2003). 2 Word alignments We address the following problem: Given a source sentence f = f1 . . . fi . . . fI and a target sentence e = e1 . . . ej . . . eJ , we wish to find words fi and ej on either side which are aligned, ie in mutual correspondence. Note that words may be aligned without being directly “dictionary translations”. In order to have proper alignments, we want to enforce the following constraints: Coverage: Every word on either side must be aligned to at least one word on the other side (Possibly taking “null” words into account). Transitive closure: If fi is aligned to ej and e` , an"
P04-1064,C00-2163,0,0.131658,") The simplest choice for the noise component is a uniform distribution, P (f, e|c = 0) ∝ 1. E-step and M-steps in eqs. (3–6) are unchanged for c > 0, and the E-step equation for c = 0 is easily adapted: P (c = 0|f, e) ∝ P (c = 0)P (f, e|c = 0). 5 Example We first illustrate the factorisation process on a simple example. We use the data provided for the French-English shared task of the 2003 HLTNAACL Workshop on Building and Using Parallel Texts (Mihalcea and Pedersen, 2003). The data is from the Canadian Hansard, and reference alignments were originally produced by Franz Och and Hermann Ney (Och and Ney, 2000). Using the entire corpus (20 million words), we trained English→French and French→English IBM4 models using GIZA++. For all sentences from the trial and test set (37 and 447 sentences), we generated up to 100 best alignments for each sentence and in each direction. For each pair of source and target words (fi , ej ), the association measure mij is simply the number of times these words were aligned together in the two N-best lists, leading to a count between 0 (never aligned) and 200 (always aligned). We focus on sentence 1023, from the trial set. Figure 5 shows the reference alignments toget"
P04-1064,W99-0604,0,0.0969423,"that factors must satisfy a number of constraints such as orthogonality. We then propose an algorithm for orthogonal non-negative matrix factorisation, based on a probabilistic model of the alignment data, and apply it to word alignment. This is illustrated on a French-English alignment task from the Hansard. 1 Introduction Aligning words from mutually translated sentences in two different languages is an important and difficult problem. It is important because a wordaligned corpus is typically used as a first step in order to identify phrases or templates in phrase-based Machine Translation (Och et al., 1999), (Tillmann and Xia, 2003), (Koehn et al., 2003, sec. 3), or for projecting linguistic annotation across languages (Yarowsky et al., 2001). Obtaining a word-aligned corpus usually involves training a word-based translation models (Brown et al., 1993) in each directions and combining the resulting alignments. Besides processing time, important issues are completeness and propriety of the resulting alignment, and the ability to reliably identify general Nto-M alignments. In the following section, we introduce the problem of aligning words from a corpus that is already aligned at the sentence lev"
P04-1064,W03-0304,0,0.0192204,"LL . and toys need entertainment e−null f−null cept6 cept5 cept4 cept3 cept2 cept1 they NULL NULL les NULL toys e−null les need enfants they enfants Table 1: Performance on the 37 trial sentences for orthogonal non-negative matrix factorisation (ONMF) using the AIC and BIC criterion for choosing the number of cepts, discounting null alignments. We also compared the performance on the 447 test sentences to 1/ the intersection of the alignments produced by the top IBM4 alignments in either directions, and 2/ the best systems from (Mihalcea and Pedersen, 2003). On limited resources, Ralign.EF.1 (Simard and Langlais, 2003) produced the best F -score, as well as the best AER when NULL alignments were taken into account, while XRCE.Nolem.EF.3 (Dejean et al., 2003) produced the best AER when NULL alignments were discounted. Tables 2 and 3 show that ONMF improves on several of these results. In particular, we get better recall and F -score on the probable alignments (and even a better precision than Ralign.EF.1 in table 2). On the other hand, the performance, and in particular the precision, on sure alignments is dismal. We attribute this at least partly to a key difference between our model and the reference data:"
P04-1064,N03-2036,0,0.12024,"tisfy a number of constraints such as orthogonality. We then propose an algorithm for orthogonal non-negative matrix factorisation, based on a probabilistic model of the alignment data, and apply it to word alignment. This is illustrated on a French-English alignment task from the Hansard. 1 Introduction Aligning words from mutually translated sentences in two different languages is an important and difficult problem. It is important because a wordaligned corpus is typically used as a first step in order to identify phrases or templates in phrase-based Machine Translation (Och et al., 1999), (Tillmann and Xia, 2003), (Koehn et al., 2003, sec. 3), or for projecting linguistic annotation across languages (Yarowsky et al., 2001). Obtaining a word-aligned corpus usually involves training a word-based translation models (Brown et al., 1993) in each directions and combining the resulting alignments. Besides processing time, important issues are completeness and propriety of the resulting alignment, and the ability to reliably identify general Nto-M alignments. In the following section, we introduce the problem of aligning words from a corpus that is already aligned at the sentence level. We show how this probl"
P04-1064,H01-1035,0,0.0280483,"rix factorisation, based on a probabilistic model of the alignment data, and apply it to word alignment. This is illustrated on a French-English alignment task from the Hansard. 1 Introduction Aligning words from mutually translated sentences in two different languages is an important and difficult problem. It is important because a wordaligned corpus is typically used as a first step in order to identify phrases or templates in phrase-based Machine Translation (Och et al., 1999), (Tillmann and Xia, 2003), (Koehn et al., 2003, sec. 3), or for projecting linguistic annotation across languages (Yarowsky et al., 2001). Obtaining a word-aligned corpus usually involves training a word-based translation models (Brown et al., 1993) in each directions and combining the resulting alignments. Besides processing time, important issues are completeness and propriety of the resulting alignment, and the ability to reliably identify general Nto-M alignments. In the following section, we introduce the problem of aligning words from a corpus that is already aligned at the sentence level. We show how this problem may be phrased in terms of matrix factorisation. We then identify a number of constraints on word alignment,"
S14-2030,P07-1019,0,0.0169033,"andard 4-gram, estimated using KneserNey smoothing (Kneser and Ney, 1995) on the target side of the bilingual corpora used for training the translation models. In the second configuration (run2), in order to further adapt the translations to the test domain, a smaller LM trained on the L2 contexts of the test data is combined to the training corpus LM in a linear mixture model (Foster and Kuhn, 2007). The linear mixture weights are estimated on the L2 context of each test set in a cross-validation fashion. 4 4.1 Decoding Algorithm and Parameter Tuning Decoding uses the cube-pruning algorithm (Huang and Chiang, 2007) with a 7-word distortion limit. Log-linear parameter tuning is performed using a lattice-based batch version of MIRA (Cherry and Foster, 2012). 3 Europarl Experimental Results Results on Trial and Simulated Data Our first evaluation was performed on the trial data provided by the Task 5 organizers. Each example was translated in context by two systems: run1: Baseline, non-adapted system (marked 1 below); Data run2: Linear LM mixture adaptation, using a context LM (marked 2 below). SMT systems require large amounts of data to estimate model parameters. In addition, translation performance larg"
S14-2030,2010.jec-1.4,0,0.0387737,"Missing"
S14-2030,2005.mtsummit-papers.11,0,0.0142599,"his results in a total of 6 feature values per phrase pair, in addition to the distance-based distortion feature, hence seven parameters to tune in the log-linear model. News Total en-de train dev 1904k - 177k 2000 2081k 2000 en-es train dev 1959k - 174k 2000 2133k 2000 fr-en train dev 2002k - 157k 2000 2158k 2000 nl-en train dev 1974k 1984 - 1974k 1984 Table 1: Number of training segments for each language pair. domain data to train on. As we had no information on the domain of the test data for Task 5, we chose to rely on general purpose publicly available data. Our main corpus is Europarl (Koehn, 2005), which is available for all 4 language pairs of the evaluation. As Europarl covers parliamentary proceedings, we added some news and commentary (henceforth ”News”) data provided for the 2013 workshop on Machine Translation shared task (Bojar et al., 2013) for language pairs other than nl-en. In all cases, we extracted from the corpus a tuning (“dev”) set of around 2000 sentence pairs. Statistics for the training data are given in Table 1. The trial and test data each consist of 500 sentences with L1 fragments in L2 context provided by the organizers. As the trial data came from Europarl, we f"
S14-2030,J03-1002,0,0.00547523,"Missing"
S14-2030,2009.mtsummit-papers.14,1,0.823815,"Missing"
S14-2030,N04-4026,0,0.01245,"op on Semantic Evaluation (SemEval 2014), pages 192–197, Dublin, Ireland, August 23-24, 2014. on the entire training data. The phrase table contains four features per phrase pair: lexical estimates of the forward and backward probabilities obtained either by relative frequencies or using the method of Zens and Ney (2004). These estimates are derived by summing counts over all possible alignments. This yields four corresponding parameters in the log-linear model. Reordering Models We use standard reordering models: a distance-based distortion feature, as well as a lexicalized distortion model (Tillmann, 2004; Koehn et al., 2005). For each phrase pair, the orientation counts required for the lexicalized distortion model are computed using HMM wordalignment on the full training corpora. We estimate lexicalized probabilities for monotone, swap, and discontinuous ordering with respect to the previous and following target phrase. This results in a total of 6 feature values per phrase pair, in addition to the distance-based distortion feature, hence seven parameters to tune in the log-linear model. News Total en-de train dev 1904k - 177k 2000 2081k 2000 en-es train dev 1959k - 174k 2000 2133k 2000 fr-e"
S14-2030,N12-1047,0,0.0151326,"anslation models. In the second configuration (run2), in order to further adapt the translations to the test domain, a smaller LM trained on the L2 contexts of the test data is combined to the training corpus LM in a linear mixture model (Foster and Kuhn, 2007). The linear mixture weights are estimated on the L2 context of each test set in a cross-validation fashion. 4 4.1 Decoding Algorithm and Parameter Tuning Decoding uses the cube-pruning algorithm (Huang and Chiang, 2007) with a 7-word distortion limit. Log-linear parameter tuning is performed using a lattice-based batch version of MIRA (Cherry and Foster, 2012). 3 Europarl Experimental Results Results on Trial and Simulated Data Our first evaluation was performed on the trial data provided by the Task 5 organizers. Each example was translated in context by two systems: run1: Baseline, non-adapted system (marked 1 below); Data run2: Linear LM mixture adaptation, using a context LM (marked 2 below). SMT systems require large amounts of data to estimate model parameters. In addition, translation performance largely depends on having inTable 2 shows that our run1 system already yields high performance on the trial data, while 193 en-de1 en-de2 en-es1 en"
S14-2030,N04-1033,0,0.02597,"ation and reordering models), as well as the decoding and parameter tuning. Translation Models We use a single static phrase table including phrase pairs extracted from the symmetrized HMM word-alignment learned c 2014, The Crown in Right of Canada. 192 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 192–197, Dublin, Ireland, August 23-24, 2014. on the entire training data. The phrase table contains four features per phrase pair: lexical estimates of the forward and backward probabilities obtained either by relative frequencies or using the method of Zens and Ney (2004). These estimates are derived by summing counts over all possible alignments. This yields four corresponding parameters in the log-linear model. Reordering Models We use standard reordering models: a distance-based distortion feature, as well as a lexicalized distortion model (Tillmann, 2004; Koehn et al., 2005). For each phrase pair, the orientation counts required for the lexicalized distortion model are computed using HMM wordalignment on the full training corpora. We estimate lexicalized probabilities for monotone, swap, and discontinuous ordering with respect to the previous and following"
S14-2030,W07-0717,0,0.0205944,"only component of the SMT system that scores how well the translation of the L1 fragment fits in the existing L2 context. We test two different LM configurations. The first of these (run1) uses a single static LM: a standard 4-gram, estimated using KneserNey smoothing (Kneser and Ney, 1995) on the target side of the bilingual corpora used for training the translation models. In the second configuration (run2), in order to further adapt the translations to the test domain, a smaller LM trained on the L2 contexts of the test data is combined to the training corpus LM in a linear mixture model (Foster and Kuhn, 2007). The linear mixture weights are estimated on the L2 context of each test set in a cross-validation fashion. 4 4.1 Decoding Algorithm and Parameter Tuning Decoding uses the cube-pruning algorithm (Huang and Chiang, 2007) with a 7-word distortion limit. Log-linear parameter tuning is performed using a lattice-based batch version of MIRA (Cherry and Foster, 2012). 3 Europarl Experimental Results Results on Trial and Simulated Data Our first evaluation was performed on the trial data provided by the Task 5 organizers. Each example was translated in context by two systems: run1: Baseline, non-adap"
S14-2030,P99-1043,0,0.0910822,"Carpuat National Research Council Canada Multilingual Text Processing 1200 Montreal Road, Ottawa, Ontario K1A 0R6, Canada FirstName.LastName@nrc.ca Abstract there the parts of the target segment that need to be modified (Bic¸ici and Dymetman, 2008; Simard and Isabelle, 2009; Koehn and Senellart, 2010). The task of translating a L1 fragment in L2 context therefore has much broader application than language learning. This motivation also provides a clear link of this task to the Machine Translation setting. There are also connections to the codeswitching and mixed language translation problems (Fung et al., 1999). In our work, we therefore investigate the use of a standard Phrase-Based Statistical Machine Translation (SMT) system to translate L1 fragments in L2 context. In the next section, we describe the SMT system that we used in our submission. We then describe the corpora used to train the SMT engine (Section 3), and our results on the trial and test data, as well as a short error analysis (Section 4). section We describe the system entered by the National Research Council Canada in the SemEval-2014 L2 writing assistant task. Our system relies on a standard Phrase-Based Statistical Machine Transl"
S14-2030,W10-1717,0,\N,Missing
S14-2030,W13-2201,0,\N,Missing
S14-2030,2005.iwslt-1.8,0,\N,Missing
S16-1102,W09-1206,0,0.0837081,"Missing"
S16-1102,N12-1047,0,0.0449274,"feature functions are used in the log-linear model: three 5-gram language models with KneserNey smoothing (Kneser and Ney, 1995), i.e. one for each of Europarl, CC and NC data, combined linearly (Foster and Kuhn, 2007) to best fit NC data; lexical estimates of the forward and backward translation probabilities obtained either by relative frequencies or using the method of (Zens and Ney, 2004); lexicalized distortion (Tillmann, 2004; Koehn et al., 2005); and word count. The parameters of the log-linear model were tuned by optimizing BLEU on the development set using the batch variant of MIRA (Cherry and Foster, 2012). Decoding uses the cube-pruning algorithm of (Huang and Chiang, 2007) with a 7-word distortion limit. For any given input in Spanish, the SMT system produces the translation that is most likely with regard to its own training data; that translation may be arbitrarily distant from the English sentence to which it will be compared in the STS task. These arbitrary surface differences may complicate the task of measuring semantic similarity. To alleviate this problem, we bias the MT system to produce a translation that is as close as possible on the surface to the English sentence. This is done b"
S16-1102,W07-0717,0,0.0431433,"(NC) – totaling approximately 110M words in each language. Phrase extraction was done by aligning the corpora at the word level using HMM, IBM2 and IBM4 models, using the union of phrases extracted from these separate alignments for the phrase table, with a maximum phrase length of 7 tokens. Phrase pairs were filtered so that the top 30 translations for each source phrase were retained. The following feature functions are used in the log-linear model: three 5-gram language models with KneserNey smoothing (Kneser and Ney, 1995), i.e. one for each of Europarl, CC and NC data, combined linearly (Foster and Kuhn, 2007) to best fit NC data; lexical estimates of the forward and backward translation probabilities obtained either by relative frequencies or using the method of (Zens and Ney, 2004); lexicalized distortion (Tillmann, 2004; Koehn et al., 2005); and word count. The parameters of the log-linear model were tuned by optimizing BLEU on the development set using the batch variant of MIRA (Cherry and Foster, 2012). Decoding uses the cube-pruning algorithm of (Huang and Chiang, 2007) with a 7-word distortion limit. For any given input in Spanish, the SMT system produces the translation that is most likely"
S16-1102,P07-1019,0,0.0734543,"age models with KneserNey smoothing (Kneser and Ney, 1995), i.e. one for each of Europarl, CC and NC data, combined linearly (Foster and Kuhn, 2007) to best fit NC data; lexical estimates of the forward and backward translation probabilities obtained either by relative frequencies or using the method of (Zens and Ney, 2004); lexicalized distortion (Tillmann, 2004; Koehn et al., 2005); and word count. The parameters of the log-linear model were tuned by optimizing BLEU on the development set using the batch variant of MIRA (Cherry and Foster, 2012). Decoding uses the cube-pruning algorithm of (Huang and Chiang, 2007) with a 7-word distortion limit. For any given input in Spanish, the SMT system produces the translation that is most likely with regard to its own training data; that translation may be arbitrarily distant from the English sentence to which it will be compared in the STS task. These arbitrary surface differences may complicate the task of measuring semantic similarity. To alleviate this problem, we bias the MT system to produce a translation that is as close as possible on the surface to the English sentence. This is done by means of loglinear model features that aim at maximizing n-gram prec"
S16-1102,P14-2124,1,0.849566,"arget English word using a word embeddings model. In our experiments, we used pretrained word2vec (Mikolov et al., 2013) embeddings.2 The resulting crosslingual lexical similarity of the targeted pair of Spanish and English words is the highest similarity between the 5 mapped words and the target English word. We then reconstruct the semantic phrasal similarity by averaging the Englishidf-weighted crosslingual embeddings mapped lexical similarity according to the 1-1 maximal matching alignment of the lexicons in the two phrases. In addition to the flat lexical semantic feature, we use XMEANT (Lo et al., 2014), the crosslingual semantic frame based machine translation evaluation metric, for generating shallow structural semantic features. We use MATE (Bj¨orkelund et al., 2009) for Spanish shallow semantic parsing and SENNA (Collobert et al., 2011) for English shallow semantic parsing. In evaluating machine translation quality, the confusion of semantic roles is a major source of errors due to reordering. However, in evaluating STS, confusion of semantic roles is less frequent while missing information in one of the test fragments is more frequent. This motivates a further simplification of the 12 s"
S16-1102,W15-3056,1,0.877844,"Missing"
S16-1102,N04-4026,0,0.0512051,"Missing"
S16-1102,N04-1033,0,0.0665645,"phrases extracted from these separate alignments for the phrase table, with a maximum phrase length of 7 tokens. Phrase pairs were filtered so that the top 30 translations for each source phrase were retained. The following feature functions are used in the log-linear model: three 5-gram language models with KneserNey smoothing (Kneser and Ney, 1995), i.e. one for each of Europarl, CC and NC data, combined linearly (Foster and Kuhn, 2007) to best fit NC data; lexical estimates of the forward and backward translation probabilities obtained either by relative frequencies or using the method of (Zens and Ney, 2004); lexicalized distortion (Tillmann, 2004; Koehn et al., 2005); and word count. The parameters of the log-linear model were tuned by optimizing BLEU on the development set using the batch variant of MIRA (Cherry and Foster, 2012). Decoding uses the cube-pruning algorithm of (Huang and Chiang, 2007) with a 7-word distortion limit. For any given input in Spanish, the SMT system produces the translation that is most likely with regard to its own training data; that translation may be arbitrarily distant from the English sentence to which it will be compared in the STS task. These arbitrary surface"
S16-1102,2005.iwslt-1.8,0,\N,Missing
W03-0305,P00-1056,0,\N,Missing
W03-0305,J93-2003,0,\N,Missing
W13-1712,C12-1025,0,0.0428067,"e, the word “at” will produce two trigrams: “ at” and “at “. These features allow us to capture for example typical spelling variants. In a language with weak morphology such as English, they may also be able to capture patterns of usage of, e.g. suffixes, which provides a low-cost proxy for syntactic information. Word ngrams: We index unigrams and bigrams of words within each sentence. For bigrams, the beginning and end of a sentence are treated as special 97 tokens. Note that we do not apply any stoplist filtering. As a consequence, function words, an oftenused feature (Koppel et al., 2005; Brooke and Hirst, 2012), are naturally included in the unigram feature space. Spelling features: Misspelled words are identified using GNU Aspell V0.60.41 and indexed with their counts. Some parser artifacts such as “n’t” are removed from the final mispelled word index. Although misspellings may seem to provide clues as to the author’s native language, we did not find these features to be useful in any of our experiments. Note however, that misspelled words will also appear in the unigram feature space. Part-of-speech ngrams: The texts were tagged with the Stanford tagger v. 3.02 using the largest and best (bidirect"
W13-1712,W13-1706,0,0.319996,"a majority vote between classifiers. Somewhat surprisingly, a classifier relying on purely lexical features performed very well and proved difficult to outperform significantly using various combinations of feature spaces. However, the combination of multiple predictors allowed to exploit their different strengths and provided a significant boost in performance. 1 Marine Carpuat National Research Council 1200 Montreal Rd, Ottawa, ON K1A 0R6 Marine.Carpuat@nrc.ca Introduction We describe the National Research Council Canada’s submissions to the Native Language Identification 2013 shared task (Tetreault et al., 2013). Our submissions rely on fairly straightforward statistical modelling techniques, applied to various feature spaces representing lexical and syntactic information. Our most successful submission was actually a combination of models trained on different sets of feature spaces using a simple majority vote. Much of the work on Natural Language Processing is motivated by the desire to have machines that can help or replace humans on language-related tasks. Many tasks such as topic or genre classification, entity extraction, disambiguation, are fairly 2 Modelling Our submissions rely on straightfo"
W14-3363,D11-1033,0,0.0376008,"ain to a single subset of the training data. In contrast, we show that the test domain can be flexibly represented by a mixture of many components. Yamamoto and Sumita (2007) cluster the parallel data using bilingual representations, and assign data to a single cluster at test time. Wang et al. (2012) show how to detect a known domain at test time in order to configure a generic translation system with domain-specific feature weights. Others select a subset of training data that is relevant to the test domain, using e.g., IR techniques (Hildebrand et al., 2005) or language model crossentropy (Axelrod et al., 2011). Related work Most previous work on domain adaptation in machine translation presupposes a clear-cut distinction between in-domain and out-of-domain data (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Duh et al., 2010; Bisazza et al., 2011; Haddow and Koehn, 2012; Sennrich, 2012b; Haddow and Koehn, 2012; Clark et al., 2012, among many Closer to this work, Sennrich (2012a) proposes a sentence-level clustering approach to automatically recover domain distinctions in a heteorogeneous corpus obtained by concatenating data from a small number of very distant domains. The tar507 tion probabilit"
W14-3363,2011.iwslt-evaluation.18,0,0.0374739,"data to a single cluster at test time. Wang et al. (2012) show how to detect a known domain at test time in order to configure a generic translation system with domain-specific feature weights. Others select a subset of training data that is relevant to the test domain, using e.g., IR techniques (Hildebrand et al., 2005) or language model crossentropy (Axelrod et al., 2011). Related work Most previous work on domain adaptation in machine translation presupposes a clear-cut distinction between in-domain and out-of-domain data (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Duh et al., 2010; Bisazza et al., 2011; Haddow and Koehn, 2012; Sennrich, 2012b; Haddow and Koehn, 2012; Clark et al., 2012, among many Closer to this work, Sennrich (2012a) proposes a sentence-level clustering approach to automatically recover domain distinctions in a heteorogeneous corpus obtained by concatenating data from a small number of very distant domains. The tar507 tion probabilities. In particular, while smoothing primarily has a large discounting effect for rare source phrases, linear mixtures yield differences in translation probabilities for phrases with a wider range of frequencies. These surprising results encoura"
W14-3363,2011.mtsummit-papers.30,1,0.896794,"that since our training data is very heterogeneous, we take into account other dimensions beyond provenance, such as genre and dialect information (Section 4). Previous work provides empirical evidence supporting this. For instance, Foster et al. (2010) found that linear mixtures outperform log linear mixtures when adapting a French-English system to the medical domain, as well as on a ChineseEnglish NIST translation task. 2.4 Estimating Conditional Translation Probabilities Within each mixture component, we extract all phrase-pairs, compute relative frequencies, and use Kneser-Ney smoothing (Chen et al., 2011) to produce the final estimate of conditional translation probabilities pk (t|s). Per-component probabilities are then combined in Eq. 1 and 3. Similarly, baseline translation probabilities are learned using Kneser-Ney smoothed frequencies collected on the entire training set. 3 3.2 We propose to use automatic text clustering techniques to organize basic elements into homogeneous clusters that are seen as sub-domains. In our experiments, we apply clustering algorithms to the target (English) side of the corpus only. Each corpus element is transformed into a vector-space format by constructing"
W14-3363,2012.amta-papers.4,0,0.0728245,"main at test time in order to configure a generic translation system with domain-specific feature weights. Others select a subset of training data that is relevant to the test domain, using e.g., IR techniques (Hildebrand et al., 2005) or language model crossentropy (Axelrod et al., 2011). Related work Most previous work on domain adaptation in machine translation presupposes a clear-cut distinction between in-domain and out-of-domain data (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Duh et al., 2010; Bisazza et al., 2011; Haddow and Koehn, 2012; Sennrich, 2012b; Haddow and Koehn, 2012; Clark et al., 2012, among many Closer to this work, Sennrich (2012a) proposes a sentence-level clustering approach to automatically recover domain distinctions in a heteorogeneous corpus obtained by concatenating data from a small number of very distant domains. The tar507 tion probabilities. In particular, while smoothing primarily has a large discounting effect for rare source phrases, linear mixtures yield differences in translation probabilities for phrases with a wider range of frequencies. These surprising results encourage us to rethink the use of mixture models, and opens up new ways of conceptualizing"
W14-3363,E12-1055,0,0.377042,"ranslation quality in very heterogeneous training conditions, even if the mixtures do not use any domain knowledge and attempt to learn generic models rather than adapt them to the target domain. This surprising finding opens new perspectives for using mixture models in machine translation beyond clear cut domain adaptation tasks. 1 • Should mixture component capture domain information? Previous work assumes that training data should be organized into domains. When manual domain distinctions are not available, previous work uses clustering approaches to approximate manual domain distinctions (Sennrich, 2012a). However, it is unclear whether it is necessary to use or mimic domain distinctions in order to define mixture components. • Mixture models are usually assumed to improve translation quality by giving more weight to parts of the training corpus that are more relevant to the test domain. Is this intuition still valid in our more complex heterogeneous training conditions? If not, how do mixture models affect translation probability estimates? Introduction While machine translation tasks used to be defined by drawing training and test data from a single well-defined domain, current systems hav"
W14-3363,W07-0717,1,0.740538,"ng bilingual representations, and assign data to a single cluster at test time. Wang et al. (2012) show how to detect a known domain at test time in order to configure a generic translation system with domain-specific feature weights. Others select a subset of training data that is relevant to the test domain, using e.g., IR techniques (Hildebrand et al., 2005) or language model crossentropy (Axelrod et al., 2011). Related work Most previous work on domain adaptation in machine translation presupposes a clear-cut distinction between in-domain and out-of-domain data (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Duh et al., 2010; Bisazza et al., 2011; Haddow and Koehn, 2012; Sennrich, 2012b; Haddow and Koehn, 2012; Clark et al., 2012, among many Closer to this work, Sennrich (2012a) proposes a sentence-level clustering approach to automatically recover domain distinctions in a heteorogeneous corpus obtained by concatenating data from a small number of very distant domains. The tar507 tion probabilities. In particular, while smoothing primarily has a large discounting effect for rare source phrases, linear mixtures yield differences in translation probabilities for phrases with a wider range of frequ"
W14-3363,D10-1044,1,0.94741,"iform mixtures where all components are weighted equally: Linear Mixtures for Translation Models K X p˜(s, t) log We use the Expectation Maximization algorithm to solve this maximization problem. Does domain knowledge yield better translation quality when learning linear mixture weights for the translation model of a phrase-based MT system? We leave the study of linear mixtures for language and reordering models for future work. 2.1 X (1) k=1 where pk (t|s) is a conditional translation probability learned on subset k of the training corpus. 500 pean parliament proceedings and movie subtitles. Foster et al. (2010) work with a slightly different setting when defining mixture components for the NIST Chinese-English translation task: while there is no single obvious “in-domain” component in the NIST training set, homogeneous domains can still be defined in a straightforward fashion based on the provenance of the data (e.g., Hong Kong Hansards vs. Hong Kong Law vs. News articles from FBIS, etc.). We take a similar approach in our experiments. However, we will see that since our training data is very heterogeneous, we take into account other dimensions beyond provenance, such as genre and dialect informatio"
W14-3363,2012.amta-papers.18,0,0.0177864,"an global estimates, including smoothed estimates. This suggests that standard smoothing techniques can be improved when learning from heterogeneous training data, and that mixture components are beneficial even when they do not explicitly capture domain distinctions. 7 Many approaches focus on mapping the test domain to a single subset of the training data. In contrast, we show that the test domain can be flexibly represented by a mixture of many components. Yamamoto and Sumita (2007) cluster the parallel data using bilingual representations, and assign data to a single cluster at test time. Wang et al. (2012) show how to detect a known domain at test time in order to configure a generic translation system with domain-specific feature weights. Others select a subset of training data that is relevant to the test domain, using e.g., IR techniques (Hildebrand et al., 2005) or language model crossentropy (Axelrod et al., 2011). Related work Most previous work on domain adaptation in machine translation presupposes a clear-cut distinction between in-domain and out-of-domain data (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Duh et al., 2010; Bisazza et al., 2011; Haddow and Koehn, 2012; Sennrich, 2"
W14-3363,2013.mtsummit-papers.23,1,0.626478,"ceedings and movie subtitles. We address training conditions where the dimensions for organizing the training data are not as clear-cut, and show that partitions that do not attempt to mimick domain distinctions can improve translation quality. It would be interesting to see whether our conclusion holds in these more artificial training settings, and whether sentence-level corpus organization could help translation quality in our settings. Finally, recent work shows that linear mixture weights can be optimized for BLEU, either directly (Haddow, 2013), or by simulating discriminative training (Foster et al., 2013). In this paper, we limited our studies to maximum likelihood and uniform mixtures, however, the various mixture component definitions proposed here can also be applied when maximizing BLEU. 8 Acknowledgments This research was supported in part by DARPA contract HR0011-12-C-0014 under subcontract to Raytheon BBN Technologies. The authors would like to thank the reviewers and the PORTAGE group at the National Research Council. Conclusion We have presented an extensive study of linear mixtures for training translation models on very heterogeneous data on Arabic-English and Chinese-English transl"
W14-3363,D07-1054,0,0.0324476,"rgences for very frequent source phrases. Overall, the linear mixtures result in very different translation probability distributions than global estimates, including smoothed estimates. This suggests that standard smoothing techniques can be improved when learning from heterogeneous training data, and that mixture components are beneficial even when they do not explicitly capture domain distinctions. 7 Many approaches focus on mapping the test domain to a single subset of the training data. In contrast, we show that the test domain can be flexibly represented by a mixture of many components. Yamamoto and Sumita (2007) cluster the parallel data using bilingual representations, and assign data to a single cluster at test time. Wang et al. (2012) show how to detect a known domain at test time in order to configure a generic translation system with domain-specific feature weights. Others select a subset of training data that is relevant to the test domain, using e.g., IR techniques (Hildebrand et al., 2005) or language model crossentropy (Axelrod et al., 2011). Related work Most previous work on domain adaptation in machine translation presupposes a clear-cut distinction between in-domain and out-of-domain dat"
W14-3363,D08-1089,0,0.0542947,"● ● 0 20 40 60 corpus component 80 #lines #words 100 4.4 Figure 1: Sizes of the 82 Arabic-English (top) and 101 Chinese-English (bottom) corpus components. 4.2 We use an in-house implementation of a Phrasebased Statistical Machine Translation system (Koehn et al., 2007) to build strong baseline systems for both language pairs. Translation hypotheses are scored according to the following features: • 4 phrase-table scores: Kneser-Ney smoothed phrasal translation probabilites and lexical weights, in both translation directions (Chen et al., 2011)2 • 6 hierarchical lexicalized reordering scores (Galley and Manning, 2008) • a word penalty, and a word-displacement distortion penalty • a Good-Turing smoothed 4-gram language model trained on the Gigaword corpus, Kneser-Ney smoothed 5-gram models trained on the English side of the training corpus, and an additional 5-gram model trained on monolingual webforum data. Weights for these features are learned using a batch version of the MIRA algorithm (Chiang, 2012). Phrase pairs are extracted from several word alignments of the training set: HMM, IBM2, and IBM4. Word alignments are kept constant across all experiments. We apply our linear mixture models to both transl"
W14-3363,W12-3154,0,0.0153077,"er at test time. Wang et al. (2012) show how to detect a known domain at test time in order to configure a generic translation system with domain-specific feature weights. Others select a subset of training data that is relevant to the test domain, using e.g., IR techniques (Hildebrand et al., 2005) or language model crossentropy (Axelrod et al., 2011). Related work Most previous work on domain adaptation in machine translation presupposes a clear-cut distinction between in-domain and out-of-domain data (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Duh et al., 2010; Bisazza et al., 2011; Haddow and Koehn, 2012; Sennrich, 2012b; Haddow and Koehn, 2012; Clark et al., 2012, among many Closer to this work, Sennrich (2012a) proposes a sentence-level clustering approach to automatically recover domain distinctions in a heteorogeneous corpus obtained by concatenating data from a small number of very distant domains. The tar507 tion probabilities. In particular, while smoothing primarily has a large discounting effect for rare source phrases, linear mixtures yield differences in translation probabilities for phrases with a wider range of frequencies. These surprising results encourage us to rethink the use"
W14-3363,N13-1035,0,0.464864,"see in Section 4. We consider four very different ways of defining mixture components by grouping the basic corpus elements: (1) manual partition of the training corpus into domains, (2) automatically learning homogeneous domains using text clustering algorithms, (3) random partitioning, (4) sampling with replacement. 3.1 Induced Domains Using Automatic Clustering Algorithms Manually Defined Domains Heterogeneous training data is usually grouped into domains manually using provenance information. In most previous work, such domain distinctions are very clear and easy to define. For instance, Haddow (2013) uses European parliament proceedings to improve translation of text in the movie subtitles and News Commentary domains; Sennrich (2012a) aims to translate Alpine Club reports using components trained on Euro3.3 Random Partitioning We consider random partitions of the training corpus. They are generated by using a random number generator to assign each basic element to one of K clusters. Resulting components therefore do not capture any domain information. Each com501 Arabic-English Training Conditions segs src en train 8.5M 262M 207M Test Domain 1: Webforum segs src en dev (tune) 4.1k 66k 72k"
W14-3363,2005.eamt-1.19,0,0.044844,"inctions. 7 Many approaches focus on mapping the test domain to a single subset of the training data. In contrast, we show that the test domain can be flexibly represented by a mixture of many components. Yamamoto and Sumita (2007) cluster the parallel data using bilingual representations, and assign data to a single cluster at test time. Wang et al. (2012) show how to detect a known domain at test time in order to configure a generic translation system with domain-specific feature weights. Others select a subset of training data that is relevant to the test domain, using e.g., IR techniques (Hildebrand et al., 2005) or language model crossentropy (Axelrod et al., 2011). Related work Most previous work on domain adaptation in machine translation presupposes a clear-cut distinction between in-domain and out-of-domain data (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Duh et al., 2010; Bisazza et al., 2011; Haddow and Koehn, 2012; Sennrich, 2012b; Haddow and Koehn, 2012; Clark et al., 2012, among many Closer to this work, Sennrich (2012a) proposes a sentence-level clustering approach to automatically recover domain distinctions in a heteorogeneous corpus obtained by concatenating data from a small numb"
W14-3363,W07-0733,0,0.0538177,"uster the parallel data using bilingual representations, and assign data to a single cluster at test time. Wang et al. (2012) show how to detect a known domain at test time in order to configure a generic translation system with domain-specific feature weights. Others select a subset of training data that is relevant to the test domain, using e.g., IR techniques (Hildebrand et al., 2005) or language model crossentropy (Axelrod et al., 2011). Related work Most previous work on domain adaptation in machine translation presupposes a clear-cut distinction between in-domain and out-of-domain data (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Duh et al., 2010; Bisazza et al., 2011; Haddow and Koehn, 2012; Sennrich, 2012b; Haddow and Koehn, 2012; Clark et al., 2012, among many Closer to this work, Sennrich (2012a) proposes a sentence-level clustering approach to automatically recover domain distinctions in a heteorogeneous corpus obtained by concatenating data from a small number of very distant domains. The tar507 tion probabilities. In particular, while smoothing primarily has a large discounting effect for rare source phrases, linear mixtures yield differences in translation probabilities for phrases with"
W14-3363,P07-2045,0,0.0073128,"● ● ●● ● ● ●● ● ●●●●●●● ●●● ● ● ●●●●●●●●●●●● ●●●●●● ● ●●●●●●●●●●●●●●● ● ● ●●●●●●●●●● ● ● ● ● ● ● ● ●●●●● ● ● 0 20 40 60 corpus component #lines #words 80 #lines / #words 1e+03 1e+05 1e+07 Chinese corpus components ● ●●●●● ● ●●●● ● ● ● ● ● ● ●●●●● ●●●●●●●●● ● ●●● ●● ●●● ●●●● ● ●● ●● ● ● ●● ●●●●●●● ● ●● ● ●●●●●●●●●●●●● ● ●●●● ●● ●●●●●●●●●●● ● ●●● ● ● ● 0 20 40 60 corpus component 80 #lines #words 100 4.4 Figure 1: Sizes of the 82 Arabic-English (top) and 101 Chinese-English (bottom) corpus components. 4.2 We use an in-house implementation of a Phrasebased Statistical Machine Translation system (Koehn et al., 2007) to build strong baseline systems for both language pairs. Translation hypotheses are scored according to the following features: • 4 phrase-table scores: Kneser-Ney smoothed phrasal translation probabilites and lexical weights, in both translation directions (Chen et al., 2011)2 • 6 hierarchical lexicalized reordering scores (Galley and Manning, 2008) • a word penalty, and a word-displacement distortion penalty • a Good-Turing smoothed 4-gram language model trained on the Gigaword corpus, Kneser-Ney smoothed 5-gram models trained on the English side of the training corpus, and an additional 5"
W14-3363,2012.eamt-1.43,0,0.555765,"ranslation quality in very heterogeneous training conditions, even if the mixtures do not use any domain knowledge and attempt to learn generic models rather than adapt them to the target domain. This surprising finding opens new perspectives for using mixture models in machine translation beyond clear cut domain adaptation tasks. 1 • Should mixture component capture domain information? Previous work assumes that training data should be organized into domains. When manual domain distinctions are not available, previous work uses clustering approaches to approximate manual domain distinctions (Sennrich, 2012a). However, it is unclear whether it is necessary to use or mimic domain distinctions in order to define mixture components. • Mixture models are usually assumed to improve translation quality by giving more weight to parts of the training corpus that are more relevant to the test domain. Is this intuition still valid in our more complex heterogeneous training conditions? If not, how do mixture models affect translation probability estimates? Introduction While machine translation tasks used to be defined by drawing training and test data from a single well-defined domain, current systems hav"
W14-3363,2010.iwslt-papers.5,0,\N,Missing
W14-5316,W13-1712,1,0.903367,"Missing"
W14-5316,W13-1706,0,0.0296805,"Missing"
W14-5316,W14-5307,0,0.35685,"Missing"
W14-5316,N12-1006,0,0.0148142,"ish). In addition, instances to classify are single sentences, a more realistic and challenging situation than full-document language identification. Our motivation for taking part in this evaluation was threefold. First, we wanted to evaluate our in-house implementation of document categorization on a real and useful task in a well controlled experimental setting.1 Second, classifiers that can discriminate between similar languages can be applied to tasks such as identifying close dialects, and may be useful for training Statistical Machine Translation systems more effectively. For instance, Zbib et al. (2012) show that small amounts of data from the right dialect can have a dramatic impact on the quality of Dialectal Arabic Machine Translation systems. Finally, we view the DSL task as a first step towards building a system that can identify code-switching in, for example, social media data, a task which has recently received increased attention from the NLP community2 (Elfardy et al., 2013). The next section reviews the modeling choices we made for the shared task, and section 3 describes our results in detail. Additional analysis and comparisons with other submitted systems are available in the s"
W15-5413,W13-1712,1,0.899865,"Missing"
W15-5413,W14-5316,1,0.880037,"Missing"
W15-5413,W14-5317,0,0.0995707,"Missing"
W15-5413,W14-5315,0,0.211933,"Missing"
W15-5413,W14-5307,0,0.334274,"Missing"
W15-5413,W15-5401,0,0.272973,"Missing"
W15-5413,N12-1006,0,0.0290851,"he 13 languages in the training set. Following some good results at last year’s evaluation (Goutte et al., 2014; Zampieri et al., 2014), we took part to this years evaluation in order to see how our system would handle the additional language pair, and the two challenges of anonymized named entities and more varied test data. In addition, we wanted to further explore the way character ngrams should be more efficiently extracted from the raw text. The overall longer term motivation is to use language and variant detection to help natural language processing, for example in machine translation (Zbib et al., 2012). Discriminating similar languages may also be a first step to identify code switching in short messages (Elfardy et al., 2013). The following section describes the models we used, and the features we extracted from the data. We then briefly describe the data we trained on (Section 3), and summarize our experimental results in Section 4. Introduction Although language identification is largely considered a solved problem in the general setting, a number of frontier cases are still under study. For example, when little data is available (eg single twitter post), when the input is mixed or when"
W16-4823,W15-5413,1,0.917986,"es from the web crawl of La Presse. We checked that the added material did not overlap with the material provided by the DSL organizers (for training or testing). For French, our training material was therefore unbalanced, with only 20k sentences for fr-FR versus 60k for fr-CA. Due to lack of time, we did not take part in the Arabic dialect sub-task, despite its great interest. 179 2.2 Features Character ngram counts have been popular and effective features since at least (Cavnar and Trenkle, 1994), and produced top results at previous evaluations (Goutte et al., 2014; Malmasi and Dras, 2015; Goutte and Leger, 2015). We therefore relied again on character ngrams. This year, however, we only used 6grams. The reasons for this choice are multiple: • Optimizing the size and combination of ngram features produces small performance improvements. However this optimization also requires significant effort and time, which we did not have this year. • In our experience from previous shared tasks, 6grams were almost always the best feature set. When they were not, they were very close. • Our main focus this year was not on maximizing performance of a single system, but on investigating the influence of training dat"
W16-4823,W14-5316,1,0.815759,"Missing"
W16-4823,L16-1284,1,0.87536,"Missing"
W16-4823,W14-1303,0,0.0300442,"listic material (language variety, frequent code switching and inventive character sequences), we also had a lot more material: segments in test set A had up to 88 words, whereas segments in test sets B1 and B2 had up to 6400. This was clearly helpful by providing better ngram statistics. It also helped that English was not among the candidate languages/variants as a lot of tweet material is clearly English. It would be interesting to check performance on single tweets. • Previous work on twitter suggested that removing hashtags and account names altogether may yield a small performance gain (Lui and Baldwin, 2014). In this work, we decided to remove the # and @ characters alone, with the motivation that the hashtag or account text itself may point to the correct variant. A systematic evaluation of the different strategies is left to future work, although based on results from Lui and Baldwin (2014), we conjecture that it is unlikely to make a significant difference. • The cross-validation estimates computed on the joint train+dev data yield optimistic estimates, especially on the open data. Although differences are expected, it is unusually large, and may suggest a domain mismatch between the test data"
W16-4823,W15-5407,0,0.158922,"sy languages (Czech and Slovak; Macedonian and Bulgarian), providing additional material for some of the harder variants (Serbo-Croat-Bosnian; Indonesian and Malay; Portuguese; Spanish), and adding new groups or variants (Mexican Spanish; French from Canada and France). Like previous years, we relied on character ngram features, and a mixture of discriminative and generative statistical classifiers. Due to lack of time, we decided to eschew a full optimization of the feature sets and model combination, despite the fact that it provided excellent results in previous years (Goutte et al., 2014; Malmasi and Dras, 2015). Instead, we focused on two issues: the influence of the amount of data on the performance (open versus closed data), and the difference between a two-stage approach (predicting language/group, then variant) and a flat approach predicting the variant directly. To be clear, the ”Advances” in the title of this paper do not relate to the performance and model we used this year, which are mostly similar to successful models of prior years. The intent is to advance our understanding of how these models works and what configurations are more effective. An overview of the results of this shared task"
W16-4823,W16-4801,0,0.0499007,"Missing"
W16-4823,W14-5307,0,0.104369,"Missing"
W16-4823,W15-5401,0,0.0602648,"Missing"
W17-5041,W17-5007,0,0.0785391,"Missing"
W17-5041,W15-0620,0,0.19401,"n the Native Language Identification (NLI) Shared Task 2017 (Malmasi et al., 2017). The task of Native Language Identification consists of predicting the native (L1) language of a foreign speaker, from textual and speech clues in a second (L2) language. Applications of this task are mostly in language learning and forensic/security, see (Malmasi, 2016, Section 1.1) for a good overview. This is an interesting example of a task that is difficult to perform for humans, especially when the number of target native languages is large. In fact, in a comparison between automated and human evaluation, Malmasi et al. (2015) could only use 5 L1 languages, 367 Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications, pages 367–373 c Copenhagen, Denmark, September 8, 2017. 2017 Association for Computational Linguistics mance on the 2013 NLI Shared Task using string kernels (Ionescu et al., 2016), considering subsequences of 5 to 9 characters. On the task of discriminating similar languages (Goutte et al., 2016), long character ngrams also reach top performance (Goutte and L´eger, 2016) using subsequences of 5 and 6 characters. We looked in more detail into how useful this type"
W17-5041,W13-1706,0,0.14526,"lso provided for both training and test data, but we did not use that information in our work. 1 Test 100 100 100 100 100 100 100 100 100 100 100 1100 Table 1: NLI-2017 collection: #doc per L1. Data and Methods 2.1 (Estimation) Train Dev 1000 100 1000 100 1000 100 1000 100 1000 100 1000 100 1000 100 1000 100 1000 100 1000 100 1000 100 11000 1100 POS: We extracted subsequences of 1 to 3 part-of-speech tags, as produced by the freely available Stanford POS tagger,2 v3.7.0 2 Speech and text prompts are different. 368 http://www-nlp.stanford.edu/software/tagger.shtml tive Language Identification (Tetreault et al., 2013; Malmasi and Dras, 2017) and many other NLP tasks (Goutte et al., 2014). Among many alternatives, we focus on voting, a conceptually and practically simple approach where each model in the ensemble casts a vote towards a class, votes are tallied and prediction goes to the most voted class. In plurality voting, all models cast a single, identical vote towards one class. Other variants weigh votes according to, for example, how confident each model is in its prediction. Two important hyper-parameters influence the resulting prediction and its quality: 1) the number of voting systems, and 2) the"
W17-5041,W16-4823,1,0.88957,"Missing"
W17-5041,N03-1033,0,0.0164431,"number of voting systems, and 2) the way these systems are selected. In a typical learning setup, it makes sense to let both of these choices be led by the resulting estimated prediction error. In previous work, we simply ranked models according to prediction error, estimated on either a separate validation/dev set or by cross-validation, and selected models in descending order of performance until the resulting combined performance started to drop. For this evaluation, we experimented with a greedy selection approach: instead of considering all models in descending order of performance, we (Toutanova et al., 2003). This produced three feature sets: pos1, pos2, pos3. For the character and word ngrams, we use a tf-idf weighting corresponding to the ltc weighting scheme (i.e. log term frequency, (log) inverse document frequency, and cosine normalization) in SMART (Manning et al., 2008, Fig. 6.7). Because most part-of-speech tags tend to occur in most documents, we did not use idf on the part-ofspeech ngrams, and only perform scaling to unit length (nnc weighting in SMART). Finally, in the SPEECH and FUSION tracks, the i-vectors were used as provided, either alone or in conjunction with another transcript"
W17-5041,W14-5316,1,0.908972,"Missing"
W17-5041,L16-1284,1,0.89957,"Missing"
W17-5041,J16-3005,0,0.0772141,"nd forensic/security, see (Malmasi, 2016, Section 1.1) for a good overview. This is an interesting example of a task that is difficult to perform for humans, especially when the number of target native languages is large. In fact, in a comparison between automated and human evaluation, Malmasi et al. (2015) could only use 5 L1 languages, 367 Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications, pages 367–373 c Copenhagen, Denmark, September 8, 2017. 2017 Association for Computational Linguistics mance on the 2013 NLI Shared Task using string kernels (Ionescu et al., 2016), considering subsequences of 5 to 9 characters. On the task of discriminating similar languages (Goutte et al., 2016), long character ngrams also reach top performance (Goutte and L´eger, 2016) using subsequences of 5 and 6 characters. We looked in more detail into how useful this type of feature could be in the context of NLI. This contrasts with many systems used in the 2013 evaluation, including ours, which used a combination of lexical and syntactic features, including short character and word ngrams, part-of-speech and syntactic dependencies. We test character ngrams up to 6grams, extrac"
W18-6480,W11-2123,0,0.0229985,"disproportionately Parallelism estimation With sentence vectors (§2.1) for the reduced corpus (§2.2) in hand, we set out to estimate the degree of parallelism of sentence pairs. A novel measure of parallelism, based on ratios of squared Mahalanobis distances, performed better on a synthetic dataset than some more obvious measurements, and the single-feature submission based on it was our best unsupervised submission. We also made several other unsupervised measurements: 2 901 https://github.com/aboSamoor/pycld2 1. Perplexity of the German sentence according to a 6-gram KenLM language model3 (Heafield, 2011) not just the one that happened to come first in the original corpus. 3 2. Perplexity of the English sentence according to a 6-gram KenLM language model Mahalanobis ratios for parallelism assessment As mentioned in §2.3, we performed several unsupervised measurements on each sentence pair; of these, the measurement that best predicted paralellism (on synthetic data and on our small 300sentence annotated set) was a novel measurement based on squared Mahalanobis distances. This measurement rests on two insights: 3. The ratio between (1) and (2), to find sentences pairs that contain different amo"
W18-6480,2005.mtsummit-papers.11,0,0.223295,"supervised entries, setting ourselves an additional constraint that we not utilize the additional clean parallel corpora. One such entry fairly consistently scored in the top ten systems in the 100M-word conditions, and for one task—translating the European Medicines Agency corpus (Tiedemann, 2009)—scored among the best systems even in the 10M-word conditions. 1 Introduction and motivation The WMT18 shared task on parallel corpus filtering assumes (but does not require) a supervised learning approach. Given 1. a set of “clean” German-English parallel corpora including past WMT data, Europarl (Koehn, 2005), etc., and 2. a large, potentially “dirty” corpus (i.e., one that may contain non-parallel data, nonlinguistic data, etc.) scraped from the internet (Koehn et al., 2018a), can one identify which sentences from (2) are clean? Supervised learning is an obvious approach in well-resourced languages like German and English, in which there exist well-cleaned parallel corpora across various domains. However, in much lower-resourced languages, we generally do not have multiple parallel corpora 2 Overall architecture The highest-ranked submission of our unsupervised submissions, NRC-seve-bicov, 1 We a"
W18-6480,W18-6481,1,0.639364,"onference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 900–907 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64107 dates, and performed comparatively poorly when tasked with training full sentences. To mitigate this, we ran an additional de-duplication step on the English side in which two sentences that differ only in numbers (e.g., “14 May 2017” and “19 May 1996”) were considered duplicates. shares the same general skeleton as NRC’s highest-ranked supervised submission, NRC-yisi-bicov (Lo et al., 2018); it differs primarily in the parallelism estimation component (§2.3). 2.1 Training sentence embeddings Without numerical de-duplication, we believe the parallelism estimation step in §2.3 would have had too much of a bias towards short numerical sentences. It is, after all, essentially just looking for sentence pairs that it considers likely given the distribution of sentence pairs in the target corpus; if the corpus has a large number of short numerical sentences (and it appears to), the measurement will come to prefer those, whether or not they are useful for the downstream task. We began b"
W18-6480,W03-0320,0,0.0898108,"craped from the internet (Koehn et al., 2018a), can one identify which sentences from (2) are clean? Supervised learning is an obvious approach in well-resourced languages like German and English, in which there exist well-cleaned parallel corpora across various domains. However, in much lower-resourced languages, we generally do not have multiple parallel corpora 2 Overall architecture The highest-ranked submission of our unsupervised submissions, NRC-seve-bicov, 1 We are thinking in particular of the English-Inuktitut translation pair, which is a long-standing research interest of NRC (e.g. Martin et al., 2003). 900 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 900–907 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64107 dates, and performed comparatively poorly when tasked with training full sentences. To mitigate this, we ran an additional de-duplication step on the English side in which two sentences that differ only in numbers (e.g., “14 May 2017” and “19 May 1996”) were considered duplicates. shares the same general skeleton as NRC’s highest-ranked supervi"
W18-6480,N18-1049,0,0.0495645,"ent (§2.3). 2.1 Training sentence embeddings Without numerical de-duplication, we believe the parallelism estimation step in §2.3 would have had too much of a bias towards short numerical sentences. It is, after all, essentially just looking for sentence pairs that it considers likely given the distribution of sentence pairs in the target corpus; if the corpus has a large number of short numerical sentences (and it appears to), the measurement will come to prefer those, whether or not they are useful for the downstream task. We began by training monolingual sentence embeddings using sent2vec (Pagliardini et al., 2018), on all available monolingual data. This included the monolingual data available in the “clean” parallel training data. That is to say, we did not completely throw out the clean parallel data for this task, we simply used it as two unaligned monolingual corpora. We trained sentence vectors of 10, 50, 100, 300, and 700 dimensions; our final submissions used the 300-dimensional vectors as a compromise between accuracy (lower-dimensional vectors had lower accuracy during sanity-checking) and efficiency (higher-dimensional vectors ended up exceeding our memory capacity in downstream components)."
W18-6481,2012.amta-papers.7,1,0.908512,"allel development corpus for tuning the weights to combine different parallelism and fluency features. 1 Introduction The WMT18 shared task on parallel corpus filtering (Koehn et al., 2018b) challenged teams to find clean sentence pairs from ParaCrawl, a humongous high-recall, low-precision web crawled parallel corpus (Koehn et al., 2018a), for training machine translation (MT) systems. Data cleanliness of parallel corpora for MT systems is affected by a wide range of factors, e.g., the parallelism of the sentence pairs, the fluency of the sentences in the output language, etc. Previous work (Goutte et al., 2012; Simard, 2014) showed that different types of errors in the parallel training data degrade MT quality at different levels. Intuitively, the crosslingual semantic textual similarity of the sentence pairs in the corpora is one of the most important factors affecting the parallelism of the target sentence pairs. Lo et al. (2016) scored crosslingual 908 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 908–916 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64108"
W18-6481,W11-2123,0,0.10867,"aining data (tokenized and lowercased) to train an 1 909 https://github.com/aboSamoor/pycld2 SMT system using Portage (Larkin et al., 2010), a conventional log-linear phrase-based SMT system. The translation model of the SMT system uses IBM4 word alignments (Brown et al., 1993) with grow-diag-final-and phrase extraction heuristics (Koehn et al., 2003). The system has two n-gram language models: a 5-gram mixture language model (LM) trained on the four corpora components using SRILM (Stolcke, 2002), and a pruned 6-gram LM trained on the WMT monolingual English training corpus built using KenLM (Heafield, 2011). The SMT system also includes a hierachical distortion model, a sparse feature model consisting of the standard sparse features proposed in Hopkins and May (2011) and sparse hierarchical distortion model features proposed in Cherry (2013), and a neural network joint model, or NNJM, with 3 words of target context and 11 words of source context, effectively a 15-gram LM (Vaswani et al., 2013; Devlin et al., 2014). The parameters of the log-linear model were tuned by optimizing BLEU on the development set (newstest2017) using the batch variant of margin infused relaxed algorithm (MIRA) by Cherry"
W18-6481,J93-2003,0,0.0529985,"placeholder token, before deciding which sentences were duplicates. Sentence pairs were filtered out if the pair was seen before or if the input side was exactly the same as the output side. 2.2.1 Parallelism YiSi-1: monolingual semantic MT evaluation metric We first used the “clean” WMT18 news translation task monolingual and parallel training data (tokenized and lowercased) to train an 1 909 https://github.com/aboSamoor/pycld2 SMT system using Portage (Larkin et al., 2010), a conventional log-linear phrase-based SMT system. The translation model of the SMT system uses IBM4 word alignments (Brown et al., 1993) with grow-diag-final-and phrase extraction heuristics (Koehn et al., 2003). The system has two n-gram language models: a 5-gram mixture language model (LM) trained on the four corpora components using SRILM (Stolcke, 2002), and a pruned 6-gram LM trained on the WMT monolingual English training corpus built using KenLM (Heafield, 2011). The SMT system also includes a hierachical distortion model, a sparse feature model consisting of the standard sparse features proposed in Hopkins and May (2011) and sparse hierarchical distortion model features proposed in Cherry (2013), and a neural network j"
W18-6481,E17-3017,0,0.0199669,"selection, it allowed for substantial BLEU score increases: +1.61 BLEU for SMT systems on average and +2.44 BLEU for NMT systems. MT quality check We used the official software to extract the 10Mword and 100M-word corpora from the original ParaCrawl according to the feature scores. We then trained SMT and NMT systems using the extracted data. The SMT systems were trained using Portage with components and parameters similar to the German-English SMT system in Williams et al. (2016). The NMT systems were transformer models with self-attention (Vaswani et al., 2017) trained using Sockeye1.18.20 (Hieber et al., 2017) with default parameter settings2 , except for the maximum sequence length, which was reduced to 60:60, and we also clip gradients to 1. We used newstest2017 and newstest2018 as the MT development and test set. Table 2 shows the BLEU scores for MT systems trained on the ParaCrawl data subselected by our scoring features. We have also included the random scoring feature (with initial filtering) as a baseline. The MT quality trained on data subselected by the feature scores showed the same trend as the results of the sanity check. That is to say, a feature that performed better in the sanity che"
W18-6481,W10-1703,0,0.0648474,"Missing"
W18-6481,D11-1125,0,0.0564437,"log-linear phrase-based SMT system. The translation model of the SMT system uses IBM4 word alignments (Brown et al., 1993) with grow-diag-final-and phrase extraction heuristics (Koehn et al., 2003). The system has two n-gram language models: a 5-gram mixture language model (LM) trained on the four corpora components using SRILM (Stolcke, 2002), and a pruned 6-gram LM trained on the WMT monolingual English training corpus built using KenLM (Heafield, 2011). The SMT system also includes a hierachical distortion model, a sparse feature model consisting of the standard sparse features proposed in Hopkins and May (2011) and sparse hierarchical distortion model features proposed in Cherry (2013), and a neural network joint model, or NNJM, with 3 words of target context and 11 words of source context, effectively a 15-gram LM (Vaswani et al., 2013; Devlin et al., 2014). The parameters of the log-linear model were tuned by optimizing BLEU on the development set (newstest2017) using the batch variant of margin infused relaxed algorithm (MIRA) by Cherry and Foster (2012). Decoding uses the cube-pruning algorithm of Huang and Chiang (2007) with a 7word distortion limit. We then translated the German side of the fi"
W18-6481,P07-1019,0,0.0250594,"sparse feature model consisting of the standard sparse features proposed in Hopkins and May (2011) and sparse hierarchical distortion model features proposed in Cherry (2013), and a neural network joint model, or NNJM, with 3 words of target context and 11 words of source context, effectively a 15-gram LM (Vaswani et al., 2013; Devlin et al., 2014). The parameters of the log-linear model were tuned by optimizing BLEU on the development set (newstest2017) using the batch variant of margin infused relaxed algorithm (MIRA) by Cherry and Foster (2012). Decoding uses the cube-pruning algorithm of Huang and Chiang (2007) with a 7word distortion limit. We then translated the German side of the filtered ParaCrawl into English. n−1 → − − sp (→ e, f)= P → − − sr (→ e, f)= P a P k=0 w(ea+k )·s(ea+k ,fb+k ) −−−−− → w − e− a..a+n−1 · max n−1 P b k=0 w(ea+k ) P − −−−−−− → w e a a..a+n−1 n−1 P −−−−−−→ k=0 w(fb+k )·s(ea+k ,fb+k ) b w fb..b+n−1 · max n−1 P a k=0 w(fb+k ) −−−−−−→ P b w fb..b+n−1 −→ −−−→ precision = sp (− e− sent , fsent ) −→ −−−→ recall = sr (− e− sent , fsent ) precision · recall YiSi-1 = α · precision + (1 − α) · recall YiSi-1 srl measures the semantic similarity with additional frame semantic or"
W18-6481,W12-3102,0,0.0959196,"Missing"
W18-6481,W18-6453,0,0.0549896,". In fact, our best performing system—NRC-yisi-bicov is one of the only four submissions ranked top 10 in both evaluations. Our submitted systems also include some initial filtering steps for scaling down the size of the test corpus and a final redundancy removal step for better semantic and token coverage of the filtered corpus. In this paper, we also describe our unsuccessful attempt in automatically synthesizing a noisy parallel development corpus for tuning the weights to combine different parallelism and fluency features. 1 Introduction The WMT18 shared task on parallel corpus filtering (Koehn et al., 2018b) challenged teams to find clean sentence pairs from ParaCrawl, a humongous high-recall, low-precision web crawled parallel corpus (Koehn et al., 2018a), for training machine translation (MT) systems. Data cleanliness of parallel corpora for MT systems is affected by a wide range of factors, e.g., the parallelism of the sentence pairs, the fluency of the sentences in the output language, etc. Previous work (Goutte et al., 2012; Simard, 2014) showed that different types of errors in the parallel training data degrade MT quality at different levels. Intuitively, the crosslingual semantic textua"
W18-6481,W11-2103,0,0.0555991,"Missing"
W18-6481,N03-1017,0,0.0132764,"ce pairs were filtered out if the pair was seen before or if the input side was exactly the same as the output side. 2.2.1 Parallelism YiSi-1: monolingual semantic MT evaluation metric We first used the “clean” WMT18 news translation task monolingual and parallel training data (tokenized and lowercased) to train an 1 909 https://github.com/aboSamoor/pycld2 SMT system using Portage (Larkin et al., 2010), a conventional log-linear phrase-based SMT system. The translation model of the SMT system uses IBM4 word alignments (Brown et al., 1993) with grow-diag-final-and phrase extraction heuristics (Koehn et al., 2003). The system has two n-gram language models: a 5-gram mixture language model (LM) trained on the four corpora components using SRILM (Stolcke, 2002), and a pruned 6-gram LM trained on the WMT monolingual English training corpus built using KenLM (Heafield, 2011). The SMT system also includes a hierachical distortion model, a sparse feature model consisting of the standard sparse features proposed in Hopkins and May (2011) and sparse hierarchical distortion model features proposed in Cherry (2013), and a neural network joint model, or NNJM, with 3 words of target context and 11 words of source"
W18-6481,N13-1003,0,0.0199856,"word alignments (Brown et al., 1993) with grow-diag-final-and phrase extraction heuristics (Koehn et al., 2003). The system has two n-gram language models: a 5-gram mixture language model (LM) trained on the four corpora components using SRILM (Stolcke, 2002), and a pruned 6-gram LM trained on the WMT monolingual English training corpus built using KenLM (Heafield, 2011). The SMT system also includes a hierachical distortion model, a sparse feature model consisting of the standard sparse features proposed in Hopkins and May (2011) and sparse hierarchical distortion model features proposed in Cherry (2013), and a neural network joint model, or NNJM, with 3 words of target context and 11 words of source context, effectively a 15-gram LM (Vaswani et al., 2013; Devlin et al., 2014). The parameters of the log-linear model were tuned by optimizing BLEU on the development set (newstest2017) using the batch variant of margin infused relaxed algorithm (MIRA) by Cherry and Foster (2012). Decoding uses the cube-pruning algorithm of Huang and Chiang (2007) with a 7word distortion limit. We then translated the German side of the filtered ParaCrawl into English. n−1 → − − sp (→ e, f)= P → − − sr (→ e, f)= P"
W18-6481,N12-1047,0,0.0247327,"2011). The SMT system also includes a hierachical distortion model, a sparse feature model consisting of the standard sparse features proposed in Hopkins and May (2011) and sparse hierarchical distortion model features proposed in Cherry (2013), and a neural network joint model, or NNJM, with 3 words of target context and 11 words of source context, effectively a 15-gram LM (Vaswani et al., 2013; Devlin et al., 2014). The parameters of the log-linear model were tuned by optimizing BLEU on the development set (newstest2017) using the batch variant of margin infused relaxed algorithm (MIRA) by Cherry and Foster (2012). Decoding uses the cube-pruning algorithm of Huang and Chiang (2007) with a 7word distortion limit. We then translated the German side of the filtered ParaCrawl into English. n−1 → − − sp (→ e, f)= P → − − sr (→ e, f)= P a P k=0 w(ea+k )·s(ea+k ,fb+k ) −−−−− → w − e− a..a+n−1 · max n−1 P b k=0 w(ea+k ) P − −−−−−− → w e a a..a+n−1 n−1 P −−−−−−→ k=0 w(fb+k )·s(ea+k ,fb+k ) b w fb..b+n−1 · max n−1 P a k=0 w(fb+k ) −−−−−−→ P b w fb..b+n−1 −→ −−−→ precision = sp (− e− sent , fsent ) −→ −−−→ recall = sr (− e− sent , fsent ) precision · recall YiSi-1 = α · precision + (1 − α) · recall YiSi-1 s"
W18-6481,P14-1129,0,0.126783,"Missing"
W18-6481,W18-6480,1,0.552227,"he larger one. When we evaluate MT output in practice, YiSi score is a weighted harmonic mean of the precision and recall. However, in this work, we segregated the precision and recall of YiSi into separate features as we planned to let the regression decide suitable weights to combine them. Further details of YiSi are provided in Lo (2018). Distance of sentence vectors Sentence vectors were trained using sent2vec (Pagliardini et al., 2018) on each side of the “clean” parallel WMT18 news translation task parallel training data. Further details on how to compute these features are described in Littell et al. (2018). YiSi-2: crosslingual semantic MT evaluation metric For the crosslingual version of YiSi, YiSi-2, instead of training a German-English MT system, we used the “clean” WMT18 news translation task parallel training data to train bilingual word embeddings using bivec (Luong et al., 2015) for evaluating crosslingual lexical semantic similarity. Similar to YiSi-1, YiSi-2 precision and recall are the weighted sum of the crosslingual lexical semantic similarity of the sentence pairs over the weighted count of tokens in the German and English sentences respectively. In this work, we set the n-gram siz"
W18-6481,W17-4767,1,0.789134,"egmental semantic precision of the semantic role fillers according to the shallow semantic structure parsed by the mateplus (Roth and Woodsend, 2014) English semantic parser over the weighted counts of roles and frames according to the shallow semantic structure of the MT output and similarly, for the frame semantic recall. Precisely, YiSi-1 srl is computed as follows: We also used the monolingual English data to train word embeddings using word2vec (Mikolov et al., 2013) for evaluating monolingual lexical semantic similarity. YiSi is new a semantic MT evaluation metric inspired by MEANT 2.0 (Lo, 2017). YiSi1 is equivalent to MEANT 2.0-nosrl. It measures the segmental semantic similarity. The segmental semantic precision and recall divide the inverse-document-frequency weighted sum of the n-gram lexical semantic similarity of the MT output and the English sentence of the target pair by the weighted count of n-grams in the MT output and the English sentences, respectively. In this work, we set the n-gram size to two. Precisely, YiSi-1 is computed as follows: 0 qi,j = ARG j of aligned frame i in MT 1 qi,j = ARG j of aligned frame i in REF #tokens filled in aligned frame i of MT = total #token"
W18-6481,D13-1140,0,0.0158726,"e models: a 5-gram mixture language model (LM) trained on the four corpora components using SRILM (Stolcke, 2002), and a pruned 6-gram LM trained on the WMT monolingual English training corpus built using KenLM (Heafield, 2011). The SMT system also includes a hierachical distortion model, a sparse feature model consisting of the standard sparse features proposed in Hopkins and May (2011) and sparse hierarchical distortion model features proposed in Cherry (2013), and a neural network joint model, or NNJM, with 3 words of target context and 11 words of source context, effectively a 15-gram LM (Vaswani et al., 2013; Devlin et al., 2014). The parameters of the log-linear model were tuned by optimizing BLEU on the development set (newstest2017) using the batch variant of margin infused relaxed algorithm (MIRA) by Cherry and Foster (2012). Decoding uses the cube-pruning algorithm of Huang and Chiang (2007) with a 7word distortion limit. We then translated the German side of the filtered ParaCrawl into English. n−1 → − − sp (→ e, f)= P → − − sr (→ e, f)= P a P k=0 w(ea+k )·s(ea+k ,fb+k ) −−−−− → w − e− a..a+n−1 · max n−1 P b k=0 w(ea+k ) P − −−−−−− → w e a a..a+n−1 n−1 P −−−−−−→ k=0 w(fb+k )·s(ea+k ,fb+"
W18-6481,S16-1102,1,0.839606,"Missing"
W18-6481,C96-2141,0,0.302528,"nguage. The target scores of these pairs are proportional to the percentage of tokens offset, deleted or introduced. Lastly, misaligned sentence pairs were added as fluent but non-parallel negative examples. The resulting development set had 11k sentence pairs of positive and synthetic negative examples. Alignment scores The SMT model trained on the “clean” WMT18 news translation task parallel training data for YiSi score computation include several alignment models as components, from which probabilities p(d|e) and p(e|d) were computed. We find the hidden markov model (HMM) alignment models (Vogel et al., 1996) are reliably useful for scoring parallelism of the sentence pairs in the target corpus. Perplexity ratio of input sentences and output sentences The perplexity ratio reflects the different amounts of information contained in each side of the sentence pairs. This is computed by dividing the smaller perplexity score of the two sentences in the target pair by the larger one. Thus, the ratio ranged from 0 to 1, where a larger value represents better parallelism. 911 features baselines random hunalign parallelism YiSi-1 precision YiSi-1 recall YiSi-1 srl (β=1) precision YiSi-1 srl (β=1) recall YiS"
W18-6481,W15-1521,0,0.159484,"Missing"
W18-6481,W16-2327,0,0.0946713,"M words) selections, the redundancy removal had virtually no effect when applied after YiSi scoring. However, on the smaller (10M words) selection, it allowed for substantial BLEU score increases: +1.61 BLEU for SMT systems on average and +2.44 BLEU for NMT systems. MT quality check We used the official software to extract the 10Mword and 100M-word corpora from the original ParaCrawl according to the feature scores. We then trained SMT and NMT systems using the extracted data. The SMT systems were trained using Portage with components and parameters similar to the German-English SMT system in Williams et al. (2016). The NMT systems were transformer models with self-attention (Vaswani et al., 2017) trained using Sockeye1.18.20 (Hieber et al., 2017) with default parameter settings2 , except for the maximum sequence length, which was reduced to 60:60, and we also clip gradients to 1. We used newstest2017 and newstest2018 as the MT development and test set. Table 2 shows the BLEU scores for MT systems trained on the ParaCrawl data subselected by our scoring features. We have also included the random scoring feature (with initial filtering) as a baseline. The MT quality trained on data subselected by the fea"
W18-6481,N18-1049,0,0.039983,"evious feature, the perplexity ratio of the input and output sentences POS tags is computed by dividing the smaller POS perplexity score of the two sentences in the target pair by the larger one. When we evaluate MT output in practice, YiSi score is a weighted harmonic mean of the precision and recall. However, in this work, we segregated the precision and recall of YiSi into separate features as we planned to let the regression decide suitable weights to combine them. Further details of YiSi are provided in Lo (2018). Distance of sentence vectors Sentence vectors were trained using sent2vec (Pagliardini et al., 2018) on each side of the “clean” parallel WMT18 news translation task parallel training data. Further details on how to compute these features are described in Littell et al. (2018). YiSi-2: crosslingual semantic MT evaluation metric For the crosslingual version of YiSi, YiSi-2, instead of training a German-English MT system, we used the “clean” WMT18 news translation task parallel training data to train bilingual word embeddings using bivec (Luong et al., 2015) for evaluating crosslingual lexical semantic similarity. Similar to YiSi-1, YiSi-2 precision and recall are the weighted sum of the cross"
W18-6481,P02-1040,0,0.115557,"on processing only, and includes non-parallel, or even non-linguistic data. It contains 104 million German-English sentence pairs, with 1 billion English tokens and 964 million German tokens before punctuation tokenization. A 10-million-word (10M-word) and a 100-millionword (100M-word) corpus sub-selected by the participating cleanliness scoring system were used to train statistical machine translation (SMT) and neural machine translation (NMT) systems. The success of the participating scoring systems was determined by the quality of the MT output from the four MT systems as measured by BLEU (Papineni et al., 2002) on some in-domain and out-ofdomain evaluation sets. In this paper, we describe the efforts in developing our supervised submissions: the initial filWe present our semantic textual similarity approach in filtering a noisy web crawled parallel corpus using YiSi—a novel semantic machine translation evaluation metric. The systems mainly based on this supervised approach perform well in the WMT18 Parallel Corpus Filtering shared task (4th place in 100-millionword evaluation, 8th place in 10-million-word evaluation, and 6th place overall, out of 48 submissions). In fact, our best performing system—"
W18-6481,D14-1045,0,0.0312771,"nformation. It uses a more principle way to compute the precision and recall of semantic similarity between the translation output and the reference when comparing to MEANT 2.0. Instead of aggregating the precision and recall at the segmental semantic similarity level, YiSi-1 srl precision is the weighted sum of the segmental semantic precision and the frame semantic precision and similarly, for YiSi-1 srl recall. The frame semantic precision is the weighted sum of the segmental semantic precision of the semantic role fillers according to the shallow semantic structure parsed by the mateplus (Roth and Woodsend, 2014) English semantic parser over the weighted counts of roles and frames according to the shallow semantic structure of the MT output and similarly, for the frame semantic recall. Precisely, YiSi-1 srl is computed as follows: We also used the monolingual English data to train word embeddings using word2vec (Mikolov et al., 2013) for evaluating monolingual lexical semantic similarity. YiSi is new a semantic MT evaluation metric inspired by MEANT 2.0 (Lo, 2017). YiSi1 is equivalent to MEANT 2.0-nosrl. It measures the segmental semantic similarity. The segmental semantic precision and recall divide"
W18-6481,2014.amta-researchers.6,1,0.700032,"pus for tuning the weights to combine different parallelism and fluency features. 1 Introduction The WMT18 shared task on parallel corpus filtering (Koehn et al., 2018b) challenged teams to find clean sentence pairs from ParaCrawl, a humongous high-recall, low-precision web crawled parallel corpus (Koehn et al., 2018a), for training machine translation (MT) systems. Data cleanliness of parallel corpora for MT systems is affected by a wide range of factors, e.g., the parallelism of the sentence pairs, the fluency of the sentences in the output language, etc. Previous work (Goutte et al., 2012; Simard, 2014) showed that different types of errors in the parallel training data degrade MT quality at different levels. Intuitively, the crosslingual semantic textual similarity of the sentence pairs in the corpora is one of the most important factors affecting the parallelism of the target sentence pairs. Lo et al. (2016) scored crosslingual 908 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 908–916 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64108 We also observ"
