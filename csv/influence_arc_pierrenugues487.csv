2005.jeptalnrecital-long.12,J93-4006,0,0.21,"Missing"
2007.jeptalnrecital-long.33,granfeldt-etal-2006-cefle,1,0.878822,"Missing"
2007.jeptalnrecital-long.33,P05-1025,0,0.0606608,"Missing"
2020.lrec-1.554,N19-4010,0,0.0113732,"nce model with attention, converting an input token sequence into a label sequence. This sequence-to-sequence model makes use of multiple contextualized word embeddings as input. Finally, a recent system by Jiang et al. (2019) improved the state-of-the-art performance on the CoNLL03 task with a differential neural network search method. Word embeddings are a key ingredient to NER; the most commonly used word embedding started with Mikolov et al. (2013), followed by Pennington et al. (2014), to more recent developments by Mikolov et al. (2018), and deeper models by Peters et al. (2018). Flair (Akbik et al., 2019) is a recent NLP framework that provides a simplified interface to many state-of-the-art word embeddings. Modern entity linking uses a variety of methods such as simple classification models (Bunescu and Pas¸ca, 2006; Cucerzan, 2007; Milne and Witten, 2008), end-to-end linkage with a voting scheme for linkage (Ferragina and Scaiella, 2010), graphical models (Hoffart et al., 2011; Guo and Barbosa, 2014), integer linear programming (Cheng and Roth, 2013), fully probabilistic models (Ganea et al., 2016) to deeper neural models (Ganea and Hofmann, 2017). Wainwright et al. (2008)4 proved that entit"
2020.lrec-1.554,E06-1002,0,0.148097,"Missing"
2020.lrec-1.554,D13-1184,0,0.0231939,"3), followed by Pennington et al. (2014), to more recent developments by Mikolov et al. (2018), and deeper models by Peters et al. (2018). Flair (Akbik et al., 2019) is a recent NLP framework that provides a simplified interface to many state-of-the-art word embeddings. Modern entity linking uses a variety of methods such as simple classification models (Bunescu and Pas¸ca, 2006; Cucerzan, 2007; Milne and Witten, 2008), end-to-end linkage with a voting scheme for linkage (Ferragina and Scaiella, 2010), graphical models (Hoffart et al., 2011; Guo and Barbosa, 2014), integer linear programming (Cheng and Roth, 2013), fully probabilistic models (Ganea et al., 2016) to deeper neural models (Ganea and Hofmann, 2017). Wainwright et al. (2008)4 proved that entity linkage which tries to maximize local and global agreement jointly during linkage is NP-hard to solve, which most authors approximate or simplify to reach feasibility. The system described in this paper, Hedwig, builds on Klang and Nugues (2018) with significant improvements and contributions in four areas: 1. Updated data sources and preprocessing; 2. A named entity recognizer based on a BILSTM neural network architecture; 3. The linker considers a"
2020.lrec-1.554,Q16-1026,0,0.0293661,"tity Recognition (Tjong Kim Sang and De Meulder, 2003), where the best model used simple linear classifiers (Florian et al., 2003). Neural models, starting with feed forward architectures, improved the recognition performance. Examples of such models include Collobert et al. (2011) and the exponential weight encoding method (Fixed-Size Ordinally-Forgetting Encoding, FOFE) (Zhang et al., 2015). These architectures were ultimately surpassed by deeper recurrent neural models using LSTMs (Hochreiter and Schmidhuber, 1997) and CRFs in different combinations with or without word character encoders (Chiu and Nichols, 2016; Ma and Hovy, 2016). Strakov´a et al. (2019) proposed a sequence to sequence model with attention, converting an input token sequence into a label sequence. This sequence-to-sequence model makes use of multiple contextualized word embeddings as input. Finally, a recent system by Jiang et al. (2019) improved the state-of-the-art performance on the CoNLL03 task with a differential neural network search method. Word embeddings are a key ingredient to NER; the most commonly used word embedding started with Mikolov et al. (2013), followed by Pennington et al. (2014), to more recent developments by"
2020.lrec-1.554,D07-1074,0,0.495416,"locations, etc. Take for instance the named entity of class location: “New York”. This mention can refer to the state1 of New York or the large city2 situated in that particular state. For the latter, a matching unique identifier could be the English Wikipedia label: New_York_City. A typical NEL pipeline consists of many phases including a name finding, mention detection (MD) phase (e.g. detecting “New York” in a text), a candidate generation (CD) phase (e.g. state or city), and an entity linking (EL) phase (e.g. assigning the label). In addition, these phases might be defined independently (Cucerzan, 2007), or trained jointly (Ganea and Hofmann, 2017). The MD phase is frequently a named entity recognizer (NER), which finds and classifies spans of strings in a set of predefined classes such as persons, organization, location, etc. The CD phase uses the classified mention as input, possibly with context, and from this information generates a list of entity candidates. Finally, the entity linking phase ranks and selects the most probable or coherent set of candidate entities. It assigns each mention a label, which corresponds to the unique identifier of the the selected candidate. The unique ident"
2020.lrec-1.554,W03-0425,0,0.0749283,"Missing"
2020.lrec-1.554,D17-1277,0,0.0808081,"e named entity of class location: “New York”. This mention can refer to the state1 of New York or the large city2 situated in that particular state. For the latter, a matching unique identifier could be the English Wikipedia label: New_York_City. A typical NEL pipeline consists of many phases including a name finding, mention detection (MD) phase (e.g. detecting “New York” in a text), a candidate generation (CD) phase (e.g. state or city), and an entity linking (EL) phase (e.g. assigning the label). In addition, these phases might be defined independently (Cucerzan, 2007), or trained jointly (Ganea and Hofmann, 2017). The MD phase is frequently a named entity recognizer (NER), which finds and classifies spans of strings in a set of predefined classes such as persons, organization, location, etc. The CD phase uses the classified mention as input, possibly with context, and from this information generates a list of entity candidates. Finally, the entity linking phase ranks and selects the most probable or coherent set of candidate entities. It assigns each mention a label, which corresponds to the unique identifier of the the selected candidate. The unique identifier can be local or global, and its concrete"
2020.lrec-1.554,W18-2501,0,0.0211211,"Missing"
2020.lrec-1.554,P16-1059,0,0.0176751,"enable efficient cluster processing. We converted this dump with a Wikidata parser, which transformed the JSON dump into Parquet files for further processing and information extraction. The information we converted was: Qnumber, description, alias, claims also known as properties and sitelinks. A subset of most common claim datatypes are supported; the rest is either ignored or encoded as plain strings. 3.2. Wikipedia We scraped the nine editions using the REST API by first downloading a list of page names from Wikimedia’s dump 4 3 Data 5 https://developers.google.com/freebase/ 4502 cited in Globerson et al. (2016). https://[lang].wikipedia.org/api/rest v1/ site6 , more specifically the [lang]wiki-[date]-all-titles-inns0.gz file which contains all page labels7 . We then processed this file in sequence, where we downloaded pages in parallel with a custom Python tool. This list of page names is slightly out of sync with the online version, resulting in some nonexisting pages; pages not found were ignored. Pages that failed to download due to server errors or network failure were retried once more at the end of the scraping process. In total, the retries amounted to 5-250 pages; most of these pages were ev"
2020.lrec-1.554,D11-1072,0,0.144531,"Missing"
2020.lrec-1.554,D19-1367,0,0.021056,"Missing"
2020.lrec-1.554,L18-1540,1,0.848965,"an, 2007; Milne and Witten, 2008), end-to-end linkage with a voting scheme for linkage (Ferragina and Scaiella, 2010), graphical models (Hoffart et al., 2011; Guo and Barbosa, 2014), integer linear programming (Cheng and Roth, 2013), fully probabilistic models (Ganea et al., 2016) to deeper neural models (Ganea and Hofmann, 2017). Wainwright et al. (2008)4 proved that entity linkage which tries to maximize local and global agreement jointly during linkage is NP-hard to solve, which most authors approximate or simplify to reach feasibility. The system described in this paper, Hedwig, builds on Klang and Nugues (2018) with significant improvements and contributions in four areas: 1. Updated data sources and preprocessing; 2. A named entity recognizer based on a BILSTM neural network architecture; 3. The linker considers a larger candidate graph with more features, and an improved PageRank solver; 4. Finally, the introduction of an entity type mapping that can remove unwanted entities with no relevance to the evaluation. 3. In the making of Hedwig, we used these data sources: • Nine Wikipedia editions: en, es, fr, de, sv, ru, zh, da, no, scraped using the Wikipedia REST API5 in October 2018. • Wikidata JSON"
2020.lrec-1.554,W19-6148,1,0.813892,"ocria layers; 2. Link, resolving Wikipedia anchors to Wikidata. In the import step, we parse HTML using JSoup9 , which converts the raw HTML string into a structured Document Object Model (DOM). We used rules applied recursively to the DOM tree to filter out the markup and produced a flattened document consisting only of readable text. In addition, during the flattening process or tree traversal, enough information was retained to produce multiple layers of spans with added metadata covering paragraphs, sections, anchors, lists, tables, italic, bold etc. These layers were stored using Docria (Klang and Nugues, 2019) that can represent this type of data and store exact string mappings. With the exception of the Chinese version, all the editions were parsed identically. The Chinese version required a pre-conversion step to produce a Mainland Chinese version using the provided translation table; this conversion might not be perfectly accurate. The link step converts all the anchors in the processed Wikipedia dump into Q-numbers using sitelinks in the Wikidata dump. It is necessary to do this link step for two reasons: links in Wikipedia refer to pages using titles not unique identifiers, and many links are"
2020.lrec-1.554,P16-1101,0,0.020017,"Kim Sang and De Meulder, 2003), where the best model used simple linear classifiers (Florian et al., 2003). Neural models, starting with feed forward architectures, improved the recognition performance. Examples of such models include Collobert et al. (2011) and the exponential weight encoding method (Fixed-Size Ordinally-Forgetting Encoding, FOFE) (Zhang et al., 2015). These architectures were ultimately surpassed by deeper recurrent neural models using LSTMs (Hochreiter and Schmidhuber, 1997) and CRFs in different combinations with or without word character encoders (Chiu and Nichols, 2016; Ma and Hovy, 2016). Strakov´a et al. (2019) proposed a sequence to sequence model with attention, converting an input token sequence into a label sequence. This sequence-to-sequence model makes use of multiple contextualized word embeddings as input. Finally, a recent system by Jiang et al. (2019) improved the state-of-the-art performance on the CoNLL03 task with a differential neural network search method. Word embeddings are a key ingredient to NER; the most commonly used word embedding started with Mikolov et al. (2013), followed by Pennington et al. (2014), to more recent developments by Mikolov et al. (201"
2020.lrec-1.554,L18-1008,0,0.0228939,"Ma and Hovy, 2016). Strakov´a et al. (2019) proposed a sequence to sequence model with attention, converting an input token sequence into a label sequence. This sequence-to-sequence model makes use of multiple contextualized word embeddings as input. Finally, a recent system by Jiang et al. (2019) improved the state-of-the-art performance on the CoNLL03 task with a differential neural network search method. Word embeddings are a key ingredient to NER; the most commonly used word embedding started with Mikolov et al. (2013), followed by Pennington et al. (2014), to more recent developments by Mikolov et al. (2018), and deeper models by Peters et al. (2018). Flair (Akbik et al., 2019) is a recent NLP framework that provides a simplified interface to many state-of-the-art word embeddings. Modern entity linking uses a variety of methods such as simple classification models (Bunescu and Pas¸ca, 2006; Cucerzan, 2007; Milne and Witten, 2008), end-to-end linkage with a voting scheme for linkage (Ferragina and Scaiella, 2010), graphical models (Hoffart et al., 2011; Guo and Barbosa, 2014), integer linear programming (Cheng and Roth, 2013), fully probabilistic models (Ganea et al., 2016) to deeper neural models"
2020.lrec-1.554,D14-1162,0,0.0794189,"Missing"
2020.lrec-1.554,N18-1202,0,0.0173426,"proposed a sequence to sequence model with attention, converting an input token sequence into a label sequence. This sequence-to-sequence model makes use of multiple contextualized word embeddings as input. Finally, a recent system by Jiang et al. (2019) improved the state-of-the-art performance on the CoNLL03 task with a differential neural network search method. Word embeddings are a key ingredient to NER; the most commonly used word embedding started with Mikolov et al. (2013), followed by Pennington et al. (2014), to more recent developments by Mikolov et al. (2018), and deeper models by Peters et al. (2018). Flair (Akbik et al., 2019) is a recent NLP framework that provides a simplified interface to many state-of-the-art word embeddings. Modern entity linking uses a variety of methods such as simple classification models (Bunescu and Pas¸ca, 2006; Cucerzan, 2007; Milne and Witten, 2008), end-to-end linkage with a voting scheme for linkage (Ferragina and Scaiella, 2010), graphical models (Hoffart et al., 2011; Guo and Barbosa, 2014), integer linear programming (Cheng and Roth, 2013), fully probabilistic models (Ganea et al., 2016) to deeper neural models (Ganea and Hofmann, 2017). Wainwright et a"
2020.lrec-1.554,P19-1527,0,0.0371204,"Missing"
2020.lrec-1.554,W03-0419,0,0.238297,"Missing"
2020.lrec-1.554,P15-2081,0,0.0119441,"sented by multiple Freebase entries. We resolved them heuristically using the lowest Q-number. 2. Related Work Multilingual named entity recognition has its modern roots with the CoNLL03 task of Language-Independent Named Entity Recognition (Tjong Kim Sang and De Meulder, 2003), where the best model used simple linear classifiers (Florian et al., 2003). Neural models, starting with feed forward architectures, improved the recognition performance. Examples of such models include Collobert et al. (2011) and the exponential weight encoding method (Fixed-Size Ordinally-Forgetting Encoding, FOFE) (Zhang et al., 2015). These architectures were ultimately surpassed by deeper recurrent neural models using LSTMs (Hochreiter and Schmidhuber, 1997) and CRFs in different combinations with or without word character encoders (Chiu and Nichols, 2016; Ma and Hovy, 2016). Strakov´a et al. (2019) proposed a sequence to sequence model with attention, converting an input token sequence into a label sequence. This sequence-to-sequence model makes use of multiple contextualized word embeddings as input. Finally, a recent system by Jiang et al. (2019) improved the state-of-the-art performance on the CoNLL03 task with a dif"
berglund-etal-2006-extraction,W01-1311,0,\N,Missing
berglund-etal-2006-extraction,P04-1074,0,\N,Missing
berglund-etal-2006-extraction,N03-2019,0,\N,Missing
berglund-etal-2006-extraction,J88-2006,0,\N,Missing
berglund-etal-2006-extraction,P05-3021,0,\N,Missing
berglund-etal-2006-extraction,E95-1035,0,\N,Missing
berglund-etal-2006-extraction,N04-1020,0,\N,Missing
C08-1050,P98-1013,0,0.28491,"ntax was used in the best-performing system in the SemEval-2007 task on Frame-semantic Structure Extraction (Baker et al., 2007), and the conversion method (in two different forms) was used for the English data in the CoNLL Shared Tasks of 2007 and 2008. 3 Automatic Semantic Role Labeling with Constituents and Dependencies To study the influence of syntactic representation on SRL performance, we developed a framework that could be easily parametrized to process either constituent or dependency input1 . This section describes its implementation. As the role-semantic paradigm, we used FrameNet (Baker et al., 1998). 3.1 Systems We built SRL systems based on six different parsers. All parsers were trained on the Penn Treebank, either directly for the constituent parsers or through the LTH constituent-to-dependency converter (Johansson and Nugues, 2007). Our systems are identified as follows: LTH. A dependency-based system using the LTH parser (Johansson and Nugues, 2008). Malt. A dependency-based system using MaltParser (Nivre et al., 2007). MST. A dependency-based system using MSTParser (McDonald et al., 2005). C&J. A constituent-based system using the reranking parser (the May 2006 version) by Charniak"
C08-1050,S07-1018,0,0.293971,"re robust to domain changes and makes them learn more efficiently with respect to the amount of training data. 1 Introduction The role-semantic paradigm has a long and rich history in linguistics, and the NLP community has recently devoted much attention to developing accurate and robust methods for performing c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. role-semantic analysis automatically (Gildea and Jurafsky, 2002; Litkowski, 2004; Carreras and Màrquez, 2005; Baker et al., 2007). It is widely conjectured that an increased SRL accuracy will lead to improvements in certain NLP applications, especially template-filling systems. SRL has also been used in prototypes of more advanced semantics-based applications such as textual entailment recognition. It has previously been shown that SRL systems need a syntactic structure as input (Gildea and Palmer, 2002; Punyakanok et al., 2008). An important consideration is then what information this input should represent. By habit, most systems for automatic role-semantic analysis have used Pennstyle constituents (Marcus et al., 199"
C08-1050,W06-2920,0,0.0142862,"s then what information this input should represent. By habit, most systems for automatic role-semantic analysis have used Pennstyle constituents (Marcus et al., 1993) produced by Collins’ (1997) or Charniak’s (2000) parsers. The influence of the syntactic formalism on SRL has only been considered in a few previous articles. For instance, Gildea and Hockenmaier (2003) reported that a CCG-based parser gives improved results over the Collins parser. Dependency syntax has only received little attention for the SRL task, despite a surge of interest in dependency parsing during the last few years (Buchholz and Marsi, 2006). Early examples of dependency-based SRL systems, which used goldstandard dependency treebanks, include Žabokrtský et al. (2002) and Hacioglu (2004). Two studies that compared the respective performances of constituent-based and dependency-based SRL systems (Pradhan et al., 2005; Swanson and Gordon, 2006), both using automatic parsers, reported that the constituent-based systems outperformed the dependency-based ones by a very wide margin. However, the figures reported in these studies can be misleading since the comparison involved a 10-year-old rule-based dependency parser versus a state-of-"
C08-1050,W08-2123,1,0.772855,"tactic representation on SRL performance, we developed a framework that could be easily parametrized to process either constituent or dependency input1 . This section describes its implementation. As the role-semantic paradigm, we used FrameNet (Baker et al., 1998). 3.1 Systems We built SRL systems based on six different parsers. All parsers were trained on the Penn Treebank, either directly for the constituent parsers or through the LTH constituent-to-dependency converter (Johansson and Nugues, 2007). Our systems are identified as follows: LTH. A dependency-based system using the LTH parser (Johansson and Nugues, 2008). Malt. A dependency-based system using MaltParser (Nivre et al., 2007). MST. A dependency-based system using MSTParser (McDonald et al., 2005). C&J. A constituent-based system using the reranking parser (the May 2006 version) by Charniak and Johnson (2005). Charniak. A constituent-based system using Charniak’s parser (Charniak, 2000). Collins. A constituent-based system using Collins’ parser (Collins, 1997). 2 Statistical Dependency Parsing for English Except for small-scale efforts, there is no dependency treebank of significant size for English. Statistical dependency parsers of English mus"
C08-1050,W04-0803,0,0.0283014,"on lexicalized features, which makes them more robust to domain changes and makes them learn more efficiently with respect to the amount of training data. 1 Introduction The role-semantic paradigm has a long and rich history in linguistics, and the NLP community has recently devoted much attention to developing accurate and robust methods for performing c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. role-semantic analysis automatically (Gildea and Jurafsky, 2002; Litkowski, 2004; Carreras and Màrquez, 2005; Baker et al., 2007). It is widely conjectured that an increased SRL accuracy will lead to improvements in certain NLP applications, especially template-filling systems. SRL has also been used in prototypes of more advanced semantics-based applications such as textual entailment recognition. It has previously been shown that SRL systems need a syntactic structure as input (Gildea and Palmer, 2002; Punyakanok et al., 2008). An important consideration is then what information this input should represent. By habit, most systems for automatic role-semantic analysis hav"
C08-1050,J93-2004,0,0.0403742,"aker et al., 2007). It is widely conjectured that an increased SRL accuracy will lead to improvements in certain NLP applications, especially template-filling systems. SRL has also been used in prototypes of more advanced semantics-based applications such as textual entailment recognition. It has previously been shown that SRL systems need a syntactic structure as input (Gildea and Palmer, 2002; Punyakanok et al., 2008). An important consideration is then what information this input should represent. By habit, most systems for automatic role-semantic analysis have used Pennstyle constituents (Marcus et al., 1993) produced by Collins’ (1997) or Charniak’s (2000) parsers. The influence of the syntactic formalism on SRL has only been considered in a few previous articles. For instance, Gildea and Hockenmaier (2003) reported that a CCG-based parser gives improved results over the Collins parser. Dependency syntax has only received little attention for the SRL task, despite a surge of interest in dependency parsing during the last few years (Buchholz and Marsi, 2006). Early examples of dependency-based SRL systems, which used goldstandard dependency treebanks, include Žabokrtský et al. (2002) and Hacioglu"
C08-1050,P05-1012,0,0.0949956,"rthermore, we show that semantic role classifiers using a dependency parser learn faster than their constituentbased counterparts and therefore need less training data to achieve similar performances. Finally, dependency-based role classifiers are more robust to vocabulary change and outperform constituentbased systems when using out-of-domain test sets. its roots in lexicalized constituent parsing (Magerman, 1994; Collins, 1997). The head rules created by Yamada and Matsumoto (2003) have been used in almost all recent work on statistical dependency parsing of English (Nivre and Scholz, 2004; McDonald et al., 2005). Recently, Johansson and Nugues (2007) extended the head percolation strategy to incorporate long-distance links such as wh-movement and topicalization, and used the full set of grammatical function tags from Penn in addition to a number of inferred tags (in total 57 function tags). A dependency parser based on this syntax was used in the best-performing system in the SemEval-2007 task on Frame-semantic Structure Extraction (Baker et al., 2007), and the conversion method (in two different forms) was used for the English data in the CoNLL Shared Tasks of 2007 and 2008. 3 Automatic Semantic Rol"
C08-1050,C04-1010,0,0.00978901,"rform nearly as well. Furthermore, we show that semantic role classifiers using a dependency parser learn faster than their constituentbased counterparts and therefore need less training data to achieve similar performances. Finally, dependency-based role classifiers are more robust to vocabulary change and outperform constituentbased systems when using out-of-domain test sets. its roots in lexicalized constituent parsing (Magerman, 1994; Collins, 1997). The head rules created by Yamada and Matsumoto (2003) have been used in almost all recent work on statistical dependency parsing of English (Nivre and Scholz, 2004; McDonald et al., 2005). Recently, Johansson and Nugues (2007) extended the head percolation strategy to incorporate long-distance links such as wh-movement and topicalization, and used the full set of grammatical function tags from Penn in addition to a number of inferred tags (in total 57 function tags). A dependency parser based on this syntax was used in the best-performing system in the SemEval-2007 task on Frame-semantic Structure Extraction (Baker et al., 2007), and the conversion method (in two different forms) was used for the English data in the CoNLL Shared Tasks of 2007 and 2008."
C08-1050,W05-0620,0,0.0662063,"eatures, which makes them more robust to domain changes and makes them learn more efficiently with respect to the amount of training data. 1 Introduction The role-semantic paradigm has a long and rich history in linguistics, and the NLP community has recently devoted much attention to developing accurate and robust methods for performing c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. role-semantic analysis automatically (Gildea and Jurafsky, 2002; Litkowski, 2004; Carreras and Màrquez, 2005; Baker et al., 2007). It is widely conjectured that an increased SRL accuracy will lead to improvements in certain NLP applications, especially template-filling systems. SRL has also been used in prototypes of more advanced semantics-based applications such as textual entailment recognition. It has previously been shown that SRL systems need a syntactic structure as input (Gildea and Palmer, 2002; Punyakanok et al., 2008). An important consideration is then what information this input should represent. By habit, most systems for automatic role-semantic analysis have used Pennstyle constituent"
C08-1050,P05-1072,0,0.0163709,"een considered in a few previous articles. For instance, Gildea and Hockenmaier (2003) reported that a CCG-based parser gives improved results over the Collins parser. Dependency syntax has only received little attention for the SRL task, despite a surge of interest in dependency parsing during the last few years (Buchholz and Marsi, 2006). Early examples of dependency-based SRL systems, which used goldstandard dependency treebanks, include Žabokrtský et al. (2002) and Hacioglu (2004). Two studies that compared the respective performances of constituent-based and dependency-based SRL systems (Pradhan et al., 2005; Swanson and Gordon, 2006), both using automatic parsers, reported that the constituent-based systems outperformed the dependency-based ones by a very wide margin. However, the figures reported in these studies can be misleading since the comparison involved a 10-year-old rule-based dependency parser versus a state-of-the-art statistical constituent parser. The recent progress in statistical dependency parsing gives grounds for a new evaluation. 393 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 393–400 Manchester, August 2008 In addition, t"
C08-1050,P05-1022,0,0.0188074,"Missing"
C08-1050,J08-2006,0,0.090515,"Missing"
C08-1050,A00-2018,0,0.232178,"Missing"
C08-1050,J08-2005,0,0.0369206,"ported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. role-semantic analysis automatically (Gildea and Jurafsky, 2002; Litkowski, 2004; Carreras and Màrquez, 2005; Baker et al., 2007). It is widely conjectured that an increased SRL accuracy will lead to improvements in certain NLP applications, especially template-filling systems. SRL has also been used in prototypes of more advanced semantics-based applications such as textual entailment recognition. It has previously been shown that SRL systems need a syntactic structure as input (Gildea and Palmer, 2002; Punyakanok et al., 2008). An important consideration is then what information this input should represent. By habit, most systems for automatic role-semantic analysis have used Pennstyle constituents (Marcus et al., 1993) produced by Collins’ (1997) or Charniak’s (2000) parsers. The influence of the syntactic formalism on SRL has only been considered in a few previous articles. For instance, Gildea and Hockenmaier (2003) reported that a CCG-based parser gives improved results over the Collins parser. Dependency syntax has only received little attention for the SRL task, despite a surge of interest in dependency parsi"
C08-1050,P97-1003,0,0.215131,"tic role labeling in English. Contrary to previously reported results, we show that dependencybased systems are on a par with constituent-based systems or perform nearly as well. Furthermore, we show that semantic role classifiers using a dependency parser learn faster than their constituentbased counterparts and therefore need less training data to achieve similar performances. Finally, dependency-based role classifiers are more robust to vocabulary change and outperform constituentbased systems when using out-of-domain test sets. its roots in lexicalized constituent parsing (Magerman, 1994; Collins, 1997). The head rules created by Yamada and Matsumoto (2003) have been used in almost all recent work on statistical dependency parsing of English (Nivre and Scholz, 2004; McDonald et al., 2005). Recently, Johansson and Nugues (2007) extended the head percolation strategy to incorporate long-distance links such as wh-movement and topicalization, and used the full set of grammatical function tags from Penn in addition to a number of inferred tags (in total 57 function tags). A dependency parser based on this syntax was used in the best-performing system in the SemEval-2007 task on Frame-semantic Str"
C08-1050,W03-1008,0,0.0176731,"prototypes of more advanced semantics-based applications such as textual entailment recognition. It has previously been shown that SRL systems need a syntactic structure as input (Gildea and Palmer, 2002; Punyakanok et al., 2008). An important consideration is then what information this input should represent. By habit, most systems for automatic role-semantic analysis have used Pennstyle constituents (Marcus et al., 1993) produced by Collins’ (1997) or Charniak’s (2000) parsers. The influence of the syntactic formalism on SRL has only been considered in a few previous articles. For instance, Gildea and Hockenmaier (2003) reported that a CCG-based parser gives improved results over the Collins parser. Dependency syntax has only received little attention for the SRL task, despite a surge of interest in dependency parsing during the last few years (Buchholz and Marsi, 2006). Early examples of dependency-based SRL systems, which used goldstandard dependency treebanks, include Žabokrtský et al. (2002) and Hacioglu (2004). Two studies that compared the respective performances of constituent-based and dependency-based SRL systems (Pradhan et al., 2005; Swanson and Gordon, 2006), both using automatic parsers, reporte"
C08-1050,J02-3001,0,0.950855,"role classifiers rely less on lexicalized features, which makes them more robust to domain changes and makes them learn more efficiently with respect to the amount of training data. 1 Introduction The role-semantic paradigm has a long and rich history in linguistics, and the NLP community has recently devoted much attention to developing accurate and robust methods for performing c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. role-semantic analysis automatically (Gildea and Jurafsky, 2002; Litkowski, 2004; Carreras and Màrquez, 2005; Baker et al., 2007). It is widely conjectured that an increased SRL accuracy will lead to improvements in certain NLP applications, especially template-filling systems. SRL has also been used in prototypes of more advanced semantics-based applications such as textual entailment recognition. It has previously been shown that SRL systems need a syntactic structure as input (Gildea and Palmer, 2002; Punyakanok et al., 2008). An important consideration is then what information this input should represent. By habit, most systems for automatic role-sema"
C08-1050,P02-1031,0,0.0193685,"ercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. role-semantic analysis automatically (Gildea and Jurafsky, 2002; Litkowski, 2004; Carreras and Màrquez, 2005; Baker et al., 2007). It is widely conjectured that an increased SRL accuracy will lead to improvements in certain NLP applications, especially template-filling systems. SRL has also been used in prototypes of more advanced semantics-based applications such as textual entailment recognition. It has previously been shown that SRL systems need a syntactic structure as input (Gildea and Palmer, 2002; Punyakanok et al., 2008). An important consideration is then what information this input should represent. By habit, most systems for automatic role-semantic analysis have used Pennstyle constituents (Marcus et al., 1993) produced by Collins’ (1997) or Charniak’s (2000) parsers. The influence of the syntactic formalism on SRL has only been considered in a few previous articles. For instance, Gildea and Hockenmaier (2003) reported that a CCG-based parser gives improved results over the Collins parser. Dependency syntax has only received little attention for the SRL task, despite a surge of in"
C08-1050,C04-1186,0,0.0179146,"l., 1993) produced by Collins’ (1997) or Charniak’s (2000) parsers. The influence of the syntactic formalism on SRL has only been considered in a few previous articles. For instance, Gildea and Hockenmaier (2003) reported that a CCG-based parser gives improved results over the Collins parser. Dependency syntax has only received little attention for the SRL task, despite a surge of interest in dependency parsing during the last few years (Buchholz and Marsi, 2006). Early examples of dependency-based SRL systems, which used goldstandard dependency treebanks, include Žabokrtský et al. (2002) and Hacioglu (2004). Two studies that compared the respective performances of constituent-based and dependency-based SRL systems (Pradhan et al., 2005; Swanson and Gordon, 2006), both using automatic parsers, reported that the constituent-based systems outperformed the dependency-based ones by a very wide margin. However, the figures reported in these studies can be misleading since the comparison involved a 10-year-old rule-based dependency parser versus a state-of-the-art statistical constituent parser. The recent progress in statistical dependency parsing gives grounds for a new evaluation. 393 Proceedings of"
C08-1050,P83-1010,0,0.569858,"as wh-word extraction and topicalization can be transparently represented by allowing nonprojective dependency links. These links also justify why dependency syntax is often considered superior for free-word-order languages; it is even very questionable whether the traditional constituent-based SRL strategies are viable for such languages. Second, grammatical function such as subject and object is an integral concept in dependency syntax. This concept is intuitive when reasoning about the link between syntax and semantics, and it has been used earlier in semantic interpreters such as Absity (Hirst, 1983). However, except from a few tentative experiments (Toutanova et al., 2005), grammatical function is not explicitly used by current automatic SRL systems, but instead emulated from constituent trees by features like the constituent position and the governing category. More generally, these linguistic reasons have made a number of linguists argue that dependency structures are more suitable for explaining the syntax-semantics interface (Mel’ˇcuk, 1988; Hudson, 1984). In this work, we provide a new evaluation of the influence of the syntactic representation on semantic role labeling in English."
C08-1050,W07-2416,1,0.406304,"ole classifiers using a dependency parser learn faster than their constituentbased counterparts and therefore need less training data to achieve similar performances. Finally, dependency-based role classifiers are more robust to vocabulary change and outperform constituentbased systems when using out-of-domain test sets. its roots in lexicalized constituent parsing (Magerman, 1994; Collins, 1997). The head rules created by Yamada and Matsumoto (2003) have been used in almost all recent work on statistical dependency parsing of English (Nivre and Scholz, 2004; McDonald et al., 2005). Recently, Johansson and Nugues (2007) extended the head percolation strategy to incorporate long-distance links such as wh-movement and topicalization, and used the full set of grammatical function tags from Penn in addition to a number of inferred tags (in total 57 function tags). A dependency parser based on this syntax was used in the best-performing system in the SemEval-2007 task on Frame-semantic Structure Extraction (Baker et al., 2007), and the conversion method (in two different forms) was used for the English data in the CoNLL Shared Tasks of 2007 and 2008. 3 Automatic Semantic Role Labeling with Constituents and Depend"
C08-1050,P06-2104,0,0.0326014,"w previous articles. For instance, Gildea and Hockenmaier (2003) reported that a CCG-based parser gives improved results over the Collins parser. Dependency syntax has only received little attention for the SRL task, despite a surge of interest in dependency parsing during the last few years (Buchholz and Marsi, 2006). Early examples of dependency-based SRL systems, which used goldstandard dependency treebanks, include Žabokrtský et al. (2002) and Hacioglu (2004). Two studies that compared the respective performances of constituent-based and dependency-based SRL systems (Pradhan et al., 2005; Swanson and Gordon, 2006), both using automatic parsers, reported that the constituent-based systems outperformed the dependency-based ones by a very wide margin. However, the figures reported in these studies can be misleading since the comparison involved a 10-year-old rule-based dependency parser versus a state-of-the-art statistical constituent parser. The recent progress in statistical dependency parsing gives grounds for a new evaluation. 393 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 393–400 Manchester, August 2008 In addition, there are a number of lingui"
C08-1050,N03-1033,0,0.00339342,"and MSTParser have achieved state-of-the-art results for a wide range of languages in the 2006 and 2007 CoNLL Shared Tasks on dependency parsing, and the LTH parser obtained the best result in the 2008 CoNLL Shared Task on joint syntactic and semantic parsing. Charniak’s and Collins’ parsers are widely used constituent parsers for English, and the C&J parser is the best-performing freely available constituent parser at the time of writing according to published figures. Charniak’s parser and the C&J parser come with a built-in part-ofspeech tagger; all other systems used the Stanford tagger (Toutanova et al., 2003). Following Gildea and Jurafsky (2002), the SRL problem is traditionally divided into two subtasks: identifying the arguments and labeling them with semantic roles. Although state-of-the-art SRL systems use sophisticated statistical models to perform these two tasks jointly (e.g. Toutanova et al., 2005, Johansson and Nugues, 2008), we implemented them as two independent support vector classifiers to be able to analyze the impact of syntactic representation on each task separately. The features used by the classifiers are traditional, although the features for the dependency-based classifiers n"
C08-1050,P05-1073,0,0.13641,"epresented by allowing nonprojective dependency links. These links also justify why dependency syntax is often considered superior for free-word-order languages; it is even very questionable whether the traditional constituent-based SRL strategies are viable for such languages. Second, grammatical function such as subject and object is an integral concept in dependency syntax. This concept is intuitive when reasoning about the link between syntax and semantics, and it has been used earlier in semantic interpreters such as Absity (Hirst, 1983). However, except from a few tentative experiments (Toutanova et al., 2005), grammatical function is not explicitly used by current automatic SRL systems, but instead emulated from constituent trees by features like the constituent position and the governing category. More generally, these linguistic reasons have made a number of linguists argue that dependency structures are more suitable for explaining the syntax-semantics interface (Mel’ˇcuk, 1988; Hudson, 1984). In this work, we provide a new evaluation of the influence of the syntactic representation on semantic role labeling in English. Contrary to previously reported results, we show that dependencybased syste"
C08-1050,zabokrtsky-etal-2002-machine,0,0.0170348,"Missing"
C08-1050,W04-3212,0,0.0578539,"A RG W ORD /POS L EFT W ORD /POS R IGHT W ORD /POS PARENT W ORD /POS C-S UBCAT C-PATH P HRASE T YPE G OV C AT D-S UBCAT D-PATH C HILD D EP S ET PARENT H AS O BJ R ELT O PARENT F UNCTION Argument identification C,D C,D C,D C,D C,D C,D C,D C,D C,D C C C C D D D D D Argument classification C,D C,D C,D C,D C,D C,D C,D C,D C C C C D D D D Table 1: Classifier features. The features used by the constituent-based and the dependency-based systems are marked C and D, respectively. fication step was preceded by a pruning stage that heuristically removes parse tree nodes unlikely to represent arguments (Xue and Palmer, 2004). To score the performance of the argument identifier, traditional evaluation procedures treat the identification as a bracketing problem, meaning that the entities scored by the evaluation procedure are labeled snippets of text; however, it is questionable whether this is the proper way to evaluate a task whose purpose is to find semantic relations between logical entities. We believe that the same criticisms that have been leveled at the PARSEVAL metric for constituent structures are equally valid for the bracket-based evaluation of SRL systems. The inappropriateness of the traditional metri"
C08-1050,W03-3023,0,0.0206916,"previously reported results, we show that dependencybased systems are on a par with constituent-based systems or perform nearly as well. Furthermore, we show that semantic role classifiers using a dependency parser learn faster than their constituentbased counterparts and therefore need less training data to achieve similar performances. Finally, dependency-based role classifiers are more robust to vocabulary change and outperform constituentbased systems when using out-of-domain test sets. its roots in lexicalized constituent parsing (Magerman, 1994; Collins, 1997). The head rules created by Yamada and Matsumoto (2003) have been used in almost all recent work on statistical dependency parsing of English (Nivre and Scholz, 2004; McDonald et al., 2005). Recently, Johansson and Nugues (2007) extended the head percolation strategy to incorporate long-distance links such as wh-movement and topicalization, and used the full set of grammatical function tags from Penn in addition to a number of inferred tags (in total 57 function tags). A dependency parser based on this syntax was used in the best-performing system in the SemEval-2007 task on Frame-semantic Structure Extraction (Baker et al., 2007), and the convers"
C08-1050,N07-1070,0,\N,Missing
C08-1050,C98-1013,0,\N,Missing
C10-1093,D07-1119,0,0.0356509,"Missing"
C10-1093,W06-2933,0,0.515764,"of rules founded on the assumption that topological neighbors of significant features in the dependency graph may also have a significant contribution. The search can be fully automated and the level of greediness adjusted with the number of features examined at each iteration of the discovery procedure. Most automatic procedures to build feature sets resort to greedy algorithms. Forward selection constructs a set by adding incrementally features from a predetermined superset while backward elimination removes them from the superset (Attardi et al., 2007). Both methods are sometimes combined (Nivre et al., 2006b). The selection procedures evaluate the relevance of a candidate feature in a set by its impact on the overall parsing score: Does this candidate improve or decrease the performance of the set? Using our automated feature discovery on two corpora, the Swedish corpus in CoNLL-X and the English corpus in CoNLL 2008, and a single parser system, we could reach results comparable or better than the best scores reported in these evaluations. The CoNLL 2008 test set contains, in addition to a Wall Street Journal (WSJ) section, an out-of-domain sample from the Brown corpus. With sets of 15 features,"
C10-1093,W03-3017,0,0.527833,"onsists solely of the first word of the queue. The search explores nodes along axes of the parser’s data structures and the partially built graph using proximity rules to uncover sequences of relevant, efficient features. Using this procedure on the Swedish corpus in CoNLL-X and the English corpus in CoNLL 2008, we built feature sets that enabled us to reach a labeled attachment score of 84.21 for Swedish, 88.11 on the Wall Street Journal section of CoNLL 2008, and 81.33 on the Brown part of it with a set cardinality of 15. 2 Transition-based Parsing Transition-based methods (Covington, 2001; Nivre, 2003; Yamada and Matsumoto, 2003; Zhang and Clark, 2009) have become a popular approach in multilingual dependency parsing because of their speed and performance. Transitionbased methods share common properties and build a dependency graph from a sequence of actions, where each action is determined using a feature function. In a data-driven context, the function is typically implemented as a classifier and the features are extracted from the partially built graph and the parser’s data structures, most often a queue and a stack. 2.1 Parser Implementation In this study, we built a parser using Nivre"
C10-1093,W06-2920,0,0.14496,"all the attributes to these destination nodes to generate the features. Initial feature Successors POS LEX PLD PLD PLD PL PLD QUEUE QUEUE QUEUE QUEUE QUEUE QUEUE STACK 0 0 0 0 0 1 0 7 lc rc pw Table 3: Features generated by the successor function SUCC({POS(QUEUE0)}). PLD stands for the three attributes POS, LEX, and DEP of the node; PL for POS and LEX. set and when applying a greedy best-first search, Feati+1 is assigned with the tuple yielding the highest score: Feati+1 ← eval best(CandFeati+1 ). Experimental Setup In a first experiment, we used the Swedish corpus of the CoNLL-X shared task (Buchholz and Marsi, 2006). In a second experiment, we applied the feature discovery procedure to the English corpus from CoNLL 2008 (Surdeanu et al., 2008), a dependency corpus converted from the Penn Treebank and the Brown corpus. In both experiments, we used the LIBSVM package (Chang and Lin, 2001) with a quadratic kernel, γ = 0.2, C = 0.4, and ε = 0.1. These parameters are identical to Nivre et al. (2006b) to enable a comparison of the scores. We evaluated the feature candidates on a development set using the labeled and unlabeled attachment scores (LAS and UAS) that we computed with the eval.pl script from CoNLL-X"
C10-1093,W03-3023,0,0.043161,"y of the first word of the queue. The search explores nodes along axes of the parser’s data structures and the partially built graph using proximity rules to uncover sequences of relevant, efficient features. Using this procedure on the Swedish corpus in CoNLL-X and the English corpus in CoNLL 2008, we built feature sets that enabled us to reach a labeled attachment score of 84.21 for Swedish, 88.11 on the Wall Street Journal section of CoNLL 2008, and 81.33 on the Brown part of it with a set cardinality of 15. 2 Transition-based Parsing Transition-based methods (Covington, 2001; Nivre, 2003; Yamada and Matsumoto, 2003; Zhang and Clark, 2009) have become a popular approach in multilingual dependency parsing because of their speed and performance. Transitionbased methods share common properties and build a dependency graph from a sequence of actions, where each action is determined using a feature function. In a data-driven context, the function is typically implemented as a classifier and the features are extracted from the partially built graph and the parser’s data structures, most often a queue and a stack. 2.1 Parser Implementation In this study, we built a parser using Nivre’s algorithm (Nivre, 2003)."
C10-1093,W09-3825,0,0.0356619,"ueue. The search explores nodes along axes of the parser’s data structures and the partially built graph using proximity rules to uncover sequences of relevant, efficient features. Using this procedure on the Swedish corpus in CoNLL-X and the English corpus in CoNLL 2008, we built feature sets that enabled us to reach a labeled attachment score of 84.21 for Swedish, 88.11 on the Wall Street Journal section of CoNLL 2008, and 81.33 on the Brown part of it with a set cardinality of 15. 2 Transition-based Parsing Transition-based methods (Covington, 2001; Nivre, 2003; Yamada and Matsumoto, 2003; Zhang and Clark, 2009) have become a popular approach in multilingual dependency parsing because of their speed and performance. Transitionbased methods share common properties and build a dependency graph from a sequence of actions, where each action is determined using a feature function. In a data-driven context, the function is typically implemented as a classifier and the features are extracted from the partially built graph and the parser’s data structures, most often a queue and a stack. 2.1 Parser Implementation In this study, we built a parser using Nivre’s algorithm (Nivre, 2003). The parser complexity is"
C10-1093,P05-1013,0,0.0690667,"y, we built a parser using Nivre’s algorithm (Nivre, 2003). The parser complexity is linear and parsing completes in at most 2n + 1 operations, where n is the length of the sentence. Table 1 shows the transitions and actions to construct a dependency graph. Given a sentence to parse, we used a classifierbased guide to predict the transition sequence to apply. At each step, the guide extracts features from the parser configuration and uses them as input to a classifier to predict the next transition. Before training the classification models, we projectivized the corpus sentences (Kunze, 1967; Nivre and Nilsson, 2005). We did not attempt to recover nonprojective sentences after parsing. 2.2 Training and Parsing Procedure We extracted the features using a gold-standard parsing of the training set. We organized the classification, and hence the feature extraction, as a Action Init. End LeftArc RightArc Reduce Shift Parser configuration nil,W, 0 / S, nil, G n|S, n |Q, G → S, n |Q, G ∪ {n , n} n|S, n |Q, G → n |n|S, Q, G ∪ {n, n } n|S, Q, G → S, Q, G S, n|Q, G → n|S, Q, G Table 1: Parser transitions (Nivre, 2003). W is the input, G, the graph, S, the stack, and Q, the queue. The t"
C10-1093,P81-1022,0,0.687634,"Missing"
C10-1093,nivre-etal-2006-maltparser,0,\N,Missing
C10-1093,W08-2121,0,\N,Missing
C10-3009,W09-1206,1,0.272157,"Missing"
C10-3009,W09-1210,1,0.918454,"gues† †Department of Computer science ‡Institute for Natural Language Processing Lund University University of Stuttgart anders.bjorkelund@cs.lth.se bohnet@ims.uni-stuttgart.de love.hafdell@cs.lth.se pierre.nugues@cs.lth.se Abstract This demonstration presents a highperformance syntactic and semantic dependency parser. The system consists of a pipeline of modules that carry out the tokenization, lemmatization, part-of-speech tagging, dependency parsing, and semantic role labeling of a sentence. The system’s two main components draw on improved versions of a state-of-the-art dependency parser (Bohnet, 2009) and semantic role labeler (Bj¨orkelund et al., 2009) developed independently by the authors. The system takes a sentence as input and produces a syntactic and semantic annotation using the CoNLL 2009 format. The processing time needed for a sentence typically ranges from 10 to 1000 milliseconds. The predicate–argument structures in the ﬁnal output are visualized in the form of segments, which are more intuitive for a user. 1 Motivation and Overview Semantic analyzers consist of processing pipelines to tokenize, lemmatize, tag, and parse sentences, where all the steps are crucial to their over"
C10-3009,burchardt-etal-2006-salsa,0,0.0332486,"Missing"
C10-3009,D07-1101,0,0.0749361,"anging from 100 to 1000 milliseconds. This demonstration is a practical semantic parser that takes an English sentence as input and produces syntactic and semantic dependency graphs using the CoNLL 2009 format. It builds on lemmatization and POS tagging preprocessing steps, as well as on two systems, one dealing with syntax and the other with semantic dependencies that reported respectively state-of-the-art results in the CoNLL 2009 shared task (Bohnet, 2009; Bj¨orkelund et al., 2009). The complete system architecture is shown in Fig. 1. The dependency parser is based on Carreras’s algorithm (Carreras, 2007) and second order spanning trees. The parser is trained with the margin infused relaxed algorithm (MIRA) (McDonald et al., 2005) and combined with a hash kernel (Shi et al., 2009). In combination with the system’s lemmatizer and POS tagger, this parser achieves an average labeled attachment score (LAS) of 89.88 when trained and tested on the English corpus of the CoNLL 2009 shared task (Surdeanu et al., 2008). The semantic role labeler (SRL) consists of a pipeline of independent, local classiﬁers that identify the predicates, their senses, the arguments of the predicates, and the argument labe"
C10-3009,W08-2123,1,0.853891,"Missing"
C10-3009,P05-1012,0,0.0393551,"input and produces syntactic and semantic dependency graphs using the CoNLL 2009 format. It builds on lemmatization and POS tagging preprocessing steps, as well as on two systems, one dealing with syntax and the other with semantic dependencies that reported respectively state-of-the-art results in the CoNLL 2009 shared task (Bohnet, 2009; Bj¨orkelund et al., 2009). The complete system architecture is shown in Fig. 1. The dependency parser is based on Carreras’s algorithm (Carreras, 2007) and second order spanning trees. The parser is trained with the margin infused relaxed algorithm (MIRA) (McDonald et al., 2005) and combined with a hash kernel (Shi et al., 2009). In combination with the system’s lemmatizer and POS tagger, this parser achieves an average labeled attachment score (LAS) of 89.88 when trained and tested on the English corpus of the CoNLL 2009 shared task (Surdeanu et al., 2008). The semantic role labeler (SRL) consists of a pipeline of independent, local classiﬁers that identify the predicates, their senses, the arguments of the predicates, and the argument labels. The SRL module achieves an average labeled semantic F1 of 80.90 when trained and tested on the English corpus of CoNLL 2009"
C10-3009,W08-2121,0,0.0470103,"Missing"
C10-3009,W09-1201,0,\N,Missing
C16-1096,P15-1039,0,0.0141918,"tp:// 1007 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1007–1017, Osaka, Japan, December 11-17 2016. The concept of transferring linguistic annotation, in the context of part-of-speech tags, across parallel corpora was introduced in Yarowsky et al. (2001). Cross-lingual annotation projection of FrameNet semantics has been described by Pad´o and Lapata (2009) and Basili et al. (2009). In Van der Plas et al. (2011), the authors describe an automatic method of direct transfer of PropBank semantics requiring no manual effort. Akbik et al. (2015) describe an approach to generate multilingual PropBanks using filtered annotation projection and bootstrap learning in order to handle errors stemming from translation shifts in corpora. Most previous approaches have used professionally translated parallel corpora, mainly EuroParl (Koehn, 2005) and United Nations Corpora (Rafalovitch and Dale, 2009), to transfer semantic annotation. However, creating these resources requires manual efforts; they are thus limited in size and in the number of languages they cover. In contrast to parallel corpora, loosely parallel corpora describe similar concep"
C16-1096,P98-1013,0,0.0681495,"ending previous attempts. In addition, it allows the generation of proposition banks upon which semantic parsers can be trained. 1 Introduction Data-driven approaches using natural language processing tackle increasingly complex tasks with ever growing scales and in more varied domains. Semantic role labeling is a type of shallow semantic parsing that is becoming an increasingly important component in information extraction (Christensen et al., 2010), question answering (Shen and Lapata, 2007), and text summarization (Khan et al., 2015). The development of semantic resources such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005) made the training of models for semantic role labelers using supervised techniques possible. However, as a consequence of the considerable manual efforts needed to build proposition banks, they exist only for a few languages. An alternative approach to using supervision is to transfer knowledge between resources, a form of distant or related supervision. Methods for directly projecting semantic labels from a resource-rich language to a resource-scarce one were introduced in Pad´o (2007). In this paper, we describe a method for aligning and projecting semanti"
C16-1096,W09-1206,1,0.876948,"Missing"
C16-1096,C10-3009,1,0.861526,"Missing"
C16-1096,E12-1009,0,0.0212687,"there is no overlap globally. 3.3 Syntactic and Semantic Annotation In our experimental setup, we used the English edition of Wikipedia as our SL, and we annotated it with syntactic and semantic dependencies. For the syntactic-semantic parsing, we used an open-source semantic role labeler (Choi, 2012) trained on OntoNotes 5.0 (Weischedel et al., 2013). We transferred the semantic annotation to two TLs, the Swedish and French editions of Wikipedia, both annotated with syntactic dependencies. For French syntactic parsing, we applied a transition-based dependency parser (Bohnet and Nivre, 2012; Bohnet and Kuhn, 2012) trained on a French Treebank described in Candito et al. (2010). Correspondingly, to preprocess the Swedish edition of Wikipedia, we ¨ applied a pipeline consisting of a POS tagger (Ostling, 2013) and a syntactic dependency parser (Nivre et al., 2006). 3.4 Extension to Entity-like Tokens Entities have the property of being uniquely identifiable across languages on a global scope. However, an obvious drawback to using entities as a means of aligning sentences and transferring roles, is that roles are not always instantiated by entities. To reclaim these instances, we extended the entity alignm"
C16-1096,D12-1133,0,0.0235784,"rlapping mentions, until there is no overlap globally. 3.3 Syntactic and Semantic Annotation In our experimental setup, we used the English edition of Wikipedia as our SL, and we annotated it with syntactic and semantic dependencies. For the syntactic-semantic parsing, we used an open-source semantic role labeler (Choi, 2012) trained on OntoNotes 5.0 (Weischedel et al., 2013). We transferred the semantic annotation to two TLs, the Swedish and French editions of Wikipedia, both annotated with syntactic dependencies. For French syntactic parsing, we applied a transition-based dependency parser (Bohnet and Nivre, 2012; Bohnet and Kuhn, 2012) trained on a French Treebank described in Candito et al. (2010). Correspondingly, to preprocess the Swedish edition of Wikipedia, we ¨ applied a pipeline consisting of a POS tagger (Ostling, 2013) and a syntactic dependency parser (Nivre et al., 2006). 3.4 Extension to Entity-like Tokens Entities have the property of being uniquely identifiable across languages on a global scope. However, an obvious drawback to using entities as a means of aligning sentences and transferring roles, is that roles are not always instantiated by entities. To reclaim these instances, we ex"
C16-1096,candito-etal-2010-statistical,0,0.0226069,"Missing"
C16-1096,candito-etal-2014-developing,0,0.0275161,"Missing"
C16-1096,W10-0907,0,0.0168992,"nguage editions of Wikipedia. Our results show that the annotation projection using entities in combination with loosely parallel corpora provides a viable approach to extending previous attempts. In addition, it allows the generation of proposition banks upon which semantic parsers can be trained. 1 Introduction Data-driven approaches using natural language processing tackle increasingly complex tasks with ever growing scales and in more varied domains. Semantic role labeling is a type of shallow semantic parsing that is becoming an increasingly important component in information extraction (Christensen et al., 2010), question answering (Shen and Lapata, 2007), and text summarization (Khan et al., 2015). The development of semantic resources such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005) made the training of models for semantic role labelers using supervised techniques possible. However, as a consequence of the considerable manual efforts needed to build proposition banks, they exist only for a few languages. An alternative approach to using supervision is to transfer knowledge between resources, a form of distant or related supervision. Methods for directly projecting semantic l"
C16-1096,L16-1601,0,0.0422818,"generated PropBank itself. The contributions of this paper are the following: (1) We extend Exner et al. (2015) by including pronouns and other linguistic units that in a local context exhibit the characteristics of entities. (2) We present and evaluate two methods for aligning sentences by using entities. (3) We demonstrate the effectiveness and generalizability of our approach by projecting semantic annotations to two languages, Swedish and French, and we evaluate it using two external proposition databases, the Swedish SweFN++ (Borin et al., 2010) and French ASFALDA (Candito et al., 2014; Djemaa et al., 2016) that are both semantically-annotated corpora using adaptations of FrameNet frames. (4) We release the source code used in the annotation projection and we provide the generated PropBanks in Swedish and French1 . 3 Method The aim of the method is to generate PropBank-like resources by fully annotating sentences in target languages using semantic content, in whole or partially, from a source language. We start with loosely parallel corpora in two languages: a source language (SL) expressing the semantic content that we want to transfer to a target language (TL). We then disambiguate and uniquel"
C16-1096,S15-1029,1,0.837422,"strap learning in order to handle errors stemming from translation shifts in corpora. Most previous approaches have used professionally translated parallel corpora, mainly EuroParl (Koehn, 2005) and United Nations Corpora (Rafalovitch and Dale, 2009), to transfer semantic annotation. However, creating these resources requires manual efforts; they are thus limited in size and in the number of languages they cover. In contrast to parallel corpora, loosely parallel corpora describe similar concepts and events, but are not necessarily the result of a focused effort to translate a large corpus. In Exner et al. (2015), we introduced the concept of using entities as a method for aligning sentences and transferring semantic content in loosely parallel corpora. However, the presented approach has the following limitations: (1) it was evaluated on one language only and (2) the evaluation was performed on the generated PropBank itself. The contributions of this paper are the following: (1) We extend Exner et al. (2015) by including pronouns and other linguistic units that in a local context exhibit the characteristics of entities. (2) We present and evaluate two methods for aligning sentences by using entities."
C16-1096,P10-1030,0,0.0118302,"Swedish and French. We provide an evaluation of the quality of the generated PropBanks, together with an evaluation on two external FrameNets. 2 Previous Work As an alternative to using supervised efforts for relation extraction, distant supervision can be employed to transfer relational knowledge representations from one resource to another. Distant supervision for relation extraction was introduced by Craven and Kumlien (1999) in the context of biomedical information extraction. Mintz et al. (2009) describe a method of using an external knowledge base as an indirect way of annotating text. Hoffmann et al. (2010) introduced the usage of Wikipedia infoboxes in distantly supervised relation extraction. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 1007 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1007–1017, Osaka, Japan, December 11-17 2016. The concept of transferring linguistic annotation, in the context of part-of-speech tags, across parallel corpora was introduced in Yarowsky et al. (2001). Cross-lingual annotation projection of"
C16-1096,W08-2123,1,0.828465,"Missing"
C16-1096,2005.mtsummit-papers.11,0,0.018996,"n Yarowsky et al. (2001). Cross-lingual annotation projection of FrameNet semantics has been described by Pad´o and Lapata (2009) and Basili et al. (2009). In Van der Plas et al. (2011), the authors describe an automatic method of direct transfer of PropBank semantics requiring no manual effort. Akbik et al. (2015) describe an approach to generate multilingual PropBanks using filtered annotation projection and bootstrap learning in order to handle errors stemming from translation shifts in corpora. Most previous approaches have used professionally translated parallel corpora, mainly EuroParl (Koehn, 2005) and United Nations Corpora (Rafalovitch and Dale, 2009), to transfer semantic annotation. However, creating these resources requires manual efforts; they are thus limited in size and in the number of languages they cover. In contrast to parallel corpora, loosely parallel corpora describe similar concepts and events, but are not necessarily the result of a focused effort to translate a large corpus. In Exner et al. (2015), we introduced the concept of using entities as a method for aligning sentences and transferring semantic content in loosely parallel corpora. However, the presented approach"
C16-1096,P09-1113,0,0.0259706,"ligning Wikipedias by entities, we constructed loosely parallel corpora and we used them to generate PropBanks in Swedish and French. We provide an evaluation of the quality of the generated PropBanks, together with an evaluation on two external FrameNets. 2 Previous Work As an alternative to using supervised efforts for relation extraction, distant supervision can be employed to transfer relational knowledge representations from one resource to another. Distant supervision for relation extraction was introduced by Craven and Kumlien (1999) in the context of biomedical information extraction. Mintz et al. (2009) describe a method of using an external knowledge base as an indirect way of annotating text. Hoffmann et al. (2010) introduced the usage of Wikipedia infoboxes in distantly supervised relation extraction. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 1007 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1007–1017, Osaka, Japan, December 11-17 2016. The concept of transferring linguistic annotation, in the context of part-of-sp"
C16-1096,nivre-etal-2006-maltparser,0,0.0267322,"n open-source semantic role labeler (Choi, 2012) trained on OntoNotes 5.0 (Weischedel et al., 2013). We transferred the semantic annotation to two TLs, the Swedish and French editions of Wikipedia, both annotated with syntactic dependencies. For French syntactic parsing, we applied a transition-based dependency parser (Bohnet and Nivre, 2012; Bohnet and Kuhn, 2012) trained on a French Treebank described in Candito et al. (2010). Correspondingly, to preprocess the Swedish edition of Wikipedia, we ¨ applied a pipeline consisting of a POS tagger (Ostling, 2013) and a syntactic dependency parser (Nivre et al., 2006). 3.4 Extension to Entity-like Tokens Entities have the property of being uniquely identifiable across languages on a global scope. However, an obvious drawback to using entities as a means of aligning sentences and transferring roles, is that roles are not always instantiated by entities. To reclaim these instances, we extended the entity alignment to include entity-like linguistic units (LU). We focused on units that have the property of being uniquely identifiable and limited to the scope of a sentence pair. Units correspond to sequences of tokens the entity disambiguator has either failed"
C16-1096,J05-1004,0,0.0149515,"ion, it allows the generation of proposition banks upon which semantic parsers can be trained. 1 Introduction Data-driven approaches using natural language processing tackle increasingly complex tasks with ever growing scales and in more varied domains. Semantic role labeling is a type of shallow semantic parsing that is becoming an increasingly important component in information extraction (Christensen et al., 2010), question answering (Shen and Lapata, 2007), and text summarization (Khan et al., 2015). The development of semantic resources such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005) made the training of models for semantic role labelers using supervised techniques possible. However, as a consequence of the considerable manual efforts needed to build proposition banks, they exist only for a few languages. An alternative approach to using supervision is to transfer knowledge between resources, a form of distant or related supervision. Methods for directly projecting semantic labels from a resource-rich language to a resource-scarce one were introduced in Pad´o (2007). In this paper, we describe a method for aligning and projecting semantic annotation in loosely parallel co"
C16-1096,2009.mtsummit-posters.15,0,0.0225676,"nnotation projection of FrameNet semantics has been described by Pad´o and Lapata (2009) and Basili et al. (2009). In Van der Plas et al. (2011), the authors describe an automatic method of direct transfer of PropBank semantics requiring no manual effort. Akbik et al. (2015) describe an approach to generate multilingual PropBanks using filtered annotation projection and bootstrap learning in order to handle errors stemming from translation shifts in corpora. Most previous approaches have used professionally translated parallel corpora, mainly EuroParl (Koehn, 2005) and United Nations Corpora (Rafalovitch and Dale, 2009), to transfer semantic annotation. However, creating these resources requires manual efforts; they are thus limited in size and in the number of languages they cover. In contrast to parallel corpora, loosely parallel corpora describe similar concepts and events, but are not necessarily the result of a focused effort to translate a large corpus. In Exner et al. (2015), we introduced the concept of using entities as a method for aligning sentences and transferring semantic content in loosely parallel corpora. However, the presented approach has the following limitations: (1) it was evaluated on"
C16-1096,D07-1002,0,0.0432833,"that the annotation projection using entities in combination with loosely parallel corpora provides a viable approach to extending previous attempts. In addition, it allows the generation of proposition banks upon which semantic parsers can be trained. 1 Introduction Data-driven approaches using natural language processing tackle increasingly complex tasks with ever growing scales and in more varied domains. Semantic role labeling is a type of shallow semantic parsing that is becoming an increasingly important component in information extraction (Christensen et al., 2010), question answering (Shen and Lapata, 2007), and text summarization (Khan et al., 2015). The development of semantic resources such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005) made the training of models for semantic role labelers using supervised techniques possible. However, as a consequence of the considerable manual efforts needed to build proposition banks, they exist only for a few languages. An alternative approach to using supervision is to transfer knowledge between resources, a form of distant or related supervision. Methods for directly projecting semantic labels from a resource-rich language to a res"
C16-1096,P11-2052,0,0.398692,"Missing"
C16-1096,H01-1035,0,0.106004,"e as an indirect way of annotating text. Hoffmann et al. (2010) introduced the usage of Wikipedia infoboxes in distantly supervised relation extraction. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 1007 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1007–1017, Osaka, Japan, December 11-17 2016. The concept of transferring linguistic annotation, in the context of part-of-speech tags, across parallel corpora was introduced in Yarowsky et al. (2001). Cross-lingual annotation projection of FrameNet semantics has been described by Pad´o and Lapata (2009) and Basili et al. (2009). In Van der Plas et al. (2011), the authors describe an automatic method of direct transfer of PropBank semantics requiring no manual effort. Akbik et al. (2015) describe an approach to generate multilingual PropBanks using filtered annotation projection and bootstrap learning in order to handle errors stemming from translation shifts in corpora. Most previous approaches have used professionally translated parallel corpora, mainly EuroParl (Koehn, 2005) and United"
C16-1096,C98-1013,0,\N,Missing
C16-2016,C10-3009,1,0.823326,"r Russian. We keep the original links occurring in the Wikipedia pages and we resolve them using Wikidata identifiers, when they exist, or to normalized page names as a fall back. These steps are common to all the language editions we process. If the input is plain text, we skip these steps. The annotation tool chains are specific to the languages. We abstracted these chains so that they are instances of a generic annotator. For English, Spanish, and German, we use CoreNLP (Manning et al., 2014) or ClearNLP (Choi, 2012). For French, we use CoreNLP for tokenizing the text and MATE for parsing (Björkelund et al., 2010). For Swedish, we use Stagger (Östling, 2013) and MaltParser (Nivre et al., 2006). For Russian, only the tokenization is available for now. We also link mentions of named entities and concepts to unique Wikidata identifiers. To carry this out, we reimplemented a variant of TAGME (Ferragina and Scaiella, 2010). 3.2 The Document Model The MLDM library3 (Klang and Nugues, 2016) defines a model for storing, querying, and extracting hypertextual information common to many NLP tasks in a standalone package. We designed this model so that it could store the original Wikipedia markup, as well as the s"
C16-2016,L16-1654,1,0.886048,"ext, where the annotation layers are selectable from a drop down menu in the block just above (black triangle to the right), here the tokens and named entities across the language versions and significant processing capacities. In addition, the scale and heterogeneity of the Wikipedia collection makes it relatively difficult to do experimentations on the whole corpus. These experimentations are rendered even more complex as, to the best of our knowledge, there is no available tool to visualize easily annotation results from different processing pipelines. Langforia builds on a document model (Klang and Nugues, 2016) that stores the linguistic annotations and enables the pipeline to abstract the components across the languages and tools. This model consists of layers, where each layer is a sequence of ranges describing a specific annotation, for instance the parts of speech or the syntactic dependencies. It provides a format common to all the pipelines that makes them insensitive to the input/output features of a tool. The list of annotated layers varies depending on the tool availability for a specific language. The layers common to all the versions are compatible with the Wikipedia markup: They include"
C16-2016,P14-5010,0,0.0111126,"instance, has the unique id: Q35765 that enables the system to retrieve the article pages in English, French, Swedish, or Russian. We keep the original links occurring in the Wikipedia pages and we resolve them using Wikidata identifiers, when they exist, or to normalized page names as a fall back. These steps are common to all the language editions we process. If the input is plain text, we skip these steps. The annotation tool chains are specific to the languages. We abstracted these chains so that they are instances of a generic annotator. For English, Spanish, and German, we use CoreNLP (Manning et al., 2014) or ClearNLP (Choi, 2012). For French, we use CoreNLP for tokenizing the text and MATE for parsing (Björkelund et al., 2010). For Swedish, we use Stagger (Östling, 2013) and MaltParser (Nivre et al., 2006). For Russian, only the tokenization is available for now. We also link mentions of named entities and concepts to unique Wikidata identifiers. To carry this out, we reimplemented a variant of TAGME (Ferragina and Scaiella, 2010). 3.2 The Document Model The MLDM library3 (Klang and Nugues, 2016) defines a model for storing, querying, and extracting hypertextual information common to many NLP"
C16-2016,P10-1023,0,0.038855,"t of this page from https://www. wikipedia.org/ and then sends it to the Langforia server. Figure 2, left part, shows the resulting annotations for the Osaka article from the Swedish Wikipedia for three layers, tokens, named entities, and dependency relations, while the right part of the figure shows the entity linking results. 2 Motivation and Significance We designed Langforia with a specific focus for Wikipedia, although the pipeline can process raw text. Wikipedia has become an essential encyclopedic corpus used in many NLP projects. In translation (Smith et al., 2010), semantic networks (Navigli and Ponzetto, 2010), named entity linking (Mihalcea and Csomai, 2007), information extraction, or question answering (Ferrucci, 2012), Wikipedia offers a multilingual coverage and an article diversity that are unequalled. However, processing articles are nontrivial tasks that require dealing with multiple markup variants, encodings issues, tool incompatibilities This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 74 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: System Demo"
C16-2016,nivre-etal-2006-maltparser,0,0.0606314,"them using Wikidata identifiers, when they exist, or to normalized page names as a fall back. These steps are common to all the language editions we process. If the input is plain text, we skip these steps. The annotation tool chains are specific to the languages. We abstracted these chains so that they are instances of a generic annotator. For English, Spanish, and German, we use CoreNLP (Manning et al., 2014) or ClearNLP (Choi, 2012). For French, we use CoreNLP for tokenizing the text and MATE for parsing (Björkelund et al., 2010). For Swedish, we use Stagger (Östling, 2013) and MaltParser (Nivre et al., 2006). For Russian, only the tokenization is available for now. We also link mentions of named entities and concepts to unique Wikidata identifiers. To carry this out, we reimplemented a variant of TAGME (Ferragina and Scaiella, 2010). 3.2 The Document Model The MLDM library3 (Klang and Nugues, 2016) defines a model for storing, querying, and extracting hypertextual information common to many NLP tasks in a standalone package. We designed this model so that it could store the original Wikipedia markup, as well as the subsequent linguistic annotations: Partof-speech tagging, coreference resolution,"
C16-2016,N10-1063,0,0.0419828,"the client first fetches the HTML content of this page from https://www. wikipedia.org/ and then sends it to the Langforia server. Figure 2, left part, shows the resulting annotations for the Osaka article from the Swedish Wikipedia for three layers, tokens, named entities, and dependency relations, while the right part of the figure shows the entity linking results. 2 Motivation and Significance We designed Langforia with a specific focus for Wikipedia, although the pipeline can process raw text. Wikipedia has become an essential encyclopedic corpus used in many NLP projects. In translation (Smith et al., 2010), semantic networks (Navigli and Ponzetto, 2010), named entity linking (Mihalcea and Csomai, 2007), information extraction, or question answering (Ferrucci, 2012), Wikipedia offers a multilingual coverage and an article diversity that are unequalled. However, processing articles are nontrivial tasks that require dealing with multiple markup variants, encodings issues, tool incompatibilities This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 74 Proceedings of COLING 2016, the 26th International Conf"
C16-2016,E12-2021,0,0.1395,"Missing"
D07-1123,W06-2920,0,0.160704,"se number of actions is linear with respect to the number of words in the sentence. Abstract We describe an incremental parser that was trained to minimize cost over sentences rather than over individual parsing actions. This is an attempt to use the advantages of the two top-scoring systems in the CoNLL-X shared task. In the evaluation, we present the performance of the parser in the Multilingual task, as well as an evaluation of the contribution of bidirectional parsing and beam search to the parsing performance. 2.2 1 Introduction The two best-performing systems in the CoNLL-X shared task (Buchholz and Marsi, 2006) can be classified along two lines depending on the method they used to train the parsing models. Although the parsers are quite different, their creators could report near-tie scores. The approach of the top system (McDonald et al., 2006) was to fit the model to minimize cost over sentences, while the secondbest system (Nivre et al., 2006) trained the model to maximize performance over individual decisions in an incremental algorithm. This difference is a natural consequence of their respective parsing strategies: CKY-style maximization of link score and incremental parsing. In this paper, we"
D07-1123,C96-1058,0,0.318666,"itions where W is the initial word list; I, the current input word list; A, the graph of dependencies; and S, the stack. (n′ , n) denotes a dependency relations between n′ and n, where n′ is the head and n the dependent. Actions Initialize Terminate Left-arc Right-arc Reduce Shift Parser actions hnil, W, ∅i hS, nil, Ai hn|S, n′ |I, Ai → hS, n′ |I, A ∪ {(n′ , n)}i hn|S, n′ |I, Ai → hn′ |n|S, I, A ∪ {(n, n′ )}i hn|S, I, Ai → hS, I, Ai hS, n|I, Ai → hn|S, I, Ai between the two parses in a manner that makes the tree projective, single-head, rooted, and cycle-free, we applied the Eisner algorithm (Eisner, 1996). 2.4 Beam Search As in our previous parser (Johansson and Nugues, 2006), we used a beam-search extension to Nivre’s original algorithm (which is greedy in its original formulation). Each parsing action was assigned a score, and the beam search allows us to find a better overall score of the sequence of actions. In this work, we used a beam width of 8 for Catalan, Chinese, Czech, and English and 16 for the other languages. 3 Learning Method 3.1 Overview We model the parsing problem for a sentence x as finding the parse yˆ = arg maxy F (x, y) that maximizes a discriminant function F . In this w"
D07-1123,W06-2930,1,0.943317,"ojective. To be able to recover the nonprojective arcs after parsing, the projectivization operation replaces the labels of the arcs it modifies with traces indicating which links should be moved and where attach to attach them (the “Head+Path” encoding). The model is trained with these new labels that makes it possible to carry out the reverse operation and produce nonprojective structures. 2.3 Bidirectional Parsing Shift-reduce is by construction a directional parser, typically applied from left to right. To make better use of the training set, we applied the algorithm in both directions as Johansson and Nugues (2006) and Sagae and Lavie (2006) for all languages except Catalan and Hungarian. This, we believe, also has the advantage of making the parser less sensitive to whether the language is head-initial or head-final. We trained the model on projectivized graphs from left to right and right to left and used a voting strategy based on link scores. Each link was assigned a score (simply by using the score of the la or ra actions for each link). To resolve the conflicts 1134 Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 1134–1138, c Prague, June 2007. 2007 Association for Computatio"
D07-1123,W07-2416,1,0.828537,"Missing"
D07-1123,J93-2004,0,0.0288737,"Missing"
D07-1123,W06-2932,0,0.226356,"use the advantages of the two top-scoring systems in the CoNLL-X shared task. In the evaluation, we present the performance of the parser in the Multilingual task, as well as an evaluation of the contribution of bidirectional parsing and beam search to the parsing performance. 2.2 1 Introduction The two best-performing systems in the CoNLL-X shared task (Buchholz and Marsi, 2006) can be classified along two lines depending on the method they used to train the parsing models. Although the parsers are quite different, their creators could report near-tie scores. The approach of the top system (McDonald et al., 2006) was to fit the model to minimize cost over sentences, while the secondbest system (Nivre et al., 2006) trained the model to maximize performance over individual decisions in an incremental algorithm. This difference is a natural consequence of their respective parsing strategies: CKY-style maximization of link score and incremental parsing. In this paper, we describe an attempt to unify the two approaches: an incremental parsing strategy that is trained to maximize performance over sentences rather than over individual parsing actions. 2 Parsing Method 2.1 Nivre’s Parser We used Nivre’s algor"
D07-1123,P05-1013,0,0.0394371,"oaches: an incremental parsing strategy that is trained to maximize performance over sentences rather than over individual parsing actions. 2 Parsing Method 2.1 Nivre’s Parser We used Nivre’s algorithm (Nivre et al., 2006), which is a variant of the shift–reduce parser. Like the regular shift–reduce, it uses a stack S and a list Handling Nonprojective Parse Trees While the parsing algorithm produces projective trees only, nonprojective arcs can be handled using a preprocessing step before training the model and a postprocessing step after parsing the sentences. The projectivization algorithm (Nivre and Nilsson, 2005) iteratively moves each nonprojective arc upward in the tree until the whole tree is projective. To be able to recover the nonprojective arcs after parsing, the projectivization operation replaces the labels of the arcs it modifies with traces indicating which links should be moved and where attach to attach them (the “Head+Path” encoding). The model is trained with these new labels that makes it possible to carry out the reverse operation and produce nonprojective structures. 2.3 Bidirectional Parsing Shift-reduce is by construction a directional parser, typically applied from left to right."
D07-1123,W06-2933,0,0.281203,"Missing"
D07-1123,N06-2033,0,0.150302,"the nonprojective arcs after parsing, the projectivization operation replaces the labels of the arcs it modifies with traces indicating which links should be moved and where attach to attach them (the “Head+Path” encoding). The model is trained with these new labels that makes it possible to carry out the reverse operation and produce nonprojective structures. 2.3 Bidirectional Parsing Shift-reduce is by construction a directional parser, typically applied from left to right. To make better use of the training set, we applied the algorithm in both directions as Johansson and Nugues (2006) and Sagae and Lavie (2006) for all languages except Catalan and Hungarian. This, we believe, also has the advantage of making the parser less sensitive to whether the language is head-initial or head-final. We trained the model on projectivized graphs from left to right and right to left and used a voting strategy based on link scores. Each link was assigned a score (simply by using the score of the la or ra actions for each link). To resolve the conflicts 1134 Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 1134–1138, c Prague, June 2007. 2007 Association for Computational Linguistics Table 1: Ni"
D08-1008,S07-1018,0,0.118818,"al., 2003). It has also been used in prototypes of NLP systems that carry out complex reasoning, such as entailment recognition systems (Haghighi et al., 2005; Hickl et al., 2006). In addition, role-semantic features have recently been used to extend vector-space representations in automatic document categorization (Persson et al., 2008). The NLP community has recently devoted much attention to developing accurate and robust methods for performing role-semantic analysis automatically, and a number of multi-system evaluations have been carried out (Litkowski, 2004; Carreras and Màrquez, 2005; Baker et al., 2007; Surdeanu et al., 2008). Following the seminal work of Gildea and Jurafsky (2002), there have been many extensions in machine learning models, feature engineering (Xue and Palmer, 2004), and inference procedures (Toutanova et al., 2005; Surdeanu et al., 2007; Punyakanok et al., 2008). With very few exceptions (e.g. Collobert and Weston, 2007), published SRL methods have used some sort of syntactic structure as input (Gildea and Palmer, 2002; Punyakanok et al., 2008). Most systems for automatic role-semantic analysis have used constituent syntax as in the Penn Treebank (Marcus et al., 1993), a"
D08-1008,W04-2412,0,0.00983598,"sky (2002), there have been many extensions in machine learning models, feature engineering (Xue and Palmer, 2004), and inference procedures (Toutanova et al., 2005; Surdeanu et al., 2007; Punyakanok et al., 2008). With very few exceptions (e.g. Collobert and Weston, 2007), published SRL methods have used some sort of syntactic structure as input (Gildea and Palmer, 2002; Punyakanok et al., 2008). Most systems for automatic role-semantic analysis have used constituent syntax as in the Penn Treebank (Marcus et al., 1993), although there has also been much research on the use of shallow syntax (Carreras and Màrquez, 2004) in SRL. In comparison, dependency syntax has received relatively little attention for the SRL task, despite the fact that dependency structures offer a more transparent encoding of predicate–argument relations. Furthermore, the few systems based on dependencies that have been presented have generally performed much worse than their constituent-based 69 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 69–78, c Honolulu, October 2008. 2008 Association for Computational Linguistics counterparts. For instance, Pradhan et al. (2005) reported that a syst"
D08-1008,W05-0620,0,0.055546,"u et al., 2003; Moschitti et al., 2003). It has also been used in prototypes of NLP systems that carry out complex reasoning, such as entailment recognition systems (Haghighi et al., 2005; Hickl et al., 2006). In addition, role-semantic features have recently been used to extend vector-space representations in automatic document categorization (Persson et al., 2008). The NLP community has recently devoted much attention to developing accurate and robust methods for performing role-semantic analysis automatically, and a number of multi-system evaluations have been carried out (Litkowski, 2004; Carreras and Màrquez, 2005; Baker et al., 2007; Surdeanu et al., 2008). Following the seminal work of Gildea and Jurafsky (2002), there have been many extensions in machine learning models, feature engineering (Xue and Palmer, 2004), and inference procedures (Toutanova et al., 2005; Surdeanu et al., 2007; Punyakanok et al., 2008). With very few exceptions (e.g. Collobert and Weston, 2007), published SRL methods have used some sort of syntactic structure as input (Gildea and Palmer, 2002; Punyakanok et al., 2008). Most systems for automatic role-semantic analysis have used constituent syntax as in the Penn Treebank (Mar"
D08-1008,D07-1101,0,0.0142776,"nitialize w to zeros repeat N times for (xt , yt ) in T let y˜t = arg max  y F (xt , y) + ρ(yt , y) yt )−F (xt ,yt )+ρ(yt ,˜ yt ) t ,˜ let τt = min C, F (xkΨ(x,y yt )k2 t )−Ψ(x,˜ w ← w + τt (Ψ(x, yt ) − Ψ(x, y˜t )) return waverage  We used a C value of 0.01, and the number of iterations was 6. 2.1.1 Features and Search The feature function Ψsyn is a factored representation, meaning that we compute the score of the complete parse tree by summing the scores of its parts, referred to as factors: X Ψ(x, y) · w = ψ(x, f ) · w f ∈y We used a second-order factorization (McDonald and Pereira, 2006; Carreras, 2007), meaning that the factors are subtrees consisting of four links: the governor–dependent link, its sibling link, and the leftmost and rightmost dependent links of the dependent. This factorization allows us to express useful features, but also forces us to adopt the expensive search procedure by Carreras (2007), which extends Eisner’s span-based dynamic programming algorithm (1996) to allow second-order feature dependencies. This algorithm has a time complexity of O(n4 ), where n is the number of words in the sentence. The search was constrained to disallow multiple root links. To evaluate the"
D08-1008,P07-1071,0,0.01719,"008). The NLP community has recently devoted much attention to developing accurate and robust methods for performing role-semantic analysis automatically, and a number of multi-system evaluations have been carried out (Litkowski, 2004; Carreras and Màrquez, 2005; Baker et al., 2007; Surdeanu et al., 2008). Following the seminal work of Gildea and Jurafsky (2002), there have been many extensions in machine learning models, feature engineering (Xue and Palmer, 2004), and inference procedures (Toutanova et al., 2005; Surdeanu et al., 2007; Punyakanok et al., 2008). With very few exceptions (e.g. Collobert and Weston, 2007), published SRL methods have used some sort of syntactic structure as input (Gildea and Palmer, 2002; Punyakanok et al., 2008). Most systems for automatic role-semantic analysis have used constituent syntax as in the Penn Treebank (Marcus et al., 1993), although there has also been much research on the use of shallow syntax (Carreras and Màrquez, 2004) in SRL. In comparison, dependency syntax has received relatively little attention for the SRL task, despite the fact that dependency structures offer a more transparent encoding of predicate–argument relations. Furthermore, the few systems based"
D08-1008,C96-1058,0,0.0458927,"Missing"
D08-1008,J02-3001,0,0.263593,"out complex reasoning, such as entailment recognition systems (Haghighi et al., 2005; Hickl et al., 2006). In addition, role-semantic features have recently been used to extend vector-space representations in automatic document categorization (Persson et al., 2008). The NLP community has recently devoted much attention to developing accurate and robust methods for performing role-semantic analysis automatically, and a number of multi-system evaluations have been carried out (Litkowski, 2004; Carreras and Màrquez, 2005; Baker et al., 2007; Surdeanu et al., 2008). Following the seminal work of Gildea and Jurafsky (2002), there have been many extensions in machine learning models, feature engineering (Xue and Palmer, 2004), and inference procedures (Toutanova et al., 2005; Surdeanu et al., 2007; Punyakanok et al., 2008). With very few exceptions (e.g. Collobert and Weston, 2007), published SRL methods have used some sort of syntactic structure as input (Gildea and Palmer, 2002; Punyakanok et al., 2008). Most systems for automatic role-semantic analysis have used constituent syntax as in the Penn Treebank (Marcus et al., 1993), although there has also been much research on the use of shallow syntax (Carreras a"
D08-1008,P02-1031,0,0.0101279,"r performing role-semantic analysis automatically, and a number of multi-system evaluations have been carried out (Litkowski, 2004; Carreras and Màrquez, 2005; Baker et al., 2007; Surdeanu et al., 2008). Following the seminal work of Gildea and Jurafsky (2002), there have been many extensions in machine learning models, feature engineering (Xue and Palmer, 2004), and inference procedures (Toutanova et al., 2005; Surdeanu et al., 2007; Punyakanok et al., 2008). With very few exceptions (e.g. Collobert and Weston, 2007), published SRL methods have used some sort of syntactic structure as input (Gildea and Palmer, 2002; Punyakanok et al., 2008). Most systems for automatic role-semantic analysis have used constituent syntax as in the Penn Treebank (Marcus et al., 1993), although there has also been much research on the use of shallow syntax (Carreras and Màrquez, 2004) in SRL. In comparison, dependency syntax has received relatively little attention for the SRL task, despite the fact that dependency structures offer a more transparent encoding of predicate–argument relations. Furthermore, the few systems based on dependencies that have been presented have generally performed much worse than their constituent"
D08-1008,H05-1049,0,0.023382,"system is the first dependency-based semantic role labeler for PropBank that rivals constituent-based systems in terms of performance. 1 Introduction Automatic semantic role labeling (SRL), the task of determining who does what to whom, is a useful intermediate step in NLP applications performing semantic analysis. It has obvious applications for template-filling tasks such as information extraction and question answering (Surdeanu et al., 2003; Moschitti et al., 2003). It has also been used in prototypes of NLP systems that carry out complex reasoning, such as entailment recognition systems (Haghighi et al., 2005; Hickl et al., 2006). In addition, role-semantic features have recently been used to extend vector-space representations in automatic document categorization (Persson et al., 2008). The NLP community has recently devoted much attention to developing accurate and robust methods for performing role-semantic analysis automatically, and a number of multi-system evaluations have been carried out (Litkowski, 2004; Carreras and Màrquez, 2005; Baker et al., 2007; Surdeanu et al., 2008). Following the seminal work of Gildea and Jurafsky (2002), there have been many extensions in machine learning model"
D08-1008,P08-1067,0,0.0234466,"n the future, we would like to further investigate whether syntactic and semantic analysis could be integrated more tightly. In this work, we used a sim1 Our system is freely available for download at http://nlp.cs.lth.se/lth_srl. plistic loose coupling by means of reranking a small set of complete structures. The same criticisms that are often leveled at reranking-based models clearly apply here too: The set of tentative analyses from the submodules is too small, and the correct analysis is often pruned too early. An example of a method to mitigate this shortcoming is the forest reranking by Huang (2008), in which complex features are evaluated as early as possible. A Classifier Features Features Used in Predicate Disambiguation P REDW ORD, P RED L EMMA. The lexical form and lemma of the predicate. P RED PARENT W ORD and P RED PARENT POS. Form and part-of-speech tag of the parent node of the predicate. C HILD D EP S ET, C HILDW ORD S ET, C HILD W ORD D EP S ET, C HILD POSS ET, C HILD POSD EP S ET. These features represent the set of dependents of the predicate using combinations of dependency labels, words, and parts of speech. D EP S UBCAT. Subcategorization frame: the concatenation of the d"
D08-1008,W08-2123,1,0.864795,"Missing"
D08-1008,C08-1050,1,0.813889,"istics counterparts. For instance, Pradhan et al. (2005) reported that a system using a rule-based dependency parser achieved much inferior results compared to a system using a state-of-the-art statistical constituent parser: The F-measure on WSJ section 23 dropped from 78.8 to 47.2, or from 83.7 to 61.7 when using a head-based evaluation. In a similar vein, Swanson and Gordon (2006) reported that parse tree path features extracted from a rule-based dependency parser are much less reliable than those from a modern constituent parser. In contrast, we recently carried out a detailed comparison (Johansson and Nugues, 2008b) between constituent-based and dependency-based SRL systems for FrameNet, in which the results of the two types of systems where almost equivalent when using modern statistical dependency parsers. We suggested that the previous lack of progress in dependency-based SRL was due to low parsing accuracy. The experiments showed that the grammatical function information available in dependency representations results in a steeper learning curve when training semantic role classifiers, and it also seemed that the dependency-based role classifiers were more resilient to lexical problems caused by ch"
D08-1008,W04-0803,0,0.0225849,"nswering (Surdeanu et al., 2003; Moschitti et al., 2003). It has also been used in prototypes of NLP systems that carry out complex reasoning, such as entailment recognition systems (Haghighi et al., 2005; Hickl et al., 2006). In addition, role-semantic features have recently been used to extend vector-space representations in automatic document categorization (Persson et al., 2008). The NLP community has recently devoted much attention to developing accurate and robust methods for performing role-semantic analysis automatically, and a number of multi-system evaluations have been carried out (Litkowski, 2004; Carreras and Màrquez, 2005; Baker et al., 2007; Surdeanu et al., 2008). Following the seminal work of Gildea and Jurafsky (2002), there have been many extensions in machine learning models, feature engineering (Xue and Palmer, 2004), and inference procedures (Toutanova et al., 2005; Surdeanu et al., 2007; Punyakanok et al., 2008). With very few exceptions (e.g. Collobert and Weston, 2007), published SRL methods have used some sort of syntactic structure as input (Gildea and Palmer, 2002; Punyakanok et al., 2008). Most systems for automatic role-semantic analysis have used constituent syntax"
D08-1008,J93-2004,0,0.0322215,"005; Baker et al., 2007; Surdeanu et al., 2008). Following the seminal work of Gildea and Jurafsky (2002), there have been many extensions in machine learning models, feature engineering (Xue and Palmer, 2004), and inference procedures (Toutanova et al., 2005; Surdeanu et al., 2007; Punyakanok et al., 2008). With very few exceptions (e.g. Collobert and Weston, 2007), published SRL methods have used some sort of syntactic structure as input (Gildea and Palmer, 2002; Punyakanok et al., 2008). Most systems for automatic role-semantic analysis have used constituent syntax as in the Penn Treebank (Marcus et al., 1993), although there has also been much research on the use of shallow syntax (Carreras and Màrquez, 2004) in SRL. In comparison, dependency syntax has received relatively little attention for the SRL task, despite the fact that dependency structures offer a more transparent encoding of predicate–argument relations. Furthermore, the few systems based on dependencies that have been presented have generally performed much worse than their constituent-based 69 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 69–78, c Honolulu, October 2008. 2008 Associatio"
D08-1008,E06-1011,0,0.0218347,"Regularization parameter C Initialize w to zeros repeat N times for (xt , yt ) in T let y˜t = arg max  y F (xt , y) + ρ(yt , y) yt )−F (xt ,yt )+ρ(yt ,˜ yt ) t ,˜ let τt = min C, F (xkΨ(x,y yt )k2 t )−Ψ(x,˜ w ← w + τt (Ψ(x, yt ) − Ψ(x, y˜t )) return waverage  We used a C value of 0.01, and the number of iterations was 6. 2.1.1 Features and Search The feature function Ψsyn is a factored representation, meaning that we compute the score of the complete parse tree by summing the scores of its parts, referred to as factors: X Ψ(x, y) · w = ψ(x, f ) · w f ∈y We used a second-order factorization (McDonald and Pereira, 2006; Carreras, 2007), meaning that the factors are subtrees consisting of four links: the governor–dependent link, its sibling link, and the leftmost and rightmost dependent links of the dependent. This factorization allows us to express useful features, but also forces us to adopt the expensive search procedure by Carreras (2007), which extends Eisner’s span-based dynamic programming algorithm (1996) to allow second-order feature dependencies. This algorithm has a time complexity of O(n4 ), where n is the number of words in the sentence. The search was constrained to disallow multiple root links"
D08-1008,P05-1013,0,0.0281817,"Missing"
D08-1008,P05-1072,0,0.0764465,"hallow syntax (Carreras and Màrquez, 2004) in SRL. In comparison, dependency syntax has received relatively little attention for the SRL task, despite the fact that dependency structures offer a more transparent encoding of predicate–argument relations. Furthermore, the few systems based on dependencies that have been presented have generally performed much worse than their constituent-based 69 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 69–78, c Honolulu, October 2008. 2008 Association for Computational Linguistics counterparts. For instance, Pradhan et al. (2005) reported that a system using a rule-based dependency parser achieved much inferior results compared to a system using a state-of-the-art statistical constituent parser: The F-measure on WSJ section 23 dropped from 78.8 to 47.2, or from 83.7 to 61.7 when using a head-based evaluation. In a similar vein, Swanson and Gordon (2006) reported that parse tree path features extracted from a rule-based dependency parser are much less reliable than those from a modern constituent parser. In contrast, we recently carried out a detailed comparison (Johansson and Nugues, 2008b) between constituent-based a"
D08-1008,J08-2005,0,0.465119,"s in automatic document categorization (Persson et al., 2008). The NLP community has recently devoted much attention to developing accurate and robust methods for performing role-semantic analysis automatically, and a number of multi-system evaluations have been carried out (Litkowski, 2004; Carreras and Màrquez, 2005; Baker et al., 2007; Surdeanu et al., 2008). Following the seminal work of Gildea and Jurafsky (2002), there have been many extensions in machine learning models, feature engineering (Xue and Palmer, 2004), and inference procedures (Toutanova et al., 2005; Surdeanu et al., 2007; Punyakanok et al., 2008). With very few exceptions (e.g. Collobert and Weston, 2007), published SRL methods have used some sort of syntactic structure as input (Gildea and Palmer, 2002; Punyakanok et al., 2008). Most systems for automatic role-semantic analysis have used constituent syntax as in the Penn Treebank (Marcus et al., 1993), although there has also been much research on the use of shallow syntax (Carreras and Màrquez, 2004) in SRL. In comparison, dependency syntax has received relatively little attention for the SRL task, despite the fact that dependency structures offer a more transparent encoding of pred"
D08-1008,P03-1002,0,0.0449187,"+Brown test set, or 78.84 if punctuation is treated consistently. Using a dependency-based metric, the F1 figure of our system is 84.29 on the test set from CoNLL-2008. Our system is the first dependency-based semantic role labeler for PropBank that rivals constituent-based systems in terms of performance. 1 Introduction Automatic semantic role labeling (SRL), the task of determining who does what to whom, is a useful intermediate step in NLP applications performing semantic analysis. It has obvious applications for template-filling tasks such as information extraction and question answering (Surdeanu et al., 2003; Moschitti et al., 2003). It has also been used in prototypes of NLP systems that carry out complex reasoning, such as entailment recognition systems (Haghighi et al., 2005; Hickl et al., 2006). In addition, role-semantic features have recently been used to extend vector-space representations in automatic document categorization (Persson et al., 2008). The NLP community has recently devoted much attention to developing accurate and robust methods for performing role-semantic analysis automatically, and a number of multi-system evaluations have been carried out (Litkowski, 2004; Carreras and M"
D08-1008,W08-2121,1,0.911861,"also been used in prototypes of NLP systems that carry out complex reasoning, such as entailment recognition systems (Haghighi et al., 2005; Hickl et al., 2006). In addition, role-semantic features have recently been used to extend vector-space representations in automatic document categorization (Persson et al., 2008). The NLP community has recently devoted much attention to developing accurate and robust methods for performing role-semantic analysis automatically, and a number of multi-system evaluations have been carried out (Litkowski, 2004; Carreras and Màrquez, 2005; Baker et al., 2007; Surdeanu et al., 2008). Following the seminal work of Gildea and Jurafsky (2002), there have been many extensions in machine learning models, feature engineering (Xue and Palmer, 2004), and inference procedures (Toutanova et al., 2005; Surdeanu et al., 2007; Punyakanok et al., 2008). With very few exceptions (e.g. Collobert and Weston, 2007), published SRL methods have used some sort of syntactic structure as input (Gildea and Palmer, 2002; Punyakanok et al., 2008). Most systems for automatic role-semantic analysis have used constituent syntax as in the Penn Treebank (Marcus et al., 1993), although there has also b"
D08-1008,P06-2104,0,0.0144666,"ted have generally performed much worse than their constituent-based 69 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 69–78, c Honolulu, October 2008. 2008 Association for Computational Linguistics counterparts. For instance, Pradhan et al. (2005) reported that a system using a rule-based dependency parser achieved much inferior results compared to a system using a state-of-the-art statistical constituent parser: The F-measure on WSJ section 23 dropped from 78.8 to 47.2, or from 83.7 to 61.7 when using a head-based evaluation. In a similar vein, Swanson and Gordon (2006) reported that parse tree path features extracted from a rule-based dependency parser are much less reliable than those from a modern constituent parser. In contrast, we recently carried out a detailed comparison (Johansson and Nugues, 2008b) between constituent-based and dependency-based SRL systems for FrameNet, in which the results of the two types of systems where almost equivalent when using modern statistical dependency parsers. We suggested that the previous lack of progress in dependency-based SRL was due to low parsing accuracy. The experiments showed that the grammatical function inf"
D08-1008,P05-1073,0,0.0343412,"Missing"
D08-1008,W04-3212,0,0.0826378,"n addition, role-semantic features have recently been used to extend vector-space representations in automatic document categorization (Persson et al., 2008). The NLP community has recently devoted much attention to developing accurate and robust methods for performing role-semantic analysis automatically, and a number of multi-system evaluations have been carried out (Litkowski, 2004; Carreras and Màrquez, 2005; Baker et al., 2007; Surdeanu et al., 2008). Following the seminal work of Gildea and Jurafsky (2002), there have been many extensions in machine learning models, feature engineering (Xue and Palmer, 2004), and inference procedures (Toutanova et al., 2005; Surdeanu et al., 2007; Punyakanok et al., 2008). With very few exceptions (e.g. Collobert and Weston, 2007), published SRL methods have used some sort of syntactic structure as input (Gildea and Palmer, 2002; Punyakanok et al., 2008). Most systems for automatic role-semantic analysis have used constituent syntax as in the Penn Treebank (Marcus et al., 1993), although there has also been much research on the use of shallow syntax (Carreras and Màrquez, 2004) in SRL. In comparison, dependency syntax has received relatively little attention for"
D08-1008,W09-4621,1,\N,Missing
dutoit-etal-2004-integral,C92-3151,1,\N,Missing
dutoit-nugues-2002-algorithm,C92-3151,1,\N,Missing
dutoit-nugues-2002-algorithm,P98-2180,0,\N,Missing
dutoit-nugues-2002-algorithm,C98-2175,0,\N,Missing
E03-2002,P84-1106,0,0.228774,"Missing"
E03-2002,W01-1301,1,0.654412,"lz.t. Pierre Nuguest tTechnische Universitat Hamburg-Harburg Schwarzenbergstrae 95 D-21071 Hamburg, Germany b.schulz@tuhh.de al., 1984; Di Manzo et al., 1986) is an early system that was designed to recreate static 2D scenes from simple phrases in Italian. WordsEye (Coyne and Sproat, 2001) is a recent and ambitious example. It features a large database of 3D objects that can be animated. CogViSys (Nagel, 2001; Arens et al., 2002) is aimed a visualizing descriptions of simple car maneuvers at crossroads. All these systems use apparently invented narratives. 2 CarSim CarSim (Egges et al., 2001; Dupuy et al., 2001) is a program that analyzes texts describing car accidents and visualizes them in a 3D environment. The CarSim architecture consists of two modules. A first module carries out a linguistic analysis of the accident and creates a template — a tabular representation — of the text. A second module creates the 3D scene from the template. The template has been designed so that it contains the information necessary to reproduce and animate the accidents (Figure 1). A first version of CarSim was designed to process texts in French. We used a corpus of 87 car accident reports written in French and prov"
E03-2002,1993.iwpt-1.22,0,0.0298876,"from the text is mapped onto a predefined XML structure that consists of three parts: the static objects, the dynamic objects, and the collision objects. The static objects are the non-moving objects such as trees, obstacles, and road signs. The dynamic objects are moving objects, the vehicles. Examples of dynamic objects are cars and trucks. The collision object structure describes the interaction between dynamic objects and/or static objects. We used two available linguistic resources to analyze the texts: the WordNet lexical database (Fellbaum, 1998) and the Link Grammar dependency parser (Sleator and Temperley, 1993). The strategy to determine the accidents and the actors is to find the collision verbs. CarSim uses regular expressions to search verb patterns in texts. Then, CarSim extracts the dependents of the verb. It evaluates the grammatical function of the word groups, examines words, classifies them using the WordNet hierarchy, and fills the XML template (Akerberg and Svensson, 2002). Table 1 shows the template corresponding to text HAR-00-02. and end positions of the vehicles from the initial directions, the configuration of the other objects in the scene, and the chain of events as if they were no"
E06-1049,W01-1301,1,0.677768,"ntified in the narratives. We finally report the results we obtained. {20 persons}o1 diede2 in a {bus accident}e1 in southern Afghanistan {on Thursday}t1 . In addition, {39 persons}o2 {were injured}e3 in the accidente4 . The buso3 {was on its way}e5 from Kandahar to the capital Kabul when ito4 {drove off}e7 the roado5 while overtakinge6 and {flipped over}e8 , saide9 General Salim Khan, assistant head of police in Kandahar. 1 Extraction of Temporal Information and Scene Visualization Carsim is a program that generates 3D scenes from narratives describing road accidents (Johansson et al., 2005; Dupuy et al., 2001). It considers authentic texts, generally collected from web sites of Swedish newspapers or transcribed from handwritten accounts by victims of accidents. One of Carsim’s key features is that it animates the generated scene to visualize events described in the narrative. The text below, a newspaper article with its translation into English, illustrates the goals and challenges of it. We bracketed the entities, time expressions, and events and we annotated them with identifiers, denoted respectively oi , tj , and ek : The text above, our translation. To create a consistent animation, the progra"
E06-1049,P04-1074,0,0.163577,"cal framework to deal with events in simple past and pluperfect sentences. Hitzeman et al. (1995) proposed a constraintbased approach taking into account tense, aspect, temporal adverbials, and rhetorical structure to analyze a discourse. Recently, groups have used machine learning techniques to determine temporal relations. They trained automatically classifiers on handannotated corpora. Mani et al. (2003) achieved the best results so far by using decision trees to order partially events of successive clauses in English texts. Boguraev and Ando (2005) is another example of it for English and Li et al. (2004) for Chinese. 3 Annotating Texts with Temporal Information Several schemes have been proposed to annotate temporal information in texts, see Setzer and Gaizauskas (2002), inter alia. Many of them were incompatible or incomplete and in an effort to reconcile and unify the field, Ingria and Pustejovsky (2002) introduced the XML-based Time markup language (TimeML). TimeML is a specification language whose goal is to capture most aspects of temporal relations between events in discourses. It is based on Allen’s (1984) relations and a variation of Vendler’s (1967) classification of verbs. It define"
E06-1049,N03-2019,0,0.122594,"(1986) investigated it and formulated a Temporal Discourse Interpretation Principle to interpret the advance of narrative time in a sequence of sentences. Lascarides and Asher (1993) described a complex logical framework to deal with events in simple past and pluperfect sentences. Hitzeman et al. (1995) proposed a constraintbased approach taking into account tense, aspect, temporal adverbials, and rhetorical structure to analyze a discourse. Recently, groups have used machine learning techniques to determine temporal relations. They trained automatically classifiers on handannotated corpora. Mani et al. (2003) achieved the best results so far by using decision trees to order partially events of successive clauses in English texts. Boguraev and Ando (2005) is another example of it for English and Li et al. (2004) for Chinese. 3 Annotating Texts with Temporal Information Several schemes have been proposed to annotate temporal information in texts, see Setzer and Gaizauskas (2002), inter alia. Many of them were incompatible or incomplete and in an effort to reconcile and unify the field, Ingria and Pustejovsky (2002) introduced the XML-based Time markup language (TimeML). TimeML is a specification lan"
E06-1049,W01-1311,0,0.0316368,"sequences of events where it decides the order between two of the events in each sequence. If e1 , ..., en are the events in the sequence they appear in the text, the trees correspond to the following functions: fdt1 (ei , ei+1 ) ⇒ trel (ei , ei+1 ) fdt2 (ei , ei+1 , ei+2 ) ⇒ trel (ei , ei+1 ) fdt3 (ei , ei+1 , ei+2 ) ⇒ trel (ei+1 , ei+2 ) fdt4 (ei , ei+1 , ei+2 ) ⇒ trel (ei , ei+2 ) fdt5 (ei , ei+1 , ei+2 , ei+3 ) ⇒ trel (ei , ei+3 ) The possible output values of the trees are: simultaneous, after, before, is_included, includes, and none. These values correspond to the relations described by Setzer and Gaizauskas (2001). The first decision tree should capture more general relations between two adjacent events without the need of a context. Decision trees dt2 and dt3 extend the context by one event to the left respectively one event to the right. They should capture more specific phenomena. However, they are not always applicable as we never apply a decision 388 tree when there is a time expression between any of the events involved. In effect, time expressions “reanchor” the narrative temporally, and we noticed that the decision trees performed very poorly across time expressions. We complemented the decisio"
E06-1049,E95-1035,0,\N,Missing
E06-2013,J02-3001,0,0.462295,"are, this is the first system that finds this information automatically. 1 Introduction Shallow semantic parsing has been an active area of research during the last few years. Semantic parsers, which are typically based on the FrameNet (Baker et al., 1998) or PropBank formalisms, have proven useful in a number of NLP projects, such as information extraction and question answering. The main reason for their popularity is that they can produce a flat layer of semantic structure with a fair degree of robustness. Building English semantic parsers for the FrameNet standard has been studied widely (Gildea and Jurafsky, 2002; Litkowski, 2004). These systems typically address the task of identifying and classifying Frame Elements (FEs), that is semantic arguments of predicates, for a given target word (predicate). Although the FE layer is arguably the most central, the FrameNet annotation standard defines a number of additional semantic layers, which contain information about support expressions (verbs and prepositions), copulas, null arguments, slotfillers, and aspectual particles. This information can for example be used in a semantic parser to refine the meaning of a predicate, to link predicates in a sentence"
E06-2013,W04-0803,0,0.0307557,"em that finds this information automatically. 1 Introduction Shallow semantic parsing has been an active area of research during the last few years. Semantic parsers, which are typically based on the FrameNet (Baker et al., 1998) or PropBank formalisms, have proven useful in a number of NLP projects, such as information extraction and question answering. The main reason for their popularity is that they can produce a flat layer of semantic structure with a fair degree of robustness. Building English semantic parsers for the FrameNet standard has been studied widely (Gildea and Jurafsky, 2002; Litkowski, 2004). These systems typically address the task of identifying and classifying Frame Elements (FEs), that is semantic arguments of predicates, for a given target word (predicate). Although the FE layer is arguably the most central, the FrameNet annotation standard defines a number of additional semantic layers, which contain information about support expressions (verbs and prepositions), copulas, null arguments, slotfillers, and aspectual particles. This information can for example be used in a semantic parser to refine the meaning of a predicate, to link predicates in a sentence together, or possi"
E06-2013,W04-0814,0,0.12476,"Missing"
E06-2013,P98-1013,0,0.235332,".lth.se Abstract We describe a system for automatic annotation of English text in the FrameNet standard. In addition to the conventional annotation of frame elements and their semantic roles, we annotate additional semantic information such as support verbs and prepositions, aspectual markers, copular verbs, null arguments, and slot fillers. As far as we are aware, this is the first system that finds this information automatically. 1 Introduction Shallow semantic parsing has been an active area of research during the last few years. Semantic parsers, which are typically based on the FrameNet (Baker et al., 1998) or PropBank formalisms, have proven useful in a number of NLP projects, such as information extraction and question answering. The main reason for their popularity is that they can produce a flat layer of semantic structure with a fair degree of robustness. Building English semantic parsers for the FrameNet standard has been studied widely (Gildea and Jurafsky, 2002; Litkowski, 2004). These systems typically address the task of identifying and classifying Frame Elements (FEs), that is semantic arguments of predicates, for a given target word (predicate). Although the FE layer is arguably the"
E06-2013,J03-4003,0,\N,Missing
E06-2013,C98-1013,0,\N,Missing
exner-nugues-2012-constructing,W10-0907,0,\N,Missing
exner-nugues-2012-constructing,W08-2121,0,\N,Missing
exner-nugues-2012-constructing,W09-1201,0,\N,Missing
exner-nugues-2012-constructing,J05-1004,0,\N,Missing
exner-nugues-2014-refractive,W10-0907,0,\N,Missing
exner-nugues-2014-refractive,C10-1011,0,\N,Missing
exner-nugues-2014-refractive,W09-1206,1,\N,Missing
exner-nugues-2014-refractive,C10-3009,1,\N,Missing
exner-nugues-2014-refractive,W10-0915,0,\N,Missing
exner-nugues-2014-refractive,W11-1902,0,\N,Missing
exner-nugues-2014-refractive,D12-1048,0,\N,Missing
exner-nugues-2014-refractive,J05-1004,0,\N,Missing
exner-nugues-2014-refractive,D11-1142,0,\N,Missing
exner-nugues-2014-refractive,cattoni-etal-2012-knowledgestore,0,\N,Missing
exner-nugues-2014-refractive,D10-1048,0,\N,Missing
johansson-nugues-2006-construction,W04-0803,0,\N,Missing
johansson-nugues-2006-construction,W04-2407,0,\N,Missing
johansson-nugues-2006-construction,H01-1035,0,\N,Missing
johansson-nugues-2006-construction,C04-1134,0,\N,Missing
johansson-nugues-2006-construction,J03-4003,0,\N,Missing
johansson-nugues-2006-construction,H05-1108,0,\N,Missing
johansson-nugues-2006-construction,P98-1013,0,\N,Missing
johansson-nugues-2006-construction,C98-1013,0,\N,Missing
johansson-nugues-2006-construction,P02-1050,0,\N,Missing
johansson-nugues-2006-construction,J02-3001,0,\N,Missing
johansson-nugues-2006-construction,J03-1002,0,\N,Missing
johansson-nugues-2006-construction,2005.mtsummit-papers.11,0,\N,Missing
johansson-nugues-2008-comparing,A00-2018,0,\N,Missing
johansson-nugues-2008-comparing,burchardt-etal-2006-salsa,0,\N,Missing
johansson-nugues-2008-comparing,E06-1011,0,\N,Missing
johansson-nugues-2008-comparing,P97-1003,0,\N,Missing
johansson-nugues-2008-comparing,S07-1048,1,\N,Missing
johansson-nugues-2008-comparing,W04-3212,0,\N,Missing
johansson-nugues-2008-comparing,P98-1013,0,\N,Missing
johansson-nugues-2008-comparing,C98-1013,0,\N,Missing
johansson-nugues-2008-comparing,W05-0620,0,\N,Missing
johansson-nugues-2008-comparing,J02-3001,0,\N,Missing
johansson-nugues-2008-comparing,W07-2416,1,\N,Missing
K15-1019,J93-1001,0,0.089202,"identifiers in its caption, we first generated all the possible pairs. We then ranked these pairs using a semantic distance derived from WordNet (Miller, 1995), statistical association metrics, and finally, a combination of both techniques. 5.1 Statistical Associations • Pointwise mutual information (P M I) (Fano, 1961) that compares the joint probability of the occurrence of a (image label, entity identifier) pair to the independent probability of the region label and the caption entity occurring by themselves; and finally Semantic Distance • The simplified Student’s t-score as described in Church and Mercer (1993). The image labels are generic English words that are semantically similar to those used in the captions. In Fig. 1, cloud and clouds are used both as label and in the caption, but the region labeled grass is described as a meadow and the region labeled river, as a lagoon. We used the WordNet Similarity for Java library, (WS4J), (Shima, 2014) to compute the semantic similarity of the region labels and the entity identifiers. WS4J comes with a number of metrics that approximate similarity as distances between WordNet synsets: PATH, WUP (Wu and Palmer, 1994), RES, (Resnik, 1995), JCN (Jiang and"
K15-1019,D13-1128,0,0.0832276,"Missing"
K15-1019,P05-1045,0,0.00590721,"related images and sentences. They used neural networks and they show that the results are improved if image objects and sentence fragments are included in the model. Sentence fragments are extracted from dependency graphs, where each edge in the graphs corresponds to a fragment. 4.2 Entities and Mentions An image caption describes a set of entities, the caption entities CE, where each entity CEi is referred to by a set of mentions M . To detect them, we applied the Stanford CoreNLP pipeline (Toutanova et al., 2003) that consists of a partof-speech tagger, lemmatizer, named entity recognizer (Finkel et al., 2005), dependency parser, and coreference solver. We considered each noun in a caption as an entity candidate. If an entity CEi had only one mention Mj , we identified it by the head noun of its mention. We represented the entities mentioned more than once by the head noun of their most representative mention. We applied the entity extraction to all the captions in the data set, and we found 3,742 different nouns or noun compounds to represent the entities. In addition to the caption entities, each image has a set of labeled segments (or regions) corresponding to the image entities, IE. The Cartesi"
K15-1019,N03-1033,0,0.0115153,"Missing"
K15-1019,O97-1002,0,0.426754,"cer (1993). The image labels are generic English words that are semantically similar to those used in the captions. In Fig. 1, cloud and clouds are used both as label and in the caption, but the region labeled grass is described as a meadow and the region labeled river, as a lagoon. We used the WordNet Similarity for Java library, (WS4J), (Shima, 2014) to compute the semantic similarity of the region labels and the entity identifiers. WS4J comes with a number of metrics that approximate similarity as distances between WordNet synsets: PATH, WUP (Wu and Palmer, 1994), RES, (Resnik, 1995), JCN (Jiang and Conrath, 1997), HSO (Hirst and StOnge, 1998), LIN (Lin, 1998), LCH (Leacock and Chodorow, 1998), and LESK (Banerjee and Banerjee, 2002). As with the semantic similarity scores, we used matrices to hold the scores for all the (image label, entity identifier) pairs for the three association metrics. 5.3 The Mapping Algorithm To associate the region labels of an image to the entities in its caption, we mapped the label Li to the caption entity Ej that had the highest score with respect to Li . We did this for the three association scores and the eight semantic metrics. Note that a region label is not systemati"
K15-1019,P94-1019,0,\N,Missing
L16-1654,C10-3009,1,0.862177,"ation of an abstraction. The role of the injector is to provide instances of requested abstractions as well as concrete classes. The injector also injects the dependencies needed to construct these instances. We used Guice (Google, 2011) as base library on top of which we developed thread-safe constructions to be able to process indices and storage. Tool chains. The tool chains are instances of annotators and are speciﬁc to the languages we process. For English, Spanish, and German, we use CoreNLP (Manning et al., 2014). For French, we use CoreNLP for tokenizing the text and MATE for parsing (Björkelund et al., 2010). For Swedish, we use Stagger (Östling, 2013) and MaltParser 4142 Wiki markup Wikimedia dump HTML XOWA DOM Multilayer document model Text html HTML/XOWA } head body jsoup title p Sections WDM parser } List items Anchors div HTML/ZIM Paragraphs Tokens Italic characters … table Nine sequences of ranges describing the structural information Figure 1: Conversion of Wikipedia dumps into abstract syntactic trees and the Multilayer Document Model (MLDM) (Nivre et al., 2006). For Russian, there is no linguistic annotation and the tool chain is reduced to nothing as of today. 3.4. speech, and Table 2 s"
L16-1654,W06-2920,0,0.0798921,"ly a grammatical annotation that is language speciﬁc. Depending on the language, and the components or resources available for it, the annotation can range from a simple tokenization to semantic-role labels or coreference chains. Multilinguality. We implemented the grammatical annotation so that it has a multilingual support at the core. All the language-dependent algorithms are stored in separate packages and tied to the system through abstractions that operate on the Multilayer Document Model. More speciﬁcally, the token annotations include a dictionary loosely inspired by the CoNLL format (Buchholz and Marsi, 2006) extended with naming conventions from Exner and Nugues (2014). Dependency Injection. The annotators apply the dependency injection (Fowler, 2004) pattern that solves the problem of hard-coded dependencies by making the code only depend on a dependency injector. The dependency injector is constructed once and reused multiple times. The injector can be conﬁgured to provide different concrete implementations which allow a high-level way of switching the implementation of an abstraction. The role of the injector is to provide instances of requested abstractions as well as concrete classes. The in"
L16-1654,P14-5010,0,0.00493727,"ovide different concrete implementations which allow a high-level way of switching the implementation of an abstraction. The role of the injector is to provide instances of requested abstractions as well as concrete classes. The injector also injects the dependencies needed to construct these instances. We used Guice (Google, 2011) as base library on top of which we developed thread-safe constructions to be able to process indices and storage. Tool chains. The tool chains are instances of annotators and are speciﬁc to the languages we process. For English, Spanish, and German, we use CoreNLP (Manning et al., 2014). For French, we use CoreNLP for tokenizing the text and MATE for parsing (Björkelund et al., 2010). For Swedish, we use Stagger (Östling, 2013) and MaltParser 4142 Wiki markup Wikimedia dump HTML XOWA DOM Multilayer document model Text html HTML/XOWA } head body jsoup title p Sections WDM parser } List items Anchors div HTML/ZIM Paragraphs Tokens Italic characters … table Nine sequences of ranges describing the structural information Figure 1: Conversion of Wikipedia dumps into abstract syntactic trees and the Multilayer Document Model (MLDM) (Nivre et al., 2006). For Russian, there is no lin"
L16-1654,nivre-etal-2006-maltparser,0,0.239408,"Missing"
L18-1540,P98-1012,0,0.491378,"Missing"
L18-1540,E06-1002,0,0.149351,"Missing"
L18-1540,D07-1074,0,0.548815,"Missing"
L18-1540,P05-1045,0,0.0186079,"ge. Piccinno and Ferragina (2014) used a dictionary of surface forms similar to Cucerzan (2014) to spot the mentions. They also used a pruner to discard unlikely annotations based on a classifier and a coherence metric with the set of neighboring entities. This final selection is done at linking time. Sil et al. (2015) used classifiers based on neural nets and conditional random fields trained on three languages. Some annotators also used an external named entity recognition module to carry out this mention detection as AIDA (Hoffart et al., 2011) and Tan et al. (2015) that used Stanford NER (Finkel et al., 2005). 2.2. Entity Linking Bagga and Baldwin (1998) is one of the earliest works that introduced the notion of linkage to unique things through the task of cross-document coreference. The main difference with entity linking is that predefined lists of entities do not exist but have to be found. Bagga and Baldwin (1998) created summary vectors and tried to cluster them to form linkages. These summary vectors were created from noun phrases contained within coreference chains in documents. Using cosine similarity with a predefined threshold, they were able to cluster coreferences that crossed the docu"
L18-1540,D11-1072,0,0.234189,"Missing"
L18-1540,L16-1654,1,0.845511,"nce between the mention and candidate title, the commonness and pagerank weights. We evaluated the system with the same method as used in the TAC 2016 competition (Ji and Nothman, 2016) and we reached the CEAFm scores of 70.0 on English, on 64.4 on Chinese, and 66.5 on Spanish. We applied our linker to Swedish without any language adaptation. We deployed the entity linker on our cluster and we used HDFS to spread the Wikipedia dump across the nodes as well as to save the final result. 5. The Document Model We represented Wikipedia and the entity annotations using the Docforia document model2 (Klang and Nugues, 2016b; Klang and Nugues, 2016a; Klang and Nugues, 2017). Docforia is designed it so that we can store the original markup, as well as any subsequent linguistic annotation. It consists of multiple layers, where each layer is dedicated to a specific type of annotation. The annotations are encoded in the form of graph nodes, where a node represents a piece of data: a token, a sentence, a named entity, etc., delimited by ranges. These nodes are possibly connected by edges as in dependency graphs. The data structure used is similar to a property graph. 6. Indexing We created an indexing tool, Panforia,"
L18-1540,W17-0227,1,0.895779,"ues Lund University, Department of Computer science, Lund, Sweden marcus.klang@cs.lth.se, pierre.nugues@cs.lth.se Abstract In this paper, we describe a new system to extract, index, search, and visualize entities in Wikipedia. To carry out the entity extraction, we designed a high-performance, multilingual, entity linker and we used a document model to store the resulting linguistic annotations. The entity linker, HEDWIG, extracts the mentions from text using a string matching engine and links them to entities with a combination of statistical rules and PageRank. The document model, Docforia (Klang and Nugues, 2017), consists of layers, where each layer is a sequence of ranges describing a specific annotation, here the entities. We evaluated HEDWIG with the TAC 2016 data and protocol (Ji and Nothman, 2016) and we reached the CEAFm scores of 70.0 on English, on 64.4 on Chinese, and 66.5 on Spanish. We applied the entity linker to the whole collection of English and Swedish articles of Wikipedia and we used Lucene to index the layers and a search module to interactively retrieve all the concordances of an entity in Wikipedia. The user can select and visualize the concordances in the articles or paragraphs."
L18-1540,P14-5010,0,0.00479676,"ase. Following Lipczak et al. (2014) and S¨odergren and Nugues (2017), we used an automaton to spot the mentions. This automaton uses Lucene’s finite-state transducers and is efficient in terms of memory usage and execution time. Depending on the language and the availability of manuallyannotated data, we can complement this candidate generation with two named-entity recognition systems trained on the annotated data: The first one being based on an extension of the fixed-size ordinally forgetting encoding (FOFE) technique (Xu et al., 2017; Zhang et al., 2015) and the second one being CoreNLP (Manning et al., 2014). The overgeneration of mention candidates impairs the quality of the downstream linker. To discard the very unlikely ones, we introduced rules based on the frequency of the manual links applied to mention M and its link probability lp. We denote Mlinked a mention with a manual hyperlink; this would correspond to the wiki markup: [[link|mention]], and Mautolinked , an autolinked mention. We define: • The count of (entity1, entity2) pairs in a window corresponding to a paragraph and limited to 20 linked mentions. This is carried out after autolinking; • Capitalization statistics for all the tok"
L18-1540,N10-1063,0,0.0416244,"trary to classic text indexing, this system does not use strings to identify the entities but unique identifiers from Wikidata. A demonstration of the entity search and visualization will be available for English at this address http://vilde.cs.lth.se:9001/en-hedwig/ and for Swedish at: http://vilde.cs.lth.se:9001/sv-hedwig/. Keywords: named entity recognition, entity linker, wikipedia 1. Introduction 2. Wikipedia has become a popular NLP resource used in many projects such as text categorization (Wang et al., 2009), information extraction, question answering (Ferrucci, 2012), or translation (Smith et al., 2010). In addition to its size and diversity, Wikipedia, through its links, also enables to create a graph that associates concepts, entities, and their mentions in text. Wu and Weld (2010), for instance, used the “wikilinks”, the Wikipedia hyperlinks, to collect the mentions of an entity and build sets of synonyms for an open information extraction system. However, according to the edition rules of Wikipedia, only the first mention of an entity should be linked in an article. An automatic wikification is then necessary to associate the subsequent mentions with an entity (Mihalcea and Csomai, 2007)"
L18-1540,W17-0211,1,0.872765,"Missing"
L18-1540,P10-1013,0,0.0196637,"ll be available for English at this address http://vilde.cs.lth.se:9001/en-hedwig/ and for Swedish at: http://vilde.cs.lth.se:9001/sv-hedwig/. Keywords: named entity recognition, entity linker, wikipedia 1. Introduction 2. Wikipedia has become a popular NLP resource used in many projects such as text categorization (Wang et al., 2009), information extraction, question answering (Ferrucci, 2012), or translation (Smith et al., 2010). In addition to its size and diversity, Wikipedia, through its links, also enables to create a graph that associates concepts, entities, and their mentions in text. Wu and Weld (2010), for instance, used the “wikilinks”, the Wikipedia hyperlinks, to collect the mentions of an entity and build sets of synonyms for an open information extraction system. However, according to the edition rules of Wikipedia, only the first mention of an entity should be linked in an article. An automatic wikification is then necessary to associate the subsequent mentions with an entity (Mihalcea and Csomai, 2007). In addition, searching entities using names in the form of strings can be tricky as names are sometimes ambiguous and entities may have more than one name. Finding all the occurrence"
L18-1540,P17-1114,0,0.0150463,"ntion segmenter that identifies the mentions to keep for the linking phase. Following Lipczak et al. (2014) and S¨odergren and Nugues (2017), we used an automaton to spot the mentions. This automaton uses Lucene’s finite-state transducers and is efficient in terms of memory usage and execution time. Depending on the language and the availability of manuallyannotated data, we can complement this candidate generation with two named-entity recognition systems trained on the annotated data: The first one being based on an extension of the fixed-size ordinally forgetting encoding (FOFE) technique (Xu et al., 2017; Zhang et al., 2015) and the second one being CoreNLP (Manning et al., 2014). The overgeneration of mention candidates impairs the quality of the downstream linker. To discard the very unlikely ones, we introduced rules based on the frequency of the manual links applied to mention M and its link probability lp. We denote Mlinked a mention with a manual hyperlink; this would correspond to the wiki markup: [[link|mention]], and Mautolinked , an autolinked mention. We define: • The count of (entity1, entity2) pairs in a window corresponding to a paragraph and limited to 20 linked mentions. This"
L18-1540,P15-2081,0,0.0473928,"hat identifies the mentions to keep for the linking phase. Following Lipczak et al. (2014) and S¨odergren and Nugues (2017), we used an automaton to spot the mentions. This automaton uses Lucene’s finite-state transducers and is efficient in terms of memory usage and execution time. Depending on the language and the availability of manuallyannotated data, we can complement this candidate generation with two named-entity recognition systems trained on the annotated data: The first one being based on an extension of the fixed-size ordinally forgetting encoding (FOFE) technique (Xu et al., 2017; Zhang et al., 2015) and the second one being CoreNLP (Manning et al., 2014). The overgeneration of mention candidates impairs the quality of the downstream linker. To discard the very unlikely ones, we introduced rules based on the frequency of the manual links applied to mention M and its link probability lp. We denote Mlinked a mention with a manual hyperlink; this would correspond to the wiki markup: [[link|mention]], and Mautolinked , an autolinked mention. We define: • The count of (entity1, entity2) pairs in a window corresponding to a paragraph and limited to 20 linked mentions. This is carried out after"
P06-2057,C04-1134,0,0.140093,"of FrameNet annotation, there have been a few projects that have studied transfer methods and evaluated the quality of the automatically produced corpus. Johansson and Nugues (2005) applied the wordbased methods of Yarowsky et al. (2001) and obtained promising results. Another recent effort (Padó and Lapata, 2005) demonstrates that deeper linguistic information, such as parse trees in the source and target language, is very beneficial for the process of FrameNet annotation transfer. A rather different method to construct bilingual semantic role annotation is the approach taken by BiFrameNet (Fung and Chen, 2004). In that work, 2.2 Transferring the Annotation We produced a Swedish-language corpus annotated with FrameNet information by applying the SRL system to the English side of Europarl (Koehn, 2005), which is a parallel corpus that is derived from the proceedings of the European Parliament. We projected the bracketing of the target words and the frame elements onto the Swedish side of the corpus by using the Giza++ word aligner (Och and Ney, 2003). Each word on the English side was mapped by the aligner onto a (possibly empty) set of words on the Swedish side. We used the maximal span method to in"
P06-2057,2005.mtsummit-papers.11,0,0.00744961,"ed methods of Yarowsky et al. (2001) and obtained promising results. Another recent effort (Padó and Lapata, 2005) demonstrates that deeper linguistic information, such as parse trees in the source and target language, is very beneficial for the process of FrameNet annotation transfer. A rather different method to construct bilingual semantic role annotation is the approach taken by BiFrameNet (Fung and Chen, 2004). In that work, 2.2 Transferring the Annotation We produced a Swedish-language corpus annotated with FrameNet information by applying the SRL system to the English side of Europarl (Koehn, 2005), which is a parallel corpus that is derived from the proceedings of the European Parliament. We projected the bracketing of the target words and the frame elements onto the Swedish side of the corpus by using the Giza++ word aligner (Och and Ney, 2003). Each word on the English side was mapped by the aligner onto a (possibly empty) set of words on the Swedish side. We used the maximal span method to infer the bracketing on the Swedish side, which means that the span of a projected entity was set to the range from the leftmost projected token to the rightmost. Figure 2 shows an example of this"
P06-2057,W04-0803,0,0.108376,"ments belongs to (“evokes”) the frame S TATEMENT. Two constituents that fill slots of the frame (S PEAKER and T OPIC) are annotated as well. 1 Introduction Semantic role labeling (SRL), the process of automatically identifying arguments of a predicate in a sentence and assigning them semantic roles, has received much attention during the recent years. SRL systems have been used in a number of projects in Information Extraction and Question Answering, and are believed to be applicable in other domains as well. Building SRL systems for English has been studied widely (Gildea and Jurafsky, 2002; Litkowski, 2004), inter alia. However, all these works rely on corpora that have been produced at the cost of a large effort by human annotators. For instance, the current FrameNet corpus (Baker et al., 1998) consists of 130,000 manually annotated sentences. For smaller languages such as Swedish, such corpora are not available. As usual in these cases, [both parties]SPEAKER agreed to make no further statements [on the matter]TOPIC . Figure 1: A sentence from the FrameNet example corpus. 436 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 436–443, c Sydney, July 2006. 2006 Association"
P06-2057,W04-2407,0,0.0249917,"Missing"
P06-2057,J03-1002,0,0.00162339,"rocess of FrameNet annotation transfer. A rather different method to construct bilingual semantic role annotation is the approach taken by BiFrameNet (Fung and Chen, 2004). In that work, 2.2 Transferring the Annotation We produced a Swedish-language corpus annotated with FrameNet information by applying the SRL system to the English side of Europarl (Koehn, 2005), which is a parallel corpus that is derived from the proceedings of the European Parliament. We projected the bracketing of the target words and the frame elements onto the Swedish side of the corpus by using the Giza++ word aligner (Och and Ney, 2003). Each word on the English side was mapped by the aligner onto a (possibly empty) set of words on the Swedish side. We used the maximal span method to infer the bracketing on the Swedish side, which means that the span of a projected entity was set to the range from the leftmost projected token to the rightmost. Figure 2 shows an example of this process. To make the brackets conform to the FrameNet annotation practices, we applied a small set of heuristics. The FrameNet conventions specify that linking words such as prepositions and subordinat437 [We]SPEAKER wanted to express [our perplexity a"
P06-2057,P05-1072,0,0.0776657,"nce we have no such resource to rely on, we are forced to accept that this problem introduces a certain amount of noise into the automatically annotated corpus. SUB PR DET ADV doktorn svarade på ett larmsamtal Figure 3: Example dependency parse tree. [ [ doktorn ] NG_nom ] Clause [ svarade ]VG_fin [ på] PP [ ett larmsamtal ]NG_nom Figure 4: Example shallow parse tree. 3.1 Frame Element Bracketing Methods We created two redundancy-based FE bracketing algorithms based on binary classification of chunks as starting or ending the FE. This is somewhat similar to the chunk-based system described by Pradhan et al. (2005a), which uses a segmentation strategy based on IOB2 bracketing. However, our system still exploits the dependency parse tree during classification. We first tried the conventional approach to the problem of FE bracketing: applying a parser to the sentence, and classifying each node in the parse tree as being an FE or not. We used a dependency parser since there is no constituent-based parser available for Swedish. This proved unsuccessful because the spans of the dependency subtrees frequently were incompatible with the spans defined by the FrameNet annotations. This was especially the case f"
P06-2057,P98-1013,0,0.398611,"g data for the system, we used an annotated corpus that we produced by transferring FrameNet annotation from the English side to the Swedish side in a parallel corpus. In addition, we describe two frame element bracketing algorithms that are suitable when no robust constituent parsers are available. We evaluated the system on a part of the FrameNet example corpus that we translated manually, and obtained an accuracy score of 0.75 on the classification of presegmented frame elements, and precision and recall scores of 0.67 and 0.47 for the complete task. 1.1 FrameNet: an Introduction FrameNet (Baker et al., 1998) is a lexical database that describes English words using Frame Semantics (Fillmore, 1976). In this framework, predicates (or in FrameNet terminology, target words) and their arguments are linked by means of semantic frames. A frame can intuitively be thought of as a template that defines a set of slots, frame elements (FEs), that represent parts of the conceptual structure and typically correspond to prototypical participants or properties. Figure 1 shows an example sentence annotated with FrameNet information. In this example, the target word statements belongs to (“evokes”) the frame S TATE"
P06-2057,H01-1035,0,0.0344077,"ds have been proposed to reduce the need for manual annotation. Many of these have relied on existing resources for English and a transfer method based on word alignment in a parallel corpus to automatically create an annotated corpus in a new language. Although these data are typically quite noisy, they have been used to train automatic systems. For the particular case of transfer of FrameNet annotation, there have been a few projects that have studied transfer methods and evaluated the quality of the automatically produced corpus. Johansson and Nugues (2005) applied the wordbased methods of Yarowsky et al. (2001) and obtained promising results. Another recent effort (Padó and Lapata, 2005) demonstrates that deeper linguistic information, such as parse trees in the source and target language, is very beneficial for the process of FrameNet annotation transfer. A rather different method to construct bilingual semantic role annotation is the approach taken by BiFrameNet (Fung and Chen, 2004). In that work, 2.2 Transferring the Annotation We produced a Swedish-language corpus annotated with FrameNet information by applying the SRL system to the English side of Europarl (Koehn, 2005), which is a parallel co"
P06-2057,J03-4003,0,\N,Missing
P06-2057,H05-1108,0,\N,Missing
P06-2057,C98-1013,0,\N,Missing
P06-2057,J02-3001,0,\N,Missing
S07-1048,W04-0814,0,0.0492404,"Missing"
S07-1048,W06-2920,0,0.0058074,"d a post-processing step to convert constituent trees into labeled dependency trees that were then used as input to a semantic role labeler. Pradhan et al. (2005) used a rule-based dependency parser, but the results were significantly worse than when using a constituent parser. This paper describes a system for frame-semantic structure extraction that is based on a dependency parser. The next section presents the dependency grammar that we rely on. We then give the details on the frame detection and disambiguation, the The last few years have seen an increasing interest in dependency parsing (Buchholz and Marsi, 2006) with significant improvements of the state of the art, and dependency treebanks are now available for a wide range of languages. The parsing algorithms are comparatively easy to implement and efficient: some of the algorithms parse sentences in linear time (Yamada and Matsumoto, 2003; Nivre et al., 2006). In the semantic structure extraction system, we used the Stanford part-of-speech tagger (Toutanova et al., 2003) to tag the training and test sentences and MaltParser, a statistical dependency parser (Nivre et al., 2006), to parse them. We trained the parser on the Penn Treebank (Marcus et a"
S07-1048,W07-2416,1,0.810098,"ges. The parsing algorithms are comparatively easy to implement and efficient: some of the algorithms parse sentences in linear time (Yamada and Matsumoto, 2003; Nivre et al., 2006). In the semantic structure extraction system, we used the Stanford part-of-speech tagger (Toutanova et al., 2003) to tag the training and test sentences and MaltParser, a statistical dependency parser (Nivre et al., 2006), to parse them. We trained the parser on the Penn Treebank (Marcus et al., 1993). The dependency trees used to train the parser were created from the constituent trees using a conversion program (Johansson and Nugues, 2007)1 . The converter handles most of the secondary edges in the Treebank and encodes those edges as (generally) nonprojective dependency arcs. Such information is available in the Penn Treebank in the form of empty categories and secondary edges, it is however not available in the output of traditional constituent parsers, although there have been some attempts to apply a post-processing step to predict it, see Ahn et al. (2004), inter alia. Figures 1 and 2 show a constituent tree from the Treebank and its corresponding dependency tree. Note that the secondary edge from the wh-trace to Why is con"
S07-1048,P03-1004,0,0.0122447,"motion verbs (which often exhibit complex patterns of polysemy): 137 lemmas could be added to the S ELF _ MOTION frame. Examples of frames with frequent errors are L EADERSHIP, which includes many insects (probably because the most frequent sense of queen in SemCor is the queen bee), and F OOD , which included many chemical substances as well as inedible plants and animals. 3.2 Frame Element Extraction None FE Argument classification Supp Asp Cop Exist Null 3.3 Named Entity Recognition In addition to the frame-semantic information, the SemEval task also scores named entities. We used YamCha (Kudo and Matsumoto, 2003) to detect named entities, and we trained it on the SemEval full-text training sets. Apart from the word and part of speech, we used suffixes up to length 5 as features. We think that results could be improved further by using an external NE tagger. 4 Results Following convention, we divided the FE extraction into two subtasks: argument identification and argument classification. We did not try to assign multiple labels to arguments. Figure 3 shows an overview. In addition to detecing the FEs, the argument identification classifier detects the dependency nodes that should be tagged on the laye"
S07-1048,J93-2004,0,0.0274117,"arsi, 2006) with significant improvements of the state of the art, and dependency treebanks are now available for a wide range of languages. The parsing algorithms are comparatively easy to implement and efficient: some of the algorithms parse sentences in linear time (Yamada and Matsumoto, 2003; Nivre et al., 2006). In the semantic structure extraction system, we used the Stanford part-of-speech tagger (Toutanova et al., 2003) to tag the training and test sentences and MaltParser, a statistical dependency parser (Nivre et al., 2006), to parse them. We trained the parser on the Penn Treebank (Marcus et al., 1993). The dependency trees used to train the parser were created from the constituent trees using a conversion program (Johansson and Nugues, 2007)1 . The converter handles most of the secondary edges in the Treebank and encodes those edges as (generally) nonprojective dependency arcs. Such information is available in the Penn Treebank in the form of empty categories and secondary edges, it is however not available in the output of traditional constituent parsers, although there have been some attempts to apply a post-processing step to predict it, see Ahn et al. (2004), inter alia. Figures 1 and"
S07-1048,nivre-etal-2006-maltparser,0,0.0360609,"frame-semantic structure extraction that is based on a dependency parser. The next section presents the dependency grammar that we rely on. We then give the details on the frame detection and disambiguation, the The last few years have seen an increasing interest in dependency parsing (Buchholz and Marsi, 2006) with significant improvements of the state of the art, and dependency treebanks are now available for a wide range of languages. The parsing algorithms are comparatively easy to implement and efficient: some of the algorithms parse sentences in linear time (Yamada and Matsumoto, 2003; Nivre et al., 2006). In the semantic structure extraction system, we used the Stanford part-of-speech tagger (Toutanova et al., 2003) to tag the training and test sentences and MaltParser, a statistical dependency parser (Nivre et al., 2006), to parse them. We trained the parser on the Penn Treebank (Marcus et al., 1993). The dependency trees used to train the parser were created from the constituent trees using a conversion program (Johansson and Nugues, 2007)1 . The converter handles most of the secondary edges in the Treebank and encodes those edges as (generally) nonprojective dependency arcs. Such informati"
S07-1048,P05-1072,0,0.0090728,"dition, some linguistic phenomena such as wh-movement and discontinuous structures are conveniently described using dependency syntax by allowing nonprojective dependency arcs. It has also been claimed that dependency syntax is easier to understand and to teach to people without a linguistic background. Despite these advantages, dependency syntax has relatively rarely been used in semantic structure extraction, with a few exceptions. Ahn et al. (2004) used a post-processing step to convert constituent trees into labeled dependency trees that were then used as input to a semantic role labeler. Pradhan et al. (2005) used a rule-based dependency parser, but the results were significantly worse than when using a constituent parser. This paper describes a system for frame-semantic structure extraction that is based on a dependency parser. The next section presents the dependency grammar that we rely on. We then give the details on the frame detection and disambiguation, the The last few years have seen an increasing interest in dependency parsing (Buchholz and Marsi, 2006) with significant improvements of the state of the art, and dependency treebanks are now available for a wide range of languages. The par"
S07-1048,N03-1033,0,0.0157634,"dency grammar that we rely on. We then give the details on the frame detection and disambiguation, the The last few years have seen an increasing interest in dependency parsing (Buchholz and Marsi, 2006) with significant improvements of the state of the art, and dependency treebanks are now available for a wide range of languages. The parsing algorithms are comparatively easy to implement and efficient: some of the algorithms parse sentences in linear time (Yamada and Matsumoto, 2003; Nivre et al., 2006). In the semantic structure extraction system, we used the Stanford part-of-speech tagger (Toutanova et al., 2003) to tag the training and test sentences and MaltParser, a statistical dependency parser (Nivre et al., 2006), to parse them. We trained the parser on the Penn Treebank (Marcus et al., 1993). The dependency trees used to train the parser were created from the constituent trees using a conversion program (Johansson and Nugues, 2007)1 . The converter handles most of the secondary edges in the Treebank and encodes those edges as (generally) nonprojective dependency arcs. Such information is available in the Penn Treebank in the form of empty categories and secondary edges, it is however not availa"
S07-1048,P05-1073,0,0.0725678,"Missing"
S07-1048,W04-3212,0,0.0425607,"h 5 as features. We think that results could be improved further by using an external NE tagger. 4 Results Following convention, we divided the FE extraction into two subtasks: argument identification and argument classification. We did not try to assign multiple labels to arguments. Figure 3 shows an overview. In addition to detecing the FEs, the argument identification classifier detects the dependency nodes that should be tagged on the layers other than the frame element layer: S UPP, C OP, N ULL, E XIST, and A SP. The A NT and R EL labels could be inserted using simple rules. Similarly to Xue and Palmer (2004), Argument identification verb targets only), the set of dependencies of the target, part of speech of the target node, path through the dependency tree from the target to the node, position (before, after, or on), word and part of speech for the head, word and part of speech for leftmost and rightmost descendent. In the path feature, we removed steps through verb chains and coordination. For instance, in the sentece I have seen and heard it, the path from heard to I is only SBJ↓ and to it OBJ↓. Path Self_mover etc The system was evaluated on three texts. Table 1 shows the results for frame de"
S07-1048,W03-3023,0,0.0345233,"paper describes a system for frame-semantic structure extraction that is based on a dependency parser. The next section presents the dependency grammar that we rely on. We then give the details on the frame detection and disambiguation, the The last few years have seen an increasing interest in dependency parsing (Buchholz and Marsi, 2006) with significant improvements of the state of the art, and dependency treebanks are now available for a wide range of languages. The parsing algorithms are comparatively easy to implement and efficient: some of the algorithms parse sentences in linear time (Yamada and Matsumoto, 2003; Nivre et al., 2006). In the semantic structure extraction system, we used the Stanford part-of-speech tagger (Toutanova et al., 2003) to tag the training and test sentences and MaltParser, a statistical dependency parser (Nivre et al., 2006), to parse them. We trained the parser on the Penn Treebank (Marcus et al., 1993). The dependency trees used to train the parser were created from the constituent trees using a conversion program (Johansson and Nugues, 2007)1 . The converter handles most of the secondary edges in the Treebank and encodes those edges as (generally) nonprojective dependency"
S15-1029,W09-1206,1,0.903548,"Missing"
S15-1029,C10-3009,1,0.940895,"Missing"
S15-1029,E06-1002,0,0.109136,"Missing"
S15-1029,W10-0907,0,0.0839533,"s in Swedish using named entities. We trained a semantic parser on the generated Swedish propositions and we report the results we obtained. Using the CoNLL 2009 evaluation script, we could reach the scores of 52.25 for labeled propositions and 62.44 for the unlabeled ones. We believe our approach can be applied to train semantic role labelers for other resource-scarce languages. 1 Introduction Semantic role labeling has become a key module for many language processing applications and its importance is growing in fields like question answering (Shen and Lapata, 2007), information extraction (Christensen et al., 2010), sentiment analysis (Johansson and Moschitti, 2011), and machine translation (Liu and Gildea, 2010; Wu et al., 2011). To build an unrestricted semantic role labeler, the first step is to develop a comprehensive proposition bank. However, building proposition banks is a costly enterprise and as a consequence of that, they only exist for a handful of languages such as English, Chinese, German, or Spanish. In this paper, we describe a technique to create proposition banks for new languages using distant supervision. Our approach builds on the transfer of semantic information through named entiti"
S15-1029,P10-1030,0,0.0348545,"al. (2009) describe a method for creating training data and relation classifiers without a hand-labeled corpus. The authors used Freebase and its binary relations between entities, such as (/location/location/contains, Belgium, Nijlen). They extracted entity pairs from the sentences of a text and matched them to those found in Freebase. Using the entity pairs, the relations, and the corresponding sentence text, they could train a relation extractor. Pad´o and Lapata (2009) used parallel corpora and constituent-based models to automatically project FrameNet annotations from English to German. Hoffmann et al. (2010) introduced Wikipedia infoboxes in relation extraction, where the authors trained a classifier to predict the infobox schema of an article prior to the extraction step. They used relation-specific lexicons created from a web crawl to train individual extractors for 5,025 relations and, rather than running all these extractors on every article and sentence, they first predicted the schema of an article and then executed the set of corresponding extractors. Early work in distant supervision assumed that an entity pair expresses a unique explicit relation type. Surdeanu et al. (2012) describe an"
S15-1029,P11-2018,0,0.0174535,"semantic parser on the generated Swedish propositions and we report the results we obtained. Using the CoNLL 2009 evaluation script, we could reach the scores of 52.25 for labeled propositions and 62.44 for the unlabeled ones. We believe our approach can be applied to train semantic role labelers for other resource-scarce languages. 1 Introduction Semantic role labeling has become a key module for many language processing applications and its importance is growing in fields like question answering (Shen and Lapata, 2007), information extraction (Christensen et al., 2010), sentiment analysis (Johansson and Moschitti, 2011), and machine translation (Liu and Gildea, 2010; Wu et al., 2011). To build an unrestricted semantic role labeler, the first step is to develop a comprehensive proposition bank. However, building proposition banks is a costly enterprise and as a consequence of that, they only exist for a handful of languages such as English, Chinese, German, or Spanish. In this paper, we describe a technique to create proposition banks for new languages using distant supervision. Our approach builds on the transfer of semantic information through named entities. Starting from an existing proposition bank, Prop"
S15-1029,W08-2123,1,0.901954,"Missing"
S15-1029,C10-1081,0,0.0152835,"d we report the results we obtained. Using the CoNLL 2009 evaluation script, we could reach the scores of 52.25 for labeled propositions and 62.44 for the unlabeled ones. We believe our approach can be applied to train semantic role labelers for other resource-scarce languages. 1 Introduction Semantic role labeling has become a key module for many language processing applications and its importance is growing in fields like question answering (Shen and Lapata, 2007), information extraction (Christensen et al., 2010), sentiment analysis (Johansson and Moschitti, 2011), and machine translation (Liu and Gildea, 2010; Wu et al., 2011). To build an unrestricted semantic role labeler, the first step is to develop a comprehensive proposition bank. However, building proposition banks is a costly enterprise and as a consequence of that, they only exist for a handful of languages such as English, Chinese, German, or Spanish. In this paper, we describe a technique to create proposition banks for new languages using distant supervision. Our approach builds on the transfer of semantic information through named entities. Starting from an existing proposition bank, PropBank in English (Palmer et al., 2005), and loos"
S15-1029,P09-1113,0,0.508995,"sed techniques have been applied to the extraction of such relations, they both suffer from drawbacks. Supervised learning relies on labor-intensive, hand-annotated corpora, while unsupervised approaches have lower precision and recall levels. Distant supervision is an alternative to these approaches that was introduced by Craven and Kumlien (1999). They used a knowledge base of existing biological relations, automatically identified sentences containing these relations, and trained a classifier to recognize the relations. Distant supervision has been successfully transferred to other fields. Mintz et al. (2009) describe a method for creating training data and relation classifiers without a hand-labeled corpus. The authors used Freebase and its binary relations between entities, such as (/location/location/contains, Belgium, Nijlen). They extracted entity pairs from the sentences of a text and matched them to those found in Freebase. Using the entity pairs, the relations, and the corresponding sentence text, they could train a relation extractor. Pad´o and Lapata (2009) used parallel corpora and constituent-based models to automatically project FrameNet annotations from English to German. Hoffmann et"
S15-1029,P81-1022,0,0.637046,"Missing"
S15-1029,J05-1004,0,0.0199779,"nslation (Liu and Gildea, 2010; Wu et al., 2011). To build an unrestricted semantic role labeler, the first step is to develop a comprehensive proposition bank. However, building proposition banks is a costly enterprise and as a consequence of that, they only exist for a handful of languages such as English, Chinese, German, or Spanish. In this paper, we describe a technique to create proposition banks for new languages using distant supervision. Our approach builds on the transfer of semantic information through named entities. Starting from an existing proposition bank, PropBank in English (Palmer et al., 2005), and loosely parallel corpora such as versions of Wikipedia in different languages, we carried out a mapping of the semantic propositions we extracted from English to syntactic structures in the target language. We parsed the English edition of Wikipedia up to the predicate–argument structures using a semantic role labeler (Bj¨orkelund et al., 2010a) and the Swedish Wikipedia using a dependency parser (Nivre et al., 2006). We extracted all the named entities we found in the propositions and we disambiguated them using the Wikidata nomenclature1 . Using recurring entities, we aligned sentences"
S15-1029,P11-1138,0,0.0111203,"62731 wikidata:Q1779457 wikidata:Q25411 Table 1: Entries in the detection dictionary, all related to the city of Helsingborg in Sweden, with their unique Wikidata Q-number and a short explanation in italics. 4 Named Entity Linking Named entity linking (or disambiguation) (NED) is the core step of distant supervision to anchor the parallel sentences and propositions. NED usually consists of two steps: first, extract the entity mentions, usually noun phrases, and if a mention corresponds to a proper noun – a named entity –, link it to a unique identifier. For the English part, we used Wikifier (Ratinov et al., 2011) to disambiguate entities. There was no similar disambiguator for Swedish and those described for English are not directly adaptable because they require resources that do not exist for this language. We created a disambiguator targeted to Swedish: NEDforia. NEDforia uses a Wikipedia dump as input and automatically collects a list of named entities from the corpus. It then extracts the links and contexts of these entities to build disambiguation models. Given an input text, NEDforia recognizes and disambiguates the named entities, and annotates them with their corresponding Wikidata number. 4."
S15-1029,Q13-1030,0,0.0232968,"Missing"
S15-1029,D07-1002,0,0.191725,"we extracted from English to syntactic structures in Swedish using named entities. We trained a semantic parser on the generated Swedish propositions and we report the results we obtained. Using the CoNLL 2009 evaluation script, we could reach the scores of 52.25 for labeled propositions and 62.44 for the unlabeled ones. We believe our approach can be applied to train semantic role labelers for other resource-scarce languages. 1 Introduction Semantic role labeling has become a key module for many language processing applications and its importance is growing in fields like question answering (Shen and Lapata, 2007), information extraction (Christensen et al., 2010), sentiment analysis (Johansson and Moschitti, 2011), and machine translation (Liu and Gildea, 2010; Wu et al., 2011). To build an unrestricted semantic role labeler, the first step is to develop a comprehensive proposition bank. However, building proposition banks is a costly enterprise and as a consequence of that, they only exist for a handful of languages such as English, Chinese, German, or Spanish. In this paper, we describe a technique to create proposition banks for new languages using distant supervision. Our approach builds on the tr"
S15-1029,D12-1042,0,0.0119395,"h to German. Hoffmann et al. (2010) introduced Wikipedia infoboxes in relation extraction, where the authors trained a classifier to predict the infobox schema of an article prior to the extraction step. They used relation-specific lexicons created from a web crawl to train individual extractors for 5,025 relations and, rather than running all these extractors on every article and sentence, they first predicted the schema of an article and then executed the set of corresponding extractors. Early work in distant supervision assumed that an entity pair expresses a unique explicit relation type. Surdeanu et al. (2012) describe an extended model, where each entity pair may link multiple instances to multiple relations. Ritter et al. 240 As far as we know, all the work on relation extraction focused on the detection of specific semantic relations between entities. In this paper, we describe an extension and a generalization of it that potentially covers all the relations tied to a predicate and results in the systematic extraction of the semantic propositions observed in a corpus. Similarly to Mintz et al. (2009), we used an external resource of relational facts and we matched the entity pairs in the relatio"
S15-1029,nivre-etal-2006-maltparser,0,\N,Missing
sundberg-etal-2012-visualizing,H05-1044,0,\N,Missing
sundberg-etal-2012-visualizing,N09-1037,0,\N,Missing
sundberg-etal-2012-visualizing,W09-1206,1,\N,Missing
sundberg-etal-2012-visualizing,P03-1054,0,\N,Missing
sundberg-etal-2012-visualizing,N10-1120,0,\N,Missing
sundberg-etal-2012-visualizing,D09-1062,0,\N,Missing
sundberg-etal-2012-visualizing,P10-1041,0,\N,Missing
W01-1301,W97-0810,0,0.0208427,"Missing"
W04-0908,P84-1106,0,0.396658,"Missing"
W04-0908,P98-1013,0,0.0377922,"Missing"
W04-0908,W01-1301,1,0.676517,"ideo images. The authors found that it could also be useful to reverse the process and generate synthetic video sequences from texts. The logic engine behind the text-to-scene converter (Arens et al., 2002) is based on the Discourse Representation Theory. The system is limited to the visualization of single vehicle maneuvers at an intersection as the one described in this two-sentence narrative: A car came from Kriegstrasse. It turned left at the intersection. The authors give no further details on the text corpus and no precise description of the results. 3 Carsim Carsim (Egges et al., 2001; Dupuy et al., 2001) is a program that analyzes texts describing car accidents and visualizes them in a 3D environment. It has been developed using real-world texts. The Carsim architecture is divided into two parts that communicate using a formal representation of Input Text Linguistic Component Formal Description Visualizer Component Output Animation Figure 1: The Carsim architecture. in i en gran. Passageraren, som var f o¨ dd -84, dog. F¨oraren som var 21 a˚ r gammal v˚ardas p˚a sjukhus med sva˚ ra skador. Polisen misst¨anker att bilen de f¨ardades i, en ny Saab, var stulen i Emmaboda och det ska under dagen"
W04-0908,J01-4004,0,0.00972728,"presented an architecture and a strategy based on information extraction and a symbolic visualization that enable to convert real texts into 3D scenes. We have obtained promising results that validate our approach. They show that the Carsim architecture is applicable to Swedish and other languages. As far as we know, Carsim is the only text-to-scene conversion system working on noninvented narratives. We are currently improving Carsim and we hope in future work to obtain better results in the resolution of coreferences. We are implementing and adapting algorithms such as the one described in (Soon et al., 2001) to handle this. We also intend to improve the visualizer to handle more complex scenes and animations. The current aim of the Carsim project is to visualize the content of a text as accurately as possible, with no external knowledge. In the future, we would like to integrate additional knowledge sources in order to make the visualization more realistic and understandable. Geographical and meteorological information systems are good examples of this, which could be helpful to improve the realism. Another topic, which has been prominent in our discussions with traffic safety experts, is how to"
W04-0908,C98-1013,0,\N,Missing
W04-0908,J02-3001,0,\N,Missing
W05-0209,J93-4006,0,0.107321,"Missing"
W05-0624,W04-0814,0,0.02376,"Missing"
W05-0624,W05-0620,0,0.0137414,"hat it is necessary to tune the parameters (typically C and γ). This makes it necessary to train repeatedly using cross-validation to find the best combination of parameter values. • It uses no C parameter, which reduces the need for cross-validation. • The decision function is adapted for probabilistic output. • Arbitrary basis functions can be used. Its significant drawback is that the training procedure relies heavily on dense linear algebra, and is thus difficult to scale up to large training sets and may be prone to numerical difficulties. For a description of the task and the data, see (Carreras and Màrquez, 2005). 2 Sparse Bayesian Learning and the Relevance Vector Machine The Sparse Bayesian method is described in detail in (Tipping, 2001). Like other generalized linear learning methods, the resulting binary classifier has the form m X signf (x) = sign αi fi (x) + b i=1 177 Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL), c pages 177–180, Ann Arbor, June 2005. 2005 Association for Computational Linguistics where the fi are basis functions. Training the model then consists of finding a suitable α = (b, α1 , . . . , αm ) given a data set (X, Y ). Analogous with the"
W05-0624,J02-3001,0,0.157719,"hm does not scale up to large training sets, training on the whole PropBank was infeasible. We instead trained the multiclass classifier on sections 15 – 18, and used an SVM for the soft-pruning classifier, which was then trained on the remaining sections. The excellent LIBSVM (Chang and Lin, 2001) package was used to train the SVM. The features used by the classifiers can be grouped into predicate and node features. Of the node features, we here pay most attention to the parse tree path features. 3.1 Predicate Features We used the following predicate features, all of which first appeared in (Gildea and Jurafsky, 2002). • Predicate lemma. • Subcategorization frame. • Voice. 3.2 Node Features • Head word and head POS. Like most previous work, we used the head rules of Collins to extract this feature. • Position. A binary feature that describes if the node is before or after the predicate token. • Phrase type (PT), that is the label of the constituent. • Named entity. Type of the first contained NE. • Governing category. As in (Gildea and Jurafsky, 2002), this was used to distinguish subjects from objects. For an NP, this is either S or VP. • Path features. (See next subsection.) For prepositional phrases, we"
W06-2930,I05-2044,0,0.0153777,"of actions. We measured the improvement over a best-first strategy incrementing values of N . We observed the largest difference between N = 1 and N = 2, then leveling off and we used the latter value. 4.2 Bidirectionality and Voting Tesnière (1966) classified languages as centrifuge (head to the left) and centripetal (head to the right) in a table (page 33 of his book) that nearly exactly fits corpus evidence from the CONLL data. Nivre’s parser is inherently left-right. This may not fit all the languages. Some dependencies may be easier to capture when proceeding from the reverse direction. Jin et al. (2005) is an example of it for Chinese, where the authors describe an adaptation of Nivre’s parser to bidirectionality. We trained the model and ran the algorithm in both directions (left to right and right to left). We used a voting strategy based on probability scores. Each link was assigned a probability score (simply by using the probability of the la or ra actions for each link). We then summed the probability scores of the links from all four trees. To construct a singlehead, rooted, and cycle-free tree, we finally applied the Chu-Liu/Edmonds optimization algorithm (Chu and Liu, 1965; Edmonds,"
W06-2930,afonso-etal-2002-floresta,0,0.0400617,"erences for the left and right dependencies (Table 7). Distance also degrades results but the slope is not as steep as with Swedish (Table 8). Prepositions are also the main source of errors (Table 9). 5.4 Acknowledgments This work was made possible because of the annotated corpora that were kindly provided to us: Arabic (Hajiˇc et al., 2004), Bulgarian (Simov et al., 2005; Simov and Osenova, 2003), Chinese (Chen et al., 2003), Czech (Böhmová et al., 2003), Danish (Kromann, 2003), Dutch (van der Beek et al., 2002), German (Brants et al., 2002), Japanese (Kawata and Bartels, 2000), Portuguese (Afonso et al., 2002), Slovene (Džeroski et al., 2006), Spanish (Civit Torruella and Martí Antonín, 2002), Swedish (Nilsson et al., 2005), and Turkish (Oflazer et al., 2003; Atalay et al., 2003). 209 Table 4: Precision and recall of binned HEAD direction. Swedish. Dir. Gold Cor. Syst. R P to_root 389 330 400 84.83 82.50 left 2745 2608 2759 95.01 94.53 right 1887 1739 1862 92.16 93.39 Table 5: Precision tance. Swedish. Dist. Gold to_root 389 1 2512 2 1107 3-6 803 7-... 210 and recall of binned HEAD disCor. 330 2262 989 652 141 Syst. 400 2363 1122 867 269 R 84.83 90.05 89.34 81.20 67.14 P 82.50 95.73 88.15 75.20 52."
W06-2930,W03-2405,0,0.0521699,"source of errors (Table 9). 5.4 Acknowledgments This work was made possible because of the annotated corpora that were kindly provided to us: Arabic (Hajiˇc et al., 2004), Bulgarian (Simov et al., 2005; Simov and Osenova, 2003), Chinese (Chen et al., 2003), Czech (Böhmová et al., 2003), Danish (Kromann, 2003), Dutch (van der Beek et al., 2002), German (Brants et al., 2002), Japanese (Kawata and Bartels, 2000), Portuguese (Afonso et al., 2002), Slovene (Džeroski et al., 2006), Spanish (Civit Torruella and Martí Antonín, 2002), Swedish (Nilsson et al., 2005), and Turkish (Oflazer et al., 2003; Atalay et al., 2003). 209 Table 4: Precision and recall of binned HEAD direction. Swedish. Dir. Gold Cor. Syst. R P to_root 389 330 400 84.83 82.50 left 2745 2608 2759 95.01 94.53 right 1887 1739 1862 92.16 93.39 Table 5: Precision tance. Swedish. Dist. Gold to_root 389 1 2512 2 1107 3-6 803 7-... 210 and recall of binned HEAD disCor. 330 2262 989 652 141 Syst. 400 2363 1122 867 269 R 84.83 90.05 89.34 81.20 67.14 P 82.50 95.73 88.15 75.20 52.42 Table 6: Focus words where most of the errors occur. Swedish. Word POS Any Head Dep Both till PR 48 20 45 17 i PR 42 25 34 17 på PR 39 22 32 15 med PR 28 11 25 8 för PR 2"
W06-2930,P05-1013,0,0.126713,"tionality. We trained the model and ran the algorithm in both directions (left to right and right to left). We used a voting strategy based on probability scores. Each link was assigned a probability score (simply by using the probability of the la or ra actions for each link). We then summed the probability scores of the links from all four trees. To construct a singlehead, rooted, and cycle-free tree, we finally applied the Chu-Liu/Edmonds optimization algorithm (Chu and Liu, 1965; Edmonds, 1967). 5 Analysis 5.1 Experimental Settings We trained the models on “projectivized” graphs following Nivre and Nilsson (2005) method. We used the complete annotated data for nine langagues. Due to time limitations, we could not complete the training for three languages, Chinese, Czech, and German. 5.2 Overview of the Results We parsed the 12 languages using exactly the same algorithms and parameters. We obtained an average score of 74.93 for the labeled arcs and of 80.39 for the unlabeled ones (resp. 74.98 and 80.80 for the languages where we could train the model using the complete annotated data sets). Table 3 shows the 208 results per language. As a possible explanation of the differences between languages, the t"
W06-2930,W03-3017,0,0.165768,"nd a dependent. We experimented two main additions to our implementation of Nivre’s parser: N best search and bidirectional parsing. We trained the parser in both left-right and right-left directions and we combined the results. To construct a single-head, rooted, and cycle-free tree, we applied the ChuLiu/Edmonds optimization algorithm. We ran the same algorithm with the same parameters on all the languages. 1. if arc(T OP, F IRST ) ∈ G, then ra; 2. else if arc(F IRST, T OP ) ∈ G, then la; 3. else if ∃k ∈ Stack, arc(F IRST, k) ∈ G or arc(k, F IRST ) ∈ G, then re; 4. else sh. 1 Nivre’s Parser Nivre (2003) proposed a dependency parser that creates a projective and acyclic graph. The parser is an extension to the shift–reduce algorithm. As with the regular shift–reduce, it uses a stack S and a list of input words W . However, instead of finding constituents, it builds a set of arcs G representing the graph of dependencies. Nivre’s parser uses two operations in addition to shift and reduce: left-arc and right-arc. Given a sequence of words, possibly annotated with their part Using the first sentence of the Swedish corpus as input (Table 1), this algorithm produces the sequence of 24 actions: sh,"
W06-2930,dzeroski-etal-2006-towards,0,0.0199432,"dependencies (Table 7). Distance also degrades results but the slope is not as steep as with Swedish (Table 8). Prepositions are also the main source of errors (Table 9). 5.4 Acknowledgments This work was made possible because of the annotated corpora that were kindly provided to us: Arabic (Hajiˇc et al., 2004), Bulgarian (Simov et al., 2005; Simov and Osenova, 2003), Chinese (Chen et al., 2003), Czech (Böhmová et al., 2003), Danish (Kromann, 2003), Dutch (van der Beek et al., 2002), German (Brants et al., 2002), Japanese (Kawata and Bartels, 2000), Portuguese (Afonso et al., 2002), Slovene (Džeroski et al., 2006), Spanish (Civit Torruella and Martí Antonín, 2002), Swedish (Nilsson et al., 2005), and Turkish (Oflazer et al., 2003; Atalay et al., 2003). 209 Table 4: Precision and recall of binned HEAD direction. Swedish. Dir. Gold Cor. Syst. R P to_root 389 330 400 84.83 82.50 left 2745 2608 2759 95.01 94.53 right 1887 1739 1862 92.16 93.39 Table 5: Precision tance. Swedish. Dist. Gold to_root 389 1 2512 2 1107 3-6 803 7-... 210 and recall of binned HEAD disCor. 330 2262 989 652 141 Syst. 400 2363 1122 867 269 R 84.83 90.05 89.34 81.20 67.14 P 82.50 95.73 88.15 75.20 52.42 Table 6: Focus words where mos"
W06-2930,W03-2403,0,0.06087,"Missing"
W07-2412,W05-0209,1,0.920163,"of texts in French as a foreign language written by 85 Swedish students with different levels of proficiency. It contains approximately 400 texts and 100,000 words. It also features a control group of 22 French native speakers. CEFLE was compiled throughout the academic year 2003/2004. During this period, each student wrote four or five texts in French at two months intervals. The aim of this study was to analyze the morphosyntactic development in written production. For the present study, we used a random selection of 317 texts from the CEFLE corpus, see Table 2. Direkt Profil Direkt Profil (Granfeldt et al., 2005; Granfeldt et al., 2006) is a morphosyntactic analyzer designed for French as a second language. The initial aim was to implement the grammatical features and constructions in Table 1. In the current version of the system, a few features are still lacking but there is also a great number of additional ones that were not present from the beginning. The system has been presented in some detail in previous papers and we only give a brief description of the main parts. Verb groups and noun groups represent the essential grammatical support of the profile classification. The majority of syntactic"
W07-2412,granfeldt-etal-2006-cefle,1,0.922683,"ary over time. The problems in this procedure are threefold: identify the relevant features, decide on cutoff points for the stages, and evaluate the degree of success of the model. The system addresses these three problems. It consists of a morphosyntactic analyzer called Direkt Profil and a machine-learning module connected to it. We first describe the usefulness and rationale behind its development. We then present the corpus we used to develop the analyzer. Finally, we present new and substantially improved results on training machine-learning classifiers compared to previous experiments (Granfeldt et al., 2006). We also introduce a method to select attributes in order to identify the most relevant grammatical features. 1 Introduction Since the beginning of systematic research in second language acquisition (SLA) in the 1970s, one line of investigation was to identify and analyze stages of development that learners pass through when acquiring a second or a foreign language. See Sharwood-Smith and Truscott (2005) for a rePierre Nugues Department of Computer science Lund Institute of Technology Box 118, S-221 00 Lund, Sweden Pierre.Nugues@cs.lth.se cent discussion. Stage identification can be applied t"
W07-2412,P05-1025,0,0.28626,"corpus we used to develop the analyzer. Finally, we present new and substantially improved results on training machine-learning classifiers compared to previous experiments (Granfeldt et al., 2006). We also introduce a method to select attributes in order to identify the most relevant grammatical features. 1 Introduction Since the beginning of systematic research in second language acquisition (SLA) in the 1970s, one line of investigation was to identify and analyze stages of development that learners pass through when acquiring a second or a foreign language. See Sharwood-Smith and Truscott (2005) for a rePierre Nugues Department of Computer science Lund Institute of Technology Box 118, S-221 00 Lund, Sweden Pierre.Nugues@cs.lth.se cent discussion. Stage identification can be applied to data from all linguistic levels, but it is perhaps most interesting for the development of morphology and syntax. Within SLA, the learner’s internal grammar is considered as its own system, an interlanguage grammar, that develops and restructures over time (Selinker, 1972). The objective of this research is to determine and model the growth of the learner’s grammar, where the identification of relevant"
W07-2416,P98-1013,0,0.0243236,"Missing"
W07-2416,J93-2004,0,0.0457351,"arguably more intuitive than constituent syntax when explaining linking, i.e. the realization of the semantic arguments of predicates as syntactic units. This may also have practical implications for “semantic parsers”, although this still remains to be seen in practice. As statistical parsing is becoming the norm, syntactically annotated data, and hence the annotation style they adopt, plays a central role. For English, no significant dependency treebank exists, although there have been some preliminary efforts to create one (Rambow et al., 2002). Instead, the constituentbased Penn Treebank (Marcus et al., 1993), which is the largest treebank for English and the most common training resource for constituent parsing of this language, has been used to train most of the datadriven dependency parsers reported in the literature. However, since it based on constituent structures, a conversion method must be applied that transforms its constituent trees into dependency graphs. The dependency trees produced by existing conversion methods (Magerman, 1994; Collins, 1999; Yamada and Matsumoto, 2003), which have been used by all recent papers on English dependency parsing, have been somewhat simplistic in view o"
W07-2416,H94-1020,0,0.050243,"Yamada and Matsumoto, 2003), which have been used by all recent papers on English dependency parsing, have been somewhat simplistic in view of original dependency treebanks such as the Danish Dependency Treebank (Trautner Kromann, 2003), in particular with respect to the set of edge labels and the treatment of complex long-distance linguistic relations such as wh-movement, topicalization, it-clefts, expletives, and gapping. However, this information is available in the Penn Treebank from version II when its syntactic representation was extended from bare bracketing to a much richer structure (Marcus et al., 1994), but with a few exceptions this has not yet been reflected by automatic parsers, neither constituent-based nor dependency-based. This article describes a new constituent-todependency conversion procedure that makes better use of the existing information in the Treebank. The Joakim Nivre, Heiki-Jaan Kaalep, Kadri Muischnek and Mare Koit (Eds.) NODALIDA 2007 Conference Proceedings, pp. 105–112 Richard Johansson and Pierre Nugues idea of the new conversion method is to make use of the extended structure of the recent versions of the Penn Treebank to derive a more “semantically useful” representa"
W07-2416,E06-1011,0,0.247839,"To quantify this, we trained and evaluated two statistical dependency 110 parsers on the new treebank. M ALT PARSER (Nivre et al., 2006) is based on a greedy parsing procedure that builds a parse tree incrementally while proceeding through the sentence one token at a time. By using a greedy strategy, a rich history-based feature set for the SVM classifier that selects the actions can be used. The parser produces projective trees only, but can handle nonprojectivity if a preprocessing step is used before training and a postprocessing step after parsing (“pseudoprojective parsing”). MSTPARSER (McDonald and Pereira, 2006) predicts a parse tree by maximizing a scoring function over the space of all possible parse trees. The scoring function is a weighted sum of features of single links or, if the “second-order” feature set is used, pairs of adjacent links. The parser can handle nonprojectivity, although the search then becomes NPhard and has to be approximated. Following convention, we trained the parsers on sections 2–21 of the WSJ part of the treebank. The training step took a few hours for M ALT PARSER using a 64-bit AMD processor running at 2.2 GHz and roughly two days for MSTPARSER using a 32-bit Intel pro"
W07-2416,nivre-etal-2006-maltparser,0,0.194887,"Missing"
W07-2416,rambow-etal-2002-dependency,0,0.0396604,"’ˇcuk, 1988). From a theoretical perspective, dependency syntax is arguably more intuitive than constituent syntax when explaining linking, i.e. the realization of the semantic arguments of predicates as syntactic units. This may also have practical implications for “semantic parsers”, although this still remains to be seen in practice. As statistical parsing is becoming the norm, syntactically annotated data, and hence the annotation style they adopt, plays a central role. For English, no significant dependency treebank exists, although there have been some preliminary efforts to create one (Rambow et al., 2002). Instead, the constituentbased Penn Treebank (Marcus et al., 1993), which is the largest treebank for English and the most common training resource for constituent parsing of this language, has been used to train most of the datadriven dependency parsers reported in the literature. However, since it based on constituent structures, a conversion method must be applied that transforms its constituent trees into dependency graphs. The dependency trees produced by existing conversion methods (Magerman, 1994; Collins, 1999; Yamada and Matsumoto, 2003), which have been used by all recent papers on"
W07-2416,W03-3023,0,0.913212,"ere have been some preliminary efforts to create one (Rambow et al., 2002). Instead, the constituentbased Penn Treebank (Marcus et al., 1993), which is the largest treebank for English and the most common training resource for constituent parsing of this language, has been used to train most of the datadriven dependency parsers reported in the literature. However, since it based on constituent structures, a conversion method must be applied that transforms its constituent trees into dependency graphs. The dependency trees produced by existing conversion methods (Magerman, 1994; Collins, 1999; Yamada and Matsumoto, 2003), which have been used by all recent papers on English dependency parsing, have been somewhat simplistic in view of original dependency treebanks such as the Danish Dependency Treebank (Trautner Kromann, 2003), in particular with respect to the set of edge labels and the treatment of complex long-distance linguistic relations such as wh-movement, topicalization, it-clefts, expletives, and gapping. However, this information is available in the Penn Treebank from version II when its syntactic representation was extended from bare bracketing to a much richer structure (Marcus et al., 1994), but w"
W07-2416,J03-4003,0,\N,Missing
W07-2416,C98-1013,0,\N,Missing
W08-2123,D07-1101,0,0.260049,"frequent trace labels. 3 Semantic Submodel Our semantic model consists of three parts: • A SRL classifier pipeline that generates a list of candidate predicate–argument structures. • A constraint system that filters the candidate list to enforce linguistic restrictions on the global configuration of arguments. • A global classifier that rescores the predicate– argument structures in the filtered candidate list. We used a C value of 0.01, and the number of iterations was 6. 2.1 Features and Search The feature function Ψ is a second-order edgefactored representation (McDonald and Pereira, 2006; Carreras, 2007). The second-order representation allows us to express features not only of head–dependent links, but also of siblings and children of the dependent. This feature set forces us to adopt the expensive search procedure by Carreras (2007), which extends Eisner’s span-based dynamic programming algorithm (1996) to allow second-order feature dependencies. Since the cost function ρ is based on the cost of single links, this procedure can also be used to find the maximizer of F (xi , yij ) + ρ(yi , yij ), which is needed at training time. The search was constrained to disallow multiple root links. 2.2"
W08-2123,C96-1058,0,0.0671199,"Missing"
W08-2123,E06-1011,0,0.0678867,"is task). Algorithm 1 shows pseudocode for the algorithm. Algorithm 1 The Online PA Algorithm input Training set T = {(xt , yt )}Tt=1 Number of iterations N Regularization parameter C Initialize w to zeros repeat N times for (xt , yt ) in T let y˜t = arg max “ y F (xt , y) + ρ(yt , y) ” ˜t )−F (x t ,yt )+ρ(yt ,y ˜t ) t ,y let τt = min C, F (xkΨ(x,y ˜t )k2 t )−Ψ(x,y w ← w + τt (Ψ(x, yt ) − Ψ(x, y˜t )) return waverage parsers that consider features of single links only, the Chu-Liu/Edmonds algorithm can be used instead. However, this algorithm cannot be generalized to the second-order setting – McDonald and Pereira (2006) proved that this problem is NPhard, and described an approximate greedy search algorithm. To simplify implementation, we instead opted for the pseudo-projective approach (Nivre and Nilsson, 2005), in which nonprojective links are lifted upwards in the tree to achieve projectivity, and special trace labels are used to enable recovery of the nonprojective links at parse time. The use of trace labels in the pseudo-projective transformation leads to a proliferation of edge label types: from 69 to 234 in the training set, many of which occur only once. Since the running time of our parser depends"
W08-2123,P05-1013,0,0.0648681,"zeros repeat N times for (xt , yt ) in T let y˜t = arg max “ y F (xt , y) + ρ(yt , y) ” ˜t )−F (x t ,yt )+ρ(yt ,y ˜t ) t ,y let τt = min C, F (xkΨ(x,y ˜t )k2 t )−Ψ(x,y w ← w + τt (Ψ(x, yt ) − Ψ(x, y˜t )) return waverage parsers that consider features of single links only, the Chu-Liu/Edmonds algorithm can be used instead. However, this algorithm cannot be generalized to the second-order setting – McDonald and Pereira (2006) proved that this problem is NPhard, and described an approximate greedy search algorithm. To simplify implementation, we instead opted for the pseudo-projective approach (Nivre and Nilsson, 2005), in which nonprojective links are lifted upwards in the tree to achieve projectivity, and special trace labels are used to enable recovery of the nonprojective links at parse time. The use of trace labels in the pseudo-projective transformation leads to a proliferation of edge label types: from 69 to 234 in the training set, many of which occur only once. Since the running time of our parser depends on the number of labels, we used only the 20 most frequent trace labels. 3 Semantic Submodel Our semantic model consists of three parts: • A SRL classifier pipeline that generates a list of candid"
W08-2123,W08-2121,1,0.167387,"Missing"
W08-2123,P05-1073,0,0.158705,"stic function (“softmax”). We carried out an initial experiment with a more complex joint feature representation, but failed to improve over the baseline. Time prevented us from exploring this direction conclusively. 5 Results The submitted results on the development and test corpora are presented in the upper part of Table 3. After the submission deadline, we corrected a bug in the predicate identification method. This resulted in improved results shown in the lower part. Corpus Development Test WSJ Test Brown Test WSJ + Brown Development Test WSJ Test Brown Test WSJ + Brown Global SRL Model Toutanova et al. (2005) have showed that a global model that scores the complete predicate– argument structure can lead to substantial performance gains. We therefore created a global SRL classifier using the following global features in addition to the features from the pipeline: C ORE A RGUMENT L ABEL S EQUENCE. The complete sequence of core argument labels. The sequence also includes the predicate and voice, for instance A0+break.01/Active+A1. M ISSING C ORE A RGUMENT L ABELS. The set of core argument labels declared in the PropBank/NomBank frame that are not present in the predicate–argument structure. Similarly"
W09-1206,burchardt-etal-2006-salsa,0,0.0137895,"Missing"
W09-1206,J02-3001,0,0.0810976,"en used a beam search to identify the arguments of each predicate and to label them, yielding a pool of candidate propositions. The second stage consists of a reranker that we applied to The pipeline of classifiers consists of a predicate disambiguation (PD) module, an argument identification module (AI), and an argument classification (AC) module. Aside from the lack of a predicate identification module, which was not needed, as predicates were given, this architecture is identical to the one adopted by recent systems (Surdeanu et al., 2008), as well as the general approach within the field (Gildea and Jurafsky, 2002; Toutanova et al., 2005). We build all the classifiers using the L2regularized linear logistic regression from the L IB L INEAR package (Fan et al., 2008). The package implementation makes models very fast to train and 43 Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 43–48, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics Local classifier pipeline Sense disambiguation greedy search Argument identification beam search Global model N candidates Argument labeling beam search Reranker Local features +"
W09-1206,W08-2123,1,0.632708,"Missing"
W09-1206,kawahara-etal-2002-construction,0,0.00995375,"dent.lth.se love hafdell@hotmail.com Pierre.Nugues@cs.lth.se Abstract the candidates using the local models and proposition features. We combined the score of the greedy classifiers and the reranker in a third stage to select the best candidate proposition. Figure 1 shows the system architecture. We evaluated our semantic parser on a set of seven languages provided by the organizers of the CoNLL2009 shared task: Catalan and Spanish (Taul´e et al., 2008), Chinese (Palmer and Xue, 2009), Czech (Hajiˇc et al., 2006), English (Surdeanu et al., 2008), German (Burchardt et al., 2006), and Japanese (Kawahara et al., 2002). Our system achieved an average labeled semantic F1 of 80.31, which corresponded to the second best semantic score overall. After the official evaluation was completed, we discovered a fault in the training procedure of the reranker for Spanish. The revised average labeled semantic F1 after correction was 80.80. This paper describes our contribution to the semantic role labeling task (SRL-only) of the CoNLL-2009 shared task in the closed challenge (Hajiˇc et al., 2009). Our system consists of a pipeline of independent, local classifiers that identify the predicate sense, the arguments of the"
W09-1206,W08-2121,0,0.0192041,"Missing"
W09-1206,taule-etal-2008-ancora,0,0.0543809,"Missing"
W09-1206,P05-1073,0,0.00993313,"Missing"
W09-3806,W06-2933,0,0.0239744,"Missing"
W09-3806,W03-3017,0,0.0213998,"emmatization. 4 Dependency Parsing Dependency syntax (Tesnière, 1966) has attracted a considerable interest in the recent years, spurred by the availability of data-driven parsers as well as annotated data in multiple languages including Arabic, Chinese, Czech, English, German, Japanese, Portuguese, or Spanish (Buchholz and Marsi, 2006; Nivre et al., 2007). We used this syntactic formalism because of its availability in many languages. 4.1 Features Parser Implementation There are two main classes of data-driven dependency parsers: graph-based (McDonald and Pereira, 2006) and transition-based (Nivre, 2003). We selected Nivre’s parser because of its implementation simplicity, small memory footprint, and linear time complexity. Parsing is always achieved in at most 2n − 1 actions, where n is the length of the sentence. Both types of parser can be combined, see Zhang and Clark (2008) for a discussion. Nivre’s parser is an extension to the shift– reduce algorithm that creates a projective and acyclic graph. It uses a stack, a list of input words, and builds a set of arcs representing the graph of dependencies. The parser uses two operations in addition to shift and reduce, left-arc and right-arc: •"
W09-3806,D08-1059,0,0.0554068,"German, Japanese, Portuguese, or Spanish (Buchholz and Marsi, 2006; Nivre et al., 2007). We used this syntactic formalism because of its availability in many languages. 4.1 Features Parser Implementation There are two main classes of data-driven dependency parsers: graph-based (McDonald and Pereira, 2006) and transition-based (Nivre, 2003). We selected Nivre’s parser because of its implementation simplicity, small memory footprint, and linear time complexity. Parsing is always achieved in at most 2n − 1 actions, where n is the length of the sentence. Both types of parser can be combined, see Zhang and Clark (2008) for a discussion. Nivre’s parser is an extension to the shift– reduce algorithm that creates a projective and acyclic graph. It uses a stack, a list of input words, and builds a set of arcs representing the graph of dependencies. The parser uses two operations in addition to shift and reduce, left-arc and right-arc: • A source: S for stack and Q for the queue; • An offset: 0 for the top of the stack and first in the queue; 1 and 2 for levels down in the stack or to the right in the queue; 40 Name Initialization Termination Lef tArc RightArc Reduce Shif t Action hnil, W, ∅i hS, nil, Ai hn|S, n"
W09-3806,E06-1011,0,\N,Missing
W09-3806,W03-2506,1,\N,Missing
W09-3806,W06-2920,0,\N,Missing
W09-3806,W03-2505,0,\N,Missing
W09-3806,D07-1123,1,\N,Missing
W09-3806,W06-2930,1,\N,Missing
W09-3806,W06-2928,0,\N,Missing
W09-3806,D07-1096,0,\N,Missing
W09-4621,W08-2123,1,0.840351,"typically employing WordNet senses, have been used in text classification, but have not resulted in any conclusive improvements. For a review of previous studies and results, see Mansuy and Hilderman (2006). plan.?? investment.?? Sense disambig. Chrysler plans new investment in Latin America plan.01 investment.01 Argument identification Chrysler plans new investment in Latin America plan.01 investment.01 Argument labeling Chrysler plans new investment in Latin America A0 A2 A0 A1 plan.01 investment.01 Figure 3: Example processed by the semantic pipeline. We used a freely available SRL system (Johansson and Nugues, 2008) to extract the predicate– argument structures1 . The system relies on a syntactic and a semantic subcomponent. The syntactic model is a bottom-up dependency parser and the semantic model uses global inference mechanisms on top of a pipeline of classifiers. The com1 144 Download site: nlp.cs.lth.se. Text Categorization Using Predicate–Argument Structures Semantic pipeline Syntactic dependency parsing Predicate identification Sense disambig. Argument identification Global semantic model Argument labeling Linguistic constraints Pred−arg reranking Syntactic−semantic reranking Figure 2: The archit"
W09-4621,W04-2705,0,0.0290335,"d to unrestricted text, at least business text, with a satisfying level of quality. Chrysler plans new investment in Latin America Predicate identification Chrysler plans new investment in Latin America Role semantics (Fillmore, 1968) is a formalism that abstracts over the bare syntactic representation by means of semantic roles like AGENT and PATIENT rather than grammatical functions such as subject and object. Figure 1 shows the first example sentence in Sect. 2.2 annotated with syntactic dependencies and role-semantic information according to the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) standard. The verb plan is a predicate defined in the PropBank lexicon, which lists its four possible core arguments: A0, planner, A1, the thing planned, A2, grounds for planning, and A3, beneficiary. Similarly, the noun investment is a NomBank predicate whose three possible core arguments are: A0, investor, A1, theme, and A2 purpose. In addition to the core arguments, predicates also accept optional adjuncts such as locations or times. For each predicate, PropBank and NomBank define a number of word senses, such as plan.01 and investment.01 in the example sentence. Features based on word sen"
W09-4621,J07-2002,0,0.0137263,"ate– argument tuples, subject–verb–object triples, and word-sense information – and we extended the document vectors with them. Predicate–argument structures are core constructs in most formalisms dealing with knowledge representation. They are equally prominent in linguistic theories of compositional semantic representation. In the simplest case, predicate– argument tuples can be approximated by subject– verb–object triples or subject–verb pairs and extracted from surface-syntactic dependency trees. SVO representations have been used in vectorspace approaches to a number of tasks (Lin, 1998; Padó and Lapata, 2007). In the widely publicized semantic web initiative, Berners-Lee et al. (2001) advocated their use as a natural way to describe the vast majority of the data processed by machines. They also correspond to binary relations in relation algebra on which we can apply a large number of mathematical properties. Nonetheless, as far as we know, strict SVO representations have never been used in automatic text categorization. Fürnkranz et al. (1998) proposed an approximated SVO representation that could increase the precision of some categorization experiments when combined with a low recall ranging fro"
W09-4621,J05-1004,0,0.0135289,"ic role labelers can now be applied to unrestricted text, at least business text, with a satisfying level of quality. Chrysler plans new investment in Latin America Predicate identification Chrysler plans new investment in Latin America Role semantics (Fillmore, 1968) is a formalism that abstracts over the bare syntactic representation by means of semantic roles like AGENT and PATIENT rather than grammatical functions such as subject and object. Figure 1 shows the first example sentence in Sect. 2.2 annotated with syntactic dependencies and role-semantic information according to the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) standard. The verb plan is a predicate defined in the PropBank lexicon, which lists its four possible core arguments: A0, planner, A1, the thing planned, A2, grounds for planning, and A3, beneficiary. Similarly, the noun investment is a NomBank predicate whose three possible core arguments are: A0, investor, A1, theme, and A2 purpose. In addition to the core arguments, predicates also accept optional adjuncts such as locations or times. For each predicate, PropBank and NomBank define a number of word senses, such as plan.01 and investment.01 in the example se"
W09-4621,W02-1001,0,0.0139503,"erage F1 while a low threshold results in a potentially large number of documents assigned to a wrong category, which has a negative impact on both the micro and the macroaverage F1 . To avoid this, the F1 score is calculated for each category in the training set. If the score is too low, the highest ranking is chosen as the threshold for that category. 4.3 Corpus Tagging and Parsing We annotated the RCV1 corpus with POS tags, dependency relations, and predicate argument structures using the SRL system mentioned in Sect. 3. The POS tagger uses techniques that are similar to those described by Collins (2002). 4.4 Feature Sets We conducted our experiments with three main sets of features. The first feature set is the baseline bag of words. The second one uses the triples consisting of the verb, subject, and object (VSO) for given predicates. The third set corresponds to predicates, their sense, and their most frequent core arguments: A0 and A1. We exemplify the features with the sentence Chrysler plans new investment in Latin America, whose syntactic and semantic graphs are shown in Figure 1. 145 Jacob Persson, Richard Johansson and Pierre Nugues As first feature set, we used the bags of words cor"
W09-4621,J02-3001,0,0.0166025,"ld the car to him. ROOT SBJ OBJ NMOD LOC PMOD NMOD Chrysler plans new investment in Latin America A0 A0 plan.01 A1 A2 investment.01 Figure 1: Example sentence with dependency syntax and role semantics annotation. Upper arrows correspond to the dependency relations and the lower ones to the semantic roles. 3 Automatic Semantic Role Labeling Role-semantic structures can be automatically extracted from free text – this task is referred to as semantic role labeling (SRL). Although early SRL systems (Hirst, 1983) used symbolic rules, modern systems to a large extent rely on statistical techniques (Gildea and Jurafsky, 2002). This has been made possible by the availability of training data, first from FrameNet (Ruppenhofer et al., 2006) and then PropBank and NomBank. Semantic role labelers can now be applied to unrestricted text, at least business text, with a satisfying level of quality. Chrysler plans new investment in Latin America Predicate identification Chrysler plans new investment in Latin America Role semantics (Fillmore, 1968) is a formalism that abstracts over the bare syntactic representation by means of semantic roles like AGENT and PATIENT rather than grammatical functions such as subject and object"
W09-4621,P83-1010,0,0.0429257,"planned by Chrysler, and diathesis alternations such as dative shifts, We sold him the car / We sold the car to him. ROOT SBJ OBJ NMOD LOC PMOD NMOD Chrysler plans new investment in Latin America A0 A0 plan.01 A1 A2 investment.01 Figure 1: Example sentence with dependency syntax and role semantics annotation. Upper arrows correspond to the dependency relations and the lower ones to the semantic roles. 3 Automatic Semantic Role Labeling Role-semantic structures can be automatically extracted from free text – this task is referred to as semantic role labeling (SRL). Although early SRL systems (Hirst, 1983) used symbolic rules, modern systems to a large extent rely on statistical techniques (Gildea and Jurafsky, 2002). This has been made possible by the availability of training data, first from FrameNet (Ruppenhofer et al., 2006) and then PropBank and NomBank. Semantic role labelers can now be applied to unrestricted text, at least business text, with a satisfying level of quality. Chrysler plans new investment in Latin America Predicate identification Chrysler plans new investment in Latin America Role semantics (Fillmore, 1968) is a formalism that abstracts over the bare syntactic representati"
W09-4621,W08-2121,1,0.769372,"Missing"
W09-4621,P98-2127,0,\N,Missing
W09-4621,C98-2122,0,\N,Missing
W11-1905,W07-2416,1,0.821912,"effective in numerous tasks of natural language processing such as partof-speech tagging or parsing. However, lexicalized models require a good deal of annotated data to avoid overfit. The data set used in the CoNLL 2011 System Architecture During both training and decoding, we employed the same mention detection and preprocessing steps. We considered all the noun phrases (NP) and possessive pronouns (PRP$) as mentions. In order to extract head words from the NP constituents, we converted the constituent trees provided in the data sets to dependency graphs using the Penn treebank converter of Johansson and Nugues (2007). Using the dependency tree, we extracted the head word of all the NPs by taking the word that dominates the subtree constructed from the NP. The dependency tree is also used later to extract features of mentions based on dependency tree paths, which is further described in Sec. 3. In the preprocessing step, we assigned a number and a gender to each mention. For the pronominal mentions, we used a manually compiled lists of pronouns, where we marked the number and gender. 45 Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 45–50, c Portland, Oreg"
W11-1905,P02-1014,0,0.659772,"Logistic regression is faster to train and allowed us to carry out an automated feature selection, which is further described in Sec. 3.4. In addition, the logistic classifiers enabled us to interpret their results in terms of probabilities, which we used for the decoding step. We trained the logistic regression classifiers using the LIBLINEAR package (Fan et al., 2008). 2.3 Decoding The decoding algorithm devised by Soon et al. (2001) selects the closest preceding mention deemed to be coreferent by the classifier. This clustering algorithm is commonly referred to as closest-first clustering. Ng and Cardie (2002) suggested a different clustering procedure, commonly referred to as best-first clustering. This algorithm selects the most likely antecedent classified as coreferent with the anaphoric mention. During early experiments, we found that while the best-first method increases 46 the performance on nonpronominal anaphoric expressions, it has the opposite effect on pronominal anaphoric expressions. Consequently, we settled on using the closest-first clustering method for pronominal mentions, and the best-first clustering method otherwise. For the best-first clustering, we used the probability output"
W11-1905,E06-2015,0,0.0179966,") built a statistical model from paths that include the lemma of the intermediate tokens, but replace the end nodes with noun, pronoun, or pronoun-self for nouns, pronouns, and reflexive pronouns, respectively. They used this model to define a measure of coreference likelihood to resolve pronouns within the same sentence. Rather than building an explicit model, we simply included these paths as features in our set. We refer to this feature template as B ERGSMA L IN PATH in Table 3. 3.3 Semantic role features We tried to exploit the semantic roles that were included in the CoNLL 2011 data set. Ponzetto and Strube (2006) suggested using the concatenation of the predicate and the role label for a mention that has a semantic role in a predicate. They introduced two new features, I S EMROLE and J S EMROLE, that correspond to the semantic roles filled by each of the mentions in a pair. We included these features in our pool of feature templates, but we could not see any contribution from them during the feature selection. We also introduced a number of feature templates that only applied to pairs of mentions that occur in the same semantic role proposition. These templates included the concatenation of the two la"
W11-1905,W11-1901,0,0.108896,"ted features using dependency tree paths and labels by converting the constituent trees provided in the shared task into dependency graphs. The final feature set was selected through an automated feature selection procedure using crossvalidation. In this paper, we describe a coreference solver based on the extensive use of lexical features and features extracted from dependency graphs of the sentences. The solver uses Soon et al. (2001)’s classical resolution algorithm based on a pairwise classification of the mentions. We applied this solver to the closed track of the CoNLL 2011 shared task (Pradhan et al., 2011). We carried out a systematic optimization of the feature set using cross-validation that led us to retain 24 features. Using this set, we reached a MUC score of 58.61 on the test set of the shared task. We analyzed the impact of the features on the development set and we show the importance of lexicalization as well as of properties related to dependency links in coreference resolution. 1 Pierre Nugues Lund University / LTH Lund / Sweden Pierre.Nugues@cs.lth.se 2 Introduction In this paper, we present our contribution to the closed track of the 2011 CoNLL shared task (Pradhan et al., 2011). W"
W11-1905,J01-4004,0,0.192054,"this size, we investigated the potential of lexicalized features. Besides lexical features, we created features that use part-of-speech tags and semantic roles. We also constructed features using dependency tree paths and labels by converting the constituent trees provided in the shared task into dependency graphs. The final feature set was selected through an automated feature selection procedure using crossvalidation. In this paper, we describe a coreference solver based on the extensive use of lexical features and features extracted from dependency graphs of the sentences. The solver uses Soon et al. (2001)’s classical resolution algorithm based on a pairwise classification of the mentions. We applied this solver to the closed track of the CoNLL 2011 shared task (Pradhan et al., 2011). We carried out a systematic optimization of the feature set using cross-validation that led us to retain 24 features. Using this set, we reached a MUC score of 58.61 on the test set of the shared task. We analyzed the impact of the features on the development set and we show the importance of lexicalization as well as of properties related to dependency links in coreference resolution. 1 Pierre Nugues Lund Univers"
W11-1905,P06-1005,0,\N,Missing
W12-4505,W11-1905,1,0.75046,"Missing"
W12-4505,P09-2056,0,0.0193729,"ure to incorporate in the classifier. That is 64 Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 64–70, c Jeju Island, Korea, July 13, 2012. 2012 Association for Computational Linguistics why, instead of writing sets of rules applicable to specific types of dependencies, we converted all the constituents in the three corpora to generic dependency graphs before starting the training and solving steps. We used the LTH converter (Johansson and Nugues, 2007) for English, the Penn2Malt converter (Nivre, 2006) with the Chinese rules for Chinese1 , and the CATiB converter (Habash and Roth, 2009) for Arabic. The CATiB converter (Habash and Roth, 2009) uses the Penn Arabic part-of-speech tagset, while the automatically tagged version of the CoNLL Arabic corpus uses a simplified tagset inspired by the English version of the Penn treebank. We translated these simplified POS tags to run the CATiB converter. We created a lookup table to map the simplified POS tags in the automatically annotated corpus to the Penn Arabic POS tags in the gold annotation. We took the most frequent association in the lookup table to carry out the translation. We then used the result to convert the constituents"
W12-4505,W07-2416,1,0.800453,"; see the CoNLL 2011 shared task (Pradhan et al., 2011), for instance. This can be tedious as we may have to write new rules for each new feature to incorporate in the classifier. That is 64 Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 64–70, c Jeju Island, Korea, July 13, 2012. 2012 Association for Computational Linguistics why, instead of writing sets of rules applicable to specific types of dependencies, we converted all the constituents in the three corpora to generic dependency graphs before starting the training and solving steps. We used the LTH converter (Johansson and Nugues, 2007) for English, the Penn2Malt converter (Nivre, 2006) with the Chinese rules for Chinese1 , and the CATiB converter (Habash and Roth, 2009) for Arabic. The CATiB converter (Habash and Roth, 2009) uses the Penn Arabic part-of-speech tagset, while the automatically tagged version of the CoNLL Arabic corpus uses a simplified tagset inspired by the English version of the Penn treebank. We translated these simplified POS tags to run the CATiB converter. We created a lookup table to map the simplified POS tags in the automatically annotated corpus to the Penn Arabic POS tags in the gold annotation. We"
W12-4505,P02-1014,0,0.0761122,"Missing"
W12-4505,W11-1901,0,0.172464,", and 49.43 on the test set, and the combined official score of 55.21. 1 System Architecture 3 Introduction In this paper, we present the LTH coreference solver used in the closed track of the CoNLL 2012 shared task (Pradhan et al., 2012). We started from an Converting Constituents to Dependency Trees Although the input to coreference solvers are pairs or sets of constituents, many systems use concepts from dependency grammars to decide if a pair is coreferent. The most frequent one is the constituent’s head that solvers need then to extract using ad-hoc rules; see the CoNLL 2011 shared task (Pradhan et al., 2011), for instance. This can be tedious as we may have to write new rules for each new feature to incorporate in the classifier. That is 64 Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 64–70, c Jeju Island, Korea, July 13, 2012. 2012 Association for Computational Linguistics why, instead of writing sets of rules applicable to specific types of dependencies, we converted all the constituents in the three corpora to generic dependency graphs before starting the training and solving steps. We used the LTH converter (Johansson and Nugues, 2007) for English, the Penn2Malt"
W12-4505,W12-4501,0,0.211986,"lth.se, Pierre.Nugues@cs.lth.se Abstract earlier version of the system by Bj¨orkelund and Nugues (2011), to which we added substantial improvements. As base learning and decoding algorithm, our solver extracts noun phrases and possessive pronouns and uses Soon et al. (2001)’s pairwise classifier to decide if a pair corefers or not. Similarly to the earlier LTH system, we constructed a primary feature set from properties extracted from the dependency graphs of the sentences. This paper describes the structure of the LTH coreference solver used in the closed track of the CoNLL 2012 shared task (Pradhan et al., 2012). The solver core is a mention classifier that uses Soon et al. (2001)’s algorithm and features extracted from the dependency graphs of the sentences. This system builds on Bj¨orkelund and Nugues (2011)’s solver that we extended so that it can be applied to the three languages of the task: English, Chinese, and Arabic. We designed a new mention detection module that removes pleonastic pronouns, prunes constituents, and recovers mentions when they do not match exactly a noun phrase. We carefully redesigned the features so that they reflect more complex linguistic phenomena as well as discourse"
W12-4505,J01-4004,0,\N,Missing
W15-2004,M93-1019,0,0.462419,"perties such his date of birth, 27 November 1942, and of death (P570): 18 September 1970. The relatively Do all the legendary musicians die at 27? 2 Previous Work The work we describe in this paper is an information extraction task using the Swedish version of Wikipedia, Wikipedia categories, and a rich semantically annotated data set: Wikidata. Information extraction systems are now ubiquitous and there are countless references that describe them. The Message Understanding Conferences (MUC) standardized their evaluation and the pipeline architecture eloquently advocated by the FASTUS system (Appelt et al., 1993; Hobbs et al., 1997) is still followed by scores of information extraction systems. Many information extraction systems use now Magnus Norrby and Pierre Nugues 2015. Extraction of lethal events from Wikipedia and a semantic repository. Proceedings of the workshop on Semantic resources and semantic annotation for Natural Language Processing and the Digital Humanities at NODALIDA 2015. NEALT Proceedings Series 27 / Linköping Electronic Conference Proceedings 112: 28–35. 28 “Avlidna” (Category:Deaths_by_year), for instance “1994 deaths”. The latter category allowed us to narrow the search from a"
W15-2004,nivre-etal-2006-maltparser,0,0.018317,"on (born 1983)”, etc. The added information is always contained within two parentheses and we extracted the name by splitting the string at the “(” character. To maintain a unique ID for each person when the names could be identical, we used the numeric page ID instead. 3 Dataset 5.1 5 Extraction from Text Parsing the Text We built a processing pipeline consisting of a partof-speech (POS) tagger and a dependency parser that we applied to the documents. We first tagged all the documents with the Stag¨ ger POS tagger for Swedish (Ostling, 2013). We then applied the MaltParser dependency parser (Nivre et al., 2006) on the tagged sentences. We saved the results in a CoNLL-like format consisting of the following columns: token counter (ID), form, lemma, coarse POS tag, POS tag, grammatical features, head, and dependency relation. In addition, Stagger output named entity tags that supplement the CoNLL columns. We chose the Swedish Wikipedia as initial dataset that we downloaded from the Wikipedia dump pages. The dump consists of a compressed XML tree that we parsed with the Bliki XML Parser (Bliki, 2014) to produce HTML pages. We then used Sweble’s Wikitext Parser (Dohrn and Riehle, 2013) to reduce the art"
W16-4410,C02-2020,0,0.0112732,"ion 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 72 Proceedings of the Open Knowledge Base and Question Answering (OKBQA) Workshop, pages 72–76, Osaka, Japan, December 11 2016. Figure 1: Left part: The first language versions of Madagascar in Wikidata. The languages appear in alphabetic order out of 195. Right part: Membership of Madagascar to ontology classes using the instance of property; three classes are listed out of five in German. Domínguez García et al. (2012) extracted hyponymy relations from the Wikipedia category system across languages. Chiao and Zweigenbaum (2002) used a set of documents in French and English collected with medical taxonomy terms to produce translational equivalents. Sproat et al. (2006) used English and Chinese stories from the Xinhua News agency to identify named entity transliterations. Finally, Smith et al. (2010) improved translation performance using sets of parallel sentences that they extracted from Wikipedia. Although these works carry out some kind of matching across languages, we could not find references on a systematic attempt to pair descriptions of an entity in multiple language versions. To the best of our knowledge, we"
W16-4410,rapp-etal-2012-identifying,0,0.070528,"Missing"
W16-4410,P14-2080,0,0.0346113,"Missing"
W16-4410,N10-1063,0,0.0164843,"n Wikidata. The languages appear in alphabetic order out of 195. Right part: Membership of Madagascar to ontology classes using the instance of property; three classes are listed out of five in German. Domínguez García et al. (2012) extracted hyponymy relations from the Wikipedia category system across languages. Chiao and Zweigenbaum (2002) used a set of documents in French and English collected with medical taxonomy terms to produce translational equivalents. Sproat et al. (2006) used English and Chinese stories from the Xinhua News agency to identify named entity transliterations. Finally, Smith et al. (2010) improved translation performance using sets of parallel sentences that they extracted from Wikipedia. Although these works carry out some kind of matching across languages, we could not find references on a systematic attempt to pair descriptions of an entity in multiple language versions. To the best of our knowledge, we are the first to propose and evaluate a method in this field. 3 Collecting the Corpus We used six dumps of Wikipedia in English, French, German, Russian, Spanish, and Swedish1 from which we extracted all the persons. We carried out this extraction using the instance of prope"
W16-4410,P06-1010,0,0.0378858,"wering (OKBQA) Workshop, pages 72–76, Osaka, Japan, December 11 2016. Figure 1: Left part: The first language versions of Madagascar in Wikidata. The languages appear in alphabetic order out of 195. Right part: Membership of Madagascar to ontology classes using the instance of property; three classes are listed out of five in German. Domínguez García et al. (2012) extracted hyponymy relations from the Wikipedia category system across languages. Chiao and Zweigenbaum (2002) used a set of documents in French and English collected with medical taxonomy terms to produce translational equivalents. Sproat et al. (2006) used English and Chinese stories from the Xinhua News agency to identify named entity transliterations. Finally, Smith et al. (2010) improved translation performance using sets of parallel sentences that they extracted from Wikipedia. Although these works carry out some kind of matching across languages, we could not find references on a systematic attempt to pair descriptions of an entity in multiple language versions. To the best of our knowledge, we are the first to propose and evaluate a method in this field. 3 Collecting the Corpus We used six dumps of Wikipedia in English, French, Germa"
W17-0206,ahrenberg-2010-alignment,0,0.0204828,"eb data in the XML format. Each language pair has alignment files to map the respective sentences in the different languages. We only used the text documents in this study and we removed unaligned sentences. Koehn (2005) evaluated the Europarl corpus using the BLEU metric (Papineni et al., 2002). High BLEU scores are preferable as they often result in better word alignments (Yarowsky et al., 2001). The BLEU values for Europarl ranged from 10.3 to 40.2, with the English-to-Swedish at 24.8 and English-to-German at 17.6, where 0 means no alignment and 100 means a perfect alignment. Additionally, Ahrenberg (2010) notes that the English-Swedish alignment of Europarl contains a high share of structurally complex relations, which makes word alignment more difficult. 5.2 Word Alignment To carry out the transfer of entity mentions, we aligned the sentences and the words of the parallel corpora, where English was the source language and Swedish and German, the target languages. Europarl aligns the documents and sentences using the Gale and Church algorithm. This introduces additional errors when aligning the words. Instead, we used the precomputed word alignments from the open parallel corpus, OPUS, where i"
W17-0206,2005.mtsummit-papers.11,0,0.40386,"as well as a script that serves as standard in the field. We carried out the evaluation for both Swedish and German with this CoNLL script. For Swedish, we used SUC-Core (Nilsson Bj¨orkenstam, 2013) as a test set, while for German, we used the T¨ubaD/Z corpus in the same manner as with SUC-Core (Henrich and Hinrichs, 2013; Henrich and Hinrichs, 2014). Approach 5 Overview 5.1 Our goal was to create a coreference solver for Swedish and German with no labelled data to train the model. Swedish has no corpora of sufficient Parallel Corpora Europarl As parallel corpora, we used the Europarl corpus (Koehn, 2005), consisting of protocols and articles 47 Figure 1 shows two examples of good and bad projections from Yarowsky et al. (2001). The figures describe two projection scenarios with varying levels of complexity from a source language on the top of the figures to a target language at the bottom. The solid lines correspond to word alignments while the dotted lines define the boundaries of their maximum span heuristic. Yarowsky et al. (2001) argue that even though individual word alignments are incorrect, a group of words corresponding to a noun phrase in the source language tends to be aligned with"
W17-0206,C10-2071,0,0.0257763,"nt of Europarl contains a high share of structurally complex relations, which makes word alignment more difficult. 5.2 Word Alignment To carry out the transfer of entity mentions, we aligned the sentences and the words of the parallel corpora, where English was the source language and Swedish and German, the target languages. Europarl aligns the documents and sentences using the Gale and Church algorithm. This introduces additional errors when aligning the words. Instead, we used the precomputed word alignments from the open parallel corpus, OPUS, where improper word alignments are mitigated (Lee et al., 2010; Tiedemann, 2012). The word alignments in OPUS use the phrase-based grow-diagfinal-and heuristic, which gave better results. Additionally, many of the challenges in aligning English to Swedish described by Ahrenberg (2010) appeared to be mitigated. 6 6.2 The maximum span heuristic uses no syntactic knowledge other than tokenization for the target language. We implemented a variation of the maximum span heuristic which utilizes syntactic knowledge of the target language. We selected the largest mention bounded by each maximum span instead of the maximum span itself. As result, the generated co"
W17-0206,P98-1012,0,0.437137,"s no prior knowledge about the test set. We also followed the shared tasks in only using machineannotated parses as input. The rationale for using gold mention boundaries is that they correspond to the use of an ideal method for mention identification, where the results are an upper bound for the solver as it does not consider singleton mentions (Pradhan et al., 2011). There are multiple metrics to evaluate coreference resolution. We used the CoNLL 2012 score as it consists of a single value (Pradhan et al., 2012). This score is the mean of three other metrics: MUC6 (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFE (Luo, 2005). We also report the values we obtained with CEAFM , and BLANC (Recasens and Hovy, 2011). 7.2 End-to-End Evaluation Test Sets Swedish: SUC-Core. The SUC-Core corpus (Nilsson Bj¨orkenstam, 2013) consists of 20,000 words and tokens in 10 documents with 2,758 coreferring mentions. The corpus is a subset of the SUC 2.0 corpus, annotated with noun phrase coreferential links (Gustafson-Capkov´a and Hartmann, 2006). The corpus is much too small to train a coreference solver, but it is more than sufficient to evaluate solvers trained on some different source material. As a prepa"
W17-0206,H05-1004,0,0.0342627,"et. We also followed the shared tasks in only using machineannotated parses as input. The rationale for using gold mention boundaries is that they correspond to the use of an ideal method for mention identification, where the results are an upper bound for the solver as it does not consider singleton mentions (Pradhan et al., 2011). There are multiple metrics to evaluate coreference resolution. We used the CoNLL 2012 score as it consists of a single value (Pradhan et al., 2012). This score is the mean of three other metrics: MUC6 (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFE (Luo, 2005). We also report the values we obtained with CEAFM , and BLANC (Recasens and Hovy, 2011). 7.2 End-to-End Evaluation Test Sets Swedish: SUC-Core. The SUC-Core corpus (Nilsson Bj¨orkenstam, 2013) consists of 20,000 words and tokens in 10 documents with 2,758 coreferring mentions. The corpus is a subset of the SUC 2.0 corpus, annotated with noun phrase coreferential links (Gustafson-Capkov´a and Hartmann, 2006). The corpus is much too small to train a coreference solver, but it is more than sufficient to evaluate solvers trained on some different source material. As a preparatory step to evaluate"
W17-0206,C10-3009,1,0.789285,"Missing"
W17-0206,P14-5010,0,0.00473557,"Missing"
W17-0206,P16-1061,0,0.0431381,"Missing"
W17-0206,P15-1138,0,0.449246,", such as simple heuristics or from the output of another model. Distant supervision will often yield models that perform less well than models using other forms of supervised learning (Yao et al., 2010). The advantage of distant supervision is that the training set does not need an initial annotation. Distant supervision covers a wide range of methods. In this paper, we used an annotation projection, where the output of a coreference resolver is transferred across a parallel corpus, from English to Swedish and English to German, and used as input for training a solver in the target language (Martins, 2015; Exner et al., 2015). 3 4.2 We used three language-dependent processing pipelines: Previous Work • We applied Stanford’s CoreNLP (Manning et al., 2014) to annotate the English part. We used the parts of speech, dependency graphs, and coreference chains; Parallel corpora have been used to transfer syntactic annotation. Hwa et al. (2005) is an example of this. In the case of coreference, Rahman and Ng (2012) used statistical machine translation to align words and sentences and transfer annotated data and other entities from one language to another. They collected a large corpus of text in Spani"
W17-0206,S15-1029,1,0.892809,"peech, dependency graphs, and coreference chains; Parallel corpora have been used to transfer syntactic annotation. Hwa et al. (2005) is an example of this. In the case of coreference, Rahman and Ng (2012) used statistical machine translation to align words and sentences and transfer annotated data and other entities from one language to another. They collected a large corpus of text in Spanish and Italian, translating each sentence using machine translation, applying a coreference solver on the generated text, and aligning the sentences using unsupervised machine translation methods. Martins (2015) developed a coreference solver for Spanish and Portuguese using distant supervision, where he transferred entity mentions from English to a target language using machinelearning techniques. In this paper, we describe a new projection method, where we use a parallel corpus similarly to Hwa et al. (2005) and, where we follow the methods and metrics described by Rahman and Ng (2012). We also reused the maximum span heuristic in Martins (2015) and the pruning of documents according to the ratio between correct and incorrect entity alignments. 4 4.1 Processing Pipelines • Mate Tools (Bj¨orkelund e"
W17-0206,P09-1113,0,0.0604824,"refer to the same entity and linking them in a body of text. The referring words and phrases are generally called mentions. Coreference resolution is instrumental in many language processing applications such as information extraction, the construction of knowledge graphs, text summarizing, question answering, etc. As most current high-performance coreference solvers use machine-learning techniques and su2 Distant Supervision Distant supervision is a form of supervised learning, though the term is sometimes used interchangeably with weak supervision and self training depending on the source (Mintz et al., 2009; Yao et al., 2010). The primary difference between distant supervision and supervised learning lies in the annotation procedure of the training data; 46 Proceedings of the 21st Nordic Conference of Computational Linguistics, pages 46–55, c Gothenburg, Sweden, 23-24 May 2017. 2017 Link¨oping University Electronic Press size to train a general coreference solver, whereas German has a large labelled corpus in the form of T¨uba D/Z (Henrich and Hinrichs, 2014). Although we could have trained a solver from the T¨uba D/Z dataset, we applied the same projection methods to German to determine if our"
W17-0206,P02-1040,0,0.0996302,"we primarily selected it for its simplicity, as well as its efficiency with coreference solvers for Spanish and Portuguese using distant supervision (Martins, 2015). from the EU parliament gathered from 1996 in 21 language pairs. Europarl is a large sentence-aligned unannotated corpus consisting of both text documents and web data in the XML format. Each language pair has alignment files to map the respective sentences in the different languages. We only used the text documents in this study and we removed unaligned sentences. Koehn (2005) evaluated the Europarl corpus using the BLEU metric (Papineni et al., 2002). High BLEU scores are preferable as they often result in better word alignments (Yarowsky et al., 2001). The BLEU values for Europarl ranged from 10.3 to 40.2, with the English-to-Swedish at 24.8 and English-to-German at 17.6, where 0 means no alignment and 100 means a perfect alignment. Additionally, Ahrenberg (2010) notes that the English-Swedish alignment of Europarl contains a high share of structurally complex relations, which makes word alignment more difficult. 5.2 Word Alignment To carry out the transfer of entity mentions, we aligned the sentences and the words of the parallel corpor"
W17-0206,D10-1099,0,0.182057,"ntity and linking them in a body of text. The referring words and phrases are generally called mentions. Coreference resolution is instrumental in many language processing applications such as information extraction, the construction of knowledge graphs, text summarizing, question answering, etc. As most current high-performance coreference solvers use machine-learning techniques and su2 Distant Supervision Distant supervision is a form of supervised learning, though the term is sometimes used interchangeably with weak supervision and self training depending on the source (Mintz et al., 2009; Yao et al., 2010). The primary difference between distant supervision and supervised learning lies in the annotation procedure of the training data; 46 Proceedings of the 21st Nordic Conference of Computational Linguistics, pages 46–55, c Gothenburg, Sweden, 23-24 May 2017. 2017 Link¨oping University Electronic Press size to train a general coreference solver, whereas German has a large labelled corpus in the form of T¨uba D/Z (Henrich and Hinrichs, 2014). Although we could have trained a solver from the T¨uba D/Z dataset, we applied the same projection methods to German to determine if our method would genera"
W17-0206,W11-1901,0,0.0917022,"allel corpus similarly to Hwa et al. (2005) and, where we follow the methods and metrics described by Rahman and Ng (2012). We also reused the maximum span heuristic in Martins (2015) and the pruning of documents according to the ratio between correct and incorrect entity alignments. 4 4.1 Processing Pipelines • Mate Tools (Bj¨orkelund et al., 2010) for German; ¨ • For Swedish, we used Stagger (Ostling, 2013) for the parts of speech and MaltParser for the dependencies (Nivre et al., 2007). 4.3 Evaluation As annotation and evaluation framework, we followed the CoNLL 2011 and 2012 shared tasks (Pradhan et al., 2011; Pradhan et al., 2012). These tasks evaluated coreference resolution systems for three languages: English, Arabic, and Chinese. To score the systems, they defined a set of metrics as well as a script that serves as standard in the field. We carried out the evaluation for both Swedish and German with this CoNLL script. For Swedish, we used SUC-Core (Nilsson Bj¨orkenstam, 2013) as a test set, while for German, we used the T¨ubaD/Z corpus in the same manner as with SUC-Core (Henrich and Hinrichs, 2013; Henrich and Hinrichs, 2014). Approach 5 Overview 5.1 Our goal was to create a coreference solv"
W17-0206,H01-1035,0,0.697854,"with this CoNLL script. For Swedish, we used SUC-Core (Nilsson Bj¨orkenstam, 2013) as a test set, while for German, we used the T¨ubaD/Z corpus in the same manner as with SUC-Core (Henrich and Hinrichs, 2013; Henrich and Hinrichs, 2014). Approach 5 Overview 5.1 Our goal was to create a coreference solver for Swedish and German with no labelled data to train the model. Swedish has no corpora of sufficient Parallel Corpora Europarl As parallel corpora, we used the Europarl corpus (Koehn, 2005), consisting of protocols and articles 47 Figure 1 shows two examples of good and bad projections from Yarowsky et al. (2001). The figures describe two projection scenarios with varying levels of complexity from a source language on the top of the figures to a target language at the bottom. The solid lines correspond to word alignments while the dotted lines define the boundaries of their maximum span heuristic. Yarowsky et al. (2001) argue that even though individual word alignments are incorrect, a group of words corresponding to a noun phrase in the source language tends to be aligned with another group in the target language. The largest span of aligned words from a noun phrase in the target language usually cor"
W17-0206,W12-4501,0,0.320523,"partment of Computer Science Lund, Sweden alexander@wallindevelopment.se Abstract pervised training (Clark and Manning, 2016), building solvers requires large amounts of texts, hand-annotated with coreference chains. Unfortunately, such corpora are expensive to produce and are far from being available for all the languages, including the Nordic languages. In the case of Swedish, there seems to be only one available corpus annotated with coreferences: SUC-Core (Nilsson Bj¨orkenstam, 2013), which consists of 20,000 words and 2,758 coreferring mentions. In comparison, the CoNLL 2012 shared task (Pradhan et al., 2012) uses a training set of more than a million word and 155,560 coreferring mentions for the English language alone. Although models trained on large corpora do not automatically result in better solver accuracies, the two orders of magnitude difference between the English CoNLL 2012 corpus and SUCCore has certainly consequences on the model quality for English. Pradhan et al. (2012) posited that larger and more consistent corpora as well as a standardized evaluation scenario would be a way to improve the results in coreference resolution. The same should apply to Swedish. Unfortunately, annotati"
W17-0206,N12-1090,0,0.311822,", where the output of a coreference resolver is transferred across a parallel corpus, from English to Swedish and English to German, and used as input for training a solver in the target language (Martins, 2015; Exner et al., 2015). 3 4.2 We used three language-dependent processing pipelines: Previous Work • We applied Stanford’s CoreNLP (Manning et al., 2014) to annotate the English part. We used the parts of speech, dependency graphs, and coreference chains; Parallel corpora have been used to transfer syntactic annotation. Hwa et al. (2005) is an example of this. In the case of coreference, Rahman and Ng (2012) used statistical machine translation to align words and sentences and transfer annotated data and other entities from one language to another. They collected a large corpus of text in Spanish and Italian, translating each sentence using machine translation, applying a coreference solver on the generated text, and aligning the sentences using unsupervised machine translation methods. Martins (2015) developed a coreference solver for Spanish and Portuguese using distant supervision, where he transferred entity mentions from English to a target language using machinelearning techniques. In this"
W17-0206,L16-1024,0,0.0502259,"Missing"
W17-0206,J01-4004,0,0.554294,"head has POS PM Otherwise Dependency head has POS PM but different grammatical case Dependency head has POS PM Otherwise The head word is och and has at least one child who is a mention Otherwise The head word is den Otherwise The head word is sj¨alv Otherwise NP No Yes Yes No Yes Yes Yes Yes Yes No Yes No Yes No Table 1: Hand-written rules for noun phrase identification for Swedish based on SUC-Core. The rules are ordered by precedence from top to bottom # 1 2 3 4 Algorithms 5 Generating a Training Set To solve coreference, we used a variation of the closest antecedent approach described in Soon et al. (2001). This approach models chains as a projected graph with mentions as vertices, where every mention has at most one antecedent and one anaphora. The modeling assumptions relaxes the complex relationship between coreferring mentions by only considering the relationship between a mention and its closest antecedent. The problem is framed as a binary classification problem, where the system only needs to decide 6 Description Remove words from the beginning or the end of the phrase if they have the POS tags ET, EF or VB. The first and last words closest to the mention head with the HP POS tag and all"
W17-0206,W12-4505,1,0.918452,"2005; Hall et al., 2009; Fan et al., 2008). 9.3 Rule Remove words from the start or the end of the phrase if they have the POS tags $. $( PROP KON. If there is a word with the POS tag VVPP after the head word the word prior to this word becomes the last word in the phrase. If there is a dependant word with the POS tag KON and its string equals und create additional mentions from the phrases left and right of this word. If there is a word with the POS tag APPRART after the head word the word prior to this word becomes the last word in the phrase. Features Swedish. As features, we used a subset Stamborg et al. (2012) and Soon et al. (2001). Table 5 shows the complete list. German. The feature set for German is described in Table 5. The primary difference between German and Swedish is the addition of gender classified names. We used the lists of names and job titles from IMS Hotcoref DE (R¨osiger and Kuhn, 2016) to train the German model. The morphological information from both CoreNLP and Mate Tools appeared to be limited when compared with Swedish, which is reflected in the feature set. German. The German EuroParl corpus consists of 8,446 documents. From these documents, we randomly selected a subset con"
W17-0206,tiedemann-2012-parallel,0,0.0101263,"tains a high share of structurally complex relations, which makes word alignment more difficult. 5.2 Word Alignment To carry out the transfer of entity mentions, we aligned the sentences and the words of the parallel corpora, where English was the source language and Swedish and German, the target languages. Europarl aligns the documents and sentences using the Gale and Church algorithm. This introduces additional errors when aligning the words. Instead, we used the precomputed word alignments from the open parallel corpus, OPUS, where improper word alignments are mitigated (Lee et al., 2010; Tiedemann, 2012). The word alignments in OPUS use the phrase-based grow-diagfinal-and heuristic, which gave better results. Additionally, many of the challenges in aligning English to Swedish described by Ahrenberg (2010) appeared to be mitigated. 6 6.2 The maximum span heuristic uses no syntactic knowledge other than tokenization for the target language. We implemented a variation of the maximum span heuristic which utilizes syntactic knowledge of the target language. We selected the largest mention bounded by each maximum span instead of the maximum span itself. As result, the generated corpus would only co"
W17-0206,M95-1005,0,0.619788,"boundaries, the solver has no prior knowledge about the test set. We also followed the shared tasks in only using machineannotated parses as input. The rationale for using gold mention boundaries is that they correspond to the use of an ideal method for mention identification, where the results are an upper bound for the solver as it does not consider singleton mentions (Pradhan et al., 2011). There are multiple metrics to evaluate coreference resolution. We used the CoNLL 2012 score as it consists of a single value (Pradhan et al., 2012). This score is the mean of three other metrics: MUC6 (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFE (Luo, 2005). We also report the values we obtained with CEAFM , and BLANC (Recasens and Hovy, 2011). 7.2 End-to-End Evaluation Test Sets Swedish: SUC-Core. The SUC-Core corpus (Nilsson Bj¨orkenstam, 2013) consists of 20,000 words and tokens in 10 documents with 2,758 coreferring mentions. The corpus is a subset of the SUC 2.0 corpus, annotated with noun phrase coreferential links (Gustafson-Capkov´a and Hartmann, 2006). The corpus is much too small to train a coreference solver, but it is more than sufficient to evaluate solvers trained on some differe"
W17-0211,C16-2016,1,0.858182,"Missing"
W17-0211,E06-1002,0,0.0465243,"mes, and Wikidata as a language-agnostic reference of entity identifiers. HERD extracts the mentions from text using a string matching engine and links them to entities with a combination of rules, PageRank, and feature vectors based on the Wikipedia categories. We evaluated HERD with the evaluation protocol of ERD’14 (Carmel et al., 2014) and we reached the competitive F1-score of 0.746 on the development set. HERD is designed to be multilingual and has versions in English, French, and Swedish. 1 2 Previous Work Entity linking has spurred a considerable amount of work over the last 10 years. Bunescu and Pasca (2006), Mihalcea and Csomai (2007), and Cucerzan (2007) used Wikipedia as a knowledge source and its articles to define the entities, its hyperlinks to find the mentions, and semantic knowledge from redirect pages and categories, to carry out disambiguation. Milne and Witten (2008) used the likelihood of an entity given a mention M, P(E|M), and a relatedness metric between two entities computed from the links to their corresponding pages to improve both recall and precision. Ferragina and Scaiella (2010) addressed shorter pieces of text with the idea to use a collective agreement between all the ent"
W17-0211,D07-1074,0,0.284914,"ty identifiers. HERD extracts the mentions from text using a string matching engine and links them to entities with a combination of rules, PageRank, and feature vectors based on the Wikipedia categories. We evaluated HERD with the evaluation protocol of ERD’14 (Carmel et al., 2014) and we reached the competitive F1-score of 0.746 on the development set. HERD is designed to be multilingual and has versions in English, French, and Swedish. 1 2 Previous Work Entity linking has spurred a considerable amount of work over the last 10 years. Bunescu and Pasca (2006), Mihalcea and Csomai (2007), and Cucerzan (2007) used Wikipedia as a knowledge source and its articles to define the entities, its hyperlinks to find the mentions, and semantic knowledge from redirect pages and categories, to carry out disambiguation. Milne and Witten (2008) used the likelihood of an entity given a mention M, P(E|M), and a relatedness metric between two entities computed from the links to their corresponding pages to improve both recall and precision. Ferragina and Scaiella (2010) addressed shorter pieces of text with the idea to use a collective agreement between all the entities. The Entity Recognition and Disambiguation"
W17-0211,W03-0419,0,0.304564,"Missing"
W17-0211,D11-1072,0,0.279119,"Missing"
W17-0227,heid-etal-2010-corpus,0,0.0266396,"sentence, a named entity, etc., delimited by ranges. These nodes are possibly connected by edges as in dependency graphs. The data structure used is similar to a property graph and Fig. 1 shows the conversion pipeline from the Wikimedia dumps to the abstract syntactic trees (AST) and Docforia layers. 3 Previous Work A few graph-based linguistic data models and serializations have structures that are similar to Docforia. They include HyGraphDB (Gleim et al., 2007), the Linguistic Framework Annotation (Ide and Suderman, 2014), Off-Road LAF (Lapponi et al., 2014), the D-SPIN Text Corpus Format (Heid et al., 2010), and the oft cited UIMA project (Ferrucci and Lally, 2004). Some tools also extend UIMA such as DKPro Core (Eckart de Castilho and Gurevych, 2014). In contrast to the UIMA project (Ferrucci and Lally, 2004), which also provides an infrastructure to represent unstructured documents, the Docforia library by itself does not define an equivalent analysis infrastructure or rich type system. Docforia’s main focus is data extraction and storage of informal heterogenous data, where the schema can change many times during a project. The primary motivation of Docforia was a faster adaptability in resea"
W17-0227,C16-2016,1,0.810079,"nguage versions, multiple tools and storage models. In addition, the application of a NLP pipeline to carry out the annotation (tokenization, POS tagging, dependency parsing, and so on) is a relatively costly operation that can take weeks on a single computer. Docforia is a multilayer document model to store formatting, lexical, syntactic, and semantic annotations on Wikipedia and other kinds of text and visualize them. To deliver results in a reasonable time, Docforia is compatible with cluster programming frameworks such as Spark or Hadoop. Using the Langforia language processing pipelines (Klang and Nugues, 2016a), we processed six language versions of Wikipedia: English, French, German, Spanish, Russian, and Swedish, up to semantic role labeling, depending on the NLP tools available for a given language. We stored the results in the document model. We designed an interactive visualization tool, part of Langforia, so that a user can select languages, documents, and linguistic layers and examine the annotation output. Introduction 2 In this paper, we describe Docforia, a multilayer document model and application programming interface (API) to store formatting, lexical, syntactic, and semantic annotati"
W17-0227,L16-1654,1,0.415958,"Missing"
W17-0227,lapponi-etal-2014-road,0,0.026219,"s, where a node represents a piece of data: A token, a sentence, a named entity, etc., delimited by ranges. These nodes are possibly connected by edges as in dependency graphs. The data structure used is similar to a property graph and Fig. 1 shows the conversion pipeline from the Wikimedia dumps to the abstract syntactic trees (AST) and Docforia layers. 3 Previous Work A few graph-based linguistic data models and serializations have structures that are similar to Docforia. They include HyGraphDB (Gleim et al., 2007), the Linguistic Framework Annotation (Ide and Suderman, 2014), Off-Road LAF (Lapponi et al., 2014), the D-SPIN Text Corpus Format (Heid et al., 2010), and the oft cited UIMA project (Ferrucci and Lally, 2004). Some tools also extend UIMA such as DKPro Core (Eckart de Castilho and Gurevych, 2014). In contrast to the UIMA project (Ferrucci and Lally, 2004), which also provides an infrastructure to represent unstructured documents, the Docforia library by itself does not define an equivalent analysis infrastructure or rich type system. Docforia’s main focus is data extraction and storage of informal heterogenous data, where the schema can change many times during a project. The primary motiva"
W17-0227,J93-2004,0,0.0596186,"edia: English, French, German, Spanish, Russian, and Swedish, up to semantic role labeling, depending on the NLP tools available for a given language. We stored the results in our document model and we created a visualization tool to inspect the annotation results. and Ponzetto, 2010), named entity linking (Mihalcea and Csomai, 2007), information extraction, or question answering (Ferrucci, 2012). Nonetheless, the Wikipedia size, where many language versions have now more that one million of articles makes it more difficult to handle than “classic” and older corpora such as the Penn treebank (Marcus et al., 1993). Processing the complete collection of Wikipedia articles, or a part of it, is a nontrivial task that requires dealing with multiple markup variants across the language versions, multiple tools and storage models. In addition, the application of a NLP pipeline to carry out the annotation (tokenization, POS tagging, dependency parsing, and so on) is a relatively costly operation that can take weeks on a single computer. Docforia is a multilayer document model to store formatting, lexical, syntactic, and semantic annotations on Wikipedia and other kinds of text and visualize them. To deliver re"
W17-0227,P10-1023,0,0.0663579,"Missing"
W17-0227,N10-1063,0,0.164326,"isualize easily the results of a processing pipeline. 1 The Document Model We created the Docforia multilayer document model library to store, query, and extract hypertextual information common to many NLP tasks such as part-of-speech tagging, coreference resolution, named entity recognition and linking, dependency parsing, semantic role labeling, etc., in Wikipedia is one of the largest freely available encyclopedic sources: It is comprehensive, multilingual, and continuously expanding. These unique properties make it a popular resource now used in scores of NLP projects such as translation (Smith et al., 2010), semantic networks (Navigli 226 Proceedings of the 21st Nordic Conference of Computational Linguistics, pages 226–230, c Gothenburg, Sweden, 23-24 May 2017. 2017 Link¨oping University Electronic Press DOM trees using jsoup1 . The second step extracts the original page structure, text styles, links, lists, and tables. We then resolve the links to unique Wikidata identifiers. These steps are common to all the language editions we process. Wikidata is central to the multilingual nature of Docforia. Wikidata is an entity database, which assigns unique identifiers across all the language editions"
W17-0227,E12-2021,0,0.0716037,"Missing"
W19-6148,W06-2920,0,0.0564608,"and white space separation for the data fields connected to the token followed by a double line separation to mark a sentence. This kind of format was used first in the CoNLL99 task on chunking (Osborne, 1999) and then on subsequent tasks. Its main merits are the ease of use with regards to writing parsers and its readability without documentation. Universal Dependencies (Nivre et al., 2019) is an example of a recent project for multilingual corpora using this format. It defines a variant called CoNLL-U, an adaption of the format used in CoNLL-X shared task on multilingual dependency parsing (Buchholz and Marsi, 2006). CoNLL-U includes field descriptions at the start of a document using hashtag (#) comments, adds subword support, and a field, if used, would allow for untokenization by including information about spacing between tokens. CoNLL-* formats are tightly connected to data used in the shared tasks. Variations of these plaintext formats in the wild have no real standard and are mostly ad-hoc development. The field separation is a practical aspect, which may vary: spaces or tabulations. Depending on the corpus, these are not interchangeable as the token field might include ordinary spaces as part of"
W19-6148,L16-1654,1,0.849181,"I) and FoLiA XML (van Gompel and Reynaert, 2013) are general purpose XML schema definitions focused on linguistic and text annotation. TEI and FoLiA provide extensive documentation and guidelines on how data should be represented in XML. Graph formats. From primarily hierarchical formats, the NLP Interchange Format (NIF) provides a graph-oriented way of connecting information which builds on existing standards such as LAF/GrAF, RFC 5147, and RDF. The main innovation in NIF is a standardized way of referring to text with offsets also known as a stand-off annotation. NIF is similar to WIKIPARQ (Klang and Nugues, 2016). 3 Docria Docria is a document model based on typed property hypergraphs. We designed it to solve scalability and tooling problems we faced with the automatic processing and annotation of Wikipedia. This corresponds notably to: • The lack of document models and storage solutions that could fit small and large corpora and that could be compatible with research practices; • The impossibility to use the same document model with potentially costly large-scale extraction algorithms on a cluster with a mapreduce computing framework such as Apache Spark. Motivation. These aspects were dominant in th"
W19-6148,pustylnikov-etal-2008-unified,0,0.0326999,"h the simplicity of Docria. 2 Related Work Linguistically annotated data have been stored in many different formats, often developed to solve practical problems. We can group prior work into three categories: Formats – the technical formats which are used to serialize the data; Document models – conceptual descriptions of how the data is connected, often mapped to concrete software implementations; Applications and tooling – user-facing applications for annotation, search, etc. in this section, we will focus on the low-level formats and libraries to parse and access the data contained within. Pustylnikov et al. (2008), in their work on unifying 11 treebanks, made a summary of formats typically used, which shows a dominance of XML variants and CoNLL-like formats. We examine some of them here. Tabular annotation. The tabular annotation in plain text is one of the simplest formats: One token per line and white space separation for the data fields connected to the token followed by a double line separation to mark a sentence. This kind of format was used first in the CoNLL99 task on chunking (Osborne, 1999) and then on subsequent tasks. Its main merits are the ease of use with regards to writing parsers and it"
