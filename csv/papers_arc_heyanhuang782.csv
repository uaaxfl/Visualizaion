2021.naacl-main.280,{I}nfo{XLM}: An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training,2021,-1,-1,9,1,4074,zewen chi,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"In this work, we present an information-theoretic framework that formulates cross-lingual language model pre-training as maximizing mutual information between multilingual-multi-granularity texts. The unified view helps us to better understand the existing methods for learning cross-lingual representations. More importantly, inspired by the framework, we propose a new pre-training task based on contrastive learning. Specifically, we regard a bilingual sentence pair as two views of the same meaning and encourage their encoded representations to be more similar than the negative examples. By leveraging both monolingual and parallel corpora, we jointly train the pretext tasks to improve the cross-lingual transferability of pre-trained models. Experimental results on several benchmarks show that our approach achieves considerably better performance. The code and pre-trained models are available at https://aka.ms/infoxlm."
2021.findings-acl.59,Prediction or Comparison: Toward Interpretable Qualitative Reasoning,2021,-1,-1,2,0,7648,mucheng ren,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.findings-acl.64,"Read, Listen, and See: Leveraging Multimodal Information Helps {C}hinese Spell Checking",2021,-1,-1,7,0,7663,hengda xu,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.findings-acl.66,Hashing based Efficient Inference for Image-Text Matching,2021,-1,-1,5,0,7665,rongcheng tu,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.emnlp-main.125,m{T}6: Multilingual Pretrained Text-to-Text Transformer with Translation Pairs,2021,-1,-1,7,1,4074,zewen chi,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Multilingual T5 pretrains a sequence-to-sequence model on massive monolingual texts, which has shown promising results on many cross-lingual tasks. In this paper, we improve multilingual text-to-text transfer Transformer with translation pairs (mT6). Specifically, we explore three cross-lingual text-to-text pre-training tasks, namely, machine translation, translation pair span corruption, and translation span corruption. In addition, we propose a partially non-autoregressive objective for text-to-text pre-training. We evaluate the methods on seven multilingual benchmark datasets, including sentence classification, named entity recognition, question answering, and abstractive summarization. Experimental results show that the proposed mT6 improves cross-lingual transferability over mT5."
2021.emnlp-main.260,Enlivening Redundant Heads in Multi-head Self-attention for Machine Translation,2021,-1,-1,2,0,9178,tianfu zhang,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Multi-head self-attention recently attracts enormous interest owing to its specialized functions, significant parallelizable computation, and flexible extensibility. However, very recent empirical studies show that some self-attention heads make little contribution and can be pruned as redundant heads. This work takes a novel perspective of identifying and then vitalizing redundant heads. We propose a redundant head enlivening (RHE) method to precisely identify redundant heads, and then vitalize their potential by learning syntactic relations and prior knowledge in the text without sacrificing the roles of important heads. Two novel syntax-enhanced attention (SEA) mechanisms: a dependency mask bias and a relative local-phrasal position bias, are introduced to revise self-attention distributions for syntactic enhancement in machine translation. The importance of individual heads is dynamically evaluated during the redundant heads identification, on which we apply SEA to vitalize redundant heads while maintaining the strength of important heads. Experimental results on widely adopted WMT14 and WMT16 English to German and English to Czech language machine translation validate the RHE effectiveness."
2021.acl-long.193,Comprehensive Study: How the Context Information of Different Granularity Affects Dialogue State Tracking?,2021,-1,-1,2,0,12978,puhai yang,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Dialogue state tracking (DST) plays a key role in task-oriented dialogue systems to monitor the user{'}s goal. In general, there are two strategies to track a dialogue state: predicting it from scratch and updating it from previous state. The scratch-based strategy obtains each slot value by inquiring all the dialogue history, and the previous-based strategy relies on the current turn dialogue to update the previous dialogue state. However, it is hard for the scratch-based strategy to correctly track short-dependency dialogue state because of noise; meanwhile, the previous-based strategy is not very useful for long-dependency dialogue state tracking. Obviously, it plays different roles for the context information of different granularity to track different kinds of dialogue states. Thus, in this paper, we will study and discuss how the context information of different granularity affects dialogue state tracking. First, we explore how greatly different granularities affect dialogue state tracking. Then, we further discuss how to combine multiple granularities for dialogue state tracking. Finally, we apply the findings about context granularity to few-shot learning scenario. Besides, we have publicly released all codes."
2021.acl-long.265,Improving Pretrained Cross-Lingual Language Models via Self-Labeled Word Alignment,2021,-1,-1,6,1,4074,zewen chi,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"The cross-lingual language models are typically pretrained with masked language modeling on multilingual text or parallel sentences. In this paper, we introduce denoising word alignment as a new cross-lingual pre-training task. Specifically, the model first self-label word alignments for parallel sentences. Then we randomly mask tokens in a bitext pair. Given a masked token, the model uses a pointer network to predict the aligned token in the other language. We alternately perform the above two steps in an expectation-maximization manner. Experimental results show that our method improves cross-lingual transferability on various datasets, especially on the token-level tasks, such as question answering, and structured prediction. Moreover, the model can serve as a pretrained word aligner, which achieves reasonably low error rate on the alignment benchmarks. The code and pretrained parameters are available at github.com/CZWin32768/XLM-Align."
2021.acl-long.538,Cross-Lingual Abstractive Summarization with Limited Parallel Resources,2021,-1,-1,3,0,13472,yu bai,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Parallel cross-lingual summarization data is scarce, requiring models to better use the limited available cross-lingual resources. Existing methods to do so often adopt sequence-to-sequence networks with multi-task frameworks. Such approaches apply multiple decoders, each of which is utilized for a specific task. However, these independent decoders share no parameters, hence fail to capture the relationships between the discrete phrases of summaries in different languages, breaking the connections in order to transfer the knowledge of the high-resource languages to low-resource languages. To bridge these connections, we propose a novel Multi-Task framework for Cross-Lingual Abstractive Summarization (MCLAS) in a low-resource setting. Employing one unified decoder to generate the sequential concatenation of monolingual and cross-lingual summaries, MCLAS makes the monolingual summarization task a prerequisite of the CLS task. In this way, the shared decoder learns interactions involving alignments and summary patterns across languages, which encourages attaining knowledge transfer. Experiments on two CLS datasets demonstrate that our model significantly outperforms three baseline models in both low-resource and full-dataset scenarios. Moreover, in-depth analysis on the generated summaries and attention heads verifies that interactions are learned well using MCLAS, which benefits the CLS task under limited parallel resources."
2020.nlpcovid19-2.34,{W}eibo-{COV}: A Large-Scale {COVID}-19 Social Media Dataset from {W}eibo,2020,8,0,2,0,16315,yong hu,Proceedings of the 1st Workshop on {NLP} for {COVID}-19 (Part 2) at {EMNLP} 2020,0,"With the rapid development of COVID-19 around the world, people are requested to maintain {``}social distance{''} and {``}stay at home{''}. In this scenario, extensive social interactions transfer to cyberspace, especially on social media platforms like Twitter and Sina Weibo. People generate posts to share information, express opinions and seek help during the pandemic outbreak, and these kinds of data on social media are valuable for studies to prevent COVID-19 transmissions, such as early warning and outbreaks detection. Therefore, in this paper, we release a novel and fine-grained large-scale COVID-19 social media dataset collected from Sina Weibo, named Weibo-COV, contains more than 40 million posts ranging from December 1, 2019 to April 30, 2020. Moreover, this dataset includes comprehensive information nuggets like post-level information, interactive information, location information, and repost network. We hope this dataset can promote studies of COVID-19 from multiple perspectives and enable better and rapid researches to suppress the spread of this pandemic."
2020.emnlp-main.548,Towards Interpretable Reasoning over Paragraph Effects in Situation,2020,-1,-1,4,0,7648,mucheng ren,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"We focus on the task of reasoning over paragraph effects in situation, which requires a model to understand the cause and effect described in a background paragraph, and apply the knowledge to a novel situation. Existing works ignore the complicated reasoning process and solve it with a one-step {``}black box{''} model. Inspired by human cognitive processes, in this paper we propose a sequential approach for this task which explicitly models each step of the reasoning process with neural network modules. In particular, five reasoning modules are designed and learned in an end-to-end manner, which leads to a more interpretable model. Experimental results on the ROPES dataset demonstrate the effectiveness and explainability of our proposed approach."
2020.aacl-main.2,Can Monolingual Pretrained Models Help Cross-Lingual Classification?,2020,-1,-1,5,1,4074,zewen chi,Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing,0,"Multilingual pretrained language models (such as multilingual BERT) have achieved impressive results for cross-lingual transfer. However, due to the constant model capacity, multilingual pre-training usually lags behind the monolingual competitors. In this work, we present two approaches to improve zero-shot cross-lingual classification, by transferring the knowledge from monolingual pretrained models to multilingual ones. Experimental results on two cross-lingual classification benchmarks show that our methods outperform vanilla multilingual fine-tuning."
P19-1276,Open Domain Event Extraction Using Neural Latent Variable Models,2019,0,2,2,0.952381,1114,xiao liu,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"We consider open domain event extraction, the task of extracting unconstraint types of events from news clusters. A novel latent variable neural model is constructed, which is scalable to very large corpus. A dataset is collected and manually annotated, with task-specific evaluation metrics being designed. Results show that the proposed unsupervised model gives better performance compared to the state-of-the-art method for event schema induction."
K19-1025,Improving Neural Machine Translation by Achieving Knowledge Transfer with Sentence Alignment Learning,2019,0,0,2,0,26317,xuewen shi,Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL),0,"Neural Machine Translation (NMT) optimized by Maximum Likelihood Estimation (MLE) lacks the guarantee of translation adequacy. To alleviate this problem, we propose an NMT approach that heightens the adequacy in machine translation by transferring the semantic knowledge learned from bilingual sentence alignment. Specifically, we first design a discriminator that learns to estimate sentence aligning score over translation candidates, and then the learned semantic knowledge is transfered to the NMT model under an adversarial learning framework. We also propose a gated self-attention based encoder for sentence embedding. Furthermore, an N-pair training loss is introduced in our framework to aid the discriminator in better capturing lexical evidence in translation candidates. Experimental results show that our proposed method outperforms baseline NMT models on Chinese-to-English and English-to-German translation tasks. Further analysis also indicates the detailed semantic knowledge transfered from the discriminator to the NMT model."
D19-1304,Concept Pointer Network for Abstractive Summarization,2019,0,4,3,0,26952,wenbo wang,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"A quality abstractive summary should not only copy salient source texts as summaries but should also tend to generate new conceptual words to express concrete details. Inspired by the popular pointer generator sequence-to-sequence model, this paper presents a concept pointer network for improving these aspects of abstractive summarization. The network leverages knowledge-based, context-aware conceptualizations to derive an extended set of candidate concepts. The model then points to the most appropriate choice using both the concept set and original source text. This joint approach generates abstractive summaries with higher-level semantic concepts. The training model is also optimized in a way that adapts to different data, which is based on a novel method of distant-supervised learning guided by reference summaries and testing set. Overall, the proposed approach provides statistically significant improvements over several state-of-the-art models on both the DUC-2004 and Gigaword datasets. A human evaluation of the model{'}s abstractive abilities also supports the quality of the summaries produced within this framework."
S18-1046,Zewen at {S}em{E}val-2018 Task 1: An Ensemble Model for Affect Prediction in Tweets,2018,0,0,2,1,4074,zewen chi,Proceedings of The 12th International Workshop on Semantic Evaluation,0,"This paper presents a method for Affect in Tweets, which is the task to automatically determine the intensity of emotions and intensity of sentiment of tweets. The term affect refers to emotion-related categories such as anger, fear, etc. Intensity of emo-tions need to be quantified into a real valued score in [0, 1]. We propose an en-semble system including four different deep learning methods which are CNN, Bidirectional LSTM (BLSTM), LSTM-CNN and a CNN-based Attention model (CA). Our system gets an average Pearson correlation score of 0.682 in the subtask EI-reg and an average Pearson correlation score of 0.784 in subtask V-reg, which ranks 17th among 48 systems in EI-reg and 19th among 38 systems in V-reg."
D18-1125,Genre Separation Network with Adversarial Training for Cross-genre Relation Extraction,2018,0,2,7,0,30483,ge shi,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"Relation Extraction suffers from dramatical performance decrease when training a model on one genre and directly applying it to a new genre, due to the distinct feature distributions. Previous studies address this problem by discovering a shared space across genres using manually crafted features, which requires great human effort. To effectively automate this process, we design a genre-separation network, which applies two encoders, one genre-independent and one genre-shared, to explicitly extract genre-specific and genre-agnostic features. Then we train a relation classifier using the genre-agnostic features on the source genre and directly apply to the target genre. Experiment results on three distinct genres of the ACE dataset show that our approach achieves up to 6.1{\%} absolute F1-score gain compared to previous methods. By incorporating a set of external linguistic features, our approach outperforms the state-of-the-art by 1.7{\%} absolute F1 gain. We make all programs of our model publicly available for research purpose"
D18-1156,Jointly Multiple Events Extraction via Attention-based Graph Information Aggregation,2018,26,2,3,0.952381,1114,xiao liu,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"Event extraction is of practical utility in natural language processing. In the real world, it is a common phenomenon that multiple events existing in the same sentence, where extracting them are more difficult than extracting a single event. Previous works on modeling the associations between events by sequential modeling methods suffer a lot from the low efficiency in capturing very long-range dependencies. In this paper, we propose a novel Jointly Multiple Events Extraction (JMEE) framework to jointly extract multiple event triggers and arguments by introducing syntactic shortcut arcs to enhance information flow and attention-based graph convolution networks to model graph information. The experiment results demonstrate that our proposed framework achieves competitive results compared with state-of-the-art methods."
C18-1172,Task-oriented Word Embedding for Text Classification,2018,0,3,2,0,7727,qian liu,Proceedings of the 27th International Conference on Computational Linguistics,0,"Distributed word representation plays a pivotal role in various natural language processing tasks. In spite of its success, most existing methods only consider contextual information, which is suboptimal when used in various tasks due to a lack of task-specific features. The rational word embeddings should have the ability to capture both the semantic features and task-specific features of words. In this paper, we propose a task-oriented word embedding method and apply it to the text classification task. With the function-aware component, our method regularizes the distribution of words to enable the embedding space to have a clear classification boundary. We evaluate our method using five text classification datasets. The experiment results show that our method significantly outperforms the state-of-the-art methods."
Y17-1021,A Parallel Recurrent Neural Network for Language Modeling with {POS} Tags,2017,19,3,2,0,31092,chao su,"Proceedings of the 31st Pacific Asia Conference on Language, Information and Computation",0,None
S17-2007,{BIT} at {S}em{E}val-2017 Task 1: Using Semantic Information Space to Evaluate Semantic Textual Similarity,2017,0,12,2,1,1906,hao wu,Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017),0,"This paper presents three systems for semantic textual similarity (STS) evaluation at SemEval-2017 STS task. One is an unsupervised system and the other two are supervised systems which simply employ the unsupervised one. All our systems mainly depend on the (SIS), which is constructed based on the semantic hierarchical taxonomy in WordNet, to compute non-overlapping information content (IC) of sentences. Our team ranked 2nd among 31 participating teams by the primary score of Pearson correlation coefficient (PCC) mean of 7 tracks and achieved the best performance on Track 1 (AR-AR) dataset."
S17-2036,{QLUT} at {S}em{E}val-2017 Task 2: Word Similarity Based on Word Embedding and Knowledge Base,2017,0,1,6,0,32245,fanqing meng,Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017),0,"This paper shows the details of our system submissions in the task 2 of SemEval 2017. We take part in the subtask 1 of this task, which is an English monolingual subtask. This task is designed to evaluate the semantic word similarity of two linguistic items. The results of runs are assessed by standard Pearson and Spearman correlation, contrast with official gold standard set. The best performance of our runs is 0.781 (Final). The techniques of our runs mainly make use of the word embeddings and the knowledge-based method. The results demonstrate that the combined method is effective for the computation of word similarity, while the word embeddings and the knowledge-based technique, respectively, needs more deeply improvement in details."
S16-1105,{BIT} at {S}em{E}val-2016 Task 1: Sentence Similarity Based on Alignments and Vector with the Weight of Information Content,2016,12,1,2,1,1906,hao wu,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,"This paper describes three unsupervised systems for determining the semantic similarity between two short texts or sentences submitted to the SemEval 2016 Task 1, all of which make use of only off-the-shelf software and data making them easy to replicate. Two systems achieved a similar Pearson correlation coefficient (0.64661 by simple vector, 0.65319 by word alignments). We include experiments on using our alignment based system on evaluation data from the 2014 and 2015 STS shared task. The results suggest that beyond the core similarity algorithm, other factors such as data preprocessing and use of domain-specific knowledge are also important to similarity prediction performance."
P16-1048,{CSE}: Conceptual Sentence Embeddings based on Attention Model,2016,31,20,2,0,34454,yashen wang,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Most sentence embedding models typically represent each sentence only using word surface, which makes these models indiscriminative for ubiquitous homonymy and polysemy. In order to enhance representation capability of sentence, we employ conceptualization model to assign associated concepts for each sentence in the text corpus, and then learn conceptual sentence embedding (CSE). Hence, this semantic representation is more expressive than some widely-used text representation models such as latent topic model, especially for short-text. Moreover, we further extend CSE models by utilizing a local attention-based model that select relevant words within the context to make more efficient prediction. In the experiments, we evaluate the CSE models on two tasks, text classification and information retrieval. The experimental results show that the proposed models outperform typical sentence embed-ding models."
C16-1315,A Novel Fast Framework for Topic Labeling Based on Similarity-preserved Hashing,2016,21,0,6,0,4080,xianling mao,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Recently, topic modeling has been widely applied in data mining due to its powerful ability. A common, major challenge in applying such topic models to other tasks is to accurately interpret the meaning of each topic. Topic labeling, as a major interpreting method, has attracted significant attention recently. However, most of previous works only focus on the effectiveness of topic labeling, and less attention has been paid to quickly creating good topic descriptors; meanwhile, it{'}s hard to assign labels for new emerging topics by using most of existing methods. To solve the problems above, in this paper, we propose a novel fast topic labeling framework that casts the labeling problem as a k-nearest neighbor (KNN) search problem in a probability vector set. Our experimental results show that the proposed sequential interleaving method based on locality sensitive hashing (LSH) technology is efficient in boosting the comparison speed among probability distributions, and the proposed framework can generate meaningful labels to interpret topics, including new emerging topics."
W15-3124,Topic-Based {C}hinese Message Polarity Classification System at {SIGHAN}8-Task2,2015,15,0,4,0,36842,chun liao,Proceedings of the Eighth {SIGHAN} Workshop on {C}hinese Language Processing,0,"This paper describes the topic-based Chinese message polarity classification system submitted by LCYS_TEAM at SIGHAN8-Task2. The system mainly includes two parts: 1) a graph-based ranking model integrating local and global information is adopted to represent the classification ability of words towards different topics. In construction of graph model, a new weighting approach and a PMI-based random jumping probability selection method is proposed. 2) For sentimental features, word embedding is employed for acquiring expanded topical words and syntactic dependency is adopted for getting topic-related sentimental words. Experiment results demonstrate the effectiveness of our system."
W14-6828,Introduction to {BIT} {C}hinese Spelling Correction System at {CLP} 2014 Bake-off,2014,7,2,3,0,38138,min liu,Proceedings of The Third {CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,"This paper describes the Chinese spelling correction system submitted by BIT at CLP Bake-off 2014 task 2. The system mainly includes two parts: 1) N-gram model is adopted to retrieve the non-words which are wrongly separated by word segmentation. The non-words are then corrected in terms of word frequency, pronunciation similarity, shape similarity and POS (part of speech) tag. 2) For wrong words, abnormal POS tag is used to indicate their location and dependency relation matching is employed to correct them. Experiment results demonstrate the effectiveness of our system."
Y12-1030,Emotional Tendency Identification for Micro-blog Topics Based on Multiple Characteristics,2012,8,12,3,0,41964,quanchao liu,"Proceedings of the 26th Pacific Asia Conference on Language, Information, and Computation",0,"Public opinion analysis for micro-blog post is a new trend, and wherein emotional tendency analysis on micro-blog topic is a hot spot in the sentiment analysis. According to the characteristics of contents and the various relations of Chinese micro-blog post, we construct the dictionaries of sentiment words, internet slang and emoticons respectively, and then implement the sentiment analysis algorithms based on phrase path and the multiple characteristics for emotional tendency of micro-blog topics. Using micro-blogsxe2x80x99 forwarding, commentaries, sharing and so on, We take a future step to optimize the algorithm based on the multiple characteristics. According to the experimental results, our approach greatly improves the performance of emotional tendency identification on micro-blog topic."
C12-2137,{C}hinese Word Sense Disambiguation based on Context Expansion,2012,9,2,2,0,43726,zhizhuo yang,Proceedings of {COLING} 2012: Posters,0,"Word Sense Disambiguation (WSD) is one of the key issues in natural language processing. Currently, supervised WSD methods are effective ways to solve the ambiguity problem. However, due to lacking of large-scale training data, they cannot achieve satisfactory results. In this paper, we suppose synonyms for context words that can provide more knowledge for WSD task, and present two different WSD methods based on context expansion. The first method regards Synonyms as topic contextual feature to train Bayesian model. The second method treats context words made up of synonyms as pseudo training data, and then derives the meaning of ambiguous words using the knowledge from both training and pseudo training data. Experimental results show that the second method can significantly improve traditional WSD accuracy by 2.21%. Furthermore, it also outperforms the best system in SemEval-2007."
Y11-1035,Unsupervised Word Sense Disambiguation Using Neighborhood Knowledge,2011,14,2,1,1,4081,heyan huang,"Proceedings of the 25th Pacific Asia Conference on Language, Information and Computation",0,None
Y11-1049,An {E}nglish-{C}hinese Cross-lingual Word Semantic Similarity Measure Exploring Attributes and Relations,2011,17,4,2,0,43962,lin dai,"Proceedings of the 25th Pacific Asia Conference on Language, Information and Computation",0,"Word semantic similarity measuring is a fundamental issue to many NLP applications and the globalization has made an urgent request for cross-lingual word similarity measure. This paper proposed a word semantic similarity measure which is able to work in cross-lingual scenarios. Basically, a concept can be defined by a set of attributes. The basic idea of this work is to compute the similarity between words by exploring their attributes and relations. For a given word pair, we first compute similarities between their attributes by combining distance, depth and relation information. Then word similarity are computed through a combination scheme. The algorithm is implemented based on an English-Chinese bilingual ontology HowNet. Experiments show that the proposed algorithm results in high correlation against human judgments, which encourages its broad application in cross-lingual applications."
W10-4134,Incorporating New Words Detection with {C}hinese Word Segmentation,2010,1,1,4,0,45169,huaping zhang,{CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,None
W10-4157,{C}hinese Personal Name Disambiguation Based on Person Modeling,2010,21,4,4,0,45169,huaping zhang,{CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,None
2008.amta-govandcom.26,Applications of {MT} during Olympic Games 2008,2008,-1,-1,2,0,6630,chengqing zong,Proceedings of the 8th Conference of the Association for Machine Translation in the Americas: Government and Commercial Uses of MT,0,None
Y06-1071,Translation {\\&} Transform Algorithm of Query Sentence in Cross-Language Information Retrieval,2006,3,0,3,0,49562,xiaofei zhang,"Proceedings of the 20th Pacific Asia Conference on Language, Information and Computation",0,"Based on large-scale bilingual corpora and the theories of vector space model and lexical mutual information, this paper explores the application of the traditional monolingual IR technology to converting the translation of query sentence to the computation of the boost value of query keyword translations in the bilingual dictionary, so that the target language query sentence is reconstructed. The experiment finds a 92.8% precision rate in the first 10 retrieved documents and an 88.9% precision rate in the first 100 retrieved documents."
