2021.eacl-main.220,{S}truct{S}um: Summarization via Structured Representations,2021,-1,-1,5,0,4383,vidhisha balachandran,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"Abstractive text summarization aims at compressing the information of a long source document into a rephrased, condensed summary. Despite advances in modeling techniques, abstractive summarization models still suffer from several key challenges: (i) layout bias: they overfit to the style of training corpora; (ii) limited abstractiveness: they are optimized to copying n-grams from the source rather than generating novel abstractive summaries; (iii) lack of transparency: they are not interpretable. In this work, we propose a framework based on document-level structure induction for summarization to address these challenges. To this end, we propose incorporating latent and explicit dependencies across sentences in the source document into end-to-end single-document summarization models. Our framework complements standard encoder-decoder summarization models by augmenting them with rich structure-aware document representations based on implicitly learned (latent) structures and externally-derived linguistic (explicit) structures. We show that our summarization framework, trained on the CNN/DM dataset, improves the coverage of content in the source documents, generates more abstractive summaries by generating more novel n-grams, and incorporates interpretable sentence-level structures, while performing on par with standard baselines."
2020.tacl-1.8,Improving Candidate Generation for Low-resource Cross-lingual Entity Linking,2020,42,0,4,0,14396,shuyan zhou,Transactions of the Association for Computational Linguistics,0,"Cross-lingual entity linking (XEL) is the task of finding referents in a target-language knowledge base (KB) for mentions extracted from source-language texts. The first step of (X)EL is candidate generation, which retrieves a list of plausible candidate entities from the target-language KB for each mention. Approaches based on resources from Wikipedia have proven successful in the realm of relatively high-resource languages, but these do not extend well to low-resource languages with few, if any, Wikipedia pages. Recently, transfer learning methods have been shown to reduce the demand for resources in the low-resource languages by utilizing resources in closely related languages, but the performance still lags far behind their high-resource counterparts. In this paper, we first assess the problems faced by current entity candidate generation methods for low-resource XEL, then propose three improvements that (1) reduce the disconnect between entity mentions and KB entries, and (2) improve the robustness of the model to low-resource scenarios. The methods are simple, but effective: We experiment with our approach on seven XEL datasets and find that they yield an average gain of 16.9{\%} in Top-30 gold candidate recall, compared with state-of-the-art baselines. Our improved model also yields an average gain of 7.9{\%} in in-KB accuracy of end-to-end XEL.1"
2020.emnlp-main.39,Efficient Meta Lifelong-Learning with Limited Memory,2020,-1,-1,4,0,20104,zirui wang,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Current natural language processing models work well on a single task, yet they often fail to continuously learn new tasks without forgetting previous ones as they are re-trained throughout their lifetime, a challenge known as lifelong learning. State-of-the-art lifelong language learning methods store past examples in episodic memory and replay them at both training and inference time. However, as we show later in our experiments, there are three significant impediments: (1) needing unrealistically large memory module to achieve good performance, (2) suffering from negative transfer, (3) requiring multiple local adaptation steps for each test example that significantly slows down the inference speed. In this paper, we identify three common principles of lifelong learning methods and propose an efficient meta-lifelong framework that combines them in a synergistic fashion. To achieve sample efficiency, our method trains the model in a manner that it learns a better initialization for local adaptation. Extensive experiments on text classification and question answering benchmarks demonstrate the effectiveness of our framework by achieving state-of-the-art performance using merely 1{\%} memory size and narrowing the gap with multi-task learning. We further show that our method alleviates both catastrophic forgetting and negative transfer at the same time."
2020.acl-main.722,Soft Gazetteers for Low-Resource Named Entity Recognition,2020,14,0,4,0,9800,shruti rijhwani,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Traditional named entity recognition models use gazetteers (lists of entities) as features to improve performance. Although modern neural network models do not require such hand-crafted features for strong performance, recent work has demonstrated their utility for named entity recognition on English data. However, designing such features for low-resource languages is challenging, because exhaustive entity gazetteers do not exist in these languages. To address this problem, we propose a method of {``}soft gazetteers{''} that incorporates ubiquitously available information from English knowledge bases, such as Wikipedia, into neural named entity recognition models through cross-lingual entity linking. Our experiments on four low-resource languages show an average improvement of 4 points in F1 score."
W19-4208,{CMU}-01 at the {SIGMORPHON} 2019 Shared Task on Crosslinguality and Context in Morphology,2019,21,0,5,1,831,aditi chaudhary,"Proceedings of the 16th Workshop on Computational Research in Phonetics, Phonology, and Morphology",0,"This paper presents the submission by the CMU-01 team to the SIGMORPHON 2019 task 2 of Morphological Analysis and Lemmatization in Context. This task requires us to produce the lemma and morpho-syntactic description of each token in a sequence, for 107 treebanks. We approach this task with a hierarchical neural conditional random field (CRF) model which predicts each coarse-grained feature (eg. POS, Case, etc.) independently. However, most treebanks are under-resourced, thus making it challenging to train deep neural models for them. Hence, we propose a multi-lingual transfer training regime where we transfer from multiple related languages that share similar typology."
P19-1285,Transformer-{XL}: Attentive Language Models beyond a Fixed-Length Context,2019,0,101,4,0,17265,zihang dai,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80{\%} longer than RNNs and 450{\%} longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch."
P19-1286,Domain Adaptation of Neural Machine Translation by Lexicon Induction,2019,34,0,4,0,3846,junjie hu,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"It has been previously noted that neural machine translation (NMT) is very sensitive to domain shift. In this paper, we argue that this is a dual effect of the highly lexicalized nature of NMT, resulting in failure for sentences with large numbers of unknown words, and lack of supervision for domain-specific words. To remedy this problem, we propose an unsupervised adaptation method which fine-tunes a pre-trained out-of-domain NMT model using a pseudo-in-domain corpus. Specifically, we perform lexicon induction to extract an in-domain lexicon, and construct a pseudo-parallel in-domain corpus by performing word-for-word back-translation of monolingual in-domain target sentences. In five domains over twenty pairwise adaptation settings and two model architectures, our method achieves consistent improvements without using any in-domain parallel sentences, improving up to 14 BLEU over unadapted models, and up to 2 BLEU over strong back-translation baselines."
D19-1520,A Little Annotation does a Lot of Good: A Study in Bootstrapping Low-resource Named Entity Recognizers,2019,30,3,5,1,831,aditi chaudhary,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"Most state-of-the-art models for named entity recognition (NER) rely on the availability of large amounts of labeled data, making them challenging to extend to new, lower-resourced languages. However, there are now many proposed solutions to this problem involving either cross-lingual transfer learning, which learns from other highly resourced languages, or active learning, which efficiently selects effective training data based on model predictions. In this paper, we ask the question: given this recent progress, and some amount of human annotation, what is the most effective method for efficiently creating high-quality entity recognizers in under-resourced languages? Based on extensive experimentation using both simulated and real human annotation, we settle on a recipe of starting with a cross-lingual transferred model, then performing targeted annotation of only uncertain entity spans in the target language, minimizing annotator effort. Results demonstrate that cross-lingual transfer is a powerful tool when very little data can be annotated, but an entity-targeted annotation strategy can achieve competitive accuracy quickly, with just one-tenth of training data."
D19-1621,Learning Rhyming Constraints using Structured Adversaries,2019,0,1,3,0,5934,harsh jhamtani,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"Existing recurrent neural language models often fail to capture higher-level structure present in text: for example, rhyming patterns present in poetry. Much prior work on poetry generation uses manually defined constraints which are satisfied during decoding using either specialized decoding procedures or rejection sampling. The rhyming constraints themselves are typically not learned by the generator. We propose an alternate approach that uses a structured discriminator to learn a poetry generator that directly captures rhyming constraints in a generative adversarial setup. By causing the discriminator to compare poems based only on a learned similarity matrix of pairs of line ending words, the proposed approach is able to successfully learn rhyming patterns in two different English poetry datasets (Sonnet and Limerick) without explicitly being provided with any phonetic information"
D18-1034,Neural Cross-Lingual Named Entity Recognition with Minimal Resources,2018,0,7,5,0,27097,jiateng xie,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"For languages with no annotated resources, unsupervised transfer of natural language processing models such as named-entity recognition (NER) from resource-rich languages would be an appealing capability. However, differences in words and word order across languages make it a challenging problem. To improve mapping of lexical items across languages, we propose a method that finds translations based on bilingual word embeddings. To improve robustness to word order differences, we propose to use self-attention, which allows for a degree of flexibility with respect to word order. We demonstrate that these methods achieve state-of-the-art or competitive NER performance on commonly tested languages under a cross-lingual setting, with much lower resource requirements than past approaches. We also evaluate the challenges of applying these methods to Uyghur, a low-resource language."
D18-1196,{D}eep{C}x: A transition-based approach for shallow semantic parsing with complex constructional triggers,2018,0,1,2,1,23085,jesse dunietz,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"This paper introduces the surface construction labeling (SCL) task, which expands the coverage of Shallow Semantic Parsing (SSP) to include frames triggered by complex constructions. We present DeepCx, a neural, transition-based system for SCL. As a test case for the approach, we apply DeepCx to the task of tagging causal language in English, which relies on a wider variety of constructions than are typically addressed in SSP. We report substantial improvements over previous tagging efforts on a causal language dataset. We also propose ways DeepCx could be extended to still more difficult constructions and to other semantic domains once appropriate datasets become available."
D18-1366,Adapting Word Embeddings to New Languages with Morphological and Phonological Subword Representations,2018,16,3,6,1,831,aditi chaudhary,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"Much work in Natural Language Processing (NLP) has been for resource-rich languages, making generalization to new, less-resourced languages challenging. We present two approaches for improving generalization to low-resourced languages by adapting continuous word representations using linguistically motivated subword units: phonemes, morphemes and graphemes. Our method requires neither parallel corpora nor bilingual dictionaries and provides a significant gain in performance over previous methods relying on these resources. We demonstrate the effectiveness of our approaches on Named Entity Recognition for four languages, namely Uyghur, Turkish, Bengali and Hindi, of which Uyghur and Bengali are low resource languages, and also perform experiments on Machine Translation. Exploiting subwords with transfer learning gives us a boost of +15.2 NER F1 for Uyghur and +9.7 F1 for Bengali. We also show improvements in the monolingual setting where we achieve (avg.) +3 F1 and (avg.) +1.35 BLEU."
D18-1538,Towards Semi-Supervised Learning for Deep Semantic Role Labeling,2018,9,2,3,0,20105,sanket mehta,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"Neural models have shown several state-of-the-art performances on Semantic Role Labeling (SRL). However, the neural models require an immense amount of semantic-role corpora and are thus not well suited for low-resource languages or domains. The paper proposes a semi-supervised semantic role labeling method that outperforms the state-of-the-art in limited SRL training corpora. The method is based on explicitly enforcing syntactic constraints by augmenting the training objective with a syntactic-inconsistency loss component and uses SRL-unlabeled instances to train a joint-objective LSTM. On CoNLL-2012 English section, the proposed semi-supervised training with 1{\%}, 10{\%} SRL-labeled data and varying amounts of SRL-unlabeled data achieves +1.58, +0.78 F1, respectively, over the pre-trained models that were trained on SOTA architecture with ELMo on the same SRL-labeled data. Additionally, by using the syntactic-inconsistency loss on inference time, the proposed model achieves +3.67, +2.1 F1 over pre-trained model on 1{\%}, 10{\%} SRL-labeled data, respectively."
W17-0812,The {BEC}au{SE} Corpus 2.0: Annotating Causality and Overlapping Relations,2017,17,2,3,1,23085,jesse dunietz,Proceedings of the 11th Linguistic Annotation Workshop,0,"Language of cause and effect captures an essential component of the semantics of a text. However, causal language is also intertwined with other semantic relations, such as temporal precedence and correlation. This makes it difficult to determine when causation is the primary intended meaning. This paper presents BECauSE 2.0, a new version of the BECauSE corpus with exhaustively annotated expressions of causal language, but also seven semantic relations that are frequently co-present with causation. The new corpus shows high inter-annotator agreement, and yields insights both about the linguistic expressions of causation and about the process of annotating co-present semantic relations."
Q17-1009,Automatically Tagging Constructions of Causation and Their Slot-Fillers,2017,41,2,3,1,23085,jesse dunietz,Transactions of the Association for Computational Linguistics,0,"This paper explores extending shallow semantic parsing beyond lexical-unit triggers, using causal relations as a test case. Semantic parsing becomes difficult in the face of the wide variety of linguistic realizations that causation can take on. We therefore base our approach on the concept of constructions from the linguistic paradigm known as Construction Grammar (CxG). In CxG, a construction is a form/function pairing that can rely on arbitrary linguistic and semantic features. Rather than codifying all aspects of each construction{'}s form, as some attempts to employ CxG in NLP have done, we propose methods that offload that problem to machine learning. We describe two supervised approaches for tagging causal constructions and their arguments. Both approaches combine automatically induced pattern-matching rules with statistical classifiers that learn the subtler parameters of the constructions. Our results show that these approaches are promising: they significantly outperform na{\""\i}ve baselines for both construction recognition and cause and effect head matches."
S16-1186,{CMU} at {S}em{E}val-2016 Task 8: Graph-based {AMR} Parsing with Infinite Ramp Loss,2016,10,11,4,1,12665,jeffrey flanigan,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,None
N16-1087,Generation from {A}bstract {M}eaning {R}epresentation using Tree Transducers,2016,20,38,4,1,12665,jeffrey flanigan,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,None
D16-1153,Phonologically Aware Neural Model for Named Entity Recognition in Low Resource Transfer Settings,2016,16,27,4,0,23087,akash bharadwaj,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
C16-1114,Leveraging Multilingual Training for Limited Resource Event Extraction,2016,25,1,3,0,22942,andrew hsi,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Event extraction has become one of the most important topics in information extraction, but to date, there is very limited work on leveraging cross-lingual training to boost performance. We propose a new event extraction approach that trains on multiple languages using a combination of both language-dependent and language-independent features, with particular focus on the case where target domain training data is of very limited size. We show empirically that multilingual training can boost performance for the tasks of event trigger extraction and event argument extraction on the Chinese ACE 2005 dataset."
W15-1622,Annotating Causal Language Using Corpus Lexicography of Constructions,2015,14,7,3,1,23085,jesse dunietz,Proceedings of The 9th Linguistic Annotation Workshop,0,"Detecting and analyzing causal language is essential to extracting semantic relationships. To that end, we present an annotation scheme for English causal language (not metaphysical causality), and discuss two methodologies for annotation. The first uses only a coding manual to train annotators in distinguishing causal from non-causal language. To address low inter-coder agreement, we adopted a second methodology, in which we first created a causal language constructicon based on corpus analysis, then required annotators only to annotate instances based on the constructicon. (This resembles the methodology used for annotating the FrameNet and PropBank corpora.) Our contributions, in addition to the annotation scheme itself, are methodological: we discuss when constructicon-based methodology is appropriate, and address the validity of annotation schemes that require expertlevel metalinguistic awareness."
S15-1020,Extending a Single-Document Summarizer to Multi-Document: a Hierarchical Approach,2015,21,3,6,1,26032,luis marujo,Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics,0,"The increasing amount of online content motivated the development of multi-document summarization methods. In this work, we explore straightforward approaches to extend single-document summarization methods to multi-document summarization. The proposed methods are based on the hierarchical combination of single-document summaries, and achieves state of the art results."
P15-2036,Frame-Semantic Role Labeling with Heterogeneous Annotations,2015,32,23,4,0,37420,meghana kshirsagar,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,We consider the task of identifying and labeling the semantic arguments of a predicate that evokes a FrameNet frame. This task is challenging because there are only a few thousand fully annotated sentences for supervised training. Our approach augments an existing model with features derived from FrameNet and PropBank and with partially annotated exemplars from FrameNet. We observe a 4% absolute increase in F1 versus the original model.
P15-2105,Automatic Keyword Extraction on {T}witter,2015,39,24,9,1,26032,luis marujo,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"In this paper, we build a corpus of tweets from Twitter annotated with keywords using crowdsourcing methods. We identify key differences between this domain and the work performed on other domains, such as news, which makes existing approaches for automatic keyword extraction not generalize well on Twitter datasets. These datasets include the small amount of content in each tweet, the frequent usage of lexical variants and the high variance of the cardinality of keywords present in each tweet. We propose methods for addressing these issues, which leads to solid improvements on this dataset for this task."
P14-5001,Cross-Lingual Information to the Rescue in Keyword Extraction,2014,15,1,3,0,33416,chungchi huang,Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"We introduce a method that extracts keywords in a language with the help of the other. In our approach, we bridge and fuse conventionally irrelevant word statistics in languages. The method involves estimating preferences for keywords w.r.t. domain topics and generating cross-lingual bridges for word statistics integration. At run-time, we transform parallel articles into word graphs, build cross-lingual edges, and exploit PageRank with word keyness information for keyword extraction. We present the system, BiKEA , that applies the method to keyword analysis. Experiments show that keyword extraction benefits from PageRank, globally learned keyword preferences, and cross-lingual word statistics interaction which respects language diversity."
P14-1134,A Discriminative Graph-Based Parser for the {A}bstract {M}eaning {R}epresentation,2014,39,141,3,1,12665,jeffrey flanigan,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Abstract Meaning Representation (AMR) is a semantic formalism for which a grow- ing set of annotated examples is avail- able. We introduce the first approach to parse sentences into this representa- tion, providing a strong baseline for fu- ture improvement. The method is based on a novel algorithm for finding a maxi- mum spanning, connected subgraph, em- bedded within a Lagrangian relaxation of an optimization problem that imposes lin- guistically inspired constraints. Our ap- proach is described in the general frame- work of structured prediction, allowing fu- ture incorporation of additional features and constraints, and may extend to other formalisms as well. Our open-source sys- tem, JAMR, is available at: http://github.com/jflanigan/jamr"
levin-etal-2014-resources,Resources for the Detection of Conventionalized Metaphors in Four Languages,2014,11,6,5,0,17380,lori levin,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This paper describes a suite of tools for extracting conventionalized metaphors in English, Spanish, Farsi, and Russian. The method depends on three significant resources for each language: a corpus of conventionalized metaphors, a table of conventionalized conceptual metaphors (CCM table), and a set of extraction rules. Conventionalized metaphors are things like {``}escape from poverty{''} and {``}burden of taxation{''}. For each metaphor, the CCM table contains the metaphorical source domain word (such as {``}escape{''}) the target domain word (such as {``}poverty{''}) and the grammatical construction in which they can be found. The extraction rules operate on the output of a dependency parser and identify the grammatical configurations (such as a verb with a prepositional phrase complement) that are likely to contain conventional metaphors. We present results on detection rates for conventional metaphors and analysis of the similarity and differences of source domains for conventional metaphors in the four languages."
P13-2134,The Effects of Lexical Resource Quality on Preference Violation Detection,2013,15,0,3,1,23085,jesse dunietz,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Lexical resources such as WordNet and VerbNet are widely used in a multitude of NLP tasks, as are annotated corpora such as treebanks. Often, the resources are used as-is, without question or examination. This practice risks missing significant performance gains and even entire techniques. This paper addresses the importance of resource quality through the lens of a challenging NLP task: detecting selectional preference violations. We present DAVID, a simple, lexical resource-based preference violation detector. With asis lexical resources, DAVID achieves an F1-measure of just 28.27%. When the resource entries and parser outputs for a small sample are corrected, however, the F1-measure on that sample jumps from 40% to 61.54%, and performance on other examples rises, suggesting that the algorithm becomes practical given refined resources. More broadly, this paper shows that resource quality matters tremendously, sometimes even more than algorithmic improvements."
N13-1025,Large-Scale Discriminative Training for Statistical Machine Translation Using Held-Out Line Search,2013,28,13,3,1,12665,jeffrey flanigan,Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We introduce a new large-scale discriminative learning algorithm for machine translation that is capable of learning parameters in models with extremely sparse features. To ensure their reliable estimation and to prevent overfitting, we use a two-phase learning algorithm. First, the contribution of individual sparse features is estimated using large amounts of parallel data. Second, a small development corpus is used to determine the relative contributions of the sparse features and standard dense features. Not only does this two-phase learning approach prevent overfitting, the second pass optimizes corpus-level BLEU of the Viterbi translation of the decoder. We demonstrate significant improvements using sparse rule indicator features in three different translation tasks. To our knowledge, this is the first large-scale discriminative training algorithm capable of showing improvements over the MERT baseline with only rule indicator features in addition to the standard MERT features."
marujo-etal-2012-supervised,"Supervised Topical Key Phrase Extraction of News Stories using Crowdsourcing, Light Filtering and Co-reference Normalization",2012,9,25,3,1,26032,luis marujo,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Fast and effective automated indexing is critical for search and personalized services. Key phrases that consist of one or more words and represent the main concepts of the document are often used for the purpose of indexing. In this paper, we investigate the use of additional semantic features and pre-processing steps to improve automatic key phrase extraction. These features include the use of signal words and freebase categories. Some of these features lead to significant improvements in the accuracy of the results. We also experimented with 2 forms of document pre-processing that we call light filtering and co-reference normalization. Light filtering removes sentences from the document, which are judged peripheral to its main content. Co-reference normalization unifies several written forms of the same named entity into a unique form. We also needed a ÂGold StandardÂ â a set of labeled documents for training and evaluation. While the subjective nature of key phrase selection precludes a true ÂGold StandardÂ, we used Amazon's Mechanical Turk service to obtain a useful approximation. Our data indicates that the biggest improvements in performance were due to shallow semantic features, news categories, and rhetorical signals (nDCG 78.47{\%} vs. 68.93{\%}). The inclusion of deeper semantic features such as Freebase sub-categories was not beneficial by itself, but in combination with pre-processing, did cause slight improvements in the nDCG scores."
C12-3041,Recognition of Named-Event Passages in News Articles,2012,22,3,4,1,26032,luis marujo,Proceedings of {COLING} 2012: Demonstration Papers,0,"We extend the concept of Named Entities to Named Events xe2x80x90 commonly occurring events such as battles and earthquakes. We propose a method for finding specific passages in news articles that contain information about such events and report our preliminary evaluation results. Collecting xe2x80x9cGold Standardxe2x80x9d data presents many problems, both practical and conceptual. We present a method for obtaining such data using the Amazon Mechanical Turk service."
W11-1210,Active Learning with Multiple Annotations for Comparable Data Classification Task,2011,18,5,4,1,44215,vamshi ambati,Proceedings of the 4th Workshop on Building and Using Comparable Corpora: Comparable Corpora and the Web,0,Supervised learning algorithms for identifying comparable sentence pairs from a dominantly non-parallel corpora require resources for computing feature functions as well as training the classifier. In this paper we propose active learning techniques for addressing the problem of building comparable data for low-resource languages. In particular we propose strategies to elicit two kinds of annotations from comparable sentence pairs: class label assignment and parallel segment extraction. We also propose an active learning strategy for these two annotations that performs significantly better than when sampling for either of the annotations independently.
2011.mtsummit-papers.12,Multi-Strategy Approaches to Active Learning for Statistical Machine Translation,2011,17,3,3,1,44215,vamshi ambati,Proceedings of Machine Translation Summit XIII: Papers,0,"This paper investigates active learning to improve statistical machine translation (SMT) for low-resource language pairs, i.e., when there is very little pre-existing parallel text. Since generating additional parallel text to train SMT may be costly, active sampling selects the sentences from a monolingual corpus which if translated would have maximal positive impact in training SMT models. We investigate different strategies such as density and diversity preferences as well as multistrategy methods such as modified version of DUAL and our new ensemble approach GraDUAL. These result in significant BLEU-score improvements over strong baselines when parallel training data is scarce."
W10-0102,Active Semi-Supervised Learning for Improving Word Alignment,2010,27,3,3,1,44215,vamshi ambati,Proceedings of the {NAACL} {HLT} 2010 Workshop on Active Learning for Natural Language Processing,0,Word alignment models form an important part of building statistical machine translation systems. Semi-supervised word alignment aims to improve the accuracy of automatic word alignment by incorporating full or partial alignments acquired from humans. Such dedicated elicitation effort is often expensive and depends on availability of bilingual speakers for the language-pair. In this paper we study active learning query strategies to carefully identify highly uncertain or most informative alignment links that are proposed under an unsupervised word alignment model. Manual correction of such informative links can then be applied to create a labeled dataset used by a semi-supervised word alignment model. Our experiments show that using active learning leads to maximal reduction of alignment error rates with reduced human effort.
P10-2067,Active Learning-Based Elicitation for Semi-Supervised Word Alignment,2010,23,3,3,1,44215,vamshi ambati,Proceedings of the {ACL} 2010 Conference Short Papers,0,"Semi-supervised word alignment aims to improve the accuracy of automatic word alignment by incorporating full or partial manual alignments. Motivated by standard active learning query sampling frameworks like uncertainty-, margin- and query-by-committee sampling we propose multiple query strategies for the alignment link selection task. Our experiments show that by active selection of uncertain and informative links, we reduce the overall manual effort involved in elicitation of alignment link data for training a semi-supervised word aligner."
ambati-etal-2010-active,Active Learning and Crowd-Sourcing for Machine Translation,2010,17,126,3,1,44215,vamshi ambati,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Large scale parallel data generation for new language pairs requires intensive human effort and availability of experts. It becomes immensely difficult and costly to provide Statistical Machine Translation (SMT) systems for most languages due to the paucity of expert translators to provide parallel data. Even if experts are present, it appears infeasible due to the impending costs. In this paper we propose Active Crowd Translation (ACT), a new paradigm where active learning and crowd-sourcing come together to enable automatic translation for low-resource language pairs. Active learning aims at reducing cost of label acquisition by prioritizing the most informative data for annotation, while crowd-sourcing reduces cost by using the power of the crowds to make do for the lack of expensive language experts. We experiment and compare our active learning strategies with strong baselines and see significant improvements in translation quality. Similarly, our experiments with crowd-sourcing on Mechanical Turk have shown that it is possible to create parallel corpora using non-experts and with sufficient quality assurance, a translation system that is trained using this corpus approaches expert quality."
C10-2037,Monolingual Distributional Profiles for Word Substitution in Machine Translation,2010,19,5,3,1,22684,rashmi gangadharaiah,Coling 2010: Posters,0,"Out-of-vocabulary (OOV) words present a significant challenge for Machine Translation. For low-resource languages, limited training data increases the frequency of OOV words and this degrades the quality of the translations. Past approaches have suggested using stems or synonyms for OOV words. Unlike the previous methods, we show how to handle not just the OOV words but rare words as well in an Example-based Machine Translation (EBMT) paradigm. Presence of OOV words and rare words in the input sentence prevents the system from finding longer phrasal matches and produces low quality translations due to less reliable language model estimates. The proposed method requires only a monolingual corpus of the source language to find candidate replacements. A new framework is introduced to score and rank the replacements by efficiently combining features extracted for the candidate replacements. A lattice representation scheme allows the decoder to select from a beam of possible replacement candidates. The new framework gives statistically significant improvements in English-Chinese and English-Haitian translation systems."
2010.eamt-1.27,Chunk-Based {EBMT},2010,18,10,3,1,46632,jae kim,Proceedings of the 14th Annual conference of the European Association for Machine Translation,0,"Corpus driven machine translation approaches such as Phrase-Based Statistical Machine Translation and Example-Based Machine Translation have been successful by using word alignment to find translation fragments for matched source parts in a bilingual training corpus. However, they still cannot properly deal with systematic translation for insertion or deletion words between two distant languages. In this work, we used syntactic chunks as translation units to alleviate this problem, improve alignments and show improvement in BLEU for Korean to English and Chinese to English translation tasks."
2010.eamt-1.40,Automatic Determination of Number of clusters for creating Templates in Example-Based Machine Translation,2010,-1,-1,3,1,22684,rashmi gangadharaiah,Proceedings of the 14th Annual conference of the European Association for Machine Translation,0,None
W09-4633,Active Learning in Example-Based Machine Translation,2009,7,11,3,1,22684,rashmi gangadharaiah,Proceedings of the 17th Nordic Conference of Computational Linguistics ({NODALIDA} 2009),0,"In data-driven Machine Translation ap-proaches, like Example-Based MachineTranslation (EBMT) (Brown, 2000) andStatistical Machine Translation (Vogel etal., 2003), the quality of the translationsproduced depends on the amount of train-ing data available. While more data is al-ways useful, a large training corpus canslow down a machine translation system.We would like to selectively sample thehugecorpustoobtainasub-corpusofmostinformative sentence pairs that would leadto good quality translations. Reducing theamount of training data also enables oneto easily port an MT system onto smalldevices that have less memory and stor-age capacity. In this paper, we proposeusingActiveLearningstrategiestosamplethemostinformativesentencepairs. Therehas not been much progress in the ap-plication of active learning theory in ma-chine translation due to the complexity ofthe translation models. We use a pool-based strategy to selectively sample in-stances from a parallel corpora which notonly outperformed a random selector butalso a previously used sampling strategy(Eck et al., 2005) in an EBMT framework(Brown, 2000) by about one BLEU point(Papineni et al., 2002)."
W09-1908,Proactive Learning for Building Machine Translation Systems for Minority Languages,2009,9,1,2,1,44215,vamshi ambati,Proceedings of the {NAACL} {HLT} 2009 Workshop on Active Learning for Natural Language Processing,0,"Building machine translation (MT) for many minority languages in the world is a serious challenge. For many minor languages there is little machine readable text, few knowledgeable linguists, and little money available for MT development. For these reasons, it becomes very important for an MT system to make best use of its resources, both labeled and unlabeled, in building a quality system. In this paper we argue that traditional active learning setup may not be the right fit for seeking annotations required for building a Syntax Based MT system for minority languages. We posit that a relatively new variant of active learning, Proactive Learning, is more suitable for this task."
2009.mtsummit-posters.2,Extraction of Syntactic Translation Models from Parallel Data using Syntax from Source and Target Languages,2009,15,13,3,1,44215,vamshi ambati,Proceedings of Machine Translation Summit XII: Posters,0,"We propose a generic rule induction framework that is informed by syntax from both sides of a parsed parallel corpus, as sets of structural, boundary and labeling related constraints. Factoring syntax in this manner empowers our framework to work with independent annotations coming from multiple resources and not necessarily a single syntactic structure. We then explore the issue of lexical coverage of translation models learned in different scenarios using syntax from one side vs. both sides. We specifically look at how the non-isomorphic nature of parse trees for the two languages affects coverage. We propose a novel technique for restructuring targetside parse trees, that generates alternate isomorphic target trees that preserve the syntactic boundaries of constituents that were aligned in the original parse trees. We also show that combining rules extracted by restructuring syntactic trees on both sides produces significantly better translation models. The improved precision and coverage of our syntax tables particularly fill in for the lack of lexical coverage in Syntax based Machine Translation approaches."
W08-0708,Evaluating an Agglutinative Segmentation Model for {P}ara{M}or,2008,20,9,3,1,44751,christian monson,Proceedings of the Tenth Meeting of {ACL} Special Interest Group on Computational Morphology and Phonology,0,"This paper describes and evaluates a modification to the segmentation model used in the unsupervised morphology induction system, ParaMor. Our improved segmentation model permits multiple morpheme boundaries in a single word. To prepare ParaMor to effectively apply the new agglutinative segmentation model, two heuristics improve ParaMor's precision. These precision-enhancing heuristics are adaptations of those used in other unsupervised morphology induction systems, including work by Hafer and Weiss (1974) and Goldsmith (2006). By reformulating the segmentation model used in ParaMor, we significantly improve ParaMor's performance in all language tracks and in both the linguistic evaluation as well as in the task based information retrieval (IR) evaluation of the peer operated competition Morpho Challenge 2007. ParaMor's improved morpheme recall in the linguistic evaluations of German, Finnish, and Turkish is higher than that of any system which competed in the Challenge. In the three languages of the IR evaluation, our enhanced ParaMor significantly outperforms, at average precision over newswire queries, a morphologically naive baseline; scoring just behind the leading system from Morpho Challenge 2007 in English and ahead of the first place system in German."
monson-etal-2008-linguistic,Linguistic Structure and Bilingual Informants Help Induce Machine Translation of Lesser-Resourced Languages,2008,14,10,8,1,44751,christian monson,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Producing machine translation (MT) for the many minority languages in the world is a serious challenge. Minority languages typically have few resources for building MT systems. For many minor languages there is little machine readable text, few knowledgeable linguists, and little money available for MT development. For these reasons, our research programs on minority language MT have focused on leveraging to the maximum extent two resources that are available for minority languages: linguistic structure and bilingual informants. All natural languages contain linguistic structure. And although the details of that linguistic structure vary from language to language, language universals such as context-free syntactic structure and the paradigmatic structure of inflectional morphology, allow us to learn the specific details of a minority language. Similarly, most minority languages possess speakers who are bilingual with the major language of the area. This paper discusses our efforts to utilize linguistic structure and the translation information that bilingual informants can provide in three sub-areas of our rapid development MT program: morphology induction, syntactic transfer rule learning, and refinement of imperfect learned rules."
I08-1056,Cluster-Based Query Expansion for Statistical Question Answering,2008,7,1,2,0.666667,48643,lucian lita,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{I},0,"Document retrieval is a critical component of question answering (QA), yet little work has been done towards statistical modeling of queries and towards automatic generation of high quality query content for QA. This paper introduces a new, cluster-based query expansion method that learns queries known to be successful when applied to similar questions. We show that cluster-based expansion improves the retrieval performance of a statistical question answering system when used in addition to existing query expansion methods. This paper presents experiments with several feature selection methods used individually and in combination. We show that documents retrieved using the cluster-based approach are inherently different than documents retrieved using existing methods and provide a higher data diversity to answers extractors."
W07-1315,{P}ara{M}or: Minimally Supervised Induction of Paradigm Structure and Morphological Analysis,2007,9,19,2,1,44751,christian monson,Proceedings of Ninth Meeting of the {ACL} Special Interest Group in Computational Morphology and Phonology,0,"Paradigms provide an inherent organizational structure to natural language morphology. ParaMor, our minimally supervised morphology induction algorithm, retrusses the word forms of raw text corpora back onto their paradigmatic skeletons; performing on par with state-of-the-art minimally supervised morphology induction algorithms at morphological analysis of English and German. ParaMor consists of two phases. Our algorithm first constructs sets of affixes closely mimicking the paradigms of a language. And with these structures in hand, ParaMor then annotates word forms with morpheme boundaries. To set ParaMor's few free parameters we analyze a training corpus of Spanish. Without adjusting parameters, we induce the morphological structure of English and German. Adopting the evaluation methodology of Morpho Challenge 2007 (Kurimo et al., 2007), we compare ParaMor's morphological analyses with Morfessor (Creutz, 2006), a modern minimally supervised morphology induction system. ParaMor consistently achieves competitive F1 measures."
N07-1041,Combining Probability-Based Rankers for Action-Item Detection,2007,19,12,2,0.952381,6866,paul bennett,Human Language Technologies 2007: The Conference of the North {A}merican Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,0,"This paper studies methods that automatically detect action-items in e-mail, an important category for assisting users in identifying new tasks, tracking ongoing ones, and searching for completed ones. Since action-items consist of a short span of text, classifiers that detect action-items can be built from a document-level or a sentence-level view. Rather than commit to either view, we adapt a contextsensitive metaclassification framework to this problem to combine the rankings produced by different algorithms as well as different views. While this framework is known to work well for standard classification, its suitability for fusing rankers has not been studied. In an empirical evaluation, the resulting approach yields improved rankings that are less sensitive to training set variation, and furthermore, the theoretically-motivated reliability indicators we introduce enable the metaclassifier to now be applicable in any problem where the base classifiers are used."
2007.mtsummit-papers.25,Improving transfer-based {MT} systems with automatic refinements,2007,-1,-1,2,1,48477,ariadna llitjos,Proceedings of Machine Translation Summit XI: Papers,0,None
2007.mtsummit-papers.32,Report on the {NSF}-sponsored Human Language Technology Workshop on Industrial Centers,2007,-1,-1,4,0,40215,mary harper,Proceedings of Machine Translation Summit XI: Papers,0,None
N06-2011,Spectral Clustering for Example Based Machine Translation,2006,7,14,3,1,22684,rashmi gangadharaiah,"Proceedings of the Human Language Technology Conference of the {NAACL}, Companion Volume: Short Papers",0,"Prior work has shown that generalization of data in an Example Based Machine Translation (EBMT) system, reduces the amount of pre-translated text required to achieve a certain level of accuracy (Brown, 2000). Several word clustering algorithms have been suggested to perform these generalizations, such as k-Means clustering or Group Average Clustering. The hypothesis is that better contextual clustering can lead to better translation accuracy with limited training data. In this paper, we use a form of spectral clustering to cluster words, and this is shown to result in as much as 29.08% improvement over the baseline EBMT system."
2006.amta-papers.3,Context-Based Machine Translation,2006,16,53,1,1,10837,jaime carbonell,Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"Context-Based Machine TranslationTM (CBMT) is a new paradigm for corpus-based translation that requires no parallel text. Instead, CBMT relies on a lightweight translation model utilizing a fullform bilingual dictionary and a sophisticated decoder using long-range context via long n-grams and cascaded overlapping. The translation process is enhanced via in-language substitution of tokens and phrases, both for source and target, when top candidates cannot be confirmed or resolved in decoding. Substitution utilizes a synonym and near-synonym generator implemented as a corpus-based unsupervised learning process. Decoding requires a very large target-language-only corpus, and while substitution in target can be performed using that same corpus, substitution in source requires a separate (and smaller) source monolingual corpus. Spanish-to-English CBMT was tested on Spanish newswire text, achieving a BLEU score of 0.6462 in June 2006, the highest BLEU reported for any language pair. Further testing also shows that quality increases above the reported score as the target corpus size increases and as dictionary coverage of source words and phrases becomes more complete."
2006.amta-panels.1,Presentation,2006,8,20,1,1,10837,jaime carbonell,Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Panel on hybrid machine translation: why and how?,0,"A computer-implemented system and method of creating a presentation is provided. The method provides an off-line presentation application to allow the addition of multimedia objects from a central repository comprising multimedia objects. This method also provides instantaneous correlation of multimedia objects for the text inputs of the user, and for embedding the selected multimedia objects to presentation slides. The multimedia objects can either be animations, movies, or diagrams. This method also allows a user to search the repositories for multimedia objects using keywords. The local repository can be updated using an update manager with newly available multimedia objects from the central repository. This system also provides means for creating and continuously playing multimedia presentation."
2006.amta-panels.1,Meadan global dialogue,2006,8,20,1,1,10837,jaime carbonell,Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Panel on machine translation for social impact,0,"A computer-implemented system and method of creating a presentation is provided. The method provides an off-line presentation application to allow the addition of multimedia objects from a central repository comprising multimedia objects. This method also provides instantaneous correlation of multimedia objects for the text inputs of the user, and for embedding the selected multimedia objects to presentation slides. The multimedia objects can either be animations, movies, or diagrams. This method also allows a user to search the repositories for multimedia objects using keywords. The local repository can be updated using an update manager with newly available multimedia objects from the central repository. This system also provides means for creating and continuously playing multimedia presentation."
W05-0813,Symmetric Probabilistic Alignment,2005,8,4,4,0,40110,ralf brown,Proceedings of the {ACL} Workshop on Building and Using Parallel Texts,0,"We recently decided to develop a new alignment algorithm for the purpose of improving our Example-Based Machine Translation (EBMT) system's performance, since subsentential alignment is critical in locating the correct translation for a matched fragment of the input. Unlike most algorithms in the literature, this new Symmetric Probabilistic Alignment (SPA) algorithm treats the source and target languages in a symmetric fashion.n n In this short paper, we outline our basic algorithm and some extensions for using context and positional information, and compare its alignment accuracy on the Romanian-English data for the shared task with IBM Model 4 and the reported results from the prior workshop."
2005.eamt-1.13,A framework for interactive and automatic refinement of transfer-based machine translation,2005,6,29,2,1,48477,ariadna llitjos,Proceedings of the 10th EAMT Conference: Practical applications of machine translation,0,"Most current Machine Translation (MT) systems do not improve with feedback from post-editors beyond the addition of corrected translations to parallel training data (for statistical and example-base MT) or to a memory database. Rule based systems to date improve only via manual debugging. In contrast, we propose a largely automated method for capturing more information from human post-editors, so that corrections may be performed automatically to translation grammar rules and lexical entries. This paper introduces a general framework for incorporating a refinement module into rule-based transfer MT systems. This framework allows for generalizing post-editing efforts in an effective way, by identifying and correcting rules semi-automatically on order to improve coverage and overall translation quality."
2005.eamt-1.21,Symmetric probabilistic alignment for example-based translation,2005,4,8,4,1,46632,jae kim,Proceedings of the 10th EAMT Conference: Practical applications of machine translation,0,"Since subsentential alignment is critically important to the translation quality of an Example-Based Machine Translation (EBMT) system which operates by finding and combining phrase-level matches against the training examples, we recently decided to de- velop a new alignment algorithm for the purpose of improving the EBMT system's per- formance. Unlike most algorithms in the literature, this new Symmetric Probabilistic Align- ment (SPA) algorithm treats the source and target languages in a symmetric fashion. In this paper, we describe our basic algorithm and some extensions for using context and posi- tional information, compare its alignment accuracy with IBM Model 4, and report on ex- periments in which either IBM Model 4 or SPA alignments are substituted for the aligner currently built into the EBMT system. Both Model 4 and SPA are significantly better than the internal aligner and SPA slightly outperforms Model 4 despite being handicapped by incomplete integration with EBMT."
W04-3251,Instance-Based Question Answering: A Data-Driven Approach,2004,25,35,2,0.666667,48643,lucian lita,Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,0,"Anticipating the availability of large questionanswer datasets, we propose a principled, datadriven Instance-Based approach to Question Answering. Most question answering systems incorporate three major steps: classify questions according to answer types, formulate queries for document retrieval, and extract actual answers. Under our approach, strategies for answering new questions are directly learned from training data. We learn models of answer type, query content, and answer extraction from clusters of similar questions. We view the answer type as a distribution, rather than a class in an ontology. In addition to query expansion, we learn general content features from training data and use them to enhance the queries. Finally, we treat answer extraction as a binary classification problem in which text snippets are labeled as correct or incorrect answers. We present a basic implementation of these concepts that achieves a good performance on TREC test data."
W04-0107,Unsupervised Induction of Natural Language Morphology Inflection Classes,2004,13,19,3,1,44751,christian monson,Proceedings of the 7th Meeting of the {ACL} Special Interest Group in Computational Phonology: Current Themes in Computational Phonology and Morphology,0,"We propose a novel language-independent framework for inducing a collection of morphological inflection classes from a monolingual corpus of full form words. Our approach involves two main stages. In the first stage, we generate a large data structure of candidate inflection classes and their interrelationships. In the second stage, search and filtering techniques are applied to this data structure, to identify a select collection of true inflection classes of the language. We describe the basic methodology involved in both stages of our approach and present an evaluation of our baseline techniques applied to induction of major inflection classes of Spanish. The preliminary results on an initial training corpus already surpass an F1 of 0.5 against ideal Spanish inflectional morphology classes."
font-llitjos-carbonell-2004-translation,The Translation Correction Tool: {E}nglish-{S}panish User Studies,2004,3,13,2,1,48477,ariadna llitjos,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"Machine translation systems should improve with feedback from post-editors, but none do beyond the very localized benefit of adding the corrected translation to parallel training data (for statistical and example-base MTS) or a memory data base. Rule based systems to date improve only via manual debugging. In contrast, we introduce a largely automated method for capturing more information from the human post-editor, so that corrections may be performed automatically to translation grammar rules and lexical entries. This paper focuses on the information capture phase and reports on an experiment with English-Spanish translation."
cavalli-sforza-etal-2004-developing,Developing Language Resources for a Transnational Digital Government System,2004,5,6,2,0,28253,violetta cavallisforza,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"We describe ongoing efforts towards developing language resources for a transnational digital government project aimed at applying information technology (IT) to a problem of international concern: detecting and monitoring activities related to the transnational movement of illicit drugs. The project seeks to support information sharing, coordination and collaboration among government agencies within a country and across national boundaries by combining a variety of technologies including a distributed query processor with form-based and conversational user interfaces, a language translation system, an event server for event filtering and notification, and an event-trigger-rule server. The prototype system is being developed by U.S. universities in collaboration with an international agency and with universities and government agencies in Belize and the Dominican Republic. This paper focuses on the linguistic resources and their use in Example-Based Machine Translation (EBMT). We are in the process of developing an EnglishSpanish parallel corpus, focused on the domain of information elicited and used at border crossings, to fuel the EBMT system. While significant parallel corpora are available for these two languages in the newswire domain, they were found to be of very limited use for the border crossings application, spurring the need to develop our own resources."
monson-etal-2004-data,Data Collection and Analysis of {M}apudungun Morphology for Spelling Correction,2004,4,2,7,1,44751,christian monson,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"This paper describes part of a three year collaboration between Carnegie Mellon University's Language Technologies Institute, the Programa de Educacion Intercultural Bilingue of the Chilean Ministry of Education, and Universidad de La Frontera (Temuco, Chile). We are currently constructing a spelling checker for Mapudungun, a polysynthetic language spoken by the Mapuche people in Chile and Argentina. The spelling checker will be built in MySpell, the spell checking system used by the open source office suite OpenOffice. This paper also describes the spoken language corpus that is used as a source of data for developing the spelling checker."
2004.eamt-1.5,Challenges in using an example-based {MT} system for a transnational digital government project,2004,7,2,3,0,28253,violetta cavallisforza,Proceedings of the 9th EAMT Workshop: Broadening horizons of machine translation and its applications,0,"We describe ongoing efforts towards and challenges in using an Example-Based Machine Translation (EBMT) system in the context of a multinational, multi-university and multi-agency transnational digital government project. The project is aimed at applying information technology to the problem of collecting and sharing information securely in a multilingual context. We report on a number of issues encountered in obtaining and using language data for the EBMT system, discuss our current solutions, and briefly describe ongoing enhancements to the system to meet some of the technical and practical challenges posed by using this machine translation approach in the project domain."
2004.eamt-1.14,A trainable transfer-based {MT} approach for languages with limited resources,2004,11,24,7,0,13539,alon lavie,Proceedings of the 9th EAMT Workshop: Broadening horizons of machine translation and its applications,0,"We describe a Machine Translation (MT) approach that is specifically designed to enable rapid development of MT for languages with limited amounts of online resources. Our approach assumes the availability of a small number of bi-lingual speakers of the two languages, but these need not be linguistic experts. The bi-lingual speakers create a comparatively small corpus of word aligned phrases and sentences (on the order of magnitude of a few thousand sentence pairs) using a specially designed elicitation tool. From this data, the learning module of our system automatically infers hierarchical syntactic transfer rules, which encode how syntactic constituent structures in the source language transfer to the target language. The collection of transfer rules is then used in our run-time system to translate previously unseen source language text into the target language. We describe the general principles underlying our approach, and present results from an experiment, where we developed a basic Hindi-to-English MT system over the course of two months, using extremely limited resources."
font-llitjos-etal-2004-error,Error analysis of two types of grammar for the purpose of automatic rule refinement,2004,6,5,3,1,48477,ariadna llitjos,Proceedings of the 6th Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"This paper compares a manually written MT grammar and a grammar learned automatically from an English-Spanish elicitation corpus with the ultimate purpose of automatically refining the translation rules. The experiment described here shows that the kind of automatic refinement operations required to correct a translation not only varies depending on the type of error, but also on the type of grammar. This paper describes the two types of grammars and gives a detailed error analysis of their output, indicating what kinds of refinements are required in each case."
2003.mtsummit-papers.4,Reducing boundary friction using translation-fragment overlap,2003,10,26,4,0.122822,40110,ralf brown,Proceedings of Machine Translation Summit IX: Papers,0,"Many corpus-based Machine Translation (MT) systems generate a number of partial translations which are then pieced together rather than immediately producing one overall translation. While this makes them more robust to ill-formed input, they are subject to disfluencies at phrasal translation boundaries even for well-formed input. We address this {``}boundary friction{''} problem by introducing a method that exploits overlapping phrasal translations and the increased confidence in translation accuracy they imply. We specify an efficient algorithm for producing translations using overlap. Finally, our empirical analysis indicates that this approach produces higher quality translations than the standard method of combining non-overlapping fragments generated by our Example-Based MT (EBMT) system in a peak-to-peak comparison."
W02-0106,Design and Evolution of a Language Technologies Curriculum,2002,0,1,4,0,39652,robert frederking,Proceedings of the {ACL}-02 Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics,0,The Language Technologies Institute (LTI) of the School of Computer Science at Carnegie Mellon University is one of the largest programs of its kind. We present here the initial design and subsequent evolution of our MS and PhD programs in Language Technologies. The motivations for the design and evolution are also presented.
carbonell-etal-2002-automatic,Automatic rule learning for resource-limited {MT},2002,8,21,1,1,10837,jaime carbonell,Proceedings of the 5th Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"Machine Translation of minority languages presents unique challenges, including the paucity of bilingual training data and the unavailability of linguistically-trained speakers. This paper focuses on a machine learning approach to transfer-based MT, where data in the form of translations and lexical alignments are elicited from bilingual speakers, and a seeded version-space learning algorithm formulates and refines transfer rules. A rule-generalization lattice is defined based on LFG-style f-structures, permitting generalization operators in the search for the most general rules consistent with the elicited data. The paper presents these methods and illustrates examples."
2001.mtsummit-road.7,Design and implementation of controlled elicitation for machine translation of low-density languages,2001,10,17,3,0.487805,48480,katharina probst,Workshop on MT2010: Towards a Road Map for MT,0,"NICE is a machine translation project for low-density languages. We are building a tool that will elicit a controlled corpus from a bilingual speaker who is not an expert in linguistics. The corpus is intended to cover major typological phenomena, as it is designed to work for any language. Using implicational universals, we strive to minimize the number of sentences that each informant has to translate. From the elicited sentences, we learn transfer rules with a version space algorithm. Our vision for MT in the future is one in which systems can be quickly trained for new languages by native speakers, so that speakers of minor languages can participate in education, health care, government, and internet without having to give up their languages."
W00-0405,Multi-Document Summarization By Sentence Extraction,2000,27,292,3,0,54304,jade goldstein,NAACL-ANLP 2000 Workshop: Automatic Summarization,0,"This paper discusses a text extraction approach to multi-document summarization that builds on single-document summarization methods by using additional, available information about the document set as a whole and the relationships between the documents. Multi-document summarization differs from single in that the issues of compression, speed, redundancy and passage selection are critical in the formation of useful summaries. Our approach addresses these issues by using domain-independent techniques based mainly on fast, statistical processing, a metric for reducing redundancy and maximizing diversity in the selected passages, and a modular framework to allow easy parameterization for different genres, corpora characteristics and user requirements."
X98-1025,Summarization: (1) Using {MMR} for Diversity- Based Reranking and (2) Evaluating Summaries,1998,24,13,2,0,54304,jade goldstein,"TIPSTER TEXT PROGRAM PHASE III: Proceedings of a Workshop held at Baltimore, {M}aryland, October 13-15, 1998",0,"This paper develops a method for combining query-relevance with information-novelty in the context of text retrieval and summarization. The Maximal Marginal Relevance (MMR) criterion strives to reduce redundancy while maintaining query relevance in reranking retrieved documents and in selecting appropriate passages for text summarization. Preliminary results indicate some benefits for MMR diversity ranking in ad-hoc query and in single document summarization. The latter are borne out by the trial-run (unofficial) TREC-style evaluation of summarization systems. However, the clearest advantage is demonstrated in the automated construction of large document and non-redundant multi-document summaries, where MMR results are clearly superior to non-MMR passage selection. This paper also discusses our preliminary evaluation of summarization methods for single documents."
1996.amta-1.26,Panel: Next steps in {MT} research,1996,-1,-1,2,0,53771,lynn carlson,Conference of the Association for Machine Translation in the Americas,0,None
C94-1013,Evaluation Metrics for Knowledge-Based Machine Translation,1994,7,20,3,0,55464,rd nyberg,{COLING} 1994 Volume 1: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"A methodology is presented for component-based machine translation (MT) evaluation through causal error analysis to complement existing global evaluation methods. This methodology is particularly appropriate for knowledge-based machine translation (KBMT) systems. After a discussion of MT evaluation criteria and the particular evaluation metrics proposed for KBMT, we apply this methodology to a large-scale application of the KANT machine translation system, and present some sample results."
1994.amta-1.31,Future Directions,1994,-1,-1,2,0,53582,joseph pentheroudakis,Proceedings of the First Conference of the Association for Machine Translation in the Americas,0,None
1994.amta-1.36,"{KANT}: Knowledge-Based, Accurate Natural Language Translation",1994,-1,-1,3,0.925926,7705,teruko mitamura,Proceedings of the First Conference of the Association for Machine Translation in the Americas,0,None
1994.amta-1.41,{PANGLOSS},1994,0,0,1,1,10837,jaime carbonell,Proceedings of the First Conference of the Association for Machine Translation in the Americas,0,None
1993.tmi-1.28,"Automated Corpus Analysis and the Acquisition of Large, Multi-Lingual Knowledge Bases for {MT}",1993,13,15,3,0,56878,teruko mitamara,Proceedings of the Fifth Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages,0,"Although knowledge-based MT systems have the potential to achieve high translation accuracy, each successful application system requires a large amount of hand-coded knowledge (lexicons, grammars, mapping rules, etc.). Systems like KBMT-89 and its descendants have demonstrated how knowledge-based translation can produce good results in technical domains with tractable domain semantics. Nevertheless, the cost of developing large-scale applications with tens of thousands of domain concepts precludes a purely hand-crafted approach. The current challenge for the next generation of knowledge-based MT systems is to utilize on-line textual resources and corpus analysis software in order to automate the most laborious aspects of the knowledge acquisition process. This partial automation can in turn maximize the productivity of human knowledge engineers and help to make large-scale applications of knowledge-based MT an economic reality. In this paper we discuss the corpus-based knowledge acquisition methodology used in KANT, a knowledge-based translation system for multi-lingual document production. This methodology can be generalized beyond the KANT interlingua approach for use with any system that requires similar kinds of knowledge."
1992.tmi-1.20,"The {KANT} perspective: a critique of pure transfer (and pure interlingua, pure statistics, .. )",1992,-1,-1,1,1,10837,jaime carbonell,Proceedings of the Fourth Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages,0,None
H91-1023,Session 3: Machine Translation,1991,0,0,1,1,10837,jaime carbonell,"Speech and Natural Language: Proceedings of a Workshop Held at Pacific Grove, California, {F}ebruary 19-22, 1991",0,"In 1965 the United States Academy of Science cemmissoned a study of he state of the art in Machine Translation, whose findings were published the following year and become popularly known as the ALPAC report. In essence, ALPAC argued that there was insufficient scientific basis in natural langauge processing to perform reliable machine t~anslatinn, and the large expensive computers of the time would make NIT eeonornically infeasible. Both situations have since changed drastically, invaLidating the ALPAC conciusions. In fact, DARPA has played a major role in fostering the development of the NLP scientific infrsstmcture in the post-ALPAC years. IBM, in which the direct-transfer paradigm is still king and translation is viewed as transduction between two character (or word) s t reams--essent ia l ly two encodings of the same message. However, the direct transfer rules are totally learned by statistical analysis of large bi-lingual corpora, rather than laboriously and incompletely hand-coded. A drawback of the statistical approach, of course, is that it carmot guarantee the accuracy of any textual passage being translated, but rather strives to minimize the total number of errors over time."
1991.mtsummit-papers.9,An Efficient Interlingua Translation System for Multi-lingual Document Production,1991,2,94,3,0.925926,7705,teruko mitamura,Proceedings of Machine Translation Summit III: Papers,0,"Knowledge-based interlingual machine translation systems produce semantically accurate translations, but typically require massive knowledge acquisition. This paper describes KANT, a system that reduces this requirement to produce practical, scalable, and accurate KBMT applications. First, the set of requirements is discussed, then the full KANT architecture is illustrated, and finally results from a fully implemented prototype are presented."
H90-1072,Machine Translation Again?,1990,0,2,2,0,37311,yorick wilks,"Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, {P}ennsylvania, June 24-27,1990",0,"Machine translation (MT) remains the paradigm task for natural language processing (NLP) since its inception in the 1950s. Unless NLP can succeed with the central task of machine translation, it cannot be considered successful as a field. We maintain that the most profitable approach to MT at the present time is an interlingual and modular one. MT is one the precious few computational tasks falling broadly within artificial intelligence (AI) that combine a fundamental intellectual research challenge with enormous proven need. To establish the latter, one only has to note that in Japan alone the current MT requirement is for 20 billion pages a year (a market of some $66 billion a year)."
H89-2078,White Paper on Natural Language Processing,1989,0,16,2,0,4279,ralph weischedel,"Speech and Natural Language: Proceedings of a Workshop Held at Cape Cod, Massachusetts, October 15-18, 1989",0,"We take the ultimate goal of natural language processing (NLP) to be the ability to use natural languages as effectively as humans do. Natural language, whether spoken, written, or typed, is the most natural means of communication between humans, and the mode of expression of choice for most of the documents they produce. As computers play a larger role in the preparation, acquisition, transmission, monitoring, storage, analysis, and transformation of information, endowing them with the ability to understand and generate information expressed in natural languages becomes more and more necessary. Some tasks currently performed by humans cannot be automated without endowing computers with natural language processing capabilities, and these provide two major challenges to NLP systems:1. Reading and writing text, applied to tasks such as message routing, abstracting, monitoring, summarizing, and entering information in databases, with applications, in such areas as intelligence, logistics, office automation, and libraries. Computers should be able to assimilate and compose extended communications.2. Translation, of documents or spoken language, with applications, in such areas as in science, diplomacy, multinational commerce, and intelligence. Computers should be able to understand input in more than one language, provide output in more than one language, and translate between languages."
C88-1021,Anaphora Resolution: A Multi-Strategy Approach,1988,14,106,1,1,10837,jaime carbonell,{C}oling {B}udapest 1988 Volume 1: {I}nternational {C}onference on {C}omputational {L}inguistics,0,"Anaphora resolution has proven to be a very difficult problem; it requires the integrated application of syntactic, semantic, and pragmatic knowledge. This paper examines the hypothesis that instead of attempting to construct a monolithic method for resolving anaphora, the combination of multiple strategies, each exploiting a different knowledge source, proves more effective - theoretically and computationally. Cognitive plausibility is established in that human judgements of the optimal anaphoric referent accord with those of the strategy-based method, and human inability to determine a unique referent corresponds to the cases where different strategies offer conflicting candidates for the anaphoric referent."
1987.mtsummit-1.19,Interlingua - Technical Prospect of Interlingua -,1987,-1,-1,1,1,10837,jaime carbonell,Proceedings of Machine Translation Summit I,0,None
1987.mtsummit-1.32,{CMU} Project,1987,-1,-1,2,1,56874,masaru tomita,Proceedings of Machine Translation Summit I,0,None
C86-1037,Requirements for Robust Natural Language Interfaces: The {L}anguage{C}raft and {XCALIBUR} experiences,1986,9,6,1,1,10837,jaime carbonell,Coling 1986 Volume 1: The 11th International Conference on Computational Linguistics,0,"Natural Language interfaces to data bases and expert systems require the integration of several crucial capabilities in order to be judged h a b i t a b l e by their end users and p r o d u c t i v e by the developers of applications. User habitability is measured in terms of linguistic coverage, robustness of behavior and speed of response, whereas implementer productivity is measured by the amount of effort required to connect the interface to a new application, to develop its syntactic and semantic grammar, and to test and test and debug the resultant system assuring a certain level of performancexe2x80xa2 These latter criteria have not been addressed directly by natural language researchers in pure laboratory settings, with the exception of user-defined extensions to an existing interface (e.g., NanoKLAUS [4], v e x [6]). But, in order to amortize the cost of developing practical, robust and efficient interfaces over multiple applications, the implementer productivity requirements are as important as user habitability. We treat each set of criteria in turn, drawing from our experience in XCALIBUR [2] and in LanguageCraft TM [5], a commercially available environment and run time module for rapid development of domain-oriented natural language interfaces, f In our discussion we distill the general lessons accrued from several years of experience using these systems, and conducting several small-scale user studies."
C86-1138,Parsing Spoken Language: a Semantic Caseframe Approach,1986,14,62,3,0,57204,philip hayes,Coling 1986 Volume 1: The 11th International Conference on Computational Linguistics,0,"Parsing spoken input introduces serious problems not present in parsing typed natural language. In particular, indeterminacies and inaccuracies of acoustic recognition must be handled in an integral manner. Many techniques for parsing typed natural language do not adapt well to these extra demands. This paper describes an extension of semantic caseframe parsing to restricted-domain spoken input. The semantic caseframe grammar representation is the same as that used for earlier work on robust parsing of typed input. Due to the uncertainty inherent in speech recognition, the caseframe grammar is applied in a quite different way, emphasizing island growing from caseframe headers. This radical change in application is possible due to the high degree of abstraction in the caseframe representation. The approach presented was tested successfully in a preliminary implementation."
C86-1149,Another Stride Towards Knowledge-Based Machine Translation,1986,20,19,2,1,56874,masaru tomita,Coling 1986 Volume 1: The 11th International Conference on Computational Linguistics,0,"Building on the well-established premise that reliable machine translation requires a significant degree of text comprehension, this paper presents a recent advance in multi-lingual knowledge-based machine translation (KBMT). Unlike previous approaches, the current method provides for separate syntactic and semantic knowledge sources that are integrated dynamically for parsing and generation. Such a separation enables the system to have syntactic grammars, language specific but domain general, and semantic knowledge bases, domain specific but language general. Subsequently, grammars and domain knowledge are precompiled automatically in any desired combination to produce very efficient and very thorough real-time parsers. A pilot implementation of our KBMT architecture using functional grammars and entity-oriented semantics demonstrates the feasibility of the new approach.1"
1985.tmi-1.4,New Approaches to Machine Translation,1985,14,15,1,1,10837,jaime carbonell,Proceedings of the first Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages,0,"The current resurgence of interest in machine translation is partially attributable to the emergence of a variety of new paradigms, ranging from better translation aids and improved pre and post-editing methods, to highly interactive approaches and fully automated knowledge-based systems. This paper discusses each basic approach and provides some comparative analysis. It is argued that both interactive and knowledge based systems offer considerable promise to remedy the deficiencies of the earlier, more ad-hoc post-editing approaches,"
P84-1041,Is There Natural Language after Data Bases?,1984,3,1,1,1,10837,jaime carbonell,10th International Conference on Computational Linguistics and 22nd Annual Meeting of the Association for Computational Linguistics,1,"1. Why Not Data Base Query? The undisputed favorite application for natural language interfaces has been data base query. Why? The reasons range from the relative simplicity of the task, including shallow semantic processing, to the potential real-world utility of the resultant system. Because of such reasons, the data base query task was an excellent paradigmatic problem for computational linguistics, and for the very same reasons it is now time for the field to abandon its protective cocoon and progress beyond this rather limiting task. But, one may ask, what task shall then become the new paradigmatic problem? Alas, such question presupposes that a single, universally acceptable, syntactically and semantically challenging task exists. I will argue that better progress can be made by diversification and focusing on different theoretically meaningful problems, with some research groups opting to investigate issues arisinq from the development of integrated multi-purpose systems."
P84-1089,Coping with Extragrammaticality,1984,23,10,1,1,10837,jaime carbonell,10th International Conference on Computational Linguistics and 22nd Annual Meeting of the Association for Computational Linguistics,1,"Practical natural language interfaces must exhibit robust behaviour in the presence of extragrammatical user input. This paper classifies different types of grammatical deviations and related phenomena at the lexical and sentential levels, discussing recovery strategies tailored to specific phenomena in the classification. Such strategies constitute a tool chest of computationally tractable methods for coping with extragrammaticality in restricted domain natural language. Some of the strategies have been tested and proven viable in existing parsers."
P83-1025,Discourse Pragmatics and Ellipsis Resolution in Task-Oriented Natural Language Interfaces,1983,14,40,1,1,10837,jaime carbonell,21st Annual Meeting of the Association for Computational Linguistics,1,"This paper reviews discourse phenomena that occur frequently in task-oriented man-machine dialogs, reporting on an empirical study that demonstrates the necessity of handling ellipsis, anaphora, extragrammaticality, inter-sentential metalanguage, and other abbreviatory devices in order to achieve convivial user interaction. Invariably, users prefer to generate terse or fragmentary utterances instead of longer, more complete standalone expressions, even when given clear instructions to the contrary. The XCALIBUR expert system interface is designed to meet these needs, including generalized ellipsis resolution by means of a rule-based caseframe method superior to previous semantic grammar approaches."
J83-3001,Recovery Strategies for Parsing Extragrammatical Language,1983,27,126,1,1,10837,jaime carbonell,American Journal of Computational Linguistics,0,"Practical natural language interfaces must exhibit robust behaviour in the presence of extragrammatical user input. This paper classifies different types of grammatical deviations and related phenomena at the lexical, sentential and dialogue levels and presents recovery strategies tailored to specific phenomena in the classification. Such strategies constitute a tool chest of computationally tractable methods for coping with extragrammaticality in restricted domain natural language. Some of the strategies have been tested and proven viable in existing parsers."
P81-1032,Dynamic Strategy Selection in Flexible Parsing,1981,10,70,1,1,10837,jaime carbonell,19th Annual Meeting of the Association for Computational Linguistics,1,"Robust natural language interpretation requires strong semantic domain models, fail-soft recovery heuristics, and very flexible control structures. Although single-strategy parsers have met with a measure of success, a multi-strategy approach is shown to provide a much higher degree of flexibility, redundancy, and ability to bring task-specific domain knowledge (in addition to general linguistic knowledge) to bear on both grammatical and ungrammatical input. A parsing algorithm is presented that integrates several different parsing strategies, with case-frame instantiation dominating. Each of these parsing strategies exploits different types of knowledge; and their combination provides a strong framework in which to process conjunctions, fragmentary input, and ungrammatical structures, as well as less exotic, grammatically correct input. Several specific heuristics for handling ungrammatical input are presented within this multi-strategy framework."
P80-1004,Metaphor - A Key to Extensible Semantic Analysis,1980,6,29,1,1,10837,jaime carbonell,18th Annual Meeting of the Association for Computational Linguistics,1,"Interpreting metaphors is an integral and inescapable process in human understanding of natural language. This paper discusses a method of analyzing metaphors based on the existence of a small number of generalized metaphor mappings. Each generalized metaphor contains a recognition network, a basic mapping, additional transfer mappings, and an implicit intention component. It is argued that the method reduces metaphor interpretation from a reconstruction to a recognition task. Implications towards automating certain aspects of language learning are also discussed."
P79-1002,Towards a Self-Extending Parser,1979,7,47,1,1,10837,jaime carbonell,17th Annual Meeting of the Association for Computational Linguistics,1,This paper discusses an approach to incremental learning in natural language processing. The technique of projecting and integrating semantic constraints to learn word definitions is analyzed as implemented in the POLITICS system. Extensions and improvements of this technique are developed. The problem of generalizing existing word meanings and understanding metaphorical uses of words is addressed in terms of semantic constraint integration.
J74-1003,Natural Semantics in Artificial Intelligence,1974,-1,-1,1,1,10837,jaime carbonell,American Journal of Computational Linguistics,0,None
