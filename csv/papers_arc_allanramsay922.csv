S18-1030,{CENTEMENT} at {S}em{E}val-2018 Task 1: Classification of Tweets using Multiple Thresholds with Self-correction and Weighted Conditional Probabilities,2018,0,0,2,0,28754,tariq ahmad,Proceedings of The 12th International Workshop on Semantic Evaluation,0,"In this paper we present our contribution to SemEval-2018, a classifier for classifying multi-label emotions of Arabic and English tweets. We attempted {``}Affect in Tweets{''}, specifically Task E-c: Detecting Emotions (multi-label classification). Our method is based on preprocessing the tweets and creating word vectors combined with a self correction step to remove noise. We also make use of emotion specific thresholds. The final submission was selected upon the best performance achieved, selected when using a range of thresholds. Our system was evaluated on the Arabic and English datasets provided for the task by the competition organisers, where it ranked 2nd for the Arabic dataset (out of 14 entries) and 12th for the English dataset (out of 35 entries)."
W17-1312,{A}rabic Tweets Treebanking and Parsing: A Bootstrapping Approach,2017,10,1,2,1,32066,fahad albogamy,Proceedings of the Third {A}rabic Natural Language Processing Workshop,0,"In this paper, we propose using a {``}bootstrapping{''} method for constructing a dependency treebank of Arabic tweets. This method uses a rule-based parser to create a small treebank of one thousand Arabic tweets and a data-driven parser to create a larger treebank by using the small treebank as a seed training set. We are able to create a dependency treebank from unlabelled tweets without any manual intervention. Experiments results show that this method can improve the speed of training the parser and the accuracy of the resulting parsers."
albogamy-ramsay-2017-universal,{U}niversal {D}ependencies for {A}rabic Tweets,2017,6,0,2,1,32066,fahad albogamy,"Proceedings of the International Conference Recent Advances in Natural Language Processing, {RANLP} 2017",0,"To facilitate cross-lingual studies, there is an increasing interest in identifying linguistic universals. Recently, a new universal scheme was designed as a part of universal dependency project. In this paper, we map the Arabic tweets dependency treebank (ATDT) to the Universal Dependency (UD) scheme to compare it to other language resources and for the purpose of cross-lingual studies."
almiman-ramsay-2017-using,Using {E}nglish Dictionaries to generate Commonsense Knowledge in Natural Language,2017,6,0,2,0,32423,ali almiman,"Proceedings of the International Conference Recent Advances in Natural Language Processing, {RANLP} 2017",0,"This paper presents an approach to generating common sense knowledge written in raw English sentences. Instead of using public contributors to feed this source, this system chose to employ expert linguistics decisions by using definitions from English dictionaries. Because the definitions in English dictionaries are not prepared to be transformed into inference rules, some preprocessing steps were taken to turn each relation of word:definition in dictionaries into an inference rule in the form left-hand side â right-hand side. In this paper, we applied this mechanism using two dictionaries: The MacMillan Dictionary and WordNet definitions. A random set of 200 inference rules were extracted equally from the two dictionaries, and then we used human judgment as to whether these rules are {`}True{'} or not. For the MacMillan Dictionary the precision reaches 0.74 with 0.508 recall, and the WordNet definitions resulted in 0.73 precision with 0.09 recall."
almiman-ramsay-2017-hybrid,A Hybrid System to apply Natural Language Inference over Dependency Trees,2017,3,0,2,0,32423,ali almiman,"Proceedings of the International Conference Recent Advances in Natural Language Processing, {RANLP} 2017",0,"This paper presents the development of a natural language inference engine that benefits from two current standard approaches; i.e., shallow and deep approaches. This system combines two non-deterministic algorithms: the approximate matching from the shallow approach and a theorem prover from the deep approach for handling multi-step inference tasks. The theorem prover is customized to accept dependency trees and apply inference rules to these trees. The inference rules are automatically generated as syllogistic rules from our test data (FraCaS test suite). The theorem prover exploits a non-deterministic matching algorithm within a standard backward chaining inference engine. We employ continuation programming as a way of seamlessly handling the combination of these two non-deterministic algorithms. Testing the matching algorithm on {``}Generalized quantifiers{''} and {``}adjectives{''} topics in FraCaS (MacCartney and Manning 2007), we achieved an accuracy of 92.8{\%} of the single-premise cases. For the multi-steps of inference, we checked the validity of our syllogistic rules and then extracted four generic instances that can be applied to more than one problem."
W16-3912,Unsupervised Stemmer for {A}rabic Tweets,2016,11,2,2,1,32066,fahad albogamy,Proceedings of the 2nd Workshop on Noisy User-generated Text ({WNUT}),0,"Stemming is an essential processing step in a wide range of high level text processing applications such as information extraction, machine translation and sentiment analysis. It is used to reduce words to their stems. Many stemming algorithms have been developed for Modern Standard Arabic (MSA). Although Arabic tweets and MSA are closely related and share many characteristics, there are substantial differences between them in lexicon and syntax. In this paper, we introduce a light Arabic stemmer for Arabic tweets. Our results show improvements over the performance of a number of well-known stemmers for Arabic."
L16-1238,Fast and Robust {POS} tagger for {A}rabic Tweets Using Agreement-based Bootstrapping,2016,22,4,2,1,32066,fahad albogamy,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Part-of-Speech(POS) tagging is a key step in many NLP algorithms. However, tweets are difficult to POS tag because they are short, are not always written maintaining formal grammar and proper spelling, and abbreviations are often used to overcome their restricted lengths. Arabic tweets also show a further range of linguistic phenomena such as usage of different dialects, romanised Arabic and borrowing foreign words. In this paper, we present an evaluation and a detailed error analysis of state-of-the-art POS taggers for Arabic when applied to Arabic tweets. On the basis of this analysis, we combine normalisation and external knowledge to handle the domain noisiness and exploit bootstrapping to construct extra training data in order to improve POS tagging for Arabic tweets. Our results show significant improvements over the performance of a number of well-known taggers for Arabic."
R15-1001,{POS} Tagging for {A}rabic Tweets,2015,20,9,2,1,32066,fahad albogamy,Proceedings of the International Conference Recent Advances in Natural Language Processing,0,"Part-of-Speech (POS) tagging is a key step in many NLP algorithms. However, tweets are difficult to POS tag because there are many phenomena that frequently appear in Twitter that are not as common, or are entirely absent, in other domains: tweets are short, are not always written maintaining formal grammar and proper spelling, and abbreviations are often used to overcome their restricted lengths. Arabic tweets also show a further range of linguistic phenomena such as usage of different dialects, romanised Arabic and borrowing foreign words. In this paper, we present an evaluation and a detailed error analysis of stateof-the-art POS taggers for Arabic when applied to Arabic tweets. The accuracy of standard Arabic taggers is typically excellent (96-97%) on Modern Standard Arabic (MSA) text; however, their accuracy declines to 49-65% on Arabic tweets. Further, we present our initial approach to improve the taggersxe2x80x99 performance. By doing some improvements based on observed errors, we are able to reach 79% tagging accuracy."
R15-1032,The Application of Constraint Rules to Data-driven Parsing,2015,9,0,2,0,37357,sardar jaf,Proceedings of the International Conference Recent Advances in Natural Language Processing,0,"The process of determining the structural relationships between words in both natural and machine languages is known as parsing. Parsers are used as core components in a number of Natural Language Processing (NLP) applications such as online tutoring applications, dialogue-based systems and textual entailment systems. They have been used widely in the development of machine languages. In order to understand the way parsers work, we will investigate and describe a number of widely used parsing algorithms. These algorithms have been utilised in a range of different contexts such as dependency frameworks and phrase structure frameworks. We will investigate and describe some of the fundamental aspects of each of these frameworks, which can function in various ways including grammar-driven approaches and data-driven approaches. Grammar-driven approaches use a set of grammatical rules for determining the syntactic structures of sentences during parsing. Data-driven approaches use a set of parsed data to generate a parse model which is used for guiding the parser during the processing of new sentences. A number of state-of-the-art parsers have been developed that use such frameworks and approaches. We will briefly highlight some of these in this thesis. There are three specific important features that it is important to integrate into the development of parsers. These are efficiency, accuracy, and robustness. Efficiency is concerned with the use of as little time and computing resources as possible when processing natural language text. Accuracy involves maximising the correctness of the analyses that a parser produces. Robustness is a measure of a parser?s ability to cope with grammatically complex sentences and produce analyses of a large proportion of a set of sentences.In this thesis, we present a parser that can efficiently, accurately, and robustly parse a set of natural language sentences. Additionally, the implementation of the parser presented here allows for some trading-off between different levels of parsing performance. For example, some NLP applications may emphasise efficiency/robustness over accuracy while some other NLP systems may require a greater focus on accuracy. In dialogue-based systems, it may be preferable to produce a correct grammatical analysis of a question, rather than incorrectly analysing the grammatical structure of a question or quickly producing a grammatically incorrect answer for a question. Alternatively, it may be desirable that document translation systems translate a document into a different language quickly but less accurately, rather than slowly but highly accurately, because users may be able to correct grammatically incorrect sentences manually if necessary. The parser presented here is based on data-driven approaches but we will allow for the application of constraint rules to it in order to improve its performance."
W14-3609,Combining strategies for tagging and parsing {A}rabic,2014,13,1,2,1,38526,maytham alabbas,Proceedings of the {EMNLP} 2014 Workshop on {A}rabic Natural Language Processing ({ANLP}),0,"We describe a simple method for combining taggers which produces substantially better performance than any of the contributing tools. The method is very simple, but it leads to considerable improvements in performance: given three taggers for Arabic whose individual accuracies range from 0.956 to 0.967, the combined tagger scores 0.995xe2x80x90a sevenfold reduction in the error rate when compared to the best of the contributing tools. Given the effectiveness of this approach to combining taggers, we have investigated its applicability to parsing. For parsing, it seems better to take pairs of similar parsers and back off to a third if they disagree."
W13-3819,Inference for Natural Language,2013,0,0,2,0,40819,amal alshahrani,Proceedings of the Joint Symposium on Semantic Processing. Textual Inference and Structures in Corpora,0,"The main aim of this study is to develop a natural language inference (NLI) engine that is more robust than typical systems that are based on post-Montague approaches to semantics and more accurate than the kinds of shallow approaches usually used for textual entailment, The term robustness is concerned with processing as many inputs as possible successfully, and the term accuracy is concerned with producing correct result. In recent years, several approaches have been proposed for NLI. These approaches range from shallow approaches to deep approaches. However, eachxefxbfxbd approach has a number of limitations, which we discuss in this paper. We argue that all approaches to NLI share a common architecture, and that it may be possible to overcome the limitations inherent in the existing approaches by combining elements of both kinds of strategy."
R13-1002,Optimising Tree Edit Distance with Subtrees for Textual Entailment,2013,31,2,2,1,38526,maytham alabbas,Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013,0,"This paper introduces a method for improving tree edit distance (TED) for textual entailment. We explore two ways of improving TED: we extend the standard TED to use edit operations that apply to subtrees as well as to single nodes; and we use the xe2x80x98artificial bee colonyxe2x80x99 algorithm (ABC) to estimate the cost of edit operations for single nodes and subtrees and to determine thresholds. The preliminary results of the current work for checking entailment between two texts are encouraging compared with the common bag-ofwords, string edit distance and standard TED algorithms."
R11-1062,Exploiting Hidden Morphophonemic Constraints for Finding the Underlying Forms of {`}weak{'} {A}rabic Verbs,2011,7,0,1,1,28755,allan ramsay,Proceedings of the International Conference Recent Advances in Natural Language Processing 2011,0,"We present a treatment of Arabic morphology which allows us to deal with xe2x80x98weakxe2x80x99 verbs by paying attention to the underlying phonological process. This provides us with a very clean way of thinking about such verbs, and also makes maintenance of the lexicon very straightforward."
W09-3717,Using {E}nglish for commonsense knowledge,2009,15,0,1,1,28755,allan ramsay,Proceedings of the Eight International Conference on Computational Semantics,0,"The work reported here arises from an attempt to provide a body of simple information about diet and its effect on various common medical conditions. Expressing this knowledge in natural language has a number of advantages. It also raises a number of difficult issues. We will consider solutions, and partial solutions, to these issues below."
W09-2013,{`}Sorry{'} is the hardest word,2009,0,0,1,1,28755,allan ramsay,Proceedings of the Workshop on Computational Approaches to Linguistic Creativity,0,None
W08-2216,Everyday Language is Highly Intensional,2008,17,0,1,1,28755,allan ramsay,Semantics in Text Processing. {STEP} 2008 Conference Proceedings,0,"There has recently been a great deal of work aimed at trying to extract information from substantial texts for tasks such as question answering. Much of this work has dealt with texts which are reasonably large, but which are known to contain reliable relevant information, e.g. FAQ lists, on-line encyclopaedias, rather than looking at huge unorganised resources such as the web. We believe, however, that even this work underestimates the complexity and subtlety of language, and hence will inevitably be restricted in what it can cope with. In particular, everyday use of language involves considerable amounts of reasoning over intensional objects (properties and propositions). In order to respond appropriately to simple-seeming questions such as 'Is going for a walk good for me?', for instance, you have to be able to talk about event-types, which are intrinsically intensional. We discuss the issues involved in handling such items, and shows the kind of background knowledge that is required for drawing the appropriate conclusions about them."
2008.eamt-1.13,Translating emphatic/contrastive focus from {E}nglish to {M}andarin {C}hinese,2008,27,2,2,0,48802,chenli kuo,Proceedings of the 12th Annual conference of the European Association for Machine Translation,0,"Despite the importance of intonation in spoken languages, deeper linguistic information encoded in prosody is rarely taken into account in speech-to-speech machine translation systems. This paper concerns the translation of spoken English into Mandarin Chinese, paying particular attention to the emphatic/contrastive focus in questions which is realised by means of phonological stress in spoken English but by lexical and syntactic devices in Mandarin. There are two main reasons to translate phonologically marked emphatic/contrastive focus with other linguistic devices: firstly, different languages tend to use different devices to express emphatic/contrastive focus; secondly, the production of prosody in text-to-speech systems is far from perfect. In this paper, a translation framework which is capable of treating emphatic/contrastive focus is outlined and focus rules are developed. The framework has been tested on a corpus of 207 utterances in the domain of asthma, although the focus rules are not domain-specific."
W07-2320,Deep-reasoning-centred Dialogue,2007,29,1,2,1,38733,debora field,Proceedings of the Eleventh {E}uropean Workshop on Natural Language Generation ({ENLG} 07),0,"This paper discusses an implemented dialogue system which generates the meanings of utterances by taking into account: the surface mood of the user's last utterance; the meanings of all the user's utterances from the current discourse; the system's expert knowledge; and the system's beliefs about the current situation arising from the discourse (including its beliefs about the user and her beliefs, and its beliefs about what is 'common knowledge'). The system formulates the content of its responses by employing an epistemic theorem prover to do deep reasoning. During the reasoning process, it remembers the proof tree it constructs, and from this derives the meaning of an explanatory response."
W06-3903,How to change a person{'}s mind: Understanding the difference between the effects and consequences of speech acts,2006,-1,-1,2,1,38733,debora field,Proceedings of the Fifth International Workshop on Inference in Computational Semantics ({IC}o{S}-5),0,None
P06-2044,Local Constraints on Sentence Markers and Focus in {S}omali,2006,11,0,2,0,49926,katherine hargreaves,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"We present a computationally tractable account of the interactions between sentence markers and focus marking in Somali. Somali, as a Cushitic language, has a basic pattern wherein a small 'core' clause is preceded, and in some cases followed by, a set of 'topics', which provide scene-seting information against which the core is interpreted. Some topics appear to carry a 'focus marker', indicating that they are particularly salient. We will outline a computationally tractable grammar for Somali in which focus marking emerges naturally from a consideration of the use of a range of sentence markers."
W03-2714,A Constructive View of Discourse Operators,2003,0,0,1,1,28755,allan ramsay,"Proceedings of the 2003 {EACL} Workshop on Dialogue Systems: interaction, adaptation and styes of management",0,None
C00-2096,Unscrambling {E}nglish word order,2000,7,5,1,1,28755,allan ramsay,{COLING} 2000 Volume 2: The 18th International Conference on Computational Linguistics,0,We propose a treatment of 'extraposition' which allows items to be assimilated directly even when they appear far from their canonical positions. This treatment supports analyses of a number of phenomena which are otherwise hard to describe. The approach requires a generalisation of standard chart parsing techniques.
C00-2112,Making Sense of Reference to the Unfamiliar,2000,6,3,2,1,54636,helen seville,{COLING} 2000 Volume 2: The 18th International Conference on Computational Linguistics,0,"Computational approaches to reference resolution, like Centering Theory, are best at resolving referring expressions which denote familiar referents. We demonstrate how, by taking a proof-theoretic approach to reference resolution within a Centering-type framework, we are able to make sense of referring expressions for unfamiliar referents. These include, in addition to bridging descriptions, definite descriptions like the first man and the first snowdrops of Spring. We claim that the first of these denotes a unique subset of a plural discourse antecedent. While the second has no discourse antecedent, we similarly treat it as denoting a unique subset of a familiar referent."
W99-0111,Reference-based Discourse Structure for Reference Resolution,1999,-1,-1,2,1,54636,helen seville,The Relation of Discourse/Dialogue Structure and Reference,0,None
C96-2150,Aspect and Aktionsart: Fighting or Cooperating?,1996,11,3,1,1,28755,allan ramsay,{COLING} 1996 Volume 2: The 16th International Conference on Computational Linguistics,0,"It is widely accepted that semantic theories should, as far as possible, be compositional. The claim that a theory is compositional, however, lacks bite if lexical and pre-lexical items are allowed to mean different things in different contexts. The aim of the current paper is to show how to deal with a well-known phenomenon by relying on combinatorial effects to infer different consequences from the same items in different contexts without altering the contributions that these items make individually."
C94-2142,Focus on {``}Only{''} and {``}Not{''},1994,8,3,1,1,28755,allan ramsay,{COLING} 1994 Volume 2: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,None
C92-1037,Genetic {NP}s and Habitual {VP}s,1992,10,3,1,1,28755,allan ramsay,{COLING} 1992 Volume 1: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"We propose a simple, intuitively satisfying treatment of the semantics of bare plural NPs. This treatment avoids the use of non-standard logics, and avoids the need for systematic ambiguity of verb semantics."
J91-2007,Intentions in Communication,1991,0,0,1,1,28755,allan ramsay,Computational Linguistics,0,"Review of Intentions in Communication, by Philip R. Cohen, Jerry Morgan, and Martha E. Pollack, eds., System Development Foundation Benchmark Series, MIT Press, Cambridge, Mass., 1990, 508 pages, $29.95, ISBN 0-262-03150-7."
E91-1055,A Common Framework for Analysis and Generation,1991,8,1,1,1,28755,allan ramsay,Fifth Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"It seems highly desirable to use a single representation of linguistic knowledge for both analysis and generation. We argue that the only part of the average NL system's knowledge that we can have any faith in is its vocabulary and, to a lesser extent, its syntactic rules, and we investigate the consequences of this for generation."
J90-3004,Disjunction without Tears,1990,5,11,1,1,28755,allan ramsay,Computational Linguistics,0,"It is now common practice to use structured sets of features to describe syntactic structures, and to compare such structured sets by some form of unification. It is extremely tempting to try to encode disjunctive information within this framework; unfortunately, general unification of disjunctive structures is NP-complete, which is not a desirable property for the basic operation involved in comparing descriptions. We note that certain kinds of disjunctive specifications can be converted to conjunctive ones. This enables us to describe a restricted set of disjunctive phenomena without incurring the costs associated with general disjunctive unification."
E89-1029,Extended Graph Unification,1989,4,0,1,1,28755,allan ramsay,Fourth Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We propose an apparently minor extension to Kay's (1985) notation for describing directed acylic graphs (DAGs). The proposed notation permits concise descriptions of phenomena which out otherwise be difficult to describe, without incurring significant extra computational overheads in the process of unification. We illustrate the notation with examples from a categorial description of a fragment of English, and discuss the computational properties of unification of DAGs specified in this way."
E85-1008,Effective Parsing With Generalised Phrase Structure Grammar,1985,2,1,1,1,28755,allan ramsay,Second Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Generalised phrase structure grammars (GPSG's) appear to offer a means by which the syntactic properties of natural languages may be very concisely described. The main reason for this is that the GPSG framework allows you to state a variety of meta-grammatical rules which generate new rules from old ones, so that you can specify rules with a wide variety of realisations via a very small number of explicit statements. Unfortunately, trying to analyse a piece of text in terms of such rules is a very awkward task, as even a small set of GPSG statements will generate a large number of underlying rules.This paper discusses some of the difficulties of parsing with GPSG's, and presents a fairly straightforward bottom-up parser for them. This parser is, in itself, no more than adequate - all its components are implemented quite efficiently, but there is nothing tremendously clever about how it searches the space of possible rules to find an analysis of the text it is working on. Its power comes from the fact that it learns from experience: not new rules, but how to recognise realisations of complex combinations of its existing rules. The improvement in the system's performance after even a few trials is dramatic. This is brought about by a mechanism for recording the analysis of text fragments. Such recordings may be used very effectively to guide the subsequent analysis of similar pieces of text. Given such guidance it becomes possible to deal even with text containing unknown or ambiguous words with very little search."
