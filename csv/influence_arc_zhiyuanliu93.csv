2020.aacl-main.75,D18-1247,1,0.843969,"red from Microsoft” with the relation founder, if (Bill Gates, founder, Microsoft) is a relational fact in KGs. The existing methods to alleviate the noise problem can be divided into three major approaches: (1) Some methods adopt multi-instance learning by combining sentences with same entity pairs and then selecting informative instances from them. Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) utilize graphical model to infer the informative sentences, while Zeng et al. (2015) use a simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora for the information consistency and complementarity (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018). (3) Many methods tend to utilize sophisticated 748 Relation Distribution on NYT-10 1"
2020.aacl-main.75,D18-1514,1,0.903034,"red from Microsoft” with the relation founder, if (Bill Gates, founder, Microsoft) is a relational fact in KGs. The existing methods to alleviate the noise problem can be divided into three major approaches: (1) Some methods adopt multi-instance learning by combining sentences with same entity pairs and then selecting informative instances from them. Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) utilize graphical model to infer the informative sentences, while Zeng et al. (2015) use a simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora for the information consistency and complementarity (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018). (3) Many methods tend to utilize sophisticated 748 Relation Distribution on NYT-10 1"
2020.aacl-main.75,P17-1004,1,0.853193,"t al. (2015) use a simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora for the information consistency and complementarity (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018). (3) Many methods tend to utilize sophisticated 748 Relation Distribution on NYT-10 104 104 Numbers of Instances Numbers of Instances Supporting Set Relation Distribution on Wiki-Distant 105 103 102 101 100 iPhone is designed by Apple Inc. product 103 Steve Jobs is the co-founder of Apple Inc. 102 Tim Cook is Apple’s current CEO. founder CEO Query Instance 101 Bill Gates founded Microsoft. ? 100 0 10 20 30 Relations 40 0 100 200 300 Relations 400 founder Figure 4: Relation distributions (log-scale) on the training part of DS datasets NYT-10 and Wiki-Distant, suggesting tha"
2020.aacl-main.75,P16-1200,1,0.830477,"ut these directions. 3.1 Utilizing More Data Supervised NRE models suffer from the lack of large-scale high-quality training data, since manually labeling data is time-consuming and humanintensive. To alleviate this issue, distant supervision (DS) assumption has been used to automatically label data by aligning existing KGs with plain text (Mintz et al., 2009; Nguyen and Moschitti, 2011; Min et al., 2013). As shown in Figure 3, for Model NYT-10 Wiki-Distant PCNN-ONE PCNN-ATT BERT 0.340 0.349 0.458 0.214 0.222 0.361 Table 2: Area under the curve (AUC) of PCNN-ONE (Zeng et al., 2015), PCNN-ATT (Lin et al., 2016) and BERT (Devlin et al., 2019) on two datasets. any entity pair in KGs, sentences mentioning both the entities will be labeled with their corresponding relations in KGs. Large-scale training examples can be easily constructed by this heuristic scheme. Although DS provides a feasible approach to utilize more data, this automatic labeling mechanism is inevitably accompanied by the wrong labeling problem. The reason is that not all sentences mentioning the two entities express their relations in KGs exactly. For example, we may mistakenly label “Bill Gates retired from Microsoft” with the relati"
2020.aacl-main.75,D17-1189,0,0.0141779,"Jobs is the co-founder of Apple Inc. 102 Tim Cook is Apple’s current CEO. founder CEO Query Instance 101 Bill Gates founded Microsoft. ? 100 0 10 20 30 Relations 40 0 100 200 300 Relations 400 founder Figure 4: Relation distributions (log-scale) on the training part of DS datasets NYT-10 and Wiki-Distant, suggesting that real-world relation distributions suffer from the long-tail problem. mechanisms and training strategies to enhance distantly supervised NRE models. Vu et al. (2016); Beltagy et al. (2019) combine different architectures and training strategies to construct hybrid frameworks. Liu et al. (2017) incorporate a softlabel scheme by changing unconfident labels during training. Furthermore, reinforcement learning (Feng et al., 2018; Zeng et al., 2018) and adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a) have also been adopted in DS. The researchers have formed a consensus that utilizing more data is a potential way towards more powerful RE models, and there still remains some open problems worth exploring: (1) Existing DS methods focus on denoising auto-labeled instances and it is certainly meaningful to follow this research direction. Besides, current DS schem"
2020.aacl-main.75,P15-2047,0,0.0397231,"Missing"
2020.aacl-main.75,Q16-1017,0,0.0159895,"arios. There are already some explorations in handling open relations: (1) Open information extraction (Open IE), as shown in Figure 8, extracts relation phrases and arguments (entities) from text (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018). Open IE does not rely on specific relation types and thus can handle all kinds of relational facts. (2) Relation discovery, as shown in Figure 9, aims at discovering unseen relation types from unsupervised data. Yao et al. (2011); Marcheggiani and Titov (2016) propose to use generative models and treat these relations as latent variables, while Shinyama and Sekine (2006); Elsahar et al. (2017); Wu et al. (2019) cast relation discovery as a clustering task. Though relation extraction in open domains has been widely studied, there are still lots of unsolved research questions remained to be answered: (1) Canonicalizing relation phrases and arguments in Open IE is crucial for downstream tasks (Niklaus et al., 2018). If not canonicalized, the extracted relational facts could be redundant and ambiguous. For example, Open IE may extract two triples (Bara"
2020.aacl-main.75,D12-1048,0,0.0405797,"pple’s current CEO. Relation B Satya Nadella became the CEO of Microsoft in 2014. Figure 9: An example of clustering-based relation discovery, which identifying potential relation types by clustering unlabeled relational instances. relation types only by humans. Thus, we need RE systems that do not rely on pre-defined relation schemas and can work in open scenarios. There are already some explorations in handling open relations: (1) Open information extraction (Open IE), as shown in Figure 8, extracts relation phrases and arguments (entities) from text (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018). Open IE does not rely on specific relation types and thus can handle all kinds of relational facts. (2) Relation discovery, as shown in Figure 9, aims at discovering unseen relation types from unsupervised data. Yao et al. (2011); Marcheggiani and Titov (2016) propose to use generative models and treat these relations as latent variables, while Shinyama and Sekine (2006); Elsahar et al. (2017); Wu et al. (2019) cast relation discovery as a clustering task. Though relation extraction"
2020.aacl-main.75,P16-1105,0,0.0181338,"much human intervention, they are still limited in model capacities. There are some surveys systematically introducing SRE models (Zelenko et al., 2003; Bach and Badaskar, 2007; Pawar et al., 2017). In this paper, we do not spend too much space for SRE and focus more on neural-based models. 82.4 82.7 2013 2014 77.6 (SRE) Before 2013 2015 2016 Now Figure 2: The performance of state-of-the-art RE models in different years on widely-used dataset SemEval2010 Task 8. The adoption of neural models (since 2013) has brought great improvement in performance. sive neural networks (Socher et al., 2012; Miwa and Bansal, 2016) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zh"
2020.aacl-main.75,D12-1104,0,0.0209936,"section, we introduce the development of RE methods following the typical supervised setting, from early pattern-based methods, statistical approaches, to recent neural models. 2.1 Pattern Extraction Models The pioneering methods use sentence analysis tools to identify syntactic elements in text, then automatically construct pattern rules from these elements (Soderland et al., 1995; Kim and Moldovan, 1995; Huffman, 1995; Califf and Mooney, 1997). In order to extract patterns with better coverage and accuracy, later work involves larger corpora (Carlson et al., 2010), more formats of patterns (Nakashole et al., 2012; Jiang et al., 2017), and more efficient ways of extraction (Zheng et al., 2019). As automatically constructed patterns may have mistakes, most of the above methods require further examinations from human experts, which is the main limitation of pattern-based models. 2.2 Statistical Relation Extraction Models As compared to using pattern rules, statistical methods bring better coverage and require less human efforts. Thus statistical relation extraction (SRE) has been extensively studied. 746 2 Sometimes there is a special class in the relation set indicating that the sentence does not expres"
2020.aacl-main.75,D16-1261,0,0.0146614,"ocuments), the current RE models for this challenge are still crude and straightforward. Followings are some directions worth further investigation: (1) Extracting relations from complicated context is a challenging task requiring reading, memorizing and reasoning for discovering relational facts across multiple sentences. Most of current RE models are still very weak in these abilities. (2) Besides documents, more forms of context is also worth exploring, such as extracting relational facts across documents, or understanding relational information based on heterogeneous data. (3) Inspired by Narasimhan et al. (2016), which utilizes search engines for acquiring external information, automatically searching and analysing context for RE may help RE models identify relational facts with more coverage and become practical for daily scenarios. 3.4 Orienting More Open Domains Most RE systems work within pre-specified relation sets designed by human experts. However, our world undergoes open-ended growth of relations and it is not possible to handle all these emerging Jeﬀ Bezos, an American entrepreneur, graduated from Princeton in 1986. Jeﬀ Bezos graduated from Princeton Figure 8: An example of open information"
2020.aacl-main.75,N07-2032,0,0.130676,"Missing"
2020.aacl-main.75,W15-1506,0,0.0165476,"al., 2017). In this paper, we do not spend too much space for SRE and focus more on neural-based models. 82.4 82.7 2013 2014 77.6 (SRE) Before 2013 2015 2016 Now Figure 2: The performance of state-of-the-art RE models in different years on widely-used dataset SemEval2010 Task 8. The adoption of neural models (since 2013) has brought great improvement in performance. sive neural networks (Socher et al., 2012; Miwa and Bansal, 2016) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE mainly utilizes word embed"
2020.aacl-main.75,P11-2048,0,0.0293116,"fact, there have been various works exploring feasible approaches that lead to better RE abilities on realworld scenarios. In this section, we summarize these exploratory efforts into four directions, and give our review and outlook about these directions. 3.1 Utilizing More Data Supervised NRE models suffer from the lack of large-scale high-quality training data, since manually labeling data is time-consuming and humanintensive. To alleviate this issue, distant supervision (DS) assumption has been used to automatically label data by aligning existing KGs with plain text (Mintz et al., 2009; Nguyen and Moschitti, 2011; Min et al., 2013). As shown in Figure 3, for Model NYT-10 Wiki-Distant PCNN-ONE PCNN-ATT BERT 0.340 0.349 0.458 0.214 0.222 0.361 Table 2: Area under the curve (AUC) of PCNN-ONE (Zeng et al., 2015), PCNN-ATT (Lin et al., 2016) and BERT (Devlin et al., 2019) on two datasets. any entity pair in KGs, sentences mentioning both the entities will be labeled with their corresponding relations in KGs. Large-scale training examples can be easily constructed by this heuristic scheme. Although DS provides a feasible approach to utilize more data, this automatic labeling mechanism is inevitably accompan"
2020.aacl-main.75,C18-1326,0,0.0187317,"(2) Relation discovery, as shown in Figure 9, aims at discovering unseen relation types from unsupervised data. Yao et al. (2011); Marcheggiani and Titov (2016) propose to use generative models and treat these relations as latent variables, while Shinyama and Sekine (2006); Elsahar et al. (2017); Wu et al. (2019) cast relation discovery as a clustering task. Though relation extraction in open domains has been widely studied, there are still lots of unsolved research questions remained to be answered: (1) Canonicalizing relation phrases and arguments in Open IE is crucial for downstream tasks (Niklaus et al., 2018). If not canonicalized, the extracted relational facts could be redundant and ambiguous. For example, Open IE may extract two triples (Barack Obama, was born in, Honolulu) and (Obama, place of birth, Honolulu) indicating an identical fact. Thus, normalizing extracted results will largely benefit the applications of Open IE. There are already some preliminary works in this area (Gal´arraga et al., 2014; 751 Vashishth et al., 2018) and more efforts are needed. (2) The not applicable (N/A) relation has been hardly addressed in relation discovery. In previous work, it is usually assumed that the s"
2020.aacl-main.75,N13-1095,0,0.0126813,"us works exploring feasible approaches that lead to better RE abilities on realworld scenarios. In this section, we summarize these exploratory efforts into four directions, and give our review and outlook about these directions. 3.1 Utilizing More Data Supervised NRE models suffer from the lack of large-scale high-quality training data, since manually labeling data is time-consuming and humanintensive. To alleviate this issue, distant supervision (DS) assumption has been used to automatically label data by aligning existing KGs with plain text (Mintz et al., 2009; Nguyen and Moschitti, 2011; Min et al., 2013). As shown in Figure 3, for Model NYT-10 Wiki-Distant PCNN-ONE PCNN-ATT BERT 0.340 0.349 0.458 0.214 0.222 0.361 Table 2: Area under the curve (AUC) of PCNN-ONE (Zeng et al., 2015), PCNN-ATT (Lin et al., 2016) and BERT (Devlin et al., 2019) on two datasets. any entity pair in KGs, sentences mentioning both the entities will be labeled with their corresponding relations in KGs. Large-scale training examples can be easily constructed by this heuristic scheme. Although DS provides a feasible approach to utilize more data, this automatic labeling mechanism is inevitably accompanied by the wrong la"
2020.aacl-main.75,Q17-1008,0,0.0129385,"here are already some works proposed to extract relations across multiple sentences: 750 (1) Syntactic methods (Wick et al., 2006; Gerber and Chai, 2010; Swampillai and Stevenson, 2011; Yoshikawa et al., 2011; Quirk and Poon, 2017) rely on textual features extracted from various syntactic structures, such as coreference annotations, dependency parsing trees and discourse relations, to connect sentences in documents. (2) Zeng et al. (2017); Christopoulou et al. (2018) build inter-sentence entity graphs, which can utilize multi-hop paths between entities for inferring the correct relations. (3) Peng et al. (2017); Song et al. (2018); Zhu et al. (2019b) employ graph-structured neural networks to model cross-sentence dependencies for relation extraction, which bring in memory and reasoning abilities. To advance this field, some document-level RE datasets have been proposed. Quirk and Poon (2017); Peng et al. (2017) build datasets by DS. Li et al. (2016); Peng et al. (2017) propose datasets for specific domains. Yao et al. (2019) construct a general document-level RE dataset annotated by crowdsourcing workers, suitable for evaluating general-purpose document-level RE systems. Although there are some effo"
2020.aacl-main.75,P09-1113,0,0.736156,"ions (Section 3) targeting more complex RE scenarios. Those feasible approaches leading to better RE abilities still require further efforts, and here we summarize them into four directions: 745 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 745–758 c December 4 - 7, 2020. 2020 Association for Computational Linguistics (1) Utilizing More Data (Section 3.1). Supervised RE methods heavily rely on expensive human annotations, while distant supervision (Mintz et al., 2009) introduces more auto-labeled data to alleviate this issue. Yet distant methods bring noise examples and just utilize single sentences mentioning entity pairs, which significantly weaken extraction performance. Designing schemas to obtain highquality and high-coverage data to train robust RE models still remains a problem to be explored. (2) Performing More Efficient Learning (Section 3.2). Lots of long-tail relations only contain a handful of training examples. However, it is hard for conventional RE methods to well generalize relation patterns from limited examples like humans. Therefore, de"
2020.aacl-main.75,E17-1110,0,0.0240382,"s many entities exhibiting complex crosssentence relations. Most existing methods focus on intra-sentence RE and thus are inadequate for collectively identifying these relational facts expressed in a long paragraph. In fact, most relational facts can only be extracted from complicated context like documents rather than single sentences (Yao et al., 2019), which should not be neglected. There are already some works proposed to extract relations across multiple sentences: 750 (1) Syntactic methods (Wick et al., 2006; Gerber and Chai, 2010; Swampillai and Stevenson, 2011; Yoshikawa et al., 2011; Quirk and Poon, 2017) rely on textual features extracted from various syntactic structures, such as coreference annotations, dependency parsing trees and discourse relations, to connect sentences in documents. (2) Zeng et al. (2017); Christopoulou et al. (2018) build inter-sentence entity graphs, which can utilize multi-hop paths between entities for inferring the correct relations. (3) Peng et al. (2017); Song et al. (2018); Zhu et al. (2019b) employ graph-structured neural networks to model cross-sentence dependencies for relation extraction, which bring in memory and reasoning abilities. To advance this field,"
2020.aacl-main.75,D16-1252,0,0.0244806,"t in 2014. Figure 9: An example of clustering-based relation discovery, which identifying potential relation types by clustering unlabeled relational instances. relation types only by humans. Thus, we need RE systems that do not rely on pre-defined relation schemas and can work in open scenarios. There are already some explorations in handling open relations: (1) Open information extraction (Open IE), as shown in Figure 8, extracts relation phrases and arguments (entities) from text (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018). Open IE does not rely on specific relation types and thus can handle all kinds of relational facts. (2) Relation discovery, as shown in Figure 9, aims at discovering unseen relation types from unsupervised data. Yao et al. (2011); Marcheggiani and Titov (2016) propose to use generative models and treat these relations as latent variables, while Shinyama and Sekine (2006); Elsahar et al. (2017); Wu et al. (2019) cast relation discovery as a clustering task. Though relation extraction in open domains has been widely studied, there are still lots of unsolved res"
2020.aacl-main.75,D12-1042,0,0.13543,"accompanied by the wrong labeling problem. The reason is that not all sentences mentioning the two entities express their relations in KGs exactly. For example, we may mistakenly label “Bill Gates retired from Microsoft” with the relation founder, if (Bill Gates, founder, Microsoft) is a relational fact in KGs. The existing methods to alleviate the noise problem can be divided into three major approaches: (1) Some methods adopt multi-instance learning by combining sentences with same entity pairs and then selecting informative instances from them. Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) utilize graphical model to infer the informative sentences, while Zeng et al. (2015) use a simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora"
2020.aacl-main.75,N13-1008,0,0.101883,"ther statistical methods focusing on extracting and inferring the latent information hidden in the text. Graphical methods (Roth and Yih, 2002, 2004; Sarawagi and Cohen, 2005; Yu and Lam, 2010) abstract the dependencies between entities, text and relations in the form of directed acyclic graphs, and then use inference models to identify the correct relations. Inspired by the success of embedding models in other NLP tasks (Mikolov et al., 2013a,b), there are also efforts in encoding text into low-dimensional semantic spaces and extracting relations from textual embeddings (Weston et al., 2013; Riedel et al., 2013; Gormley et al., 2015). Furthermore, Bordes et al. (2013),Wang et al. (2014) and Lin et al. (2015) utilize KG embeddings for RE. Although SRE has been widely studied, it still faces some challenges. Feature-based and kernelbased models require many efforts to design features or kernel functions. While graphical and embedding methods can predict relations without too much human intervention, they are still limited in model capacities. There are some surveys systematically introducing SRE models (Zelenko et al., 2003; Bach and Badaskar, 2007; Pawar et al., 2017). In this paper, we do not spend"
2020.aacl-main.75,C02-1151,0,0.535663,"s mentioned in this work are collected into the following paper list https://github. com/thunlp/NREPapers. † to researching relation extraction (RE), which aims at extracting relational facts from plain text. More specifically, after identifying entity mentions (e.g., USA and New York) in text, the main goal of RE is to classify relations (e.g., contains) between these entity mentions from their context. The pioneering explorations of RE lie in statistical approaches, such as pattern mining (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004) and graphical models (Roth and Yih, 2002). Recently, with the development of deep learning, neural models have been widely adopted for RE (Zeng et al., 2014; Zhang et al., 2015) and achieved superior results. These RE methods have bridged the gap between unstructured text and structured knowledge, and shown their effectiveness on several public benchmarks. Despite the success of existing RE methods, most of them still work in a simplified setting. These methods mainly focus on training models with large amounts of human annotations to classify two given entities within one sentence into pre-defined relations. However, the real world"
2020.aacl-main.75,W04-2401,0,0.331956,"Missing"
2020.aacl-main.75,P15-1061,0,0.0207109,"askar, 2007; Pawar et al., 2017). In this paper, we do not spend too much space for SRE and focus more on neural-based models. 82.4 82.7 2013 2014 77.6 (SRE) Before 2013 2015 2016 Now Figure 2: The performance of state-of-the-art RE models in different years on widely-used dataset SemEval2010 Task 8. The adoption of neural models (since 2013) has brought great improvement in performance. sive neural networks (Socher et al., 2012; Miwa and Bansal, 2016) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE"
2020.aacl-main.75,R11-1004,0,0.0168865,"ext As shown in Figure 7, one document generally mentions many entities exhibiting complex crosssentence relations. Most existing methods focus on intra-sentence RE and thus are inadequate for collectively identifying these relational facts expressed in a long paragraph. In fact, most relational facts can only be extracted from complicated context like documents rather than single sentences (Yao et al., 2019), which should not be neglected. There are already some works proposed to extract relations across multiple sentences: 750 (1) Syntactic methods (Wick et al., 2006; Gerber and Chai, 2010; Swampillai and Stevenson, 2011; Yoshikawa et al., 2011; Quirk and Poon, 2017) rely on textual features extracted from various syntactic structures, such as coreference annotations, dependency parsing trees and discourse relations, to connect sentences in documents. (2) Zeng et al. (2017); Christopoulou et al. (2018) build inter-sentence entity graphs, which can utilize multi-hop paths between entities for inferring the correct relations. (3) Peng et al. (2017); Song et al. (2018); Zhu et al. (2019b) employ graph-structured neural networks to model cross-sentence dependencies for relation extraction, which bring in memory a"
2020.aacl-main.75,P10-1040,0,0.00513333,", recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE mainly utilizes word embeddings and position embeddings instead of hand-craft features as inputs. Word embeddings (Turian et al., 2010; Mikolov et al., 2013b) are the most used input representations in NLP, which encode the semantic meaning of words into vectors. In order to capture the entity information in text, position embeddings (Zeng et al., 2014) are introduced to specify the relative distances between words and entities. Except for word embeddings and position embeddings, there are also other works integrating syntactic information into NRE models. Xu et al. (2015a) and Xu et al. (2015b) adopt CNNs and RNNs over shortest dependency paths respectively. Liu et al. (2015) propose a recursive neural network based on augm"
2020.aacl-main.75,N16-1103,0,0.0654802,"tences, while Zeng et al. (2015) use a simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora for the information consistency and complementarity (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018). (3) Many methods tend to utilize sophisticated 748 Relation Distribution on NYT-10 104 104 Numbers of Instances Numbers of Instances Supporting Set Relation Distribution on Wiki-Distant 105 103 102 101 100 iPhone is designed by Apple Inc. product 103 Steve Jobs is the co-founder of Apple Inc. 102 Tim Cook is Apple’s current CEO. founder CEO Query Instance 101 Bill Gates founded Microsoft. ? 100 0 10 20 30 Relations 40 0 100 200 300 Relations 400 founder Figure 4: Relation distributions (log-scale) on the training part of DS datasets NYT-10 and Wiki-Dista"
2020.aacl-main.75,W16-1312,0,0.049412,"Missing"
2020.aacl-main.75,N18-1080,0,0.0617987,"ces, and proportions of N/A instances respectively. iPhone is designed by Apple Inc. iPhone is a iconic product of Apple. Dataset Tim Cook I looked up Apple Inc. on my iPhone. Figure 3: An example of distantly supervised relation extraction. With the fact (Apple Inc., product, iPhone), DS finds all sentences mentioning the two entities and annotates them with the relation product, which inevitably brings noise labels. 2016; Riedel et al., 2013). Recently, Transformers (Vaswani et al., 2017) and pre-trained language models (Devlin et al., 2019) have also been explored for NRE (Du et al., 2018; Verga et al., 2018; Wu and He, 2019; Baldini Soares et al., 2019) and have achieved new state-of-the-arts. By concisely reviewing the above techniques, we are able to track the development of RE from pattern and statistical methods to neural models. Comparing the performance of state-of-the-art RE models in years (Figure 2), we can see the vast increase since the emergence of NRE, which demonstrates the power of neural methods. 3 “More” Directions for RE Although the above-mentioned NRE models have achieved superior results on benchmarks, they are still far from solving the problem of RE. Most of these models u"
2020.aacl-main.75,N06-1039,0,0.0823614,"shown in Figure 8, extracts relation phrases and arguments (entities) from text (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018). Open IE does not rely on specific relation types and thus can handle all kinds of relational facts. (2) Relation discovery, as shown in Figure 9, aims at discovering unseen relation types from unsupervised data. Yao et al. (2011); Marcheggiani and Titov (2016) propose to use generative models and treat these relations as latent variables, while Shinyama and Sekine (2006); Elsahar et al. (2017); Wu et al. (2019) cast relation discovery as a clustering task. Though relation extraction in open domains has been widely studied, there are still lots of unsolved research questions remained to be answered: (1) Canonicalizing relation phrases and arguments in Open IE is crucial for downstream tasks (Niklaus et al., 2018). If not canonicalized, the extracted relational facts could be redundant and ambiguous. For example, Open IE may extract two triples (Barack Obama, was born in, Honolulu) and (Obama, place of birth, Honolulu) indicating an identical fact. Thus, normal"
2020.aacl-main.75,D12-1110,0,0.0508116,"relations without too much human intervention, they are still limited in model capacities. There are some surveys systematically introducing SRE models (Zelenko et al., 2003; Bach and Badaskar, 2007; Pawar et al., 2017). In this paper, we do not spend too much space for SRE and focus more on neural-based models. 82.4 82.7 2013 2014 77.6 (SRE) Before 2013 2015 2016 Now Figure 2: The performance of state-of-the-art RE models in different years on widely-used dataset SemEval2010 Task 8. The adoption of neural models (since 2013) has brought great improvement in performance. sive neural networks (Socher et al., 2012; Miwa and Bansal, 2016) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-b"
2020.aacl-main.75,N16-1065,0,0.147165,"-the-art RE models in different years on widely-used dataset SemEval2010 Task 8. The adoption of neural models (since 2013) has brought great improvement in performance. sive neural networks (Socher et al., 2012; Miwa and Bansal, 2016) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE mainly utilizes word embeddings and position embeddings instead of hand-craft features as inputs. Word embeddings (Turian et al., 2010; Mikolov et al., 2013b) are the most used input representations in NLP, which enc"
2020.aacl-main.75,P16-1123,1,0.90021,"Missing"
2020.aacl-main.75,D18-1246,0,0.0377077,"Missing"
2020.aacl-main.75,I08-2119,0,0.0392005,"thods (Kambhatla, 2004; Zhou et al., 2005; Jiang and Zhai, 2007; Nguyen et al., 2007), which design lexical, syntactic and semantic features for entity pairs and their corresponding context, and then input these features into relation classifiers. Due to the wide use of support vector machines (SVM), kernel-based methods have been widely explored, which design kernel functions for SVM to measure the similarities between relation representations and textual instances (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Mooney and Bunescu, 2006; Zhang et al., 2006b,a; Wang, 2008). There are also some other statistical methods focusing on extracting and inferring the latent information hidden in the text. Graphical methods (Roth and Yih, 2002, 2004; Sarawagi and Cohen, 2005; Yu and Lam, 2010) abstract the dependencies between entities, text and relations in the form of directed acyclic graphs, and then use inference models to identify the correct relations. Inspired by the success of embedding models in other NLP tasks (Mikolov et al., 2013a,b), there are also efforts in encoding text into low-dimensional semantic spaces and extracting relations from textual embeddings"
2020.aacl-main.75,C18-1099,1,0.853339,"simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora for the information consistency and complementarity (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018). (3) Many methods tend to utilize sophisticated 748 Relation Distribution on NYT-10 104 104 Numbers of Instances Numbers of Instances Supporting Set Relation Distribution on Wiki-Distant 105 103 102 101 100 iPhone is designed by Apple Inc. product 103 Steve Jobs is the co-founder of Apple Inc. 102 Tim Cook is Apple’s current CEO. founder CEO Query Instance 101 Bill Gates founded Microsoft. ? 100 0 10 20 30 Relations 40 0 100 200 300 Relations 400 founder Figure 4: Relation distributions (log-scale) on the training part of DS datasets NYT-10 and Wiki-Distant, suggesting that real-world relatio"
2020.aacl-main.75,D11-1135,0,0.0169809,"n work in open scenarios. There are already some explorations in handling open relations: (1) Open information extraction (Open IE), as shown in Figure 8, extracts relation phrases and arguments (entities) from text (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018). Open IE does not rely on specific relation types and thus can handle all kinds of relational facts. (2) Relation discovery, as shown in Figure 9, aims at discovering unseen relation types from unsupervised data. Yao et al. (2011); Marcheggiani and Titov (2016) propose to use generative models and treat these relations as latent variables, while Shinyama and Sekine (2006); Elsahar et al. (2017); Wu et al. (2019) cast relation discovery as a clustering task. Though relation extraction in open domains has been widely studied, there are still lots of unsolved research questions remained to be answered: (1) Canonicalizing relation phrases and arguments in Open IE is crucial for downstream tasks (Niklaus et al., 2018). If not canonicalized, the extracted relational facts could be redundant and ambiguous. For example, Open I"
2020.aacl-main.75,P19-1074,1,0.902979,"dels may overfit simple textual cues between relations instead of really understanding the semantics of the context. More details about the experiments are in Appendix A. 3.3 Handling More Complicated Context As shown in Figure 7, one document generally mentions many entities exhibiting complex crosssentence relations. Most existing methods focus on intra-sentence RE and thus are inadequate for collectively identifying these relational facts expressed in a long paragraph. In fact, most relational facts can only be extracted from complicated context like documents rather than single sentences (Yao et al., 2019), which should not be neglected. There are already some works proposed to extract relations across multiple sentences: 750 (1) Syntactic methods (Wick et al., 2006; Gerber and Chai, 2010; Swampillai and Stevenson, 2011; Yoshikawa et al., 2011; Quirk and Poon, 2017) rely on textual features extracted from various syntactic structures, such as coreference annotations, dependency parsing trees and discourse relations, to connect sentences in documents. (2) Zeng et al. (2017); Christopoulou et al. (2018) build inter-sentence entity graphs, which can utilize multi-hop paths between entities for inf"
2020.aacl-main.75,D13-1136,0,0.0269434,"There are also some other statistical methods focusing on extracting and inferring the latent information hidden in the text. Graphical methods (Roth and Yih, 2002, 2004; Sarawagi and Cohen, 2005; Yu and Lam, 2010) abstract the dependencies between entities, text and relations in the form of directed acyclic graphs, and then use inference models to identify the correct relations. Inspired by the success of embedding models in other NLP tasks (Mikolov et al., 2013a,b), there are also efforts in encoding text into low-dimensional semantic spaces and extracting relations from textual embeddings (Weston et al., 2013; Riedel et al., 2013; Gormley et al., 2015). Furthermore, Bordes et al. (2013),Wang et al. (2014) and Lin et al. (2015) utilize KG embeddings for RE. Although SRE has been widely studied, it still faces some challenges. Feature-based and kernelbased models require many efforts to design features or kernel functions. While graphical and embedding methods can predict relations without too much human intervention, they are still limited in model capacities. There are some surveys systematically introducing SRE models (Zelenko et al., 2003; Bach and Badaskar, 2007; Pawar et al., 2017). In this pa"
2020.aacl-main.75,P19-1277,0,0.0237425,"Missing"
2020.aacl-main.75,W06-1671,0,0.0514824,"Missing"
2020.aacl-main.75,D19-1021,1,0.854571,"rguments (entities) from text (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018). Open IE does not rely on specific relation types and thus can handle all kinds of relational facts. (2) Relation discovery, as shown in Figure 9, aims at discovering unseen relation types from unsupervised data. Yao et al. (2011); Marcheggiani and Titov (2016) propose to use generative models and treat these relations as latent variables, while Shinyama and Sekine (2006); Elsahar et al. (2017); Wu et al. (2019) cast relation discovery as a clustering task. Though relation extraction in open domains has been widely studied, there are still lots of unsolved research questions remained to be answered: (1) Canonicalizing relation phrases and arguments in Open IE is crucial for downstream tasks (Niklaus et al., 2018). If not canonicalized, the extracted relational facts could be redundant and ambiguous. For example, Open IE may extract two triples (Barack Obama, was born in, Honolulu) and (Obama, place of birth, Honolulu) indicating an identical fact. Thus, normalizing extracted results will largely bene"
2020.aacl-main.75,D17-1187,0,0.0159463,"der Figure 4: Relation distributions (log-scale) on the training part of DS datasets NYT-10 and Wiki-Distant, suggesting that real-world relation distributions suffer from the long-tail problem. mechanisms and training strategies to enhance distantly supervised NRE models. Vu et al. (2016); Beltagy et al. (2019) combine different architectures and training strategies to construct hybrid frameworks. Liu et al. (2017) incorporate a softlabel scheme by changing unconfident labels during training. Furthermore, reinforcement learning (Feng et al., 2018; Zeng et al., 2018) and adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a) have also been adopted in DS. The researchers have formed a consensus that utilizing more data is a potential way towards more powerful RE models, and there still remains some open problems worth exploring: (1) Existing DS methods focus on denoising auto-labeled instances and it is certainly meaningful to follow this research direction. Besides, current DS schemes are still similar to the original one in (Mintz et al., 2009), which just covers the case that the entity pairs are mentioned in the same sentences. To achieve better coverage and less noise, e"
2020.aacl-main.75,C16-1119,0,0.0178955,"tions for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE mainly utilizes word embeddings and position embeddings instead of hand-craft features as inputs. Word embeddings (Turian et al., 2010; Mikolov et al., 2013b) are the most used input representations in NLP, which encode the semantic meaning of words into vectors. In order to capture the entity information in text, position embeddings (Zeng et al., 2014) are introduced to specify the relative distances between words and entities. Except for word embeddings and position embeddings, there"
2020.aacl-main.75,D15-1062,0,0.0540036,"Missing"
2020.aacl-main.75,C16-1138,0,0.0498916,"Missing"
2020.aacl-main.75,D15-1206,1,0.888587,"Missing"
2020.aacl-main.75,C10-2160,0,0.0215937,"e features into relation classifiers. Due to the wide use of support vector machines (SVM), kernel-based methods have been widely explored, which design kernel functions for SVM to measure the similarities between relation representations and textual instances (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Mooney and Bunescu, 2006; Zhang et al., 2006b,a; Wang, 2008). There are also some other statistical methods focusing on extracting and inferring the latent information hidden in the text. Graphical methods (Roth and Yih, 2002, 2004; Sarawagi and Cohen, 2005; Yu and Lam, 2010) abstract the dependencies between entities, text and relations in the form of directed acyclic graphs, and then use inference models to identify the correct relations. Inspired by the success of embedding models in other NLP tasks (Mikolov et al., 2013a,b), there are also efforts in encoding text into low-dimensional semantic spaces and extracting relations from textual embeddings (Weston et al., 2013; Riedel et al., 2013; Gormley et al., 2015). Furthermore, Bordes et al. (2013),Wang et al. (2014) and Lin et al. (2015) utilize KG embeddings for RE. Although SRE has been widely studied, it sti"
2020.aacl-main.75,D15-1203,0,0.0647772,"we do not spend too much space for SRE and focus more on neural-based models. 82.4 82.7 2013 2014 77.6 (SRE) Before 2013 2015 2016 Now Figure 2: The performance of state-of-the-art RE models in different years on widely-used dataset SemEval2010 Task 8. The adoption of neural models (since 2013) has brought great improvement in performance. sive neural networks (Socher et al., 2012; Miwa and Bansal, 2016) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE mainly utilizes word embeddings and position e"
2020.aacl-main.75,C14-1220,0,0.226372,"rching relation extraction (RE), which aims at extracting relational facts from plain text. More specifically, after identifying entity mentions (e.g., USA and New York) in text, the main goal of RE is to classify relations (e.g., contains) between these entity mentions from their context. The pioneering explorations of RE lie in statistical approaches, such as pattern mining (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004) and graphical models (Roth and Yih, 2002). Recently, with the development of deep learning, neural models have been widely adopted for RE (Zeng et al., 2014; Zhang et al., 2015) and achieved superior results. These RE methods have bridged the gap between unstructured text and structured knowledge, and shown their effectiveness on several public benchmarks. Despite the success of existing RE methods, most of them still work in a simplified setting. These methods mainly focus on training models with large amounts of human annotations to classify two given entities within one sentence into pre-defined relations. However, the real world is much more complicated than this simple setting: (1) collecting high-quality human annotations is expensive and t"
2020.aacl-main.75,D17-1186,1,0.860499,". In fact, most relational facts can only be extracted from complicated context like documents rather than single sentences (Yao et al., 2019), which should not be neglected. There are already some works proposed to extract relations across multiple sentences: 750 (1) Syntactic methods (Wick et al., 2006; Gerber and Chai, 2010; Swampillai and Stevenson, 2011; Yoshikawa et al., 2011; Quirk and Poon, 2017) rely on textual features extracted from various syntactic structures, such as coreference annotations, dependency parsing trees and discourse relations, to connect sentences in documents. (2) Zeng et al. (2017); Christopoulou et al. (2018) build inter-sentence entity graphs, which can utilize multi-hop paths between entities for inferring the correct relations. (3) Peng et al. (2017); Song et al. (2018); Zhu et al. (2019b) employ graph-structured neural networks to model cross-sentence dependencies for relation extraction, which bring in memory and reasoning abilities. To advance this field, some document-level RE datasets have been proposed. Quirk and Poon (2017); Peng et al. (2017) build datasets by DS. Li et al. (2016); Peng et al. (2017) propose datasets for specific domains. Yao et al. (2019) c"
2020.aacl-main.75,N06-1037,0,0.077591,"ach is feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Jiang and Zhai, 2007; Nguyen et al., 2007), which design lexical, syntactic and semantic features for entity pairs and their corresponding context, and then input these features into relation classifiers. Due to the wide use of support vector machines (SVM), kernel-based methods have been widely explored, which design kernel functions for SVM to measure the similarities between relation representations and textual instances (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Mooney and Bunescu, 2006; Zhang et al., 2006b,a; Wang, 2008). There are also some other statistical methods focusing on extracting and inferring the latent information hidden in the text. Graphical methods (Roth and Yih, 2002, 2004; Sarawagi and Cohen, 2005; Yu and Lam, 2010) abstract the dependencies between entities, text and relations in the form of directed acyclic graphs, and then use inference models to identify the correct relations. Inspired by the success of embedding models in other NLP tasks (Mikolov et al., 2013a,b), there are also efforts in encoding text into low-dimensional semantic spaces and extracting relations from te"
2020.aacl-main.75,P06-1104,0,0.0618459,"ach is feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Jiang and Zhai, 2007; Nguyen et al., 2007), which design lexical, syntactic and semantic features for entity pairs and their corresponding context, and then input these features into relation classifiers. Due to the wide use of support vector machines (SVM), kernel-based methods have been widely explored, which design kernel functions for SVM to measure the similarities between relation representations and textual instances (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Mooney and Bunescu, 2006; Zhang et al., 2006b,a; Wang, 2008). There are also some other statistical methods focusing on extracting and inferring the latent information hidden in the text. Graphical methods (Roth and Yih, 2002, 2004; Sarawagi and Cohen, 2005; Yu and Lam, 2010) abstract the dependencies between entities, text and relations in the form of directed acyclic graphs, and then use inference models to identify the correct relations. Inspired by the success of embedding models in other NLP tasks (Mikolov et al., 2013a,b), there are also efforts in encoding text into low-dimensional semantic spaces and extracting relations from te"
2020.aacl-main.75,N19-1306,0,0.067956,"hem. Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) utilize graphical model to infer the informative sentences, while Zeng et al. (2015) use a simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora for the information consistency and complementarity (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018). (3) Many methods tend to utilize sophisticated 748 Relation Distribution on NYT-10 104 104 Numbers of Instances Numbers of Instances Supporting Set Relation Distribution on Wiki-Distant 105 103 102 101 100 iPhone is designed by Apple Inc. product 103 Steve Jobs is the co-founder of Apple Inc. 102 Tim Cook is Apple’s current CEO. founder CEO Query Instance 101 Bill Gates founded Microsoft. ? 100 0 10 20 30 Relations 40 0 100 200 30"
2020.aacl-main.75,Y15-1009,0,0.0785539,"raction (RE), which aims at extracting relational facts from plain text. More specifically, after identifying entity mentions (e.g., USA and New York) in text, the main goal of RE is to classify relations (e.g., contains) between these entity mentions from their context. The pioneering explorations of RE lie in statistical approaches, such as pattern mining (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004) and graphical models (Roth and Yih, 2002). Recently, with the development of deep learning, neural models have been widely adopted for RE (Zeng et al., 2014; Zhang et al., 2015) and achieved superior results. These RE methods have bridged the gap between unstructured text and structured knowledge, and shown their effectiveness on several public benchmarks. Despite the success of existing RE methods, most of them still work in a simplified setting. These methods mainly focus on training models with large amounts of human annotations to classify two given entities within one sentence into pre-defined relations. However, the real world is much more complicated than this simple setting: (1) collecting high-quality human annotations is expensive and time-consuming, (2) ma"
2020.aacl-main.75,D18-1244,0,0.013069,"since 2013) has brought great improvement in performance. sive neural networks (Socher et al., 2012; Miwa and Bansal, 2016) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE mainly utilizes word embeddings and position embeddings instead of hand-craft features as inputs. Word embeddings (Turian et al., 2010; Mikolov et al., 2013b) are the most used input representations in NLP, which encode the semantic meaning of words into vectors. In order to capture the entity information in text, position embedd"
2020.aacl-main.75,D17-1004,0,0.217937,"abel “Bill Gates retired from Microsoft” with the relation founder, if (Bill Gates, founder, Microsoft) is a relational fact in KGs. The existing methods to alleviate the noise problem can be divided into three major approaches: (1) Some methods adopt multi-instance learning by combining sentences with same entity pairs and then selecting informative instances from them. Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) utilize graphical model to infer the informative sentences, while Zeng et al. (2015) use a simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora for the information consistency and complementarity (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018). (3) Many methods tend to utilize sophisticated 748 Relation Distri"
2020.aacl-main.75,P19-1139,1,0.91764,"hem. Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) utilize graphical model to infer the informative sentences, while Zeng et al. (2015) use a simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora for the information consistency and complementarity (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018). (3) Many methods tend to utilize sophisticated 748 Relation Distribution on NYT-10 104 104 Numbers of Instances Numbers of Instances Supporting Set Relation Distribution on Wiki-Distant 105 103 102 101 100 iPhone is designed by Apple Inc. product 103 Steve Jobs is the co-founder of Apple Inc. 102 Tim Cook is Apple’s current CEO. founder CEO Query Instance 101 Bill Gates founded Microsoft. ? 100 0 10 20 30 Relations 40 0 100 200 30"
2020.aacl-main.75,P05-1052,0,0.0462651,"Missing"
2020.aacl-main.75,P19-1137,1,0.923863,"setting, from early pattern-based methods, statistical approaches, to recent neural models. 2.1 Pattern Extraction Models The pioneering methods use sentence analysis tools to identify syntactic elements in text, then automatically construct pattern rules from these elements (Soderland et al., 1995; Kim and Moldovan, 1995; Huffman, 1995; Califf and Mooney, 1997). In order to extract patterns with better coverage and accuracy, later work involves larger corpora (Carlson et al., 2010), more formats of patterns (Nakashole et al., 2012; Jiang et al., 2017), and more efficient ways of extraction (Zheng et al., 2019). As automatically constructed patterns may have mistakes, most of the above methods require further examinations from human experts, which is the main limitation of pattern-based models. 2.2 Statistical Relation Extraction Models As compared to using pattern rules, statistical methods bring better coverage and require less human efforts. Thus statistical relation extraction (SRE) has been extensively studied. 746 2 Sometimes there is a special class in the relation set indicating that the sentence does not express any pre-specified relation (usually named as N/A). 2.3 Neural Relation Extracti"
2020.aacl-main.75,P05-1053,0,0.264276,"Missing"
2020.aacl-main.75,P16-2034,0,0.0170838,"6) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE mainly utilizes word embeddings and position embeddings instead of hand-craft features as inputs. Word embeddings (Turian et al., 2010; Mikolov et al., 2013b) are the most used input representations in NLP, which encode the semantic meaning of words into vectors. In order to capture the entity information in text, position embeddings (Zeng et al., 2014) are introduced to specify the relative distances between words and entities. Except for word em"
2020.aacl-main.75,P19-1128,1,0.923461,"ght great improvement in performance. sive neural networks (Socher et al., 2012; Miwa and Bansal, 2016) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE mainly utilizes word embeddings and position embeddings instead of hand-craft features as inputs. Word embeddings (Turian et al., 2010; Mikolov et al., 2013b) are the most used input representations in NLP, which encode the semantic meaning of words into vectors. In order to capture the entity information in text, position embeddings (Zeng et al.,"
2020.aacl-main.75,N07-1015,0,\N,Missing
2020.aacl-main.75,D14-1067,0,\N,Missing
2020.aacl-main.75,C96-1079,0,\N,Missing
2020.aacl-main.75,P10-1160,0,\N,Missing
2020.aacl-main.75,P04-1054,0,\N,Missing
2020.aacl-main.75,P11-1055,0,\N,Missing
2020.aacl-main.75,H05-1091,0,\N,Missing
2020.aacl-main.75,D11-1142,0,\N,Missing
2020.aacl-main.75,P15-1034,0,\N,Missing
2020.aacl-main.75,P16-1072,0,\N,Missing
2020.aacl-main.75,P18-2014,0,\N,Missing
2020.aacl-main.75,D18-1245,0,\N,Missing
2020.aacl-main.75,P18-2065,0,\N,Missing
2020.aacl-main.75,N19-1184,0,\N,Missing
2020.aacl-main.75,D17-1191,0,\N,Missing
2020.aacl-main.75,N19-1423,0,\N,Missing
2020.aacl-main.75,D19-1395,0,\N,Missing
2020.aacl-main.75,D19-3029,1,\N,Missing
2020.aacl-main.75,D19-1649,1,\N,Missing
2020.aacl-main.77,N19-1423,0,0.00777534,"ying graph based ranking is SEAL (Wang and Cohen, 2007) • SEISA. SEISA (He and Xin, 2011) is an entity set expansion system developed by Microsoft after SEAL and outperforms traditional graph-based methods by an original unsupervised similarity metric. We implement its Dynamic Thresholding algorithm to sort expanded concepts. • EMB. Embedding based method mainly utilizes context information to examine the similarity between expanded concepts and seeds according to (Mamou et al., 2018). For each expanded concept e, we calculate the sum of its cosine similarities with course concepts M in BERT (Devlin et al., 2019) and use the average as golden standard to rank the expanded concept list. • PUL. PU learning is a semi-supervised learning model regarding set expansion as a binary classification task. We employ the same setting as (Wang et al., 2017) to classify and sort concepts. • PIP. It is a pipeline method for course concept expansion (Yu et al., 2019a), which first uses an online clustering method during candidate generation and then classify them to obtain final expansion results. We follow the workflow of this work to sort expanded concepts. 4.1.4 Evaluation Metrics Our objective is to generate a ra"
2020.aacl-main.77,I17-1088,1,0.731139,"ed as N t the likelihood can be formalized as P (ct0 ∈ N t ⊂ Lt |K t , t0 ). The expansion set is refreshed as Ect+1 = Ect ∪ N t until its size reaches the preset upper limit τ or cannot find new candidates (He and Xin, 2011). 2.3 Preliminaries 2.1 base KB as an external source, the task is to return a ranked list of expanded concepts Ec . In this formulation, a course corpus is defined as D = {C j }|n| , which is composed of n courses’ j=1 video subtitles in the same subject area. Course concepts are the subjects taught in the course (such |M| as LSTM in Figure 1), denoted as M = {ci }i=1 . (Pan et al., 2017). Knowledge base KB = (E, R) is consist of concepts E and relations R, which is utilized as an external source to obtain expansion candidates. Though other source (such as Web tables) can also take on this role, we still employ a KB to search for expansion candidates like the prior work, i.e., Ec ⊂ E. Interactive MOOC Environment The workflow above has been experimentally proven to be effective in many concept expansion tasks (Shen et al., 2018; Rastogi et al., 2019). However, such methods only consider the course concepts’ semantic information, which makes their expansion results hard to matc"
2020.aacl-main.77,C10-1112,0,0.0130339,"(The overlap rises from 0.005 to 0.091), which indicates that our model can provide more high-quality concepts. 5 Related Work Our work follows the task of concept expansion in MOOCs (Yu et al., 2019a), a particular type of set expansion problem, which takes several seeds as input and expands the entity set. Set expansion was born to serve knowledge acquisition applications on the Internet. Google Sets was a pioneer which leaded a series of early research, e.g. Bayesian Sets (Ghahramani and Heller, 2006), SEAL (Wang and Cohen, 2007), SEISA (He and Xin, 2011) and others (Sarmento et al., 2007; Shi et al., 2010; Wang et al., 2015). These efforts utilize web tables as a resource and mainly serves for search engines. Recently, more related research has turned its attention to other application fields, such as news mining (Redondo-Garc´ıa et al., 2014), knowledge graphs (Zhang et al., 2017), education assistance (Yu et al., 2019a), etc. Meanwhile, corpus-based expansion methods snowball, and iterative bootstrapping became a common solution (Shen et al., 2017; Yu et al., 2019b; Yan et al., 2019), which expands the set in round and select high-quality results to extract feature iteratively. ExpanRL is in"
2020.aacl-main.77,D18-2004,0,0.062772,"24 user behaviors from a real MOOC website. 2 Problem Formulation Following (Yu et al., 2019a), Course Concept Expansion is formally defined as: given the course corpus D, course concepts M, and a knowledge 2 A course from the University of London in Coursera. 2.2 Basic Model for Concept Expansion The general idea of concept expansion is first to characterize the concept set according to its representative elements, then find new candidates and rank them to expand the set. Seed Selection Stage. A group of representative concepts are called seeds and formalized to K ⊂ Ec (Wang and Cohen, 2007; Mamou et al., 2018). While the expansion process is often carried out iteratively, we also formalize the expansion set of round t to Ect . Seed selection is to calculate the possibility that each concept in Ect becomes a seed, i.e., P (ci ∈ K t ⊂ Ect |t), where K t contains the seeds of t-th round. Based on these seeds, we can extract features of the current set and search for candidate concepts for expansion from external sources. Expansion Stage. After finding a new list of candi t dates L = c1 , ..., ct0 , ..., c|Lt |, expansion stage aims to calculate the likelihood of ct0 to be a expanded concept. The top"
2020.aacl-main.77,D17-1059,0,0.0276969,"larity metric. We implement its Dynamic Thresholding algorithm to sort expanded concepts. • EMB. Embedding based method mainly utilizes context information to examine the similarity between expanded concepts and seeds according to (Mamou et al., 2018). For each expanded concept e, we calculate the sum of its cosine similarities with course concepts M in BERT (Devlin et al., 2019) and use the average as golden standard to rank the expanded concept list. • PUL. PU learning is a semi-supervised learning model regarding set expansion as a binary classification task. We employ the same setting as (Wang et al., 2017) to classify and sort concepts. • PIP. It is a pipeline method for course concept expansion (Yu et al., 2019a), which first uses an online clustering method during candidate generation and then classify them to obtain final expansion results. We follow the workflow of this work to sort expanded concepts. 4.1.4 Evaluation Metrics Our objective is to generate a ranked list of expanded concepts. Thus, we use the Mean Average Precision(MAP) as our evaluation metric, which is the preferred metric in information retrieval for evaluating ranked lists. 4.2 Overall Evaluation Table 2 summarizes the com"
2020.aacl-main.77,D19-1028,0,0.0601988,"Missing"
2020.aacl-main.77,P19-1421,1,0.5905,"lassrooms, these concepts are often considerately introduced by teachers. ∗ 1 Corresponding author. https://www.coursera.org However, in the era of Massive Open Online Courses (MOOCs), thousands of courses are prerecorded for with millions of students with various backgrounds (Shah, 2019), which makes it infeasible to pick out these essential concepts manually. Therefore, there is a clear need to automatically discover course-related concepts so that they can easily acquire additional knowledge and achieve better educational outcomes. This task is formally defined as Course Concept Expansion (Yu et al., 2019a), a special type of Concept Expansion or Set Expansion (Wang and Cohen, 2007), which refers to the task of expanding a small set of seed concepts into a complete set of concepts that belong to the same course or subject from external resources. Despite abundant efforts in related topics (He and Xin, 2011; Shen et al., 2017; Yan et al., 2019), existing methods still face three challenges when applied to MOOCs. First, distinct from the task of enriching a certain concept set, the purpose of course concept expansion is to benefit students’ learning, making the context information insufficient t"
2020.acl-main.100,C18-1057,1,0.889971,"Missing"
2020.acl-main.100,D18-1021,1,0.877217,"Missing"
2020.acl-main.100,D19-1025,1,0.824252,"Missing"
2020.acl-main.100,P17-1149,1,0.869886,"Missing"
2020.acl-main.100,E99-1042,0,0.410389,"expertise level of text, which is also a key difference from conventional styles. We identify two major types of knowledge gaps in MSD: terminology, e.g., dyspnea in the first example; and empirical evidence. As shown in the third pair, doctors prefer to use statistics (About 1/1000), while laymen do not (quite small). Lexical & Structural Modification. Fu et al. (2019) has indicated that most ST models only perform lexical modification, while leaving structures unchanged. Actually, syntactic structures play a significant role in language styles, especially regarding complexity or simplicity (Carroll et al., 1999). As shown in the last example, a complex sentence can be expressed with several simple sentences by appropriately splitting content. However, available datasets rarely contain such cases. Our main contributions can be summarized as: • We propose the new task of expertise style transfer, which aims to facilitate communication between experts and laymen. 1 https://en.wikipedia.org/wiki/The_ Merck_Manuals • We contribute a challenging dataset that requires knowledge-aware and structural modification techniques. • We establish benchmark performance and discuss key challenges of datasets, models a"
2020.acl-main.100,W11-1601,0,0.0153766,"ne a sentence as simple, if it has more frequent words, shorter length and fewer syllables per word, etc. This motivates a variety of syntactic rule-based methods, such as reducing sentence length (Chandrasekar and Srinivas, 1997; Vickrey and Koller, 2008), lexical substitution (Glavas and Stajner, 2015; Paetzold and Specia, 2016) or sentence splitting (Woodsend and Lapata, 2011; Sulem et al., 2018b). Another line of work follows the success of machine translation (MT) (Klein et al., 2017), and regards TS as a monolingual translation from complex language to simple language (Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012). Zhang and Lapata (2017) incorporate reinforcement learning into the encoder–decoder framework to encourage three types of simplification rewards concerning language simplicity, relevance and fluency, while Shardlow and Nawaz (2019) improve the performance of MT models by introducing explanatory synonyms. To alleviate the heavy burden of parallel training corpora, Surya et al. (2019) propose an unsupervised model via adversarial learning between a shared encoder and separate decoders. The simplicity of language in the medical domain is particularly important. Terminologi"
2020.acl-main.100,P19-1601,0,0.0500939,"oder frameworks to capture both common and distinguishing features. Some variants further investigate the auxiliary tasks to better preserve contents (John et al., 2019), or domain adaptation (Li et al., 2019). 1062 Another line of work argues that it is difficult to disentangle style from content. Thus, their main idea is to learn style-specific translations, which are trained using unaligned data based on backtranslation (Zhang et al., 2019; Prabhumoye et al., 2018; Lample et al., 2019), pseudo parallel sentences according to semantic similarity (Jin et al., 2019), or cyclic reconstruction (Dai et al., 2019), marked with Translation methods. The third group is Manipulation methods. Li et al. (2018) first identify the style words by their statistics, then replace them with similar retrieved sentences with a target style. Xu et al. (2018) jointly train the two steps with a neutralization module and a stylization module based on reinforcement learning. For better stylization, Zhang et al. (2018b) introduce a learned sentiment memory network, while John et al. (2019) utilize hierarchical reinforcement learning. 2.2 Zweigenbaum (2008) detect paraphrases from comparable medical corpora of specialized a"
2020.acl-main.100,P15-2011,0,0.0195333,"ed targets) with respect to both model training and testing. Besides, it is usually ignored that the opposite direction of TS — improving the expertise levels of layman language for accuracy and professionality — is also critical for better communication. 2.3 Text Simplification Earlier work on text simplification define a sentence as simple, if it has more frequent words, shorter length and fewer syllables per word, etc. This motivates a variety of syntactic rule-based methods, such as reducing sentence length (Chandrasekar and Srinivas, 1997; Vickrey and Koller, 2008), lexical substitution (Glavas and Stajner, 2015; Paetzold and Specia, 2016) or sentence splitting (Woodsend and Lapata, 2011; Sulem et al., 2018b). Another line of work follows the success of machine translation (MT) (Klein et al., 2017), and regards TS as a monolingual translation from complex language to simple language (Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012). Zhang and Lapata (2017) incorporate reinforcement learning into the encoder–decoder framework to encourage three types of simplification rewards concerning language simplicity, relevance and fluency, while Shardlow and Nawaz (2019) improve the performance"
2020.acl-main.100,W17-4902,0,0.0470796,"techniques. • We establish benchmark performance and discuss key challenges of datasets, models and evaluation metrics. 2 2.1 Related Work Text Style Transfer Existing ST work has achieved promising results on the styles of sentiment (Hu et al., 2017; Shen et al., 2017), formality (Rao and Tetreault, 2018), offensiveness (dos Santos et al., 2018), politeness (Sennrich et al., 2016), authorship (Xu et al., 2012), gender and ages (Prabhumoye et al., 2018; Lample et al., 2019), etc. Nevertheless, only a few of them focus on supervised methods due to the limited availability of parallel corpora. Jhamtani et al. (2017) extract modern language based Shakespeare’s play from the educational site, while Rao and Tetreault (2018) and Li et al. (2018) utilize crowdsourcing techniques to rewrite sentences from Yahoo Answers, Yelp and Amazon reviews, which are then utilized for training neural machine translation (NMT) models and evaluation. More practically, there is an enthusiasm for unsupervised methods without parallel data. There are three groups. The first group is Disentanglement methods that learn disentangled representations of style and content, and then directly manipulating these latent representations t"
2020.acl-main.100,D19-1306,0,0.0944358,"Missing"
2020.acl-main.100,P19-1041,0,0.0278074,"gma and Welling, 2013), to represent the content as continuous variables with standard Gaussian prior, and reconstruct style vector from the generated samples via an attribute discriminator. To improve the ability of style-specific generation, Fu et al. (2018) utilize multiple generators, which are then extended by a Wasserstein distance regularizer (Zhao et al., 2018). SHAPED (Zhang et al., 2018a) learns a shared and several private encoder–decoder frameworks to capture both common and distinguishing features. Some variants further investigate the auxiliary tasks to better preserve contents (John et al., 2019), or domain adaptation (Li et al., 2019). 1062 Another line of work argues that it is difficult to disentangle style from content. Thus, their main idea is to learn style-specific translations, which are trained using unaligned data based on backtranslation (Zhang et al., 2019; Prabhumoye et al., 2018; Lample et al., 2019), pseudo parallel sentences according to semantic similarity (Jin et al., 2019), or cyclic reconstruction (Dai et al., 2019), marked with Translation methods. The third group is Manipulation methods. Li et al. (2018) first identify the style words by their statistics, then re"
2020.acl-main.100,E17-2068,0,0.0261161,"incorporates a phrase table into OpenNMT (Klein et al., 2017), which provides guidance for replacing complex words with their simple synonym (Shardlow and Nawaz, 2019); and (2) Unsupervised model UNTS that utilizes adversarial learning (Surya et al., 2019). The models for ST task selected are: (1) Disentanglement method ControlledGen (Hu et al., Following Dai et al. (2019), we make an automatic evaluation on three aspects: Style Accuracy (marked as Acc) aims to measure how accurate the model controls sentence style. We train two classifiers on the training set of each dataset using fasttext (Joulin et al., 2017). Fluency (marked as PPL) is usually measured by the perplexity of the transferred sentence. We fine-tune the state-of-the-art pretrained language model, Bert (Devlin et al., 2019), on the training set of each dataset for each style. Content Similarity measures how much content is preserved during style transfer. We calculate 4-gram BLEU (Papineni et al., 2002) between model outputs and inputs (marked as self-BLEU), and between outputs and gold human references (marked as ref-BLEU). Automatic metrics for content similarity are arguably unreliable, since the original inputs usually achieve the"
2020.acl-main.100,P17-4012,0,0.0742569,"professionality — is also critical for better communication. 2.3 Text Simplification Earlier work on text simplification define a sentence as simple, if it has more frequent words, shorter length and fewer syllables per word, etc. This motivates a variety of syntactic rule-based methods, such as reducing sentence length (Chandrasekar and Srinivas, 1997; Vickrey and Koller, 2008), lexical substitution (Glavas and Stajner, 2015; Paetzold and Specia, 2016) or sentence splitting (Woodsend and Lapata, 2011; Sulem et al., 2018b). Another line of work follows the success of machine translation (MT) (Klein et al., 2017), and regards TS as a monolingual translation from complex language to simple language (Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012). Zhang and Lapata (2017) incorporate reinforcement learning into the encoder–decoder framework to encourage three types of simplification rewards concerning language simplicity, relevance and fluency, while Shardlow and Nawaz (2019) improve the performance of MT models by introducing explanatory synonyms. To alleviate the heavy burden of parallel training corpora, Surya et al. (2019) propose an unsupervised model via adversarial learning betwe"
2020.acl-main.100,N19-1423,0,0.0221769,"supervised model UNTS that utilizes adversarial learning (Surya et al., 2019). The models for ST task selected are: (1) Disentanglement method ControlledGen (Hu et al., Following Dai et al. (2019), we make an automatic evaluation on three aspects: Style Accuracy (marked as Acc) aims to measure how accurate the model controls sentence style. We train two classifiers on the training set of each dataset using fasttext (Joulin et al., 2017). Fluency (marked as PPL) is usually measured by the perplexity of the transferred sentence. We fine-tune the state-of-the-art pretrained language model, Bert (Devlin et al., 2019), on the training set of each dataset for each style. Content Similarity measures how much content is preserved during style transfer. We calculate 4-gram BLEU (Papineni et al., 2002) between model outputs and inputs (marked as self-BLEU), and between outputs and gold human references (marked as ref-BLEU). Automatic metrics for content similarity are arguably unreliable, since the original inputs usually achieve the highest scores (Fu et al., 2019). We 5 We only report TS models for expertise to laymen language, since they do not claim the opposite direction. 6 https://github.com/senisioi/ Neu"
2020.acl-main.100,W19-8604,0,0.171898,"rs to annotate the parallel sentences between the two versions (examples shown in Figure 1). Compared with both ST and TS datasets, MSD is more challenging from two aspects: Knowledge Gap. Domain knowledge is the key factor that influences the expertise level of text, which is also a key difference from conventional styles. We identify two major types of knowledge gaps in MSD: terminology, e.g., dyspnea in the first example; and empirical evidence. As shown in the third pair, doctors prefer to use statistics (About 1/1000), while laymen do not (quite small). Lexical & Structural Modification. Fu et al. (2019) has indicated that most ST models only perform lexical modification, while leaving structures unchanged. Actually, syntactic structures play a significant role in language styles, especially regarding complexity or simplicity (Carroll et al., 1999). As shown in the last example, a complex sentence can be expressed with several simple sentences by appropriately splitting content. However, available datasets rarely contain such cases. Our main contributions can be summarized as: • We propose the new task of expertise style transfer, which aims to facilitate communication between experts and lay"
2020.acl-main.100,D19-1325,0,0.0648545,"ontent as continuous variables with standard Gaussian prior, and reconstruct style vector from the generated samples via an attribute discriminator. To improve the ability of style-specific generation, Fu et al. (2018) utilize multiple generators, which are then extended by a Wasserstein distance regularizer (Zhao et al., 2018). SHAPED (Zhang et al., 2018a) learns a shared and several private encoder–decoder frameworks to capture both common and distinguishing features. Some variants further investigate the auxiliary tasks to better preserve contents (John et al., 2019), or domain adaptation (Li et al., 2019). 1062 Another line of work argues that it is difficult to disentangle style from content. Thus, their main idea is to learn style-specific translations, which are trained using unaligned data based on backtranslation (Zhang et al., 2019; Prabhumoye et al., 2018; Lample et al., 2019), pseudo parallel sentences according to semantic similarity (Jin et al., 2019), or cyclic reconstruction (Dai et al., 2019), marked with Translation methods. The third group is Manipulation methods. Li et al. (2018) first identify the style words by their statistics, then replace them with similar retrieved senten"
2020.acl-main.100,N18-1169,0,0.329339,"the other hand, it also aims to improve the expertise level based on context, so that laymen’s expressions can be more accurate and professional. For example, in the second pair, causing further damage is not as accurate as ulcerates, omitting the important mucous and disintegrative conditions of the sores. There are two related tasks, but neither serve as suitable prior art. The first is text style transfer (ST), which generates texts with different attributes but with the same content. However, although existing approaches have achieved a great success regarding the attributes of sentiment (Li et al., 2018) and formality (Rao and Tetreault, 2018) among oth1061 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1061–1071 c July 5 - 10, 2020. 2020 Association for Computational Linguistics ers, expertise “styling” has not been explored yet. Another similar task is Text Simplification (TS), which rewrites a complex sentence with simple structures (Sulem et al., 2018b) while constrained by limited vocabulary (Paetzold and Specia, 2016). This task can be regarded as similar to our subtask: reducing the expertise level from expert to layman language without c"
2020.acl-main.100,W16-4912,0,0.0454583,"but with the same content. However, although existing approaches have achieved a great success regarding the attributes of sentiment (Li et al., 2018) and formality (Rao and Tetreault, 2018) among oth1061 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1061–1071 c July 5 - 10, 2020. 2020 Association for Computational Linguistics ers, expertise “styling” has not been explored yet. Another similar task is Text Simplification (TS), which rewrites a complex sentence with simple structures (Sulem et al., 2018b) while constrained by limited vocabulary (Paetzold and Specia, 2016). This task can be regarded as similar to our subtask: reducing the expertise level from expert to layman language without considering the opposing direction. However, most existing TS datasets are derived from Wikipedia, and contain numerous noise (misaligned instances) and inadequacies (instances having non-simplified targets) (Xu et al., 2015; Surya et al., 2019); in which further detailed discussion can be found in Section 3.2. In this paper, we construct a manually-annotated dataset for expertise style transfer in medical domain, named MSD, and conduct deep analysis by implementing state-"
2020.acl-main.100,P02-1040,0,0.110222,"i et al. (2019), we make an automatic evaluation on three aspects: Style Accuracy (marked as Acc) aims to measure how accurate the model controls sentence style. We train two classifiers on the training set of each dataset using fasttext (Joulin et al., 2017). Fluency (marked as PPL) is usually measured by the perplexity of the transferred sentence. We fine-tune the state-of-the-art pretrained language model, Bert (Devlin et al., 2019), on the training set of each dataset for each style. Content Similarity measures how much content is preserved during style transfer. We calculate 4-gram BLEU (Papineni et al., 2002) between model outputs and inputs (marked as self-BLEU), and between outputs and gold human references (marked as ref-BLEU). Automatic metrics for content similarity are arguably unreliable, since the original inputs usually achieve the highest scores (Fu et al., 2019). We 5 We only report TS models for expertise to laymen language, since they do not claim the opposite direction. 6 https://github.com/senisioi/ NeuralTextSimplification/ 4.1 Baselines 1066 E2L L2E Dataset Metrics OpenNMT+PT UNTS ControlledGen DeleteAndRetrieve StyleTransformer Gold ControlledGen DeleteAndRetrieve StyleTransforme"
2020.acl-main.100,D14-1162,0,0.0827572,"Missing"
2020.acl-main.100,P18-1080,0,0.179009,"experts and laymen. 1 https://en.wikipedia.org/wiki/The_ Merck_Manuals • We contribute a challenging dataset that requires knowledge-aware and structural modification techniques. • We establish benchmark performance and discuss key challenges of datasets, models and evaluation metrics. 2 2.1 Related Work Text Style Transfer Existing ST work has achieved promising results on the styles of sentiment (Hu et al., 2017; Shen et al., 2017), formality (Rao and Tetreault, 2018), offensiveness (dos Santos et al., 2018), politeness (Sennrich et al., 2016), authorship (Xu et al., 2012), gender and ages (Prabhumoye et al., 2018; Lample et al., 2019), etc. Nevertheless, only a few of them focus on supervised methods due to the limited availability of parallel corpora. Jhamtani et al. (2017) extract modern language based Shakespeare’s play from the educational site, while Rao and Tetreault (2018) and Li et al. (2018) utilize crowdsourcing techniques to rewrite sentences from Yahoo Answers, Yelp and Amazon reviews, which are then utilized for training neural machine translation (NMT) models and evaluation. More practically, there is an enthusiasm for unsupervised methods without parallel data. There are three groups. T"
2020.acl-main.100,N18-1012,0,0.40865,"improve the expertise level based on context, so that laymen’s expressions can be more accurate and professional. For example, in the second pair, causing further damage is not as accurate as ulcerates, omitting the important mucous and disintegrative conditions of the sores. There are two related tasks, but neither serve as suitable prior art. The first is text style transfer (ST), which generates texts with different attributes but with the same content. However, although existing approaches have achieved a great success regarding the attributes of sentiment (Li et al., 2018) and formality (Rao and Tetreault, 2018) among oth1061 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1061–1071 c July 5 - 10, 2020. 2020 Association for Computational Linguistics ers, expertise “styling” has not been explored yet. Another similar task is Text Simplification (TS), which rewrites a complex sentence with simple structures (Sulem et al., 2018b) while constrained by limited vocabulary (Paetzold and Specia, 2016). This task can be regarded as similar to our subtask: reducing the expertise level from expert to layman language without considering the opposing direction. Howev"
2020.acl-main.100,P18-2031,0,0.0241445,"s: • We propose the new task of expertise style transfer, which aims to facilitate communication between experts and laymen. 1 https://en.wikipedia.org/wiki/The_ Merck_Manuals • We contribute a challenging dataset that requires knowledge-aware and structural modification techniques. • We establish benchmark performance and discuss key challenges of datasets, models and evaluation metrics. 2 2.1 Related Work Text Style Transfer Existing ST work has achieved promising results on the styles of sentiment (Hu et al., 2017; Shen et al., 2017), formality (Rao and Tetreault, 2018), offensiveness (dos Santos et al., 2018), politeness (Sennrich et al., 2016), authorship (Xu et al., 2012), gender and ages (Prabhumoye et al., 2018; Lample et al., 2019), etc. Nevertheless, only a few of them focus on supervised methods due to the limited availability of parallel corpora. Jhamtani et al. (2017) extract modern language based Shakespeare’s play from the educational site, while Rao and Tetreault (2018) and Li et al. (2018) utilize crowdsourcing techniques to rewrite sentences from Yahoo Answers, Yelp and Amazon reviews, which are then utilized for training neural machine translation (NMT) models and evaluation. More p"
2020.acl-main.100,N16-1005,0,0.030617,"pertise style transfer, which aims to facilitate communication between experts and laymen. 1 https://en.wikipedia.org/wiki/The_ Merck_Manuals • We contribute a challenging dataset that requires knowledge-aware and structural modification techniques. • We establish benchmark performance and discuss key challenges of datasets, models and evaluation metrics. 2 2.1 Related Work Text Style Transfer Existing ST work has achieved promising results on the styles of sentiment (Hu et al., 2017; Shen et al., 2017), formality (Rao and Tetreault, 2018), offensiveness (dos Santos et al., 2018), politeness (Sennrich et al., 2016), authorship (Xu et al., 2012), gender and ages (Prabhumoye et al., 2018; Lample et al., 2019), etc. Nevertheless, only a few of them focus on supervised methods due to the limited availability of parallel corpora. Jhamtani et al. (2017) extract modern language based Shakespeare’s play from the educational site, while Rao and Tetreault (2018) and Li et al. (2018) utilize crowdsourcing techniques to rewrite sentences from Yahoo Answers, Yelp and Amazon reviews, which are then utilized for training neural machine translation (NMT) models and evaluation. More practically, there is an enthusiasm f"
2020.acl-main.100,P19-1037,0,0.0531654,"8), lexical substitution (Glavas and Stajner, 2015; Paetzold and Specia, 2016) or sentence splitting (Woodsend and Lapata, 2011; Sulem et al., 2018b). Another line of work follows the success of machine translation (MT) (Klein et al., 2017), and regards TS as a monolingual translation from complex language to simple language (Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012). Zhang and Lapata (2017) incorporate reinforcement learning into the encoder–decoder framework to encourage three types of simplification rewards concerning language simplicity, relevance and fluency, while Shardlow and Nawaz (2019) improve the performance of MT models by introducing explanatory synonyms. To alleviate the heavy burden of parallel training corpora, Surya et al. (2019) propose an unsupervised model via adversarial learning between a shared encoder and separate decoders. The simplicity of language in the medical domain is particularly important. Terminologies are one of the main obstacles to understanding, and extracting their explanations could be helpful for TS (Shardlow and Nawaz, 2019). Del´eger and Discussion To sum up, both tasks lack parallel data for training and evaluation. This prevents researcher"
2020.acl-main.100,D18-1081,0,0.279357,"ransfer (ST), which generates texts with different attributes but with the same content. However, although existing approaches have achieved a great success regarding the attributes of sentiment (Li et al., 2018) and formality (Rao and Tetreault, 2018) among oth1061 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1061–1071 c July 5 - 10, 2020. 2020 Association for Computational Linguistics ers, expertise “styling” has not been explored yet. Another similar task is Text Simplification (TS), which rewrites a complex sentence with simple structures (Sulem et al., 2018b) while constrained by limited vocabulary (Paetzold and Specia, 2016). This task can be regarded as similar to our subtask: reducing the expertise level from expert to layman language without considering the opposing direction. However, most existing TS datasets are derived from Wikipedia, and contain numerous noise (misaligned instances) and inadequacies (instances having non-simplified targets) (Xu et al., 2015; Surya et al., 2019); in which further detailed discussion can be found in Section 3.2. In this paper, we construct a manually-annotated dataset for expertise style transfer in medic"
2020.acl-main.100,P08-1040,0,0.027235,"es) and inadequate (instances having non-simplified targets) with respect to both model training and testing. Besides, it is usually ignored that the opposite direction of TS — improving the expertise levels of layman language for accuracy and professionality — is also critical for better communication. 2.3 Text Simplification Earlier work on text simplification define a sentence as simple, if it has more frequent words, shorter length and fewer syllables per word, etc. This motivates a variety of syntactic rule-based methods, such as reducing sentence length (Chandrasekar and Srinivas, 1997; Vickrey and Koller, 2008), lexical substitution (Glavas and Stajner, 2015; Paetzold and Specia, 2016) or sentence splitting (Woodsend and Lapata, 2011; Sulem et al., 2018b). Another line of work follows the success of machine translation (MT) (Klein et al., 2017), and regards TS as a monolingual translation from complex language to simple language (Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012). Zhang and Lapata (2017) incorporate reinforcement learning into the encoder–decoder framework to encourage three types of simplification rewards concerning language simplicity, relevance and fluency, while Sh"
2020.acl-main.100,D11-1038,0,0.0297356,"usually ignored that the opposite direction of TS — improving the expertise levels of layman language for accuracy and professionality — is also critical for better communication. 2.3 Text Simplification Earlier work on text simplification define a sentence as simple, if it has more frequent words, shorter length and fewer syllables per word, etc. This motivates a variety of syntactic rule-based methods, such as reducing sentence length (Chandrasekar and Srinivas, 1997; Vickrey and Koller, 2008), lexical substitution (Glavas and Stajner, 2015; Paetzold and Specia, 2016) or sentence splitting (Woodsend and Lapata, 2011; Sulem et al., 2018b). Another line of work follows the success of machine translation (MT) (Klein et al., 2017), and regards TS as a monolingual translation from complex language to simple language (Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012). Zhang and Lapata (2017) incorporate reinforcement learning into the encoder–decoder framework to encourage three types of simplification rewards concerning language simplicity, relevance and fluency, while Shardlow and Nawaz (2019) improve the performance of MT models by introducing explanatory synonyms. To alleviate the heavy burd"
2020.acl-main.100,P12-1107,0,0.0302466,"Missing"
2020.acl-main.100,Q15-1021,0,0.0221396,"ational Linguistics ers, expertise “styling” has not been explored yet. Another similar task is Text Simplification (TS), which rewrites a complex sentence with simple structures (Sulem et al., 2018b) while constrained by limited vocabulary (Paetzold and Specia, 2016). This task can be regarded as similar to our subtask: reducing the expertise level from expert to layman language without considering the opposing direction. However, most existing TS datasets are derived from Wikipedia, and contain numerous noise (misaligned instances) and inadequacies (instances having non-simplified targets) (Xu et al., 2015; Surya et al., 2019); in which further detailed discussion can be found in Section 3.2. In this paper, we construct a manually-annotated dataset for expertise style transfer in medical domain, named MSD, and conduct deep analysis by implementing state-of-the-art (SOTA) TS and ST models. The dataset is derived from human-written medical references, The Merck Manuals1 , which include two parallel versions of texts, one tailored for consumers and the other for healthcare professionals. For automatic evaluation, we hire doctors to annotate the parallel sentences between the two versions (examples"
2020.acl-main.100,Q16-1029,0,0.0438076,"Missing"
2020.acl-main.100,C12-1177,0,0.367398,"o facilitate communication between experts and laymen. 1 https://en.wikipedia.org/wiki/The_ Merck_Manuals • We contribute a challenging dataset that requires knowledge-aware and structural modification techniques. • We establish benchmark performance and discuss key challenges of datasets, models and evaluation metrics. 2 2.1 Related Work Text Style Transfer Existing ST work has achieved promising results on the styles of sentiment (Hu et al., 2017; Shen et al., 2017), formality (Rao and Tetreault, 2018), offensiveness (dos Santos et al., 2018), politeness (Sennrich et al., 2016), authorship (Xu et al., 2012), gender and ages (Prabhumoye et al., 2018; Lample et al., 2019), etc. Nevertheless, only a few of them focus on supervised methods due to the limited availability of parallel corpora. Jhamtani et al. (2017) extract modern language based Shakespeare’s play from the educational site, while Rao and Tetreault (2018) and Li et al. (2018) utilize crowdsourcing techniques to rewrite sentences from Yahoo Answers, Yelp and Amazon reviews, which are then utilized for training neural machine translation (NMT) models and evaluation. More practically, there is an enthusiasm for unsupervised methods withou"
2020.acl-main.100,D17-1062,0,0.0260756,"words, shorter length and fewer syllables per word, etc. This motivates a variety of syntactic rule-based methods, such as reducing sentence length (Chandrasekar and Srinivas, 1997; Vickrey and Koller, 2008), lexical substitution (Glavas and Stajner, 2015; Paetzold and Specia, 2016) or sentence splitting (Woodsend and Lapata, 2011; Sulem et al., 2018b). Another line of work follows the success of machine translation (MT) (Klein et al., 2017), and regards TS as a monolingual translation from complex language to simple language (Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012). Zhang and Lapata (2017) incorporate reinforcement learning into the encoder–decoder framework to encourage three types of simplification rewards concerning language simplicity, relevance and fluency, while Shardlow and Nawaz (2019) improve the performance of MT models by introducing explanatory synonyms. To alleviate the heavy burden of parallel training corpora, Surya et al. (2019) propose an unsupervised model via adversarial learning between a shared encoder and separate decoders. The simplicity of language in the medical domain is particularly important. Terminologies are one of the main obstacles to understandi"
2020.acl-main.100,N18-1138,0,0.0276499,"autoencoder that learns a shared latent content space between true samples and generated samples through an adversarial classifier. Hu et al. (2017) utilize neural generative model, Variational Autoencoders (VAEs) (Kingma and Welling, 2013), to represent the content as continuous variables with standard Gaussian prior, and reconstruct style vector from the generated samples via an attribute discriminator. To improve the ability of style-specific generation, Fu et al. (2018) utilize multiple generators, which are then extended by a Wasserstein distance regularizer (Zhao et al., 2018). SHAPED (Zhang et al., 2018a) learns a shared and several private encoder–decoder frameworks to capture both common and distinguishing features. Some variants further investigate the auxiliary tasks to better preserve contents (John et al., 2019), or domain adaptation (Li et al., 2019). 1062 Another line of work argues that it is difficult to disentangle style from content. Thus, their main idea is to learn style-specific translations, which are trained using unaligned data based on backtranslation (Zhang et al., 2019; Prabhumoye et al., 2018; Lample et al., 2019), pseudo parallel sentences according to semantic similar"
2020.acl-main.100,D18-1138,0,0.0168144,"autoencoder that learns a shared latent content space between true samples and generated samples through an adversarial classifier. Hu et al. (2017) utilize neural generative model, Variational Autoencoders (VAEs) (Kingma and Welling, 2013), to represent the content as continuous variables with standard Gaussian prior, and reconstruct style vector from the generated samples via an attribute discriminator. To improve the ability of style-specific generation, Fu et al. (2018) utilize multiple generators, which are then extended by a Wasserstein distance regularizer (Zhao et al., 2018). SHAPED (Zhang et al., 2018a) learns a shared and several private encoder–decoder frameworks to capture both common and distinguishing features. Some variants further investigate the auxiliary tasks to better preserve contents (John et al., 2019), or domain adaptation (Li et al., 2019). 1062 Another line of work argues that it is difficult to disentangle style from content. Thus, their main idea is to learn style-specific translations, which are trained using unaligned data based on backtranslation (Zhang et al., 2019; Prabhumoye et al., 2018; Lample et al., 2019), pseudo parallel sentences according to semantic similar"
2020.acl-main.100,C10-1152,0,0.011789,"implification define a sentence as simple, if it has more frequent words, shorter length and fewer syllables per word, etc. This motivates a variety of syntactic rule-based methods, such as reducing sentence length (Chandrasekar and Srinivas, 1997; Vickrey and Koller, 2008), lexical substitution (Glavas and Stajner, 2015; Paetzold and Specia, 2016) or sentence splitting (Woodsend and Lapata, 2011; Sulem et al., 2018b). Another line of work follows the success of machine translation (MT) (Klein et al., 2017), and regards TS as a monolingual translation from complex language to simple language (Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012). Zhang and Lapata (2017) incorporate reinforcement learning into the encoder–decoder framework to encourage three types of simplification rewards concerning language simplicity, relevance and fluency, while Shardlow and Nawaz (2019) improve the performance of MT models by introducing explanatory synonyms. To alleviate the heavy burden of parallel training corpora, Surya et al. (2019) propose an unsupervised model via adversarial learning between a shared encoder and separate decoders. The simplicity of language in the medical domain is particula"
2020.acl-main.100,P18-1016,0,0.139175,"ransfer (ST), which generates texts with different attributes but with the same content. However, although existing approaches have achieved a great success regarding the attributes of sentiment (Li et al., 2018) and formality (Rao and Tetreault, 2018) among oth1061 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1061–1071 c July 5 - 10, 2020. 2020 Association for Computational Linguistics ers, expertise “styling” has not been explored yet. Another similar task is Text Simplification (TS), which rewrites a complex sentence with simple structures (Sulem et al., 2018b) while constrained by limited vocabulary (Paetzold and Specia, 2016). This task can be regarded as similar to our subtask: reducing the expertise level from expert to layman language without considering the opposing direction. However, most existing TS datasets are derived from Wikipedia, and contain numerous noise (misaligned instances) and inadequacies (instances having non-simplified targets) (Xu et al., 2015; Surya et al., 2019); in which further detailed discussion can be found in Section 3.2. In this paper, we construct a manually-annotated dataset for expertise style transfer in medic"
2020.acl-main.100,P19-1198,0,0.306698,"cs ers, expertise “styling” has not been explored yet. Another similar task is Text Simplification (TS), which rewrites a complex sentence with simple structures (Sulem et al., 2018b) while constrained by limited vocabulary (Paetzold and Specia, 2016). This task can be regarded as similar to our subtask: reducing the expertise level from expert to layman language without considering the opposing direction. However, most existing TS datasets are derived from Wikipedia, and contain numerous noise (misaligned instances) and inadequacies (instances having non-simplified targets) (Xu et al., 2015; Surya et al., 2019); in which further detailed discussion can be found in Section 3.2. In this paper, we construct a manually-annotated dataset for expertise style transfer in medical domain, named MSD, and conduct deep analysis by implementing state-of-the-art (SOTA) TS and ST models. The dataset is derived from human-written medical references, The Merck Manuals1 , which include two parallel versions of texts, one tailored for consumers and the other for healthcare professionals. For automatic evaluation, we hire doctors to annotate the parallel sentences between the two versions (examples shown in Figure 1)."
2020.acl-main.100,P18-1090,0,\N,Missing
2020.acl-main.184,P16-1154,0,0.0611646,"ation generation with external knowledge, for example, incorporating additional texts (Ghazvininejad et al., 2018; Vougiouklis et al., 2016; Xu et al., 2017; Long et al., 2017), or knowledge graphs (Long et al., 2017; Ghazvininejad et al., 2018). They have 2032 shown external knowledge effectively improves conversation response generation. The structured knowledge graphs include rich semantics represented via entities and relations (Hayashi et al., 2019). Lots of previous studies focus on task-targeted dialog systems based on domain-specific knowledge bases (Xu et al., 2017; Zhu et al., 2017; Gu et al., 2016). To generate responses with a large-scale knowledge base, Zhou et al. (2018a) and Liu et al. (2018) utilize graph attention and knowledge diffusion to select knowledge semantics for utterance understanding and response generation. Moon et al. (2019) focuses on the task of entity selection, and takes advantage of positive entities that appear in the golden response. Different from previous research, ConceptFlow models the conversation flow explicitly with the commonsense knowledge graph and presents a novel attention mechanism on all concepts to guide the conversation flow in the latent concep"
2020.acl-main.184,W07-0734,0,0.142447,"Missing"
2020.acl-main.184,N16-1014,0,0.649399,"idea card text faith hope future talk voice dream Response：yeah it ’s not a dream to have a talk with robot Zero-hop Concept One-hop Concept Two-hop Concept Figure 1: An Example of Concept Shift in a Conversation. Darker green indicates higher relevance and wider arrow indicates stronger concept shift (captured by ConceptFlow). Introduction The rapid advancements of language modeling and natural language generation (NLG) techniques have enabled fully data-driven conversation models, which directly generate natural language responses for conversations (Shang et al., 2015; Vinyals and Le, 2015; Li et al., 2016b). However, it is a common problem that the generation models may degenerate dull and repetitive contents (Holtzman et al., 2019; Welleck et al., 2019), which, in conversation assistants, leads to off-topic and useless responses. (Tang et al., 2019; Zhang et al., 2018; Gao et al., 2019). Conversations often develop around Knowledge. A promising way to address the degeneration prob∗ † Indicates equal contribution. Part of work is conducted at Tsinghua University. lem is to ground conversations with external knowledge (Xing et al., 2017), such as open-domain knowledge graph (Ghazvininejad et al"
2020.acl-main.184,D16-1127,0,0.486153,"idea card text faith hope future talk voice dream Response：yeah it ’s not a dream to have a talk with robot Zero-hop Concept One-hop Concept Two-hop Concept Figure 1: An Example of Concept Shift in a Conversation. Darker green indicates higher relevance and wider arrow indicates stronger concept shift (captured by ConceptFlow). Introduction The rapid advancements of language modeling and natural language generation (NLG) techniques have enabled fully data-driven conversation models, which directly generate natural language responses for conversations (Shang et al., 2015; Vinyals and Le, 2015; Li et al., 2016b). However, it is a common problem that the generation models may degenerate dull and repetitive contents (Holtzman et al., 2019; Welleck et al., 2019), which, in conversation assistants, leads to off-topic and useless responses. (Tang et al., 2019; Zhang et al., 2018; Gao et al., 2019). Conversations often develop around Knowledge. A promising way to address the degeneration prob∗ † Indicates equal contribution. Part of work is conducted at Tsinghua University. lem is to ground conversations with external knowledge (Xing et al., 2017), such as open-domain knowledge graph (Ghazvininejad et al"
2020.acl-main.184,W04-1013,0,0.0613999,"Missing"
2020.acl-main.184,P18-1138,0,0.0258895,"d et al., 2018; Vougiouklis et al., 2016; Xu et al., 2017; Long et al., 2017), or knowledge graphs (Long et al., 2017; Ghazvininejad et al., 2018). They have 2032 shown external knowledge effectively improves conversation response generation. The structured knowledge graphs include rich semantics represented via entities and relations (Hayashi et al., 2019). Lots of previous studies focus on task-targeted dialog systems based on domain-specific knowledge bases (Xu et al., 2017; Zhu et al., 2017; Gu et al., 2016). To generate responses with a large-scale knowledge base, Zhou et al. (2018a) and Liu et al. (2018) utilize graph attention and knowledge diffusion to select knowledge semantics for utterance understanding and response generation. Moon et al. (2019) focuses on the task of entity selection, and takes advantage of positive entities that appear in the golden response. Different from previous research, ConceptFlow models the conversation flow explicitly with the commonsense knowledge graph and presents a novel attention mechanism on all concepts to guide the conversation flow in the latent concept space. 3 3.1 Preliminary Given a user utterance X = {x1 , ..., xm } with m words, conversation gen"
2020.acl-main.184,P19-1598,0,0.0193843,"(Ghazvininejad et al., 2018), commonsense knowledge base (Zhou et al., 2018a), or background documents (Zhou et al., 2018b). Recent research leverages such external knowledge by using them to ground conversations, integrating them as additional representations, and then generating responses conditioned on both the texts and the grounded semantics (Ghazvininejad et al., 2018; Zhou et al., 2018a,b). Integrating external knowledge as extra semantic representations and additional inputs to the conversation model effectively improves the quality of generated responses (Ghazvininejad et al., 2018; Logan et al., 2019; Zhou et al., 2018a). Never2031 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2031–2043 c July 5 - 10, 2020. 2020 Association for Computational Linguistics theless, some research on discourse development suggests that human conversations are not “still”: People chat around a number of related concepts, and shift their focus from one concept to others. Grosz and Sidner (1986) models such concept shift by breaking discourse into several segments, and demonstrating different concepts, such as objects and properties, are needed to interpret differe"
2020.acl-main.184,P19-1081,0,0.0282689,"hey have 2032 shown external knowledge effectively improves conversation response generation. The structured knowledge graphs include rich semantics represented via entities and relations (Hayashi et al., 2019). Lots of previous studies focus on task-targeted dialog systems based on domain-specific knowledge bases (Xu et al., 2017; Zhu et al., 2017; Gu et al., 2016). To generate responses with a large-scale knowledge base, Zhou et al. (2018a) and Liu et al. (2018) utilize graph attention and knowledge diffusion to select knowledge semantics for utterance understanding and response generation. Moon et al. (2019) focuses on the task of entity selection, and takes advantage of positive entities that appear in the golden response. Different from previous research, ConceptFlow models the conversation flow explicitly with the commonsense knowledge graph and presents a novel attention mechanism on all concepts to guide the conversation flow in the latent concept space. 3 3.1 Preliminary Given a user utterance X = {x1 , ..., xm } with m words, conversation generation models often use an encoder-decoder architecture to generate a response Y = {y1 , ..., yn }. The encoder represents the user utterance X as a"
2020.acl-main.184,P02-1040,0,0.107976,"Missing"
2020.acl-main.184,D14-1162,0,0.0856595,"Missing"
2020.acl-main.184,P19-1078,0,0.0241698,"graph; incorporating distant concepts significantly improves the quality of generated responses with more on-topic semantic information added. Our analyses further confirm the effectiveness of our graph attention mechanism in selecting useful concepts, and ConceptFlow’s ability in leveraging them to generate more relevant, informative, and less repetitive responses. 2 Related Work Sequence-to-sequence models, e.g., Sutskever et al. (2014), have been widely used for natural language generation (NLG), and to build conversation systems (Shang et al., 2015; Vinyals and Le, 2015; Li et al., 2016b; Wu et al., 2019). Recently, pretrained language models, such as ELMO (Devlin et al., 2019), UniLM (Dong et al., 2019) and GPT2 (Radford et al., 2018), further boost the NLG performance with large scale pretraining. Nevertheless, the degenerating of irrelevant, off-topic, and non-useful responses is still one of the main challenges in conversational generation (Rosset et al., 2020; Tang et al., 2019; Zhang et al., 2018; Gao et al., 2019). Recent work focuses on improving conversation generation with external knowledge, for example, incorporating additional texts (Ghazvininejad et al., 2018; Vougiouklis et al.,"
2020.acl-main.184,P15-1152,0,0.377979,"l Graph write chat faith write word dream idea card text faith hope future talk voice dream Response：yeah it ’s not a dream to have a talk with robot Zero-hop Concept One-hop Concept Two-hop Concept Figure 1: An Example of Concept Shift in a Conversation. Darker green indicates higher relevance and wider arrow indicates stronger concept shift (captured by ConceptFlow). Introduction The rapid advancements of language modeling and natural language generation (NLG) techniques have enabled fully data-driven conversation models, which directly generate natural language responses for conversations (Shang et al., 2015; Vinyals and Le, 2015; Li et al., 2016b). However, it is a common problem that the generation models may degenerate dull and repetitive contents (Holtzman et al., 2019; Welleck et al., 2019), which, in conversation assistants, leads to off-topic and useless responses. (Tang et al., 2019; Zhang et al., 2018; Gao et al., 2019). Conversations often develop around Knowledge. A promising way to address the degeneration prob∗ † Indicates equal contribution. Part of work is conducted at Tsinghua University. lem is to ground conversations with external knowledge (Xing et al., 2017), such as open-doma"
2020.acl-main.184,D18-1455,0,0.0145504,"generation. It starts from the user utterance, traversing through central graph Gcentral , to outer graph Gouter . This is modeled by encoding the central and outer concept flows according to the user utterance. Central Flow Encoding. The central concept graph Gcentral is encoded by a graph neural network that propagates information from user utterance H to the central concept graph. Specifically, it encodes concept ei ∈ Gcentral to representation ~gei : ~gei = GNN(~ei , Gcentral , H), (4) where ~ei is the concept embedding of ei . There is no restriction of which GNN model to use. We choose Sun et al. (2018)’s GNN (GraftNet), which shows strong effectiveness in encoding knowledge graphs. More details of GraftNet can be found in Appendix A.3. Outer Flow Encoding. The outer flow fep , hopping from ep ∈ V1 to its connected two-hop concept ek , is encoded to f~ep by an attention mechanism: X f~ep = θ ek · [~ep ◦ ~ek ], (5) time decoding with the encodings of the utterance and the latent concept flow. Specifically, ~st is calculated by updating the (t − 1)-th step output representation ~st−1 with the (t − 1)-th step context representation ~ct−1 : ~st = GRU(~st−1 , [~ct−1 ◦ ~ yt−1 ]), where ~yt−1 is th"
2020.acl-main.184,D18-1076,0,0.322779,"at the generation models may degenerate dull and repetitive contents (Holtzman et al., 2019; Welleck et al., 2019), which, in conversation assistants, leads to off-topic and useless responses. (Tang et al., 2019; Zhang et al., 2018; Gao et al., 2019). Conversations often develop around Knowledge. A promising way to address the degeneration prob∗ † Indicates equal contribution. Part of work is conducted at Tsinghua University. lem is to ground conversations with external knowledge (Xing et al., 2017), such as open-domain knowledge graph (Ghazvininejad et al., 2018), commonsense knowledge base (Zhou et al., 2018a), or background documents (Zhou et al., 2018b). Recent research leverages such external knowledge by using them to ground conversations, integrating them as additional representations, and then generating responses conditioned on both the texts and the grounded semantics (Ghazvininejad et al., 2018; Zhou et al., 2018a,b). Integrating external knowledge as extra semantic representations and additional inputs to the conversation model effectively improves the quality of generated responses (Ghazvininejad et al., 2018; Logan et al., 2019; Zhou et al., 2018a). Never2031 Proceedings of the 58th A"
2020.acl-main.184,C16-1318,0,0.017333,"b; Wu et al., 2019). Recently, pretrained language models, such as ELMO (Devlin et al., 2019), UniLM (Dong et al., 2019) and GPT2 (Radford et al., 2018), further boost the NLG performance with large scale pretraining. Nevertheless, the degenerating of irrelevant, off-topic, and non-useful responses is still one of the main challenges in conversational generation (Rosset et al., 2020; Tang et al., 2019; Zhang et al., 2018; Gao et al., 2019). Recent work focuses on improving conversation generation with external knowledge, for example, incorporating additional texts (Ghazvininejad et al., 2018; Vougiouklis et al., 2016; Xu et al., 2017; Long et al., 2017), or knowledge graphs (Long et al., 2017; Ghazvininejad et al., 2018). They have 2032 shown external knowledge effectively improves conversation response generation. The structured knowledge graphs include rich semantics represented via entities and relations (Hayashi et al., 2019). Lots of previous studies focus on task-targeted dialog systems based on domain-specific knowledge bases (Xu et al., 2017; Zhu et al., 2017; Gu et al., 2016). To generate responses with a large-scale knowledge base, Zhou et al. (2018a) and Liu et al. (2018) utilize graph attentio"
2020.acl-main.285,W17-5029,0,0.01302,"ught in the course videos. For each video, we extract 10 most representative course concepts from subtitles (Pan et al., 2017b). We also record the concept description from Wikidata and search top 10 related papers for each concept via AMiner3 (Tang et al., 2008) as external resource. Moreover, as many NLP types of research are interested in discovering semantic relationships among concepts, we further build a novel concept taxonomy with prerequisite chains as a concept graph (Gordon et al., 2016). Concept Taxonomy. A solid concept taxonomy is favorable for further research in course content (Gordon et al., 2017). However, existing taxonomies like ConceptNet (Liu and Singh, 2004) or Wiki Taxonomy (Ponzetto and Strube, 2007) cannot be directly applied to course concepts because course concepts are mostly academic terms and the non-academic categories greatly interfere with the quality of taxonomy. Thus, we select a crosslingual term taxonomy from CNCTST4 as a basis and lead manual annotation to build a serviceable course concept taxonomy for MOOCCube. Prerequisite Chain. Prerequisite relation is defined as: If concept A can help understanding concept B, then there is a prerequisite relation from A to B"
2020.acl-main.285,P16-1082,0,0.195076,"ikidata2 as an external resource. 1 2 2.3 Concept and Concept Graph Course concepts refer to the knowledge concepts taught in the course videos. For each video, we extract 10 most representative course concepts from subtitles (Pan et al., 2017b). We also record the concept description from Wikidata and search top 10 related papers for each concept via AMiner3 (Tang et al., 2008) as external resource. Moreover, as many NLP types of research are interested in discovering semantic relationships among concepts, we further build a novel concept taxonomy with prerequisite chains as a concept graph (Gordon et al., 2016). Concept Taxonomy. A solid concept taxonomy is favorable for further research in course content (Gordon et al., 2017). However, existing taxonomies like ConceptNet (Liu and Singh, 2004) or Wiki Taxonomy (Ponzetto and Strube, 2007) cannot be directly applied to course concepts because course concepts are mostly academic terms and the non-academic categories greatly interfere with the quality of taxonomy. Thus, we select a crosslingual term taxonomy from CNCTST4 as a basis and lead manual annotation to build a serviceable course concept taxonomy for MOOCCube. Prerequisite Chain. Prerequisite re"
2020.acl-main.285,D15-1193,0,0.0605022,"Missing"
2020.acl-main.285,P17-1133,1,0.842146,"p: //moocdata.cn/data/MOOCCube. 1 Introduction Massive open online courses (MOOCs) boom swiftly in recent years and have provided convenient education for over 100 million users worldwide (Shah, 2019). As a multi-media, large-scale online interactive system, MOOC is an excellent platform for advanced application research (Volery and Lord, 2000). Since MOOC is committed to helping students learn implicit knowledge concepts from diverse courses, many efforts from NLP and AI raise topics to build novel applications for assistance. From extracting course concepts and their prerequisite relations (Pan et al., 2017b; Roy et al., 2019; Li et al., 2019) to analyzing student behaviors (Zhang et al., 2019; Feng et al., 2019), MOOC-related topics, tasks, and methods snowball in recent years. Despite the plentiful research interests, the resource from real MOOCs is still impoverished. ∗ † Equal Contribution. Corresponding author. Most of the publicly available datasets are designed for a specific task or method, e.g., Zhang et al.(2019) build a MOOC enrollment dataset for course recommendation and (Yu et al., 2019) is only for course concept expansion, which merely contains a subset of MOOC elements. Conseque"
2020.acl-main.285,I17-1088,1,0.833296,"p: //moocdata.cn/data/MOOCCube. 1 Introduction Massive open online courses (MOOCs) boom swiftly in recent years and have provided convenient education for over 100 million users worldwide (Shah, 2019). As a multi-media, large-scale online interactive system, MOOC is an excellent platform for advanced application research (Volery and Lord, 2000). Since MOOC is committed to helping students learn implicit knowledge concepts from diverse courses, many efforts from NLP and AI raise topics to build novel applications for assistance. From extracting course concepts and their prerequisite relations (Pan et al., 2017b; Roy et al., 2019; Li et al., 2019) to analyzing student behaviors (Zhang et al., 2019; Feng et al., 2019), MOOC-related topics, tasks, and methods snowball in recent years. Despite the plentiful research interests, the resource from real MOOCs is still impoverished. ∗ † Equal Contribution. Corresponding author. Most of the publicly available datasets are designed for a specific task or method, e.g., Zhang et al.(2019) build a MOOC enrollment dataset for course recommendation and (Yu et al., 2019) is only for course concept expansion, which merely contains a subset of MOOC elements. Conseque"
2020.acl-main.285,P19-1421,1,0.843803,"applications for assistance. From extracting course concepts and their prerequisite relations (Pan et al., 2017b; Roy et al., 2019; Li et al., 2019) to analyzing student behaviors (Zhang et al., 2019; Feng et al., 2019), MOOC-related topics, tasks, and methods snowball in recent years. Despite the plentiful research interests, the resource from real MOOCs is still impoverished. ∗ † Equal Contribution. Corresponding author. Most of the publicly available datasets are designed for a specific task or method, e.g., Zhang et al.(2019) build a MOOC enrollment dataset for course recommendation and (Yu et al., 2019) is only for course concept expansion, which merely contains a subset of MOOC elements. Consequently, they are not feasible enough to support ideas that demand more types of information. Moreover, these datasets only contain a small size of specific entities or relation instances, e.g., prerequisite relation of TutorialBank (Fabbri et al., 2018) only has 794 cases, making it insufficient for advanced models (such as graph neural networks). Therefore, we present MOOCCube, a data repository that integrates courses, concepts, student behaviors, relationships, and external resources. Compared with"
2020.acl-main.466,N19-1078,0,0.0186388,"as structured prediction methods. Symbol-based methods are involved in utilizing legal domain symbols and knowledge for the tasks of LegalAI. The symbolic legal knowledge, such as events and relationships, can provide interpretability. Deep learning methods can be employed for symbol-based methods for better performance. 3.1 Information Extraction Information extraction (IE) has been widely studied in NLP. IE emphasizes on extracting valuable information from texts, and there are many NLP works which concentrate on IE, including name entity recognition (Lample et al., 2016; Kuru et al., 2016; Akbik et al., 2019), relation extraction (Zeng et al., 2015; Miwa and Bansal, 2016; Lin et al., 2016; Christopoulou et al., 2018), and event extraction (Chen et al., 2015; Nguyen et al., 2016; Nguyen and Grishman, 2018). IE in LegalAI has also attracted the interests of many researchers. To make better use of the particularity of legal texts, researchers try to use ontology (Bruckschen et al., 2010; Cardellino et al., 2017; Lenci et al., 2009; Zhang et al., 2017) or global consistency (Yin et al., 2018) for named entity recognition in LegalAI. To extract relationship and events from legal documents, re5220 searc"
2020.acl-main.466,bartolini-etal-2004-semantic,0,0.0859868,"6; Christopoulou et al., 2018), and event extraction (Chen et al., 2015; Nguyen et al., 2016; Nguyen and Grishman, 2018). IE in LegalAI has also attracted the interests of many researchers. To make better use of the particularity of legal texts, researchers try to use ontology (Bruckschen et al., 2010; Cardellino et al., 2017; Lenci et al., 2009; Zhang et al., 2017) or global consistency (Yin et al., 2018) for named entity recognition in LegalAI. To extract relationship and events from legal documents, re5220 searchers attempt to apply different NLP technologies, including hand-crafted rules (Bartolini et al., 2004; Truyens and Eecke, 2014), CRF (Vacek and Schilder, 2017), joint models like SVM, CNN, GRU (Vacek et al., 2019), or scale-free identifier network (Yan et al., 2017) for promising results. Existing works have made lots of efforts to improve the effect of IE, but we need to pay more attention to the benefits of the extracted information. The extracted symbols have a legal basis and can provide interpretability to legal applications, so we cannot just aim at the performance of methods. Here, we show two examples of utilizing the extracted symbols for interpretability of LegalAI: Relation Extract"
2020.acl-main.466,P19-1424,0,0.509635,"Missing"
2020.acl-main.466,P19-1636,0,0.0419149,"Missing"
2020.acl-main.466,D19-1667,0,0.512858,"lways use hand-crafted rules or features due to computational limitations at the time. In recent years, with rapid developments in deep learning, researchers begin to apply deep learning techniques to LegalAI. Several new LegalAI datasets have been proposed (Kano et al., 2018; Xiao et al., 2018; Duan et al., 2019; Chalkidis et al., 2019b,a), which can serve as benchmarks for research in the field. Based on these datasets, researchers began exploring NLP-based solutions to a variety of LegalAI tasks, such as Legal Judgment Prediction (Aletras et al., 2016; Luo et al., 2017; Zhong et al., 2018; Chen et al., 2019), Court View Generation (Ye et al., 2018), Legal Entity Recognition and Classification (Cardellino et al., 2017; ANGELIDIS et al., 2018), Legal Question Answering (Monroy et al., 2009; Taniguchi and Kano, 2016; Kim and Goebel, 2017), Legal Summarization (Hachey and Grover, 2006; Bhattacharya et al., 2019). As previously mentioned, researchers’ efforts over the years led to tremendous advances in LegalAI. To summarize, some efforts concentrate on symbol-based methods, which apply interpretable hand-crafted symbols to legal tasks (Ashley, 2017; Surden, 2018). Meanwhile, other efforts with embedd"
2020.acl-main.466,P15-1017,0,0.0265215,"c legal knowledge, such as events and relationships, can provide interpretability. Deep learning methods can be employed for symbol-based methods for better performance. 3.1 Information Extraction Information extraction (IE) has been widely studied in NLP. IE emphasizes on extracting valuable information from texts, and there are many NLP works which concentrate on IE, including name entity recognition (Lample et al., 2016; Kuru et al., 2016; Akbik et al., 2019), relation extraction (Zeng et al., 2015; Miwa and Bansal, 2016; Lin et al., 2016; Christopoulou et al., 2018), and event extraction (Chen et al., 2015; Nguyen et al., 2016; Nguyen and Grishman, 2018). IE in LegalAI has also attracted the interests of many researchers. To make better use of the particularity of legal texts, researchers try to use ontology (Bruckschen et al., 2010; Cardellino et al., 2017; Lenci et al., 2009; Zhang et al., 2017) or global consistency (Yin et al., 2018) for named entity recognition in LegalAI. To extract relationship and events from legal documents, re5220 searchers attempt to apply different NLP technologies, including hand-crafted rules (Bartolini et al., 2004; Truyens and Eecke, 2014), CRF (Vacek and Schild"
2020.acl-main.466,P18-2014,0,0.0292769,"and knowledge for the tasks of LegalAI. The symbolic legal knowledge, such as events and relationships, can provide interpretability. Deep learning methods can be employed for symbol-based methods for better performance. 3.1 Information Extraction Information extraction (IE) has been widely studied in NLP. IE emphasizes on extracting valuable information from texts, and there are many NLP works which concentrate on IE, including name entity recognition (Lample et al., 2016; Kuru et al., 2016; Akbik et al., 2019), relation extraction (Zeng et al., 2015; Miwa and Bansal, 2016; Lin et al., 2016; Christopoulou et al., 2018), and event extraction (Chen et al., 2015; Nguyen et al., 2016; Nguyen and Grishman, 2018). IE in LegalAI has also attracted the interests of many researchers. To make better use of the particularity of legal texts, researchers try to use ontology (Bruckschen et al., 2010; Cardellino et al., 2017; Lenci et al., 2009; Zhang et al., 2017) or global consistency (Yin et al., 2018) for named entity recognition in LegalAI. To extract relationship and events from legal documents, re5220 searchers attempt to apply different NLP technologies, including hand-crafted rules (Bartolini et al., 2004; Truyen"
2020.acl-main.466,cvrcek-etal-2012-legal,0,0.0385196,"Missing"
2020.acl-main.466,N19-1423,0,0.487569,"ome researchers tried to embed legal dictionaries (Cvrˇcek et al., 2012), which can be regarded as an alternative method. Secondly, a generalized legal knowledge graph is different in the form with those commonly used in NLP. Existing knowledge graphs concern the relationship between entities and concepts, but LegalAI focuses more on the explanation of legal concepts. These two challenges make knowledge modelling via embedding in LegalAI non-trivial, and researchers can try to overcome the challenges in the future. 2.2 Pretrained Language Models Pretrained language models (PLMs) such as BERT (Devlin et al., 2019) have been the recent focus in many fields in NLP (Radford et al., 2019; Yang et al., 2019; Liu et al., 2019a). Given the success of PLM, using PLM in LegalAI is also a very reasonable and direct choice. However, there are differences between the text used by existing PLMs and legal text, which also lead to unsatisfactory performances when directly applying PLMs to legal tasks. The differences stem from the terminology and knowledge involved in legal texts. To address this issue, Zhong et al. (2019b) propose a language model pretrained on Chinese legal documents, including civil and criminal c"
2020.acl-main.466,L16-1538,0,0.0308904,"problems bring considerable challenges to LQA, and we conduct experiments to demonstrate the difficulties of LQA better in the following parts. Related Work In LegalAI, there are many datasets of question answering. Duan et al. (2019) propose CJRC, a legal reading comprehension dataset with the same format as SQUAD 2.0 (Rajpurkar et al., 2018), which includes span extraction, yes/no questions, and unanswerable questions. Besides, COLIEE (Kano et al., 2018) contains about 500 yes/no questions. Moreover, the bar exam is a professional qualification examination for lawyers, so bar exam datasets (Fawei et al., 2016; Zhong et al., 2019a) may be quite hard as they require professional legal knowledge and skills. In addition to these datasets, researchers have also worked on lots of methods on LQA. The rulebased systems (Buscaldi et al., 2010; Kim et al., 2013; Kim and Goebel, 2017) are prevalent in early research. In order to reach better performance, researchers utilize more information like the explanation of concepts (Taniguchi and Kano, 2016; Fawei et al., 2015) or formalize relevant documents as graphs to help reasoning (Monroy et al., 2009, 2008; Tran et al., 2013). Machine learning and deep learnin"
2020.acl-main.466,C18-1041,1,0.809907,"minal judgment prediction dataset, C-LJP. The dataset contains over 2.68 million legal documents published by the Chinese government, making C-LJP a qualified benchmark for LJP. C-LJP contains three subtasks, including relevant articles, applicable charges, and the term of penalty. The first two can be formalized as multi-label classification tasks, while the last one is a regression task. Besides, English LJP datasets also exist (Chalkidis et al., 2019a), but the size is limited. With the development of the neural network, many researchers begin to explore LJP using deep learning technology (Hu et al., 2018; Wang et al., 2019; Li et al., 2019b; Liu et al., 2019b; Li et al., 2019a; Kang et al., 2019). These works can be divided into two primary directions. The first one is to use more novel models to improve performance. Chen et al. (2019) use the gating mechanism to enhance the performance of predicting the term of penalty. Pan et al. (2019) propose multi-scale attention to handle the cases with multiple defendants. Besides, other researchers explore how to utilize legal knowledge or the properties of LJP. Luo et al. (2017) use the attention mechanism between facts and law articles to help the p"
2020.acl-main.466,P17-1052,0,0.124736,"element extraction, we have conducted experiments on the dataset, and the results can be found in Table 2. Divorce Labor Loan Model MiF MaF MiF MaF MiF MaF TextCNN DPCNN LSTM BiDAF BERT BERT-MS 78.7 81.3 80.6 83.1 83.3 84.9 65.9 64.0 67.3 68.7 69.6 72.7 76.4 79.8 81.0 81.5 76.8 79.7 54.4 47.4 52.9 59.4 43.7 54.5 80.3 81.4 80.4 80.5 78.6 81.9 60.6 42.5 53.1 63.1 39.5 64.1 Table 2: Experimental results on extracting elements. Here MiF and MaF denotes micro-F1 and macro-F1. We have implemented several classical encoding models in NLP for element extraction, including TextCNN (Kim, 2014), DPCNN (Johnson and Zhang, 2017), LSTM (Hochreiter and Schmidhuber, 1997), BiDAF (Seo et al., 2016), and BERT (Devlin et al., 2019). We have tried two different versions of pretrained parameters of BERT, including the origin parameters (BERT) and the parameters pretrained on Chinese legal documents (BERT-MS) (Zhong et al., 2019b). From the results, we can see that the language model pretrained on the general domain performs worse 5221 than domain-specific PLM, which proves the necessity of PLM in LegalAI. For the following parts of our paper, we will use BERT pretrained on legal documents for better performance. From the res"
2020.acl-main.466,P16-1200,1,0.806136,"al domain symbols and knowledge for the tasks of LegalAI. The symbolic legal knowledge, such as events and relationships, can provide interpretability. Deep learning methods can be employed for symbol-based methods for better performance. 3.1 Information Extraction Information extraction (IE) has been widely studied in NLP. IE emphasizes on extracting valuable information from texts, and there are many NLP works which concentrate on IE, including name entity recognition (Lample et al., 2016; Kuru et al., 2016; Akbik et al., 2019), relation extraction (Zeng et al., 2015; Miwa and Bansal, 2016; Lin et al., 2016; Christopoulou et al., 2018), and event extraction (Chen et al., 2015; Nguyen et al., 2016; Nguyen and Grishman, 2018). IE in LegalAI has also attracted the interests of many researchers. To make better use of the particularity of legal texts, researchers try to use ontology (Bruckschen et al., 2010; Cardellino et al., 2017; Lenci et al., 2009; Zhang et al., 2017) or global consistency (Yin et al., 2018) for named entity recognition in LegalAI. To extract relationship and events from legal documents, re5220 searchers attempt to apply different NLP technologies, including hand-crafted rules (B"
2020.acl-main.466,D14-1181,0,0.103661,"existing methods on element extraction, we have conducted experiments on the dataset, and the results can be found in Table 2. Divorce Labor Loan Model MiF MaF MiF MaF MiF MaF TextCNN DPCNN LSTM BiDAF BERT BERT-MS 78.7 81.3 80.6 83.1 83.3 84.9 65.9 64.0 67.3 68.7 69.6 72.7 76.4 79.8 81.0 81.5 76.8 79.7 54.4 47.4 52.9 59.4 43.7 54.5 80.3 81.4 80.4 80.5 78.6 81.9 60.6 42.5 53.1 63.1 39.5 64.1 Table 2: Experimental results on extracting elements. Here MiF and MaF denotes micro-F1 and macro-F1. We have implemented several classical encoding models in NLP for element extraction, including TextCNN (Kim, 2014), DPCNN (Johnson and Zhang, 2017), LSTM (Hochreiter and Schmidhuber, 1997), BiDAF (Seo et al., 2016), and BERT (Devlin et al., 2019). We have tried two different versions of pretrained parameters of BERT, including the origin parameters (BERT) and the parameters pretrained on Chinese legal documents (BERT-MS) (Zhong et al., 2019b). From the results, we can see that the language model pretrained on the general domain performs worse 5221 than domain-specific PLM, which proves the necessity of PLM in LegalAI. For the following parts of our paper, we will use BERT pretrained on legal documents for"
2020.acl-main.466,C16-1087,0,0.015175,"ethods, also named as structured prediction methods. Symbol-based methods are involved in utilizing legal domain symbols and knowledge for the tasks of LegalAI. The symbolic legal knowledge, such as events and relationships, can provide interpretability. Deep learning methods can be employed for symbol-based methods for better performance. 3.1 Information Extraction Information extraction (IE) has been widely studied in NLP. IE emphasizes on extracting valuable information from texts, and there are many NLP works which concentrate on IE, including name entity recognition (Lample et al., 2016; Kuru et al., 2016; Akbik et al., 2019), relation extraction (Zeng et al., 2015; Miwa and Bansal, 2016; Lin et al., 2016; Christopoulou et al., 2018), and event extraction (Chen et al., 2015; Nguyen et al., 2016; Nguyen and Grishman, 2018). IE in LegalAI has also attracted the interests of many researchers. To make better use of the particularity of legal texts, researchers try to use ontology (Bruckschen et al., 2010; Cardellino et al., 2017; Lenci et al., 2009; Zhang et al., 2017) or global consistency (Yin et al., 2018) for named entity recognition in LegalAI. To extract relationship and events from legal do"
2020.acl-main.466,N16-1030,0,0.0292328,"scribe symbol-based methods, also named as structured prediction methods. Symbol-based methods are involved in utilizing legal domain symbols and knowledge for the tasks of LegalAI. The symbolic legal knowledge, such as events and relationships, can provide interpretability. Deep learning methods can be employed for symbol-based methods for better performance. 3.1 Information Extraction Information extraction (IE) has been widely studied in NLP. IE emphasizes on extracting valuable information from texts, and there are many NLP works which concentrate on IE, including name entity recognition (Lample et al., 2016; Kuru et al., 2016; Akbik et al., 2019), relation extraction (Zeng et al., 2015; Miwa and Bansal, 2016; Lin et al., 2016; Christopoulou et al., 2018), and event extraction (Chen et al., 2015; Nguyen et al., 2016; Nguyen and Grishman, 2018). IE in LegalAI has also attracted the interests of many researchers. To make better use of the particularity of legal texts, researchers try to use ontology (Bruckschen et al., 2010; Cardellino et al., 2017; Lenci et al., 2009; Zhang et al., 2017) or global consistency (Yin et al., 2018) for named entity recognition in LegalAI. To extract relationship and e"
2020.acl-main.466,2021.ccl-1.108,0,0.0598105,"Missing"
2020.acl-main.466,D17-1289,0,0.408636,"l, 1963; Segal, 1984; Gardner, 1984) always use hand-crafted rules or features due to computational limitations at the time. In recent years, with rapid developments in deep learning, researchers begin to apply deep learning techniques to LegalAI. Several new LegalAI datasets have been proposed (Kano et al., 2018; Xiao et al., 2018; Duan et al., 2019; Chalkidis et al., 2019b,a), which can serve as benchmarks for research in the field. Based on these datasets, researchers began exploring NLP-based solutions to a variety of LegalAI tasks, such as Legal Judgment Prediction (Aletras et al., 2016; Luo et al., 2017; Zhong et al., 2018; Chen et al., 2019), Court View Generation (Ye et al., 2018), Legal Entity Recognition and Classification (Cardellino et al., 2017; ANGELIDIS et al., 2018), Legal Question Answering (Monroy et al., 2009; Taniguchi and Kano, 2016; Kim and Goebel, 2017), Legal Summarization (Hachey and Grover, 2006; Bhattacharya et al., 2019). As previously mentioned, researchers’ efforts over the years led to tremendous advances in LegalAI. To summarize, some efforts concentrate on symbol-based methods, which apply interpretable hand-crafted symbols to legal tasks (Ashley, 2017; Surden, 201"
2020.acl-main.466,P16-1105,0,0.0136318,"volved in utilizing legal domain symbols and knowledge for the tasks of LegalAI. The symbolic legal knowledge, such as events and relationships, can provide interpretability. Deep learning methods can be employed for symbol-based methods for better performance. 3.1 Information Extraction Information extraction (IE) has been widely studied in NLP. IE emphasizes on extracting valuable information from texts, and there are many NLP works which concentrate on IE, including name entity recognition (Lample et al., 2016; Kuru et al., 2016; Akbik et al., 2019), relation extraction (Zeng et al., 2015; Miwa and Bansal, 2016; Lin et al., 2016; Christopoulou et al., 2018), and event extraction (Chen et al., 2015; Nguyen et al., 2016; Nguyen and Grishman, 2018). IE in LegalAI has also attracted the interests of many researchers. To make better use of the particularity of legal texts, researchers try to use ontology (Bruckschen et al., 2010; Cardellino et al., 2017; Lenci et al., 2009; Zhang et al., 2017) or global consistency (Yin et al., 2018) for named entity recognition in LegalAI. To extract relationship and events from legal documents, re5220 searchers attempt to apply different NLP technologies, including han"
2020.acl-main.466,W16-5607,0,0.0298178,"LAIM https://github.com/thunlp/LegalPapers continuous vector space. Many embedding methods have been proved effective (Mikolov et al., 2013; Joulin et al., 2016; Pennington et al., 2014; Peters et al., 2018; Yang et al., 2014; Bordes et al., 2013; Lin et al., 2015) and they are crucial for the effectiveness of the downstream tasks. In LegalAI, embedding methods are also essential as they can bridge the gap between texts and vectors. However, it seems impossible to learn the meaning of a professional term directly from some legal factual description. Existing works (Chalkidis and Kampas, 2019; Nay, 2016) mainly revolve around applying existing embedding methods like Word2Vec to legal domain corpora. To overcome the difficulty of learning professional vocabulary representations, we can try to capture both grammatical information and legal knowledge in word embedding for corresponding tasks. Knowledge modelling is significant to LegalAI, as many results should be decided according to legal rules and knowledge. Although knowledge graph methods in the legal domain are promising, there are still two major challenges before their practical usage. Firstly, the construction of the knowledge graph in"
2020.acl-main.466,N16-1034,0,0.0187153,"such as events and relationships, can provide interpretability. Deep learning methods can be employed for symbol-based methods for better performance. 3.1 Information Extraction Information extraction (IE) has been widely studied in NLP. IE emphasizes on extracting valuable information from texts, and there are many NLP works which concentrate on IE, including name entity recognition (Lample et al., 2016; Kuru et al., 2016; Akbik et al., 2019), relation extraction (Zeng et al., 2015; Miwa and Bansal, 2016; Lin et al., 2016; Christopoulou et al., 2018), and event extraction (Chen et al., 2015; Nguyen et al., 2016; Nguyen and Grishman, 2018). IE in LegalAI has also attracted the interests of many researchers. To make better use of the particularity of legal texts, researchers try to use ontology (Bruckschen et al., 2010; Cardellino et al., 2017; Lenci et al., 2009; Zhang et al., 2017) or global consistency (Yin et al., 2018) for named entity recognition in LegalAI. To extract relationship and events from legal documents, re5220 searchers attempt to apply different NLP technologies, including hand-crafted rules (Bartolini et al., 2004; Truyens and Eecke, 2014), CRF (Vacek and Schilder, 2017), joint mode"
2020.acl-main.466,P18-2124,0,0.029539,"ons will emphasize on the explanation of some legal concepts, while others may concern the analysis of specific cases. Besides, questions can also be expressed very differently between professionals and non-professionals, especially when describing domain-specific terms. These problems bring considerable challenges to LQA, and we conduct experiments to demonstrate the difficulties of LQA better in the following parts. Related Work In LegalAI, there are many datasets of question answering. Duan et al. (2019) propose CJRC, a legal reading comprehension dataset with the same format as SQUAD 2.0 (Rajpurkar et al., 2018), which includes span extraction, yes/no questions, and unanswerable questions. Besides, COLIEE (Kano et al., 2018) contains about 500 yes/no questions. Moreover, the bar exam is a professional qualification examination for lawyers, so bar exam datasets (Fawei et al., 2016; Zhong et al., 2019a) may be quite hard as they require professional legal knowledge and skills. In addition to these datasets, researchers have also worked on lots of methods on LQA. The rulebased systems (Buscaldi et al., 2010; Kim et al., 2013; Kim and Goebel, 2017) are prevalent in early research. In order to reach bette"
2020.acl-main.466,D14-1162,0,0.0876394,"Missing"
2020.acl-main.466,N18-1202,0,0.0126811,"as representation learning. Embedding-based methods emphasize on representing legal facts and knowledge in embedding space, and they can utilize deep learning methods for corresponding tasks. 2.1 Character, Word, Concept Embeddings Character and word embeddings play a significant role in NLP, as it can embed the discrete texts into The main contributions of this work are con5219 1 2 https://github.com/thunlp/CLAIM https://github.com/thunlp/LegalPapers continuous vector space. Many embedding methods have been proved effective (Mikolov et al., 2013; Joulin et al., 2016; Pennington et al., 2014; Peters et al., 2018; Yang et al., 2014; Bordes et al., 2013; Lin et al., 2015) and they are crucial for the effectiveness of the downstream tasks. In LegalAI, embedding methods are also essential as they can bridge the gap between texts and vectors. However, it seems impossible to learn the meaning of a professional term directly from some legal factual description. Existing works (Chalkidis and Kampas, 2019; Nay, 2016) mainly revolve around applying existing embedding methods like Word2Vec to legal domain corpora. To overcome the difficulty of learning professional vocabulary representations, we can try to capt"
2020.acl-main.466,D19-1005,0,0.0166133,"pose a language model pretrained on Chinese legal documents, including civil and criminal case documents. Legal domain-specific PLMs provide a more qualified baseline system for the tasks of LegalAI. We will show several experiments comparing different BERT models in LegalAI tasks. For the future exploration of PLMs in LegalAI, researchers can aim more at integrating knowledge into PLMs. Integrating knowledge into pretrained models can help the reasoning ability between legal concepts. Lots of work has been done on integrating knowledge from the general domain into models (Zhang et al., 2019; Peters et al., 2019; Hayashi et al., 2019). Such technology can also be considered for future application in LegalAI. 3 Symbol-based Methods In this section, we describe symbol-based methods, also named as structured prediction methods. Symbol-based methods are involved in utilizing legal domain symbols and knowledge for the tasks of LegalAI. The symbolic legal knowledge, such as events and relationships, can provide interpretability. Deep learning methods can be employed for symbol-based methods for better performance. 3.1 Information Extraction Information extraction (IE) has been widely studied in NLP. IE emp"
2020.acl-main.466,truyens-van-eecke-2014-legal,0,0.0743219,"Missing"
2020.acl-main.466,W19-2206,0,0.0328903,"2018). IE in LegalAI has also attracted the interests of many researchers. To make better use of the particularity of legal texts, researchers try to use ontology (Bruckschen et al., 2010; Cardellino et al., 2017; Lenci et al., 2009; Zhang et al., 2017) or global consistency (Yin et al., 2018) for named entity recognition in LegalAI. To extract relationship and events from legal documents, re5220 searchers attempt to apply different NLP technologies, including hand-crafted rules (Bartolini et al., 2004; Truyens and Eecke, 2014), CRF (Vacek and Schilder, 2017), joint models like SVM, CNN, GRU (Vacek et al., 2019), or scale-free identifier network (Yan et al., 2017) for promising results. Existing works have made lots of efforts to improve the effect of IE, but we need to pay more attention to the benefits of the extracted information. The extracted symbols have a legal basis and can provide interpretability to legal applications, so we cannot just aim at the performance of methods. Here, we show two examples of utilizing the extracted symbols for interpretability of LegalAI: Relation Extraction and Inheritance Dispute. Inheritance dispute is a type of cases in Civil Law that focuses on the distributio"
2020.acl-main.466,P18-2118,0,0.0139189,"JEC-QA (Zhong et al., 2019a) as the dataset of the experiments, as it is the largest dataset collected from the bar exam, which guarantees its difficulty. JEC-QA contains 28, 641 multiple-choice and multiple-answer questions, together with 79, 433 relevant articles to help to answer the questions. JEC-QA classifies questions into knowledge-driven questions (KD-Questions) and case-analysis questions (CA-Questions) and reports the performances of humans. We implemented several representative question answering models, including BiDAF (Seo et al., 2016), BERT (Devlin et al., 2019), Co-matching (Wang et al., 2018), and HAF (Zhu et al., 2018). The experimental results can be found in Table 6. From the experimental results, we can learn the 5225 models cannot answer the legal questions well compared with their promising results in open-domain question answering and there is still a huge gap between existing models and humans in LQA. For more qualified LQA methods, there are several significant difficulties to overcome: (1) Legal multi-hop reasoning. As Zhong et al. (2019a) state, existing models can perform inference but not multi-hop reasoning. However, legal cases are very complicated, which cannot be"
2020.acl-main.466,N18-1168,0,0.160824,"e to computational limitations at the time. In recent years, with rapid developments in deep learning, researchers begin to apply deep learning techniques to LegalAI. Several new LegalAI datasets have been proposed (Kano et al., 2018; Xiao et al., 2018; Duan et al., 2019; Chalkidis et al., 2019b,a), which can serve as benchmarks for research in the field. Based on these datasets, researchers began exploring NLP-based solutions to a variety of LegalAI tasks, such as Legal Judgment Prediction (Aletras et al., 2016; Luo et al., 2017; Zhong et al., 2018; Chen et al., 2019), Court View Generation (Ye et al., 2018), Legal Entity Recognition and Classification (Cardellino et al., 2017; ANGELIDIS et al., 2018), Legal Question Answering (Monroy et al., 2009; Taniguchi and Kano, 2016; Kim and Goebel, 2017), Legal Summarization (Hachey and Grover, 2006; Bhattacharya et al., 2019). As previously mentioned, researchers’ efforts over the years led to tremendous advances in LegalAI. To summarize, some efforts concentrate on symbol-based methods, which apply interpretable hand-crafted symbols to legal tasks (Ashley, 2017; Surden, 2018). Meanwhile, other efforts with embedding-based methods aim at designing effici"
2020.acl-main.466,Q16-1019,0,0.0794717,"Missing"
2020.acl-main.466,D15-1203,0,0.0112145,"ased methods are involved in utilizing legal domain symbols and knowledge for the tasks of LegalAI. The symbolic legal knowledge, such as events and relationships, can provide interpretability. Deep learning methods can be employed for symbol-based methods for better performance. 3.1 Information Extraction Information extraction (IE) has been widely studied in NLP. IE emphasizes on extracting valuable information from texts, and there are many NLP works which concentrate on IE, including name entity recognition (Lample et al., 2016; Kuru et al., 2016; Akbik et al., 2019), relation extraction (Zeng et al., 2015; Miwa and Bansal, 2016; Lin et al., 2016; Christopoulou et al., 2018), and event extraction (Chen et al., 2015; Nguyen et al., 2016; Nguyen and Grishman, 2018). IE in LegalAI has also attracted the interests of many researchers. To make better use of the particularity of legal texts, researchers try to use ontology (Bruckschen et al., 2010; Cardellino et al., 2017; Lenci et al., 2009; Zhang et al., 2017) or global consistency (Yin et al., 2018) for named entity recognition in LegalAI. To extract relationship and events from legal documents, re5220 searchers attempt to apply different NLP tech"
2020.acl-main.466,P19-1139,1,0.850258,"g et al. (2019b) propose a language model pretrained on Chinese legal documents, including civil and criminal case documents. Legal domain-specific PLMs provide a more qualified baseline system for the tasks of LegalAI. We will show several experiments comparing different BERT models in LegalAI tasks. For the future exploration of PLMs in LegalAI, researchers can aim more at integrating knowledge into PLMs. Integrating knowledge into pretrained models can help the reasoning ability between legal concepts. Lots of work has been done on integrating knowledge from the general domain into models (Zhang et al., 2019; Peters et al., 2019; Hayashi et al., 2019). Such technology can also be considered for future application in LegalAI. 3 Symbol-based Methods In this section, we describe symbol-based methods, also named as structured prediction methods. Symbol-based methods are involved in utilizing legal domain symbols and knowledge for the tasks of LegalAI. The symbolic legal knowledge, such as events and relationships, can provide interpretability. Deep learning methods can be employed for symbol-based methods for better performance. 3.1 Information Extraction Information extraction (IE) has been widely s"
2020.acl-main.466,E17-2041,0,\N,Missing
2020.acl-main.466,D18-1390,1,\N,Missing
2020.acl-main.540,P17-1187,1,0.873387,"s. In the field of NLP, sememe knowledge bases are built to utilize sememes in practical applications, where sememes are generally regarded as semantic labels of words (as shown in Figure 1). HowNet (Dong and Dong, 2006) is the most wellknown one. It annotates over one hundred thousand English and Chinese words with a predefined sets of about 2,000 sememes. Its sememe annotations are sense-level, i.e., each sense of a (polysemous) word is annotated with sememes separately. With the help of HowNet, sememes have been successfully applied to many NLP tasks including word representation learning (Niu et al., 2017), sentiment analysis (Fu et al., 2013), semantic composition (Qi et al., 2019), sequence modeling (Qin et al., 2019), reverse dictionary (Zhang et al., 2019b), etc. 6067 2.2 Particle Swarm Optimization Inspired by the social behaviors like bird flocking, particle swarm optimization (PSO) is a kind of metaheuristic population-based evolutionary computation paradigms (Eberhart and Kennedy, 1995). It has been proved effective in solving the optimization problems such as image classification (Omran et al., 2004), part-of-speech tagging (Silva et al., 2012) and text clustering (Cagnina et al., 2014"
2020.acl-main.540,D14-1162,0,0.093128,"challenging. For natural language inference (NLI), we use the popular Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015). Each instance in SNLI comprises a premise-hypothesis sentence pair and is labelled one of three relations including entailment, contradiction and neutral. As for victim models, we choose two widely used universal sentence encoding models, namely bidirectional LSTM (BiLSTM) with max pooling (Conneau et al., 2017) and BERTBASE (BERT) (Devlin et al., 2019). For BiLSTM, its hidden states are 128-dimensional, and it uses 300-dimensional pre-trained GloVe (Pennington et al., 2014) word embeddings. Details of the datasets and the classification accuracy results of the victim models are listed in Table 1. 4.2 Baseline Methods We select two recent open-source word-level adversarial attack models as the baselines, which are typical and involve different search space reduction methods (step 1) and search algorithms (step 2). The first baseline method (Alzantot et al., 2018) uses the combination of restrictions on word embedding distance and language model prediction score to reduce search space. As for search algorithm, it adopts genetic algorithm, another popular metaheuri"
2020.acl-main.540,P19-1561,0,0.0649884,"ity and naturality. Unfortunately, the change of true label will make the adversarial attack invalid. For example, supposing an adversary changes “she” to “he” in an input 6066 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6066–6080 c July 5 - 10, 2020. 2020 Association for Computational Linguistics sentence to attack a gender identification model, although the victim model alters its prediction result, this is not a valid attack. And the adversarial examples with broken grammaticality and naturality (i.e., poor quality) can be easily defended (Pruthi et al., 2019). Various textual adversarial attack models have been proposed (Wang et al., 2019a), ranging from character-level flipping (Ebrahimi et al., 2018) to sentence-level paraphrasing (Iyyer et al., 2018). Among them, word-level attack models, mostly word substitution-based models, perform comparatively well on both attack efficiency and adversarial example quality (Wang et al., 2019b). Word-level adversarial attacking is actually a problem of combinatorial optimization (Wolsey and Nemhauser, 1999), as its goal is to craft adversarial examples which can successfully fool the victim model using a lim"
2020.acl-main.540,P19-1571,1,0.818847,"practical applications, where sememes are generally regarded as semantic labels of words (as shown in Figure 1). HowNet (Dong and Dong, 2006) is the most wellknown one. It annotates over one hundred thousand English and Chinese words with a predefined sets of about 2,000 sememes. Its sememe annotations are sense-level, i.e., each sense of a (polysemous) word is annotated with sememes separately. With the help of HowNet, sememes have been successfully applied to many NLP tasks including word representation learning (Niu et al., 2017), sentiment analysis (Fu et al., 2013), semantic composition (Qi et al., 2019), sequence modeling (Qin et al., 2019), reverse dictionary (Zhang et al., 2019b), etc. 6067 2.2 Particle Swarm Optimization Inspired by the social behaviors like bird flocking, particle swarm optimization (PSO) is a kind of metaheuristic population-based evolutionary computation paradigms (Eberhart and Kennedy, 1995). It has been proved effective in solving the optimization problems such as image classification (Omran et al., 2004), part-of-speech tagging (Silva et al., 2012) and text clustering (Cagnina et al., 2014). Empirical studies have proven it is more efficient than some other optimiza"
2020.acl-main.540,D13-1170,0,0.0290862,"Pmax − where k is a positive constant, xo represents the original input, and E measures the word-level edit distance (number of different words between two n o sentences). E(xD,x ) is defined as the modification rate of an adversarial example. After mutation, the algorithm returns to the Record step. 4 Experiments In this section, we conduct comprehensive experiments to evaluate our attack model on the tasks of sentiment analysis and natural language inference. 4.1 Datasets and Victim Models For sentiment analysis, we choose two benchmark datasets including IMDB (Maas et al., 2011) and SST-2 (Socher et al., 2013). Both of them are binary sentiment classification datasets. But the average sentence length of SST-2 (17 words) is much shorter than that of IMDB (234 words), which renders attacks on SST-2 more challenging. For natural language inference (NLI), we use the popular Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015). Each instance in SNLI comprises a premise-hypothesis sentence pair and is labelled one of three relations including entailment, contradiction and neutral. As for victim models, we choose two widely used universal sentence encoding models, namely bidirectional"
2020.acl-main.573,P15-1034,0,0.019032,"pidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012"
2020.acl-main.573,P19-1279,0,0.0403217,"old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern ext"
2020.acl-main.573,D11-1142,0,0.0492626,"fined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. F"
2020.acl-main.573,P07-1073,0,0.0684863,"ther experiments and analyses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two i"
2020.acl-main.573,P18-2065,0,0.0197777,"ver all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012) in machine learning, Wang et al. (2019) first explore continual relation learnin"
2020.acl-main.573,N19-1423,0,0.0290448,"hus add special tokens into the tokenized tokens to indicate the beginning and ending positions of those entities. For simplicity, we denote such an example encoding operation as the following equation, x = f (x), (1) where x ∈ Rd is the semantic embedding of x, and d is the embedding dimension. Note that the encoder is not our focus in this paper, we select bidirectional long short-term memory (BiLSTM) (Bengio et al., 1994) as representative encoders to encode examples. In fact, other neural text encoders like convolutional neural networks (Zeng et al., 2014) and pre-trained language models (Devlin et al., 2019) can also be adopted as example encoders. 3.3 Learning for New Tasks When the k-th task is arising, the example encoder has not touched any examples of new relations before, and cannot extract the semantic features of them. Hence, we first fine-tune the example Tk )} to encoder on Tk = {(xT1 k , y1Tk ), . . . , (xTNk , yN grasp new relation patterns in Rk . The loss function of learning the k-th task is as follows, ˜ L(θ) = − Rk | N |X X i=1 j=1 log P δyTk =r × j i exp(g(f (xTi k ), rj )) ˜k| |R Tk l=1 exp(g(f (xi ), rl )) (2) , where rj is the embedding of the j-th relation ˜ k in the all kno"
2020.acl-main.573,P15-1026,0,0.0299963,"ation exercise to keep a stable understanding of old relations. The experimental results show that EMAR could get rid of catastrophically forgetting old relations and outperform the state-of-the-art continual learning models. The code and datasets are released on https://github.com/thunlp/ ContinualRE. 1 Introduction Relation extraction aims at detecting relations between entities from text, e.g., extracting the relation “the president of ” from the given sentence “Newton served as the president of the Royal Society”, which could serve as external resource for various downstream applications (Dong et al., 2015; Xiong et al., 2017; Schlichtkrull et al., ∗ † indicates equal contribution Corresponding author 2018). The conventional RE methods (Riedel et al., 2013; Zeng et al., 2014; Lin et al., 2016) mostly focus on recognizing relations for a fixed pre-defined relation set, and cannot handle rapidly emerging novel relations in the real world. Some researchers therefore explore to detect and learn incessantly emerging relations in an open scenario. As shown in Figure 1, their efforts can be formulated into a two-step pipeline: (1) Open Relation Learning extracts phrases and arguments to construct patt"
2020.acl-main.573,D15-1205,0,0.115988,"Missing"
2020.acl-main.573,D18-1247,1,0.811523,"old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation"
2020.acl-main.573,D18-1514,1,0.884,"old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation"
2020.acl-main.573,P11-1055,0,0.0562083,"eness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation le"
2020.acl-main.573,P16-1200,1,0.933526,"the-art continual learning models. The code and datasets are released on https://github.com/thunlp/ ContinualRE. 1 Introduction Relation extraction aims at detecting relations between entities from text, e.g., extracting the relation “the president of ” from the given sentence “Newton served as the president of the Royal Society”, which could serve as external resource for various downstream applications (Dong et al., 2015; Xiong et al., 2017; Schlichtkrull et al., ∗ † indicates equal contribution Corresponding author 2018). The conventional RE methods (Riedel et al., 2013; Zeng et al., 2014; Lin et al., 2016) mostly focus on recognizing relations for a fixed pre-defined relation set, and cannot handle rapidly emerging novel relations in the real world. Some researchers therefore explore to detect and learn incessantly emerging relations in an open scenario. As shown in Figure 1, their efforts can be formulated into a two-step pipeline: (1) Open Relation Learning extracts phrases and arguments to construct patterns of specific relations, and then discovers unseen relation types by clustering patterns, and finally expands sufficient examples of new relation types from large-scale textual corpora; (2"
2020.acl-main.573,W15-1506,0,0.039566,"MAR effectively alleviates the catastrophic forgetting problem and significantly outperforms the stateof-the-art continual learning models. Further experiments and analyses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to de"
2020.acl-main.573,P06-1015,0,0.060577,"relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012) in machine learning, Wang et al. (2019) first explore continual relation learning. Existing continual learning methods focus on three research directions: (1) consolidation-based methods (Kirkpatrick et al., 2017; Zenke et al., 2017; Li and Hoiem, 2017; Liu et al., 2018; Ritter et al., 2018) which consolidate the model parameters i"
2020.acl-main.573,P15-2047,0,0.0194688,"gnificantly outperforms the stateof-the-art continual learning models. Further experiments and analyses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets."
2020.acl-main.573,D15-1204,0,0.0130382,"t is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012) in machine learning,"
2020.acl-main.573,Q16-1017,0,0.0168298,"ntion to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012) in machine learning, Wang et al. (2019) first explore continual relation learning. Existing continual learning methods focus on three research direct"
2020.acl-main.573,D12-1048,0,0.0436977,"text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual le"
2020.acl-main.573,P09-1113,0,0.293741,"ses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open"
2020.acl-main.573,P16-1105,0,0.0195004,"forms the stateof-the-art continual learning models. Further experiments and analyses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before,"
2020.acl-main.573,N13-1008,0,0.0615208,"d relations and outperform the state-of-the-art continual learning models. The code and datasets are released on https://github.com/thunlp/ ContinualRE. 1 Introduction Relation extraction aims at detecting relations between entities from text, e.g., extracting the relation “the president of ” from the given sentence “Newton served as the president of the Royal Society”, which could serve as external resource for various downstream applications (Dong et al., 2015; Xiong et al., 2017; Schlichtkrull et al., ∗ † indicates equal contribution Corresponding author 2018). The conventional RE methods (Riedel et al., 2013; Zeng et al., 2014; Lin et al., 2016) mostly focus on recognizing relations for a fixed pre-defined relation set, and cannot handle rapidly emerging novel relations in the real world. Some researchers therefore explore to detect and learn incessantly emerging relations in an open scenario. As shown in Figure 1, their efforts can be formulated into a two-step pipeline: (1) Open Relation Learning extracts phrases and arguments to construct patterns of specific relations, and then discovers unseen relation types by clustering patterns, and finally expands sufficient examples of new relation type"
2020.acl-main.573,P15-1061,0,0.0223495,"catastrophic forgetting problem and significantly outperforms the stateof-the-art continual learning models. Further experiments and analyses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations"
2020.acl-main.573,D11-1135,0,0.0368578,"ers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012) in machine learning, Wang et al. (2019) first explore continual relation learning. Existing continual learning methods"
2020.acl-main.573,N06-1039,0,0.105623,", in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012) in machine learning, Wang et al. (2019) first explore continual relation learning. Existing continual learning methods focus on three research directions: (1) consolidation-based methods (Kirkpatri"
2020.acl-main.573,D12-1110,0,0.0747449,"iments on several RE datasets, and the results show that EMAR effectively alleviates the catastrophic forgetting problem and significantly outperforms the stateof-the-art continual learning models. Further experiments and analyses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attentio"
2020.acl-main.573,D15-1203,0,0.0411191,"that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have"
2020.acl-main.573,D16-1252,0,0.01481,"in models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012) in machine learning, Wang et al. (2019) first e"
2020.acl-main.573,C14-1220,0,0.768848,"rform the state-of-the-art continual learning models. The code and datasets are released on https://github.com/thunlp/ ContinualRE. 1 Introduction Relation extraction aims at detecting relations between entities from text, e.g., extracting the relation “the president of ” from the given sentence “Newton served as the president of the Royal Society”, which could serve as external resource for various downstream applications (Dong et al., 2015; Xiong et al., 2017; Schlichtkrull et al., ∗ † indicates equal contribution Corresponding author 2018). The conventional RE methods (Riedel et al., 2013; Zeng et al., 2014; Lin et al., 2016) mostly focus on recognizing relations for a fixed pre-defined relation set, and cannot handle rapidly emerging novel relations in the real world. Some researchers therefore explore to detect and learn incessantly emerging relations in an open scenario. As shown in Figure 1, their efforts can be formulated into a two-step pipeline: (1) Open Relation Learning extracts phrases and arguments to construct patterns of specific relations, and then discovers unseen relation types by clustering patterns, and finally expands sufficient examples of new relation types from large-scale"
2020.acl-main.573,D17-1004,0,0.0606583,"Missing"
2020.acl-main.573,N19-1086,0,0.330723,"set. Although continual relation learning is vital for learning emerging relations, there are rare explorations for this field. A straightforward solution is to store all historical data and re-train models every time new relations and examples come in. Nevertheless, it is computationally expensive since relations are in sustainable growth. Moreover, the huge example number of each relation makes frequently mixing new and old examples become infeasible in the real world. Therefore, storing all data is not practical in continual relation learning. In view of this, the recent preliminary work (Wang et al., 2019) indicates that the main challenge of continual relation learning is the catastrophic forgetting problem, i.e., it is hard to learn new relations and meanwhile avoid forgetting old relations, considering memorizing all the data is almost impossible. 6429 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6429–6440 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Open Relation Learning Continual Relation Learning David Bowie was born in 8th Jan. 1947. Learn Date of Birth Date of Birth Detect New Relations … Data for Date of Birth Hi"
2020.acl-main.573,P05-1053,0,0.180337,"n prototypes. We conduct sufficient experiments on several RE datasets, and the results show that EMAR effectively alleviates the catastrophic forgetting problem and significantly outperforms the stateof-the-art continual learning models. Further experiments and analyses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations."
2020.acl-main.573,D19-1021,1,0.842706,"pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012) in machine learning, Wang et al. (2019) first explore continual relation learning. Existing continual learning methods focus on three research directions: (1) consolidation-based methods (Kirkpatrick et al., 2017; Zenke et al., 2017; Li"
2020.acl-main.573,D15-1206,0,0.0411578,"ng problem and significantly outperforms the stateof-the-art continual learning models. Further experiments and analyses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defin"
2020.acl-main.655,P17-1171,0,0.249533,"entions provide more sparse and focused attention patterns, which are the main source of KGAT’s effectiveness. 2 Related Work The FEVER shared task (Thorne et al., 2018a) aims to develop automatic fact verification systems to check the veracity of human-generated claims by extracting evidence from Wikipedia. The recently launched FEVER shared task 1.0 is hosted as a competition on Codalab1 with a blind test set and has drawn lots of attention from NLP community. Existing fact verification models usually employ FEVER’s official baseline (Thorne et al., 2018a) with a three-step pipeline system (Chen et al., 2017a): document retrieval, sentence retrieval and claim verification. Many of them mainly focus on the claim verification step. Nie et al. (2019a) concatenates all evidence together to verify the claim. One can also conduct reasoning for each claim evidence pair and aggregate them to the claim label (Luken et al., 2018; Yoneda et al., 2018; Hanselowski et al., 2018). TwoWingOS (Yin and Roth, 2018) further incorporates evidence identification to improve claim verification. GEAR (Zhou et al., 2019) formulates claim verification as a graph reasoning task and provides two kinds of attentions. It cond"
2020.acl-main.655,P17-1152,0,0.495687,"entions provide more sparse and focused attention patterns, which are the main source of KGAT’s effectiveness. 2 Related Work The FEVER shared task (Thorne et al., 2018a) aims to develop automatic fact verification systems to check the veracity of human-generated claims by extracting evidence from Wikipedia. The recently launched FEVER shared task 1.0 is hosted as a competition on Codalab1 with a blind test set and has drawn lots of attention from NLP community. Existing fact verification models usually employ FEVER’s official baseline (Thorne et al., 2018a) with a three-step pipeline system (Chen et al., 2017a): document retrieval, sentence retrieval and claim verification. Many of them mainly focus on the claim verification step. Nie et al. (2019a) concatenates all evidence together to verify the claim. One can also conduct reasoning for each claim evidence pair and aggregate them to the claim label (Luken et al., 2018; Yoneda et al., 2018; Hanselowski et al., 2018). TwoWingOS (Yin and Roth, 2018) further incorporates evidence identification to improve claim verification. GEAR (Zhou et al., 2019) formulates claim verification as a graph reasoning task and provides two kinds of attentions. It cond"
2020.acl-main.655,W19-4828,0,0.120667,"aper focuses on the reasoning stage. Fine-Grained Evidence Propagation. The third analysis studies the distribution of KGATEdge’s attention which is used to propagate the evidence clues in the evidence graph. Figure 5 plots the attention weight distribution of the edge attention scores in KGAT and GAT, one from kernels and one from dot-products. The kernel attentions again are more concentrated: KGAT focuses fewer words while GAT’s dot-product attentions are almost equally distributed among all words. This observation of the scattered dotproduct attention is consistent with previous research (Clark et al., 2019). As shown in the next case study, the edge kernels provide a fine-grained and intuitive attention pattern when combining evidence clues from multiple pieces. Table 5: An example claim (Zhou et al., 2019) whose verification requires multiple pieces of evidence. 6 Case Study Table 5 shows the example claim used in GEAR (Zhou et al., 2019) and the evidence sentences retrieved by ESIM, among which the first two are required evidence pieces. Figure 6 presents the distribution of attentions from the first evidence to the tokens in the second evidence (αi2→1 ) in KGAT (Edge Kernel) and GAT (dot-prod"
2020.acl-main.655,N19-1423,0,0.10054,"mise and hypothesis as either entailment, contradiction or neutral, similar to the FEVER task, though the later requires systems to find the evidence pieces themselves and there are often multiple evidence pieces. One of the most widely used NLI models in FEVER is Enhanced Sequential Inference Model (ESIM) (Chen et al., 2017b), which employs some forms of hard or soft alignment to associate the relevant sub-components between premise and hypothesis. BERT, the pre-trained deep bidirectional Transformer, has also been used for better text representation in FEVER and achieved better performance (Devlin et al., 2019; Li et al., 2019; Zhou et al., 2019; Soleimani et al., 2019). The recent development of neural information retrieval models, especially the interaction based ones, have shown promising effectiveness in extracting soft match patterns from query-document interactions (Hu et al., 2014; Pang et al., 2016; Guo et al., 2016; Xiong et al., 2017; Dai et al., 2018). One of the effective ways to model text matches is to leverage matching kernels (Xiong et al., 2017; Dai et al., 2018), which summarize word or phrase interactions in the learned embedding space between query and documents. The kernel extr"
2020.acl-main.655,N18-1132,0,0.0798128,"Missing"
2020.acl-main.655,W18-5516,0,0.122495,"o avoid the undesired social influences of maliciously fabricated statements, is urgently needed for our society. Recent research formulates this problem as the fact verification task, which targets to automatically verify the integrity of statements using trustworthy corpora, e.g., Wikipedia (Thorne et al., 2018a). For example, as shown in Figure 1, a system could first retrieve related evidence sentences from the background corpus, conduct joint reasoning over these sentences, and aggregate the signals to verify the claim integrity (Nie et al., 2019a; Zhou et al., 2019; Yoneda et al., 2018; Hanselowski et al., 2018). There are two challenges for evidence reasoning and aggregation in fact verification. One is that no ground truth evidence is given; the evidence sentences are retrieved from background corpora, which inevitably contain noise. The other is that the false claims are often deliberately fabricated; they may be semantically correct but are not supported. This makes fact verification a rather challenging task, as it requires the fine-grained reasoning ability to distinguish the subtle differences between truth and false statements (Zhou et al., 2019). This paper presents a new neural structural r"
2020.acl-main.655,2021.ccl-1.108,0,0.155789,"Missing"
2020.acl-main.655,W18-5526,0,0.0920928,"Missing"
2020.acl-main.655,D19-1258,0,0.39007,"textual contents, to prevent the spread of fake news, and to avoid the undesired social influences of maliciously fabricated statements, is urgently needed for our society. Recent research formulates this problem as the fact verification task, which targets to automatically verify the integrity of statements using trustworthy corpora, e.g., Wikipedia (Thorne et al., 2018a). For example, as shown in Figure 1, a system could first retrieve related evidence sentences from the background corpus, conduct joint reasoning over these sentences, and aggregate the signals to verify the claim integrity (Nie et al., 2019a; Zhou et al., 2019; Yoneda et al., 2018; Hanselowski et al., 2018). There are two challenges for evidence reasoning and aggregation in fact verification. One is that no ground truth evidence is given; the evidence sentences are retrieved from background corpora, which inevitably contain noise. The other is that the false claims are often deliberately fabricated; they may be semantically correct but are not supported. This makes fact verification a rather challenging task, as it requires the fine-grained reasoning ability to distinguish the subtle differences between truth and false statement"
2020.acl-main.655,D16-1244,0,0.141858,"Missing"
2020.acl-main.655,N18-1202,0,0.0129804,"regation over claim evidence pairs with 1 https://competitions.codalab.org/ competitions/18814 a graph model (Veliˇckovi´c et al., 2017; Scarselli et al., 2008; Kipf and Welling, 2017). Zhong et al. (2019) further employs XLNet (Yang et al., 2019) and establishes a semantic-level graph for reasoning for a better performance. These graph based models establish node interactions for joint reasoning over several evidence pieces. Many fact verification systems leverage Natural Language Inference (NLI) techniques (Chen et al., 2017b; Ghaeini et al., 2018; Parikh et al., 2016; Radford et al., 2018; Peters et al., 2018; Li et al., 2019) to verify the claim. The NLI task aims to classify the relationship between a pair of premise and hypothesis as either entailment, contradiction or neutral, similar to the FEVER task, though the later requires systems to find the evidence pieces themselves and there are often multiple evidence pieces. One of the most widely used NLI models in FEVER is Enhanced Sequential Inference Model (ESIM) (Chen et al., 2017b), which employs some forms of hard or soft alignment to associate the relevant sub-components between premise and hypothesis. BERT, the pre-trained deep bidirection"
2020.acl-main.655,W18-5501,0,0.135442,"Missing"
2020.acl-main.655,2020.emnlp-main.582,1,0.756424,"etrieved pages. There are two sentence retrieval models in our experiments: ESIM based sentence retrieval and BERT based sentence retrieval. The ESIM based sentence retrieval keeps the same as the previous work (Hanselowski et al., 2018; Zhou et al., 2019). The base version of BERT is used to implement our BERT based sentence retrieval model. We use the “[CLS]” hidden state to represent claim and evidence sentence pair. Then a learning to rank layer is leveraged to project “[CLS]” hidden state to ranking score. Pairwise loss is used to optimize the ranking model. Some work (Zhao et al., 2020; Ye et al., 2020) also employs our BERT based sentence retrieval in their experiments. Claim verification. During training, we set the 2 https://github.com/sheffieldnlp/ fever-scorer 3 https://www.mediawiki.org/wiki/API: Main_page 4 Experimental Methodology 7346 Model Athene (Hanselowski et al., 2018) UCL MRG (Yoneda et al., 2018) UNC NLP (Nie et al., 2019a) BERT Concat (Zhou et al., 2019) BERT Pair (Zhou et al., 2019) GEAR (Zhou et al., 2019) GAT (BERT Base) w. ESIM Retrieval KGAT (BERT Base) w. ESIM Retrieval SR-MRS (Nie et al., 2019b) BERT (Base) (Soleimani et al., 2019) KGAT (BERT Base) BERT (Large) (Solei"
2020.acl-main.655,D18-1010,0,0.0843691,"th a blind test set and has drawn lots of attention from NLP community. Existing fact verification models usually employ FEVER’s official baseline (Thorne et al., 2018a) with a three-step pipeline system (Chen et al., 2017a): document retrieval, sentence retrieval and claim verification. Many of them mainly focus on the claim verification step. Nie et al. (2019a) concatenates all evidence together to verify the claim. One can also conduct reasoning for each claim evidence pair and aggregate them to the claim label (Luken et al., 2018; Yoneda et al., 2018; Hanselowski et al., 2018). TwoWingOS (Yin and Roth, 2018) further incorporates evidence identification to improve claim verification. GEAR (Zhou et al., 2019) formulates claim verification as a graph reasoning task and provides two kinds of attentions. It conducts reasoning and aggregation over claim evidence pairs with 1 https://competitions.codalab.org/ competitions/18814 a graph model (Veliˇckovi´c et al., 2017; Scarselli et al., 2008; Kipf and Welling, 2017). Zhong et al. (2019) further employs XLNet (Yang et al., 2019) and establishes a semantic-level graph for reasoning for a better performance. These graph based models establish node interact"
2020.acl-main.655,W18-5515,0,0.568683,"d of fake news, and to avoid the undesired social influences of maliciously fabricated statements, is urgently needed for our society. Recent research formulates this problem as the fact verification task, which targets to automatically verify the integrity of statements using trustworthy corpora, e.g., Wikipedia (Thorne et al., 2018a). For example, as shown in Figure 1, a system could first retrieve related evidence sentences from the background corpus, conduct joint reasoning over these sentences, and aggregate the signals to verify the claim integrity (Nie et al., 2019a; Zhou et al., 2019; Yoneda et al., 2018; Hanselowski et al., 2018). There are two challenges for evidence reasoning and aggregation in fact verification. One is that no ground truth evidence is given; the evidence sentences are retrieved from background corpora, which inevitably contain noise. The other is that the false claims are often deliberately fabricated; they may be semantically correct but are not supported. This makes fact verification a rather challenging task, as it requires the fine-grained reasoning ability to distinguish the subtle differences between truth and false statements (Zhou et al., 2019). This paper present"
2020.acl-main.655,N18-1074,0,0.333649,"em. Introduction Online contents with false information, such as fake news, political deception, and online rumors, have been growing significantly and spread widely over the past several years. How to automatically “fact check” the integrity of textual contents, to prevent the spread of fake news, and to avoid the undesired social influences of maliciously fabricated statements, is urgently needed for our society. Recent research formulates this problem as the fact verification task, which targets to automatically verify the integrity of statements using trustworthy corpora, e.g., Wikipedia (Thorne et al., 2018a). For example, as shown in Figure 1, a system could first retrieve related evidence sentences from the background corpus, conduct joint reasoning over these sentences, and aggregate the signals to verify the claim integrity (Nie et al., 2019a; Zhou et al., 2019; Yoneda et al., 2018; Hanselowski et al., 2018). There are two challenges for evidence reasoning and aggregation in fact verification. One is that no ground truth evidence is given; the evidence sentences are retrieved from background corpora, which inevitably contain noise. The other is that the false claims are often deliberately fa"
2020.acl-main.655,P19-1085,1,0.510747,"o prevent the spread of fake news, and to avoid the undesired social influences of maliciously fabricated statements, is urgently needed for our society. Recent research formulates this problem as the fact verification task, which targets to automatically verify the integrity of statements using trustworthy corpora, e.g., Wikipedia (Thorne et al., 2018a). For example, as shown in Figure 1, a system could first retrieve related evidence sentences from the background corpus, conduct joint reasoning over these sentences, and aggregate the signals to verify the claim integrity (Nie et al., 2019a; Zhou et al., 2019; Yoneda et al., 2018; Hanselowski et al., 2018). There are two challenges for evidence reasoning and aggregation in fact verification. One is that no ground truth evidence is given; the evidence sentences are retrieved from background corpora, which inevitably contain noise. The other is that the false claims are often deliberately fabricated; they may be semantically correct but are not supported. This makes fact verification a rather challenging task, as it requires the fine-grained reasoning ability to distinguish the subtle differences between truth and false statements (Zhou et al., 2019"
2020.coling-main.140,N19-1423,0,0.15562,"alized parameters according to the performance on the support set, and selects informative support instances to 1596 contribute more to the adaptation gradients (with meta-parameters φa ), which can be viewed as accurate fine-tuning from concrete instances. Meta-Optimization. In meta-optimization phase, the meta-parameters Φ = {φe , φn , φa } are optimized based on the performance of the adapted model on the query set. The framework is shown in Algorithm 1. 3.1 Instance Encoder Given a sentence and the corresponding target entity pair (i.e., head entity and tail entity), we employ BERT model (Devlin et al., 2019) to encode the instance into contextualized representations, due to its effectiveness on a broad variety of NLP tasks. Specifically, sentences are first tokenized into word pieces (Wu et al., 2016). Inspired by Soares et al. (2019), to mark the positions of entities, we adopt four special tokens as entity markers, and insert them to the start and end of each entity. We select the representations of the start tokens of the head entity and tail entity on the top layer, and concatenate them to obtain the instance representation. The instance encoder can be formulated as follows: xj = g(xj , h, t;"
2020.coling-main.140,D19-1649,1,0.78596,"s to grasp new tasks with only a handful of training data. There are mainly two lines of approaches for few-shot learning: (1) Metric-Learning methods learn an embedding space that can well measure the similarities between instances. Koch et al. (2015; Vinyals et al. (2016) use vector distance functions to measure the similarities of examples, while Sung et al. (2018; Garcia and Estrach (2018) use neural networks to learn the metrics. Besides, Snell et al. (2017) propose to calculate prototypes of each few-shot class for classification. Specifically targeting few-shot relation classification, Gao et al. (2019a) introduce a hybrid attention mechanism to alleviate noise data problems. Ye and Ling (2019; Soares et al. (2019; Gao et al. (2019b; Sui et al. (2020) utilize local feature comparison to further improve few-shot performance. (2) Meta-Learning models, on the other hand, transfer the experience about how to “learn” a new class from the training set to the test domain. One way of meta-learning is to use recurrent networks to grasp the meta knowledge and predict the updated parameters in a black-box manner (Ravi and Larochelle, 2017; Munkhdalai and Yu, 2017; Mishra et al., 2018). Another directi"
2020.coling-main.140,D18-1514,1,0.742534,"is scaled to 1e-3 in L2 norm, and added to ci to obtain the perturbed representation. 4 Experiments In this section, we empirically evaluate MIML on few-shot relation classification. To evaluate the robustness of MIML, we conduct experiments in the presence of noisy instances. We also show the potential of MIML in zero-shot classification. Ablation study and visualization are conducted to better understand the inner mechanism of MIML. 4.1 Experiment Settings We first introduce the experiment settings, including datasets, evaluation protocol and baselines. Dataset. We evaluate MIML on FewRel (Han et al., 2018), a widely-used few-shot relation classification dataset. FewRel contains 70, 000 labeled sentences in 100 relations (i.e., each relation has 700 sentences). The relation annotations are first generated under distant supervision assumption (Mintz et al., 2009) by aligning Wikipedia and Wikidata (Vrandeˇci´c and Kr¨otzsch, 2014), and then labeled by human annotators. The training set contains 44, 800 sentences in 64 relations, the valid set has 11, 200 sentences in 16 relations, and the test set has the rest 14, 000 sentences in 20 relations. Evaluation Protocol. Following the same settings in"
2020.coling-main.140,P09-1113,0,0.132459,"of noisy instances. We also show the potential of MIML in zero-shot classification. Ablation study and visualization are conducted to better understand the inner mechanism of MIML. 4.1 Experiment Settings We first introduce the experiment settings, including datasets, evaluation protocol and baselines. Dataset. We evaluate MIML on FewRel (Han et al., 2018), a widely-used few-shot relation classification dataset. FewRel contains 70, 000 labeled sentences in 100 relations (i.e., each relation has 700 sentences). The relation annotations are first generated under distant supervision assumption (Mintz et al., 2009) by aligning Wikipedia and Wikidata (Vrandeˇci´c and Kr¨otzsch, 2014), and then labeled by human annotators. The training set contains 44, 800 sentences in 64 relations, the valid set has 11, 200 sentences in 16 relations, and the test set has the rest 14, 000 sentences in 20 relations. Evaluation Protocol. Following the same settings in Han et al. (2018), we consider four types of fewshot settings in evaluation, namely 5-way-1-shot, 5-way-5-shot, 10-way-1-shot and 10-way-5-shot. The N -way-K-shot setting indicates that each evaluation batch has N classes that do not appear in training set and"
2020.coling-main.140,D14-1162,0,0.0851287,"this way, MIML learns meta-parameters that can effectively customize initialization parameters for each class, and select informative support instances for fast adaptation, so as to produce good classification results on the query set. 3.5 Implementation Details All hyper-parameters are selected by grid-search on the development set. The class distribution p(C) is implemented by uniform distribution. We adopt Adam (Kingma and Ba, 2015) to optimize metaparameters. The meta learning rate β is 1 for meta-initializer and meta-querier, and 5e-5 for instance encoder. We employ 50 dimensional GloVe (Pennington et al., 2014) for word embeddings and BERTBASE (Devlin et al., 2019) implemented by Wolf et al. (2019) as the instance encoder. The hidden state dimensions ds and dw are 1, 536 and 50 respectively. The number of adaptation steps T is 150. In virtual adversarial training, we first randomly generate a perturbation vector δ1 for meta-information 1598 Encoder Model 5-way-1-shot 5-way-5-shot 10-way-1-shot 10-way-5-shot CNN Meta Network* GNN* SNAIL* Proto Network* MLMAN* 64.46 ± 0.54 66.23 ± 0.75 67.29 ± 0.26 74.52 ± 0.07 82.98 ± 0.20 80.57 ± 0.48 81.28 ± 0.62 79.40 ± 0.22 88.40 ± 0.06 92.66 ± 0.09 53.96 ± 0.56"
2020.coling-main.140,P19-1279,0,0.0830438,"g from concrete instances. Meta-Optimization. In meta-optimization phase, the meta-parameters Φ = {φe , φn , φa } are optimized based on the performance of the adapted model on the query set. The framework is shown in Algorithm 1. 3.1 Instance Encoder Given a sentence and the corresponding target entity pair (i.e., head entity and tail entity), we employ BERT model (Devlin et al., 2019) to encode the instance into contextualized representations, due to its effectiveness on a broad variety of NLP tasks. Specifically, sentences are first tokenized into word pieces (Wu et al., 2016). Inspired by Soares et al. (2019), to mark the positions of entities, we adopt four special tokens as entity markers, and insert them to the start and end of each entity. We select the representations of the start tokens of the head entity and tail entity on the top layer, and concatenate them to obtain the instance representation. The instance encoder can be formulated as follows: xj = g(xj , h, t; φe ), (1) where xj is the sentence, h and t are head and tail entities respectively. g(·) is the encoder, φe is the parameters of the encoder, and xj ∈ Rds is the instance representation. 3.2 Meta-Information Guided Fast Initializ"
2020.coling-main.140,P19-1277,0,0.0153968,"roaches for few-shot learning: (1) Metric-Learning methods learn an embedding space that can well measure the similarities between instances. Koch et al. (2015; Vinyals et al. (2016) use vector distance functions to measure the similarities of examples, while Sung et al. (2018; Garcia and Estrach (2018) use neural networks to learn the metrics. Besides, Snell et al. (2017) propose to calculate prototypes of each few-shot class for classification. Specifically targeting few-shot relation classification, Gao et al. (2019a) introduce a hybrid attention mechanism to alleviate noise data problems. Ye and Ling (2019; Soares et al. (2019; Gao et al. (2019b; Sui et al. (2020) utilize local feature comparison to further improve few-shot performance. (2) Meta-Learning models, on the other hand, transfer the experience about how to “learn” a new class from the training set to the test domain. One way of meta-learning is to use recurrent networks to grasp the meta knowledge and predict the updated parameters in a black-box manner (Ravi and Larochelle, 2017; Munkhdalai and Yu, 2017; Mishra et al., 2018). Another direction is to learn how to better initialize parameters for new classes (Finn et al., 2017; Finn e"
2020.coling-main.140,N19-1108,0,0.019921,"ion phase in 5-way and 10-way setting, and ask the model to classify query instances with class-aware initialization parameters. We compare MIML with strong zero-shot classification baselines. DeViSE (Frome et al., 2013) utilizes word embeddings of class names to classify 1600 Setting Random DeViSE SK4 MIML 5-way-0-shot 10-way-0-shot 20.00 10.00 55.90 ± 0.09 42.29 ± 0.08 79.68 ± 0.12 66.17 ± 0.11 79.54 ± 0.06 61.14 ± 0.10 Table 3: Experimental results of zero-shot classification on FewRel development set. instances from unseen classes, and we implement the DeViSE model with BERT encoder. SK4 (Zhang et al., 2019) incorporates rich semantic knowledge of classes, including word embeddings, class descriptions, class hierarchy, and commonsense knowledge graphs. We report the results in Table 3, from which we observe that: Compared to models tailored for zero-shot classification problem, MIML achieves reasonable performance. This is because that the class-aware fast initialization parameters in MIML are guided by meta-information, and thus can potentially be used to severe as classifiers without further adaptation using support instances. In summary, the results show that MIML can effectively integrate hig"
2020.coling-main.155,C14-1151,0,0.0307615,"2019). In contrast, unsupervised knowledge-based WSD relies on only an external lexical knowledge base (LKB) as the sense inventory and thus has wider practical use. Existing unsupervised knowledge-based WSD approaches mainly comprise gloss-based and graphbased methods. The gloss-based methods utilize glosses (sense definitions) to conduct disambiguation. Lesk algorithm (Lesk, 1986) is a seminal gloss-based method which disambiguates a word by selecting the sense whose gloss overlaps most with the context. There are many subsequent methods based on Lesk algorithm (Banerjee and Pedersen, 2003; Basile et al., 2014; Wang et al., 2020). The graph-based methods is the other major type of knowledge-based WSD approaches, which exploit the structures of the LKB for disambiguation (Agirre et al., 2014; Moro et al., 2014; Chaplot et al., 2015; Chaplot and Salakhutdinov, 2018). Besides, a recent method uses both glosses and structural information of LKBs in knowledge-based WSD and achieves state-of-the-art performance (Scarlini et al., 2020). In Chinese WSD, HowNet (Dong and Dong, 2006) is the most widely used LKB (Wu, 2009). Different from other LKBs, HowNet contains neither glosses nor structures of different"
2020.coling-main.155,D15-1084,0,0.0138215,"monstrate that our model achieves significantly better performance than all the baseline methods. All the code and data of this paper are available at https://github.com/thunlp/SememeWSD. 1 Introduction Word sense disambiguation (WSD) is a long-standing natural language processing task which aims to identify the correct sense of a polysemous word in the context (Navigli, 2009). WSD is fundamental to natural language understanding (Navigli, 2018) and has been proven to be beneficial to many other tasks such as machine translation (Vickrey et al., 2005; Pu et al., 2018), information extraction (Bovi et al., 2015) and information retrieval (Zhong and Ng, 2012). There are two main kinds of WSD, namely supervised disambiguation and unsupervised knowledgebased disambiguation. Supervised WSD requires large amounts of sense-annotated training corpora that are difficult to obtain (Tripodi and Navigli, 2019). In contrast, unsupervised knowledge-based WSD relies on only an external lexical knowledge base (LKB) as the sense inventory and thus has wider practical use. Existing unsupervised knowledge-based WSD approaches mainly comprise gloss-based and graphbased methods. The gloss-based methods utilize glosses ("
2020.coling-main.155,N19-1423,0,0.0594527,"nsupervised WSD based on HowNet. Yang et al. (2001) propose a representative statistical method which utilizes the co-occurrence of sememes of the target word and context to conduct disambiguation. Tang et al. (2015) learn sememe and sense embeddings and disambiguates a word by choosing the sense that has the closest embedding similarity with the context. These methods work well but far from perfectly. In this paper, we propose a new unsupervised HowNet-based WSD model with the help of large pretrained language models. Previous studies have shown that pre-trained language models such as BERT (Devlin et al., 2019) incorporate much sense information (Reif et al., 2019), which can be utilized in WSD. Their pre-training task of masked language model (MLM) is supposed to predict appropriate words for a specified position in the context. In other words, a word with higher MLM prediction score is more suitable for the given context and should have more similar meaning to the original word. Based on this assumption, we design our lexical substitution-based WSD model. For each sense of the target polysemous word, we can find a set of substitution words that involve a sense annotated with the same sememes as th"
2020.coling-main.155,D18-1493,1,0.833008,"y outperforms all the baseline methods and achieves state-of-the-art performance. 2 Methodology In this section, we elaborate on our HowNet-based unsupervised WSD model. Before description of the model, we first give a brief introduction to HowNet. 2.1 Introduction to HowNet HowNet (Dong and Dong, 2006) is the most famous sememe knowledge base. It pre-defines a set of about 2, 000 sememes and uses them to annotate senses of more than 100, 000 Chinese words and phrases. In recent years, HowNet has been successfully applied to diverse natural language processing tasks such as language modeling (Gu et al., 2018), semantic composition (Qi et al., 2019a), sequence modeling (Qin et al., 2020), textual adversarial attack (Zang et al., 2020) and reverse dictionary (Zhang et al., 2020). Sememe annotations in HowNet are hierarchical, and the sememes of a sense form a tree, as illustrated in Figure 1. But in this paper, following previous work (Yang et al., 2001; Tang et al., 2015), we ignore the hierarchy of sememe annotations and simply regard sememes as discrete semantic labels. According to the definition of sememe and the philosophy of HowNet, sememes of a sense can convey its meaning, and two senses an"
2020.coling-main.155,S07-1004,0,0.656943,"s characters. 3 Experiments In this section, we evaluate our model on the newly built HowNet-based WSD dataset by us. 3.1 Construction of the HowNet-based WSD Dataset To the best of our knowledge, the only HowNet-based Chinese WSD dataset1 is based on an outdated version of HowNet that cannot be found now, which actually makes the dataset unusable. And besides, it is a little small (containing only 1, 173 instances for 20 target polysemous words). Therefore, we build a new and larger HowNet-based Chinese WSD dataset based on the Chinese Word Sense Annotated Corpus used in SemEval-2007 task 5 (Jin et al., 2007), whose sense inventory is Chinese Semantic Dictionary. This corpus comprises 3, 632 word-segmented and part-of-speech tagged instance sentences for 40 Chinese polysemous words (19 nouns and 21 verbs). We ask Chinese native speakers to manually annotate the target polysemous words in each instance sentence of the corpus with corresponding senses of HowNet or a special option of “no appropriate sense”, where each instance is annotated by 3 annotators. Among the 40 target polysemous words, 4 words have only one sense in HowNet and thus the other 36 target words’ 3, 328 instances are annotated in"
2020.coling-main.155,Q14-1019,0,0.0441971,"SD approaches mainly comprise gloss-based and graphbased methods. The gloss-based methods utilize glosses (sense definitions) to conduct disambiguation. Lesk algorithm (Lesk, 1986) is a seminal gloss-based method which disambiguates a word by selecting the sense whose gloss overlaps most with the context. There are many subsequent methods based on Lesk algorithm (Banerjee and Pedersen, 2003; Basile et al., 2014; Wang et al., 2020). The graph-based methods is the other major type of knowledge-based WSD approaches, which exploit the structures of the LKB for disambiguation (Agirre et al., 2014; Moro et al., 2014; Chaplot et al., 2015; Chaplot and Salakhutdinov, 2018). Besides, a recent method uses both glosses and structural information of LKBs in knowledge-based WSD and achieves state-of-the-art performance (Scarlini et al., 2020). In Chinese WSD, HowNet (Dong and Dong, 2006) is the most widely used LKB (Wu, 2009). Different from other LKBs, HowNet contains neither glosses nor structures of different senses. Instead, HowNet ∗ Indicates equal contribution. Corresponding author. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/."
2020.coling-main.155,Q18-1044,0,0.0193422,"based WSD dataset. Experimental results demonstrate that our model achieves significantly better performance than all the baseline methods. All the code and data of this paper are available at https://github.com/thunlp/SememeWSD. 1 Introduction Word sense disambiguation (WSD) is a long-standing natural language processing task which aims to identify the correct sense of a polysemous word in the context (Navigli, 2009). WSD is fundamental to natural language understanding (Navigli, 2018) and has been proven to be beneficial to many other tasks such as machine translation (Vickrey et al., 2005; Pu et al., 2018), information extraction (Bovi et al., 2015) and information retrieval (Zhong and Ng, 2012). There are two main kinds of WSD, namely supervised disambiguation and unsupervised knowledgebased disambiguation. Supervised WSD requires large amounts of sense-annotated training corpora that are difficult to obtain (Tripodi and Navigli, 2019). In contrast, unsupervised knowledge-based WSD relies on only an external lexical knowledge base (LKB) as the sense inventory and thus has wider practical use. Existing unsupervised knowledge-based WSD approaches mainly comprise gloss-based and graphbased method"
2020.coling-main.155,P19-1571,1,0.823097,"nd achieves state-of-the-art performance. 2 Methodology In this section, we elaborate on our HowNet-based unsupervised WSD model. Before description of the model, we first give a brief introduction to HowNet. 2.1 Introduction to HowNet HowNet (Dong and Dong, 2006) is the most famous sememe knowledge base. It pre-defines a set of about 2, 000 sememes and uses them to annotate senses of more than 100, 000 Chinese words and phrases. In recent years, HowNet has been successfully applied to diverse natural language processing tasks such as language modeling (Gu et al., 2018), semantic composition (Qi et al., 2019a), sequence modeling (Qin et al., 2020), textual adversarial attack (Zang et al., 2020) and reverse dictionary (Zhang et al., 2020). Sememe annotations in HowNet are hierarchical, and the sememes of a sense form a tree, as illustrated in Figure 1. But in this paper, following previous work (Yang et al., 2001; Tang et al., 2015), we ignore the hierarchy of sememe annotations and simply regard sememes as discrete semantic labels. According to the definition of sememe and the philosophy of HowNet, sememes of a sense can convey its meaning, and two senses annotated with the same sememes are suppo"
2020.coling-main.155,D19-1009,0,0.0142486,"k which aims to identify the correct sense of a polysemous word in the context (Navigli, 2009). WSD is fundamental to natural language understanding (Navigli, 2018) and has been proven to be beneficial to many other tasks such as machine translation (Vickrey et al., 2005; Pu et al., 2018), information extraction (Bovi et al., 2015) and information retrieval (Zhong and Ng, 2012). There are two main kinds of WSD, namely supervised disambiguation and unsupervised knowledgebased disambiguation. Supervised WSD requires large amounts of sense-annotated training corpora that are difficult to obtain (Tripodi and Navigli, 2019). In contrast, unsupervised knowledge-based WSD relies on only an external lexical knowledge base (LKB) as the sense inventory and thus has wider practical use. Existing unsupervised knowledge-based WSD approaches mainly comprise gloss-based and graphbased methods. The gloss-based methods utilize glosses (sense definitions) to conduct disambiguation. Lesk algorithm (Lesk, 1986) is a seminal gloss-based method which disambiguates a word by selecting the sense whose gloss overlaps most with the context. There are many subsequent methods based on Lesk algorithm (Banerjee and Pedersen, 2003; Basil"
2020.coling-main.155,L18-1164,0,0.0171479,"e two most representative ones as the baseline methods. Besides, we compare our model with another three baseline methods that are not specially designed for but can be applied to unsupervised HowNet-based WSD. • SemCo (Yang et al., 2001). This method utilizes the statistics on the co-occurrence of sememes of the target polysemous word and context to conduct WSD. • SemEmbed (Tang et al., 2015). This method first learns sememe embeddings and further obtains sense embeddings, and then employs the embedding similarity between senses of the target word and the context for disambiguation. • Dense (Ustalov et al., 2018). This model is originally designed for WordNet-based WSD, which first obtains sense embeddings from the word embeddings of the corresponding senses’ synonyms and then selects the sense that has the closest embedding similarity with the context. In HowNetbased WSD, we regard the words whose one sense has the same sememes as the target sense as the synonyms. • Random. This baseline method randomly selects a sense of the target word as the WSD result. A common WSD baseline method is choosing the most frequent sense. But HowNet provides no information about the sense frequency. Therefore, we use"
2020.coling-main.155,H05-1097,0,0.0234945,"new and larger HowNet-based WSD dataset. Experimental results demonstrate that our model achieves significantly better performance than all the baseline methods. All the code and data of this paper are available at https://github.com/thunlp/SememeWSD. 1 Introduction Word sense disambiguation (WSD) is a long-standing natural language processing task which aims to identify the correct sense of a polysemous word in the context (Navigli, 2009). WSD is fundamental to natural language understanding (Navigli, 2018) and has been proven to be beneficial to many other tasks such as machine translation (Vickrey et al., 2005; Pu et al., 2018), information extraction (Bovi et al., 2015) and information retrieval (Zhong and Ng, 2012). There are two main kinds of WSD, namely supervised disambiguation and unsupervised knowledgebased disambiguation. Supervised WSD requires large amounts of sense-annotated training corpora that are difficult to obtain (Tripodi and Navigli, 2019). In contrast, unsupervised knowledge-based WSD relies on only an external lexical knowledge base (LKB) as the sense inventory and thus has wider practical use. Existing unsupervised knowledge-based WSD approaches mainly comprise gloss-based and"
2020.coling-main.155,S07-1044,0,0.0643448,"r the given context and should have more similar meaning to the original word. Based on this assumption, we design our lexical substitution-based WSD model. For each sense of the target polysemous word, we can find a set of substitution words that involve a sense annotated with the same sememes as the target sense. We calculate the MLM prediction score for each substitution word, and the average of prediction scores of a sense’s all substitution words can reflect the probability that the target word conveys this sense in the context. The idea of lexical substitution has been applied to WSD in Yuret (2007). Different from our model, it uses a statistical language model to calculate substitution word scores and more importantly, it is not fully unsupervised and requires some sense-annotated corpora in the WSD procedure. Besides, our model resembles the end-to-end BERT-based lexical substitution model in Zhou et al. (2019). However, it is not aimed at WSD and has different calculation methods of substitution word score from us. In experiments, considering existing HowNet-based WSD dataset is unavailable and based on an outdated version of HowNet, we build a new and larger HowNet-based WSD dataset"
2020.coling-main.155,2020.acl-main.540,1,0.722998,"on our HowNet-based unsupervised WSD model. Before description of the model, we first give a brief introduction to HowNet. 2.1 Introduction to HowNet HowNet (Dong and Dong, 2006) is the most famous sememe knowledge base. It pre-defines a set of about 2, 000 sememes and uses them to annotate senses of more than 100, 000 Chinese words and phrases. In recent years, HowNet has been successfully applied to diverse natural language processing tasks such as language modeling (Gu et al., 2018), semantic composition (Qi et al., 2019a), sequence modeling (Qin et al., 2020), textual adversarial attack (Zang et al., 2020) and reverse dictionary (Zhang et al., 2020). Sememe annotations in HowNet are hierarchical, and the sememes of a sense form a tree, as illustrated in Figure 1. But in this paper, following previous work (Yang et al., 2001; Tang et al., 2015), we ignore the hierarchy of sememe annotations and simply regard sememes as discrete semantic labels. According to the definition of sememe and the philosophy of HowNet, sememes of a sense can convey its meaning, and two senses annotated with the same sememes are supposed to have the same meaning. Therefore, in our model, we select the substitution words"
2020.coling-main.155,P12-1029,0,0.019542,"y better performance than all the baseline methods. All the code and data of this paper are available at https://github.com/thunlp/SememeWSD. 1 Introduction Word sense disambiguation (WSD) is a long-standing natural language processing task which aims to identify the correct sense of a polysemous word in the context (Navigli, 2009). WSD is fundamental to natural language understanding (Navigli, 2018) and has been proven to be beneficial to many other tasks such as machine translation (Vickrey et al., 2005; Pu et al., 2018), information extraction (Bovi et al., 2015) and information retrieval (Zhong and Ng, 2012). There are two main kinds of WSD, namely supervised disambiguation and unsupervised knowledgebased disambiguation. Supervised WSD requires large amounts of sense-annotated training corpora that are difficult to obtain (Tripodi and Navigli, 2019). In contrast, unsupervised knowledge-based WSD relies on only an external lexical knowledge base (LKB) as the sense inventory and thus has wider practical use. Existing unsupervised knowledge-based WSD approaches mainly comprise gloss-based and graphbased methods. The gloss-based methods utilize glosses (sense definitions) to conduct disambiguation. L"
2020.coling-main.155,P19-1328,0,0.0220336,". We calculate the MLM prediction score for each substitution word, and the average of prediction scores of a sense’s all substitution words can reflect the probability that the target word conveys this sense in the context. The idea of lexical substitution has been applied to WSD in Yuret (2007). Different from our model, it uses a statistical language model to calculate substitution word scores and more importantly, it is not fully unsupervised and requires some sense-annotated corpora in the WSD procedure. Besides, our model resembles the end-to-end BERT-based lexical substitution model in Zhou et al. (2019). However, it is not aimed at WSD and has different calculation methods of substitution word score from us. In experiments, considering existing HowNet-based WSD dataset is unavailable and based on an outdated version of HowNet, we build a new and larger HowNet-based WSD dataset for evaluation. Experimental results demonstrate that our model significantly outperforms all the baseline methods and achieves state-of-the-art performance. 2 Methodology In this section, we elaborate on our HowNet-based unsupervised WSD model. Before description of the model, we first give a brief introduction to How"
2020.emnlp-demos.23,C86-1107,0,0.727398,"Missing"
2020.emnlp-demos.23,N19-1423,0,0.0159031,"ut. The words in the query description are excluded since they are unlikely to be the target word. Different filters, other sort methods and clustering may be further employed to adjust the final results. 3.2 Max-Pooling Word Score Confidence Score Sentence Vector BERT Dictionary Definition / Query Description Part-of-speech Score & Category Score Figure 3: Revised version of the multi-channel reverse dictionary model. Multi-channel Reverse Dictionary Model The multi-channel reverse dictionary model (MRDM) is the core module of our system. We use an improved version of MRDM that employs BERT (Devlin et al., 2019) rather than BiLSTM as the sentence encoder. Figure 3 illustrates the model. For a given query description, MRDM calculates a confidence score for each candidate word in the vocabulary. The confidence score is composed of five parts: (1) The first part is word score. To obtain it, the input query description is first encoded into a sentence vector by BERT, then the sentence vector is mapped into the space of word embeddings by a single-layer perceptron, and finally word score is the dot product of the mapped sentence vector and the candidate word’s embedding. (2) The second part is part-of-spe"
2020.emnlp-demos.23,W19-0421,0,0.0220052,"Missing"
2020.emnlp-demos.23,Q16-1002,0,0.273363,"ased on sentence matching (Bilac et al., 2004; Zock and Bilac, 2004; M´endez et al., 2013; Shaw et al., 2013). Its main idea is to return the words whose dictionary definitions are most similar to the query description. Although effective in some cases, this method cannot cope with the problem that human-written query descriptions might differ widely from dictionary definitions. The second method uses a neural language model (NLM) to encode the query description into a vector in the word embedding space, and returns the words with the closest embeddings to the vector of the query description (Hill et al., 2016; Morinaga and Yamaguchi, 2018; Kartsaklis et al., 2018; Hedderich et al., 2019; Pilehvar, 2019). Performance of this method depends largely on the quality of word embeddings. Unfortunately, according to Zipf’s law (Zipf, 1949), many words are low-frequency and usually have poor embeddings. To tackle this issue of the NLM-based method, we proposed a multi-channel reverse dictionary model (Zhang et al., 2020). This model is composed of a sentence encoder, more specifically, a bi-directional LSTM (BiLSTM) (Hochreiter and Schmidhuber, 1997) with attention (Bahdanau et al., 2015), and four charact"
2020.emnlp-demos.23,P82-1020,0,0.807762,"Missing"
2020.emnlp-demos.23,D18-1221,0,0.0126569,"ck and Bilac, 2004; M´endez et al., 2013; Shaw et al., 2013). Its main idea is to return the words whose dictionary definitions are most similar to the query description. Although effective in some cases, this method cannot cope with the problem that human-written query descriptions might differ widely from dictionary definitions. The second method uses a neural language model (NLM) to encode the query description into a vector in the word embedding space, and returns the words with the closest embeddings to the vector of the query description (Hill et al., 2016; Morinaga and Yamaguchi, 2018; Kartsaklis et al., 2018; Hedderich et al., 2019; Pilehvar, 2019). Performance of this method depends largely on the quality of word embeddings. Unfortunately, according to Zipf’s law (Zipf, 1949), many words are low-frequency and usually have poor embeddings. To tackle this issue of the NLM-based method, we proposed a multi-channel reverse dictionary model (Zhang et al., 2020). This model is composed of a sentence encoder, more specifically, a bi-directional LSTM (BiLSTM) (Hochreiter and Schmidhuber, 1997) with attention (Bahdanau et al., 2015), and four characteristic predictors. The four predictors are used to pre"
2020.emnlp-demos.23,N19-1222,0,0.0153301,"al., 2013). Its main idea is to return the words whose dictionary definitions are most similar to the query description. Although effective in some cases, this method cannot cope with the problem that human-written query descriptions might differ widely from dictionary definitions. The second method uses a neural language model (NLM) to encode the query description into a vector in the word embedding space, and returns the words with the closest embeddings to the vector of the query description (Hill et al., 2016; Morinaga and Yamaguchi, 2018; Kartsaklis et al., 2018; Hedderich et al., 2019; Pilehvar, 2019). Performance of this method depends largely on the quality of word embeddings. Unfortunately, according to Zipf’s law (Zipf, 1949), many words are low-frequency and usually have poor embeddings. To tackle this issue of the NLM-based method, we proposed a multi-channel reverse dictionary model (Zhang et al., 2020). This model is composed of a sentence encoder, more specifically, a bi-directional LSTM (BiLSTM) (Hochreiter and Schmidhuber, 1997) with attention (Bahdanau et al., 2015), and four characteristic predictors. The four predictors are used to predict the part-ofspeech, morphemes, word c"
2020.emnlp-demos.23,W04-2105,0,0.0284269,"very user-friendly. It includes multiple filters and sort methods, and can automatically cluster the candidate words, all of which help users find the target words as quickly as possible. 2 Cross-lingual (en-zh/zh-en) =1 (Word) Cross-lingual Dictionary Mode Query Length Monolingual (en/zh) >1 (Sentence) Translation Query Length =1 (Word) >1 (Sentence) Multi-channel Reverse Dictionary Model Word Similarity Thesaurus Confidence Score Related Work Filter / Sort / Cluster There are mainly two methods for reverse dictionary building. The first one is based on sentence matching (Bilac et al., 2004; Zock and Bilac, 2004; M´endez et al., 2013; Shaw et al., 2013). Its main idea is to return the words whose dictionary definitions are most similar to the query description. Although effective in some cases, this method cannot cope with the problem that human-written query descriptions might differ widely from dictionary definitions. The second method uses a neural language model (NLM) to encode the query description into a vector in the word embedding space, and returns the words with the closest embeddings to the vector of the query description (Hill et al., 2016; Morinaga and Yamaguchi, 2018; Kartsaklis et al.,"
2020.emnlp-demos.29,D11-1029,0,0.035567,"Missing"
2020.emnlp-demos.29,W19-1402,0,0.0824936,"Missing"
2020.emnlp-demos.29,W19-1420,0,0.0164561,"the ancient text, Snyder et al. (2010) first show the feasibility of automatically deciphering a dead language by designing a Bayesian model to match the alphabet with non-parallel data. Then, BergKirkpatrick and Klein (2011) propose a more effective decipherment approach and achieve promising results. Pourdamghani and Knight (2017) adopt a method similar to non-parallel machine translation (Mukherjee et al., 2018; Lample et al., 2018) to decipher related languages, which further inspires Luo et al. (2019) to propose a novel neural approach for automatic decipherment of Ugaritic and Linear B. Doostmohammadi and Nassajian (2019); Bernier-Colborne et al. (2019) explore to learn language models for Cuneiform Text. These previous efforts have inspired us to apply machine learning methods to the task of processing OBS. However, there are still three main challenges: 227 Proceedings of the 2020 EMNLP (Systems Demonstrations), pages 227–233 c November 16-20, 2020. 2020 Association for Computational Linguistics 1300BC 1046BC 771BC 475BC 222BC 220AD Figure 1: The historical evolution of the character “horse” from OBS to modern Chinese. Figure 2: An example of an OBS document used in divination. (1) Different from those ancie"
2020.emnlp-demos.29,W16-2103,0,0.0408051,"and to analyze and understand OBS is of great significance to historical research. Considering that it is often sophisticated and time-consuming to manually process ancient languages, some efforts have been devoted to utilizing machine learning techniques in this field. In order to detect and recognize ancient characters, Anderson and Levoy (2002); Rothacker et al. (2015); Mousavi and Lyashenko (2017); Rahma et al. (2017); Yamauchi et al. (2018) utilize computer vision techniques to visualize Cuneiform tablets and recognize Cuneiform characters, Franken and van Gemert (2013); Nederhof (2015); Iglesias-Franjo and Vilares (2016) apply similar techniques to recognize Egyptian hieroglyphs. For understanding the ancient text, Snyder et al. (2010) first show the feasibility of automatically deciphering a dead language by designing a Bayesian model to match the alphabet with non-parallel data. Then, BergKirkpatrick and Klein (2011) propose a more effective decipherment approach and achieve promising results. Pourdamghani and Knight (2017) adopt a method similar to non-parallel machine translation (Mukherjee et al., 2018; Lample et al., 2018) to decipher related languages, which further inspires Luo et al. (2019) to propos"
2020.emnlp-demos.29,P19-1303,0,0.0109063,"s-Franjo and Vilares (2016) apply similar techniques to recognize Egyptian hieroglyphs. For understanding the ancient text, Snyder et al. (2010) first show the feasibility of automatically deciphering a dead language by designing a Bayesian model to match the alphabet with non-parallel data. Then, BergKirkpatrick and Klein (2011) propose a more effective decipherment approach and achieve promising results. Pourdamghani and Knight (2017) adopt a method similar to non-parallel machine translation (Mukherjee et al., 2018; Lample et al., 2018) to decipher related languages, which further inspires Luo et al. (2019) to propose a novel neural approach for automatic decipherment of Ugaritic and Linear B. Doostmohammadi and Nassajian (2019); Bernier-Colborne et al. (2019) explore to learn language models for Cuneiform Text. These previous efforts have inspired us to apply machine learning methods to the task of processing OBS. However, there are still three main challenges: 227 Proceedings of the 2020 EMNLP (Systems Demonstrations), pages 227–233 c November 16-20, 2020. 2020 Association for Computational Linguistics 1300BC 1046BC 771BC 475BC 222BC 220AD Figure 1: The historical evolution of the character “h"
2020.emnlp-demos.29,D18-1063,0,0.0129114,"tablets and recognize Cuneiform characters, Franken and van Gemert (2013); Nederhof (2015); Iglesias-Franjo and Vilares (2016) apply similar techniques to recognize Egyptian hieroglyphs. For understanding the ancient text, Snyder et al. (2010) first show the feasibility of automatically deciphering a dead language by designing a Bayesian model to match the alphabet with non-parallel data. Then, BergKirkpatrick and Klein (2011) propose a more effective decipherment approach and achieve promising results. Pourdamghani and Knight (2017) adopt a method similar to non-parallel machine translation (Mukherjee et al., 2018; Lample et al., 2018) to decipher related languages, which further inspires Luo et al. (2019) to propose a novel neural approach for automatic decipherment of Ugaritic and Linear B. Doostmohammadi and Nassajian (2019); Bernier-Colborne et al. (2019) explore to learn language models for Cuneiform Text. These previous efforts have inspired us to apply machine learning methods to the task of processing OBS. However, there are still three main challenges: 227 Proceedings of the 2020 EMNLP (Systems Demonstrations), pages 227–233 c November 16-20, 2020. 2020 Association for Computational Linguistic"
2020.emnlp-demos.29,D17-1266,0,0.0177155,". (2017); Yamauchi et al. (2018) utilize computer vision techniques to visualize Cuneiform tablets and recognize Cuneiform characters, Franken and van Gemert (2013); Nederhof (2015); Iglesias-Franjo and Vilares (2016) apply similar techniques to recognize Egyptian hieroglyphs. For understanding the ancient text, Snyder et al. (2010) first show the feasibility of automatically deciphering a dead language by designing a Bayesian model to match the alphabet with non-parallel data. Then, BergKirkpatrick and Klein (2011) propose a more effective decipherment approach and achieve promising results. Pourdamghani and Knight (2017) adopt a method similar to non-parallel machine translation (Mukherjee et al., 2018; Lample et al., 2018) to decipher related languages, which further inspires Luo et al. (2019) to propose a novel neural approach for automatic decipherment of Ugaritic and Linear B. Doostmohammadi and Nassajian (2019); Bernier-Colborne et al. (2019) explore to learn language models for Cuneiform Text. These previous efforts have inspired us to apply machine learning methods to the task of processing OBS. However, there are still three main challenges: 227 Proceedings of the 2020 EMNLP (Systems Demonstrations),"
2020.emnlp-demos.29,P10-1107,0,0.0277237,"ime-consuming to manually process ancient languages, some efforts have been devoted to utilizing machine learning techniques in this field. In order to detect and recognize ancient characters, Anderson and Levoy (2002); Rothacker et al. (2015); Mousavi and Lyashenko (2017); Rahma et al. (2017); Yamauchi et al. (2018) utilize computer vision techniques to visualize Cuneiform tablets and recognize Cuneiform characters, Franken and van Gemert (2013); Nederhof (2015); Iglesias-Franjo and Vilares (2016) apply similar techniques to recognize Egyptian hieroglyphs. For understanding the ancient text, Snyder et al. (2010) first show the feasibility of automatically deciphering a dead language by designing a Bayesian model to match the alphabet with non-parallel data. Then, BergKirkpatrick and Klein (2011) propose a more effective decipherment approach and achieve promising results. Pourdamghani and Knight (2017) adopt a method similar to non-parallel machine translation (Mukherjee et al., 2018; Lample et al., 2018) to decipher related languages, which further inspires Luo et al. (2019) to propose a novel neural approach for automatic decipherment of Ugaritic and Linear B. Doostmohammadi and Nassajian (2019); B"
2020.emnlp-demos.29,L18-1115,0,0.0281369,"ifice, agriculture, as well as births, illnesses, and deaths of royal members (Flad et al., 2008). Therefore, OBS documents constitute the earliest Chinese textual corpora, and to analyze and understand OBS is of great significance to historical research. Considering that it is often sophisticated and time-consuming to manually process ancient languages, some efforts have been devoted to utilizing machine learning techniques in this field. In order to detect and recognize ancient characters, Anderson and Levoy (2002); Rothacker et al. (2015); Mousavi and Lyashenko (2017); Rahma et al. (2017); Yamauchi et al. (2018) utilize computer vision techniques to visualize Cuneiform tablets and recognize Cuneiform characters, Franken and van Gemert (2013); Nederhof (2015); Iglesias-Franjo and Vilares (2016) apply similar techniques to recognize Egyptian hieroglyphs. For understanding the ancient text, Snyder et al. (2010) first show the feasibility of automatically deciphering a dead language by designing a Bayesian model to match the alphabet with non-parallel data. Then, BergKirkpatrick and Klein (2011) propose a more effective decipherment approach and achieve promising results. Pourdamghani and Knight (2017) a"
2020.emnlp-main.129,buyko-etal-2010-genereg,0,0.0403765,"erent ways. The early MUC series datasets (Grishman and Sundheim, 1996) define event extraction as a slot-filling task. The TDT corpus (Allan, 2012) and some recent datasets (Minard et al., 2016; Araki and Mitamura, 2018; Sims et al., 2019; Liu et al., 2019) follow the open-domain paradigm, which does not require models to classify events into pre-defined event types for better coverage but limits the downstream application of the extracted events. Some datasets are developed for ED on specific domains, like the biomedical domain (Pyysalo et al., 2007; Kim et al., 2008; Thompson et al., 2009; Buyko et al., 2010; N´edellec et al., 2013), literature (Sims et al., 2019), Twitter (Ritter et al., 2012; Guo et al., 2013) and breaking news (Pustejovsky et al., 2003). These datasets are also typically small-scale due to the inherent complexity of event annotation, but their different settings are complementary to our work. 7 Conclusion and Future work In this paper, we present a massive general domain event detection dataset (MAVEN), which significantly alleviates the data scarcity and low coverage problems of existing datasets. We conduct a thorough evaluation of the state-of-the-art ED models on MAVEN. Th"
2020.emnlp-main.129,P17-1038,0,0.0587272,"vember 16–20, 2020. 2020 Association for Computational Linguistics ern sophisticated models. Moreover, the covered event types in existing datasets are limited. The ACE 2005 English dataset only contains 8 event types and 33 specific subtypes. The Rich ERE ontology (Song et al., 2015) used by TAC KBP challenges (Ellis et al., 2015, 2016) covers 9 event types and 38 subtypes. The coverage of these datasets is low for general domain events, which results in the models trained on these datasets cannot be easily transferred and applied on general applications. Recent research (Huang et al., 2016; Chen et al., 2017) has shown that the existing datasets suffering from the data scarcity and low coverage problems are now inadequate for benchmarking emerging methods, i.e., the evaluation results are difficult to reflect the effectiveness of novel methods. To tackle these issues, some works adopt the distantly supervised methods (Mintz et al., 2009) to automatically annotate data with existing event facts in knowledge bases (Chen et al., 2017; Zeng et al., 2018; Araki and Mitamura, 2018) or use bootstrapping methods to generate new data (Ferguson et al., 2018; Wang et al., 2019b). However, the generated data"
2020.emnlp-main.129,P15-1017,0,0.690388,"uld recognize that the word “founded” is the trigger of a Found event. ED ∗ Elect: 183 问ure: 142 Transfer-Ownership: 127 Phone-Write: 123 Start-Position: 118 Trial-Hearing: 109 Charge-Indict: 106 The Other 20 Types (&lt;100 instances): 889 Corresponding author: Z.Liu (liuzy@tsinghua.edu.cn) Due to the rising requirement of event understanding, many efforts have been devoted to ED in recent years. The advanced models have been continuously proposed, including the feature-based models (Ji and Grishman, 2008; Gupta and Ji, 2009; Li et al., 2013; Araki and Mitamura, 2015) and advanced neural models (Chen et al., 2015; Nguyen and Grishman, 2015; Nguyen et al., 2016; Feng et al., 2016; Ghaeini et al., 2016; Liu et al., 2017; Zhao et al., 2018; Chen et al., 2018; Ding et al., 2019; Yan et al., 2019). Nevertheless, the benchmark datasets for ED are upgraded slowly. As event annotation is complex and expensive, the existing datasets are mostly small-scale. As shown in Figure 1, the most widely-used ACE 2005 English dataset (Walker et al., 2006) only contains 599 documents and 5, 349 annotated instances. Due to the inherent data imbalance problem, 20 of its 33 event types only have fewer than 100 annotated inst"
2020.emnlp-main.129,D18-1158,0,0.305267,"osition: 118 Trial-Hearing: 109 Charge-Indict: 106 The Other 20 Types (&lt;100 instances): 889 Corresponding author: Z.Liu (liuzy@tsinghua.edu.cn) Due to the rising requirement of event understanding, many efforts have been devoted to ED in recent years. The advanced models have been continuously proposed, including the feature-based models (Ji and Grishman, 2008; Gupta and Ji, 2009; Li et al., 2013; Araki and Mitamura, 2015) and advanced neural models (Chen et al., 2015; Nguyen and Grishman, 2015; Nguyen et al., 2016; Feng et al., 2016; Ghaeini et al., 2016; Liu et al., 2017; Zhao et al., 2018; Chen et al., 2018; Ding et al., 2019; Yan et al., 2019). Nevertheless, the benchmark datasets for ED are upgraded slowly. As event annotation is complex and expensive, the existing datasets are mostly small-scale. As shown in Figure 1, the most widely-used ACE 2005 English dataset (Walker et al., 2006) only contains 599 documents and 5, 349 annotated instances. Due to the inherent data imbalance problem, 20 of its 33 event types only have fewer than 100 annotated instances. As recent neural methods are typically data-hungry, these small-scale datasets are not sufficient for training and stably benchmarking mod"
2020.emnlp-main.129,N18-1076,0,0.0271789,"mains a challenging task and requires further research efforts. We also discuss further directions for general domain ED with empirical analyses. The source code and dataset can be obtained from https:// github.com/THU-KEG/MAVEN-dataset. 1 End-Position: 212 Transfer-Money: 198 Attack: 1543 Figure 1: Data distribution of the most widely-used ACE 2005 English dataset. It contains 33 event types, 599 documents and 5, 349 instances in total. is the first stage to extract event knowledge from text (Ahn, 2006) and also fundamental to various NLP applications (Yang et al., 2003; Basile et al., 2014; Cheng and Erk, 2018; Yang et al., 2019). Introduction Event detection (ED) is an important task of information extraction, which aims to identify event triggers (the words or phrases evoking events in text) and classify event types. For instance, in the sentence “Bill Gates founded Microsoft in 1975”, an ED model should recognize that the word “founded” is the trigger of a Found event. ED ∗ Elect: 183 问ure: 142 Transfer-Ownership: 127 Phone-Write: 123 Start-Position: 118 Trial-Hearing: 109 Charge-Indict: 106 The Other 20 Types (&lt;100 instances): 889 Corresponding author: Z.Liu (liuzy@tsinghua.edu.cn) Due to the r"
2020.emnlp-main.129,N19-1423,0,0.0330633,"eural network baseline, which adopts the widely-used bi-directional long shortterm memory network to learn textual representations, and then uses the hidden states at the positions of trigger candidates for classifying event types. (3) MOGANED (Yan et al., 2019) is an advanced graph neural network (GNN) model. It proposes a multi-order graph attention network to effectively model the multi-order syntactic relations in dependency trees and improve ED. (4) DMBERT (Wang et al., 2019b) is a vanilla BERTbased model. It takes advantage of the effective pretrained language representation model BERT (Devlin et al., 2019) and also adopts the dynamic multi-pooling mechanism to aggregate features for ED. We use the BERTBASE architecture in our experiments. (5) Different from the above tokenlevel classification models, BiLSTM+CRF and BERT+CRF are sequence labeling models. To verify the effectiveness of modeling multiple event correlations, the two models both adopt the conditional random field (CRF) (Lafferty et al., 2001) as their output layers, which can model structured output dependencies. And they use BiLSTM and BERTBASE as their feature extractors respectively. As we manually tune hyperparameters and some t"
2020.emnlp-main.129,D19-1033,1,0.851706,"Hearing: 109 Charge-Indict: 106 The Other 20 Types (&lt;100 instances): 889 Corresponding author: Z.Liu (liuzy@tsinghua.edu.cn) Due to the rising requirement of event understanding, many efforts have been devoted to ED in recent years. The advanced models have been continuously proposed, including the feature-based models (Ji and Grishman, 2008; Gupta and Ji, 2009; Li et al., 2013; Araki and Mitamura, 2015) and advanced neural models (Chen et al., 2015; Nguyen and Grishman, 2015; Nguyen et al., 2016; Feng et al., 2016; Ghaeini et al., 2016; Liu et al., 2017; Zhao et al., 2018; Chen et al., 2018; Ding et al., 2019; Yan et al., 2019). Nevertheless, the benchmark datasets for ED are upgraded slowly. As event annotation is complex and expensive, the existing datasets are mostly small-scale. As shown in Figure 1, the most widely-used ACE 2005 English dataset (Walker et al., 2006) only contains 599 documents and 5, 349 annotated instances. Due to the inherent data imbalance problem, 20 of its 33 event types only have fewer than 100 annotated instances. As recent neural methods are typically data-hungry, these small-scale datasets are not sufficient for training and stably benchmarking mod1652 Proceedings of"
2020.emnlp-main.129,D16-1264,0,0.124091,"Missing"
2020.emnlp-main.129,P19-1276,0,0.0175457,"traction models (Ji and Grishman, 2008; Li et al., 2013; Chen et al., 2015; Feng et al., 2016; Liu et al., 2017; Zhao et al., 2018; Yan et al., 2019) are developed on these datasets. Our MAVEN follows the effective framework and extends it to numerous general domain event types and data instances. There are also various datasets defining the ED task in different ways. The early MUC series datasets (Grishman and Sundheim, 1996) define event extraction as a slot-filling task. The TDT corpus (Allan, 2012) and some recent datasets (Minard et al., 2016; Araki and Mitamura, 2018; Sims et al., 2019; Liu et al., 2019) follow the open-domain paradigm, which does not require models to classify events into pre-defined event types for better coverage but limits the downstream application of the extracted events. Some datasets are developed for ED on specific domains, like the biomedical domain (Pyysalo et al., 2007; Kim et al., 2008; Thompson et al., 2009; Buyko et al., 2010; N´edellec et al., 2013), literature (Sims et al., 2019), Twitter (Ritter et al., 2012; Guo et al., 2013) and breaking news (Pustejovsky et al., 2003). These datasets are also typically small-scale due to the inherent complexity of event a"
2020.emnlp-main.129,D18-1156,0,0.0338402,"Missing"
2020.emnlp-main.129,P19-1353,0,0.0176268,"of ED and event extraction models (Ji and Grishman, 2008; Li et al., 2013; Chen et al., 2015; Feng et al., 2016; Liu et al., 2017; Zhao et al., 2018; Yan et al., 2019) are developed on these datasets. Our MAVEN follows the effective framework and extends it to numerous general domain event types and data instances. There are also various datasets defining the ED task in different ways. The early MUC series datasets (Grishman and Sundheim, 1996) define event extraction as a slot-filling task. The TDT corpus (Allan, 2012) and some recent datasets (Minard et al., 2016; Araki and Mitamura, 2018; Sims et al., 2019; Liu et al., 2019) follow the open-domain paradigm, which does not require models to classify events into pre-defined event types for better coverage but limits the downstream application of the extracted events. Some datasets are developed for ED on specific domains, like the biomedical domain (Pyysalo et al., 2007; Kim et al., 2008; Thompson et al., 2009; Buyko et al., 2010; N´edellec et al., 2013), literature (Sims et al., 2019), Twitter (Ritter et al., 2012; Guo et al., 2013) and breaking news (Pustejovsky et al., 2003). These datasets are also typically small-scale due to the inherent co"
2020.emnlp-main.129,P19-1429,0,0.374622,"Missing"
2020.emnlp-main.129,W15-0812,0,0.406914,"problem, 20 of its 33 event types only have fewer than 100 annotated instances. As recent neural methods are typically data-hungry, these small-scale datasets are not sufficient for training and stably benchmarking mod1652 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1652–1671, c November 16–20, 2020. 2020 Association for Computational Linguistics ern sophisticated models. Moreover, the covered event types in existing datasets are limited. The ACE 2005 English dataset only contains 8 event types and 33 specific subtypes. The Rich ERE ontology (Song et al., 2015) used by TAC KBP challenges (Ellis et al., 2015, 2016) covers 9 event types and 38 subtypes. The coverage of these datasets is low for general domain events, which results in the models trained on these datasets cannot be easily transferred and applied on general applications. Recent research (Huang et al., 2016; Chen et al., 2017) has shown that the existing datasets suffering from the data scarcity and low coverage problems are now inadequate for benchmarking emerging methods, i.e., the evaluation results are difficult to reflect the effectiveness of novel methods. To tackle these issues, so"
2020.emnlp-main.129,L16-1699,0,0.0619031,"Missing"
2020.emnlp-main.129,P09-1113,0,0.125379,"2016) covers 9 event types and 38 subtypes. The coverage of these datasets is low for general domain events, which results in the models trained on these datasets cannot be easily transferred and applied on general applications. Recent research (Huang et al., 2016; Chen et al., 2017) has shown that the existing datasets suffering from the data scarcity and low coverage problems are now inadequate for benchmarking emerging methods, i.e., the evaluation results are difficult to reflect the effectiveness of novel methods. To tackle these issues, some works adopt the distantly supervised methods (Mintz et al., 2009) to automatically annotate data with existing event facts in knowledge bases (Chen et al., 2017; Zeng et al., 2018; Araki and Mitamura, 2018) or use bootstrapping methods to generate new data (Ferguson et al., 2018; Wang et al., 2019b). However, the generated data are inevitably noisy and homogeneous due to the limited number and low diversity of event facts and seed data instances. In this paper, we present MAVEN, a humanannotated massive general domain event detection dataset constructed from English Wikipedia and FrameNet (Baker et al., 1998), which can alleviate the data scarcity and low c"
2020.emnlp-main.129,P18-2066,0,0.180224,"-Write: 123 Start-Position: 118 Trial-Hearing: 109 Charge-Indict: 106 The Other 20 Types (&lt;100 instances): 889 Corresponding author: Z.Liu (liuzy@tsinghua.edu.cn) Due to the rising requirement of event understanding, many efforts have been devoted to ED in recent years. The advanced models have been continuously proposed, including the feature-based models (Ji and Grishman, 2008; Gupta and Ji, 2009; Li et al., 2013; Araki and Mitamura, 2015) and advanced neural models (Chen et al., 2015; Nguyen and Grishman, 2015; Nguyen et al., 2016; Feng et al., 2016; Ghaeini et al., 2016; Liu et al., 2017; Zhao et al., 2018; Chen et al., 2018; Ding et al., 2019; Yan et al., 2019). Nevertheless, the benchmark datasets for ED are upgraded slowly. As event annotation is complex and expensive, the existing datasets are mostly small-scale. As shown in Figure 1, the most widely-used ACE 2005 English dataset (Walker et al., 2006) only contains 599 documents and 5, 349 annotated instances. Due to the inherent data imbalance problem, 20 of its 33 event types only have fewer than 100 annotated instances. As recent neural methods are typically data-hungry, these small-scale datasets are not sufficient for training and stab"
2020.emnlp-main.129,N19-1105,1,0.947896,"research (Huang et al., 2016; Chen et al., 2017) has shown that the existing datasets suffering from the data scarcity and low coverage problems are now inadequate for benchmarking emerging methods, i.e., the evaluation results are difficult to reflect the effectiveness of novel methods. To tackle these issues, some works adopt the distantly supervised methods (Mintz et al., 2009) to automatically annotate data with existing event facts in knowledge bases (Chen et al., 2017; Zeng et al., 2018; Araki and Mitamura, 2018) or use bootstrapping methods to generate new data (Ferguson et al., 2018; Wang et al., 2019b). However, the generated data are inevitably noisy and homogeneous due to the limited number and low diversity of event facts and seed data instances. In this paper, we present MAVEN, a humanannotated massive general domain event detection dataset constructed from English Wikipedia and FrameNet (Baker et al., 1998), which can alleviate the data scarcity and low coverage problems: (1) Our MAVEN dataset contains 111, 611 different events, 118, 732 event mentions, which is twenty times larger than the most widely-used ACE 2005 dataset, and 4, 480 annotated documents in total. To the best of our"
2020.emnlp-main.129,D19-1584,1,0.922895,"research (Huang et al., 2016; Chen et al., 2017) has shown that the existing datasets suffering from the data scarcity and low coverage problems are now inadequate for benchmarking emerging methods, i.e., the evaluation results are difficult to reflect the effectiveness of novel methods. To tackle these issues, some works adopt the distantly supervised methods (Mintz et al., 2009) to automatically annotate data with existing event facts in knowledge bases (Chen et al., 2017; Zeng et al., 2018; Araki and Mitamura, 2018) or use bootstrapping methods to generate new data (Ferguson et al., 2018; Wang et al., 2019b). However, the generated data are inevitably noisy and homogeneous due to the limited number and low diversity of event facts and seed data instances. In this paper, we present MAVEN, a humanannotated massive general domain event detection dataset constructed from English Wikipedia and FrameNet (Baker et al., 1998), which can alleviate the data scarcity and low coverage problems: (1) Our MAVEN dataset contains 111, 611 different events, 118, 732 event mentions, which is twenty times larger than the most widely-used ACE 2005 dataset, and 4, 480 annotated documents in total. To the best of our"
2020.emnlp-main.129,D19-1582,0,0.510191,"-Indict: 106 The Other 20 Types (&lt;100 instances): 889 Corresponding author: Z.Liu (liuzy@tsinghua.edu.cn) Due to the rising requirement of event understanding, many efforts have been devoted to ED in recent years. The advanced models have been continuously proposed, including the feature-based models (Ji and Grishman, 2008; Gupta and Ji, 2009; Li et al., 2013; Araki and Mitamura, 2015) and advanced neural models (Chen et al., 2015; Nguyen and Grishman, 2015; Nguyen et al., 2016; Feng et al., 2016; Ghaeini et al., 2016; Liu et al., 2017; Zhao et al., 2018; Chen et al., 2018; Ding et al., 2019; Yan et al., 2019). Nevertheless, the benchmark datasets for ED are upgraded slowly. As event annotation is complex and expensive, the existing datasets are mostly small-scale. As shown in Figure 1, the most widely-used ACE 2005 English dataset (Walker et al., 2006) only contains 599 documents and 5, 349 annotated instances. Due to the inherent data imbalance problem, 20 of its 33 event types only have fewer than 100 annotated instances. As recent neural methods are typically data-hungry, these small-scale datasets are not sufficient for training and stably benchmarking mod1652 Proceedings of the 2020 Conferenc"
2020.emnlp-main.129,C96-1079,0,\N,Missing
2020.emnlp-main.129,W06-0901,0,\N,Missing
2020.emnlp-main.129,P98-1013,0,\N,Missing
2020.emnlp-main.129,C98-1013,0,\N,Missing
2020.emnlp-main.129,P09-2093,0,\N,Missing
2020.emnlp-main.129,P06-4018,0,\N,Missing
2020.emnlp-main.129,P13-1024,0,\N,Missing
2020.emnlp-main.129,P08-1030,0,\N,Missing
2020.emnlp-main.129,P13-1008,0,\N,Missing
2020.emnlp-main.129,D14-1162,0,\N,Missing
2020.emnlp-main.129,P15-2060,0,\N,Missing
2020.emnlp-main.129,D15-1247,0,\N,Missing
2020.emnlp-main.129,doddington-etal-2004-automatic,0,\N,Missing
2020.emnlp-main.129,N16-1034,0,\N,Missing
2020.emnlp-main.129,P16-1025,0,\N,Missing
2020.emnlp-main.129,P17-1164,0,\N,Missing
2020.emnlp-main.129,N18-2058,0,\N,Missing
2020.emnlp-main.129,C18-1075,0,\N,Missing
2020.emnlp-main.129,D18-1259,0,\N,Missing
2020.emnlp-main.129,W13-2001,0,\N,Missing
2020.emnlp-main.129,P19-1521,0,\N,Missing
2020.emnlp-main.129,P16-2060,0,\N,Missing
2020.emnlp-main.129,P18-1201,0,\N,Missing
2020.emnlp-main.298,2020.acl-main.142,0,0.0155763,", 2019) has become a standard procedure. Based on BERT, Baldini Soares et al. (2019) propose matching the blanks, an RE-oriented pre-trained model to learn relational patterns from text. A different direction is to inject entity knowledge, in the form of entity embeddings, into BERT (Zhang et al., 2019; Peters et al., 2019; Liu et al., 2020). We do not discuss this line of work here for their promotion comes from relational knowledge of external sources, while we focus on text itself in the paper. Analysis of RE Han et al. (2020) suggest to study how RE models learn from context and mentions. Alt et al. (2020) also point out that there may exist shallow cues in entity mentions. However, there have not been systematical analyses about the topic and to the best of our knowledge, we are the first one to thoroughly carry out these studies. 6 Conclusion In this paper, we thoroughly study how textual context and entity mentions affect RE models respectively. Experiments and case studies prove that (i) both context and entity mentions (mainly as type information) provide critical information for relation extraction, and (ii) existing RE datasets may leak superficial cues through entity mentions and models"
2020.emnlp-main.298,P19-1279,0,0.356781,"n Musk) from the sentence in Figure 1. Utilizing the structured knowledge captured by RE, we can construct or complete knowledge graphs (KGs), and eventually support downstream applications like question answering (Bordes et al., 2014), dialog systems (Madotto et al., 2018) and search ∗ † Type: person ID: Q317521 Other info: citizenship: US occupation: entrepreneur … Equal contribution Corresponding author e-mail: liuzy@tsinghua.edu.cn engines (Xiong et al., 2017). With the recent advance of deep learning, neural relation extraction (NRE) models (Socher et al., 2012; Liu et al., 2013; Baldini Soares et al., 2019) have achieved the latest state-of-the-art results and some of them are even comparable with human performance on several public RE benchmarks. The success of NRE models on current RE benchmarks makes us wonder which type of information these models actually grasp to help them extract correct relations. The analysis of this problem may indicate the nature of these models and reveal their remaining problems to be further explored. Generally, in a typical RE setting, there are two main sources of information in text that might help RE models classify relations: textual context and entity mention"
2020.emnlp-main.298,D14-1067,0,0.0277137,"ion provided by textual context and entity mentions in a typical RE scenario. From mentions, we can acquire type information and link entities to KGs, and access further knowledge about them. The IDs in the figure are from Wikidata. Introduction Relation extraction (RE) aims at extracting relational facts between entities from text, e.g., extracting the fact (SpaceX, founded by, Elon Musk) from the sentence in Figure 1. Utilizing the structured knowledge captured by RE, we can construct or complete knowledge graphs (KGs), and eventually support downstream applications like question answering (Bordes et al., 2014), dialog systems (Madotto et al., 2018) and search ∗ † Type: person ID: Q317521 Other info: citizenship: US occupation: entrepreneur … Equal contribution Corresponding author e-mail: liuzy@tsinghua.edu.cn engines (Xiong et al., 2017). With the recent advance of deep learning, neural relation extraction (NRE) models (Socher et al., 2012; Liu et al., 2013; Baldini Soares et al., 2019) have achieved the latest state-of-the-art results and some of them are even comparable with human performance on several public RE benchmarks. The success of NRE models on current RE benchmarks makes us wonder whic"
2020.emnlp-main.298,H05-1091,0,0.188054,"harder for models to learn to extract relational patterns from context and easier to overfit to superficial cues of mentions, due to the limited training data. However, with the contrastive pre-training, our model can relatively take better use of textual context while avoiding being biased by entities, and outperform the other baselines by a large margin. 5 Related Work Development of RE RE of early days has gone through pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), kernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relations by only a handful of s"
2020.emnlp-main.298,W97-1002,0,0.699414,"of these models and reveal their remaining problems to be further explored. Generally, in a typical RE setting, there are two main sources of information in text that might help RE models classify relations: textual context and entity mentions (names). From human intuition, textual context should be the main source of information for RE. Researchers have reached a consensus that there exist interpretable patterns in textual context that express relational facts. For example, in Figure 1, “... be founded ... by ...” is a pattern for the relation founded by. The early RE systems (Huffman, 1995; Califf and Mooney, 1997) formalize patterns into string templates and determine relations by 3661 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3661–3672, c November 16–20, 2020. 2020 Association for Computational Linguistics matching these templates. The later neural models (Socher et al., 2012; Liu et al., 2013) prefer to encode patterns into distributed representations and then predict relations via representation matching. Compared with rigid string templates, distributed representations used in neural models are more generalized and perform better. Besides, entity"
2020.emnlp-main.298,P04-1054,0,0.103347,"nd few-shot settings, it is harder for models to learn to extract relational patterns from context and easier to overfit to superficial cues of mentions, due to the limited training data. However, with the contrastive pre-training, our model can relatively take better use of textual context while avoiding being biased by entities, and outperform the other baselines by a large margin. 5 Related Work Development of RE RE of early days has gone through pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), kernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relat"
2020.emnlp-main.298,N19-1423,0,0.0521177,"se to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relations by only a handful of samples. Pre-training for RE With the recent advance of pre-trained language models (Devlin et al., 2019), applying BERT-like models as the backbone of RE systems (Baldini Soares et al., 2019) has become a standard procedure. Based on BERT, Baldini Soares et al. (2019) propose matching the blanks, an RE-oriented pre-trained model to learn relational patterns from text. A different direction is to inject entity knowledge, in the form of entity embeddings, into BERT (Zhang et al., 2019; Peters et al., 2019; Liu et al., 2020). We do not discuss this line of work here for their promotion comes from relational knowledge of external sources, while we focus on text itself in the paper. Analysis of RE Ha"
2020.emnlp-main.298,D19-1649,1,0.865961,"ernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relations by only a handful of samples. Pre-training for RE With the recent advance of pre-trained language models (Devlin et al., 2019), applying BERT-like models as the backbone of RE systems (Baldini Soares et al., 2019) has become a standard procedure. Based on BERT, Baldini Soares et al. (2019) propose matching the blanks, an RE-oriented pre-trained model to learn relational patterns from text. A different direction is to inject entity knowledge, in the form of entity embeddings, into BERT (Zhang et al., 2019; Peters et al., 2019; Liu et al., 2020). We do not disc"
2020.emnlp-main.298,D19-3029,1,0.896375,"Missing"
2020.emnlp-main.298,D18-1514,1,0.881235,"u et al., 2005), kernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relations by only a handful of samples. Pre-training for RE With the recent advance of pre-trained language models (Devlin et al., 2019), applying BERT-like models as the backbone of RE systems (Baldini Soares et al., 2019) has become a standard procedure. Based on BERT, Baldini Soares et al. (2019) propose matching the blanks, an RE-oriented pre-trained model to learn relational patterns from text. A different direction is to inject entity knowledge, in the form of entity embeddings, into BERT (Zhang et al., 2019; Peters et al., 2019; Liu et al., 20"
2020.emnlp-main.298,W09-2415,0,0.134815,"Missing"
2020.emnlp-main.298,P16-1200,1,0.903989,"sed methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), kernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relations by only a handful of samples. Pre-training for RE With the recent advance of pre-trained language models (Devlin et al., 2019), applying BERT-like models as the backbone of RE systems (Baldini Soares et al., 2019) has become a standard procedure. Based on BERT, Baldini Soares et al. (2019) propose matching the blanks, an RE-oriented pre-trained model to learn relational patterns from text. A different direction is to inject entity knowledge, in t"
2020.emnlp-main.298,P18-1136,0,0.0177318,"tity mentions in a typical RE scenario. From mentions, we can acquire type information and link entities to KGs, and access further knowledge about them. The IDs in the figure are from Wikidata. Introduction Relation extraction (RE) aims at extracting relational facts between entities from text, e.g., extracting the fact (SpaceX, founded by, Elon Musk) from the sentence in Figure 1. Utilizing the structured knowledge captured by RE, we can construct or complete knowledge graphs (KGs), and eventually support downstream applications like question answering (Bordes et al., 2014), dialog systems (Madotto et al., 2018) and search ∗ † Type: person ID: Q317521 Other info: citizenship: US occupation: entrepreneur … Equal contribution Corresponding author e-mail: liuzy@tsinghua.edu.cn engines (Xiong et al., 2017). With the recent advance of deep learning, neural relation extraction (NRE) models (Socher et al., 2012; Liu et al., 2013; Baldini Soares et al., 2019) have achieved the latest state-of-the-art results and some of them are even comparable with human performance on several public RE benchmarks. The success of NRE models on current RE benchmarks makes us wonder which type of information these models actu"
2020.emnlp-main.298,N13-1095,0,0.0136486,"lopment of RE RE of early days has gone through pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), kernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relations by only a handful of samples. Pre-training for RE With the recent advance of pre-trained language models (Devlin et al., 2019), applying BERT-like models as the backbone of RE systems (Baldini Soares et al., 2019) has become a standard procedure. Based on BERT, Baldini Soares et al. (2019) propose matching the blanks, an RE-oriented pre-trained model to learn relational patterns from text"
2020.emnlp-main.298,D15-1203,0,0.0908899,"through pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), kernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relations by only a handful of samples. Pre-training for RE With the recent advance of pre-trained language models (Devlin et al., 2019), applying BERT-like models as the backbone of RE systems (Baldini Soares et al., 2019) has become a standard procedure. Based on BERT, Baldini Soares et al. (2019) propose matching the blanks, an RE-oriented pre-trained model to learn relational patterns from text. A different direction is to inject ent"
2020.emnlp-main.298,P09-1113,0,0.906212,"d to MTB (in the dotted box), our method samples data with better diversity, which can not only increase the coverage of entity types and diverse context but also reduce the possibility of memorizing entity names. we adopt the idea of contrastive learning (Hadsell et al., 2006), which aims to learn representations by pulling “neighbors” together and pushing “nonneighbors” apart. After this, “neighbor” instances will have similar representations. So it is important to define “neighbors” in contrastive learning and we utilize the information from KGs to to that. Inspired by distant supervision (Mintz et al., 2009), we assume that sentences with entity pairs sharing the same relation in KGs are “neighbors”. Formally, denote the KG we use as K, which is composed of relational facts. Denote two random sentences as XA and XB , which have entity mentions hA , tA and hB , tB respectively. We define XA and XB as “neighbors” if there is a relation r such that (hA , r, tA ) ∈ K and (hB , r, tB ) ∈ K. We take Wikidata as the KG since it can be easily linked to the Wikipedia corpus used for pretraining. When training, we first sample a relation r with respect to its proportion in the KG, and then sample a sentenc"
2020.emnlp-main.298,C14-1220,0,0.793202,"xt+Mention (C+M) This is the most widely-used RE setting, where the whole sentence 3662 Model C+M C+T OnlyC OnlyM OnlyT C+M CNN BERT MTB 0.547 0.683 0.691 0.591 0.686 0.696 0.441 0.570 0.581 0.434 0.466 0.433 0.295 0.277 0.304 Although her family was from Arkansas, she was born in Washington state, where ... Label: per:state of birth Prediction: per:state of residence Table 1: TACRED results (micro F1 ) with CNN, BERT and MTB on different settings. (with both context and highlighted entity mentions) is provided. To let the models know where the entity mentions are, we use position embeddings (Zeng et al., 2014) for the CNN model and special entity markers (Zhang et al., 2019; Baldini Soares et al., 2019) for the pre-trained BERT. Context+Type (C+T) We replace entity mentions with their types provided in TACRED. We use special tokens to represent them: for example, we use [person] and [date] to represent an entity with type person and date respectively. Different from Zhang et al. (2017), we do not repeat the special tokens for entity-length times to avoid leaking entity length information. Besides the above settings, we also adopt three synthetic settings to study how much information context or men"
2020.emnlp-main.298,W15-1506,0,0.0213763,"better pre-training technique is a reliable direction towards better RE. 2 Pilot Experiment and Analysis To study which type of information affects existing neural RE models to make decisions, we first introduce some preliminaries of RE models and settings and then conduct pilot experiments as well as empirical analyses in this section. 2.1 Models and Dataset There are various NRE models proposed in previous work (refer to Section 5), and we select the following three representative neural models for our pilot experiments and analyses: CNN We use the convolutional neural networks described in Nguyen and Grishman (2015) and augment the inputs with part-of-speech, named entity recognition and position embeddings following Zhang et al. (2017). BERT BERT is a pre-trained language model that has been widely used in NLP tasks. We use BERT for RE following Baldini Soares et al. (2019). In short, we highlight entity mentions in sentences by special markers and use the concatenations of entity representations for classification. Matching the blanks (MTB) MTB (Baldini Soares et al., 2019) is an RE-oriented pre-trained model based on BERT. It is pre-trained by classifying whether two sentences mention the same entity"
2020.emnlp-main.298,D19-1005,0,0.0995482,"3) prefer to encode patterns into distributed representations and then predict relations via representation matching. Compared with rigid string templates, distributed representations used in neural models are more generalized and perform better. Besides, entity mentions also provide much information for relation classification. As shown in Figure 1, we can acquire the types of entities from their mentions, which could help to filter out those impossible relations. Besides, if these entities can be linked to KGs, models can introduce external knowledge from KGs to help RE (Zhang et al., 2019; Peters et al., 2019). Moreover, for pre-trained language models, which are widely adopted for recent RE models, there may be knowledge about entities inherently stored in their parameters after pre-training (Petroni et al., 2019). In this paper, we carry out extensive experiments to study to what extent RE models rely on the two information sources. We find out that: (1) Both context and entity mentions are crucial for RE. As shown in our experiments, while context is the main source to support classification, entity mentions also provide critical information, most of which is the type information of entities. (2"
2020.emnlp-main.298,D19-1250,0,0.0644701,"Missing"
2020.emnlp-main.298,D17-1004,0,0.334618,"formation affects existing neural RE models to make decisions, we first introduce some preliminaries of RE models and settings and then conduct pilot experiments as well as empirical analyses in this section. 2.1 Models and Dataset There are various NRE models proposed in previous work (refer to Section 5), and we select the following three representative neural models for our pilot experiments and analyses: CNN We use the convolutional neural networks described in Nguyen and Grishman (2015) and augment the inputs with part-of-speech, named entity recognition and position embeddings following Zhang et al. (2017). BERT BERT is a pre-trained language model that has been widely used in NLP tasks. We use BERT for RE following Baldini Soares et al. (2019). In short, we highlight entity mentions in sentences by special markers and use the concatenations of entity representations for classification. Matching the blanks (MTB) MTB (Baldini Soares et al., 2019) is an RE-oriented pre-trained model based on BERT. It is pre-trained by classifying whether two sentences mention the same entity pair with entity mentions randomly masked. It is fine-tuned for RE in the same way as BERT. Since it is not publicly releas"
2020.emnlp-main.298,P19-1139,1,0.940049,"012; Liu et al., 2013) prefer to encode patterns into distributed representations and then predict relations via representation matching. Compared with rigid string templates, distributed representations used in neural models are more generalized and perform better. Besides, entity mentions also provide much information for relation classification. As shown in Figure 1, we can acquire the types of entities from their mentions, which could help to filter out those impossible relations. Besides, if these entities can be linked to KGs, models can introduce external knowledge from KGs to help RE (Zhang et al., 2019; Peters et al., 2019). Moreover, for pre-trained language models, which are widely adopted for recent RE models, there may be knowledge about entities inherently stored in their parameters after pre-training (Petroni et al., 2019). In this paper, we carry out extensive experiments to study to what extent RE models rely on the two information sources. We find out that: (1) Both context and entity mentions are crucial for RE. As shown in our experiments, while context is the main source to support classification, entity mentions also provide critical information, most of which is the type infor"
2020.emnlp-main.298,P05-1053,0,0.416681,"for OnlyC and OnlyM. In the low resource and few-shot settings, it is harder for models to learn to extract relational patterns from context and easier to overfit to superficial cues of mentions, due to the limited training data. However, with the contrastive pre-training, our model can relatively take better use of textual context while avoiding being biased by entities, and outperform the other baselines by a large margin. 5 Related Work Development of RE RE of early days has gone through pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), kernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 201"
2020.emnlp-main.298,C02-1151,0,0.404876,"nal patterns from context and easier to overfit to superficial cues of mentions, due to the limited training data. However, with the contrastive pre-training, our model can relatively take better use of textual context while avoiding being biased by entities, and outperform the other baselines by a large margin. 5 Related Work Development of RE RE of early days has gone through pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), kernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relations by only a handful of samples. Pre-training for RE With the r"
2020.emnlp-main.298,W04-2401,0,0.450234,"Missing"
2020.emnlp-main.298,D12-1110,0,0.488425,"., extracting the fact (SpaceX, founded by, Elon Musk) from the sentence in Figure 1. Utilizing the structured knowledge captured by RE, we can construct or complete knowledge graphs (KGs), and eventually support downstream applications like question answering (Bordes et al., 2014), dialog systems (Madotto et al., 2018) and search ∗ † Type: person ID: Q317521 Other info: citizenship: US occupation: entrepreneur … Equal contribution Corresponding author e-mail: liuzy@tsinghua.edu.cn engines (Xiong et al., 2017). With the recent advance of deep learning, neural relation extraction (NRE) models (Socher et al., 2012; Liu et al., 2013; Baldini Soares et al., 2019) have achieved the latest state-of-the-art results and some of them are even comparable with human performance on several public RE benchmarks. The success of NRE models on current RE benchmarks makes us wonder which type of information these models actually grasp to help them extract correct relations. The analysis of this problem may indicate the nature of these models and reveal their remaining problems to be further explored. Generally, in a typical RE setting, there are two main sources of information in text that might help RE models classi"
2020.emnlp-main.300,D19-1498,0,0.0662053,"g tasks. Experimental results on the large-scale DocRE benchmark show that our model can capture useful information from noisy DS data and achieve promising results. The source code of this paper can be found in https://github.com/thunlp/DSDocRE. 1 Now James serves for the Lakers ... [8] James locate_in Lebron James member_of educated_at Vincent–St. Mary High School in his hometown. St. Vincent–St. Mary High School Lakers Figure 1: An example of DocRE. Given a document, DocRE models should capture the relational semantics across sentences to extract multiple relational facts. tence relations (Christopoulou et al., 2019). Fig. 1 gives a brief illustration of DocRE. Relation extraction (RE) aims to identify relational facts between entities from texts. Recently, neural relation extraction (NRE) models have been verified in sentence-level RE (Zeng et al., 2014). Distant supervision (DS) (Mintz et al., 2009) provides large-scale distantly-supervised data that multiplies instances and enables sufficient model training. Sentence-level RE focuses on extracting intrasentence relations between entities in a sentence. However, it is extremely restricted with generality and coverage in practice, since there are plenty"
2020.emnlp-main.300,N19-1423,0,0.174441,"entity pairs within three consecutive sentences. Different from these works, we bring in document-level DS to DocRE and conduct pre-training to denoise these DS data. 3 Methodology In this section, we present our proposed model in detail. Fig. 2 gives an illustration of our framework. We first apply the pre-denoising module to screen out some NA instances from all documents. Then we pre-train the document encoder with three pre-training tasks on the document-level distantly supervised dataset. Finally, we fine-tune the model on the human-annotated dataset. 3.1 Document Encoder We adopt BERT (Devlin et al., 2019) as the document encoder to encode documents into representations of entity mentions, entities and relational instances. Let D = {ωi }ni=1 denote the input docu|V | ment which consists of n tokens, and V = {ei }i=1 be the set of entities mentioned in the document, i where entity ei = {mji }lj=1 contains li mentions in the document. Following Soares et al. (2019), we use entity markers [Ei] and [/Ei] for each entity ei . The start marker [Ei] is inserted at the begin of all mentions of entity ei , and the end marker [/Ei] is inserted at the end. 3684 We use BERT to encode the token sequence wit"
2020.emnlp-main.300,P16-1200,1,0.799391,"f DS data and verify the effectiveness of our pre-trained model for DocRE. To the best of our knowledge, we are the first to denoise document-level DS with pre-trained models. We will release our codes in the future. 2 Related Work Sentence-level RE. Conventional NRE models focus on sentence-level supervised RE (Zeng et al., 2014; Takanobu et al., 2019), which have achieved superior results on various benchmarks. Other approaches focus on using more data with distant supervision mechanism (Mintz et al., 2009; Min et al., 2013). To denoise distantly supervised corpus, they introduce attention (Lin et al., 2016; Zhou et al., 2018), generative adversarial training (Qin et al., 2018) and reinforcement learning (Feng et al., 2018) to select informative instances. It is hard to directly adopt these models to DocRE, since DocRE should extract multiple relational facts from each document. Soares et al. (2019) propose a pretrained model for sentence-level RE. Document-level RE. Document-level RE attempts Bilinear Relation Detection Mention-Entity Matching Pre-training Tasks Linear Linear Relational Fact Alignment Relational Instance Mention/Entity Document Encoder Sample data for pre-training Bilinear Ment"
2020.emnlp-main.300,N13-1095,0,0.0321775,"nduct detailed analysis and ablation test, which further highlight the significance of DS data and verify the effectiveness of our pre-trained model for DocRE. To the best of our knowledge, we are the first to denoise document-level DS with pre-trained models. We will release our codes in the future. 2 Related Work Sentence-level RE. Conventional NRE models focus on sentence-level supervised RE (Zeng et al., 2014; Takanobu et al., 2019), which have achieved superior results on various benchmarks. Other approaches focus on using more data with distant supervision mechanism (Mintz et al., 2009; Min et al., 2013). To denoise distantly supervised corpus, they introduce attention (Lin et al., 2016; Zhou et al., 2018), generative adversarial training (Qin et al., 2018) and reinforcement learning (Feng et al., 2018) to select informative instances. It is hard to directly adopt these models to DocRE, since DocRE should extract multiple relational facts from each document. Soares et al. (2019) propose a pretrained model for sentence-level RE. Document-level RE. Document-level RE attempts Bilinear Relation Detection Mention-Entity Matching Pre-training Tasks Linear Linear Relational Fact Alignment Relational"
2020.emnlp-main.300,P09-1113,0,0.825376,"aselines. We also conduct detailed analysis and ablation test, which further highlight the significance of DS data and verify the effectiveness of our pre-trained model for DocRE. To the best of our knowledge, we are the first to denoise document-level DS with pre-trained models. We will release our codes in the future. 2 Related Work Sentence-level RE. Conventional NRE models focus on sentence-level supervised RE (Zeng et al., 2014; Takanobu et al., 2019), which have achieved superior results on various benchmarks. Other approaches focus on using more data with distant supervision mechanism (Mintz et al., 2009; Min et al., 2013). To denoise distantly supervised corpus, they introduce attention (Lin et al., 2016; Zhou et al., 2018), generative adversarial training (Qin et al., 2018) and reinforcement learning (Feng et al., 2018) to select informative instances. It is hard to directly adopt these models to DocRE, since DocRE should extract multiple relational facts from each document. Soares et al. (2019) propose a pretrained model for sentence-level RE. Document-level RE. Document-level RE attempts Bilinear Relation Detection Mention-Entity Matching Pre-training Tasks Linear Linear Relational Fact A"
2020.emnlp-main.300,Q17-1008,0,0.0213299,"eep Transformer (BERT) Pre-denoise doc B Entity Pooling Document Encoder doc A Score Relation … [CLS] … [Ei] Entityi [/Ei] … [Ej] Entityj [/Ej] … [Ei] Entityi [/Ei] … [Ej] Entityj [/Ej] [SEP] Figure 2: The framework of our proposed model. to extend the scope of knowledge acquisition to the document level, which has attracted great attention recently (Yao et al., 2019). Some works use linguistic features (Xu et al., 2016; Gu et al., 2017) and graph-based models (Christopoulou et al., 2019; Sahu et al., 2019) to extract inter-sentence relations on human-annotated data. Quirk and Poon (2017) and Peng et al. (2017) attempt to extract inter-sentence relations with distantly supervised data. However, they only use entity pairs within three consecutive sentences. Different from these works, we bring in document-level DS to DocRE and conduct pre-training to denoise these DS data. 3 Methodology In this section, we present our proposed model in detail. Fig. 2 gives an illustration of our framework. We first apply the pre-denoising module to screen out some NA instances from all documents. Then we pre-train the document encoder with three pre-training tasks on the document-level distantly supervised dataset. F"
2020.emnlp-main.300,P18-1046,0,0.14999,"RE. To the best of our knowledge, we are the first to denoise document-level DS with pre-trained models. We will release our codes in the future. 2 Related Work Sentence-level RE. Conventional NRE models focus on sentence-level supervised RE (Zeng et al., 2014; Takanobu et al., 2019), which have achieved superior results on various benchmarks. Other approaches focus on using more data with distant supervision mechanism (Mintz et al., 2009; Min et al., 2013). To denoise distantly supervised corpus, they introduce attention (Lin et al., 2016; Zhou et al., 2018), generative adversarial training (Qin et al., 2018) and reinforcement learning (Feng et al., 2018) to select informative instances. It is hard to directly adopt these models to DocRE, since DocRE should extract multiple relational facts from each document. Soares et al. (2019) propose a pretrained model for sentence-level RE. Document-level RE. Document-level RE attempts Bilinear Relation Detection Mention-Entity Matching Pre-training Tasks Linear Linear Relational Fact Alignment Relational Instance Mention/Entity Document Encoder Sample data for pre-training Bilinear Mention Deep Transformer (BERT) Pre-denoise doc B Entity Pooling Document En"
2020.emnlp-main.300,P19-1423,0,0.0180819,"gnment Relational Instance Mention/Entity Document Encoder Sample data for pre-training Bilinear Mention Deep Transformer (BERT) Pre-denoise doc B Entity Pooling Document Encoder doc A Score Relation … [CLS] … [Ei] Entityi [/Ei] … [Ej] Entityj [/Ej] … [Ei] Entityi [/Ei] … [Ej] Entityj [/Ej] [SEP] Figure 2: The framework of our proposed model. to extend the scope of knowledge acquisition to the document level, which has attracted great attention recently (Yao et al., 2019). Some works use linguistic features (Xu et al., 2016; Gu et al., 2017) and graph-based models (Christopoulou et al., 2019; Sahu et al., 2019) to extract inter-sentence relations on human-annotated data. Quirk and Poon (2017) and Peng et al. (2017) attempt to extract inter-sentence relations with distantly supervised data. However, they only use entity pairs within three consecutive sentences. Different from these works, we bring in document-level DS to DocRE and conduct pre-training to denoise these DS data. 3 Methodology In this section, we present our proposed model in detail. Fig. 2 gives an illustration of our framework. We first apply the pre-denoising module to screen out some NA instances from all documents. Then we pre-trai"
2020.emnlp-main.300,P19-1279,0,0.246383,"evel supervised RE (Zeng et al., 2014; Takanobu et al., 2019), which have achieved superior results on various benchmarks. Other approaches focus on using more data with distant supervision mechanism (Mintz et al., 2009; Min et al., 2013). To denoise distantly supervised corpus, they introduce attention (Lin et al., 2016; Zhou et al., 2018), generative adversarial training (Qin et al., 2018) and reinforcement learning (Feng et al., 2018) to select informative instances. It is hard to directly adopt these models to DocRE, since DocRE should extract multiple relational facts from each document. Soares et al. (2019) propose a pretrained model for sentence-level RE. Document-level RE. Document-level RE attempts Bilinear Relation Detection Mention-Entity Matching Pre-training Tasks Linear Linear Relational Fact Alignment Relational Instance Mention/Entity Document Encoder Sample data for pre-training Bilinear Mention Deep Transformer (BERT) Pre-denoise doc B Entity Pooling Document Encoder doc A Score Relation … [CLS] … [Ei] Entityi [/Ei] … [Ej] Entityj [/Ej] … [Ei] Entityi [/Ei] … [Ej] Entityj [/Ej] [SEP] Figure 2: The framework of our proposed model. to extend the scope of knowledge acquisition to the do"
2020.emnlp-main.300,D17-1188,0,0.0179509,"e-tuning. We keep 2Nent entity pairs after pre-denoising for each document during fine-tuning, where Nent is the number of entities mentioned in the document. And we keep 20 entity pairs for each document during pre-training. We train our model with GeForce RTX 2080 Ti. All the special tokens including entity markers and the special blank symbol are implemented with unused tokens in the BERTBASE vocabulary. 4.2 4.4 Baseline We compare our model with the following baselines. (1) CNN/LSTM/BiLSTM (Yao et al., 2019): these models capture relational semantics via various encoder. (2) ContextAware (Sorokin and Gurevych, 2017): it considers the relations’ interactions with attention to jointly learn all entity 4.3 Implementation Details Main Result The main results are shown in Tab. 1. Specifically, D refers to the pre-denoising module and P indicates pre-training tasks. From the results, we can observe that: (1) Our model outperforms all baselines by a significant margin. It is due to the effectiveness of the pre-denoising mechanism 3686 F1 Model Dev IgnF1 F1 58.65 57.00 58.43 56.68 w/o MM w/o RD w/o RA 58.39 57.19 58.48 56.76 55.61 56.73 57.60 56.71 58.13 55.81 54.94 56.30 w/o Inter w/o Intra 58.68 57.78 56.96 56"
2020.emnlp-main.300,P19-1074,1,0.914989,"level RE. Document-level RE attempts Bilinear Relation Detection Mention-Entity Matching Pre-training Tasks Linear Linear Relational Fact Alignment Relational Instance Mention/Entity Document Encoder Sample data for pre-training Bilinear Mention Deep Transformer (BERT) Pre-denoise doc B Entity Pooling Document Encoder doc A Score Relation … [CLS] … [Ei] Entityi [/Ei] … [Ej] Entityj [/Ej] … [Ei] Entityi [/Ei] … [Ej] Entityj [/Ej] [SEP] Figure 2: The framework of our proposed model. to extend the scope of knowledge acquisition to the document level, which has attracted great attention recently (Yao et al., 2019). Some works use linguistic features (Xu et al., 2016; Gu et al., 2017) and graph-based models (Christopoulou et al., 2019; Sahu et al., 2019) to extract inter-sentence relations on human-annotated data. Quirk and Poon (2017) and Peng et al. (2017) attempt to extract inter-sentence relations with distantly supervised data. However, they only use entity pairs within three consecutive sentences. Different from these works, we bring in document-level DS to DocRE and conduct pre-training to denoise these DS data. 3 Methodology In this section, we present our proposed model in detail. Fig. 2 gives"
2020.emnlp-main.300,C14-1220,0,0.0453237,"sions and denoising irrelevant information from the document. In experiments, we evaluate our model on an open DocRE benchmark and achieve significant improvement over competitive baselines. We also conduct detailed analysis and ablation test, which further highlight the significance of DS data and verify the effectiveness of our pre-trained model for DocRE. To the best of our knowledge, we are the first to denoise document-level DS with pre-trained models. We will release our codes in the future. 2 Related Work Sentence-level RE. Conventional NRE models focus on sentence-level supervised RE (Zeng et al., 2014; Takanobu et al., 2019), which have achieved superior results on various benchmarks. Other approaches focus on using more data with distant supervision mechanism (Mintz et al., 2009; Min et al., 2013). To denoise distantly supervised corpus, they introduce attention (Lin et al., 2016; Zhou et al., 2018), generative adversarial training (Qin et al., 2018) and reinforcement learning (Feng et al., 2018) to select informative instances. It is hard to directly adopt these models to DocRE, since DocRE should extract multiple relational facts from each document. Soares et al. (2019) propose a pretra"
2020.emnlp-main.459,D19-1522,0,0.0916921,"cepts related to singer in Wikidata, then use the entities corresponding to these concepts to build the entity list. After that, we expand the entity list appropriately, and finally use the triples containing entities in the entity list to form the final dataset. The statistics of our five datasets are listed in Table 2. 4.2 Experiment Setup Baseline Models In our experiments, we select some KGE models and multi-hop reasoning models for comparison. For embedding-based models, we compared with TransE (Bordes et al., 2013), DistMult (Yang et al., 2015), ConvE (Dettmers et al., 2018) and TuckER (Balazevic et al., 2019). For multi-hop reasoning, we evaluate the following five models 1 , Neural Logical Programming (NeuralLP) (Yang et al., 2017), Neural Theorem Prover (NTP) (Rockt¨aschel and Riedel, 2017), MINERVA (Das et al., 2018), MultiHopKG (Lin et al., 2018) and CPL 2 (Fu et al., 2019) . Besides, our model has three variations, DacKGR (sample), DacKGR (top) and DacKGR (avg), which use sample, top-one and average strategy (introduced in Section 3.3) respectively. Evaluation Protocol For every triple (es , rq , eo ) in the test set, we convert it to a triple query (es , rq , ?), and then use embedding-based"
2020.emnlp-main.459,N18-1165,0,0.0178678,"y of their predictions. 5.2 Multi-Hop Reasoning Different from embedding-based models, multihop reasoning for KGs aims to predict the tail entity for every triple query (es , rq , ?) and meanwhile provide a reasoning path to support the prediction. Before multi-hop reasoning task is formalized, there are some models on relation path reasoning task, which aims to predict the relation between entities like (es , ?, eo ) using path information. DeepPath (Xiong et al., 2017) first adopts reinforcement learning (RL) framework for relation path reasoning, which inspires much later work (e.g., DIVA (Chen et al., 2018) and AttnPath (Wang et al., 2019)). MINERVA (Das et al., 2018) is the first model that uses REINFORCE algorithm to do the multihop reasoning task. To make the training process of RL models stable, Shen et al. propose M-Walk to solve the reward sparsity problem using off-policy learning. MultiHopKG (Lin et al., 2018) further improves MINERVA using action dropout and reward shaping. Lv et al. (2019) propose MetaKGR to address the new task that multi-hop reasoning on few-shot relations. In order to adapt RL models to a dynamically growing KG, Fu et al. (2019) propose CPL to do multi-hop reasoning"
2020.emnlp-main.459,D19-1269,0,0.0620329,"gh paths between them as reasoning evidence, which makes it difficult for the agent to carry out the reasoning process. As shown in the lower part of Figure 1, there is no evidential path between Mark Twain and English since the relation publish area is missing. From Table 1 we can learn that some sampled KG datasets are actually sparse. Besides, some domain-specific KGs (e.g., WD-singer) do not have abundant knowledge and also face the problem of sparsity. As the performance of most existing multi-hop reasoning methods drops significantly on sparse KGs, some preliminary efforts, such as CPL (Fu et al., 2019), explore to introduce additional text information to ease the sparsity of KGs. Although these explorations have achieved promising results, they are still limited to those specific KGs whose entities have additional text information. Thus, reasoning over sparse KGs is still an important but not fully resolved problem, and requires a more generalized approach to this problem. In this paper, we propose a multi-hop reasoning model named DacKGR, along with two dynamic strategies to solve the two problems mentioned above: Dynamic Anticipation makes use of the limited information in a sparse KG to"
2020.emnlp-main.459,D15-1038,0,0.0305378,"glish Figure 1: An illustration of multi-hop reasoning task over sparse KG. The missing relations (black dashed arrows) between entities can be inferred from existing triples (solid black arrows) through reasoning paths (bold arrows). However, some relations in the reasoning path are missing (red dashed arrows) in sparse KG, which makes multi-hop reasoning difficult. their further development and adaption for related downstream tasks. Knowledge graphs (KGs) represent the world knowledge in a structured way, and have been proven to be helpful for many downstream NLP tasks like query answering (Guu et al., 2015), dialogue generation (He et al., 2017) and machine reading comprehension (Yang et al., 2019). Despite their wide applications, many KGs still face serious incompleteness (Bordes et al., 2013), which limits Corresponding Author isa child Mark Twain Roughing It American child? spouse Introduction ∗ isa Olivia Langdon To alleviate this issue, some embedding-based models (Bordes et al., 2013; Dettmers et al., 2018) are proposed, most of which embed entities and relations into a vector space and make link predictions to complete KGs. These models focus on efficiently predicting knowledge but lack"
2020.emnlp-main.459,P17-1162,0,0.0279627,"-hop reasoning task over sparse KG. The missing relations (black dashed arrows) between entities can be inferred from existing triples (solid black arrows) through reasoning paths (bold arrows). However, some relations in the reasoning path are missing (red dashed arrows) in sparse KG, which makes multi-hop reasoning difficult. their further development and adaption for related downstream tasks. Knowledge graphs (KGs) represent the world knowledge in a structured way, and have been proven to be helpful for many downstream NLP tasks like query answering (Guu et al., 2015), dialogue generation (He et al., 2017) and machine reading comprehension (Yang et al., 2019). Despite their wide applications, many KGs still face serious incompleteness (Bordes et al., 2013), which limits Corresponding Author isa child Mark Twain Roughing It American child? spouse Introduction ∗ isa Olivia Langdon To alleviate this issue, some embedding-based models (Bordes et al., 2013; Dettmers et al., 2018) are proposed, most of which embed entities and relations into a vector space and make link predictions to complete KGs. These models focus on efficiently predicting knowledge but lack necessary interpretability. In order to"
2020.emnlp-main.459,D18-1362,0,0.336179,"al., 2019). Despite their wide applications, many KGs still face serious incompleteness (Bordes et al., 2013), which limits Corresponding Author isa child Mark Twain Roughing It American child? spouse Introduction ∗ isa Olivia Langdon To alleviate this issue, some embedding-based models (Bordes et al., 2013; Dettmers et al., 2018) are proposed, most of which embed entities and relations into a vector space and make link predictions to complete KGs. These models focus on efficiently predicting knowledge but lack necessary interpretability. In order to solve this problem, Das et al. (2018) and Lin et al. (2018) propose multihop reasoning models, which use the REINFORCE algorithm (Williams, 1992) to train an agent to search over KGs. These models can not only give the predicted result but also an interpretable path to indicate the reasoning process. As shown in the upper part of Figure 1, for a triple query (Olivia Langdon, child, ?), multi-hop reasoning models can predict the tail entity Susy Clemens through a reasoning path (bold arrows). Although existing multi-hop reasoning models have achieved good results, they still suffer two problems on sparse KGs: (1) Insufficient information. Compared with"
2020.emnlp-main.459,D19-1334,1,0.818399,"like (es , ?, eo ) using path information. DeepPath (Xiong et al., 2017) first adopts reinforcement learning (RL) framework for relation path reasoning, which inspires much later work (e.g., DIVA (Chen et al., 2018) and AttnPath (Wang et al., 2019)). MINERVA (Das et al., 2018) is the first model that uses REINFORCE algorithm to do the multihop reasoning task. To make the training process of RL models stable, Shen et al. propose M-Walk to solve the reward sparsity problem using off-policy learning. MultiHopKG (Lin et al., 2018) further improves MINERVA using action dropout and reward shaping. Lv et al. (2019) propose MetaKGR to address the new task that multi-hop reasoning on few-shot relations. In order to adapt RL models to a dynamically growing KG, Fu et al. (2019) propose CPL to do multi-hop reasoning and fact extraction jointly. In addition to the above RL-based reasoning models, there are some other neural symbolic models for multi-hop reasoning. NTP (Rockt¨aschel and Riedel, 2017) and NeuralLP (Yang et al., 2017) are two end-to-end reasoning models that can learn logic rules from KGs automatically. Compared with KGE models, multi-hop reasoning models sacrifice some accuracy for interpretabi"
2020.emnlp-main.459,N18-2053,0,0.0223203,"ories (Wang et al., 2017): (1) Translation-based models (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Sun et al., 2018) formalize the relation as a translation from a head entity to a tail entity, and often use distance-based score functions derived from these translation operations. (2) Tensor-factorization based models (Nickel et al., 2011; Yang et al., 2015; Balazevic et al., 2019) formulate KGE as a three-way tensor decomposition task and define the score function according to the decomposition operations. (3) Neural network models (Socher et al., 2013; Dettmers et al., 2018; Nguyen et al., 2018; Shang et al., 2019) usually design neural network modules to enhance the expressive abilities. Generally, given a triple query (es , rq , ?), KGE models select the entity eo , whose score function f (es , rq , eo ) has the highest score as the final prediction. Although KGE models are efficient, they lack interpretability of their predictions. 5.2 Multi-Hop Reasoning Different from embedding-based models, multihop reasoning for KGs aims to predict the tail entity for every triple query (es , rq , ?) and meanwhile provide a reasoning path to support the prediction. Before multi-hop reasoning"
2020.emnlp-main.459,D15-1174,0,0.0908108,"Missing"
2020.emnlp-main.459,D19-1264,0,0.0203564,"-Hop Reasoning Different from embedding-based models, multihop reasoning for KGs aims to predict the tail entity for every triple query (es , rq , ?) and meanwhile provide a reasoning path to support the prediction. Before multi-hop reasoning task is formalized, there are some models on relation path reasoning task, which aims to predict the relation between entities like (es , ?, eo ) using path information. DeepPath (Xiong et al., 2017) first adopts reinforcement learning (RL) framework for relation path reasoning, which inspires much later work (e.g., DIVA (Chen et al., 2018) and AttnPath (Wang et al., 2019)). MINERVA (Das et al., 2018) is the first model that uses REINFORCE algorithm to do the multihop reasoning task. To make the training process of RL models stable, Shen et al. propose M-Walk to solve the reward sparsity problem using off-policy learning. MultiHopKG (Lin et al., 2018) further improves MINERVA using action dropout and reward shaping. Lv et al. (2019) propose MetaKGR to address the new task that multi-hop reasoning on few-shot relations. In order to adapt RL models to a dynamically growing KG, Fu et al. (2019) propose CPL to do multi-hop reasoning and fact extraction jointly. In"
2020.emnlp-main.459,D17-1060,0,0.0187415,"ose score function f (es , rq , eo ) has the highest score as the final prediction. Although KGE models are efficient, they lack interpretability of their predictions. 5.2 Multi-Hop Reasoning Different from embedding-based models, multihop reasoning for KGs aims to predict the tail entity for every triple query (es , rq , ?) and meanwhile provide a reasoning path to support the prediction. Before multi-hop reasoning task is formalized, there are some models on relation path reasoning task, which aims to predict the relation between entities like (es , ?, eo ) using path information. DeepPath (Xiong et al., 2017) first adopts reinforcement learning (RL) framework for relation path reasoning, which inspires much later work (e.g., DIVA (Chen et al., 2018) and AttnPath (Wang et al., 2019)). MINERVA (Das et al., 2018) is the first model that uses REINFORCE algorithm to do the multihop reasoning task. To make the training process of RL models stable, Shen et al. propose M-Walk to solve the reward sparsity problem using off-policy learning. MultiHopKG (Lin et al., 2018) further improves MINERVA using action dropout and reward shaping. Lv et al. (2019) propose MetaKGR to address the new task that multi-hop r"
2020.emnlp-main.459,P19-1226,0,0.0265119,"tions (black dashed arrows) between entities can be inferred from existing triples (solid black arrows) through reasoning paths (bold arrows). However, some relations in the reasoning path are missing (red dashed arrows) in sparse KG, which makes multi-hop reasoning difficult. their further development and adaption for related downstream tasks. Knowledge graphs (KGs) represent the world knowledge in a structured way, and have been proven to be helpful for many downstream NLP tasks like query answering (Guu et al., 2015), dialogue generation (He et al., 2017) and machine reading comprehension (Yang et al., 2019). Despite their wide applications, many KGs still face serious incompleteness (Bordes et al., 2013), which limits Corresponding Author isa child Mark Twain Roughing It American child? spouse Introduction ∗ isa Olivia Langdon To alleviate this issue, some embedding-based models (Bordes et al., 2013; Dettmers et al., 2018) are proposed, most of which embed entities and relations into a vector space and make link predictions to complete KGs. These models focus on efficiently predicting knowledge but lack necessary interpretability. In order to solve this problem, Das et al. (2018) and Lin et al."
2020.emnlp-main.515,D19-1609,0,0.0126103,"ment Graph Alignment unifies the two KGs’ representations of each channel into a unified vector space by reducing the distance between the seed equivalent entities. We separately train the four channels and ensemble their outputs afterward for final evaluation (see Section 3.5). Following Li et al. (2019), we generate negative samples of (e, e0 ) ∈ ψ s by searching the nearest entities of e (or e0 ) in the entity embedding space. We denote the final output k k hL e of the channel GC as the entity embedding e . k For each channel GC , we optimize the following objective function: 2 As shown by Andor et al. (2019), BERT embedding can be used for simple numerical computation. 6358 Lk = X ( X k k [d(ek , e0 ) − d(ek− , e0 ) + γ]+ (e,e0 )∈ψ s e− ∈NS(e) + X k k [d(ek , e0 ) − d(e, e0 − ) + γ]+ ) e0− ∈NS(e0 ) (3) where ψ s is the seed set of equivalent entities, NS(e) denotes the negative samples of e; [·]+ = max{·, 0}, d(·, ·) = 1 − cos(·, ·) is the cosine distance, and γ is a margin hyperparameter. 3.5 Channel Ensemble We use the entity embedding of each channel 0 to infer the similarity matrices Sk ∈ R|E|×|E | k (k ∈ {1, 2, 3, 4}), where Ske,e0 = cos(ek , e0 ) is the cosine similarity score between e ∈ E"
2020.emnlp-main.515,C18-1057,1,0.846295,"demonstrate the effectiveness of our method. Source code and data can be found at https://github.com/ thunlp/explore-and-evaluate. 1 Introduction The prosperity of data mining has spawned Knowledge Graphs (KGs) in many domains that are often complementary to each other. Entity Alignment (EA) provides an effective way to integrate the complementary knowledge in these KGs into a unified KG by linking equivalent entities, thus benefiting knowledge-driven applications such as Question Answering (Yang et al., 2017, 2018), Recommendation (Cao et al., 2019b) and Information Extraction (Kumar, 2017; Cao et al., 2018). However, EA is a non-trivial task that it could be formulated as * Corresponding author. a quadratic assignment problem (Yan et al., 2016), which is NP-complete (Garey and Johnson, 1990). A KG comprises a set of triples, with each triple consisting of a subject, predicate, and object. There are two types of triples: (1) relation triples, in which both the subject and object are entities, and the predicate is often called relation (see Figure 1(a)); and (2) attribute triples, in which the subject is an entity and the object is a value, which is either a number or literal string (see Figure 1("
2020.emnlp-main.515,P17-1149,1,0.908382,"Missing"
2020.emnlp-main.515,P19-1140,1,0.800608,"ent subgraphs and a case study about attribute types further demonstrate the effectiveness of our method. Source code and data can be found at https://github.com/ thunlp/explore-and-evaluate. 1 Introduction The prosperity of data mining has spawned Knowledge Graphs (KGs) in many domains that are often complementary to each other. Entity Alignment (EA) provides an effective way to integrate the complementary knowledge in these KGs into a unified KG by linking equivalent entities, thus benefiting knowledge-driven applications such as Question Answering (Yang et al., 2017, 2018), Recommendation (Cao et al., 2019b) and Information Extraction (Kumar, 2017; Cao et al., 2018). However, EA is a non-trivial task that it could be formulated as * Corresponding author. a quadratic assignment problem (Yan et al., 2016), which is NP-complete (Garey and Johnson, 1990). A KG comprises a set of triples, with each triple consisting of a subject, predicate, and object. There are two types of triples: (1) relation triples, in which both the subject and object are entities, and the predicate is often called relation (see Figure 1(a)); and (2) attribute triples, in which the subject is an entity and the object is a val"
2020.emnlp-main.515,N19-1423,0,0.00910856,"two GNN layers. Next, we describe attributed value encoder and mean aggregator in details. 3.3.1 Attributed Value Encoder Attributed value encoder can selectively gather discriminative information from the initial feature of attributes and values to the central entity. As an example, we show how to obtain e’s first layer hidden state h1e . The same method applies to all the entities. We obtain the sequence of attribute features {a1 , · · · , an } and value features {v1 , · · · , vn } given the attribute triples {(e, a1 , v1 ), · · · , (e, an , vn )} of e as inputs. Specifically, we use BERT (Devlin et al., 2019) to obtain the features of both literal and digital values2 . BERT is a language model that is pre-trained on a more than 3000M words corpora. It is popularly used as a feature extractor in NLP tasks. By adding (1) oj = LeakyReLU(uT [h0e ; aj ]), where j ∈ {1, · · · , n}, W1 ∈ RDh1 ×(Da +Dv ) and u ∈ R(De +Da )×1 are learnable matrices, σ is the ELU(·) function, and h0e is the initial entity feature. 3.3.2 Mean Aggregator Mean aggregator layer utilizes the features of the target entity and its neighbors to generate the entity embedding. The neighbor entities of e are defined by relation triple"
2020.emnlp-main.515,2020.acl-main.89,0,0.0330725,"Missing"
2020.emnlp-main.515,D19-1274,1,0.887955,"2019) and MultiKE (Zhang et al., 2019) encode values as extra entity embeddings. However, the diversity of attributes and uninformative values limit the performance of the above methods. 2.2 GNN-based Methods Following Graph Convolutional Networks (Kipf and Welling, 2017), many GNN-based models are proposed because of GNN’s strong ability to model graph structure. These methods present promising results on EA because GNN can propagate the alignment signal to the entity’s distant neighbors. Previous GNN-based methods focus on extending GNN’s ability to model relation types (Wu et al., 2019a,b; Li et al., 2019), aligning entities via matching subgraphs (Xu et al., 2019; Wu et al., 2020), and reducing the heterogeneity between KGs (Cao et al., 2019a). With the exception of Wang et al. (2018) that have incorporated attributes as the initial feature of entities, most of the current GNN-based methods fail to incorporate the attributes and values to further improve the performance of EA. In this paper, we add values as nodes into graph and use an attributed value encoder to conduct attribute-aware value aggregation. 3 Methodology The key idea of AttrGNN is to use graph partition and attributed value enco"
2020.emnlp-main.515,D18-1259,0,0.067005,"Missing"
2020.emnlp-main.515,D18-1032,0,0.0846666,"ic assignment problem (Yan et al., 2016), which is NP-complete (Garey and Johnson, 1990). A KG comprises a set of triples, with each triple consisting of a subject, predicate, and object. There are two types of triples: (1) relation triples, in which both the subject and object are entities, and the predicate is often called relation (see Figure 1(a)); and (2) attribute triples, in which the subject is an entity and the object is a value, which is either a number or literal string (see Figure 1(c)), and the predicate is often called attribute. Most of the previous EA models (Sun et al., 2017; Wang et al., 2018; Wu et al., 2019a) rely on the structure assumption that, the adjacencies of two equivalent entities in KGs usually contain equivalent entities (Wang et al., 2018) (see Figure 1(a)). These models mainly focus on modeling KG structure defined by the relation triples. However, we argue that attribute triples can also provide important clues for judging whether two entities are the same, based on the attribute assumption that: equivalent entities often share similar attributes and values in KGs. For example, in Figure 1(b), the equivalent entities e and e0 share the attribute Area with similar v"
2020.emnlp-main.515,D19-1023,0,0.144565,"em (Yan et al., 2016), which is NP-complete (Garey and Johnson, 1990). A KG comprises a set of triples, with each triple consisting of a subject, predicate, and object. There are two types of triples: (1) relation triples, in which both the subject and object are entities, and the predicate is often called relation (see Figure 1(a)); and (2) attribute triples, in which the subject is an entity and the object is a value, which is either a number or literal string (see Figure 1(c)), and the predicate is often called attribute. Most of the previous EA models (Sun et al., 2017; Wang et al., 2018; Wu et al., 2019a) rely on the structure assumption that, the adjacencies of two equivalent entities in KGs usually contain equivalent entities (Wang et al., 2018) (see Figure 1(a)). These models mainly focus on modeling KG structure defined by the relation triples. However, we argue that attribute triples can also provide important clues for judging whether two entities are the same, based on the attribute assumption that: equivalent entities often share similar attributes and values in KGs. For example, in Figure 1(b), the equivalent entities e and e0 share the attribute Area with similar values of 153, 909"
2020.emnlp-main.515,2020.acl-main.578,0,0.297622,"gs. However, the diversity of attributes and uninformative values limit the performance of the above methods. 2.2 GNN-based Methods Following Graph Convolutional Networks (Kipf and Welling, 2017), many GNN-based models are proposed because of GNN’s strong ability to model graph structure. These methods present promising results on EA because GNN can propagate the alignment signal to the entity’s distant neighbors. Previous GNN-based methods focus on extending GNN’s ability to model relation types (Wu et al., 2019a,b; Li et al., 2019), aligning entities via matching subgraphs (Xu et al., 2019; Wu et al., 2020), and reducing the heterogeneity between KGs (Cao et al., 2019a). With the exception of Wang et al. (2018) that have incorporated attributes as the initial feature of entities, most of the current GNN-based methods fail to incorporate the attributes and values to further improve the performance of EA. In this paper, we add values as nodes into graph and use an attributed value encoder to conduct attribute-aware value aggregation. 3 Methodology The key idea of AttrGNN is to use graph partition and attributed value encoder to deal with various types of attribute triples. In this section, we firs"
2020.emnlp-main.515,P19-1304,0,0.133774,"a entity embeddings. However, the diversity of attributes and uninformative values limit the performance of the above methods. 2.2 GNN-based Methods Following Graph Convolutional Networks (Kipf and Welling, 2017), many GNN-based models are proposed because of GNN’s strong ability to model graph structure. These methods present promising results on EA because GNN can propagate the alignment signal to the entity’s distant neighbors. Previous GNN-based methods focus on extending GNN’s ability to model relation types (Wu et al., 2019a,b; Li et al., 2019), aligning entities via matching subgraphs (Xu et al., 2019; Wu et al., 2020), and reducing the heterogeneity between KGs (Cao et al., 2019a). With the exception of Wang et al. (2018) that have incorporated attributes as the initial feature of entities, most of the current GNN-based methods fail to incorporate the attributes and values to further improve the performance of EA. In this paper, we add values as nodes into graph and use an attributed value encoder to conduct attribute-aware value aggregation. 3 Methodology The key idea of AttrGNN is to use graph partition and attributed value encoder to deal with various types of attribute triples. In thi"
2020.emnlp-main.566,W19-1909,0,0.110918,"Missing"
2020.emnlp-main.566,D19-1539,0,0.021328,"r method is both effective and efficient. The source code of this paper can be obtained from https://github. com/thunlp/SelectiveMasking. 1 In-domain Unsupervised Data Random Masking Selective Masking General Pre-train Task-guided Pre-train Downstream Supervised Data Fine-tune Figure 1: The overall three-stage framework. We add task-guided pre-training between general pre-training and fine-tuning to efficiently and effectively learn the domain-specific and task-specific language patterns. Introduction Pre-trained Language Models (PLMs) have achieved superior performances on various NLP tasks (Baevski et al., 2019; Joshi et al., 2020; Liu et al., 2019; Yang et al., 2019; Clark et al., 2020) and have attracted wide research interests. Inspired by the success of GPT (Radford et al., 2018) and BERT (Devlin et al., 2019), most PLMs follow the pre-train-then-fine-tuning paradigm, which adopts unsupervised pre-training on large general-domain † General Unsupervised Data Corresponding author: Z.Liu (liuzy@tsinghua.edu.cn) corpora to learn general language patterns and supervised fine-tuning to adapt to downstream tasks. Recently, Gururangan et al. (2020) shows that learning domain-specific and task-specific p"
2020.emnlp-main.566,D19-1371,0,0.035946,"ing author: Z.Liu (liuzy@tsinghua.edu.cn) corpora to learn general language patterns and supervised fine-tuning to adapt to downstream tasks. Recently, Gururangan et al. (2020) shows that learning domain-specific and task-specific patterns during pre-training can be helpful to the models for certain domains and tasks. However, conventional pre-training is aimless with respect to specific downstream tasks, and fine-tuning usually suffers from insufficient supervised data, preventing PLMs from effectively capturing these patterns. To learn domain-specific language patterns, some previous works (Beltagy et al., 2019; Huang et al., 2020) pre-train a BERT-like model from scratch using large-scale in-domain data. However, they are computation-intensive and require largescale in-domain data, which is hard to obtain in many domains. To learn task-specific language patterns, some previous works (Phang et al., 2018) add intermediate supervised pre-training after general pre-training, whose pre-training task is similar to the downstream task but has a larger dataset. However, Wang et al. (2019) shows that this kind of intermediate pre-training often negatively impacts 6966 Proceedings of the 2020 Conference on E"
2020.emnlp-main.566,D18-1407,0,0.0417078,"Missing"
2020.emnlp-main.566,2020.acl-main.740,0,0.0383792,"Missing"
2020.emnlp-main.566,2020.tacl-1.5,0,0.0416949,"tive and efficient. The source code of this paper can be obtained from https://github. com/thunlp/SelectiveMasking. 1 In-domain Unsupervised Data Random Masking Selective Masking General Pre-train Task-guided Pre-train Downstream Supervised Data Fine-tune Figure 1: The overall three-stage framework. We add task-guided pre-training between general pre-training and fine-tuning to efficiently and effectively learn the domain-specific and task-specific language patterns. Introduction Pre-trained Language Models (PLMs) have achieved superior performances on various NLP tasks (Baevski et al., 2019; Joshi et al., 2020; Liu et al., 2019; Yang et al., 2019; Clark et al., 2020) and have attracted wide research interests. Inspired by the success of GPT (Radford et al., 2018) and BERT (Devlin et al., 2019), most PLMs follow the pre-train-then-fine-tuning paradigm, which adopts unsupervised pre-training on large general-domain † General Unsupervised Data Corresponding author: Z.Liu (liuzy@tsinghua.edu.cn) corpora to learn general language patterns and supervised fine-tuning to adapt to downstream tasks. Recently, Gururangan et al. (2020) shows that learning domain-specific and task-specific patterns during pre-t"
2020.emnlp-main.566,2020.emnlp-main.567,0,0.0718477,"Missing"
2020.emnlp-main.566,P05-1015,0,\N,Missing
2020.emnlp-main.566,S14-2004,0,\N,Missing
2020.emnlp-main.566,N18-1112,0,\N,Missing
2020.emnlp-main.566,N19-1423,0,\N,Missing
2020.emnlp-main.566,D19-1628,0,\N,Missing
2020.emnlp-main.582,S17-2001,0,0.0370299,"Missing"
2020.emnlp-main.582,P18-1078,0,0.0246983,"o tokens could typically cover the major information of the whole word (Lee et al., 2017; He et al., 2018). For a masked noun wi consisting of a (i) (i) sequence of tokens (xs , . . . , xt ), we recover wi by copying its referring context word, and define the probability of choosing word wj as: (j) (i) (i) Pr(wj |wi ) = Pr(x(j) s |xs ) × Pr(xt |xt ). (3) A masked noun possibly has multiple referring words in the sequence, for which we collectively maximize the similarity of all referring words. It is an approach widely used in question answering (Kadlec et al., 2016; Swayamdipta et al., 2018; Clark and Gardner, 2018) designed to handle multiple answers. Finally, we define the loss of Mention Reference Prediction (MRP) as: X X LMRP = − log Pr(wj |wi ), (4) wi ∈M wj ∈Cwi where M is the set of all masked mentions for mention reference masking, and Cwi is the set of all corresponding words of word wi . 4 Experiment In this section, we first introduce the training details of CorefBERT. After that, we present the finetuning results on a comprehensive suite of tasks, including extractive question answering, documentlevel relation extraction, fact extraction and verification, coreference resolution, and eight tas"
2020.emnlp-main.582,D19-1606,0,0.323037,"9), fact extraction and verification (Zhou et al., 2019), and coreference resolution (Joshi et al., 2019). However, existing pre-training tasks, such as masked language modeling, usually only require models to collect local semantic and syntactic information to recover the masked tokens. Hence, language representation models may not well model the long-distance connections beyond sentence boundary in a text, such as coreference. Previous work has shown that the performance of these models is not as good as human performance on the tasks requiring coreferential reasoning (Paperno et al., 2016; Dasigi et al., 2019), and they can be further improved on long-text tasks with external coreference information (Cheng and Erk, 2020; Xu et al., 2020; Zhao et al., 2020). Coreference occurs when two or more expressions in a text refer to the same entity, which is an important element for a coherent understanding of the whole discourse. For example, for comprehending the whole context of “Antoine published The Little Prince in 1943. The book follows a young prince who visits various planets in space.”, we must realize that The book refers to The Little Prince. Therefore, resolving coreference is an essential step"
2020.emnlp-main.582,N19-1423,0,0.584796,"issue, we present CorefBERT, a novel language representation model that can capture the coreferential relations in context. The experimental results show that, compared with existing baseline models, CorefBERT can achieve significant improvements consistently on various downstream NLP tasks that require coreferential reasoning, while maintaining comparable performance to previous models on other common NLP tasks. The source code and experiment details of this paper can be obtained from https://github. com/thunlp/CorefBERT. 1 Introduction Recently, language representation models such as BERT (Devlin et al., 2019) have attracted considerable attention. These models usually conduct self-supervised pre-training tasks over large-scale corpus to obtain informative language representation, which could capture the contextual semantic of the input text. Benefiting from this, language representation models have made significant strides in many natural language understanding tasks including natural language inference (Zhang et al., 2020), sentiment classification (Sun et al., 2019b), question answering (Talmor and Berant, 2019), relation extraction (Peters et al., 2019), fact extraction and verification (Zhou e"
2020.emnlp-main.582,D19-5801,0,0.0239374,"Missing"
2020.emnlp-main.582,W07-1401,0,0.0718487,"Missing"
2020.emnlp-main.582,P16-1154,0,0.0608072,"Missing"
2020.emnlp-main.582,P18-2058,0,0.0204785,"p((V hk ) hi ) Pr(xj |xi ) = P where denotes element-wise product function and V is a trainable parameter to measure the importance of each dimension for token’s similarity. Moreover, since we split a word into several word pieces as BERT does and we adopt whole word masking strategy for MRP, we need to extend our copy-based objective into word-level. To this end, we apply the token-level copy-based training objective on both start and end tokens of the masked word, because the representations of these two tokens could typically cover the major information of the whole word (Lee et al., 2017; He et al., 2018). For a masked noun wi consisting of a (i) (i) sequence of tokens (xs , . . . , xt ), we recover wi by copying its referring context word, and define the probability of choosing word wj as: (j) (i) (i) Pr(wj |wi ) = Pr(x(j) s |xs ) × Pr(xt |xt ). (3) A masked noun possibly has multiple referring words in the sequence, for which we collectively maximize the similarity of all referring words. It is an approach widely used in question answering (Kadlec et al., 2016; Swayamdipta et al., 2018; Clark and Gardner, 2018) designed to handle multiple answers. Finally, we define the loss of Mention Refer"
2020.emnlp-main.582,P82-1020,0,0.813763,"Missing"
2020.emnlp-main.582,P18-1031,0,0.0280754,"language representation models aim to capture language information from the text, which facilitate various downstream NLP applications (Kim, 2014; Lin et al., 2016; Seo et al., 2017). Early works (Mikolov et al., 2013; Pennington et al., 2014) focus on learning static word embeddings from the unlabeled corpus, which have the limitation that they cannot handle the polysemy well. Recent years, contextual language representation models pre-trained on large-scale unlabeled corpora have attracted intensive attention and efforts from both academia and industry. SALSTM (Dai and Le, 2015) and ULMFiT (Howard and Ruder, 2018) pre-trains language models on unlabeled text and perform task-specific fine-tuning. ELMo (Peters et al., 2018) further employs a bidirectional LSTM-based language model to extract context-aware word embeddings. Moreover, OpenAI GPT (Radford et al., 2018) and BERT (Devlin et al., 2019) learn pre-trained language representation with Transformer architecture (Vaswani et al., 2017), achieving state-of-the-art results on various NLP tasks. Beyond them, various improvements on pre-training language representation have been proposed more recently, including (1) designing new pre-trainning tasks or o"
2020.emnlp-main.582,D19-1170,0,0.0311913,"ntation Details Following BERT’s setting (Devlin et al., 2019), given the question Q = (q1 , q2 , . . . , qm ) and the passage P = (p1 , p2 , . . . , pn ), we represent them as a sequence X = ([CLS], q1 , q2 , . . . , qm , [SEP], p1 , p2 , . . . , pn , [SEP]), feed the sequence X into the pre-trained encoder and train two classifiers on the top of it to seek answer’s start and end positions simultaneously. For MRQA, CorefBERT maintains the same framework as BERT. For QUOREF, we further employ two extra components to process multiple mentions of the answers: (1) Spurred by the idea from MTMSN (Hu et al., 2019) in handling the problem of multiple answer spans, we utilize the representation of [CLS] to predict the number of answers. After that, we first selects the answer span of the current highest scores, then continues to choose that of the second-highest score with no overlap to previous spans, until reaching the predicted answer number. (2) When answering a question from QUOREF, the relevant mention could possibly be a pronoun, so we attach a reasoning Transformer layer for pronoun resolution before the span boundary classifier. 7174 Model SQuAD NewsQA TriviaQA SearchQA HotpotQA NaturalQA Averag"
2020.emnlp-main.582,2020.tacl-1.5,0,0.0654805,"on unlabeled text and perform task-specific fine-tuning. ELMo (Peters et al., 2018) further employs a bidirectional LSTM-based language model to extract context-aware word embeddings. Moreover, OpenAI GPT (Radford et al., 2018) and BERT (Devlin et al., 2019) learn pre-trained language representation with Transformer architecture (Vaswani et al., 2017), achieving state-of-the-art results on various NLP tasks. Beyond them, various improvements on pre-training language representation have been proposed more recently, including (1) designing new pre-trainning tasks or objectives such as SpanBERT (Joshi et al., 2020) with span-based learning, XLNet (Yang et al., 2019) considering masked positions dependency with auto-regressive loss, 7171 MASS (Song et al., 2019) and BART (Wang et al., 2019b) with sequence-to-sequence pre-training, ELECTRA (Clark et al., 2020) learning from replaced token detection with generative adversarial networks and InfoWord (Kong et al., 2020) with contrastive learning; (2) integrating external knowledge such as factual knowledge in knowledge graphs (Zhang et al., 2019; Peters et al., 2019; Liu et al., 2020a); and (3) exploring multilingual learning (Conneau and Lample, 2019; Tan a"
2020.emnlp-main.582,P17-1147,0,0.0453347,"Missing"
2020.emnlp-main.582,D19-1588,0,0.0315944,"ese models usually conduct self-supervised pre-training tasks over large-scale corpus to obtain informative language representation, which could capture the contextual semantic of the input text. Benefiting from this, language representation models have made significant strides in many natural language understanding tasks including natural language inference (Zhang et al., 2020), sentiment classification (Sun et al., 2019b), question answering (Talmor and Berant, 2019), relation extraction (Peters et al., 2019), fact extraction and verification (Zhou et al., 2019), and coreference resolution (Joshi et al., 2019). However, existing pre-training tasks, such as masked language modeling, usually only require models to collect local semantic and syntactic information to recover the masked tokens. Hence, language representation models may not well model the long-distance connections beyond sentence boundary in a text, such as coreference. Previous work has shown that the performance of these models is not as good as human performance on the tasks requiring coreferential reasoning (Paperno et al., 2016; Dasigi et al., 2019), and they can be further improved on long-text tasks with external coreference infor"
2020.emnlp-main.582,P16-1086,0,0.0310095,"d word, because the representations of these two tokens could typically cover the major information of the whole word (Lee et al., 2017; He et al., 2018). For a masked noun wi consisting of a (i) (i) sequence of tokens (xs , . . . , xt ), we recover wi by copying its referring context word, and define the probability of choosing word wj as: (j) (i) (i) Pr(wj |wi ) = Pr(x(j) s |xs ) × Pr(xt |xt ). (3) A masked noun possibly has multiple referring words in the sequence, for which we collectively maximize the similarity of all referring words. It is an approach widely used in question answering (Kadlec et al., 2016; Swayamdipta et al., 2018; Clark and Gardner, 2018) designed to handle multiple answers. Finally, we define the loss of Mention Reference Prediction (MRP) as: X X LMRP = − log Pr(wj |wi ), (4) wi ∈M wj ∈Cwi where M is the set of all masked mentions for mention reference masking, and Cwi is the set of all corresponding words of word wi . 4 Experiment In this section, we first introduce the training details of CorefBERT. After that, we present the finetuning results on a comprehensive suite of tasks, including extractive question answering, documentlevel relation extraction, fact extraction and"
2020.emnlp-main.582,D14-1181,0,0.00377708,"efBERT outperforms the vanilla BERT on almost all benchmarks and even strengthens the performance of the strong RoBERTa model. To verify the model’s robustness, we also evaluate CorefBERT on other common NLP tasks where CorefBERT still achieves comparable results to BERT. It demonstrates that the introduction of the new pre-training task about coreferential reasoning would not impair BERT’s ability in common language understanding. 2 Related Work Pre-training language representation models aim to capture language information from the text, which facilitate various downstream NLP applications (Kim, 2014; Lin et al., 2016; Seo et al., 2017). Early works (Mikolov et al., 2013; Pennington et al., 2014) focus on learning static word embeddings from the unlabeled corpus, which have the limitation that they cannot handle the polysemy well. Recent years, contextual language representation models pre-trained on large-scale unlabeled corpora have attracted intensive attention and efforts from both academia and industry. SALSTM (Dai and Le, 2015) and ULMFiT (Howard and Ruder, 2018) pre-trains language models on unlabeled text and perform task-specific fine-tuning. ELMo (Peters et al., 2018) further em"
2020.emnlp-main.582,D19-1439,0,0.273092,"ign a mention reference prediction task to enhance language representation models in terms of coreferential reasoning. Our work, which acquires coreference resolution ability from an unlabeled corpus, can also be viewed as a special form of unsupervised coreference resolution. Formerly, researchers have made efforts to explore feature-based unsupervised coreference resolution methods (Bejan et al., 2009; Ma et al., 2016). After that, Word-LM (Trinh and Le, 2018) uncovers that it is natural to resolve pronouns in the sentence according to the probability of language models. Moreover, WikiCREM (Kocijan et al., 2019) builds sentence-level unsupervised coreference resolution dataset for learning coreference discriminator. However, these methods cannot be directly transferred to language representation models since their task-specific design could weaken the model’s performance on other NLP tasks. To address this issue, we introduce a mention reference prediction objective, complementary to masked language modeling, which could make the obtained coreferential reasoning ability compatible with more downstream tasks. 3 Methodology In this section, we present CorefBERT, a language representation model, which a"
2020.emnlp-main.582,D19-1279,0,0.0262174,"learning, XLNet (Yang et al., 2019) considering masked positions dependency with auto-regressive loss, 7171 MASS (Song et al., 2019) and BART (Wang et al., 2019b) with sequence-to-sequence pre-training, ELECTRA (Clark et al., 2020) learning from replaced token detection with generative adversarial networks and InfoWord (Kong et al., 2020) with contrastive learning; (2) integrating external knowledge such as factual knowledge in knowledge graphs (Zhang et al., 2019; Peters et al., 2019; Liu et al., 2020a); and (3) exploring multilingual learning (Conneau and Lample, 2019; Tan and Bansal, 2019; Kondratyuk and Straka, 2019) or multimodal learning (Lu et al., 2019; Sun et al., 2019a; Su et al., 2020). Though existing language representation models have achieved a great success, their coreferential reasoning capability are still far less than that of human beings (Paperno et al., 2016; Dasigi et al., 2019). In this paper, we design a mention reference prediction task to enhance language representation models in terms of coreferential reasoning. Our work, which acquires coreference resolution ability from an unlabeled corpus, can also be viewed as a special form of unsupervised coreference resolution. Formerly, res"
2020.emnlp-main.582,P16-1200,1,0.781375,"erforms the vanilla BERT on almost all benchmarks and even strengthens the performance of the strong RoBERTa model. To verify the model’s robustness, we also evaluate CorefBERT on other common NLP tasks where CorefBERT still achieves comparable results to BERT. It demonstrates that the introduction of the new pre-training task about coreferential reasoning would not impair BERT’s ability in common language understanding. 2 Related Work Pre-training language representation models aim to capture language information from the text, which facilitate various downstream NLP applications (Kim, 2014; Lin et al., 2016; Seo et al., 2017). Early works (Mikolov et al., 2013; Pennington et al., 2014) focus on learning static word embeddings from the unlabeled corpus, which have the limitation that they cannot handle the polysemy well. Recent years, contextual language representation models pre-trained on large-scale unlabeled corpora have attracted intensive attention and efforts from both academia and industry. SALSTM (Dai and Le, 2015) and ULMFiT (Howard and Ruder, 2018) pre-trains language models on unlabeled text and perform task-specific fine-tuning. ELMo (Peters et al., 2018) further employs a bidirectio"
2020.emnlp-main.582,2021.ccl-1.108,0,0.305008,"Missing"
2020.emnlp-main.582,P16-1144,0,0.060922,"Missing"
2020.emnlp-main.582,D14-1162,0,0.101657,"Missing"
2020.emnlp-main.582,D19-1005,0,0.114922,"guage representation models such as BERT (Devlin et al., 2019) have attracted considerable attention. These models usually conduct self-supervised pre-training tasks over large-scale corpus to obtain informative language representation, which could capture the contextual semantic of the input text. Benefiting from this, language representation models have made significant strides in many natural language understanding tasks including natural language inference (Zhang et al., 2020), sentiment classification (Sun et al., 2019b), question answering (Talmor and Berant, 2019), relation extraction (Peters et al., 2019), fact extraction and verification (Zhou et al., 2019), and coreference resolution (Joshi et al., 2019). However, existing pre-training tasks, such as masked language modeling, usually only require models to collect local semantic and syntactic information to recover the masked tokens. Hence, language representation models may not well model the long-distance connections beyond sentence boundary in a text, such as coreference. Previous work has shown that the performance of these models is not as good as human performance on the tasks requiring coreferential reasoning (Paperno et al., 2016; Da"
2020.emnlp-main.582,N18-1202,0,0.0561499,"eam NLP applications (Kim, 2014; Lin et al., 2016; Seo et al., 2017). Early works (Mikolov et al., 2013; Pennington et al., 2014) focus on learning static word embeddings from the unlabeled corpus, which have the limitation that they cannot handle the polysemy well. Recent years, contextual language representation models pre-trained on large-scale unlabeled corpora have attracted intensive attention and efforts from both academia and industry. SALSTM (Dai and Le, 2015) and ULMFiT (Howard and Ruder, 2018) pre-trains language models on unlabeled text and perform task-specific fine-tuning. ELMo (Peters et al., 2018) further employs a bidirectional LSTM-based language model to extract context-aware word embeddings. Moreover, OpenAI GPT (Radford et al., 2018) and BERT (Devlin et al., 2019) learn pre-trained language representation with Transformer architecture (Vaswani et al., 2017), achieving state-of-the-art results on various NLP tasks. Beyond them, various improvements on pre-training language representation have been proposed more recently, including (1) designing new pre-trainning tasks or objectives such as SpanBERT (Joshi et al., 2020) with span-based learning, XLNet (Yang et al., 2019) considering"
2020.emnlp-main.582,D12-1071,0,0.0699835,"art on FEVER benchmark. It again demonstrates the effectiveness of our model. CorefBERT, which incorporates coreference information in distant-supervised pre-training, contributes to verify if the claim and evidence discuss about the same mentions, such as a person or an object. Coreference Resolution Coreference resolution aims to link referring expressions that evoke the same discourse entity. We examine models’ coreference resolution ability under the setting that all mentions have been detected. We evaluate models on several widely-used datasets, including GAP (Webster et al., 2018), DPR (Rahman and Ng, 2012), WSC (Levesque, 2011), Winogender (Rudinger et al., 2018) and 7 Details are in the appendix due to space limit. LA FEVER BERT Concat GEAR∗ SR-MRS+ KGAT (BERTBASE ) # KGAT (CorefBERTBASE ) 71.01 71.60 72.56 72.81 72.88 65.64 67.10 67.26 69.40 69.82 KGAT (BERTLARGE ) # KGAT (CorefBERTLARGE ) 73.61 74.37 70.24 70.86 KGAT (RoBERTaLARGE ) # KGAT (CorefRoBERTaLarge ) 74.07 75.96 70.38 72.30 ∗ Fact Extraction and Verification Fact extraction and verification aim to verify deliberately fabricated claims with trust-worthy corpora. We evaluate our model on a large-scale public fact verification dataset"
2020.emnlp-main.582,N18-2002,0,0.0595922,"Missing"
2020.emnlp-main.582,D13-1170,0,0.0111053,"Missing"
2020.emnlp-main.582,N18-1074,0,0.0800586,"Missing"
2020.emnlp-main.582,W18-5446,0,0.0194534,"ERT model significantly outperforms BERT-LM, which demonstrates that the intrinsic coreference resolution ability of CorefBERT has been enhanced by involving the mention reference prediction training task. Moreover, it achieves comparable performance with state-of-the-art baseline WikiCREM. Note that, WikiCREM is specially designed for sentence-level coreference resolution and is not suitable for other NLP tasks. On the contrary, the coreferential reasoning capability of CorefBERT can be transferred to other NLP tasks. 4.6 GLUE The Generalized Language Understanding Evaluation dataset (GLUE) (Wang et al., 2018) is designed to evaluate and analyze the performance of models across a diverse range of existing natural language understanding tasks. We evaluate CorefBERT on the main GLUE benchmark used in BERT. Implementation Details Following BERT’s setting, we add [CLS] token in front of the input sentences, and extract its representation on the top layer as the whole sentence or sentence pair’s representation for classification or regression. Results Table 6 shows the performance on GLUE. We notice that CorefBERT achieves comparable results to BERT. Though GLUE does not require much coreference resolut"
2020.emnlp-main.582,D18-1259,0,0.0747868,"Missing"
2020.emnlp-main.582,P19-1074,1,0.726892,"ured by micro ignore F1 (IgnF1) and micro F1. IgnF1 metrics ignores the relational facts shared by the training and dev/test sets. Results with ∗ , + , # are from Yao et al. (2019), Wang et al. (2019a), and Tang et al. (2020) respectively. Baselines We compare our model with the following baselines for document-level relation extraction: (1) CNN / LSTM / BiLSTM / BERT. CNN (Zeng et al., 2014), LSTM (Hochreiter and Schmidhuber, 1997), bidirectional LSTM (BiLSTM) (Cai et al., 2016), BERT (Devlin et al., 2019) are widely adopted as text encoders in relation extraction tasks. With these encoders, Yao et al. (2019) generates representations of entities for further predicting of the relationships between entities. (2) ContextAware (Sorokin and Gurevych, 2017) takes relations’ interaction into account, which demonstrates that other relations in the context are beneficial for target relation prediction. (3) BERTTS (Wang et al., 2019a) applies a two-step prediction to deal with the large number of irrelevant entities, which first predicts whether two entities have a relationship and then predicts the specific relation. (4) HinBERT (Tang et al., 2020) proposes a hierarchical inference network to aggregate th"
2020.emnlp-main.582,C14-1220,0,0.111063,".70 53.93 54.54 53.92 55.60 56.27 56.96 BERTLARGE CorefBERTLARGE 56.51 56.82 58.70 59.01 56.01 56.40 58.31 58.83 RoBERTaLARGE CorefRoBERTaLARGE 57.19 57.35 59.40 59.43 57.74 57.90 60.06 60.25 Table 3: Results on DocRED measured by micro ignore F1 (IgnF1) and micro F1. IgnF1 metrics ignores the relational facts shared by the training and dev/test sets. Results with ∗ , + , # are from Yao et al. (2019), Wang et al. (2019a), and Tang et al. (2020) respectively. Baselines We compare our model with the following baselines for document-level relation extraction: (1) CNN / LSTM / BiLSTM / BERT. CNN (Zeng et al., 2014), LSTM (Hochreiter and Schmidhuber, 1997), bidirectional LSTM (BiLSTM) (Cai et al., 2016), BERT (Devlin et al., 2019) are widely adopted as text encoders in relation extraction tasks. With these encoders, Yao et al. (2019) generates representations of entities for further predicting of the relationships between entities. (2) ContextAware (Sorokin and Gurevych, 2017) takes relations’ interaction into account, which demonstrates that other relations in the context are beneficial for target relation prediction. (3) BERTTS (Wang et al., 2019a) applies a two-step prediction to deal with the large n"
2020.emnlp-main.582,P19-1139,1,0.848241,"have been proposed more recently, including (1) designing new pre-trainning tasks or objectives such as SpanBERT (Joshi et al., 2020) with span-based learning, XLNet (Yang et al., 2019) considering masked positions dependency with auto-regressive loss, 7171 MASS (Song et al., 2019) and BART (Wang et al., 2019b) with sequence-to-sequence pre-training, ELECTRA (Clark et al., 2020) learning from replaced token detection with generative adversarial networks and InfoWord (Kong et al., 2020) with contrastive learning; (2) integrating external knowledge such as factual knowledge in knowledge graphs (Zhang et al., 2019; Peters et al., 2019; Liu et al., 2020a); and (3) exploring multilingual learning (Conneau and Lample, 2019; Tan and Bansal, 2019; Kondratyuk and Straka, 2019) or multimodal learning (Lu et al., 2019; Sun et al., 2019a; Su et al., 2020). Though existing language representation models have achieved a great success, their coreferential reasoning capability are still far less than that of human beings (Paperno et al., 2016; Dasigi et al., 2019). In this paper, we design a mention reference prediction task to enhance language representation models in terms of coreferential reasoning. Our work, wh"
2020.emnlp-main.582,I05-5002,0,\N,Missing
2020.emnlp-main.582,P07-1107,0,\N,Missing
2020.emnlp-main.582,N16-1030,0,\N,Missing
2020.emnlp-main.582,P16-1072,0,\N,Missing
2020.emnlp-main.582,W17-2623,0,\N,Missing
2020.emnlp-main.582,P17-1152,0,\N,Missing
2020.emnlp-main.582,P17-1019,0,\N,Missing
2020.emnlp-main.582,Q19-1026,0,\N,Missing
2020.emnlp-main.582,P19-1478,0,\N,Missing
2020.emnlp-main.582,P19-1085,1,\N,Missing
2020.emnlp-main.582,N18-2108,0,\N,Missing
2020.emnlp-main.582,N18-1101,0,\N,Missing
2020.emnlp-main.582,P19-1285,0,\N,Missing
2020.emnlp-main.582,D19-1258,0,\N,Missing
2020.emnlp-main.582,Q19-1040,0,\N,Missing
2020.emnlp-main.738,W05-0909,0,0.118945,"ength and KB number, the data are mean, median, min and max respectively. 4 4.1 Experiments Experimental Setup We split WITA into a training set, a development set, and a testing set of 50,000, 5,000, and 400 records respectively. For the purpose of evaluating the performance of the models, we ask human helpers to annotate the testing set sentences. The human helpers are asked to revise the input KB triples and the corresponding target sentences making them exactly consistent with each other. We use several evaluation metrics including BLEU (Papineni et al., 2002), ROUGEL (Lin, 2004), METEOR (Banerjee and Lavie, 2005), NIST (Doddington, 2002) and CIDEr (Vedantam et al., 2015) with the package provided by Novikova et al. (2017). We follow the default setting in ROUGEL where β is set to 1.2. We build our model based on the Transformer model (Vaswani et al., 2017; Ott et al., 2019) and use Byte Pair Encoding (BPE) (Sennrich et al., 2016) to build the subword dictionary. We use Fairseq (Ott et al., 2019) to build our model and keep all hyper-parameters for Transformer unchanged. We set κ = 0.75 from {0.1, 0.25, 0.5, 0.75, 0.9} by extracting samples and ask human helper to evaluate. We use grid search to tune h"
2020.emnlp-main.738,2020.emnlp-main.90,1,0.659943,"ully et al. (2019) propose to generate a match report according to the match briefing. All these datasets are restricted to a few domains where well-aligned data is happened to be available. No existing works are focusing on handling partiallyaligned data. To solve the dataset scarcity problem, Fu et al. (2020c) propose to use dual learning to train generation models based on unaligned text and knowledge triples. The model generates text based on input triples and then predict the input triples with a dual extraction model. The two models are trained alternatively with dual learning. Although Cheng et al. (2020) proposed the ENTDESC task aiming at generating better text description for a few entities by exploring the knowledge from KB, their focus is more on distilling the useful part from the input knowledge. Text aligning has been studied for many years. Dyer et al. (2013) propose the Fast Align model which is a log-linear reparameterization of IBM Model 2. Legrand et al. (2016) propose a new score aggregation method to improve the alignment result. Moreover, attention-based models (Bahdanau et al., 2014) can also be recognized as a kind of alignment. However, these models focus on aligning source"
2020.emnlp-main.738,D16-1128,0,0.259989,"Missing"
2020.emnlp-main.738,W16-2207,0,0.0553607,"Missing"
2020.emnlp-main.738,N13-1073,0,0.0370695,"dataset scarcity problem, Fu et al. (2020c) propose to use dual learning to train generation models based on unaligned text and knowledge triples. The model generates text based on input triples and then predict the input triples with a dual extraction model. The two models are trained alternatively with dual learning. Although Cheng et al. (2020) proposed the ENTDESC task aiming at generating better text description for a few entities by exploring the knowledge from KB, their focus is more on distilling the useful part from the input knowledge. Text aligning has been studied for many years. Dyer et al. (2013) propose the Fast Align model which is a log-linear reparameterization of IBM Model 2. Legrand et al. (2016) propose a new score aggregation method to improve the alignment result. Moreover, attention-based models (Bahdanau et al., 2014) can also be recognized as a kind of alignment. However, these models focus on aligning source words to target words, and no existing models have been proposed to directly calculate supportiveness for generation tasks. In generation systems, Fu et al. (2020b) propose to dynamically align the current generation state with topics to improve the generation perform"
2020.emnlp-main.738,D19-1052,0,0.0237787,"Missing"
2020.emnlp-main.738,2020.coling-main.215,1,0.900372,"y Frank Herberfor the Dune univer ... ” which is even not a human-readable sentence. 5 Related Works During the past few years, many tasks have been proposed to generate human-readable text from the structured data. WebNLG (Gardent et al., 2017a,b; Ferreira et al., 2019) is proposed to describe KB triples sampled from DBPedia (Auer et al., 2007). 9190 The E2E (Novikova et al., 2017; Duˇsek et al., 2020) task is proposed for generating restaurant reviews based on the given attributes. Lebret et al. (2016) propose the Wikibio task to generate people’s biography based on given Wikipedia infobox. Fu et al. (2020a) propose to generate text based on event chains. Moreover, Liang et al. (2009) propose to generate weather reports for weather records and Wiseman et al. (2017), Chen and Mooney (2008) and Puduppully et al. (2019) propose to generate a match report according to the match briefing. All these datasets are restricted to a few domains where well-aligned data is happened to be available. No existing works are focusing on handling partiallyaligned data. To solve the dataset scarcity problem, Fu et al. (2020c) propose to use dual learning to train generation models based on unaligned text and knowl"
2020.emnlp-main.738,2020.aacl-main.29,1,0.902915,"y Frank Herberfor the Dune univer ... ” which is even not a human-readable sentence. 5 Related Works During the past few years, many tasks have been proposed to generate human-readable text from the structured data. WebNLG (Gardent et al., 2017a,b; Ferreira et al., 2019) is proposed to describe KB triples sampled from DBPedia (Auer et al., 2007). 9190 The E2E (Novikova et al., 2017; Duˇsek et al., 2020) task is proposed for generating restaurant reviews based on the given attributes. Lebret et al. (2016) propose the Wikibio task to generate people’s biography based on given Wikipedia infobox. Fu et al. (2020a) propose to generate text based on event chains. Moreover, Liang et al. (2009) propose to generate weather reports for weather records and Wiseman et al. (2017), Chen and Mooney (2008) and Puduppully et al. (2019) propose to generate a match report according to the match briefing. All these datasets are restricted to a few domains where well-aligned data is happened to be available. No existing works are focusing on handling partiallyaligned data. To solve the dataset scarcity problem, Fu et al. (2020c) propose to use dual learning to train generation models based on unaligned text and knowl"
2020.emnlp-main.738,P17-1017,0,0.432533,"nding to the text “developed in Canada”. The model is likely to bind the text to existing triples incorrectly. As a result, during the testing or operational stage, the model is likely to overly generate this kind of excerpt for similar triples. given structured data. For example, given the input knowledge base (KB) triple hCompany of Heroes, developer, Relic Entertainmenti, the aim is to generate a text description such as “Company of Heroes is developed by Relic Entertainment.”. In recent years, many works have been proposed to give impetus to the Data-to-Text generation task. For instance, Gardent et al. (2017a; 2017b) proposed the WebNLG task aiming at generating description text of the given KB triples. Novikova et al. (2017) proposed the E2E task aiming at generating restaurant reviews according to the given restaurant attributes. Lebret et al. (2016) proposed the WikiBio task in which the biography of each person is generated according to the given Wikipedia infobox. Introduction The Data-to-Text generation task focuses on generating human-readable text corresponding to some 1 Train The data and source code of this paper can be obtained from https://github.com/fuzihaofzh/ distant_supervision_nl"
2020.emnlp-main.738,W17-3518,0,0.628287,"nding to the text “developed in Canada”. The model is likely to bind the text to existing triples incorrectly. As a result, during the testing or operational stage, the model is likely to overly generate this kind of excerpt for similar triples. given structured data. For example, given the input knowledge base (KB) triple hCompany of Heroes, developer, Relic Entertainmenti, the aim is to generate a text description such as “Company of Heroes is developed by Relic Entertainment.”. In recent years, many works have been proposed to give impetus to the Data-to-Text generation task. For instance, Gardent et al. (2017a; 2017b) proposed the WebNLG task aiming at generating description text of the given KB triples. Novikova et al. (2017) proposed the E2E task aiming at generating restaurant reviews according to the given restaurant attributes. Lebret et al. (2016) proposed the WikiBio task in which the biography of each person is generated according to the given Wikipedia infobox. Introduction The Data-to-Text generation task focuses on generating human-readable text corresponding to some 1 Train The data and source code of this paper can be obtained from https://github.com/fuzihaofzh/ distant_supervision_nl"
2020.emnlp-main.738,P16-1154,0,0.0932135,"Missing"
2020.emnlp-main.738,P09-1011,0,0.115782,"entence. 5 Related Works During the past few years, many tasks have been proposed to generate human-readable text from the structured data. WebNLG (Gardent et al., 2017a,b; Ferreira et al., 2019) is proposed to describe KB triples sampled from DBPedia (Auer et al., 2007). 9190 The E2E (Novikova et al., 2017; Duˇsek et al., 2020) task is proposed for generating restaurant reviews based on the given attributes. Lebret et al. (2016) propose the Wikibio task to generate people’s biography based on given Wikipedia infobox. Fu et al. (2020a) propose to generate text based on event chains. Moreover, Liang et al. (2009) propose to generate weather reports for weather records and Wiseman et al. (2017), Chen and Mooney (2008) and Puduppully et al. (2019) propose to generate a match report according to the match briefing. All these datasets are restricted to a few domains where well-aligned data is happened to be available. No existing works are focusing on handling partiallyaligned data. To solve the dataset scarcity problem, Fu et al. (2020c) propose to use dual learning to train generation models based on unaligned text and knowledge triples. The model generates text based on input triples and then predict t"
2020.emnlp-main.738,D15-1166,0,0.0442865,"Missing"
2020.emnlp-main.738,W17-5525,0,0.103702,"Missing"
2020.emnlp-main.738,N19-4009,0,0.113018,"ce of the models, we ask human helpers to annotate the testing set sentences. The human helpers are asked to revise the input KB triples and the corresponding target sentences making them exactly consistent with each other. We use several evaluation metrics including BLEU (Papineni et al., 2002), ROUGEL (Lin, 2004), METEOR (Banerjee and Lavie, 2005), NIST (Doddington, 2002) and CIDEr (Vedantam et al., 2015) with the package provided by Novikova et al. (2017). We follow the default setting in ROUGEL where β is set to 1.2. We build our model based on the Transformer model (Vaswani et al., 2017; Ott et al., 2019) and use Byte Pair Encoding (BPE) (Sennrich et al., 2016) to build the subword dictionary. We use Fairseq (Ott et al., 2019) to build our model and keep all hyper-parameters for Transformer unchanged. We set κ = 0.75 from {0.1, 0.25, 0.5, 0.75, 0.9} by extracting samples and ask human helper to evaluate. We use grid search to tune hyper-parameters on the development set and choose ωw = 0.05 from {0.02,0.05,0.1,0.2,0.5,1.0,2.0,5.0}, choose ωc = 1.0 from {0.02,0.05,0.1,0.2,0.5,1.0, 2.0,5.0} and choose α = 0.1 from {0.02,0.05,0.1,0.2,0.5,1.0}. The model has 49M parameters and it takes 2.4 hours t"
2020.emnlp-main.738,P02-1040,0,0.107014,"ble 1: Statistics of WITA and WebNLG. For the text length and KB number, the data are mean, median, min and max respectively. 4 4.1 Experiments Experimental Setup We split WITA into a training set, a development set, and a testing set of 50,000, 5,000, and 400 records respectively. For the purpose of evaluating the performance of the models, we ask human helpers to annotate the testing set sentences. The human helpers are asked to revise the input KB triples and the corresponding target sentences making them exactly consistent with each other. We use several evaluation metrics including BLEU (Papineni et al., 2002), ROUGEL (Lin, 2004), METEOR (Banerjee and Lavie, 2005), NIST (Doddington, 2002) and CIDEr (Vedantam et al., 2015) with the package provided by Novikova et al. (2017). We follow the default setting in ROUGEL where β is set to 1.2. We build our model based on the Transformer model (Vaswani et al., 2017; Ott et al., 2019) and use Byte Pair Encoding (BPE) (Sennrich et al., 2016) to build the subword dictionary. We use Fairseq (Ott et al., 2019) to build our model and keep all hyper-parameters for Transformer unchanged. We set κ = 0.75 from {0.1, 0.25, 0.5, 0.75, 0.9} by extracting samples and ask"
2020.emnlp-main.738,P19-1195,0,0.0381904,"Missing"
2020.emnlp-main.738,P16-1162,0,0.0103105,"he testing set sentences. The human helpers are asked to revise the input KB triples and the corresponding target sentences making them exactly consistent with each other. We use several evaluation metrics including BLEU (Papineni et al., 2002), ROUGEL (Lin, 2004), METEOR (Banerjee and Lavie, 2005), NIST (Doddington, 2002) and CIDEr (Vedantam et al., 2015) with the package provided by Novikova et al. (2017). We follow the default setting in ROUGEL where β is set to 1.2. We build our model based on the Transformer model (Vaswani et al., 2017; Ott et al., 2019) and use Byte Pair Encoding (BPE) (Sennrich et al., 2016) to build the subword dictionary. We use Fairseq (Ott et al., 2019) to build our model and keep all hyper-parameters for Transformer unchanged. We set κ = 0.75 from {0.1, 0.25, 0.5, 0.75, 0.9} by extracting samples and ask human helper to evaluate. We use grid search to tune hyper-parameters on the development set and choose ωw = 0.05 from {0.02,0.05,0.1,0.2,0.5,1.0,2.0,5.0}, choose ωc = 1.0 from {0.02,0.05,0.1,0.2,0.5,1.0, 2.0,5.0} and choose α = 0.1 from {0.02,0.05,0.1,0.2,0.5,1.0}. The model has 49M parameters and it takes 2.4 hours to train it on a NVIDIA TITAN RTX graphics card. 4.2 Compa"
2020.emnlp-main.738,W18-6543,0,0.0851559,"TAN RTX graphics card. 4.2 Comparison Models We compare our full DSG model with the following baselines, state-of-the-art models, and ablations. S2S utilizes the traditional S2S model (Sutskever et al., 2014; Cho et al., 2014) equipped with atten0.463 0.496 0.518 0.500 0.555 0.540 0.522 7.97 8.05 8.36 8.61 8.71 8.59 8.38 0.385 0.417 0.421 0.403 0.425 0.421 0.421 0.693 0.721 0.730 0.711 0.742 0.740 0.734 4.12 4.53 4.75 4.65 5.02 4.97 4.83 Table 2: Main results. tion (Bahdanau et al., 2014; Luong et al., 2015) and copy (Gu et al., 2016) mechanism. It is recognised as the state-of-the-art model (Shimorina and Gardent, 2018) in the WebNLG (Gardent et al., 2017b) task. S2ST utilizes the prevalent Transformer model (Vaswani et al., 2017; Ott et al., 2019) which outperforms the traditional S2S model in many generation tasks. DSG-A utilizes the attention adaptor which adapts attention as the supportiveness scores in the loss. DSG-H is almost the same as our DSG model. The only difference is that the supportiveness scores are adapted with hard adaptor while our DSG model uses the soft adaptor. DSG w/o RBS is an ablation model. It removes the Rebalanced Beam Search component from our DSG model. DSG w/o SA is an ablatio"
2020.findings-emnlp.216,P17-1171,0,0.0684769,"Missing"
2020.findings-emnlp.216,N19-1423,0,0.0402623,"adden et al., 2020). The preliminary fact verification methods concatenate all evidence pieces (Nie et al., 2019; Wad2395 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2395–2400 c November 16 - 20, 2020. 2020 Association for Computational Linguistics den et al., 2020) for fact verification. KGAT (Liu et al., 2020) conducts fine-grained multiple evidence reasoning with a graph and achieves the stateof-the-art for fact verification (Ye et al., 2020). The reasoning ability of the pre-trained language model is crucial and helps improve fact verification performance (Devlin et al., 2019; Li et al., 2019; Zhou et al., 2019; Soleimani et al., 2019). Some work (Beltagy et al., 2019; Lee et al., 2020) transfers medical domain knowledge into pre-trained language models for better medical semantic understanding, which provides a potential way to deal with COVID-FACT checking problem. 3 Methodology This section describes our SciKGAT for fact extraction and verification. We first introduce the pipeline of fact extraction and verification (Sec. 3.1) and then continuously train the BERT based model (Sec. 3.2) for the whole. 3.1 Similarly, for the evidence e of the retrieved abstract a"
2020.findings-emnlp.216,2020.acl-main.740,0,0.0450638,"Missing"
2020.findings-emnlp.216,2020.nlpcovid19-2.11,0,0.0638369,"Missing"
2020.findings-emnlp.216,2020.nlpcovid19-acl.9,0,0.061591,"Missing"
2020.findings-emnlp.216,2021.ccl-1.108,0,0.172535,"Missing"
2020.findings-emnlp.216,2020.acl-main.655,1,0.902622,"h as COVID-19. 2 Related Work Existing fact extraction and verification models usually employ a three-step pipeline system (Chen et al., 2017): document retrieval (abstract retrieval), sentence selection (rationale selection) and fact verification (Thorne et al., 2018; Wadden et al., 2020). The preliminary fact verification methods concatenate all evidence pieces (Nie et al., 2019; Wad2395 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2395–2400 c November 16 - 20, 2020. 2020 Association for Computational Linguistics den et al., 2020) for fact verification. KGAT (Liu et al., 2020) conducts fine-grained multiple evidence reasoning with a graph and achieves the stateof-the-art for fact verification (Ye et al., 2020). The reasoning ability of the pre-trained language model is crucial and helps improve fact verification performance (Devlin et al., 2019; Li et al., 2019; Zhou et al., 2019; Soleimani et al., 2019). Some work (Beltagy et al., 2019; Lee et al., 2020) transfers medical domain knowledge into pre-trained language models for better medical semantic understanding, which provides a potential way to deal with COVID-FACT checking problem. 3 Methodology This section de"
2020.findings-emnlp.216,2020.emnlp-main.609,0,0.0601026,"Missing"
2020.findings-emnlp.216,2020.nlpcovid19-acl.17,0,0.0243354,"Missing"
2020.findings-emnlp.216,2020.emnlp-main.582,1,0.887037,"2017): document retrieval (abstract retrieval), sentence selection (rationale selection) and fact verification (Thorne et al., 2018; Wadden et al., 2020). The preliminary fact verification methods concatenate all evidence pieces (Nie et al., 2019; Wad2395 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2395–2400 c November 16 - 20, 2020. 2020 Association for Computational Linguistics den et al., 2020) for fact verification. KGAT (Liu et al., 2020) conducts fine-grained multiple evidence reasoning with a graph and achieves the stateof-the-art for fact verification (Ye et al., 2020). The reasoning ability of the pre-trained language model is crucial and helps improve fact verification performance (Devlin et al., 2019; Li et al., 2019; Zhou et al., 2019; Soleimani et al., 2019). Some work (Beltagy et al., 2019; Lee et al., 2020) transfers medical domain knowledge into pre-trained language models for better medical semantic understanding, which provides a potential way to deal with COVID-FACT checking problem. 3 Methodology This section describes our SciKGAT for fact extraction and verification. We first introduce the pipeline of fact extraction and verification (Sec. 3.1)"
2020.findings-emnlp.216,P19-1085,1,0.837738,"act verification methods concatenate all evidence pieces (Nie et al., 2019; Wad2395 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2395–2400 c November 16 - 20, 2020. 2020 Association for Computational Linguistics den et al., 2020) for fact verification. KGAT (Liu et al., 2020) conducts fine-grained multiple evidence reasoning with a graph and achieves the stateof-the-art for fact verification (Ye et al., 2020). The reasoning ability of the pre-trained language model is crucial and helps improve fact verification performance (Devlin et al., 2019; Li et al., 2019; Zhou et al., 2019; Soleimani et al., 2019). Some work (Beltagy et al., 2019; Lee et al., 2020) transfers medical domain knowledge into pre-trained language models for better medical semantic understanding, which provides a potential way to deal with COVID-FACT checking problem. 3 Methodology This section describes our SciKGAT for fact extraction and verification. We first introduce the pipeline of fact extraction and verification (Sec. 3.1) and then continuously train the BERT based model (Sec. 3.2) for the whole. 3.1 Similarly, for the evidence e of the retrieved abstract a, we can get the representation H of"
2020.repl4nlp-1.21,P17-2021,0,0.0160814,"can substantially improve model robustness.1 1 Introduction Self-supervised pretraining has significantly improved the performance of Transformer (Vaswani et al., 2017) on a wide range of NLP tasks (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While no explicit linguistic rules and concepts are introduced, models can achieve remarkable performances with extensive training signals provided by large-scale data. Nonetheless, recent works still demonstrate that external syntactic information can improve various NLP tasks, including machine translation (Sennrich and Haddow, 2016; Aharoni and Goldberg, 2017; Bastings et al., 2017) and semantic role labeling (Marcheggiani and Titov, 2017; Strubell et al., 2018). Can external semantic information benefit the widely-adopted pretraining and fine-tuning ∗ Indicates equal contribution. Work done at Tsinghua University. Y.Z. and C.Y. designed and evaluated the model architecture and performed the adversarial test. Z.Z. performed the data ablation study and case study. Z.L. supervised the work and is the corresponding author. 1 Codes are available at https://github.com/ yuhui-zh15/SememeTransformer/. framework as well? In response, we explore incorporat"
2020.repl4nlp-1.21,D18-1316,0,0.0125327,"dversarial examples by replacing nouns, adjectives, and adverbs for cases that both models can predict correctly. We report error rate (lower the better) categorized by part-of-speech and the number of generated adversarial examples. with less training data. As semantically similar words would share similar sememes, the sememeinformed model would better understand semantics and outperform the baseline by a large margin. 3.5 Adversarial Test and Case Study Recent research has demonstrated that neural networks are vulnerable to adversarial examples (Goodfellow et al., 2015; Jia and Liang, 2017; Alzantot et al., 2018). To evaluate the robustness of our models, we generate adversarial examples by replacing similar nouns, adjectives and adverbs for the cases that both Transformer and TransformerSEP can predict correctly. Intuitively, these words are generally more informative for prediction and models are more likely to overfit such words. Specifically, we compute the word similarity based on the novel Cilin metric (Tian and Zhao, 2010) and we use THULAC (Sun et al., 2016) for part-of-speech (POS) tagging. For the semantic matching task, we only replace words that occur in both sentences to ensure semantic c"
2020.repl4nlp-1.21,D17-1209,0,0.0401578,"Missing"
2020.repl4nlp-1.21,Q17-1010,0,0.0135673,"he input layer, a sequence of words (w1 , w2 , ..., wT ) are embedded as H0 = (w1 , w2 , ..., wT ) ∈ RT ×D , where D indicates the hidden size of the model. A positional embedding is then added to inject position information into Transformer. After L residual multi-head self-attention layers with feed-forward connections, we obtain the contextualized sequence embedding L L T ×D . HL = (hL 1 , h2 , ..., hT ) ∈ R 2.2 Aggregated Sememe Embeddings Enhancing word representation is a common approach to introduce linguistic knowledge into neural networks (Sennrich and Haddow, 2016; Niu et al., 2017; Bojanowski et al., 2017). For each word w, Transformer-SE considers all of its sememes and enhances word representation by adding its average sememe embeddings to word embedding. Formally, we have: ˜ = w 1 X xs + w nw embedding of the sememe s, w refers to the em˜ refers to the sememebedding of word w and w enhanced word embedding. Sememe-enhanced rep˜ is directly fed into Transformer. resentation w The Transformer-SE model complies with the linguistic assumption that implicit word semantics can be composed of a limited set of sememes. Also, as sememe embeddings are shared among words, latent semantic correlations be"
2020.repl4nlp-1.21,N19-1423,0,0.0360418,"anced Transformer models. Sememes, by linguistic definition, are the minimum semantic units of language, which can well represent implicit semantic meanings behind words. Our experiments demonstrate that introducing sememe knowledge into Transformer can consistently improve language modeling and downstream tasks. The adversarial test further demonstrates that sememe knowledge can substantially improve model robustness.1 1 Introduction Self-supervised pretraining has significantly improved the performance of Transformer (Vaswani et al., 2017) on a wide range of NLP tasks (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While no explicit linguistic rules and concepts are introduced, models can achieve remarkable performances with extensive training signals provided by large-scale data. Nonetheless, recent works still demonstrate that external syntactic information can improve various NLP tasks, including machine translation (Sennrich and Haddow, 2016; Aharoni and Goldberg, 2017; Bastings et al., 2017) and semantic role labeling (Marcheggiani and Titov, 2017; Strubell et al., 2018). Can external semantic information benefit the widely-adopted pretraining and fine-tuning ∗ Indicates equal"
2020.repl4nlp-1.21,D18-1493,1,0.892286,"Missing"
2020.repl4nlp-1.21,D17-1215,0,0.0335939,"r-SEP). We generate adversarial examples by replacing nouns, adjectives, and adverbs for cases that both models can predict correctly. We report error rate (lower the better) categorized by part-of-speech and the number of generated adversarial examples. with less training data. As semantically similar words would share similar sememes, the sememeinformed model would better understand semantics and outperform the baseline by a large margin. 3.5 Adversarial Test and Case Study Recent research has demonstrated that neural networks are vulnerable to adversarial examples (Goodfellow et al., 2015; Jia and Liang, 2017; Alzantot et al., 2018). To evaluate the robustness of our models, we generate adversarial examples by replacing similar nouns, adjectives and adverbs for the cases that both Transformer and TransformerSEP can predict correctly. Intuitively, these words are generally more informative for prediction and models are more likely to overfit such words. Specifically, we compute the word similarity based on the novel Cilin metric (Tian and Zhao, 2010) and we use THULAC (Sun et al., 2016) for part-of-speech (POS) tagging. For the semantic matching task, we only replace words that occur in both senten"
2020.repl4nlp-1.21,C18-1166,0,0.024691,"Missing"
2020.repl4nlp-1.21,D17-1159,0,0.0170953,"training has significantly improved the performance of Transformer (Vaswani et al., 2017) on a wide range of NLP tasks (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While no explicit linguistic rules and concepts are introduced, models can achieve remarkable performances with extensive training signals provided by large-scale data. Nonetheless, recent works still demonstrate that external syntactic information can improve various NLP tasks, including machine translation (Sennrich and Haddow, 2016; Aharoni and Goldberg, 2017; Bastings et al., 2017) and semantic role labeling (Marcheggiani and Titov, 2017; Strubell et al., 2018). Can external semantic information benefit the widely-adopted pretraining and fine-tuning ∗ Indicates equal contribution. Work done at Tsinghua University. Y.Z. and C.Y. designed and evaluated the model architecture and performed the adversarial test. Z.Z. performed the data ablation study and case study. Z.L. supervised the work and is the corresponding author. 1 Codes are available at https://github.com/ yuhui-zh15/SememeTransformer/. framework as well? In response, we explore incorporating sememe knowledge into Transformer (Vaswani et al., 2017). Sememes are the min"
2020.repl4nlp-1.21,P17-1187,1,0.915023,"://github.com/ yuhui-zh15/SememeTransformer/. framework as well? In response, we explore incorporating sememe knowledge into Transformer (Vaswani et al., 2017). Sememes are the minimum semantic units of meaning for natural language, as some linguists assume that a limited closed set of sememes can be composed to represent the semantic meaning of each word (Bloomfield, 1926). In this work, we adopt a highquality sememe-based lexical knowledge base, HowNet (Dong and Dong, 2006; Qi et al., 2019), which can provide powerful support for models to understand Chinese word semantics (Gu et al., 2018; Niu et al., 2017). Some examples of sememe annotations can be found in Figure 1. We propose to combine two simple methods to incorporate sememe knowledge into our framework: 1) based on the linguistic assumption, we add aggregated sememe embeddings to each word embedding to enhance its semantic representation; 2) we use sememe prediction as an auxiliary task to help the model gain deeper understandings of word semantics. We verify the effectiveness of our methods on several Chinese NLP tasks that are closely related to word-level and sentence-level semantics. Following general settings of pretraining and fine-"
2020.repl4nlp-1.21,C86-1107,0,0.707517,"Missing"
2020.repl4nlp-1.21,W16-2209,0,0.129311,"ates that sememe knowledge can substantially improve model robustness.1 1 Introduction Self-supervised pretraining has significantly improved the performance of Transformer (Vaswani et al., 2017) on a wide range of NLP tasks (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While no explicit linguistic rules and concepts are introduced, models can achieve remarkable performances with extensive training signals provided by large-scale data. Nonetheless, recent works still demonstrate that external syntactic information can improve various NLP tasks, including machine translation (Sennrich and Haddow, 2016; Aharoni and Goldberg, 2017; Bastings et al., 2017) and semantic role labeling (Marcheggiani and Titov, 2017; Strubell et al., 2018). Can external semantic information benefit the widely-adopted pretraining and fine-tuning ∗ Indicates equal contribution. Work done at Tsinghua University. Y.Z. and C.Y. designed and evaluated the model architecture and performed the adversarial test. Z.Z. performed the data ablation study and case study. Z.L. supervised the work and is the corresponding author. 1 Codes are available at https://github.com/ yuhui-zh15/SememeTransformer/. framework as well? In res"
2020.repl4nlp-1.21,D18-1548,0,0.052388,"Missing"
2020.repl4nlp-1.21,P15-1150,0,0.0578478,"w enhanced word embedding. Sememe-enhanced rep˜ is directly fed into Transformer. resentation w The Transformer-SE model complies with the linguistic assumption that implicit word semantics can be composed of a limited set of sememes. Also, as sememe embeddings are shared among words, latent semantic correlations between words can be well encoded. While our method to incorporate sememe knowledge is rather straightforward, our main purpose is to verify the effectiveness of sememe knowledge. We leave more potential methods to enrich word-level semantics with sememe knowledge such as tree LSTM (Tai et al., 2015) and graph convolutional network (Bastings et al., 2017) in future work. 2.3 Sememe Prediction Auxiliary Task Sememe prediction task aims to predict sememes for the next word and can be formulated as a multilabel classification task. Inspired by the multitask learning (Caruana, 1997; Collobert et al., 2011), we add the sememe prediction task in addition to the language modeling task for Transformer-SP. This task challenges the model’s capability to incorporate sememe knowledge, and can be viewed as a complementary task for language modeling, as predicting the sememes of the next word is closel"
2021.acl-demo.43,D18-1316,0,0.138032,"Missing"
2021.acl-demo.43,D18-2029,0,0.0717703,"Missing"
2021.acl-demo.43,N19-1423,0,0.0272006,"Missing"
2021.acl-demo.43,P18-2006,0,0.0300371,"Missing"
2021.acl-demo.43,2020.emnlp-main.498,0,0.0205402,"Missing"
2021.acl-demo.43,P82-1020,0,0.727813,"Missing"
2021.acl-demo.43,N18-1170,0,0.0509818,"Missing"
2021.acl-demo.43,D17-1215,0,0.0837115,"Missing"
2021.acl-demo.43,P02-1040,0,0.108913,"Missing"
2021.acl-demo.43,2020.emnlp-main.500,0,0.364903,"Missing"
2021.acl-demo.43,N19-1314,0,0.0266469,"Missing"
2021.acl-demo.43,2020.emnlp-demos.16,0,0.429524,"Missing"
2021.acl-demo.43,D19-1410,0,0.027452,"Missing"
2021.acl-demo.43,P19-1103,0,0.0607744,"Missing"
2021.acl-demo.43,P18-1079,0,0.0424559,"Missing"
2021.acl-demo.43,D13-1170,0,0.00553843,"Missing"
2021.acl-demo.43,D19-1221,0,0.0225437,"Missing"
2021.acl-demo.43,D19-3002,0,0.0416651,"Missing"
2021.acl-demo.43,2020.acl-main.540,1,0.79696,"Missing"
2021.acl-demo.43,P19-1559,0,0.0408221,"Missing"
2021.acl-long.248,Q16-1026,0,0.0342972,"he outer circle represents the fine-grained entity types, some types are denoted by abbreviations. with an additional classifier achieve significant success on this task and gradually become the base paradigm. Such studies demonstrate that deep models could yield remarkable results accompanied by a large amount of annotated corpora. Named entity recognition (NER), as a fundamental task in information extraction, aims to locate and classify named entities from unstructured natural language. A considerable number of approaches equipped with deep neural networks have shown promising performance (Chiu and Nichols, 2016) on fully supervised NER. Notably, pre-trained language models (e.g., BERT (Devlin et al., 2019a)) equal contributions corresponding authors 1 The baselines are available at https://github. com/thunlp/Few-NERD † HoHoteraryr LibOthe on Introduction ∗ rt Airpsopitall Ele cti Dis 1 r Waaster Recently, considerable literature has grown up around the theme of few-shot named entity recognition (NER), but little published benchmark data specifically focused on the practical and challenging task. Current approaches collect existing supervised NER datasets and reorganize them into the few-shot setting"
2021.acl-long.248,P18-1009,0,0.0209705,"uld facilitate the understanding of textual knowledge for neural model (Huang et al., 2020). Due to the lack of specific benchmarks of few-shot NER, current methods collect existing NER datasets and use different few-shot settings. To provide a benchmark that could comprehensively assess the generalization of models under few examples, we annotate F EW-NERD. To make the dataset practical and close to reality, we adopt a fine-grained schema of 3199 entity annotation, which is inspired and modified from previous fine-grained entity recognition studies (Ling and Weld, 2012; Gillick et al., 2014; Choi et al., 2018; Ringland et al., 2019). 3 3.1 with dense entities. Thus, as shown in Algorithm 1 we adopt a N -way K∼2K-shot setting in our paper, the primary principle of which is to ensure that each class in S contain K∼2K examples, effectively alleviating the limitations of sampling. Problem Formulation Named Entity Recognition NER is normally formulated as a sequence labeling problem. Specifically, for an input sequence of tokens x = {x1 , x2 , ..., xt }, NER aims to assign each token xi a label yi ∈ Y to indicate either the token is a part of a named entity (such as Person, Organization, Location) or n"
2021.acl-long.248,W17-4418,0,0.0463172,"Missing"
2021.acl-long.248,N19-1423,0,0.188345,"with an additional classifier achieve significant success on this task and gradually become the base paradigm. Such studies demonstrate that deep models could yield remarkable results accompanied by a large amount of annotated corpora. Named entity recognition (NER), as a fundamental task in information extraction, aims to locate and classify named entities from unstructured natural language. A considerable number of approaches equipped with deep neural networks have shown promising performance (Chiu and Nichols, 2016) on fully supervised NER. Notably, pre-trained language models (e.g., BERT (Devlin et al., 2019a)) equal contributions corresponding authors 1 The baselines are available at https://github. com/thunlp/Few-NERD † HoHoteraryr LibOthe on Introduction ∗ rt Airpsopitall Ele cti Dis 1 r Waaster Recently, considerable literature has grown up around the theme of few-shot named entity recognition (NER), but little published benchmark data specifically focused on the practical and challenging task. Current approaches collect existing supervised NER datasets and reorganize them into the few-shot setting for empirical study. These strategies conventionally aim to recognize coarse-grained entity typ"
2021.acl-long.248,D19-1033,1,0.833167,"pe-level generalization and knowledge transfer of NER methods, respectively. We implement models based on the recent state-of-theart approaches and evaluate them on F EW-NERD (Section 7). And empirical results show that F EW-NERD is challenging on all these three settings. We also conduct sets of subsidiary experiments to analyze promising directions of few-shot NER. Hopefully, the research of few-shot NER could be further facilitated by F EW-NERD. 2 Related Work As a pivotal task of information extraction, NER is essential for a wide range of technologies (Cui et al., 2017; Li et al., 2019b; Ding et al., 2019; Shen et al., 2020). And a considerable number of NER datasets have been proposed over the years. For example, CoNLL’03 (Tjong Kim Sang, 2002) is regarded as one of the most popular datasets, which is curated from Reuters News and includes 4 coarsegrained entity types. Subsequently, a series of NER datasets from various domains are proposed (Balasuriya et al., 2009; Ritter et al., 2011; Weischedel et al., 2013; Stubbs and Uzuner, 2015; Derczynski et al., 2017). These datasets formulate a sequence labeling task and most of them contain 4-18 entity types. Among them, due to the high quality and"
2021.acl-long.248,D18-1514,1,0.532942,". In the testing procedure, all the classes are unseen in the training phase, and by using few labeled examples of support set Stest , few-shot learning systems need to make T predictions of the unlabeled query set Qtest (S Q = ∅). However, in the sequence labeling problem like NER, a sentence may contain multiple entities from different classes. And it is imperative to sample examples in sentence-level since contextual information is crucial for sequence labeling problems, especially for NER. Thus the sampling is more difficult than conventional classification tasks like relation extraction (Han et al., 2018). Some previous works (Yang and Katiyar, 2020; Li et al., 2020a) use greedy-based sampling strategies to iteratively judge if a sentence could be added into the support set, but the limitation becomes gradually strict during the sampling. For example, when it comes to a 5-way 5-shot setting, if the support set already had 4 classes with 5 examples and 1 class with 4 examples, the next sampled sentence must only contain the specific one entity to strictly meet the requirement of 5 way 5 shot. It is not suitable for F EW-NERD since it is annotated for i = 1 to N do Count[i] = 0 ; 7 8 9 10 4.1 Sc"
2021.acl-long.248,2020.acl-main.128,0,0.0919742,"Missing"
2021.acl-long.248,N16-1030,0,0.0178282,"udes 4 coarsegrained entity types. Subsequently, a series of NER datasets from various domains are proposed (Balasuriya et al., 2009; Ritter et al., 2011; Weischedel et al., 2013; Stubbs and Uzuner, 2015; Derczynski et al., 2017). These datasets formulate a sequence labeling task and most of them contain 4-18 entity types. Among them, due to the high quality and size, OntoNotes 5.0 (Weischedel et al., 2013) is considered as one of the most widely used NER datasets recently. As approaches equipped with deep neural networks have shown satisfactory performance on NER with sufficient supervision (Lample et al., 2016; Ma and Hovy, 2016), few-shot NER has received increasing attention (Hofer et al., 2018; Fritzler et al., 2019; Yang and Katiyar, 2020; Li et al., 2020a). Few-shot NER is a considerably challenging and practical problem that could facilitate the understanding of textual knowledge for neural model (Huang et al., 2020). Due to the lack of specific benchmarks of few-shot NER, current methods collect existing NER datasets and use different few-shot settings. To provide a benchmark that could comprehensively assess the generalization of models under few examples, we annotate F EW-NERD. To make the"
2021.acl-long.248,2020.acl-main.519,0,0.42669,"la r Otholiticianr Dir er ect or https://ningding97.github.io/fewnerd/ With the emerging of knowledge from various domains, named entities, especially ones that need professional knowledge to understand, are difficult to be manually annotated on a large scale. Under this circumstance, studying NER systems that could learn unseen entity types with few examples, i.e., few-shot NER, plays a critical role in this area. There is a growing body of literature that recognizes the importance of few-shot NER and contributes to the task (Hofer et al., 2018; Fritzler et al., 2019; Yang and Katiyar, 2020; Li et al., 2020a; Huang et al., 2020). Unfortunately, there is still no dataset specifically designed for 3198 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3198–3213 August 1–6, 2021. ©2021 Association for Computational Linguistics few-shot NER. Hence, these methods collect previously proposed supervised NER datasets and reorganize them into a few-shot setting. Common options of datasets include OntoNotes (Weischedel et al., 2013), CoNLL’03 (Tjong Kim Sang, 2002), WNUT’17 (Derczynski e"
2021.acl-long.248,P19-1430,1,0.820394,"generalization, type-level generalization and knowledge transfer of NER methods, respectively. We implement models based on the recent state-of-theart approaches and evaluate them on F EW-NERD (Section 7). And empirical results show that F EW-NERD is challenging on all these three settings. We also conduct sets of subsidiary experiments to analyze promising directions of few-shot NER. Hopefully, the research of few-shot NER could be further facilitated by F EW-NERD. 2 Related Work As a pivotal task of information extraction, NER is essential for a wide range of technologies (Cui et al., 2017; Li et al., 2019b; Ding et al., 2019; Shen et al., 2020). And a considerable number of NER datasets have been proposed over the years. For example, CoNLL’03 (Tjong Kim Sang, 2002) is regarded as one of the most popular datasets, which is curated from Reuters News and includes 4 coarsegrained entity types. Subsequently, a series of NER datasets from various domains are proposed (Balasuriya et al., 2009; Ritter et al., 2011; Weischedel et al., 2013; Stubbs and Uzuner, 2015; Derczynski et al., 2017). These datasets formulate a sequence labeling task and most of them contain 4-18 entity types. Among them, due to"
2021.acl-long.248,P16-1101,0,0.0303931,"entity types. Subsequently, a series of NER datasets from various domains are proposed (Balasuriya et al., 2009; Ritter et al., 2011; Weischedel et al., 2013; Stubbs and Uzuner, 2015; Derczynski et al., 2017). These datasets formulate a sequence labeling task and most of them contain 4-18 entity types. Among them, due to the high quality and size, OntoNotes 5.0 (Weischedel et al., 2013) is considered as one of the most widely used NER datasets recently. As approaches equipped with deep neural networks have shown satisfactory performance on NER with sufficient supervision (Lample et al., 2016; Ma and Hovy, 2016), few-shot NER has received increasing attention (Hofer et al., 2018; Fritzler et al., 2019; Yang and Katiyar, 2020; Li et al., 2020a). Few-shot NER is a considerably challenging and practical problem that could facilitate the understanding of textual knowledge for neural model (Huang et al., 2020). Due to the lack of specific benchmarks of few-shot NER, current methods collect existing NER datasets and use different few-shot settings. To provide a benchmark that could comprehensively assess the generalization of models under few examples, we annotate F EW-NERD. To make the dataset practical a"
2021.acl-long.248,P19-1510,0,0.0199172,"understanding of textual knowledge for neural model (Huang et al., 2020). Due to the lack of specific benchmarks of few-shot NER, current methods collect existing NER datasets and use different few-shot settings. To provide a benchmark that could comprehensively assess the generalization of models under few examples, we annotate F EW-NERD. To make the dataset practical and close to reality, we adopt a fine-grained schema of 3199 entity annotation, which is inspired and modified from previous fine-grained entity recognition studies (Ling and Weld, 2012; Gillick et al., 2014; Choi et al., 2018; Ringland et al., 2019). 3 3.1 with dense entities. Thus, as shown in Algorithm 1 we adopt a N -way K∼2K-shot setting in our paper, the primary principle of which is to ensure that each class in S contain K∼2K examples, effectively alleviating the limitations of sampling. Problem Formulation Named Entity Recognition NER is normally formulated as a sequence labeling problem. Specifically, for an input sequence of tokens x = {x1 , x2 , ..., xt }, NER aims to assign each token xi a label yi ∈ Y to indicate either the token is a part of a named entity (such as Person, Organization, Location) or not belong to any entitie"
2021.acl-long.248,D11-1141,0,0.0392524,"search of few-shot NER could be further facilitated by F EW-NERD. 2 Related Work As a pivotal task of information extraction, NER is essential for a wide range of technologies (Cui et al., 2017; Li et al., 2019b; Ding et al., 2019; Shen et al., 2020). And a considerable number of NER datasets have been proposed over the years. For example, CoNLL’03 (Tjong Kim Sang, 2002) is regarded as one of the most popular datasets, which is curated from Reuters News and includes 4 coarsegrained entity types. Subsequently, a series of NER datasets from various domains are proposed (Balasuriya et al., 2009; Ritter et al., 2011; Weischedel et al., 2013; Stubbs and Uzuner, 2015; Derczynski et al., 2017). These datasets formulate a sequence labeling task and most of them contain 4-18 entity types. Among them, due to the high quality and size, OntoNotes 5.0 (Weischedel et al., 2013) is considered as one of the most widely used NER datasets recently. As approaches equipped with deep neural networks have shown satisfactory performance on NER with sufficient supervision (Lample et al., 2016; Ma and Hovy, 2016), few-shot NER has received increasing attention (Hofer et al., 2018; Fritzler et al., 2019; Yang and Katiyar, 202"
2021.acl-long.248,W02-2024,0,0.528147,"and Katiyar, 2020; Li et al., 2020a; Huang et al., 2020). Unfortunately, there is still no dataset specifically designed for 3198 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3198–3213 August 1–6, 2021. ©2021 Association for Computational Linguistics few-shot NER. Hence, these methods collect previously proposed supervised NER datasets and reorganize them into a few-shot setting. Common options of datasets include OntoNotes (Weischedel et al., 2013), CoNLL’03 (Tjong Kim Sang, 2002), WNUT’17 (Derczynski et al., 2017), etc. These research efforts of few-shot learning for named entities mainly face two challenges: First, most datasets used for few-shot learning have only 418 coarse-grained entity types, making it hard to construct an adequate variety of “N-way” metatasks and learn correlation features. And in reality, we observe that most unseen entities are finegrained. Second, because of the lack of benchmark datasets, the settings of different works are inconsistent (Huang et al., 2020; Yang and Katiyar, 2020), leading to unclear comparisons. To sum up, these methods ma"
2021.acl-long.248,2020.emnlp-main.129,1,0.877673,"Missing"
2021.acl-long.248,2020.emnlp-main.516,0,0.358306,"Claar ne S S oldie P chola r Otholiticianr Dir er ect or https://ningding97.github.io/fewnerd/ With the emerging of knowledge from various domains, named entities, especially ones that need professional knowledge to understand, are difficult to be manually annotated on a large scale. Under this circumstance, studying NER systems that could learn unseen entity types with few examples, i.e., few-shot NER, plays a critical role in this area. There is a growing body of literature that recognizes the importance of few-shot NER and contributes to the task (Hofer et al., 2018; Fritzler et al., 2019; Yang and Katiyar, 2020; Li et al., 2020a; Huang et al., 2020). Unfortunately, there is still no dataset specifically designed for 3198 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3198–3213 August 1–6, 2021. ©2021 Association for Computational Linguistics few-shot NER. Hence, these methods collect previously proposed supervised NER datasets and reorganize them into a few-shot setting. Common options of datasets include OntoNotes (Weischedel et al., 2013), CoNLL’03 (Tjong Kim Sang, 2002), WNUT"
2021.acl-long.248,D18-1259,0,0.0541568,"Missing"
2021.acl-long.260,P17-1147,0,0.0212785,"tions. From the results listed in Table 4, we observe that ERICA outperforms baselines in both settings, indicating that ERICA can better understand entities and their relations in the documents and extract the true answer according to queries. The significant improvements in the masked setting also indicate that ERICA can better perform multi-hop reasoning to synthesize and analyze information from contexts, instead of relying on entity mention “shortcuts” (Jiang and Bansal, 2019). Extractive QA For extractive QA, we adopt three widely-used datasets: SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017) and NaturalQA (Kwiatkowski et al., 2019) in MRQA (Fisch et al., 2019) to evaluate ERICA in various domains. Since MRQA does not provide the test set for each dataset, we randomly split the original dev set into two halves and obtain the new dev/test set. We follow the QA setting of BERT (Devlin et al., 2018): we concatenate the given question and passage into one long sequence, encode the sequence by PLMs and adopt two classifiers to predict the start and end index of the answer. We choose BERT, RoBERTa, MTB and CP as baselines. From the results listed in Table 5, we observe that ERICA outper"
2021.acl-long.260,Q19-1026,0,0.0120968,"able 4, we observe that ERICA outperforms baselines in both settings, indicating that ERICA can better understand entities and their relations in the documents and extract the true answer according to queries. The significant improvements in the masked setting also indicate that ERICA can better perform multi-hop reasoning to synthesize and analyze information from contexts, instead of relying on entity mention “shortcuts” (Jiang and Bansal, 2019). Extractive QA For extractive QA, we adopt three widely-used datasets: SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017) and NaturalQA (Kwiatkowski et al., 2019) in MRQA (Fisch et al., 2019) to evaluate ERICA in various domains. Since MRQA does not provide the test set for each dataset, we randomly split the original dev set into two halves and obtain the new dev/test set. We follow the QA setting of BERT (Devlin et al., 2018): we concatenate the given question and passage into one long sequence, encode the sequence by PLMs and adopt two classifiers to predict the start and end index of the answer. We choose BERT, RoBERTa, MTB and CP as baselines. From the results listed in Table 5, we observe that ERICA outperforms all baselines, indicating that thro"
2021.acl-long.260,2021.ccl-1.108,0,0.049509,"Missing"
2021.acl-long.260,2020.emnlp-main.298,1,0.863483,"Missing"
2021.acl-long.260,D19-1005,0,0.0318608,"heir relations, which are crucial for understanding the whole text. To improve the entity and relation understanding of PLMs, a typical line of work is knowledgeguided PLM, which incorporates external knowledge such as Knowledge Graphs (KGs) into PLMs to enhance the entity and relation understanding. Some enforce PLMs to memorize information about real-world entities and propose novel pretraining objectives (Xiong et al., 2019; Wang et al., 2019; Sun et al., 2020; Yamada et al., 2020). Others modify the internal structures of PLMs to fuse both textual and KG’s information (Zhang et al., 2019; Peters et al., 2019; Wang et al., 2020; He et al., 2020). Although knowledge-guided PLMs introduce extra factual knowledge in KGs, these methods ignore the intrinsic relational facts in text, making it hard to understand out-of-KG entities or knowledge in downstream tasks, let alone the errors and incompleteness of KGs. This verifies the necessity of teaching PLMs to understand relational facts from contexts. Another line of work is to directly model entities or relations in text in pre-training stage to break the limitations of individual token representations. Some focus on obtaining better span representation"
2021.acl-long.260,D16-1264,0,0.0361457,"hich are introduced in previous sections. From the results listed in Table 4, we observe that ERICA outperforms baselines in both settings, indicating that ERICA can better understand entities and their relations in the documents and extract the true answer according to queries. The significant improvements in the masked setting also indicate that ERICA can better perform multi-hop reasoning to synthesize and analyze information from contexts, instead of relying on entity mention “shortcuts” (Jiang and Bansal, 2019). Extractive QA For extractive QA, we adopt three widely-used datasets: SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017) and NaturalQA (Kwiatkowski et al., 2019) in MRQA (Fisch et al., 2019) to evaluate ERICA in various domains. Since MRQA does not provide the test set for each dataset, we randomly split the original dev set into two halves and obtain the new dev/test set. We follow the QA setting of BERT (Devlin et al., 2018): we concatenate the given question and passage into one long sequence, encode the sequence by PLMs and adopt two classifiers to predict the start and end index of the answer. We choose BERT, RoBERTa, MTB and CP as baselines. From the results listed in Table"
2021.acl-long.260,W04-2401,0,0.417814,"Missing"
2021.acl-long.260,W03-0419,0,0.619684,"Missing"
2021.acl-long.260,P19-1279,0,0.23953,"s of KGs. This verifies the necessity of teaching PLMs to understand relational facts from contexts. Another line of work is to directly model entities or relations in text in pre-training stage to break the limitations of individual token representations. Some focus on obtaining better span representations, including entity mentions, via span-based pre-training (Sun et al., 2019; Joshi et al., 2020; Kong et al., 2020; Ye et al., 2020). Others learn to extract relation-aware semantics from text by comparing the sentences that share the same entity pair or distantly supervised relation in KGs (Soares et al., 2019; Peng et al., 2020). However, these methods only consider either individual entities or within-sentence relations, which limits the performance in dealing with multiple entities and relations at document level. In contrast, our ERICA considers the interactions among multiple entities 3351 {h1 , h2 , ..., h|di |}, then we apply mean pooling operation over the consecutive tokens that mention eij to obtain local entity representations. Note eij may appear multiple times in di , the k-th occurrence of eij , which contains the tokens from index nkstart to nkend , is represented as: mkeij = MeanPoo"
2021.acl-long.260,2020.coling-main.327,0,0.0338092,"y. Although achieving great success, these PLMs usually regard words as basic units in textual understanding, ignoring the informative entities and their relations, which are crucial for understanding the whole text. To improve the entity and relation understanding of PLMs, a typical line of work is knowledgeguided PLM, which incorporates external knowledge such as Knowledge Graphs (KGs) into PLMs to enhance the entity and relation understanding. Some enforce PLMs to memorize information about real-world entities and propose novel pretraining objectives (Xiong et al., 2019; Wang et al., 2019; Sun et al., 2020; Yamada et al., 2020). Others modify the internal structures of PLMs to fuse both textual and KG’s information (Zhang et al., 2019; Peters et al., 2019; Wang et al., 2020; He et al., 2020). Although knowledge-guided PLMs introduce extra factual knowledge in KGs, these methods ignore the intrinsic relational facts in text, making it hard to understand out-of-KG entities or knowledge in downstream tasks, let alone the errors and incompleteness of KGs. This verifies the necessity of teaching PLMs to understand relational facts from contexts. Another line of work is to directly model entities or"
2021.acl-long.260,P19-1485,0,0.020828,"elf is considerably smaller, measuring only. [6] Culiacán is a rail junction and is located on the Panamerican Highway that runs south to Guadalajara and Mexico City. [7] Culiacán is connected to the north with Los Mochis, and to the south with Mazatlán, Tepic. 1 Q: where is Guadalajara? Mexico Pre-trained Language Models (PLMs) (Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019) have shown superior performance on various Natural Language Processing (NLP) tasks such as text classification (Wang et al., 2018), named entity recognition (Sang and De Meulder, 2003), and question answering (Talmor and Berant, 2019). Benefiting from designing various effective self-supervised learning objectives, such as masked language modeling (Devlin et al., 2018), PLMs can effectively capture the syntax and semantics in text to generate informative language representations for downstream NLP tasks. Corresponding author. Our code and data are publicly available at https:// github.com/thunlp/ERICA. 1 A: Mexico. o Panamerican Highway Los Mochis Sinaloa Mexico City Guadalajara Figure 1: An example for a document “Culiacán”, in which all entities are underlined. We show entities and their relations as a relational graph,"
2021.acl-long.260,W18-5446,0,0.0555444,"Missing"
2021.acl-long.260,K17-1028,0,0.0151597,"reading multiple documents and conducting multi-hop reasoning. It has both standard and masked settings, where the latter setting masks all entities with random IDs to avoid information leakage. We first concatenate the question and documents into a long sequence, then we find all the occurrences of an entity in the documents, encode them into hidden representations and obtain the global entity representation by applying mean pooling on these hidden representations. Finally, we use a classifier on top of the entity representation for prediction. We choose the following baselines: (1) FastQA (Weissenborn et al., 2017) and BiDAF (Seo et al., 2016), which are widely used question answering systems; (2) BERT, RoBERTa, CorefBERT, SpanBERT, MTB and CP, which are introduced in previous sections. From the results listed in Table 4, we observe that ERICA outperforms baselines in both settings, indicating that ERICA can better understand entities and their relations in the documents and extract the true answer according to queries. The significant improvements in the masked setting also indicate that ERICA can better perform multi-hop reasoning to synthesize and analyze information from contexts, instead of relying"
2021.acl-long.260,Q18-1021,0,0.0536187,"Missing"
2021.acl-long.260,C14-1220,0,0.139289,"Missing"
2021.acl-long.260,D17-1004,0,0.026425,"easoning patterns in the pre-training; (2) both MTB and CP achieve worse results than BERT, which means sentence-level pre-training, lacking consideration for complex reasoning patterns, hurts PLM’s performance on document-level RE tasks to some extent; (3) ERICA outperforms baselines by a larger margin on smaller training sets, which means ERICA has gained pretty good document-level relation reasoning ability in contrastive learning, and thus obtains improvements more extensively under low-resource settings. Sentence-level RE For sentence-level RE, we choose two widely used datasets: TACRED (Zhang et al., 2017) and SemEval-2010 Task 8 (Hendrickx et al., 2019). We insert extra marker tokens to indicate the head and tail entities in each sentence. For baselines, we compare ERICA with BERT, RoBERTa, MTB and CP. From the results shown in Table 2, we observe that ERICA achieves almost comparable results on sentence-level RE tasks with CP, which means document-level pre-training in 10 In practice, documents are split into sentences and we only keep within-sentence entity pairs. 11 https://github.com/thunlp/ RE-Context-or-Names - 27.2 49.7 53.7 54.4 56.4 51.7 50.4 57.8 69.5 68.8 70.7 68.4 67.4 69.7 37.9 39"
2021.acl-long.260,P19-1139,1,0.80299,"ative entities and their relations, which are crucial for understanding the whole text. To improve the entity and relation understanding of PLMs, a typical line of work is knowledgeguided PLM, which incorporates external knowledge such as Knowledge Graphs (KGs) into PLMs to enhance the entity and relation understanding. Some enforce PLMs to memorize information about real-world entities and propose novel pretraining objectives (Xiong et al., 2019; Wang et al., 2019; Sun et al., 2020; Yamada et al., 2020). Others modify the internal structures of PLMs to fuse both textual and KG’s information (Zhang et al., 2019; Peters et al., 2019; Wang et al., 2020; He et al., 2020). Although knowledge-guided PLMs introduce extra factual knowledge in KGs, these methods ignore the intrinsic relational facts in text, making it hard to understand out-of-KG entities or knowledge in downstream tasks, let alone the errors and incompleteness of KGs. This verifies the necessity of teaching PLMs to understand relational facts from contexts. Another line of work is to directly model entities or relations in text in pre-training stage to break the limitations of individual token representations. Some focus on obtaining bette"
2021.acl-long.260,2020.emnlp-main.523,0,0.162157,"ing great success, these PLMs usually regard words as basic units in textual understanding, ignoring the informative entities and their relations, which are crucial for understanding the whole text. To improve the entity and relation understanding of PLMs, a typical line of work is knowledgeguided PLM, which incorporates external knowledge such as Knowledge Graphs (KGs) into PLMs to enhance the entity and relation understanding. Some enforce PLMs to memorize information about real-world entities and propose novel pretraining objectives (Xiong et al., 2019; Wang et al., 2019; Sun et al., 2020; Yamada et al., 2020). Others modify the internal structures of PLMs to fuse both textual and KG’s information (Zhang et al., 2019; Peters et al., 2019; Wang et al., 2020; He et al., 2020). Although knowledge-guided PLMs introduce extra factual knowledge in KGs, these methods ignore the intrinsic relational facts in text, making it hard to understand out-of-KG entities or knowledge in downstream tasks, let alone the errors and incompleteness of KGs. This verifies the necessity of teaching PLMs to understand relational facts from contexts. Another line of work is to directly model entities or relations in text in p"
2021.acl-long.260,P19-1074,1,0.884737,"Missing"
2021.acl-long.260,2020.emnlp-main.582,1,0.844179,"e methods ignore the intrinsic relational facts in text, making it hard to understand out-of-KG entities or knowledge in downstream tasks, let alone the errors and incompleteness of KGs. This verifies the necessity of teaching PLMs to understand relational facts from contexts. Another line of work is to directly model entities or relations in text in pre-training stage to break the limitations of individual token representations. Some focus on obtaining better span representations, including entity mentions, via span-based pre-training (Sun et al., 2019; Joshi et al., 2020; Kong et al., 2020; Ye et al., 2020). Others learn to extract relation-aware semantics from text by comparing the sentences that share the same entity pair or distantly supervised relation in KGs (Soares et al., 2019; Peng et al., 2020). However, these methods only consider either individual entities or within-sentence relations, which limits the performance in dealing with multiple entities and relations at document level. In contrast, our ERICA considers the interactions among multiple entities 3351 {h1 , h2 , ..., h|di |}, then we apply mean pooling operation over the consecutive tokens that mention eij to obtain local entity"
2021.acl-long.37,N19-1423,0,0.0864666,"Missing"
2021.acl-long.37,P82-1020,0,0.71241,"Missing"
2021.acl-long.37,N18-1170,0,0.0754064,"Missing"
2021.acl-long.37,2020.acl-main.249,0,0.347442,"Missing"
2021.acl-long.37,P14-5010,0,0.00882952,"Missing"
2021.acl-long.37,2021.acl-long.377,1,0.797483,"Missing"
2021.acl-long.37,N19-1144,0,0.0944813,"Missing"
2021.acl-long.37,P18-1079,0,0.052074,"Missing"
2021.acl-long.37,2020.acl-main.540,1,0.792848,"Missing"
2021.acl-long.37,P17-1099,0,0.081181,"Missing"
2021.acl-long.37,J05-2002,0,0.126771,"Missing"
2021.acl-long.37,D13-1170,0,0.0491743,"Missing"
2021.acl-long.377,D18-1316,0,0.0758004,"Missing"
2021.acl-long.377,Q18-1036,0,0.0296289,"Missing"
2021.acl-long.377,N19-1423,0,0.0570118,"Missing"
2021.acl-long.377,2020.coling-main.155,1,0.7636,"Missing"
2021.acl-long.377,2020.acl-main.249,0,0.164488,"Missing"
2021.acl-long.377,P19-1571,1,0.818064,"Missing"
2021.acl-long.377,2021.acl-long.37,1,0.797483,"Missing"
2021.acl-long.377,C86-1107,0,0.422633,"Missing"
2021.acl-long.377,P19-1103,0,0.12002,"Missing"
2021.acl-long.377,W17-1101,0,0.079212,"Missing"
2021.acl-long.377,D13-1170,0,0.0164523,"Missing"
2021.acl-long.377,N19-1144,0,0.0913317,"Missing"
2021.acl-long.377,2020.acl-main.540,1,0.876268,"Missing"
2021.acl-long.377,P19-1559,0,0.0476727,"Missing"
2021.acl-long.390,2021.eacl-main.92,0,0.237041,"b queries that innately lack large supervision (Downey et al., 2007), applications with strong privacy constraints like personal and enterprise search (Chirita et al., 2005; Hawking, 2004), and domains where labeling requires professional expertise such as biomedical and legal search (Roberts et al., 2020; Arora et al., 2018). To broaden the benefits of Neu-IR to few-shot scenarios, we present an adaptive learning method MetaAdaptRank that meta-learns to adapt Neu-IR models to target domains with synthetic weak supervision. For synthesizing weak supervision, we take inspiration from the work (Ma et al., 2021) that generates related queries for unlabeled documents in a zero-shot way, but we generate discriminative queries based on contrastive pairs of relevant (positive) and irrelevant (negative) documents. By introducing the negative contrast, MetaAdaptRank can subtly capture the difference between documents to synthesize more ranking-aware weak supervision signals. Given that synthetic weak supervision inevitably contains noises, MetaAdaptRank metalearns to reweight these synthetic weak data and trains Neu-IR models to achieve the best accuracy on a small volume of target data. In this way, neura"
2021.acl-long.390,2020.nlpcovid19-acl.1,0,0.0359683,"ble in web domains. Directly applying them to non-web domains may suffer from suboptimal outcomes due to domain gaps. To obtain weak supervision that adapts arbitrary domains, Ma et al. (2021) present a synthetic query generation method, which can be trained with source-domain relevance signals and applied on target-domain documents to generate related queries. More recently, a novel meta-learning technique has shown encouraging progress on solving data noises and label biases in computer vision (Ren et al., 2018; Shu et al., 2019; Zheng et al., 2019a) and some NLP tasks (Zheng et al., 2019a; Wang et al., 2020b). To the best of our knowledge, this novel technique has not been well utilized in information retrieval and synthetic supervision settings. 3 Methodology This section first recaps the preliminary of Neu-IR and then introduces our proposed MetaAdaptRank. The framework of our method is shown in Figure 1. 3.1 Preliminary of Neu-IR The ad-hoc retrieval task is to calculate a ranking score f (q, d; θ) for a query q and a document d from a document set. In Neu-IR, the ranking score f (·; θ) is calculated by a neural model, e.g., BERT, with parameters θ. The query q and the document d are encoded"
2021.acl-long.390,2020.acl-main.754,0,0.145874,"ble in web domains. Directly applying them to non-web domains may suffer from suboptimal outcomes due to domain gaps. To obtain weak supervision that adapts arbitrary domains, Ma et al. (2021) present a synthetic query generation method, which can be trained with source-domain relevance signals and applied on target-domain documents to generate related queries. More recently, a novel meta-learning technique has shown encouraging progress on solving data noises and label biases in computer vision (Ren et al., 2018; Shu et al., 2019; Zheng et al., 2019a) and some NLP tasks (Zheng et al., 2019a; Wang et al., 2020b). To the best of our knowledge, this novel technique has not been well utilized in information retrieval and synthetic supervision settings. 3 Methodology This section first recaps the preliminary of Neu-IR and then introduces our proposed MetaAdaptRank. The framework of our method is shown in Figure 1. 3.1 Preliminary of Neu-IR The ad-hoc retrieval task is to calculate a ranking score f (q, d; θ) for a query q and a document d from a document set. In Neu-IR, the ranking score f (·; θ) is calculated by a neural model, e.g., BERT, with parameters θ. The query q and the document d are encoded"
2021.acl-long.390,2020.nlpcovid19-acl.2,0,0.0642388,"n et al., 2017) and fact verification (Liu et al., 2020). Neural information retrieval (Neu-IR) models have recently shown advanced results in many ranking scenarios where massive relevance labels or clickthrough data are available (Mitra et al., 2018; Craswell et al., 2020). The flip side is that the “data-hungry” nature of Neu-IR models yields mixed results in few-shot ranking scenarios that suffer from the shortage of labeled data and implicit user feedback (Lin, 2019; Yang et al., 2019). On ranking benchmarks with debates about whether Neu-IR, even with billions of pre-trained parameters (Zhang et al., 2020a), really outperforms traditional IR techniques such as feature-based models and latent semantic indexing (Yang et al., 2019; Roberts et al., 2020). In fact, many real-world ranking scenarios are fewshot, e.g., tail web queries that innately lack large supervision (Downey et al., 2007), applications with strong privacy constraints like personal and enterprise search (Chirita et al., 2005; Hawking, 2004), and domains where labeling requires professional expertise such as biomedical and legal search (Roberts et al., 2020; Arora et al., 2018). To broaden the benefits of Neu-IR to few-shot scenar"
2021.acl-long.491,W13-2322,0,0.111426,"Missing"
2021.acl-long.491,D13-1185,0,0.0173718,"2006) and similar datasets (Ellis et al., 2015, 2016; Getman et al., 2017; Wang et al., 2020), these PLM-based works solely focus on better finetuning rather than pre-training for EE. In this paper, we study pre-training to better utilize rich event knowledge in large-scale unsupervised data. Event Schema Induction. Supervised EE models cannot generalize to continually-emerging new event types and argument roles. To this end, Chambers and Jurafsky (2011) explore to induce event schemata from raw text by unsupervised clustering. Following works introduce more features like coreference chains (Chambers, 2013) and entities (Nguyen et al., 2015; Sha et al., 2016). Recently, Huang and Ji (2020) move to the semi6284 Event Semantic Pre-training Unsupervised Corpora Trigger-Argument Pair Discrimination attack CNN&apos;s Kelly Wallace reports on today&apos;s attack in Netanya. Text Encoder The army said two soldiers were also among the dead. Trigger Replacement … Netanya reports CNN&apos;s Kelly Wallace today&apos;s reports Argument Replacement AMR Parsing Event Structure Pre-training Parsed AMR Graphs ARG0 ARG1 ARG1 attack-01 time ARG1 today dead ARG1 ARG1 soldier quant today Netanya say-01 army time mod 2 also Subgraph Sa"
2021.acl-long.491,P11-1098,0,0.0297505,"ang et al., 2019a,b; Yang et al., 2019; Wadden et al., 2019; Tong et al., 2020). Although achieving remarkable performance in benchmarks such as ACE 2005 (Walker et al., 2006) and similar datasets (Ellis et al., 2015, 2016; Getman et al., 2017; Wang et al., 2020), these PLM-based works solely focus on better finetuning rather than pre-training for EE. In this paper, we study pre-training to better utilize rich event knowledge in large-scale unsupervised data. Event Schema Induction. Supervised EE models cannot generalize to continually-emerging new event types and argument roles. To this end, Chambers and Jurafsky (2011) explore to induce event schemata from raw text by unsupervised clustering. Following works introduce more features like coreference chains (Chambers, 2013) and entities (Nguyen et al., 2015; Sha et al., 2016). Recently, Huang and Ji (2020) move to the semi6284 Event Semantic Pre-training Unsupervised Corpora Trigger-Argument Pair Discrimination attack CNN&apos;s Kelly Wallace reports on today&apos;s attack in Netanya. Text Encoder The army said two soldiers were also among the dead. Trigger Replacement … Netanya reports CNN&apos;s Kelly Wallace today&apos;s reports Argument Replacement AMR Parsing Event Structur"
2021.acl-long.491,P17-1038,0,0.0187182,"tune general PLMs (e.g, BERT (Devlin et al., 2019)) for EE. Benefiting from the strong general language understanding ability learnt from large-scale unsupervised data, these PLM-based methods have achieved state-ofthe-art performance in various public benchmarks. Although leveraging unsupervised data with pretraining has gradually become a consensus for EE and NLP community, there still lacks a pre-training method orienting event modeling to take full advantage of rich event knowledge lying in largescale unsupervised data. The key challenge here is to find reasonable self-supervised signals (Chen et al., 2017; Wang et al., 2019a) for the diverse semantics and complex structures of events. Fortunately, previous work (Aguilar et al., 2014; Huang et al., 2016) has suggested that sentence semantic structures, such as abstract meaning representation (AMR) (Banarescu et al., 2013), contain broad and diverse semantic and structure information relating to events. As shown in Figure 1, the parsed AMR structure covers not only the annotated event (Attack) but also the event that is not defined in the ACE 2005 schema (Report). Considering the fact that the AMR structures of large-scale unsupervised data can"
2021.acl-long.491,P15-1017,0,0.34232,"1 CNN’s Kelly Wallace attack-01 Introduction ∗ Event Schema report-01 classify event types (Attack), as well as event argument extraction task to identify entities serving as event arguments (“today” and “Netanya”) and classify their argument roles (Time-within and Place) (Ahn, 2006). By explicitly capturing the event structure in the text, EE can benefit various downstream tasks such as information reˇ trieval (Glavaˇs and Snajder, 2014) and knowledge base population (Ji and Grishman, 2011). Existing EE methods mainly follow the supervised-learning paradigm to train advanced neural networks (Chen et al., 2015; Nguyen et al., 2016; Nguyen and Grishman, 2018) with humanannotated datasets and pre-defined event schemata. These methods work well in lots of public benchmarks such as ACE 2005 (Walker et al., 2006) and TAC KBP (Ellis et al., 2016), yet they still suffer from data scarcity and limited generalizability. Since annotating event data and defining event schemata are especially expensive and laborintensive, existing EE datasets typically only contain thousands of instances and cover limited event types. Thus they are inadequate to train large neural models (Wang et al., 2020) and develop methods"
2021.acl-long.491,2020.emnlp-main.444,0,0.0363461,"et al., 2018; Oord et al., 2018; Hjelm et al., 2019; Chen et al., 2020; He et al., 2020) and graph (Qiu et al., 2020; You et al., 2020; Zhu et al., 2020). In the context of NLP, many established representation learning works can be viewed as contrastive learning methods, such as Word2Vec (Mikolov et al., 2013), BERT (Devlin et al., 2019; Kong et al., 2020) and ELECTRA (Clark et al., 2020). Similar to this work, contrastive learning is also widely-used to help specific tasks, including question answering (Yeh and Chen, 2019), discourse modeling (Iter et al., 2020), natural language inference (Cui et al., 2020) and relation extraction (Peng et al., 2020). 3 Methodology The overall CLEVE framework is illustrated in Figure 2. As shown in the illustration, our contrastive pre-training framework CLEVE consists of two components: event semantic pre-training and event structure pre-training, of which details are introduced in Section 3.2 and Section 3.3, respectively. At the beginning of this section, we first introduce the required preprocessing in Section 3.1, including the AMR parsing and how we modify the parsed AMR structures for our pre-training. 3.1 Preprocessing CLEVE relies on AMR structures (Ban"
2021.acl-long.491,P09-2093,0,0.0217807,"y. Meanwhile, the pre-trained representations can also directly help extract events and discover new event schemata without any known event schema or annotated instances, leading to better generalizability. This is a challenging unsupervised setting named “liberal event extraction” (Huang et al., 2016). Experiments on the widely-used ACE 2005 and the large MAVEN datasets indicate that CLEVE can achieve significant improvements in both settings. 2 Related Work Event Extraction. Most of the existing EE works follow the supervised learning paradigm. Traditional EE methods (Ji and Grishman, 2008; Gupta and Ji, 2009; Li et al., 2013) rely on manually-crafted features to extract events. In recent years, the neural models become mainstream, which automatically learn effective features with neural networks, including convolutional neural networks (Nguyen and Grishman, 2015; Chen et al., 2015), recurrent neural networks (Nguyen et al., 2016), graph convolutional networks (Nguyen and Grishman, 2018; Lai et al., 2020). With the recent successes of BERT (Devlin et al., 2019), PLMs have also been used for EE (Wang et al., 2019a,b; Yang et al., 2019; Wadden et al., 2019; Tong et al., 2020). Although achieving rem"
2021.acl-long.491,2020.acl-main.740,0,0.0611314,"Missing"
2021.acl-long.491,P16-1025,0,0.155473,"unsupervised data, these PLM-based methods have achieved state-ofthe-art performance in various public benchmarks. Although leveraging unsupervised data with pretraining has gradually become a consensus for EE and NLP community, there still lacks a pre-training method orienting event modeling to take full advantage of rich event knowledge lying in largescale unsupervised data. The key challenge here is to find reasonable self-supervised signals (Chen et al., 2017; Wang et al., 2019a) for the diverse semantics and complex structures of events. Fortunately, previous work (Aguilar et al., 2014; Huang et al., 2016) has suggested that sentence semantic structures, such as abstract meaning representation (AMR) (Banarescu et al., 2013), contain broad and diverse semantic and structure information relating to events. As shown in Figure 1, the parsed AMR structure covers not only the annotated event (Attack) but also the event that is not defined in the ACE 2005 schema (Report). Considering the fact that the AMR structures of large-scale unsupervised data can be easily obtained with automatic parsers (Wang et al., 2015), we propose CLEVE, an event-oriented contrastive pre-training framework utilizing AMR str"
2021.acl-long.491,2020.emnlp-main.53,0,0.264824,"notated datasets and pre-defined event schemata. These methods work well in lots of public benchmarks such as ACE 2005 (Walker et al., 2006) and TAC KBP (Ellis et al., 2016), yet they still suffer from data scarcity and limited generalizability. Since annotating event data and defining event schemata are especially expensive and laborintensive, existing EE datasets typically only contain thousands of instances and cover limited event types. Thus they are inadequate to train large neural models (Wang et al., 2020) and develop methods that can generalize to continually-emerging new event types (Huang and Ji, 2020). Inspired by the success of recent pre-trained language models (PLMs) for NLP tasks, some pio6283 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6283–6297 August 1–6, 2021. ©2021 Association for Computational Linguistics neering work (Wang et al., 2019a; Wadden et al., 2019) attempts to fine-tune general PLMs (e.g, BERT (Devlin et al., 2019)) for EE. Benefiting from the strong general language understanding ability learnt from large-scale unsupervised data, these PLM-base"
2021.acl-long.491,P18-1201,0,0.019335,"employ a PLM as the text encoder and encourage the representations of the word pairs connected by the ARG, time, location edges in AMR structures to be closer in the semantic space than other unrelated words, since these pairs usually refer to the trigger-argument pairs of the same events (as shown in Figure 1) (Huang et al., 2016). This is done by contrastive learning with the connected word pairs as positive samples and unrelated words as negative samples. Moreover, considering event structures are also helpful in extracting events (Lai et al., 2020) and generalizing to new event schemata (Huang et al., 2018), we need to learn transferable event structure representations. Hence we further introduce a graph neural network (GNN) as the graph encoder to encode AMR structures as structure representations. The graph encoder is contrastively pre-trained on the parsed AMR structures of large unsupervised corpora with AMR subgraph discrimination as the objective. By fine-tuning the two pre-trained models on downstream EE datasets and jointly using the two representations, CLEVE can benefit the conventional supervised EE suffering from data scarcity. Meanwhile, the pre-trained representations can also dire"
2021.acl-long.491,2020.acl-main.439,0,0.0207388,"in various domains, such as computer vision (Wu et al., 2018; Oord et al., 2018; Hjelm et al., 2019; Chen et al., 2020; He et al., 2020) and graph (Qiu et al., 2020; You et al., 2020; Zhu et al., 2020). In the context of NLP, many established representation learning works can be viewed as contrastive learning methods, such as Word2Vec (Mikolov et al., 2013), BERT (Devlin et al., 2019; Kong et al., 2020) and ELECTRA (Clark et al., 2020). Similar to this work, contrastive learning is also widely-used to help specific tasks, including question answering (Yeh and Chen, 2019), discourse modeling (Iter et al., 2020), natural language inference (Cui et al., 2020) and relation extraction (Peng et al., 2020). 3 Methodology The overall CLEVE framework is illustrated in Figure 2. As shown in the illustration, our contrastive pre-training framework CLEVE consists of two components: event semantic pre-training and event structure pre-training, of which details are introduced in Section 3.2 and Section 3.3, respectively. At the beginning of this section, we first introduce the required preprocessing in Section 3.1, including the AMR parsing and how we modify the parsed AMR structures for our pre-training. 3.1 Pr"
2021.acl-long.491,2020.findings-emnlp.326,0,0.0290477,"he golden trigger-argument pairs and event structures of ACE 2005 training set instead of the AMR structures of NYT. Similarly, the on ACE (AMR) model is pre-trained with the parsed AMR structures of ACE 2005 training set. We also compare CLEVE with various baselines, including: (1) feature-based method, the top-performing JointBeam (Li et al., 2013); (2) vanilla neural model DMCNN (Chen et al., 2015); (3) the model incorporating syntactic knowledge, dbRNN (Sha et al., 2018); (4) stateof-the-art models on ED and EAE respectively, including GatedGCN (Lai et al., 2020) and SemSynGTN (Pouran Ben Veyseh et al., 2020); (5) a stateof-the-art EE model RCEE ER (Liu et al., 2020), which tackle EE with machine reading comprehension (MRC) techniques. The last four models adopt PLMs to learn representations. On MAVEN, we compare CLEVE with the official ED baselines set by Wang et al. (2020), including DMCNN (Chen et al., 2015), BiLSTM (Hochreiter and Schmidhuber, 1997), BiLSTM+CRF, MOGANED (Yan et al., 2019), DMBERT (Wang et al., 2019a), BERT+CRF. Evaluation Results The evaluation results are shown in Table 1 and Table 2. We can observe that: (1) CLEVE achieves significant improvements to its basic model RoBERTa"
2021.acl-long.491,N16-1049,0,0.0182403,"16; Getman et al., 2017; Wang et al., 2020), these PLM-based works solely focus on better finetuning rather than pre-training for EE. In this paper, we study pre-training to better utilize rich event knowledge in large-scale unsupervised data. Event Schema Induction. Supervised EE models cannot generalize to continually-emerging new event types and argument roles. To this end, Chambers and Jurafsky (2011) explore to induce event schemata from raw text by unsupervised clustering. Following works introduce more features like coreference chains (Chambers, 2013) and entities (Nguyen et al., 2015; Sha et al., 2016). Recently, Huang and Ji (2020) move to the semi6284 Event Semantic Pre-training Unsupervised Corpora Trigger-Argument Pair Discrimination attack CNN&apos;s Kelly Wallace reports on today&apos;s attack in Netanya. Text Encoder The army said two soldiers were also among the dead. Trigger Replacement … Netanya reports CNN&apos;s Kelly Wallace today&apos;s reports Argument Replacement AMR Parsing Event Structure Pre-training Parsed AMR Graphs ARG0 ARG1 ARG1 attack-01 time ARG1 today dead ARG1 ARG1 soldier quant today Netanya say-01 army time mod 2 also Subgraph Sampling say-01 quant 2 AMR Subgraph Discrimination rep"
2021.acl-long.491,2020.emnlp-main.128,0,0.0362011,"005 training set instead of the AMR structures of NYT. Similarly, the on ACE (AMR) model is pre-trained with the parsed AMR structures of ACE 2005 training set. We also compare CLEVE with various baselines, including: (1) feature-based method, the top-performing JointBeam (Li et al., 2013); (2) vanilla neural model DMCNN (Chen et al., 2015); (3) the model incorporating syntactic knowledge, dbRNN (Sha et al., 2018); (4) stateof-the-art models on ED and EAE respectively, including GatedGCN (Lai et al., 2020) and SemSynGTN (Pouran Ben Veyseh et al., 2020); (5) a stateof-the-art EE model RCEE ER (Liu et al., 2020), which tackle EE with machine reading comprehension (MRC) techniques. The last four models adopt PLMs to learn representations. On MAVEN, we compare CLEVE with the official ED baselines set by Wang et al. (2020), including DMCNN (Chen et al., 2015), BiLSTM (Hochreiter and Schmidhuber, 1997), BiLSTM+CRF, MOGANED (Yan et al., 2019), DMBERT (Wang et al., 2019a), BERT+CRF. Evaluation Results The evaluation results are shown in Table 1 and Table 2. We can observe that: (1) CLEVE achieves significant improvements to its basic model RoBERTa on both ACE 2005 and MAVEN. The p-values under the t-test a"
2021.acl-long.491,2021.ccl-1.108,0,0.0684993,"Missing"
2021.acl-long.491,2020.acl-main.522,1,0.754155,"(Ji and Grishman, 2008; Gupta and Ji, 2009; Li et al., 2013) rely on manually-crafted features to extract events. In recent years, the neural models become mainstream, which automatically learn effective features with neural networks, including convolutional neural networks (Nguyen and Grishman, 2015; Chen et al., 2015), recurrent neural networks (Nguyen et al., 2016), graph convolutional networks (Nguyen and Grishman, 2018; Lai et al., 2020). With the recent successes of BERT (Devlin et al., 2019), PLMs have also been used for EE (Wang et al., 2019a,b; Yang et al., 2019; Wadden et al., 2019; Tong et al., 2020). Although achieving remarkable performance in benchmarks such as ACE 2005 (Walker et al., 2006) and similar datasets (Ellis et al., 2015, 2016; Getman et al., 2017; Wang et al., 2020), these PLM-based works solely focus on better finetuning rather than pre-training for EE. In this paper, we study pre-training to better utilize rich event knowledge in large-scale unsupervised data. Event Schema Induction. Supervised EE models cannot generalize to continually-emerging new event types and argument roles. To this end, Chambers and Jurafsky (2011) explore to induce event schemata from raw text by"
2021.acl-long.491,P15-1019,0,0.0450974,"Missing"
2021.acl-long.491,N19-1105,1,0.917634,"only contain thousands of instances and cover limited event types. Thus they are inadequate to train large neural models (Wang et al., 2020) and develop methods that can generalize to continually-emerging new event types (Huang and Ji, 2020). Inspired by the success of recent pre-trained language models (PLMs) for NLP tasks, some pio6283 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6283–6297 August 1–6, 2021. ©2021 Association for Computational Linguistics neering work (Wang et al., 2019a; Wadden et al., 2019) attempts to fine-tune general PLMs (e.g, BERT (Devlin et al., 2019)) for EE. Benefiting from the strong general language understanding ability learnt from large-scale unsupervised data, these PLM-based methods have achieved state-ofthe-art performance in various public benchmarks. Although leveraging unsupervised data with pretraining has gradually become a consensus for EE and NLP community, there still lacks a pre-training method orienting event modeling to take full advantage of rich event knowledge lying in largescale unsupervised data. The key challenge here is to"
2021.acl-long.491,2020.emnlp-main.129,1,0.921459,"ced neural networks (Chen et al., 2015; Nguyen et al., 2016; Nguyen and Grishman, 2018) with humanannotated datasets and pre-defined event schemata. These methods work well in lots of public benchmarks such as ACE 2005 (Walker et al., 2006) and TAC KBP (Ellis et al., 2016), yet they still suffer from data scarcity and limited generalizability. Since annotating event data and defining event schemata are especially expensive and laborintensive, existing EE datasets typically only contain thousands of instances and cover limited event types. Thus they are inadequate to train large neural models (Wang et al., 2020) and develop methods that can generalize to continually-emerging new event types (Huang and Ji, 2020). Inspired by the success of recent pre-trained language models (PLMs) for NLP tasks, some pio6283 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6283–6297 August 1–6, 2021. ©2021 Association for Computational Linguistics neering work (Wang et al., 2019a; Wadden et al., 2019) attempts to fine-tune general PLMs (e.g, BERT (Devlin et al., 2019)) for EE. Benefiting from the st"
2021.acl-long.491,D19-1584,1,0.881831,"only contain thousands of instances and cover limited event types. Thus they are inadequate to train large neural models (Wang et al., 2020) and develop methods that can generalize to continually-emerging new event types (Huang and Ji, 2020). Inspired by the success of recent pre-trained language models (PLMs) for NLP tasks, some pio6283 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6283–6297 August 1–6, 2021. ©2021 Association for Computational Linguistics neering work (Wang et al., 2019a; Wadden et al., 2019) attempts to fine-tune general PLMs (e.g, BERT (Devlin et al., 2019)) for EE. Benefiting from the strong general language understanding ability learnt from large-scale unsupervised data, these PLM-based methods have achieved state-ofthe-art performance in various public benchmarks. Although leveraging unsupervised data with pretraining has gradually become a consensus for EE and NLP community, there still lacks a pre-training method orienting event modeling to take full advantage of rich event knowledge lying in largescale unsupervised data. The key challenge here is to"
2021.acl-long.491,2020.emnlp-main.196,0,0.527338,"f two components: event semantic pre-training and event structure pre-training, of which details are introduced in Section 3.2 and Section 3.3, respectively. At the beginning of this section, we first introduce the required preprocessing in Section 3.1, including the AMR parsing and how we modify the parsed AMR structures for our pre-training. 3.1 Preprocessing CLEVE relies on AMR structures (Banarescu et al., 2013) to build broad and diverse self-supervision signals for learning event knowledge from largescale unsupervised corpora. To do this, we use automatic AMR parsers (Wang et al., 2015; Xu et al., 2020) to parse the sentences in unsupervised corpora into AMR structures. Each AMR structure is a directed acyclic graph with concepts as nodes and semantic relations as edges. Moreover, each node typically only corresponds to at most one word, and a multi-word entity will be represented as a list of nodes connected with name and op (conjunction operator) edges. Considering pretraining entity representations will naturally benefits event argument extraction, we merge these lists into single nodes representing multi-word entities (like the “CNN’s Kelly Wallace” in Figure 1) during both event semanti"
2021.acl-long.491,D19-1582,0,0.0155406,"al., 2015); (3) the model incorporating syntactic knowledge, dbRNN (Sha et al., 2018); (4) stateof-the-art models on ED and EAE respectively, including GatedGCN (Lai et al., 2020) and SemSynGTN (Pouran Ben Veyseh et al., 2020); (5) a stateof-the-art EE model RCEE ER (Liu et al., 2020), which tackle EE with machine reading comprehension (MRC) techniques. The last four models adopt PLMs to learn representations. On MAVEN, we compare CLEVE with the official ED baselines set by Wang et al. (2020), including DMCNN (Chen et al., 2015), BiLSTM (Hochreiter and Schmidhuber, 1997), BiLSTM+CRF, MOGANED (Yan et al., 2019), DMBERT (Wang et al., 2019a), BERT+CRF. Evaluation Results The evaluation results are shown in Table 1 and Table 2. We can observe that: (1) CLEVE achieves significant improvements to its basic model RoBERTa on both ACE 2005 and MAVEN. The p-values under the t-test are 4×10−8 , 2×10−8 and 6 × 10−4 for ED on ACE 2005, EAE on ACE 2005, and ED on MAVEN, respectively. It also outperforms or achieves comparable results with 6288 ED Metric (B-Cubed) P R EAE F1 P R ED F1 LiberalEE 55.7 45.1 49.8 36.2 26.5 30.6 RoBERTa RoBERTa+VGAE 44.3 24.9 31.9 24.2 17.3 20.2 47.0 26.8 34.1 25.6 17.9 21.1 CLEVE w/o"
2021.emnlp-main.366,P19-1279,0,0.0401504,"Missing"
2021.emnlp-main.366,P17-1171,0,0.0267983,"The second challenge is that RE models need to synthesize all information in multiple text paths to obtain the final relation. Open Setting. This setting fully tests the ability of RE in the wild. Given a target entity pair, models need to first retrieve relevant documents for the entity pair from full English Wikipedia corpus (5, 882, 234 documents in total, 3, 646 reasoning text path candidates for each entity pair on average), then perform cross-document reasoning with the retrieved documents to predict the relation. Compared with natural language queries in open domain question answering (Chen et al., 2017), the sparse query information in entity pairs presents unique challenges to document retrieval ability. The second challenge comes from both the quadratic number of potential paths (efficiency), and the finegrained influence of document retrieval on the extraction of relations (effectiveness). 4 Data Analysis In this section, we present data analysis of CodRED, including data statistics, required abilities in our dataset, and cross-document relation instances. Data Statistics. CodRED enjoys diversity in open 4455 Here we use entity names to predict the relations, since we find it can effectiv"
2021.emnlp-main.366,N19-1423,0,0.0353152,"Missing"
2021.emnlp-main.366,Q17-1008,0,0.0179577,"g on shallow correlation between relations and entity names. In this sense, CodRED provides a more reasonable benchmark for knowledge acquisition systems. End-to-end Ent. Ctx. X AUC F1 P@500 10.46 21.19 21.70 X X X 12.72 17.45 25.46 30.54 25.40 30.60 X X X 41.76 47.94 47.33 51.26 58.60 62.80 Table 7: Ablation results on entity names (Ent.) and context (Ctx.). Han et al., 2018; Mesquita et al., 2019) or distant supervision (Riedel et al., 2010; Zhang et al., 2017; Elsahar et al., 2018). (2) Cross-sentence RE datasets focus on extracting cross-sentence relations from documents (Li et al., 2016; Peng et al., 2017; Quirk and Poon, 2017; Yao et al., 2019) or dialogues (Yu et al., 2020). Notably, NIST TAC SM-KBP 2019 Track6 aims to extract and link document-level KBs from different languages and modalities. However, these datasets are still limited at sentence-level or document-level without considering cross-document reasoning, which restricts the coverage of knowledge acquisition. Hence, we extend RE to cross-document level, and construct a large-scale human-annotated dataset CodRED to facilitate further research. Cross-document natural language understanding has received increasing interest in recent"
2021.emnlp-main.366,2020.acl-main.444,0,0.0784142,"s of CodRED with existing RE datasets in Ta- several strategies to retrieve the relevant documents ble 2, including (1) sentence-level RE datasets TA- and connect them into text paths. Specifically, we CRED (Zhang et al., 2017), FewRel (Han et al., enumerate all possible text paths between the tar2018) and KnowledgeNet (Mesquita et al., 2019), get entity pairs (i.e., two documents that contain and (2) document-level RE datasets BC5CDR (Li h and t respectively with shared entities) as candiet al., 2016), DocRED (Yao et al., 2019) and Di- dates. We first present a random baseline, where alogRE (Yu et al., 2020). Compared with existing the candidate paths are randomly sampled. We also RE datasets that mainly focus on extracting rela- experiment with several heuristic retrieval stratetions from local contexts, i.e., single sentences or gies, where text paths are ranked by the heuristic documents, CodRED presents unique challenges in scores. Specifically, the score of a text path (dh , dt ) document retrieval and cross-document reasoning. is given by: (1) entity count: multiplication of the Intra- and Cross-Document Reasoning. Cross- occurrence number of h in dh and the occurrence number of t in dt , ("
2021.emnlp-main.366,P18-1199,0,0.0127727,", which aims to extract which Amun-her-khepeshef and Merneptah do not relations between entities from plain text, serves co-appear in a single document. To identify their as an essential resource in populating knowledge bases (KBs) from large-scale corpora automati- relation, we need to first retrieve the relevant documents for each entity and then recognize two reacally. Existing RE systems typically focus on either sentence-level RE (Socher et al., 2012; Zeng et al., soning text paths in these documents. The first reasoning text path (the documents titled “Nefer2014, 2015; Lin et al., 2016; Qin et al., 2018) or document-level RE (Li et al., 2016; Peng et al., tari” and “Memeptah”) shows that both target entities are the son of Ramesses II, and the second 2017; Quirk and Poon, 2017; Yao et al., 2019), and have achieved promising results on several pub- one indicates that they also share a common sister lic benchmarks. However, these works can only Meritamen. The information of these two reasonextract relational facts from single sentences or doc- ing text paths is complementary to each other and suggests the relation between Amun-her-khepeshef uments containing both two target entities, which inev"
2021.emnlp-main.366,E17-1110,0,0.164516,"an essential resource in populating knowledge bases (KBs) from large-scale corpora automati- relation, we need to first retrieve the relevant documents for each entity and then recognize two reacally. Existing RE systems typically focus on either sentence-level RE (Socher et al., 2012; Zeng et al., soning text paths in these documents. The first reasoning text path (the documents titled “Nefer2014, 2015; Lin et al., 2016; Qin et al., 2018) or document-level RE (Li et al., 2016; Peng et al., tari” and “Memeptah”) shows that both target entities are the son of Ramesses II, and the second 2017; Quirk and Poon, 2017; Yao et al., 2019), and have achieved promising results on several pub- one indicates that they also share a common sister lic benchmarks. However, these works can only Meritamen. The information of these two reasonextract relational facts from single sentences or doc- ing text paths is complementary to each other and suggests the relation between Amun-her-khepeshef uments containing both two target entities, which inevitably limits the coverage of knowledge acqui- and Merneptah is sibling. sition. According to our statistics on Wikipedia Although several datasets have been proposed ∗ for inv"
2021.emnlp-main.366,D15-1203,0,0.0180787,"all text paths. gated representation x. The aggregated entity pair representation x is then fed into a fully connected layer followed by a softmax layer to obtain the distribution of the relation between the entity pair. Besides the entity-level supervision, we also incorporate path-level supervision using an auxiliary classification task, where models need to predict the relation expressed in each path based on pi . 6 Experiments In this section, we assess the challenges of CodRED in both closed and open benchmark settings. 6.1 Evaluation Metrics In closed setting, following previous works (Zeng et al., 2015; Lin et al., 2016), we evaluate our model using aggregate precision-recall curves, and report the area under curve (AUC), the maximum F1 on the curve and Precision@K (P@K). In open setting, we first retrieve relevant documents (top 16 paths) from full Wikipedia corpus, and then use the models trained in the closed setting to infer the relation between the entity pair. We report the mean average precision (MAP), Recall@K (R@K) and mean reciprocal rank (MRR) to show the performance of document retrieval. 5.2.2 End-to-end Model 6.2 Overall Results Despite their simplicity, pipeline models usuall"
2021.emnlp-main.366,C14-1220,0,0.0791534,"Missing"
2021.emnlp-main.366,D17-1004,0,0.157455,"words, presenting challenges for model- a document set D (i.e., full Wikipedia corpus), we ing long text in both efficiency and effectiveness. first find relevant documents to extract their relaWe refer readers to the appendix for more details. tion. Due to the large number of possible docuRequired Abilities. We compare required abili- ments containing h and t respectively, we explore ties of CodRED with existing RE datasets in Ta- several strategies to retrieve the relevant documents ble 2, including (1) sentence-level RE datasets TA- and connect them into text paths. Specifically, we CRED (Zhang et al., 2017), FewRel (Han et al., enumerate all possible text paths between the tar2018) and KnowledgeNet (Mesquita et al., 2019), get entity pairs (i.e., two documents that contain and (2) document-level RE datasets BC5CDR (Li h and t respectively with shared entities) as candiet al., 2016), DocRED (Yao et al., 2019) and Di- dates. We first present a random baseline, where alogRE (Yu et al., 2020). Compared with existing the candidate paths are randomly sampled. We also RE datasets that mainly focus on extracting rela- experiment with several heuristic retrieval stratetions from local contexts, i.e., sin"
2021.emnlp-main.366,D12-1110,0,0.0561042,"re of each phase, but also 1 Introduction the intrinsic inter-dependence among the phases. Fig. 1 shows an example for cross-doc RE, in Relation extraction (RE), which aims to extract which Amun-her-khepeshef and Merneptah do not relations between entities from plain text, serves co-appear in a single document. To identify their as an essential resource in populating knowledge bases (KBs) from large-scale corpora automati- relation, we need to first retrieve the relevant documents for each entity and then recognize two reacally. Existing RE systems typically focus on either sentence-level RE (Socher et al., 2012; Zeng et al., soning text paths in these documents. The first reasoning text path (the documents titled “Nefer2014, 2015; Lin et al., 2016; Qin et al., 2018) or document-level RE (Li et al., 2016; Peng et al., tari” and “Memeptah”) shows that both target entities are the son of Ramesses II, and the second 2017; Quirk and Poon, 2017; Yao et al., 2019), and have achieved promising results on several pub- one indicates that they also share a common sister lic benchmarks. However, these works can only Meritamen. The information of these two reasonextract relational facts from single sentences or"
2021.emnlp-main.366,Q18-1021,0,0.0152335,"aims to extract and link document-level KBs from different languages and modalities. However, these datasets are still limited at sentence-level or document-level without considering cross-document reasoning, which restricts the coverage of knowledge acquisition. Hence, we extend RE to cross-document level, and construct a large-scale human-annotated dataset CodRED to facilitate further research. Cross-document natural language understanding has received increasing interest in recent years. Several datasets have been constructed including cross-document question answering (Yang et al., 2018; Welbl et al., 2018) and cross-document summarization (Over and Yen, 2004; Owczarzak and Dang, 2011; Fabbri et al., 2019). In comparison with existing datasets, our dataset is tailored for the task of RE with fine-grained path and evidence annotations, and investigates the more open and challenging scenario of knowledge acquisition. 8 Conclusion A variety of RE datasets have been constructed to promote the development of RE systems in recent years, which can be categorized in two main categories: (1) Sentence-level RE datasets focus on extracting relations on sentence-level, where the composing entities of a rela"
2021.emnlp-main.374,D18-1316,0,0.0347819,"Missing"
2021.emnlp-main.374,P19-1601,0,0.0365553,"Missing"
2021.emnlp-main.374,W18-5102,0,0.0571332,"Missing"
2021.emnlp-main.374,N19-1423,0,0.0481844,"Missing"
2021.emnlp-main.374,J93-3002,0,0.78388,"Missing"
2021.emnlp-main.374,P18-2006,0,0.0385854,"Missing"
2021.emnlp-main.374,N19-1165,0,0.0558301,"Missing"
2021.emnlp-main.374,N19-1320,0,0.0389048,"Missing"
2021.emnlp-main.374,N18-1170,0,0.0263816,"Missing"
2021.emnlp-main.374,D17-1215,0,0.055642,"Missing"
2021.emnlp-main.374,P19-1041,0,0.0352154,"Missing"
2021.emnlp-main.374,2020.emnlp-main.55,0,0.0776638,"Missing"
2021.emnlp-main.374,N18-1169,0,0.0609159,"Missing"
2021.emnlp-main.374,2020.emnlp-main.500,0,0.0267556,"Missing"
2021.emnlp-main.374,P18-1080,0,0.0532628,"Missing"
2021.emnlp-main.374,2021.acl-long.37,1,0.7899,"Missing"
2021.emnlp-main.374,2021.acl-long.377,1,0.769151,"Missing"
2021.emnlp-main.374,2020.acl-main.249,0,0.0691969,"Missing"
2021.emnlp-main.374,N18-1012,0,0.0600824,"Missing"
2021.emnlp-main.374,W16-5603,0,0.060911,"Missing"
2021.emnlp-main.374,D19-1410,0,0.0412958,"Missing"
2021.emnlp-main.374,P19-1103,0,0.0413712,"Missing"
2021.emnlp-main.374,D13-1170,0,0.00581873,"Missing"
2021.emnlp-main.374,D19-1322,0,0.0275882,"Missing"
2021.emnlp-main.374,2020.emnlp-main.495,0,0.0673248,"Missing"
2021.emnlp-main.374,2020.acl-main.540,1,0.841864,"Missing"
2021.emnlp-main.374,N18-1138,0,0.0603791,"Missing"
2021.emnlp-main.374,2020.emnlp-main.417,0,0.0611894,"Missing"
2021.emnlp-main.700,N18-1165,0,0.0193961,"g triple completion. Most of We explore two methods to give each rule an the existing multi-hop reasoning models are based interpretability score, namely manual annotation on the reinforcement learning (RL) framework. and automatic generation by rule mining methods. Among them, DeepPath (Xiong et al., 2017) is the The former is the focus of this paper. Specifically, first work to formally propose and solve the task we invite annotators to manually annotate inter- of multi-hop reasoning using RL, which inspires pretability scores for all possible rules to establish much later work, e.g., DIVA (Chen et al., 2018), a manually-annotated benchmark (A-benchmark). and AttnPath (Wang et al., 2019). MINERVA (Das This labeling process also faces a challenge, i.e., et al., 2018) is an end-to-end model with a wide interpretability is highly subjective and hard to an- impact that solves multi-hop reasoning task. On notate. Different annotators may give various ex- the basis of this model, M-Walk (Shen et al., 2018) planations. To reduce the variations, we provide and MultiHop (Lin et al., 2018) solve the problem the annotators with a number of interpretable op- of reward sparsity through off-policy learning and"
2021.emnlp-main.700,D19-1269,0,0.0166075,"re some other models such as the DIVINE (Li annotate and take their average score as the final re- and Cheng, 2019), R2D2 (Hildebrandt et al., 2020), sult. In addition to A-benchmark, we also provide RLH (Wan et al., 2020) and RuleGuider (Lei et al., 8900 Dataset # Entity # Relation # Triple WD15K 15,817 182 176,524 Table 1: Statistics of WD15K. The three columns denote the number of entities, relations and triples, respectively. 2020) models that improve multi-hop reasoning from the four directions of imitation learning, debate dynamics, hierarchical RL, and rule guidance, respectively. CPL (Fu et al., 2019) and DacKGR (Lv et al., 2020) enhance the effect of models by adding additional triples to KG. 2.2 Rule-based Reasoning Similar to multi-hop reasoning, rule-based reasoning can also perform interpretable triple completion, except that they give the corresponding rules instead of specific paths. Rule-based reasoning can be divided into two categories, namely, neuralbased models and rule mining models. Among them, neural-based models (Yang et al., 2017; Rocktäschel and Riedel, 2017; Sadeghian et al., 2019; Minervini et al., 2020) give the corresponding rules while performing triple completion, w"
2021.emnlp-main.700,2020.emnlp-main.10,0,0.0424178,"while rule mining models (Galárraga et al., 2015; Omran et al., 2018; Ho et al., 2018; Meilicke et al., 2019) first mine the rules and then use them for completion. 2.3 Interpretability Evaluation Few research work targets interpretability evaluation, although they admit the importance. Most multi-hop reasoning models rely on case study (Hildebrandt et al., 2020; Wan et al., 2020) to present the interpretability quality, while RuleGuider(Lei et al., 2020) samples tests and computes their differences for evaluation. There are some works in other areas (Gilpin et al., 2018; Yang and Kim, 2019; Jhamtani and Clark, 2020) to test interpretability, but they cannot be directly applied to multi-hop reasoning tasks for knowledge graphs. 3 Preliminary predict the correct tail entity t, but also give a path (h, r, t) ← (h, r1 , e1 ) ∧ (e1 , r2 , e2 ) ∧ · · · ∧ (en−1 , rn , t) as an explanation. Rule-based reasoning can be considered as generalized multi-hop reasoning and can also be evaluated on our benchmark. Given a triple query (h, r, ?), it needs to predict the tail entity t and give some Horn rules with confidence as an explanation, where the rule f is of the following form: r(X, Y ) ← r1 (X, A1 ) ∧ · · · ∧ rn"
2021.emnlp-main.700,2020.emnlp-main.688,0,0.0645641,"et al., 2017; Rocktäschel and Riedel, 2017; Sadeghian et al., 2019; Minervini et al., 2020) give the corresponding rules while performing triple completion, while rule mining models (Galárraga et al., 2015; Omran et al., 2018; Ho et al., 2018; Meilicke et al., 2019) first mine the rules and then use them for completion. 2.3 Interpretability Evaluation Few research work targets interpretability evaluation, although they admit the importance. Most multi-hop reasoning models rely on case study (Hildebrandt et al., 2020; Wan et al., 2020) to present the interpretability quality, while RuleGuider(Lei et al., 2020) samples tests and computes their differences for evaluation. There are some works in other areas (Gilpin et al., 2018; Yang and Kim, 2019; Jhamtani and Clark, 2020) to test interpretability, but they cannot be directly applied to multi-hop reasoning tasks for knowledge graphs. 3 Preliminary predict the correct tail entity t, but also give a path (h, r, t) ← (h, r1 , e1 ) ∧ (e1 , r2 , e2 ) ∧ · · · ∧ (en−1 , rn , t) as an explanation. Rule-based reasoning can be considered as generalized multi-hop reasoning and can also be evaluated on our benchmark. Given a triple query (h, r, ?), it needs to"
2021.emnlp-main.700,D19-1266,0,0.0250642,"h the highest and lowest interpretability scores. Besides, we also analyze the relation between interpretability scores and confidence scores. Due to space constraints, we put these contents in Appendix B. 5 5.1 Experiments Experimental Setup Models. We choose two types of multi-hop reasoning models and rule-based reasoning models to evaluate their interpretability. For multi-hop reasoning, we use the following five models: MINERVA where c(f ) is the confidence score of rule f . We (Das et al., 2018), MultiHop (Lin et al., 2018), DIcan regard it as a three-classification task, where the VINE (Li and Cheng, 2019), R2D2 (Hildebrandt type of prediction is Type(f ), and the golden type et al., 2020) and RuleGuider (Lei et al., 2020). For is the annotation result. We use Micro-F1 score rule-based reasoning, we evaluate the interpretabilto find the best h1 and h2 , i.e., we search the best h1 and h2 that can get the highest Micro-F1 score. ity on the following four models: AMIE+ (Galárraga et al., 2015), NeuralLP (Yang et al., 2017), Finally, for every rules f ∈ F, if f ∈ / F ∗ , S(f ) = RuLES (Ho et al., 2018) and AnyBURL (Meilicke 0. Otherwise, we can obtain Type(f ) according et al., 2019). We choose th"
2021.emnlp-main.700,D17-1060,0,0.0265611,"isting multi-hop reasoning models in terms of performance and interpretability, which points us to a possible future research direction, i.e., how to better incorporate rules into multi-hop reasoning. 2 2.1 Related Work Multi-Hop Reasoning Multi-hop reasoning models can give interpretable paths while performing triple completion. Most of We explore two methods to give each rule an the existing multi-hop reasoning models are based interpretability score, namely manual annotation on the reinforcement learning (RL) framework. and automatic generation by rule mining methods. Among them, DeepPath (Xiong et al., 2017) is the The former is the focus of this paper. Specifically, first work to formally propose and solve the task we invite annotators to manually annotate inter- of multi-hop reasoning using RL, which inspires pretability scores for all possible rules to establish much later work, e.g., DIVA (Chen et al., 2018), a manually-annotated benchmark (A-benchmark). and AttnPath (Wang et al., 2019). MINERVA (Das This labeling process also faces a challenge, i.e., et al., 2018) is an end-to-end model with a wide interpretability is highly subjective and hard to an- impact that solves multi-hop reasoning t"
2021.emnlp-main.752,2021.acl-long.37,1,0.815416,"Missing"
2021.emnlp-main.752,2021.acl-long.377,1,0.831006,"Missing"
2021.emnlp-main.752,D13-1170,0,0.00571466,"Missing"
2021.emnlp-main.752,N19-1144,0,0.0601721,"Missing"
2021.findings-acl.112,W97-1002,0,0.455921,"oroughly study previous DS-RE methods using both held-out and human-labeled test sets, and find that human-labeled data can reveal inconsistent results compared to the held-out ones. • We discuss some novel and important observations revealed by manual evaluation, especially with respect to pre-trained models, which calls for more research in these directions. 2 Related Work Relation extraction is an important NLP task and has gone through significant development in the past decades. In the early days, RE models mainly take statistical approaches, such as pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), graphical methods (Roth and Yih, 2002), etc. With the increasing computing power and the development of deep learning, neural RE methods have shown a great success (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015; Zhang et al., 2017). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated"
2021.findings-acl.112,P17-1171,0,0.0181524,"ssumption that one sentence mentioning two entities is likely to express the relational facts between the two entities from KGs. As DS can bring hundreds of thousands of autolabeled training instances for RE without any human labor, DS-RE has been widely explored in the past few years (Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Feng et al., 2018; Vashishth et al., 2018) and has also been widely extended to other related domains, such as biomedical information extraction (Peng et al., 2017; Quirk and Poon, 2017) and question answering (Bordes et al., 2015; Chen et al., 2017). Although DS-RE has achieved great success, we identify one severe problem for the current DSRE research—its evaluation. Existing works usually take two kinds of evaluation methods following Mintz et al. (2009): held-out evaluation, which directly uses DS-generated test data to approximate the trend of model performance, and human evaluation, which manually checks the most confident relational facts predicted by DS-RE models. Since manually checking is costly, most works with human evaluation only examine a small proportion of the predictions. Moreover, different works may sample different sp"
2021.findings-acl.112,2020.bionlp-1.20,0,0.0132577,"rvations that have not been clearly demonstrated with the DS evaluation: Pre-trained Models First of all, BERT-based models have achieved supreme performance across all three metrics. To thoroughly examine BERT and its variants in the DS-RE scenario, we further plot their P-R curves with the bag-level manual test in Figure 4. It is surprising to see that all bag-level training strategies, especially the ATT strategy which brings significant improvements for PCNN-based models, do not help or even degenerate the performance with pre-trained ones. This observation is also consistent with that in Amin et al. (2020), though they only compare BERT+bag+AVG and BERT+bag+ATT. We hypothesize the reasons are that solely using pre-trained models already makes a strong baseline, since they exploit more parameters and they have gained pre-encoded knowledge from pretraining (Petroni et al., 2019), all of which make them easier to directly capture relational patterns from noisy data; and bag-level training, which essentially increases the batch size, may raise the optimization difficulty for these large models. Another unexpected observation is that, though the P-R curve of BERT is far above other models in the hel"
2021.findings-acl.112,P19-1279,0,0.0111756,"pment in the past decades. In the early days, RE models mainly take statistical approaches, such as pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), graphical methods (Roth and Yih, 2002), etc. With the increasing computing power and the development of deep learning, neural RE methods have shown a great success (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015; Zhang et al., 2017). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated data. To generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at"
2021.findings-acl.112,D18-1247,1,0.809527,"annotated data. To generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informative sentences from the noisy dataset, using heuristics (Zeng et al., 2015), attention mechanisms (Lin et al., 2016; Han et al., 2018c; Zhu et al., 2019), adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a), and reinforcement learning (Feng et al., 2018; Qin et al., 2018); the other incorporates external information like KGs (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Hu et al., 2019), multilingual corpora (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018), as well as relation ontology and aliases (Vashishth et al., 2018). Recently, pretrained DS-RE models have also been explored, including both domain-general (Alt et al., 2019; Xiao et al., 2020) and domain-specific (Amin et al"
2021.findings-acl.112,P11-1055,0,0.0641729,"cale auto-labeled data by aligning relational facts in knowledge graphs (KGs) to text corpora, with the * Corresponding author e-mail: liuzy@tsinghua.edu.cn Our code and data are publicly available at https:// github.com/thunlp/opennre. 1 CEO Shareholder Place of birth Capital Founder core assumption that one sentence mentioning two entities is likely to express the relational facts between the two entities from KGs. As DS can bring hundreds of thousands of autolabeled training instances for RE without any human labor, DS-RE has been widely explored in the past few years (Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Feng et al., 2018; Vashishth et al., 2018) and has also been widely extended to other related domains, such as biomedical information extraction (Peng et al., 2017; Quirk and Poon, 2017) and question answering (Bordes et al., 2015; Chen et al., 2017). Although DS-RE has achieved great success, we identify one severe problem for the current DSRE research—its evaluation. Existing works usually take two kinds of evaluation methods following Mintz et al. (2009): held-out evaluation, which directly uses DS-generated test data to approximate the trend of model"
2021.findings-acl.112,D19-1395,0,0.015522,"nn et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informative sentences from the noisy dataset, using heuristics (Zeng et al., 2015), attention mechanisms (Lin et al., 2016; Han et al., 2018c; Zhu et al., 2019), adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a), and reinforcement learning (Feng et al., 2018; Qin et al., 2018); the other incorporates external information like KGs (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Hu et al., 2019), multilingual corpora (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018), as well as relation ontology and aliases (Vashishth et al., 2018). Recently, pretrained DS-RE models have also been explored, including both domain-general (Alt et al., 2019; Xiao et al., 2020) and domain-specific (Amin et al., 2020) models. Some other latest works (Peng et al., 2020) utilize DS data for intermediate pre-training in order to boost supervised RE tasks. As mentioned in our introduction, the evalua1307 #facts Train #sents N/A #facts Validation #sents N/A #facts Test #sents N/A 53 25 18,409 17,137 52"
2021.findings-acl.112,D19-1250,0,0.0233314,"Missing"
2021.findings-acl.112,P18-1199,0,0.0588086,"eir relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informative sentences from the noisy dataset, using heuristics (Zeng et al., 2015), attention mechanisms (Lin et al., 2016; Han et al., 2018c; Zhu et al., 2019), adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a), and reinforcement learning (Feng et al., 2018; Qin et al., 2018); the other incorporates external information like KGs (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Hu et al., 2019), multilingual corpora (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018), as well as relation ontology and aliases (Vashishth et al., 2018). Recently, pretrained DS-RE models have also been explored, including both domain-general (Alt et al., 2019; Xiao et al., 2020) and domain-specific (Amin et al., 2020) models. Some other latest works (Peng et al., 2020) utilize DS data for intermediate pre-training in order to boost supervised RE tasks. As mentioned in ou"
2021.findings-acl.112,L18-1566,0,0.0357812,"Missing"
2021.findings-acl.112,E17-1110,0,0.0117008,"lp/opennre. 1 CEO Shareholder Place of birth Capital Founder core assumption that one sentence mentioning two entities is likely to express the relational facts between the two entities from KGs. As DS can bring hundreds of thousands of autolabeled training instances for RE without any human labor, DS-RE has been widely explored in the past few years (Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Feng et al., 2018; Vashishth et al., 2018) and has also been widely extended to other related domains, such as biomedical information extraction (Peng et al., 2017; Quirk and Poon, 2017) and question answering (Bordes et al., 2015; Chen et al., 2017). Although DS-RE has achieved great success, we identify one severe problem for the current DSRE research—its evaluation. Existing works usually take two kinds of evaluation methods following Mintz et al. (2009): held-out evaluation, which directly uses DS-generated test data to approximate the trend of model performance, and human evaluation, which manually checks the most confident relational facts predicted by DS-RE models. Since manually checking is costly, most works with human evaluation only examine a small proportion of th"
2021.findings-acl.112,N13-1008,0,0.0117331,"e DS relations or no relation at all, while we find that a large proportion of held-out data actually express some other relations; Li et al. (2020) propose active testing, an iterative method to correct the bias of DS evaluation. However, it still requires consistent human efforts during each evaluation phase. To the best of our knowledge, our work, building benchmarks with large-scale manuallylabeled test data, conducts the most comprehensive human evaluations of DS-RE methods so far. 3 DS-RE Datasets In this section, we introduce the way we build the manually-annotated test sets for NYT10 (Riedel et al., 2013) and Wiki20 (Han et al., 2020). We show the statistics of these datasets in Table 1. 3.1 NYT10 Dataset NYT10 is constructed by aligning facts from the FreeBase (Bollacker et al., 2008) with the New York Times (NYT) corpus (Sandhaus, 2008). The original NYT10 dataset contains 53 relations (including N/A). After thoroughly examining the dataset, we found that (1) there are many duplicate instances in the dataset, (2) there is no public validation set, and some previous works directly take the test set to tune the model, and (3) the relation ontology is not reasonable for an RE task. Therefore, w"
2021.findings-acl.112,C02-1151,0,0.0835297,"n-labeled data can reveal inconsistent results compared to the held-out ones. • We discuss some novel and important observations revealed by manual evaluation, especially with respect to pre-trained models, which calls for more research in these directions. 2 Related Work Relation extraction is an important NLP task and has gone through significant development in the past decades. In the early days, RE models mainly take statistical approaches, such as pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), graphical methods (Roth and Yih, 2002), etc. With the increasing computing power and the development of deep learning, neural RE methods have shown a great success (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015; Zhang et al., 2017). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated data. To generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use"
2021.findings-acl.112,2020.findings-emnlp.20,0,0.0351636,"manual test. tion of DS-RE has long been a problem, especially since many existing methods solely rely on autolabeled test data. Some preliminaries have noticed this problem: Jiang et al. (2018); Zhu et al. (2020) also annotate the test set of NYT10, yet Jiang et al. (2018) only sample 2, 040 sentences from it, and Zhu et al. (2020) discard all N/A data from DS, which are an important part of DS evaluation, and assume that the original held-out data have either the DS relations or no relation at all, while we find that a large proportion of held-out data actually express some other relations; Li et al. (2020) propose active testing, an iterative method to correct the bias of DS evaluation. However, it still requires consistent human efforts during each evaluation phase. To the best of our knowledge, our work, building benchmarks with large-scale manuallylabeled test data, conducts the most comprehensive human evaluations of DS-RE methods so far. 3 DS-RE Datasets In this section, we introduce the way we build the manually-annotated test sets for NYT10 (Riedel et al., 2013) and Wiki20 (Han et al., 2020). We show the statistics of these datasets in Table 1. 3.1 NYT10 Dataset NYT10 is constructed by a"
2021.findings-acl.112,P17-1004,1,0.862832,"Missing"
2021.findings-acl.112,D12-1042,0,0.0264152,"17). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated data. To generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informative sentences from the noisy dataset, using heuristics (Zeng et al., 2015), attention mechanisms (Lin et al., 2016; Han et al., 2018c; Zhu et al., 2019), adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a), and reinforcement learning (Feng et al., 2018; Qin et al., 2018); the other incorporates external information like KGs (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Hu et al., 2019), multilingual corpora ("
2021.findings-acl.112,P16-1200,1,0.914386,"nal facts in knowledge graphs (KGs) to text corpora, with the * Corresponding author e-mail: liuzy@tsinghua.edu.cn Our code and data are publicly available at https:// github.com/thunlp/opennre. 1 CEO Shareholder Place of birth Capital Founder core assumption that one sentence mentioning two entities is likely to express the relational facts between the two entities from KGs. As DS can bring hundreds of thousands of autolabeled training instances for RE without any human labor, DS-RE has been widely explored in the past few years (Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Feng et al., 2018; Vashishth et al., 2018) and has also been widely extended to other related domains, such as biomedical information extraction (Peng et al., 2017; Quirk and Poon, 2017) and question answering (Bordes et al., 2015; Chen et al., 2017). Although DS-RE has achieved great success, we identify one severe problem for the current DSRE research—its evaluation. Existing works usually take two kinds of evaluation methods following Mintz et al. (2009): held-out evaluation, which directly uses DS-generated test data to approximate the trend of model performance, and human evaluation, wh"
2021.findings-acl.112,P09-1113,0,0.549708,"and observations can help advance future DS-RE research.1 1 Musk owns 28.9M Tesla shares. Tesla Inc. Elon Musk Figure 1: Typical errors made by DS evaluation. In the figure, DS labels the bag with only the relation CEO, while none of the sentences express the relation. Also, it misses a correct relation shareholder due to the incompleteness of the knowledge graphs. Introduction Relation extraction (RE) aims at extracting relational facts between entities from the text. One crucial challenge for building an effective RE system is how to obtain sufficient annotated data. To tackle this problem, Mintz et al. (2009) propose distant supervision (DS) to generate large-scale auto-labeled data by aligning relational facts in knowledge graphs (KGs) to text corpora, with the * Corresponding author e-mail: liuzy@tsinghua.edu.cn Our code and data are publicly available at https:// github.com/thunlp/opennre. 1 CEO Shareholder Place of birth Capital Founder core assumption that one sentence mentioning two entities is likely to express the relational facts between the two entities from KGs. As DS can bring hundreds of thousands of autolabeled training instances for RE without any human labor, DS-RE has been widely"
2021.findings-acl.112,2020.emnlp-main.298,1,0.820774,"ginal bag-level training, we carry out a pilot experiment to examine the effect of the sampled training. From Table 3, we can see that our sampling strategy does not significantly hurt the performance of the bag-level training. We also add another variant, BERT-M, in our evaluation. We observe from the top predictions of BERT models (Figure 3) that BERT tends to make false-positive errors for entity pairs that express a relation in the KG but do not have any sentence truly expressing the relation in the data, probably due to that model learns shallow cues solely from entities. Thus, following Peng et al. (2020), we mask entity mentions during training and inference to avoid learning biased heuristics from entities. 5 5.1 Experiment Implementation Details We use the OpenNRE toolkit (Han et al., 2019) for most of our experiments, including both sentencelevel and bag-level training. For CNN and PCNN, we follow the hyper-parameters of Han et al. (2019). For BERT, we use pre-trained checkpoint bert-base-uncased for initialization, take a batch size of 64, a bag size of 4 and a learning rate of 2 × 10−5 ,3 and train the model for 3 epochs. For RL-DSRE, RESIDE and BGWA, we directly use their original imple"
2021.findings-acl.112,Q17-1008,0,0.0120081,":// github.com/thunlp/opennre. 1 CEO Shareholder Place of birth Capital Founder core assumption that one sentence mentioning two entities is likely to express the relational facts between the two entities from KGs. As DS can bring hundreds of thousands of autolabeled training instances for RE without any human labor, DS-RE has been widely explored in the past few years (Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Feng et al., 2018; Vashishth et al., 2018) and has also been widely extended to other related domains, such as biomedical information extraction (Peng et al., 2017; Quirk and Poon, 2017) and question answering (Bordes et al., 2015; Chen et al., 2017). Although DS-RE has achieved great success, we identify one severe problem for the current DSRE research—its evaluation. Existing works usually take two kinds of evaluation methods following Mintz et al. (2009): held-out evaluation, which directly uses DS-generated test data to approximate the trend of model performance, and human evaluation, which manually checks the most confident relational facts predicted by DS-RE models. Since manually checking is costly, most works with human evaluation only examine a"
2021.findings-acl.112,D18-1157,0,0.024953,"Missing"
2021.findings-acl.112,N16-1103,0,0.0275135,"Missing"
2021.findings-acl.112,C18-1099,1,0.824993,"t, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informative sentences from the noisy dataset, using heuristics (Zeng et al., 2015), attention mechanisms (Lin et al., 2016; Han et al., 2018c; Zhu et al., 2019), adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a), and reinforcement learning (Feng et al., 2018; Qin et al., 2018); the other incorporates external information like KGs (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Hu et al., 2019), multilingual corpora (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018), as well as relation ontology and aliases (Vashishth et al., 2018). Recently, pretrained DS-RE models have also been explored, including both domain-general (Alt et al., 2019; Xiao et al., 2020) and domain-specific (Amin et al., 2020) models. Some other latest works (Peng et al., 2020) utilize DS data f"
2021.findings-acl.112,D17-1187,0,0.0175736,"thout human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informative sentences from the noisy dataset, using heuristics (Zeng et al., 2015), attention mechanisms (Lin et al., 2016; Han et al., 2018c; Zhu et al., 2019), adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a), and reinforcement learning (Feng et al., 2018; Qin et al., 2018); the other incorporates external information like KGs (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Hu et al., 2019), multilingual corpora (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018), as well as relation ontology and aliases (Vashishth et al., 2018). Recently, pretrained DS-RE models have also been explored, including both domain-general (Alt et al., 2019; Xiao et al., 2020) and domain-specific (Amin et al., 2020) models. Some other latest works (Peng et al., 2020"
2021.findings-acl.112,D15-1203,0,0.1272,"by aligning relational facts in knowledge graphs (KGs) to text corpora, with the * Corresponding author e-mail: liuzy@tsinghua.edu.cn Our code and data are publicly available at https:// github.com/thunlp/opennre. 1 CEO Shareholder Place of birth Capital Founder core assumption that one sentence mentioning two entities is likely to express the relational facts between the two entities from KGs. As DS can bring hundreds of thousands of autolabeled training instances for RE without any human labor, DS-RE has been widely explored in the past few years (Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Feng et al., 2018; Vashishth et al., 2018) and has also been widely extended to other related domains, such as biomedical information extraction (Peng et al., 2017; Quirk and Poon, 2017) and question answering (Bordes et al., 2015; Chen et al., 2017). Although DS-RE has achieved great success, we identify one severe problem for the current DSRE research—its evaluation. Existing works usually take two kinds of evaluation methods following Mintz et al. (2009): held-out evaluation, which directly uses DS-generated test data to approximate the trend of model performance, and hu"
2021.findings-acl.112,C14-1220,0,0.0174976,"cially with respect to pre-trained models, which calls for more research in these directions. 2 Related Work Relation extraction is an important NLP task and has gone through significant development in the past decades. In the early days, RE models mainly take statistical approaches, such as pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), graphical methods (Roth and Yih, 2002), etc. With the increasing computing power and the development of deep learning, neural RE methods have shown a great success (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015; Zhang et al., 2017). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated data. To generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel"
2021.findings-acl.112,N19-1306,0,0.0138632,"ades. In the early days, RE models mainly take statistical approaches, such as pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), graphical methods (Roth and Yih, 2002), etc. With the increasing computing power and the development of deep learning, neural RE methods have shown a great success (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015; Zhang et al., 2017). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated data. To generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informativ"
2021.findings-acl.112,D17-1004,0,0.0123124,", which calls for more research in these directions. 2 Related Work Relation extraction is an important NLP task and has gone through significant development in the past decades. In the early days, RE models mainly take statistical approaches, such as pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), graphical methods (Roth and Yih, 2002), etc. With the increasing computing power and the development of deep learning, neural RE methods have shown a great success (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015; Zhang et al., 2017). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated data. To generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Sur"
2021.findings-acl.112,P19-1139,1,0.799285,"ades. In the early days, RE models mainly take statistical approaches, such as pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), graphical methods (Roth and Yih, 2002), etc. With the increasing computing power and the development of deep learning, neural RE methods have shown a great success (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015; Zhang et al., 2017). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated data. To generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informativ"
2021.findings-acl.112,P05-1053,0,0.0494374,"n-labeled test sets, and find that human-labeled data can reveal inconsistent results compared to the held-out ones. • We discuss some novel and important observations revealed by manual evaluation, especially with respect to pre-trained models, which calls for more research in these directions. 2 Related Work Relation extraction is an important NLP task and has gone through significant development in the past decades. In the early days, RE models mainly take statistical approaches, such as pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), graphical methods (Roth and Yih, 2002), etc. With the increasing computing power and the development of deep learning, neural RE methods have shown a great success (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015; Zhang et al., 2017). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated data. To generate large-scale auto-labeled data without hum"
2021.findings-acl.112,P19-1128,1,0.843226,"generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informative sentences from the noisy dataset, using heuristics (Zeng et al., 2015), attention mechanisms (Lin et al., 2016; Han et al., 2018c; Zhu et al., 2019), adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a), and reinforcement learning (Feng et al., 2018; Qin et al., 2018); the other incorporates external information like KGs (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Hu et al., 2019), multilingual corpora (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018), as well as relation ontology and aliases (Vashishth et al., 2018). Recently, pretrained DS-RE models have also been explored, including both domain-general (Alt et al., 2019; Xiao et al., 2020) and domain-specific (Amin et al., 2020) models. Som"
2021.findings-acl.112,2020.coling-main.566,0,0.0927166,"Missing"
2021.findings-acl.137,N19-1423,0,0.0622221,"Missing"
2021.findings-acl.137,2020.emnlp-main.498,0,0.188847,"Missing"
2021.findings-acl.137,D19-1419,0,0.0442042,"Missing"
2021.findings-acl.137,D19-1423,0,0.13923,"Missing"
2021.findings-acl.137,P19-1561,0,0.0161835,"atus quo. However, recent works have shown the adversarial vulnerabilities of PLMs, where PLMs finetuned on various downstream datasets are fooled by different types † Perturbation Space Linear Interpolation Introduction ∗ Class B of adversarial attacks (Jin et al., 2020; Zang et al., 2020; Si et al., 2021; Li et al., 2020; Garg and Ramakrishnan, 2020; Wang et al., 2020a). To improve adversarial robustness, two types of defense strategies have been proposed. The first type targets at specific attacks, such as spelling correction modules and pretraining tasks to defend character-level attacks (Pruthi et al., 2019; Jones et al., 2020; Ma et al., 2020) and certified robustness for word-substitution attacks (Huang et al., 2019; Jia et al., 2019). However, they are limited in practice as they are not generally applicable to other types of attacks. The other type of defense is Adversarial Data Augmentation (ADA), which augments the training set by the adversarial examples and is widely used in the training (finetuning) process to enhance model robustness (Alzantot et al.; Ren et al., 2019; Zhang et al., 2020; Jin et al., 2020; 1569 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,"
2021.findings-acl.137,P19-1103,0,0.365226,"ts at specific attacks, such as spelling correction modules and pretraining tasks to defend character-level attacks (Pruthi et al., 2019; Jones et al., 2020; Ma et al., 2020) and certified robustness for word-substitution attacks (Huang et al., 2019; Jia et al., 2019). However, they are limited in practice as they are not generally applicable to other types of attacks. The other type of defense is Adversarial Data Augmentation (ADA), which augments the training set by the adversarial examples and is widely used in the training (finetuning) process to enhance model robustness (Alzantot et al.; Ren et al., 2019; Zhang et al., 2020; Jin et al., 2020; 1569 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1569–1576 August 1–6, 2021. ©2021 Association for Computational Linguistics Li et al., 2020; Tan et al., 2020; Yin et al., 2020; Zheng et al., 2020; Zou et al., 2020; Wang et al., 2020b). ADA is generally applicable to any type of adversarial attacks but is not very effective in improving model performance under attacks. In this work, we aim to improve ADA and devise a general defense strategy to effectively improve model robustness during finetuning.1 ADA has two majo"
2021.findings-acl.137,2021.findings-acl.56,1,0.33783,"sents the resultant decision boundary. AMDA helps achieve a more robust decision boundary. Pretrained language models (PLMs) have established state-of-the-art results on various NLP tasks (Devlin et al., 2019; Liu et al., 2019; Lan et al., 2020) and the pretraining-then-finetuning paradigm has become the status quo. However, recent works have shown the adversarial vulnerabilities of PLMs, where PLMs finetuned on various downstream datasets are fooled by different types † Perturbation Space Linear Interpolation Introduction ∗ Class B of adversarial attacks (Jin et al., 2020; Zang et al., 2020; Si et al., 2021; Li et al., 2020; Garg and Ramakrishnan, 2020; Wang et al., 2020a). To improve adversarial robustness, two types of defense strategies have been proposed. The first type targets at specific attacks, such as spelling correction modules and pretraining tasks to defend character-level attacks (Pruthi et al., 2019; Jones et al., 2020; Ma et al., 2020) and certified robustness for word-substitution attacks (Huang et al., 2019; Jia et al., 2019). However, they are limited in practice as they are not generally applicable to other types of attacks. The other type of defense is Adversarial Data Augmen"
2021.findings-acl.137,2020.acl-main.245,0,0.341757,"cent works have shown the adversarial vulnerabilities of PLMs, where PLMs finetuned on various downstream datasets are fooled by different types † Perturbation Space Linear Interpolation Introduction ∗ Class B of adversarial attacks (Jin et al., 2020; Zang et al., 2020; Si et al., 2021; Li et al., 2020; Garg and Ramakrishnan, 2020; Wang et al., 2020a). To improve adversarial robustness, two types of defense strategies have been proposed. The first type targets at specific attacks, such as spelling correction modules and pretraining tasks to defend character-level attacks (Pruthi et al., 2019; Jones et al., 2020; Ma et al., 2020) and certified robustness for word-substitution attacks (Huang et al., 2019; Jia et al., 2019). However, they are limited in practice as they are not generally applicable to other types of attacks. The other type of defense is Adversarial Data Augmentation (ADA), which augments the training set by the adversarial examples and is widely used in the training (finetuning) process to enhance model robustness (Alzantot et al.; Ren et al., 2019; Zhang et al., 2020; Jin et al., 2020; 1569 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1569–1576 Aug"
2021.findings-acl.137,D13-1170,0,0.048696,"Missing"
2021.findings-acl.137,2020.acl-main.263,0,0.275487,"et al., 2019; Jia et al., 2019). However, they are limited in practice as they are not generally applicable to other types of attacks. The other type of defense is Adversarial Data Augmentation (ADA), which augments the training set by the adversarial examples and is widely used in the training (finetuning) process to enhance model robustness (Alzantot et al.; Ren et al., 2019; Zhang et al., 2020; Jin et al., 2020; 1569 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1569–1576 August 1–6, 2021. ©2021 Association for Computational Linguistics Li et al., 2020; Tan et al., 2020; Yin et al., 2020; Zheng et al., 2020; Zou et al., 2020; Wang et al., 2020b). ADA is generally applicable to any type of adversarial attacks but is not very effective in improving model performance under attacks. In this work, we aim to improve ADA and devise a general defense strategy to effectively improve model robustness during finetuning.1 ADA has two major limitations for NLP models. Firstly, unlike images, it is harder to create new augmented textual data due to their discrete nature. Moreover, for textual adversarial attacks, the attack search space is prohibitively large. For example"
2021.findings-acl.137,2020.emnlp-main.500,0,0.399236,"nt decision boundary. AMDA helps achieve a more robust decision boundary. Pretrained language models (PLMs) have established state-of-the-art results on various NLP tasks (Devlin et al., 2019; Liu et al., 2019; Lan et al., 2020) and the pretraining-then-finetuning paradigm has become the status quo. However, recent works have shown the adversarial vulnerabilities of PLMs, where PLMs finetuned on various downstream datasets are fooled by different types † Perturbation Space Linear Interpolation Introduction ∗ Class B of adversarial attacks (Jin et al., 2020; Zang et al., 2020; Si et al., 2021; Li et al., 2020; Garg and Ramakrishnan, 2020; Wang et al., 2020a). To improve adversarial robustness, two types of defense strategies have been proposed. The first type targets at specific attacks, such as spelling correction modules and pretraining tasks to defend character-level attacks (Pruthi et al., 2019; Jones et al., 2020; Ma et al., 2020) and certified robustness for word-substitution attacks (Huang et al., 2019; Jia et al., 2019). However, they are limited in practice as they are not generally applicable to other types of attacks. The other type of defense is Adversarial Data Augmentation (ADA), whi"
2021.findings-acl.137,2020.emnlp-main.495,0,0.0772852,"e robust decision boundary. Pretrained language models (PLMs) have established state-of-the-art results on various NLP tasks (Devlin et al., 2019; Liu et al., 2019; Lan et al., 2020) and the pretraining-then-finetuning paradigm has become the status quo. However, recent works have shown the adversarial vulnerabilities of PLMs, where PLMs finetuned on various downstream datasets are fooled by different types † Perturbation Space Linear Interpolation Introduction ∗ Class B of adversarial attacks (Jin et al., 2020; Zang et al., 2020; Si et al., 2021; Li et al., 2020; Garg and Ramakrishnan, 2020; Wang et al., 2020a). To improve adversarial robustness, two types of defense strategies have been proposed. The first type targets at specific attacks, such as spelling correction modules and pretraining tasks to defend character-level attacks (Pruthi et al., 2019; Jones et al., 2020; Ma et al., 2020) and certified robustness for word-substitution attacks (Huang et al., 2019; Jia et al., 2019). However, they are limited in practice as they are not generally applicable to other types of attacks. The other type of defense is Adversarial Data Augmentation (ADA), which augments the training set by the adversarial"
2021.findings-acl.137,2020.emnlp-main.417,0,0.0523825,"Missing"
2021.findings-acl.137,2020.acl-main.310,0,0.0965234,"et al., 2019). However, they are limited in practice as they are not generally applicable to other types of attacks. The other type of defense is Adversarial Data Augmentation (ADA), which augments the training set by the adversarial examples and is widely used in the training (finetuning) process to enhance model robustness (Alzantot et al.; Ren et al., 2019; Zhang et al., 2020; Jin et al., 2020; 1569 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1569–1576 August 1–6, 2021. ©2021 Association for Computational Linguistics Li et al., 2020; Tan et al., 2020; Yin et al., 2020; Zheng et al., 2020; Zou et al., 2020; Wang et al., 2020b). ADA is generally applicable to any type of adversarial attacks but is not very effective in improving model performance under attacks. In this work, we aim to improve ADA and devise a general defense strategy to effectively improve model robustness during finetuning.1 ADA has two major limitations for NLP models. Firstly, unlike images, it is harder to create new augmented textual data due to their discrete nature. Moreover, for textual adversarial attacks, the attack search space is prohibitively large. For example, the search space"
2021.findings-acl.137,2020.coling-main.4,1,0.778555,"n the adversarial vulnerabilities of PLMs, where PLMs finetuned on various downstream datasets are fooled by different types † Perturbation Space Linear Interpolation Introduction ∗ Class B of adversarial attacks (Jin et al., 2020; Zang et al., 2020; Si et al., 2021; Li et al., 2020; Garg and Ramakrishnan, 2020; Wang et al., 2020a). To improve adversarial robustness, two types of defense strategies have been proposed. The first type targets at specific attacks, such as spelling correction modules and pretraining tasks to defend character-level attacks (Pruthi et al., 2019; Jones et al., 2020; Ma et al., 2020) and certified robustness for word-substitution attacks (Huang et al., 2019; Jia et al., 2019). However, they are limited in practice as they are not generally applicable to other types of attacks. The other type of defense is Adversarial Data Augmentation (ADA), which augments the training set by the adversarial examples and is widely used in the training (finetuning) process to enhance model robustness (Alzantot et al.; Ren et al., 2019; Zhang et al., 2020; Jin et al., 2020; 1569 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1569–1576 August 1–6, 2021. ©20"
2021.findings-acl.137,2020.acl-main.540,1,0.905788,"Missing"
2021.findings-acl.137,P11-1015,0,0.0143693,"Missing"
2021.findings-acl.137,2020.acl-main.590,0,0.0456194,"Missing"
2021.findings-acl.137,2020.acl-main.319,0,0.170701,"ted in practice as they are not generally applicable to other types of attacks. The other type of defense is Adversarial Data Augmentation (ADA), which augments the training set by the adversarial examples and is widely used in the training (finetuning) process to enhance model robustness (Alzantot et al.; Ren et al., 2019; Zhang et al., 2020; Jin et al., 2020; 1569 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1569–1576 August 1–6, 2021. ©2021 Association for Computational Linguistics Li et al., 2020; Tan et al., 2020; Yin et al., 2020; Zheng et al., 2020; Zou et al., 2020; Wang et al., 2020b). ADA is generally applicable to any type of adversarial attacks but is not very effective in improving model performance under attacks. In this work, we aim to improve ADA and devise a general defense strategy to effectively improve model robustness during finetuning.1 ADA has two major limitations for NLP models. Firstly, unlike images, it is harder to create new augmented textual data due to their discrete nature. Moreover, for textual adversarial attacks, the attack search space is prohibitively large. For example, the search space of word-substitution attacks consists"
2021.findings-acl.153,2020.emnlp-demos.22,0,0.0410419,"ould achieve higher scores on these metrics. We use the “Filtered” setting for all the evaluations, which filters out other true answers from the prediction results to get the final rank for each test case. 5.4 Hyperparameter Settings According to Ruffinelli et al. (2020), performances of KGE methods are sensitive to hyperparameters. Following them, we run 30 quasi-random trails for all models from predefined hyperparameter spaces. We list the hyperparameter spaces we use in Appendix A.5. We run all trails for 100 epochs. For all single-view KE methods, we use the implementations from LibKGE (Broscheit et al., 2020), which utilizes the Ax framework to perform quasi-random hyperparameter search. For AttH, we use the implementation from the authors1 . For JOIE, we use the implementation from the authors2 . We use TransE as the backend and adopt the suggested hyperparameter space from the paper. 6 Experimental Results In this section, we provide the experimental results and further propose several future directions. 6.1 Knowledge Abstraction The results of knowledge abstraction are shown in Table 4. From the results, we can see that AttH has 1 2 https://github.com/HazyResearch/KGEmb https://github.com/Junhe"
2021.findings-acl.153,2020.acl-main.617,0,0.0496769,"Missing"
2021.findings-acl.153,P15-1067,0,0.0372355,") and YAGO39K (Lv et al., 2018). However, they do not provide the full concept graphs with logical relations. Thirdly, some datasets provide the full concept graphs (Hao et al., 2019), but both the scale and the depth of the concept hierarchy are limited. For example, the entity numbers of DB111K-174 (Hao et al., 2019) and our dataset KACC-M are similar, but KACC-M has 38 times more concepts than DB111K-174 (see Table 1). 2.2 Knowledge Embedding Methods Existing knowledge embedding (KE) methods can be categorized as translation models (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Ji et al., 2015; Sun et al., 2019), tensor factorization based models (Yang et al., 2015; Nickel et al., 2016; Trouillon et al., 2016; Balaˇzevi´c et al., 2019), and neural models (Socher et al., 2013; Dettmers et al., 2018; Nguyen et al., 2018). These methods are typically designed for single-view KGs. Although they can be directly applied to EC-KGs by ignoring different characteristics between entity graphs and concept graphs, they cannot take full advantage of the information in EC-KGs. Several works (Krompaß et al., 2015; Xie et al., 2016; Ma et al., 2017; Moon et al., 2017) incorporate the type informat"
2021.findings-acl.153,D18-1222,1,0.91582,"EC-KG is shown in Figure 1. During the last decade, there are massive works focusing on learning representations for KGs such as TransE (Bordes et al., 2013), DistMult (Yang et al., 2015), ComplEx (Trouillon et al., 2016), and TuckER (Balaˇzevi´c et al., 2019). Though they have achieved promising results on knowledge graph completion, most of them focus on a single graph, especially the entity graph. Beyond modeling a single graph of KGs, recent studies demonstrate that jointly modeling the two graphs in the EC-KG can improve the understanding of each one (Xie et al., 2016; Moon et al., 2017; Lv et al., 2018; Hao et al., 2019). They also propose 1751 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1751–1763 August 1–6, 2021. ©2021 Association for Computational Linguistics several tasks on the EC-KG, such as link prediction and entity typing. These tasks focus on partial aspects of knowledge abstraction, concretization, and completion, which are essential abilities for humans to recognize the world and acquire knowledge. For example, in entity typing, a model may link the entity “Da Vinci” to the concept “painter” which reflects the model’s abstraction ability. Ho"
2021.findings-acl.153,N18-2053,0,0.0200075,"concept hierarchy are limited. For example, the entity numbers of DB111K-174 (Hao et al., 2019) and our dataset KACC-M are similar, but KACC-M has 38 times more concepts than DB111K-174 (see Table 1). 2.2 Knowledge Embedding Methods Existing knowledge embedding (KE) methods can be categorized as translation models (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Ji et al., 2015; Sun et al., 2019), tensor factorization based models (Yang et al., 2015; Nickel et al., 2016; Trouillon et al., 2016; Balaˇzevi´c et al., 2019), and neural models (Socher et al., 2013; Dettmers et al., 2018; Nguyen et al., 2018). These methods are typically designed for single-view KGs. Although they can be directly applied to EC-KGs by ignoring different characteristics between entity graphs and concept graphs, they cannot take full advantage of the information in EC-KGs. Several works (Krompaß et al., 2015; Xie et al., 2016; Ma et al., 2017; Moon et al., 2017) incorporate the type information into KE methods to help the completion of entity graphs. ETE (Moon 1752 et al., 2017) further conducts entity typing, which can be seen as a simplified version of our knowledge abstraction task. Though types of entities can be"
2021.findings-acl.153,2020.emnlp-main.669,0,0.0266774,"on abstraction and concretization tasks, they are not competitive to some general KGE models on logical relations. Moreover, all methods have drastic performance degradation on multi-hop tasks, and the knowledge transfer between the entity graph and the concept graph is still obscure. Finally, we present useful insights for future model design. 2 2.1 Related Work Knowledge Graph Datasets Existing datasets for knowledge graph completion are usually subgraphs of large-scale KGs, such as FB15K, FB15K-237, WN18, WN18RR and CoDEx (Bordes et al., 2013; Toutanova et al., 2015; Dettmers et al., 2018; Safavi and Koutra, 2020). These datasets are all single-view KGs, in which FB15K, FB15K-237, and CoDEx focus on the entity view while WN18 and WN18RR can be regarded as concept view KGs. Several datasets try to link the two views in different ways. Firstly, some datasets provide additional type information to the entity graph, such as FB15K+, FB15K-ET and YAGO43K-ET (Xie et al., 2016; Moon et al., 2017). Secondly, some datasets provide concept hierarchies for the entity graph, such as Probase (Wu et al., 2012) and YAGO39K (Lv et al., 2018). However, they do not provide the full concept graphs with logical relations."
2021.findings-acl.153,D15-1174,0,0.541521,"archies perform better than general KGE models on abstraction and concretization tasks, they are not competitive to some general KGE models on logical relations. Moreover, all methods have drastic performance degradation on multi-hop tasks, and the knowledge transfer between the entity graph and the concept graph is still obscure. Finally, we present useful insights for future model design. 2 2.1 Related Work Knowledge Graph Datasets Existing datasets for knowledge graph completion are usually subgraphs of large-scale KGs, such as FB15K, FB15K-237, WN18, WN18RR and CoDEx (Bordes et al., 2013; Toutanova et al., 2015; Dettmers et al., 2018; Safavi and Koutra, 2020). These datasets are all single-view KGs, in which FB15K, FB15K-237, and CoDEx focus on the entity view while WN18 and WN18RR can be regarded as concept view KGs. Several datasets try to link the two views in different ways. Firstly, some datasets provide additional type information to the entity graph, such as FB15K+, FB15K-ET and YAGO43K-ET (Xie et al., 2016; Moon et al., 2017). Secondly, some datasets provide concept hierarchies for the entity graph, such as Probase (Wu et al., 2012) and YAGO39K (Lv et al., 2018). However, they do not provide"
2021.findings-acl.411,P18-1230,0,0.0607444,"Missing"
2021.findings-acl.411,2020.emnlp-main.369,0,0.0477643,"Missing"
2021.findings-acl.411,J93-2004,0,0.0776744,"Missing"
2021.findings-acl.411,P19-1568,0,0.0581781,"Missing"
2021.findings-acl.411,P17-1187,1,0.819087,"Missing"
2021.findings-acl.411,N19-1222,0,0.0610183,"Missing"
2021.findings-acl.411,P19-1571,1,0.892498,"Missing"
2021.findings-acl.411,D18-1033,1,0.891087,"Missing"
2021.findings-acl.411,D17-1024,0,0.0481256,"Missing"
2021.findings-acl.411,C86-1107,0,0.653547,"Missing"
2021.findings-acl.411,2021.acl-long.377,1,0.808599,"Missing"
2021.findings-acl.411,2020.acl-demos.14,0,0.0408862,"Missing"
2021.findings-acl.411,N18-1101,0,0.022265,"Missing"
2021.findings-acl.411,P19-1103,0,0.0514168,"Missing"
2021.findings-acl.411,D13-1170,0,0.00872208,"Missing"
2021.findings-acl.411,2020.acl-main.540,1,0.833938,"Missing"
2021.findings-acl.411,D15-1031,0,0.0699933,"Missing"
2021.findings-emnlp.145,2020.acl-main.184,1,0.925771,"alog models (Ghandeharioun et al., (Zhang et al., 2018) with topic-shift annotations. 2019; Einolghozati et al., 2019; Liu et al., 2018) To the best of our knowledge, TIAGE is the have been reported to perform well in generating first dataset that focuses on topic-shift behaviors on-topic utterances in dialog scenarios. However, in open-domain dialog data. TIAGE contains those models still struggle to proactively gener- a human annotated dataset with 7,861 gold stanate appropriate topic-shift utterances in conversa- dard topic-shift annotations, and a weak supervitions (Holtzman et al., 2020; Zhang et al., 2020a). sion dataset to adapt pretrained NLG systems to It is beneficial for dialog systems to be able to PersonaChat-style data. The inter-annotator agreeshift topics fluently. As shown in Figure 1, topic- ment for topic-shift annotations in TIAGE is 0.479. shift behaviors are commonly observed in human With TIAGE, we propose three tasks to study conversations (Brown and Yule, 1983). Fluent topic topic-shift behaviors: topic-shift detection, topicshifts therefore are crucial for dialog models to shift triggered response generation and topic-aware be able to model or mimic human conversational dia"
2021.findings-emnlp.145,P18-1205,0,0.101746,"ntly? Topic B I finally had some spare time, so I tended my rose garden. Figure 1: An example of topic-shift behaviors in human conversations. Topic-shift utterances are highlighted in green and in italic. Changing the topic helps keep the conversation going on. shift topics away from tired topics, chatbots risk generating dull responses or repeating themselves regarding a specific topic. To facilitate research on topic-shift dialog modeling, we curate a Topic-shIft Aware dialoG datasEt 1 Introduction (TIAGE) by augmenting the PersonaChat dataset Existing dialog models (Ghandeharioun et al., (Zhang et al., 2018) with topic-shift annotations. 2019; Einolghozati et al., 2019; Liu et al., 2018) To the best of our knowledge, TIAGE is the have been reported to perform well in generating first dataset that focuses on topic-shift behaviors on-topic utterances in dialog scenarios. However, in open-domain dialog data. TIAGE contains those models still struggle to proactively gener- a human annotated dataset with 7,861 gold stanate appropriate topic-shift utterances in conversa- dard topic-shift annotations, and a weak supervitions (Holtzman et al., 2020; Zhang et al., 2020a). sion dataset to adapt pretrained"
2021.findings-emnlp.145,2020.acl-demos.30,0,0.546247,"alog models (Ghandeharioun et al., (Zhang et al., 2018) with topic-shift annotations. 2019; Einolghozati et al., 2019; Liu et al., 2018) To the best of our knowledge, TIAGE is the have been reported to perform well in generating first dataset that focuses on topic-shift behaviors on-topic utterances in dialog scenarios. However, in open-domain dialog data. TIAGE contains those models still struggle to proactively gener- a human annotated dataset with 7,861 gold stanate appropriate topic-shift utterances in conversa- dard topic-shift annotations, and a weak supervitions (Holtzman et al., 2020; Zhang et al., 2020a). sion dataset to adapt pretrained NLG systems to It is beneficial for dialog systems to be able to PersonaChat-style data. The inter-annotator agreeshift topics fluently. As shown in Figure 1, topic- ment for topic-shift annotations in TIAGE is 0.479. shift behaviors are commonly observed in human With TIAGE, we propose three tasks to study conversations (Brown and Yule, 1983). Fluent topic topic-shift behaviors: topic-shift detection, topicshifts therefore are crucial for dialog models to shift triggered response generation and topic-aware be able to model or mimic human conversational dia"
2021.findings-emnlp.145,N18-2075,0,0.0434018,"Missing"
2021.findings-emnlp.145,N18-1187,0,0.14335,"An example of topic-shift behaviors in human conversations. Topic-shift utterances are highlighted in green and in italic. Changing the topic helps keep the conversation going on. shift topics away from tired topics, chatbots risk generating dull responses or repeating themselves regarding a specific topic. To facilitate research on topic-shift dialog modeling, we curate a Topic-shIft Aware dialoG datasEt 1 Introduction (TIAGE) by augmenting the PersonaChat dataset Existing dialog models (Ghandeharioun et al., (Zhang et al., 2018) with topic-shift annotations. 2019; Einolghozati et al., 2019; Liu et al., 2018) To the best of our knowledge, TIAGE is the have been reported to perform well in generating first dataset that focuses on topic-shift behaviors on-topic utterances in dialog scenarios. However, in open-domain dialog data. TIAGE contains those models still struggle to proactively gener- a human annotated dataset with 7,861 gold stanate appropriate topic-shift utterances in conversa- dard topic-shift annotations, and a weak supervitions (Holtzman et al., 2020; Zhang et al., 2020a). sion dataset to adapt pretrained NLG systems to It is beneficial for dialog systems to be able to PersonaChat-styl"
2021.findings-emnlp.145,J97-1005,0,0.514072,"isting work in dialog systems falls into two broad categories. Task-oriented dialog systems (Budzianowski et al., 2018; Liu et al., 2018) help users complete tasks in specific domains. Opendomain dialog systems (Chen and Gao, 2017; Tang et al., 2019) allow agents to have open-ended conversations with users. Most existing dialog models (Fang et al., 2018; Zhang et al., 2020b; Ghandeharioun et al., 2019) emphasize end-to-end response generation, and do not explicitly address the topicshift problem in dialog generation. Early work in topic detection and segmentation (Hirschberg and Litman, 1993; Passonneau and Litman, 1997) focused on identifying cue phrases (such as on a different note) or examining lexical cohesion to segment topical chunks. Other work (Fiscus and Doddington, 2002) investigated topic detection and tracking (TDT) in a stream of broadcast news stories. More recent work (Glavas and Somasundaran, 2020) has explored utilizing neural networks to address topic segmentation. Although some of the existing work (Galley et al., 2003; Arnold et al., 2019) has investigated topic detection in dialog-style data, the generation aspect of topicshift modeling in dialog settings is still unclear. #Dialogs #Insta"
2021.naacl-main.452,P15-1034,0,0.023785,"Comprehensive experiments on two real-world datasets demonstrate the effectiveness of OHRE on both relation clustering and hierarchy expansion. 2 Related Works Open Relation Extraction. Recent years have witnessed an upsurge of interest in open relation extraction (OpenRE) that aims to identify new relations in unsupervised data. Existing OpenRE methods can be divided into tagging-based methods and clustering-based methods. Tagging-based methods seek to extract surface form of relational phrases from text in unsupervised (Banko et al., 2007; Banko and Etzioni, 2008), or supervised paradigms (Angeli et al., 2015; Cui et al., 2018; Stanovsky et al., 2018). However, many relations cannot be explicitly represented as surface forms, and it is hard to align different relational tokens with the same meanings. In contrast, traditional clustering-based OpenRE methods extract rich features of sentences and cluster features into novel relation types (Lin and Pantel, 2001; Yao et al., 2011, 2012; Elsahar et al., 2017). Marcheggiani and Titov (2016) propose discrete-state variational autoencoder (VAE) that optimizes a relation classifier by reconstruction signals. Simon et al. (2019) introduce skewness loss to e"
2021.naacl-main.452,P98-1012,0,0.0117585,"(RWHAC) (Elsahar et al., 2017) is the state-of-the-art rich feature-based method. RW-HAC first extracts rich features, such as entity types, then reduces feature dimension via principal component analysis, and finally clusters the features with HAC. (4) Discrete-state variational autoencoder (VAE) (Elsahar et al., 2017) optimizes a relations classifier via reconstruction signals, with rich features including dependency paths and POS tags. Evaluation Metrics. Following Wu et al. (2019); Hu et al. (2020), we adopt instance-level evaluation metrics to evaluate relation clustering, including B3 (Bagga and Baldwin, 1998), V-measure (Rosenberg and Hirschberg, 2007) and Adjusted Rand Index (ARI) (Hubert and Arabie, 1985). We refer readers to the appendix for more detailed descriptions about the evaluation metrics. 4.2.2 Hierarchy Expansion Setting In this setting, models are required to first cluster novel relations, and then further add the extracted relations into the existing hierarchy in train set. Baselines. To the best of our knowledge, there are no existing OpenRE methods designed to directly expand an existing relation hierarchy. We design two strong baselines based on state-of-theart OpenRE architectur"
2021.naacl-main.452,P08-1004,0,0.484805,"relation hierarchies with a top-down algorithm. (3) Comprehensive experiments on two real-world datasets demonstrate the effectiveness of OHRE on both relation clustering and hierarchy expansion. 2 Related Works Open Relation Extraction. Recent years have witnessed an upsurge of interest in open relation extraction (OpenRE) that aims to identify new relations in unsupervised data. Existing OpenRE methods can be divided into tagging-based methods and clustering-based methods. Tagging-based methods seek to extract surface form of relational phrases from text in unsupervised (Banko et al., 2007; Banko and Etzioni, 2008), or supervised paradigms (Angeli et al., 2015; Cui et al., 2018; Stanovsky et al., 2018). However, many relations cannot be explicitly represented as surface forms, and it is hard to align different relational tokens with the same meanings. In contrast, traditional clustering-based OpenRE methods extract rich features of sentences and cluster features into novel relation types (Lin and Pantel, 2001; Yao et al., 2011, 2012; Elsahar et al., 2017). Marcheggiani and Titov (2016) propose discrete-state variational autoencoder (VAE) that optimizes a relation classifier by reconstruction signals. Si"
2021.naacl-main.452,P18-2065,0,0.141205,"ering and hierarchy expansion. The source code and experiment details of this paper can be obtained from https://github.com/thunlp/OHRE. Training instances significant person participant … Test instances winner Hierarchy Expansion relative spouse father child Representation Learning OHRE Novel relations Relation Clustering … … Figure 1: The workflow of OHRE framework. Trained with relation hierarchy and labeled instances, OHRE extracts novel relations from open-domain corpora and adds them into the existing hierarchy. task, which extracts relational phrases from sentences (Banko et al., 2007; Cui et al., 2018). In contrast, clustering-based methods aim to cluster relation instances into groups based on their semantic similarities, and regard each cluster as a relation (Yao et al., 2011; Wu et al., 2019). However, most OpenRE models cast different relation types in isolation, without considering their rich hierarchical dependencies. Hierarchical organization of relations has been shown to play a central role in the abstraction and generalization ability 1 Introduction of human (Tenenbaum et al., 2011). This hierarchical organization of relations also constitutes the Open relation extraction (OpenRE)"
2021.naacl-main.452,N19-1423,0,0.0216324,"Missing"
2021.naacl-main.452,D18-1247,1,0.819224,"downstream tasks. Hierarchical informa1 tion derived from concept ontologies can reveal E.g., the number of relations in Wikidata has grown to more than 8, 000 in the last 6 years. semantic similarity (Leacock and Chodorow, 1998; 5683 Ponzetto and Strube, 2007), and is widely applied in enhancing classification models (Rousu et al., 2005; Weinberger and Chapelle, 2009) and knowledge representation learning models (Hu et al., 2015; Xie et al., 2016). Similar to concept hierarchy, some recent works try to exploit semantic connections from relation hierarchy. In the field of relation extraction, Han et al. (2018a) propose a hierarchical attention scheme to alleviate the noise in distant supervision. Zhang et al. (2019) leverage implicit hierarchical knowledge from KBs and propose coarse-to-fine grained attention for long-tail relations. However, these methods are designed to identify pre-defined relations, and cannot be applied to OpenRE that aims to discover novel relations in open-domain corpora. 3 OHRE Framework We divide the open hierarchical relation extraction problem into two phases: (1) learning relation representations with hierarchical information and (2) clustering and linking novel relati"
2021.naacl-main.452,D18-1514,1,0.942413,"downstream tasks. Hierarchical informa1 tion derived from concept ontologies can reveal E.g., the number of relations in Wikidata has grown to more than 8, 000 in the last 6 years. semantic similarity (Leacock and Chodorow, 1998; 5683 Ponzetto and Strube, 2007), and is widely applied in enhancing classification models (Rousu et al., 2005; Weinberger and Chapelle, 2009) and knowledge representation learning models (Hu et al., 2015; Xie et al., 2016). Similar to concept hierarchy, some recent works try to exploit semantic connections from relation hierarchy. In the field of relation extraction, Han et al. (2018a) propose a hierarchical attention scheme to alleviate the noise in distant supervision. Zhang et al. (2019) leverage implicit hierarchical knowledge from KBs and propose coarse-to-fine grained attention for long-tail relations. However, these methods are designed to identify pre-defined relations, and cannot be applied to OpenRE that aims to discover novel relations in open-domain corpora. 3 OHRE Framework We divide the open hierarchical relation extraction problem into two phases: (1) learning relation representations with hierarchical information and (2) clustering and linking novel relati"
2021.naacl-main.452,2020.emnlp-main.299,0,0.713973,"et al., 2018). However, many relations cannot be explicitly represented as surface forms, and it is hard to align different relational tokens with the same meanings. In contrast, traditional clustering-based OpenRE methods extract rich features of sentences and cluster features into novel relation types (Lin and Pantel, 2001; Yao et al., 2011, 2012; Elsahar et al., 2017). Marcheggiani and Titov (2016) propose discrete-state variational autoencoder (VAE) that optimizes a relation classifier by reconstruction signals. Simon et al. (2019) introduce skewness loss to enable stable training of VAE. Hu et al. (2020) learn relation representations and clusters iteratively via self-training. Wu et al. (2019) improve conventional unsupervised clustering-based methods by combining supervised and unsupervised data via siamese networks, and achieve state-of-the-art performance. However, existing OpenRE methods cast different relation types in isolation without considering their rich hierarchical dependencies. Hierarchy Information Exploitation. Wellorganized taxonomy and hierarchies can facilitate many downstream tasks. Hierarchical informa1 tion derived from concept ontologies can reveal E.g., the number of r"
2021.naacl-main.452,Q16-1017,0,0.766042,"ods. Tagging-based methods seek to extract surface form of relational phrases from text in unsupervised (Banko et al., 2007; Banko and Etzioni, 2008), or supervised paradigms (Angeli et al., 2015; Cui et al., 2018; Stanovsky et al., 2018). However, many relations cannot be explicitly represented as surface forms, and it is hard to align different relational tokens with the same meanings. In contrast, traditional clustering-based OpenRE methods extract rich features of sentences and cluster features into novel relation types (Lin and Pantel, 2001; Yao et al., 2011, 2012; Elsahar et al., 2017). Marcheggiani and Titov (2016) propose discrete-state variational autoencoder (VAE) that optimizes a relation classifier by reconstruction signals. Simon et al. (2019) introduce skewness loss to enable stable training of VAE. Hu et al. (2020) learn relation representations and clusters iteratively via self-training. Wu et al. (2019) improve conventional unsupervised clustering-based methods by combining supervised and unsupervised data via siamese networks, and achieve state-of-the-art performance. However, existing OpenRE methods cast different relation types in isolation without considering their rich hierarchical depend"
2021.naacl-main.452,D14-1162,0,0.0860062,"Missing"
2021.naacl-main.452,D19-1021,1,0.212409,"ances winner Hierarchy Expansion relative spouse father child Representation Learning OHRE Novel relations Relation Clustering … … Figure 1: The workflow of OHRE framework. Trained with relation hierarchy and labeled instances, OHRE extracts novel relations from open-domain corpora and adds them into the existing hierarchy. task, which extracts relational phrases from sentences (Banko et al., 2007; Cui et al., 2018). In contrast, clustering-based methods aim to cluster relation instances into groups based on their semantic similarities, and regard each cluster as a relation (Yao et al., 2011; Wu et al., 2019). However, most OpenRE models cast different relation types in isolation, without considering their rich hierarchical dependencies. Hierarchical organization of relations has been shown to play a central role in the abstraction and generalization ability 1 Introduction of human (Tenenbaum et al., 2011). This hierarchical organization of relations also constitutes the Open relation extraction (OpenRE) aims to extract novel relations types between entities from open- foundation of most modern KBs (Auer et al., 2007; Bollacker et al., 2008). Figure 1 illustrates an exdomain corpora, which plays a"
2021.naacl-main.452,D11-1135,0,0.228983,"cipant … Test instances winner Hierarchy Expansion relative spouse father child Representation Learning OHRE Novel relations Relation Clustering … … Figure 1: The workflow of OHRE framework. Trained with relation hierarchy and labeled instances, OHRE extracts novel relations from open-domain corpora and adds them into the existing hierarchy. task, which extracts relational phrases from sentences (Banko et al., 2007; Cui et al., 2018). In contrast, clustering-based methods aim to cluster relation instances into groups based on their semantic similarities, and regard each cluster as a relation (Yao et al., 2011; Wu et al., 2019). However, most OpenRE models cast different relation types in isolation, without considering their rich hierarchical dependencies. Hierarchical organization of relations has been shown to play a central role in the abstraction and generalization ability 1 Introduction of human (Tenenbaum et al., 2011). This hierarchical organization of relations also constitutes the Open relation extraction (OpenRE) aims to extract novel relations types between entities from open- foundation of most modern KBs (Auer et al., 2007; Bollacker et al., 2008). Figure 1 illustrates an exdomain corp"
2021.naacl-main.452,P12-1075,0,0.724872,"Missing"
2021.naacl-main.452,D07-1043,0,0.0263829,"e state-of-the-art rich feature-based method. RW-HAC first extracts rich features, such as entity types, then reduces feature dimension via principal component analysis, and finally clusters the features with HAC. (4) Discrete-state variational autoencoder (VAE) (Elsahar et al., 2017) optimizes a relations classifier via reconstruction signals, with rich features including dependency paths and POS tags. Evaluation Metrics. Following Wu et al. (2019); Hu et al. (2020), we adopt instance-level evaluation metrics to evaluate relation clustering, including B3 (Bagga and Baldwin, 1998), V-measure (Rosenberg and Hirschberg, 2007) and Adjusted Rand Index (ARI) (Hubert and Arabie, 1985). We refer readers to the appendix for more detailed descriptions about the evaluation metrics. 4.2.2 Hierarchy Expansion Setting In this setting, models are required to first cluster novel relations, and then further add the extracted relations into the existing hierarchy in train set. Baselines. To the best of our knowledge, there are no existing OpenRE methods designed to directly expand an existing relation hierarchy. We design two strong baselines based on state-of-theart OpenRE architectures. (1) RW-HAC for hierarchy expansion (RW-H"
2021.naacl-main.452,C14-1220,0,0.0267381,"erarchical than ri2 and rj2 in representation space, since ri1 and curriculum learning for robust model training. Pair- rj1 are close to each other in the relation hierarchy. wise virtual adversarial training is also introduced We design a hierarchical triplet objective with to improve the representation generalization ability. a dynamic margin which is determined by the disRelation Embedding Encoder. We adopt CNN to tance between relations in hierarchy. Specifically, encode sentences into relation representations. Fol- the dynamic margin is conducted over the instances lowing previous works (Zeng et al., 2014), given of the relations. As shown in Figure 2, given two a sentence s and target entity pair (eh , et ), each relations ri and rj sampled by hierarchical curricuword in the sentence is first transformed into in- lum training strategy (which will be introduced put representations by the concatenation of word later), we randomly sample two instances (namely embedding and position embedding indicating the anchor instance a and positive instance p) from ri , position of each entity. Then the input representa- and an instance (namely negative instance n) from tion is fed into a convolutional layer"
2021.naacl-main.452,N19-1306,0,0.0189905,"f relations in Wikidata has grown to more than 8, 000 in the last 6 years. semantic similarity (Leacock and Chodorow, 1998; 5683 Ponzetto and Strube, 2007), and is widely applied in enhancing classification models (Rousu et al., 2005; Weinberger and Chapelle, 2009) and knowledge representation learning models (Hu et al., 2015; Xie et al., 2016). Similar to concept hierarchy, some recent works try to exploit semantic connections from relation hierarchy. In the field of relation extraction, Han et al. (2018a) propose a hierarchical attention scheme to alleviate the noise in distant supervision. Zhang et al. (2019) leverage implicit hierarchical knowledge from KBs and propose coarse-to-fine grained attention for long-tail relations. However, these methods are designed to identify pre-defined relations, and cannot be applied to OpenRE that aims to discover novel relations in open-domain corpora. 3 OHRE Framework We divide the open hierarchical relation extraction problem into two phases: (1) learning relation representations with hierarchical information and (2) clustering and linking novel relations to existing hierarchies. Curriculum Learning *#! !! &quot;! #! *#$ !$ &quot;$ #$ layer 1 *&quot;! layer 2 *&quot;$ Dynamic Ma"
2021.naacl-main.452,P19-1133,0,0.0538988,"8), or supervised paradigms (Angeli et al., 2015; Cui et al., 2018; Stanovsky et al., 2018). However, many relations cannot be explicitly represented as surface forms, and it is hard to align different relational tokens with the same meanings. In contrast, traditional clustering-based OpenRE methods extract rich features of sentences and cluster features into novel relation types (Lin and Pantel, 2001; Yao et al., 2011, 2012; Elsahar et al., 2017). Marcheggiani and Titov (2016) propose discrete-state variational autoencoder (VAE) that optimizes a relation classifier by reconstruction signals. Simon et al. (2019) introduce skewness loss to enable stable training of VAE. Hu et al. (2020) learn relation representations and clusters iteratively via self-training. Wu et al. (2019) improve conventional unsupervised clustering-based methods by combining supervised and unsupervised data via siamese networks, and achieve state-of-the-art performance. However, existing OpenRE methods cast different relation types in isolation without considering their rich hierarchical dependencies. Hierarchy Information Exploitation. Wellorganized taxonomy and hierarchies can facilitate many downstream tasks. Hierarchical inf"
2021.naacl-main.452,N18-1081,0,0.0182471,"world datasets demonstrate the effectiveness of OHRE on both relation clustering and hierarchy expansion. 2 Related Works Open Relation Extraction. Recent years have witnessed an upsurge of interest in open relation extraction (OpenRE) that aims to identify new relations in unsupervised data. Existing OpenRE methods can be divided into tagging-based methods and clustering-based methods. Tagging-based methods seek to extract surface form of relational phrases from text in unsupervised (Banko et al., 2007; Banko and Etzioni, 2008), or supervised paradigms (Angeli et al., 2015; Cui et al., 2018; Stanovsky et al., 2018). However, many relations cannot be explicitly represented as surface forms, and it is hard to align different relational tokens with the same meanings. In contrast, traditional clustering-based OpenRE methods extract rich features of sentences and cluster features into novel relation types (Lin and Pantel, 2001; Yao et al., 2011, 2012; Elsahar et al., 2017). Marcheggiani and Titov (2016) propose discrete-state variational autoencoder (VAE) that optimizes a relation classifier by reconstruction signals. Simon et al. (2019) introduce skewness loss to enable stable training of VAE. Hu et al. (20"
C12-1105,W06-1615,0,0.0350802,"Missing"
C12-1105,J93-2003,0,0.0924315,"s content c, keyphrase extraction aims to seek a set of keyphrases k that maximizes Pr(k|c).Q By simply assuming the keyphrases are independent conditional over d, we have Pr(k|c) = k∈k Pr(k|c). The optimal set of keyphrases k∗ can be represented as follows: k∗ = arg max Pr(k|c) = arg max k k Y Pr(k|c). (1) k∈k Suppose the number of keyphrases is pre-defined as N k , we can simply find k∗ by ranking 1718 each candidate keyphrase v ∈ V according to its score Pr(v|c) in descending order and selecting top-N k keyphrases. 2.2 Word Trigger Model Word Trigger Model (WTM) is inspired by IBM Model-1 (Brown et al., 1993), the most widely used word alignment models in SMT. WTM assumes the content and the keyphrases of a document are describing the same themes while written in two different languages: document content in one language while keyphrases in the other. From this perspective, keyphrase extraction can be regarded as a translation process from a given document content to keyphrases. In more detail, the translation process is modeled as a trigger process as follows. First, WTM finds several important words in the document content as trigger words. Then, activated by these trigger words, WTM maps the doc"
C12-1105,D11-1146,1,0.819599,"category label. Various methods such as Naive Bayes (Garg and Weber, 2008) and kNN (Li et al., 2009) have been explored. Some researchers proposed using latent topics to build semantic relations between words and tags. The representative methods include TagLDA (Krestel et al., 2009; Si and Sun, 2009) and Content Relevance Model (CRM) (Iwata et al., 2009). However, these methods usually suffer from the over-generalization problem. Recently, a new approach based on word alignment models (WAM) in statistical machine translation (SMT) has been proposed for keyphrase extraction (Ravi et al., 2010; Liu et al., 2011b,a, 2012). WAM-based methods assume the content and keyphrases of a document are describing the same themes but written in different languages. Under this assumption, WAM-based methods regard keyphrase extraction as a translation process from document content to keyphrases. This process is modeled as a trigger from important words in document content to keyphrases according to trigger probabilities between words and keyphrases. WAM-based methods will learn trigger probabilities from sufficient documentkeyphrase pairs. With the trigger probabilities, given a novel document, WAM-based methods a"
C12-1105,W11-0316,1,0.834608,"category label. Various methods such as Naive Bayes (Garg and Weber, 2008) and kNN (Li et al., 2009) have been explored. Some researchers proposed using latent topics to build semantic relations between words and tags. The representative methods include TagLDA (Krestel et al., 2009; Si and Sun, 2009) and Content Relevance Model (CRM) (Iwata et al., 2009). However, these methods usually suffer from the over-generalization problem. Recently, a new approach based on word alignment models (WAM) in statistical machine translation (SMT) has been proposed for keyphrase extraction (Ravi et al., 2010; Liu et al., 2011b,a, 2012). WAM-based methods assume the content and keyphrases of a document are describing the same themes but written in different languages. Under this assumption, WAM-based methods regard keyphrase extraction as a translation process from document content to keyphrases. This process is modeled as a trigger from important words in document content to keyphrases according to trigger probabilities between words and keyphrases. WAM-based methods will learn trigger probabilities from sufficient documentkeyphrase pairs. With the trigger probabilities, given a novel document, WAM-based methods a"
C12-1105,D10-1036,1,0.25901,"ses should primarily rely on their semantic relatedness with the document rather than being constrained by their occurrence frequencies in the document. This requires keyphrase extraction can bridge the vocabulary gap between document content and keyphrases. Many unsupervised methods have also been extensively explored for keyphrase extraction. The most simple unsupervised method is ranking the candidate keyphrases according to TFIDF (Salton and Buckley, 1988) and then selecting top-ranked ones as keyphrases. There are also graph-based methods (Mihalcea and Tarau, 2004; Wan and Xiao, 2008b,a; Liu et al., 2010), clustering-based methods (Grineva et al., 2009; Liu et al., 2009) and latent topic models (Heinrich, 2005; Blei and Lafferty, 2009) proposed for keyphrase extraction. Most of these methods take frequencies of candidate keyphrases as the crucial decision criteria, and thus tend to select those high-frequency ones as keyphrases. Given sufficient annotation data for training, we can adopt the classification-based approach for keyphrase extraction. For example, some methods (Frank et al., 1999; Witten et al., 1999; Turney, 2000) regard keyphrase extraction as a binary classification problem (is-"
C12-1105,D09-1027,1,0.643699,"cument rather than being constrained by their occurrence frequencies in the document. This requires keyphrase extraction can bridge the vocabulary gap between document content and keyphrases. Many unsupervised methods have also been extensively explored for keyphrase extraction. The most simple unsupervised method is ranking the candidate keyphrases according to TFIDF (Salton and Buckley, 1988) and then selecting top-ranked ones as keyphrases. There are also graph-based methods (Mihalcea and Tarau, 2004; Wan and Xiao, 2008b,a; Liu et al., 2010), clustering-based methods (Grineva et al., 2009; Liu et al., 2009) and latent topic models (Heinrich, 2005; Blei and Lafferty, 2009) proposed for keyphrase extraction. Most of these methods take frequencies of candidate keyphrases as the crucial decision criteria, and thus tend to select those high-frequency ones as keyphrases. Given sufficient annotation data for training, we can adopt the classification-based approach for keyphrase extraction. For example, some methods (Frank et al., 1999; Witten et al., 1999; Turney, 2000) regard keyphrase extraction as a binary classification problem (is-keyphrase 1716 or non-keyphrase). Keyphrase extraction can also be"
C12-1105,D09-1092,0,0.0205123,"en a novel document d with its content c, we can rank each candidate keyphrase v as follows: Pr(v|c) = X w∈c Pr(v|w, ψ) Pr(w|c) = X ψ vw Pr(w|c), (4) w∈c where Pr(w|c) indicates the weight of the word w in c, which can be calculated using the TFIDF score of w in c. From the ranking list in descending order, we can select the topranked ones as keyphrases of the given document. 2.3 Topic Trigger Model WTM triggers keyphrases at the word level. We can also trigger at the topic level, and thus propose Topic Trigger Model (TTM) for keyphrase extraction. TTM is inspired by Polylingual Topic Models (Mimno et al., 2009), which is originally proposed to model parallel documents in multiple languages. TTM is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003). Suppose there are T latent topics in TTM, and the number of topics |T |can be pre-defined by users. TTM assumes that the content c and keyphrases k of a document d share the same distribution over |T |topics (i.e., θd ), which is drawn from a symmetric Dirichlet prior with concentration parameter α. TTM also assumes that each topic t ∈ T corresponds to two different multinomial distributions over words, one for keyphrases (i.e., φ tk )"
C12-1105,voorhees-tice-2000-trec,0,0.0521863,"gher. However, precision/recall/F-measure does not take the order of extracted keyphrases into account. To address the problem, we select the following two additional metrics. One metric is binary preference measure (Bpref) (Buckley and Voorhees, 2004). Bpref can consider the order of extracted keyphrases for evaluation. For a document, if there are R correct keyphrases within M extracted keyphrases by a method, in which r is a correct keyphrase  P |n ranked higher than r| and n is an incorrect keyphrase. It is defined as Bpref = R1 r∈R 1 − . M The other metric is mean reciprocal rank (MRR) (Voorhees, 2000) which is usually used to evaluate how the first correct keyphrase for each document is ranked. For a document d, rankd is denoted as the P rank of the first correct keyphrase with all extracted keyphrases, It 1 1 is defined as MRR = |D| d∈D rank , where D is the document set for keyphrase extraction. d 3.2 Case Studies Before quantitative evaluation, we perform case studies by looking into the topics learned by TWTM. By setting T = 100 of TWTM, We select two topics, i.e., Topic-59 and Topic-92 for study. In first several rows of Table 2, we list the top-10 words and top-10 keyphrases given th"
C12-1105,C08-1122,0,0.0306823,"he selection of keyphrases should primarily rely on their semantic relatedness with the document rather than being constrained by their occurrence frequencies in the document. This requires keyphrase extraction can bridge the vocabulary gap between document content and keyphrases. Many unsupervised methods have also been extensively explored for keyphrase extraction. The most simple unsupervised method is ranking the candidate keyphrases according to TFIDF (Salton and Buckley, 1988) and then selecting top-ranked ones as keyphrases. There are also graph-based methods (Mihalcea and Tarau, 2004; Wan and Xiao, 2008b,a; Liu et al., 2010), clustering-based methods (Grineva et al., 2009; Liu et al., 2009) and latent topic models (Heinrich, 2005; Blei and Lafferty, 2009) proposed for keyphrase extraction. Most of these methods take frequencies of candidate keyphrases as the crucial decision criteria, and thus tend to select those high-frequency ones as keyphrases. Given sufficient annotation data for training, we can adopt the classification-based approach for keyphrase extraction. For example, some methods (Frank et al., 1999; Witten et al., 1999; Turney, 2000) regard keyphrase extraction as a binary class"
C12-1105,W04-3252,0,\N,Missing
C12-2064,P98-2127,0,0.0642856,"context-aware relations of tags from a large collection of annotation data? (2) After obtaining the context-aware relations of tags, how to construct a tag graph given a resource for random walks? To address the challenges, we propose a probabilistic model to learn the context-aware relations of tags, and propose a random walk algorithm over context-aware relation graphs to suggest tags. To investigate the efficiency of our method, we carry out experiments using real-world datasets. Related work. Measuring semantic relations have been studied in many tasks such as measuring term similarities (Lin, 1998; Gabrilovich and Markovitch, 2007) and query similarities (Wen et al., 2002; Mei et al., 2008). Meanwhile, context-aware setting is being considered in many applications including recommender systems (Adomavicius and Tuzhilin, 2011) and query suggestion (Brown and Jones, 2001), which is a critical research issue for all applications under real-world complex scene. In social tagging, co-occurrencebased tag relations have been explored to group tags into clusters (Wu et al., 2006b; Brooks and Montanez, 2006; Shepitsen et al., 2008), and have been adopted in personalized tag suggestion (Shepitse"
C12-2064,D11-1146,1,0.632126,"d the user-annotated tags as annotation. (a) An example book. (b) TCM Figure 1: (a) An example book. (b) Graphical model of TCM. Social tagging provides a convenient management scheme compared to strict taxonomy in libraries. In order to attract more users to contribute social annotations, many social tagging systems facilitate users through automatic tag suggestion. There are two main approaches: graph-based and content-based. The former approach (Jaschke et al., 2008; Rendle et al., 2009) suggests tags according to users’ annotation history, while the latter approach (Si et al., 2009, 2010; Liu et al., 2011) according to resource meta-data. Since graph-based methods often suffer from the cold-start problem when they face new users or resources, content-based methods are usually regarded as an important component in social tagging systems especially in the initial stage. In this paper we focus on the content-based approach. Many social tagging methods are based on independence assumption, which is widely adopted in computational linguistics (Manning and Schutze, 2000) and information retrieval (Manning et al., 2008). Under this assumption, tags are regarded independent with each other given the re"
C12-2064,D10-1036,1,0.88967,"Missing"
C12-2064,J00-2011,0,0.0715914,"e et al., 2008; Rendle et al., 2009) suggests tags according to users’ annotation history, while the latter approach (Si et al., 2009, 2010; Liu et al., 2011) according to resource meta-data. Since graph-based methods often suffer from the cold-start problem when they face new users or resources, content-based methods are usually regarded as an important component in social tagging systems especially in the initial stage. In this paper we focus on the content-based approach. Many social tagging methods are based on independence assumption, which is widely adopted in computational linguistics (Manning and Schutze, 2000) and information retrieval (Manning et al., 2008). Under this assumption, tags are regarded independent with each other given the resource. Although this makes methods easier to implement, it does not accord with the real world, in which the annotated tags of a resource are usually semantically correlated with each other. Hence, if we can find an effective approach to model tag relations, it may improve the suggestion quality significantly. It is non-trivial to model tag relations. Given a resource, the tag relations are context-aware. Two tags may be more related with each other given a resou"
C12-2064,C98-2122,0,\N,Missing
C12-2069,J93-2003,0,0.0409633,"uggestion has been well studied (Gupta et al., 2010). There are two approaches for social tag suggestion: graphbased approach and content-based approach. Since we have to suggest tags according to the content of m, we follow the content-based approach. The specialty of our problem compared to previous problems lies in: (1) the method should be robust to noise and informal format of microblog messages; and (2) m is short with no more than 140 Chinese characters in Sina Weibo. Taking the specialty in consideration, we propose to use word alignment model (WAM) in statistical machine translation (Brown et al., 1993) for social tag suggestion, which has been verified to outperform other existing content-based methods (Liu et al., 2011, 2012). Here we give a brief introduction to WAM, and introduce some important extensions to make the method appropriate to suggest social tags for microblog messages. WAM for Social Tag Suggestion. Given P a message m, WAM ranks candidate tags by computing their likelihood Pr WAM (t|m) = w∈m Pr(t|w) Pr(w|m), where Pr(w|m) is the weight of the word w in m, and Pr(t|w) is the translation probability from w to t obtained from the translation models. Pr(w|m) is estimated using"
C12-2069,C08-1049,0,0.0549553,"Missing"
C12-2069,D11-1146,1,0.842685,"ch and content-based approach. Since we have to suggest tags according to the content of m, we follow the content-based approach. The specialty of our problem compared to previous problems lies in: (1) the method should be robust to noise and informal format of microblog messages; and (2) m is short with no more than 140 Chinese characters in Sina Weibo. Taking the specialty in consideration, we propose to use word alignment model (WAM) in statistical machine translation (Brown et al., 1993) for social tag suggestion, which has been verified to outperform other existing content-based methods (Liu et al., 2011, 2012). Here we give a brief introduction to WAM, and introduce some important extensions to make the method appropriate to suggest social tags for microblog messages. WAM for Social Tag Suggestion. Given P a message m, WAM ranks candidate tags by computing their likelihood Pr WAM (t|m) = w∈m Pr(t|w) Pr(w|m), where Pr(w|m) is the weight of the word w in m, and Pr(t|w) is the translation probability from w to t obtained from the translation models. Pr(w|m) is estimated using term-frequency and inverse message frequency (TFIMF), which is similar to TFIDF. According to the ranking scores, we sug"
C12-2069,J03-1002,0,0.0040868,"ity of the tag t. Using messages and their corresponding extracted tags, we build the translation pairs for WAM training. We use IBM Model 1 (Brown et al., 1993) for WAM training. IBM Model 1 is a widely used word alignment algorithm which does not require linguistic knowledge for two languages. We have also tested more sophisticated word alignment algorithms such as IBM Model 3 for tag suggestion. However, these methods do not achieve better performance than IBM Model 1. Therefore, in this paper we only demonstrate the experimental results using IBM Model 1. In experiments, we select GIZA++ (Och and Ney, 2003) to train IBM Model 1. 3.2 Measuring Authority Some works have been devoted to authority analysis of social media (Pal and Counts, 2011). The basic conclusion is that a microblog user has more authority if it has more followers and posts more original messages. Therefore, in this paper, we simply compute authority of a user e as:  |F | Pr(e) = P log e∈Em e |Ae | log × log(|Me |) |Fe | |Ae |  × log(|Me |) , (3) where Fe is the follower set of e and Ae is the user set followed by e. The score is normalized over all experts in Em . 3.3 Restricting Candidate Expert Set for TSC We denote the name"
C12-2069,D11-1147,0,0.0419291,"8). However, quantitative studies of rumors have just begun, and microblog services provide a chance. Recently, researchers have developed different approaches to study rumors or misinformation. Some researchers devoted to finding information diffusion patterns over social networks (Kempe et al., 2003; Gruhl et al., 2004; Leskovec et al., 2009; Romero et al., 2011) and limiting the spread of misinformation by means of network structure (Budak et al., 2011). The spread patterns of rumors with respect to the content and conversations were also studied (Ennals et al., 2010; Mendoza et al., 2010; Qazvinian et al., 2011; Castillo et al., 2011). On one hand, most of these methods all focused on external features of rumors, which cannot ultimately determine whether a message is misinformation. On the other hand, the features can be obtained only after the information has spread over social networks. Existing methods find experts based on either people relations (graph-based approach) or people meta-data (content-based approach). In the graph-based approach, users are ranked according to their authority scores computed by the algorithms such as HITS and PageRank (Zhang et al., 2007; Jurczyk and Agichtein, 2007)"
C12-2069,N10-1072,0,0.042198,"Missing"
C12-2074,D11-1146,1,0.713637,"Missing"
C12-2074,D09-1026,0,0.142585,"Missing"
C18-1041,D14-1067,0,0.0156667,"res or manual labels which are hard to gather on a larger dataset. Whats more, the conventional models could not catch the subtle difference between similar crimes, thus they wouldnt perform well when the number of classes increases and more similar crimes appear. With the successful usage of neural network methods on speech (Mikolov et al., 2011; Hinton et al., 2012; Dahl et al., 2012; Sainath et al., 2013), computer vision (CV) (Krizhevsky et al., 2012; Farabet et al., 2013; Tompson et al., 2014; Szegedy et al., 2015) and natural language processing (NLP) (Collobert et al., 2011; Kim, 2014; Bordes et al., 2014; Sutskever et al., 2014; Jean et al., 2015; Yang et al., 2016), researchers propose to employ neural models for legal tasks. Luo et al. (2017) proposes a hierarchical attentional network to predict charges and extract relevant articles jointly. However, this work only focuses on high-frequency charges, without paying attention to few-shot and confusing ones. To address these issues, we propose an attention-based neural model by incorporating several discriminative legal attributes. 3 Method In this section, we propose a few-shot neural model which jointly models charge prediction task and leg"
C18-1041,J81-4005,0,0.745887,"Missing"
C18-1041,P15-1001,0,0.027774,"on a larger dataset. Whats more, the conventional models could not catch the subtle difference between similar crimes, thus they wouldnt perform well when the number of classes increases and more similar crimes appear. With the successful usage of neural network methods on speech (Mikolov et al., 2011; Hinton et al., 2012; Dahl et al., 2012; Sainath et al., 2013), computer vision (CV) (Krizhevsky et al., 2012; Farabet et al., 2013; Tompson et al., 2014; Szegedy et al., 2015) and natural language processing (NLP) (Collobert et al., 2011; Kim, 2014; Bordes et al., 2014; Sutskever et al., 2014; Jean et al., 2015; Yang et al., 2016), researchers propose to employ neural models for legal tasks. Luo et al. (2017) proposes a hierarchical attentional network to predict charges and extract relevant articles jointly. However, this work only focuses on high-frequency charges, without paying attention to few-shot and confusing ones. To address these issues, we propose an attention-based neural model by incorporating several discriminative legal attributes. 3 Method In this section, we propose a few-shot neural model which jointly models charge prediction task and legal attribute prediction task in a unified f"
C18-1041,D14-1181,0,0.121215,"text or case profiles. For example, some works (Liu et al., 2004; Liu and Hsieh, 2006) utilize shallow textual features, including characters, words, and phrases, to predict charges. Katz et al. (2017) predict the US Supreme Court’s decisions with efficient features extracted from case profiles (e.g., dates, locations, terms, and types). All these approaches require numerous human effort to design features and annotate training instances. Besides, these methods are hard to scale to other scenarios. Inspired by the successful usage of deep neural networks on natural language processing tasks (Kim, 2014; Baharudin et al., 2010; Tang et al., 2015), researchers propose to employ deep neural networks to model legal documents. For example, Luo et al. (2017) propose an attention-based neural network for charge prediction by incorporating the relevant law articles. However, charge prediction is still confronted with two major challenges which make it non-trivial: ∗ † Indicates equal contribution. Corresponding Author. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 487 Proceedings of the 27th Inter"
C18-1041,O12-5004,0,0.412552,"Missing"
C18-1041,D17-1289,0,0.600661,"ords, and phrases, to predict charges. Katz et al. (2017) predict the US Supreme Court’s decisions with efficient features extracted from case profiles (e.g., dates, locations, terms, and types). All these approaches require numerous human effort to design features and annotate training instances. Besides, these methods are hard to scale to other scenarios. Inspired by the successful usage of deep neural networks on natural language processing tasks (Kim, 2014; Baharudin et al., 2010; Tang et al., 2015), researchers propose to employ deep neural networks to model legal documents. For example, Luo et al. (2017) propose an attention-based neural network for charge prediction by incorporating the relevant law articles. However, charge prediction is still confronted with two major challenges which make it non-trivial: ∗ † Indicates equal contribution. Corresponding Author. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 487 Proceedings of the 27th International Conference on Computational Linguistics, pages 487–498 Santa Fe, New Mexico, USA, August 20-26, 2018. Fact On June 24, 2015, the defendant pry o"
C18-1041,D15-1167,0,0.0235342,", some works (Liu et al., 2004; Liu and Hsieh, 2006) utilize shallow textual features, including characters, words, and phrases, to predict charges. Katz et al. (2017) predict the US Supreme Court’s decisions with efficient features extracted from case profiles (e.g., dates, locations, terms, and types). All these approaches require numerous human effort to design features and annotate training instances. Besides, these methods are hard to scale to other scenarios. Inspired by the successful usage of deep neural networks on natural language processing tasks (Kim, 2014; Baharudin et al., 2010; Tang et al., 2015), researchers propose to employ deep neural networks to model legal documents. For example, Luo et al. (2017) propose an attention-based neural network for charge prediction by incorporating the relevant law articles. However, charge prediction is still confronted with two major challenges which make it non-trivial: ∗ † Indicates equal contribution. Corresponding Author. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 487 Proceedings of the 27th International Conference on Computational Linguis"
C18-1041,N16-1174,0,0.414897,"t. Whats more, the conventional models could not catch the subtle difference between similar crimes, thus they wouldnt perform well when the number of classes increases and more similar crimes appear. With the successful usage of neural network methods on speech (Mikolov et al., 2011; Hinton et al., 2012; Dahl et al., 2012; Sainath et al., 2013), computer vision (CV) (Krizhevsky et al., 2012; Farabet et al., 2013; Tompson et al., 2014; Szegedy et al., 2015) and natural language processing (NLP) (Collobert et al., 2011; Kim, 2014; Bordes et al., 2014; Sutskever et al., 2014; Jean et al., 2015; Yang et al., 2016), researchers propose to employ neural models for legal tasks. Luo et al. (2017) proposes a hierarchical attentional network to predict charges and extract relevant articles jointly. However, this work only focuses on high-frequency charges, without paying attention to few-shot and confusing ones. To address these issues, we propose an attention-based neural model by incorporating several discriminative legal attributes. 3 Method In this section, we propose a few-shot neural model which jointly models charge prediction task and legal attribute prediction task in a unified framework. In the fol"
C18-1041,D17-1099,0,0.0278547,"ttribute-based classification to the label-embedding task. Jayaraman and Grauman (2014) introduces a random forest method stressing the unreliability of attribute prediction for unseen classes. They also extend it to the few-shot scenario. Other than attributes, other external information can also be introduced to promote zero-shot classification. Elhoseiny et al. (2014) makes use of text description of the class label to transfer knowledge between text features and visual features. Zero-shot learning has also been used in applications besides object recognition, such as activity recognition (Zellers and Choi, 2017) and event recognition (Wu et al., 2014). 2.2 Charge Prediction Researchers in the legal area have been working on automatically making the legal judgment for a long time. Kort (1957) applies quantitative methods to predict judgment by calculation numerical values for factual elements. Nagel (1963) makes use of correlation analysis to make predictions for reapportioning cases. Keown (1980) introduced mathematical models used for legal prediction such as linear models and the scheme of nearest neighbors. These methods are usually mathematical or quantitative, and they are restricted to a small"
C18-1057,P17-1149,1,0.665717,"h Convolution Decoder Figure 2: Framework of NCEL. The inputs of a set of mentions in a document are listed in the left side. The words in red indicate the current mention mi , where mi−1 , mi+1 are neighbor mentions, and Φ(mi ) = {ei1 , ei2 , ei3 } denotes the candidate entity set for mi . 3 Feature Extraction The main goal of NCEL is to find a solution for collective entity linking using an end-to-end neural model, rather than to improve the measurements of local textual similarity or global mention/entity relatedness. Therefore, we use joint embeddings of words and entities at sense level (Cao et al., 2017) to represent mentions and its contexts for feature extraction. In this section, we give a brief description of our embeddings followed by our features used in the neural model. 3.1 Learning Joint Embeddings of Word and Entity Following (Cao et al., 2017), we use Wikipedia articles, hyperlinks, and entity outlinks to jointly learn word/mention and entity embeddings in a unified vector space, so that similar words/mentions and entities have similar vectors. To address the ambiguity of words/mentions, (Cao et al., 2017) represents each word/mention with multiple vectors, and each vector denotes"
C18-1057,C12-1028,0,0.0206839,"vance between candidates and the corresponding mentions based on the information both in texts and KBs (Nguyen et al., 2016). In terms of the features used for ranking, we classify existing EL models into two groups: local models to resolve mentions independently relying on textual context information from the surrounding words (Chen and Ji, 2011; Chisholm and Hachey, 2015; Lazic et al., 2015; Yamada et al., 2016), and global (collective) models, which are the main focus of this paper, that encourage the target entities of all mentions in a document to be topically coherent (Han et al., 2011; Cassidy et al., 2012; He et al., 2013b; Cheng and Roth, 2013; Durrett and Klein, 2014; Huang et al., 2014). Global models usually build an entity graph based on KBs to capture coherent entities for all identified mentions in a document, where the nodes are entities, and edges denote their relations. The graph provides highly discriminative semantic signals (e.g., entity relatedness) that are unavailable to local model (Eshel et al., 2017). For example (Figure 1), an EL model seemly cannot find sufficient disambiguation clues for the mention England from its surrounding words, unless it utilizes the coherence ∗ Co"
C18-1057,D11-1071,0,0.0225205,"main phases: (i) candidate generation obtains a set of referent entities in KB for each mention, and (ii) named entity disambiguation selects the possible candidate entity by solving a ranking problem. The key challenge lies in the ranking model that computes the relevance between candidates and the corresponding mentions based on the information both in texts and KBs (Nguyen et al., 2016). In terms of the features used for ranking, we classify existing EL models into two groups: local models to resolve mentions independently relying on textual context information from the surrounding words (Chen and Ji, 2011; Chisholm and Hachey, 2015; Lazic et al., 2015; Yamada et al., 2016), and global (collective) models, which are the main focus of this paper, that encourage the target entities of all mentions in a document to be topically coherent (Han et al., 2011; Cassidy et al., 2012; He et al., 2013b; Cheng and Roth, 2013; Durrett and Klein, 2014; Huang et al., 2014). Global models usually build an entity graph based on KBs to capture coherent entities for all identified mentions in a document, where the nodes are entities, and edges denote their relations. The graph provides highly discriminative semant"
C18-1057,D13-1184,0,0.041381,"onding mentions based on the information both in texts and KBs (Nguyen et al., 2016). In terms of the features used for ranking, we classify existing EL models into two groups: local models to resolve mentions independently relying on textual context information from the surrounding words (Chen and Ji, 2011; Chisholm and Hachey, 2015; Lazic et al., 2015; Yamada et al., 2016), and global (collective) models, which are the main focus of this paper, that encourage the target entities of all mentions in a document to be topically coherent (Han et al., 2011; Cassidy et al., 2012; He et al., 2013b; Cheng and Roth, 2013; Durrett and Klein, 2014; Huang et al., 2014). Global models usually build an entity graph based on KBs to capture coherent entities for all identified mentions in a document, where the nodes are entities, and edges denote their relations. The graph provides highly discriminative semantic signals (e.g., entity relatedness) that are unavailable to local model (Eshel et al., 2017). For example (Figure 1), an EL model seemly cannot find sufficient disambiguation clues for the mention England from its surrounding words, unless it utilizes the coherence ∗ Corresponding author. This work is license"
C18-1057,Q15-1011,0,0.221198,"andidate generation obtains a set of referent entities in KB for each mention, and (ii) named entity disambiguation selects the possible candidate entity by solving a ranking problem. The key challenge lies in the ranking model that computes the relevance between candidates and the corresponding mentions based on the information both in texts and KBs (Nguyen et al., 2016). In terms of the features used for ranking, we classify existing EL models into two groups: local models to resolve mentions independently relying on textual context information from the surrounding words (Chen and Ji, 2011; Chisholm and Hachey, 2015; Lazic et al., 2015; Yamada et al., 2016), and global (collective) models, which are the main focus of this paper, that encourage the target entities of all mentions in a document to be topically coherent (Han et al., 2011; Cassidy et al., 2012; He et al., 2013b; Cheng and Roth, 2013; Durrett and Klein, 2014; Huang et al., 2014). Global models usually build an entity graph based on KBs to capture coherent entities for all identified mentions in a document, where the nodes are entities, and edges denote their relations. The graph provides highly discriminative semantic signals (e.g., entity re"
C18-1057,Q14-1037,0,0.0266306,"on the information both in texts and KBs (Nguyen et al., 2016). In terms of the features used for ranking, we classify existing EL models into two groups: local models to resolve mentions independently relying on textual context information from the surrounding words (Chen and Ji, 2011; Chisholm and Hachey, 2015; Lazic et al., 2015; Yamada et al., 2016), and global (collective) models, which are the main focus of this paper, that encourage the target entities of all mentions in a document to be topically coherent (Han et al., 2011; Cassidy et al., 2012; He et al., 2013b; Cheng and Roth, 2013; Durrett and Klein, 2014; Huang et al., 2014). Global models usually build an entity graph based on KBs to capture coherent entities for all identified mentions in a document, where the nodes are entities, and edges denote their relations. The graph provides highly discriminative semantic signals (e.g., entity relatedness) that are unavailable to local model (Eshel et al., 2017). For example (Figure 1), an EL model seemly cannot find sufficient disambiguation clues for the mention England from its surrounding words, unless it utilizes the coherence ∗ Corresponding author. This work is licensed under a Creative Common"
C18-1057,K17-1008,0,0.148188,"global (collective) models, which are the main focus of this paper, that encourage the target entities of all mentions in a document to be topically coherent (Han et al., 2011; Cassidy et al., 2012; He et al., 2013b; Cheng and Roth, 2013; Durrett and Klein, 2014; Huang et al., 2014). Global models usually build an entity graph based on KBs to capture coherent entities for all identified mentions in a document, where the nodes are entities, and edges denote their relations. The graph provides highly discriminative semantic signals (e.g., entity relatedness) that are unavailable to local model (Eshel et al., 2017). For example (Figure 1), an EL model seemly cannot find sufficient disambiguation clues for the mention England from its surrounding words, unless it utilizes the coherence ∗ Corresponding author. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 675 Proceedings of the 27th International Conference on Computational Linguistics, pages 675–686 Santa Fe, New Mexico, USA, August 20-26, 2018. information of consistent topic “cricket” among adjacent mentions England, Hussain, and Essex. Although the gl"
C18-1057,D17-1277,0,0.755279,"(Yamada et al., 2017; Gupta et al., 2017) or merely use word/entity embeddings for feature extraction and rely on another modules for collective disambiguation, which thus cannot fully utilize the power of NN models for collective EL (Globerson et al., 2016; Guo and Barbosa, 2017; Phan et al., 2018). The second drawback of the global approach has been alleviated through approximate optimization techniques, such as PageRank/random walks (Pershina et al., 2015), graph pruning (Hoffart et al., 2011), ranking SVMs (Ratinov et al., 2011), or loopy belief propagation (LBP) (Globerson et al., 2016; Ganea and Hofmann, 2017). However, these methods are not differentiable and thus difficult to be integrated into neural network models (the solution for the first limitation). To overcome the third issue of inadequate training data, (Gupta et al., 2017) has explored a massive amount of hyperlinks in Wikipedia, but these potential annotations for EL contain much noise, which may distract a naive disambiguation model (Chisholm and Hachey, 2015). In this paper, we propose a novel Neural Collective Entity Linking model (NCEL), which performs global EL combining deep neural networks with Graph Convolutional Network (GCN)"
C18-1057,D17-1284,0,0.0530058,"EL training data is usually expensive to obtain or only available in narrow domains, which results in possible overfitting issue or domain bias. To mitigate the first limitation, recent EL studies introduce neural network (NN) models due to its amazing feature abstraction and generalization ability. In such models, words/entities are represented by low dimensional vectors in a continuous space, and features for mention as well as candidate entities are automatically learned from data (Nguyen et al., 2016). However, existing NN-based methods for EL are either local models (Yamada et al., 2017; Gupta et al., 2017) or merely use word/entity embeddings for feature extraction and rely on another modules for collective disambiguation, which thus cannot fully utilize the power of NN models for collective EL (Globerson et al., 2016; Guo and Barbosa, 2017; Phan et al., 2018). The second drawback of the global approach has been alleviated through approximate optimization techniques, such as PageRank/random walks (Pershina et al., 2015), graph pruning (Hoffart et al., 2011), ranking SVMs (Ratinov et al., 2011), or loopy belief propagation (LBP) (Globerson et al., 2016; Ganea and Hofmann, 2017). However, these m"
C18-1057,P13-2006,0,0.333159,"es and the corresponding mentions based on the information both in texts and KBs (Nguyen et al., 2016). In terms of the features used for ranking, we classify existing EL models into two groups: local models to resolve mentions independently relying on textual context information from the surrounding words (Chen and Ji, 2011; Chisholm and Hachey, 2015; Lazic et al., 2015; Yamada et al., 2016), and global (collective) models, which are the main focus of this paper, that encourage the target entities of all mentions in a document to be topically coherent (Han et al., 2011; Cassidy et al., 2012; He et al., 2013b; Cheng and Roth, 2013; Durrett and Klein, 2014; Huang et al., 2014). Global models usually build an entity graph based on KBs to capture coherent entities for all identified mentions in a document, where the nodes are entities, and edges denote their relations. The graph provides highly discriminative semantic signals (e.g., entity relatedness) that are unavailable to local model (Eshel et al., 2017). For example (Figure 1), an EL model seemly cannot find sufficient disambiguation clues for the mention England from its surrounding words, unless it utilizes the coherence ∗ Corresponding autho"
C18-1057,D13-1041,0,0.123394,"es and the corresponding mentions based on the information both in texts and KBs (Nguyen et al., 2016). In terms of the features used for ranking, we classify existing EL models into two groups: local models to resolve mentions independently relying on textual context information from the surrounding words (Chen and Ji, 2011; Chisholm and Hachey, 2015; Lazic et al., 2015; Yamada et al., 2016), and global (collective) models, which are the main focus of this paper, that encourage the target entities of all mentions in a document to be topically coherent (Han et al., 2011; Cassidy et al., 2012; He et al., 2013b; Cheng and Roth, 2013; Durrett and Klein, 2014; Huang et al., 2014). Global models usually build an entity graph based on KBs to capture coherent entities for all identified mentions in a document, where the nodes are entities, and edges denote their relations. The graph provides highly discriminative semantic signals (e.g., entity relatedness) that are unavailable to local model (Eshel et al., 2017). For example (Figure 1), an EL model seemly cannot find sufficient disambiguation clues for the mention England from its surrounding words, unless it utilizes the coherence ∗ Corresponding autho"
C18-1057,D11-1072,0,0.747804,"Missing"
C18-1057,P14-1036,0,0.0173926,"n texts and KBs (Nguyen et al., 2016). In terms of the features used for ranking, we classify existing EL models into two groups: local models to resolve mentions independently relying on textual context information from the surrounding words (Chen and Ji, 2011; Chisholm and Hachey, 2015; Lazic et al., 2015; Yamada et al., 2016), and global (collective) models, which are the main focus of this paper, that encourage the target entities of all mentions in a document to be topically coherent (Han et al., 2011; Cassidy et al., 2012; He et al., 2013b; Cheng and Roth, 2013; Durrett and Klein, 2014; Huang et al., 2014). Global models usually build an entity graph based on KBs to capture coherent entities for all identified mentions in a document, where the nodes are entities, and edges denote their relations. The graph provides highly discriminative semantic signals (e.g., entity relatedness) that are unavailable to local model (Eshel et al., 2017). For example (Figure 1), an EL model seemly cannot find sufficient disambiguation clues for the mention England from its surrounding words, unless it utilizes the coherence ∗ Corresponding author. This work is licensed under a Creative Commons Attribution 4.0 Int"
C18-1057,Q15-1036,0,0.0368524,"a set of referent entities in KB for each mention, and (ii) named entity disambiguation selects the possible candidate entity by solving a ranking problem. The key challenge lies in the ranking model that computes the relevance between candidates and the corresponding mentions based on the information both in texts and KBs (Nguyen et al., 2016). In terms of the features used for ranking, we classify existing EL models into two groups: local models to resolve mentions independently relying on textual context information from the surrounding words (Chen and Ji, 2011; Chisholm and Hachey, 2015; Lazic et al., 2015; Yamada et al., 2016), and global (collective) models, which are the main focus of this paper, that encourage the target entities of all mentions in a document to be topically coherent (Han et al., 2011; Cassidy et al., 2012; He et al., 2013b; Cheng and Roth, 2013; Durrett and Klein, 2014; Huang et al., 2014). Global models usually build an entity graph based on KBs to capture coherent entities for all identified mentions in a document, where the nodes are entities, and edges denote their relations. The graph provides highly discriminative semantic signals (e.g., entity relatedness) that are"
C18-1057,Q14-1019,0,0.0258057,"inks, respectively, and NTEE (Yamada et al., 2017) achieves the best performance based on joint embeddings of words and entities. 2. Iterative model: AIDA (Hoffart et al., 2011) links entities by iteratively finding a dense subgraph. 3. Loopy Belief Propagation: Globerson (Globerson et al., 2016) and PBoH (Ganea et al., 2016) introduce LBP (Murphy et al., 1999) techniques for collective inference, and Ganea (Ganea and Hofmann, 2017) solves the global training problem via truncated fitting LBP. 4. PageRank/Random Walk: Boosting (Kulkarni et al., 2009), AGDISTISG (Usbeck et al., 2014), Babelfy (Moro et al., 2014), WAT (Piccinno and Ferragina, 2014), xLisa (Zhang and Rettinger, 2014) and WNED (Guo and Barbosa, 2017) performs PageRank (Page et al., 1999) or random walk (Tong et al., 2006) on the mention-entity graph and use the convergence score for disambiguation. 3 Our codes can be found in https://github.com/TaoMiner/NCEL 681 For fairly comparison, we report the original scores of the baselines in the papers. Following these methods, we evaluate NCEL on the following five datasets: (1) CoNLL-YAGO (Hoffart et al., 2011): the CoNLL 2003 shared task including testa of 4791 mentions in 216 documents, and"
C18-1057,N15-1026,0,0.31709,"ion as well as candidate entities are automatically learned from data (Nguyen et al., 2016). However, existing NN-based methods for EL are either local models (Yamada et al., 2017; Gupta et al., 2017) or merely use word/entity embeddings for feature extraction and rely on another modules for collective disambiguation, which thus cannot fully utilize the power of NN models for collective EL (Globerson et al., 2016; Guo and Barbosa, 2017; Phan et al., 2018). The second drawback of the global approach has been alleviated through approximate optimization techniques, such as PageRank/random walks (Pershina et al., 2015), graph pruning (Hoffart et al., 2011), ranking SVMs (Ratinov et al., 2011), or loopy belief propagation (LBP) (Globerson et al., 2016; Ganea and Hofmann, 2017). However, these methods are not differentiable and thus difficult to be integrated into neural network models (the solution for the first limitation). To overcome the third issue of inadequate training data, (Gupta et al., 2017) has explored a massive amount of hyperlinks in Wikipedia, but these potential annotations for EL contain much noise, which may distract a naive disambiguation model (Chisholm and Hachey, 2015). In this paper, w"
C18-1057,P11-1138,0,0.445414,"n et al., 2016). However, existing NN-based methods for EL are either local models (Yamada et al., 2017; Gupta et al., 2017) or merely use word/entity embeddings for feature extraction and rely on another modules for collective disambiguation, which thus cannot fully utilize the power of NN models for collective EL (Globerson et al., 2016; Guo and Barbosa, 2017; Phan et al., 2018). The second drawback of the global approach has been alleviated through approximate optimization techniques, such as PageRank/random walks (Pershina et al., 2015), graph pruning (Hoffart et al., 2011), ranking SVMs (Ratinov et al., 2011), or loopy belief propagation (LBP) (Globerson et al., 2016; Ganea and Hofmann, 2017). However, these methods are not differentiable and thus difficult to be integrated into neural network models (the solution for the first limitation). To overcome the third issue of inadequate training data, (Gupta et al., 2017) has explored a massive amount of hyperlinks in Wikipedia, but these potential annotations for EL contain much noise, which may distract a naive disambiguation model (Chisholm and Hachey, 2015). In this paper, we propose a novel Neural Collective Entity Linking model (NCEL), which perf"
C18-1057,spitkovsky-chang-2012-cross,0,0.0303845,"linking system outputs NIL for a mention when no assignment score is higher than a threshold. This is application-specific and thus outside of the scope of this work. 677 2.1 Candidate Generation Similar to previous work (Ganea and Hofmann, 2017), we use the prior probability pˆ(ei |mj ) of entity ei conditioned on mention mj both as a local feature and to generate candidate entities: Φ(mj ) = {ei |ˆ p(ei |mj ) > 0}. We compute pˆ(·) based on statistics of mention-entity pairs from: (i) Wikipedia page titles, redirect titles and hyperlinks, (ii) the dictionary derived from a large Web Corpus (Spitkovsky and Chang, 2012), and (iii) the YAGO dictionary with a uniform distribution (Hoffart et al., 2011). We pick up the maximal prior if a mention-entity pair occurs in different resources. In experiments, to optimize for memory and run time, we keep only top n entities based on pˆ(ei |mj ). In the following two sections, we will present the key components of NECL, namely feature extraction and neural network for collective entity linking. … yorkshire m i 1 … Hussain, considered mi surplus to England’s one-day requirements , struck 158, his first championshi p century of the season, m i+1 as Essex reached… surrey"
C18-1057,K16-1025,0,0.146034,"ntities in KB for each mention, and (ii) named entity disambiguation selects the possible candidate entity by solving a ranking problem. The key challenge lies in the ranking model that computes the relevance between candidates and the corresponding mentions based on the information both in texts and KBs (Nguyen et al., 2016). In terms of the features used for ranking, we classify existing EL models into two groups: local models to resolve mentions independently relying on textual context information from the surrounding words (Chen and Ji, 2011; Chisholm and Hachey, 2015; Lazic et al., 2015; Yamada et al., 2016), and global (collective) models, which are the main focus of this paper, that encourage the target entities of all mentions in a document to be topically coherent (Han et al., 2011; Cassidy et al., 2012; He et al., 2013b; Cheng and Roth, 2013; Durrett and Klein, 2014; Huang et al., 2014). Global models usually build an entity graph based on KBs to capture coherent entities for all identified mentions in a document, where the nodes are entities, and edges denote their relations. The graph provides highly discriminative semantic signals (e.g., entity relatedness) that are unavailable to local m"
C18-1057,Q17-1028,0,0.462228,"ts. 3. The annotated EL training data is usually expensive to obtain or only available in narrow domains, which results in possible overfitting issue or domain bias. To mitigate the first limitation, recent EL studies introduce neural network (NN) models due to its amazing feature abstraction and generalization ability. In such models, words/entities are represented by low dimensional vectors in a continuous space, and features for mention as well as candidate entities are automatically learned from data (Nguyen et al., 2016). However, existing NN-based methods for EL are either local models (Yamada et al., 2017; Gupta et al., 2017) or merely use word/entity embeddings for feature extraction and rely on another modules for collective disambiguation, which thus cannot fully utilize the power of NN models for collective EL (Globerson et al., 2016; Guo and Barbosa, 2017; Phan et al., 2018). The second drawback of the global approach has been alleviated through approximate optimization techniques, such as PageRank/random walks (Pershina et al., 2015), graph pruning (Hoffart et al., 2011), ranking SVMs (Ratinov et al., 2011), or loopy belief propagation (LBP) (Globerson et al., 2016; Ganea and Hofmann, 20"
C18-1099,P17-1110,0,0.0236166,"t adverarial training strategies to transfer the features of one source domain to its corresponding target domain. Inspired by Ganin et al. (2016), adversarial training has also been explored in some typical NLP tasks for multi-feature fusion. Park and Im (2016) propose a multi-modal representation learning model based on adversarial training. Then, Liu et al. (2017a) employ adversarial training to construct a multi-task learning model for text classification by extending the original binary adversarial training to the multiclass version. And a similar adversarial framework is also adapted by Chen et al. (2017) to learn features from different datasets for chinese word segmentation. In this paper, we adopt adversarial training to boost feature fusion to grasp the consistency among different languages. 3 Methodology In this section, we introduce the overall framework of our proposed AMNRE in detail. As shown in Figure 1, for each entity pair, AMNRE encodes its corresponding sentences in different languages into several semantic spaces to grasp their individual language patterns. Meanwhile, a unified space is also set up to encode consistent features among languages. By explicitly encoding the consist"
C18-1099,W14-4012,0,0.133982,"Missing"
C18-1099,N15-1151,0,0.0761329,"roblems respectively. Then, Zeng et al. (2015) attempt to integrate neural models into distant supervision. Lin et al. (2016) further propose a sentence-level attention to jointly consider all sentences containing same entity pairs for RE. The attention-based neural relation extraction (NRE) model has become a foundation for some recent works (Ji et al., 2017; Zeng et al., 2017; Liu et al., 2017b; Wu et al., 2017; Feng et al., 2018; Zeng et al., 2018). Most existing RE models are devoted to extracting relations from mono-lingual data and ignore information lying in text of multiple languages. Faruqui and Kumar (2015) and Verga et al. (2016) first attempt to adopt multi-lingual transfer learning for RE. However, both of these works learn predictive 1157 models on a new language for existing KBs, without fully leveraging semantic information in text. Then, Lin et al. (2017) construct a multi-lingual NRE (MNRE) model to jointly represent text of multiple languages to enhance RE. In this paper, we propose a novel multi-lingual NRE framework to explicitly encode language consistency and diversity into different semantic spaces, which can achieve more effective representations for RE. 2.2 Adversarial Training G"
C18-1099,P11-1055,0,0.162418,"versarial training strategy could help AMNRE to capture language-consistent relation patterns. 2 2.1 Related Works Relation Extraction Traditional supervised RE models (Zelenko et al., 2003; Socher et al., 2012; Santos et al., 2015) heavily rely on abundant amounts of high-quality annotated data. Hence, Mintz et al. (2009) propose a distantly supervised model for RE. Distant supervision aligns knowledge bases (KBs) and text to automatically annotate data, and thus distantly supervised models inevitably suffer from wrong labeling problems. To alleviate the noise issue, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms for single-label and multi-label problems respectively. Then, Zeng et al. (2015) attempt to integrate neural models into distant supervision. Lin et al. (2016) further propose a sentence-level attention to jointly consider all sentences containing same entity pairs for RE. The attention-based neural relation extraction (NRE) model has become a foundation for some recent works (Ji et al., 2017; Zeng et al., 2017; Liu et al., 2017b; Wu et al., 2017; Feng et al., 2018; Zeng et al., 2018). Most existing RE models are devoted to extracting relations"
C18-1099,P16-1200,1,0.944037,"lications, such as knowledge base construction (Zhong et al., 2015; Han et al., 2018) and question answering (Xiang et al., 2017). Recently, neural models have shown their great abilities in RE. Zeng et al. (2014) introduce a convolutional neural network (CNN) to extract relational facts with automatically learning features from text. To address the issue of lack of data, Zeng et al. (2015) incorporate multi-instance learning with a piece-wise convolutional neural network (PCNN) to extract relations in distantly supervised data. Because distant supervision suffer from wrong labeling problems, Lin et al. (2016) further employ a sentence-level selective attention to filter out those noisy sentences in distantly supervised data and achieve state-of-the-art performance. All these neural relation extraction (NRE) models merely focus on extracting relational facts from mono-lingual data, ignoring the rich information in multi-lingual data. Lin et al. (2017) propose a multi-lingual attention-based neural relation extraction (MNRE) model, which considers the consistency and complementarity in multi-lingual data. MNRE builds a sentence representation for each sentence in various languages and employs a mult"
C18-1099,P17-1004,1,0.867008,"the issue of lack of data, Zeng et al. (2015) incorporate multi-instance learning with a piece-wise convolutional neural network (PCNN) to extract relations in distantly supervised data. Because distant supervision suffer from wrong labeling problems, Lin et al. (2016) further employ a sentence-level selective attention to filter out those noisy sentences in distantly supervised data and achieve state-of-the-art performance. All these neural relation extraction (NRE) models merely focus on extracting relational facts from mono-lingual data, ignoring the rich information in multi-lingual data. Lin et al. (2017) propose a multi-lingual attention-based neural relation extraction (MNRE) model, which considers the consistency and complementarity in multi-lingual data. MNRE builds a sentence representation for each sentence in various languages and employs a multi-lingual attention to capture the pattern consistency and complementarity among languages. Although MNRE achieves great success in multi-lingual RE, it still has some problems. MNRE learns a single representation for each sentence in various languages, which cannot well capture both the consistency and diversity of relation patterns in different"
C18-1099,P17-1001,0,0.0571961,"Missing"
C18-1099,D17-1189,0,0.248594,"d models inevitably suffer from wrong labeling problems. To alleviate the noise issue, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms for single-label and multi-label problems respectively. Then, Zeng et al. (2015) attempt to integrate neural models into distant supervision. Lin et al. (2016) further propose a sentence-level attention to jointly consider all sentences containing same entity pairs for RE. The attention-based neural relation extraction (NRE) model has become a foundation for some recent works (Ji et al., 2017; Zeng et al., 2017; Liu et al., 2017b; Wu et al., 2017; Feng et al., 2018; Zeng et al., 2018). Most existing RE models are devoted to extracting relations from mono-lingual data and ignore information lying in text of multiple languages. Faruqui and Kumar (2015) and Verga et al. (2016) first attempt to adopt multi-lingual transfer learning for RE. However, both of these works learn predictive 1157 models on a new language for existing KBs, without fully leveraging semantic information in text. Then, Lin et al. (2017) construct a multi-lingual NRE (MNRE) model to jointly represent text of multiple languages to enhance RE. In this"
C18-1099,P09-1113,0,0.957424,"take Chinese and English to show the effectiveness of AMNRE. The experimental results show that AMNRE outperforms all baseline models significantly by explicitly encoding the consistency and diversity among languages. And we further give a case study and an ablation study to demonstrate the adversarial training strategy could help AMNRE to capture language-consistent relation patterns. 2 2.1 Related Works Relation Extraction Traditional supervised RE models (Zelenko et al., 2003; Socher et al., 2012; Santos et al., 2015) heavily rely on abundant amounts of high-quality annotated data. Hence, Mintz et al. (2009) propose a distantly supervised model for RE. Distant supervision aligns knowledge bases (KBs) and text to automatically annotate data, and thus distantly supervised models inevitably suffer from wrong labeling problems. To alleviate the noise issue, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms for single-label and multi-label problems respectively. Then, Zeng et al. (2015) attempt to integrate neural models into distant supervision. Lin et al. (2016) further propose a sentence-level attention to jointly consider all sentences containing same"
C18-1099,P15-1061,0,0.0339267,"dividual representations and consistent representations for each language. In experiments, we take Chinese and English to show the effectiveness of AMNRE. The experimental results show that AMNRE outperforms all baseline models significantly by explicitly encoding the consistency and diversity among languages. And we further give a case study and an ablation study to demonstrate the adversarial training strategy could help AMNRE to capture language-consistent relation patterns. 2 2.1 Related Works Relation Extraction Traditional supervised RE models (Zelenko et al., 2003; Socher et al., 2012; Santos et al., 2015) heavily rely on abundant amounts of high-quality annotated data. Hence, Mintz et al. (2009) propose a distantly supervised model for RE. Distant supervision aligns knowledge bases (KBs) and text to automatically annotate data, and thus distantly supervised models inevitably suffer from wrong labeling problems. To alleviate the noise issue, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms for single-label and multi-label problems respectively. Then, Zeng et al. (2015) attempt to integrate neural models into distant supervision. Lin et al. (2016)"
C18-1099,D12-1110,0,0.069634,"ifferences between individual representations and consistent representations for each language. In experiments, we take Chinese and English to show the effectiveness of AMNRE. The experimental results show that AMNRE outperforms all baseline models significantly by explicitly encoding the consistency and diversity among languages. And we further give a case study and an ablation study to demonstrate the adversarial training strategy could help AMNRE to capture language-consistent relation patterns. 2 2.1 Related Works Relation Extraction Traditional supervised RE models (Zelenko et al., 2003; Socher et al., 2012; Santos et al., 2015) heavily rely on abundant amounts of high-quality annotated data. Hence, Mintz et al. (2009) propose a distantly supervised model for RE. Distant supervision aligns knowledge bases (KBs) and text to automatically annotate data, and thus distantly supervised models inevitably suffer from wrong labeling problems. To alleviate the noise issue, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms for single-label and multi-label problems respectively. Then, Zeng et al. (2015) attempt to integrate neural models into distant supervisi"
C18-1099,N16-1103,0,0.0745242,"eng et al. (2015) attempt to integrate neural models into distant supervision. Lin et al. (2016) further propose a sentence-level attention to jointly consider all sentences containing same entity pairs for RE. The attention-based neural relation extraction (NRE) model has become a foundation for some recent works (Ji et al., 2017; Zeng et al., 2017; Liu et al., 2017b; Wu et al., 2017; Feng et al., 2018; Zeng et al., 2018). Most existing RE models are devoted to extracting relations from mono-lingual data and ignore information lying in text of multiple languages. Faruqui and Kumar (2015) and Verga et al. (2016) first attempt to adopt multi-lingual transfer learning for RE. However, both of these works learn predictive 1157 models on a new language for existing KBs, without fully leveraging semantic information in text. Then, Lin et al. (2017) construct a multi-lingual NRE (MNRE) model to jointly represent text of multiple languages to enhance RE. In this paper, we propose a novel multi-lingual NRE framework to explicitly encode language consistency and diversity into different semantic spaces, which can achieve more effective representations for RE. 2.2 Adversarial Training Goodfellow et al. (2015)"
C18-1099,D17-1187,0,0.141618,"suffer from wrong labeling problems. To alleviate the noise issue, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms for single-label and multi-label problems respectively. Then, Zeng et al. (2015) attempt to integrate neural models into distant supervision. Lin et al. (2016) further propose a sentence-level attention to jointly consider all sentences containing same entity pairs for RE. The attention-based neural relation extraction (NRE) model has become a foundation for some recent works (Ji et al., 2017; Zeng et al., 2017; Liu et al., 2017b; Wu et al., 2017; Feng et al., 2018; Zeng et al., 2018). Most existing RE models are devoted to extracting relations from mono-lingual data and ignore information lying in text of multiple languages. Faruqui and Kumar (2015) and Verga et al. (2016) first attempt to adopt multi-lingual transfer learning for RE. However, both of these works learn predictive 1157 models on a new language for existing KBs, without fully leveraging semantic information in text. Then, Lin et al. (2017) construct a multi-lingual NRE (MNRE) model to jointly represent text of multiple languages to enhance RE. In this paper, we propose"
C18-1099,C14-1220,0,0.647507,"nlp/AMNRE. 1 Introduction Relation extraction (RE) is a crucial task in NLP, which aims to extract semantic relations between entity pairs from the sentences containing them. For example, given an entity pair (Bill Gates, Microsoft) and a sentence “Bill Gates is the co-founder and CEO of Microsoft”, we want to figure out the relation Founder between the two entities. RE can potentially benefit many applications, such as knowledge base construction (Zhong et al., 2015; Han et al., 2018) and question answering (Xiang et al., 2017). Recently, neural models have shown their great abilities in RE. Zeng et al. (2014) introduce a convolutional neural network (CNN) to extract relational facts with automatically learning features from text. To address the issue of lack of data, Zeng et al. (2015) incorporate multi-instance learning with a piece-wise convolutional neural network (PCNN) to extract relations in distantly supervised data. Because distant supervision suffer from wrong labeling problems, Lin et al. (2016) further employ a sentence-level selective attention to filter out those noisy sentences in distantly supervised data and achieve state-of-the-art performance. All these neural relation extraction"
C18-1099,D15-1203,0,0.599989,"xample, given an entity pair (Bill Gates, Microsoft) and a sentence “Bill Gates is the co-founder and CEO of Microsoft”, we want to figure out the relation Founder between the two entities. RE can potentially benefit many applications, such as knowledge base construction (Zhong et al., 2015; Han et al., 2018) and question answering (Xiang et al., 2017). Recently, neural models have shown their great abilities in RE. Zeng et al. (2014) introduce a convolutional neural network (CNN) to extract relational facts with automatically learning features from text. To address the issue of lack of data, Zeng et al. (2015) incorporate multi-instance learning with a piece-wise convolutional neural network (PCNN) to extract relations in distantly supervised data. Because distant supervision suffer from wrong labeling problems, Lin et al. (2016) further employ a sentence-level selective attention to filter out those noisy sentences in distantly supervised data and achieve state-of-the-art performance. All these neural relation extraction (NRE) models merely focus on extracting relational facts from mono-lingual data, ignoring the rich information in multi-lingual data. Lin et al. (2017) propose a multi-lingual att"
C18-1099,D17-1186,1,0.868287,"distantly supervised models inevitably suffer from wrong labeling problems. To alleviate the noise issue, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms for single-label and multi-label problems respectively. Then, Zeng et al. (2015) attempt to integrate neural models into distant supervision. Lin et al. (2016) further propose a sentence-level attention to jointly consider all sentences containing same entity pairs for RE. The attention-based neural relation extraction (NRE) model has become a foundation for some recent works (Ji et al., 2017; Zeng et al., 2017; Liu et al., 2017b; Wu et al., 2017; Feng et al., 2018; Zeng et al., 2018). Most existing RE models are devoted to extracting relations from mono-lingual data and ignore information lying in text of multiple languages. Faruqui and Kumar (2015) and Verga et al. (2016) first attempt to adopt multi-lingual transfer learning for RE. However, both of these works learn predictive 1157 models on a new language for existing KBs, without fully leveraging semantic information in text. Then, Lin et al. (2017) construct a multi-lingual NRE (MNRE) model to jointly represent text of multiple languages to e"
C18-1099,D15-1031,0,0.0138338,"at our AMNRE model significantly outperforms the state-of-the-art models. The source code of this paper can be obtained from https://github.com/thunlp/AMNRE. 1 Introduction Relation extraction (RE) is a crucial task in NLP, which aims to extract semantic relations between entity pairs from the sentences containing them. For example, given an entity pair (Bill Gates, Microsoft) and a sentence “Bill Gates is the co-founder and CEO of Microsoft”, we want to figure out the relation Founder between the two entities. RE can potentially benefit many applications, such as knowledge base construction (Zhong et al., 2015; Han et al., 2018) and question answering (Xiang et al., 2017). Recently, neural models have shown their great abilities in RE. Zeng et al. (2014) introduce a convolutional neural network (CNN) to extract relational facts with automatically learning features from text. To address the issue of lack of data, Zeng et al. (2015) incorporate multi-instance learning with a piece-wise convolutional neural network (PCNN) to extract relations in distantly supervised data. Because distant supervision suffer from wrong labeling problems, Lin et al. (2016) further employ a sentence-level selective attent"
D09-1027,W08-1404,0,0.48597,"he web era with enormous information. The rest of the paper is organized as follows. In Section 2, we introduce and discuss the related work in this area. In Section 3, we give an overview of our method for keyphrase extraction. From Section 4 to Section 7, the algorithm is described in detail. Empirical experiment results are demonstrated in Section 8, followed by our conclusions and plans for future work in Section 9. 2 Starting with TextRank (Mihalcea and Tarau, 2004), graph-based ranking methods are becoming the most widely used unsupervised approach for keyphrase extraction. The work in (Litvak and Last, 2008) applies HITS algorithm on the word graph of a document under the assumption that the top-ranked nodes should be the document keywords. Experiments show that classificationbased supervised method provides the highest keyword identification accuracy, while the HITS algorithm gets the highest F-measure. Work in (Huang et al., 2006) also considers each document as a term graph where the structural dynamics of these graphs can be used to identify keyphrases. Wan and Xiao (Wan and Xiao, 2008b) use a small number of nearest neighbor documents to provide more knowledge to improve graph-based keyphras"
D09-1027,C08-1122,0,0.631285,"g methods are becoming the most widely used unsupervised approach for keyphrase extraction. The work in (Litvak and Last, 2008) applies HITS algorithm on the word graph of a document under the assumption that the top-ranked nodes should be the document keywords. Experiments show that classificationbased supervised method provides the highest keyword identification accuracy, while the HITS algorithm gets the highest F-measure. Work in (Huang et al., 2006) also considers each document as a term graph where the structural dynamics of these graphs can be used to identify keyphrases. Wan and Xiao (Wan and Xiao, 2008b) use a small number of nearest neighbor documents to provide more knowledge to improve graph-based keyphrase extraction algorithm for single document. Motivated by similar idea, Wan and Xiao (Wan and Xiao, 2008a) propose to adopt clustering methods to find a small number of similar documents to provide more knowledge for building word graphs for keyword extraction. Moreover, after our submission of this paper, we find that a method using community detection on semantic term graphs is proposed for keyphrase extraction from multi-theme documents (Grineva et al., 2009). In addition, some practi"
D09-1027,W04-3252,0,\N,Missing
D09-1027,W03-1028,0,\N,Missing
D10-1036,W03-1028,0,0.911462,"h P word w as its random jump probability with w∈V pz (w) = 1. The words that are more relevant to topic z will be assigned larger probabilities when performing the PageRank. For topic z, the topic-specific PageRank scores are defined as follows: Both PageRank and TPR are all iterative algorithms. We terminate the algorithms when the number of iterations reaches 100 or the difference of each vertex between two neighbor iterations is less than 0.001. 3.3 Extract Keyphrases Using Ranking Scores After obtaining word ranking scores using TPR, we begin to rank candidate keyphrases. As reported in (Hulth, 2003), most manually assigned keyphrases turn out to be noun phrases. We thus select noun phrases from a document as candidate keyphrases for ranking. The candidate keyphrases of a document is obtained as follows. The document is first tokenized. After that, we annotate the document with partof-speech (POS) tags 1 . Third, we extract noun phrases with pattern (adjective)*(noun)+, which represents zero or more adjectives followed by one or more nouns. We regard these noun phrases as candidate keyphrases. After identifying candidate keyphrases, we rank them using the ranking scores obtained by TPR. I"
D10-1036,W08-1404,0,0.781596,"ent, alleged document, Arab government, slaying Wazir, State Department spokesman Charles Redman, Khalil Wazir(+) TPR, Rank 3 Topic on “terrorism” terrorist attacks(+), PLO leader Yasser Arafat(+), Abu Jihad, United States(+), alleged document, U.S. government document, Palestine Liberation Organization leader, State Department spokesman Charles Redman, political assassination(+), full cooperation Table 6: Extracted keyphrases by TPR. Starting with TextRank (Mihalcea and Tarau, 2004), graph-based ranking methods are becoming the most widely used unsupervised approach for keyphrase extraction. Litvak and Last (2008) applied HITS algorithm on the word graph of a document for keyphrase extraction. Although HITS itself worked the similar performance to PageRank, we plan to explore the integration of topics and HITS in future work. Wan (2008b; 2008a) used a small number of nearest neighbor documents to provide more knowledge for keyphrase extraction. Some methods used clustering techniques on word graphs for keyphrase extraction (Grineva et al., 2009; Liu et al., 2009). The clustering-based method performed well on short abstracts (with F-measure 0.382 on RESEARCH) but poorly on long articles (NEWS with F-me"
D10-1036,D09-1027,1,0.436746,"Mihalcea and Tarau, 2004), graph-based ranking methods are becoming the most widely used unsupervised approach for keyphrase extraction. Litvak and Last (2008) applied HITS algorithm on the word graph of a document for keyphrase extraction. Although HITS itself worked the similar performance to PageRank, we plan to explore the integration of topics and HITS in future work. Wan (2008b; 2008a) used a small number of nearest neighbor documents to provide more knowledge for keyphrase extraction. Some methods used clustering techniques on word graphs for keyphrase extraction (Grineva et al., 2009; Liu et al., 2009). The clustering-based method performed well on short abstracts (with F-measure 0.382 on RESEARCH) but poorly on long articles (NEWS with F-measure score 0.216) due to two non-trivial issues: (1) how to determine the number of clus374 TFIDF (+5) PLO leader Yasser Arafat(+), PLO attacks, PLO offices, PLO officials(+), PLO leaders, Abu Jihad, terrorist attacks(+), Khalil Wazir(+), slaying wazir, political assassination(+) PageRank (+3) PLO leader Yasser Arafat(+), PLO officials(+), PLO attacks, United States(+), PLO offices, PLO leaders, State Department spokesman Charles Redman, U.S. government"
D10-1036,J00-2011,0,0.531618,"nd extract the top ranked ones as keyphrases. Experimental results show that TPR outperforms state-of-the-art keyphrase extraction methods on two datasets under various evaluation metrics. 1 Introduction Keyphrases are defined as a set of terms in a document that give a brief summary of its content for readers. Automatic keyphrase extraction is widely used in information retrieval and digital library (Turney, 2000; Nguyen and Kan, 2007). Keyphrase extraction is also an essential step in various tasks of natural language processing such as document categorization, clustering and summarization (Manning and Schutze, 2000). There are two principled approaches to extracting keyphrases: supervised and unsupervised. The supervised approach (Turney, 1999) regards keyphrase extraction as a classification task, in which a model is trained to determine whether a candidate phrase is a keyphrase. Supervised methods require a document set with human-assigned keyphrases as training set. In Web era, articles increase exponentially and change dynamically, which demands keyphrase extraction to be efficient and adaptable. However, since human labeling is time consuming, it is impractical to label training set from time to tim"
D10-1036,voorhees-tice-2000-trec,0,0.0555382,"order of extracted keyphrases into account. To address the problem, we select the following two additional metrics. One metric is binary preference measure (Bpref) (Buckley and Voorhees, 2004). Bpref is desirable to evaluate the performance considering the order in which the extracted keyphrases are ranked. For a document, if there are R correct keyphrases within M extracted keyphrases by a method, in which r is a correct keyphrase and n is an incorrect keyphrase, Bpref is defined as follows, Bpref = 1 X |n ranked higher than r| . (7) 1− R M r∈R The other metric is mean reciprocal rank (MRR) (Voorhees, 2000) which is used to evaluate how the first correct keyphrase for each document is ranked. For a document d, rankd is denoted as the rank of the first correct keyphrase with all extracted keyphrases, MRR is defined as follows, MRR = 1 X 1 , |D| rankd (8) d∈D 4.2 Evaluation Metrics For evaluation, the words in both standard and extracted keyphrases are reduced to base forms using 2 http://wanxiaojun1979.googlepages.com. It was obtained from the author. 4 http://en.wikipedia.org/wiki/Wikipedia_ database. 3 370 where D is the document set for keyphrase extraction. Note that although the evaluation s"
D10-1036,C08-1122,0,0.828591,"t noun phrases with pattern (adjective)*(noun)+, which represents zero or more adjectives followed by one or more nouns. We regard these noun phrases as candidate keyphrases. After identifying candidate keyphrases, we rank them using the ranking scores obtained by TPR. In PageRank for keyphrase extraction, the ranking X e(wj , wi ) Rz (wi ) = λ Rz (wj )+(1−λ)pz (wi ). score of a candidate keyphrase p is computed by O(wj ) j:wj →wi summing up the ranking scores of all words within (3) the phrase: R(p) = P wi ∈p R(wi ) (Mihalcea and TaIn Fig. 1, we show an example with two topics. In rau, 2004; Wan and Xiao, 2008a; Wan and Xiao, this figure, we use the size of circles to indicate how 2008b). Then candidate keyphrases are ranked in relevant the word is to the topic. In the PageRanks descending order of ranking scores. The top M canof the two topics, high preference values will be as- didates are selected as keyphrases. signed to different words with respect to the topic. In TPR for keyphrase extraction, we first comFinally, the words will get different PageRank val- pute the ranking scores of candidate keyphrases sepues in the two PageRanks. arately for each topic. That is for each topic z we The setti"
D10-1036,W04-3252,0,\N,Missing
D11-1146,P00-1041,0,0.0105252,"in many tasks of information retrieval and natural language processing to bridge the vocabulary gap between two types of objects. Some typical tasks are document information retrieval (Berger and Lafferty, 1999; Murdock and Croft, 2004; Karimzadehgan and Zhai, 2010), question answering (Berger et al., 2000; Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Surdeanu et al., 2008; Xue et al., 2008), query expansions (Riezler et al., 2007; Riezler et al., 2008; Riezler and Liu, 2010), paraphrasing (Quirk et al., 2004; Zhao et al., 2010a; Zhao et al., 2010b), summarization (Banko et al., 2000), collocation extraction (Liu et al., 2009b; Liu et al., 2010c), keyphrase extraction (Liu et al., 2011), sentiment analysis (Dalvi et al., 2009), computational advertising (Ravi et al., 2010), and image/video annotation and retrieval (Duygulu et al., 2002; Jeon et al., 2003). 3 Word Trigger Method for Social Tag Suggestion 3.1 Method Framework We describe the word trigger method (WTM) for social tag suggestion as a 3-stage process: 1. Preparing description-annotation pairs. Given a collection of annotated resources, we first prepare description-annotation pairs for learning translation probab"
D11-1146,J93-2003,0,0.068816,"in Table 1 are triggered to the tags in annotation. • Some tags in the annotation do appear in the corresponding description, but they may not be statistically significant. Figure 1: An example of the word trigger method for suggesting tags given a description. • Some tags may even not appear in the description. 2 It is not trivial to reduce the vocabulary gap and find the semantic correspondence between descriptions and annotations. By regarding both the description and the annotation as parallel summaries of a resource, we use word alignment models in statistical machine translation (SMT) (Brown et al., 1993) to estimate the translation probabilities between the words in descriptions and annotations. SMT has been successfully applied in many applications to bridge vocabulary gap. For detailed descriptions of related work, readers can refer to Section 2.2. In this paper, besides employing word alignment models to social tagging, we also propose a method to efficiently build description-annotation pairs for sufficient learning translation probabilities by word alignment models. Based on the learned translation probabilities between words in descriptions and annotations, 1578 Related Work 2.1 Social"
D11-1146,P03-1003,0,0.0202143,"ggested tags do not necessarily appear in the given resource description. We can thus regard social tag suggestion as a task of selecting appropriate tags from a controlled tag vocabulary for the given resource description. 2.2 Applications of SMT SMT techniques have been successfully used in many tasks of information retrieval and natural language processing to bridge the vocabulary gap between two types of objects. Some typical tasks are document information retrieval (Berger and Lafferty, 1999; Murdock and Croft, 2004; Karimzadehgan and Zhai, 2010), question answering (Berger et al., 2000; Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Surdeanu et al., 2008; Xue et al., 2008), query expansions (Riezler et al., 2007; Riezler et al., 2008; Riezler and Liu, 2010), paraphrasing (Quirk et al., 2004; Zhao et al., 2010a; Zhao et al., 2010b), summarization (Banko et al., 2000), collocation extraction (Liu et al., 2009b; Liu et al., 2010c), keyphrase extraction (Liu et al., 2011), sentiment analysis (Dalvi et al., 2009), computational advertising (Ravi et al., 2010), and image/video annotation and retrieval (Duygulu et al., 2002; Jeon et al., 2003). 3 Word Trigger Method for Social Tag"
D11-1146,D09-1027,1,0.796786,"those specific tags such as named entities, e.g., the tags “Dumas” and “Count of Monte Cristo” in Table 1. To remedy the problem, Si et al. (2010) proposed a generative model, Tag Allocation Model (TAM), which considers the words in descriptions as the possible topics to generate tags. However, TAM assumes each tag can only have at most one word as its reason. This is against the fact that a tag may be annotated triggered by multiple words in the description. It should also be noted that social tag suggestion is different from automatic keyphrase extraction (Turney, 2000; Frank et al., 1999; Liu et al., 2009a; Liu et al., 2010b; Liu et al., 2011). Keyphrase extraction aims at selecting terms from the given document to represent the main topics of the document. On the contrary, in social tag suggestion, the suggested tags do not necessarily appear in the given resource description. We can thus regard social tag suggestion as a task of selecting appropriate tags from a controlled tag vocabulary for the given resource description. 2.2 Applications of SMT SMT techniques have been successfully used in many tasks of information retrieval and natural language processing to bridge the vocabulary gap betw"
D11-1146,D09-1051,0,0.0140124,"those specific tags such as named entities, e.g., the tags “Dumas” and “Count of Monte Cristo” in Table 1. To remedy the problem, Si et al. (2010) proposed a generative model, Tag Allocation Model (TAM), which considers the words in descriptions as the possible topics to generate tags. However, TAM assumes each tag can only have at most one word as its reason. This is against the fact that a tag may be annotated triggered by multiple words in the description. It should also be noted that social tag suggestion is different from automatic keyphrase extraction (Turney, 2000; Frank et al., 1999; Liu et al., 2009a; Liu et al., 2010b; Liu et al., 2011). Keyphrase extraction aims at selecting terms from the given document to represent the main topics of the document. On the contrary, in social tag suggestion, the suggested tags do not necessarily appear in the given resource description. We can thus regard social tag suggestion as a task of selecting appropriate tags from a controlled tag vocabulary for the given resource description. 2.2 Applications of SMT SMT techniques have been successfully used in many tasks of information retrieval and natural language processing to bridge the vocabulary gap betw"
D11-1146,J10-3002,0,0.101687,"s such as named entities, e.g., the tags “Dumas” and “Count of Monte Cristo” in Table 1. To remedy the problem, Si et al. (2010) proposed a generative model, Tag Allocation Model (TAM), which considers the words in descriptions as the possible topics to generate tags. However, TAM assumes each tag can only have at most one word as its reason. This is against the fact that a tag may be annotated triggered by multiple words in the description. It should also be noted that social tag suggestion is different from automatic keyphrase extraction (Turney, 2000; Frank et al., 1999; Liu et al., 2009a; Liu et al., 2010b; Liu et al., 2011). Keyphrase extraction aims at selecting terms from the given document to represent the main topics of the document. On the contrary, in social tag suggestion, the suggested tags do not necessarily appear in the given resource description. We can thus regard social tag suggestion as a task of selecting appropriate tags from a controlled tag vocabulary for the given resource description. 2.2 Applications of SMT SMT techniques have been successfully used in many tasks of information retrieval and natural language processing to bridge the vocabulary gap between two types of ob"
D11-1146,D10-1036,1,0.904207,"s such as named entities, e.g., the tags “Dumas” and “Count of Monte Cristo” in Table 1. To remedy the problem, Si et al. (2010) proposed a generative model, Tag Allocation Model (TAM), which considers the words in descriptions as the possible topics to generate tags. However, TAM assumes each tag can only have at most one word as its reason. This is against the fact that a tag may be annotated triggered by multiple words in the description. It should also be noted that social tag suggestion is different from automatic keyphrase extraction (Turney, 2000; Frank et al., 1999; Liu et al., 2009a; Liu et al., 2010b; Liu et al., 2011). Keyphrase extraction aims at selecting terms from the given document to represent the main topics of the document. On the contrary, in social tag suggestion, the suggested tags do not necessarily appear in the given resource description. We can thus regard social tag suggestion as a task of selecting appropriate tags from a controlled tag vocabulary for the given resource description. 2.2 Applications of SMT SMT techniques have been successfully used in many tasks of information retrieval and natural language processing to bridge the vocabulary gap between two types of ob"
D11-1146,P10-1085,0,0.0388215,"s such as named entities, e.g., the tags “Dumas” and “Count of Monte Cristo” in Table 1. To remedy the problem, Si et al. (2010) proposed a generative model, Tag Allocation Model (TAM), which considers the words in descriptions as the possible topics to generate tags. However, TAM assumes each tag can only have at most one word as its reason. This is against the fact that a tag may be annotated triggered by multiple words in the description. It should also be noted that social tag suggestion is different from automatic keyphrase extraction (Turney, 2000; Frank et al., 1999; Liu et al., 2009a; Liu et al., 2010b; Liu et al., 2011). Keyphrase extraction aims at selecting terms from the given document to represent the main topics of the document. On the contrary, in social tag suggestion, the suggested tags do not necessarily appear in the given resource description. We can thus regard social tag suggestion as a task of selecting appropriate tags from a controlled tag vocabulary for the given resource description. 2.2 Applications of SMT SMT techniques have been successfully used in many tasks of information retrieval and natural language processing to bridge the vocabulary gap between two types of ob"
D11-1146,W11-0316,1,0.776677,"ities, e.g., the tags “Dumas” and “Count of Monte Cristo” in Table 1. To remedy the problem, Si et al. (2010) proposed a generative model, Tag Allocation Model (TAM), which considers the words in descriptions as the possible topics to generate tags. However, TAM assumes each tag can only have at most one word as its reason. This is against the fact that a tag may be annotated triggered by multiple words in the description. It should also be noted that social tag suggestion is different from automatic keyphrase extraction (Turney, 2000; Frank et al., 1999; Liu et al., 2009a; Liu et al., 2010b; Liu et al., 2011). Keyphrase extraction aims at selecting terms from the given document to represent the main topics of the document. On the contrary, in social tag suggestion, the suggested tags do not necessarily appear in the given resource description. We can thus regard social tag suggestion as a task of selecting appropriate tags from a controlled tag vocabulary for the given resource description. 2.2 Applications of SMT SMT techniques have been successfully used in many tasks of information retrieval and natural language processing to bridge the vocabulary gap between two types of objects. Some typical"
D11-1146,J03-1002,0,0.0208445,"iption is usually limited to hundreds of words. Meanwhile, it is common that some popular resources are annotated by multiple users with thousands of tags. For example, the tag Dumas is annotated by 2, 748 users for the book in Table 1. We have to deal with the lengthunbalance between a resource description and its corresponding annotation for two reasons. • It is impossible to list all annotated tags on the annotation side of a description-annotation pair. The performance of word alignment models will also suffer from the unbalanced length of sentence pairs in the parallel training data set (Och and Ney, 2003). • Moreover, the annotated tags may have different importance for the resource. It would be unfair to treat these tags without distinction. Here we propose a sampling method to prepare length-balanced description-annotation pairs for word alignment. The basic idea is to sample a bag of tags from the annotation according to tag weights and make the generated bag of tags with comparable length with the description. We consider two parameters when sampling tags. First, we have to select a tag weighting type for sampling. In this paper, we investigate two straightforward sampling types, including"
D11-1146,W04-3219,0,0.0127481,"ription. 2.2 Applications of SMT SMT techniques have been successfully used in many tasks of information retrieval and natural language processing to bridge the vocabulary gap between two types of objects. Some typical tasks are document information retrieval (Berger and Lafferty, 1999; Murdock and Croft, 2004; Karimzadehgan and Zhai, 2010), question answering (Berger et al., 2000; Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Surdeanu et al., 2008; Xue et al., 2008), query expansions (Riezler et al., 2007; Riezler et al., 2008; Riezler and Liu, 2010), paraphrasing (Quirk et al., 2004; Zhao et al., 2010a; Zhao et al., 2010b), summarization (Banko et al., 2000), collocation extraction (Liu et al., 2009b; Liu et al., 2010c), keyphrase extraction (Liu et al., 2011), sentiment analysis (Dalvi et al., 2009), computational advertising (Ravi et al., 2010), and image/video annotation and retrieval (Duygulu et al., 2002; Jeon et al., 2003). 3 Word Trigger Method for Social Tag Suggestion 3.1 Method Framework We describe the word trigger method (WTM) for social tag suggestion as a 3-stage process: 1. Preparing description-annotation pairs. Given a collection of annotated resources,"
D11-1146,J10-3010,0,0.0367492,"vocabulary for the given resource description. 2.2 Applications of SMT SMT techniques have been successfully used in many tasks of information retrieval and natural language processing to bridge the vocabulary gap between two types of objects. Some typical tasks are document information retrieval (Berger and Lafferty, 1999; Murdock and Croft, 2004; Karimzadehgan and Zhai, 2010), question answering (Berger et al., 2000; Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Surdeanu et al., 2008; Xue et al., 2008), query expansions (Riezler et al., 2007; Riezler et al., 2008; Riezler and Liu, 2010), paraphrasing (Quirk et al., 2004; Zhao et al., 2010a; Zhao et al., 2010b), summarization (Banko et al., 2000), collocation extraction (Liu et al., 2009b; Liu et al., 2010c), keyphrase extraction (Liu et al., 2011), sentiment analysis (Dalvi et al., 2009), computational advertising (Ravi et al., 2010), and image/video annotation and retrieval (Duygulu et al., 2002; Jeon et al., 2003). 3 Word Trigger Method for Social Tag Suggestion 3.1 Method Framework We describe the word trigger method (WTM) for social tag suggestion as a 3-stage process: 1. Preparing description-annotation pairs. Given a c"
D11-1146,P07-1059,0,0.00788117,"resource description. We can thus regard social tag suggestion as a task of selecting appropriate tags from a controlled tag vocabulary for the given resource description. 2.2 Applications of SMT SMT techniques have been successfully used in many tasks of information retrieval and natural language processing to bridge the vocabulary gap between two types of objects. Some typical tasks are document information retrieval (Berger and Lafferty, 1999; Murdock and Croft, 2004; Karimzadehgan and Zhai, 2010), question answering (Berger et al., 2000; Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Surdeanu et al., 2008; Xue et al., 2008), query expansions (Riezler et al., 2007; Riezler et al., 2008; Riezler and Liu, 2010), paraphrasing (Quirk et al., 2004; Zhao et al., 2010a; Zhao et al., 2010b), summarization (Banko et al., 2000), collocation extraction (Liu et al., 2009b; Liu et al., 2010c), keyphrase extraction (Liu et al., 2011), sentiment analysis (Dalvi et al., 2009), computational advertising (Ravi et al., 2010), and image/video annotation and retrieval (Duygulu et al., 2002; Jeon et al., 2003). 3 Word Trigger Method for Social Tag Suggestion 3.1 Method Framework We describe th"
D11-1146,C08-1093,0,0.0276697,"from a controlled tag vocabulary for the given resource description. 2.2 Applications of SMT SMT techniques have been successfully used in many tasks of information retrieval and natural language processing to bridge the vocabulary gap between two types of objects. Some typical tasks are document information retrieval (Berger and Lafferty, 1999; Murdock and Croft, 2004; Karimzadehgan and Zhai, 2010), question answering (Berger et al., 2000; Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Surdeanu et al., 2008; Xue et al., 2008), query expansions (Riezler et al., 2007; Riezler et al., 2008; Riezler and Liu, 2010), paraphrasing (Quirk et al., 2004; Zhao et al., 2010a; Zhao et al., 2010b), summarization (Banko et al., 2000), collocation extraction (Liu et al., 2009b; Liu et al., 2010c), keyphrase extraction (Liu et al., 2011), sentiment analysis (Dalvi et al., 2009), computational advertising (Ravi et al., 2010), and image/video annotation and retrieval (Duygulu et al., 2002; Jeon et al., 2003). 3 Word Trigger Method for Social Tag Suggestion 3.1 Method Framework We describe the word trigger method (WTM) for social tag suggestion as a 3-stage process: 1. Preparing description-ann"
D11-1146,P08-1082,0,0.0133568,"We can thus regard social tag suggestion as a task of selecting appropriate tags from a controlled tag vocabulary for the given resource description. 2.2 Applications of SMT SMT techniques have been successfully used in many tasks of information retrieval and natural language processing to bridge the vocabulary gap between two types of objects. Some typical tasks are document information retrieval (Berger and Lafferty, 1999; Murdock and Croft, 2004; Karimzadehgan and Zhai, 2010), question answering (Berger et al., 2000; Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Surdeanu et al., 2008; Xue et al., 2008), query expansions (Riezler et al., 2007; Riezler et al., 2008; Riezler and Liu, 2010), paraphrasing (Quirk et al., 2004; Zhao et al., 2010a; Zhao et al., 2010b), summarization (Banko et al., 2000), collocation extraction (Liu et al., 2009b; Liu et al., 2010c), keyphrase extraction (Liu et al., 2011), sentiment analysis (Dalvi et al., 2009), computational advertising (Ravi et al., 2010), and image/video annotation and retrieval (Duygulu et al., 2002; Jeon et al., 2003). 3 Word Trigger Method for Social Tag Suggestion 3.1 Method Framework We describe the word trigger method ("
D11-1146,C10-1149,0,0.0122965,"tions of SMT SMT techniques have been successfully used in many tasks of information retrieval and natural language processing to bridge the vocabulary gap between two types of objects. Some typical tasks are document information retrieval (Berger and Lafferty, 1999; Murdock and Croft, 2004; Karimzadehgan and Zhai, 2010), question answering (Berger et al., 2000; Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Surdeanu et al., 2008; Xue et al., 2008), query expansions (Riezler et al., 2007; Riezler et al., 2008; Riezler and Liu, 2010), paraphrasing (Quirk et al., 2004; Zhao et al., 2010a; Zhao et al., 2010b), summarization (Banko et al., 2000), collocation extraction (Liu et al., 2009b; Liu et al., 2010c), keyphrase extraction (Liu et al., 2011), sentiment analysis (Dalvi et al., 2009), computational advertising (Ravi et al., 2010), and image/video annotation and retrieval (Duygulu et al., 2002; Jeon et al., 2003). 3 Word Trigger Method for Social Tag Suggestion 3.1 Method Framework We describe the word trigger method (WTM) for social tag suggestion as a 3-stage process: 1. Preparing description-annotation pairs. Given a collection of annotated resources, we first prepare de"
D11-1146,C10-1148,0,0.013484,"tions of SMT SMT techniques have been successfully used in many tasks of information retrieval and natural language processing to bridge the vocabulary gap between two types of objects. Some typical tasks are document information retrieval (Berger and Lafferty, 1999; Murdock and Croft, 2004; Karimzadehgan and Zhai, 2010), question answering (Berger et al., 2000; Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Surdeanu et al., 2008; Xue et al., 2008), query expansions (Riezler et al., 2007; Riezler et al., 2008; Riezler and Liu, 2010), paraphrasing (Quirk et al., 2004; Zhao et al., 2010a; Zhao et al., 2010b), summarization (Banko et al., 2000), collocation extraction (Liu et al., 2009b; Liu et al., 2010c), keyphrase extraction (Liu et al., 2011), sentiment analysis (Dalvi et al., 2009), computational advertising (Ravi et al., 2010), and image/video annotation and retrieval (Duygulu et al., 2002; Jeon et al., 2003). 3 Word Trigger Method for Social Tag Suggestion 3.1 Method Framework We describe the word trigger method (WTM) for social tag suggestion as a 3-stage process: 1. Preparing description-annotation pairs. Given a collection of annotated resources, we first prepare de"
D11-1146,W04-3252,0,\N,Missing
D14-1110,S07-1054,0,0.00826163,"We also evaluate our WSD model on the Semeval-2007 coarse-grained all-words WSD task (Navigli et al., 2007). There are multiple reasons that we perform experiments in a coarse-grained setting: first, it has been argued that the fine granularity of WordNet is one of the main obstacles to accurate WSD (cf. the discussion in (Navigli, 2009)); second, the training corpus of word representations is Wikipedia, which is quite different from WordNet. Baseline methods. We compare our model with the best unsupervised system SUSSX-FR (Koeling and McCarthy, 2007), and the best supervised system, NUS-PT (Chan et al., 2007), participating in the Semeval-2007 coarse-grained all-words task. We also compare with SSI (Navigli and Velardi, 2005) and the state-of-the-art system Degree (Navigli and Lapata, 2010). We use different baseline methods for the two WSD tasks because we want to compare our model with the stateof-the-art systems that are applicable to different datasets and show that our WSD method can perform robustly in these different WSD tasks. 1031 Results and discussion. We report our results in terms of F1 -measure on the Semeval-2007 coarsegrained all-words dataset (Navigli et al., 2007). Table 5 report"
D14-1110,W06-1663,0,0.0148635,"e disambiguation (WSD) is to computationally identify the meaning of words in context (Navigli, 2009). There are two approaches of WSD that assign meaning of words from a fixed sense inventory, supervised and knowledge-based methods. Supervised approaches require large labeled training sets, which are time consuming to create. In this paper, we only focus on knowledge-based word sense disambiguation. Knowledge-based approaches exploit knowledge resources (such as dictionaries, thesauri, ontologies, collocations, etc.) to determine the senses of words in context. However, it has been shown in (Cuadros and Rigau, 2006) that the amount of lexical and semantic information contained in such resources is typically insufficient for high-performance WSD. Much work has been presented to automatically extend existing resources, including automatically linking Wikipedia to WordNet to include full use of the first WordNet sense heuristic (Suchanek et al., 2008), a graph-based mapping of Wikipedia categories to WordNet synsets (Ponzetto and Navigli, 2009), and automatically mapping Wikipedia pages to WordNet synsets (Ponzetto and Navigli, 2010). It was recently shown that word representations can capture semantic and"
D14-1110,P12-1092,0,0.864337,"chnology Department of Computer Science and Technology Tsinghua University, Beijing 100084, China cxx.thu@gmail.com, {lzy, sms}@tsinghua.edu.cn Abstract Most word representation methods assume each word owns a single vector. However, this is usually problematic due to the homonymy and polysemy of many words. To remedy the issue, Reisinger and Mooney (2010) proposed a multi-prototype vector space model, where the contexts of each word are first clustered into groups, and then each cluster generates a distinct prototype vector for a word by averaging over all context vectors within the cluster. Huang et al. (2012) followed this idea, but introduced continuous distributed vectors based on probabilistic neural language models for word representations. These cluster-based models conduct unsupervised word sense induction by clustering word contexts and, thus, suffer from the following issues: Most word representation methods assume that each word owns a single semantic vector. This is usually problematic because lexical ambiguity is ubiquitous, which is also the problem to be resolved by word sense disambiguation. In this paper, we present a unified model for joint word sense representation and disambiguat"
D14-1110,S07-1068,0,0.0182737,"ich are hard to acquire. 3.4 Coarse-grained WSD Experimental setting. We also evaluate our WSD model on the Semeval-2007 coarse-grained all-words WSD task (Navigli et al., 2007). There are multiple reasons that we perform experiments in a coarse-grained setting: first, it has been argued that the fine granularity of WordNet is one of the main obstacles to accurate WSD (cf. the discussion in (Navigli, 2009)); second, the training corpus of word representations is Wikipedia, which is quite different from WordNet. Baseline methods. We compare our model with the best unsupervised system SUSSX-FR (Koeling and McCarthy, 2007), and the best supervised system, NUS-PT (Chan et al., 2007), participating in the Semeval-2007 coarse-grained all-words task. We also compare with SSI (Navigli and Velardi, 2005) and the state-of-the-art system Degree (Navigli and Lapata, 2010). We use different baseline methods for the two WSD tasks because we want to compare our model with the stateof-the-art systems that are applicable to different datasets and show that our WSD method can perform robustly in these different WSD tasks. 1031 Results and discussion. We report our results in terms of F1 -measure on the Semeval-2007 coarsegrai"
D14-1110,H05-1053,0,0.00865122,"es traditional PageRank over the semantic graph based on WordNet and obtains a context-independent ranking of word senses. k-NN is a widely used classification method, where neighbors are the k labeled examples most 7 We compare only with those systems performing tokenbased WSD, i.e., disambiguating each instance of a target word separately. 1030 Algorithm Random BL MFS BL k-NN Static PR Personalized PR Degree Our Model Sports Recall 19.5 19.6 30.3 20.1 35.6 42.0 57.3 Finance Recall 19.6 37.1 43.4 39.6 46.9 47.8 60.6 Table 4: Performance on the Sports and Finance sections of the dataset from (Koeling et al., 2005). similar to the test example. The k-NN system is trained on SemCor (Miller et al., 1993), the largest publicly available annotated corpus. Degree and Personalized PageRank are stateof-the-art systems that exploit WordNet to build a semantic graph and exploit the structural properties of the graph in order to choose the appropriate senses of words in context. Results and discussion. Similar to other work on this dataset, we use recall (the ratio of correct sense labels to the total labels in the gold standard) as our evaluation measure. Table 4 shows the results of different WSD systems on the"
D14-1110,P10-1116,0,0.00544857,"s on domain-specific WSD, and achieves competitive performance on coarsegrained all-words WSD. Our model only requires large-scale unlabeled text corpora and a sense inventory for WSD, thus it can be easily applied to other corpora and tasks. There are still several open problems that should be investigated further: 1033 1. Because the senses of words change over time (new senses appear), we will incorporate cluster-based methods in our model to find senses that are not in the sense inventory. 2. We can explore other WSD methods based on sense vectors to improve our performance. For example, (Li et al., 2010) used LDA to perform data-driven WSD in a manner similar to our model. We may integrate the advantages of these models and our model together to build a more powerful WSD system. Katrin Erk and Sebastian Pado. 2010. Exemplar-based models for word meaning in context. In Proceedings of ACL, pages 92–97. 3. To learn better sense vectors, we can exploit the semantic relations (such as the hypernym and hyponym relations defined in WordNet) between senses in our model. Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting Liu. 2014. Learning sense-specific word embeddings by exploiting bilingual resources"
D14-1110,D10-1036,1,0.478393,"Missing"
D14-1110,D11-1146,1,0.436874,"proposed in (Rumelhart et al., 1986) and have been successfully used in language models (Bengio et al., 2006; Mnih and Hinton, 2008) and many natural language processing tasks, such as word representation learning (Mikolov, 2012), named entity recognition (Turian et al., 2010), disambiguation (Collobert et al., 2011), parsing and tagging (Socher et al., 2011; Socher et al., 2013). They are very useful in NLP tasks because they can be used as inputs to learning algorithms or as extra word features in NLP systems. Hence, many NLP applications, such as keyword extraction (Li1032 u et al., 2010; Liu et al., 2011b; Liu et al., 2012), social tag suggestion (Liu et al., 2011a) and text classification (Baker and McCallum, 1998), may also potentially benefit from distributed word representation. The main advantage is that the representations of similar words are close in vector space, which makes generalization to novel patterns easier and model estimation more robust. Word representations are hard to train due to the computational complexity. Recently, (Mikolov et al., 2013) proposed two particular models, Skipgram and CBOW, to learn word representations in large amounts of text data. The training object"
D14-1110,W11-0316,1,0.276159,"proposed in (Rumelhart et al., 1986) and have been successfully used in language models (Bengio et al., 2006; Mnih and Hinton, 2008) and many natural language processing tasks, such as word representation learning (Mikolov, 2012), named entity recognition (Turian et al., 2010), disambiguation (Collobert et al., 2011), parsing and tagging (Socher et al., 2011; Socher et al., 2013). They are very useful in NLP tasks because they can be used as inputs to learning algorithms or as extra word features in NLP systems. Hence, many NLP applications, such as keyword extraction (Li1032 u et al., 2010; Liu et al., 2011b; Liu et al., 2012), social tag suggestion (Liu et al., 2011a) and text classification (Baker and McCallum, 1998), may also potentially benefit from distributed word representation. The main advantage is that the representations of similar words are close in vector space, which makes generalization to novel patterns easier and model estimation more robust. Word representations are hard to train due to the computational complexity. Recently, (Mikolov et al., 2013) proposed two particular models, Skipgram and CBOW, to learn word representations in large amounts of text data. The training object"
D14-1110,H93-1061,0,0.90814,"ependent ranking of word senses. k-NN is a widely used classification method, where neighbors are the k labeled examples most 7 We compare only with those systems performing tokenbased WSD, i.e., disambiguating each instance of a target word separately. 1030 Algorithm Random BL MFS BL k-NN Static PR Personalized PR Degree Our Model Sports Recall 19.5 19.6 30.3 20.1 35.6 42.0 57.3 Finance Recall 19.6 37.1 43.4 39.6 46.9 47.8 60.6 Table 4: Performance on the Sports and Finance sections of the dataset from (Koeling et al., 2005). similar to the test example. The k-NN system is trained on SemCor (Miller et al., 1993), the largest publicly available annotated corpus. Degree and Personalized PageRank are stateof-the-art systems that exploit WordNet to build a semantic graph and exploit the structural properties of the graph in order to choose the appropriate senses of words in context. Results and discussion. Similar to other work on this dataset, we use recall (the ratio of correct sense labels to the total labels in the gold standard) as our evaluation measure. Table 4 shows the results of different WSD systems on the dataset, and the best results are shown in bold. The differences between other results a"
D14-1110,P13-1045,0,0.0196764,"confident disambiguation results to update the context vector if the score margin is larger than ε. Our model achieves the best performance when ε = 0.1. 4 4.1 Related Work Word Representations Distributed representations for words were proposed in (Rumelhart et al., 1986) and have been successfully used in language models (Bengio et al., 2006; Mnih and Hinton, 2008) and many natural language processing tasks, such as word representation learning (Mikolov, 2012), named entity recognition (Turian et al., 2010), disambiguation (Collobert et al., 2011), parsing and tagging (Socher et al., 2011; Socher et al., 2013). They are very useful in NLP tasks because they can be used as inputs to learning algorithms or as extra word features in NLP systems. Hence, many NLP applications, such as keyword extraction (Li1032 u et al., 2010; Liu et al., 2011b; Liu et al., 2012), social tag suggestion (Liu et al., 2011a) and text classification (Baker and McCallum, 1998), may also potentially benefit from distributed word representation. The main advantage is that the representations of similar words are close in vector space, which makes generalization to novel patterns easier and model estimation more robust. Word re"
D14-1110,C14-1016,0,0.524868,"architecture of Skip-gram. Most of the previous vector-space models use one representation per word. This is problematic because many words have multiple senses. The multi-prototype approach has been widely studied. (Reisinger and Mooney, 2010) proposed the multi-prototype vector-space model. (Huang et al., 2012) used the multi-prototype models to learn the vector for different senses of a word. All of these models use the clustering of contexts as a word sense and can not be directly used in word sense disambiguation. After our paper was submitted, we perceive the following recent advances: (Tian et al., 2014) proposed a probabilistic model for multi-prototype word representation. (Guo et al., 2014) explored bilingual resources to learn sense-specific word representation. (Neelakantan et al., 2014) proposed an efficient non-parametric model for multiprototype word representation. 4.2 Knowledge-based WSD The objective of word sense disambiguation (WSD) is to computationally identify the meaning of words in context (Navigli, 2009). There are two approaches of WSD that assign meaning of words from a fixed sense inventory, supervised and knowledge-based methods. Supervised approaches require large labe"
D14-1110,P10-1040,0,0.156264,"en ε = 0.0, we use every disambiguation result to update the context vector. When ε 6= 0, we only use the confident disambiguation results to update the context vector if the score margin is larger than ε. Our model achieves the best performance when ε = 0.1. 4 4.1 Related Work Word Representations Distributed representations for words were proposed in (Rumelhart et al., 1986) and have been successfully used in language models (Bengio et al., 2006; Mnih and Hinton, 2008) and many natural language processing tasks, such as word representation learning (Mikolov, 2012), named entity recognition (Turian et al., 2010), disambiguation (Collobert et al., 2011), parsing and tagging (Socher et al., 2011; Socher et al., 2013). They are very useful in NLP tasks because they can be used as inputs to learning algorithms or as extra word features in NLP systems. Hence, many NLP applications, such as keyword extraction (Li1032 u et al., 2010; Liu et al., 2011b; Liu et al., 2012), social tag suggestion (Liu et al., 2011a) and text classification (Baker and McCallum, 1998), may also potentially benefit from distributed word representation. The main advantage is that the representations of similar words are close in ve"
D14-1110,S07-1006,0,0.042888,"81.6 82.5 85.3 All words F1 62.7 78.9 77.0 82.5 83.2 81.7 73.9 75.8 79.6 82.6 Table 5: Performance on Semeval-2007 coarsegrained all-words WSD. In the type column, U, Semi and S stand for unsupervised, semisupervised and supervised, respectively. The differences between the results in bold in each column of the table are not statistically significant at p < 0.05. methods rely greatly on high-quality semantic relations or annotated data, which are hard to acquire. 3.4 Coarse-grained WSD Experimental setting. We also evaluate our WSD model on the Semeval-2007 coarse-grained all-words WSD task (Navigli et al., 2007). There are multiple reasons that we perform experiments in a coarse-grained setting: first, it has been argued that the fine granularity of WordNet is one of the main obstacles to accurate WSD (cf. the discussion in (Navigli, 2009)); second, the training corpus of word representations is Wikipedia, which is quite different from WordNet. Baseline methods. We compare our model with the best unsupervised system SUSSX-FR (Koeling and McCarthy, 2007), and the best supervised system, NUS-PT (Chan et al., 2007), participating in the Semeval-2007 coarse-grained all-words task. We also compare with SS"
D14-1110,D14-1113,0,0.508916,"roach has been widely studied. (Reisinger and Mooney, 2010) proposed the multi-prototype vector-space model. (Huang et al., 2012) used the multi-prototype models to learn the vector for different senses of a word. All of these models use the clustering of contexts as a word sense and can not be directly used in word sense disambiguation. After our paper was submitted, we perceive the following recent advances: (Tian et al., 2014) proposed a probabilistic model for multi-prototype word representation. (Guo et al., 2014) explored bilingual resources to learn sense-specific word representation. (Neelakantan et al., 2014) proposed an efficient non-parametric model for multiprototype word representation. 4.2 Knowledge-based WSD The objective of word sense disambiguation (WSD) is to computationally identify the meaning of words in context (Navigli, 2009). There are two approaches of WSD that assign meaning of words from a fixed sense inventory, supervised and knowledge-based methods. Supervised approaches require large labeled training sets, which are time consuming to create. In this paper, we only focus on knowledge-based word sense disambiguation. Knowledge-based approaches exploit knowledge resources (such a"
D14-1110,P10-1154,0,0.0289566,"Missing"
D14-1110,N10-1013,0,0.421671,"pgram and CBOW, to learn word representations in large amounts of text data. The training objective of the CBOW model is to combine the representations of the surrounding words to predict the word in the middle, while the Skip-gram model’s is to learn word representations that are good at predicting its context in the same sentence (Mikolov et al., 2013). Our paper uses the model architecture of Skip-gram. Most of the previous vector-space models use one representation per word. This is problematic because many words have multiple senses. The multi-prototype approach has been widely studied. (Reisinger and Mooney, 2010) proposed the multi-prototype vector-space model. (Huang et al., 2012) used the multi-prototype models to learn the vector for different senses of a word. All of these models use the clustering of contexts as a word sense and can not be directly used in word sense disambiguation. After our paper was submitted, we perceive the following recent advances: (Tian et al., 2014) proposed a probabilistic model for multi-prototype word representation. (Guo et al., 2014) explored bilingual resources to learn sense-specific word representation. (Neelakantan et al., 2014) proposed an efficient non-paramet"
D14-1110,C14-1048,0,\N,Missing
D14-1110,P10-2017,0,\N,Missing
D15-1082,D13-1080,0,0.031571,"s. Relation paths have already been widely considered in social networks and recommender systems. Most of these works regard each relation and path as discrete symbols, and deal with them using graph-based algorithms, such as random walks with restart (Tong et al., 2006). Relation paths have also been used for inference on large-scale KBs, such as Path Ranking algorithm (PRA) (Lao and Cohen, 2010), which has been adopted for expert finding (Lao and Cohen, 2010) and information retrieval (Lao et al., 2012). PRA has also been used for relation extraction based on KB structure (Lao et al., 2011; Gardner et al., 2013). (Neelakantan et al., 2015) further learns a recurrent neural network (RNN) to represent unseen relation paths according to involved relations. We note that, these methods focus on modeling relation paths for relation extraction without considering any information of entities. In contrast, by successfully integrating the merits of modeling entities and relation paths, PTransE can learn superior representations of both entities and relations for knowledge graph completion and relation extraction as shown in our experiments. Case Study of Relation Inference We have shown that PTransE can achiev"
D15-1082,P11-1055,0,0.238952,"Missing"
D15-1082,D11-1049,0,0.241022,"n in relation paths. Relation paths have already been widely considered in social networks and recommender systems. Most of these works regard each relation and path as discrete symbols, and deal with them using graph-based algorithms, such as random walks with restart (Tong et al., 2006). Relation paths have also been used for inference on large-scale KBs, such as Path Ranking algorithm (PRA) (Lao and Cohen, 2010), which has been adopted for expert finding (Lao and Cohen, 2010) and information retrieval (Lao et al., 2012). PRA has also been used for relation extraction based on KB structure (Lao et al., 2011; Gardner et al., 2013). (Neelakantan et al., 2015) further learns a recurrent neural network (RNN) to represent unseen relation paths according to involved relations. We note that, these methods focus on modeling relation paths for relation extraction without considering any information of entities. In contrast, by successfully integrating the merits of modeling entities and relation paths, PTransE can learn superior representations of both entities and relations for knowledge graph completion and relation extraction as shown in our experiments. Case Study of Relation Inference We have shown"
D15-1082,D12-1093,0,0.0709095,"s of knowledge bases only use direct relations between entities, ignoring rich information in relation paths. Relation paths have already been widely considered in social networks and recommender systems. Most of these works regard each relation and path as discrete symbols, and deal with them using graph-based algorithms, such as random walks with restart (Tong et al., 2006). Relation paths have also been used for inference on large-scale KBs, such as Path Ranking algorithm (PRA) (Lao and Cohen, 2010), which has been adopted for expert finding (Lao and Cohen, 2010) and information retrieval (Lao et al., 2012). PRA has also been used for relation extraction based on KB structure (Lao et al., 2011; Gardner et al., 2013). (Neelakantan et al., 2015) further learns a recurrent neural network (RNN) to represent unseen relation paths according to involved relations. We note that, these methods focus on modeling relation paths for relation extraction without considering any information of entities. In contrast, by successfully integrating the merits of modeling entities and relation paths, PTransE can learn superior representations of both entities and relations for knowledge graph completion and relation"
D15-1082,P09-1113,0,0.211437,"Missing"
D15-1082,P08-1028,0,0.00866539,"ource obtained from the entity n. For each relation path p, we set the initial resource in h as Rp (h) = 1. By performing resource allocation recursively from h through the path p, the tail entity t eventually obtains the resource Rp (t) which indicates how much information of the head entity h can be well translated. We use Rp (t) to measure the reliability of the path p given (h, t), i.e., R(p|h, t) = Rp (t). 2.3 BornInCity p = r1 · . . . · rl . (6) Both addition and multiplication operations are simple and have been extensively investigated in semantic composition of phrases and sentences (Mitchell and Lapata, 2008). Recurrent Neural Network (RNN). RNN is a recent neural-based model for semantic composition (Mikolov et al., 2010). The composition operation is realized using a matrix W: ci = f (W [ci−1 ; ri ]), Relation Path Representation (7) where f is a non-linearity or identical function, and [a; b] represents the concatenation of two vecBesides relation path reliability, we also need to define energy function E(h, p, t) for the path triple 707 tors. By setting c1 = r1 and recursively performing RNN following the relation path, we will finally obtain p = cn . RNN has also been used for representation"
D15-1082,P15-1016,0,0.503149,"NN is a recent neural-based model for semantic composition (Mikolov et al., 2010). The composition operation is realized using a matrix W: ci = f (W [ci−1 ; ri ]), Relation Path Representation (7) where f is a non-linearity or identical function, and [a; b] represents the concatenation of two vecBesides relation path reliability, we also need to define energy function E(h, p, t) for the path triple 707 tors. By setting c1 = r1 and recursively performing RNN following the relation path, we will finally obtain p = cn . RNN has also been used for representation learning of relation paths in KBs (Neelakantan et al., 2015). For a multiple-step relation path triple (h, p, t), we could have followed TransE and define the energy function as E(h, p, t) = ||h + p − t||. However, since we have minimized ||h + r − t|| with the direct relation triple (h, r, t) to make sure r ≈ t−h, we may directly define the energy function of (h, p, t) as 2.5 For optimization, we employ stochastic gradient descent (SGD) to minimize the loss function. We randomly select a valid triple from the training set iteratively for learning. In the implementation, we also enforce constraints on the norms of the embeddings h, r, t. That is, we se"
D15-1082,D12-1042,0,0.0629002,"21, 034 training mentions. We use FB40K as the KB, consisting all entities mentioned in NYT and 1, 336 relations. In the experiments, we implemented the textbased model Sm2r presented in (Weston et al., 2013). We combine the ranking scores from the text-based model with those from KB representations to rank testing triples, and generate precision-recall curves for both TransE and PTransE. For learning of TransE and PTransE, we set the dimensions of entities/relations embeddings k = 50, the learning rate λ = 0.001, the margin γ = 1.0 and dissimilarity metric as L1. We also compare with MIMLRE (Surdeanu et al., 2012) which is the state-of-art method using distant supervision. The evaluation curves are shown in Figure 3. Table 4: Evaluation results on relation prediction. Metric TransE (Our) +Rev +Rev+Path PTransE (ADD, 2-step) -TransE -Path PTransE (MUL, 2-step) PTransE (RNN, 2-step) PTransE (ADD, 3-step) 3.3 Mean Rank Raw Filter 2.8 2.5 2.6 2.3 2.4 1.9 1.7 1.2 135.8 135.3 2.0 1.6 2.5 2.0 1.9 1.4 1.8 1.4 Predicting Tail Entities (Hits@10) 1-to-1 1-to-N N-to-1 N-to-N 34.9 14.6 68.3 41.3 32.7 14.9 61.6 43.3 28.2 13.1 76.0 41.8 43.7 19.7 66.7 50.0 65.5 39.8 83.3 67.2 79.2 37.4 90.4 72.1 71.5 49.0 85.0 72.9 9"
D15-1082,D13-1136,0,0.130463,"8 89.2 34.1 69.2 74.6 86.6 43.7 70.6 91.0 92.8 60.9 83.8 89.0 86.8 57.6 79.8 88.9 84.0 56.3 84.5 90.1 92.0 58.7 86.1 tion paths. As compared with TransE, the inferior of PTransE-TransE also indicates that entity representations are informative and crucial for relation prediction. as part-of-speech tags, dependency tree paths for each mention. There are 53 relations (including non-relation denoted as NA) and 121, 034 training mentions. We use FB40K as the KB, consisting all entities mentioned in NYT and 1, 336 relations. In the experiments, we implemented the textbased model Sm2r presented in (Weston et al., 2013). We combine the ranking scores from the text-based model with those from KB representations to rank testing triples, and generate precision-recall curves for both TransE and PTransE. For learning of TransE and PTransE, we set the dimensions of entities/relations embeddings k = 50, the learning rate λ = 0.001, the margin γ = 1.0 and dissimilarity metric as L1. We also compare with MIMLRE (Surdeanu et al., 2012) which is the state-of-art method using distant supervision. The evaluation curves are shown in Figure 3. Table 4: Evaluation results on relation prediction. Metric TransE (Our) +Rev +Re"
D15-1196,N09-1003,0,0.0123629,"presentation performance is evaluated with the word similarity computation task, and the interpretability is evaluated with the word intrusion detection task. For the both tasks, we train our OIWE models using the text8 corpus obtained from word2vec website1 , and the OIWE models achieve the best performance by setting the dimension number K = 300, β = 0.6, δ = 1/60, and γL = 2.5 × 10−6 . 3.1 Word Similarity Computation Following the settings in (Murphy et al., 2012), we also select the following three sets for word similarity computation: (1) WS-203, the strictsimilarity subset of 203 pairs (Agirre et al., 2009) selected from the wordsim-353 (Finkelstein et al., 2001), (2) RG-65, 65 concrete word pairs built by (Rubenstein and Goodenough, 1965) and (3) MEN, 3, 000 word pairs built by (Bruni et al., 2014). The performance is evaluated with the Spearman coefficient between human judgements and similarities calculated using word embeddings. We select three baselines including Skip-Gram (Mikolov et al., 2013b), recurrent neural networks (RNN) (Mikolov et al., 2011) and NNSE (Murphy et al., 2012). For Skip-Gram, we report the result we learned using word2vec on text8 corpus. The result of RNN is from (Far"
D15-1196,D14-1110,1,0.714851,"Missing"
D15-1196,P14-5004,0,0.0135374,"09) selected from the wordsim-353 (Finkelstein et al., 2001), (2) RG-65, 65 concrete word pairs built by (Rubenstein and Goodenough, 1965) and (3) MEN, 3, 000 word pairs built by (Bruni et al., 2014). The performance is evaluated with the Spearman coefficient between human judgements and similarities calculated using word embeddings. We select three baselines including Skip-Gram (Mikolov et al., 2013b), recurrent neural networks (RNN) (Mikolov et al., 2011) and NNSE (Murphy et al., 2012). For Skip-Gram, we report the result we learned using word2vec on text8 corpus. The result of RNN is from (Faruqui and Dyer, 2014) and the one of NNSE is from (Murphy et al., 2012). The evaluation results of word similarity computation are shown in Table 1. We can observe that: (1) The OIWE models consistently outperform other baselines. (2) IPG generally achieves better representation performance than 1689 1 https://code.google.com/p/word2vec/ Model Skip-Gram RNN NNSE OIWE-NPG OIWE-IPG WS-203 67.35 49.28 51.06 63.71 71.74 RG-65 50.49 50.19 56.48 56.85 57.16 MEN 52.56 43.44 57.60 56.68 meanings of these dimensions. One can also refer to http://github.com/skTim/OIWE to find top-5 words for all dimensions. No. 1 2 Table 1:"
D15-1196,P12-1092,0,0.0305669,"Missing"
D15-1196,C12-1118,0,0.582632,"l (CBOW) (Mikolov et al., 2013a; Mikolov et al., 2013b), have achieved impressive impact due to their simplicity and efficiency. For most word embedding methods, a critical issue is that, we are unaware of what each dimension represent in word embeddings. Hence, the latent dimension for which a word has its largest value is difficult to interpret. This makes word embeddings like a black-box, and prevents them from being human-readable and further manipulation. People have proposed non-negative matrix factorization (NMF) for word representation, denoted as non-negative sparse embedding (NNSE) (Murphy et al., 2012). NNSE realizes interpretable word embeddings by applying non-negative constraints for word embeddings. Although NNSE learns word embeddings with good interpretabilities, like other MF methods, it also requires a global matrix for learning, thus suffers from heavy memory usage and cannot well deal with streaming text data. Inspired by the characteristics of NMF methods (Lee and Seung, 1999), we note that, nonnegative constraints only allow additive combinations instead of subtractive combinations, and lead to a parts-based representation. Hence, the non-negative constraints derive interpretabi"
D15-1196,D14-1162,0,0.130543,"paper can be obtained from http: //github.com/skTim/OIWE. 1 Introduction Word embeddings (Turian et al., 2010) aim to encode semantic meanings of words into lowdimensional dense vectors. As compared with traditional one-hot representation and distributional representation, word embeddings can better address the sparsity issue and have achieved success in many NLP applications recent years. There are two typical approaches for word embeddings. The neural-network (NN) approach (Bengio et al., 2006) employs neural-based techniques to learn word embeddings. The matrix factorization (MF) approach (Pennington et al., 2014) builds word embeddings by factorizing wordcontext co-occurrence matrices. The MF approach requires a global statistical matrix, while the NN approach can flexibly perform learning from ∗ Corresponding author: Z. Liu (liuzy@tsinghua.edu.cn) streaming text data, which is efficient in both computation and memory. For example, two recent NN methods, Skip-Gram and Continuous Bagof-Word Model (CBOW) (Mikolov et al., 2013a; Mikolov et al., 2013b), have achieved impressive impact due to their simplicity and efficiency. For most word embedding methods, a critical issue is that, we are unaware of what"
D15-1196,P12-1027,0,0.0324909,"s (Lee and Seung, 1999), we note that, nonnegative constraints only allow additive combinations instead of subtractive combinations, and lead to a parts-based representation. Hence, the non-negative constraints derive interpretabilities of word embeddings. In this paper, we aim to design an online NN method to efficiently learn interpretable word embeddings. In order to achieve the goal of interpretable embeddings, we design projected gradient descent (Lin, 2007) for optimization so as to apply non-negative constraints on NN methods such as Skip-Gram. We also employ adaptive gradient descent (Sun et al., 2012) to speedup learning convergence. We name the proposed models as online interpretable word embeddings (OIWE). 1687 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1687–1692, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. For experiments, we implement OIWE based on Skip-Gram. We evaluate the representation performance of word embedding methods on the word similarity computation task. Experiment results show that, our OIWE models are significantly superior to other baselines including SkipGram, RNN and NNSE."
D15-1196,P10-1040,0,0.0188767,"ization (NMF) has been proposed to learn interpretable word embeddings via non-negative constraints. However, NMF methods suffer from scale and memory issue because they have to maintain a global matrix for learning. To alleviate this challenge, we propose online learning of interpretable word embeddings from streaming text data. Experiments show that our model consistently outperforms the state-of-the-art word embedding methods in both representation ability and interpretability. The source code of this paper can be obtained from http: //github.com/skTim/OIWE. 1 Introduction Word embeddings (Turian et al., 2010) aim to encode semantic meanings of words into lowdimensional dense vectors. As compared with traditional one-hot representation and distributional representation, word embeddings can better address the sparsity issue and have achieved success in many NLP applications recent years. There are two typical approaches for word embeddings. The neural-network (NN) approach (Bengio et al., 2006) employs neural-based techniques to learn word embeddings. The matrix factorization (MF) approach (Pennington et al., 2014) builds word embeddings by factorizing wordcontext co-occurrence matrices. The MF appr"
D16-1171,D15-1263,0,0.00545718,"her et al., 2013). Besides, (Kim, 2014) and (Johnson and Zhang, 2014) adopt convolution neural network (CNN) to learn sentence representations and achieve outstanding performance in sentiment classification. Recurrent neural network also benefits sentiment classification because it is capable of capturing the sequential information. (Li et al., 2015), (Tai et al., 2015) investigate tree-structured long-short term memory (LSTM) networks on text or sentiment classification. There are also some hierarchical models proposed to deal with document-level sentiment classification (Tang et al., 2015a; Bhatia et al., 2015), which generate different levels (e.g., phrase, sentence or document) of semantic representations within a document. Moreover, attention mechanism is also introduced into sentiment classification, which aims to select important words from a send Document Representation Sentence Attention Sentence Level LSTM Layer h2 h1 p u hn Word Attention Sentence Representation LSTM Layer s1 s2 sn h11 h12 h1l1 h21 h22 h2l2 hn1 hn2 hnln w11 w21 wl11 w12 w22 wl22 w1n w2n wlnn Word Level Word Representation S1 S2 Sn Figure 1: The architecture of User Product Attention based Neural Sentiment Classification mod"
D16-1171,I13-1156,0,0.0472259,"i )2 , (13) where T is the numbers of predicted sentiment ratings that are identical with gold sentiment ratings, 1654 Baselines We compare our NSC model with several baseline methods for document sentiment classification: Majority regards the majority sentiment category in training set as the sentiment category of each document in test set. Trigram trains a SVM classifier with unigrams, bigrams and trigrams as features. TextFeature extracts text features including word and character n-grams, sentiment lexicon features, etc, and then train a SVM classifier. UPF extracts use-leniency features (Gao et al., 2013) and corresponding product features from training data, which is further concatenated with the features in Trigram an TextFeature. AvgWordvec averages word embeddings in a document to obtain document representation which is fed into a SVM classifier as features. SSWE generates features with sentiment-specific word embeddings (SSWE) (Tang et al., 2014) and then trains a SVM classifier. RNTN + RNN represents each sentence with the Recursive Neural Tensor Network (RNTN) (Socher et al., 2013) and feeds sentence representations into IMDB Yelp2013 Acc. RMSE Acc. RMSE Models without user and product"
D16-1171,D14-1181,0,0.0694938,"ge processing. Recently, sentiment analysis draws increasing attention of researchers with the rapid growth of online review ∗ Corresponding author: M. Sun (sms@tsinghua.edu.cn) Motivated by the successful utilization of deep neural networks in computer vision (Ciresan et al., 2012), speech recognition (Dahl et al., 2012) and natural language processing (Bengio et al., 2006), some neural network based sentiment analysis models are proposed to learn low-dimensional text features without any feature engineering (Glorot et al., 2011; Socher et al., 2011; Socher et al., 2012; Socher et al., 2013; Kim, 2014). Most proposed neural network models take the text information in a sentence or a document as input and generate the semantic representations using well-designed neural networks. However, these methods only focus 1650 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1650–1659, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics on the text content and ignore the crucial characteristics of users and products. It is a common sense that the user’s preference and product’s characteristics make significant influence on th"
D16-1171,D15-1278,0,0.0124337,"k models to learn representations based on the recursive tree structure of sentences, including Recursive Autoencoder (RAE) (Socher et al., 2011), Matrix-Vector Recursive Neural Network (MV-RNN) (Socher et al., 2012) and Recursive Neural Tensor Network (RNTN) (Socher et al., 2013). Besides, (Kim, 2014) and (Johnson and Zhang, 2014) adopt convolution neural network (CNN) to learn sentence representations and achieve outstanding performance in sentiment classification. Recurrent neural network also benefits sentiment classification because it is capable of capturing the sequential information. (Li et al., 2015), (Tai et al., 2015) investigate tree-structured long-short term memory (LSTM) networks on text or sentiment classification. There are also some hierarchical models proposed to deal with document-level sentiment classification (Tang et al., 2015a; Bhatia et al., 2015), which generate different levels (e.g., phrase, sentence or document) of semantic representations within a document. Moreover, attention mechanism is also introduced into sentiment classification, which aims to select important words from a send Document Representation Sentence Attention Sentence Level LSTM Layer h2 h1 p u hn Wor"
D16-1171,P14-5010,0,0.0086801,"Missing"
D16-1171,W02-1011,0,0.0421348,"p/NSC. 1 In this work, we focus on the task of documentlevel sentiment classification, which is a fundamental problem of sentiment analysis. Document-level sentiment classification assumes that each document expresses a sentiment on a single product and targets to determine the overall sentiment about the product. Most existing methods take sentiment classification as a special case of text classification problem. Such methods treat annotated sentiment polarities or ratings as categories and apply machine learning algorithms to train classifiers with text features, e.g., bag-of-words vectors (Pang et al., 2002). Since the performance of text classifiers heavily depends on the extracted features, such studies usually attend to design effective features from text or additional sentiment lexicons (Ding et al., 2008; Taboada et al., 2011). Introduction Sentiment analysis aims to analyze people’s sentiments or opinions according to their generated texts and plays a critical role in the area of data mining and natural language processing. Recently, sentiment analysis draws increasing attention of researchers with the rapid growth of online review ∗ Corresponding author: M. Sun (sms@tsinghua.edu.cn) Motiva"
D16-1171,D11-1014,0,0.171796,"s a critical role in the area of data mining and natural language processing. Recently, sentiment analysis draws increasing attention of researchers with the rapid growth of online review ∗ Corresponding author: M. Sun (sms@tsinghua.edu.cn) Motivated by the successful utilization of deep neural networks in computer vision (Ciresan et al., 2012), speech recognition (Dahl et al., 2012) and natural language processing (Bengio et al., 2006), some neural network based sentiment analysis models are proposed to learn low-dimensional text features without any feature engineering (Glorot et al., 2011; Socher et al., 2011; Socher et al., 2012; Socher et al., 2013; Kim, 2014). Most proposed neural network models take the text information in a sentence or a document as input and generate the semantic representations using well-designed neural networks. However, these methods only focus 1650 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1650–1659, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics on the text content and ignore the crucial characteristics of users and products. It is a common sense that the user’s preference and prod"
D16-1171,D12-1110,0,0.0813482,"the area of data mining and natural language processing. Recently, sentiment analysis draws increasing attention of researchers with the rapid growth of online review ∗ Corresponding author: M. Sun (sms@tsinghua.edu.cn) Motivated by the successful utilization of deep neural networks in computer vision (Ciresan et al., 2012), speech recognition (Dahl et al., 2012) and natural language processing (Bengio et al., 2006), some neural network based sentiment analysis models are proposed to learn low-dimensional text features without any feature engineering (Glorot et al., 2011; Socher et al., 2011; Socher et al., 2012; Socher et al., 2013; Kim, 2014). Most proposed neural network models take the text information in a sentence or a document as input and generate the semantic representations using well-designed neural networks. However, these methods only focus 1650 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1650–1659, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics on the text content and ignore the crucial characteristics of users and products. It is a common sense that the user’s preference and product’s characteristics"
D16-1171,D13-1170,0,0.290543,"ng and natural language processing. Recently, sentiment analysis draws increasing attention of researchers with the rapid growth of online review ∗ Corresponding author: M. Sun (sms@tsinghua.edu.cn) Motivated by the successful utilization of deep neural networks in computer vision (Ciresan et al., 2012), speech recognition (Dahl et al., 2012) and natural language processing (Bengio et al., 2006), some neural network based sentiment analysis models are proposed to learn low-dimensional text features without any feature engineering (Glorot et al., 2011; Socher et al., 2011; Socher et al., 2012; Socher et al., 2013; Kim, 2014). Most proposed neural network models take the text information in a sentence or a document as input and generate the semantic representations using well-designed neural networks. However, these methods only focus 1650 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1650–1659, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics on the text content and ignore the crucial characteristics of users and products. It is a common sense that the user’s preference and product’s characteristics make significant inf"
D16-1171,J11-2001,0,0.0271202,"ment on a single product and targets to determine the overall sentiment about the product. Most existing methods take sentiment classification as a special case of text classification problem. Such methods treat annotated sentiment polarities or ratings as categories and apply machine learning algorithms to train classifiers with text features, e.g., bag-of-words vectors (Pang et al., 2002). Since the performance of text classifiers heavily depends on the extracted features, such studies usually attend to design effective features from text or additional sentiment lexicons (Ding et al., 2008; Taboada et al., 2011). Introduction Sentiment analysis aims to analyze people’s sentiments or opinions according to their generated texts and plays a critical role in the area of data mining and natural language processing. Recently, sentiment analysis draws increasing attention of researchers with the rapid growth of online review ∗ Corresponding author: M. Sun (sms@tsinghua.edu.cn) Motivated by the successful utilization of deep neural networks in computer vision (Ciresan et al., 2012), speech recognition (Dahl et al., 2012) and natural language processing (Bengio et al., 2006), some neural network based sentime"
D16-1171,P15-1150,0,0.0119584,"epresentations based on the recursive tree structure of sentences, including Recursive Autoencoder (RAE) (Socher et al., 2011), Matrix-Vector Recursive Neural Network (MV-RNN) (Socher et al., 2012) and Recursive Neural Tensor Network (RNTN) (Socher et al., 2013). Besides, (Kim, 2014) and (Johnson and Zhang, 2014) adopt convolution neural network (CNN) to learn sentence representations and achieve outstanding performance in sentiment classification. Recurrent neural network also benefits sentiment classification because it is capable of capturing the sequential information. (Li et al., 2015), (Tai et al., 2015) investigate tree-structured long-short term memory (LSTM) networks on text or sentiment classification. There are also some hierarchical models proposed to deal with document-level sentiment classification (Tang et al., 2015a; Bhatia et al., 2015), which generate different levels (e.g., phrase, sentence or document) of semantic representations within a document. Moreover, attention mechanism is also introduced into sentiment classification, which aims to select important words from a send Document Representation Sentence Attention Sentence Level LSTM Layer h2 h1 p u hn Word Attention Sentence"
D16-1171,P14-1146,0,0.099911,"Missing"
D16-1171,D15-1167,0,0.531648,"ment as input and generate the semantic representations using well-designed neural networks. However, these methods only focus 1650 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1650–1659, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics on the text content and ignore the crucial characteristics of users and products. It is a common sense that the user’s preference and product’s characteristics make significant influence on the ratings. To incorporate user and product information into sentiment classification, (Tang et al., 2015b) bring in a text preference matrix and a representation vector for each user and product into CNN sentiment classifier. It modifies the word meaning in the input layer with the preference matrix and concatenates the user/product representation vectors with generated document representation before softmax layer. The proposed model achieves some improvements but suffers the following two problems: (1) The introduction of preference matrix for each user/product is insufficient and difficult to be well trained with limited reviews. For example, most users in IMDB and Yelp only have several tens"
D16-1171,P15-1098,0,0.869986,"ment as input and generate the semantic representations using well-designed neural networks. However, these methods only focus 1650 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1650–1659, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics on the text content and ignore the crucial characteristics of users and products. It is a common sense that the user’s preference and product’s characteristics make significant influence on the ratings. To incorporate user and product information into sentiment classification, (Tang et al., 2015b) bring in a text preference matrix and a representation vector for each user and product into CNN sentiment classifier. It modifies the word meaning in the input layer with the preference matrix and concatenates the user/product representation vectors with generated document representation before softmax layer. The proposed model achieves some improvements but suffers the following two problems: (1) The introduction of preference matrix for each user/product is insufficient and difficult to be well trained with limited reviews. For example, most users in IMDB and Yelp only have several tens"
D16-1171,N16-1174,0,0.252278,"phrase, sentence or document) of semantic representations within a document. Moreover, attention mechanism is also introduced into sentiment classification, which aims to select important words from a send Document Representation Sentence Attention Sentence Level LSTM Layer h2 h1 p u hn Word Attention Sentence Representation LSTM Layer s1 s2 sn h11 h12 h1l1 h21 h22 h2l2 hn1 hn2 hnln w11 w21 wl11 w12 w22 wl22 w1n w2n wlnn Word Level Word Representation S1 S2 Sn Figure 1: The architecture of User Product Attention based Neural Sentiment Classification model. tence or sentences from a document (Yang et al., 2016). Most existing sentiment classification models ignore the global user preference and product characteristics, which have crucial effects on the sentiment polarities. To address this issue, (Tang et al., 2015b) propose to add user/product preference matrices and representation vectors into CNN models. Nevertheless, it suffers from high model complexity and only considers word-level preference rather than semantic levels. In contrast, we propose an efficient neural sentiment classification model with users and products to serve as attentions in both word and semantic levels. 3 Methods In this s"
D17-1186,D13-1080,0,0.00904531,"Given a set of sentences of an entity pair, we first transform each sentence s into its distributed representation s, and then predict relation using the most representative sentence via a multi-instance learning mechanism. Output Vector Max Pooling Relation Path Modeling Relation paths have been taken into consideration on large-scale KBs for relation inference. Path Ranking algorithm (PRA) (Lao and Cohen, 2010) has been adopted for expert finding (Lao and Cohen, 2010), information retrieval (Lao et al., 2012), and further for relation classification based on KB structure (Lao et al., 2011; Gardner et al., 2013). (Neelakantan et al., 2015; Lin et al., 2015; Das et al., 2016; Wu et al., 2016) use recurrent neural networks (RNN) to represent relation paths based on all involved relations in KBs.(Guu et al., 2015) proposes an embedding-based compositional training method to connect the triple knowledge for KB completion. Different from the above work of modeling relation paths in KBs, our model aims to utilize relation paths in text corpus, and help to extract knowledge directly from plain text. 3 Non-linear Activation Max Operation Convolution Layer Word Embeddings Vector Representation Position Embedd"
D17-1186,D15-1038,0,0.0104799,"arning mechanism. Output Vector Max Pooling Relation Path Modeling Relation paths have been taken into consideration on large-scale KBs for relation inference. Path Ranking algorithm (PRA) (Lao and Cohen, 2010) has been adopted for expert finding (Lao and Cohen, 2010), information retrieval (Lao et al., 2012), and further for relation classification based on KB structure (Lao et al., 2011; Gardner et al., 2013). (Neelakantan et al., 2015; Lin et al., 2015; Das et al., 2016; Wu et al., 2016) use recurrent neural networks (RNN) to represent relation paths based on all involved relations in KBs.(Guu et al., 2015) proposes an embedding-based compositional training method to connect the triple knowledge for KB completion. Different from the above work of modeling relation paths in KBs, our model aims to utilize relation paths in text corpus, and help to extract knowledge directly from plain text. 3 Non-linear Activation Max Operation Convolution Layer Word Embeddings Vector Representation Position Embeddings Sentence: Jack married Lily ten years ago. Figure 2: The architecture of CNN used for text encoder. 3.1.1 Input Vector First, we transform the words {w1 , w2 , · · · , wl } in sentence s into vector"
D17-1186,W09-2415,0,0.0260005,"Missing"
D17-1186,P11-1055,0,0.352923,"the source of distant supervision. Afterward, (Mintz et al., 2009) aligns plain text with Freebase, by using distant supervision . However, most of these methods heuristically transform distant supervision to traditional supervised learning, by regarding it as a single-instance single-label problem, while in reality, one instance could correspond with multiple labels in different scenarios and vice versa. To alleviate the issue, (Riedel et al., 2010) regards each sentence as a training instance and allows multiple instances to share the same label but disallows more than one label. Further, (Hoffmann et al., 2011; Surdeanu et al., 2012) adopt multi-instance multilabel learning in relation extraction. The main drawback of these methods is that they obtain most features directly from NLP tools with inevitable errors, and these errors will propagate to the relation extraction system and limit the performance. 2.2 Neural Relation Extraction Recently, deep learning (Bengio, 2009) has been successfully applied in various areas, including computer vision, speech recognition and so on. Meanwhile, its effectiveness has also been verified in many NLP tasks such as sentiment analysis (dos Santos and Gatti, 2014)"
D17-1186,D11-1049,0,0.0351932,"rmation from text. Given a set of sentences of an entity pair, we first transform each sentence s into its distributed representation s, and then predict relation using the most representative sentence via a multi-instance learning mechanism. Output Vector Max Pooling Relation Path Modeling Relation paths have been taken into consideration on large-scale KBs for relation inference. Path Ranking algorithm (PRA) (Lao and Cohen, 2010) has been adopted for expert finding (Lao and Cohen, 2010), information retrieval (Lao et al., 2012), and further for relation classification based on KB structure (Lao et al., 2011; Gardner et al., 2013). (Neelakantan et al., 2015; Lin et al., 2015; Das et al., 2016; Wu et al., 2016) use recurrent neural networks (RNN) to represent relation paths based on all involved relations in KBs.(Guu et al., 2015) proposes an embedding-based compositional training method to connect the triple knowledge for KB completion. Different from the above work of modeling relation paths in KBs, our model aims to utilize relation paths in text corpus, and help to extract knowledge directly from plain text. 3 Non-linear Activation Max Operation Convolution Layer Word Embeddings Vector Represe"
D17-1186,D12-1093,0,0.0531395,"f each relation. 3.1 Text Encoder As shown in Fig. 2, we use a CNN to extract information from text. Given a set of sentences of an entity pair, we first transform each sentence s into its distributed representation s, and then predict relation using the most representative sentence via a multi-instance learning mechanism. Output Vector Max Pooling Relation Path Modeling Relation paths have been taken into consideration on large-scale KBs for relation inference. Path Ranking algorithm (PRA) (Lao and Cohen, 2010) has been adopted for expert finding (Lao and Cohen, 2010), information retrieval (Lao et al., 2012), and further for relation classification based on KB structure (Lao et al., 2011; Gardner et al., 2013). (Neelakantan et al., 2015; Lin et al., 2015; Das et al., 2016; Wu et al., 2016) use recurrent neural networks (RNN) to represent relation paths based on all involved relations in KBs.(Guu et al., 2015) proposes an embedding-based compositional training method to connect the triple knowledge for KB completion. Different from the above work of modeling relation paths in KBs, our model aims to utilize relation paths in text corpus, and help to extract knowledge directly from plain text. 3 Non"
D17-1186,D15-1082,1,0.176633,"rst transform each sentence s into its distributed representation s, and then predict relation using the most representative sentence via a multi-instance learning mechanism. Output Vector Max Pooling Relation Path Modeling Relation paths have been taken into consideration on large-scale KBs for relation inference. Path Ranking algorithm (PRA) (Lao and Cohen, 2010) has been adopted for expert finding (Lao and Cohen, 2010), information retrieval (Lao et al., 2012), and further for relation classification based on KB structure (Lao et al., 2011; Gardner et al., 2013). (Neelakantan et al., 2015; Lin et al., 2015; Das et al., 2016; Wu et al., 2016) use recurrent neural networks (RNN) to represent relation paths based on all involved relations in KBs.(Guu et al., 2015) proposes an embedding-based compositional training method to connect the triple knowledge for KB completion. Different from the above work of modeling relation paths in KBs, our model aims to utilize relation paths in text corpus, and help to extract knowledge directly from plain text. 3 Non-linear Activation Max Operation Convolution Layer Word Embeddings Vector Representation Position Embeddings Sentence: Jack married Lily ten years ag"
D17-1186,P16-1200,1,0.504287,"annotation is very time consuming and labor intensive. One promising approach to address this limitation is distant supervision. (Mintz et al., 2009) generates training data automatically by aligning a KB with plain text. They assume that if two target entities have a relation in KB, then all sentences that contain these two entities will express this relation and can be regarded as a positive training instance. Since neural models have been verified to be effective for classifying relations from plain text (Socher et al., 2012; Zeng et al., 2014; dos Santos et al., 2015), (Zeng et al., 2015; Lin et al., 2016) incorporate neural networks method with distant supervision relation extraction. Further, (Ye et al., 2016) considers finer-grained information, and achieves the state-of-the-art performance. Although existing RE systems have achieved promising results with the help of distant supervision and neural models, they still suffer from a major drawback: the models only learn from those sentences contain both two target entities. However, those sentences containing only one of the entities could also provide useful information and help build inference chains. For example, if we know that “h is the f"
D17-1186,P09-1113,0,0.875034,"ntly, petabytes of natural-language text containing thousands of different structure types are readily available, which is an important resource for automatically finding unknown relational facts. Hence, relation extraction (RE), defined as the task of extracting structured information from plain text, has attracted much interest. Most existing supervised RE systems usually suffer from the issue that lacks sufficient labelled relation-specific training data. Manual annotation is very time consuming and labor intensive. One promising approach to address this limitation is distant supervision. (Mintz et al., 2009) generates training data automatically by aligning a KB with plain text. They assume that if two target entities have a relation in KB, then all sentences that contain these two entities will express this relation and can be regarded as a positive training instance. Since neural models have been verified to be effective for classifying relations from plain text (Socher et al., 2012; Zeng et al., 2014; dos Santos et al., 2015), (Zeng et al., 2015; Lin et al., 2016) incorporate neural networks method with distant supervision relation extraction. Further, (Ye et al., 2016) considers finer-grained"
D17-1186,P16-1105,0,0.0561127,"Recently, deep learning (Bengio, 2009) has been successfully applied in various areas, including computer vision, speech recognition and so on. Meanwhile, its effectiveness has also been verified in many NLP tasks such as sentiment analysis (dos Santos and Gatti, 2014), parsing (Socher et al., 2013), summarization (Rush et al., 2015) and machine translation (Sutskever et al., 2014). With the advances of deep learning, there are growing works that design neural networks for relation extraction. (Socher et al., 2012) uses a recursive neural network in relation extraction, and (Xu et al., 2015; Miwa and Bansal, 2016) further use LSTM. (Zeng et al., 2014; dos Santos et al., 2015) adopt CNN in this task, and (Zeng et al., 2015; Lin et al., 2016) combine attention-based multiinstance learning which shows promising results. However, these above models merely learn from those sentences which directly contain both two target entities. The important information of those relation paths hidden in the text is ignored. In this paper, we propose a novel path-based neural RE 1769 model to address this issue. Besides, although we choose CNN to test the effectiveness of our model, other neural models could also be easil"
D17-1186,P15-1016,0,0.0143213,"s of an entity pair, we first transform each sentence s into its distributed representation s, and then predict relation using the most representative sentence via a multi-instance learning mechanism. Output Vector Max Pooling Relation Path Modeling Relation paths have been taken into consideration on large-scale KBs for relation inference. Path Ranking algorithm (PRA) (Lao and Cohen, 2010) has been adopted for expert finding (Lao and Cohen, 2010), information retrieval (Lao et al., 2012), and further for relation classification based on KB structure (Lao et al., 2011; Gardner et al., 2013). (Neelakantan et al., 2015; Lin et al., 2015; Das et al., 2016; Wu et al., 2016) use recurrent neural networks (RNN) to represent relation paths based on all involved relations in KBs.(Guu et al., 2015) proposes an embedding-based compositional training method to connect the triple knowledge for KB completion. Different from the above work of modeling relation paths in KBs, our model aims to utilize relation paths in text corpus, and help to extract knowledge directly from plain text. 3 Non-linear Activation Max Operation Convolution Layer Word Embeddings Vector Representation Position Embeddings Sentence: Jack married"
D17-1186,D15-1044,0,0.0415961,"ltilabel learning in relation extraction. The main drawback of these methods is that they obtain most features directly from NLP tools with inevitable errors, and these errors will propagate to the relation extraction system and limit the performance. 2.2 Neural Relation Extraction Recently, deep learning (Bengio, 2009) has been successfully applied in various areas, including computer vision, speech recognition and so on. Meanwhile, its effectiveness has also been verified in many NLP tasks such as sentiment analysis (dos Santos and Gatti, 2014), parsing (Socher et al., 2013), summarization (Rush et al., 2015) and machine translation (Sutskever et al., 2014). With the advances of deep learning, there are growing works that design neural networks for relation extraction. (Socher et al., 2012) uses a recursive neural network in relation extraction, and (Xu et al., 2015; Miwa and Bansal, 2016) further use LSTM. (Zeng et al., 2014; dos Santos et al., 2015) adopt CNN in this task, and (Zeng et al., 2015; Lin et al., 2016) combine attention-based multiinstance learning which shows promising results. However, these above models merely learn from those sentences which directly contain both two target entit"
D17-1186,C14-1008,0,0.00772067,"(Hoffmann et al., 2011; Surdeanu et al., 2012) adopt multi-instance multilabel learning in relation extraction. The main drawback of these methods is that they obtain most features directly from NLP tools with inevitable errors, and these errors will propagate to the relation extraction system and limit the performance. 2.2 Neural Relation Extraction Recently, deep learning (Bengio, 2009) has been successfully applied in various areas, including computer vision, speech recognition and so on. Meanwhile, its effectiveness has also been verified in many NLP tasks such as sentiment analysis (dos Santos and Gatti, 2014), parsing (Socher et al., 2013), summarization (Rush et al., 2015) and machine translation (Sutskever et al., 2014). With the advances of deep learning, there are growing works that design neural networks for relation extraction. (Socher et al., 2012) uses a recursive neural network in relation extraction, and (Xu et al., 2015; Miwa and Bansal, 2016) further use LSTM. (Zeng et al., 2014; dos Santos et al., 2015) adopt CNN in this task, and (Zeng et al., 2015; Lin et al., 2016) combine attention-based multiinstance learning which shows promising results. However, these above models merely learn"
D17-1186,P15-1061,0,0.200168,"d relation-specific training data. Manual annotation is very time consuming and labor intensive. One promising approach to address this limitation is distant supervision. (Mintz et al., 2009) generates training data automatically by aligning a KB with plain text. They assume that if two target entities have a relation in KB, then all sentences that contain these two entities will express this relation and can be regarded as a positive training instance. Since neural models have been verified to be effective for classifying relations from plain text (Socher et al., 2012; Zeng et al., 2014; dos Santos et al., 2015), (Zeng et al., 2015; Lin et al., 2016) incorporate neural networks method with distant supervision relation extraction. Further, (Ye et al., 2016) considers finer-grained information, and achieves the state-of-the-art performance. Although existing RE systems have achieved promising results with the help of distant supervision and neural models, they still suffer from a major drawback: the models only learn from those sentences contain both two target entities. However, those sentences containing only one of the entities could also provide useful information and help build inference chains. F"
D17-1186,P13-1045,0,0.0177896,"et al., 2012) adopt multi-instance multilabel learning in relation extraction. The main drawback of these methods is that they obtain most features directly from NLP tools with inevitable errors, and these errors will propagate to the relation extraction system and limit the performance. 2.2 Neural Relation Extraction Recently, deep learning (Bengio, 2009) has been successfully applied in various areas, including computer vision, speech recognition and so on. Meanwhile, its effectiveness has also been verified in many NLP tasks such as sentiment analysis (dos Santos and Gatti, 2014), parsing (Socher et al., 2013), summarization (Rush et al., 2015) and machine translation (Sutskever et al., 2014). With the advances of deep learning, there are growing works that design neural networks for relation extraction. (Socher et al., 2012) uses a recursive neural network in relation extraction, and (Xu et al., 2015; Miwa and Bansal, 2016) further use LSTM. (Zeng et al., 2014; dos Santos et al., 2015) adopt CNN in this task, and (Zeng et al., 2015; Lin et al., 2016) combine attention-based multiinstance learning which shows promising results. However, these above models merely learn from those sentences which dir"
D17-1186,D12-1110,0,0.15958,"from the issue that lacks sufficient labelled relation-specific training data. Manual annotation is very time consuming and labor intensive. One promising approach to address this limitation is distant supervision. (Mintz et al., 2009) generates training data automatically by aligning a KB with plain text. They assume that if two target entities have a relation in KB, then all sentences that contain these two entities will express this relation and can be regarded as a positive training instance. Since neural models have been verified to be effective for classifying relations from plain text (Socher et al., 2012; Zeng et al., 2014; dos Santos et al., 2015), (Zeng et al., 2015; Lin et al., 2016) incorporate neural networks method with distant supervision relation extraction. Further, (Ye et al., 2016) considers finer-grained information, and achieves the state-of-the-art performance. Although existing RE systems have achieved promising results with the help of distant supervision and neural models, they still suffer from a major drawback: the models only learn from those sentences contain both two target entities. However, those sentences containing only one of the entities could also provide useful i"
D17-1186,D12-1042,0,0.306622,"supervision. Afterward, (Mintz et al., 2009) aligns plain text with Freebase, by using distant supervision . However, most of these methods heuristically transform distant supervision to traditional supervised learning, by regarding it as a single-instance single-label problem, while in reality, one instance could correspond with multiple labels in different scenarios and vice versa. To alleviate the issue, (Riedel et al., 2010) regards each sentence as a training instance and allows multiple instances to share the same label but disallows more than one label. Further, (Hoffmann et al., 2011; Surdeanu et al., 2012) adopt multi-instance multilabel learning in relation extraction. The main drawback of these methods is that they obtain most features directly from NLP tools with inevitable errors, and these errors will propagate to the relation extraction system and limit the performance. 2.2 Neural Relation Extraction Recently, deep learning (Bengio, 2009) has been successfully applied in various areas, including computer vision, speech recognition and so on. Meanwhile, its effectiveness has also been verified in many NLP tasks such as sentiment analysis (dos Santos and Gatti, 2014), parsing (Socher et al."
D17-1186,D15-1206,0,0.0616817,"lation Extraction Recently, deep learning (Bengio, 2009) has been successfully applied in various areas, including computer vision, speech recognition and so on. Meanwhile, its effectiveness has also been verified in many NLP tasks such as sentiment analysis (dos Santos and Gatti, 2014), parsing (Socher et al., 2013), summarization (Rush et al., 2015) and machine translation (Sutskever et al., 2014). With the advances of deep learning, there are growing works that design neural networks for relation extraction. (Socher et al., 2012) uses a recursive neural network in relation extraction, and (Xu et al., 2015; Miwa and Bansal, 2016) further use LSTM. (Zeng et al., 2014; dos Santos et al., 2015) adopt CNN in this task, and (Zeng et al., 2015; Lin et al., 2016) combine attention-based multiinstance learning which shows promising results. However, these above models merely learn from those sentences which directly contain both two target entities. The important information of those relation paths hidden in the text is ignored. In this paper, we propose a novel path-based neural RE 1769 model to address this issue. Besides, although we choose CNN to test the effectiveness of our model, other neural mo"
D17-1186,P17-1166,0,0.0427788,"Missing"
D17-1186,D15-1203,0,0.654901,"ining data. Manual annotation is very time consuming and labor intensive. One promising approach to address this limitation is distant supervision. (Mintz et al., 2009) generates training data automatically by aligning a KB with plain text. They assume that if two target entities have a relation in KB, then all sentences that contain these two entities will express this relation and can be regarded as a positive training instance. Since neural models have been verified to be effective for classifying relations from plain text (Socher et al., 2012; Zeng et al., 2014; dos Santos et al., 2015), (Zeng et al., 2015; Lin et al., 2016) incorporate neural networks method with distant supervision relation extraction. Further, (Ye et al., 2016) considers finer-grained information, and achieves the state-of-the-art performance. Although existing RE systems have achieved promising results with the help of distant supervision and neural models, they still suffer from a major drawback: the models only learn from those sentences contain both two target entities. However, those sentences containing only one of the entities could also provide useful information and help build inference chains. For example, if we kn"
D17-1186,C14-1220,0,0.549229,"acks sufficient labelled relation-specific training data. Manual annotation is very time consuming and labor intensive. One promising approach to address this limitation is distant supervision. (Mintz et al., 2009) generates training data automatically by aligning a KB with plain text. They assume that if two target entities have a relation in KB, then all sentences that contain these two entities will express this relation and can be regarded as a positive training instance. Since neural models have been verified to be effective for classifying relations from plain text (Socher et al., 2012; Zeng et al., 2014; dos Santos et al., 2015), (Zeng et al., 2015; Lin et al., 2016) incorporate neural networks method with distant supervision relation extraction. Further, (Ye et al., 2016) considers finer-grained information, and achieves the state-of-the-art performance. Although existing RE systems have achieved promising results with the help of distant supervision and neural models, they still suffer from a major drawback: the models only learn from those sentences contain both two target entities. However, those sentences containing only one of the entities could also provide useful information and help"
D18-1021,P17-1019,0,0.0307064,"ness, and cross-lingual entity linking. The results, both qualitatively and quantitatively, demonstrate the signiﬁcance of our method. 1 Introduction Multi-lingual knowledge bases (KB) store millions of entities and facts in various languages, and provide rich background structural knowledge for understanding texts. On the other hand, text corpus contains huge amount of statistical information complementary to KBs. Many researchers leverage both types of resources to improve various natural language processing (NLP) tasks, such as machine reading (Yang and Mitchell, 2017), question answering (He et al., 2017; Hao et al., 2017). Most existing work jointly models KB and text corpus to enhance each other by learning word and entity representations in a uniﬁed vector space. For example, Wang et al. (2014); Yamada et al. (2016); Cao et al. (2017) utilize the co-occurrence information to align similar words and entities with similar embedding vectors. Toutanova et al. (2015); ∗ Corresponding author. 227 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 227–237 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics m"
D18-1021,P14-1006,0,0.0192455,"gual settings. Inspiringly, we investigate the task of crosslingual word embedding models (Ruder et al., 2017), and classify them into three groups according to parallel corpora used as supervisions: (i) methods requiring parallel corpus with aligned words as constraint for bilingual word embedding learning (Klementiev et al., 2012; Zou et al., 2013; Wu et al., 2014; Luong et al., 2015; Ammar et al., 2016; Soricut and Ding, 2016). (ii) methods using parallel sentences (i.e. translated sentence pairs) as the semantic composition of multi-lingual words (Gouws et al., 2015; Kociský et al., 2014; Hermann and Blunsom, 2014; Chandar et al., 2014; Shi et al., 2015; Mogadala and Rettinger, 2016). (iii) methods requiring bilingual lexicon to map words from one language into the other (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Xiao and Guo, 2014). The major weakness of these methods is the limited availability of parallel corpora. One remedy is to use existing multi-lingual resources (i.e. multilingual KB). Camacho-Collados et al. (2015) combines several KBs (Wikipedia, WordNet and BabelNet) and leverages multi-lingual synsets to learn word embeddings at sense level through an extra post-processing step. Artetx"
D18-1021,P17-1042,0,0.0188047,", 2014; Chandar et al., 2014; Shi et al., 2015; Mogadala and Rettinger, 2016). (iii) methods requiring bilingual lexicon to map words from one language into the other (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Xiao and Guo, 2014). The major weakness of these methods is the limited availability of parallel corpora. One remedy is to use existing multi-lingual resources (i.e. multilingual KB). Camacho-Collados et al. (2015) combines several KBs (Wikipedia, WordNet and BabelNet) and leverages multi-lingual synsets to learn word embeddings at sense level through an extra post-processing step. Artetxe et al. (2017) starts from a small bilingual lexicon and using a self-learning approach to induce the structural similarity of embedding spaces. Vulic and Moens (2015, 2016) collect comparable documents on same themes from multi-lingual Wikipedia, shufﬂe and merge them to build “pseudo bilingual documents” as training corpora. However, the quality of “pseudo bilingual documents” are difﬁcult to control, resulting in poor performance in several cross-lingual tasks (Vulic and Moens, 2016). Another remedy matches linguistic distribu• We proposed a novel method that jointly learns representations of not only cr"
D18-1021,W16-1614,0,0.0760808,"ation in different languages. For instance, two different meanings of word center in English are expressed by two different words in Chinese: center as the activity-specific building is expressed by 中心, center as the basketball player role is 中 锋. Our main challenge is the limited availability of parallel corpus, which is usually either expensive to obtain, or only available for certain narrow domains (Gouws et al., 2015). Many work has been done to alleviate the problem. One school of methods uses adversarial technique or domain adaption to match linguistic distribution (Zhang et al., 2017b; Barone, 2016; Cao et al., 2016). These methods do not require parallel corpora. The weakness is that the training process is unstable and that the high complexity restricts the methods only to small-scale data. Another line of work uses pre-existing multi-lingual resources to automatically generate “pseudo bilingual documents” (Vulic and Moens, 2015, 2016). However, negative results have been observed due to the occasional poor quality of training data (Vulic and Moens, 2016). All above methods only focus on words. We consider both words and entities, which Joint representation learning of words and entit"
D18-1021,D11-1072,0,0.345336,"Missing"
D18-1021,N15-1059,0,0.0262499,"and Ding, 2016). (ii) methods using parallel sentences (i.e. translated sentence pairs) as the semantic composition of multi-lingual words (Gouws et al., 2015; Kociský et al., 2014; Hermann and Blunsom, 2014; Chandar et al., 2014; Shi et al., 2015; Mogadala and Rettinger, 2016). (iii) methods requiring bilingual lexicon to map words from one language into the other (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Xiao and Guo, 2014). The major weakness of these methods is the limited availability of parallel corpora. One remedy is to use existing multi-lingual resources (i.e. multilingual KB). Camacho-Collados et al. (2015) combines several KBs (Wikipedia, WordNet and BabelNet) and leverages multi-lingual synsets to learn word embeddings at sense level through an extra post-processing step. Artetxe et al. (2017) starts from a small bilingual lexicon and using a self-learning approach to induce the structural similarity of embedding spaces. Vulic and Moens (2015, 2016) collect comparable documents on same themes from multi-lingual Wikipedia, shufﬂe and merge them to build “pseudo bilingual documents” as training corpora. However, the quality of “pseudo bilingual documents” are difﬁcult to control, resulting in po"
D18-1021,P11-1055,0,0.0450081,"ing (Barone, 2016; Zhang et al., 2017b; Lample et al., 2018), domain adaption (Cao et al., 2016). However, these methods suffer from the instability of training process and the high complexity. This either limits the scalability of vocabulary size or relies on a strong distribution assumption. Inspired by Vulic and Moens (2016), we generate highly qualiﬁed comparable sentences via distant supervision, which is one of the most promising approaches to addressing the issue of sparse training data, and performs well in relation extraction (Lin et al., 2017a; Mintz et al., 2009; Zeng et al., 2015; Hoffmann et al., 2011; Surdeanu et al., 2012). Our comparable sentences may further beneﬁt many other cross-lingual analysis, such as information retrieval (Dong et al., 2014). We use multi-lingual Wikipedia as KB including a set of entities E y = {eyi } and their articles. We concatenate these articles together, and form y ⟩. Hytext corpus Dy = ⟨w1y , . . . , wiy , . . . , w|D| per links in articles are denoted by Anchors Ay = {⟨wiy , eyj ⟩}, which indicates that word wiy refers to entity eyj . G y = (E y , Ry ) is the mono-lingual Entity Network (EN), where Ry = {⟨eyi , eyj ⟩} if there is a link between eyi , ey"
D18-1021,C16-1171,0,0.142029,"rent languages. For instance, two different meanings of word center in English are expressed by two different words in Chinese: center as the activity-specific building is expressed by 中心, center as the basketball player role is 中 锋. Our main challenge is the limited availability of parallel corpus, which is usually either expensive to obtain, or only available for certain narrow domains (Gouws et al., 2015). Many work has been done to alleviate the problem. One school of methods uses adversarial technique or domain adaption to match linguistic distribution (Zhang et al., 2017b; Barone, 2016; Cao et al., 2016). These methods do not require parallel corpora. The weakness is that the training process is unstable and that the high complexity restricts the methods only to small-scale data. Another line of work uses pre-existing multi-lingual resources to automatically generate “pseudo bilingual documents” (Vulic and Moens, 2015, 2016). However, negative results have been observed due to the occasional poor quality of training data (Vulic and Moens, 2016). All above methods only focus on words. We consider both words and entities, which Joint representation learning of words and entities beneﬁts many NL"
D18-1021,C18-1057,1,0.732516,"on mechanisms, knowledge attention and cross-lingual attention, to select informative data in comparable sentences. Our contributions can be concluded as follows: • We did qualitative analysis to have an intuitive impression of our embeddings, and quantitative analysis in three tasks: word translation, entity relatedness, and crosslingual entity linking. Experiment results show that our method demonstrates signiﬁcant improvements in all three tasks. 2 Related Work Jointly representation learning of words and entities attracts much attention in the ﬁelds of Entity Linking (Zhang et al., 2017a; Cao et al., 2018), Relation Extraction (Weston et al., 2013b) and so on, yet little work focuses on cross-lingual settings. Inspiringly, we investigate the task of crosslingual word embedding models (Ruder et al., 2017), and classify them into three groups according to parallel corpora used as supervisions: (i) methods requiring parallel corpus with aligned words as constraint for bilingual word embedding learning (Klementiev et al., 2012; Zou et al., 2013; Wu et al., 2014; Luong et al., 2015; Ammar et al., 2016; Soricut and Ding, 2016). (ii) methods using parallel sentences (i.e. translated sentence pairs) as"
D18-1021,P17-1149,1,0.0940722,"guages, and provide rich background structural knowledge for understanding texts. On the other hand, text corpus contains huge amount of statistical information complementary to KBs. Many researchers leverage both types of resources to improve various natural language processing (NLP) tasks, such as machine reading (Yang and Mitchell, 2017), question answering (He et al., 2017; Hao et al., 2017). Most existing work jointly models KB and text corpus to enhance each other by learning word and entity representations in a uniﬁed vector space. For example, Wang et al. (2014); Yamada et al. (2016); Cao et al. (2017) utilize the co-occurrence information to align similar words and entities with similar embedding vectors. Toutanova et al. (2015); ∗ Corresponding author. 227 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 227–237 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics makes the parallel data issue more challenging. In this paper, we propose a novel method for joint representation learning of cross-lingual words and entities. The basic idea is to capture mutually complementary knowledge in a shared seman"
D18-1021,D15-1077,1,0.905689,"Missing"
D18-1021,C12-1089,0,0.0596731,"t improvements in all three tasks. 2 Related Work Jointly representation learning of words and entities attracts much attention in the ﬁelds of Entity Linking (Zhang et al., 2017a; Cao et al., 2018), Relation Extraction (Weston et al., 2013b) and so on, yet little work focuses on cross-lingual settings. Inspiringly, we investigate the task of crosslingual word embedding models (Ruder et al., 2017), and classify them into three groups according to parallel corpora used as supervisions: (i) methods requiring parallel corpus with aligned words as constraint for bilingual word embedding learning (Klementiev et al., 2012; Zou et al., 2013; Wu et al., 2014; Luong et al., 2015; Ammar et al., 2016; Soricut and Ding, 2016). (ii) methods using parallel sentences (i.e. translated sentence pairs) as the semantic composition of multi-lingual words (Gouws et al., 2015; Kociský et al., 2014; Hermann and Blunsom, 2014; Chandar et al., 2014; Shi et al., 2015; Mogadala and Rettinger, 2016). (iii) methods requiring bilingual lexicon to map words from one language into the other (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Xiao and Guo, 2014). The major weakness of these methods is the limited availability of parallel co"
D18-1021,P14-2037,0,0.0166923,"k focuses on cross-lingual settings. Inspiringly, we investigate the task of crosslingual word embedding models (Ruder et al., 2017), and classify them into three groups according to parallel corpora used as supervisions: (i) methods requiring parallel corpus with aligned words as constraint for bilingual word embedding learning (Klementiev et al., 2012; Zou et al., 2013; Wu et al., 2014; Luong et al., 2015; Ammar et al., 2016; Soricut and Ding, 2016). (ii) methods using parallel sentences (i.e. translated sentence pairs) as the semantic composition of multi-lingual words (Gouws et al., 2015; Kociský et al., 2014; Hermann and Blunsom, 2014; Chandar et al., 2014; Shi et al., 2015; Mogadala and Rettinger, 2016). (iii) methods requiring bilingual lexicon to map words from one language into the other (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Xiao and Guo, 2014). The major weakness of these methods is the limited availability of parallel corpora. One remedy is to use existing multi-lingual resources (i.e. multilingual KB). Camacho-Collados et al. (2015) combines several KBs (Wikipedia, WordNet and BabelNet) and leverages multi-lingual synsets to learn word embeddings at sense level through an extra p"
D18-1021,C14-1192,0,0.0124692,"ing process and the high complexity. This either limits the scalability of vocabulary size or relies on a strong distribution assumption. Inspired by Vulic and Moens (2016), we generate highly qualiﬁed comparable sentences via distant supervision, which is one of the most promising approaches to addressing the issue of sparse training data, and performs well in relation extraction (Lin et al., 2017a; Mintz et al., 2009; Zeng et al., 2015; Hoffmann et al., 2011; Surdeanu et al., 2012). Our comparable sentences may further beneﬁt many other cross-lingual analysis, such as information retrieval (Dong et al., 2014). We use multi-lingual Wikipedia as KB including a set of entities E y = {eyi } and their articles. We concatenate these articles together, and form y ⟩. Hytext corpus Dy = ⟨w1y , . . . , wiy , . . . , w|D| per links in articles are denoted by Anchors Ay = {⟨wiy , eyj ⟩}, which indicates that word wiy refers to entity eyj . G y = (E y , Ry ) is the mono-lingual Entity Network (EN), where Ry = {⟨eyi , eyj ⟩} if there is a link between eyi , eyj . We use interlanguage links in Wikipedia as cross-lingual links en zh zh Ren−zh = {⟨een i , ei′ ⟩}, indicating ei , ei′ refer to the same thing in Engl"
D18-1021,E14-1049,0,0.0294359,"s requiring parallel corpus with aligned words as constraint for bilingual word embedding learning (Klementiev et al., 2012; Zou et al., 2013; Wu et al., 2014; Luong et al., 2015; Ammar et al., 2016; Soricut and Ding, 2016). (ii) methods using parallel sentences (i.e. translated sentence pairs) as the semantic composition of multi-lingual words (Gouws et al., 2015; Kociský et al., 2014; Hermann and Blunsom, 2014; Chandar et al., 2014; Shi et al., 2015; Mogadala and Rettinger, 2016). (iii) methods requiring bilingual lexicon to map words from one language into the other (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Xiao and Guo, 2014). The major weakness of these methods is the limited availability of parallel corpora. One remedy is to use existing multi-lingual resources (i.e. multilingual KB). Camacho-Collados et al. (2015) combines several KBs (Wikipedia, WordNet and BabelNet) and leverages multi-lingual synsets to learn word embeddings at sense level through an extra post-processing step. Artetxe et al. (2017) starts from a small bilingual lexicon and using a self-learning approach to induce the structural similarity of embedding spaces. Vulic and Moens (2015, 2016) collect comparable documents on"
D18-1021,P17-1004,1,0.0472832,"lines are cross-lingual links. tion via adversarial training (Barone, 2016; Zhang et al., 2017b; Lample et al., 2018), domain adaption (Cao et al., 2016). However, these methods suffer from the instability of training process and the high complexity. This either limits the scalability of vocabulary size or relies on a strong distribution assumption. Inspired by Vulic and Moens (2016), we generate highly qualiﬁed comparable sentences via distant supervision, which is one of the most promising approaches to addressing the issue of sparse training data, and performs well in relation extraction (Lin et al., 2017a; Mintz et al., 2009; Zeng et al., 2015; Hoffmann et al., 2011; Surdeanu et al., 2012). Our comparable sentences may further beneﬁt many other cross-lingual analysis, such as information retrieval (Dong et al., 2014). We use multi-lingual Wikipedia as KB including a set of entities E y = {eyi } and their articles. We concatenate these articles together, and form y ⟩. Hytext corpus Dy = ⟨w1y , . . . , wiy , . . . , w|D| per links in articles are denoted by Anchors Ay = {⟨wiy , eyj ⟩}, which indicates that word wiy refers to entity eyj . G y = (E y , Ry ) is the mono-lingual Entity Network (EN)"
D18-1021,D17-1277,0,0.0700897,"Missing"
D18-1021,W15-1521,0,0.0215281,"presentation learning of words and entities attracts much attention in the ﬁelds of Entity Linking (Zhang et al., 2017a; Cao et al., 2018), Relation Extraction (Weston et al., 2013b) and so on, yet little work focuses on cross-lingual settings. Inspiringly, we investigate the task of crosslingual word embedding models (Ruder et al., 2017), and classify them into three groups according to parallel corpora used as supervisions: (i) methods requiring parallel corpus with aligned words as constraint for bilingual word embedding learning (Klementiev et al., 2012; Zou et al., 2013; Wu et al., 2014; Luong et al., 2015; Ammar et al., 2016; Soricut and Ding, 2016). (ii) methods using parallel sentences (i.e. translated sentence pairs) as the semantic composition of multi-lingual words (Gouws et al., 2015; Kociský et al., 2014; Hermann and Blunsom, 2014; Chandar et al., 2014; Shi et al., 2015; Mogadala and Rettinger, 2016). (iii) methods requiring bilingual lexicon to map words from one language into the other (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Xiao and Guo, 2014). The major weakness of these methods is the limited availability of parallel corpora. One remedy is to use existing multi-lingual reso"
D18-1021,P17-1021,0,0.0296582,"ingual entity linking. The results, both qualitatively and quantitatively, demonstrate the signiﬁcance of our method. 1 Introduction Multi-lingual knowledge bases (KB) store millions of entities and facts in various languages, and provide rich background structural knowledge for understanding texts. On the other hand, text corpus contains huge amount of statistical information complementary to KBs. Many researchers leverage both types of resources to improve various natural language processing (NLP) tasks, such as machine reading (Yang and Mitchell, 2017), question answering (He et al., 2017; Hao et al., 2017). Most existing work jointly models KB and text corpus to enhance each other by learning word and entity representations in a uniﬁed vector space. For example, Wang et al. (2014); Yamada et al. (2016); Cao et al. (2017) utilize the co-occurrence information to align similar words and entities with similar embedding vectors. Toutanova et al. (2015); ∗ Corresponding author. 227 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 227–237 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics makes the parallel d"
D18-1021,D13-1136,0,0.144022,"ross-lingual attention, to select informative data in comparable sentences. Our contributions can be concluded as follows: • We did qualitative analysis to have an intuitive impression of our embeddings, and quantitative analysis in three tasks: word translation, entity relatedness, and crosslingual entity linking. Experiment results show that our method demonstrates signiﬁcant improvements in all three tasks. 2 Related Work Jointly representation learning of words and entities attracts much attention in the ﬁelds of Entity Linking (Zhang et al., 2017a; Cao et al., 2018), Relation Extraction (Weston et al., 2013b) and so on, yet little work focuses on cross-lingual settings. Inspiringly, we investigate the task of crosslingual word embedding models (Ruder et al., 2017), and classify them into three groups according to parallel corpora used as supervisions: (i) methods requiring parallel corpus with aligned words as constraint for bilingual word embedding learning (Klementiev et al., 2012; Zou et al., 2013; Wu et al., 2014; Luong et al., 2015; Ammar et al., 2016; Soricut and Ding, 2016). (ii) methods using parallel sentences (i.e. translated sentence pairs) as the semantic composition of multi-lingual"
D18-1021,D14-1015,0,0.0152179,"d Work Jointly representation learning of words and entities attracts much attention in the ﬁelds of Entity Linking (Zhang et al., 2017a; Cao et al., 2018), Relation Extraction (Weston et al., 2013b) and so on, yet little work focuses on cross-lingual settings. Inspiringly, we investigate the task of crosslingual word embedding models (Ruder et al., 2017), and classify them into three groups according to parallel corpora used as supervisions: (i) methods requiring parallel corpus with aligned words as constraint for bilingual word embedding learning (Klementiev et al., 2012; Zou et al., 2013; Wu et al., 2014; Luong et al., 2015; Ammar et al., 2016; Soricut and Ding, 2016). (ii) methods using parallel sentences (i.e. translated sentence pairs) as the semantic composition of multi-lingual words (Gouws et al., 2015; Kociský et al., 2014; Hermann and Blunsom, 2014; Chandar et al., 2014; Shi et al., 2015; Mogadala and Rettinger, 2016). (iii) methods requiring bilingual lexicon to map words from one language into the other (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Xiao and Guo, 2014). The major weakness of these methods is the limited availability of parallel corpora. One remedy is to use existin"
D18-1021,P09-1113,0,0.0343801,"ngual links. tion via adversarial training (Barone, 2016; Zhang et al., 2017b; Lample et al., 2018), domain adaption (Cao et al., 2016). However, these methods suffer from the instability of training process and the high complexity. This either limits the scalability of vocabulary size or relies on a strong distribution assumption. Inspired by Vulic and Moens (2016), we generate highly qualiﬁed comparable sentences via distant supervision, which is one of the most promising approaches to addressing the issue of sparse training data, and performs well in relation extraction (Lin et al., 2017a; Mintz et al., 2009; Zeng et al., 2015; Hoffmann et al., 2011; Surdeanu et al., 2012). Our comparable sentences may further beneﬁt many other cross-lingual analysis, such as information retrieval (Dong et al., 2014). We use multi-lingual Wikipedia as KB including a set of entities E y = {eyi } and their articles. We concatenate these articles together, and form y ⟩. Hytext corpus Dy = ⟨w1y , . . . , wiy , . . . , w|D| per links in articles are denoted by Anchors Ay = {⟨wiy , eyj ⟩}, which indicates that word wiy refers to entity eyj . G y = (E y , Ry ) is the mono-lingual Entity Network (EN), where Ry = {⟨eyi ,"
D18-1021,N16-1083,0,0.0126709,"word embedding models (Ruder et al., 2017), and classify them into three groups according to parallel corpora used as supervisions: (i) methods requiring parallel corpus with aligned words as constraint for bilingual word embedding learning (Klementiev et al., 2012; Zou et al., 2013; Wu et al., 2014; Luong et al., 2015; Ammar et al., 2016; Soricut and Ding, 2016). (ii) methods using parallel sentences (i.e. translated sentence pairs) as the semantic composition of multi-lingual words (Gouws et al., 2015; Kociský et al., 2014; Hermann and Blunsom, 2014; Chandar et al., 2014; Shi et al., 2015; Mogadala and Rettinger, 2016). (iii) methods requiring bilingual lexicon to map words from one language into the other (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Xiao and Guo, 2014). The major weakness of these methods is the limited availability of parallel corpora. One remedy is to use existing multi-lingual resources (i.e. multilingual KB). Camacho-Collados et al. (2015) combines several KBs (Wikipedia, WordNet and BabelNet) and leverages multi-lingual synsets to learn word embeddings at sense level through an extra post-processing step. Artetxe et al. (2017) starts from a small bilingual lexicon and using a self-"
D18-1021,W14-1613,0,0.0240869,"pus with aligned words as constraint for bilingual word embedding learning (Klementiev et al., 2012; Zou et al., 2013; Wu et al., 2014; Luong et al., 2015; Ammar et al., 2016; Soricut and Ding, 2016). (ii) methods using parallel sentences (i.e. translated sentence pairs) as the semantic composition of multi-lingual words (Gouws et al., 2015; Kociský et al., 2014; Hermann and Blunsom, 2014; Chandar et al., 2014; Shi et al., 2015; Mogadala and Rettinger, 2016). (iii) methods requiring bilingual lexicon to map words from one language into the other (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Xiao and Guo, 2014). The major weakness of these methods is the limited availability of parallel corpora. One remedy is to use existing multi-lingual resources (i.e. multilingual KB). Camacho-Collados et al. (2015) combines several KBs (Wikipedia, WordNet and BabelNet) and leverages multi-lingual synsets to learn word embeddings at sense level through an extra post-processing step. Artetxe et al. (2017) starts from a small bilingual lexicon and using a self-learning approach to induce the structural similarity of embedding spaces. Vulic and Moens (2015, 2016) collect comparable documents on same themes from mult"
D18-1021,K16-1025,0,0.105095,"d facts in various languages, and provide rich background structural knowledge for understanding texts. On the other hand, text corpus contains huge amount of statistical information complementary to KBs. Many researchers leverage both types of resources to improve various natural language processing (NLP) tasks, such as machine reading (Yang and Mitchell, 2017), question answering (He et al., 2017; Hao et al., 2017). Most existing work jointly models KB and text corpus to enhance each other by learning word and entity representations in a uniﬁed vector space. For example, Wang et al. (2014); Yamada et al. (2016); Cao et al. (2017) utilize the co-occurrence information to align similar words and entities with similar embedding vectors. Toutanova et al. (2015); ∗ Corresponding author. 227 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 227–237 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics makes the parallel data issue more challenging. In this paper, we propose a novel method for joint representation learning of cross-lingual words and entities. The basic idea is to capture mutually complementary knowledg"
D18-1021,Q17-1028,0,0.154248,"nces are not translated paired sentences, but sentences with the same topic in different languages. As shown in the middle layer (Figure 1), the pair of sentences are comparable sentences: (1) “Lawrence Michael Foust was an American basketball player who spent 12 seasons in NBA”, (2) “拉里·福斯特 (Lawrence Foust) 是 (was) 美国 (American) NBA 联盟 (association) 的 (of) 前 (former) 职业 (professional) 篮球 (basketball) 运动员 (player)”. Inspired by the distant supervision technique in relation extraction, we assume that sentence en sen k in Wikipedia articles of entity ei explicitly en or implicitly describes ei (Yamada et al., 2017), en and that sen k shall express a relation between ei en en en and ej if another entity ej is in sk . Meanwhile, we ﬁnd a comparable sentence szh k′ in another language which satisﬁes szh containing ezh k′ j′ Cross-lingual Supervision Data Generation This section introduces how to build a bilingual entity network G en−zh and comparable sentences S en−zh from a multi-lingual KB. 230 where xyi is either a word or an entity, and C(xyi ) denotes: (i) contextual words in a pre-deﬁned window of xyi if xyi ∈ Dy , (ii) neighbor entities that linked to xyi if xyi ∈ G y , (iii) contextual words of wjy"
D18-1021,P15-2093,1,0.857664,"sk of crosslingual word embedding models (Ruder et al., 2017), and classify them into three groups according to parallel corpora used as supervisions: (i) methods requiring parallel corpus with aligned words as constraint for bilingual word embedding learning (Klementiev et al., 2012; Zou et al., 2013; Wu et al., 2014; Luong et al., 2015; Ammar et al., 2016; Soricut and Ding, 2016). (ii) methods using parallel sentences (i.e. translated sentence pairs) as the semantic composition of multi-lingual words (Gouws et al., 2015; Kociský et al., 2014; Hermann and Blunsom, 2014; Chandar et al., 2014; Shi et al., 2015; Mogadala and Rettinger, 2016). (iii) methods requiring bilingual lexicon to map words from one language into the other (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Xiao and Guo, 2014). The major weakness of these methods is the limited availability of parallel corpora. One remedy is to use existing multi-lingual resources (i.e. multilingual KB). Camacho-Collados et al. (2015) combines several KBs (Wikipedia, WordNet and BabelNet) and leverages multi-lingual synsets to learn word embeddings at sense level through an extra post-processing step. Artetxe et al. (2017) starts from a small bili"
D18-1021,P17-1132,0,0.0240495,"three tasks: word translation, entity relatedness, and cross-lingual entity linking. The results, both qualitatively and quantitatively, demonstrate the signiﬁcance of our method. 1 Introduction Multi-lingual knowledge bases (KB) store millions of entities and facts in various languages, and provide rich background structural knowledge for understanding texts. On the other hand, text corpus contains huge amount of statistical information complementary to KBs. Many researchers leverage both types of resources to improve various natural language processing (NLP) tasks, such as machine reading (Yang and Mitchell, 2017), question answering (He et al., 2017; Hao et al., 2017). Most existing work jointly models KB and text corpus to enhance each other by learning word and entity representations in a uniﬁed vector space. For example, Wang et al. (2014); Yamada et al. (2016); Cao et al. (2017) utilize the co-occurrence information to align similar words and entities with similar embedding vectors. Toutanova et al. (2015); ∗ Corresponding author. 227 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 227–237 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Associ"
D18-1021,D15-1203,0,0.0622039,"a adversarial training (Barone, 2016; Zhang et al., 2017b; Lample et al., 2018), domain adaption (Cao et al., 2016). However, these methods suffer from the instability of training process and the high complexity. This either limits the scalability of vocabulary size or relies on a strong distribution assumption. Inspired by Vulic and Moens (2016), we generate highly qualiﬁed comparable sentences via distant supervision, which is one of the most promising approaches to addressing the issue of sparse training data, and performs well in relation extraction (Lin et al., 2017a; Mintz et al., 2009; Zeng et al., 2015; Hoffmann et al., 2011; Surdeanu et al., 2012). Our comparable sentences may further beneﬁt many other cross-lingual analysis, such as information retrieval (Dong et al., 2014). We use multi-lingual Wikipedia as KB including a set of entities E y = {eyi } and their articles. We concatenate these articles together, and form y ⟩. Hytext corpus Dy = ⟨w1y , . . . , wiy , . . . , w|D| per links in articles are denoted by Anchors Ay = {⟨wiy , eyj ⟩}, which indicates that word wiy refers to entity eyj . G y = (E y , Ry ) is the mono-lingual Entity Network (EN), where Ry = {⟨eyi , eyj ⟩} if there is"
D18-1021,D12-1042,0,0.0208627,"g et al., 2017b; Lample et al., 2018), domain adaption (Cao et al., 2016). However, these methods suffer from the instability of training process and the high complexity. This either limits the scalability of vocabulary size or relies on a strong distribution assumption. Inspired by Vulic and Moens (2016), we generate highly qualiﬁed comparable sentences via distant supervision, which is one of the most promising approaches to addressing the issue of sparse training data, and performs well in relation extraction (Lin et al., 2017a; Mintz et al., 2009; Zeng et al., 2015; Hoffmann et al., 2011; Surdeanu et al., 2012). Our comparable sentences may further beneﬁt many other cross-lingual analysis, such as information retrieval (Dong et al., 2014). We use multi-lingual Wikipedia as KB including a set of entities E y = {eyi } and their articles. We concatenate these articles together, and form y ⟩. Hytext corpus Dy = ⟨w1y , . . . , wiy , . . . , w|D| per links in articles are denoted by Anchors Ay = {⟨wiy , eyj ⟩}, which indicates that word wiy refers to entity eyj . G y = (E y , Ry ) is the mono-lingual Entity Network (EN), where Ry = {⟨eyi , eyj ⟩} if there is a link between eyi , eyj . We use interlanguage"
D18-1021,D15-1174,0,0.0359593,"ge amount of statistical information complementary to KBs. Many researchers leverage both types of resources to improve various natural language processing (NLP) tasks, such as machine reading (Yang and Mitchell, 2017), question answering (He et al., 2017; Hao et al., 2017). Most existing work jointly models KB and text corpus to enhance each other by learning word and entity representations in a uniﬁed vector space. For example, Wang et al. (2014); Yamada et al. (2016); Cao et al. (2017) utilize the co-occurrence information to align similar words and entities with similar embedding vectors. Toutanova et al. (2015); ∗ Corresponding author. 227 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 227–237 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics makes the parallel data issue more challenging. In this paper, we propose a novel method for joint representation learning of cross-lingual words and entities. The basic idea is to capture mutually complementary knowledge in a shared semantic space, which enables joint inference among cross-lingual knowledge base and texts without additional translations. We achieve"
D18-1021,P17-1179,0,0.0418929,"Missing"
D18-1021,P15-2118,0,0.0754233,"y either expensive to obtain, or only available for certain narrow domains (Gouws et al., 2015). Many work has been done to alleviate the problem. One school of methods uses adversarial technique or domain adaption to match linguistic distribution (Zhang et al., 2017b; Barone, 2016; Cao et al., 2016). These methods do not require parallel corpora. The weakness is that the training process is unstable and that the high complexity restricts the methods only to small-scale data. Another line of work uses pre-existing multi-lingual resources to automatically generate “pseudo bilingual documents” (Vulic and Moens, 2015, 2016). However, negative results have been observed due to the occasional poor quality of training data (Vulic and Moens, 2016). All above methods only focus on words. We consider both words and entities, which Joint representation learning of words and entities beneﬁts many NLP tasks, but has not been well explored in cross-lingual settings. In this paper, we propose a novel method for joint representation learning of cross-lingual words and entities. It captures mutually complementary knowledge, and enables cross-lingual inferences among knowledge bases and texts. Our method does not requi"
D18-1021,N16-1156,0,0.0217522,"Missing"
D18-1021,D14-1167,0,0.100342,"lions of entities and facts in various languages, and provide rich background structural knowledge for understanding texts. On the other hand, text corpus contains huge amount of statistical information complementary to KBs. Many researchers leverage both types of resources to improve various natural language processing (NLP) tasks, such as machine reading (Yang and Mitchell, 2017), question answering (He et al., 2017; Hao et al., 2017). Most existing work jointly models KB and text corpus to enhance each other by learning word and entity representations in a uniﬁed vector space. For example, Wang et al. (2014); Yamada et al. (2016); Cao et al. (2017) utilize the co-occurrence information to align similar words and entities with similar embedding vectors. Toutanova et al. (2015); ∗ Corresponding author. 227 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 227–237 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics makes the parallel data issue more challenging. In this paper, we propose a novel method for joint representation learning of cross-lingual words and entities. The basic idea is to capture mutually"
D18-1021,D13-1141,0,0.0258456,"ee tasks. 2 Related Work Jointly representation learning of words and entities attracts much attention in the ﬁelds of Entity Linking (Zhang et al., 2017a; Cao et al., 2018), Relation Extraction (Weston et al., 2013b) and so on, yet little work focuses on cross-lingual settings. Inspiringly, we investigate the task of crosslingual word embedding models (Ruder et al., 2017), and classify them into three groups according to parallel corpora used as supervisions: (i) methods requiring parallel corpus with aligned words as constraint for bilingual word embedding learning (Klementiev et al., 2012; Zou et al., 2013; Wu et al., 2014; Luong et al., 2015; Ammar et al., 2016; Soricut and Ding, 2016). (ii) methods using parallel sentences (i.e. translated sentence pairs) as the semantic composition of multi-lingual words (Gouws et al., 2015; Kociský et al., 2014; Hermann and Blunsom, 2014; Chandar et al., 2014; Shi et al., 2015; Mogadala and Rettinger, 2016). (iii) methods requiring bilingual lexicon to map words from one language into the other (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Xiao and Guo, 2014). The major weakness of these methods is the limited availability of parallel corpora. One remedy"
D18-1033,P17-1042,0,0.0122721,"16). Nevertheless, these works are all applied to word-based KBs such as WordNet, few works pay attention to how to incorporate the knowledge from sememe-based linguistic KBs. There also have been plenty of studies working on cross-lingual WRL (Upadhyay et al., 2016; Ruder, 2017). Most of them require parallel corpora (Zou et al., 2013; AP et al., 2014; Hermann and Blunsom, 2014; Kočiskỳ et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015). Some of them adopt unsupervised or weakly supervised methods (Mikolov et al., 2013b; Vulić and Moens, 2015; Conneau et al., 2017; Artetxe et al., 2017). There are also some works using a seed lexicon as the cross-lingual signal (Dinu et al., 2014; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Shi et al., 2015; Lu et al., 2015; Gouws et al., 2015; Wick et al., 2016; Ammar et al., 2016; Duong et al., 2016; Vulić and Korhonen, 2016). Related Work Since HowNet was published (Dong and Dong, 2003), it has attracted wide attention of re359 methods are not appropriate either as they are generally hard to learn high-quality bilingual word embeddings. Therefore, we choose the seed lexicon method in our model, and further introduce matching mechanism"
D18-1033,D15-1131,0,0.0220224,"Therefore, some works try to inject semantic information of KBs into WRL (Faruqui et al., 2015; Liu et al., 2015; Mrkšic et al., 2016; Bollegala et al., 2016). Nevertheless, these works are all applied to word-based KBs such as WordNet, few works pay attention to how to incorporate the knowledge from sememe-based linguistic KBs. There also have been plenty of studies working on cross-lingual WRL (Upadhyay et al., 2016; Ruder, 2017). Most of them require parallel corpora (Zou et al., 2013; AP et al., 2014; Hermann and Blunsom, 2014; Kočiskỳ et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015). Some of them adopt unsupervised or weakly supervised methods (Mikolov et al., 2013b; Vulić and Moens, 2015; Conneau et al., 2017; Artetxe et al., 2017). There are also some works using a seed lexicon as the cross-lingual signal (Dinu et al., 2014; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Shi et al., 2015; Lu et al., 2015; Gouws et al., 2015; Wick et al., 2016; Ammar et al., 2016; Duong et al., 2016; Vulić and Korhonen, 2016). Related Work Since HowNet was published (Dong and Dong, 2003), it has attracted wide attention of re359 methods are not appropriate either as they are generally"
D18-1033,P15-1027,0,0.0324231,"from sememe-based linguistic KBs. There also have been plenty of studies working on cross-lingual WRL (Upadhyay et al., 2016; Ruder, 2017). Most of them require parallel corpora (Zou et al., 2013; AP et al., 2014; Hermann and Blunsom, 2014; Kočiskỳ et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015). Some of them adopt unsupervised or weakly supervised methods (Mikolov et al., 2013b; Vulić and Moens, 2015; Conneau et al., 2017; Artetxe et al., 2017). There are also some works using a seed lexicon as the cross-lingual signal (Dinu et al., 2014; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Shi et al., 2015; Lu et al., 2015; Gouws et al., 2015; Wick et al., 2016; Ammar et al., 2016; Duong et al., 2016; Vulić and Korhonen, 2016). Related Work Since HowNet was published (Dong and Dong, 2003), it has attracted wide attention of re359 methods are not appropriate either as they are generally hard to learn high-quality bilingual word embeddings. Therefore, we choose the seed lexicon method in our model, and further introduce matching mechanism that is inspired by Zhang et al. (2017) to enhance its performance. 3 word embeddings. Skip-gram model is aimed at maximizing the predictive p"
D18-1033,J09-4006,1,0.731001,"wNet annotates sememes for 118, 346 Chinese words and 104, 025 English words. The number of sememes in total is 1, 983. Since some sememes only appear few times in HowNet, which are expected to be unimportant, we filter out those low-frequency sememes. Specifically, the frequency threshold is 5, and the final number of distinct sememes used in our experiments is 1, 400. In our experiments, Chinese is source language and English is target language. To learn Chinese and English monolingual word embeddings, we extract about 2.0G text from Sogou-T1 and Wikipedia2 respectively. And we use THULAC3 (Li and Sun, 2009) for Chinese word segmentation. As for seed lexicon, we build it in a similar way to Zhang et al. (2017). First, we employ Google Translation API4 to translate the source side (Chinese) vocabulary. Then the translations in the target language (English) are queried again in the reverse direction to translate back to the source language (Chinese). And we only keep the translation pairs whose back translated words match with the original source words. In the task of bilingual lexicon induction, we opt for Chinese-English Translation Lexicon Version 3.05 to be the gold standard. In the task of wor"
D18-1033,D16-1136,0,0.0173221,", 2016; Ruder, 2017). Most of them require parallel corpora (Zou et al., 2013; AP et al., 2014; Hermann and Blunsom, 2014; Kočiskỳ et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015). Some of them adopt unsupervised or weakly supervised methods (Mikolov et al., 2013b; Vulić and Moens, 2015; Conneau et al., 2017; Artetxe et al., 2017). There are also some works using a seed lexicon as the cross-lingual signal (Dinu et al., 2014; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Shi et al., 2015; Lu et al., 2015; Gouws et al., 2015; Wick et al., 2016; Ammar et al., 2016; Duong et al., 2016; Vulić and Korhonen, 2016). Related Work Since HowNet was published (Dong and Dong, 2003), it has attracted wide attention of re359 methods are not appropriate either as they are generally hard to learn high-quality bilingual word embeddings. Therefore, we choose the seed lexicon method in our model, and further introduce matching mechanism that is inspired by Zhang et al. (2017) to enhance its performance. 3 word embeddings. Skip-gram model is aimed at maximizing the predictive probability of context words conditioned on the centered word. Formally, taking the source side for example, given"
D18-1033,P15-1145,0,0.019521,"no large parallel corpora. Besides, unsupervised Our novel model adopts the method of word representation learning (WRL). Recent years have witnessed great advances in WRL. Models like Skip-gram, CBOW (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014) are immensely popular and achieve remarkable performance in many NLP tasks. However, most WRL methods learn distributional information of words from large corpora while the valuable information contained in semantic lexicons are disregarded. Therefore, some works try to inject semantic information of KBs into WRL (Faruqui et al., 2015; Liu et al., 2015; Mrkšic et al., 2016; Bollegala et al., 2016). Nevertheless, these works are all applied to word-based KBs such as WordNet, few works pay attention to how to incorporate the knowledge from sememe-based linguistic KBs. There also have been plenty of studies working on cross-lingual WRL (Upadhyay et al., 2016; Ruder, 2017). Most of them require parallel corpora (Zou et al., 2013; AP et al., 2014; Hermann and Blunsom, 2014; Kočiskỳ et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015). Some of them adopt unsupervised or weakly supervised methods (Mikolov et al., 2013b; Vu"
D18-1033,N15-1184,0,0.415789,"st language pairs have no large parallel corpora. Besides, unsupervised Our novel model adopts the method of word representation learning (WRL). Recent years have witnessed great advances in WRL. Models like Skip-gram, CBOW (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014) are immensely popular and achieve remarkable performance in many NLP tasks. However, most WRL methods learn distributional information of words from large corpora while the valuable information contained in semantic lexicons are disregarded. Therefore, some works try to inject semantic information of KBs into WRL (Faruqui et al., 2015; Liu et al., 2015; Mrkšic et al., 2016; Bollegala et al., 2016). Nevertheless, these works are all applied to word-based KBs such as WordNet, few works pay attention to how to incorporate the knowledge from sememe-based linguistic KBs. There also have been plenty of studies working on cross-lingual WRL (Upadhyay et al., 2016; Ruder, 2017). Most of them require parallel corpora (Zou et al., 2013; AP et al., 2014; Hermann and Blunsom, 2014; Kočiskỳ et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015). Some of them adopt unsupervised or weakly supervised methods (Mikolov"
D18-1033,O02-2003,0,0.939939,"ifferent from WordNet (Miller, 1995) which focuses on the relations between senses, it annotates each word with one or more relevant sememes. As illustrated in Fig. 1, the word apple has two senses including apple (fruit) and apple (brand) in HowNet. The sense apple (fruit) has one sememe fruit, and the sense apple (brand) has five sememes including computer, PatternValue, able, bring and SpecificBrand. There exist about 2, 000 sememes and over 100 thousand labeled Chinese and English words in HowNet. HowNet has been widely used in various NLP applications such as word similarity computation (Liu and Li, 2002), word sense disambiguation (Zhang et al., 2005), question classification (Sun et al., 2007) and sentiment classification (Dang and Zhang, 2010). However, most languages do not have such sememe-based linguistic KBs, which prevents us understanding and utilizing human languages to a greater extent. Therefore, it is important to build sememe-based linguistic KBs for various languages. Manual construction for sememebased linguistic KBs requires efforts of many linguistic experts, which is time-consuming and Words are regarded as the smallest meaningful unit of speech or writing that can stand by"
D18-1033,E14-1049,0,0.0357019,"corporate the knowledge from sememe-based linguistic KBs. There also have been plenty of studies working on cross-lingual WRL (Upadhyay et al., 2016; Ruder, 2017). Most of them require parallel corpora (Zou et al., 2013; AP et al., 2014; Hermann and Blunsom, 2014; Kočiskỳ et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015). Some of them adopt unsupervised or weakly supervised methods (Mikolov et al., 2013b; Vulić and Moens, 2015; Conneau et al., 2017; Artetxe et al., 2017). There are also some works using a seed lexicon as the cross-lingual signal (Dinu et al., 2014; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Shi et al., 2015; Lu et al., 2015; Gouws et al., 2015; Wick et al., 2016; Ammar et al., 2016; Duong et al., 2016; Vulić and Korhonen, 2016). Related Work Since HowNet was published (Dong and Dong, 2003), it has attracted wide attention of re359 methods are not appropriate either as they are generally hard to learn high-quality bilingual word embeddings. Therefore, we choose the seed lexicon method in our model, and further introduce matching mechanism that is inspired by Zhang et al. (2017) to enhance its performance. 3 word embeddings. Skip-gram model is aimed at max"
D18-1033,N15-1028,0,0.0172286,"so have been plenty of studies working on cross-lingual WRL (Upadhyay et al., 2016; Ruder, 2017). Most of them require parallel corpora (Zou et al., 2013; AP et al., 2014; Hermann and Blunsom, 2014; Kočiskỳ et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015). Some of them adopt unsupervised or weakly supervised methods (Mikolov et al., 2013b; Vulić and Moens, 2015; Conneau et al., 2017; Artetxe et al., 2017). There are also some works using a seed lexicon as the cross-lingual signal (Dinu et al., 2014; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Shi et al., 2015; Lu et al., 2015; Gouws et al., 2015; Wick et al., 2016; Ammar et al., 2016; Duong et al., 2016; Vulić and Korhonen, 2016). Related Work Since HowNet was published (Dong and Dong, 2003), it has attracted wide attention of re359 methods are not appropriate either as they are generally hard to learn high-quality bilingual word embeddings. Therefore, we choose the seed lexicon method in our model, and further introduce matching mechanism that is inspired by Zhang et al. (2017) to enhance its performance. 3 word embeddings. Skip-gram model is aimed at maximizing the predictive probability of context words conditi"
D18-1033,W15-1521,0,0.0159648,"ns are disregarded. Therefore, some works try to inject semantic information of KBs into WRL (Faruqui et al., 2015; Liu et al., 2015; Mrkšic et al., 2016; Bollegala et al., 2016). Nevertheless, these works are all applied to word-based KBs such as WordNet, few works pay attention to how to incorporate the knowledge from sememe-based linguistic KBs. There also have been plenty of studies working on cross-lingual WRL (Upadhyay et al., 2016; Ruder, 2017). Most of them require parallel corpora (Zou et al., 2013; AP et al., 2014; Hermann and Blunsom, 2014; Kočiskỳ et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015). Some of them adopt unsupervised or weakly supervised methods (Mikolov et al., 2013b; Vulić and Moens, 2015; Conneau et al., 2017; Artetxe et al., 2017). There are also some works using a seed lexicon as the cross-lingual signal (Dinu et al., 2014; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Shi et al., 2015; Lu et al., 2015; Gouws et al., 2015; Wick et al., 2016; Ammar et al., 2016; Duong et al., 2016; Vulić and Korhonen, 2016). Related Work Since HowNet was published (Dong and Dong, 2003), it has attracted wide attention of re359 methods are not appropriate eith"
D18-1033,D18-1493,1,0.722699,"mental results show that our proposed model could effectively predict lexical sememes for words with different frequencies in other languages. Our model also has consistent improvements on two auxiliary experiments including bilingual lexicon induction and monolingual word similarity computation by jointly learning the representations of sememes, words in source and target languages. searchers. Most of related works focus on applying HowNet to specific NLP tasks (Liu and Li, 2002; Zhang et al., 2005; Sun et al., 2007; Dang and Zhang, 2010; Fu et al., 2013; Niu et al., 2017; Zeng et al., 2018; Gu et al., 2018). To the best of our knowledge, only Xie et al. (2017) and Jin et al. (2018) conduct studies of augmenting HowNet by recommending sememes for new words. However, both of the two works are aimed to recommend sememes for monolingual words and not applicable to cross-lingual circumstance. Accordingly, our work is the first effort to automatically perform cross-lingual sememe prediction to enrich sememe-based linguistic KBs. 2 In terms of our cross-lingual sememe prediction task, parallel data-based bilingual WRL methods are unsuitable because most language pairs have no large parallel corpora. Be"
D18-1033,J15-4004,0,0.0334436,"source side (Chinese) vocabulary. Then the translations in the target language (English) are queried again in the reverse direction to translate back to the source language (Chinese). And we only keep the translation pairs whose back translated words match with the original source words. In the task of bilingual lexicon induction, we opt for Chinese-English Translation Lexicon Version 3.05 to be the gold standard. In the task of word similarity computation, we choose WordSim-240 and WordSim-297 (Jin and Wu, 2012) datasets for Chinese, and WordSim-353 (Finkelstein et al., 2002) and SimLex-999 (Hill et al., 2015) datasets for English to evaluate the performance of our 4.3 Cross-lingual Lexical Sememe Prediction We evaluate our model by recommending sememes for English words. In HowNet, many words have multiple sememes, so that sememe prediction can be regarded as a multi-label classification task. We use mean average precision (MAP) and F1 score to evaluate the sememe prediction results. We compare our model that incorporates sememe information with word relation-based approach (named CLSP-WR) and our model which jointly trains word and sememe embeddings (named CLSP-SE) with a baseline method BiLex (Z"
D18-1033,P17-1187,1,0.598384,"he effectiveness of our model. Experimental results show that our proposed model could effectively predict lexical sememes for words with different frequencies in other languages. Our model also has consistent improvements on two auxiliary experiments including bilingual lexicon induction and monolingual word similarity computation by jointly learning the representations of sememes, words in source and target languages. searchers. Most of related works focus on applying HowNet to specific NLP tasks (Liu and Li, 2002; Zhang et al., 2005; Sun et al., 2007; Dang and Zhang, 2010; Fu et al., 2013; Niu et al., 2017; Zeng et al., 2018; Gu et al., 2018). To the best of our knowledge, only Xie et al. (2017) and Jin et al. (2018) conduct studies of augmenting HowNet by recommending sememes for new words. However, both of the two works are aimed to recommend sememes for monolingual words and not applicable to cross-lingual circumstance. Accordingly, our work is the first effort to automatically perform cross-lingual sememe prediction to enrich sememe-based linguistic KBs. 2 In terms of our cross-lingual sememe prediction task, parallel data-based bilingual WRL methods are unsuitable because most language pai"
D18-1033,P18-1227,1,0.393378,"al sememes for words with different frequencies in other languages. Our model also has consistent improvements on two auxiliary experiments including bilingual lexicon induction and monolingual word similarity computation by jointly learning the representations of sememes, words in source and target languages. searchers. Most of related works focus on applying HowNet to specific NLP tasks (Liu and Li, 2002; Zhang et al., 2005; Sun et al., 2007; Dang and Zhang, 2010; Fu et al., 2013; Niu et al., 2017; Zeng et al., 2018; Gu et al., 2018). To the best of our knowledge, only Xie et al. (2017) and Jin et al. (2018) conduct studies of augmenting HowNet by recommending sememes for new words. However, both of the two works are aimed to recommend sememes for monolingual words and not applicable to cross-lingual circumstance. Accordingly, our work is the first effort to automatically perform cross-lingual sememe prediction to enrich sememe-based linguistic KBs. 2 In terms of our cross-lingual sememe prediction task, parallel data-based bilingual WRL methods are unsuitable because most language pairs have no large parallel corpora. Besides, unsupervised Our novel model adopts the method of word representation"
D18-1033,D14-1162,0,0.0869063,"sememes for monolingual words and not applicable to cross-lingual circumstance. Accordingly, our work is the first effort to automatically perform cross-lingual sememe prediction to enrich sememe-based linguistic KBs. 2 In terms of our cross-lingual sememe prediction task, parallel data-based bilingual WRL methods are unsuitable because most language pairs have no large parallel corpora. Besides, unsupervised Our novel model adopts the method of word representation learning (WRL). Recent years have witnessed great advances in WRL. Models like Skip-gram, CBOW (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014) are immensely popular and achieve remarkable performance in many NLP tasks. However, most WRL methods learn distributional information of words from large corpora while the valuable information contained in semantic lexicons are disregarded. Therefore, some works try to inject semantic information of KBs into WRL (Faruqui et al., 2015; Liu et al., 2015; Mrkšic et al., 2016; Bollegala et al., 2016). Nevertheless, these works are all applied to word-based KBs such as WordNet, few works pay attention to how to incorporate the knowledge from sememe-based linguistic KBs. There also have been plent"
D18-1033,S12-1049,0,0.0262445,"in a similar way to Zhang et al. (2017). First, we employ Google Translation API4 to translate the source side (Chinese) vocabulary. Then the translations in the target language (English) are queried again in the reverse direction to translate back to the source language (Chinese). And we only keep the translation pairs whose back translated words match with the original source words. In the task of bilingual lexicon induction, we opt for Chinese-English Translation Lexicon Version 3.05 to be the gold standard. In the task of word similarity computation, we choose WordSim-240 and WordSim-297 (Jin and Wu, 2012) datasets for Chinese, and WordSim-353 (Finkelstein et al., 2002) and SimLex-999 (Hill et al., 2015) datasets for English to evaluate the performance of our 4.3 Cross-lingual Lexical Sememe Prediction We evaluate our model by recommending sememes for English words. In HowNet, many words have multiple sememes, so that sememe prediction can be regarded as a multi-label classification task. We use mean average precision (MAP) and F1 score to evaluate the sememe prediction results. We compare our model that incorporates sememe information with word relation-based approach (named CLSP-WR) and our m"
D18-1033,P14-2037,0,0.0204874,"e information contained in semantic lexicons are disregarded. Therefore, some works try to inject semantic information of KBs into WRL (Faruqui et al., 2015; Liu et al., 2015; Mrkšic et al., 2016; Bollegala et al., 2016). Nevertheless, these works are all applied to word-based KBs such as WordNet, few works pay attention to how to incorporate the knowledge from sememe-based linguistic KBs. There also have been plenty of studies working on cross-lingual WRL (Upadhyay et al., 2016; Ruder, 2017). Most of them require parallel corpora (Zou et al., 2013; AP et al., 2014; Hermann and Blunsom, 2014; Kočiskỳ et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015). Some of them adopt unsupervised or weakly supervised methods (Mikolov et al., 2013b; Vulić and Moens, 2015; Conneau et al., 2017; Artetxe et al., 2017). There are also some works using a seed lexicon as the cross-lingual signal (Dinu et al., 2014; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Shi et al., 2015; Lu et al., 2015; Gouws et al., 2015; Wick et al., 2016; Ammar et al., 2016; Duong et al., 2016; Vulić and Korhonen, 2016). Related Work Since HowNet was published (Dong and Dong, 2003), it has attracted wide attention o"
D18-1033,P15-2093,1,0.853601,"stic KBs. There also have been plenty of studies working on cross-lingual WRL (Upadhyay et al., 2016; Ruder, 2017). Most of them require parallel corpora (Zou et al., 2013; AP et al., 2014; Hermann and Blunsom, 2014; Kočiskỳ et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015). Some of them adopt unsupervised or weakly supervised methods (Mikolov et al., 2013b; Vulić and Moens, 2015; Conneau et al., 2017; Artetxe et al., 2017). There are also some works using a seed lexicon as the cross-lingual signal (Dinu et al., 2014; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Shi et al., 2015; Lu et al., 2015; Gouws et al., 2015; Wick et al., 2016; Ammar et al., 2016; Duong et al., 2016; Vulić and Korhonen, 2016). Related Work Since HowNet was published (Dong and Dong, 2003), it has attracted wide attention of re359 methods are not appropriate either as they are generally hard to learn high-quality bilingual word embeddings. Therefore, we choose the seed lexicon method in our model, and further introduce matching mechanism that is inspired by Zhang et al. (2017) to enhance its performance. 3 word embeddings. Skip-gram model is aimed at maximizing the predictive probability of cont"
D18-1033,P16-1157,0,0.0124002,"le performance in many NLP tasks. However, most WRL methods learn distributional information of words from large corpora while the valuable information contained in semantic lexicons are disregarded. Therefore, some works try to inject semantic information of KBs into WRL (Faruqui et al., 2015; Liu et al., 2015; Mrkšic et al., 2016; Bollegala et al., 2016). Nevertheless, these works are all applied to word-based KBs such as WordNet, few works pay attention to how to incorporate the knowledge from sememe-based linguistic KBs. There also have been plenty of studies working on cross-lingual WRL (Upadhyay et al., 2016; Ruder, 2017). Most of them require parallel corpora (Zou et al., 2013; AP et al., 2014; Hermann and Blunsom, 2014; Kočiskỳ et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015). Some of them adopt unsupervised or weakly supervised methods (Mikolov et al., 2013b; Vulić and Moens, 2015; Conneau et al., 2017; Artetxe et al., 2017). There are also some works using a seed lexicon as the cross-lingual signal (Dinu et al., 2014; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Shi et al., 2015; Lu et al., 2015; Gouws et al., 2015; Wick et al., 2016; Ammar et al., 2016; Duong"
D18-1033,P16-1024,0,0.0127337,". Most of them require parallel corpora (Zou et al., 2013; AP et al., 2014; Hermann and Blunsom, 2014; Kočiskỳ et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015). Some of them adopt unsupervised or weakly supervised methods (Mikolov et al., 2013b; Vulić and Moens, 2015; Conneau et al., 2017; Artetxe et al., 2017). There are also some works using a seed lexicon as the cross-lingual signal (Dinu et al., 2014; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Shi et al., 2015; Lu et al., 2015; Gouws et al., 2015; Wick et al., 2016; Ammar et al., 2016; Duong et al., 2016; Vulić and Korhonen, 2016). Related Work Since HowNet was published (Dong and Dong, 2003), it has attracted wide attention of re359 methods are not appropriate either as they are generally hard to learn high-quality bilingual word embeddings. Therefore, we choose the seed lexicon method in our model, and further introduce matching mechanism that is inspired by Zhang et al. (2017) to enhance its performance. 3 word embeddings. Skip-gram model is aimed at maximizing the predictive probability of context words conditioned on the centered word. Formally, taking the source side for example, given a training word sequence {w"
D18-1033,P15-2118,0,0.0164701,"15; Mrkšic et al., 2016; Bollegala et al., 2016). Nevertheless, these works are all applied to word-based KBs such as WordNet, few works pay attention to how to incorporate the knowledge from sememe-based linguistic KBs. There also have been plenty of studies working on cross-lingual WRL (Upadhyay et al., 2016; Ruder, 2017). Most of them require parallel corpora (Zou et al., 2013; AP et al., 2014; Hermann and Blunsom, 2014; Kočiskỳ et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015). Some of them adopt unsupervised or weakly supervised methods (Mikolov et al., 2013b; Vulić and Moens, 2015; Conneau et al., 2017; Artetxe et al., 2017). There are also some works using a seed lexicon as the cross-lingual signal (Dinu et al., 2014; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Shi et al., 2015; Lu et al., 2015; Gouws et al., 2015; Wick et al., 2016; Ammar et al., 2016; Duong et al., 2016; Vulić and Korhonen, 2016). Related Work Since HowNet was published (Dong and Dong, 2003), it has attracted wide attention of re359 methods are not appropriate either as they are generally hard to learn high-quality bilingual word embeddings. Therefore, we choose the seed lexicon method in our mo"
D18-1033,D13-1141,0,0.039799,"onal information of words from large corpora while the valuable information contained in semantic lexicons are disregarded. Therefore, some works try to inject semantic information of KBs into WRL (Faruqui et al., 2015; Liu et al., 2015; Mrkšic et al., 2016; Bollegala et al., 2016). Nevertheless, these works are all applied to word-based KBs such as WordNet, few works pay attention to how to incorporate the knowledge from sememe-based linguistic KBs. There also have been plenty of studies working on cross-lingual WRL (Upadhyay et al., 2016; Ruder, 2017). Most of them require parallel corpora (Zou et al., 2013; AP et al., 2014; Hermann and Blunsom, 2014; Kočiskỳ et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015). Some of them adopt unsupervised or weakly supervised methods (Mikolov et al., 2013b; Vulić and Moens, 2015; Conneau et al., 2017; Artetxe et al., 2017). There are also some works using a seed lexicon as the cross-lingual signal (Dinu et al., 2014; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Shi et al., 2015; Lu et al., 2015; Gouws et al., 2015; Wick et al., 2016; Ammar et al., 2016; Duong et al., 2016; Vulić and Korhonen, 2016). Related Work Since HowNet was"
D18-1121,D15-1103,0,0.0488134,"labels has been considered (Ren et al., 2016b; Karn et al., 2017; Xu and Barbosa, 2018). However, to the best of our knowledge, precious Introduction Entity typing classifies semantic types of an entity mention in a context sentence, and can be beneficial for a large number of natural language processing tasks (Neelakantan and Chang, 2015), such as entity linking (Chabchoub et al., 2016), relation extraction (Miwa and Sasaki, 2014), and question answering (Yahya et al., 2013). Finegrained entity typing (FET) (Ling and Weld, 2012; Yosef et al., 2012; Yao et al., 2013; Gillick et al., 2014; Del Corro et al., 2015; Yogatama et al., 2015; Yaghoobzadeh and Sch¨utze, 2015; Ren et al., 2016a; Yuan and Downey, 2018) is based on a large set of fine-grained types and is therefore more challenging. So far, neural models (Dong et al., 2015; Shimaoka et al., 2017; Xin et al., 2018) have achieved state-of-the-art results on FET. All current FET models rely on distant supervision (DS) (Mintz et al., 2009) to obtain training ∗ Schwarzenegger was elected to be the governor. Schwarzenegger acted in the film Terminator. 1 Since entities are classified into labels of types, type and label have the same meaning in this"
D18-1121,E17-2119,0,0.131243,"Missing"
D18-1121,P09-1113,0,0.0272593,"Missing"
D18-1121,D14-1200,0,0.0221804,"the compatibility between the context sentence and each distantly supervised label, in an unsupervised manner using meaning of the label. In previous works, the hierarchical structure of labels has been considered (Ren et al., 2016b; Karn et al., 2017; Xu and Barbosa, 2018). However, to the best of our knowledge, precious Introduction Entity typing classifies semantic types of an entity mention in a context sentence, and can be beneficial for a large number of natural language processing tasks (Neelakantan and Chang, 2015), such as entity linking (Chabchoub et al., 2016), relation extraction (Miwa and Sasaki, 2014), and question answering (Yahya et al., 2013). Finegrained entity typing (FET) (Ling and Weld, 2012; Yosef et al., 2012; Yao et al., 2013; Gillick et al., 2014; Del Corro et al., 2015; Yogatama et al., 2015; Yaghoobzadeh and Sch¨utze, 2015; Ren et al., 2016a; Yuan and Downey, 2018) is based on a large set of fine-grained types and is therefore more challenging. So far, neural models (Dong et al., 2015; Shimaoka et al., 2017; Xin et al., 2018) have achieved state-of-the-art results on FET. All current FET models rely on distant supervision (DS) (Mintz et al., 2009) to obtain training ∗ Schwarze"
D18-1121,N15-1054,0,0.0241505,"this problem, we propose Entity Typing with Language Model Enhancement (LME). It is able to measure the compatibility between the context sentence and each distantly supervised label, in an unsupervised manner using meaning of the label. In previous works, the hierarchical structure of labels has been considered (Ren et al., 2016b; Karn et al., 2017; Xu and Barbosa, 2018). However, to the best of our knowledge, precious Introduction Entity typing classifies semantic types of an entity mention in a context sentence, and can be beneficial for a large number of natural language processing tasks (Neelakantan and Chang, 2015), such as entity linking (Chabchoub et al., 2016), relation extraction (Miwa and Sasaki, 2014), and question answering (Yahya et al., 2013). Finegrained entity typing (FET) (Ling and Weld, 2012; Yosef et al., 2012; Yao et al., 2013; Gillick et al., 2014; Del Corro et al., 2015; Yogatama et al., 2015; Yaghoobzadeh and Sch¨utze, 2015; Ren et al., 2016a; Yuan and Downey, 2018) is based on a large set of fine-grained types and is therefore more challenging. So far, neural models (Dong et al., 2015; Shimaoka et al., 2017; Xin et al., 2018) have achieved state-of-the-art results on FET. All current"
D18-1121,D14-1162,0,0.0812471,"IKI O NTO N OTES Train Development Test 2,000,000 251,039 10,000 2,202 563 8,963 Table 2: Number of instances in each part of datasets. 3.2 Experiment Settings The baseline for comparison is the hybrid model NFGEC proposed by Shimaoka et al. (2017). It is described as the ET module of our model. Our own model is referred to as NFGEC+LME. We implement our model based on the source code of NFGEC.3 For a fair comparison, the ET module is unchanged, including all hyperparameters and methods of parameter random initialization. Word embeddings are initialized with pretrained embeddings provided by Pennington et al. (2014). There are a few additional hyperparameters in our model. The most important one is λ, the weight between two parts of the loss function. Other ones include the learning rate r for pretraining the language model and the hidden size h of LSTM used in the language model. We perform (5) (6) where L is the matrix of all label embeddings, and Jlm is loss function of the language model used in the training phase. In order to ensure that label embeddings are in the same semantic space with word embeddings, L is initialized with word embeddings of the labels’ names. In the training phase, parameters"
D18-1121,D16-1144,0,0.702433,"tion. On the other hand, entity typing aims to predict context-dependent types of the entity mention, and test datasets are all human-labeled. The difference between DS and human annotation leads to a huge gap in performances between training/development and test dataset.2 To address this problem, we propose Entity Typing with Language Model Enhancement (LME). It is able to measure the compatibility between the context sentence and each distantly supervised label, in an unsupervised manner using meaning of the label. In previous works, the hierarchical structure of labels has been considered (Ren et al., 2016b; Karn et al., 2017; Xu and Barbosa, 2018). However, to the best of our knowledge, precious Introduction Entity typing classifies semantic types of an entity mention in a context sentence, and can be beneficial for a large number of natural language processing tasks (Neelakantan and Chang, 2015), such as entity linking (Chabchoub et al., 2016), relation extraction (Miwa and Sasaki, 2014), and question answering (Yahya et al., 2013). Finegrained entity typing (FET) (Ling and Weld, 2012; Yosef et al., 2012; Yao et al., 2013; Gillick et al., 2014; Del Corro et al., 2015; Yogatama et al., 2015; Y"
D18-1121,Q13-1030,0,0.0465428,"Missing"
D18-1121,E17-1119,0,0.216419,"language model predicts high probability for a reasonable sentence. Before applying the LME module to enhance the ET module, the language model is pre-trained with sentences from the training set. The loss function for s in the pre-train phase is: Jpre = LM({l1 , l2 , ..., e, r1 , r2 , ...}), 3 3.1 PT i=1 yi Li , Jlm = LM({l1 , l2 , ..., h, r1 , r2 , ...}), (4) Dataset W IKI O NTO N OTES Train Development Test 2,000,000 251,039 10,000 2,202 563 8,963 Table 2: Number of instances in each part of datasets. 3.2 Experiment Settings The baseline for comparison is the hybrid model NFGEC proposed by Shimaoka et al. (2017). It is described as the ET module of our model. Our own model is referred to as NFGEC+LME. We implement our model based on the source code of NFGEC.3 For a fair comparison, the ET module is unchanged, including all hyperparameters and methods of parameter random initialization. Word embeddings are initialized with pretrained embeddings provided by Pennington et al. (2014). There are a few additional hyperparameters in our model. The most important one is λ, the weight between two parts of the loss function. Other ones include the learning rate r for pretraining the language model and the hidd"
D18-1121,P12-1076,0,0.0756863,"Missing"
D18-1121,N18-1002,0,0.290333,"aims to predict context-dependent types of the entity mention, and test datasets are all human-labeled. The difference between DS and human annotation leads to a huge gap in performances between training/development and test dataset.2 To address this problem, we propose Entity Typing with Language Model Enhancement (LME). It is able to measure the compatibility between the context sentence and each distantly supervised label, in an unsupervised manner using meaning of the label. In previous works, the hierarchical structure of labels has been considered (Ren et al., 2016b; Karn et al., 2017; Xu and Barbosa, 2018). However, to the best of our knowledge, precious Introduction Entity typing classifies semantic types of an entity mention in a context sentence, and can be beneficial for a large number of natural language processing tasks (Neelakantan and Chang, 2015), such as entity linking (Chabchoub et al., 2016), relation extraction (Miwa and Sasaki, 2014), and question answering (Yahya et al., 2013). Finegrained entity typing (FET) (Ling and Weld, 2012; Yosef et al., 2012; Yao et al., 2013; Gillick et al., 2014; Del Corro et al., 2015; Yogatama et al., 2015; Yaghoobzadeh and Sch¨utze, 2015; Ren et al.,"
D18-1121,D15-1083,0,0.0905318,"Missing"
D18-1121,P15-2048,0,0.36254,"idered (Ren et al., 2016b; Karn et al., 2017; Xu and Barbosa, 2018). However, to the best of our knowledge, precious Introduction Entity typing classifies semantic types of an entity mention in a context sentence, and can be beneficial for a large number of natural language processing tasks (Neelakantan and Chang, 2015), such as entity linking (Chabchoub et al., 2016), relation extraction (Miwa and Sasaki, 2014), and question answering (Yahya et al., 2013). Finegrained entity typing (FET) (Ling and Weld, 2012; Yosef et al., 2012; Yao et al., 2013; Gillick et al., 2014; Del Corro et al., 2015; Yogatama et al., 2015; Yaghoobzadeh and Sch¨utze, 2015; Ren et al., 2016a; Yuan and Downey, 2018) is based on a large set of fine-grained types and is therefore more challenging. So far, neural models (Dong et al., 2015; Shimaoka et al., 2017; Xin et al., 2018) have achieved state-of-the-art results on FET. All current FET models rely on distant supervision (DS) (Mintz et al., 2009) to obtain training ∗ Schwarzenegger was elected to be the governor. Schwarzenegger acted in the film Terminator. 1 Since entities are classified into labels of types, type and label have the same meaning in this paper. 2 In the W IKI d"
D18-1121,C12-2133,0,0.0329572,"of the label. In previous works, the hierarchical structure of labels has been considered (Ren et al., 2016b; Karn et al., 2017; Xu and Barbosa, 2018). However, to the best of our knowledge, precious Introduction Entity typing classifies semantic types of an entity mention in a context sentence, and can be beneficial for a large number of natural language processing tasks (Neelakantan and Chang, 2015), such as entity linking (Chabchoub et al., 2016), relation extraction (Miwa and Sasaki, 2014), and question answering (Yahya et al., 2013). Finegrained entity typing (FET) (Ling and Weld, 2012; Yosef et al., 2012; Yao et al., 2013; Gillick et al., 2014; Del Corro et al., 2015; Yogatama et al., 2015; Yaghoobzadeh and Sch¨utze, 2015; Ren et al., 2016a; Yuan and Downey, 2018) is based on a large set of fine-grained types and is therefore more challenging. So far, neural models (Dong et al., 2015; Shimaoka et al., 2017; Xin et al., 2018) have achieved state-of-the-art results on FET. All current FET models rely on distant supervision (DS) (Mintz et al., 2009) to obtain training ∗ Schwarzenegger was elected to be the governor. Schwarzenegger acted in the film Terminator. 1 Since entities are classified int"
D18-1222,P15-1067,0,0.143848,"asets can be obtained from https:// github.com/davidlvxin/TransC. 1 Bob Chris David Instances Knowledge graphs (KGs) aim at semantically representing the world’s truth in the form of machinereadable graphs composed of triple facts. Knowledge graph embedding encodes each element (entities and relations) in knowledge graph into a continuous low-dimensional vector space. The learned representations make the knowledge graph essentially computable and have been proved to be helpful for knowledge graph completion and information extraction (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015b; Ji et al., 2015, 2016). corresponding author Associate Professor Assistant Professor Academic Staff Member Staff Member Concepts Figure 1: An example of concepts, instances, and isA transitivity. Introduction ∗ IsA Transitivity Professor In recent years, various knowledge graph embedding methods have been proposed, among which the translation-based models are simple and effective with good performances. Inspired by word2vec (Mikolov et al., 2013), given a triple (h, r, t), TransE learns vector embeddings h, r and t which satisfy r ≈ t − h. Afterwards, TransH (Wang et al., 2014), TransR/CTransR (Lin et al., 2"
D18-1222,D15-1082,1,0.85985,"Missing"
D18-1222,N15-1118,0,0.0724923,"Missing"
D18-1222,P16-1219,0,0.0158387,"types of entities and relations at the same time. Each relation-entity pair (r, e) will have a mapping matrix Mre to map entity embedding into relation vector space. And the projected vectors could be defined as h⊥ = Mrh h and t⊥ = Mrt t. The loss function of TransD is fr (h, t) = ||h⊥ + r − t⊥ ||22 . (4) There are many other translation-based models in recent years. For example, TranSparse (Ji et al., 2016) simplifies TransR by enforcing the sparseness on the projection matrix, PTransE (Lin et al., 2015a) considers relation paths as translations between entities for representation learning, (Xiao et al., 2016a) proposes a manifold-based embedding principle (ManifoldE) for precise link prediction, TransF (Feng et al., 2016) regards relation as translation between head entity vector and tail entity vector with flexible magnitude, (Xiao et al., 2016b) proposes a new generative model TransG, and KG2E (He et al., 2015) uses Gaussian embedding to model the data uncertainty. All these models can be seen in (Wang et al.). 2.2 3 External Information Learning Models External information like textual information is significant for knowledge representation. TEKE (Wang and Li, 2016) uses external context infor"
D18-1247,P11-1055,0,0.879452,"Parent Relation indicates equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) the corresponding relation of the entity pair in KGs. RE relies on distant supervision to scale up to large-scale training corpora. However, this automatic mechanism is inevitably accompanied by the wrong labeling problem, because not all sentences containing two entities can exactly express their relations in KGs, e.g., we may mistakenly label “Bill Gates retired from Microsoft” with the relation business/company/founders. To alleviate the wrong labeling problem, many efforts (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015) have been devoted to identifying valid instances from noisy data, especially the recent state-of-the-art attention-based methods (Lin et al., 2016; Ji et al., 2017; Liu et al., 2017; Wu et al., 2017). Nevertheless, each relation is handled in isolation in most existing methods. For each relation, there is often a separate model (e.g. neural attention scheme) to select relation-related informative instances from noisy data, regardless of rich semantic correlations among relations, typically located in the form of relation hierarchies. We take the KG F"
D18-1247,D17-1191,0,0.558533,"; Verga et al., 2016; Verga and McCallum, 2016). Zeng et al. (2015) employ the MIL scheme by selecting one most valid instance for distantly supervised neural relation extraction (NRE), whose denoising capability is far from satisfactory because most informative instances are neglected. Lin et al. (2016) and Zhang et al. (2017) propose neural attention schemes to select those informative instances. To further improve the attention performance, some works incorporate knowledge information (Zeng et al., 2017; Ji et al., 2017; Han et al., 2018) and advanced training strategies (Liu et al., 2017; Huang and Wang, 2017). More sophisticated mechanisms, such as reinforcement learning (Feng et al., 2018; Zeng et al., 2018) and adversarial training (Wu et al., 2017), have also been adapted for RE recently. However, most existing works model each relation in isolation to identify informative instances, neglecting rich correlations among relations, especially the hierarchical information of those relations. Hierarchical information is widely applied for model enhancement, especially for classification models (McCallum et al., 1998; Rousu et al., 2005; Weinberger and Chapelle, 2009; Zhao et al., 2011; Bi and Kwok,"
D18-1247,P16-1200,1,0.84698,"to large-scale training corpora. However, this automatic mechanism is inevitably accompanied by the wrong labeling problem, because not all sentences containing two entities can exactly express their relations in KGs, e.g., we may mistakenly label “Bill Gates retired from Microsoft” with the relation business/company/founders. To alleviate the wrong labeling problem, many efforts (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015) have been devoted to identifying valid instances from noisy data, especially the recent state-of-the-art attention-based methods (Lin et al., 2016; Ji et al., 2017; Liu et al., 2017; Wu et al., 2017). Nevertheless, each relation is handled in isolation in most existing methods. For each relation, there is often a separate model (e.g. neural attention scheme) to select relation-related informative instances from noisy data, regardless of rich semantic correlations among relations, typically located in the form of relation hierarchies. We take the KG Freebase (Bollacker et al., 2008) as an example, in which relations are labeled as hierarchical structures. For example, the 2236 Proceedings of the 2018 Conference on Empirical Methods in Na"
D18-1247,D17-1189,0,0.645343,"wever, this automatic mechanism is inevitably accompanied by the wrong labeling problem, because not all sentences containing two entities can exactly express their relations in KGs, e.g., we may mistakenly label “Bill Gates retired from Microsoft” with the relation business/company/founders. To alleviate the wrong labeling problem, many efforts (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015) have been devoted to identifying valid instances from noisy data, especially the recent state-of-the-art attention-based methods (Lin et al., 2016; Ji et al., 2017; Liu et al., 2017; Wu et al., 2017). Nevertheless, each relation is handled in isolation in most existing methods. For each relation, there is often a separate model (e.g. neural attention scheme) to select relation-related informative instances from noisy data, regardless of rich semantic correlations among relations, typically located in the form of relation hierarchies. We take the KG Freebase (Bollacker et al., 2008) as an example, in which relations are labeled as hierarchical structures. For example, the 2236 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 22"
D18-1247,P09-1113,0,0.988836,"Parent Relation Parent Relation Tail Entity: Davos Ernst Haefliger died on Saturday in Davos. 0.8 0.6 0.4 Ernst Haefliger was born in Davos on July 6, 1919 0.1 0.2 0.3 Ernst Haefliger was born in Davos, Switzerland. 0.1 0.2 0.3 Attention Scores Figure 1: An example of hierarchical relation extraction. Relation extraction (RE) aims to predict relational facts from plain text. Conventional supervised RE models (Zelenko et al., 2003; Mooney and Bunescu, 2006) usually suffer from the lack of high-quality training data, because manual labeling of training data is time-consuming and humanintensive. Mintz et al. (2009) propose distant supervision to automatically label training instances by aligning existing knowledge graphs (KGs) and text: For an entity pair in KGs, those sentences containing both the entities will be labeled with † /people/deceased_person Parent Relation /people/deceased_person/place_of_death Introduction ∗ Parent Relation indicates equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) the corresponding relation of the entity pair in KGs. RE relies on distant supervision to scale up to large-scale training corpora. However, this automatic mechanism is inevitably accompanie"
D18-1247,N13-1008,0,0.0863505,", especially for those long-tail relations. 2 Related Works Supervised models (Zelenko et al., 2003; Zhou et al., 2005; Mooney and Bunescu, 2006) for RE require adequate amounts of annotated data for their training. It is time-consuming to manually label large-scale training data. Hence, Mintz et al. (2009) propose distant supervision to automatically label data. Distant supervision inevitably accompanies with the wrong labeling problem. To alleviate the noise issue caused by distant supervision, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms. Riedel et al. (2013) propose universal schema to transmit information between relations of KGs and textual patterns to enhance extraction performance. These early RE methods mainly extract semantic features using NLP tools to build relation classifiers. Recently, neural models have been widely used for RE. These neural models can accurately capture textual relations without explicit linguistic analysis (Zeng et al., 2014; Xu et al., 2015; Santos et al., 2015; Zhang and Wang, 2015; Verga et al., 2016; Verga and McCallum, 2016). Zeng et al. (2015) employ the MIL scheme by selecting one most valid instance for dista"
D18-1247,D15-1206,0,0.0488813,"beling problem. To alleviate the noise issue caused by distant supervision, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms. Riedel et al. (2013) propose universal schema to transmit information between relations of KGs and textual patterns to enhance extraction performance. These early RE methods mainly extract semantic features using NLP tools to build relation classifiers. Recently, neural models have been widely used for RE. These neural models can accurately capture textual relations without explicit linguistic analysis (Zeng et al., 2014; Xu et al., 2015; Santos et al., 2015; Zhang and Wang, 2015; Verga et al., 2016; Verga and McCallum, 2016). Zeng et al. (2015) employ the MIL scheme by selecting one most valid instance for distantly supervised neural relation extraction (NRE), whose denoising capability is far from satisfactory because most informative instances are neglected. Lin et al. (2016) and Zhang et al. (2017) propose neural attention schemes to select those informative instances. To further improve the attention performance, some works incorporate knowledge information (Zeng et al., 2017; Ji et al., 2017; Han et al., 2018) and advan"
D18-1247,D15-1203,0,0.837594,"orresponding author: Z.Liu(liuzy@tsinghua.edu.cn) the corresponding relation of the entity pair in KGs. RE relies on distant supervision to scale up to large-scale training corpora. However, this automatic mechanism is inevitably accompanied by the wrong labeling problem, because not all sentences containing two entities can exactly express their relations in KGs, e.g., we may mistakenly label “Bill Gates retired from Microsoft” with the relation business/company/founders. To alleviate the wrong labeling problem, many efforts (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015) have been devoted to identifying valid instances from noisy data, especially the recent state-of-the-art attention-based methods (Lin et al., 2016; Ji et al., 2017; Liu et al., 2017; Wu et al., 2017). Nevertheless, each relation is handled in isolation in most existing methods. For each relation, there is often a separate model (e.g. neural attention scheme) to select relation-related informative instances from noisy data, regardless of rich semantic correlations among relations, typically located in the form of relation hierarchies. We take the KG Freebase (Bollacker et al., 2008) as an exam"
D18-1247,C14-1220,0,0.921179,"s with the wrong labeling problem. To alleviate the noise issue caused by distant supervision, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms. Riedel et al. (2013) propose universal schema to transmit information between relations of KGs and textual patterns to enhance extraction performance. These early RE methods mainly extract semantic features using NLP tools to build relation classifiers. Recently, neural models have been widely used for RE. These neural models can accurately capture textual relations without explicit linguistic analysis (Zeng et al., 2014; Xu et al., 2015; Santos et al., 2015; Zhang and Wang, 2015; Verga et al., 2016; Verga and McCallum, 2016). Zeng et al. (2015) employ the MIL scheme by selecting one most valid instance for distantly supervised neural relation extraction (NRE), whose denoising capability is far from satisfactory because most informative instances are neglected. Lin et al. (2016) and Zhang et al. (2017) propose neural attention schemes to select those informative instances. To further improve the attention performance, some works incorporate knowledge information (Zeng et al., 2017; Ji et al., 2017; Han et al."
D18-1247,D17-1186,1,0.936619,"xplicit linguistic analysis (Zeng et al., 2014; Xu et al., 2015; Santos et al., 2015; Zhang and Wang, 2015; Verga et al., 2016; Verga and McCallum, 2016). Zeng et al. (2015) employ the MIL scheme by selecting one most valid instance for distantly supervised neural relation extraction (NRE), whose denoising capability is far from satisfactory because most informative instances are neglected. Lin et al. (2016) and Zhang et al. (2017) propose neural attention schemes to select those informative instances. To further improve the attention performance, some works incorporate knowledge information (Zeng et al., 2017; Ji et al., 2017; Han et al., 2018) and advanced training strategies (Liu et al., 2017; Huang and Wang, 2017). More sophisticated mechanisms, such as reinforcement learning (Feng et al., 2018; Zeng et al., 2018) and adversarial training (Wu et al., 2017), have also been adapted for RE recently. However, most existing works model each relation in isolation to identify informative instances, neglecting rich correlations among relations, especially the hierarchical information of those relations. Hierarchical information is widely applied for model enhancement, especially for classification mode"
D18-1247,P15-1061,0,0.0757283,"o alleviate the noise issue caused by distant supervision, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms. Riedel et al. (2013) propose universal schema to transmit information between relations of KGs and textual patterns to enhance extraction performance. These early RE methods mainly extract semantic features using NLP tools to build relation classifiers. Recently, neural models have been widely used for RE. These neural models can accurately capture textual relations without explicit linguistic analysis (Zeng et al., 2014; Xu et al., 2015; Santos et al., 2015; Zhang and Wang, 2015; Verga et al., 2016; Verga and McCallum, 2016). Zeng et al. (2015) employ the MIL scheme by selecting one most valid instance for distantly supervised neural relation extraction (NRE), whose denoising capability is far from satisfactory because most informative instances are neglected. Lin et al. (2016) and Zhang et al. (2017) propose neural attention schemes to select those informative instances. To further improve the attention performance, some works incorporate knowledge information (Zeng et al., 2017; Ji et al., 2017; Han et al., 2018) and advanced training strategi"
D18-1247,D12-1042,0,0.778504,"es equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) the corresponding relation of the entity pair in KGs. RE relies on distant supervision to scale up to large-scale training corpora. However, this automatic mechanism is inevitably accompanied by the wrong labeling problem, because not all sentences containing two entities can exactly express their relations in KGs, e.g., we may mistakenly label “Bill Gates retired from Microsoft” with the relation business/company/founders. To alleviate the wrong labeling problem, many efforts (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015) have been devoted to identifying valid instances from noisy data, especially the recent state-of-the-art attention-based methods (Lin et al., 2016; Ji et al., 2017; Liu et al., 2017; Wu et al., 2017). Nevertheless, each relation is handled in isolation in most existing methods. For each relation, there is often a separate model (e.g. neural attention scheme) to select relation-related informative instances from noisy data, regardless of rich semantic correlations among relations, typically located in the form of relation hierarchies. We take the KG Freebase (Bollacker et a"
D18-1247,D17-1004,0,0.0499094,"tic features using NLP tools to build relation classifiers. Recently, neural models have been widely used for RE. These neural models can accurately capture textual relations without explicit linguistic analysis (Zeng et al., 2014; Xu et al., 2015; Santos et al., 2015; Zhang and Wang, 2015; Verga et al., 2016; Verga and McCallum, 2016). Zeng et al. (2015) employ the MIL scheme by selecting one most valid instance for distantly supervised neural relation extraction (NRE), whose denoising capability is far from satisfactory because most informative instances are neglected. Lin et al. (2016) and Zhang et al. (2017) propose neural attention schemes to select those informative instances. To further improve the attention performance, some works incorporate knowledge information (Zeng et al., 2017; Ji et al., 2017; Han et al., 2018) and advanced training strategies (Liu et al., 2017; Huang and Wang, 2017). More sophisticated mechanisms, such as reinforcement learning (Feng et al., 2018; Zeng et al., 2018) and adversarial training (Wu et al., 2017), have also been adapted for RE recently. However, most existing works model each relation in isolation to identify informative instances, neglecting rich correlat"
D18-1247,N16-1103,0,0.0621293,"nt supervision, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms. Riedel et al. (2013) propose universal schema to transmit information between relations of KGs and textual patterns to enhance extraction performance. These early RE methods mainly extract semantic features using NLP tools to build relation classifiers. Recently, neural models have been widely used for RE. These neural models can accurately capture textual relations without explicit linguistic analysis (Zeng et al., 2014; Xu et al., 2015; Santos et al., 2015; Zhang and Wang, 2015; Verga et al., 2016; Verga and McCallum, 2016). Zeng et al. (2015) employ the MIL scheme by selecting one most valid instance for distantly supervised neural relation extraction (NRE), whose denoising capability is far from satisfactory because most informative instances are neglected. Lin et al. (2016) and Zhang et al. (2017) propose neural attention schemes to select those informative instances. To further improve the attention performance, some works incorporate knowledge information (Zeng et al., 2017; Ji et al., 2017; Han et al., 2018) and advanced training strategies (Liu et al., 2017; Huang and Wang, 2017"
D18-1247,W16-1312,0,0.0321395,"el et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms. Riedel et al. (2013) propose universal schema to transmit information between relations of KGs and textual patterns to enhance extraction performance. These early RE methods mainly extract semantic features using NLP tools to build relation classifiers. Recently, neural models have been widely used for RE. These neural models can accurately capture textual relations without explicit linguistic analysis (Zeng et al., 2014; Xu et al., 2015; Santos et al., 2015; Zhang and Wang, 2015; Verga et al., 2016; Verga and McCallum, 2016). Zeng et al. (2015) employ the MIL scheme by selecting one most valid instance for distantly supervised neural relation extraction (NRE), whose denoising capability is far from satisfactory because most informative instances are neglected. Lin et al. (2016) and Zhang et al. (2017) propose neural attention schemes to select those informative instances. To further improve the attention performance, some works incorporate knowledge information (Zeng et al., 2017; Ji et al., 2017; Han et al., 2018) and advanced training strategies (Liu et al., 2017; Huang and Wang, 2017). More sophisticated mecha"
D18-1247,P05-1053,0,0.487507,"ction. Since there are more sufficient data for training the top-layer attention, the whole hierarchical attention scheme can enhance RE models for solving those long-tail relations. We conduct experiments on a large-scale benchmark dataset for RE in this paper. The experimental results show that the proposed coarse-tofine grained attention scheme based on relation hierarchies significantly outperforms other baseline methods, even as compared to the recent stateof-the-art attention-based models, especially for those long-tail relations. 2 Related Works Supervised models (Zelenko et al., 2003; Zhou et al., 2005; Mooney and Bunescu, 2006) for RE require adequate amounts of annotated data for their training. It is time-consuming to manually label large-scale training data. Hence, Mintz et al. (2009) propose distant supervision to automatically label data. Distant supervision inevitably accompanies with the wrong labeling problem. To alleviate the noise issue caused by distant supervision, Riedel et al. (2010) and Hoffmann et al. (2011) propose multi-instance learning (MIL) mechanisms. Riedel et al. (2013) propose universal schema to transmit information between relations of KGs and textual patterns to"
D18-1247,D17-1187,0,0.700834,"tic mechanism is inevitably accompanied by the wrong labeling problem, because not all sentences containing two entities can exactly express their relations in KGs, e.g., we may mistakenly label “Bill Gates retired from Microsoft” with the relation business/company/founders. To alleviate the wrong labeling problem, many efforts (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015) have been devoted to identifying valid instances from noisy data, especially the recent state-of-the-art attention-based methods (Lin et al., 2016; Ji et al., 2017; Liu et al., 2017; Wu et al., 2017). Nevertheless, each relation is handled in isolation in most existing methods. For each relation, there is often a separate model (e.g. neural attention scheme) to select relation-related informative instances from noisy data, regardless of rich semantic correlations among relations, typically located in the form of relation hierarchies. We take the KG Freebase (Bollacker et al., 2008) as an example, in which relations are labeled as hierarchical structures. For example, the 2236 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2236–2245 c Brussels"
D18-1390,N18-1172,0,0.0246302,"ommon approach in MTL. It assumes that each task owns its specific parameters and the distance between parameters in different tasks should be close to each other. For example, Duong et al. (2015) employ L2 distance for regularization, while Yang and Hospedales (2017) use the trace norm. Liu et al. (2016) introduce gates among task-specific RNN layers to control the information flow. Ruder et al. (2017) introduce a model which can decide the amount of sharing between different NLP tasks. There are also some works focusing on increasing tasks (Hashimoto et al., 2017) or handing unlabeled data (Augenstein et al., 2018). y1 y3 Task 1 Task 3 f f Fact Fact Encoder y4 y2 f y5 Task 5 f Task 2 Task 4 f f RNN Cell Fully Connection Encoder DAG Predictor Softmax Layer Figure 2: The framework of T OP J UDGE. In this work, we introduce a topological learning framework TOPJUDGE to handle multiple subtasks in LJP. Different to conventional MTL models which focus on how to share parameters among relevant tasks, TOPJUDGE models the explicit dependencies among these subtasks with scalable DAG forms. 3 Method In the following parts, we will first give the essential definitions of LJP task. We then introduce the DAG dependen"
D18-1390,P15-1166,0,0.034693,"nge of areas, including NLP (Collobert and Weston, 2008), speech recognition (Deng et al., 2013), and computer vision (Girshick, 2015; Mao et al., 2017). There have been numerous successful usages of MTL in NLP tasks. Most works follow the hard parameter sharing setting by sharing representations or some encoding layers among relevant tasks. For example, Collobert and Weston (2008) use shared word embeddings in solving part-of-speech tagging and semantic role labeling tasks. Liu et al. (2015) share the encoding layers of input queries to address query classification and information retrieval. Dong et al. (2015) and Luong et al. (2016) propose to share encoders or decoders to improve one (many) to many neural machine translation. Firat et al. (2016) propose to share attention mechanism in multi-way, multilingual machine translation. Besides hard parameter sharing, soft parameter sharing is another common approach in MTL. It assumes that each task owns its specific parameters and the distance between parameters in different tasks should be close to each other. For example, Duong et al. (2015) employ L2 distance for regularization, while Yang and Hospedales (2017) use the trace norm. Liu et al. (2016)"
D18-1390,P15-2139,0,0.0142335,"l. (2015) share the encoding layers of input queries to address query classification and information retrieval. Dong et al. (2015) and Luong et al. (2016) propose to share encoders or decoders to improve one (many) to many neural machine translation. Firat et al. (2016) propose to share attention mechanism in multi-way, multilingual machine translation. Besides hard parameter sharing, soft parameter sharing is another common approach in MTL. It assumes that each task owns its specific parameters and the distance between parameters in different tasks should be close to each other. For example, Duong et al. (2015) employ L2 distance for regularization, while Yang and Hospedales (2017) use the trace norm. Liu et al. (2016) introduce gates among task-specific RNN layers to control the information flow. Ruder et al. (2017) introduce a model which can decide the amount of sharing between different NLP tasks. There are also some works focusing on increasing tasks (Hashimoto et al., 2017) or handing unlabeled data (Augenstein et al., 2018). y1 y3 Task 1 Task 3 f f Fact Fact Encoder y4 y2 f y5 Task 5 f Task 2 Task 4 f f RNN Cell Fully Connection Encoder DAG Predictor Softmax Layer Figure 2: The framework of T"
D18-1390,N16-1101,0,0.0177558,"et al., 2017). There have been numerous successful usages of MTL in NLP tasks. Most works follow the hard parameter sharing setting by sharing representations or some encoding layers among relevant tasks. For example, Collobert and Weston (2008) use shared word embeddings in solving part-of-speech tagging and semantic role labeling tasks. Liu et al. (2015) share the encoding layers of input queries to address query classification and information retrieval. Dong et al. (2015) and Luong et al. (2016) propose to share encoders or decoders to improve one (many) to many neural machine translation. Firat et al. (2016) propose to share attention mechanism in multi-way, multilingual machine translation. Besides hard parameter sharing, soft parameter sharing is another common approach in MTL. It assumes that each task owns its specific parameters and the distance between parameters in different tasks should be close to each other. For example, Duong et al. (2015) employ L2 distance for regularization, while Yang and Hospedales (2017) use the trace norm. Liu et al. (2016) introduce gates among task-specific RNN layers to control the information flow. Ruder et al. (2017) introduce a model which can decide the a"
D18-1390,D17-1206,0,0.0212298,"ameter sharing, soft parameter sharing is another common approach in MTL. It assumes that each task owns its specific parameters and the distance between parameters in different tasks should be close to each other. For example, Duong et al. (2015) employ L2 distance for regularization, while Yang and Hospedales (2017) use the trace norm. Liu et al. (2016) introduce gates among task-specific RNN layers to control the information flow. Ruder et al. (2017) introduce a model which can decide the amount of sharing between different NLP tasks. There are also some works focusing on increasing tasks (Hashimoto et al., 2017) or handing unlabeled data (Augenstein et al., 2018). y1 y3 Task 1 Task 3 f f Fact Fact Encoder y4 y2 f y5 Task 5 f Task 2 Task 4 f f RNN Cell Fully Connection Encoder DAG Predictor Softmax Layer Figure 2: The framework of T OP J UDGE. In this work, we introduce a topological learning framework TOPJUDGE to handle multiple subtasks in LJP. Different to conventional MTL models which focus on how to share parameters among relevant tasks, TOPJUDGE models the explicit dependencies among these subtasks with scalable DAG forms. 3 Method In the following parts, we will first give the essential definit"
D18-1390,C18-1041,1,0.819815,"ical technique for the legal assistant system. On the one hand, LJP can provide low-cost but high-quality legal consulting services to the masses who are unfamiliar with legal terminology and the complex judgment procedures. On the other hand, it can serve as the handy reference for professionals (e.g., lawyers and judges) and improve their work efficiency. ∗ Indicates equal contribution. The order is determined by dice rolling. † Corresponding author. LJP has been studied for decades (Kort, 1957; Ulmer, 1963; Nagel, 1963; Keown, 1980; Segal, 1984; Lauderdale and Clark, 2012; Ye et al., 2018; Hu et al., 2018), and most existing works formalize LJP as a text classification task. For example, some works (Liu et al., 2004; Liu and Hsieh, 2006) propose to extract shallow textual features (e.g. characters, words, and phrases) for charge prediction. Katz et al. (2017) predict the US Supreme Court’s decisions based on efficient features from case profiles. Luo et al. (2017) propose an attention-based neural model for charge prediction by incorporating the relevant law articles. Despite these efforts in designing efficient features and employing advanced NLP techniques, LJP is still confronted with two ma"
D18-1390,D14-1181,0,0.128909,"ore researchersformalize this task under text classification frameworks. Most of these studies attempt to extract efficient features from text content (Liu and Hsieh, 2006; Lin et al., 2012; Aletras et al., 2016; Sulea et al., 2017) or case annotations (e.g., dates, terms, locations, and types) (Katz et al., 2017). However, these conventional methods can only utilize shallow textual features and manually designed factors, both require massive human efforts and usually suffer from the generalization issue when applied to other scenarios. Inspired by the success of neural networks on NLP tasks (Kim, 2014; Baharudin et al., 2010; Tang et al., 2015), researchers began to handle LJP by incorporating neural models with legal knowl3541 edge. For example, Luo et al. (2017) present an attention-based neural network that jointly models charge prediction and relevant article extraction. Hu et al. (2018) incorporate 10 discriminative legal attributes to predict few-shot and confusing charges. Nevertheless, these models are designed for specific subtasks and thus non-trivial to be extended to more subtasks of LJP with complex dependencies. Besides, Ye et al. (2018) utilize a Seq2Seq model to generate co"
D18-1390,O12-5004,0,0.158723,"diction Employing automatic analysis techniques for legal judgment has drawn attention from researchers in the legal field for decades. Early works usually focus on analyzing existing legal cases in specific scenarios with mathematical and statistical algorithms (Kort, 1957; Ulmer, 1963; Nagel, 1963; Keown, 1980; Segal, 1984; Lauderdale and Clark, 2012). With the development of machine learning and text mining techniques, more researchersformalize this task under text classification frameworks. Most of these studies attempt to extract efficient features from text content (Liu and Hsieh, 2006; Lin et al., 2012; Aletras et al., 2016; Sulea et al., 2017) or case annotations (e.g., dates, terms, locations, and types) (Katz et al., 2017). However, these conventional methods can only utilize shallow textual features and manually designed factors, both require massive human efforts and usually suffer from the generalization issue when applied to other scenarios. Inspired by the success of neural networks on NLP tasks (Kim, 2014; Baharudin et al., 2010; Tang et al., 2015), researchers began to handle LJP by incorporating neural models with legal knowl3541 edge. For example, Luo et al. (2017) present an at"
D18-1390,N15-1092,0,0.0247561,"solving them at the same time. It can transfer useful information among various tasks and has been applied to a wide range of areas, including NLP (Collobert and Weston, 2008), speech recognition (Deng et al., 2013), and computer vision (Girshick, 2015; Mao et al., 2017). There have been numerous successful usages of MTL in NLP tasks. Most works follow the hard parameter sharing setting by sharing representations or some encoding layers among relevant tasks. For example, Collobert and Weston (2008) use shared word embeddings in solving part-of-speech tagging and semantic role labeling tasks. Liu et al. (2015) share the encoding layers of input queries to address query classification and information retrieval. Dong et al. (2015) and Luong et al. (2016) propose to share encoders or decoders to improve one (many) to many neural machine translation. Firat et al. (2016) propose to share attention mechanism in multi-way, multilingual machine translation. Besides hard parameter sharing, soft parameter sharing is another common approach in MTL. It assumes that each task owns its specific parameters and the distance between parameters in different tasks should be close to each other. For example, Duong et"
D18-1390,D17-1289,0,0.460995,"dicates equal contribution. The order is determined by dice rolling. † Corresponding author. LJP has been studied for decades (Kort, 1957; Ulmer, 1963; Nagel, 1963; Keown, 1980; Segal, 1984; Lauderdale and Clark, 2012; Ye et al., 2018; Hu et al., 2018), and most existing works formalize LJP as a text classification task. For example, some works (Liu et al., 2004; Liu and Hsieh, 2006) propose to extract shallow textual features (e.g. characters, words, and phrases) for charge prediction. Katz et al. (2017) predict the US Supreme Court’s decisions based on efficient features from case profiles. Luo et al. (2017) propose an attention-based neural model for charge prediction by incorporating the relevant law articles. Despite these efforts in designing efficient features and employing advanced NLP techniques, LJP is still confronted with two major challenges: Multiple Subtasks in Legal Judgment: Practically, legal judgment usually consists of detailed and complicated subclauses, such as charges, the term of penalty, and fines. Specifically, for those countries with the civil law system (e.g., China, France, and Germany), the prediction of relevant articles is also considered to be one of the fundamenta"
D18-1390,N18-1168,0,0.0807583,"ons. It is a critical technique for the legal assistant system. On the one hand, LJP can provide low-cost but high-quality legal consulting services to the masses who are unfamiliar with legal terminology and the complex judgment procedures. On the other hand, it can serve as the handy reference for professionals (e.g., lawyers and judges) and improve their work efficiency. ∗ Indicates equal contribution. The order is determined by dice rolling. † Corresponding author. LJP has been studied for decades (Kort, 1957; Ulmer, 1963; Nagel, 1963; Keown, 1980; Segal, 1984; Lauderdale and Clark, 2012; Ye et al., 2018; Hu et al., 2018), and most existing works formalize LJP as a text classification task. For example, some works (Liu et al., 2004; Liu and Hsieh, 2006) propose to extract shallow textual features (e.g. characters, words, and phrases) for charge prediction. Katz et al. (2017) predict the US Supreme Court’s decisions based on efficient features from case profiles. Luo et al. (2017) propose an attention-based neural model for charge prediction by incorporating the relevant law articles. Despite these efforts in designing efficient features and employing advanced NLP techniques, LJP is still conf"
D18-1390,D15-1167,0,0.133013,"under text classification frameworks. Most of these studies attempt to extract efficient features from text content (Liu and Hsieh, 2006; Lin et al., 2012; Aletras et al., 2016; Sulea et al., 2017) or case annotations (e.g., dates, terms, locations, and types) (Katz et al., 2017). However, these conventional methods can only utilize shallow textual features and manually designed factors, both require massive human efforts and usually suffer from the generalization issue when applied to other scenarios. Inspired by the success of neural networks on NLP tasks (Kim, 2014; Baharudin et al., 2010; Tang et al., 2015), researchers began to handle LJP by incorporating neural models with legal knowl3541 edge. For example, Luo et al. (2017) present an attention-based neural network that jointly models charge prediction and relevant article extraction. Hu et al. (2018) incorporate 10 discriminative legal attributes to predict few-shot and confusing charges. Nevertheless, these models are designed for specific subtasks and thus non-trivial to be extended to more subtasks of LJP with complex dependencies. Besides, Ye et al. (2018) utilize a Seq2Seq model to generate court views with fact descriptions and predict"
D18-1493,P00-1041,0,0.0263717,"l. Introduction Language Modeling (LM) aims to measure the probability of a word sequence, reflecting its fluency and likelihood as a feasible sentence in a human language. Language Modeling is an essential component in a wide range of natural language processing (NLP) tasks, such as Machine Translation (Brown et al., 1990; Brants et al., 2007), Speech Recognition (Katz, 1987), Information Retrieval (Berger and Lafferty, 1999; Ponte ⇤ Equal contribution. † Correspondence author. context vector and Croft, 1998; Miller et al., 1999; Hiemstra, 1998) and Document Summarization (Rush et al., 2015; Banko et al., 2000). A probabilistic language model calculates the conditional probability of the next word given their contextual words, which are typically learned from large-scale text corpora. Taking the simplest language model for example, N-Gram estimates the conditional probabilities according to maximum likelihood over text corpora (Jurafsky, 2000). Recent years have also witnessed the advances of Recurrent Neural Networks (RNNs) as the state-of-the-art approach for language modeling (Mikolov et al., 2010), in which the context is represented as a low-dimensional hidden state to predict the next word. Th"
D18-1493,D07-1090,0,0.05538,". 1 (b) context vector word distribution Conventional Decoder Sememe-Driven Decoder Sememe Predictor sememe distribution Sense Predictor sense distribution Word Predictor word distribution Figure 1: Decoder of (a) Conventional Language Model, (b) Sememe-Driven Language Model. Introduction Language Modeling (LM) aims to measure the probability of a word sequence, reflecting its fluency and likelihood as a feasible sentence in a human language. Language Modeling is an essential component in a wide range of natural language processing (NLP) tasks, such as Machine Translation (Brown et al., 1990; Brants et al., 2007), Speech Recognition (Katz, 1987), Information Retrieval (Berger and Lafferty, 1999; Ponte ⇤ Equal contribution. † Correspondence author. context vector and Croft, 1998; Miller et al., 1999; Hiemstra, 1998) and Document Summarization (Rush et al., 2015; Banko et al., 2000). A probabilistic language model calculates the conditional probability of the next word given their contextual words, which are typically learned from large-scale text corpora. Taking the simplest language model for example, N-Gram estimates the conditional probabilities according to maximum likelihood over text corpora (Jur"
D18-1493,J90-2002,0,0.377825,"/thunlp/SDLM-pytorch. 1 (b) context vector word distribution Conventional Decoder Sememe-Driven Decoder Sememe Predictor sememe distribution Sense Predictor sense distribution Word Predictor word distribution Figure 1: Decoder of (a) Conventional Language Model, (b) Sememe-Driven Language Model. Introduction Language Modeling (LM) aims to measure the probability of a word sequence, reflecting its fluency and likelihood as a feasible sentence in a human language. Language Modeling is an essential component in a wide range of natural language processing (NLP) tasks, such as Machine Translation (Brown et al., 1990; Brants et al., 2007), Speech Recognition (Katz, 1987), Information Retrieval (Berger and Lafferty, 1999; Ponte ⇤ Equal contribution. † Correspondence author. context vector and Croft, 1998; Miller et al., 1999; Hiemstra, 1998) and Document Summarization (Rush et al., 2015; Banko et al., 2000). A probabilistic language model calculates the conditional probability of the next word given their contextual words, which are typically learned from large-scale text corpora. Taking the simplest language model for example, N-Gram estimates the conditional probabilities according to maximum likelihood"
D18-1493,P16-1154,0,0.0156496,"nal distribution is a categorical distribution, each expert is only responsible for making predictions on a subset of the categories (usually less than 10), so we call it Sparse Product of Experts. Headline Generation. Headline generation is a kind of text summarization tasks. In recent years, with the advances of RNNs, a lot of works have been done in this domain. The encoderdecoder models (Sutskever et al., 2014; Cho et al., 2014) have achieved great success in sequenceto-sequence learning. Rush et al. (2015) propose a local attention-based model for abstractive 4649 sentence summarization. Gu et al. (2016) introduce the copying mechanism which is close to the rote memorization of the human being. Ayana et al. (2016) employ the minimum risk training strategy to optimize model parameters. Different from these works, we focus on the decoder of the sequence-to-sequence model, and adopt SDLM to utilize sememe knowledge for sentence generation. 6 Conclusion and Further Work In this paper, we propose an interpretable Sememe-Driven Language Model with a hierarchical sememe-sense-word decoder. Besides interpretability, our model also achieves stateof-the-art performance in the Chinese Language Modeling"
D18-1493,D15-1229,0,0.0130324,"able senses. (3) Finally, the distribution of words could be easily calculated by marginalizing out the distribution of senses. We evaluate the performance of SDLM on the language modeling task using a Chinese newsi Note that although sememes are defined as the minimum semantic units, there still exist several sememes for capturing syntactic information. For example, the word å “with” corresponds to one specific sememe ü˝Õ “FunctWord”. paper corpus People’s Daily ii (Renmin Ribao), and also on the headline generation task using the Large Scale Chinese Short Text Summarization (LCSTS) dataset (Hu et al., 2015). Experimental results show that SDLM outperforms all those data-driven baseline models. We also conduct case studies to show that our model can effectively predict relevant sememes given context, which can improve the interpretability and robustness of language models. 2 Background Language models target at learning the joint probability of a sequence of words P (w1 , w2 , · · · , wn ), which isQ usually factorized as P (w1 , w2 , · · · , wn ) = nt=1 P (wt |w&lt;t ). Bengio et al. (2003) propose the first Neural Language Model as a feed-forward neural network. Mikolov et al. (2010) use RNN and a"
D18-1493,P18-1227,1,0.412388,"are organized in a tree structure and the word probability is calculated as the probability of always choosing the correct child along the path from the root node to the word node. While Morin and Bengio (2005) utilize knowledge from WordSememe. Recently, there are a lot of works concentrating on utilizing sememe knowledge in traditional natural language processing tasks. For example, Niu et al. (2017) use sememe knowledge to improve the quality of word embeddings and cope with the problem of word sense disambiguation. Xie et al. (2017) apply matrix factorization to predict sememes for words. Jin et al. (2018) improve their work by incorporating character-level information. Our work extends the previous works and tries to combine word-sense-sememe hierarchy with the sequential model. To be specific, this is the first work to improve the performance and interpretability of Neural Language Modeling with sememe knowledge. Product of Experts. As Hinton (1999, 2002) propose, the final probability can be calculated as the product of probabilities given by experts.Gales and Airey (2006) apply PoE to the speech recognition where each expert is a Gaussian mixture model. Unlike their work, in our SDLM, each"
D18-1493,O02-2003,0,0.830586,"s). These atomic semantic units are named sememes (Dong and Dong, 2006).i Since sememes are naturally implicit in human languages, linguists have devoted much effort to explicitly annotate lexical sememes for words and build linguistic common-sense knowledge bases. HowNet (Dong and Dong, 2006) is one of the representative sememe knowledge bases, which annotates each Chinese word sense with its sememes. The philosophy of HowNet regards the parts and attributes of a concept can be well represented by sememes. HowNet has been widely utilized in many NLP tasks such as word similarity computation (Liu, 2002) and sentiment analysis (Fu et al., 2013). However, less effort has been devoted to exploring its effectiveness in language models, especially neural language models. It is non-trivial for neural language models to incorporate discrete sememe knowledge, as it is not compatible with continuous representations in neural models. In this paper, we propose a Sememe-Driven Language Model (SDLM) to leverage lexical sememe knowledge. In order to predict the next word, we design a novel sememesense-word generation process: (1) We first estimate sememes’ distribution according to the context. (2) Regard"
D18-1493,J93-2004,0,0.060876,"Missing"
D18-1493,P17-1187,1,0.530918,"he whole vocabulary into different classes and uses a hierarchical softmax decoder to model the probability as P(word) = P(word|class)P(class), which is similar to our model. For the tree-based models, all words are organized in a tree structure and the word probability is calculated as the probability of always choosing the correct child along the path from the root node to the word node. While Morin and Bengio (2005) utilize knowledge from WordSememe. Recently, there are a lot of works concentrating on utilizing sememe knowledge in traditional natural language processing tasks. For example, Niu et al. (2017) use sememe knowledge to improve the quality of word embeddings and cope with the problem of word sense disambiguation. Xie et al. (2017) apply matrix factorization to predict sememes for words. Jin et al. (2018) improve their work by incorporating character-level information. Our work extends the previous works and tries to combine word-sense-sememe hierarchy with the sequential model. To be specific, this is the first work to improve the performance and interpretability of Neural Language Modeling with sememe knowledge. Product of Experts. As Hinton (1999, 2002) propose, the final probabilit"
D18-1493,E17-2025,0,0.015615,"Sense Predictor, that is P (w|g) = X P (s|g). (9) s2S (w) 3.4 Implementation Details Basis Matrix Actually, HowNet contains K ⇡ 2000 sememes. In practice, we cannot directly introduce K ⇥ H1 ⇥ H2 parameters, which might be computationally infeasible and lead to overfitting. To address this problem, we apply a weightsharing trick called the basis matrix. We use R basis matrices and their weighted sum to estimate Uk : Uk = R X ↵k,r Qr , (10) r=1 where Qr 2 RH1 ⇥H2 , ↵k,r &gt; 0 are trainable paP rameters, and R r=1 ↵k,r = 1. Weight Tying To incorporate the weight tying strategy (Inan et al., 2017; Press and Wolf, 2017), we use the same output embedding for multiple 4645 senses of a word. To be specific, the sense output embedding ws for each s 2 S (w) is the same as the word input embedding xw . 4 Experiments We evaluate our SDLM on a Chinese language modeling dataset, namely People’s Daily based on perplexity.iii Furthermore, to show that our SDLM structure can be a generic Chinese word-level decoder for sequence-to-sequence learning, we conduct a Chinese headline generation experiment on the LCSTS dataset. Finally, we explore the interpretability of our model with cases, showing the effectiveness of utili"
D18-1493,D18-1033,1,0.722743,"for large LSTM. Employing the weight tying strategy, we get Tied LSTM with better performance. We set LSTM and Tied LSTM of medium and large size as our baseline models and use the code from PyTorch examplesiv as their implementations. AWD-LSTM Based on several strategies for regularizing and optimizing LSTM-based language models, Merity et al. (2018) propose AWD-LSTM iii Although we only conduct experiments on Chinese corpora, we argue that this model has the potential to be applied to other languages in the light of works on construction sememe knowledge bases for other languages, such as (Qi et al., 2018). iv https://github.com/pytorch/examples/ tree/master/word_language_model as a three-layer neural network, which serves as a very strong baseline for word-level language modeling. We build it with the code released by the authorsv . Variants of Softmax Meanwhile, to compare our SDLM with other language modeling decoders, we set cHSM (Class-based Hierarchical Softmax) (Goodman, 2001), tHSM (Tree-based Hierarchical Softmax) (Mikolov et al., 2013) and MoS (Mixture of Softmaxes) (Yang et al., 2018) as the baseline add-on structures to the architectures above. Experimental Settings We apply our SDL"
D18-1493,D15-1044,0,0.116532,"riven Language Model. Introduction Language Modeling (LM) aims to measure the probability of a word sequence, reflecting its fluency and likelihood as a feasible sentence in a human language. Language Modeling is an essential component in a wide range of natural language processing (NLP) tasks, such as Machine Translation (Brown et al., 1990; Brants et al., 2007), Speech Recognition (Katz, 1987), Information Retrieval (Berger and Lafferty, 1999; Ponte ⇤ Equal contribution. † Correspondence author. context vector and Croft, 1998; Miller et al., 1999; Hiemstra, 1998) and Document Summarization (Rush et al., 2015; Banko et al., 2000). A probabilistic language model calculates the conditional probability of the next word given their contextual words, which are typically learned from large-scale text corpora. Taking the simplest language model for example, N-Gram estimates the conditional probabilities according to maximum likelihood over text corpora (Jurafsky, 2000). Recent years have also witnessed the advances of Recurrent Neural Networks (RNNs) as the state-of-the-art approach for language modeling (Mikolov et al., 2010), in which the context is represented as a low-dimensional hidden state to pred"
D18-1514,D17-1189,0,0.0448817,"Missing"
D18-1514,P17-1040,0,0.0127205,"ion. Recently, Yu et al. (2018) propose a multi-metric method for few-shot text classification. However, there lack systematic researches about adopting fewshot learning for NLP tasks. We propose FewRel: a new large-scale supervised Few-shot Relation Classification dataset. To address the wrong labeling problem in most distantly supervised RC datasets, we apply crowd-sourcing to manually remove the noise.i Besides constructing the dataset, we systematically implement the most recent state-of-theart few-shot learning methods and adapt them for i Many previous works, such as (Roth et al., 2013; Luo et al., 2017; Xin et al., 2018) have worked on automatically removing noise from distantly supervision. Instead, we use crowd-sourcing methods to achieve a high accuracy. RC. We conduct a detailed evaluation for all these models on our dataset. Though the state-of-theart few-shot learning methods have much lower results than humans on our challenging dataset, they significantly outperform the vanilla RC models, indicating that incorporating few-shot learning is promising and needs further research. In summary, our contribution is three-fold: (1) We formulate RC as a few-shot learning task, and propose a n"
D18-1514,C16-1017,0,0.0297203,"to learn fast-learning abilities from previous experience and rapidly generalize to new concepts. Many meta-learning models (Ravi and Larochelle, 2017; Santoro et al., 2016; Finn et al., 2017; Munkhdalai and Yu, 2017) achieve the state-of-the-art results on several few-shot benchmarks. Though meta-learning methods develop fast, most of these works evaluate on two popular datasets, Omniglot (Lake et al., 2015) and miniImageNet (Vinyals et al., 2016). Both the datasets concentrate on image classification. Many works in NLP mainly focus on the zero-shot/semisupervised scenario (Xie et al., 2016; Ma et al., 2016; Carlson et al., 2009), which incorporate extra information to classify objects never appearing in the training sets. However, the few-shot scenario needs models to classify objects with few instances without any extra information. Recently, Yu et al. (2018) propose a multi-metric method for few-shot text classification. However, there lack systematic researches about adopting fewshot learning for NLP tasks. We propose FewRel: a new large-scale supervised Few-shot Relation Classification dataset. To address the wrong labeling problem in most distantly supervised RC datasets, we apply crowd-so"
D18-1514,D15-1205,0,0.0717884,"Missing"
D18-1514,W09-2415,0,0.370152,"Missing"
D18-1514,P11-1055,0,0.360509,"Missing"
D18-1514,D17-1191,0,0.0382899,"Missing"
D18-1514,P16-1200,1,0.932224,"Missing"
D18-1514,strassel-etal-2008-linguistic,0,0.0427918,"irater kappa (Randolph, 2005), and keep the top 100 relations. 2.3 Dataset #cls. #insts. 9 24 42 57 100 6, 674 16, 771 21, 784 143, 391 70, 000 Table 3: Comparison of FewRel with existing RC datasets. Note that negative (no relation) instances in some datasets are ignored. idation, and testing respectively. Table 2 provides a comparison of our FewRel dataset to two other popular few-shot classification datasets, Omniglot and mini-ImageNet. Table 3 provides a comparison of FewRel to the previous RC datasets, including SemEval-2010 Task 8 dataset (Hendrickx et al., 2009), ACE 2003-2004 dataset (Strassel et al., 2008), TACRED dataset (Zhang et al., 2017), and NYT-10 dataset (Riedel et al., 2010). While some RC datasets contain instances with no relations (negative), we ignore such instances for comparison. 3 Experiments We conduct comprehensive evaluations of vanilla RC models with simple strategies such as finetune or kNN on our new dataset. We also evaluate the recent state-of-the-art few-shot learning methods. 3.1 Task Formulation In few-shot relation classification, we intend to obtain a function F : (R, S, x) 7→ y. Here R = {r1 , . . . , rm } defines the relations that the instances are classified int"
D18-1514,D12-1042,0,0.153708,"Missing"
D18-1514,D17-1187,0,0.0652808,"Missing"
D18-1514,D18-1121,1,0.796734,"et al. (2018) propose a multi-metric method for few-shot text classification. However, there lack systematic researches about adopting fewshot learning for NLP tasks. We propose FewRel: a new large-scale supervised Few-shot Relation Classification dataset. To address the wrong labeling problem in most distantly supervised RC datasets, we apply crowd-sourcing to manually remove the noise.i Besides constructing the dataset, we systematically implement the most recent state-of-theart few-shot learning methods and adapt them for i Many previous works, such as (Roth et al., 2013; Luo et al., 2017; Xin et al., 2018) have worked on automatically removing noise from distantly supervision. Instead, we use crowd-sourcing methods to achieve a high accuracy. RC. We conduct a detailed evaluation for all these models on our dataset. Though the state-of-theart few-shot learning methods have much lower results than humans on our challenging dataset, they significantly outperform the vanilla RC models, indicating that incorporating few-shot learning is promising and needs further research. In summary, our contribution is three-fold: (1) We formulate RC as a few-shot learning task, and propose a new large supervised"
D18-1514,N18-1109,0,0.0692706,"several few-shot benchmarks. Though meta-learning methods develop fast, most of these works evaluate on two popular datasets, Omniglot (Lake et al., 2015) and miniImageNet (Vinyals et al., 2016). Both the datasets concentrate on image classification. Many works in NLP mainly focus on the zero-shot/semisupervised scenario (Xie et al., 2016; Ma et al., 2016; Carlson et al., 2009), which incorporate extra information to classify objects never appearing in the training sets. However, the few-shot scenario needs models to classify objects with few instances without any extra information. Recently, Yu et al. (2018) propose a multi-metric method for few-shot text classification. However, there lack systematic researches about adopting fewshot learning for NLP tasks. We propose FewRel: a new large-scale supervised Few-shot Relation Classification dataset. To address the wrong labeling problem in most distantly supervised RC datasets, we apply crowd-sourcing to manually remove the noise.i Besides constructing the dataset, we systematically implement the most recent state-of-theart few-shot learning methods and adapt them for i Many previous works, such as (Roth et al., 2013; Luo et al., 2017; Xin et al., 2"
D18-1514,W02-1010,0,0.201432,"Missing"
D18-1514,D15-1203,0,0.119209,"nder four different settings. In recent research on few-shot learning, N way K shot setting is widely adopted. We follow this setting for the few-shot relation classification problem. To be exact, for N way K shot learning N = m = |R|, K = n1 = . . . = nm 3.2 (2) Experiment Settings We consider four types of few-shot tasks in our experiments: 5 way 1 shot, 5 way 5 shot, 10 way 1 shot, 10 way 5 shot. Under this setting, we evaluate different few-shot training strategies and stateof-the-art few-shot learning methods built upon two widely used instance encoders, CNN (Zeng et al., 2014) and PCNN (Zeng et al., 2015). For both CNN and PCNN, the sentence is first represented to the input vectors by transforming each word into concatenation of word embeddings and position embeddings. In CNN, the input vectors pass a convolution layer, a max-pooling layer, and a non-linear activation layer to get the final output sentence embedding. PCNN is a variant of CNN, which replaces the max-pooling operation with a piecewise max-pooling operation. To evaluate this two vanilla models in few-shot RC task, we first consider two training strategies, namely Finetune and kNN. For the Finetune baseline, it learns to classify"
D18-1514,C14-1220,0,0.364541,"viet writer. Test Instance (A) or (B) or (C) Euler was elected a foreign member of the Royal Swedish Academy of Sciences. Table 1: An example for a 3 way 2 shot scenario. Different colors indicate different entities, blue for head entity, and red for tail entity. Introduction Relation classification (RC) is an important task in NLP, aiming to determine the correct relation between two entities in a given sentence. Many works have been proposed for this task, including kernel methods (Zelenko et al., 2002; Mooney and Bunescu, 2006), embedding methods (Gormley et al., 2015), and neural methods (Zeng et al., 2014). The performance of these conventional models heavily depends on time-consuming and labor-intensive annotated data, which make themselves hard to generalize well. Adopting distant supervision is a primary approach to alleviate this problem for RC (Mintz et al.; Riedel et al.; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng ∗ The first four authors contribute equally. The order is determined by dice rolling. † Z. Wang is now at New York University. ‡ Correspondence author. et al., 2015; Lin et al., 2016), which heuristically aligns knowledge bases (KBs) and text to automatically annotate ad"
D18-1514,D17-1186,1,0.886952,"Missing"
D18-1514,D17-1004,0,0.0706106,"the top 100 relations. 2.3 Dataset #cls. #insts. 9 24 42 57 100 6, 674 16, 771 21, 784 143, 391 70, 000 Table 3: Comparison of FewRel with existing RC datasets. Note that negative (no relation) instances in some datasets are ignored. idation, and testing respectively. Table 2 provides a comparison of our FewRel dataset to two other popular few-shot classification datasets, Omniglot and mini-ImageNet. Table 3 provides a comparison of FewRel to the previous RC datasets, including SemEval-2010 Task 8 dataset (Hendrickx et al., 2009), ACE 2003-2004 dataset (Strassel et al., 2008), TACRED dataset (Zhang et al., 2017), and NYT-10 dataset (Riedel et al., 2010). While some RC datasets contain instances with no relations (negative), we ignore such instances for comparison. 3 Experiments We conduct comprehensive evaluations of vanilla RC models with simple strategies such as finetune or kNN on our new dataset. We also evaluate the recent state-of-the-art few-shot learning methods. 3.1 Task Formulation In few-shot relation classification, we intend to obtain a function F : (R, S, x) 7→ y. Here R = {r1 , . . . , rm } defines the relations that the instances are classified into. S is a support set S = {(x11 , r1"
D18-2024,P15-1067,0,0.573991,"nce and Technology, Beijing Normal University, Beijing, China Abstract knowledge embedding (KE) approaches have been proposed to embed both entities and relations in KGs into a continuous low-dimensional space, such as linear models (Bordes et al., 2011, 2012, 2014), latent factor models (Sutskever et al., 2009; Jenatton et al., 2012; Yang et al., 2015; Liu et al., 2017), neural models (Socher et al., 2013; Dong et al., 2014), matrix factorization models (Nickel et al., 2011, 2012, 2016; Trouillon et al., 2016), and translation models (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Ji et al., 2015). These models have achieved great performance on benchmark datasets. However, there exist two main issues which may lead to difficulty in full utilization and further development. On the one hand, the existing implementations are scattered and unsystematic to some extent. For example, the interfaces of these model implementations are inconsistent with each other. On the other hand, these model implementations mainly focus on model validation and are often time-consuming, which makes it difficult to apply them for realworld applications. Hence, it becomes urgent to develop an efficient and eff"
D19-1021,P08-1004,0,0.552062,"Missing"
D19-1021,P18-2065,0,0.125725,"-ended growth of new relation types in the open-domain corpora. To solve this problem, recently many efforts have been invested in exploring methods for open relation extraction (OpenRE), which aims to discover new relation types from unsupervised open-domain corpora. OpenRE methods can be roughly divided into two categories: taggingbased and clustering-based. Tagging-based methods cast OpenRE as a sequence labeling problem, and extract relational phrases consisting of words from sentences in unsupervised (Banko et al., 2007; Banko and Etzioni, 2008) or supervised paradigms (Jia et al., 2018; Cui et al., 2018; Stanovsky et al., 2018). However, tagging-based methods often extract multiple overly-speciﬁc relational phrases for the same relation type, and cannot be readily utilized for downstream tasks. In comparison, conventional clustering-based OpenRE methods extract rich features for relation instances via external linguistic tools, and cluster semantic patterns into several relation types (Lin and Pantel, 2001; Yao et al., 2011, 2012). Marcheggiani (2016) proposes a reconstructionbased model discrete-state variational autoencoder for OpenRE via unlabeled instances. Elsahar (2017) utilizes a clus"
D19-1021,D18-1514,1,0.907647,"01; Yao et al., 2011, 2012). Marcheggiani (2016) proposes a reconstructionbased model discrete-state variational autoencoder for OpenRE via unlabeled instances. Elsahar (2017) utilizes a clustering algorithm over linguistic features. In this paper, we focus on the clustering-based OpenRE methods, which have the advantage of discovering highly distinguishable relation types. Few-shot Learning. Few-shot learning aims to classify instances with a handful of labeled samples. Many efforts are devoted to few-shot image classiﬁcation (Koch et al., 2015) and relation classiﬁcation (Yuan et al., 2017; Han et al., 2018). Notably, (Koch et al., 2015) introduces Convolu220 Twain was a writer of America max FC vl classifier distance Kenji was a poet of Japan vd max FC a max-pooling layer, and a fully-connected (FC) layer. The embedding layer transforms the words in a sentence x and the positions of entities ehead and etail into pre-trained word embeddings and random-initialized position embeddings. Following (Zeng et al., 2014), we concatenate these embeddings to form a vector sequence. Next, a one-dimensional convolutional layer and a maxpooling layer transform the vector sequence into features. Finally, an FC"
D19-1021,P16-1200,1,0.917398,"Missing"
D19-1021,P14-5010,0,0.00429393,"via unlabeled instances. It optimizes a relation classiﬁer by reconstructing entities from pairing entities and predicted relation types. Rich features including entity words, context words, trigger words, dependency paths, and context POS tags are used to predict the relation type. RW-HAC and VAE both rely on external linguistic tools to extract rich features from plain texts. Speciﬁcally, we ﬁrst align entities to Wikidata and get their KB types. Next, we preprocess the instances with part-of-speech (POS) tagging, named-entity recognition (NER), and dependency parsing with Stanford CoreNLP (Manning et al., 2014). It is worth noting that these features are only used by baseline models. Our models, in contrast, only use sentences and entity pairs as inputs. Evaluation Protocol. In evaluation, we use B 3 metric (Bagga and Baldwin, 1998) as the scoring function. B 3 metric is a standard measure to balance the precision and recall of clustering tasks, and is commonly used in previous OpenRE works (Marcheggiani and Titov, 2016; Elsahar et al., 2017). To be speciﬁc, we use F1 measure, the harmonic mean of precision and recall. First, we report the result of supervised RSN with different clustering methods."
D19-1021,Q16-1017,0,0.394598,"ht our model’s ability to extract new relations, testing instances only contain new relations. indicates equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) 219 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 219–228, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 ers OpenRE as a clustering task for extracting triples with new relation types. However, previous clustering-based OpenRE methods (Yao et al., 2011, 2012; Marcheggiani and Titov, 2016; Elsahar et al., 2017) are mostly unsupervised, and cannot effectively select meaningful relation patterns and discard irrelevant information. In this paper, we propose to take advantage of high-quality supervised data of pre-deﬁned relations for OpenRE. The approach is non-trivial, however, due to the considerable gap between the pre-deﬁned relations and novel relations of interest in open domain. To bridge the gap, we propose Relational Siamese Networks (RSNs) to learn transferable relational knowledge from supervised data for OpenRE. Speciﬁcally, RSNs learn relational similarity metrics fr"
D19-1021,P09-1113,0,0.147833,"relations. To the best of our knowledge, RSN is the ﬁrst model to consider knowledge transfer in clustering-based OpenRE task. (2) We further propose Semi-supervised RSNs and Distantly-supervised RSNs that can learn from various weakly supervised scenarios. The experimental results show that all these RSN models achieve signiﬁcant improvements in F-measure compared with state-of-the-art baselines. Related Work Open Relation Extraction. Relation extraction (RE) is an important task in NLP. Traditional RE methods mainly concentrate on classifying relational facts into pre-deﬁned relation types (Mintz et al., 2009; Yu et al., 2017). Zeng (2014) utilizes CNN encoders to build sentence representations with the help of position embeddings. Lin (2016) further improves RE performance on distantlysupervised data via instance-level attention. These methods take advantage of supervised or distantlysupervised data to learn neural sentence encoders for distributed representations, and have achieved promising results. However, these methods cannot handle the open-ended growth of new relation types in the open-domain corpora. To solve this problem, recently many efforts have been invested in exploring methods for"
D19-1021,D14-1162,0,0.0822568,"for convenience. Experimental results show that the sample ratio decides RSN’s tendency to predict larger or smaller clusters. In other words, it controls the granularity of the predicted relation types. This phenomenon suggests a potential application of our model in hierarchical relation extraction. However, we leave any serious discussion to future work. Hyperparameter Settings. Following (Lin et al., 2016) and (Zeng et al., 2014), we ﬁx the less inﬂuencing hyperparameters for sentence encoding as their reported optimal values. For word embeddings, we use pre-trained 50-dimensional Glove (Pennington et al., 2014) word embeddings. For position embeddings, we use randominitialized 5-dimensional position embeddings. During training, all the embeddings are trainable. For the neural network, the number of feature 4.3 Experiment Results on OpenRE In this section, we demonstrate the effectiveness of our RSN models by comparing our models with state-of-the-art clustering-based OpenRE methods. We also conduct ablation experiments to detailedly investigate the contributions of different mechanisms of Semi-supervised RSN and Distantly-supervised RSN. Baselines. Conventional clustering-based OpenRE models usually"
D19-1021,N18-1081,0,0.039658,"ew relation types in the open-domain corpora. To solve this problem, recently many efforts have been invested in exploring methods for open relation extraction (OpenRE), which aims to discover new relation types from unsupervised open-domain corpora. OpenRE methods can be roughly divided into two categories: taggingbased and clustering-based. Tagging-based methods cast OpenRE as a sequence labeling problem, and extract relational phrases consisting of words from sentences in unsupervised (Banko et al., 2007; Banko and Etzioni, 2008) or supervised paradigms (Jia et al., 2018; Cui et al., 2018; Stanovsky et al., 2018). However, tagging-based methods often extract multiple overly-speciﬁc relational phrases for the same relation type, and cannot be readily utilized for downstream tasks. In comparison, conventional clustering-based OpenRE methods extract rich features for relation instances via external linguistic tools, and cluster semantic patterns into several relation types (Lin and Pantel, 2001; Yao et al., 2011, 2012). Marcheggiani (2016) proposes a reconstructionbased model discrete-state variational autoencoder for OpenRE via unlabeled instances. Elsahar (2017) utilizes a clustering algorithm over lin"
D19-1021,D11-1135,0,0.870996,"(Unlabeled) 1 To highlight our model’s ability to extract new relations, testing instances only contain new relations. indicates equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) 219 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 219–228, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 ers OpenRE as a clustering task for extracting triples with new relation types. However, previous clustering-based OpenRE methods (Yao et al., 2011, 2012; Marcheggiani and Titov, 2016; Elsahar et al., 2017) are mostly unsupervised, and cannot effectively select meaningful relation patterns and discard irrelevant information. In this paper, we propose to take advantage of high-quality supervised data of pre-deﬁned relations for OpenRE. The approach is non-trivial, however, due to the considerable gap between the pre-deﬁned relations and novel relations of interest in open domain. To bridge the gap, we propose Relational Siamese Networks (RSNs) to learn transferable relational knowledge from supervised data for OpenRE. Speciﬁcally, RSNs le"
D19-1021,P12-1075,0,0.11007,"n embeddings. During training, all the embeddings are trainable. For the neural network, the number of feature 4.3 Experiment Results on OpenRE In this section, we demonstrate the effectiveness of our RSN models by comparing our models with state-of-the-art clustering-based OpenRE methods. We also conduct ablation experiments to detailedly investigate the contributions of different mechanisms of Semi-supervised RSN and Distantly-supervised RSN. Baselines. Conventional clustering-based OpenRE models usually cluster instances by either clustering their linguistic features (Lin and Pantel, 2001; Yao et al., 2012; Elsahar et al., 2017) or imposing reconstruction constraints (Yao et al., 2011; Marcheggiani and Titov, 2016). To demonstrate 224 the effectiveness of our RSN models, we compare our models with two state-of-the-art models: (1) HAC with re-weighted word embeddings (RW-HAC) (Elsahar et al., 2017): RW-HAC is the state-of-the-art feature clustering model for OpenRE. The model ﬁrst extracts KB types and NER tags of entities as well as re-weighted word embeddings from sentences, then adopts principal component analysis (PCA) to reduce feature dimensionality, and ﬁnally uses HAC to cluster the conc"
D19-1021,I17-1086,0,0.0548203,"st of our knowledge, RSN is the ﬁrst model to consider knowledge transfer in clustering-based OpenRE task. (2) We further propose Semi-supervised RSNs and Distantly-supervised RSNs that can learn from various weakly supervised scenarios. The experimental results show that all these RSN models achieve signiﬁcant improvements in F-measure compared with state-of-the-art baselines. Related Work Open Relation Extraction. Relation extraction (RE) is an important task in NLP. Traditional RE methods mainly concentrate on classifying relational facts into pre-deﬁned relation types (Mintz et al., 2009; Yu et al., 2017). Zeng (2014) utilizes CNN encoders to build sentence representations with the help of position embeddings. Lin (2016) further improves RE performance on distantlysupervised data via instance-level attention. These methods take advantage of supervised or distantlysupervised data to learn neural sentence encoders for distributed representations, and have achieved promising results. However, these methods cannot handle the open-ended growth of new relation types in the open-domain corpora. To solve this problem, recently many efforts have been invested in exploring methods for open relation extr"
D19-1021,C14-1220,0,0.139899,"g aims to classify instances with a handful of labeled samples. Many efforts are devoted to few-shot image classiﬁcation (Koch et al., 2015) and relation classiﬁcation (Yuan et al., 2017; Han et al., 2018). Notably, (Koch et al., 2015) introduces Convolu220 Twain was a writer of America max FC vl classifier distance Kenji was a poet of Japan vd max FC a max-pooling layer, and a fully-connected (FC) layer. The embedding layer transforms the words in a sentence x and the positions of entities ehead and etail into pre-trained word embeddings and random-initialized position embeddings. Following (Zeng et al., 2014), we concatenate these embeddings to form a vector sequence. Next, a one-dimensional convolutional layer and a maxpooling layer transform the vector sequence into features. Finally, an FC layer with sigmoid activation maps features into a relational vector v. To summarize, we obtain a vector representation v for a relational sentence with our CNN module: 0.7 p vr word position embeddings embeddings Figure 2: The architecture of Relational Siamese Networks. The output is the similarity between two relational instances. v = CNN(s), in which we denote the joint information of a sentence x and two"
D19-1025,C18-1057,1,0.811724,"ifficult for people to accurately annotate those entities that they do not know or are not interested in. We can construct them from online resources, such as the anchors in Wikipedia. However, the following natures of WL data make learning name tagging from them more challenging: Partially-Labeled Sequence Automatically Introduction Name tagging2 is the task of identifying the boundaries of entity mentions in texts and classifying them into the pre-defined entity types (e.g., person). It serves as a fundamental role as providing the essential inputs for many IE tasks, such as Entity Linking (Cao et al., 2018a) and Relation Extraction (Lin et al., 2017). Many recent methods utilize a neural network (NN) with Conditional Random Fields (CRFs) (Lafferty et al., 2001) by treating name tagging as a sequence labeling problem (Lample 1 Our project can be found in https://github.com/ zig-kwin-hu/Low-Resource-Name-Tagging. 2 Someone may call it Named Entity Recognition (NER). 261 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 261–270, c Hong Kong, China, November 3–7, 2019. 2019 Associat"
D19-1025,D18-1021,1,0.843847,"ifficult for people to accurately annotate those entities that they do not know or are not interested in. We can construct them from online resources, such as the anchors in Wikipedia. However, the following natures of WL data make learning name tagging from them more challenging: Partially-Labeled Sequence Automatically Introduction Name tagging2 is the task of identifying the boundaries of entity mentions in texts and classifying them into the pre-defined entity types (e.g., person). It serves as a fundamental role as providing the essential inputs for many IE tasks, such as Entity Linking (Cao et al., 2018a) and Relation Extraction (Lin et al., 2017). Many recent methods utilize a neural network (NN) with Conditional Random Fields (CRFs) (Lafferty et al., 2001) by treating name tagging as a sequence labeling problem (Lample 1 Our project can be found in https://github.com/ zig-kwin-hu/Low-Resource-Name-Tagging. 2 Someone may call it Named Entity Recognition (NER). 261 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 261–270, c Hong Kong, China, November 3–7, 2019. 2019 Associat"
D19-1025,P17-1149,1,0.905559,"Missing"
D19-1025,P19-1140,1,0.887209,"Missing"
D19-1025,Q16-1026,0,0.0350068,"mation, which shall benefit many applications, such as information extraction (Zhang et al., 2017; Kuang et al., 2019; Cao et al., 2019a) and recommendation (Wang et al., 2019; Cao et al., 2019b). It can be treated as either a multiclass classification problem (Hammerton, 2003; Xu et al., 2017) or a sequence labeling problem (Collobert et al., 2011), but very little work combined them together. The difference between them mainly lies in whether the method models sequential label constraints, which have been demonstrated effective in many NN-CRFs models (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016). However, they require a large amount of human annotated corpora, which are usually expensive to obtain. The above issue motivates a lot of work on name tagging in low-resource languages or domains. A typical line of effort focuses on introducing external knowledge via transfer learning (Fritzler et al., 2018; Hofer et al., 2018), such as the use of crossdomain (Yang et al., 2017), cross-task (Peng and Dredze, 2016; Lin et al., 2018) and cross-lingual resources (Ni et al., 2017; Xie et al., 2018; Zafarian et al., 2015; Zhang et al., 2016; Mayhew et al., 2017; Tsai et al., 2016; Feng et al., 2"
D19-1025,R11-1017,0,0.030182,"languages or domains. A typical line of effort focuses on introducing external knowledge via transfer learning (Fritzler et al., 2018; Hofer et al., 2018), such as the use of crossdomain (Yang et al., 2017), cross-task (Peng and Dredze, 2016; Lin et al., 2018) and cross-lingual resources (Ni et al., 2017; Xie et al., 2018; Zafarian et al., 2015; Zhang et al., 2016; Mayhew et al., 2017; Tsai et al., 2016; Feng et al., 2018; Pan et al., 2017). Although they achieve promising results, there are a large amount of weak annotations on the Web, which have not been well studied (Nothman et al., 2008; Ehrmann et al., 2011). Yang et al. (2018); Shang et al. (2018) utilized PartialCRFs (T¨ackstr¨om et al., 2013) to model incomplete annotations for specific domains, but they still rely on seed annotations or a domain dictionary. Therefore, we aim at filling the gap in lowresource name tagging research by using only WL • We propose a novel neural name tagging model that merely relies on WL data without feature engineering. It can thus be adapted for both low-resource languages and domains, while no previous work deals with them at the same time. • We consider name tagging from two perspec262 O B-ORG I-ORG … O … O B"
D19-1025,D14-1162,0,0.0817182,"Missing"
D19-1025,W03-0426,0,0.243404,"Missing"
D19-1025,D18-1230,0,0.367798,"tives of sequence labeling and classification, to efficiently take the best advantage of both high-quality and noisy WL data. derived WL data does not contain complete annotations, thus can not be directly used for training. Ni et al. (2017) select the sentences with highest confidence, and assume missing labels as O (i.e., non-entity), but it will introduce a bias to recognize mentions as non-entity. Another line of work is to replace CRFs with Partial-CRFs (T¨ackstr¨om et al., 2013), which assign unlabeled words with all possible labels and maximize the total probability (Yang et al., 2018; Shang et al., 2018). However, they still rely on seed annotations or domain dictionaries for high-quality training. Massive Noisy Data WL corpora are usually generated with massive noisy data including missing labels, incorrect boundaries and types. Previous work filtered out WL sentences by statistical methods (Ni et al., 2017) or the output of a trainable classifier (Yang et al., 2018). However, abandoning training data may exacerbate the issue of inadequate annotation. Therefore, maximizing the potential of massive noisy data as well as highquality part, yet being efficient, is challenging. To address these i"
D19-1025,Q13-1001,0,0.132177,"Missing"
D19-1025,K16-1022,0,0.0256046,"vy, 2016; Chiu and Nichols, 2016). However, they require a large amount of human annotated corpora, which are usually expensive to obtain. The above issue motivates a lot of work on name tagging in low-resource languages or domains. A typical line of effort focuses on introducing external knowledge via transfer learning (Fritzler et al., 2018; Hofer et al., 2018), such as the use of crossdomain (Yang et al., 2017), cross-task (Peng and Dredze, 2016; Lin et al., 2018) and cross-lingual resources (Ni et al., 2017; Xie et al., 2018; Zafarian et al., 2015; Zhang et al., 2016; Mayhew et al., 2017; Tsai et al., 2016; Feng et al., 2018; Pan et al., 2017). Although they achieve promising results, there are a large amount of weak annotations on the Web, which have not been well studied (Nothman et al., 2008; Ehrmann et al., 2011). Yang et al. (2018); Shang et al. (2018) utilized PartialCRFs (T¨ackstr¨om et al., 2013) to model incomplete annotations for specific domains, but they still rely on seed annotations or a domain dictionary. Therefore, we aim at filling the gap in lowresource name tagging research by using only WL • We propose a novel neural name tagging model that merely relies on WL data without f"
D19-1025,N16-1030,0,0.0729178,"damental task of extracting entity information, which shall benefit many applications, such as information extraction (Zhang et al., 2017; Kuang et al., 2019; Cao et al., 2019a) and recommendation (Wang et al., 2019; Cao et al., 2019b). It can be treated as either a multiclass classification problem (Hammerton, 2003; Xu et al., 2017) or a sequence labeling problem (Collobert et al., 2011), but very little work combined them together. The difference between them mainly lies in whether the method models sequential label constraints, which have been demonstrated effective in many NN-CRFs models (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016). However, they require a large amount of human annotated corpora, which are usually expensive to obtain. The above issue motivates a lot of work on name tagging in low-resource languages or domains. A typical line of effort focuses on introducing external knowledge via transfer learning (Fritzler et al., 2018; Hofer et al., 2018), such as the use of crossdomain (Yang et al., 2017), cross-task (Peng and Dredze, 2016; Lin et al., 2018) and cross-lingual resources (Ni et al., 2017; Xie et al., 2018; Zafarian et al., 2015; Zhang et al., 2016; Mayhew et"
D19-1025,P17-1004,1,0.804855,"hose entities that they do not know or are not interested in. We can construct them from online resources, such as the anchors in Wikipedia. However, the following natures of WL data make learning name tagging from them more challenging: Partially-Labeled Sequence Automatically Introduction Name tagging2 is the task of identifying the boundaries of entity mentions in texts and classifying them into the pre-defined entity types (e.g., person). It serves as a fundamental role as providing the essential inputs for many IE tasks, such as Entity Linking (Cao et al., 2018a) and Relation Extraction (Lin et al., 2017). Many recent methods utilize a neural network (NN) with Conditional Random Fields (CRFs) (Lafferty et al., 2001) by treating name tagging as a sequence labeling problem (Lample 1 Our project can be found in https://github.com/ zig-kwin-hu/Low-Resource-Name-Tagging. 2 Someone may call it Named Entity Recognition (NER). 261 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 261–270, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics tives of se"
D19-1025,D18-1034,0,0.17044,"I-ORG Weakly Labelled, extensively exists on the web Figure 1: Example of weakly labeled data. B-NT and I-NT denote incomplete labels without types. et al., 2016), which has became a basic architecture due to its superior performance. Nevertheless, NN-CRFs require exhaustive human efforts for training annotations, and may not perform well in low-resource settings (Ni et al., 2017). Many approaches thus focus on transferring cross-domain, cross-task and cross-lingual knowledge into name tagging (Yang et al., 2017; Peng and Dredze, 2016; Mayhew et al., 2017; Pan et al., 2017; Lin et al., 2018; Xie et al., 2018). However, they are usually limited by the extra knowledge resources that are effective only in specific languages or domains. Actually, in many low-resource settings, there are extensive noisy annotations that naturally exist on the web yet to be explored (Ni et al., 2017). In this paper, we propose a novel model for name tagging that maximizes the potential of weakly labeled (WL) data. As shown in Figure 1, s2 is weakly labeled, since only Formula shell and Barangay Ginebra are annotated, leaving the remaining words unannotated. WL data is more practical to obtain, since it is difficult for"
D19-1025,P18-1074,1,0.911104,"t> B-NT I-NT B-ORG I-ORG Weakly Labelled, extensively exists on the web Figure 1: Example of weakly labeled data. B-NT and I-NT denote incomplete labels without types. et al., 2016), which has became a basic architecture due to its superior performance. Nevertheless, NN-CRFs require exhaustive human efforts for training annotations, and may not perform well in low-resource settings (Ni et al., 2017). Many approaches thus focus on transferring cross-domain, cross-task and cross-lingual knowledge into name tagging (Yang et al., 2017; Peng and Dredze, 2016; Mayhew et al., 2017; Pan et al., 2017; Lin et al., 2018; Xie et al., 2018). However, they are usually limited by the extra knowledge resources that are effective only in specific languages or domains. Actually, in many low-resource settings, there are extensive noisy annotations that naturally exist on the web yet to be explored (Ni et al., 2017). In this paper, we propose a novel model for name tagging that maximizes the potential of weakly labeled (WL) data. As shown in Figure 1, s2 is weakly labeled, since only Formula shell and Barangay Ginebra are annotated, leaving the remaining words unannotated. WL data is more practical to obtain, since i"
D19-1025,P17-1114,0,0.0456129,"Missing"
D19-1025,P16-1101,0,0.396346,"acting entity information, which shall benefit many applications, such as information extraction (Zhang et al., 2017; Kuang et al., 2019; Cao et al., 2019a) and recommendation (Wang et al., 2019; Cao et al., 2019b). It can be treated as either a multiclass classification problem (Hammerton, 2003; Xu et al., 2017) or a sequence labeling problem (Collobert et al., 2011), but very little work combined them together. The difference between them mainly lies in whether the method models sequential label constraints, which have been demonstrated effective in many NN-CRFs models (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016). However, they require a large amount of human annotated corpora, which are usually expensive to obtain. The above issue motivates a lot of work on name tagging in low-resource languages or domains. A typical line of effort focuses on introducing external knowledge via transfer learning (Fritzler et al., 2018; Hofer et al., 2018), such as the use of crossdomain (Yang et al., 2017), cross-task (Peng and Dredze, 2016; Lin et al., 2018) and cross-lingual resources (Ni et al., 2017; Xie et al., 2018; Zafarian et al., 2015; Zhang et al., 2016; Mayhew et al., 2017; Tsai et"
D19-1025,D17-1269,0,0.130772,"1bDiq3MuvtMtQq5ZhfflvXwAQSBkB0=</latexit> B-NT I-NT B-ORG I-ORG Weakly Labelled, extensively exists on the web Figure 1: Example of weakly labeled data. B-NT and I-NT denote incomplete labels without types. et al., 2016), which has became a basic architecture due to its superior performance. Nevertheless, NN-CRFs require exhaustive human efforts for training annotations, and may not perform well in low-resource settings (Ni et al., 2017). Many approaches thus focus on transferring cross-domain, cross-task and cross-lingual knowledge into name tagging (Yang et al., 2017; Peng and Dredze, 2016; Mayhew et al., 2017; Pan et al., 2017; Lin et al., 2018; Xie et al., 2018). However, they are usually limited by the extra knowledge resources that are effective only in specific languages or domains. Actually, in many low-resource settings, there are extensive noisy annotations that naturally exist on the web yet to be explored (Ni et al., 2017). In this paper, we propose a novel model for name tagging that maximizes the potential of weakly labeled (WL) data. As shown in Figure 1, s2 is weakly labeled, since only Formula shell and Barangay Ginebra are annotated, leaving the remaining words unannotated. WL data"
D19-1025,C18-1183,0,0.328348,"tional Linguistics tives of sequence labeling and classification, to efficiently take the best advantage of both high-quality and noisy WL data. derived WL data does not contain complete annotations, thus can not be directly used for training. Ni et al. (2017) select the sentences with highest confidence, and assume missing labels as O (i.e., non-entity), but it will introduce a bias to recognize mentions as non-entity. Another line of work is to replace CRFs with Partial-CRFs (T¨ackstr¨om et al., 2013), which assign unlabeled words with all possible labels and maximize the total probability (Yang et al., 2018; Shang et al., 2018). However, they still rely on seed annotations or domain dictionaries for high-quality training. Massive Noisy Data WL corpora are usually generated with massive noisy data including missing labels, incorrect boundaries and types. Previous work filtered out WL sentences by statistical methods (Ni et al., 2017) or the output of a trainable classifier (Yang et al., 2018). However, abandoning training data may exacerbate the issue of inadequate annotation. Therefore, maximizing the potential of massive noisy data as well as highquality part, yet being efficient, is challengin"
D19-1025,P17-1135,0,0.153128,"E8QVifZpt4Zpw1+5v31Hjqu03o7+deEbEKN8T+pZtl/lena1EY4sTUwKmm1DC6uiB3yUxX9M3tL1UpckiJ03hAcUE4MMpZn22jkaZ23VvPxN9Mpmb1PshzM7zrW9KA3Z/jnAftWtV1qu75UaV+mo+6iD3s45DmeYw6GmiiRd4jPOIJz1bDiq3MuvtMtQq5ZhfflvXwAQSBkB0=</latexit> B-NT I-NT B-ORG I-ORG Weakly Labelled, extensively exists on the web Figure 1: Example of weakly labeled data. B-NT and I-NT denote incomplete labels without types. et al., 2016), which has became a basic architecture due to its superior performance. Nevertheless, NN-CRFs require exhaustive human efforts for training annotations, and may not perform well in low-resource settings (Ni et al., 2017). Many approaches thus focus on transferring cross-domain, cross-task and cross-lingual knowledge into name tagging (Yang et al., 2017; Peng and Dredze, 2016; Mayhew et al., 2017; Pan et al., 2017; Lin et al., 2018; Xie et al., 2018). However, they are usually limited by the extra knowledge resources that are effective only in specific languages or domains. Actually, in many low-resource settings, there are extensive noisy annotations that naturally exist on the web yet to be explored (Ni et al., 2017). In this paper, we propose a novel model for name tagging that maximizes the potential of we"
D19-1025,U08-1016,0,0.0450237,"gging in low-resource languages or domains. A typical line of effort focuses on introducing external knowledge via transfer learning (Fritzler et al., 2018; Hofer et al., 2018), such as the use of crossdomain (Yang et al., 2017), cross-task (Peng and Dredze, 2016; Lin et al., 2018) and cross-lingual resources (Ni et al., 2017; Xie et al., 2018; Zafarian et al., 2015; Zhang et al., 2016; Mayhew et al., 2017; Tsai et al., 2016; Feng et al., 2018; Pan et al., 2017). Although they achieve promising results, there are a large amount of weak annotations on the Web, which have not been well studied (Nothman et al., 2008; Ehrmann et al., 2011). Yang et al. (2018); Shang et al. (2018) utilized PartialCRFs (T¨ackstr¨om et al., 2013) to model incomplete annotations for specific domains, but they still rely on seed annotations or a domain dictionary. Therefore, we aim at filling the gap in lowresource name tagging research by using only WL • We propose a novel neural name tagging model that merely relies on WL data without feature engineering. It can thus be adapted for both low-resource languages and domains, while no previous work deals with them at the same time. • We consider name tagging from two perspec262"
D19-1025,P17-1178,1,0.902871,"XwAQSBkB0=</latexit> B-NT I-NT B-ORG I-ORG Weakly Labelled, extensively exists on the web Figure 1: Example of weakly labeled data. B-NT and I-NT denote incomplete labels without types. et al., 2016), which has became a basic architecture due to its superior performance. Nevertheless, NN-CRFs require exhaustive human efforts for training annotations, and may not perform well in low-resource settings (Ni et al., 2017). Many approaches thus focus on transferring cross-domain, cross-task and cross-lingual knowledge into name tagging (Yang et al., 2017; Peng and Dredze, 2016; Mayhew et al., 2017; Pan et al., 2017; Lin et al., 2018; Xie et al., 2018). However, they are usually limited by the extra knowledge resources that are effective only in specific languages or domains. Actually, in many low-resource settings, there are extensive noisy annotations that naturally exist on the web yet to be explored (Ni et al., 2017). In this paper, we propose a novel model for name tagging that maximizes the potential of weakly labeled (WL) data. As shown in Figure 1, s2 is weakly labeled, since only Formula shell and Barangay Ginebra are annotated, leaving the remaining words unannotated. WL data is more practical"
D19-1025,N16-1029,1,0.847236,"Fs models (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016). However, they require a large amount of human annotated corpora, which are usually expensive to obtain. The above issue motivates a lot of work on name tagging in low-resource languages or domains. A typical line of effort focuses on introducing external knowledge via transfer learning (Fritzler et al., 2018; Hofer et al., 2018), such as the use of crossdomain (Yang et al., 2017), cross-task (Peng and Dredze, 2016; Lin et al., 2018) and cross-lingual resources (Ni et al., 2017; Xie et al., 2018; Zafarian et al., 2015; Zhang et al., 2016; Mayhew et al., 2017; Tsai et al., 2016; Feng et al., 2018; Pan et al., 2017). Although they achieve promising results, there are a large amount of weak annotations on the Web, which have not been well studied (Nothman et al., 2008; Ehrmann et al., 2011). Yang et al. (2018); Shang et al. (2018) utilized PartialCRFs (T¨ackstr¨om et al., 2013) to model incomplete annotations for specific domains, but they still rely on seed annotations or a domain dictionary. Therefore, we aim at filling the gap in lowresource name tagging research by using only WL • We propose a novel neural name tagging model"
D19-1025,P16-2025,0,0.167213,"3s45DmeYw6GmiiRd4jPOIJz1bDiq3MuvtMtQq5ZhfflvXwAQSBkB0=</latexit> B-NT I-NT B-ORG I-ORG Weakly Labelled, extensively exists on the web Figure 1: Example of weakly labeled data. B-NT and I-NT denote incomplete labels without types. et al., 2016), which has became a basic architecture due to its superior performance. Nevertheless, NN-CRFs require exhaustive human efforts for training annotations, and may not perform well in low-resource settings (Ni et al., 2017). Many approaches thus focus on transferring cross-domain, cross-task and cross-lingual knowledge into name tagging (Yang et al., 2017; Peng and Dredze, 2016; Mayhew et al., 2017; Pan et al., 2017; Lin et al., 2018; Xie et al., 2018). However, they are usually limited by the extra knowledge resources that are effective only in specific languages or domains. Actually, in many low-resource settings, there are extensive noisy annotations that naturally exist on the web yet to be explored (Ni et al., 2017). In this paper, we propose a novel model for name tagging that maximizes the potential of weakly labeled (WL) data. As shown in Figure 1, s2 is weakly labeled, since only Formula shell and Barangay Ginebra are annotated, leaving the remaining words"
D19-1033,N09-2053,0,0.0717263,"Missing"
D19-1033,P08-1030,0,0.548045,"Missing"
D19-1033,C12-1099,0,0.0425312,"Missing"
D19-1033,P19-1430,1,0.69152,"Missing"
D19-1033,P10-1081,0,0.372593,"Missing"
D19-1033,W06-0901,0,0.72409,"Missing"
D19-1033,P15-1017,0,0.214285,"Missing"
D19-1033,P18-1145,0,0.0621476,"Missing"
D19-1033,D18-1517,0,0.0325355,"Missing"
D19-1033,P11-1163,0,0.055638,"Missing"
D19-1033,N16-1033,0,0.0269847,"Missing"
D19-1033,N16-1034,0,0.106326,"Missing"
D19-1033,P18-1144,0,0.0530588,"Missing"
D19-1033,P15-2060,0,0.277605,"Missing"
D19-1033,D16-1085,0,0.0309781,"Missing"
D19-1033,P17-1187,1,0.898195,"Missing"
D19-1033,D09-1016,0,0.0956177,"Missing"
D19-1251,P16-1223,0,0.0307514,"atasets, a great number of neural network-based MRC models have been proposed to answer questions for a given document including Attentive Reader (Kadlec et al., 2016), BiDAF (Seo et al., 2017), Interactive AoA Reader (Cui et al., 2017), Gated Attention Reader (Dhingra et al., 2017), R-Net (Wang et al., 2017a), DCN (Xiong et al., 2017), QANet (Yu et al., 2018), and achieve promising results in most existing public MRC datasets. Despite the success of neural network-based MRC models, researchers began to analyze the data and rethink to what extent we have solved the problem of MRC. Some works (Chen et al., 2016; Sugawara et al., 2018; Kaushik and Lipton, 2018) classify the reasoning skills required to answer the questions into the following types: (1) Exact matching/Paraphrasing; (2) Summary; (3) Logic reasoning; (4) Utilizing external knowledge; (5) Numerical reasoning. They found that most existing MRC models are focusing on dealing with the first three types of questions. However, all these models suffer from problems when answering the questions requiring numerical reasoning. To the best of our knowledge, our work is the first one that explicitly incorporates numerical reasoning into the MRC sys"
D19-1251,P17-1055,0,0.0155433,"eading Comprehension Machine reading comprehension (MRC) has become an important research area in NLP. In recent years, researchers have published a large number of annotated MRC datasets such as CNN/Daily Mail (Hermann et al., 2015), SQuAD (Rajpurkar et al., 2016), RACE (Lai et al., 2017), TriviaQA (Joshi et al., 2017) and so on. With the blooming of available large-scale MRC datasets, a great number of neural network-based MRC models have been proposed to answer questions for a given document including Attentive Reader (Kadlec et al., 2016), BiDAF (Seo et al., 2017), Interactive AoA Reader (Cui et al., 2017), Gated Attention Reader (Dhingra et al., 2017), R-Net (Wang et al., 2017a), DCN (Xiong et al., 2017), QANet (Yu et al., 2018), and achieve promising results in most existing public MRC datasets. Despite the success of neural network-based MRC models, researchers began to analyze the data and rethink to what extent we have solved the problem of MRC. Some works (Chen et al., 2016; Sugawara et al., 2018; Kaushik and Lipton, 2018) classify the reasoning skills required to answer the questions into the following types: (1) Exact matching/Paraphrasing; (2) Summary; (3) Logic reasoning; (4) Utilizin"
D19-1251,N19-1423,0,0.0237718,"a single function v 0 = Reasoning-Step(G, v). (13) As the graph G constructed in Sec. 3.2 has encoded the numerical relations via its topology, the reasoning process is numerically-aware. 2478 Multi-step Reasoning By single-step reasoning, we can only infer relations between adjacent nodes. However, relations between multiple nodes may be required for certain tasks, e.g., sorting. Therefore, it is essential to perform multi-step reasoning, which can be done as follows v t = Reasoning-Step(v t−1 ), (14) where t ≥ 1. Suppose we perform K steps of reasoning, v K is used as U in Eq. 7. 4 • BERT (Devlin et al., 2019), a pre-trained bidirectional Transformer-based language model which achieves state-of-the-art performance on lots of public MRC datasets recently; and numerical MRC models: • NAQANet (Dua et al., 2019), a numerical version of QANet model. • NAQANet+, an enhanced version of NAQANet implemented by ourselves, which further considers real number (e.g. “2.5”), richer arithmetic expression, data augmentation, etc. The enhancements are also used in our NumNet model and the details are given in the supplemental material. Experiments 4.1 Dataset and Evaluation Metrics We evaluate our proposed model on"
D19-1251,P17-1168,0,0.0238174,"hension (MRC) has become an important research area in NLP. In recent years, researchers have published a large number of annotated MRC datasets such as CNN/Daily Mail (Hermann et al., 2015), SQuAD (Rajpurkar et al., 2016), RACE (Lai et al., 2017), TriviaQA (Joshi et al., 2017) and so on. With the blooming of available large-scale MRC datasets, a great number of neural network-based MRC models have been proposed to answer questions for a given document including Attentive Reader (Kadlec et al., 2016), BiDAF (Seo et al., 2017), Interactive AoA Reader (Cui et al., 2017), Gated Attention Reader (Dhingra et al., 2017), R-Net (Wang et al., 2017a), DCN (Xiong et al., 2017), QANet (Yu et al., 2018), and achieve promising results in most existing public MRC datasets. Despite the success of neural network-based MRC models, researchers began to analyze the data and rethink to what extent we have solved the problem of MRC. Some works (Chen et al., 2016; Sugawara et al., 2018; Kaushik and Lipton, 2018) classify the reasoning skills required to answer the questions into the following types: (1) Exact matching/Paraphrasing; (2) Summary; (3) Logic reasoning; (4) Utilizing external knowledge; (5) Numerical reasoning."
D19-1251,N19-1246,0,0.113245,"Missing"
D19-1251,D14-1058,0,0.0369168,"merical reasoning. To the best of our knowledge, our work is the first one that explicitly incorporates numerical reasoning into the MRC system. The most relevant work to ours is NAQANet (Dua et al., 2019), which adapts the output layer of QANet (Yu et al., 2018) to support predicting answers based on counting and addition/subtraction over numbers. However, it does not consider numerical reasoning explicitly during encoding or inference. 2.2 Arithmetic Word Problem Solving Recently, understanding and solving arithmetic word problems (AWP) has attracted the growing interest of NLP researchers. Hosseini et al. (2014) proposed a simple method to address arithmetic word problems, but mostly focusing on subsets of problems which only require addition and subtraction. After that, Roy and Roth (2015) proposed an algorithmic approach which could handle arithmetic word problems with multiple steps and operations. Koncel-Kedziorski et al. (2015) further 2475 5 5 5 &gt; 2 6 2 6 2 2 6 Answer &gt; Prediction Module ≤ 6 2 2 Passage Encoding Module Question 5 ≤ 6 6 Reasoning Module Figure 1: The framework of our NumNet model. Our model consists of an encoding module, a reasoning module and a prediction module. The numerical"
D19-1251,P16-1084,0,0.0149131,"ng such as addition, counting, or sorting over numbers. formalized the AWP problem as that of generating and scoring equation trees via integer linear programming. Wang et al. (2017b) and Ling et al. (2017) proposed sequence to sequence solvers for the AWP problems, which are capable of generating unseen expressions and do not rely on sophisticated manual features. Wang et al. (2018) leveraged deep Q-network to solve the AWP problems, achieving a good balance between effectiveness and efficiency. However, all the existing AWP systems are only trained and validated on small benchmark datasets. Huang et al. (2016) found that the performance of these AWP systems sharply degrades on larger datasets. Moreover, from the perspective of NLP, MRC problems are more challenging than AWP since the passages in MRC are mostly real-world texts which require more complex skills to be understood. Above all, it is nontrivial to adapt most existing AWP models to the MRC scenario. Therefore, we focus on enhancing MRC models with numerical reasoning abilities in this work. 3 Encoding Module Without loss of generality, we use the encoding components of QANet and NAQANet to encode the question and passage into vector-space"
D19-1251,P17-1147,0,0.0312321,"orming numerical reasoning over numbers in the question and passage. In particular, we show that our model could effectively deal with questions requiring sorting with multi-layer NumGNN. The source code of our paper is available at https://github.com/ ranqiu92/NumNet. 2 2.1 Related Work Machine Reading Comprehension Machine reading comprehension (MRC) has become an important research area in NLP. In recent years, researchers have published a large number of annotated MRC datasets such as CNN/Daily Mail (Hermann et al., 2015), SQuAD (Rajpurkar et al., 2016), RACE (Lai et al., 2017), TriviaQA (Joshi et al., 2017) and so on. With the blooming of available large-scale MRC datasets, a great number of neural network-based MRC models have been proposed to answer questions for a given document including Attentive Reader (Kadlec et al., 2016), BiDAF (Seo et al., 2017), Interactive AoA Reader (Cui et al., 2017), Gated Attention Reader (Dhingra et al., 2017), R-Net (Wang et al., 2017a), DCN (Xiong et al., 2017), QANet (Yu et al., 2018), and achieve promising results in most existing public MRC datasets. Despite the success of neural network-based MRC models, researchers began to analyze the data and rethink to"
D19-1251,P16-1086,0,0.0241577,"ble at https://github.com/ ranqiu92/NumNet. 2 2.1 Related Work Machine Reading Comprehension Machine reading comprehension (MRC) has become an important research area in NLP. In recent years, researchers have published a large number of annotated MRC datasets such as CNN/Daily Mail (Hermann et al., 2015), SQuAD (Rajpurkar et al., 2016), RACE (Lai et al., 2017), TriviaQA (Joshi et al., 2017) and so on. With the blooming of available large-scale MRC datasets, a great number of neural network-based MRC models have been proposed to answer questions for a given document including Attentive Reader (Kadlec et al., 2016), BiDAF (Seo et al., 2017), Interactive AoA Reader (Cui et al., 2017), Gated Attention Reader (Dhingra et al., 2017), R-Net (Wang et al., 2017a), DCN (Xiong et al., 2017), QANet (Yu et al., 2018), and achieve promising results in most existing public MRC datasets. Despite the success of neural network-based MRC models, researchers began to analyze the data and rethink to what extent we have solved the problem of MRC. Some works (Chen et al., 2016; Sugawara et al., 2018; Kaushik and Lipton, 2018) classify the reasoning skills required to answer the questions into the following types: (1) Exact"
D19-1251,D18-1546,0,0.019043,"based MRC models have been proposed to answer questions for a given document including Attentive Reader (Kadlec et al., 2016), BiDAF (Seo et al., 2017), Interactive AoA Reader (Cui et al., 2017), Gated Attention Reader (Dhingra et al., 2017), R-Net (Wang et al., 2017a), DCN (Xiong et al., 2017), QANet (Yu et al., 2018), and achieve promising results in most existing public MRC datasets. Despite the success of neural network-based MRC models, researchers began to analyze the data and rethink to what extent we have solved the problem of MRC. Some works (Chen et al., 2016; Sugawara et al., 2018; Kaushik and Lipton, 2018) classify the reasoning skills required to answer the questions into the following types: (1) Exact matching/Paraphrasing; (2) Summary; (3) Logic reasoning; (4) Utilizing external knowledge; (5) Numerical reasoning. They found that most existing MRC models are focusing on dealing with the first three types of questions. However, all these models suffer from problems when answering the questions requiring numerical reasoning. To the best of our knowledge, our work is the first one that explicitly incorporates numerical reasoning into the MRC system. The most relevant work to ours is NAQANet (Du"
D19-1251,Q15-1042,0,0.0228607,"dition/subtraction over numbers. However, it does not consider numerical reasoning explicitly during encoding or inference. 2.2 Arithmetic Word Problem Solving Recently, understanding and solving arithmetic word problems (AWP) has attracted the growing interest of NLP researchers. Hosseini et al. (2014) proposed a simple method to address arithmetic word problems, but mostly focusing on subsets of problems which only require addition and subtraction. After that, Roy and Roth (2015) proposed an algorithmic approach which could handle arithmetic word problems with multiple steps and operations. Koncel-Kedziorski et al. (2015) further 2475 5 5 5 &gt; 2 6 2 6 2 2 6 Answer &gt; Prediction Module ≤ 6 2 2 Passage Encoding Module Question 5 ≤ 6 6 Reasoning Module Figure 1: The framework of our NumNet model. Our model consists of an encoding module, a reasoning module and a prediction module. The numerical relations between numbers are encoded with the topology of the graph. For example, the edge pointing from “6” to “5” denotes “6” is greater than “5”. And the reasoning module leverages a numerically-aware graph neural network to perform numerical reasoning on the graph. As numerical comparison is modeled explicitly in our mo"
D19-1251,D17-1160,0,0.0317798,"over numbers in the passages. There are 77, 409 training samples, 9, 536 development samples and 9, 622 testing samples in the dataset. In this paper, we adopt two metrics including Exact Match (EM) and numerically-focused F1 scores to evaluate our model following Dua et al. (2019). The numerically-focused F1 is set to be 0 when the predicted answer is mismatched for those questions with the numeric golden answer. 4.2 Baselines For comparison, we select several public models as baselines including semantic parsing models: • Syn Dep (Dua et al., 2019), the neural semantic parsing model (KDG) (Krishnamurthy et al., 2017) with Stanford dependencies based sentence representations; • OpenIE (Dua et al., 2019), KDG with open information extraction based sentence representations; • SRL (Dua et al., 2019), KDG with semantic role labeling based sentence representations; and traditional MRC models: • BiDAF (Seo et al., 2017), an MRC model which utilizes a bi-directional attention flow network to encode the question and passage; • QANet (Yu et al., 2018), which utilizes convolutions and self-attentions as the building blocks of encoders to represent the question and passage; 4.3 Experimental Settings In this paper, we"
D19-1251,D17-1082,0,0.0239897,"ne methods by explicitly performing numerical reasoning over numbers in the question and passage. In particular, we show that our model could effectively deal with questions requiring sorting with multi-layer NumGNN. The source code of our paper is available at https://github.com/ ranqiu92/NumNet. 2 2.1 Related Work Machine Reading Comprehension Machine reading comprehension (MRC) has become an important research area in NLP. In recent years, researchers have published a large number of annotated MRC datasets such as CNN/Daily Mail (Hermann et al., 2015), SQuAD (Rajpurkar et al., 2016), RACE (Lai et al., 2017), TriviaQA (Joshi et al., 2017) and so on. With the blooming of available large-scale MRC datasets, a great number of neural network-based MRC models have been proposed to answer questions for a given document including Attentive Reader (Kadlec et al., 2016), BiDAF (Seo et al., 2017), Interactive AoA Reader (Cui et al., 2017), Gated Attention Reader (Dhingra et al., 2017), R-Net (Wang et al., 2017a), DCN (Xiong et al., 2017), QANet (Yu et al., 2018), and achieve promising results in most existing public MRC datasets. Despite the success of neural network-based MRC models, researchers began to"
D19-1251,P17-1015,0,0.0224498,"umerical relations between numbers are encoded with the topology of the graph. For example, the edge pointing from “6” to “5” denotes “6” is greater than “5”. And the reasoning module leverages a numerically-aware graph neural network to perform numerical reasoning on the graph. As numerical comparison is modeled explicitly in our model, it is more effective for answering questions requiring numerical reasoning such as addition, counting, or sorting over numbers. formalized the AWP problem as that of generating and scoring equation trees via integer linear programming. Wang et al. (2017b) and Ling et al. (2017) proposed sequence to sequence solvers for the AWP problems, which are capable of generating unseen expressions and do not rely on sophisticated manual features. Wang et al. (2018) leveraged deep Q-network to solve the AWP problems, achieving a good balance between effectiveness and efficiency. However, all the existing AWP systems are only trained and validated on small benchmark datasets. Huang et al. (2016) found that the performance of these AWP systems sharply degrades on larger datasets. Moreover, from the perspective of NLP, MRC problems are more challenging than AWP since the passages"
D19-1251,D16-1264,0,0.0977847,"ement as compared to all baseline methods by explicitly performing numerical reasoning over numbers in the question and passage. In particular, we show that our model could effectively deal with questions requiring sorting with multi-layer NumGNN. The source code of our paper is available at https://github.com/ ranqiu92/NumNet. 2 2.1 Related Work Machine Reading Comprehension Machine reading comprehension (MRC) has become an important research area in NLP. In recent years, researchers have published a large number of annotated MRC datasets such as CNN/Daily Mail (Hermann et al., 2015), SQuAD (Rajpurkar et al., 2016), RACE (Lai et al., 2017), TriviaQA (Joshi et al., 2017) and so on. With the blooming of available large-scale MRC datasets, a great number of neural network-based MRC models have been proposed to answer questions for a given document including Attentive Reader (Kadlec et al., 2016), BiDAF (Seo et al., 2017), Interactive AoA Reader (Cui et al., 2017), Gated Attention Reader (Dhingra et al., 2017), R-Net (Wang et al., 2017a), DCN (Xiong et al., 2017), QANet (Yu et al., 2018), and achieve promising results in most existing public MRC datasets. Despite the success of neural network-based MRC mode"
D19-1251,D15-1202,0,0.0530747,"t (Dua et al., 2019), which adapts the output layer of QANet (Yu et al., 2018) to support predicting answers based on counting and addition/subtraction over numbers. However, it does not consider numerical reasoning explicitly during encoding or inference. 2.2 Arithmetic Word Problem Solving Recently, understanding and solving arithmetic word problems (AWP) has attracted the growing interest of NLP researchers. Hosseini et al. (2014) proposed a simple method to address arithmetic word problems, but mostly focusing on subsets of problems which only require addition and subtraction. After that, Roy and Roth (2015) proposed an algorithmic approach which could handle arithmetic word problems with multiple steps and operations. Koncel-Kedziorski et al. (2015) further 2475 5 5 5 &gt; 2 6 2 6 2 2 6 Answer &gt; Prediction Module ≤ 6 2 2 Passage Encoding Module Question 5 ≤ 6 6 Reasoning Module Figure 1: The framework of our NumNet model. Our model consists of an encoding module, a reasoning module and a prediction module. The numerical relations between numbers are encoded with the topology of the graph. For example, the edge pointing from “6” to “5” denotes “6” is greater than “5”. And the reasoning module levera"
D19-1251,P17-1018,0,0.171003,"portant research area in NLP. In recent years, researchers have published a large number of annotated MRC datasets such as CNN/Daily Mail (Hermann et al., 2015), SQuAD (Rajpurkar et al., 2016), RACE (Lai et al., 2017), TriviaQA (Joshi et al., 2017) and so on. With the blooming of available large-scale MRC datasets, a great number of neural network-based MRC models have been proposed to answer questions for a given document including Attentive Reader (Kadlec et al., 2016), BiDAF (Seo et al., 2017), Interactive AoA Reader (Cui et al., 2017), Gated Attention Reader (Dhingra et al., 2017), R-Net (Wang et al., 2017a), DCN (Xiong et al., 2017), QANet (Yu et al., 2018), and achieve promising results in most existing public MRC datasets. Despite the success of neural network-based MRC models, researchers began to analyze the data and rethink to what extent we have solved the problem of MRC. Some works (Chen et al., 2016; Sugawara et al., 2018; Kaushik and Lipton, 2018) classify the reasoning skills required to answer the questions into the following types: (1) Exact matching/Paraphrasing; (2) Summary; (3) Logic reasoning; (4) Utilizing external knowledge; (5) Numerical reasoning. They found that most exist"
D19-1251,D17-1088,0,0.0586902,"Missing"
D19-1334,N18-1165,0,0.0352096,"ng et al. (2018) predict new facts under a challenging setting where only one training triple for a given relation r is available, which can be seen as a one-shot scenario. Although these models are effective, they lack interpretability for their decisions. Multi-Hop Reasoning over KGs aims to learn symbolic inference rules from relational paths in G and has been formulated as sequential decision problems in recent years. DeepPath (Xiong et al., 2017) first applies RL to search reasoning paths in KGs for a given query, which inspires much later work (e.g., MINERVA (Das et al., 2018) and DIVA (Chen et al., 2018)). Because it is hard to train an RL model, ReinforceWalk (Shen et al., 2018) proposes to solve the reward sparsity problem using off-policy learning. MultiHop (Lin et al., 2018) further extends MINERVA with reward shaping and action dropout, achieveing the state-of-the-art performance. These reasoning methods are intuitive and explainable. However, all of them have a weak performance in few-shot scenarios. In addition to multi-hop reasoning over KGs, there are also some multi-hop QA methods in recent years. Yang et al. (2018) proposes a high quality dataset, which greatly promotes the develop"
D19-1334,P19-1259,0,0.0136091,"(Shen et al., 2018) proposes to solve the reward sparsity problem using off-policy learning. MultiHop (Lin et al., 2018) further extends MINERVA with reward shaping and action dropout, achieveing the state-of-the-art performance. These reasoning methods are intuitive and explainable. However, all of them have a weak performance in few-shot scenarios. In addition to multi-hop reasoning over KGs, there are also some multi-hop QA methods in recent years. Yang et al. (2018) proposes a high quality dataset, which greatly promotes the development of this field. After that, many methods like CogQA (Ding et al., 2019) and DFGN (Xiao et al., 2019) are also proposed. Meta-Learning tries to solve the problem of “fast adaptation on a new training task”. It has been proved to be very successful on few-shot task (Lake et al., 2015; Gu et al., 2018). Previous metalearning models mainly focus on computer vision and imitation learning domains. In this paper, we propose a new model (Meta-KGR) using the metalearning algorithm MAML (Finn et al., 2017) for 3377 rLra ✓ Meta-Learning Algorithm 1 Meta-Learning for multi-hop reasoning over knowledge graphs &lt;latexit sha1_base64=&quot;(null)&quot;&gt;(null)&lt;/latexit&gt; Fast Adaptation &lt;lat"
D19-1334,D18-1398,0,0.0413263,"These reasoning methods are intuitive and explainable. However, all of them have a weak performance in few-shot scenarios. In addition to multi-hop reasoning over KGs, there are also some multi-hop QA methods in recent years. Yang et al. (2018) proposes a high quality dataset, which greatly promotes the development of this field. After that, many methods like CogQA (Ding et al., 2019) and DFGN (Xiao et al., 2019) are also proposed. Meta-Learning tries to solve the problem of “fast adaptation on a new training task”. It has been proved to be very successful on few-shot task (Lake et al., 2015; Gu et al., 2018). Previous metalearning models mainly focus on computer vision and imitation learning domains. In this paper, we propose a new model (Meta-KGR) using the metalearning algorithm MAML (Finn et al., 2017) for 3377 rLra ✓ Meta-Learning Algorithm 1 Meta-Learning for multi-hop reasoning over knowledge graphs &lt;latexit sha1_base64=&quot;(null)&quot;&gt;(null)&lt;/latexit&gt; Fast Adaptation &lt;latexit sha1_base64=&quot;(null)&quot;&gt;(null)&lt;/latexit&gt; &lt;latexit rLrb &lt;latexit sha1_base64=&quot;(null)&quot;&gt;(null)&lt;/latexit&gt; ✓ r3 &lt;latexit sha1_base64=&quot;(null)&quot;&gt;(null)&lt;/latexit&gt; rLrc &lt;latexit sha1_base64=&quot;(null)&quot;&gt;(null)&lt;/latexit&gt; ✓⇤ &lt;latexit sha1_base"
D19-1334,D18-1514,1,0.731523,"ing methods, which leverage the symbolic compositionality of relations in KGs to achieve explainable reasoning results. For example, when queried with (Mark Twain, nationality, ?), multi-hop reasoning models can give not only the target entity America but also multi-hop explainable paths (Mark Twain, bornIn, Florida) ∧ (Florida, locatedIn, America) as well. Most previous work assumes that there are enough triples to train an effective and robust reasoning models for each relation in KGs. However, as shown in Figure 1, a large portion of KG relations are actually long-tail (Xiong et al., 2018; Han et al., 2018) and only contain few triples, which can be called few-shot relations. Some pilot experiments show that the performance of 3376 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3376–3381, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics previous multi-hop reasoning models, e.g., MINERVA (Das et al., 2018) and MultiHop (Lin et al., 2018), on these few-shot relations will drop significantly. Note that, there are some knowledge graph embeddin"
D19-1334,D18-1362,0,0.357769,"me knowledge graph embedding methods (Bordes et al., 2013; Corresponding Author 0 2000 4000 6000 Relation frequency 8000 10000 Figure 1: The histogram of relation frequency in the real-world knowledge graph Wikidata. Introduction ∗ 400 Dettmers et al., 2018) have been proposed to embed entities and relations into semantic spaces to capture inner connections, and then use the learned embeddings for final predictions. Although these embedding-based methods have shown strong abilities in predicting target entities for queries, they only give answers and lack interpretability for their decisions (Lin et al., 2018). In order to make models more intuitive, Das et al. (2018) and Lin et al. (2018) propose multi-hop reasoning methods, which leverage the symbolic compositionality of relations in KGs to achieve explainable reasoning results. For example, when queried with (Mark Twain, nationality, ?), multi-hop reasoning models can give not only the target entity America but also multi-hop explainable paths (Mark Twain, bornIn, Florida) ∧ (Florida, locatedIn, America) as well. Most previous work assumes that there are enough triples to train an effective and robust reasoning models for each relation in KGs. H"
D19-1334,D15-1174,0,0.0874501,"te the meta policy network with parameters θ. Usually, we will go over many tasks in a batch and update θ as follows: X DQ θ = θ − β∇θ Lr (θr0 ), (5) Tr where β is the meta-learning rate. We detail the meta-learning algorithm in Algorithm 1. After previous meta-learning steps, Meta-KGR can fast adapt to a relation-specific policy network for every few-shot relation by using θ as wellinitialized parameters θ∗ . #Ent #Rel #Triples 14,448 3,078 63,524 2,951 200 37 170 30 268,039 4,076 115,454 2,680 Table 2: Statistics of datasets. 5 5.1 Experiments Datasets We use two typical datasets FB15K-237 (Toutanova et al., 2015) and NELL-995 (Xiong et al., 2017) for training and evaluation. Specifically, we set K = 137 and K = 114 to select few-shot relations from FB15K-237 and NELL995 respectively. Besides, we rebuild NELL-995 to generate few-shot relations in valid and test set. Statistics are given separately for normal relations and few-shot relations in Table 2. 5.2 Baselines We compare with four multi-hop reasoning models in experiments: (1) Neural Logical Programming (NerualLP) (Yang et al., 2017); (2) NTP-λ (Rockt¨aschel and Riedel, 2017); (3) MINERVA (Das et al., 2018) and (4) MultiHop (Lin et al., 2018). Fo"
D19-1334,P19-1617,0,0.0179871,"to solve the reward sparsity problem using off-policy learning. MultiHop (Lin et al., 2018) further extends MINERVA with reward shaping and action dropout, achieveing the state-of-the-art performance. These reasoning methods are intuitive and explainable. However, all of them have a weak performance in few-shot scenarios. In addition to multi-hop reasoning over KGs, there are also some multi-hop QA methods in recent years. Yang et al. (2018) proposes a high quality dataset, which greatly promotes the development of this field. After that, many methods like CogQA (Ding et al., 2019) and DFGN (Xiao et al., 2019) are also proposed. Meta-Learning tries to solve the problem of “fast adaptation on a new training task”. It has been proved to be very successful on few-shot task (Lake et al., 2015; Gu et al., 2018). Previous metalearning models mainly focus on computer vision and imitation learning domains. In this paper, we propose a new model (Meta-KGR) using the metalearning algorithm MAML (Finn et al., 2017) for 3377 rLra ✓ Meta-Learning Algorithm 1 Meta-Learning for multi-hop reasoning over knowledge graphs &lt;latexit sha1_base64=&quot;(null)&quot;&gt;(null)&lt;/latexit&gt; Fast Adaptation &lt;latexit sha1_base64=&quot;(null)&quot;&gt;(nu"
D19-1334,D17-1060,0,0.176655,"al., 2016; Shi and Weninger, 2018) incorporate additional entity descriptions to learn embeddings for unseen entities, which can be seen as zero-shot scenarios. Xiong et al. (2018) predict new facts under a challenging setting where only one training triple for a given relation r is available, which can be seen as a one-shot scenario. Although these models are effective, they lack interpretability for their decisions. Multi-Hop Reasoning over KGs aims to learn symbolic inference rules from relational paths in G and has been formulated as sequential decision problems in recent years. DeepPath (Xiong et al., 2017) first applies RL to search reasoning paths in KGs for a given query, which inspires much later work (e.g., MINERVA (Das et al., 2018) and DIVA (Chen et al., 2018)). Because it is hard to train an RL model, ReinforceWalk (Shen et al., 2018) proposes to solve the reward sparsity problem using off-policy learning. MultiHop (Lin et al., 2018) further extends MINERVA with reward shaping and action dropout, achieveing the state-of-the-art performance. These reasoning methods are intuitive and explainable. However, all of them have a weak performance in few-shot scenarios. In addition to multi-hop r"
D19-1334,D18-1223,0,0.311594,"ose multi-hop reasoning methods, which leverage the symbolic compositionality of relations in KGs to achieve explainable reasoning results. For example, when queried with (Mark Twain, nationality, ?), multi-hop reasoning models can give not only the target entity America but also multi-hop explainable paths (Mark Twain, bornIn, Florida) ∧ (Florida, locatedIn, America) as well. Most previous work assumes that there are enough triples to train an effective and robust reasoning models for each relation in KGs. However, as shown in Figure 1, a large portion of KG relations are actually long-tail (Xiong et al., 2018; Han et al., 2018) and only contain few triples, which can be called few-shot relations. Some pilot experiments show that the performance of 3376 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3376–3381, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics previous multi-hop reasoning models, e.g., MINERVA (Das et al., 2018) and MultiHop (Lin et al., 2018), on these few-shot relations will drop significantly. Note that, there are some knowl"
D19-1334,D18-1259,0,0.0386084,"nspires much later work (e.g., MINERVA (Das et al., 2018) and DIVA (Chen et al., 2018)). Because it is hard to train an RL model, ReinforceWalk (Shen et al., 2018) proposes to solve the reward sparsity problem using off-policy learning. MultiHop (Lin et al., 2018) further extends MINERVA with reward shaping and action dropout, achieveing the state-of-the-art performance. These reasoning methods are intuitive and explainable. However, all of them have a weak performance in few-shot scenarios. In addition to multi-hop reasoning over KGs, there are also some multi-hop QA methods in recent years. Yang et al. (2018) proposes a high quality dataset, which greatly promotes the development of this field. After that, many methods like CogQA (Ding et al., 2019) and DFGN (Xiao et al., 2019) are also proposed. Meta-Learning tries to solve the problem of “fast adaptation on a new training task”. It has been proved to be very successful on few-shot task (Lake et al., 2015; Gu et al., 2018). Previous metalearning models mainly focus on computer vision and imitation learning domains. In this paper, we propose a new model (Meta-KGR) using the metalearning algorithm MAML (Finn et al., 2017) for 3377 rLra ✓ Meta-Learn"
D19-1584,P13-1008,0,0.810312,"al., 2018) in recent years, EAE becomes the bottleneck of EE. † Org Time Seller Steve Jobs sold Buyer Pixar to Disney in Timewithin 2006. Figure 1: An example of the concept hierarchy. Introduction ∗ Person indicates equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) Since EE benefits many NLP applications (Yang et al., 2003; Basile et al., 2014; Cheng and Erk, 2018), intensive efforts have been devoted to detecting events and extracting their event arguments. Traditional feature-based methods (Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al., 2013) rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-modal integration (Zhang et al., 2017), and weakly supervised methods (C"
D19-1584,C10-1077,0,0.409617,"is well-studied (Nguyen and Grishman, 2018; Zhao et al., 2018) in recent years, EAE becomes the bottleneck of EE. † Org Time Seller Steve Jobs sold Buyer Pixar to Disney in Timewithin 2006. Figure 1: An example of the concept hierarchy. Introduction ∗ Person indicates equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) Since EE benefits many NLP applications (Yang et al., 2003; Basile et al., 2014; Cheng and Erk, 2018), intensive efforts have been devoted to detecting events and extracting their event arguments. Traditional feature-based methods (Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al., 2013) rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-modal integration (Zhang et"
D19-1584,P17-1038,0,0.138572,") rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-modal integration (Zhang et al., 2017), and weakly supervised methods (Chen et al., 2017; Wang et al., 2019). However, the existing methods all treat argument roles as independent of each other, regardless of the fact that some argument roles are conceptually closer than others. Taking Figure 1 as an example, “Seller” is conceptually closer to “Buyer” than “Time-within”, because they share the same superordinate concepts “Person” and “Org” in the concept hierarchy. Intuitively, the concept hierarchy will provide extra informa5777 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pr"
D19-1584,P10-1081,0,0.697552,"is well-studied (Nguyen and Grishman, 2018; Zhao et al., 2018) in recent years, EAE becomes the bottleneck of EE. † Org Time Seller Steve Jobs sold Buyer Pixar to Disney in Timewithin 2006. Figure 1: An example of the concept hierarchy. Introduction ∗ Person indicates equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) Since EE benefits many NLP applications (Yang et al., 2003; Basile et al., 2014; Cheng and Erk, 2018), intensive efforts have been devoted to detecting events and extracting their event arguments. Traditional feature-based methods (Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al., 2013) rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-modal integration (Zhang et"
D19-1584,P15-1017,0,0.611756,"t al., 2003; Basile et al., 2014; Cheng and Erk, 2018), intensive efforts have been devoted to detecting events and extracting their event arguments. Traditional feature-based methods (Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al., 2013) rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-modal integration (Zhang et al., 2017), and weakly supervised methods (Chen et al., 2017; Wang et al., 2019). However, the existing methods all treat argument roles as independent of each other, regardless of the fact that some argument roles are conceptually closer than others. Taking Figure 1 as an example, “Seller” is conceptually closer to “Buyer” than “Time-within”, because they share the same supero"
D19-1584,D15-1166,0,0.0125031,"tention score for each hidden embedding to model its correlation with the specific superordinate concept. As an argument role can belong to more than one superordinate concept, we set a logic union module to combine the scores from different superordinate modules together. For each argument role, we hierarchically compose its superordinate concept modules into the integrated hierarchical modular attention component to build its role-oriented embedding. Superordinate Concept Module For a specific superordinate concept c, we represent its semantic features with a trainable vector uc . Following Luong et al. (2015), we adopt a multi-layer perceptron to calculate the attention scores. We first calculate the hidden state, hci = tanh(Wa [hi ; uc ]). (3) Then, we apply a softmax operation to get the attention score for the hidden embedding hi , exp(Wb hci ) sci = Pn c , j=1 exp(Wb hj ) (4) where Wa and Wb are trainable matrices shared among different superordinate concept modules. Logic Union Module Given an argument role r ∈ R, we denote its k superordinate concepts as c1 , c2 , . . . , ck , and the corresponding attention n X sri hi . (6) i=1 2.3 Argument Role Classifier We concatenate the instance embedd"
D19-1584,N18-1076,0,0.0621074,"ller”. Most event extraction (EE) methods treat EE as a two-stage problem, including event detection (ED, to identify the trigger word and determine the event type) and EAE. As ED is well-studied (Nguyen and Grishman, 2018; Zhao et al., 2018) in recent years, EAE becomes the bottleneck of EE. † Org Time Seller Steve Jobs sold Buyer Pixar to Disney in Timewithin 2006. Figure 1: An example of the concept hierarchy. Introduction ∗ Person indicates equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) Since EE benefits many NLP applications (Yang et al., 2003; Basile et al., 2014; Cheng and Erk, 2018), intensive efforts have been devoted to detecting events and extracting their event arguments. Traditional feature-based methods (Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al., 2013) rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguy"
D19-1584,N19-1423,0,0.0182971,"der We denote an instance as an n-word sequence x = {w1 , . . . , t, . . . , a, . . . , wn }, where t, a denote the trigger word and the candidate argument respectively. The trigger word is detected by the previous event detection models (independent of our work) and each named entity in the sentence is a candidate argument. Sentence Encoder is adopted to encode the word sequence into hidden embeddings,  {h1 , h2 . . . , hn } = E w1 , . . . , t, . . . , a, . . . , wn , (1) 5778 where E(·) is the neural network to encode the sentence. In this paper, we select CNN (Chen et al., 2015) and BERT (Devlin et al., 2019) as encoders. Feature Aggregator aggregates the hidden embeddings into an instance embedding. Our method is independent of the feature aggregator mechanism. Here, we follow Chen et al. (2015) and use dynamic multi-pooling as the feature aggregator: scores for hi are sci 1 , sci 2 , . . . , sci k computed by Eq. (4). As information about all the superordinate concepts should be retained in the role-oriented embedding, we calculate the mean of the attention scores as the role-oriented attention score, sri k 1 X cj = si , k (5) j=1 [x1,pt ]i = max{[h1 ]i , . . . , [hpt ]i }, [xpt +1,pa ]i = max{["
D19-1584,N16-1034,0,0.62235,"018), intensive efforts have been devoted to detecting events and extracting their event arguments. Traditional feature-based methods (Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al., 2013) rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-modal integration (Zhang et al., 2017), and weakly supervised methods (Chen et al., 2017; Wang et al., 2019). However, the existing methods all treat argument roles as independent of each other, regardless of the fact that some argument roles are conceptually closer than others. Taking Figure 1 as an example, “Seller” is conceptually closer to “Buyer” than “Time-within”, because they share the same superordinate concepts “Person” and “Org” in the concept"
D19-1584,D18-1247,1,0.781371,"score Superordinate modules, corresponding to the superordinate concepts of the argument role to classify. Argument Role Classifier, varies with the argument role to classify. Figure 2: The overall architecture of HMEAE. Take the argument role “Seller” as an example. tion about the correlation between argument roles and help the argument role classification. To leverage the concept hierarchy information to improve EAE, we propose the Hierarchical Modular Event Argument Extraction (HMEAE) model. Inspired by the previous hierarchical classification works (Qiu et al., 2011; Shimura et al., 2018; Han et al., 2018) and the neural module networks (NMNs) (Andreas et al., 2016), HMEAE adopts the NMNs to enable a flexible network architecture imitating the concept hierarchical structure, which can provide effective inductive bias for better classification performance. As Figure 1 shows, we divide the concepts into two types: the superordinate concepts representing more abstractive concepts, and the finegrained argument roles. An argument role can belong to more than one superordinate concept, e.g., “Seller” belongs to both “Person” and “Org”. As shown in Figure 2, we set a neural module network for each con"
D19-1584,D09-1016,0,0.244851,"e event type) and EAE. As ED is well-studied (Nguyen and Grishman, 2018; Zhao et al., 2018) in recent years, EAE becomes the bottleneck of EE. † Org Time Seller Steve Jobs sold Buyer Pixar to Disney in Timewithin 2006. Figure 1: An example of the concept hierarchy. Introduction ∗ Person indicates equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) Since EE benefits many NLP applications (Yang et al., 2003; Basile et al., 2014; Cheng and Erk, 2018), intensive efforts have been devoted to detecting events and extracting their event arguments. Traditional feature-based methods (Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al., 2013) rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-mo"
D19-1584,P11-2105,0,0.0152203,"ng att score input embeddings + ATT att score Superordinate modules, corresponding to the superordinate concepts of the argument role to classify. Argument Role Classifier, varies with the argument role to classify. Figure 2: The overall architecture of HMEAE. Take the argument role “Seller” as an example. tion about the correlation between argument roles and help the argument role classification. To leverage the concept hierarchy information to improve EAE, we propose the Hierarchical Modular Event Argument Extraction (HMEAE) model. Inspired by the previous hierarchical classification works (Qiu et al., 2011; Shimura et al., 2018; Han et al., 2018) and the neural module networks (NMNs) (Andreas et al., 2016), HMEAE adopts the NMNs to enable a flexible network architecture imitating the concept hierarchical structure, which can provide effective inductive bias for better classification performance. As Figure 1 shows, we divide the concepts into two types: the superordinate concepts representing more abstractive concepts, and the finegrained argument roles. An argument role can belong to more than one superordinate concept, e.g., “Seller” belongs to both “Person” and “Org”. As shown in Figure 2, we"
D19-1584,P16-1116,0,0.285339,"BERT and HMEAE (BERT) are the same as the BERTBASE model. To utilize the event type information in our model, we append a special token into each input sequence for BERT to indicate the event type. Additional hyperparameters used in our experiments are shown in Table 2. Learning Rate Batch Size Kernel Size Warmup Rate uc dimension Wb dimension 6e-05 50 3 0.1 900 900 Table 2: Hyperparameter settings for BERT models. 3.2 Overall Evaluation Results We compare our models with various state-of-theart baselines on ACE 2005: (1) Feature-based methods, including Li’s joint (Li et al., 2013) and RBPB (Sha et al., 2016). (2) Vanilla neural network methods, including DMCNN (Chen et al., 2015) and JRNN (Nguyen et al., 2016). (3) Neural network with syntax information, like dbRNN (Sha et al., 2018) enhancing the recurrent neural network with dependency bridges to consider syntactically related information. On TAC KBP 2016, we compare our models with the top systems (Dubbin et al., 2016; Hsi et al., 2016; Ferguson et al., 2016) of the competition as well as DMCNN and DMBERT. 5780 Barry Diller on Wednesday quit as chief of Vivendi Universal Entertainment Argument Role Classification P R F1 Method Li’s Joint (Li e"
D19-1584,D18-1093,0,0.0221031,"embeddings + ATT att score Superordinate modules, corresponding to the superordinate concepts of the argument role to classify. Argument Role Classifier, varies with the argument role to classify. Figure 2: The overall architecture of HMEAE. Take the argument role “Seller” as an example. tion about the correlation between argument roles and help the argument role classification. To leverage the concept hierarchy information to improve EAE, we propose the Hierarchical Modular Event Argument Extraction (HMEAE) model. Inspired by the previous hierarchical classification works (Qiu et al., 2011; Shimura et al., 2018; Han et al., 2018) and the neural module networks (NMNs) (Andreas et al., 2016), HMEAE adopts the NMNs to enable a flexible network architecture imitating the concept hierarchical structure, which can provide effective inductive bias for better classification performance. As Figure 1 shows, we divide the concepts into two types: the superordinate concepts representing more abstractive concepts, and the finegrained argument roles. An argument role can belong to more than one superordinate concept, e.g., “Seller” belongs to both “Person” and “Org”. As shown in Figure 2, we set a neural module n"
D19-1584,P18-1201,0,0.114469,"(Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al., 2013) rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-modal integration (Zhang et al., 2017), and weakly supervised methods (Chen et al., 2017; Wang et al., 2019). However, the existing methods all treat argument roles as independent of each other, regardless of the fact that some argument roles are conceptually closer than others. Taking Figure 1 as an example, “Seller” is conceptually closer to “Buyer” than “Time-within”, because they share the same superordinate concepts “Person” and “Org” in the concept hierarchy. Intuitively, the concept hierarchy will provide extra informa5777 Proceedings of the 2019 Conference on Empirical Methods"
D19-1584,N19-1105,1,0.884827,"ted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-modal integration (Zhang et al., 2017), and weakly supervised methods (Chen et al., 2017; Wang et al., 2019). However, the existing methods all treat argument roles as independent of each other, regardless of the fact that some argument roles are conceptually closer than others. Taking Figure 1 as an example, “Seller” is conceptually closer to “Buyer” than “Time-within”, because they share the same superordinate concepts “Person” and “Org” in the concept hierarchy. Intuitively, the concept hierarchy will provide extra informa5777 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5777"
D19-1584,P18-2066,0,0.173398,"1 Argument Role Instance Event argument extraction (EAE) aims to identify the entities serving as event arguments and classify the roles they play in an event. For instance, given that the word “sold” triggers a Transfer-Ownership event in the sentence “Steve Jobs sold Pixar to Disney”, EAE aims to identify that “Steve Jobs” is an event argument and its argument role is “Seller”. Most event extraction (EE) methods treat EE as a two-stage problem, including event detection (ED, to identify the trigger word and determine the event type) and EAE. As ED is well-studied (Nguyen and Grishman, 2018; Zhao et al., 2018) in recent years, EAE becomes the bottleneck of EE. † Org Time Seller Steve Jobs sold Buyer Pixar to Disney in Timewithin 2006. Figure 1: An example of the concept hierarchy. Introduction ∗ Person indicates equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) Since EE benefits many NLP applications (Yang et al., 2003; Basile et al., 2014; Cheng and Erk, 2018), intensive efforts have been devoted to detecting events and extracting their event arguments. Traditional feature-based methods (Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al.,"
D19-1649,W06-1615,0,0.189413,"can sample instances from the training set as supporting instances for NOTA relation (this method is described explicitly in Section 4). Also note that to better demonstrate the effects of the NOTA relation, we use the original FewRel dataset for fewshot NOTA, instead of the new test set, which can get rid of the influence of domain adaptation. 3 Approaches for Few-Shot DA Many efforts have been devoted for domain adaptation, like subspace mapping (Pan et al., 2010; Fernando et al., 2013), finding domain-invariant spaces (Baktashmotlagh et al., 2013; Ganin et al., 2016), feature augmentation (Blitzer et al., 2006) and minimax estimators (Provost and Fawcett, 2001). Among them, adversarial training (Goodfellow et al., 2015; Ganin et al., 2016; Wang et al., 2018) has been proved to be efficient in finding domain-invariant features. It is a game process between an encoder and a discriminator, where the encoder tries to generate domain-invariant features while the discriminator tries to tell which domain the features are from. Here we follow the adversarial training setting in Wang et al. (2018), where a two-layer perceptron network is used as the discriminator. While training the few-shot learning task, w"
D19-1649,P07-1073,0,0.0597172,"mpled from the test set, each of which consists of (R, S, x, r), where R = {r1 , r2 , ..., rN } is the sampled relation set, r ∈ R is the correct relation label for the query x, and S is the supporting set containing K instances for each relation, S = {(xjri , ri )}, 1 ≤ i ≤ N, 1 ≤ j ≤ K. (1) Models should predict the relation label y ∈ R for the query instance x based on the given S and R. Both of the following two challenges are based on this N -way K-shot setting. Both the training and test sets of the original FewRel dataset are constructed by manually annotating the distantly supervised (Bunescu and Mooney, 2007; Mintz et al., 2009) results on Wikipedia corpus and Wikidata (Vrandeˇci´c and Kr¨otzsch, 2014) knowledge bases. In other words, they are from the same domain, yet in a real-world scenario, we might train models on one domain and perform few-shot learning on a different one. For example, we may train models on Wikipedia, which has large amounts of data and adequate annotations, and then perform few-shot learning on some domains suffering data sparsity, like literature, finance and medicine. Note that, not only do these corpora differ vastly from each other in morphology and syntax, but there"
D19-1649,N19-1423,0,0.0360439,"le NOTA is to regard it as an extra class in the N -way K-shot setting. To be more specific, we can sample instances outside the N relations as the supporting data of NOTA, and perform the (N + 1)-way K-shot learning. As compared to the current methods ignoring NOTA, this approach does not bring much improvements, since the supporting data for NOTA actually belong to several different relations and are scattered in the feature space, making it hard to perform classification. To better address few-shot NOTA, we propose a model named BERT-PAIR based on the sequence classification model in BERT (Devlin et al., 2019). We pair each query instance with all the supporting instances, concatenate each pair as one sequence, and send the concatenated sequence to the BERT sequence classification model to get the score of the two instances expressing the same relation. Denote the BERT model as B, the query instance as x and the paired supporting instance as xjr (the j-th supporting instance for the relation r), B(x, xjr ) outputs a two-element vector corresponding to scores of the pair sharing the same relation and not sharing the same relation. The probability over each relation in the few-shot scenario, includin"
D19-1649,P19-1279,0,0.106636,"Missing"
D19-1649,C18-1099,1,0.86151,"o better demonstrate the effects of the NOTA relation, we use the original FewRel dataset for fewshot NOTA, instead of the new test set, which can get rid of the influence of domain adaptation. 3 Approaches for Few-Shot DA Many efforts have been devoted for domain adaptation, like subspace mapping (Pan et al., 2010; Fernando et al., 2013), finding domain-invariant spaces (Baktashmotlagh et al., 2013; Ganin et al., 2016), feature augmentation (Blitzer et al., 2006) and minimax estimators (Provost and Fawcett, 2001). Among them, adversarial training (Goodfellow et al., 2015; Ganin et al., 2016; Wang et al., 2018) has been proved to be efficient in finding domain-invariant features. It is a game process between an encoder and a discriminator, where the encoder tries to generate domain-invariant features while the discriminator tries to tell which domain the features are from. Here we follow the adversarial training setting in Wang et al. (2018), where a two-layer perceptron network is used as the discriminator. While training the few-shot learning task, we feed the sentence encoder E and the discriminator D with the corpora from the training domain and the test domain, and optimize the min-max game, X"
D19-1649,D18-1514,1,0.42617,"Missing"
D19-1649,W09-2415,0,0.332326,"Missing"
D19-1649,P09-1113,0,0.589697,"ach of which consists of (R, S, x, r), where R = {r1 , r2 , ..., rN } is the sampled relation set, r ∈ R is the correct relation label for the query x, and S is the supporting set containing K instances for each relation, S = {(xjri , ri )}, 1 ≤ i ≤ N, 1 ≤ j ≤ K. (1) Models should predict the relation label y ∈ R for the query instance x based on the given S and R. Both of the following two challenges are based on this N -way K-shot setting. Both the training and test sets of the original FewRel dataset are constructed by manually annotating the distantly supervised (Bunescu and Mooney, 2007; Mintz et al., 2009) results on Wikipedia corpus and Wikidata (Vrandeˇci´c and Kr¨otzsch, 2014) knowledge bases. In other words, they are from the same domain, yet in a real-world scenario, we might train models on one domain and perform few-shot learning on a different one. For example, we may train models on Wikipedia, which has large amounts of data and adequate annotations, and then perform few-shot learning on some domains suffering data sparsity, like literature, finance and medicine. Note that, not only do these corpora differ vastly from each other in morphology and syntax, but there are wide disparities"
D19-3029,N16-1030,0,0.0461089,"ween the entity pair. Hence Riedel et al. (2010) and Hoffmann et al. (2011) introduce to aggregate the sentences mentioning the same entity pair into Entity-Oriented Applications For extracting structured information from plain text, it requires to extract entities from text and then predict relations between entities. In normal RE scenarios, all entity mentions have been already annotated and RE models are just required to classify relations for all annotated entity pairs. Although the entity-oriented applications are not the focus of our toolkit, we still implement specific modules for NER (Lample et al., 2016) and EL (Han et al., 2011). The NER modules can detect words or phrases (also named entity mentions) representing real-world objects. In OpenNRE, we provide two approaches for NER, one is 170 Architecture of OpenNRE a entity-pair bag. As shown in Figure 1, synthesizing the features of different sentences in a bag can provide more reliable information and result in more accurate predictions. The Bag-level setting is widely applied by various distantly supervised RE methods (Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b), and thus it is also integrated into OpenNRE. Sentence-Level RE Da"
D19-3029,P16-1200,1,0.949986,"ng, China Institute for Artificial Intelligence, Tsinghua University, Beijing, China State Key Lab on Intelligent Technology and Systems, Tsinghua University, Beijing, China {hanxu17,gty16,yy18,ydm18}@mails.tsinghua.edu.cn Abstract which makes more and more researchers and industry developers pay attention to this field. Although the current NRE models are effective and have been applied for various scenarios, including supervised learning paradigm (Zeng et al., 2014a; Nguyen and Grishman, 2015; Zhang et al., 2015; Zhou et al., 2016), distantly supervised learning paradigm (Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b), few-shot learning paradigm (Han et al., 2018c; Gao et al., 2019; Ye and Ling, 2019; Soares et al., 2019; Zhang et al., 2019), there still lack an effective and stable toolkit to support the implementation, deployment and evaluation of models. In fact, for other tasks related to RE, there have been already some effective and long-term maintained toolkits, such as Spacy1 for named entity recognition (NER), TagMe (Ferragina and Scaiella, 2010) for entity linking (EL), OpenKE (Han et al., 2018a) for knowledge embedding, and Stanford OpenIE (Angeli et al., 2015) for open infor"
D19-3029,P09-1113,0,0.489525,"sentence-level RE, bag-level RE, document-level RE, and few-shot RE. For completing a full pipeline of extracting structured information, we also enable OpenNRE to have the capacity of entity-oriented applications to a certain extent, e.g., NER and EL. The examples of these application scenarios are all shown in Figure 1. 2.1 Sentence-Level Relation Extraction 2.3 Bag-Level Relation Extraction The supervised RE methods suffer from several problems, especially their requirements of adequate annotated data for training. As manually labeling large amounts of data is expensive and time-consuming, Mintz et al. (2009) introduce distant supervision to automatically label large amounts of data for RE by aligning knowledge graphs and text. Although distant supervision brings sufficient auto-labeled data, it also leads to the wrong labeling problem. Considering an entity pair may occur several times in different sentences, and there is a significant probability that some of these sentences can express the relation between the entity pair. Hence Riedel et al. (2010) and Hoffmann et al. (2011) introduce to aggregate the sentences mentioning the same entity pair into Entity-Oriented Applications For extracting st"
D19-3029,W15-1506,0,0.0733895,"ao∗ , Yuan Yao, Demin Ye, Zhiyuan Liu† , Maosong Sun Department of Computer Science and Technology, Tsinghua University, Beijing, China Institute for Artificial Intelligence, Tsinghua University, Beijing, China State Key Lab on Intelligent Technology and Systems, Tsinghua University, Beijing, China {hanxu17,gty16,yy18,ydm18}@mails.tsinghua.edu.cn Abstract which makes more and more researchers and industry developers pay attention to this field. Although the current NRE models are effective and have been applied for various scenarios, including supervised learning paradigm (Zeng et al., 2014a; Nguyen and Grishman, 2015; Zhang et al., 2015; Zhou et al., 2016), distantly supervised learning paradigm (Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b), few-shot learning paradigm (Han et al., 2018c; Gao et al., 2019; Ye and Ling, 2019; Soares et al., 2019; Zhang et al., 2019), there still lack an effective and stable toolkit to support the implementation, deployment and evaluation of models. In fact, for other tasks related to RE, there have been already some effective and long-term maintained toolkits, such as Spacy1 for named entity recognition (NER), TagMe (Ferragina and Scaiella, 2010) for entity linki"
D19-3029,P15-1034,0,0.0609609,"Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b), few-shot learning paradigm (Han et al., 2018c; Gao et al., 2019; Ye and Ling, 2019; Soares et al., 2019; Zhang et al., 2019), there still lack an effective and stable toolkit to support the implementation, deployment and evaluation of models. In fact, for other tasks related to RE, there have been already some effective and long-term maintained toolkits, such as Spacy1 for named entity recognition (NER), TagMe (Ferragina and Scaiella, 2010) for entity linking (EL), OpenKE (Han et al., 2018a) for knowledge embedding, and Stanford OpenIE (Angeli et al., 2015) for open information extraction. Hence, it becomes necessary and significant to systematically develop an efficient and effective toolkit for RE. To this end, we develop an open and extensible toolkit for designing and implementing RE models, especially for NRE models, which is named “OpenNRE”. The toolkit prioritizes operational efficiency based on TensorFlow and PyTorch, which support quick model training and validation. Meanwhile, the toolkit maintains sufficient system encapsulation and model extensibility, which can meet some individual requirements of incorporating new models. To keep t"
D19-3029,N19-1423,0,0.0171983,"bniz was a member of the Prussian Academy of Sciences Samuel Langhorne Clemens, better known by his pen name Mark Twain child of place of death [Olivia Langdon] [writer] Newton served as the president of the Royal Society member of birth name sibling of spouse of occupation Query … Supporting Set member of In 1921, Ernest Hemingway married Hadley Richardson, the first of his four wives [Jean Clemens] Euler was elected a foreign member of the Royal Swedish Academy of Sciences Figure 1: The examples of all application scenarios in OpenNRE. based on spaCy, the other is based on fine-tuning BERT (Devlin et al., 2019). The EL modules can align those entity mentions to the entities in Wikidata (Vrandeˇci´c and Kr¨otzsch, 2014) based on TagMe (Ferragina and Scaiella, 2010). models, they can quickly start up their RE system based on OpenNRE, without knowing too many technical details and writing tedious glue code. An online system is also available to extract structured relational facts from the text with friendly interactive interfaces and fast reaction speed. We will provide long-term maintenance to fix bugs and meet new requests for OpenNRE, and we think both researchers and industry developers can benefit"
D19-3029,P19-1279,0,0.172058,"gy and Systems, Tsinghua University, Beijing, China {hanxu17,gty16,yy18,ydm18}@mails.tsinghua.edu.cn Abstract which makes more and more researchers and industry developers pay attention to this field. Although the current NRE models are effective and have been applied for various scenarios, including supervised learning paradigm (Zeng et al., 2014a; Nguyen and Grishman, 2015; Zhang et al., 2015; Zhou et al., 2016), distantly supervised learning paradigm (Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b), few-shot learning paradigm (Han et al., 2018c; Gao et al., 2019; Ye and Ling, 2019; Soares et al., 2019; Zhang et al., 2019), there still lack an effective and stable toolkit to support the implementation, deployment and evaluation of models. In fact, for other tasks related to RE, there have been already some effective and long-term maintained toolkits, such as Spacy1 for named entity recognition (NER), TagMe (Ferragina and Scaiella, 2010) for entity linking (EL), OpenKE (Han et al., 2018a) for knowledge embedding, and Stanford OpenIE (Angeli et al., 2015) for open information extraction. Hence, it becomes necessary and significant to systematically develop an efficient and effective toolkit f"
D19-3029,D17-1187,0,0.633015,"ired to be capable of accurately capturing relation patterns of these small amounts of training instances. Considering few-shot RE is important for handling long-tail relations, OpenNRE also provides a custom platform for further research in this direction. 3 can maximize the reuse of code to avoid unnecessary redundant model implementations. For operational efficiency, OpenNRE is based on TensorFlow and PyTorch, which enables developers to train models on GPUs. For model extensibility, we systematically implement various neural modules and some special algorithms (e.g., adversarial training (Wu et al., 2017) and reinforcement learning (Feng et al., 2018)). Hence, it is easy to implement new RE models based on OpenNRE. We also implement some typical RE models so as to conveniently train custom models for specific application scenarios. More specifically, OpenNRE attains the above four design objects through implementing the following five components. 3.1 Tokenization The tokenization component is responsible for tokenizing input text into several input tokens. In OpenNRE, we implement both word-level tokenization and subword-level tokenization. These two operations satisfy most tokenization demand"
D19-3029,P19-1074,1,0.835257,"te predictions. The Bag-level setting is widely applied by various distantly supervised RE methods (Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b), and thus it is also integrated into OpenNRE. Sentence-Level RE Data Loader Model Train Method Eval Method Model Encoder Forward Bag-Level RE Few-Shot RE … Softmax Classifier Instance-Level Attention Prototypical Networks … Module Tokenizer Forward Encoder Tokenization CNN BERT … Tokenization Word Tokenization Word Piece Tokenization … 2.4 Framework Framework Module Convolutional NN Pooling … Document-Level Relation Extraction Example Code Yao et al. (2019) have pointed out that multiple entities in documents often exhibit complex intersentence relations rather than intra-sentence relations. Besides, as shown in Figure 1, a large number of relational facts are expressed in multiple sentences, e.g., Langdon is the sibling of Jean Clemens. Hence, it is hard to extract these intersentence relations with both the sentence-level and bag-level settings. Although the document-level RE setting is not widely explored by the current work, we argue that this scenario remains an open problem for future research, and still integrate document-level RE into Op"
D19-3029,P19-1277,0,0.217226,"ntelligent Technology and Systems, Tsinghua University, Beijing, China {hanxu17,gty16,yy18,ydm18}@mails.tsinghua.edu.cn Abstract which makes more and more researchers and industry developers pay attention to this field. Although the current NRE models are effective and have been applied for various scenarios, including supervised learning paradigm (Zeng et al., 2014a; Nguyen and Grishman, 2015; Zhang et al., 2015; Zhou et al., 2016), distantly supervised learning paradigm (Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b), few-shot learning paradigm (Han et al., 2018c; Gao et al., 2019; Ye and Ling, 2019; Soares et al., 2019; Zhang et al., 2019), there still lack an effective and stable toolkit to support the implementation, deployment and evaluation of models. In fact, for other tasks related to RE, there have been already some effective and long-term maintained toolkits, such as Spacy1 for named entity recognition (NER), TagMe (Ferragina and Scaiella, 2010) for entity linking (EL), OpenKE (Han et al., 2018a) for knowledge embedding, and Stanford OpenIE (Angeli et al., 2015) for open information extraction. Hence, it becomes necessary and significant to systematically develop an efficient an"
D19-3029,D18-2024,1,0.917593,"e for Artificial Intelligence, Tsinghua University, Beijing, China State Key Lab on Intelligent Technology and Systems, Tsinghua University, Beijing, China {hanxu17,gty16,yy18,ydm18}@mails.tsinghua.edu.cn Abstract which makes more and more researchers and industry developers pay attention to this field. Although the current NRE models are effective and have been applied for various scenarios, including supervised learning paradigm (Zeng et al., 2014a; Nguyen and Grishman, 2015; Zhang et al., 2015; Zhou et al., 2016), distantly supervised learning paradigm (Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b), few-shot learning paradigm (Han et al., 2018c; Gao et al., 2019; Ye and Ling, 2019; Soares et al., 2019; Zhang et al., 2019), there still lack an effective and stable toolkit to support the implementation, deployment and evaluation of models. In fact, for other tasks related to RE, there have been already some effective and long-term maintained toolkits, such as Spacy1 for named entity recognition (NER), TagMe (Ferragina and Scaiella, 2010) for entity linking (EL), OpenKE (Han et al., 2018a) for knowledge embedding, and Stanford OpenIE (Angeli et al., 2015) for open information extraction."
D19-3029,D15-1203,0,0.816632,"a University, Beijing, China Institute for Artificial Intelligence, Tsinghua University, Beijing, China State Key Lab on Intelligent Technology and Systems, Tsinghua University, Beijing, China {hanxu17,gty16,yy18,ydm18}@mails.tsinghua.edu.cn Abstract which makes more and more researchers and industry developers pay attention to this field. Although the current NRE models are effective and have been applied for various scenarios, including supervised learning paradigm (Zeng et al., 2014a; Nguyen and Grishman, 2015; Zhang et al., 2015; Zhou et al., 2016), distantly supervised learning paradigm (Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b), few-shot learning paradigm (Han et al., 2018c; Gao et al., 2019; Ye and Ling, 2019; Soares et al., 2019; Zhang et al., 2019), there still lack an effective and stable toolkit to support the implementation, deployment and evaluation of models. In fact, for other tasks related to RE, there have been already some effective and long-term maintained toolkits, such as Spacy1 for named entity recognition (NER), TagMe (Ferragina and Scaiella, 2010) for entity linking (EL), OpenKE (Han et al., 2018a) for knowledge embedding, and Stanford OpenIE (Angeli et al., 20"
D19-3029,D18-1247,1,0.904919,"e for Artificial Intelligence, Tsinghua University, Beijing, China State Key Lab on Intelligent Technology and Systems, Tsinghua University, Beijing, China {hanxu17,gty16,yy18,ydm18}@mails.tsinghua.edu.cn Abstract which makes more and more researchers and industry developers pay attention to this field. Although the current NRE models are effective and have been applied for various scenarios, including supervised learning paradigm (Zeng et al., 2014a; Nguyen and Grishman, 2015; Zhang et al., 2015; Zhou et al., 2016), distantly supervised learning paradigm (Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b), few-shot learning paradigm (Han et al., 2018c; Gao et al., 2019; Ye and Ling, 2019; Soares et al., 2019; Zhang et al., 2019), there still lack an effective and stable toolkit to support the implementation, deployment and evaluation of models. In fact, for other tasks related to RE, there have been already some effective and long-term maintained toolkits, such as Spacy1 for named entity recognition (NER), TagMe (Ferragina and Scaiella, 2010) for entity linking (EL), OpenKE (Han et al., 2018a) for knowledge embedding, and Stanford OpenIE (Angeli et al., 2015) for open information extraction."
D19-3029,C14-1220,0,0.712368,"n Xu Han∗ , Tianyu Gao∗ , Yuan Yao, Demin Ye, Zhiyuan Liu† , Maosong Sun Department of Computer Science and Technology, Tsinghua University, Beijing, China Institute for Artificial Intelligence, Tsinghua University, Beijing, China State Key Lab on Intelligent Technology and Systems, Tsinghua University, Beijing, China {hanxu17,gty16,yy18,ydm18}@mails.tsinghua.edu.cn Abstract which makes more and more researchers and industry developers pay attention to this field. Although the current NRE models are effective and have been applied for various scenarios, including supervised learning paradigm (Zeng et al., 2014a; Nguyen and Grishman, 2015; Zhang et al., 2015; Zhou et al., 2016), distantly supervised learning paradigm (Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b), few-shot learning paradigm (Han et al., 2018c; Gao et al., 2019; Ye and Ling, 2019; Soares et al., 2019; Zhang et al., 2019), there still lack an effective and stable toolkit to support the implementation, deployment and evaluation of models. In fact, for other tasks related to RE, there have been already some effective and long-term maintained toolkits, such as Spacy1 for named entity recognition (NER), TagMe (Ferragina and Scai"
D19-3029,D18-1514,1,0.864567,"e for Artificial Intelligence, Tsinghua University, Beijing, China State Key Lab on Intelligent Technology and Systems, Tsinghua University, Beijing, China {hanxu17,gty16,yy18,ydm18}@mails.tsinghua.edu.cn Abstract which makes more and more researchers and industry developers pay attention to this field. Although the current NRE models are effective and have been applied for various scenarios, including supervised learning paradigm (Zeng et al., 2014a; Nguyen and Grishman, 2015; Zhang et al., 2015; Zhou et al., 2016), distantly supervised learning paradigm (Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b), few-shot learning paradigm (Han et al., 2018c; Gao et al., 2019; Ye and Ling, 2019; Soares et al., 2019; Zhang et al., 2019), there still lack an effective and stable toolkit to support the implementation, deployment and evaluation of models. In fact, for other tasks related to RE, there have been already some effective and long-term maintained toolkits, such as Spacy1 for named entity recognition (NER), TagMe (Ferragina and Scaiella, 2010) for entity linking (EL), OpenKE (Han et al., 2018a) for knowledge embedding, and Stanford OpenIE (Angeli et al., 2015) for open information extraction."
D19-3029,Y15-1009,0,0.292487,"hiyuan Liu† , Maosong Sun Department of Computer Science and Technology, Tsinghua University, Beijing, China Institute for Artificial Intelligence, Tsinghua University, Beijing, China State Key Lab on Intelligent Technology and Systems, Tsinghua University, Beijing, China {hanxu17,gty16,yy18,ydm18}@mails.tsinghua.edu.cn Abstract which makes more and more researchers and industry developers pay attention to this field. Although the current NRE models are effective and have been applied for various scenarios, including supervised learning paradigm (Zeng et al., 2014a; Nguyen and Grishman, 2015; Zhang et al., 2015; Zhou et al., 2016), distantly supervised learning paradigm (Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b), few-shot learning paradigm (Han et al., 2018c; Gao et al., 2019; Ye and Ling, 2019; Soares et al., 2019; Zhang et al., 2019), there still lack an effective and stable toolkit to support the implementation, deployment and evaluation of models. In fact, for other tasks related to RE, there have been already some effective and long-term maintained toolkits, such as Spacy1 for named entity recognition (NER), TagMe (Ferragina and Scaiella, 2010) for entity linking (EL), OpenKE (Han"
D19-3029,W09-2415,0,0.128206,"Missing"
D19-3029,P19-1139,1,0.92994,"hua University, Beijing, China {hanxu17,gty16,yy18,ydm18}@mails.tsinghua.edu.cn Abstract which makes more and more researchers and industry developers pay attention to this field. Although the current NRE models are effective and have been applied for various scenarios, including supervised learning paradigm (Zeng et al., 2014a; Nguyen and Grishman, 2015; Zhang et al., 2015; Zhou et al., 2016), distantly supervised learning paradigm (Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b), few-shot learning paradigm (Han et al., 2018c; Gao et al., 2019; Ye and Ling, 2019; Soares et al., 2019; Zhang et al., 2019), there still lack an effective and stable toolkit to support the implementation, deployment and evaluation of models. In fact, for other tasks related to RE, there have been already some effective and long-term maintained toolkits, such as Spacy1 for named entity recognition (NER), TagMe (Ferragina and Scaiella, 2010) for entity linking (EL), OpenKE (Han et al., 2018a) for knowledge embedding, and Stanford OpenIE (Angeli et al., 2015) for open information extraction. Hence, it becomes necessary and significant to systematically develop an efficient and effective toolkit for RE. To this end, w"
D19-3029,P16-2034,0,0.322331,"g Sun Department of Computer Science and Technology, Tsinghua University, Beijing, China Institute for Artificial Intelligence, Tsinghua University, Beijing, China State Key Lab on Intelligent Technology and Systems, Tsinghua University, Beijing, China {hanxu17,gty16,yy18,ydm18}@mails.tsinghua.edu.cn Abstract which makes more and more researchers and industry developers pay attention to this field. Although the current NRE models are effective and have been applied for various scenarios, including supervised learning paradigm (Zeng et al., 2014a; Nguyen and Grishman, 2015; Zhang et al., 2015; Zhou et al., 2016), distantly supervised learning paradigm (Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b), few-shot learning paradigm (Han et al., 2018c; Gao et al., 2019; Ye and Ling, 2019; Soares et al., 2019; Zhang et al., 2019), there still lack an effective and stable toolkit to support the implementation, deployment and evaluation of models. In fact, for other tasks related to RE, there have been already some effective and long-term maintained toolkits, such as Spacy1 for named entity recognition (NER), TagMe (Ferragina and Scaiella, 2010) for entity linking (EL), OpenKE (Han et al., 2018a) for"
D19-3029,P11-1055,0,0.138259,"ements of adequate annotated data for training. As manually labeling large amounts of data is expensive and time-consuming, Mintz et al. (2009) introduce distant supervision to automatically label large amounts of data for RE by aligning knowledge graphs and text. Although distant supervision brings sufficient auto-labeled data, it also leads to the wrong labeling problem. Considering an entity pair may occur several times in different sentences, and there is a significant probability that some of these sentences can express the relation between the entity pair. Hence Riedel et al. (2010) and Hoffmann et al. (2011) introduce to aggregate the sentences mentioning the same entity pair into Entity-Oriented Applications For extracting structured information from plain text, it requires to extract entities from text and then predict relations between entities. In normal RE scenarios, all entity mentions have been already annotated and RE models are just required to classify relations for all annotated entity pairs. Although the entity-oriented applications are not the focus of our toolkit, we still implement specific modules for NER (Lample et al., 2016) and EL (Han et al., 2011). The NER modules can detect"
I17-1024,P12-1092,0,0.26475,"pe embedding models, and achieves more stable performance when trained on smaller data. 1 Figure 1: Relatedness among senses of the word “book”. To enhance the expression ability of the embedding model, recent research has a rising enthusiasm for representing words at sense level. That is, an individual word is represented as multiple vectors, where each vector corresponds to one of its meanings. Pervious work mostly focus on using clustering to induce word senses (each cluster refers to one of the senses) and then learn the word sense representations respectively (Reisinger and Mooney, 2010; Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014; Li and Jurafsky, 2015). However, the above approaches ignore the relatedness among the word senses. Hence the following limitations arise in the usage of hard clustering. First of all, many clustering errors will be caused by using hard clustering based method because the senses of the polysemous word actuIntroduction Word embedding, representing words in a low dimentional vector space, plays an increasing important role in various IR and NLP related tasks, such as language modeling (Bengio et al., 2006; ∗ Corresponding author. 233 Proceedings of"
I17-1024,S13-2049,0,0.0189139,"from more data sparsity issue as compared to the Skip-gram model. Thirdly, the embedding quality is considerably sensitive to the clustering results due to the isolation of different sense clusters. To address this problem, we learn the embedding vectors of the word senses with some common features if the senses are related. Instead of clearly cutting the sense cluster boundaries, one occurrence of the word will be assigned into multiple sense clusters with different probabilities, which agrees with a classic task of word sense annotation, Graded Word Sense Assignment (Erk and McCarthy, 2009; Jurgens and Klapaftis, 2013). Actually, the senses of a polysemous word are related not only by the contiguity of meaning within a semantic field1 , but also by the extended relationship between the original meaning and the extended meaning (Von Engelhardt and Zimmermann, 1988). We investigate the relatedness of the synsets (word senses) in WordNet (Miller, 1995) through the Wu & Palmer measure2 (Wu and Palmer, 1994), and present an interesting example of the word “book” in Figure 1. The right side is the similarity matrix of its 11 nominal synsets, where si denotes the ith synset. Each tile represents a similarity value"
I17-1024,D15-1200,0,0.253581,"ained on smaller data. 1 Figure 1: Relatedness among senses of the word “book”. To enhance the expression ability of the embedding model, recent research has a rising enthusiasm for representing words at sense level. That is, an individual word is represented as multiple vectors, where each vector corresponds to one of its meanings. Pervious work mostly focus on using clustering to induce word senses (each cluster refers to one of the senses) and then learn the word sense representations respectively (Reisinger and Mooney, 2010; Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014; Li and Jurafsky, 2015). However, the above approaches ignore the relatedness among the word senses. Hence the following limitations arise in the usage of hard clustering. First of all, many clustering errors will be caused by using hard clustering based method because the senses of the polysemous word actuIntroduction Word embedding, representing words in a low dimentional vector space, plays an increasing important role in various IR and NLP related tasks, such as language modeling (Bengio et al., 2006; ∗ Corresponding author. 233 Proceedings of the The 8th International Joint Conference on Natural Language Proces"
I17-1024,P17-1149,1,0.885707,"Missing"
I17-1024,D14-1110,1,0.878368,"Missing"
I17-1024,D14-1113,0,0.396051,"stable performance when trained on smaller data. 1 Figure 1: Relatedness among senses of the word “book”. To enhance the expression ability of the embedding model, recent research has a rising enthusiasm for representing words at sense level. That is, an individual word is represented as multiple vectors, where each vector corresponds to one of its meanings. Pervious work mostly focus on using clustering to induce word senses (each cluster refers to one of the senses) and then learn the word sense representations respectively (Reisinger and Mooney, 2010; Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014; Li and Jurafsky, 2015). However, the above approaches ignore the relatedness among the word senses. Hence the following limitations arise in the usage of hard clustering. First of all, many clustering errors will be caused by using hard clustering based method because the senses of the polysemous word actuIntroduction Word embedding, representing words in a low dimentional vector space, plays an increasing important role in various IR and NLP related tasks, such as language modeling (Bengio et al., 2006; ∗ Corresponding author. 233 Proceedings of the The 8th International Joint Conference on"
I17-1024,N10-1013,0,0.0366961,"Missing"
I17-1024,P13-1045,0,0.0293059,"Missing"
I17-1024,C14-1016,0,0.0207572,"and achieves more stable performance when trained on smaller data. 1 Figure 1: Relatedness among senses of the word “book”. To enhance the expression ability of the embedding model, recent research has a rising enthusiasm for representing words at sense level. That is, an individual word is represented as multiple vectors, where each vector corresponds to one of its meanings. Pervious work mostly focus on using clustering to induce word senses (each cluster refers to one of the senses) and then learn the word sense representations respectively (Reisinger and Mooney, 2010; Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014; Li and Jurafsky, 2015). However, the above approaches ignore the relatedness among the word senses. Hence the following limitations arise in the usage of hard clustering. First of all, many clustering errors will be caused by using hard clustering based method because the senses of the polysemous word actuIntroduction Word embedding, representing words in a low dimentional vector space, plays an increasing important role in various IR and NLP related tasks, such as language modeling (Bengio et al., 2006; ∗ Corresponding author. 233 Proceedings of the The 8th Interna"
I17-1024,P10-1040,0,0.15214,"Missing"
I17-1024,D09-1046,0,\N,Missing
N19-1105,D15-1247,0,0.110478,"Missing"
N19-1105,C18-1075,0,0.479342,"Missing"
N19-1105,P98-1013,0,0.408429,"Missing"
N19-1105,D18-1021,1,0.869581,"Missing"
N19-1105,P18-1241,0,0.0663714,"Missing"
N19-1105,P17-1038,0,0.449592,"Missing"
N19-1105,P15-1017,0,0.704641,"Missing"
N19-1105,N18-1076,0,0.147721,"Missing"
N19-1105,I17-1036,0,0.128152,"Missing"
N19-1105,P16-2011,0,0.135941,"Missing"
N19-1105,N18-2058,0,0.476683,"Missing"
N19-1105,P16-2060,0,0.130043,"Missing"
N19-1105,P09-2093,0,0.207456,"Missing"
N19-1105,P11-1113,0,0.73024,"Missing"
N19-1105,P18-1048,0,0.155771,"Missing"
N19-1105,P16-1025,0,0.240411,"Missing"
N19-1105,C10-1077,0,0.711492,"Missing"
N19-1105,P10-1081,0,0.823295,"Missing"
N19-1105,P18-1145,0,0.0214274,"Missing"
N19-1105,D18-1127,0,0.406701,"Missing"
N19-1105,P16-1201,0,0.0323032,"Missing"
N19-1105,P17-1164,0,0.540835,"Missing"
N19-1105,P11-1163,0,0.257108,"Missing"
N19-1105,E12-1029,0,0.137736,"Missing"
N19-1105,P09-1113,0,0.272376,"Missing"
N19-1105,P08-1030,0,0.791729,"Missing"
N19-1105,P13-1008,0,0.756973,"Missing"
N19-1105,C18-1007,0,0.0627013,"Missing"
N19-1105,N16-1034,0,0.425474,"Missing"
N19-1105,N16-1033,0,0.379533,"Missing"
N19-1105,P15-2060,0,0.493355,"Missing"
N19-1105,P18-4009,0,0.10604,"Missing"
N19-1105,P18-1046,0,0.0622712,"Missing"
N19-1105,D15-1203,0,0.0776927,"Missing"
N19-1105,P18-2066,0,0.731835,"Missing"
N19-1105,C18-1099,1,0.90081,"Missing"
N19-1105,D17-1187,0,0.0611022,"Missing"
N19-1105,1983.tc-1.13,0,0.586136,"Missing"
N19-1297,D13-1160,0,0.0192398,"many large-scale knowledge bases (KBs) such as Freebase (Bollacker et al., 2008), DBpedia (Lehmann et al., 2015), YAGO (Suchanek et al., 2007) and Wikidata (Vrandeˇci´c author: Female English Best Female Tennis Player Award Introduction ∗ Corresponding uzy@tsinghua.edu.cn). Saginaw (liand Kr¨otzsch, 2014) to store facts of the real world. Most KBs typically organize the complex structured information about facts in the form of triples (head entity, relation, tail entity), e.g., (Bill Gates, CEOof, Microsoft Inc.). These KBs have been widely used in many AI and NLP tasks such as text analysis (Berant et al., 2013), question answering (Bordes et al., 2014a), and information retrieval (Hoffmann et al., 2011). The construction of these KBs is always an ongoing process due to the endless growth of realworld facts. Hence, many tasks such as knowledge base completion (KBC) and relation prediction (RP) are proposed to enrich KBs. The KBC task usually assumes that one entity and the relation r are given, and another entity is missing and required to be predicted. In general, we wish to predict the missing entity in (h, r, ?) or (?, r, t), where h and t denote a head and tail entity respectively. Similarly, the"
N19-1297,D14-1067,0,0.103026,"h as Freebase (Bollacker et al., 2008), DBpedia (Lehmann et al., 2015), YAGO (Suchanek et al., 2007) and Wikidata (Vrandeˇci´c author: Female English Best Female Tennis Player Award Introduction ∗ Corresponding uzy@tsinghua.edu.cn). Saginaw (liand Kr¨otzsch, 2014) to store facts of the real world. Most KBs typically organize the complex structured information about facts in the form of triples (head entity, relation, tail entity), e.g., (Bill Gates, CEOof, Microsoft Inc.). These KBs have been widely used in many AI and NLP tasks such as text analysis (Berant et al., 2013), question answering (Bordes et al., 2014a), and information retrieval (Hoffmann et al., 2011). The construction of these KBs is always an ongoing process due to the endless growth of realworld facts. Hence, many tasks such as knowledge base completion (KBC) and relation prediction (RP) are proposed to enrich KBs. The KBC task usually assumes that one entity and the relation r are given, and another entity is missing and required to be predicted. In general, we wish to predict the missing entity in (h, r, ?) or (?, r, t), where h and t denote a head and tail entity respectively. Similarly, the RP task predicts the missing relation gi"
N19-1297,P11-1055,0,0.0408868,"ehmann et al., 2015), YAGO (Suchanek et al., 2007) and Wikidata (Vrandeˇci´c author: Female English Best Female Tennis Player Award Introduction ∗ Corresponding uzy@tsinghua.edu.cn). Saginaw (liand Kr¨otzsch, 2014) to store facts of the real world. Most KBs typically organize the complex structured information about facts in the form of triples (head entity, relation, tail entity), e.g., (Bill Gates, CEOof, Microsoft Inc.). These KBs have been widely used in many AI and NLP tasks such as text analysis (Berant et al., 2013), question answering (Bordes et al., 2014a), and information retrieval (Hoffmann et al., 2011). The construction of these KBs is always an ongoing process due to the endless growth of realworld facts. Hence, many tasks such as knowledge base completion (KBC) and relation prediction (RP) are proposed to enrich KBs. The KBC task usually assumes that one entity and the relation r are given, and another entity is missing and required to be predicted. In general, we wish to predict the missing entity in (h, r, ?) or (?, r, t), where h and t denote a head and tail entity respectively. Similarly, the RP task predicts the missing relation given the head and tail entities and their evidence sen"
N19-1297,P15-1067,0,0.271261,"KBR models typically embed the semantics of both entities and relations into low-dimensional semantic space, i.e., embeddings. For example, TransE (Bordes et al., 2013) learns low-dimensional and real-valued embeddings for both entities and relations by regarding the relation of each triple fact as a translation from its head entity to the tail entity. TransE can thus compute the valid score for each triple by measuring how well the relation can play a translation between the head and tail entities. Many methods have been proposed to extend TransE to deal with various characteristics of KBs (Ji et al., 2015, 2016; He et al., 2015; Lin et al., 2015a). To solve the FDKB task using KBR, one feasible way is to exhaustively calculate the scores of all (r, t) combinations for the given head entity h. Afterwards, the highly-scored facts are returned as results. However, this idea has some drawbacks: (1) It takes all relations to calculate ranking scores for each head entity, ignoring the nature of the head entity. The combination of all possible relations and tail entities will lead to huge amount of computations. (2) A large set of candidate triples immerses the correct triples into a lot of noisy tri"
N19-1297,D15-1082,1,0.887079,"Missing"
N19-1297,P09-1113,0,0.0284715,"nsive analysis of the framework in discovering different kinds of facts. The contributions of this paper can be summarized as follows: (1) We introduce a new task of fact discovery from knowledge base, which is more practical. (2) We propose a new framework based on the facet decomposition which achieves promising results. 2 Related Work In recent years, many tasks (Wang et al., 2017) have been proposed to help represent and enrich KBs. Tasks such as knowledge base completion (KBC) (Bordes et al., 2013; Wang et al., 2014; Ji et al., 2015, 2016; Wang et al., 2017) and relation prediction (RP) (Mintz et al., 2009; Lin et al., 2015a; Xie et al., 2016) are widely studied and many models are proposed to improve the performance on these tasks. However, the intention of these tasks is to test the performance of models in representing KBs and thus they cannot be used directly to discover new facts of KBs. Moreover, our FDKB task is not a simple combination of the KBC and RP task since both of these two tasks require to know two of the triples while we assume we only know the head entity. A common approach to solving these tasks is to build a knowledge base representation (KBR) model with different kinds of"
P10-3009,D07-1081,0,\N,Missing
P11-2085,P00-1031,0,0.724151,"ith English typos, we observe some language-specific properties in Chinese have impact on errors. All in all, user behaviors (Zheng et al., 2009; Zheng et al., 2010; Zheng et al., 2011b) in Chinese Pinyin input method provide novel perspectives for natural language processing tasks. Below we sketch three possible directions for the future work: (1) we should consider position features in analyzing Pinyin errors. For example, it is less likely that users make errors in the first letter of an input Pinyin. (2) we aim at designing a selfadaptive input method that provide error-tolerant features (Chen and Lee, 2000; Zheng et al., 2011a). (3) we want to build a Chinese spelling correction system based on extracted error-correction pairs. Acknowledgments This work is supported by a Tsinghua-Sogou joint research project and the National Natural Science Foundation of China under Grant No. 60873174. References F. Ahmad and G. Kondrak. 2005. Learning a spelling error model from search query logs. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 955–962. K. Atkinson. 2008. Gnu aspell 0.60.6. http://aspell.sourceforge.net. E. Brill and R.C"
P11-2085,D07-1019,0,0.0164549,"pos suggests that some language-specific properties of Chinese lead to a part of input errors. To the best of our knowledge, this paper is the first one which analyzes user input behaviors in Chinese Pinyin input method. The rest of this paper is organized as follows. Section 2 discusses related works. Section 3 introduces how we collect errors in Chinese Pinyin input method. In Section 4, we investigate the reasons that result in these errors. Section 5 concludes the whole paper and discusses future work. 2 Previous Work For English spelling correction (Kukich, 1992; Ahmad and Kondrak, 2005; Chen et al., 2007; Whitelaw et al., 2009; Gao et al., 2010), most approaches make use of a lexicon which contains a list of well-spelled words (Hirst and Budanitsky, 2005; Islam and Inkpen, 2009). Context features (Rozovskaya and Roth, 2010) of words provide useful evidences for spelling correction. These features are usually represented by an n-gram language model (Cucerzan and Brill, 2004; Wilcox-O’Hearn et al., 2010). Phonetic features (Toutanova and Moore, 2002; Atkinson, 2008) are proved to be useful in English spelling correction. A spelling correction system is trained using these features by a noisy ch"
P11-2085,W04-3238,0,0.103425,"t method. In Section 4, we investigate the reasons that result in these errors. Section 5 concludes the whole paper and discusses future work. 2 Previous Work For English spelling correction (Kukich, 1992; Ahmad and Kondrak, 2005; Chen et al., 2007; Whitelaw et al., 2009; Gao et al., 2010), most approaches make use of a lexicon which contains a list of well-spelled words (Hirst and Budanitsky, 2005; Islam and Inkpen, 2009). Context features (Rozovskaya and Roth, 2010) of words provide useful evidences for spelling correction. These features are usually represented by an n-gram language model (Cucerzan and Brill, 2004; Wilcox-O’Hearn et al., 2010). Phonetic features (Toutanova and Moore, 2002; Atkinson, 2008) are proved to be useful in English spelling correction. A spelling correction system is trained using these features by a noisy channel model (Kernighan et al., 1990; Ristad et al., 1998; Brill and Moore, 2000). Chang (1994) first proposes a representative approach for Chinese spelling correction, which relies on sets of confusing characters. Zhang et al. (2000) propose an approximate word-matching algorithm for Chinese to solve Chinese spell detection and correction task. Zhang et al. (1999) present"
P11-2085,C10-1041,0,0.359851,"roperties of Chinese lead to a part of input errors. To the best of our knowledge, this paper is the first one which analyzes user input behaviors in Chinese Pinyin input method. The rest of this paper is organized as follows. Section 2 discusses related works. Section 3 introduces how we collect errors in Chinese Pinyin input method. In Section 4, we investigate the reasons that result in these errors. Section 5 concludes the whole paper and discusses future work. 2 Previous Work For English spelling correction (Kukich, 1992; Ahmad and Kondrak, 2005; Chen et al., 2007; Whitelaw et al., 2009; Gao et al., 2010), most approaches make use of a lexicon which contains a list of well-spelled words (Hirst and Budanitsky, 2005; Islam and Inkpen, 2009). Context features (Rozovskaya and Roth, 2010) of words provide useful evidences for spelling correction. These features are usually represented by an n-gram language model (Cucerzan and Brill, 2004; Wilcox-O’Hearn et al., 2010). Phonetic features (Toutanova and Moore, 2002; Atkinson, 2008) are proved to be useful in English spelling correction. A spelling correction system is trained using these features by a noisy channel model (Kernighan et al., 1990; Rista"
P11-2085,D09-1129,0,0.0303874,"input behaviors in Chinese Pinyin input method. The rest of this paper is organized as follows. Section 2 discusses related works. Section 3 introduces how we collect errors in Chinese Pinyin input method. In Section 4, we investigate the reasons that result in these errors. Section 5 concludes the whole paper and discusses future work. 2 Previous Work For English spelling correction (Kukich, 1992; Ahmad and Kondrak, 2005; Chen et al., 2007; Whitelaw et al., 2009; Gao et al., 2010), most approaches make use of a lexicon which contains a list of well-spelled words (Hirst and Budanitsky, 2005; Islam and Inkpen, 2009). Context features (Rozovskaya and Roth, 2010) of words provide useful evidences for spelling correction. These features are usually represented by an n-gram language model (Cucerzan and Brill, 2004; Wilcox-O’Hearn et al., 2010). Phonetic features (Toutanova and Moore, 2002; Atkinson, 2008) are proved to be useful in English spelling correction. A spelling correction system is trained using these features by a noisy channel model (Kernighan et al., 1990; Ristad et al., 1998; Brill and Moore, 2000). Chang (1994) first proposes a representative approach for Chinese spelling correction, which rel"
P11-2085,C90-2036,0,0.751524,"Missing"
P11-2085,O04-2007,0,0.0643163,"Missing"
P11-2085,P09-2007,0,0.0668477,"Missing"
P11-2085,C10-2085,0,0.0425557,"Missing"
P11-2085,D10-1094,0,0.0221483,"thod. The rest of this paper is organized as follows. Section 2 discusses related works. Section 3 introduces how we collect errors in Chinese Pinyin input method. In Section 4, we investigate the reasons that result in these errors. Section 5 concludes the whole paper and discusses future work. 2 Previous Work For English spelling correction (Kukich, 1992; Ahmad and Kondrak, 2005; Chen et al., 2007; Whitelaw et al., 2009; Gao et al., 2010), most approaches make use of a lexicon which contains a list of well-spelled words (Hirst and Budanitsky, 2005; Islam and Inkpen, 2009). Context features (Rozovskaya and Roth, 2010) of words provide useful evidences for spelling correction. These features are usually represented by an n-gram language model (Cucerzan and Brill, 2004; Wilcox-O’Hearn et al., 2010). Phonetic features (Toutanova and Moore, 2002; Atkinson, 2008) are proved to be useful in English spelling correction. A spelling correction system is trained using these features by a noisy channel model (Kernighan et al., 1990; Ristad et al., 1998; Brill and Moore, 2000). Chang (1994) first proposes a representative approach for Chinese spelling correction, which relies on sets of confusing characters. Zhang et"
P11-2085,P02-1019,0,0.100427,"ors. Section 5 concludes the whole paper and discusses future work. 2 Previous Work For English spelling correction (Kukich, 1992; Ahmad and Kondrak, 2005; Chen et al., 2007; Whitelaw et al., 2009; Gao et al., 2010), most approaches make use of a lexicon which contains a list of well-spelled words (Hirst and Budanitsky, 2005; Islam and Inkpen, 2009). Context features (Rozovskaya and Roth, 2010) of words provide useful evidences for spelling correction. These features are usually represented by an n-gram language model (Cucerzan and Brill, 2004; Wilcox-O’Hearn et al., 2010). Phonetic features (Toutanova and Moore, 2002; Atkinson, 2008) are proved to be useful in English spelling correction. A spelling correction system is trained using these features by a noisy channel model (Kernighan et al., 1990; Ristad et al., 1998; Brill and Moore, 2000). Chang (1994) first proposes a representative approach for Chinese spelling correction, which relies on sets of confusing characters. Zhang et al. (2000) propose an approximate word-matching algorithm for Chinese to solve Chinese spell detection and correction task. Zhang et al. (1999) present a winnow-based approach for Chinese spelling correction which takes both loc"
P11-2085,D09-1093,0,0.240351,"ome language-specific properties of Chinese lead to a part of input errors. To the best of our knowledge, this paper is the first one which analyzes user input behaviors in Chinese Pinyin input method. The rest of this paper is organized as follows. Section 2 discusses related works. Section 3 introduces how we collect errors in Chinese Pinyin input method. In Section 4, we investigate the reasons that result in these errors. Section 5 concludes the whole paper and discusses future work. 2 Previous Work For English spelling correction (Kukich, 1992; Ahmad and Kondrak, 2005; Chen et al., 2007; Whitelaw et al., 2009; Gao et al., 2010), most approaches make use of a lexicon which contains a list of well-spelled words (Hirst and Budanitsky, 2005; Islam and Inkpen, 2009). Context features (Rozovskaya and Roth, 2010) of words provide useful evidences for spelling correction. These features are usually represented by an n-gram language model (Cucerzan and Brill, 2004; Wilcox-O’Hearn et al., 2010). Phonetic features (Toutanova and Moore, 2002; Atkinson, 2008) are proved to be useful in English spelling correction. A spelling correction system is trained using these features by a noisy channel model (Kernighan"
P11-2085,P00-1032,0,0.0939944,"Missing"
P11-2085,P10-3009,1,0.773866,"study user input behaviors in Chinese Pinyin input method from backspace operations. We aim at analyzing the reasons that cause these errors. Users signal that they are very likely to make errors if they press backspace on the keyboard. Then they modify the errors and type in the correct words they want. Different from the previous research, we extract abundant Pinyin-correction and Chinese word-correction pairs from backspace operations. Compared with English typos, we observe some language-specific properties in Chinese have impact on errors. All in all, user behaviors (Zheng et al., 2009; Zheng et al., 2010; Zheng et al., 2011b) in Chinese Pinyin input method provide novel perspectives for natural language processing tasks. Below we sketch three possible directions for the future work: (1) we should consider position features in analyzing Pinyin errors. For example, it is less likely that users make errors in the first letter of an input Pinyin. (2) we aim at designing a selfadaptive input method that provide error-tolerant features (Chen and Lee, 2000; Zheng et al., 2011a). (3) we want to build a Chinese spelling correction system based on extracted error-correction pairs. Acknowledgments This"
P11-2085,P00-1037,0,\N,Missing
P11-2085,H05-1120,0,\N,Missing
P14-1079,D12-1042,0,\N,Missing
P14-1079,N13-1008,0,\N,Missing
P14-1079,P05-1053,0,\N,Missing
P14-1079,P09-1113,0,\N,Missing
P14-1079,P13-2141,0,\N,Missing
P14-1079,P11-1055,0,\N,Missing
P14-1079,P12-1076,0,\N,Missing
P14-1079,P13-2117,0,\N,Missing
P14-1079,N13-1095,0,\N,Missing
P15-2093,P14-2037,0,0.339882,"Missing"
P15-2093,2005.mtsummit-papers.11,0,0.131041,"Missing"
P15-2093,D13-1167,0,0.0174992,"Missing"
P15-2093,E14-1049,0,0.229795,"zy, liuyang2011, sms}@tsinghua.edu.cn Abstract transfer models across languages. This is especially important for those low-resource languages, where it allows one to develop accurate word representations of one language by exploiting the abundant textual resources in another language, e.g., English, which has a high resource density. The joint-space models are not only technically plausible, but also useful for cross-lingual model transfer. Further, studies have shown that using cross-lingual correlation can improve the quality of word representations trained solely with monolingual corpora (Faruqui and Dyer, 2014). Defining a cross-lingual learning objective is crucial at the core of the joint-space model. Hermann and Blunsom (2014) and Chandar A P et al. (2014) tried to calculate parallel sentence (or document) representations and to minimize the differences between the semantically equivalent pairs. These methods are useful in capturing semantic information carried by high-level units (such as phrases and beyond) and usually do not rely on word alignments. However, they suffer from reduced accuracy for representing rare tokens, whose semantic information may not be well generalized. In these cases, f"
P15-2093,D14-1162,0,0.103768,"for cross-lingual word embeddings are very desirable, as language-invariant semantic features can be generalized to make it easy to 567 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 567–572, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 2 Ltotal = li l1 l1 ?? ? ≈ ?? ⋅ ??? ? bilingual relations and constraints li li P · j,k Xjk Xjk P P li li , j Xjk · k Xjk where X li is the matrix of word-context co-occurrence counts. As Pennington et al. (2014), we add separate terms blwi j , blcik for each word and context to absorb the effect of any possible word-specific biases. We also add an additional matrix bias bli for the ease of sharing embeddings among matrices. The loss function is written as the sum of the weighted square error, i Llmono = X  2 li li f (Xjk ) wjli · clki + blwi j + blcik + bli − Mjk , j,k (2) where we choose the same weighting function as the GloVe model to place less confidence on those word-context pairs with rare occurrences, ( f (x) = (x/xmax )α 1 if x &lt; xmax . otherwise (3) li Notice that we only have to optimize"
P15-2093,D11-1014,0,0.0441177,"e cross-lingual constraints can be derived from parallel corpora, with or without word alignments. Empirical results on a task of cross-lingual document classification show that our method is effective to encode cross-lingual knowledge as constraints for cross-lingual word embeddings. 1 Introduction Word embeddings allow one to represent words in a continuous vector space, which characterizes the lexico-semanic relations among words. In many NLP tasks, they prove to be high-quality features, successful applications of which include language modelling (Bengio et al., 2003), sentiment analysis (Socher et al., 2011) and word sense discrimination (Huang et al., 2012). Like words having synonyms in the same language, there are also word pairs across languages which share resembling semantic properties. Mikolov et al. (2013a) observed a strong similarity of the geometric arrangements of corresponding concepts between the vector spaces of different languages, and suggested that a crosslingual mapping between the two vector spaces is technically plausible. In the meantime, the jointspace models for cross-lingual word embeddings are very desirable, as language-invariant semantic features can be generalized to"
P15-2093,P14-1006,0,0.033965,"e low-resource languages, where it allows one to develop accurate word representations of one language by exploiting the abundant textual resources in another language, e.g., English, which has a high resource density. The joint-space models are not only technically plausible, but also useful for cross-lingual model transfer. Further, studies have shown that using cross-lingual correlation can improve the quality of word representations trained solely with monolingual corpora (Faruqui and Dyer, 2014). Defining a cross-lingual learning objective is crucial at the core of the joint-space model. Hermann and Blunsom (2014) and Chandar A P et al. (2014) tried to calculate parallel sentence (or document) representations and to minimize the differences between the semantically equivalent pairs. These methods are useful in capturing semantic information carried by high-level units (such as phrases and beyond) and usually do not rely on word alignments. However, they suffer from reduced accuracy for representing rare tokens, whose semantic information may not be well generalized. In these cases, finer-grained information at lexical level, such as aligned word pairs, dictionaries, and word translation probabilities,"
P15-2093,P12-1092,0,0.0672644,"llel corpora, with or without word alignments. Empirical results on a task of cross-lingual document classification show that our method is effective to encode cross-lingual knowledge as constraints for cross-lingual word embeddings. 1 Introduction Word embeddings allow one to represent words in a continuous vector space, which characterizes the lexico-semanic relations among words. In many NLP tasks, they prove to be high-quality features, successful applications of which include language modelling (Bengio et al., 2003), sentiment analysis (Socher et al., 2011) and word sense discrimination (Huang et al., 2012). Like words having synonyms in the same language, there are also word pairs across languages which share resembling semantic properties. Mikolov et al. (2013a) observed a strong similarity of the geometric arrangements of corresponding concepts between the vector spaces of different languages, and suggested that a crosslingual mapping between the two vector spaces is technically plausible. In the meantime, the jointspace models for cross-lingual word embeddings are very desirable, as language-invariant semantic features can be generalized to make it easy to 567 Proceedings of the 53rd Annual"
P15-2093,D13-1141,0,0.17614,"ly on word alignments. However, they suffer from reduced accuracy for representing rare tokens, whose semantic information may not be well generalized. In these cases, finer-grained information at lexical level, such as aligned word pairs, dictionaries, and word translation probabilities, is considered to be helpful. Koˇcisk`y et al. (2014) integrated word aligning process and word embedding in machine translation models. This method makes full use of parallel corpora and produces high-quality word alignments. However, it is unable to exploit the richer monolingual corpora. On the other hand, Zou et al. (2013) and Faruqui and Dyer (2014) learnt word embeddings of different languages in separate spaces with monolingual corpora and projected the embeddings into a joint space, but they can only capture linear transformation. In this paper, we address the above challenges with a framework of matrix co-factorization. We A joint-space model for cross-lingual distributed representations generalizes language-invariant semantic features. In this paper, we present a matrix cofactorization framework for learning cross-lingual word embeddings. We explicitly define monolingual training objectives in the form of"
P15-2093,C12-1089,0,0.655529,"tance, distance(wjl1 , wkl2 ) = ||wjl1 − wkl2 ||2 . Notice that similar to the monolingual objective, we may optimize for only those sim(j, k) 6= 0, which is efficient as the matrix of translation probabilities or dictionary is sparse. We call this method CLSim. 5 Experiments To evaluate the quality of the relatedness between words in different languages, we induce the task of cross-lingual document classification for the English-German language pair, where a classifier is trained in one language and later used to classify documents in another. We exactly replicated the experiment settings of Klementiev et al. (2012). Cross-lingual Similarities 5.1 An alternative way to set cross-lingual constraints is to minimize the distances between similar word pairs. Here the semantic similarities can be measured by equivalence in translation, sim(j, k), which is produced by a machine translation system. In this paper, we use the translation probabilities produced by a machine translation system. Minimizing the distances of related words in the two languages weighted by their similarities gives us the cross-lingual objective Data and Training For optimizing the monolingual objectives, We used exactly the same subset"
P15-2093,C14-1048,0,\N,Missing
P15-2093,D10-1025,0,\N,Missing
P15-2093,P14-2095,0,\N,Missing
P15-2093,D13-1168,0,\N,Missing
P16-1123,H05-1091,0,0.81175,"c and syntactic features not only for the meanings of the target entities at the lexical level, but also for their immediate context and for the overall sentence structure. Thus, it is not surprising that numerous featureand kernel-based approaches have been proposed, many of which rely on a full-fledged NLP stack, including POS tagging, morphological analysis, dependency parsing, and occasionally semantic analysis, as well as on knowledge resources to capture lexical and semantic features (Kambhatla, 2004; Zhou et al., 2005; Suchanek et al., 2006; Qian et al., 2008; Mooney and Bunescu, 2005; Bunescu and Mooney, 2005). In recent years, we have seen a move towards deep architectures that are capable of learning relevant representations and features without extensive manual feature engineering or use of external resources. A number of convolutional neural network (CNN), recurrent neural network (RNN), and other neural architectures have been proposed for relation classification (Zeng et al., 2014; dos Santos et al., 2015; Xu et al., 2015b). Still, these models often fail to identify critical cues, and many of them still require an external dependency parser. We propose a novel CNN architecture that addresses"
P16-1123,D13-1137,0,0.0230714,"), who proposed the Ranking CNN (CR-CNN) model with a class embedding matrix, Miwa and Bansal (2016) similarly observed that LSTM-based RNNs are outperformed by models using CNNs, due to limited linguistic structure captured in the network architecture. Some more elaborate variants have been proposed to address this, including bidirectional LSTMs (Zhang et al., 2015), deep recurrent neural networks (Xu et al., 2016), and bidirectional treestructured LSTM-RNNs (Miwa and Bansal, 2016). Several recent works also reintroduce a dependency tree-based design, e.g., RNNs operating on syntactic trees (Hashimoto et al., 2013), shortest dependency path-based CNNs (Xu et al., 2015a), and the SDP-LSTM model (Xu et al., 2015b). Finally, Nguyen and Grishman (2015) train both CNNs and RNNs and variously aggregate their outputs using voting, stacking, or log-linear modeling (Nguyen and Grishman, 2015). Although these recent models achieve solid results, ideally, we would want a simple yet effective architecture that does not require dependency parsing or training multiple models. Our experiments in Section 4 demonstrate that we can indeed achieve this, while also obtaining substantial improvements in terms of the obtaine"
P16-1123,S10-1006,0,0.385997,"Missing"
P16-1123,P15-2047,0,0.371751,"Missing"
P16-1123,C10-1018,0,0.27468,"Missing"
P16-1123,P15-1003,0,0.0118049,"gonal Input att. matrix Input att. matrix (S,drinks) (S,diabetes) Input Attention Mechanism While position-based encodings are useful, we conjecture that they do not suffice to fully capture the relationships of specific words with the target entities and the influence that they may bear on the target relations of interest. We design our model so as to automatically identify the parts of the input sentence that are relevant for relation classification. Attention mechanisms have successfully been applied to sequence-to-sequence learning tasks such as machine translation (Bahdanau et al., 2015; Meng et al., 2015) and abstractive sentence summarization (Rush et al., 2015), as well as to tasks such as modeling sentence pairs (Yin et al., 2015) and question answering (Santos et al., 2016). To date, these mechanisms have generally been used to allow for an alignment of the input and output sequence, e.g. the source and target sentence in machine translation, or for an alignment between two input sentences as in sentence similarity scoring and question answering. Figure 2: Input and Primary Attention In our work, we apply the idea of modeling attention to a rather different kind of scenario involving heter"
P16-1123,I05-2045,0,0.0703878,"Missing"
P16-1123,N13-1090,0,0.0916889,"label embedding W L and the ground truth label y and δθ (S, yˆ− ) refers to the distance between wO and a selected incorrect relation label yˆ− . The latter is chosen as the one with the highest score among all incorrect classes (Weston et al., 2011; dos Santos et al., 2015), i.e. yˆ− = argmax δ(S, y 0 ). y 0 ∈Y,y 0 6=y (3) This margin-based objective has the advantage of a strong interpretability and effectiveness compared with empirical loss functions such as the ranking loss function in the CR-CNN approach by dos Santos et al. (2015). Based on a distance function motived by word analogies (Mikolov et al., 2013b), we minimize the gap between predicted outputs and ground-truth labels, while maximizing the distance with the selected incorrect class. By minimizing this pairwise loss function iteratively (see Section 3.5), δθ (S, y) are encouraged to decrease, while δθ (S, yˆ− ) increase. 3.2 Input Representation Given a sentence S = (w1 , w2 , ..., wn ) with marked entity mentions e1 (=wp ) and e2 (=wt ), (p, t ∈ [1, n], p 6= t), we first transform every word into a real-valued vector to provide lexicalsemantic features. Given a word embedding matrix WV of dimensionality dw × |V |, where V is the input"
P16-1123,P15-1061,0,0.543388,"sis, as well as on knowledge resources to capture lexical and semantic features (Kambhatla, 2004; Zhou et al., 2005; Suchanek et al., 2006; Qian et al., 2008; Mooney and Bunescu, 2005; Bunescu and Mooney, 2005). In recent years, we have seen a move towards deep architectures that are capable of learning relevant representations and features without extensive manual feature engineering or use of external resources. A number of convolutional neural network (CNN), recurrent neural network (RNN), and other neural architectures have been proposed for relation classification (Zeng et al., 2014; dos Santos et al., 2015; Xu et al., 2015b). Still, these models often fail to identify critical cues, and many of them still require an external dependency parser. We propose a novel CNN architecture that addresses some of the shortcomings of previous approaches. Our key contributions are as follows: 1. Our CNN architecture relies on a novel multi1298 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1298–1307, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics level attention mechanism to capture both entity-specific attention (primary a"
P16-1123,P04-1053,0,0.043296,"Missing"
P16-1123,P16-1105,0,0.38535,"ure the compositional aspects of the sentence semantics by exploiting syntactic trees. Zeng et al. (2014) proposed a deep convolutional neural network with softmax classification, extracting lexical and sentence level features. However, these approaches still depend on additional features from lexical resources and NLP toolkits. Yu et al. (2014) proposed the Factor-based Compositional Embedding Model, which uses syntactic dependency trees together with sentence-level embeddings. In addition to dos Santos et al. (2015), who proposed the Ranking CNN (CR-CNN) model with a class embedding matrix, Miwa and Bansal (2016) similarly observed that LSTM-based RNNs are outperformed by models using CNNs, due to limited linguistic structure captured in the network architecture. Some more elaborate variants have been proposed to address this, including bidirectional LSTMs (Zhang et al., 2015), deep recurrent neural networks (Xu et al., 2016), and bidirectional treestructured LSTM-RNNs (Miwa and Bansal, 2016). Several recent works also reintroduce a dependency tree-based design, e.g., RNNs operating on syntactic trees (Hashimoto et al., 2013), shortest dependency path-based CNNs (Xu et al., 2015a), and the SDP-LSTM mo"
P16-1123,P14-2012,0,0.189735,"Missing"
P16-1123,C08-1088,0,0.388895,"eeds to be able to account for useful semantic and syntactic features not only for the meanings of the target entities at the lexical level, but also for their immediate context and for the overall sentence structure. Thus, it is not surprising that numerous featureand kernel-based approaches have been proposed, many of which rely on a full-fledged NLP stack, including POS tagging, morphological analysis, dependency parsing, and occasionally semantic analysis, as well as on knowledge resources to capture lexical and semantic features (Kambhatla, 2004; Zhou et al., 2005; Suchanek et al., 2006; Qian et al., 2008; Mooney and Bunescu, 2005; Bunescu and Mooney, 2005). In recent years, we have seen a move towards deep architectures that are capable of learning relevant representations and features without extensive manual feature engineering or use of external resources. A number of convolutional neural network (CNN), recurrent neural network (RNN), and other neural architectures have been proposed for relation classification (Zeng et al., 2014; dos Santos et al., 2015; Xu et al., 2015b). Still, these models often fail to identify critical cues, and many of them still require an external dependency parse"
P16-1123,D09-1149,0,0.0188072,"tated target entity mentions e1 = “drinks” and e2 = “diabetes”, the goal would be to automatically recognize that this sentence expresses a causeeffect relationship between e1 and e2 , for which we use the notation Cause-Effect(e1 ,e2 ). Accurate relation classification facilitates precise sentence interpretations, discourse processing, and higherlevel NLP tasks (Hendrickx et al., 2010). Thus, ∗ † Equal contribution. Corresponding author. Email: liuzy@tsinghua.edu.cn relation classification has attracted considerable attention from researchers over the course of the past decades (Zhang, 2004; Qian et al., 2009; Rink and Harabagiu, 2010). In the example given above, the verb corresponds quite closely to the desired target relation. However, in the wild, we encounter a multitude of different ways of expressing the same kind of relationship. This challenging variability can be lexical, syntactic, or even pragmatic in nature. An effective solution needs to be able to account for useful semantic and syntactic features not only for the meanings of the target entities at the lexical level, but also for their immediate context and for the overall sentence structure. Thus, it is not surprising that numerous"
P16-1123,S10-1057,0,0.78958,"mentions e1 = “drinks” and e2 = “diabetes”, the goal would be to automatically recognize that this sentence expresses a causeeffect relationship between e1 and e2 , for which we use the notation Cause-Effect(e1 ,e2 ). Accurate relation classification facilitates precise sentence interpretations, discourse processing, and higherlevel NLP tasks (Hendrickx et al., 2010). Thus, ∗ † Equal contribution. Corresponding author. Email: liuzy@tsinghua.edu.cn relation classification has attracted considerable attention from researchers over the course of the past decades (Zhang, 2004; Qian et al., 2009; Rink and Harabagiu, 2010). In the example given above, the verb corresponds quite closely to the desired target relation. However, in the wild, we encounter a multitude of different ways of expressing the same kind of relationship. This challenging variability can be lexical, syntactic, or even pragmatic in nature. An effective solution needs to be able to account for useful semantic and syntactic features not only for the meanings of the target entities at the lexical level, but also for their immediate context and for the overall sentence structure. Thus, it is not surprising that numerous featureand kernel-based ap"
P16-1123,D15-1044,0,0.0430919,"betes) Input Attention Mechanism While position-based encodings are useful, we conjecture that they do not suffice to fully capture the relationships of specific words with the target entities and the influence that they may bear on the target relations of interest. We design our model so as to automatically identify the parts of the input sentence that are relevant for relation classification. Attention mechanisms have successfully been applied to sequence-to-sequence learning tasks such as machine translation (Bahdanau et al., 2015; Meng et al., 2015) and abstractive sentence summarization (Rush et al., 2015), as well as to tasks such as modeling sentence pairs (Yin et al., 2015) and question answering (Santos et al., 2016). To date, these mechanisms have generally been used to allow for an alignment of the input and output sequence, e.g. the source and target sentence in machine translation, or for an alignment between two input sentences as in sentence similarity scoring and question answering. Figure 2: Input and Primary Attention In our work, we apply the idea of modeling attention to a rather different kind of scenario involving heterogeneous objects, namely a sentence and two entities. With"
P16-1123,D12-1110,0,0.0321798,"are often derived from other pre-trained NLP tools or lexical and semantic resources. Although such approaches can benefit from the external NLP tools to discover the discrete structure of a sentence, syntactic parsing is error-prone and relying on its success may also impede performance (Bach and Badaskar, 2007). Further downsides include their limited lexical generalization abilities for unseen words and their lack of robustness when applied to new domains, genres, or languages. In recent years, deep neural networks have shown promising results. The Recursive MatrixVector Model (MV-RNN) by Socher et al. (2012) sought to capture the compositional aspects of the sentence semantics by exploiting syntactic trees. Zeng et al. (2014) proposed a deep convolutional neural network with softmax classification, extracting lexical and sentence level features. However, these approaches still depend on additional features from lexical resources and NLP toolkits. Yu et al. (2014) proposed the Factor-based Compositional Embedding Model, which uses syntactic dependency trees together with sentence-level embeddings. In addition to dos Santos et al. (2015), who proposed the Ranking CNN (CR-CNN) model with a class emb"
P16-1123,D15-1206,0,0.849633,"owledge resources to capture lexical and semantic features (Kambhatla, 2004; Zhou et al., 2005; Suchanek et al., 2006; Qian et al., 2008; Mooney and Bunescu, 2005; Bunescu and Mooney, 2005). In recent years, we have seen a move towards deep architectures that are capable of learning relevant representations and features without extensive manual feature engineering or use of external resources. A number of convolutional neural network (CNN), recurrent neural network (RNN), and other neural architectures have been proposed for relation classification (Zeng et al., 2014; dos Santos et al., 2015; Xu et al., 2015b). Still, these models often fail to identify critical cues, and many of them still require an external dependency parser. We propose a novel CNN architecture that addresses some of the shortcomings of previous approaches. Our key contributions are as follows: 1. Our CNN architecture relies on a novel multi1298 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1298–1307, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics level attention mechanism to capture both entity-specific attention (primary attention at the i"
P16-1123,C16-1138,0,0.363235,"kits. Yu et al. (2014) proposed the Factor-based Compositional Embedding Model, which uses syntactic dependency trees together with sentence-level embeddings. In addition to dos Santos et al. (2015), who proposed the Ranking CNN (CR-CNN) model with a class embedding matrix, Miwa and Bansal (2016) similarly observed that LSTM-based RNNs are outperformed by models using CNNs, due to limited linguistic structure captured in the network architecture. Some more elaborate variants have been proposed to address this, including bidirectional LSTMs (Zhang et al., 2015), deep recurrent neural networks (Xu et al., 2016), and bidirectional treestructured LSTM-RNNs (Miwa and Bansal, 2016). Several recent works also reintroduce a dependency tree-based design, e.g., RNNs operating on syntactic trees (Hashimoto et al., 2013), shortest dependency path-based CNNs (Xu et al., 2015a), and the SDP-LSTM model (Xu et al., 2015b). Finally, Nguyen and Grishman (2015) train both CNNs and RNNs and variously aggregate their outputs using voting, stacking, or log-linear modeling (Nguyen and Grishman, 2015). Although these recent models achieve solid results, ideally, we would want a simple yet effective architecture that does"
P16-1123,C14-1220,0,0.761797,"sionally semantic analysis, as well as on knowledge resources to capture lexical and semantic features (Kambhatla, 2004; Zhou et al., 2005; Suchanek et al., 2006; Qian et al., 2008; Mooney and Bunescu, 2005; Bunescu and Mooney, 2005). In recent years, we have seen a move towards deep architectures that are capable of learning relevant representations and features without extensive manual feature engineering or use of external resources. A number of convolutional neural network (CNN), recurrent neural network (RNN), and other neural architectures have been proposed for relation classification (Zeng et al., 2014; dos Santos et al., 2015; Xu et al., 2015b). Still, these models often fail to identify critical cues, and many of them still require an external dependency parser. We propose a novel CNN architecture that addresses some of the shortcomings of previous approaches. Our key contributions are as follows: 1. Our CNN architecture relies on a novel multi1298 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1298–1307, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics level attention mechanism to capture both entity-spec"
P16-1123,Y15-1009,0,0.185236,"ditional features from lexical resources and NLP toolkits. Yu et al. (2014) proposed the Factor-based Compositional Embedding Model, which uses syntactic dependency trees together with sentence-level embeddings. In addition to dos Santos et al. (2015), who proposed the Ranking CNN (CR-CNN) model with a class embedding matrix, Miwa and Bansal (2016) similarly observed that LSTM-based RNNs are outperformed by models using CNNs, due to limited linguistic structure captured in the network architecture. Some more elaborate variants have been proposed to address this, including bidirectional LSTMs (Zhang et al., 2015), deep recurrent neural networks (Xu et al., 2016), and bidirectional treestructured LSTM-RNNs (Miwa and Bansal, 2016). Several recent works also reintroduce a dependency tree-based design, e.g., RNNs operating on syntactic trees (Hashimoto et al., 2013), shortest dependency path-based CNNs (Xu et al., 2015a), and the SDP-LSTM model (Xu et al., 2015b). Finally, Nguyen and Grishman (2015) train both CNNs and RNNs and variously aggregate their outputs using voting, stacking, or log-linear modeling (Nguyen and Grishman, 2015). Although these recent models achieve solid results, ideally, we would"
P16-1123,P05-1053,0,0.614057,"agmatic in nature. An effective solution needs to be able to account for useful semantic and syntactic features not only for the meanings of the target entities at the lexical level, but also for their immediate context and for the overall sentence structure. Thus, it is not surprising that numerous featureand kernel-based approaches have been proposed, many of which rely on a full-fledged NLP stack, including POS tagging, morphological analysis, dependency parsing, and occasionally semantic analysis, as well as on knowledge resources to capture lexical and semantic features (Kambhatla, 2004; Zhou et al., 2005; Suchanek et al., 2006; Qian et al., 2008; Mooney and Bunescu, 2005; Bunescu and Mooney, 2005). In recent years, we have seen a move towards deep architectures that are capable of learning relevant representations and features without extensive manual feature engineering or use of external resources. A number of convolutional neural network (CNN), recurrent neural network (RNN), and other neural architectures have been proposed for relation classification (Zeng et al., 2014; dos Santos et al., 2015; Xu et al., 2015b). Still, these models often fail to identify critical cues, and many of them"
P16-1123,D15-1062,0,0.788036,"owledge resources to capture lexical and semantic features (Kambhatla, 2004; Zhou et al., 2005; Suchanek et al., 2006; Qian et al., 2008; Mooney and Bunescu, 2005; Bunescu and Mooney, 2005). In recent years, we have seen a move towards deep architectures that are capable of learning relevant representations and features without extensive manual feature engineering or use of external resources. A number of convolutional neural network (CNN), recurrent neural network (RNN), and other neural architectures have been proposed for relation classification (Zeng et al., 2014; dos Santos et al., 2015; Xu et al., 2015b). Still, these models often fail to identify critical cues, and many of them still require an external dependency parser. We propose a novel CNN architecture that addresses some of the shortcomings of previous approaches. Our key contributions are as follows: 1. Our CNN architecture relies on a novel multi1298 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1298–1307, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics level attention mechanism to capture both entity-specific attention (primary attention at the i"
P16-1200,P15-1061,0,0.768279,"o philanthropy was linked to the antitrust problems Microsoft had in the U.S. and the European union.” does not express the relation founder but will still be regarded as an active instance. Hence, (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) adopt multi-instance learning to alleviate the wrong labelling problem. The main weakness of these conventional methods is that most features are explicitly derived from NLP tools such as POS tagging and the errors generated by NLP tools will propagate in these methods. Some recent works (Socher et al., 2012; Zeng et al., 2014; dos Santos et al., 2015) attempt to use deep neural networks in relation classification without handcrafted features. These methods build classifier based on sentence-level annotated data, which cannot be applied in large-scale rized as follows: s α1 α2 α3 • As compared to existing neural relation extraction model, our model can make full use of all informative sentences of each entity pair. αn x1 x2 x3 xn CNN CNN CNN CNN x1 x2 x3 xn Figure 1: The architecture of sentence-level attention-based CNN, where xi and xi indicate the original sentence for an entity pair and its corresponding sentence representation, αi is t"
P16-1200,P05-1045,0,0.0389364,"of our model. And then we evaluate the effects of our selective attention and show its performance on the data with different set size. Finally, we compare the performance of our method to several state-of-the-art feature-based methods. 4.1 Dataset and Evaluation Metrics We evaluate our model on a widely used dataset1 which is developed by (Riedel et al., 2010) and has also been used by (Hoffmann et al., 2011; Surdeanu et al., 2012). This dataset was generated by aligning Freebase relations with the New York Times corpus (NYT). Entity mentions are found using the Stanford named entity tagger (Finkel et al., 2005), and are further matched to the names of Freebase entities. The Freebase relations are divided into two parts, one for training and one for testing. It aligns the the sentences from the corpus of the years 2005-2006 and regards them as training instances. And the testing instances are the aligned sentences from 2007. There are 53 possible relationships including a special relation NA which indicates there is no relation between head and tail entities. The training data contains 522,611 sentences, 281,270 entity pairs and 18,252 relational facts. The testing set contains 172,448 sentences, 96,"
P16-1200,P11-1055,0,0.982056,"or example, (Microsoft, founder, Bill Gates) is a relational fact in KB. Distant supervision will regard all sentences that contain these two entities as active instances for relation founder. Although distant supervision is an effective strategy to automatically label training data, it always suffers from wrong labelling problem. For example, the sentence “Bill Gates ’s turn to philanthropy was linked to the antitrust problems Microsoft had in the U.S. and the European union.” does not express the relation founder but will still be regarded as an active instance. Hence, (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) adopt multi-instance learning to alleviate the wrong labelling problem. The main weakness of these conventional methods is that most features are explicitly derived from NLP tools such as POS tagging and the errors generated by NLP tools will propagate in these methods. Some recent works (Socher et al., 2012; Zeng et al., 2014; dos Santos et al., 2015) attempt to use deep neural networks in relation classification without handcrafted features. These methods build classifier based on sentence-level annotated data, which cannot be applied in large-scale rized as follows:"
P16-1200,P09-1113,0,0.990881,"crosoft, founder, Bill Gates). Although existing KBs contain a ∗ Corresponding uzy@tsinghua.edu.cn). author: Zhiyuan Liu (limassive amount of facts, they are still far from complete compared to the infinite real-world facts. To enrich KBs, many efforts have been invested in automatically finding unknown relational facts. Therefore, relation extraction (RE), the process of generating relational data from plain text, is a crucial task in NLP. Most existing supervised RE systems require a large amount of labelled relation-specific training data, which is very time consuming and labor intensive. (Mintz et al., 2009) proposes distant supervision to automatically generate training data via aligning KBs and texts. They assume that if two entities have a relation in KBs, then all sentences that contain these two entities will express this relation. For example, (Microsoft, founder, Bill Gates) is a relational fact in KB. Distant supervision will regard all sentences that contain these two entities as active instances for relation founder. Although distant supervision is an effective strategy to automatically label training data, it always suffers from wrong labelling problem. For example, the sentence “Bill"
P16-1200,D12-1110,0,0.068894,"example, the sentence “Bill Gates ’s turn to philanthropy was linked to the antitrust problems Microsoft had in the U.S. and the European union.” does not express the relation founder but will still be regarded as an active instance. Hence, (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) adopt multi-instance learning to alleviate the wrong labelling problem. The main weakness of these conventional methods is that most features are explicitly derived from NLP tools such as POS tagging and the errors generated by NLP tools will propagate in these methods. Some recent works (Socher et al., 2012; Zeng et al., 2014; dos Santos et al., 2015) attempt to use deep neural networks in relation classification without handcrafted features. These methods build classifier based on sentence-level annotated data, which cannot be applied in large-scale rized as follows: s α1 α2 α3 • As compared to existing neural relation extraction model, our model can make full use of all informative sentences of each entity pair. αn x1 x2 x3 xn CNN CNN CNN CNN x1 x2 x3 xn Figure 1: The architecture of sentence-level attention-based CNN, where xi and xi indicate the original sentence for an entity pair and its c"
P16-1200,P13-1045,0,0.00975957,"h instance. (Bunescu and Mooney, 2007) connects weak supervision with multi-instance learning and extends it to relation extraction. But all the feature-based methods depend strongly on the quality of the features generated by NLP tools, which will suffer from error propagation problem. Recently, deep learning (Bengio, 2009) has been widely used for various areas, including computer vision, speech recognition and so on. It has also been successfully applied to different NLP tasks such as part-of-speech tagging (Collobert et al., 2011), sentiment analysis (dos Santos and Gatti, 2014), parsing (Socher et al., 2013), and machine translation (Sutskever et al., 2014). Due to the recent success in deep learning, many researchers have investigated the possibility of using neural networks to automatically learn features for relation extraction. (Socher et al., 2012) uses a recursive neural network in relation extraction. They parse the sentences first and then represent each node in the parsing tree as a vector. Moreover, (Zeng et al., 2014; dos Santos et al., 2015) adopt an end-to-end convolutional neural network for relation extraction. Besides, (Xie et al., 2016) attempts to incorporate the text informatio"
P16-1200,P07-1073,0,0.0221986,"nt supervision inevitably accompanies with the wrong labelling problem. To alleviate the wrong labelling problem, (Riedel et al., 2010) models distant supervision for relation extraction as a multiinstance single-label problem, and (Hoffmann et al., 2011; Surdeanu et al., 2012) adopt multiinstance multi-label learning in relation extraction. Multi-instance learning was originally proposed to address the issue of ambiguously-labelled training data when predicting the activity of drugs (Dietterich et al., 1997). Multi-instance learning considers the reliability of the labels for each instance. (Bunescu and Mooney, 2007) connects weak supervision with multi-instance learning and extends it to relation extraction. But all the feature-based methods depend strongly on the quality of the features generated by NLP tools, which will suffer from error propagation problem. Recently, deep learning (Bengio, 2009) has been widely used for various areas, including computer vision, speech recognition and so on. It has also been successfully applied to different NLP tasks such as part-of-speech tagging (Collobert et al., 2011), sentiment analysis (dos Santos and Gatti, 2014), parsing (Socher et al., 2013), and machine tran"
P16-1200,D12-1042,0,0.957488,"founder, Bill Gates) is a relational fact in KB. Distant supervision will regard all sentences that contain these two entities as active instances for relation founder. Although distant supervision is an effective strategy to automatically label training data, it always suffers from wrong labelling problem. For example, the sentence “Bill Gates ’s turn to philanthropy was linked to the antitrust problems Microsoft had in the U.S. and the European union.” does not express the relation founder but will still be regarded as an active instance. Hence, (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) adopt multi-instance learning to alleviate the wrong labelling problem. The main weakness of these conventional methods is that most features are explicitly derived from NLP tools such as POS tagging and the errors generated by NLP tools will propagate in these methods. Some recent works (Socher et al., 2012; Zeng et al., 2014; dos Santos et al., 2015) attempt to use deep neural networks in relation classification without handcrafted features. These methods build classifier based on sentence-level annotated data, which cannot be applied in large-scale rized as follows: s α1 α2 α3 • As compare"
P16-1200,C14-1008,0,0.0864291,"reliability of the labels for each instance. (Bunescu and Mooney, 2007) connects weak supervision with multi-instance learning and extends it to relation extraction. But all the feature-based methods depend strongly on the quality of the features generated by NLP tools, which will suffer from error propagation problem. Recently, deep learning (Bengio, 2009) has been widely used for various areas, including computer vision, speech recognition and so on. It has also been successfully applied to different NLP tasks such as part-of-speech tagging (Collobert et al., 2011), sentiment analysis (dos Santos and Gatti, 2014), parsing (Socher et al., 2013), and machine translation (Sutskever et al., 2014). Due to the recent success in deep learning, many researchers have investigated the possibility of using neural networks to automatically learn features for relation extraction. (Socher et al., 2012) uses a recursive neural network in relation extraction. They parse the sentences first and then represent each node in the parsing tree as a vector. Moreover, (Zeng et al., 2014; dos Santos et al., 2015) adopt an end-to-end convolutional neural network for relation extraction. Besides, (Xie et al., 2016) attempts to"
P16-1200,C14-1220,0,0.886404,"e “Bill Gates ’s turn to philanthropy was linked to the antitrust problems Microsoft had in the U.S. and the European union.” does not express the relation founder but will still be regarded as an active instance. Hence, (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) adopt multi-instance learning to alleviate the wrong labelling problem. The main weakness of these conventional methods is that most features are explicitly derived from NLP tools such as POS tagging and the errors generated by NLP tools will propagate in these methods. Some recent works (Socher et al., 2012; Zeng et al., 2014; dos Santos et al., 2015) attempt to use deep neural networks in relation classification without handcrafted features. These methods build classifier based on sentence-level annotated data, which cannot be applied in large-scale rized as follows: s α1 α2 α3 • As compared to existing neural relation extraction model, our model can make full use of all informative sentences of each entity pair. αn x1 x2 x3 xn CNN CNN CNN CNN x1 x2 x3 xn Figure 1: The architecture of sentence-level attention-based CNN, where xi and xi indicate the original sentence for an entity pair and its corresponding senten"
P16-1200,D15-1203,0,0.676771,"ich cannot be applied in large-scale rized as follows: s α1 α2 α3 • As compared to existing neural relation extraction model, our model can make full use of all informative sentences of each entity pair. αn x1 x2 x3 xn CNN CNN CNN CNN x1 x2 x3 xn Figure 1: The architecture of sentence-level attention-based CNN, where xi and xi indicate the original sentence for an entity pair and its corresponding sentence representation, αi is the weight given by sentence-level attention, and s indicates the representation of the sentence set. KBs due to the lack of human-annotated training data. Therefore, (Zeng et al., 2015) incorporates multi-instance learning with neural network model, which can build relation extractor based on distant supervision data. Although the method achieves significant improvement in relation extraction, it is still far from satisfactory. The method assumes that at least one sentence that mentions these two entities will express their relation, and only selects the most likely sentence for each entity pair in training and prediction. It’s apparent that the method will lose a large amount of rich information containing in neglected sentences. In this paper, we propose a sentence-level a"
P17-1004,C14-1192,1,0.776459,"Missing"
P17-1004,P15-1061,0,0.0328139,"Work Recent years KBs have been widely used on various AI and NLP applications. As an important approach to enrich KBs, relation extraction from plain text has attracted many research interests. Relation extraction typically classiﬁes each entity pair into various relation types according to supporting sentences that the both entities appear, which needs human-labelled relationspeciﬁc training instances. Many works have been invested to relation extraction including kernelbased model (Zelenko et al., 2003), embeddingbased model (Gormley et al., 2015), CNN-based models (Zeng et al., 2014; dos Santos et al., 2015), and RNN-based model (Socher et al., 2012). Nevertheless, these RE systems are insufﬁcient due to the lack of training data. To address this issue, Mintz et al. (2009) align plain text with Freebase to automatically generate training instances following the distant supervision assumption. To further alleviate the wrong labelling problem, Riedel et al. (2010) model distant supervision for relation extraction as a multiinstance single-label learning problem, and Hoffmann et al. (2011); Surdeanu et al. (2012) regard it as a multi-instance multi-label learning problem. Recently, Zeng et al. (2015"
P17-1004,N15-1151,0,0.0855589,"ing English Sentence Representation Chinese 1 x1 n 2 x11 x1 English 1 x2 2 x2 n x22 Chinese Figure 1: Overall architecture of our multi-lingual attention which contains two languages including English and Chinese. The solid lines indicates mono-lingual attention and the dashed lines indicates cross-lingual attention. et al. (2016) further utilize sentence-level attention mechanism to consider all informative sentences jointly. Most existing RE systems are absorbed in extracting relations from mono-lingual data, ignoring massive information lying in texts from multiple languages. In this area, Faruqui and Kumar (2015) present a language independent open domain relation extraction system, and Verga et al. (2015) further employ Universal Schema to combine OpenIE and link-prediction perspective for multi-lingual relation extraction. Both the works focus on multi-lingual transfer learning and learn a predictive model on a new language for existing KBs, by leveraging uniﬁed representation learning for cross-lingual entities. Different from these works, our framework aims to jointly model the texts in multiple languages to enhance relation extraction with distant supervision. To the best of our knowledge, this i"
P17-1004,D15-1205,0,0.0230895,"Missing"
P17-1004,P11-1055,0,0.577715,"ence and Technology, State Key Lab on Intelligent Technology and Systems, National Lab for Information Science and Technology, Tsinghua University, Beijing, China 2 Jiangsu Collaborative Innovation Center for Language Competence, Jiangsu, China Abstract human-intensive, many works have been devoted to automated extraction of novel facts from various Web resources, where relation extraction (RE) from plain texts is one the most important knowledge sources. Among various methods for relation extraction, distant supervision is the most promising approach (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012), which can automatically generate training instances via aligning KBs and texts to address the issue of lacking supervised data. As the development of deep learning, Zeng et al. (2015) introduce neural networks to extract relations with automatically learned features from training instances. To address the wrong labelling issue of distant supervision data, Lin et al. (2016) further employ sentence-level attention mechanism in neural relation extraction, and achieves the state-of-the-art performance. However, most RE systems concentrate on extracting relational facts fr"
P17-1004,P16-1200,1,0.43391,"ain texts is one the most important knowledge sources. Among various methods for relation extraction, distant supervision is the most promising approach (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012), which can automatically generate training instances via aligning KBs and texts to address the issue of lacking supervised data. As the development of deep learning, Zeng et al. (2015) introduce neural networks to extract relations with automatically learned features from training instances. To address the wrong labelling issue of distant supervision data, Lin et al. (2016) further employ sentence-level attention mechanism in neural relation extraction, and achieves the state-of-the-art performance. However, most RE systems concentrate on extracting relational facts from mono-lingual data. In fact, people describe knowledge about the world using various languages. And people speaking different languages also share similar knowledge about the world due to the similarities of human experiences and human cognitive systems. For instance, though New York and United States are expressed as 纽约 and 美国 respectively in Chinese, both Americans and Chinese share the fact th"
P17-1004,P09-1113,0,0.777584,"osong Sun1,2 1 Department of Computer Science and Technology, State Key Lab on Intelligent Technology and Systems, National Lab for Information Science and Technology, Tsinghua University, Beijing, China 2 Jiangsu Collaborative Innovation Center for Language Competence, Jiangsu, China Abstract human-intensive, many works have been devoted to automated extraction of novel facts from various Web resources, where relation extraction (RE) from plain texts is one the most important knowledge sources. Among various methods for relation extraction, distant supervision is the most promising approach (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012), which can automatically generate training instances via aligning KBs and texts to address the issue of lacking supervised data. As the development of deep learning, Zeng et al. (2015) introduce neural networks to extract relations with automatically learned features from training instances. To address the wrong labelling issue of distant supervision data, Lin et al. (2016) further employ sentence-level attention mechanism in neural relation extraction, and achieves the state-of-the-art performance. However, most RE systems c"
P17-1004,D12-1110,0,0.0830015,"d on various AI and NLP applications. As an important approach to enrich KBs, relation extraction from plain text has attracted many research interests. Relation extraction typically classiﬁes each entity pair into various relation types according to supporting sentences that the both entities appear, which needs human-labelled relationspeciﬁc training instances. Many works have been invested to relation extraction including kernelbased model (Zelenko et al., 2003), embeddingbased model (Gormley et al., 2015), CNN-based models (Zeng et al., 2014; dos Santos et al., 2015), and RNN-based model (Socher et al., 2012). Nevertheless, these RE systems are insufﬁcient due to the lack of training data. To address this issue, Mintz et al. (2009) align plain text with Freebase to automatically generate training instances following the distant supervision assumption. To further alleviate the wrong labelling problem, Riedel et al. (2010) model distant supervision for relation extraction as a multiinstance single-label learning problem, and Hoffmann et al. (2011); Surdeanu et al. (2012) regard it as a multi-instance multi-label learning problem. Recently, Zeng et al. (2015) attempt to connect neural networks with d"
P17-1004,D12-1042,0,0.239985,"ate Key Lab on Intelligent Technology and Systems, National Lab for Information Science and Technology, Tsinghua University, Beijing, China 2 Jiangsu Collaborative Innovation Center for Language Competence, Jiangsu, China Abstract human-intensive, many works have been devoted to automated extraction of novel facts from various Web resources, where relation extraction (RE) from plain texts is one the most important knowledge sources. Among various methods for relation extraction, distant supervision is the most promising approach (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012), which can automatically generate training instances via aligning KBs and texts to address the issue of lacking supervised data. As the development of deep learning, Zeng et al. (2015) introduce neural networks to extract relations with automatically learned features from training instances. To address the wrong labelling issue of distant supervision data, Lin et al. (2016) further employ sentence-level attention mechanism in neural relation extraction, and achieves the state-of-the-art performance. However, most RE systems concentrate on extracting relational facts from mono-lingual data. In"
P17-1004,D15-1203,0,0.34937,"guage Competence, Jiangsu, China Abstract human-intensive, many works have been devoted to automated extraction of novel facts from various Web resources, where relation extraction (RE) from plain texts is one the most important knowledge sources. Among various methods for relation extraction, distant supervision is the most promising approach (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012), which can automatically generate training instances via aligning KBs and texts to address the issue of lacking supervised data. As the development of deep learning, Zeng et al. (2015) introduce neural networks to extract relations with automatically learned features from training instances. To address the wrong labelling issue of distant supervision data, Lin et al. (2016) further employ sentence-level attention mechanism in neural relation extraction, and achieves the state-of-the-art performance. However, most RE systems concentrate on extracting relational facts from mono-lingual data. In fact, people describe knowledge about the world using various languages. And people speaking different languages also share similar knowledge about the world due to the similarities of"
P17-1004,C14-1220,0,0.352646,"in various lan2 Related Work Recent years KBs have been widely used on various AI and NLP applications. As an important approach to enrich KBs, relation extraction from plain text has attracted many research interests. Relation extraction typically classiﬁes each entity pair into various relation types according to supporting sentences that the both entities appear, which needs human-labelled relationspeciﬁc training instances. Many works have been invested to relation extraction including kernelbased model (Zelenko et al., 2003), embeddingbased model (Gormley et al., 2015), CNN-based models (Zeng et al., 2014; dos Santos et al., 2015), and RNN-based model (Socher et al., 2012). Nevertheless, these RE systems are insufﬁcient due to the lack of training data. To address this issue, Mintz et al. (2009) align plain text with Freebase to automatically generate training instances following the distant supervision assumption. To further alleviate the wrong labelling problem, Riedel et al. (2010) model distant supervision for relation extraction as a multiinstance single-label learning problem, and Hoffmann et al. (2011); Surdeanu et al. (2012) regard it as a multi-instance multi-label learning problem. R"
P17-1158,P14-1062,0,0.245708,"assigns dynamic embeddings to a vertex according to different neighbors it interacts with, named as context-aware embedding. Take a vertex u and its neighbor vertex v for example. The contextfree embedding of u remains unchanged when interacting with different neighbors. On the contrary, the context-aware embedding of u is dynamic when confronting different neighbors. When u interacting with v, their context embeddings concerning each other are derived from their text information, Su and Sv respectively. For each vertex, we can easily use neural models, such as convolutional neural networks (Blunsom et al., 2014; Johnson and Zhang, 2014; Kim, 2014) and recurrent neural networks (Kiros et al., 2015; Tai et al., 2015), to build context-free text-based embedding. In order to realize context-aware textbased embeddings, we introduce the selective attention scheme and build mutual attention between u and v into these neural models. The mutual attention is expected to guide neural models to emphasize those words that are focused by its neighbor vertices and eventually obtain contextaware embeddings. Both context-free embeddings and contextaware embeddings of each vertex can be efficiently learned together v"
P17-1158,D14-1181,0,0.00592123,"ng to different neighbors it interacts with, named as context-aware embedding. Take a vertex u and its neighbor vertex v for example. The contextfree embedding of u remains unchanged when interacting with different neighbors. On the contrary, the context-aware embedding of u is dynamic when confronting different neighbors. When u interacting with v, their context embeddings concerning each other are derived from their text information, Su and Sv respectively. For each vertex, we can easily use neural models, such as convolutional neural networks (Blunsom et al., 2014; Johnson and Zhang, 2014; Kim, 2014) and recurrent neural networks (Kiros et al., 2015; Tai et al., 2015), to build context-free text-based embedding. In order to realize context-aware textbased embeddings, we introduce the selective attention scheme and build mutual attention between u and v into these neural models. The mutual attention is expected to guide neural models to emphasize those words that are focused by its neighbor vertices and eventually obtain contextaware embeddings. Both context-free embeddings and contextaware embeddings of each vertex can be efficiently learned together via concatenation using existing NE me"
P17-1158,P15-1150,0,0.082723,"aware embedding. Take a vertex u and its neighbor vertex v for example. The contextfree embedding of u remains unchanged when interacting with different neighbors. On the contrary, the context-aware embedding of u is dynamic when confronting different neighbors. When u interacting with v, their context embeddings concerning each other are derived from their text information, Su and Sv respectively. For each vertex, we can easily use neural models, such as convolutional neural networks (Blunsom et al., 2014; Johnson and Zhang, 2014; Kim, 2014) and recurrent neural networks (Kiros et al., 2015; Tai et al., 2015), to build context-free text-based embedding. In order to realize context-aware textbased embeddings, we introduce the selective attention scheme and build mutual attention between u and v into these neural models. The mutual attention is expected to guide neural models to emphasize those words that are focused by its neighbor vertices and eventually obtain contextaware embeddings. Both context-free embeddings and contextaware embeddings of each vertex can be efficiently learned together via concatenation using existing NE methods such as DeepWalk (Perozzi et al., 2014), LINE (Tang et al., 201"
P17-1187,N15-1070,0,0.017728,"external knowledge resources such as knowledge bases or dictionaries to suggest possible senses for a word. (Banerjee and Pedersen, 2002) exploits the rich hierarchy of semantic relations in WordNet (Miller, 1995) for an adapted dictionarybased WSD algorithm. (Bordes et al., 2011) introduces synset information in WordNet to WRL. (Chen et al., 2014) considers synsets in WordNet as different word senses, and jointly conducts word sense disambiguation and word / sense representation learning. (Guo et al., 2014) considers bilingual datasets to learn sense-specific word representations. Moreover, (Jauhar et al., 2015) proposes two approaches to learn sense-specific word representations that are grounded to ontologies. (Pilehvar and Collier, 2016) utilizes personalized PageRank to learn de-conflated semantic representations of words. In this paper, we follow the knowledge-based approach and automatically detect word senses according to the contexts with the favor of sememe information in HowNet. To the best of our knowledge, this is the first attempt to apply attentionbased models to encode sememe information for word representation learning. 3 word “apple”. The word “apple” actually has two main senses sho"
P17-1187,D14-1082,0,0.0252926,"ds using one-hot representations, but it usually struggles with the data sparsity issue and the neglect of semantic relations between words. To address these issues, (Rumelhart et al., 1988) proposes the idea of distributed representation which projects all words into a continuous low-dimensional semantic space, considering each word as a vector. Distributed word representations are powerful and have been widely utilized in many NLP tasks, including neural language models (Bengio et al., 2003; Mikolov et al., 2010), machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), parsing (Chen and Manning, 2014) and text classification (Zhang et al., 2015). Word distributed representations are capable of encoding semantic meanings in vector space, serving as the fundamental and essential inputs of many NLP tasks. There are large amounts of efforts devoted to learning better word representations. As the exponential growth of text corpora, model efficiency becomes an important issue. (Mikolov et al., 2013) proposes two models, CBOW and Skipgram, achieving a good balance between effectiveness and efficiency. These models assume that the meanings of words can be well reflected by their contexts, and lear"
P17-1187,D14-1110,1,0.839273,"Missing"
P17-1187,P12-1092,0,0.0574238,"to incorporate word sememes into word representation learning (WRL) and learn improved word embeddings in a lowdimensional semantic space. WRL is a fundamental and critical step in many NLP tasks such as language modeling (Bengio et al., 2003) and neural machine translation (Sutskever et al., 2014). There have been a lot of researches for learning word representations, among which word2vec (Mikolov et al., 2013) achieves a nice balance between effectiveness and efficiency. In word2vec, each word corresponds to one single embedding, ignoring the polysemy of most words. To address this issue, (Huang et al., 2012) introduces a multiprototype model for WRL, conducting unsupervised word sense induction and embeddings according to context clusters. (Chen et al., 2014) further utilizes the synset information in WordNet to instruct word sense representation learning. From these previous studies, we conclude that word sense disambiguation are critical for WRL, and we believe that the sememe annotation of word senses in HowNet can provide necessary semantic regularization for the both tasks. To explore its feasibility, we propose a novel Sememe-Encoded Word Representation Learning 2049 Proceedings of the 55th"
P17-1187,W04-0834,0,0.0419152,"provides useful semantic regularization for WRL. Moreover, the unified representations incorporated with sememes also provide us more explicit explanations of both word and sense embeddings. 2.2 Word Sense Disambiguation and Representation Learning Word sense disambiguation (WSD) aims to identify word senses or meanings in a certain context computationally. There are mainly two approaches for WSD, namely the supervised methods and the knowledge-based methods. Supervised methods usually take the surrounding words or senses as features and use classifiers like SVM for word sense disambiguation (Lee et al., 2004), which are intensively limited to the time-consuming human annotation of training data. On contrary, knowledge-based methods utilize 2050 large external knowledge resources such as knowledge bases or dictionaries to suggest possible senses for a word. (Banerjee and Pedersen, 2002) exploits the rich hierarchy of semantic relations in WordNet (Miller, 1995) for an adapted dictionarybased WSD algorithm. (Bordes et al., 2011) introduces synset information in WordNet to WRL. (Chen et al., 2014) considers synsets in WordNet as different word senses, and jointly conducts word sense disambiguation an"
P17-1187,O02-2003,0,0.0779094,"of concepts (i.e. word sense). However, sememes are not explicit ∗ † indicates equal contribution Corresponding author: Z. Liu (liuzy@tsinghua.edu.cn) for each word. Hence, people manually annotate word sememes and build linguistic common-sense knowledge bases. HowNet (Dong and Dong, 2003) is one of such knowledge bases, which annotates each concept in Chinese with one or more relevant sememes. Different from WordNet (Miller, 1995), the philosophy of HowNet emphasizes the significance of part and attribute represented by sememes. HowNet has been widely utilized in word similarity computation (Liu and Li, 2002) and sentiment analysis (Xianghua et al., 2013), and in section 3.2 we will give a detailed introduction to sememes, senses and words in HowNet. In this paper, we aim to incorporate word sememes into word representation learning (WRL) and learn improved word embeddings in a lowdimensional semantic space. WRL is a fundamental and critical step in many NLP tasks such as language modeling (Bengio et al., 2003) and neural machine translation (Sutskever et al., 2014). There have been a lot of researches for learning word representations, among which word2vec (Mikolov et al., 2013) achieves a nice b"
P17-1187,D14-1162,0,0.120776,"ng semantic meanings in vector space, serving as the fundamental and essential inputs of many NLP tasks. There are large amounts of efforts devoted to learning better word representations. As the exponential growth of text corpora, model efficiency becomes an important issue. (Mikolov et al., 2013) proposes two models, CBOW and Skipgram, achieving a good balance between effectiveness and efficiency. These models assume that the meanings of words can be well reflected by their contexts, and learn word representations by maximizing the predictive probabilities between words and their contexts. (Pennington et al., 2014) further utilizes matrix factorization on word affinity matrix to learn word representations. However, these models merely arrange only one vector for each word, regardless of the fact that many words have multiple senses. (Huang et al., 2012; Tian et al., 2014) utilize multi-prototype vector models to learn word representations and build distinct vectors for each word sense. (Neelakantan et al., 2015) presents an extension to Skip-gram model for learning non-parametric multiple embeddings per word. (Rothe and Sch¨utze, 2015) also utilizes an Autoencoder to jointly learn word, sense and synset"
P17-1187,D16-1174,0,0.0124902,"dersen, 2002) exploits the rich hierarchy of semantic relations in WordNet (Miller, 1995) for an adapted dictionarybased WSD algorithm. (Bordes et al., 2011) introduces synset information in WordNet to WRL. (Chen et al., 2014) considers synsets in WordNet as different word senses, and jointly conducts word sense disambiguation and word / sense representation learning. (Guo et al., 2014) considers bilingual datasets to learn sense-specific word representations. Moreover, (Jauhar et al., 2015) proposes two approaches to learn sense-specific word representations that are grounded to ontologies. (Pilehvar and Collier, 2016) utilizes personalized PageRank to learn de-conflated semantic representations of words. In this paper, we follow the knowledge-based approach and automatically detect word senses according to the contexts with the favor of sememe information in HowNet. To the best of our knowledge, this is the first attempt to apply attentionbased models to encode sememe information for word representation learning. 3 word “apple”. The word “apple” actually has two main senses shown on the second layer: one is a sort of juicy fruit (apple), and another is a famous computer brand (Apple brand). The third and f"
P17-1187,P15-1173,0,0.0675097,"Missing"
P17-1187,C14-1016,0,0.0195536,"t issue. (Mikolov et al., 2013) proposes two models, CBOW and Skipgram, achieving a good balance between effectiveness and efficiency. These models assume that the meanings of words can be well reflected by their contexts, and learn word representations by maximizing the predictive probabilities between words and their contexts. (Pennington et al., 2014) further utilizes matrix factorization on word affinity matrix to learn word representations. However, these models merely arrange only one vector for each word, regardless of the fact that many words have multiple senses. (Huang et al., 2012; Tian et al., 2014) utilize multi-prototype vector models to learn word representations and build distinct vectors for each word sense. (Neelakantan et al., 2015) presents an extension to Skip-gram model for learning non-parametric multiple embeddings per word. (Rothe and Sch¨utze, 2015) also utilizes an Autoencoder to jointly learn word, sense and synset representations in the same semantic space. This paper, for the first time, jointly learns representations of sememes, senses and words. The sememe annotation in HowNet provides useful semantic regularization for WRL. Moreover, the unified representations incor"
P17-1187,D14-1113,0,\N,Missing
P18-1161,D13-1160,0,0.532208,"Missing"
P18-1161,P17-1171,0,0.576465,"ow them to reason for the question. To some ex∗ Corresponding author: Zhiyuan Liu tent, reading comprehension has shown the ability of recent neural models for reading, processing, and comprehending natural language text. Despite their success, existing reading comprehension systems rely on pre-identified relevant texts, which do not always exist in real-world question answering (QA) scenarios. Hence, reading comprehension technique cannot be directly applied to the task of open domain QA. In recent years, researchers attempt to answer opendomain questions with a large-scale unlabeled corpus. Chen et al. (2017) propose a distantly supervised open-domain question answering (DS-QA) system which uses information retrieval technique to obtain relevant text from Wikipedia, and then applies reading comprehension technique to extract the answer. Although DS-QA proposes an effective strategy to collect relevant texts automatically, it always suffers from the noise issue. For example, for the question “Which country’s capital is Dublin?”, we may encounter that: (1) The retrieved paragraph “Dublin is the largest city of Ireland ...” does not actually answer the question; (2) The second “Dublin” in the retriev"
P18-1161,E17-2114,0,0.0552077,"Missing"
P18-1161,P16-1046,0,0.00746877,"hes, which can aggregate the results extracted from each paragraph by existing DS-QA system to better determine the answer. However, the method relies on the pre-extracted answers of existing DS-QA models and still suffers from the noise issue in distant supervision data because it considers all retrieved paragraphs indiscriminately. Different from these methods, our model employs a paragraph selector to filter out those noisy paragraphs and keep those informative paragraphs, which can make full use of the noisy DS-QA data. Our work is also inspired by the idea of coarseto-fine models in NLP. Cheng and Lapata (2016) and Choi et al. (2017) propose a coarse-to-fine model, which first selects essential sentences and then performs text summarization or reading comprehension on the chosen sentences respectively. Lin et al. (2016) utilize selective attention to aggregate the information of all sentences to extract relational facts. Yang et al. (2016) propose a hierarchical attention network which has two levels of attentions applied at the word and sentence level for document classification. Our model also employs a coarse-to-fine model to handle the noise issue in DS-QA, which first selects informative retrie"
P18-1161,P17-1020,0,0.26701,"effective strategy to collect relevant texts automatically, it always suffers from the noise issue. For example, for the question “Which country’s capital is Dublin?”, we may encounter that: (1) The retrieved paragraph “Dublin is the largest city of Ireland ...” does not actually answer the question; (2) The second “Dublin” in the retrieved paragraph ‘Dublin is the capital of Ireland. Besides, Dublin is one of the famous tourist cities in Ireland and ...” is not the correct token of the answer. These noisy paragraphs and tokens are regarded as valid instances in DS-QA. To address this issue, Choi et al. (2017) separate the answer generation in DS-QA into two modules including selecting a target paragraph in document and extracting the correct answer from the target paragraph by reading comprehension. Further, Wang et al. (2018a) use reinforcement learning to train target paragraph selection and answer extraction jointly. These methods only extract the answer according to the most related paragraph, which will lose a large amount of rich information contained in 1736 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1736–1745 c Melbourne, Au"
P18-1161,P17-1055,0,0.171124,"noisy paragraphs and a paragraph reader to extract the correct answer from those denoised paragraphs. Experimental results on real-world datasets show that our model can capture useful information from noisy data and achieve significant improvements on DS-QA as compared to all baselines. The source code and data of this paper can be obtained from https: //github.com/thunlp/OpenQA 1 Introduction Reading comprehension, which aims to answer questions about a document, has recently become a major focus of NLP research. Many reading comprehension systems (Chen et al., 2016; Dhingra et al., 2017a; Cui et al., 2017; Shen et al., 2017; Wang et al., 2017) have been proposed and achieved promising results since their multilayer architectures and attention mechanisms allow them to reason for the question. To some ex∗ Corresponding author: Zhiyuan Liu tent, reading comprehension has shown the ability of recent neural models for reading, processing, and comprehending natural language text. Despite their success, existing reading comprehension systems rely on pre-identified relevant texts, which do not always exist in real-world question answering (QA) scenarios. Hence, reading comprehension technique cannot b"
P18-1161,P17-1168,0,0.544434,"tor to filter out those noisy paragraphs and a paragraph reader to extract the correct answer from those denoised paragraphs. Experimental results on real-world datasets show that our model can capture useful information from noisy data and achieve significant improvements on DS-QA as compared to all baselines. The source code and data of this paper can be obtained from https: //github.com/thunlp/OpenQA 1 Introduction Reading comprehension, which aims to answer questions about a document, has recently become a major focus of NLP research. Many reading comprehension systems (Chen et al., 2016; Dhingra et al., 2017a; Cui et al., 2017; Shen et al., 2017; Wang et al., 2017) have been proposed and achieved promising results since their multilayer architectures and attention mechanisms allow them to reason for the question. To some ex∗ Corresponding author: Zhiyuan Liu tent, reading comprehension has shown the ability of recent neural models for reading, processing, and comprehending natural language text. Despite their success, existing reading comprehension systems rely on pre-identified relevant texts, which do not always exist in real-world question answering (QA) scenarios. Hence, reading comprehension"
P18-1161,N16-1174,0,0.00903545,"erent from these methods, our model employs a paragraph selector to filter out those noisy paragraphs and keep those informative paragraphs, which can make full use of the noisy DS-QA data. Our work is also inspired by the idea of coarseto-fine models in NLP. Cheng and Lapata (2016) and Choi et al. (2017) propose a coarse-to-fine model, which first selects essential sentences and then performs text summarization or reading comprehension on the chosen sentences respectively. Lin et al. (2016) utilize selective attention to aggregate the information of all sentences to extract relational facts. Yang et al. (2016) propose a hierarchical attention network which has two levels of attentions applied at the word and sentence level for document classification. Our model also employs a coarse-to-fine model to handle the noise issue in DS-QA, which first selects informative retrieved paragraphs and then extracts answers from those selected paragraphs. 2. Paragraph Reader. Given the question q and a paragraph pi , the paragraph reader calculates the probability Pr(a|q, pi ) of extracting answer a through a multi-layer long short-term memory network. Overall, the probability Pr(a|q, P ) of extracting answer a g"
P18-1161,P17-1147,0,0.166145,"Missing"
P18-1161,P16-1200,1,0.0376917,"ers from the noise issue in distant supervision data because it considers all retrieved paragraphs indiscriminately. Different from these methods, our model employs a paragraph selector to filter out those noisy paragraphs and keep those informative paragraphs, which can make full use of the noisy DS-QA data. Our work is also inspired by the idea of coarseto-fine models in NLP. Cheng and Lapata (2016) and Choi et al. (2017) propose a coarse-to-fine model, which first selects essential sentences and then performs text summarization or reading comprehension on the chosen sentences respectively. Lin et al. (2016) utilize selective attention to aggregate the information of all sentences to extract relational facts. Yang et al. (2016) propose a hierarchical attention network which has two levels of attentions applied at the word and sentence level for document classification. Our model also employs a coarse-to-fine model to handle the noise issue in DS-QA, which first selects informative retrieved paragraphs and then extracts answers from those selected paragraphs. 2. Paragraph Reader. Given the question q and a paragraph pi , the paragraph reader calculates the probability Pr(a|q, pi ) of extracting an"
P18-1161,D14-1162,0,0.103004,"512}, the number of LSTM layers for document and question encoder among {1, 2, 3, 4}, regularization weight α among {0.1, 0.5, 1.0, 2.0} and the batch size among {4, 8, 16, 32, 64, 128}. The optimal parameters are highlighted with bold faces. For other 1740 parameters, since they have little effect on the results, we simply follow the settings used in (Chen et al., 2017). For training, our Our+FULL model is first initialized by pre-training using Our+AVG model, and we set the iteration number over all the training data as 10. For pre-trained word embeddings, we use the 300-dimensional GloVe6 (Pennington et al., 2014) word embeddings learned from 840B Web crawl data. 4.4 Effect of Different Paragraph Selectors As our model incorporates different types of neural networks including MLP and RNN as our paragraph selector, we investigate the effect of different paragraph selector on the Quasar-T and SearchQA development set. As shown in Table 3, our RNN paragraph selector leads to statistically significant improvements on both Quasar-T and SearchQA. Note that Our+FULL which uses MLP paragraph selector even performs worse on Quasar-T dataset as compared to Our+AVG. It indicates that MLP paragraph selector is ins"
P18-1161,P17-1018,0,0.124425,"der to extract the correct answer from those denoised paragraphs. Experimental results on real-world datasets show that our model can capture useful information from noisy data and achieve significant improvements on DS-QA as compared to all baselines. The source code and data of this paper can be obtained from https: //github.com/thunlp/OpenQA 1 Introduction Reading comprehension, which aims to answer questions about a document, has recently become a major focus of NLP research. Many reading comprehension systems (Chen et al., 2016; Dhingra et al., 2017a; Cui et al., 2017; Shen et al., 2017; Wang et al., 2017) have been proposed and achieved promising results since their multilayer architectures and attention mechanisms allow them to reason for the question. To some ex∗ Corresponding author: Zhiyuan Liu tent, reading comprehension has shown the ability of recent neural models for reading, processing, and comprehending natural language text. Despite their success, existing reading comprehension systems rely on pre-identified relevant texts, which do not always exist in real-world question answering (QA) scenarios. Hence, reading comprehension technique cannot be directly applied to the task of open"
P18-1223,D17-1284,0,0.031617,"query to documents through related entities. Latent Entity Space (LES) builds an unsupervised model using latent entities’ descriptions (Liu and Fang, 2015). EsdRank uses related entities as a latent space, and performs learning to rank with various information retrieval features (Xiong and Callan, 2015). AttR-Duet develops a four-way interaction to involve cross matches between entity and word representations to catch more semantic relevance patterns (Xiong et al., 2017a). 2396 There are many other attempts to integrate knowledge graphs in neural models in related tasks (Miller et al., 2016; Gupta et al., 2017; Ghazvininejad et al., 2018). Our work shares a similar spirit and focuses on exploring the effectiveness of knowledge graph semantics in neuralIR. 3 Entity-Duet Neural Ranking Model ~veemb = Embe (e). This section first describes the standard architecture in current interaction based neural ranking models. Then it presents our Entity-Duet Neural Ranking Model, including the semantic entity representation which integrates the knowledge graph semantics, and then the entity-duet ranking framework. The overall architecture of EDRM is shown in Figure 1. 3.1 Given a query q and a document d, inter"
P18-1223,D17-1110,0,0.0567975,"Missing"
P18-1223,D16-1147,0,0.0160732,"nal connections from query to documents through related entities. Latent Entity Space (LES) builds an unsupervised model using latent entities’ descriptions (Liu and Fang, 2015). EsdRank uses related entities as a latent space, and performs learning to rank with various information retrieval features (Xiong and Callan, 2015). AttR-Duet develops a four-way interaction to involve cross matches between entity and word representations to catch more semantic relevance patterns (Xiong et al., 2017a). 2396 There are many other attempts to integrate knowledge graphs in neural models in related tasks (Miller et al., 2016; Gupta et al., 2017; Ghazvininejad et al., 2018). Our work shares a similar spirit and focuses on exploring the effectiveness of knowledge graph semantics in neuralIR. 3 Entity-Duet Neural Ranking Model ~veemb = Embe (e). This section first describes the standard architecture in current interaction based neural ranking models. Then it presents our Entity-Duet Neural Ranking Model, including the semantic entity representation which integrates the knowledge graph semantics, and then the entity-duet ranking framework. The overall architecture of EDRM is shown in Figure 1. 3.1 Given a query q and"
P18-1223,W03-1730,0,0.204162,"Missing"
P18-1227,Q17-1010,0,0.25932,"formation of words, which is especially useful to address the out-of-vocabulary (OOV) problem. Morphology is a typical research area of subword level NLP. Subword level NLP has also been widely considered in many NLP applications, such as keyword spotting (Narasimhan et al., 2014), parsing (Seeker and C ¸ etino˘glu, 2015), machine translation (Dyer et al., 2010), speech recognition (Creutz et al., 2007), and paradigm completion (Sutskever et al., 2014; Bahdanau et al., 2015; Cotterell et al., 2016a; Kann et al., 2017; Jin and Kann, 2017). Incorporating subword information for word embeddings (Bojanowski et al., 2017; Cotterell et al., 2016b; Chen et al., 2015; Wieting et al., 2016; Yin et al., 2016) facilitates modeling rare words and can improve the performance of several NLP tasks to which the embeddings are applied. Besides, people also consider character embeddings which have been utilized in Chinese word segmentation (Sun et al., 2014). The success of previous work verifies the feasibility of utilizing internal character information of words. We design our framework for lexical sememe prediction inspired by these methods. In this paper, we focus on the relationships between the words and the sememes"
P18-1227,P16-1156,0,0.0575657,"Missing"
P18-1227,P10-4002,0,0.0157426,"s, and fail to handle low-frequency words and outof-vocabulary words. In this paper, we propose to incorporate internal information for lexical sememe prediction. 3.1 Subword and Character Level NLP. Subword and character level NLP models the internal information of words, which is especially useful to address the out-of-vocabulary (OOV) problem. Morphology is a typical research area of subword level NLP. Subword level NLP has also been widely considered in many NLP applications, such as keyword spotting (Narasimhan et al., 2014), parsing (Seeker and C ¸ etino˘glu, 2015), machine translation (Dyer et al., 2010), speech recognition (Creutz et al., 2007), and paradigm completion (Sutskever et al., 2014; Bahdanau et al., 2015; Cotterell et al., 2016a; Kann et al., 2017; Jin and Kann, 2017). Incorporating subword information for word embeddings (Bojanowski et al., 2017; Cotterell et al., 2016b; Chen et al., 2015; Wieting et al., 2016; Yin et al., 2016) facilitates modeling rare words and can improve the performance of several NLP tasks to which the embeddings are applied. Besides, people also consider character embeddings which have been utilized in Chinese word segmentation (Sun et al., 2014). The succ"
P18-1227,N15-1184,0,0.120806,"Missing"
P18-1227,P14-1050,0,0.138855,"finite number of sememes. However, the sememe set of a word is not explicit, which is why linguists build knowledge bases (KBs) to annotate words with sememes manually. HowNet is a classical widely-used sememe KB (Dong and Dong, 2006). In HowNet, linguists manually define approximately 2, 000 sememes, and annotate more than 100, 000 common words in Chinese and English with their relevant sememes in hierarchical structures. HowNet is well developed and has a wide range of applications in many NLP tasks, such as word sense disambiguation (Duan et al., 2007), sentiment analysis (Fu et al., 2013; Huang et al., 2014) and cross-lingual word similarity (Xia et al., 2011). Since new words and phrases are emerging every day and the semantic meanings of existing concepts keep changing, it is time-consuming and work-intensive for human experts to annotate new ∗ Work done while doing internship at Tsinghua University. † Equal contribution. Huiming Jin proposed the overall idea, designed the first experiment, conducted both experiments, and wrote the paper; Hao Zhu made suggestions on ensembling, proposed the second experiment, and spent a lot of time on proofreading the paper and making revisions. All authors he"
P18-1227,W17-4110,1,0.843902,"Character Level NLP. Subword and character level NLP models the internal information of words, which is especially useful to address the out-of-vocabulary (OOV) problem. Morphology is a typical research area of subword level NLP. Subword level NLP has also been widely considered in many NLP applications, such as keyword spotting (Narasimhan et al., 2014), parsing (Seeker and C ¸ etino˘glu, 2015), machine translation (Dyer et al., 2010), speech recognition (Creutz et al., 2007), and paradigm completion (Sutskever et al., 2014; Bahdanau et al., 2015; Cotterell et al., 2016a; Kann et al., 2017; Jin and Kann, 2017). Incorporating subword information for word embeddings (Bojanowski et al., 2017; Cotterell et al., 2016b; Chen et al., 2015; Wieting et al., 2016; Yin et al., 2016) facilitates modeling rare words and can improve the performance of several NLP tasks to which the embeddings are applied. Besides, people also consider character embeddings which have been utilized in Chinese word segmentation (Sun et al., 2014). The success of previous work verifies the feasibility of utilizing internal character information of words. We design our framework for lexical sememe prediction inspired by these methods"
P18-1227,P17-1182,0,0.0363986,"Missing"
P18-1227,D14-1095,0,0.0255867,"13) on large-scale text corpus. These methods do not exploit internal information of words, and fail to handle low-frequency words and outof-vocabulary words. In this paper, we propose to incorporate internal information for lexical sememe prediction. 3.1 Subword and Character Level NLP. Subword and character level NLP models the internal information of words, which is especially useful to address the out-of-vocabulary (OOV) problem. Morphology is a typical research area of subword level NLP. Subword level NLP has also been widely considered in many NLP applications, such as keyword spotting (Narasimhan et al., 2014), parsing (Seeker and C ¸ etino˘glu, 2015), machine translation (Dyer et al., 2010), speech recognition (Creutz et al., 2007), and paradigm completion (Sutskever et al., 2014; Bahdanau et al., 2015; Cotterell et al., 2016a; Kann et al., 2017; Jin and Kann, 2017). Incorporating subword information for word embeddings (Bojanowski et al., 2017; Cotterell et al., 2016b; Chen et al., 2015; Wieting et al., 2016; Yin et al., 2016) facilitates modeling rare words and can improve the performance of several NLP tasks to which the embeddings are applied. Besides, people also consider character embeddings"
P18-1227,P17-1187,1,0.477596,"2017) to build, verify and enrich their contents. WordNet (Miller, 1995) and BabelNet (Navigli and Ponzetto, 2012) are the representative of linguist KBs, where words of similar meanings are grouped to form thesaurus (Nastase and Szpakowicz, 2001). Apart from other linguistic KBs, sememe KBs such as HowNet (Dong and Dong, 2006) can play a significant role in understanding the semantic meanings of concepts in human languages and are favorable for various NLP tasks: information structure annotation (Gan and Wong, 2000), word sense disambiguation (Gan et al., 2002), word representation learning (Niu et al., 2017; Faruqui et al., 2015), and sentiment analysis (Fu et al., 2013) inter alia. Hence, lexical sememe prediction is an important task to construct sememe KBs. Automatic Sememe Prediction. Automatic sememe prediction is proposed by Xie et al. (2017). 2440 For this task, they propose SPWE and SPSE, which are inspired by collaborative filtering (Sarwar et al., 2001) and matrix factorization (Koren et al., 2009) respectively. SPWE recommends the sememes of those words that are close to the unlabelled word in the embedding space. SPSE learns sememe embeddings by matrix factorization (Koren et al., 20"
P18-1227,D14-1162,0,0.0892683,"hunlp/Character-enhanced-Sememe-Prediction 2439 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2439–2449 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics concepts and maintain consistency for large-scale sememe KBs. To address this issue, Xie et al. (2017) propose an automatic sememe prediction framework to assist linguist annotation. They assumed that words which have similar semantic meanings are likely to share similar sememes. Thus, they propose to represent word meanings as embeddings (Pennington et al., 2014; Mikolov et al., 2013) learned from a large-scale text corpus, and they adopt collaborative filtering (Sarwar et al., 2001) and matrix factorization (Koren et al., 2009) for sememe prediction, which are concluded as Sememe Prediction with Word Embeddings (SPWE) and Sememe Prediction with Sememe Embeddings (SPSE) respectively. However, those methods ignore the internal information within words (e.g., the characters in Chinese words), which is also significant for word understanding, especially for words which are of lowfrequency or do not appear in the corpus at all. In this paper, we take Chi"
P18-1227,W00-1213,0,0.812936,"7), automatic completion and alignment (Bordes et al., 2013; Toutanova et al., 2015; Zhu et al., 2017) to build, verify and enrich their contents. WordNet (Miller, 1995) and BabelNet (Navigli and Ponzetto, 2012) are the representative of linguist KBs, where words of similar meanings are grouped to form thesaurus (Nastase and Szpakowicz, 2001). Apart from other linguistic KBs, sememe KBs such as HowNet (Dong and Dong, 2006) can play a significant role in understanding the semantic meanings of concepts in human languages and are favorable for various NLP tasks: information structure annotation (Gan and Wong, 2000), word sense disambiguation (Gan et al., 2002), word representation learning (Niu et al., 2017; Faruqui et al., 2015), and sentiment analysis (Fu et al., 2013) inter alia. Hence, lexical sememe prediction is an important task to construct sememe KBs. Automatic Sememe Prediction. Automatic sememe prediction is proposed by Xie et al. (2017). 2440 For this task, they propose SPWE and SPSE, which are inspired by collaborative filtering (Sarwar et al., 2001) and matrix factorization (Koren et al., 2009) respectively. SPWE recommends the sememes of those words that are close to the unlabelled word i"
P18-1227,Q15-1026,0,0.0648466,"Missing"
P18-1227,D15-1174,0,0.0268145,"sememe prediction framework considering both external and internal information, and show the effectiveness and robustness of our models on a real-world dataset. 2 Related Work Knowledge Bases. Knowledge Bases (KBs), aiming to organize human knowledge in structural forms, are playing an increasingly important role as infrastructural facilities of artificial intelligence and natural language processing. KBs rely on manual efforts (Bollacker et al., 2008), automatic extraction (Auer et al., 2007), manual evaluation (Suchanek et al., 2007), automatic completion and alignment (Bordes et al., 2013; Toutanova et al., 2015; Zhu et al., 2017) to build, verify and enrich their contents. WordNet (Miller, 1995) and BabelNet (Navigli and Ponzetto, 2012) are the representative of linguist KBs, where words of similar meanings are grouped to form thesaurus (Nastase and Szpakowicz, 2001). Apart from other linguistic KBs, sememe KBs such as HowNet (Dong and Dong, 2006) can play a significant role in understanding the semantic meanings of concepts in human languages and are favorable for various NLP tasks: information structure annotation (Gan and Wong, 2000), word sense disambiguation (Gan et al., 2002), word representat"
P18-1227,D16-1157,0,0.0211799,"vocabulary (OOV) problem. Morphology is a typical research area of subword level NLP. Subword level NLP has also been widely considered in many NLP applications, such as keyword spotting (Narasimhan et al., 2014), parsing (Seeker and C ¸ etino˘glu, 2015), machine translation (Dyer et al., 2010), speech recognition (Creutz et al., 2007), and paradigm completion (Sutskever et al., 2014; Bahdanau et al., 2015; Cotterell et al., 2016a; Kann et al., 2017; Jin and Kann, 2017). Incorporating subword information for word embeddings (Bojanowski et al., 2017; Cotterell et al., 2016b; Chen et al., 2015; Wieting et al., 2016; Yin et al., 2016) facilitates modeling rare words and can improve the performance of several NLP tasks to which the embeddings are applied. Besides, people also consider character embeddings which have been utilized in Chinese word segmentation (Sun et al., 2014). The success of previous work verifies the feasibility of utilizing internal character information of words. We design our framework for lexical sememe prediction inspired by these methods. In this paper, we focus on the relationships between the words and the sememes. Following the settings of Xie et al. (2017), we simply ignore th"
P18-1227,D16-1100,0,0.0311036,"em. Morphology is a typical research area of subword level NLP. Subword level NLP has also been widely considered in many NLP applications, such as keyword spotting (Narasimhan et al., 2014), parsing (Seeker and C ¸ etino˘glu, 2015), machine translation (Dyer et al., 2010), speech recognition (Creutz et al., 2007), and paradigm completion (Sutskever et al., 2014; Bahdanau et al., 2015; Cotterell et al., 2016a; Kann et al., 2017; Jin and Kann, 2017). Incorporating subword information for word embeddings (Bojanowski et al., 2017; Cotterell et al., 2016b; Chen et al., 2015; Wieting et al., 2016; Yin et al., 2016) facilitates modeling rare words and can improve the performance of several NLP tasks to which the embeddings are applied. Besides, people also consider character embeddings which have been utilized in Chinese word segmentation (Sun et al., 2014). The success of previous work verifies the feasibility of utilizing internal character information of words. We design our framework for lexical sememe prediction inspired by these methods. In this paper, we focus on the relationships between the words and the sememes. Following the settings of Xie et al. (2017), we simply ignore the senses and the hi"
P19-1074,P04-1035,0,0.0142302,"ngle sentences. References Rui Cai, Xiaodong Zhang, and Houfeng Wang. 2016. Bidirectional recurrent convolutional neural network for relation classification. In Proceedings of ACL, pages 756–765. As documents provide richer information than sentences, moving research from sentence level to document level is a popular trend for many areas, including document-level event extraction (Walker et al., 2006; Mitamura et al., 2015, 2017), fact extraction and verification (Thorne et al., 2018), reading comprehension (Nguyen et al., 2016; Joshi et al., 2017; Lai et al., 2017), sentiment classification (Pang and Lee, 2004; Prettenhofer and Stein, 2010), summarization (Nallapati et al., 2016) and machine translation (Zhang et al., 2018). Recently, some document-level RE datasets have also been constructed. However, these datasets are either constructed via distant supervision (Quirk and Poon, 2017; Peng et al., 2017) with inevitable wrong labeling problem, or limited in specific domain (Li et al., 2016; Peng et al., 2017). In contrast, DocRED is constructed by crowd-workers with rich information, and is not limited in any specific domain, which makes it suitable to train and evaluate general-purpose document-le"
P19-1074,P11-1055,0,0.148806,"Missing"
P19-1074,Q17-1008,0,0.237663,"level is a popular trend for many areas, including document-level event extraction (Walker et al., 2006; Mitamura et al., 2015, 2017), fact extraction and verification (Thorne et al., 2018), reading comprehension (Nguyen et al., 2016; Joshi et al., 2017; Lai et al., 2017), sentiment classification (Pang and Lee, 2004; Prettenhofer and Stein, 2010), summarization (Nallapati et al., 2016) and machine translation (Zhang et al., 2018). Recently, some document-level RE datasets have also been constructed. However, these datasets are either constructed via distant supervision (Quirk and Poon, 2017; Peng et al., 2017) with inevitable wrong labeling problem, or limited in specific domain (Li et al., 2016; Peng et al., 2017). In contrast, DocRED is constructed by crowd-workers with rich information, and is not limited in any specific domain, which makes it suitable to train and evaluate general-purpose document-level RE systems. 7 Acknowledgement Fenia Christopoulou, Makoto Miwa, and Sophia Ananiadou. 2018. A walk-based model on entity graphs for relation extraction. In Proceedings of ACL, pages 81–88. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: pre-training of deep bidirect"
P19-1074,D14-1162,0,0.0928063,"dels differ only at the encoder used for encoding the document and will be explained in detail in the rest of this section. We refer the readers to the original paper for the details of the Context-Aware model for space limitation. The CNN/LSTM/BiLSTM based models first encode a document D = {wi }ni=1 consisting of n words into a hidden state vector sequence {hi }ni=1 with CNN/LSTM/BiLSTM as encoder, then compute the representations for entities, and finally predict relations for each entity pair. For each word, the features fed to the encoder is the concatenation of its GloVe word embedding (Pennington et al., 2014), entity type embedding and coreference embedding. The entity type embedding is obtained by mapping the entity type (e.g., PER, LOC, ORG) assigned to the word into a vector using an embedding matrix. The entity type is assigned by human for the humanannotated data, and by a fine-tuned BERT model for the distantly supervised data. Named entity mentions corresponding to the same entity are assigned with the same entity id, which is determined by the order of its first appearance in the document. And the entity ids are mapped into vectors as the coreference embeddings. For each named entity menti"
P19-1074,P17-1147,0,0.0349888,"uality datasets. However, these RE datasets limit relations to single sentences. References Rui Cai, Xiaodong Zhang, and Houfeng Wang. 2016. Bidirectional recurrent convolutional neural network for relation classification. In Proceedings of ACL, pages 756–765. As documents provide richer information than sentences, moving research from sentence level to document level is a popular trend for many areas, including document-level event extraction (Walker et al., 2006; Mitamura et al., 2015, 2017), fact extraction and verification (Thorne et al., 2018), reading comprehension (Nguyen et al., 2016; Joshi et al., 2017; Lai et al., 2017), sentiment classification (Pang and Lee, 2004; Prettenhofer and Stein, 2010), summarization (Nallapati et al., 2016) and machine translation (Zhang et al., 2018). Recently, some document-level RE datasets have also been constructed. However, these datasets are either constructed via distant supervision (Quirk and Poon, 2017; Peng et al., 2017) with inevitable wrong labeling problem, or limited in specific domain (Li et al., 2016; Peng et al., 2017). In contrast, DocRED is constructed by crowd-workers with rich information, and is not limited in any specific domain, which ma"
P19-1074,P10-1114,0,0.0111808,"rences Rui Cai, Xiaodong Zhang, and Houfeng Wang. 2016. Bidirectional recurrent convolutional neural network for relation classification. In Proceedings of ACL, pages 756–765. As documents provide richer information than sentences, moving research from sentence level to document level is a popular trend for many areas, including document-level event extraction (Walker et al., 2006; Mitamura et al., 2015, 2017), fact extraction and verification (Thorne et al., 2018), reading comprehension (Nguyen et al., 2016; Joshi et al., 2017; Lai et al., 2017), sentiment classification (Pang and Lee, 2004; Prettenhofer and Stein, 2010), summarization (Nallapati et al., 2016) and machine translation (Zhang et al., 2018). Recently, some document-level RE datasets have also been constructed. However, these datasets are either constructed via distant supervision (Quirk and Poon, 2017; Peng et al., 2017) with inevitable wrong labeling problem, or limited in specific domain (Li et al., 2016; Peng et al., 2017). In contrast, DocRED is constructed by crowd-workers with rich information, and is not limited in any specific domain, which makes it suitable to train and evaluate general-purpose document-level RE systems. 7 Acknowledgeme"
P19-1074,D17-1082,0,0.028811,"ever, these RE datasets limit relations to single sentences. References Rui Cai, Xiaodong Zhang, and Houfeng Wang. 2016. Bidirectional recurrent convolutional neural network for relation classification. In Proceedings of ACL, pages 756–765. As documents provide richer information than sentences, moving research from sentence level to document level is a popular trend for many areas, including document-level event extraction (Walker et al., 2006; Mitamura et al., 2015, 2017), fact extraction and verification (Thorne et al., 2018), reading comprehension (Nguyen et al., 2016; Joshi et al., 2017; Lai et al., 2017), sentiment classification (Pang and Lee, 2004; Prettenhofer and Stein, 2010), summarization (Nallapati et al., 2016) and machine translation (Zhang et al., 2018). Recently, some document-level RE datasets have also been constructed. However, these datasets are either constructed via distant supervision (Quirk and Poon, 2017; Peng et al., 2017) with inevitable wrong labeling problem, or limited in specific domain (Li et al., 2016; Peng et al., 2017). In contrast, DocRED is constructed by crowd-workers with rich information, and is not limited in any specific domain, which makes it suitable to"
P19-1074,D12-1110,0,0.0504203,"tion instances annotated for this example document are presented, with named entity mentions involved in these instances colored in blue and other named entity mentions underlined for clarity. Note that mentions of the same subject (e.g., Kungliga Hovkapellet and Royal Court Orchestra) are identified as shown in the first relation instance. work focuses on sentence-level RE, i.e., extracting relational facts from a single sentence. In recent years, various neural models have been explored to encode relational patterns of entities for sentence-level RE, and achieve state-of-theart performance (Socher et al., 2012; Zeng et al., 2014, 2015; dos Santos et al., 2015; Xiao and Liu, 2016; Cai et al., 2016; Lin et al., 2016; Wu et al., 2017; Qin et al., 2018; Han et al., 2018a). Despite these successful efforts, sentence-level RE suffers from an inevitable restriction in practice: a large number of relational facts are expressed in multiple sentences. Taking Figure 1 as an example, multiple entities are mentioned in the document and exhibit complex interactions. In Introduction The task of relation extraction (RE) is to identify relational facts between entities from plain text, which plays an important role"
P19-1074,D17-1188,0,0.159305,"n higher computational complexity such as (Sorokin Benchmark Settings We design two benchmark settings for supervised and weakly supervised scenarios respectively. For both settings, RE systems are evaluated on the high-quality human-annotated dataset, which provides more reliable evaluation results for document-level RE systems. The statistics of data used for the two settings are shown in Table 3. Supervised Setting. In this setting, only humanannotated data is used, which are randomly split 768 model, a bidirectional LSTM (BiLSTM) (Cai et al., 2016) based model and the Context-Aware model (Sorokin and Gurevych, 2017) originally designed for leveraging contextual relations to improve intra-sentence RE. The first three models differ only at the encoder used for encoding the document and will be explained in detail in the rest of this section. We refer the readers to the original paper for the details of the Context-Aware model for space limitation. The CNN/LSTM/BiLSTM based models first encode a document D = {wi }ni=1 consisting of n words into a hidden state vector sequence {hi }ni=1 with CNN/LSTM/BiLSTM as encoder, then compute the representations for entities, and finally predict relations for each entit"
P19-1074,D12-1042,0,0.159686,"Missing"
P19-1074,swampillai-stevenson-2010-inter,0,\N,Missing
P19-1074,P09-1113,0,\N,Missing
P19-1074,W03-0419,0,\N,Missing
P19-1074,C14-1220,0,\N,Missing
P19-1074,doddington-etal-2004-automatic,0,\N,Missing
P19-1074,P16-1072,0,\N,Missing
P19-1074,P16-1200,1,\N,Missing
P19-1074,D17-1004,0,\N,Missing
P19-1074,D17-1187,0,\N,Missing
P19-1074,P18-2014,0,\N,Missing
P19-1074,D18-1259,0,\N,Missing
P19-1074,D18-1247,1,\N,Missing
P19-1074,N19-1423,0,\N,Missing
P19-1074,D18-1514,1,\N,Missing
P19-1074,E17-1110,0,\N,Missing
P19-1074,P15-1061,0,\N,Missing
P19-1074,D15-1203,0,\N,Missing
P19-1074,C16-1119,0,\N,Missing
P19-1085,W13-3819,0,0.0691858,"Missing"
P19-1085,W17-5307,0,0.125125,"“SUPPORTED” example and “REFUTED” example, we cannot verify the given claims via checking any evidence in isolation. The claims can be verified only by understanding and reasoning over the multiple evidence. To integrate and reason over information from multiple pieces of evidence, we propose a Introduction Due to the rapid development of information extraction (IE), huge volumes of data have been extracted. How to automatically verify the data becomes a vital problem for various datadriven applications, e.g., knowledge graph completion (Wang et al., 2017) and open domain question answering (Chen et al., 2017a). Hence, many recent research efforts have been devoted to fact verification (FV), which aims to verify given claims with the evidence retrieved from plain text. † The Rodney King riots took place in the most populous county in the USA. Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) 892 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 892–901 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics graph-based evidence aggregating and reasoning (GEAR) framework. Specifically, we first build a fully-connected"
P19-1085,D17-1070,0,0.0306419,"guage Inference The natural language inference (NLI) task requires a system to label the relationship between a pair of premise and hypothesis as entailment, contradiction or neutral. Several large-scale datasets have been proposed to promote the research in this direction, such as SNLI (Bowman et al., 2015) and Multi-NLI (Williams et al., 2018). These datasets have made it feasible to train complicated neural models which have achieved the state-of-the-art results (Bowman et al., 2015; Parikh et al., 2016; Sha et al., 2016; Chen et al., 2017b,c; Munkhdalai and Yu, 2017; Nie and Bansal, 2017; Conneau et al., 2017; Gong et al., 2018; Tay et al., 2018; Ghaeini et al., 2018). It is intuitive to transfer NLI models into the claim verification stage of the FEVER task and several teams from the shared task have achieved promising results by this way. Related Work FEVER Shared Task The FEVER shared task (Thorne et al., 2018b) challenges participants to develop automatic fact verification systems to check the veracity of human-generated claims by extracting evidence from Wikipedia. The shared task is hosted as a competition on Codalab1 with a blind test set. Nie et al. (2019); Yoneda et al. (2018) and Hanselo"
P19-1085,N19-1423,0,0.49966,"r is not sufficient for the claim. Intuitively, by sufficiently exchanging and reasoning over evidence information on the evidence graph, the proposed model can make the best of the information for verifying claims. For example, by delivering the information “Los Angeles County is the most populous county in the USA” to “the Rodney King riots occurred in Los Angeles County” through the evidence graph, the synthetic information can support “The Rodney King riots took place in the most populous county in the USA”. Furthermore, we adopt an effective pretrained language representation model BERT (Devlin et al., 2019) to better grasp both evidence and claim semantics. We conduct experiments on the large-scale benchmark dataset for Fact Extraction and VERification (FEVER) (Thorne et al., 2018a). Experimental results show that the proposed framework outperforms recent state-of-the-art baseline systems. The further case study indicates that our framework could better leverage multi-evidence information and reason over the evidence for FV. 2 2.1 NLI predictions for final verification. Then, Hanselowski et al. (2018); Yoneda et al. (2018); Hidey and Diab (2018) adopt the enhanced sequential inference model (ESI"
P19-1085,W18-2501,0,0.0172141,"ose our Graph-based Evidence Aggregating and Reasoning (GEAR) framework in the final claim verification stage. The full pipeline of our method is illustrated in Figure 1. 3.1 Document Retrieval and Sentence Selection In this section, we describe our document retrieval and sentence selection components. Additionally, we add a threshold filter after the sentence selection component to filter out those noisy evidence. In the document retrieval step, we adopt the entity linking approach from Hanselowski et al. (2018). Given a claim, the method first utilizes the constituency parser from AllenNLP (Gardner et al., 2018) to extract potential entities from the claim. Then it uses the entities as search queries and finds relevant Wikipedia documents via the online MediaWiki API2 . The seven highest-ranked results for each query are stored to form a candidate article set. Finally, the method drops the articles which are not in the offline Wikipedia dump and filters the articles by the word overlap between their titles and the claim. The sentence selection component selects the most relevant evidence for the claim from all sentences in the retrieved documents. Hanselowski et al. (2018) modify the ESIM 3.2 Claim V"
P19-1085,J84-3009,0,0.693828,"Missing"
P19-1085,W18-5516,0,0.229796,"ounty in the USA”. Furthermore, we adopt an effective pretrained language representation model BERT (Devlin et al., 2019) to better grasp both evidence and claim semantics. We conduct experiments on the large-scale benchmark dataset for Fact Extraction and VERification (FEVER) (Thorne et al., 2018a). Experimental results show that the proposed framework outperforms recent state-of-the-art baseline systems. The further case study indicates that our framework could better leverage multi-evidence information and reason over the evidence for FV. 2 2.1 NLI predictions for final verification. Then, Hanselowski et al. (2018); Yoneda et al. (2018); Hidey and Diab (2018) adopt the enhanced sequential inference model (ESIM) (Chen et al., 2017b), a more effective NLI model, to infer the relevance between evidence and claims instead of DAM. As pre-trained language models have achieved great results on various NLP applications, Malon (2018) fine-tunes the generative pretraining transformer (GPT) (Radford et al., 2018) for FV. Based on the methods mentioned above, Nie et al. (2019) specially design the neural semantic matching network (NSMN), which is a modification of ESIM and achieves the best results in the competiti"
P19-1085,W18-5525,0,0.0407684,"tive pretrained language representation model BERT (Devlin et al., 2019) to better grasp both evidence and claim semantics. We conduct experiments on the large-scale benchmark dataset for Fact Extraction and VERification (FEVER) (Thorne et al., 2018a). Experimental results show that the proposed framework outperforms recent state-of-the-art baseline systems. The further case study indicates that our framework could better leverage multi-evidence information and reason over the evidence for FV. 2 2.1 NLI predictions for final verification. Then, Hanselowski et al. (2018); Yoneda et al. (2018); Hidey and Diab (2018) adopt the enhanced sequential inference model (ESIM) (Chen et al., 2017b), a more effective NLI model, to infer the relevance between evidence and claims instead of DAM. As pre-trained language models have achieved great results on various NLP applications, Malon (2018) fine-tunes the generative pretraining transformer (GPT) (Radford et al., 2018) for FV. Based on the methods mentioned above, Nie et al. (2019) specially design the neural semantic matching network (NSMN), which is a modification of ESIM and achieves the best results in the competition. Unlike these methods, Yin and Roth (2018)"
P19-1085,D14-1059,0,0.0998272,"twork is an American basic cable and satellite television channel. Table 1: Some examples of reasoning over several pieces of evidence together for verification. The italic words are the key information to verify the claim. Both of the claims require to reason and aggregate multiple evidence sentences for verification. More specifically, given a claim, an FV system is asked to label it as “SUPPORTED”, “REFUTED”, or “NOT ENOUGH INFO”, which indicate that the evidence can support, refute, or is not sufficient for the claim. Existing FV methods formulate FV as a natural language inference (NLI) (Angeli and Manning, 2014) task. However, they utilize simple evidence combination methods such as concatenating the evidence or just dealing with each evidence-claim pair. These methods are unable to grasp sufficient relational and logical information among the evidence. In fact, many claims require to simultaneously integrate and reason over several pieces of evidence for verification. As shown in Table 1, for both of the “SUPPORTED” example and “REFUTED” example, we cannot verify the given claims via checking any evidence in isolation. The claims can be verified only by understanding and reasoning over the multiple"
P19-1085,D15-1075,0,0.0499913,"cially design the neural semantic matching network (NSMN), which is a modification of ESIM and achieves the best results in the competition. Unlike these methods, Yin and Roth (2018) propose the T WOW ING OS system which trains the evidence identification and claim verification modules jointly. 2.2 Natural Language Inference The natural language inference (NLI) task requires a system to label the relationship between a pair of premise and hypothesis as entailment, contradiction or neutral. Several large-scale datasets have been proposed to promote the research in this direction, such as SNLI (Bowman et al., 2015) and Multi-NLI (Williams et al., 2018). These datasets have made it feasible to train complicated neural models which have achieved the state-of-the-art results (Bowman et al., 2015; Parikh et al., 2016; Sha et al., 2016; Chen et al., 2017b,c; Munkhdalai and Yu, 2017; Nie and Bansal, 2017; Conneau et al., 2017; Gong et al., 2018; Tay et al., 2018; Ghaeini et al., 2018). It is intuitive to transfer NLI models into the claim verification stage of the FEVER task and several teams from the shared task have achieved promising results by this way. Related Work FEVER Shared Task The FEVER shared task"
P19-1085,W18-5526,0,0.190196,"Missing"
P19-1085,P17-1171,0,0.224451,"“SUPPORTED” example and “REFUTED” example, we cannot verify the given claims via checking any evidence in isolation. The claims can be verified only by understanding and reasoning over the multiple evidence. To integrate and reason over information from multiple pieces of evidence, we propose a Introduction Due to the rapid development of information extraction (IE), huge volumes of data have been extracted. How to automatically verify the data becomes a vital problem for various datadriven applications, e.g., knowledge graph completion (Wang et al., 2017) and open domain question answering (Chen et al., 2017a). Hence, many recent research efforts have been devoted to fact verification (FV), which aims to verify given claims with the evidence retrieved from plain text. † The Rodney King riots took place in the most populous county in the USA. Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) 892 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 892–901 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics graph-based evidence aggregating and reasoning (GEAR) framework. Specifically, we first build a fully-connected"
P19-1085,W18-5517,0,0.0467547,"hat the proposed framework outperforms recent state-of-the-art baseline systems. The further case study indicates that our framework could better leverage multi-evidence information and reason over the evidence for FV. 2 2.1 NLI predictions for final verification. Then, Hanselowski et al. (2018); Yoneda et al. (2018); Hidey and Diab (2018) adopt the enhanced sequential inference model (ESIM) (Chen et al., 2017b), a more effective NLI model, to infer the relevance between evidence and claims instead of DAM. As pre-trained language models have achieved great results on various NLP applications, Malon (2018) fine-tunes the generative pretraining transformer (GPT) (Radford et al., 2018) for FV. Based on the methods mentioned above, Nie et al. (2019) specially design the neural semantic matching network (NSMN), which is a modification of ESIM and achieves the best results in the competition. Unlike these methods, Yin and Roth (2018) propose the T WOW ING OS system which trains the evidence identification and claim verification modules jointly. 2.2 Natural Language Inference The natural language inference (NLI) task requires a system to label the relationship between a pair of premise and hypothesis"
P19-1085,E17-1002,0,0.0154565,"m verification modules jointly. 2.2 Natural Language Inference The natural language inference (NLI) task requires a system to label the relationship between a pair of premise and hypothesis as entailment, contradiction or neutral. Several large-scale datasets have been proposed to promote the research in this direction, such as SNLI (Bowman et al., 2015) and Multi-NLI (Williams et al., 2018). These datasets have made it feasible to train complicated neural models which have achieved the state-of-the-art results (Bowman et al., 2015; Parikh et al., 2016; Sha et al., 2016; Chen et al., 2017b,c; Munkhdalai and Yu, 2017; Nie and Bansal, 2017; Conneau et al., 2017; Gong et al., 2018; Tay et al., 2018; Ghaeini et al., 2018). It is intuitive to transfer NLI models into the claim verification stage of the FEVER task and several teams from the shared task have achieved promising results by this way. Related Work FEVER Shared Task The FEVER shared task (Thorne et al., 2018b) challenges participants to develop automatic fact verification systems to check the veracity of human-generated claims by extracting evidence from Wikipedia. The shared task is hosted as a competition on Codalab1 with a blind test set. Nie et"
P19-1085,W17-5308,0,0.0203664,"intly. 2.2 Natural Language Inference The natural language inference (NLI) task requires a system to label the relationship between a pair of premise and hypothesis as entailment, contradiction or neutral. Several large-scale datasets have been proposed to promote the research in this direction, such as SNLI (Bowman et al., 2015) and Multi-NLI (Williams et al., 2018). These datasets have made it feasible to train complicated neural models which have achieved the state-of-the-art results (Bowman et al., 2015; Parikh et al., 2016; Sha et al., 2016; Chen et al., 2017b,c; Munkhdalai and Yu, 2017; Nie and Bansal, 2017; Conneau et al., 2017; Gong et al., 2018; Tay et al., 2018; Ghaeini et al., 2018). It is intuitive to transfer NLI models into the claim verification stage of the FEVER task and several teams from the shared task have achieved promising results by this way. Related Work FEVER Shared Task The FEVER shared task (Thorne et al., 2018b) challenges participants to develop automatic fact verification systems to check the veracity of human-generated claims by extracting evidence from Wikipedia. The shared task is hosted as a competition on Codalab1 with a blind test set. Nie et al. (2019); Yoneda et"
P19-1085,D18-1010,0,0.0874661,"dey and Diab (2018) adopt the enhanced sequential inference model (ESIM) (Chen et al., 2017b), a more effective NLI model, to infer the relevance between evidence and claims instead of DAM. As pre-trained language models have achieved great results on various NLP applications, Malon (2018) fine-tunes the generative pretraining transformer (GPT) (Radford et al., 2018) for FV. Based on the methods mentioned above, Nie et al. (2019) specially design the neural semantic matching network (NSMN), which is a modification of ESIM and achieves the best results in the competition. Unlike these methods, Yin and Roth (2018) propose the T WOW ING OS system which trains the evidence identification and claim verification modules jointly. 2.2 Natural Language Inference The natural language inference (NLI) task requires a system to label the relationship between a pair of premise and hypothesis as entailment, contradiction or neutral. Several large-scale datasets have been proposed to promote the research in this direction, such as SNLI (Bowman et al., 2015) and Multi-NLI (Williams et al., 2018). These datasets have made it feasible to train complicated neural models which have achieved the state-of-the-art results ("
P19-1085,W18-5515,0,0.449838,"ore, we adopt an effective pretrained language representation model BERT (Devlin et al., 2019) to better grasp both evidence and claim semantics. We conduct experiments on the large-scale benchmark dataset for Fact Extraction and VERification (FEVER) (Thorne et al., 2018a). Experimental results show that the proposed framework outperforms recent state-of-the-art baseline systems. The further case study indicates that our framework could better leverage multi-evidence information and reason over the evidence for FV. 2 2.1 NLI predictions for final verification. Then, Hanselowski et al. (2018); Yoneda et al. (2018); Hidey and Diab (2018) adopt the enhanced sequential inference model (ESIM) (Chen et al., 2017b), a more effective NLI model, to infer the relevance between evidence and claims instead of DAM. As pre-trained language models have achieved great results on various NLP applications, Malon (2018) fine-tunes the generative pretraining transformer (GPT) (Radford et al., 2018) for FV. Based on the methods mentioned above, Nie et al. (2019) specially design the neural semantic matching network (NSMN), which is a modification of ESIM and achieves the best results in the competition. Unlike these metho"
P19-1085,D16-1244,0,0.131708,"Missing"
P19-1085,N18-1202,0,0.0381169,"nd test set. Nie et al. (2019); Yoneda et al. (2018) and Hanselowski et al. (2018) have achieved the top three results among 23 teams. Existing methods mainly formulate FV as an NLI task. Thorne et al. (2018a) simply concatenate all evidence together, and then feed the concatenated evidence and the given claim into the NLI model. Luken et al. (2018) adopt the decomposable attention model (DAM) (Parikh et al., 2016) to generate NLI predictions for each claimevidence pair individually and then aggregate all 2.3 Pre-trained Language Models Pre-trained language representation models such as ELMo (Peters et al., 2018) and OpenAI GPT (Radford et al., 2018) are proven to be effective on many NLP tasks. BERT (Devlin et al., 2019) employs bidirectional transformer and welldesigned pre-training tasks to fuse bidirectional context information and obtains the state-of-theart results on the NLI task. In our experiments, we find the fine-tuned BERT model outperforms other NLI-based models on the claim verification subtask of FEVER. Hence, we use BERT as the sentence encoder in our framework to better encoding semantic information of evidence and claims. 1 https://competitions.codalab.org/ competitions/18814 893 Cla"
P19-1085,C16-1270,0,0.0227379,"ins the evidence identification and claim verification modules jointly. 2.2 Natural Language Inference The natural language inference (NLI) task requires a system to label the relationship between a pair of premise and hypothesis as entailment, contradiction or neutral. Several large-scale datasets have been proposed to promote the research in this direction, such as SNLI (Bowman et al., 2015) and Multi-NLI (Williams et al., 2018). These datasets have made it feasible to train complicated neural models which have achieved the state-of-the-art results (Bowman et al., 2015; Parikh et al., 2016; Sha et al., 2016; Chen et al., 2017b,c; Munkhdalai and Yu, 2017; Nie and Bansal, 2017; Conneau et al., 2017; Gong et al., 2018; Tay et al., 2018; Ghaeini et al., 2018). It is intuitive to transfer NLI models into the claim verification stage of the FEVER task and several teams from the shared task have achieved promising results by this way. Related Work FEVER Shared Task The FEVER shared task (Thorne et al., 2018b) challenges participants to develop automatic fact verification systems to check the veracity of human-generated claims by extracting evidence from Wikipedia. The shared task is hosted as a competi"
P19-1085,D18-1185,0,0.0219262,"Missing"
P19-1085,N18-1074,0,0.605773,"the information for verifying claims. For example, by delivering the information “Los Angeles County is the most populous county in the USA” to “the Rodney King riots occurred in Los Angeles County” through the evidence graph, the synthetic information can support “The Rodney King riots took place in the most populous county in the USA”. Furthermore, we adopt an effective pretrained language representation model BERT (Devlin et al., 2019) to better grasp both evidence and claim semantics. We conduct experiments on the large-scale benchmark dataset for Fact Extraction and VERification (FEVER) (Thorne et al., 2018a). Experimental results show that the proposed framework outperforms recent state-of-the-art baseline systems. The further case study indicates that our framework could better leverage multi-evidence information and reason over the evidence for FV. 2 2.1 NLI predictions for final verification. Then, Hanselowski et al. (2018); Yoneda et al. (2018); Hidey and Diab (2018) adopt the enhanced sequential inference model (ESIM) (Chen et al., 2017b), a more effective NLI model, to infer the relevance between evidence and claims instead of DAM. As pre-trained language models have achieved great result"
P19-1085,W18-5501,0,0.12184,"Missing"
P19-1128,P18-2014,0,0.0429986,"en and Grishman (2015) propose a multi-window version of CNN for relation extraction. Lin et al. (2016) study an attention mechanism for relation extraction tasks. Peng et al. (2017) predict n-ary relations of entities in different sentences with Graph LSTMs. Le and Titov (2018) treat relations as latent variables which are capable of inducing the relations without any supervision signals. Zeng et al. (2017) show that the relation path has an important role in relation extraction. Miwa and Bansal (2016) show the effectiveness of LSTMs (Hochreiter and Schmidhuber, 1997) in relation extraction. Christopoulou et al. (2018) proposed a walk-based model to do relation extraction. The most related work is Sorokin and Gurevych (2017), where the proposed model incorporates contextual relations with an attention mechanism when predicting the relation of a target entity pair. The drawback of existing approaches is that they could not make full use of the multihop inference patterns among multiple entity pairs and their relations within the sentence. 3 (n) i,j i,j n Ai,j = f (E(xi,j 0 ), E(x1 ), · · · , E(xl−1 ); θe ), (1) where f (·) could be any model that could encode sequential data, such as LSTMs, GRUs, CNNs, E(·)"
P19-1128,N19-1240,0,0.0430003,"Missing"
P19-1128,P18-1148,0,0.0240849,"15; Lin et al., 2016) have demonstrated that neural networks are capa1332 ble of capturing the pair-wise relationship between entities in certain situations. For example, Zeng et al. (2014) is one of the earliest works that applies a simple CNN to this task, and Zeng et al. (2015) further extends it with piece-wise maxpooling. Nguyen and Grishman (2015) propose a multi-window version of CNN for relation extraction. Lin et al. (2016) study an attention mechanism for relation extraction tasks. Peng et al. (2017) predict n-ary relations of entities in different sentences with Graph LSTMs. Le and Titov (2018) treat relations as latent variables which are capable of inducing the relations without any supervision signals. Zeng et al. (2017) show that the relation path has an important role in relation extraction. Miwa and Bansal (2016) show the effectiveness of LSTMs (Hochreiter and Schmidhuber, 1997) in relation extraction. Christopoulou et al. (2018) proposed a walk-based model to do relation extraction. The most related work is Sorokin and Gurevych (2017), where the proposed model incorporates contextual relations with an attention mechanism when predicting the relation of a target entity pair. T"
P19-1128,D17-1188,0,0.624311,"y an attention mechanism for relation extraction tasks. Peng et al. (2017) predict n-ary relations of entities in different sentences with Graph LSTMs. Le and Titov (2018) treat relations as latent variables which are capable of inducing the relations without any supervision signals. Zeng et al. (2017) show that the relation path has an important role in relation extraction. Miwa and Bansal (2016) show the effectiveness of LSTMs (Hochreiter and Schmidhuber, 1997) in relation extraction. Christopoulou et al. (2018) proposed a walk-based model to do relation extraction. The most related work is Sorokin and Gurevych (2017), where the proposed model incorporates contextual relations with an attention mechanism when predicting the relation of a target entity pair. The drawback of existing approaches is that they could not make full use of the multihop inference patterns among multiple entity pairs and their relations within the sentence. 3 (n) i,j i,j n Ai,j = f (E(xi,j 0 ), E(x1 ), · · · , E(xl−1 ); θe ), (1) where f (·) could be any model that could encode sequential data, such as LSTMs, GRUs, CNNs, E(·) indicates an embedding function, and θen denotes the parameters of the encoding module of n-th layer. 3.2 Pr"
P19-1128,P16-1200,1,0.891879,"and demonstrate its effectiveness empirically. Gilmer et al. (2017) propose to apply GNNs to molecular property prediction tasks. Garcia and Bruna Relational Reasoning Relational reasoning has been explored in various fields. For example, Santoro et al. (2017) propose a simple neural network to reason the relationship of objects in a picture, Xu et al. (2017) build up a scene graph according to an image, and Kipf et al. (2018) model the interaction of physical objects. In this paper, we focus on the relational reasoning in the natural language domain. Existing works (Zeng et al., 2014, 2015; Lin et al., 2016) have demonstrated that neural networks are capa1332 ble of capturing the pair-wise relationship between entities in certain situations. For example, Zeng et al. (2014) is one of the earliest works that applies a simple CNN to this task, and Zeng et al. (2015) further extends it with piece-wise maxpooling. Nguyen and Grishman (2015) propose a multi-window version of CNN for relation extraction. Lin et al. (2016) study an attention mechanism for relation extraction tasks. Peng et al. (2017) predict n-ary relations of entities in different sentences with Graph LSTMs. Le and Titov (2018) treat re"
P19-1128,D17-1159,0,0.0382641,"ning as compared to those models which extract relationships separately. Moreover, we also present three datasets, which could help future researchers compare their models in different settings. 2 2.1 (2018) shows how to use GNNs to learn classifiers on image datasets in a few-shot manner. Gilmer et al. (2017) study the effectiveness of message-passing in quantum chemistry. Dhingra et al. (2017) apply message-passing on a graph constructed by coreference links to answer relational questions. There are relatively fewer papers discussing how to adapt GNNs to natural language tasks. For example, Marcheggiani and Titov (2017) propose to apply GNNs to semantic role labeling and Schlichtkrull et al. (2017) apply GNNs to knowledge base completion tasks. Zhang et al. (2018) apply GNNs to relation extraction by encoding dependency trees, and De Cao et al. (2018) apply GNNs to multi-hop question answering by encoding co-occurence and coreference relationships. Although they also consider applying GNNs to natural language processing tasks, they still perform message-passing on predefined graphs. Johnson (2017) introduces a novel neural architecture to generate a graph based on the textual input and dynamically update the"
P19-1128,P16-1105,0,0.0569637,"that applies a simple CNN to this task, and Zeng et al. (2015) further extends it with piece-wise maxpooling. Nguyen and Grishman (2015) propose a multi-window version of CNN for relation extraction. Lin et al. (2016) study an attention mechanism for relation extraction tasks. Peng et al. (2017) predict n-ary relations of entities in different sentences with Graph LSTMs. Le and Titov (2018) treat relations as latent variables which are capable of inducing the relations without any supervision signals. Zeng et al. (2017) show that the relation path has an important role in relation extraction. Miwa and Bansal (2016) show the effectiveness of LSTMs (Hochreiter and Schmidhuber, 1997) in relation extraction. Christopoulou et al. (2018) proposed a walk-based model to do relation extraction. The most related work is Sorokin and Gurevych (2017), where the proposed model incorporates contextual relations with an attention mechanism when predicting the relation of a target entity pair. The drawback of existing approaches is that they could not make full use of the multihop inference patterns among multiple entity pairs and their relations within the sentence. 3 (n) i,j i,j n Ai,j = f (E(xi,j 0 ), E(x1 ), · · · ,"
P19-1128,W15-1506,0,0.261678,"bjects in a picture, Xu et al. (2017) build up a scene graph according to an image, and Kipf et al. (2018) model the interaction of physical objects. In this paper, we focus on the relational reasoning in the natural language domain. Existing works (Zeng et al., 2014, 2015; Lin et al., 2016) have demonstrated that neural networks are capa1332 ble of capturing the pair-wise relationship between entities in certain situations. For example, Zeng et al. (2014) is one of the earliest works that applies a simple CNN to this task, and Zeng et al. (2015) further extends it with piece-wise maxpooling. Nguyen and Grishman (2015) propose a multi-window version of CNN for relation extraction. Lin et al. (2016) study an attention mechanism for relation extraction tasks. Peng et al. (2017) predict n-ary relations of entities in different sentences with Graph LSTMs. Le and Titov (2018) treat relations as latent variables which are capable of inducing the relations without any supervision signals. Zeng et al. (2017) show that the relation path has an important role in relation extraction. Miwa and Bansal (2016) show the effectiveness of LSTMs (Hochreiter and Schmidhuber, 1997) in relation extraction. Christopoulou et al. ("
P19-1128,D15-1203,0,0.647955,"7) propose a simple neural network to reason the relationship of objects in a picture, Xu et al. (2017) build up a scene graph according to an image, and Kipf et al. (2018) model the interaction of physical objects. In this paper, we focus on the relational reasoning in the natural language domain. Existing works (Zeng et al., 2014, 2015; Lin et al., 2016) have demonstrated that neural networks are capa1332 ble of capturing the pair-wise relationship between entities in certain situations. For example, Zeng et al. (2014) is one of the earliest works that applies a simple CNN to this task, and Zeng et al. (2015) further extends it with piece-wise maxpooling. Nguyen and Grishman (2015) propose a multi-window version of CNN for relation extraction. Lin et al. (2016) study an attention mechanism for relation extraction tasks. Peng et al. (2017) predict n-ary relations of entities in different sentences with Graph LSTMs. Le and Titov (2018) treat relations as latent variables which are capable of inducing the relations without any supervision signals. Zeng et al. (2017) show that the relation path has an important role in relation extraction. Miwa and Bansal (2016) show the effectiveness of LSTMs (Hochre"
P19-1128,C14-1220,0,0.816506,"e generic backpropagation and demonstrate its effectiveness empirically. Gilmer et al. (2017) propose to apply GNNs to molecular property prediction tasks. Garcia and Bruna Relational Reasoning Relational reasoning has been explored in various fields. For example, Santoro et al. (2017) propose a simple neural network to reason the relationship of objects in a picture, Xu et al. (2017) build up a scene graph according to an image, and Kipf et al. (2018) model the interaction of physical objects. In this paper, we focus on the relational reasoning in the natural language domain. Existing works (Zeng et al., 2014, 2015; Lin et al., 2016) have demonstrated that neural networks are capa1332 ble of capturing the pair-wise relationship between entities in certain situations. For example, Zeng et al. (2014) is one of the earliest works that applies a simple CNN to this task, and Zeng et al. (2015) further extends it with piece-wise maxpooling. Nguyen and Grishman (2015) propose a multi-window version of CNN for relation extraction. Lin et al. (2016) study an attention mechanism for relation extraction tasks. Peng et al. (2017) predict n-ary relations of entities in different sentences with Graph LSTMs. Le"
P19-1128,D17-1186,1,0.883827,"entities in certain situations. For example, Zeng et al. (2014) is one of the earliest works that applies a simple CNN to this task, and Zeng et al. (2015) further extends it with piece-wise maxpooling. Nguyen and Grishman (2015) propose a multi-window version of CNN for relation extraction. Lin et al. (2016) study an attention mechanism for relation extraction tasks. Peng et al. (2017) predict n-ary relations of entities in different sentences with Graph LSTMs. Le and Titov (2018) treat relations as latent variables which are capable of inducing the relations without any supervision signals. Zeng et al. (2017) show that the relation path has an important role in relation extraction. Miwa and Bansal (2016) show the effectiveness of LSTMs (Hochreiter and Schmidhuber, 1997) in relation extraction. Christopoulou et al. (2018) proposed a walk-based model to do relation extraction. The most related work is Sorokin and Gurevych (2017), where the proposed model incorporates contextual relations with an attention mechanism when predicting the relation of a target entity pair. The drawback of existing approaches is that they could not make full use of the multihop inference patterns among multiple entity pai"
P19-1128,D18-1244,0,0.158096,"e their models in different settings. 2 2.1 (2018) shows how to use GNNs to learn classifiers on image datasets in a few-shot manner. Gilmer et al. (2017) study the effectiveness of message-passing in quantum chemistry. Dhingra et al. (2017) apply message-passing on a graph constructed by coreference links to answer relational questions. There are relatively fewer papers discussing how to adapt GNNs to natural language tasks. For example, Marcheggiani and Titov (2017) propose to apply GNNs to semantic role labeling and Schlichtkrull et al. (2017) apply GNNs to knowledge base completion tasks. Zhang et al. (2018) apply GNNs to relation extraction by encoding dependency trees, and De Cao et al. (2018) apply GNNs to multi-hop question answering by encoding co-occurence and coreference relationships. Although they also consider applying GNNs to natural language processing tasks, they still perform message-passing on predefined graphs. Johnson (2017) introduces a novel neural architecture to generate a graph based on the textual input and dynamically update the relationship during the learning process. In sharp contrast, this paper focuses on extracting relations from real-world relation datasets. 2.2 Rel"
P19-1128,D14-1162,0,0.0823489,"e n denotes the index of layer 1 , [·] means reshaping a vector as a matrix, BiLSTM encodes a sequence by concatenating tail hidden states of the forward LSTM and head hidden states of the backward LSTM together and MLP denotes a multilayer perceptron with non-linear activation σ. Word Representations We first map each token xt of sentence {x0 , x1 , . . . , xl−1 } to a kdimensional embedding vector xt using a word embedding matrix We ∈ R|V |×dw , where |V |is the size of the vocabulary. Throughout this paper, we stick to 50-dimensional GloVe embeddings pre-trained on a 6-billion-word corpus (Pennington et al., 2014). 1 Adding index to neural models means their parameters are different among layers. Position Embedding In this work, we consider a simple entity marking scheme2 : we mark each token in the sentence as either belonging to the first entity vi , the second entity vj or to neither of those. Each position marker is also mapped to a dp -dimensional vector by a position embedding matrix P ∈ R3×dp . We use notation pi,j t to represent the position embedding for xt corresponding to entity pair (vi , vj ). 4.2 Propagation Module Next, we use Eq. (2) to propagate information among nodes where the initia"
P19-1137,P17-1038,0,0.185671,"Missing"
P19-1137,D14-1164,0,0.0357125,"Missing"
P19-1137,P11-1055,0,0.181204,"a small number of human annotations; • presenting both significant and interpretable performance improvements as well as intuitive diagnostic analyses. Particularly, for one relation with severe false negative noises, we improve the F1 score by about 0.4. To the best of our knowledge, we are the first to explicitly reveal and address this severe noise problem for that dataset. 2 Related Work To reduce labeling noises of DS, earlier work attempted to design specific model architectures that can better tolerate labeling noises, such as the multi-instance learning paradigm (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016; Wu et al., 2017). These models relax the raw assumption of DS by grouping multiple sentences that mention the same entity pair together as a bag and then assuming that at least one sentence in this bag expresses the relation. This weaker assumption can alleviate the noisy-labeling problem to some extent, but this problem still exists at the bag level, and Feng et al. (2018) discovered that bag-level models struggled to do sentence-level predictions. Later work tried to design a dynamic labeladjustment strategy for training (Liu et a"
P19-1137,P18-1161,1,0.894749,"Missing"
P19-1137,P16-1200,1,0.908825,"e a large number of human-annotated data to train, which are both expensive and time-consuming to collect. Therefore, Craven et al. (1999); Mintz et al. (2009) proposed distant supervision (DS) to automatically generate large-scale training data for relation extraction, by aligning relational facts from a knowledge base (KB) to plain text and assuming that every sentence mentioning two entities can describe their relationships in the KB. As DS can acquire large-scale data without human annotation, it has been widely adopted by recent neural relation extraction (NRE) models (Zeng et al., 2015; Lin et al., 2016). Although DS is both simple and effective in many cases, it inevitably introduces intolerable labeling noises. As Figure 1 shows, there are two types of error labels, false negatives and false positives. The reason for false negatives is that a sentence does describe two entities about a target relation, but the fact has not been covered by the KB yet. While for false positives, it is because not all sentences mentioning entity pairs actually express their relations in the KB. The noisy-labeling problem can become severe when the KB and text do not match well and as a result heavily weaken th"
P19-1137,N16-1104,0,0.153144,"a sentence does describe two entities about a target relation, but the fact has not been covered by the KB yet. While for false positives, it is because not all sentences mentioning entity pairs actually express their relations in the KB. The noisy-labeling problem can become severe when the KB and text do not match well and as a result heavily weaken the model performance (Riedel et al., 2010). Recent research has realized that introducing appropriate human efforts is essential for reducing such labeling noises. For example, Zhang et al. (2012); Pershina et al. (2014); Angeli et al. (2014); Liu et al. (2016) mixed a small set of crowd-annotated labels with purely DS-generated noise labels. However, they found that only sufficiently large and high-quality human labels can bring notable improvements, because there are 1419 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1419–1429 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics significantly larger number of noise labels. To enlarge the impact of human efforts, Ratner et al. (2016); Liu et al. (2017a) proposed to incorporate pattern-based labeling, where the k"
P19-1137,D17-1005,0,0.213057,"et al. (2012); Pershina et al. (2014); Angeli et al. (2014); Liu et al. (2016) mixed a small set of crowd-annotated labels with purely DS-generated noise labels. However, they found that only sufficiently large and high-quality human labels can bring notable improvements, because there are 1419 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1419–1429 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics significantly larger number of noise labels. To enlarge the impact of human efforts, Ratner et al. (2016); Liu et al. (2017a) proposed to incorporate pattern-based labeling, where the key idea was to regard both DS and pattern-based heuristics as the weak supervision sources and develop a weak-label-fusion (WLF) model to produce denoised labels. However, the major limitation of the WLF paradigm lies in the requirement of human experts to write relation-specific patterns. Unfortunately, writing good patterns is both a highskill and labor-intensive task that requires experts to learn detailed pattern-composing instructions, examine adequate examples, tune patterns for different corner cases, etc. For example, the sp"
P19-1137,D17-1189,0,0.680844,"et al. (2012); Pershina et al. (2014); Angeli et al. (2014); Liu et al. (2016) mixed a small set of crowd-annotated labels with purely DS-generated noise labels. However, they found that only sufficiently large and high-quality human labels can bring notable improvements, because there are 1419 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1419–1429 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics significantly larger number of noise labels. To enlarge the impact of human efforts, Ratner et al. (2016); Liu et al. (2017a) proposed to incorporate pattern-based labeling, where the key idea was to regard both DS and pattern-based heuristics as the weak supervision sources and develop a weak-label-fusion (WLF) model to produce denoised labels. However, the major limitation of the WLF paradigm lies in the requirement of human experts to write relation-specific patterns. Unfortunately, writing good patterns is both a highskill and labor-intensive task that requires experts to learn detailed pattern-composing instructions, examine adequate examples, tune patterns for different corner cases, etc. For example, the sp"
P19-1137,P17-1040,0,0.0220481,"et al., 2012; Zeng et al., 2015; Lin et al., 2016; Wu et al., 2017). These models relax the raw assumption of DS by grouping multiple sentences that mention the same entity pair together as a bag and then assuming that at least one sentence in this bag expresses the relation. This weaker assumption can alleviate the noisy-labeling problem to some extent, but this problem still exists at the bag level, and Feng et al. (2018) discovered that bag-level models struggled to do sentence-level predictions. Later work tried to design a dynamic labeladjustment strategy for training (Liu et al., 2017b; Luo et al., 2017). Especially, the most recent work (Feng et al., 2018; Qin et al., 2018) adopted RL to train an agent that interacts with the NRE model to learn how to remove or alter noise labels. These methods work without human intervention by utilizing the consistency and difference between DS-generated labels and model-predicted ones. However, such methods can neither discover 1420 noise labels that coincide with the model predictions nor explain the reasons for removed or altered labels. As discussed in the introduction, introducing human efforts is a promising direction to contribute both significant a"
P19-1137,P09-1113,0,0.925853,"s defined as a relation between a head entity and a tail entity, e.g., (Letizia Moratti, Birthplace, Milan). The conventional methods often regard relation extraction as a supervised classification task that predicts the relation type between two detected entities mentioned in a sentence, including both statistical models (Zelenko et al., 2003; Zhou et al., 2005) and neural models (Zeng et al., 2014; dos Santos et al., 2015). These supervised models require a large number of human-annotated data to train, which are both expensive and time-consuming to collect. Therefore, Craven et al. (1999); Mintz et al. (2009) proposed distant supervision (DS) to automatically generate large-scale training data for relation extraction, by aligning relational facts from a knowledge base (KB) to plain text and assuming that every sentence mentioning two entities can describe their relationships in the KB. As DS can acquire large-scale data without human annotation, it has been widely adopted by recent neural relation extraction (NRE) models (Zeng et al., 2015; Lin et al., 2016). Although DS is both simple and effective in many cases, it inevitably introduces intolerable labeling noises. As Figure 1 shows, there are t"
P19-1137,D12-1104,0,0.0947384,"improved the performance of the vanilla LSTM (Hochreiter and Schmidhuber, 1997) by utilizing RL to discover structured representations and Li et al. (2016) interpreted the sentiment prediction of neural models by employing RL to find the decision-changing phrases. However, NRE models are unique because we only care about the semantic inter-entity relation mentioned in the sentence. To the best of our knowledge, we are the first to extract patterns from NRE models by RL. We also note that the relational-pattern mining has been extensively studied (Califf and Mooney, 1999; Carlson et al., 2010; Nakashole et al., 2012; Jiang et al., 2017). Different from those studies, our pattern-extraction method 1) is simply based on RL, 2) does not rely on any lexical or syntactic annotation, and 3) can be aware of the pattern importance via the prediction of NRE models. Besides, Takamatsu et al. (2012) inferred negative syntactic patterns via the example-pattern-relation co-occurrence and removed the false-positive labels accordingly. In contrast, built upon modern neural models, our method not only reduces negative patterns to alleviate false positives but also reinforces positive patterns to address false negatives"
P19-1137,C14-1220,0,0.100927,"atives (FN) and false positives (FP), caused by DS. Introduction Relation extraction aims to extract relational facts from the plain text and can benefit downstream knowledge-driven applications. A relational fact is defined as a relation between a head entity and a tail entity, e.g., (Letizia Moratti, Birthplace, Milan). The conventional methods often regard relation extraction as a supervised classification task that predicts the relation type between two detected entities mentioned in a sentence, including both statistical models (Zelenko et al., 2003; Zhou et al., 2005) and neural models (Zeng et al., 2014; dos Santos et al., 2015). These supervised models require a large number of human-annotated data to train, which are both expensive and time-consuming to collect. Therefore, Craven et al. (1999); Mintz et al. (2009) proposed distant supervision (DS) to automatically generate large-scale training data for relation extraction, by aligning relational facts from a knowledge base (KB) to plain text and assuming that every sentence mentioning two entities can describe their relationships in the KB. As DS can acquire large-scale data without human annotation, it has been widely adopted by recent ne"
P19-1137,D14-1162,0,0.0810357,"Missing"
P19-1137,P12-1087,0,0.0275027,"Missing"
P19-1137,P14-2119,1,0.82305,"itives. The reason for false negatives is that a sentence does describe two entities about a target relation, but the fact has not been covered by the KB yet. While for false positives, it is because not all sentences mentioning entity pairs actually express their relations in the KB. The noisy-labeling problem can become severe when the KB and text do not match well and as a result heavily weaken the model performance (Riedel et al., 2010). Recent research has realized that introducing appropriate human efforts is essential for reducing such labeling noises. For example, Zhang et al. (2012); Pershina et al. (2014); Angeli et al. (2014); Liu et al. (2016) mixed a small set of crowd-annotated labels with purely DS-generated noise labels. However, they found that only sufficiently large and high-quality human labels can bring notable improvements, because there are 1419 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1419–1429 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics significantly larger number of noise labels. To enlarge the impact of human efforts, Ratner et al. (2016); Liu et al. (2017a) proposed to incorp"
P19-1137,P18-1199,0,0.0429382,"se models relax the raw assumption of DS by grouping multiple sentences that mention the same entity pair together as a bag and then assuming that at least one sentence in this bag expresses the relation. This weaker assumption can alleviate the noisy-labeling problem to some extent, but this problem still exists at the bag level, and Feng et al. (2018) discovered that bag-level models struggled to do sentence-level predictions. Later work tried to design a dynamic labeladjustment strategy for training (Liu et al., 2017b; Luo et al., 2017). Especially, the most recent work (Feng et al., 2018; Qin et al., 2018) adopted RL to train an agent that interacts with the NRE model to learn how to remove or alter noise labels. These methods work without human intervention by utilizing the consistency and difference between DS-generated labels and model-predicted ones. However, such methods can neither discover 1420 noise labels that coincide with the model predictions nor explain the reasons for removed or altered labels. As discussed in the introduction, introducing human efforts is a promising direction to contribute both significant and interpretable improvements, which is also the focus of this paper. As"
P19-1137,P05-1053,0,0.214768,": Two types of error labels, false negatives (FN) and false positives (FP), caused by DS. Introduction Relation extraction aims to extract relational facts from the plain text and can benefit downstream knowledge-driven applications. A relational fact is defined as a relation between a head entity and a tail entity, e.g., (Letizia Moratti, Birthplace, Milan). The conventional methods often regard relation extraction as a supervised classification task that predicts the relation type between two detected entities mentioned in a sentence, including both statistical models (Zelenko et al., 2003; Zhou et al., 2005) and neural models (Zeng et al., 2014; dos Santos et al., 2015). These supervised models require a large number of human-annotated data to train, which are both expensive and time-consuming to collect. Therefore, Craven et al. (1999); Mintz et al. (2009) proposed distant supervision (DS) to automatically generate large-scale training data for relation extraction, by aligning relational facts from a knowledge base (KB) to plain text and assuming that every sentence mentioning two entities can describe their relationships in the KB. As DS can acquire large-scale data without human annotation, it"
P19-1137,P16-2034,0,0.0665773,"Missing"
P19-1137,P15-1061,0,0.0475427,"ositives (FP), caused by DS. Introduction Relation extraction aims to extract relational facts from the plain text and can benefit downstream knowledge-driven applications. A relational fact is defined as a relation between a head entity and a tail entity, e.g., (Letizia Moratti, Birthplace, Milan). The conventional methods often regard relation extraction as a supervised classification task that predicts the relation type between two detected entities mentioned in a sentence, including both statistical models (Zelenko et al., 2003; Zhou et al., 2005) and neural models (Zeng et al., 2014; dos Santos et al., 2015). These supervised models require a large number of human-annotated data to train, which are both expensive and time-consuming to collect. Therefore, Craven et al. (1999); Mintz et al. (2009) proposed distant supervision (DS) to automatically generate large-scale training data for relation extraction, by aligning relational facts from a knowledge base (KB) to plain text and assuming that every sentence mentioning two entities can describe their relationships in the KB. As DS can acquire large-scale data without human annotation, it has been widely adopted by recent neural relation extraction ("
P19-1137,D12-1042,0,0.111105,"n annotations; • presenting both significant and interpretable performance improvements as well as intuitive diagnostic analyses. Particularly, for one relation with severe false negative noises, we improve the F1 score by about 0.4. To the best of our knowledge, we are the first to explicitly reveal and address this severe noise problem for that dataset. 2 Related Work To reduce labeling noises of DS, earlier work attempted to design specific model architectures that can better tolerate labeling noises, such as the multi-instance learning paradigm (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016; Wu et al., 2017). These models relax the raw assumption of DS by grouping multiple sentences that mention the same entity pair together as a bag and then assuming that at least one sentence in this bag expresses the relation. This weaker assumption can alleviate the noisy-labeling problem to some extent, but this problem still exists at the bag level, and Feng et al. (2018) discovered that bag-level models struggled to do sentence-level predictions. Later work tried to design a dynamic labeladjustment strategy for training (Liu et al., 2017b; Luo et al.,"
P19-1137,P12-1076,0,0.0302716,"models are unique because we only care about the semantic inter-entity relation mentioned in the sentence. To the best of our knowledge, we are the first to extract patterns from NRE models by RL. We also note that the relational-pattern mining has been extensively studied (Califf and Mooney, 1999; Carlson et al., 2010; Nakashole et al., 2012; Jiang et al., 2017). Different from those studies, our pattern-extraction method 1) is simply based on RL, 2) does not rely on any lexical or syntactic annotation, and 3) can be aware of the pattern importance via the prediction of NRE models. Besides, Takamatsu et al. (2012) inferred negative syntactic patterns via the example-pattern-relation co-occurrence and removed the false-positive labels accordingly. In contrast, built upon modern neural models, our method not only reduces negative patterns to alleviate false positives but also reinforces positive patterns to address false negatives at the same time. 3 Methodology Provided with DS-generated data and NRE models trained on them, DIAG-NRE can generate high-quality patterns for the WLF stage to produce denoised labels. As Figure 2 shows, DIAG-NRE contains two key stages in general: pattern extraction (Section"
P19-1137,D17-1187,0,0.107221,"le performance improvements as well as intuitive diagnostic analyses. Particularly, for one relation with severe false negative noises, we improve the F1 score by about 0.4. To the best of our knowledge, we are the first to explicitly reveal and address this severe noise problem for that dataset. 2 Related Work To reduce labeling noises of DS, earlier work attempted to design specific model architectures that can better tolerate labeling noises, such as the multi-instance learning paradigm (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016; Wu et al., 2017). These models relax the raw assumption of DS by grouping multiple sentences that mention the same entity pair together as a bag and then assuming that at least one sentence in this bag expresses the relation. This weaker assumption can alleviate the noisy-labeling problem to some extent, but this problem still exists at the bag level, and Feng et al. (2018) discovered that bag-level models struggled to do sentence-level predictions. Later work tried to design a dynamic labeladjustment strategy for training (Liu et al., 2017b; Luo et al., 2017). Especially, the most recent work (Feng et al., 2"
P19-1137,D15-1203,0,0.38233,"vised models require a large number of human-annotated data to train, which are both expensive and time-consuming to collect. Therefore, Craven et al. (1999); Mintz et al. (2009) proposed distant supervision (DS) to automatically generate large-scale training data for relation extraction, by aligning relational facts from a knowledge base (KB) to plain text and assuming that every sentence mentioning two entities can describe their relationships in the KB. As DS can acquire large-scale data without human annotation, it has been widely adopted by recent neural relation extraction (NRE) models (Zeng et al., 2015; Lin et al., 2016). Although DS is both simple and effective in many cases, it inevitably introduces intolerable labeling noises. As Figure 1 shows, there are two types of error labels, false negatives and false positives. The reason for false negatives is that a sentence does describe two entities about a target relation, but the fact has not been covered by the KB yet. While for false positives, it is because not all sentences mentioning entity pairs actually express their relations in the KB. The noisy-labeling problem can become severe when the KB and text do not match well and as a resul"
P19-1139,D15-1075,0,0.0415293,"nington et al., 2014; Peters et al., 2017, 2018) and fine-tuning (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019) approaches, can capture rich language information from text and then benefit many NLP applications. BERT (Devlin et al., 2019), as one of the most recently proposed models, obtains the stateof-the-art results on various NLP applications by simple fine-tuning, including named entity recognition (Sang and De Meulder, 2003), question † mp is_a Introduction ∗ is_a answering (Rajpurkar et al., 2016; Zellers et al., 2018), natural language inference (Bowman et al., 2015), and text classification (Wang et al., 2018). Although pre-trained language representation models have achieved promising results and worked as a routine component in many NLP tasks, they neglect to incorporate knowledge information for language understanding. As shown in Figure 1, without knowing Blowin’ in the Wind and Chronicles: Volume One are song and book respectively, it is difficult to recognize the two occupations of Bob Dylan, i.e., songwriter and writer, on the entity typing task. Furthermore, it is nearly impossible to extract the fine-grained relations, such as composer and autho"
P19-1139,D18-1021,1,0.795851,"Missing"
P19-1139,P17-1149,0,0.0267538,"token embedding and entity embedding. After information fusion, it outputs new token embeddings and entity embeddings for the next layer. inference (Chen et al., 2018), knowledge acquisition (Han et al., 2018a), and dialog systems (Madotto et al., 2018). Hence, we argue that extra knowledge information can effectively benefit existing pre-training models. In fact, some work has attempted to joint representation learning of words and entities for effectively leveraging external KGs and achieved promising results (Wang et al., 2014; Toutanova et al., 2015; Han et al., 2016; Yamada et al., 2016; Cao et al., 2017, 2018). Sun et al. (2019) propose the knowledge masking strategy for masked language model to enhance language representation by knowledge 1 . In this paper, we further utilize both corpora and KGs to train an enhanced language representation model based on BERT. 3.1 We denote a token sequence as {w1 , . . . , wn } 2 , where n is the length of the token sequence. Meanwhile, we denote the entity sequence aligning to the given tokens as {e1 , . . . , em }, where m is the length of the entity sequence. Note that m is not equal to n in most cases, as not every token can be aligned to an entity in"
P19-1139,P18-1224,0,0.0426992,"low ··· wn(i 1) 1962 (i 1) e1 (i 1) e2 Entity Input Bob Dylan Blowin’ in the Wind Token Input Bob Dylan wrote Blowin’ in the Wind in 1962 (a) Model Achitecture (b) Aggregator Figure 2: The left part is the architecture of ERNIE. The right part is the aggregator for the mutual integration of the input of tokens and entities. Information fusion layer takes two kinds of input: one is the token embedding, and the other one is the concatenation of the token embedding and entity embedding. After information fusion, it outputs new token embeddings and entity embeddings for the next layer. inference (Chen et al., 2018), knowledge acquisition (Han et al., 2018a), and dialog systems (Madotto et al., 2018). Hence, we argue that extra knowledge information can effectively benefit existing pre-training models. In fact, some work has attempted to joint representation learning of words and entities for effectively leveraging external KGs and achieved promising results (Wang et al., 2014; Toutanova et al., 2015; Han et al., 2016; Yamada et al., 2016; Cao et al., 2017, 2018). Sun et al. (2019) propose the knowledge masking strategy for masked language model to enhance language representation by knowledge 1 . In this"
P19-1139,P18-1009,0,0.0439758,"to avoid overfitting, and keep the range of learning rate unchanged, i.e., batch size: 2048, number of epochs: 2, 3. As most datasets do not have entity annotations, we use TAGME (Ferragina and Scaiella, 2010) to extract the entity mentions in the sentences and link them to their corresponding entities in KGs. 4.3 Entity Typing Given an entity mention and its context, entity typing requires systems to label the entity mention with its respective semantic types. To evaluate performance on this task, we fine-tune ERNIE on two well-established datasets FIGER (Ling et al., 2015) and Open Entity (Choi et al., 2018). The training set of FIGER is labeled with distant supervision, and its test set is annotated by human. Open Entity is a completely manually-annotated dataset. The statistics of these two datasets are shown in Table 1. We compare our model with the following baseline models for entity typing: NFGEC. NFGEC is a hybrid model proposed by Shimaoka et al. (2016). NFGEC combines the representations of entity mention, context and extra hand-craft features as input, and is the stateof-the-art model on FIGER. As this paper focuses on comparing the general language representation abilities of various n"
P19-1139,N19-1423,0,0.626127,"e Blowin’ in the Wind in 1962, and wrote Chronicles: Volume One in 2004. Figure 1: An example of incorporating extra knowledge information for language understanding. The solid lines present the existing knowledge facts. The red dotted lines present the facts extracted from the sentence in red. The green dotdash lines present the facts extracted from the sentence in green. Pre-trained language representation models, including feature-based (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2017, 2018) and fine-tuning (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019) approaches, can capture rich language information from text and then benefit many NLP applications. BERT (Devlin et al., 2019), as one of the most recently proposed models, obtains the stateof-the-art results on various NLP applications by simple fine-tuning, including named entity recognition (Sang and De Meulder, 2003), question † mp is_a Introduction ∗ is_a answering (Rajpurkar et al., 2016; Zellers et al., 2018), natural language inference (Bowman et al., 2015), and text classification (Wang et al., 2018). Although pre-trained language representation models have achieved promising results"
P19-1139,I05-5002,0,0.0118207,"ovide more information for relation classification than the vanilla encoder CNN and RNN. And ERNIE outperforms BERT on both of the relation classification datasets, especially on the FewRel which has a much smaller training set. It demonstrates extra knowledge helps the model make full use of small training data, which is important for most NLP tasks as large-scale annotated data is unavailable. 4.5 GLUE The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018) is a collection of diverse natural language understanding tasks (Warstadt et al., 2018; Socher et al., 2013; Dolan and Brockett, 2005; Agirre et al., 2007; Williams et al., 2018; Rajpurkar et al., 2016; Dagan et al., 2006; Levesque et al., 2011), which is the main benchmark used in Devlin et al. (2019). To explore whether our knowledgeable module degenerates the performance on common NLP tasks, we evaluate ERNIE on 8 datasets of GLUE and compare it with BERT. In Table 6, we report the results of our evaluation submissions and those of BERT from the leaderboard. We notice that ERNIE is consistent with BERTBASE on big datasets like MNLI, QQP, QNLI, and SST-2. The results become more unstable on small datasets, that is, ERNIE"
P19-1139,D18-1247,1,0.937706,"ty Input Bob Dylan Blowin’ in the Wind Token Input Bob Dylan wrote Blowin’ in the Wind in 1962 (a) Model Achitecture (b) Aggregator Figure 2: The left part is the architecture of ERNIE. The right part is the aggregator for the mutual integration of the input of tokens and entities. Information fusion layer takes two kinds of input: one is the token embedding, and the other one is the concatenation of the token embedding and entity embedding. After information fusion, it outputs new token embeddings and entity embeddings for the next layer. inference (Chen et al., 2018), knowledge acquisition (Han et al., 2018a), and dialog systems (Madotto et al., 2018). Hence, we argue that extra knowledge information can effectively benefit existing pre-training models. In fact, some work has attempted to joint representation learning of words and entities for effectively leveraging external KGs and achieved promising results (Wang et al., 2014; Toutanova et al., 2015; Han et al., 2016; Yamada et al., 2016; Cao et al., 2017, 2018). Sun et al. (2019) propose the knowledge masking strategy for masked language model to enhance language representation by knowledge 1 . In this paper, we further utilize both corpora a"
P19-1139,D18-1514,1,0.936476,"ty Input Bob Dylan Blowin’ in the Wind Token Input Bob Dylan wrote Blowin’ in the Wind in 1962 (a) Model Achitecture (b) Aggregator Figure 2: The left part is the architecture of ERNIE. The right part is the aggregator for the mutual integration of the input of tokens and entities. Information fusion layer takes two kinds of input: one is the token embedding, and the other one is the concatenation of the token embedding and entity embedding. After information fusion, it outputs new token embeddings and entity embeddings for the next layer. inference (Chen et al., 2018), knowledge acquisition (Han et al., 2018a), and dialog systems (Madotto et al., 2018). Hence, we argue that extra knowledge information can effectively benefit existing pre-training models. In fact, some work has attempted to joint representation learning of words and entities for effectively leveraging external KGs and achieved promising results (Wang et al., 2014; Toutanova et al., 2015; Han et al., 2016; Yamada et al., 2016; Cao et al., 2017, 2018). Sun et al. (2019) propose the knowledge masking strategy for masked language model to enhance language representation by knowledge 1 . In this paper, we further utilize both corpora a"
P19-1139,P18-1031,0,0.153256,"olume One is_a Bob Dylan Writer Bob Dylan wrote Blowin’ in the Wind in 1962, and wrote Chronicles: Volume One in 2004. Figure 1: An example of incorporating extra knowledge information for language understanding. The solid lines present the existing knowledge facts. The red dotted lines present the facts extracted from the sentence in red. The green dotdash lines present the facts extracted from the sentence in green. Pre-trained language representation models, including feature-based (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2017, 2018) and fine-tuning (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019) approaches, can capture rich language information from text and then benefit many NLP applications. BERT (Devlin et al., 2019), as one of the most recently proposed models, obtains the stateof-the-art results on various NLP applications by simple fine-tuning, including named entity recognition (Sang and De Meulder, 2003), question † mp is_a Introduction ∗ is_a answering (Rajpurkar et al., 2016; Zellers et al., 2018), natural language inference (Bowman et al., 2015), and text classification (Wang et al., 2018). Although pre-trained language represent"
P19-1139,P16-1200,1,0.845415,"from each class for the training set, and sample 200 instances for the development and test respectively. There are 80 classes in FewRel, and there are 42 classes (including a special relation “no relation”) in TACRED. We compare our model with the following baseline models for relation classification: CNN. With a convolution layer, a max-pooling layer, and a non-linear activation layer, CNN gets the output sentence embedding, and then feeds it into a relation classifier. To better capture the position of head and tail entities, position embeddings are introduced into CNN (Zeng et al., 2015; Lin et al., 2016; Wu et al., 2017; Han et al., 2018b). PA-LSTM. Zhang et al. (2017) propose PALSTM introducing a position-aware attention mechanism over an LSTM network, which evaluates the relative contribution of each word in the sequence for the final sentence representation. C-GCN. Zhang et al. (2018) adopt the graph convolution operations to model dependency trees for relation classification. To encode the word order and reduce the side effect of errors in dependency parsing, Contextualized GCN (C-GCN) firstly uses Bi-LSTM to generate contextualized representations as input for GCN models. In addition to"
P19-1139,Q15-1023,0,0.119712,"Missing"
P19-1139,P18-1136,0,0.0322388,"oken Input Bob Dylan wrote Blowin’ in the Wind in 1962 (a) Model Achitecture (b) Aggregator Figure 2: The left part is the architecture of ERNIE. The right part is the aggregator for the mutual integration of the input of tokens and entities. Information fusion layer takes two kinds of input: one is the token embedding, and the other one is the concatenation of the token embedding and entity embedding. After information fusion, it outputs new token embeddings and entity embeddings for the next layer. inference (Chen et al., 2018), knowledge acquisition (Han et al., 2018a), and dialog systems (Madotto et al., 2018). Hence, we argue that extra knowledge information can effectively benefit existing pre-training models. In fact, some work has attempted to joint representation learning of words and entities for effectively leveraging external KGs and achieved promising results (Wang et al., 2014; Toutanova et al., 2015; Han et al., 2016; Yamada et al., 2016; Cao et al., 2017, 2018). Sun et al. (2019) propose the knowledge masking strategy for masked language model to enhance language representation by knowledge 1 . In this paper, we further utilize both corpora and KGs to train an enhanced language represen"
P19-1139,W03-0419,0,0.686759,"Missing"
P19-1139,W16-1313,0,0.0292746,"nd its context, entity typing requires systems to label the entity mention with its respective semantic types. To evaluate performance on this task, we fine-tune ERNIE on two well-established datasets FIGER (Ling et al., 2015) and Open Entity (Choi et al., 2018). The training set of FIGER is labeled with distant supervision, and its test set is annotated by human. Open Entity is a completely manually-annotated dataset. The statistics of these two datasets are shown in Table 1. We compare our model with the following baseline models for entity typing: NFGEC. NFGEC is a hybrid model proposed by Shimaoka et al. (2016). NFGEC combines the representations of entity mention, context and extra hand-craft features as input, and is the stateof-the-art model on FIGER. As this paper focuses on comparing the general language representation abilities of various neural models, we thus do not use the hand-craft features in this work. UFET. For Open Entity, we add a new hybrid model UFET (Choi et al., 2018) for comparison. UFET is proposed with the Open Entity dataset, which uses a Bi-LSTM for context representation instead of two Bi-LSTMs separated by entity mentions in NFGEC. Besides NFGEC and UFET, we also report th"
P19-1139,D13-1170,0,0.0050581,"anguage models can provide more information for relation classification than the vanilla encoder CNN and RNN. And ERNIE outperforms BERT on both of the relation classification datasets, especially on the FewRel which has a much smaller training set. It demonstrates extra knowledge helps the model make full use of small training data, which is important for most NLP tasks as large-scale annotated data is unavailable. 4.5 GLUE The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018) is a collection of diverse natural language understanding tasks (Warstadt et al., 2018; Socher et al., 2013; Dolan and Brockett, 2005; Agirre et al., 2007; Williams et al., 2018; Rajpurkar et al., 2016; Dagan et al., 2006; Levesque et al., 2011), which is the main benchmark used in Devlin et al. (2019). To explore whether our knowledgeable module degenerates the performance on common NLP tasks, we evaluate ERNIE on 8 datasets of GLUE and compare it with BERT. In Table 6, we report the results of our evaluation submissions and those of BERT from the leaderboard. We notice that ERNIE is consistent with BERTBASE on big datasets like MNLI, QQP, QNLI, and SST-2. The results become more unstable on small"
P19-1139,speer-havasi-2012-representing,0,0.0818577,"Missing"
P19-1139,D15-1174,0,0.032581,"token embedding, and the other one is the concatenation of the token embedding and entity embedding. After information fusion, it outputs new token embeddings and entity embeddings for the next layer. inference (Chen et al., 2018), knowledge acquisition (Han et al., 2018a), and dialog systems (Madotto et al., 2018). Hence, we argue that extra knowledge information can effectively benefit existing pre-training models. In fact, some work has attempted to joint representation learning of words and entities for effectively leveraging external KGs and achieved promising results (Wang et al., 2014; Toutanova et al., 2015; Han et al., 2016; Yamada et al., 2016; Cao et al., 2017, 2018). Sun et al. (2019) propose the knowledge masking strategy for masked language model to enhance language representation by knowledge 1 . In this paper, we further utilize both corpora and KGs to train an enhanced language representation model based on BERT. 3.1 We denote a token sequence as {w1 , . . . , wn } 2 , where n is the length of the token sequence. Meanwhile, we denote the entity sequence aligning to the given tokens as {e1 , . . . , em }, where m is the length of the entity sequence. Note that m is not equal to n in most"
P19-1139,P18-1076,0,0.0381481,"T). Radford et al. (2018) propose a generative pre-trained Transformer (Vaswani et al., 2017) (GPT) to learn language representations. Devlin et al. (2019) propose a deep bidirectional model with multiplelayer Transformers (BERT), which achieves the state-of-the-art results for various NLP tasks. Though both feature-based and fine-tuning language representation models have achieved great success, they ignore the incorporation of knowledge information. As demonstrated in recent work, injecting extra knowledge information can significantly enhance original models, such as reading comprehension (Mihaylov and Frank, 2018; Zhong et al., 2018), machine translation (Zaremoodi et al., 2018), natural language 1442 Token Output (i) Token Output Aggregator Information Fusion (i) e1 Entity Output e2 (i) (i) (i) w2 w1 w3 ··· wn(i) e1 ··· w˜n(i) e˜1 (i) e2 (i) e˜2 (i) Entity Output Aggregator K-Encoder Mx Information Fusion Multi-Head Attention Multi-Head Attention (i) (i) e˜2 e˜1 Feed Forward (i) (i) (i) w˜3 w˜2 w˜1 Transformer T-Encoder Nx (i) w4 (i) w˜4 (i) Entity Input Multi-Head Attention Multi-Head Attention Multi-Head Attention (i 1) Token Input w1 bob (i 1) w2 dylan (i 1) w3 wrote (i 1) w4 blow ··· wn(i 1) 1962"
P19-1139,P10-1040,0,0.0598698,"cific NLP tasks. These pre-training approaches can be divided into two classes, i.e., feature-based approaches and finetuning approaches. The early work (Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014) focuses on adopting feature-based approaches to transform words into distributed representations. As these pre-trained word representations capture syntactic and semantic information in textual corpora, they are often used as input embeddings and initialization parameters for various NLP models, and offer significant improvements over random initialization parameters (Turian et al., 2010). Since these word-level models often suffer from the word polysemy, Peters et al. (2018) further adopt the sequence-level model (ELMo) to capture complex word features across different linguistic contexts and use ELMo to generate context-aware word embeddings. Different from the above-mentioned featurebased language approaches only using the pretrained language representations as input features, Dai and Le (2015) train auto-encoders on unlabeled text, and then use the pre-trained model architecture and parameters as a starting point for other specific NLP models. Inspired by Dai and Le (2015)"
P19-1139,D14-1162,0,0.102807,"rresponding author: Z.Liu(liuzy@tsinghua.edu.cn) os er Song Book r tho au Chronicles: Volume One is_a Bob Dylan Writer Bob Dylan wrote Blowin’ in the Wind in 1962, and wrote Chronicles: Volume One in 2004. Figure 1: An example of incorporating extra knowledge information for language understanding. The solid lines present the existing knowledge facts. The red dotted lines present the facts extracted from the sentence in red. The green dotdash lines present the facts extracted from the sentence in green. Pre-trained language representation models, including feature-based (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2017, 2018) and fine-tuning (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019) approaches, can capture rich language information from text and then benefit many NLP applications. BERT (Devlin et al., 2019), as one of the most recently proposed models, obtains the stateof-the-art results on various NLP applications by simple fine-tuning, including named entity recognition (Sang and De Meulder, 2003), question † mp is_a Introduction ∗ is_a answering (Rajpurkar et al., 2016; Zellers et al., 2018), natural language inference (Bowman et al., 2015)"
P19-1139,P17-1161,0,0.0313441,"(liuzy@tsinghua.edu.cn) os er Song Book r tho au Chronicles: Volume One is_a Bob Dylan Writer Bob Dylan wrote Blowin’ in the Wind in 1962, and wrote Chronicles: Volume One in 2004. Figure 1: An example of incorporating extra knowledge information for language understanding. The solid lines present the existing knowledge facts. The red dotted lines present the facts extracted from the sentence in red. The green dotdash lines present the facts extracted from the sentence in green. Pre-trained language representation models, including feature-based (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2017, 2018) and fine-tuning (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019) approaches, can capture rich language information from text and then benefit many NLP applications. BERT (Devlin et al., 2019), as one of the most recently proposed models, obtains the stateof-the-art results on various NLP applications by simple fine-tuning, including named entity recognition (Sang and De Meulder, 2003), question † mp is_a Introduction ∗ is_a answering (Rajpurkar et al., 2016; Zellers et al., 2018), natural language inference (Bowman et al., 2015), and text classifica"
P19-1139,N18-1202,0,0.102763,"ture-based approaches and finetuning approaches. The early work (Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014) focuses on adopting feature-based approaches to transform words into distributed representations. As these pre-trained word representations capture syntactic and semantic information in textual corpora, they are often used as input embeddings and initialization parameters for various NLP models, and offer significant improvements over random initialization parameters (Turian et al., 2010). Since these word-level models often suffer from the word polysemy, Peters et al. (2018) further adopt the sequence-level model (ELMo) to capture complex word features across different linguistic contexts and use ELMo to generate context-aware word embeddings. Different from the above-mentioned featurebased language approaches only using the pretrained language representations as input features, Dai and Le (2015) train auto-encoders on unlabeled text, and then use the pre-trained model architecture and parameters as a starting point for other specific NLP models. Inspired by Dai and Le (2015), more pre-trained language representation models for fine-tuning have been proposed. How"
P19-1139,D16-1264,0,0.274955,"e representation models, including feature-based (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2017, 2018) and fine-tuning (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019) approaches, can capture rich language information from text and then benefit many NLP applications. BERT (Devlin et al., 2019), as one of the most recently proposed models, obtains the stateof-the-art results on various NLP applications by simple fine-tuning, including named entity recognition (Sang and De Meulder, 2003), question † mp is_a Introduction ∗ is_a answering (Rajpurkar et al., 2016; Zellers et al., 2018), natural language inference (Bowman et al., 2015), and text classification (Wang et al., 2018). Although pre-trained language representation models have achieved promising results and worked as a routine component in many NLP tasks, they neglect to incorporate knowledge information for language understanding. As shown in Figure 1, without knowing Blowin’ in the Wind and Chronicles: Volume One are song and book respectively, it is difficult to recognize the two occupations of Bob Dylan, i.e., songwriter and writer, on the entity typing task. Furthermore, it is nearly imp"
P19-1139,W18-5446,0,0.172541,") and fine-tuning (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019) approaches, can capture rich language information from text and then benefit many NLP applications. BERT (Devlin et al., 2019), as one of the most recently proposed models, obtains the stateof-the-art results on various NLP applications by simple fine-tuning, including named entity recognition (Sang and De Meulder, 2003), question † mp is_a Introduction ∗ is_a answering (Rajpurkar et al., 2016; Zellers et al., 2018), natural language inference (Bowman et al., 2015), and text classification (Wang et al., 2018). Although pre-trained language representation models have achieved promising results and worked as a routine component in many NLP tasks, they neglect to incorporate knowledge information for language understanding. As shown in Figure 1, without knowing Blowin’ in the Wind and Chronicles: Volume One are song and book respectively, it is difficult to recognize the two occupations of Bob Dylan, i.e., songwriter and writer, on the entity typing task. Furthermore, it is nearly impossible to extract the fine-grained relations, such as composer and author on the relation classification task. For th"
P19-1139,D14-1167,0,0.0268165,"input: one is the token embedding, and the other one is the concatenation of the token embedding and entity embedding. After information fusion, it outputs new token embeddings and entity embeddings for the next layer. inference (Chen et al., 2018), knowledge acquisition (Han et al., 2018a), and dialog systems (Madotto et al., 2018). Hence, we argue that extra knowledge information can effectively benefit existing pre-training models. In fact, some work has attempted to joint representation learning of words and entities for effectively leveraging external KGs and achieved promising results (Wang et al., 2014; Toutanova et al., 2015; Han et al., 2016; Yamada et al., 2016; Cao et al., 2017, 2018). Sun et al. (2019) propose the knowledge masking strategy for masked language model to enhance language representation by knowledge 1 . In this paper, we further utilize both corpora and KGs to train an enhanced language representation model based on BERT. 3.1 We denote a token sequence as {w1 , . . . , wn } 2 , where n is the length of the token sequence. Meanwhile, we denote the entity sequence aligning to the given tokens as {e1 , . . . , em }, where m is the length of the entity sequence. Note that m i"
P19-1139,N18-1101,0,0.0178587,"ion than the vanilla encoder CNN and RNN. And ERNIE outperforms BERT on both of the relation classification datasets, especially on the FewRel which has a much smaller training set. It demonstrates extra knowledge helps the model make full use of small training data, which is important for most NLP tasks as large-scale annotated data is unavailable. 4.5 GLUE The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018) is a collection of diverse natural language understanding tasks (Warstadt et al., 2018; Socher et al., 2013; Dolan and Brockett, 2005; Agirre et al., 2007; Williams et al., 2018; Rajpurkar et al., 2016; Dagan et al., 2006; Levesque et al., 2011), which is the main benchmark used in Devlin et al. (2019). To explore whether our knowledgeable module degenerates the performance on common NLP tasks, we evaluate ERNIE on 8 datasets of GLUE and compare it with BERT. In Table 6, we report the results of our evaluation submissions and those of BERT from the leaderboard. We notice that ERNIE is consistent with BERTBASE on big datasets like MNLI, QQP, QNLI, and SST-2. The results become more unstable on small datasets, that is, ERNIE is better on CoLA and RTE, but worse on STS-"
P19-1139,D17-1187,0,0.0321162,"or the training set, and sample 200 instances for the development and test respectively. There are 80 classes in FewRel, and there are 42 classes (including a special relation “no relation”) in TACRED. We compare our model with the following baseline models for relation classification: CNN. With a convolution layer, a max-pooling layer, and a non-linear activation layer, CNN gets the output sentence embedding, and then feeds it into a relation classifier. To better capture the position of head and tail entities, position embeddings are introduced into CNN (Zeng et al., 2015; Lin et al., 2016; Wu et al., 2017; Han et al., 2018b). PA-LSTM. Zhang et al. (2017) propose PALSTM introducing a position-aware attention mechanism over an LSTM network, which evaluates the relative contribution of each word in the sequence for the final sentence representation. C-GCN. Zhang et al. (2018) adopt the graph convolution operations to model dependency trees for relation classification. To encode the word order and reduce the side effect of errors in dependency parsing, Contextualized GCN (C-GCN) firstly uses Bi-LSTM to generate contextualized representations as input for GCN models. In addition to these three base"
P19-1139,D18-1121,1,0.825023,"Missing"
P19-1139,E17-1055,0,0.0423233,"Missing"
P19-1139,K16-1025,0,0.0384676,"concatenation of the token embedding and entity embedding. After information fusion, it outputs new token embeddings and entity embeddings for the next layer. inference (Chen et al., 2018), knowledge acquisition (Han et al., 2018a), and dialog systems (Madotto et al., 2018). Hence, we argue that extra knowledge information can effectively benefit existing pre-training models. In fact, some work has attempted to joint representation learning of words and entities for effectively leveraging external KGs and achieved promising results (Wang et al., 2014; Toutanova et al., 2015; Han et al., 2016; Yamada et al., 2016; Cao et al., 2017, 2018). Sun et al. (2019) propose the knowledge masking strategy for masked language model to enhance language representation by knowledge 1 . In this paper, we further utilize both corpora and KGs to train an enhanced language representation model based on BERT. 3.1 We denote a token sequence as {w1 , . . . , wn } 2 , where n is the length of the token sequence. Meanwhile, we denote the entity sequence aligning to the given tokens as {e1 , . . . , em }, where m is the length of the entity sequence. Note that m is not equal to n in most cases, as not every token can be align"
P19-1139,P18-2104,0,0.0250317,"er (Vaswani et al., 2017) (GPT) to learn language representations. Devlin et al. (2019) propose a deep bidirectional model with multiplelayer Transformers (BERT), which achieves the state-of-the-art results for various NLP tasks. Though both feature-based and fine-tuning language representation models have achieved great success, they ignore the incorporation of knowledge information. As demonstrated in recent work, injecting extra knowledge information can significantly enhance original models, such as reading comprehension (Mihaylov and Frank, 2018; Zhong et al., 2018), machine translation (Zaremoodi et al., 2018), natural language 1442 Token Output (i) Token Output Aggregator Information Fusion (i) e1 Entity Output e2 (i) (i) (i) w2 w1 w3 ··· wn(i) e1 ··· w˜n(i) e˜1 (i) e2 (i) e˜2 (i) Entity Output Aggregator K-Encoder Mx Information Fusion Multi-Head Attention Multi-Head Attention (i) (i) e˜2 e˜1 Feed Forward (i) (i) (i) w˜3 w˜2 w˜1 Transformer T-Encoder Nx (i) w4 (i) w˜4 (i) Entity Input Multi-Head Attention Multi-Head Attention Multi-Head Attention (i 1) Token Input w1 bob (i 1) w2 dylan (i 1) w3 wrote (i 1) w4 blow ··· wn(i 1) 1962 (i 1) e1 (i 1) e2 Entity Input Bob Dylan Blowin’ in the Wind Token"
P19-1139,D18-1009,0,0.0262803,"including feature-based (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2017, 2018) and fine-tuning (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019) approaches, can capture rich language information from text and then benefit many NLP applications. BERT (Devlin et al., 2019), as one of the most recently proposed models, obtains the stateof-the-art results on various NLP applications by simple fine-tuning, including named entity recognition (Sang and De Meulder, 2003), question † mp is_a Introduction ∗ is_a answering (Rajpurkar et al., 2016; Zellers et al., 2018), natural language inference (Bowman et al., 2015), and text classification (Wang et al., 2018). Although pre-trained language representation models have achieved promising results and worked as a routine component in many NLP tasks, they neglect to incorporate knowledge information for language understanding. As shown in Figure 1, without knowing Blowin’ in the Wind and Chronicles: Volume One are song and book respectively, it is difficult to recognize the two occupations of Bob Dylan, i.e., songwriter and writer, on the entity typing task. Furthermore, it is nearly impossible to extract the"
P19-1139,D15-1203,0,0.231948,"rom Devlin et al. (2019). The overall pre-training loss is the sum of the dEA, MLM and NSP loss. to fine-tune ERNIE for relation classification is to apply the pooling layer to the final output embeddings of the given entity mentions, and represent the given entity pair with the concatenation of their mention embeddings for classification. In this paper, we design another method, which modifies the input token sequence by adding two mark tokens to highlight entity mentions. These extra mark tokens play a similar role like position embeddings in the conventional relation classification models (Zeng et al., 2015). Then, we also take the [CLS] token embedding for classification. Note that we design different tokens [HD] and [TL] for head entities and tail entities respectively. The specific fine-tuning procedure for entity typing is a simplified version of relation classification. As previous typing models make full use of both context embeddings and entity mention embeddings (Shimaoka et al., 2016; Yaghoobzadeh and Sch¨utze, 2017; Xin et al., 2018), we argue that the modified input sequence with the mention mark token [ENT] can guide ERNIE to combine both context information and entity mention informa"
P19-1139,D18-1244,0,0.0362932,"n classification: CNN. With a convolution layer, a max-pooling layer, and a non-linear activation layer, CNN gets the output sentence embedding, and then feeds it into a relation classifier. To better capture the position of head and tail entities, position embeddings are introduced into CNN (Zeng et al., 2015; Lin et al., 2016; Wu et al., 2017; Han et al., 2018b). PA-LSTM. Zhang et al. (2017) propose PALSTM introducing a position-aware attention mechanism over an LSTM network, which evaluates the relative contribution of each word in the sequence for the final sentence representation. C-GCN. Zhang et al. (2018) adopt the graph convolution operations to model dependency trees for relation classification. To encode the word order and reduce the side effect of errors in dependency parsing, Contextualized GCN (C-GCN) firstly uses Bi-LSTM to generate contextualized representations as input for GCN models. In addition to these three baselines, we also finetune BERT with the same input format introduced in Section 3.5 for fair comparison. 1447 Model MNLI-(m/mm) 392k QQP 363k QNLI 104k SST-2 67k BERTBASE 84.6/83.4 71.2 - 93.5 ERNIE 84.0/83.2 71.2 91.3 93.5 Model CoLA 8.5k STS-B 5.7k MRPC 3.5k RTE 2.5k BERTB"
P19-1139,D17-1004,0,0.0498373,"labels more precisely. In summary, ERNIE effectively reduces the noisy label challenge in FIGER, which is a widely-used distantly supervised entity typing dataset, by injecting the information from KGs. Besides, ERNIE also outperforms the baselines on Open Entity which has gold annotations. Relation Classification Relation classification aims to determine the correct relation between two entities in a given sentence, which is an important knowledge-driven NLP task. To evaluate performance on this task, we fine-tune ERNIE on two well-established datasets FewRel (Han et al., 2018c) and TACRED (Zhang et al., 2017). The statistics of these two datasets are shown in Table 4. As the original experimental setting of FewRel is few-shot learning, we rearrange the FewRel dataset for the common relation classification setting. Specifically, we sample 100 instances from each class for the training set, and sample 200 instances for the development and test respectively. There are 80 classes in FewRel, and there are 42 classes (including a special relation “no relation”) in TACRED. We compare our model with the following baseline models for relation classification: CNN. With a convolution layer, a max-pooling lay"
P19-1139,W07-1401,0,\N,Missing
P19-1140,C18-1057,1,0.816626,"re 1: Illustration of the structural differences (dashed lines and ellipse) between different KGs. Introduction Knowledge Graphs (KGs) store the world knowledge in the form of directed graphs, where nodes denote entities and edges are their relations. Since it was proposed, many KGs are constructed (e.g., YAGO (Rebele et al., 2016)) to provide structural knowledge for different applications and languages. These KGs usually contain complementary contents, attracting researchers to integrate them into a unified KG, which shall benefit many knowledge driven tasks, such as information extraction (Cao et al., 2018a) and recommendation (Wang et al., 2018a). It is non-trivial to align different KGs due to their distinct surface forms, which makes the symbolic based methods (Suchanek et al., 2011) not always effective. Instead, recent work utilizes general KG embedding methods (e.g., TransE (Bordes et al., 2013)) and align equivalent entities into a unified vector space based on a few seed alignments (Chen et al., 2017; Sun et al., 2017; Zhu et al., 2017; Chen et al., 2018; Sun et al., 2018; Wang et al., 2018b). The assumption is that entities and their counterparts in different KGs should have similar st"
P19-1140,D18-1021,1,0.856141,"re 1: Illustration of the structural differences (dashed lines and ellipse) between different KGs. Introduction Knowledge Graphs (KGs) store the world knowledge in the form of directed graphs, where nodes denote entities and edges are their relations. Since it was proposed, many KGs are constructed (e.g., YAGO (Rebele et al., 2016)) to provide structural knowledge for different applications and languages. These KGs usually contain complementary contents, attracting researchers to integrate them into a unified KG, which shall benefit many knowledge driven tasks, such as information extraction (Cao et al., 2018a) and recommendation (Wang et al., 2018a). It is non-trivial to align different KGs due to their distinct surface forms, which makes the symbolic based methods (Suchanek et al., 2011) not always effective. Instead, recent work utilizes general KG embedding methods (e.g., TransE (Bordes et al., 2013)) and align equivalent entities into a unified vector space based on a few seed alignments (Chen et al., 2017; Sun et al., 2017; Zhu et al., 2017; Chen et al., 2018; Sun et al., 2018; Wang et al., 2018b). The assumption is that entities and their counterparts in different KGs should have similar st"
P19-1140,P17-1149,1,0.846504,"rom YAGO3 to DBpedia are grounded to 92,923 new ground rule triples, which is shocking and not informative. Further investigation finds that the rule (a, team, b) ⇒ (a, affiliation, b) alone contributes 92,743 ground rule triples. Although the rule is logically correct, it is suspicious such a rule that establishes similar relations between entities would benefit entity alignment. We will deal with such noise in future. 6 Related Work Merging different KGs into a unified one has attracted much attention since it shall benefit many Knowledge-driven applications, such as information extraction (Cao et al., 2017a, 2018b), question answering (Zhang et al., 2015) and recommendation (Cao et al., 2019). Early approaches for entity alignment leverage various features to overcome the heterogeneity between KGs, such as machine translation and external lexicons (Suchanek et al., 2011; Wang et al., 2013). Following the success of KG representation learning, recent work embeds entities in different KGs into a low-dimensional vector space with the help of seed alignments (Chen et al., 2017). However, the limited seeds and structural differences take great negative impacts on the quality of KG embeddings, which"
P19-1140,D15-1077,1,0.879704,"Missing"
P19-1140,I17-1024,1,0.8572,"rom YAGO3 to DBpedia are grounded to 92,923 new ground rule triples, which is shocking and not informative. Further investigation finds that the rule (a, team, b) ⇒ (a, affiliation, b) alone contributes 92,743 ground rule triples. Although the rule is logically correct, it is suspicious such a rule that establishes similar relations between entities would benefit entity alignment. We will deal with such noise in future. 6 Related Work Merging different KGs into a unified one has attracted much attention since it shall benefit many Knowledge-driven applications, such as information extraction (Cao et al., 2017a, 2018b), question answering (Zhang et al., 2015) and recommendation (Cao et al., 2019). Early approaches for entity alignment leverage various features to overcome the heterogeneity between KGs, such as machine translation and external lexicons (Suchanek et al., 2011; Wang et al., 2013). Following the success of KG representation learning, recent work embeds entities in different KGs into a low-dimensional vector space with the help of seed alignments (Chen et al., 2017). However, the limited seeds and structural differences take great negative impacts on the quality of KG embeddings, which"
P19-1140,D18-1032,0,0.1473,"fferences (dashed lines and ellipse) between different KGs. Introduction Knowledge Graphs (KGs) store the world knowledge in the form of directed graphs, where nodes denote entities and edges are their relations. Since it was proposed, many KGs are constructed (e.g., YAGO (Rebele et al., 2016)) to provide structural knowledge for different applications and languages. These KGs usually contain complementary contents, attracting researchers to integrate them into a unified KG, which shall benefit many knowledge driven tasks, such as information extraction (Cao et al., 2018a) and recommendation (Wang et al., 2018a). It is non-trivial to align different KGs due to their distinct surface forms, which makes the symbolic based methods (Suchanek et al., 2011) not always effective. Instead, recent work utilizes general KG embedding methods (e.g., TransE (Bordes et al., 2013)) and align equivalent entities into a unified vector space based on a few seed alignments (Chen et al., 2017; Sun et al., 2017; Zhu et al., 2017; Chen et al., 2018; Sun et al., 2018; Wang et al., 2018b). The assumption is that entities and their counterparts in different KGs should have similar structures and thus similar embeddings. Ho"
P19-1140,D16-1019,0,0.0312459,"ly, and γ1 &gt; 0 and γ2 &gt; 0 are margin hyper-parameters separating positive and negative entity and relation alignments. During the experiments, by calculating cosine similarity, we select 25 entities closest to the corresponding entity in the same KG as negative samples (Sun et al., 2018). Negative samples will be re-calculated every 5 epochs. 1456 Rule Knowledge Constraints Since we have changed the KG structure by adding new triplets (i.e., grounded rules), we also introduce the triplet loss to hold the grounded rules as valid in the unified vector space. Taking KG G as an example, following Guo et al. (2016), we define the loss function as follows: Lr = X X [γr − I(g + ) + I(g − )]+ g + ∈G(K)g − ∈G − (K) + X X are extracted from multilingual DBpedia and include 15,000 entity pairs as seed alignments. DWY100K consists of two large-scale crossresource datasets: DWY-WD (DBpedia to Wikidata) and DWY-YG (DBpedia to YAGO3). Each dataset includes 100,000 alignments of entities in advance. As for the seed alignments of relations, we employ the official relation alignment list published by DBpedia for DWY100K. As for DWY-YG, we manually align the relations because there are only a small set of relation ty"
P19-1227,P17-1171,0,0.241113,"guages, while the performance of cross-lingual OpenQA is still much lower than that of English. Our analysis indicates that the performance of cross-lingual OpenQA is related to not only how similar the target language and English are, but also how difficult the question set of the target language is. The XQA dataset is publicly available at http://github.com/thunlp/XQA. 1 Introduction In recent years, open-domain question answering (OpenQA), which aims to answer open-domain questions with a large-scale text corpus, has attracted lots of attention from natural language processing researchers. Chen et al. (2017) proposed DrQA model, which used a text retriever to obtain relevant documents from Wikipedia, and further applied a trained reading comprehension model ∗ Corresponding author: Maosong Sun to extract the answer from the retrieved documents. Moreover, researchers have introduced more sophisticated models, which either aggregate all informative evidence (Lin et al., 2018; Wang et al., 2018b) or filter out those noisy retrieved text (Clark and Gardner, 2018; Choi et al., 2017; Wang et al., 2018a) to better predict the answers for open-domain questions. Benefiting from the power of neural networks"
P19-1227,P17-1020,0,0.170533,"n questions with a large-scale text corpus, has attracted lots of attention from natural language processing researchers. Chen et al. (2017) proposed DrQA model, which used a text retriever to obtain relevant documents from Wikipedia, and further applied a trained reading comprehension model ∗ Corresponding author: Maosong Sun to extract the answer from the retrieved documents. Moreover, researchers have introduced more sophisticated models, which either aggregate all informative evidence (Lin et al., 2018; Wang et al., 2018b) or filter out those noisy retrieved text (Clark and Gardner, 2018; Choi et al., 2017; Wang et al., 2018a) to better predict the answers for open-domain questions. Benefiting from the power of neural networks, these models have achieved remarkable results in OpenQA. However, these neural-based models must be trained with a huge volume of labeled data. Collecting and labeling large-size training data for each language is often intractable and unrealistic, especially for those low-resource languages. In this case, it is impossible to directly apply existing OpenQA models to many different languages. To address this problem, an alternative approach is to build a cross-lingual Ope"
P19-1227,P18-1078,0,0.399956,"aims to answer open-domain questions with a large-scale text corpus, has attracted lots of attention from natural language processing researchers. Chen et al. (2017) proposed DrQA model, which used a text retriever to obtain relevant documents from Wikipedia, and further applied a trained reading comprehension model ∗ Corresponding author: Maosong Sun to extract the answer from the retrieved documents. Moreover, researchers have introduced more sophisticated models, which either aggregate all informative evidence (Lin et al., 2018; Wang et al., 2018b) or filter out those noisy retrieved text (Clark and Gardner, 2018; Choi et al., 2017; Wang et al., 2018a) to better predict the answers for open-domain questions. Benefiting from the power of neural networks, these models have achieved remarkable results in OpenQA. However, these neural-based models must be trained with a huge volume of labeled data. Collecting and labeling large-size training data for each language is often intractable and unrealistic, especially for those low-resource languages. In this case, it is impossible to directly apply existing OpenQA models to many different languages. To address this problem, an alternative approach is to build"
P19-1227,D18-1269,0,0.190208,"ly apply existing OpenQA models to many different languages. To address this problem, an alternative approach is to build a cross-lingual OpenQA system. It is trained on data in one high-resource source language such as English, and predicts answers for open-domain questions in other target languages. In fact, cross-lingual OpenQA can be viewed as a particular task of cross-lingual language understanding (XLU). Recently, XLU has been applied to many natural language processing tasks such as cross-lingual document classification (Schwenk and Li, 2018), cross-lingual natural language inference (Conneau et al., 2018b), and machine translation (Lample et al., 2018). Most cross-lingual models focus on word or sentence level understanding, while the interaction between questions and documents as well as the overall understanding of the documents are essential to OpenQA. To the best of our knowledge, there is still no dataset for cross-lingual OpenQA. In this paper, we introduce a cross-lingual OpenQA dataset called XQA. It consists of a training set in English, and development and test sets in English, French, German, Portuguese, Polish, 2358 Proceedings of the 57th Annual Meeting of the Association for Com"
P19-1227,P17-1055,0,0.0333341,"h the hope that this could contribute to the research of cross-lingual OpenQA and overall cross-lingual language understanding. 2 2.1 Related Work Open-domain Question Answering OpenQA, first proposed by Green et al. (1986), aims to answer an open-domain question by utilizing external resources. In the past years, most work in this area has focused on using documents (Voorhees et al., 1999), online webpages (Kwok et al., 2001), and structured knowledge graphs (Bordes et al., 2015). Recently, with the advancement of reading comprehension technique (Chen 2359 et al., 2016; Dhingra et al., 2017; Cui et al., 2017), Chen et al. (2017) utilized both the information retrieval and reading comprehension techniques to answer open-domain questions. However, it usually suffers from the noise problem since the data is constructed under the distant supervision assumption. Hence researchers have made various attempts to alleviate the noise problem in OpenQA. Wang et al. (2018a) and Choi et al. (2017) performed paragraph selection before extracting answer of the question. Min et al. (2018) proposed to select a minimal set of sentences with sufficient information to answer the questions, while Lin et al. (2018) and"
P19-1227,N19-1423,0,0.63948,"nglish questionanswer pairs along with relevant documents. The development and test sets contain a total amount of 17, 358 and 16, 973 question-answer pairs respectively. All questions are naturally produced by native speakers, and potentially reflect cultural differences in different languages. Moreover, we build several baseline systems that use the information of multilingual data from publicly available corpora for cross-lingual OpenQA, including two translation-based methods that translate training data and test data respectively and one zero-shot cross-lingual method (multilingual BERT (Devlin et al., 2019)). We evaluate the performance of the proposed baselines in terms of text retrieval and reading comprehension for different target languages on the XQA dataset. The experimental results demonstrate that there is a gap between the performance in English and that in cross-lingual setting. The multilingual BERT model achieves the best performance in almost all target languages, while translation-based methods suffer from the problem of translating name entities. We show that the performance on the XQA dataset depends on not only how similar the target language and English are, but also how diffic"
P19-1227,P17-1168,0,0.0200036,"ine systems online with the hope that this could contribute to the research of cross-lingual OpenQA and overall cross-lingual language understanding. 2 2.1 Related Work Open-domain Question Answering OpenQA, first proposed by Green et al. (1986), aims to answer an open-domain question by utilizing external resources. In the past years, most work in this area has focused on using documents (Voorhees et al., 1999), online webpages (Kwok et al., 2001), and structured knowledge graphs (Bordes et al., 2015). Recently, with the advancement of reading comprehension technique (Chen 2359 et al., 2016; Dhingra et al., 2017; Cui et al., 2017), Chen et al. (2017) utilized both the information retrieval and reading comprehension techniques to answer open-domain questions. However, it usually suffers from the noise problem since the data is constructed under the distant supervision assumption. Hence researchers have made various attempts to alleviate the noise problem in OpenQA. Wang et al. (2018a) and Choi et al. (2017) performed paragraph selection before extracting answer of the question. Min et al. (2018) proposed to select a minimal set of sentences with sufficient information to answer the questions, while Li"
P19-1227,W18-2413,0,0.0219766,"lated as “Fuyang Palace” in the question, while correctly translated in the retrieved document. In addition, as we can see from the underlined parts, highly similar expressions in the question and the retrieved document are translated into largely different ones. Compared to other words or phrases which occur more frequently in the training corpus, name entities are more flexible and various, and thus have worse translation results from prevailing Neural Machine Translation systems (Li et al., 2018). While some work has focused on solving this problem (Hassan et al., 2007; Jiang et al., 2007; Grundkiewicz and Heafield, 2018; Li et al., 2018), it remains largely underresearched. With a translation system that handles name entities better, we can potentially obtain better results from translation-based methods. 6.3 Zero-shot Cross-lingual Method Trained on pure English data without the involvement of machine translation systems, much effort has been saved using zero-shot cross-lingual methods. Moreover, a single model could be applied directly to various languages. Thus, compared to Origin Question: &lt;Query&gt;位于汉长安城外西南侧，与未央宫 之间曾有跨越城墙的复道相连？ Retrieved Text: ...在长安城外修建了建章宫...并且与未 央宫之间有跨越宫墙和城墙的复道相通... Answer: 建章宫 Transla"
P19-1227,C12-1089,0,0.0596235,"ith vast volumes of labeled data, and cannot be easily extended to the cross-lingual scenario. 2.2 Cross-lingual Language Understanding Recent years, plenty of work has focused on multilingual word representation learning, including learning from parallel corpus (Gouws et al., 2015; Luong et al., 2015), with a bilingual dictionary (Zhang et al., 2016; Artetxe et al., 2018), and even in a fully unsupervised manner (Conneau et al., 2018a). These multilingual word representation models could be easily extended to multilingual sentence representation by averaging the representations of all words (Klementiev et al., 2012). Nevertheless, this method does not take into account the structure information of sentences. To address this issue, much effort has been devoted to using the context vector of NMT system as multilingual sentence representation (Schwenk and Douze, 2017; Espana-Bonet et al., 2017). Recently, Artetxe and Schwenk (2018) proposed to utilize a single encoder to learn joint multilingual sentence representations for 93 languages. Besides, Devlin et al. (2019) also released a multilingual version of BERT which encoded over 100 languages with a unified encoder. These models have shown their effectiven"
P19-1227,P18-1160,0,0.019114,"t al., 2015). Recently, with the advancement of reading comprehension technique (Chen 2359 et al., 2016; Dhingra et al., 2017; Cui et al., 2017), Chen et al. (2017) utilized both the information retrieval and reading comprehension techniques to answer open-domain questions. However, it usually suffers from the noise problem since the data is constructed under the distant supervision assumption. Hence researchers have made various attempts to alleviate the noise problem in OpenQA. Wang et al. (2018a) and Choi et al. (2017) performed paragraph selection before extracting answer of the question. Min et al. (2018) proposed to select a minimal set of sentences with sufficient information to answer the questions, while Lin et al. (2018) and Wang et al. (2018b) took all informative paragraphs into consideration by aggregating evidence in multiple paragraphs. Moreover, Clark and Gardner (2018) applied a shared-normalization learning objective on sampling paragraphs. All the models mentioned above were only verified in a single language (usually in English) with vast volumes of labeled data, and cannot be easily extended to the cross-lingual scenario. 2.2 Cross-lingual Language Understanding Recent years, p"
P19-1227,N19-1380,0,0.035498,"tence representation (Schwenk and Douze, 2017; Espana-Bonet et al., 2017). Recently, Artetxe and Schwenk (2018) proposed to utilize a single encoder to learn joint multilingual sentence representations for 93 languages. Besides, Devlin et al. (2019) also released a multilingual version of BERT which encoded over 100 languages with a unified encoder. These models have shown their effectiveness in several cross-lingual NLP tasks such as document classification (Klementiev et al., 2012), textual similarity (Cer et al., 2017), natural language inference (Conneau et al., 2018b), and dialog system (Schuster et al., 2019). However, there is still no existing benchmark for cross-lingual OpenQA. In addition, another line of research attempts to answer questions in one language using documents in other languages (Magnini et al., 2004; Vallin et al., 2005; Magnini et al., 2006). Different from their setting, we emphasize on building question answering systems for other languages using labeled data from a rich source language such as English, while the documents are in the same language as the questions. 3 Cross-lingual Open-domain Question Answering Existing OpenQA models usually first retrieve documents related t"
P19-1227,W17-2619,0,0.0173788,"l corpus (Gouws et al., 2015; Luong et al., 2015), with a bilingual dictionary (Zhang et al., 2016; Artetxe et al., 2018), and even in a fully unsupervised manner (Conneau et al., 2018a). These multilingual word representation models could be easily extended to multilingual sentence representation by averaging the representations of all words (Klementiev et al., 2012). Nevertheless, this method does not take into account the structure information of sentences. To address this issue, much effort has been devoted to using the context vector of NMT system as multilingual sentence representation (Schwenk and Douze, 2017; Espana-Bonet et al., 2017). Recently, Artetxe and Schwenk (2018) proposed to utilize a single encoder to learn joint multilingual sentence representations for 93 languages. Besides, Devlin et al. (2019) also released a multilingual version of BERT which encoded over 100 languages with a unified encoder. These models have shown their effectiveness in several cross-lingual NLP tasks such as document classification (Klementiev et al., 2012), textual similarity (Cer et al., 2017), natural language inference (Conneau et al., 2018b), and dialog system (Schuster et al., 2019). However, there is sti"
P19-1227,L18-1560,0,0.0418847,"low-resource languages. In this case, it is impossible to directly apply existing OpenQA models to many different languages. To address this problem, an alternative approach is to build a cross-lingual OpenQA system. It is trained on data in one high-resource source language such as English, and predicts answers for open-domain questions in other target languages. In fact, cross-lingual OpenQA can be viewed as a particular task of cross-lingual language understanding (XLU). Recently, XLU has been applied to many natural language processing tasks such as cross-lingual document classification (Schwenk and Li, 2018), cross-lingual natural language inference (Conneau et al., 2018b), and machine translation (Lample et al., 2018). Most cross-lingual models focus on word or sentence level understanding, while the interaction between questions and documents as well as the overall understanding of the documents are essential to OpenQA. To the best of our knowledge, there is still no dataset for cross-lingual OpenQA. In this paper, we introduce a cross-lingual OpenQA dataset called XQA. It consists of a training set in English, and development and test sets in English, French, German, Portuguese, Polish, 2358 P"
P19-1227,P18-1161,1,0.910792,"1 Introduction In recent years, open-domain question answering (OpenQA), which aims to answer open-domain questions with a large-scale text corpus, has attracted lots of attention from natural language processing researchers. Chen et al. (2017) proposed DrQA model, which used a text retriever to obtain relevant documents from Wikipedia, and further applied a trained reading comprehension model ∗ Corresponding author: Maosong Sun to extract the answer from the retrieved documents. Moreover, researchers have introduced more sophisticated models, which either aggregate all informative evidence (Lin et al., 2018; Wang et al., 2018b) or filter out those noisy retrieved text (Clark and Gardner, 2018; Choi et al., 2017; Wang et al., 2018a) to better predict the answers for open-domain questions. Benefiting from the power of neural networks, these models have achieved remarkable results in OpenQA. However, these neural-based models must be trained with a huge volume of labeled data. Collecting and labeling large-size training data for each language is often intractable and unrealistic, especially for those low-resource languages. In this case, it is impossible to directly apply existing OpenQA models to"
P19-1227,W15-1521,0,0.0275638,"et al. (2018b) took all informative paragraphs into consideration by aggregating evidence in multiple paragraphs. Moreover, Clark and Gardner (2018) applied a shared-normalization learning objective on sampling paragraphs. All the models mentioned above were only verified in a single language (usually in English) with vast volumes of labeled data, and cannot be easily extended to the cross-lingual scenario. 2.2 Cross-lingual Language Understanding Recent years, plenty of work has focused on multilingual word representation learning, including learning from parallel corpus (Gouws et al., 2015; Luong et al., 2015), with a bilingual dictionary (Zhang et al., 2016; Artetxe et al., 2018), and even in a fully unsupervised manner (Conneau et al., 2018a). These multilingual word representation models could be easily extended to multilingual sentence representation by averaging the representations of all words (Klementiev et al., 2012). Nevertheless, this method does not take into account the structure information of sentences. To address this issue, much effort has been devoted to using the context vector of NMT system as multilingual sentence representation (Schwenk and Douze, 2017; Espana-Bonet et al., 201"
P19-1227,2020.amta-research.11,1,0.825841,"Missing"
P19-1278,D15-1082,1,0.727498,"Missing"
P19-1278,N13-1008,0,0.0278094,"on of KBs (Suchanek et al., 2007; Bollacker et al., 2008; Bizer et al., 2009), relations are further defined as various types of latent connections between objects more than semantic relations. These general relations play a core role in expressing relational facts in the real world. Hence, there are accordingly various methods proposed for discovering more relations and their facts, including open information extraction (Brin, 1998; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Banko et al., 2007; Zhu et al., 2009; Etzioni et al., 2011; Saha et al., 2017) and relation extraction (Riedel et al., 2013; Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Zeng et al., 2015; Lin et al., 2016), and relation prediction (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015b,a; Xie et al., 2016). For both semantic relations and general relations, identifying them is a crucial problem, requiring systems to provide a fine-grained relation similarity metric. However, the existing methods suffer from sparse data, which makes it difficult to achieve an effective and stable similarity metric. Motivated by this, we propose to measure relation similarity by leveraging their fact distribution so"
P19-1278,S12-1055,0,0.0259867,", researchers have empirically found there are various different categorizations of semantic relations among words and contexts. For promoting research on these different semantic relations, Bejar et al. (1991) explicitly defining these relations and Miller (1995) further systematically organize rich semantic relations between words via a database. For identifying correlation and distinction between different semantic relations so as to support learning semantic similarity, various methods have attempted to measure relational similarity (Turney, 2005, 2006; Zhila et al., 2013; Pedersen, 2012; Rink and Harabagiu, 2012; Mikolov et al., 2013b,a). 2889 With the ongoing development of information extraction and effective construction of KBs (Suchanek et al., 2007; Bollacker et al., 2008; Bizer et al., 2009), relations are further defined as various types of latent connections between objects more than semantic relations. These general relations play a core role in expressing relational facts in the real world. Hence, there are accordingly various methods proposed for discovering more relations and their facts, including open information extraction (Brin, 1998; Agichtein and Gravano, 2000; Ravichandran and Hovy"
P19-1278,P17-2050,0,0.0462859,"Missing"
P19-1278,P16-1200,1,0.832915,"er defined as various types of latent connections between objects more than semantic relations. These general relations play a core role in expressing relational facts in the real world. Hence, there are accordingly various methods proposed for discovering more relations and their facts, including open information extraction (Brin, 1998; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Banko et al., 2007; Zhu et al., 2009; Etzioni et al., 2011; Saha et al., 2017) and relation extraction (Riedel et al., 2013; Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Zeng et al., 2015; Lin et al., 2016), and relation prediction (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015b,a; Xie et al., 2016). For both semantic relations and general relations, identifying them is a crucial problem, requiring systems to provide a fine-grained relation similarity metric. However, the existing methods suffer from sparse data, which makes it difficult to achieve an effective and stable similarity metric. Motivated by this, we propose to measure relation similarity by leveraging their fact distribution so that we can identify nuances between similar relations, and merge those distant surface forms o"
P19-1278,P15-1061,0,0.0277445,"Bizer et al., 2009), relations are further defined as various types of latent connections between objects more than semantic relations. These general relations play a core role in expressing relational facts in the real world. Hence, there are accordingly various methods proposed for discovering more relations and their facts, including open information extraction (Brin, 1998; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Banko et al., 2007; Zhu et al., 2009; Etzioni et al., 2011; Saha et al., 2017) and relation extraction (Riedel et al., 2013; Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Zeng et al., 2015; Lin et al., 2016), and relation prediction (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015b,a; Xie et al., 2016). For both semantic relations and general relations, identifying them is a crucial problem, requiring systems to provide a fine-grained relation similarity metric. However, the existing methods suffer from sparse data, which makes it difficult to achieve an effective and stable similarity metric. Motivated by this, we propose to measure relation similarity by leveraging their fact distribution so that we can identify nuances between similar relations, a"
P19-1278,Q16-1017,0,0.018265,"ever, these extractors only yield relation patterns between entities, without aggregating and clustering their results. Accordingly, there are a fair amount of redundant relation patterns after extracting those relation patterns. Furthermore, the redundant patterns lead to |E| #Fact 112,946 194,556 14,951 29,943 426,067 266,645 483,142 68,124 Section §5 and §6.1 §6.2 §7.1 and §8 §7.2 and §9 Table 3: Statistics of the triple sets used in this paper. some redundant relations in KBs. Recently, some efforts are devoted to Open Relation Extraction (Open RE) (Lin and Pantel, 2001; Yao et al., 2011; Marcheggiani and Titov, 2016; ElSahar et al., 2017), aiming to cluster relation patterns into several relation types instead of redundant relation patterns. Whenas, these Open RE methods adopt distantly supervised labels as golden relation types, suffering from both false positive and false negative problems on the one hand. On the other hand, these methods still rely on the conventional similarity metrics mentioned above. In this section, we will show that our defined similarity quantification could help Open IE by identifying redundant relations. To be specific, we set a toy experiment to remove redundant relations in"
P19-1278,J06-3003,0,0.156138,"Missing"
P19-1278,S12-1070,0,0.033295,"1; Resnik, 1999), researchers have empirically found there are various different categorizations of semantic relations among words and contexts. For promoting research on these different semantic relations, Bejar et al. (1991) explicitly defining these relations and Miller (1995) further systematically organize rich semantic relations between words via a database. For identifying correlation and distinction between different semantic relations so as to support learning semantic similarity, various methods have attempted to measure relational similarity (Turney, 2005, 2006; Zhila et al., 2013; Pedersen, 2012; Rink and Harabagiu, 2012; Mikolov et al., 2013b,a). 2889 With the ongoing development of information extraction and effective construction of KBs (Suchanek et al., 2007; Bollacker et al., 2008; Bizer et al., 2009), relations are further defined as various types of latent connections between objects more than semantic relations. These general relations play a core role in expressing relational facts in the real world. Hence, there are accordingly various methods proposed for discovering more relations and their facts, including open information extraction (Brin, 1998; Agichtein and Gravano, 2"
P19-1278,D15-1206,0,0.0592151,"Missing"
P19-1278,D11-1135,0,0.0282773,"et al., 2015). However, these extractors only yield relation patterns between entities, without aggregating and clustering their results. Accordingly, there are a fair amount of redundant relation patterns after extracting those relation patterns. Furthermore, the redundant patterns lead to |E| #Fact 112,946 194,556 14,951 29,943 426,067 266,645 483,142 68,124 Section §5 and §6.1 §6.2 §7.1 and §8 §7.2 and §9 Table 3: Statistics of the triple sets used in this paper. some redundant relations in KBs. Recently, some efforts are devoted to Open Relation Extraction (Open RE) (Lin and Pantel, 2001; Yao et al., 2011; Marcheggiani and Titov, 2016; ElSahar et al., 2017), aiming to cluster relation patterns into several relation types instead of redundant relation patterns. Whenas, these Open RE methods adopt distantly supervised labels as golden relation types, suffering from both false positive and false negative problems on the one hand. On the other hand, these methods still rely on the conventional similarity metrics mentioned above. In this section, we will show that our defined similarity quantification could help Open IE by identifying redundant relations. To be specific, we set a toy experiment to"
P19-1278,N07-4013,0,0.0157421,". On the other hand, our model shows a stronger correlation (0.63) with human judgment, indicating that considering the probability over whole entity pair space helps to gain a similarity closer to human judgments. These results provide evidence for our claim raised in §3.2. 6 Wikidata ReVerb Extractions FB15K TACRED Open IE extracts concise token patterns from plain text to represent various relations between entities, e.g.„ (Mark Twain, was born in, Florida). As Open IE is significant for constructing KBs, many effective extractors have been proposed to extract triples, such as Text-Runner (Yates et al., 2007), ReVerb (Fader et al., 2011), and Standford Open IE (Angeli et al., 2015). However, these extractors only yield relation patterns between entities, without aggregating and clustering their results. Accordingly, there are a fair amount of redundant relation patterns after extracting those relation patterns. Furthermore, the redundant patterns lead to |E| #Fact 112,946 194,556 14,951 29,943 426,067 266,645 483,142 68,124 Section §5 and §6.1 §6.2 §7.1 and §8 §7.2 and §9 Table 3: Statistics of the triple sets used in this paper. some redundant relations in KBs. Recently, some efforts are devoted"
P19-1278,D15-1203,0,0.0346458,"relations are further defined as various types of latent connections between objects more than semantic relations. These general relations play a core role in expressing relational facts in the real world. Hence, there are accordingly various methods proposed for discovering more relations and their facts, including open information extraction (Brin, 1998; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Banko et al., 2007; Zhu et al., 2009; Etzioni et al., 2011; Saha et al., 2017) and relation extraction (Riedel et al., 2013; Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Zeng et al., 2015; Lin et al., 2016), and relation prediction (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015b,a; Xie et al., 2016). For both semantic relations and general relations, identifying them is a crucial problem, requiring systems to provide a fine-grained relation similarity metric. However, the existing methods suffer from sparse data, which makes it difficult to achieve an effective and stable similarity metric. Motivated by this, we propose to measure relation similarity by leveraging their fact distribution so that we can identify nuances between similar relations, and merge those dist"
P19-1278,C14-1220,0,0.0489211,"cker et al., 2008; Bizer et al., 2009), relations are further defined as various types of latent connections between objects more than semantic relations. These general relations play a core role in expressing relational facts in the real world. Hence, there are accordingly various methods proposed for discovering more relations and their facts, including open information extraction (Brin, 1998; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Banko et al., 2007; Zhu et al., 2009; Etzioni et al., 2011; Saha et al., 2017) and relation extraction (Riedel et al., 2013; Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Zeng et al., 2015; Lin et al., 2016), and relation prediction (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015b,a; Xie et al., 2016). For both semantic relations and general relations, identifying them is a crucial problem, requiring systems to provide a fine-grained relation similarity metric. However, the existing methods suffer from sparse data, which makes it difficult to achieve an effective and stable similarity metric. Motivated by this, we propose to measure relation similarity by leveraging their fact distribution so that we can identify nuances between"
P19-1278,D17-1004,0,0.316785,"11) is a program that automatically identifies and extracts binary relationships from English sentences. We use the extractions from running ReVerb on Wikipedia9. We only keep the relations appear more than 10 times and their corresponding triples to construct our dataset. 4.1 4.3 4 Dataset Construction Wikidata In Wikidata (Vrandeˇci´c and Krötzsch, 2014), facts can be described as (Head item/property, Property, Tail item/property). To construct a dataset suitable for our task, we only consider the facts whose head FB15K and TACRED FB15K (Bordes et al., 2013) is a subset of freebase. TACRED (Zhang et al., 2017) is a large supervised relation extraction dataset obtained via crowdsourcing. We directly use these two dataset, no extra processing steps were applied. 8Embeddings used in this graph are from a trained TransE model. 2885 9http://reverb.cs.washington.edu/ 5 Triple Set Human Judgments Following Miller and Charles (1991); Resnik (1999) and the vast amount of previous work on semantic similarity, we ask nine undergraduate subjects to assess the similarity of 360 pairs of relations from a subset of Wikidata (Vrandeˇci´c and Krötzsch, 2014)10 that are chosen to cover from high to low levels of sim"
P19-1278,N13-1120,0,0.0183395,"ler and Charles, 1991; Resnik, 1999), researchers have empirically found there are various different categorizations of semantic relations among words and contexts. For promoting research on these different semantic relations, Bejar et al. (1991) explicitly defining these relations and Miller (1995) further systematically organize rich semantic relations between words via a database. For identifying correlation and distinction between different semantic relations so as to support learning semantic similarity, various methods have attempted to measure relational similarity (Turney, 2005, 2006; Zhila et al., 2013; Pedersen, 2012; Rink and Harabagiu, 2012; Mikolov et al., 2013b,a). 2889 With the ongoing development of information extraction and effective construction of KBs (Suchanek et al., 2007; Bollacker et al., 2008; Bizer et al., 2009), relations are further defined as various types of latent connections between objects more than semantic relations. These general relations play a core role in expressing relational facts in the real world. Hence, there are accordingly various methods proposed for discovering more relations and their facts, including open information extraction (Brin, 1998; Agichtei"
P19-1421,P18-2023,0,0.0152933,"n correlation coefficient is applied to assess inter-annotator agreement. A candidate is labeled as a related concept only if the two annotators are in agreement. For each dataset, we split it into training (400), validation (200) and test set (200). Table 1 presents the detailed statistics, where #courses, #videos, |M|, 1-Label and 0-Label are the number of courses, videos, course concepts, positive and negative labels. We can only obtain #deletions from game for Chinese datasets. 4.2 Experiment Settings Basic Setting. We choose GloVe (Pennington et al., 2014) as our English word embedding, (Li et al., 2018) as our Chinese word embedding. We 10 The datasets will be publicly available later. The three courses: Algorithms (Princeton), Algorithms (Stanford), Data Structure and Algorithm (UC San Diego). 11 DSA ZH EN 1 3 490 465 305 201 398 232 402 568 0.696 0.734 6939 - PSY ZH EN 1 1 57 478 575 470 237 246 563 554 0.712 0.681 4920 - Table 1: Datasets Statistics follow the same process of (Cho et al., 2014) to train the path encoder and (Pan et al., 2017a) to get prerequisite features for classifier in Section 3.2. Baseline Methods. We compare our models (simple candidate generation results denoted as"
P19-1421,P10-2066,0,0.0545836,"to involve human interactions. How to properly leverage the feedback from MOOC users to obtain a better performance for concept expansion remains a challenging issue. To address the above problems, we propose a three-stage course concept expansion model. Inspired by the idea of concept space (Hori, 1997), we first build an accurate boundary for a given course to alleviate the semantic drift during candidate concept generation from an external knowledge base. Then we transform the expansion into a binary classification problem as previous positive unlabeled learning methods for set expansion (Li et al., 2010; Wang et al., 2017). Three types of features are proposed to incorporate heterogeneous information into classifier to identify high-quality concepts among candidates. Finally, we design a lightweight but attractive top-student game to subtly collect MOOC users’ feedback and iteratively optimize the expansion results. For evaluation, we compare the proposed method with 4 representative set expansion methods on real courses from Coursera and XuetangX, and further conduct online evaluation in the game mechanism. Contributions. Our contributions include: a) the first attempt, to the best of our k"
P19-1421,D15-1193,0,0.365065,"Missing"
P19-1421,D18-2004,0,0.125422,"Missing"
P19-1421,P17-1133,1,0.64153,"ch tree that achieves an O(log log n) competitive ratio. (Demaine et al., 2007) 1 MOOCs, teachers need to keep a moderate length of the course to face with thousands of students with various backgrounds (Jordan, 2015), making it infeasible to manually pick out these helpful concepts. Therefore, there is a clear need to automatically identify course-related concepts, so that the students can easily acquire additional knowledge and achieve better educational outcomes. Although much work concerned with extracting course concepts from teaching materials (Kay and Holden, 2002) or course subtitles (Pan et al., 2017b) has been done, the research in finding the concepts absent in course materials, which we call Course Concept Expansion, has not been explored. Despite abundant work on related topics, including concept expansion or set expansion (Wang and Cohen, 2007; Wang et al., 2015; Adrian and Manna, 2018), it is far from sufficient to directly apply these methods in the MOOC environments due to the following challenges. First, unlike the set expansion for a clear general category (e.g., country), course concepts are often the combinations of multiple categories, which is easy to cause semantic drift (C"
P19-1421,I17-1088,1,0.371848,"ch tree that achieves an O(log log n) competitive ratio. (Demaine et al., 2007) 1 MOOCs, teachers need to keep a moderate length of the course to face with thousands of students with various backgrounds (Jordan, 2015), making it infeasible to manually pick out these helpful concepts. Therefore, there is a clear need to automatically identify course-related concepts, so that the students can easily acquire additional knowledge and achieve better educational outcomes. Although much work concerned with extracting course concepts from teaching materials (Kay and Holden, 2002) or course subtitles (Pan et al., 2017b) has been done, the research in finding the concepts absent in course materials, which we call Course Concept Expansion, has not been explored. Despite abundant work on related topics, including concept expansion or set expansion (Wang and Cohen, 2007; Wang et al., 2015; Adrian and Manna, 2018), it is far from sufficient to directly apply these methods in the MOOC environments due to the following challenges. First, unlike the set expansion for a clear general category (e.g., country), course concepts are often the combinations of multiple categories, which is easy to cause semantic drift (C"
P19-1421,D17-1059,0,0.371902,"interactions. How to properly leverage the feedback from MOOC users to obtain a better performance for concept expansion remains a challenging issue. To address the above problems, we propose a three-stage course concept expansion model. Inspired by the idea of concept space (Hori, 1997), we first build an accurate boundary for a given course to alleviate the semantic drift during candidate concept generation from an external knowledge base. Then we transform the expansion into a binary classification problem as previous positive unlabeled learning methods for set expansion (Li et al., 2010; Wang et al., 2017). Three types of features are proposed to incorporate heterogeneous information into classifier to identify high-quality concepts among candidates. Finally, we design a lightweight but attractive top-student game to subtly collect MOOC users’ feedback and iteratively optimize the expansion results. For evaluation, we compare the proposed method with 4 representative set expansion methods on real courses from Coursera and XuetangX, and further conduct online evaluation in the game mechanism. Contributions. Our contributions include: a) the first attempt, to the best of our knowledge, systematic"
P19-1421,D14-1162,0,0.0960387,"ledge. Thus, each dataset is doubly annotated, and pearson correlation coefficient is applied to assess inter-annotator agreement. A candidate is labeled as a related concept only if the two annotators are in agreement. For each dataset, we split it into training (400), validation (200) and test set (200). Table 1 presents the detailed statistics, where #courses, #videos, |M|, 1-Label and 0-Label are the number of courses, videos, course concepts, positive and negative labels. We can only obtain #deletions from game for Chinese datasets. 4.2 Experiment Settings Basic Setting. We choose GloVe (Pennington et al., 2014) as our English word embedding, (Li et al., 2018) as our Chinese word embedding. We 10 The datasets will be publicly available later. The three courses: Algorithms (Princeton), Algorithms (Stanford), Data Structure and Algorithm (UC San Diego). 11 DSA ZH EN 1 3 490 465 305 201 398 232 402 568 0.696 0.734 6939 - PSY ZH EN 1 1 57 478 575 470 237 246 563 554 0.712 0.681 4920 - Table 1: Datasets Statistics follow the same process of (Cho et al., 2014) to train the path encoder and (Pan et al., 2017a) to get prerequisite features for classifier in Section 3.2. Baseline Methods. We compare our model"
P19-1421,W12-2037,0,0.0227967,"as input and output the same sequence. Thus, we can obtain a fixed-length vector representation of path(e) from the final hidden state of the RNN encoder. Prerequisite Features. The course concepts also have an unique relationship called Prerequisite (Margolis and Laurence, 1999). Prerequisite concept pair (A, B) means if someone wants to study A, he/she is better to understand B in advance (e.g., Binary Tree is a prerequisite concept to Black-Red Tree), which indicates how concepts in the course are connected. There are a few previous efforts to extract prerequisite relations from Wikipedia (Talukdar and Cohen, 2012; Liang et al., 2015), textbooks (Yosef et al., 2011; Wang et al., 2016) and MOOCs (Pan et al., 2017a). In this paper, we select five features from (Pan et al., 2017a) that only rely on the course text, and P v(a, b) is the combination of these five features reflecting the prerequisite likelihood of a to b. Since these features can only measure the relationship between the two concepts that exist in the course, we calculate the prerequisite feature of e 4295 concept ei ’s (denoted as del(ei )). using its search root phrasePci as follows: cos he, ci i ∗ P f (e) = cj ∈M P v(ci , cj ) |M| (2) Par"
P19-1430,D15-1141,0,0.0301012,"al., 2017; Xu et al., 2017). In most cases, these methods only focus on the improvement of the model itself, ignoring the fact that different granularity of input will have a significant impact on the RE models. The character-based model can not utilize the information of words, capturing fewer features than the word-based model. On the other side, the performance of the word-based model is significantly impacted by the quality of segmentation (Zhang and Yang, 2018). Although some methods are used to combine character-level and word-level information in other NLP tasks like character-bigrams (Chen et al., 2015; Yang et al., 2017) and soft words (Zhao and Kit, 2008; Chen et al., 2014; Peng and Dredze, 2016), the information utilization is still very limited. Then, tree-structured RNNs was proposed to address the shortcomings. Tai et al. (2015) proposed a tree-like LSTM model to improve the semantic representation. This type of structure has been applied into various tasks, including human action recognition (Sun et al., 2017), NMT encoders (Su et al., 2017), speech tokenization 4378 (Sperber et al., 2017) and NRE (Zhang and Yang, 2018). Although the lattice LSTM model can exploit word and word seque"
P19-1430,P14-1054,0,0.0311099,"e improvement of the model itself, ignoring the fact that different granularity of input will have a significant impact on the RE models. The character-based model can not utilize the information of words, capturing fewer features than the word-based model. On the other side, the performance of the word-based model is significantly impacted by the quality of segmentation (Zhang and Yang, 2018). Although some methods are used to combine character-level and word-level information in other NLP tasks like character-bigrams (Chen et al., 2015; Yang et al., 2017) and soft words (Zhao and Kit, 2008; Chen et al., 2014; Peng and Dredze, 2016), the information utilization is still very limited. Then, tree-structured RNNs was proposed to address the shortcomings. Tai et al. (2015) proposed a tree-like LSTM model to improve the semantic representation. This type of structure has been applied into various tasks, including human action recognition (Sun et al., 2017), NMT encoders (Su et al., 2017), speech tokenization 4378 (Sperber et al., 2017) and NRE (Zhang and Yang, 2018). Although the lattice LSTM model can exploit word and word sequence information, it still could be severely affected by the ambiguity of p"
P19-1430,C16-1139,0,0.319132,"ion ambiguity and polysemy ambiguity in Chinese RE. Introduction Relation extraction (RE) has a pivotal role in information extraction (IE), aiming to extract semantic relations between entity pairs in natural language sentences. In downstream applications, this technology is a key module for constructing largescale knowledge graphs. Recent developments in deep learning have heightened the interest for neural relation extractions (NRE), which attempt to use neural networks to automatically learn semantic features (Liu et al., 2013; Zeng et al., 2014, 2015; Lin et al., 2016; Zhou et al., 2016; Jiang et al., 2016). ∗ indicates equal contribution Corresponding author: Hai-Tao Zheng. zheng.haitao@sz.tsinghua.edu.cn ) † ( E-mail: Although it is not necessary for NRE to perform feature engineering, they ignore the fact that different language granularity of input will have a significant impact on the model, especially for Chinese RE. Conventionally, according to the difference in granularity, most existing methods for Chinese RE can be divided into two types: character-based RE and word-based RE. For the character-based RE, it regards each input sentence as a character sequence. The shortcoming of this kin"
P19-1430,P16-1200,1,0.920553,". 1 Figure 1: An example of segmentation ambiguity and polysemy ambiguity in Chinese RE. Introduction Relation extraction (RE) has a pivotal role in information extraction (IE), aiming to extract semantic relations between entity pairs in natural language sentences. In downstream applications, this technology is a key module for constructing largescale knowledge graphs. Recent developments in deep learning have heightened the interest for neural relation extractions (NRE), which attempt to use neural networks to automatically learn semantic features (Liu et al., 2013; Zeng et al., 2014, 2015; Lin et al., 2016; Zhou et al., 2016; Jiang et al., 2016). ∗ indicates equal contribution Corresponding author: Hai-Tao Zheng. zheng.haitao@sz.tsinghua.edu.cn ) † ( E-mail: Although it is not necessary for NRE to perform feature engineering, they ignore the fact that different language granularity of input will have a significant impact on the model, especially for Chinese RE. Conventionally, according to the difference in granularity, most existing methods for Chinese RE can be divided into two types: character-based RE and word-based RE. For the character-based RE, it regards each input sentence as a charact"
P19-1430,P17-1187,1,0.680596,"al., 2013) to convert it into a real-valued vector dw xw b,e ∈ R . However, the word2vec method maps each word to only one single embedding, ignoring the fact that many words have multiple senses. To tackle this problem, we incorporate HowNet as an external knowledge base into our model to represent word senses rather than words. Hence, given a word wb,e , we first obtain all K senses of it by retrieving the HowNet. Using Sense(wb,e ) to denote the senses set of wb,e , we (w ) then convert each sense senk b,e ∈ Sense(wb,e ) dsen through into a real-valued vector xsen b,e,k ∈ R the SAT model (Niu et al., 2017). The SAT model is on the basis of the Skip-gram, which can jointly learn word and sense representations. Finally, the representation of wb,e is a vector set denoted as sen sen xsen b,e = {xb,e,1 , ..., xb,e,K }. In the next section, we will introduce how our model utilizes sense embeddings. 3.2 Basic Lattice LSTM Encoder Generally, a classical LSTM (Hochreiter and Schmidhuber, 1997) unit is composed of four basic gates structure: one input gate ij controls which information enters into the unit; one output gate oj controls which information would be outputted from the unit; one forget gate fj"
P19-1430,P16-2025,0,0.0267252,"e model itself, ignoring the fact that different granularity of input will have a significant impact on the RE models. The character-based model can not utilize the information of words, capturing fewer features than the word-based model. On the other side, the performance of the word-based model is significantly impacted by the quality of segmentation (Zhang and Yang, 2018). Although some methods are used to combine character-level and word-level information in other NLP tasks like character-bigrams (Chen et al., 2015; Yang et al., 2017) and soft words (Zhao and Kit, 2008; Chen et al., 2014; Peng and Dredze, 2016), the information utilization is still very limited. Then, tree-structured RNNs was proposed to address the shortcomings. Tai et al. (2015) proposed a tree-like LSTM model to improve the semantic representation. This type of structure has been applied into various tasks, including human action recognition (Sun et al., 2017), NMT encoders (Su et al., 2017), speech tokenization 4378 (Sperber et al., 2017) and NRE (Zhang and Yang, 2018). Although the lattice LSTM model can exploit word and word sequence information, it still could be severely affected by the ambiguity of polysemy. In other words,"
P19-1430,C86-1107,0,0.181515,"Missing"
P19-1430,P17-2040,0,0.0448763,"Missing"
P19-1430,D17-1145,0,0.0226804,"ed to combine character-level and word-level information in other NLP tasks like character-bigrams (Chen et al., 2015; Yang et al., 2017) and soft words (Zhao and Kit, 2008; Chen et al., 2014; Peng and Dredze, 2016), the information utilization is still very limited. Then, tree-structured RNNs was proposed to address the shortcomings. Tai et al. (2015) proposed a tree-like LSTM model to improve the semantic representation. This type of structure has been applied into various tasks, including human action recognition (Sun et al., 2017), NMT encoders (Su et al., 2017), speech tokenization 4378 (Sperber et al., 2017) and NRE (Zhang and Yang, 2018). Although the lattice LSTM model can exploit word and word sequence information, it still could be severely affected by the ambiguity of polysemy. In other words, these models cannot handle the polysemy of words with the change of language situation. Therefore, the introduction of external linguistic knowledge is very necessary. We utilize sense-level information with the help of HowNet proposed by Dong and Dong (2003), which is a concept knowledge base that annotates Chinese with correlative word senses. In addition, the open-sourced HowNet API (Qi et al., 2019"
P19-1430,P15-1150,0,0.0278647,"can not utilize the information of words, capturing fewer features than the word-based model. On the other side, the performance of the word-based model is significantly impacted by the quality of segmentation (Zhang and Yang, 2018). Although some methods are used to combine character-level and word-level information in other NLP tasks like character-bigrams (Chen et al., 2015; Yang et al., 2017) and soft words (Zhao and Kit, 2008; Chen et al., 2014; Peng and Dredze, 2016), the information utilization is still very limited. Then, tree-structured RNNs was proposed to address the shortcomings. Tai et al. (2015) proposed a tree-like LSTM model to improve the semantic representation. This type of structure has been applied into various tasks, including human action recognition (Sun et al., 2017), NMT encoders (Su et al., 2017), speech tokenization 4378 (Sperber et al., 2017) and NRE (Zhang and Yang, 2018). Although the lattice LSTM model can exploit word and word sequence information, it still could be severely affected by the ambiguity of polysemy. In other words, these models cannot handle the polysemy of words with the change of language situation. Therefore, the introduction of external linguistic"
P19-1430,P17-1078,0,0.0251321,"., 2017). In most cases, these methods only focus on the improvement of the model itself, ignoring the fact that different granularity of input will have a significant impact on the RE models. The character-based model can not utilize the information of words, capturing fewer features than the word-based model. On the other side, the performance of the word-based model is significantly impacted by the quality of segmentation (Zhang and Yang, 2018). Although some methods are used to combine character-level and word-level information in other NLP tasks like character-bigrams (Chen et al., 2015; Yang et al., 2017) and soft words (Zhao and Kit, 2008; Chen et al., 2014; Peng and Dredze, 2016), the information utilization is still very limited. Then, tree-structured RNNs was proposed to address the shortcomings. Tai et al. (2015) proposed a tree-like LSTM model to improve the semantic representation. This type of structure has been applied into various tasks, including human action recognition (Sun et al., 2017), NMT encoders (Su et al., 2017), speech tokenization 4378 (Sperber et al., 2017) and NRE (Zhang and Yang, 2018). Although the lattice LSTM model can exploit word and word sequence information, it"
P19-1430,D15-1203,0,0.241208,"e results indicate that our model significantly outperforms multiple existing methods, achieving state-of-theart results on various datasets across different domains. 2 Related Work Recent years RE, especially NRE, has been widely studied in the NLP field. As a pioneer, (Liu et al., 2013) proposed a simple CNN RE model and it is regarded as one seminal work that uses a neural network to automatically learn features. On this basis, (Zeng et al., 2014) developed a CNN model with max-pooling, where positional embeddings were first used to represent the position information. Then the PCNNs model (Zeng et al., 2015) designed the multi-instance learning paradigm for RE. However, the PCNNs model suffers the issue of the selection of sentences. To address the problem, Lin et al. (2016) applied the attention mechanism over all the instances in the bag. Further, Jiang et al. (2016) proposed a model with multiinstance and multi-label paradigms. Although PCNNs models are more efficient, they cannot exploit contextual information like RNNs. Hence, LSTM with attention mechanism was also applied to the RE task (Zhang and Wang, 2015; Zhou et al., 2016; Lee et al., 2019). Existing methods for Chinese RE are mostly c"
P19-1430,C14-1220,0,0.709228,"b.com/ thunlp/Chinese_NRE. 1 Figure 1: An example of segmentation ambiguity and polysemy ambiguity in Chinese RE. Introduction Relation extraction (RE) has a pivotal role in information extraction (IE), aiming to extract semantic relations between entity pairs in natural language sentences. In downstream applications, this technology is a key module for constructing largescale knowledge graphs. Recent developments in deep learning have heightened the interest for neural relation extractions (NRE), which attempt to use neural networks to automatically learn semantic features (Liu et al., 2013; Zeng et al., 2014, 2015; Lin et al., 2016; Zhou et al., 2016; Jiang et al., 2016). ∗ indicates equal contribution Corresponding author: Hai-Tao Zheng. zheng.haitao@sz.tsinghua.edu.cn ) † ( E-mail: Although it is not necessary for NRE to perform feature engineering, they ignore the fact that different language granularity of input will have a significant impact on the model, especially for Chinese RE. Conventionally, according to the difference in granularity, most existing methods for Chinese RE can be divided into two types: character-based RE and word-based RE. For the character-based RE, it regards each inp"
P19-1430,P18-1144,0,0.164104,"or Chinese RE are mostly character-based or word-based implementations of mainstream NRE models (Chen and Hsu, 2016; R¨onnqvist et al., 2017; ZHANG et al., 2017; Xu et al., 2017). In most cases, these methods only focus on the improvement of the model itself, ignoring the fact that different granularity of input will have a significant impact on the RE models. The character-based model can not utilize the information of words, capturing fewer features than the word-based model. On the other side, the performance of the word-based model is significantly impacted by the quality of segmentation (Zhang and Yang, 2018). Although some methods are used to combine character-level and word-level information in other NLP tasks like character-bigrams (Chen et al., 2015; Yang et al., 2017) and soft words (Zhao and Kit, 2008; Chen et al., 2014; Peng and Dredze, 2016), the information utilization is still very limited. Then, tree-structured RNNs was proposed to address the shortcomings. Tai et al. (2015) proposed a tree-like LSTM model to improve the semantic representation. This type of structure has been applied into various tasks, including human action recognition (Sun et al., 2017), NMT encoders (Su et al., 201"
P19-1430,I08-4017,0,0.0439987,"ods only focus on the improvement of the model itself, ignoring the fact that different granularity of input will have a significant impact on the RE models. The character-based model can not utilize the information of words, capturing fewer features than the word-based model. On the other side, the performance of the word-based model is significantly impacted by the quality of segmentation (Zhang and Yang, 2018). Although some methods are used to combine character-level and word-level information in other NLP tasks like character-bigrams (Chen et al., 2015; Yang et al., 2017) and soft words (Zhao and Kit, 2008; Chen et al., 2014; Peng and Dredze, 2016), the information utilization is still very limited. Then, tree-structured RNNs was proposed to address the shortcomings. Tai et al. (2015) proposed a tree-like LSTM model to improve the semantic representation. This type of structure has been applied into various tasks, including human action recognition (Sun et al., 2017), NMT encoders (Su et al., 2017), speech tokenization 4378 (Sperber et al., 2017) and NRE (Zhang and Yang, 2018). Although the lattice LSTM model can exploit word and word sequence information, it still could be severely affected by"
P19-1430,P16-2034,0,0.508745,"xample of segmentation ambiguity and polysemy ambiguity in Chinese RE. Introduction Relation extraction (RE) has a pivotal role in information extraction (IE), aiming to extract semantic relations between entity pairs in natural language sentences. In downstream applications, this technology is a key module for constructing largescale knowledge graphs. Recent developments in deep learning have heightened the interest for neural relation extractions (NRE), which attempt to use neural networks to automatically learn semantic features (Liu et al., 2013; Zeng et al., 2014, 2015; Lin et al., 2016; Zhou et al., 2016; Jiang et al., 2016). ∗ indicates equal contribution Corresponding author: Hai-Tao Zheng. zheng.haitao@sz.tsinghua.edu.cn ) † ( E-mail: Although it is not necessary for NRE to perform feature engineering, they ignore the fact that different language granularity of input will have a significant impact on the model, especially for Chinese RE. Conventionally, according to the difference in granularity, most existing methods for Chinese RE can be divided into two types: character-based RE and word-based RE. For the character-based RE, it regards each input sentence as a character sequence. The sh"
P19-1571,D10-1115,0,0.327112,"2016). In the field of NLP, SC has proved effective in many tasks including language model∗ Indicates equal contribution † Work done during internship at Tsinghua University ‡ Corresponding author (1) where f is the compositionality function, p denotes the embedding of an MWE, w1 and w2 represent the embeddings of the MWE’s two constituents, R stands for the combination rule and K refers to the additional knowledge which is needed to construct the semantics of the MWE. Among the proposed approaches for this task, most of them ignore R and K, centering on reforming compositionality function f (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012, 2013b). Some try to integrate combination rule R into SC models (Blacoe and Lapata, 2012; Zhao et al., 2015; Weir et al., 2016; Kober et al., 2016). Few works consider external knowledge K. Zhu et al. (2016) try to incorporate task-specific knowledge into an LSTM model for sentence-level SC. As far as we know, however, no previous work attempts to use general knowledge in modeling SC. In fact, there exists general linguistic knowledge which can be used in modeling SC, e.g., sememes. Sememes are defined as the minimum semantic units of hu"
P19-1571,D12-1050,0,0.364815,"p at Tsinghua University ‡ Corresponding author (1) where f is the compositionality function, p denotes the embedding of an MWE, w1 and w2 represent the embeddings of the MWE’s two constituents, R stands for the combination rule and K refers to the additional knowledge which is needed to construct the semantics of the MWE. Among the proposed approaches for this task, most of them ignore R and K, centering on reforming compositionality function f (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012, 2013b). Some try to integrate combination rule R into SC models (Blacoe and Lapata, 2012; Zhao et al., 2015; Weir et al., 2016; Kober et al., 2016). Few works consider external knowledge K. Zhu et al. (2016) try to incorporate task-specific knowledge into an LSTM model for sentence-level SC. As far as we know, however, no previous work attempts to use general knowledge in modeling SC. In fact, there exists general linguistic knowledge which can be used in modeling SC, e.g., sememes. Sememes are defined as the minimum semantic units of human languages (Bloomfield, 1926). It is believed that the meanings of all the 1 This formula only applies to two-word MWEs but can be easily exte"
P19-1571,W00-1213,0,0.185735,"ole of external knowledge in SC. Zhu et al. (2016) incorporate prior sentimental knowledge into LSTM models, aiming to improve sentiment analysis performance of sentences. To the best our knowledge, there is no work trying to take account of general linguistic knowledge in SC, especially for the MWE representation learning task. 5.2 Sememes and HowNet HowNet, as the most well-known sememe KB, has attracted wide research attention. Previous work applies the sememe knowledge of HowNet to various NLP applications, such as word similarity computation (Liu and Li, 2002), word sense disambiguation (Gan and Wong, 2000; Zhang et al., 2005; Duan et al., 2007), sentiment analysis (Zhu et al., 2006; Dang and Zhang, 2010; Fu et al., 2013), word representation learning (Niu et al., 2017), language modeling (Gu et al., 2018), lexicon expansion (Zeng et al., 2018) and semantic rationality evaluation (Liu et al., 2018). To tackle the challenge of high cost of annotating sememes for new words, Xie et al. (2017) propose the task of automatic sememe prediction to facilitate sememe annotation. And they also propose two simple but effective models. Jin et al. 5713 (2018) further incorporate Chinese character information"
P19-1571,W13-0112,0,0.0153263,"the development of distributional semantics, vector-based SC modeling has been extensively studied in recent years. Most existing work concentrates on using better compositionality functions. Mitchell and Lapata (2008) first make a detailed comparison of several simple compositionality functions including addition and element-wise multiplication. Then various complicated models are proposed in succession, such as vector-matrix models (Baroni and Zamparelli, 2010; Socher et al., 2012), matrix-space models (Yessenalina and Cardie, 2011; Grefenstette and Sadrzadeh, 2011) and tensor-based models (Grefenstette et al., 2013; Van de Cruys et al., 2013; Socher et al., 2013b). There are also some works trying to integrate combination rules into semantic composition models (Blacoe and Lapata, 2012; Zhao et al., 2015; Kober et al., 2016; Weir et al., 2016). But few works explore the role of external knowledge in SC. Zhu et al. (2016) incorporate prior sentimental knowledge into LSTM models, aiming to improve sentiment analysis performance of sentences. To the best our knowledge, there is no work trying to take account of general linguistic knowledge in SC, especially for the MWE representation learning task. 5.2 Seme"
P19-1571,D11-1129,0,0.357139,"C has proved effective in many tasks including language model∗ Indicates equal contribution † Work done during internship at Tsinghua University ‡ Corresponding author (1) where f is the compositionality function, p denotes the embedding of an MWE, w1 and w2 represent the embeddings of the MWE’s two constituents, R stands for the combination rule and K refers to the additional knowledge which is needed to construct the semantics of the MWE. Among the proposed approaches for this task, most of them ignore R and K, centering on reforming compositionality function f (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012, 2013b). Some try to integrate combination rule R into SC models (Blacoe and Lapata, 2012; Zhao et al., 2015; Weir et al., 2016; Kober et al., 2016). Few works consider external knowledge K. Zhu et al. (2016) try to incorporate task-specific knowledge into an LSTM model for sentence-level SC. As far as we know, however, no previous work attempts to use general knowledge in modeling SC. In fact, there exists general linguistic knowledge which can be used in modeling SC, e.g., sememes. Sememes are defined as the minimum semantic units of human languages (Bloomfield, 1926)."
P19-1571,D18-1493,1,0.890044,"es of constituents are shared with the constituents’ corresponding MWE. words can be composed of a limited set of sememes, which is similar to the idea of semantic primes (Wierzbicka, 1996). HowNet (Dong and Dong, 2003) is a widely acknowledged sememe knowledge base (KB), which defines about 2,000 sememes and uses them to annotate over 100,000 Chinese words together with their English translations. Sememes and HowNet have been successfully utilized in a variety of NLP tasks including sentiment analysis (Dang and Zhang, 2010), word representation learning (Niu et al., 2017), language modeling (Gu et al., 2018), etc. In this paper, we argue that sememes are beneficial to modeling SC2 . To verify this, we first design a simple SC degree (SCD) measurement experiment and find that the SCDs of MWEs computed by simple sememe-based formulae are highly correlated with human judgment. This result shows that sememes can finely depict meanings of MWEs and their constituents, and capture the semantic relations between the two sides. Therefore, we believe that sememes are appropriate for modeling SC and can improve the performance of SC-related tasks like MWE representation learning. We propose two sememe-incor"
P19-1571,P18-1227,1,0.920458,"MWE p, its training loss is: Lp = kpc − pr k22 , (9) where pc ∈ Rd is the embedding of p obtained by our SC models , i.e., previous p, and pr ∈ Rd is the corresponding reference embedding, which might be obtained by regarding the MWE as a whole and applying word representation learning methods. And the overall loss function is as follows: L= X p∈Pt Lp + λX kθk22 , 2 (10) θ∈Θ where Pt is the training set, Θ refers to the parameter set including Wc and Wa , and λ is the regularization parameter. 5709 Training for MWE Sememe Prediction Sememe prediction is a well-defined task (Xie et al., 2017; Jin et al., 2018; Qi et al., 2018), aimed at selecting appropriate sememes for unannotated words or phrases from the set of all the sememes. Existing works model sememe prediction as a multi-label classification problem, where sememes are regarded as the labels of words and phrases. For doing MWE sememe prediction, we employ a single-layer perceptron as the classifier: (Pennington et al., 2014) on the Sogou-T corpus4 . We also utilize pretrained sememe embeddings obtained from the results of a sememe-based word representation learning model5 (Niu et al., 2017). And we build a dataset consisting of 51,034 Chin"
P19-1571,D16-1175,0,0.405979,"the compositionality function, p denotes the embedding of an MWE, w1 and w2 represent the embeddings of the MWE’s two constituents, R stands for the combination rule and K refers to the additional knowledge which is needed to construct the semantics of the MWE. Among the proposed approaches for this task, most of them ignore R and K, centering on reforming compositionality function f (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012, 2013b). Some try to integrate combination rule R into SC models (Blacoe and Lapata, 2012; Zhao et al., 2015; Weir et al., 2016; Kober et al., 2016). Few works consider external knowledge K. Zhu et al. (2016) try to incorporate task-specific knowledge into an LSTM model for sentence-level SC. As far as we know, however, no previous work attempts to use general knowledge in modeling SC. In fact, there exists general linguistic knowledge which can be used in modeling SC, e.g., sememes. Sememes are defined as the minimum semantic units of human languages (Bloomfield, 1926). It is believed that the meanings of all the 1 This formula only applies to two-word MWEs but can be easily extended to longer MWEs. In fact, we also focus on modeling SC"
P19-1571,O02-2003,1,0.5746,"eir et al., 2016). But few works explore the role of external knowledge in SC. Zhu et al. (2016) incorporate prior sentimental knowledge into LSTM models, aiming to improve sentiment analysis performance of sentences. To the best our knowledge, there is no work trying to take account of general linguistic knowledge in SC, especially for the MWE representation learning task. 5.2 Sememes and HowNet HowNet, as the most well-known sememe KB, has attracted wide research attention. Previous work applies the sememe knowledge of HowNet to various NLP applications, such as word similarity computation (Liu and Li, 2002), word sense disambiguation (Gan and Wong, 2000; Zhang et al., 2005; Duan et al., 2007), sentiment analysis (Zhu et al., 2006; Dang and Zhang, 2010; Fu et al., 2013), word representation learning (Niu et al., 2017), language modeling (Gu et al., 2018), lexicon expansion (Zeng et al., 2018) and semantic rationality evaluation (Liu et al., 2018). To tackle the challenge of high cost of annotating sememes for new words, Xie et al. (2017) propose the task of automatic sememe prediction to facilitate sememe annotation. And they also propose two simple but effective models. Jin et al. 5713 (2018) fu"
P19-1571,D11-1014,0,0.0408112,"-based word representation learning model5 (Niu et al., 2017). And we build a dataset consisting of 51,034 Chinese MWEs, each of which and its two constituents are annotated with sememes in HowNet and have pretrained word embeddings simultaneously. We randomly split the dataset into training, validation and test sets in the ratio of 8 : 1 : 1. ˆ p = σ(Ws · p), y Baseline Methods We choose several typical SC models as the baseline methods, including: (1) ADD and MUL, the simple additive and elementwise multiplicative models (Mitchell and Lapata, 2008); (2) RAE, the recursive autoencoder model (Socher et al., 2011); (3) RNTN, the recursive neural tensor network (Socher et al., 2013b); (4) TIM, the tensor index model (Zhao et al., 2015); and (5) SCAS-S, the ablated version of our SCAS model which removes sememe knowledge6 . These baseline methods range from the simplest additive model to complicated tensor-based model, all of which take no knowledge into consideration. (11) ˆ p ∈ R|S |, Ws ∈ R|S|×d and σ is the sigwhere y ˆ p , demoid function. [ˆ yp ]i , the i-th element of y notes the predicted score of i-th sememe, where the higher the score is, the more probable the sememe is selected. And Ws = [s1 ,"
P19-1571,D13-1170,0,0.606441,"ao Qi1∗, Junjie Huang2∗†, Chenghao Yang3† , Zhiyuan Liu1 , Xiao Chen4 , Qun Liu4 , Maosong Sun1‡ 1 Department of Computer Science and Technology, Tsinghua University Institute for Artificial Intelligence, Tsinghua University State Key Lab on Intelligent Technology and Systems, Tsinghua University 2 School of ASEE, Beihang University 3 Software College, Beihang Unviersity 4 Huawei Noah’s Ark Lab qfc17@mails.tsinghua.edu.cn, {hjj1997,alanyang}@buaa.edu.cn {liuzy,sms}@tsinghua.edu.cn, {chen.xiao2,qun.liu}@huawei.com Abstract ing (Mitchell and Lapata, 2009), sentiment analysis (Maas et al., 2011; Socher et al., 2013b), syntactic parsing (Socher et al., 2013a), etc. Most literature on SC pays attention to using vector-based distributional models of semantics to learn representations of multiword expressions (MWEs), i.e., embeddings of phrases or compounds. Mitchell and Lapata (2008) conduct a pioneering work in which they introduce a general framework to formulate this task: Semantic compositionality (SC) refers to the phenomenon that the meaning of a complex linguistic unit can be composed of the meanings of its constituents. Most related works focus on using complicated compositionality functions to mod"
P19-1571,N13-1134,0,0.0453151,"Missing"
P19-1571,J16-4006,0,0.170424,"hor (1) where f is the compositionality function, p denotes the embedding of an MWE, w1 and w2 represent the embeddings of the MWE’s two constituents, R stands for the combination rule and K refers to the additional knowledge which is needed to construct the semantics of the MWE. Among the proposed approaches for this task, most of them ignore R and K, centering on reforming compositionality function f (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012, 2013b). Some try to integrate combination rule R into SC models (Blacoe and Lapata, 2012; Zhao et al., 2015; Weir et al., 2016; Kober et al., 2016). Few works consider external knowledge K. Zhu et al. (2016) try to incorporate task-specific knowledge into an LSTM model for sentence-level SC. As far as we know, however, no previous work attempts to use general knowledge in modeling SC. In fact, there exists general linguistic knowledge which can be used in modeling SC, e.g., sememes. Sememes are defined as the minimum semantic units of human languages (Bloomfield, 1926). It is believed that the meanings of all the 1 This formula only applies to two-word MWEs but can be easily extended to longer MWEs. In fact, we also"
P19-1571,W13-3512,0,0.121261,"Missing"
P19-1571,P11-1015,0,0.0702493,"eme Knowledge Fanchao Qi1∗, Junjie Huang2∗†, Chenghao Yang3† , Zhiyuan Liu1 , Xiao Chen4 , Qun Liu4 , Maosong Sun1‡ 1 Department of Computer Science and Technology, Tsinghua University Institute for Artificial Intelligence, Tsinghua University State Key Lab on Intelligent Technology and Systems, Tsinghua University 2 School of ASEE, Beihang University 3 Software College, Beihang Unviersity 4 Huawei Noah’s Ark Lab qfc17@mails.tsinghua.edu.cn, {hjj1997,alanyang}@buaa.edu.cn {liuzy,sms}@tsinghua.edu.cn, {chen.xiao2,qun.liu}@huawei.com Abstract ing (Mitchell and Lapata, 2009), sentiment analysis (Maas et al., 2011; Socher et al., 2013b), syntactic parsing (Socher et al., 2013a), etc. Most literature on SC pays attention to using vector-based distributional models of semantics to learn representations of multiword expressions (MWEs), i.e., embeddings of phrases or compounds. Mitchell and Lapata (2008) conduct a pioneering work in which they introduce a general framework to formulate this task: Semantic compositionality (SC) refers to the phenomenon that the meaning of a complex linguistic unit can be composed of the meanings of its constituents. Most related works focus on using complicated compositiona"
P19-1571,P08-1028,0,0.646777,"and Systems, Tsinghua University 2 School of ASEE, Beihang University 3 Software College, Beihang Unviersity 4 Huawei Noah’s Ark Lab qfc17@mails.tsinghua.edu.cn, {hjj1997,alanyang}@buaa.edu.cn {liuzy,sms}@tsinghua.edu.cn, {chen.xiao2,qun.liu}@huawei.com Abstract ing (Mitchell and Lapata, 2009), sentiment analysis (Maas et al., 2011; Socher et al., 2013b), syntactic parsing (Socher et al., 2013a), etc. Most literature on SC pays attention to using vector-based distributional models of semantics to learn representations of multiword expressions (MWEs), i.e., embeddings of phrases or compounds. Mitchell and Lapata (2008) conduct a pioneering work in which they introduce a general framework to formulate this task: Semantic compositionality (SC) refers to the phenomenon that the meaning of a complex linguistic unit can be composed of the meanings of its constituents. Most related works focus on using complicated compositionality functions to model SC while few works consider external knowledge in models. In this paper, we verify the effectiveness of sememes, the minimum semantic units of human languages, in modeling SC by a confirmatory experiment. Furthermore, we make the first attempt to incorporate sememe kn"
P19-1571,D11-1016,0,0.0347472,"del can take advantage of sememes. 5 5.1 Related Work Semantic Compositionality Based on the development of distributional semantics, vector-based SC modeling has been extensively studied in recent years. Most existing work concentrates on using better compositionality functions. Mitchell and Lapata (2008) first make a detailed comparison of several simple compositionality functions including addition and element-wise multiplication. Then various complicated models are proposed in succession, such as vector-matrix models (Baroni and Zamparelli, 2010; Socher et al., 2012), matrix-space models (Yessenalina and Cardie, 2011; Grefenstette and Sadrzadeh, 2011) and tensor-based models (Grefenstette et al., 2013; Van de Cruys et al., 2013; Socher et al., 2013b). There are also some works trying to integrate combination rules into semantic composition models (Blacoe and Lapata, 2012; Zhao et al., 2015; Kober et al., 2016; Weir et al., 2016). But few works explore the role of external knowledge in SC. Zhu et al. (2016) incorporate prior sentimental knowledge into LSTM models, aiming to improve sentiment analysis performance of sentences. To the best our knowledge, there is no work trying to take account of general lin"
P19-1571,D09-1045,0,0.0970384,"Missing"
P19-1571,P17-1187,1,0.86977,"tion formulae and examples. Bold sememes of constituents are shared with the constituents’ corresponding MWE. words can be composed of a limited set of sememes, which is similar to the idea of semantic primes (Wierzbicka, 1996). HowNet (Dong and Dong, 2003) is a widely acknowledged sememe knowledge base (KB), which defines about 2,000 sememes and uses them to annotate over 100,000 Chinese words together with their English translations. Sememes and HowNet have been successfully utilized in a variety of NLP tasks including sentiment analysis (Dang and Zhang, 2010), word representation learning (Niu et al., 2017), language modeling (Gu et al., 2018), etc. In this paper, we argue that sememes are beneficial to modeling SC2 . To verify this, we first design a simple SC degree (SCD) measurement experiment and find that the SCDs of MWEs computed by simple sememe-based formulae are highly correlated with human judgment. This result shows that sememes can finely depict meanings of MWEs and their constituents, and capture the semantic relations between the two sides. Therefore, we believe that sememes are appropriate for modeling SC and can improve the performance of SC-related tasks like MWE representation"
P19-1571,D14-1162,0,0.0942188,") θ∈Θ where Pt is the training set, Θ refers to the parameter set including Wc and Wa , and λ is the regularization parameter. 5709 Training for MWE Sememe Prediction Sememe prediction is a well-defined task (Xie et al., 2017; Jin et al., 2018; Qi et al., 2018), aimed at selecting appropriate sememes for unannotated words or phrases from the set of all the sememes. Existing works model sememe prediction as a multi-label classification problem, where sememes are regarded as the labels of words and phrases. For doing MWE sememe prediction, we employ a single-layer perceptron as the classifier: (Pennington et al., 2014) on the Sogou-T corpus4 . We also utilize pretrained sememe embeddings obtained from the results of a sememe-based word representation learning model5 (Niu et al., 2017). And we build a dataset consisting of 51,034 Chinese MWEs, each of which and its two constituents are annotated with sememes in HowNet and have pretrained word embeddings simultaneously. We randomly split the dataset into training, validation and test sets in the ratio of 8 : 1 : 1. ˆ p = σ(Ws · p), y Baseline Methods We choose several typical SC models as the baseline methods, including: (1) ADD and MUL, the simple additive a"
P19-1571,D18-1033,1,0.906717,"ng loss is: Lp = kpc − pr k22 , (9) where pc ∈ Rd is the embedding of p obtained by our SC models , i.e., previous p, and pr ∈ Rd is the corresponding reference embedding, which might be obtained by regarding the MWE as a whole and applying word representation learning methods. And the overall loss function is as follows: L= X p∈Pt Lp + λX kθk22 , 2 (10) θ∈Θ where Pt is the training set, Θ refers to the parameter set including Wc and Wa , and λ is the regularization parameter. 5709 Training for MWE Sememe Prediction Sememe prediction is a well-defined task (Xie et al., 2017; Jin et al., 2018; Qi et al., 2018), aimed at selecting appropriate sememes for unannotated words or phrases from the set of all the sememes. Existing works model sememe prediction as a multi-label classification problem, where sememes are regarded as the labels of words and phrases. For doing MWE sememe prediction, we employ a single-layer perceptron as the classifier: (Pennington et al., 2014) on the Sogou-T corpus4 . We also utilize pretrained sememe embeddings obtained from the results of a sememe-based word representation learning model5 (Niu et al., 2017). And we build a dataset consisting of 51,034 Chinese MWEs, each of"
P19-1571,P13-1045,0,0.687064,"ao Qi1∗, Junjie Huang2∗†, Chenghao Yang3† , Zhiyuan Liu1 , Xiao Chen4 , Qun Liu4 , Maosong Sun1‡ 1 Department of Computer Science and Technology, Tsinghua University Institute for Artificial Intelligence, Tsinghua University State Key Lab on Intelligent Technology and Systems, Tsinghua University 2 School of ASEE, Beihang University 3 Software College, Beihang Unviersity 4 Huawei Noah’s Ark Lab qfc17@mails.tsinghua.edu.cn, {hjj1997,alanyang}@buaa.edu.cn {liuzy,sms}@tsinghua.edu.cn, {chen.xiao2,qun.liu}@huawei.com Abstract ing (Mitchell and Lapata, 2009), sentiment analysis (Maas et al., 2011; Socher et al., 2013b), syntactic parsing (Socher et al., 2013a), etc. Most literature on SC pays attention to using vector-based distributional models of semantics to learn representations of multiword expressions (MWEs), i.e., embeddings of phrases or compounds. Mitchell and Lapata (2008) conduct a pioneering work in which they introduce a general framework to formulate this task: Semantic compositionality (SC) refers to the phenomenon that the meaning of a complex linguistic unit can be composed of the meanings of its constituents. Most related works focus on using complicated compositionality functions to mod"
P19-1571,N16-1106,0,0.372405,"WE, w1 and w2 represent the embeddings of the MWE’s two constituents, R stands for the combination rule and K refers to the additional knowledge which is needed to construct the semantics of the MWE. Among the proposed approaches for this task, most of them ignore R and K, centering on reforming compositionality function f (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012, 2013b). Some try to integrate combination rule R into SC models (Blacoe and Lapata, 2012; Zhao et al., 2015; Weir et al., 2016; Kober et al., 2016). Few works consider external knowledge K. Zhu et al. (2016) try to incorporate task-specific knowledge into an LSTM model for sentence-level SC. As far as we know, however, no previous work attempts to use general knowledge in modeling SC. In fact, there exists general linguistic knowledge which can be used in modeling SC, e.g., sememes. Sememes are defined as the minimum semantic units of human languages (Bloomfield, 1926). It is believed that the meanings of all the 1 This formula only applies to two-word MWEs but can be easily extended to longer MWEs. In fact, we also focus on modeling SC for two-word MWEs in this paper because they are the most co"
P19-1571,D12-1110,0,0.539642,"ks including language model∗ Indicates equal contribution † Work done during internship at Tsinghua University ‡ Corresponding author (1) where f is the compositionality function, p denotes the embedding of an MWE, w1 and w2 represent the embeddings of the MWE’s two constituents, R stands for the combination rule and K refers to the additional knowledge which is needed to construct the semantics of the MWE. Among the proposed approaches for this task, most of them ignore R and K, centering on reforming compositionality function f (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012, 2013b). Some try to integrate combination rule R into SC models (Blacoe and Lapata, 2012; Zhao et al., 2015; Weir et al., 2016; Kober et al., 2016). Few works consider external knowledge K. Zhu et al. (2016) try to incorporate task-specific knowledge into an LSTM model for sentence-level SC. As far as we know, however, no previous work attempts to use general knowledge in modeling SC. In fact, there exists general linguistic knowledge which can be used in modeling SC, e.g., sememes. Sememes are defined as the minimum semantic units of human languages (Bloomfield, 1926). It is believed that t"
W11-0316,P00-1041,0,0.116716,"g appropriate keyphrases, which may not necessarily frequent in their corresponding documents. As a promising approach to solve the problem of vocabulary gap, SMT has been widely exploited in many applications such as information retrieval (Berger and Lafferty, 1999; Karimzadehgan and Zhai, 2010), image and video annotation (Duygulu et al., 2002), question answering (Berger et al., 2000; Echihabi and Marcu, 2003; Murdock and Croft, 2004; Soricut and Brill, 2006; Xue et al., 2008), query expansion and rewriting (Riezler et al., 2007; Riezler et al., 2008; Riezler and Liu, 2010), summarization (Banko et al., 2000), collocation extraction (Liu et al., 2009b; Liu et al., 2010b) and paraphrasing (Quirk et al., 2004; Zhao et al., 2010). Although SMT is a widely adopted solution to vocabulary gap, for various applications using SMT, the crucial and non-trivial problem is to find appropriate and enough translation pairs for SMT. The most straightforward translation pairs for keyphrase extraction is document-keyphrase pairs. In practice, however, it is time-consuming to annotate a large collection of documents with keyphrases for sufficient WAM training. In order to solve the problem, we use titles and summar"
W11-0316,J93-2003,0,0.0857214,"5 Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 135–144, c Portland, Oregon, USA, 23–24 June 2011. 2011 Association for Computational Linguistics documents and their keyphrases: each document and its keyphrases are descriptions to the same object, but the document is written using one language, while keyphrases are written using another language. Therefore, keyphrase extraction can be regarded as a translation problem from the language of documents into the language of keyphrases. Based on the idea of translation, we use word alignment models (WAM) (Brown et al., 1993) in statistical machine translation (SMT) (Koehn, 2010) and propose a unified framework for keyphrase extraction: (1) From a collection of translation pairs of two languages, WAM learns translation probabilities between the words in the two languages. (2) According to the translation model, we are able to bridge the vocabulary gap and succeed in suggesting appropriate keyphrases, which may not necessarily frequent in their corresponding documents. As a promising approach to solve the problem of vocabulary gap, SMT has been widely exploited in many applications such as information retrieval (Be"
W11-0316,P03-1003,0,0.0261329,"on pairs of two languages, WAM learns translation probabilities between the words in the two languages. (2) According to the translation model, we are able to bridge the vocabulary gap and succeed in suggesting appropriate keyphrases, which may not necessarily frequent in their corresponding documents. As a promising approach to solve the problem of vocabulary gap, SMT has been widely exploited in many applications such as information retrieval (Berger and Lafferty, 1999; Karimzadehgan and Zhai, 2010), image and video annotation (Duygulu et al., 2002), question answering (Berger et al., 2000; Echihabi and Marcu, 2003; Murdock and Croft, 2004; Soricut and Brill, 2006; Xue et al., 2008), query expansion and rewriting (Riezler et al., 2007; Riezler et al., 2008; Riezler and Liu, 2010), summarization (Banko et al., 2000), collocation extraction (Liu et al., 2009b; Liu et al., 2010b) and paraphrasing (Quirk et al., 2004; Zhao et al., 2010). Although SMT is a widely adopted solution to vocabulary gap, for various applications using SMT, the crucial and non-trivial problem is to find appropriate and enough translation pairs for SMT. The most straightforward translation pairs for keyphrase extraction is document-"
W11-0316,W00-0405,0,0.011384,"forms than other existing methods. Moreover, the second method will improve much if we use more effective measures to identify the most important sentence. Method First Importance Precision 0.290 0.260 Recall 0.410 0.367 F-measure 0.327±0.013 0.293±0.010 Table 2: Precision, recall and F-measure of keyphrase extraction when Md = 2 by extracting one sentence to construct translation pairs. 4.1.3 When Titles/Summaries Are Unavailable Suppose in some special cases, the titles or summaries are unavailable, how can we construct translation pairs? Inspired by extraction-based document summarization (Goldstein et al., 2000; Mihalcea and Tarau, 2004), we can extract one or more important sentences from the given document to construct translation pairs. Unsupervised sentence extraction 141 4.2 Beyond Extraction: Keyphrase Generation In Section 4.1, we evaluate our method on keyphrase extraction by suggesting keyphrases from documents. In fact, our method is also able to suggest keyphrases that have not appeared in the content of given document. The ability is important especially when the length of each document is short, which itself may not contain appropriate keyphrases. We name the new task keyphrase generati"
W11-0316,W03-1028,0,0.629116,"t and the latter as Prt2d . We define Pr⟨D,T ⟩ (t|w) in Eq.(1) as the harmonic mean of the two models: ( )−1 λ) Pr⟨D,T ⟩ (t|w) ∝ Prd2tλ(t|w) + Pr(1− (3) t2d (t|w) where λ is the harmonic factor to combine the two models. When λ = 1.0 or λ = 0.0, it simply uses model Prd2t or Prt2d , correspondingly. Using the translation probabilities Pr(t|w) we can bridge the vocabulary gap between documents and keyphrases. 3.3 Keyphrase Extraction Given a document d, we rank candidate keyphrases by computing their likelihood Pr(p|d). Each candidate keyphrase p may be composed of multiple words. As shown in (Hulth, 2003), most keyphrases are noun phrases. Following (Mihalcea and Tarau, 2004; Wan and Xiao, 2008b), we simply select noun phrases from the given document as candidate keyphrases with the help of POS tags. For each word t, we compute its likelihood given d, Pr(t|d) = ∑w∈d Pr(t|w) Pr(w|d), where Pr(w|d) is the weight of the word w in d, which is measured using normalized TFIDF scores. Pr(t|w) is the translation probabilities obtained from WAM training. Using the scores of all words in candidate keyphrases, we compute the ranking score of each candidate keyphrase by summing up the scores of each word"
W11-0316,J10-4005,0,0.0159254,"ural Language Learning, pages 135–144, c Portland, Oregon, USA, 23–24 June 2011. 2011 Association for Computational Linguistics documents and their keyphrases: each document and its keyphrases are descriptions to the same object, but the document is written using one language, while keyphrases are written using another language. Therefore, keyphrase extraction can be regarded as a translation problem from the language of documents into the language of keyphrases. Based on the idea of translation, we use word alignment models (WAM) (Brown et al., 1993) in statistical machine translation (SMT) (Koehn, 2010) and propose a unified framework for keyphrase extraction: (1) From a collection of translation pairs of two languages, WAM learns translation probabilities between the words in the two languages. (2) According to the translation model, we are able to bridge the vocabulary gap and succeed in suggesting appropriate keyphrases, which may not necessarily frequent in their corresponding documents. As a promising approach to solve the problem of vocabulary gap, SMT has been widely exploited in many applications such as information retrieval (Berger and Lafferty, 1999; Karimzadehgan and Zhai, 2010),"
W11-0316,D09-1027,1,0.724961,"sarily frequent in their corresponding documents. As a promising approach to solve the problem of vocabulary gap, SMT has been widely exploited in many applications such as information retrieval (Berger and Lafferty, 1999; Karimzadehgan and Zhai, 2010), image and video annotation (Duygulu et al., 2002), question answering (Berger et al., 2000; Echihabi and Marcu, 2003; Murdock and Croft, 2004; Soricut and Brill, 2006; Xue et al., 2008), query expansion and rewriting (Riezler et al., 2007; Riezler et al., 2008; Riezler and Liu, 2010), summarization (Banko et al., 2000), collocation extraction (Liu et al., 2009b; Liu et al., 2010b) and paraphrasing (Quirk et al., 2004; Zhao et al., 2010). Although SMT is a widely adopted solution to vocabulary gap, for various applications using SMT, the crucial and non-trivial problem is to find appropriate and enough translation pairs for SMT. The most straightforward translation pairs for keyphrase extraction is document-keyphrase pairs. In practice, however, it is time-consuming to annotate a large collection of documents with keyphrases for sufficient WAM training. In order to solve the problem, we use titles and summaries to build translation pairs with docume"
W11-0316,D09-1051,0,0.0397125,"sarily frequent in their corresponding documents. As a promising approach to solve the problem of vocabulary gap, SMT has been widely exploited in many applications such as information retrieval (Berger and Lafferty, 1999; Karimzadehgan and Zhai, 2010), image and video annotation (Duygulu et al., 2002), question answering (Berger et al., 2000; Echihabi and Marcu, 2003; Murdock and Croft, 2004; Soricut and Brill, 2006; Xue et al., 2008), query expansion and rewriting (Riezler et al., 2007; Riezler et al., 2008; Riezler and Liu, 2010), summarization (Banko et al., 2000), collocation extraction (Liu et al., 2009b; Liu et al., 2010b) and paraphrasing (Quirk et al., 2004; Zhao et al., 2010). Although SMT is a widely adopted solution to vocabulary gap, for various applications using SMT, the crucial and non-trivial problem is to find appropriate and enough translation pairs for SMT. The most straightforward translation pairs for keyphrase extraction is document-keyphrase pairs. In practice, however, it is time-consuming to annotate a large collection of documents with keyphrases for sufficient WAM training. In order to solve the problem, we use titles and summaries to build translation pairs with docume"
W11-0316,D10-1036,1,0.737756,"their corresponding documents. As a promising approach to solve the problem of vocabulary gap, SMT has been widely exploited in many applications such as information retrieval (Berger and Lafferty, 1999; Karimzadehgan and Zhai, 2010), image and video annotation (Duygulu et al., 2002), question answering (Berger et al., 2000; Echihabi and Marcu, 2003; Murdock and Croft, 2004; Soricut and Brill, 2006; Xue et al., 2008), query expansion and rewriting (Riezler et al., 2007; Riezler et al., 2008; Riezler and Liu, 2010), summarization (Banko et al., 2000), collocation extraction (Liu et al., 2009b; Liu et al., 2010b) and paraphrasing (Quirk et al., 2004; Zhao et al., 2010). Although SMT is a widely adopted solution to vocabulary gap, for various applications using SMT, the crucial and non-trivial problem is to find appropriate and enough translation pairs for SMT. The most straightforward translation pairs for keyphrase extraction is document-keyphrase pairs. In practice, however, it is time-consuming to annotate a large collection of documents with keyphrases for sufficient WAM training. In order to solve the problem, we use titles and summaries to build translation pairs with documents. Titles and sum"
W11-0316,P10-1085,0,0.171686,"their corresponding documents. As a promising approach to solve the problem of vocabulary gap, SMT has been widely exploited in many applications such as information retrieval (Berger and Lafferty, 1999; Karimzadehgan and Zhai, 2010), image and video annotation (Duygulu et al., 2002), question answering (Berger et al., 2000; Echihabi and Marcu, 2003; Murdock and Croft, 2004; Soricut and Brill, 2006; Xue et al., 2008), query expansion and rewriting (Riezler et al., 2007; Riezler et al., 2008; Riezler and Liu, 2010), summarization (Banko et al., 2000), collocation extraction (Liu et al., 2009b; Liu et al., 2010b) and paraphrasing (Quirk et al., 2004; Zhao et al., 2010). Although SMT is a widely adopted solution to vocabulary gap, for various applications using SMT, the crucial and non-trivial problem is to find appropriate and enough translation pairs for SMT. The most straightforward translation pairs for keyphrase extraction is document-keyphrase pairs. In practice, however, it is time-consuming to annotate a large collection of documents with keyphrases for sufficient WAM training. In order to solve the problem, we use titles and summaries to build translation pairs with documents. Titles and sum"
W11-0316,J03-1002,0,0.0357929,"ticles are manually annotated with keyphrases by website editors, and all these keyphrases come from the corresponding documents. Each news article is also provided with a title and a short summary. In this dataset, there are 72, 900 unique words in documents, and 12, 405 unique words in keyphrases. The average lengths of documents, titles and summaries are 971.7 words, 11.6 words, and 45.8 words, respectively. The average number of keyphrases for each document is 2.4. In experiments, we use the annotated titles and summaries to construct translation pairs. In experiments, we select GIZA++ 3 (Och and Ney, 2003) to train IBM Model-1 using translation pairs. GIZA++, widely used in various applications of statistical machine translation, implements IBM Models 1-5 and an HMM word alignment model. To evaluate methods, we use the annotated keyphrases by www.163.com as the standard keyphrases. If one suggested keyphrase exactly matches one of the standard keyphrases, it is a correct keyphrase. We use precision p = ccorrect /cmethod , recall r = ccorrect /cstandard and Fmeasure f = 2pr/(p + r) for evaluation, where ccorrect is the number of keyphrases correctly suggested by the given method, cmethod is the"
W11-0316,W04-3219,0,0.0225548,"romising approach to solve the problem of vocabulary gap, SMT has been widely exploited in many applications such as information retrieval (Berger and Lafferty, 1999; Karimzadehgan and Zhai, 2010), image and video annotation (Duygulu et al., 2002), question answering (Berger et al., 2000; Echihabi and Marcu, 2003; Murdock and Croft, 2004; Soricut and Brill, 2006; Xue et al., 2008), query expansion and rewriting (Riezler et al., 2007; Riezler et al., 2008; Riezler and Liu, 2010), summarization (Banko et al., 2000), collocation extraction (Liu et al., 2009b; Liu et al., 2010b) and paraphrasing (Quirk et al., 2004; Zhao et al., 2010). Although SMT is a widely adopted solution to vocabulary gap, for various applications using SMT, the crucial and non-trivial problem is to find appropriate and enough translation pairs for SMT. The most straightforward translation pairs for keyphrase extraction is document-keyphrase pairs. In practice, however, it is time-consuming to annotate a large collection of documents with keyphrases for sufficient WAM training. In order to solve the problem, we use titles and summaries to build translation pairs with documents. Titles and summaries are usually accompanying with th"
W11-0316,J10-3010,0,0.0338656,"vocabulary gap and succeed in suggesting appropriate keyphrases, which may not necessarily frequent in their corresponding documents. As a promising approach to solve the problem of vocabulary gap, SMT has been widely exploited in many applications such as information retrieval (Berger and Lafferty, 1999; Karimzadehgan and Zhai, 2010), image and video annotation (Duygulu et al., 2002), question answering (Berger et al., 2000; Echihabi and Marcu, 2003; Murdock and Croft, 2004; Soricut and Brill, 2006; Xue et al., 2008), query expansion and rewriting (Riezler et al., 2007; Riezler et al., 2008; Riezler and Liu, 2010), summarization (Banko et al., 2000), collocation extraction (Liu et al., 2009b; Liu et al., 2010b) and paraphrasing (Quirk et al., 2004; Zhao et al., 2010). Although SMT is a widely adopted solution to vocabulary gap, for various applications using SMT, the crucial and non-trivial problem is to find appropriate and enough translation pairs for SMT. The most straightforward translation pairs for keyphrase extraction is document-keyphrase pairs. In practice, however, it is time-consuming to annotate a large collection of documents with keyphrases for sufficient WAM training. In order to solve t"
W11-0316,P07-1059,0,0.0105715,"ranslation model, we are able to bridge the vocabulary gap and succeed in suggesting appropriate keyphrases, which may not necessarily frequent in their corresponding documents. As a promising approach to solve the problem of vocabulary gap, SMT has been widely exploited in many applications such as information retrieval (Berger and Lafferty, 1999; Karimzadehgan and Zhai, 2010), image and video annotation (Duygulu et al., 2002), question answering (Berger et al., 2000; Echihabi and Marcu, 2003; Murdock and Croft, 2004; Soricut and Brill, 2006; Xue et al., 2008), query expansion and rewriting (Riezler et al., 2007; Riezler et al., 2008; Riezler and Liu, 2010), summarization (Banko et al., 2000), collocation extraction (Liu et al., 2009b; Liu et al., 2010b) and paraphrasing (Quirk et al., 2004; Zhao et al., 2010). Although SMT is a widely adopted solution to vocabulary gap, for various applications using SMT, the crucial and non-trivial problem is to find appropriate and enough translation pairs for SMT. The most straightforward translation pairs for keyphrase extraction is document-keyphrase pairs. In practice, however, it is time-consuming to annotate a large collection of documents with keyphrases fo"
W11-0316,C08-1093,0,0.0246522,"re able to bridge the vocabulary gap and succeed in suggesting appropriate keyphrases, which may not necessarily frequent in their corresponding documents. As a promising approach to solve the problem of vocabulary gap, SMT has been widely exploited in many applications such as information retrieval (Berger and Lafferty, 1999; Karimzadehgan and Zhai, 2010), image and video annotation (Duygulu et al., 2002), question answering (Berger et al., 2000; Echihabi and Marcu, 2003; Murdock and Croft, 2004; Soricut and Brill, 2006; Xue et al., 2008), query expansion and rewriting (Riezler et al., 2007; Riezler et al., 2008; Riezler and Liu, 2010), summarization (Banko et al., 2000), collocation extraction (Liu et al., 2009b; Liu et al., 2010b) and paraphrasing (Quirk et al., 2004; Zhao et al., 2010). Although SMT is a widely adopted solution to vocabulary gap, for various applications using SMT, the crucial and non-trivial problem is to find appropriate and enough translation pairs for SMT. The most straightforward translation pairs for keyphrase extraction is document-keyphrase pairs. In practice, however, it is time-consuming to annotate a large collection of documents with keyphrases for sufficient WAM train"
W11-0316,C08-1122,0,0.416422,"is alleviates the problem of vocabulary gap to some extent. TextRank, however, still tends to extract high-frequency words as keyphrases because these words have more opportunities to get linked with other words and obtain higher PageRank scores. Moreover, TextRank usually constructs a word graph simply according to word co-occurrences as an approximation of the semantic relations between words. This will introduce much noise because of connecting semantically unrelated words and highly influence extraction performance. Some methods have been proposed to improve TextRank, of which ExpandRank (Wan and Xiao, 2008b; Wan and Xiao, 2008a) uses a small number, namely k, of neighbor documents to provide more information of word relatedness for the construction of word graphs. Compared to TextRank, ExpandRank performs better when facing the vocabulary gap by borrowing the information on document level. However, the finding of neighbor documents are usually arbitrary. This process may introduce much noise and result in topic drift when the document and its so-called neighbor documents are not exactly talking about the same topics. Another potential approach to alleviate vocabulary gap is latent topic models"
W11-0316,C10-1148,0,0.0499411,"solve the problem of vocabulary gap, SMT has been widely exploited in many applications such as information retrieval (Berger and Lafferty, 1999; Karimzadehgan and Zhai, 2010), image and video annotation (Duygulu et al., 2002), question answering (Berger et al., 2000; Echihabi and Marcu, 2003; Murdock and Croft, 2004; Soricut and Brill, 2006; Xue et al., 2008), query expansion and rewriting (Riezler et al., 2007; Riezler et al., 2008; Riezler and Liu, 2010), summarization (Banko et al., 2000), collocation extraction (Liu et al., 2009b; Liu et al., 2010b) and paraphrasing (Quirk et al., 2004; Zhao et al., 2010). Although SMT is a widely adopted solution to vocabulary gap, for various applications using SMT, the crucial and non-trivial problem is to find appropriate and enough translation pairs for SMT. The most straightforward translation pairs for keyphrase extraction is document-keyphrase pairs. In practice, however, it is time-consuming to annotate a large collection of documents with keyphrases for sufficient WAM training. In order to solve the problem, we use titles and summaries to build translation pairs with documents. Titles and summaries are usually accompanying with the corresponding docu"
W11-0316,W04-3252,0,\N,Missing
