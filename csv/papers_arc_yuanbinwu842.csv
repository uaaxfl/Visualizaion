2021.findings-acl.189,Attending via both Fine-tuning and Compressing,2021,-1,-1,2,0.588046,3630,jie zhou,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.findings-acl.277,Probabilistic Graph Reasoning for Natural Proof Generation,2021,-1,-1,5,1,8176,changzhi sun,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.emnlp-main.226,From Alignment to Assignment: Frustratingly Simple Unsupervised Entity Alignment,2021,-1,-1,3,0,9117,xin mao,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Cross-lingual entity alignment (EA) aims to find the equivalent entities between crosslingual KGs (Knowledge Graphs), which is a crucial step for integrating KGs. Recently, many GNN-based EA methods are proposed and show decent performance improvements on several public datasets. However, existing GNN-based EA methods inevitably inherit poor interpretability and low efficiency from neural networks. Motivated by the isomorphic assumption of GNN-based methods, we successfully transform the cross-lingual EA problem into an assignment problem. Based on this re-definition, we propose a frustratingly Simple but Effective Unsupervised entity alignment method (SEU) without neural networks. Extensive experiments have been conducted to show that our proposed unsupervised approach even beats advanced supervised methods across all public datasets while having high efficiency, interpretability, and stability."
2021.emnlp-main.338,Word Reordering for Zero-shot Cross-lingual Structured Prediction,2021,-1,-1,6,1,9401,tao ji,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Adapting word order from one language to another is a key problem in cross-lingual structured prediction. Current sentence encoders (e.g., RNN, Transformer with position embeddings) are usually word order sensitive. Even with uniform word form representations (MUSE, mBERT), word order discrepancies may hurt the adaptation of models. In this paper, we build structured prediction models with bag-of-words inputs, and introduce a new reordering module to organizing words following the source language order, which learns task-specific reordering strategies from a general-purpose order predictor model. Experiments on zero-shot cross-lingual dependency parsing, POS tagging, and morphological tagging show that our model can significantly improve target language performances, especially for languages that are distant from the source language."
2021.emnlp-main.339,A Unified Encoding of Structures in Transition Systems,2021,-1,-1,6,1,9401,tao ji,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Transition systems usually contain various dynamic structures (e.g., stacks, buffers). An ideal transition-based model should encode these structures completely and efficiently. Previous works relying on templates or neural network structures either only encode partial structure information or suffer from computation efficiency. In this paper, we propose a novel attention-based encoder unifying representation of all structures in a transition system. Specifically, we separate two views of items on structures, namely structure-invariant view and structure-dependent view. With the help of parallel-friendly attention network, we are able to encoding transition states with O(1) additional complexity (with respect to basic feature extractors). Experiments on the PTB and UD show that our proposed method significantly improves the test speed and achieves the best transition-based model, and is comparable to state-of-the-art methods."
2021.eacl-main.49,Is {``}hot pizza{''} Positive or Negative? Mining Target-aware Sentiment Lexicons,2021,-1,-1,2,0.588046,3630,jie zhou,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"Modelling a word{'}s polarity in different contexts is a key task in sentiment analysis. Previous works mainly focus on domain dependencies, and assume words{'} sentiments are invariant within a specific domain. In this paper, we relax this assumption by binding a word{'}s sentiment to its collocation words instead of domain labels. This finer view of sentiment contexts is particularly useful for identifying commonsense sentiments expressed in neural words such as {``}big{''} and {``}long{''}. Given a target (e.g., an aspect), we propose an effective {``}perturb-and-see{''} method to extract sentiment words modifying it from large-scale datasets. The reliability of the obtained target-aware sentiment lexicons is extensively evaluated both manually and automatically. We also show that a simple application of the lexicon is able to achieve highly competitive performances on the unsupervised opinion relation extraction task."
2021.eacl-main.251,{ENPAR}:Enhancing Entity and Entity Pair Representations for Joint Entity Relation Extraction,2021,-1,-1,3,1,10882,yijun wang,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"Current state-of-the-art systems for joint entity relation extraction (Luan et al., 2019; Wad-den et al., 2019) usually adopt the multi-task learning framework. However, annotations for these additional tasks such as coreference resolution and event extraction are always equally hard (or even harder) to obtain. In this work, we propose a pre-training method ENPAR to improve the joint extraction performance. ENPAR requires only the additional entity annotations that are much easier to collect. Unlike most existing works that only consider incorporating entity information into the sentence encoder, we further utilize the entity pair information. Specifically, we devise four novel objectives,i.e., masked entity typing, masked entity prediction, adversarial context discrimination, and permutation prediction, to pre-train an entity encoder and an entity pair encoder. Comprehensive experiments show that the proposed pre-training method achieves significant improvement over BERT on ACE05, SciERC, and NYT, and outperforms current state-of-the-art on ACE05."
2021.acl-long.19,{U}ni{RE}: A Unified Label Space for Entity Relation Extraction,2021,-1,-1,3,1,10882,yijun wang,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Many joint entity relation extraction models setup two separated label spaces for the two sub-tasks (i.e., entity detection and relation classification). We argue that this setting may hinder the information interaction between entities and relations. In this work, we propose to eliminate the different treatment on the two sub-tasks{'} label spaces. The input of our model is a table containing all word pairs from a sentence. Entities and relations are represented by squares and rectangles in the table. We apply a unified classifier to predict each cell{'}s label, which unifies the learning of two sub-tasks. For testing, an effective (yet fast) approximate decoder is proposed for finding squares and rectangles from tables. Experiments on three benchmarks (ACE04, ACE05, SciERC) show that, using only half the number of parameters, our model achieves competitive accuracy with the best extractor, and is faster."
2020.emnlp-main.132,Pre-training Entity Relation Encoder with Intra-span and Inter-span Information,2020,-1,-1,3,1,10882,yijun wang,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"In this paper, we integrate span-related information into pre-trained encoder for entity relation extraction task. Instead of using general-purpose sentence encoder (e.g., existing universal pre-trained models), we introduce a span encoder and a span pair encoder to the pre-training network, which makes it easier to import intra-span and inter-span information into the pre-trained model. To learn the encoders, we devise three customized pre-training objectives from different perspectives, which target on tokens, spans, and span pairs. In particular, a span encoder is trained to recover a random shuffling of tokens in a span, and a span pair encoder is trained to predict positive pairs that are from the same sentences and negative pairs that are from different sentences using contrastive loss. Experimental results show that the proposed pre-training method outperforms distantly supervised pre-training, and achieves promising performance on two entity relation extraction benchmark datasets (ACE05, SciERC)."
2020.coling-main.49,{S}enti{X}: A Sentiment-Aware Pre-Trained Model for Cross-Domain Sentiment Analysis,2020,-1,-1,4,0.588046,3630,jie zhou,Proceedings of the 28th International Conference on Computational Linguistics,0,"Pre-trained language models have been widely applied to cross-domain NLP tasks like sentiment analysis, achieving state-of-the-art performance. However, due to the variety of users{'} emotional expressions across domains, fine-tuning the pre-trained models on the source domain tends to overfit, leading to inferior results on the target domain. In this paper, we pre-train a sentiment-aware language model (SentiX) via domain-invariant sentiment knowledge from large-scale review datasets, and utilize it for cross-domain sentiment analysis task without fine-tuning. We propose several pre-training tasks based on existing lexicons and annotations at both token and sentence levels, such as emoticons, sentiment words, and ratings, without human interference. A series of experiments are conducted and the results indicate the great advantages of our model. We obtain new state-of-the-art results in all the cross-domain sentiment analysis tasks, and our proposed SentiX can be trained with only 1{\%} samples (18 samples) and it achieves better performance than BERT with 90{\%} samples."
2020.acl-main.299,A Span-based Linearization for Constituent Trees,2020,30,0,2,0,4718,yang wei,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"We propose a novel linearization of a constituent tree, together with a new locally normalized model. For each split point in a sentence, our model computes the normalizer on all spans ending with that split point, and then predicts a tree span from them. Compared with global models, our model is fast and parallelizable. Different from previous local models, our linearization method is tied on the spans directly and considers more local features when performing span prediction, which is more interpretable and effective. Experiments on PTB (95.8 F1) and CTB (92.4 F1) show that our model significantly outperforms existing local models and efficiently achieves competitive results with global models."
P19-1131,Joint Type Inference on Entities and Relations via Graph Convolutional Networks,2019,0,6,3,1,8176,changzhi sun,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"We develop a new paradigm for the task of joint entity relation extraction. It first identifies entity spans, then performs a joint inference on entity types and relation types. To tackle the joint type inference task, we propose a novel graph convolutional network (GCN) running on an entity-relation bipartite graph. By introducing a binary relation classification task, we are able to utilize the structure of entity-relation bipartite graph in a more efficient and interpretable way. Experiments on ACE05 show that our model outperforms existing joint models in entity performance and is competitive with the state-of-the-art in relation performance."
P19-1237,Graph-based Dependency Parsing with Graph Neural Networks,2019,0,7,2,1,9401,tao ji,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"We investigate the problem of efficiently incorporating high-order features into neural graph-based dependency parsing. Instead of explicitly extracting high-order features from intermediate parse trees, we develop a more powerful dependency tree node representation which captures high-order information concisely and efficiently. We use graph neural networks (GNNs) to learn the representations and discuss several new configurations of GNN{'}s updating and aggregation functions. Experiments on PTB show that our parser achieves the best UAS and LAS on PTB (96.0{\%}, 94.3{\%}) among systems without using any external resources."
D19-1635,Exploring Human Gender Stereotypes with Word Association Test,2019,0,2,2,0,10190,yupei du,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"Word embeddings have been widely used to study gender stereotypes in texts. One key problem regarding existing bias scores is to evaluate their validities: do they really reflect true bias levels? For a small set of words (e.g. occupations), we can rely on human annotations or external data. However, for most words, evaluating the correctness of them is still an open problem. In this work, we utilize word association test, which contains rich types of word connections annotated by human participants, to explore how gender stereotypes spread within our minds. Specifically, we use random walk on word association graph to derive bias scores for a large amount of words. Experiments show that these bias scores correlate well with bias in the real world. More importantly, comparing with word-embedding-based bias scores, it provides a different perspective on gender stereotypes in words."
S18-1035,{ECNU} at {S}em{E}val-2018 Task 1: Emotion Intensity Prediction Using Effective Features and Machine Learning Models,2018,0,1,3,0,25857,huimin xu,Proceedings of The 12th International Workshop on Semantic Evaluation,0,"This paper describes our submissions to SemEval 2018 task 1. The task is affect intensity prediction in tweets, including five subtasks. We participated in all subtasks of English tweets. We extracted several traditional NLP, sentiment lexicon, emotion lexicon and domain specific features from tweets, adopted supervised machine learning algorithms to perform emotion intensity prediction."
S18-1068,{ECNU} at {S}em{E}val-2018 Task 2: Leverage Traditional {NLP} Features and Neural Networks Methods to Address {T}witter Emoji Prediction Task,2018,0,1,4,0,28805,xingwu lu,Proceedings of The 12th International Workshop on Semantic Evaluation,0,"This paper describes our submissions to Task 2 in SemEval 2018, i.e., Multilingual Emoji Prediction. We first investigate several traditional Natural Language Processing (NLP) features, and then design several deep learning models. For subtask 1: Emoji Prediction in English, we combine two different methods to represent tweet, i.e., supervised model using traditional features and deep learning model. For subtask 2: Emoji Prediction in Spanish, we only use deep learning model."
S18-1165,{ECNU} at {S}em{E}val-2018 Task 10: Evaluating Simple but Effective Features on Machine Learning Methods for Semantic Difference Detection,2018,0,1,3,1,28907,yunxiao zhou,Proceedings of The 12th International Workshop on Semantic Evaluation,0,"This paper describes the system we submitted to Task 10 (Capturing Discriminative Attributes) in SemEval 2018. Given a triple (word1, word2, attribute), this task is to predict whether it exemplifies a semantic difference or not. We design and investigate several word embedding features, PMI features and WordNet features together with supervised machine learning methods to address this task. Officially released results show that our system ranks above average."
S18-1175,{ECNU} at {S}em{E}val-2018 Task 11: Using Deep Learning Method to Address Machine Comprehension Task,2018,0,1,3,0,28915,yixuan sheng,Proceedings of The 12th International Workshop on Semantic Evaluation,0,"This paper describes the system we submitted to the Task 11 in SemEval 2018, i.e., Machine Comprehension using Commonsense Knowledge. Given a passage and some questions that each have two candidate answers, this task requires the participate system to select out one answer meet the meaning of original text or commonsense knowledge from the candidate answers. For this task, we use a deep learning method to obtain final predict answer by calculating relevance of choices representations and question-aware document representation."
S18-1184,{ECNU} at {S}em{E}val-2018 Task 12: An End-to-End Attention-based Neural Network for the Argument Reasoning Comprehension Task,2018,0,0,3,1,2031,junfeng tian,Proceedings of The 12th International Workshop on Semantic Evaluation,0,"This paper presents our submissions to SemEval 2018 Task 12: the Argument Reasoning Comprehension Task. We investigate an end-to-end attention-based neural network to represent the two lexically close candidate warrants. On the one hand, we extract their different parts as attention vectors to obtain distinguishable representations. On the other hand, we use their surrounds (i.e., claim, reason, debate context) as another attention vectors to get contextual representations, which work as final clues to select the correct warrant. Our model achieves 60.4{\%} accuracy and ranks 3rd among 22 participating systems."
K18-2025,{A}nt{NLP} at {C}o{NLL} 2018 Shared Task: A Graph-Based Parser for {U}niversal {D}ependency Parsing,2018,0,1,4,1,9401,tao ji,Proceedings of the {C}o{NLL} 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies,0,"We describe the graph-based dependency parser in our system (AntNLP) submitted to the CoNLL 2018 UD Shared Task. We use bidirectional lstm to get the word representation, then a bi-affine pointer networks to compute scores of candidate dependency edges and the MST algorithm to get the final dependency tree. From the official testing results, our system gets 70.90 LAS F1 score (rank 9/26), 55.92 MLAS (10/26) and 60.91 BLEX (8/26)."
D18-1249,Extracting Entities and Relations with Joint Minimum Risk Training,2018,0,4,2,1,8176,changzhi sun,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"We investigate the task of joint entity relation extraction. Unlike prior efforts, we propose a new lightweight joint learning paradigm based on minimum risk training (MRT). Specifically, our algorithm optimizes a global loss function which is flexible and effective to explore interactions between the entity model and the relation model. We implement a strong and simple neural network where the MRT is executed. Experiment results on the benchmark ACE05 and NYT datasets show that our model is able to achieve state-of-the-art joint extraction performances."
S17-2028,{ECNU} at {S}em{E}val-2017 Task 1: Leverage Kernel-based Traditional {NLP} features and Neural Networks to Build a Universal Model for Multilingual and Cross-lingual Semantic Textual Similarity,2017,0,25,4,1,2031,junfeng tian,Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017),0,"To address semantic similarity on multilingual and cross-lingual sentences, we firstly translate other foreign languages into English, and then feed our monolingual English system with various interactive features. Our system is further supported by combining with deep learning semantic similarity and our best run achieves the mean Pearson correlation 73.16{\%} in primary track."
S17-2060,{ECNU} at {S}em{E}val-2017 Task 3: Using Traditional and Deep Learning Methods to Address Community Question Answering Task,2017,0,3,4,0,32290,guoshun wu,Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017),0,"This paper describes the systems we submitted to the task 3 (Community Question Answering) in SemEval 2017 which contains three subtasks on English corpora, i.e., subtask A: Question-Comment Similarity, subtask B: Question-Question Similarity, and subtask C: Question-External Comment Similarity. For subtask A, we combined two different methods to represent question-comment pair, i.e., supervised model using traditional features and Convolutional Neural Network. For subtask B, we utilized the information of snippets returned from Search Engine with question subject as query. For subtask C, we ranked the comments by multiplying the probability of the pair related question comment being Good by the reciprocal rank of the related question."
S17-2078,{ECNU} at {S}em{E}val-2017 Task 7: Using Supervised and Unsupervised Methods to Detect and Locate {E}nglish Puns,2017,0,3,3,0,32309,yuhuan xiu,Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017),0,"This paper describes our submissions to task 7 in SemEval 2017, i.e., Detection and Interpretation of English Puns. We participated in the first two subtasks, which are to detect and locate English puns respectively. For subtask 1, we presented a supervised system to determine whether or not a sentence contains a pun using similarity features calculated on sense vectors or cluster center vectors. For subtask 2, we established an unsupervised system to locate the pun by scoring each word in the sentence and we assumed that the word with the smallest score is the pun."
S17-2086,{ECNU} at {S}em{E}val-2017 Task 8: Rumour Evaluation Using Effective Features and Supervised Ensemble Models,2017,0,9,3,0,28843,feixiang wang,Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017),0,"This paper describes our submissions to task 8 in SemEval 2017, i.e., Determining rumour veracity and support for rumours. Given a rumoured tweet and a lot of reply tweets, the subtask A is to label whether these tweets are support, deny, query or comment, and the subtask B aims to predict the veracity (i.e., true, false, and unverified) with a confidence (in range of 0-1) of the given rumoured tweet. For both subtasks, we adopted supervised machine learning methods, incorporating rich features. Since training data is imbalanced, we specifically designed a two-step classifier to address subtask A ."
S17-2137,{ECNU} at {S}em{E}val-2017 Task 4: Evaluating Effective Features on Machine Learning Methods for {T}witter Message Polarity Classification,2017,0,2,3,1,28907,yunxiao zhou,Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017),0,"This paper reports our submission to subtask A of task 4 (Sentiment Analysis in Twitter, SAT) in SemEval 2017, i.e., Message Polarity Classification. We investigated several traditional Natural Language Processing (NLP) features, domain specific features and word embedding features together with supervised machine learning methods to address this task. Officially released results showed that our system ranked above average."
S17-2152,{ECNU} at {S}em{E}val-2017 Task 5: An Ensemble of Regression Algorithms with Effective Features for Fine-Grained Sentiment Analysis in Financial Domain,2017,0,11,3,0,32374,mengxiao jiang,Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017),0,"This paper describes our systems submitted to the Fine-Grained Sentiment Analysis on Financial Microblogs and News task (i.e., Task 5) in SemEval-2017. This task includes two subtasks in microblogs and news headline domain respectively. To settle this problem, we extract four types of effective features, including linguistic features, sentiment lexicon features, domain-specific features and word embedding features. Then we employ these features to construct models by using ensemble regression algorithms. Our submissions rank 1st and rank 5th in subtask 1 and subtask 2 respectively."
K17-3025,A Fast and Lightweight System for Multilingual Dependency Parsing,2017,5,1,2,1,9401,tao ji,Proceedings of the {C}o{NLL} 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies,0,"We present a multilingual dependency parser with a bidirectional-LSTM (BiLSTM) feature extractor and a multi-layer perceptron (MLP) classifier. We trained our transition-based projective parser in UD version 2.0 datasets without any additional data. The parser is fast, lightweight and effective on big treebanks. In the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, the official results show that the macro-averaged LAS F1 score of our system Mengest is 61.33{\%}."
E17-1097,Large-scale Opinion Relation Extraction with Distantly Supervised Neural Network,2017,25,0,2,1,8176,changzhi sun,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",0,"We investigate the task of open domain opinion relation extraction. Different from works on manually labeled corpus, we propose an efficient distantly supervised framework based on pattern matching and neural network classifiers. The patterns are designed to automatically generate training data, and the deep learning model is design to capture various lexical and syntactic features. The result algorithm is fast and scalable on large-scale corpus. We test the system on the Amazon online review dataset. The result shows that our model is able to achieve promising performances without any human annotations."
D17-1134,Multi-task Attention-based Neural Networks for Implicit Discourse Relationship Representation and Identification,2017,0,12,3,0.892245,9119,man lan,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"We present a novel multi-task attention based neural network model to address implicit discourse relationship representation and identification through two types of representation learning, an attention based neural network for learning discourse relationship representation with two arguments and a multi-task framework for learning knowledge from annotated and unannotated corpora. The extensive experiments have been performed on two benchmark corpora (i.e., PDTB and CoNLL-2016 datasets). Experimental results show that our proposed model outperforms the state-of-the-art systems on benchmark corpora."
W13-3601,The {C}o{NLL}-2013 Shared Task on Grammatical Error Correction,2013,20,104,3,0,7314,hwee ng,Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,0,"The CoNLL-2013 shared task was devoted to grammatical error correction. In this paper, we give the task definition, present the data sets, and describe the evaluation metric and scorer used in the shared task. We also give an overview of the various approaches adopted by the participating teams, and present the evaluation results."
P13-1143,Grammatical Error Correction Using Integer Linear Programming,2013,23,13,1,1,7967,yuanbin wu,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We propose a joint inference algorithm for grammatical error correction. Different from most previous work where different error types are corrected independently, our proposed inference process considers all possible errors in a uni ed framework. We use integer linear programming (ILP) to model the inference process, which can easily incorporate both the power of existing error classi ers and prior knowledge on grammatical error correction. Experimental results on the Helping Our Own shared task show that our method is competitive with state-of-the-art systems."
D11-1123,Structural Opinion Mining for Graph-based Sentiment Representation,2011,21,25,1,1,7967,yuanbin wu,Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,0,"Based on analysis of on-line review corpus we observe that most sentences have complicated opinion structures and they cannot be well represented by existing methods, such as frame-based and feature-based ones. In this work, we propose a novel graph-based representation for sentence level sentiment. An integer linear programming-based structural learning method is then introduced to produce the graph representations of input sentences. Experimental evaluations on a manually labeled Chinese corpus demonstrate the effectiveness of the proposed approach."
D09-1159,Phrase Dependency Parsing for Opinion Mining,2009,22,253,1,1,7967,yuanbin wu,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"In this paper, we present a novel approach for mining opinions from product reviews, where it converts opinion mining task to identify product features, expressions of opinions and relations between them. By taking advantage of the observation that a lot of product features are phrases, a concept of phrase dependency parsing is introduced, which extends traditional dependency parsing to phrase level. This concept is then implemented for extracting relations between product features and expressions of opinions. Experimental evaluations show that the mining task can benefit from phrase dependency parsing."
