2020.acl-main.412,D19-1522,0,0.0769187,"Missing"
2020.acl-main.412,C16-1236,0,0.0329755,"able constraint imposed by previous methods for this task. 3. Through extensive experiments on multiple real-world datasets, we demonstrate EmbedKGQA’s effectiveness over state-of-the-art baselines. We have made EmbedKGQA’s source code available to encourage reproducibility. 4499 2 Related Work KGQA: In prior work (Li et al., 2018) TransE, (Bordes et al., 2013) embeddings have been used to answer factoid based questions. However, this requires ground truth relation labeling for each question and it does not work for multi-hop question answering. In another line of work (Yih et al., 2015) and (Bao et al., 2016) proposed extracting a particular sub-graph to answer the question. The method presented in (Bordes et al., 2014a), the sub-graph generated for a head entity is projected in a high dimensional space for question answering. Memory Networks have also been used to learn high dimensional embeddings of the facts present in the KG to perform QA (Bordes et al., 2015). Methods like (Bordes et al., 2014b) learn a similarity function between the question and the corresponding triple during training, and score the question with all the candidate triples at the test time. (Yang et al., 2014b) and (Yang et"
2020.acl-main.412,D14-1067,0,0.0999539,"Missing"
2020.acl-main.412,P16-1076,0,0.0183609,"14a), the sub-graph generated for a head entity is projected in a high dimensional space for question answering. Memory Networks have also been used to learn high dimensional embeddings of the facts present in the KG to perform QA (Bordes et al., 2015). Methods like (Bordes et al., 2014b) learn a similarity function between the question and the corresponding triple during training, and score the question with all the candidate triples at the test time. (Yang et al., 2014b) and (Yang et al., 2015) utilize embedding based methods to map natural language questions to logical forms. Methods like (Dai et al., 2016; Dong et al., 2015; Hao et al., 2017; Lukovnikov et al., 2017; Yin et al., 2016) utilize neural networks to learn a scoring functions to rank the candidate answers. Some works like (Mohammed et al., 2017; Ture and Jojic, 2016) consider each relation as a label and model QA task as a classification problem. Extending these kinds of approaches for multi-hop question answering is non-trivial. Recently, there has been some work in which text corpus is incorporated as a knowledge source in addition to KG to answer complex questions on KGs (Sun et al., 2018, 2019a). Such approaches are useful in ca"
2020.acl-main.412,P15-1026,0,0.0294419,"h generated for a head entity is projected in a high dimensional space for question answering. Memory Networks have also been used to learn high dimensional embeddings of the facts present in the KG to perform QA (Bordes et al., 2015). Methods like (Bordes et al., 2014b) learn a similarity function between the question and the corresponding triple during training, and score the question with all the candidate triples at the test time. (Yang et al., 2014b) and (Yang et al., 2015) utilize embedding based methods to map natural language questions to logical forms. Methods like (Dai et al., 2016; Dong et al., 2015; Hao et al., 2017; Lukovnikov et al., 2017; Yin et al., 2016) utilize neural networks to learn a scoring functions to rank the candidate answers. Some works like (Mohammed et al., 2017; Ture and Jojic, 2016) consider each relation as a label and model QA task as a classification problem. Extending these kinds of approaches for multi-hop question answering is non-trivial. Recently, there has been some work in which text corpus is incorporated as a knowledge source in addition to KG to answer complex questions on KGs (Sun et al., 2018, 2019a). Such approaches are useful in case the KG is incomp"
2020.acl-main.412,P17-1021,0,0.0282899,"ead entity is projected in a high dimensional space for question answering. Memory Networks have also been used to learn high dimensional embeddings of the facts present in the KG to perform QA (Bordes et al., 2015). Methods like (Bordes et al., 2014b) learn a similarity function between the question and the corresponding triple during training, and score the question with all the candidate triples at the test time. (Yang et al., 2014b) and (Yang et al., 2015) utilize embedding based methods to map natural language questions to logical forms. Methods like (Dai et al., 2016; Dong et al., 2015; Hao et al., 2017; Lukovnikov et al., 2017; Yin et al., 2016) utilize neural networks to learn a scoring functions to rank the candidate answers. Some works like (Mohammed et al., 2017; Ture and Jojic, 2016) consider each relation as a label and model QA task as a classification problem. Extending these kinds of approaches for multi-hop question answering is non-trivial. Recently, there has been some work in which text corpus is incorporated as a knowledge source in addition to KG to answer complex questions on KGs (Sun et al., 2018, 2019a). Such approaches are useful in case the KG is incomplete. However, thi"
2020.acl-main.412,2021.ccl-1.108,0,0.143499,"Missing"
2020.acl-main.412,D16-1147,0,0.121195,"final output of the last hidden layer of RoBERTa (hq ) and the embedding of relation r (hr ). hq = RoBERTa(q 0 ) S(r, q) = sigmoid(hTq hr ) Among all the relations, we select those relations which have score greater than 0.5 It is denoted as the set Ra . For each candidate entity a0 that we have obtained so far (Section 4.4), we find the relations in the shortest path between head entity h and a0 . Let this set of relations be Ra0 . Now the relation score for each candidate answer entity is defined as the size of their intersection. 5.2 We compare our model with the Key-Value Memory Network (Miller et al., 2016), the GraftNet (Sun et al., 2018) and the Pullnet (Sun et al., 2019a) for WebQuestionsSP dataset. For MetaQA dataset we also compare with the VRN (Zhang et al., 2018). These methods implement multi-hop KGQA, and except VRN, use additional text corpus to mitigate the KG sparsity problem. RelScorea0 = |Ra ∩ Ra0 | We use a linear combination of the relation score and ComplEx score to find the answer entity. eans = arg max φ(eh , eq , ea0 ) + γ ∗ RelScorea0 a0 ∈Nh where γ is a tunable hyperparameter. 5 Experimental Details In this section, we first describe the datasets that we evaluated our metho"
2020.acl-main.412,D19-1242,0,0.188246,"4. Introduction Knowledge Graphs (KG) are multi-relational graphs consisting of millions of entities (e.g., San Jose, California, etc.) and relationships among them (e.g., San Jose-cityInState-California). Examples of a few large KGs include Wikidata (Google, 2013), DBPedia (Lehmann et al., 2015), Yago (Suchanek et al., 2007), and NELL (Mitchell ∗ Equal contribution EmbedKGQA’s source code is available https://github.com/malllabiisc/EmbedKGQA at et al., 2018). Question Answering over Knowledge Graphs (KGQA) has emerged as an important research area over the last few years (Zhang et al., 2018; Sun et al., 2019a). In KGQA systems, given a natural language (NL) question and a KG, the right answer is derived based on analysis of the question in the context of the KG. In multi-hop KGQA, the system needs to perform reasoning over multiple edges of the KG to infer the right answer. KGs are often incomplete, which creates additional challenges for KGQA systems, especially in case of multi-hop KGQA. Recent methods have used an external text corpus to handle KG sparsity (Sun et al., 2019a, 2018). For example, the method proposed in (Sun et al., 2019a) constructs a question-specific sub-graph from the KG, wh"
2020.acl-main.412,D18-1455,0,0.541264,"questions to logical forms. Methods like (Dai et al., 2016; Dong et al., 2015; Hao et al., 2017; Lukovnikov et al., 2017; Yin et al., 2016) utilize neural networks to learn a scoring functions to rank the candidate answers. Some works like (Mohammed et al., 2017; Ture and Jojic, 2016) consider each relation as a label and model QA task as a classification problem. Extending these kinds of approaches for multi-hop question answering is non-trivial. Recently, there has been some work in which text corpus is incorporated as a knowledge source in addition to KG to answer complex questions on KGs (Sun et al., 2018, 2019a). Such approaches are useful in case the KG is incomplete. However, this leads to another level of complexity in the QA system, and text corpora might not always be available. KG completion methods: Link prediction in Knowledge Graphs using KG embeddings has become a popular area of research in recent years. The general framework is to define a score function for a set of triples (h, r, t) in a KG and constraining them in such a way that the score for a correct triple is higher than the score for an incorrect triple. RESCAL (Nickel et al., 2011) and DistMult (Yang et al., 2015) learn a"
2020.acl-main.412,D14-1071,0,0.0866861,"G, then the question could have been answered rather easily. However, since this edge is missing from the KG, as is often the case with similar incomplete and sparse KGs, the KGQA model has to potentially reason over a longer path over the KG (marked by bolded edges in the graph). Moreover, the KGQA model imposed a neighborhood size of 3-hops, which made the true answer Crime out of reach. In a separate line of research, there has been a large body of work that utilizes KG embeddings to predict missing links in the KG, thereby reducing KG sparsity (Bordes et al., 2013; Trouillon et al., 2016; Yang et al., 2014a; Nickel et al., 2011). KG embedding methods learn high-dimensional embeddings for entities and relations in the KG, which are then used for link prediction. In spite of its high relevance, KG embedding methods have not been used for multi-hop KGQA – we fill this gap in this paper. In particular, we propose EmbedKGQA, a novel system which leverages KG embeddings to perform multi-hop KGQA. We make the following contributions in this paper: 1. We propose EmbedKGQA, a novel method for the multi-hop KGQA task. To the best of our knowledge, EmbedKGQA is the first method to use KG embeddings for th"
2020.acl-main.412,P15-1128,0,0.09463,"eighborhood, an undesirable constraint imposed by previous methods for this task. 3. Through extensive experiments on multiple real-world datasets, we demonstrate EmbedKGQA’s effectiveness over state-of-the-art baselines. We have made EmbedKGQA’s source code available to encourage reproducibility. 4499 2 Related Work KGQA: In prior work (Li et al., 2018) TransE, (Bordes et al., 2013) embeddings have been used to answer factoid based questions. However, this requires ground truth relation labeling for each question and it does not work for multi-hop question answering. In another line of work (Yih et al., 2015) and (Bao et al., 2016) proposed extracting a particular sub-graph to answer the question. The method presented in (Bordes et al., 2014a), the sub-graph generated for a head entity is projected in a high dimensional space for question answering. Memory Networks have also been used to learn high dimensional embeddings of the facts present in the KG to perform QA (Bordes et al., 2015). Methods like (Bordes et al., 2014b) learn a similarity function between the question and the corresponding triple during training, and score the question with all the candidate triples at the test time. (Yang et a"
2020.acl-main.412,P16-2033,0,0.236253,"QA. The pruning strategy is described in the following section. 4501 MetaQA 1-hop MetaQA 2-hop MetaQA 3-hop WebQSP Train 96,106 118,948 114,196 2,998 Dev 9,992 14,872 14,274 100 more than 400k questions in the movie domain. It has 1-hop, 2-hop, and 3-hop questions. In our experiments, we used the “vanilla” version of the questions. Along with the QA data, MetaQA also provides a KG with 135k triples, 43k entities, and nine relations. Test 9.947 14,872 14,274 1,639 Table 1: Statistics for MetaQA and WebQuestionsSP datasets. Please refer section 5.1 for more details. 4.4.1 2. WebQuestionsSP (tau Yih et al., 2016) is a smaller QA dataset with 4,737 questions. The questions in this dataset are 1-hop and 2-hop questions and are answerable through Freebase KG. For ease of experimentation, we restrict the KB to be a subset of Freebase which contains all facts that are within 2-hops of any entity mentioned in the questions of WebQuestionsSP. We further prune it to contain only those relations that are mentioned in the dataset. This smaller KB has 1.8 million entities and 5.7 million triples. Relation matching Similar to PullNet (Sun et al., 2019a) we learn a scoring function S(r, q) which ranks each relatio"
2020.acl-main.412,C16-1164,0,0.036923,"Missing"
2020.acl-main.489,D19-1522,0,0.140772,"Missing"
2020.acl-main.489,W17-2609,0,0.0153608,"an the invalid ones. KGC Evaluation During KGC evaluation, for predicting t in a given triplet (h, r, t), a KGC model scores all the triplets in the set T 0 = {(h, r, t0 ) |t0 ∈ E}. Based on the score, the model first sorts all the triplets and subsequently finds the rank of the valid triplet (h, r, t) in the list. In a more relaxed setting called filtered setting, all the known correct triplets (from train, valid, and test triplets) are removed from T 0 except the one being evaluated (Bordes et al., 2013). The triplets in T 0 − {t} are called negative samples. Related Work Prior to our work, Kadlec et al. (2017) cast doubt on the claim that performance improvement of several models is due to architectural changes as opposed to hyperparameter tuning or different training objective. In our work, we raise similar concerns but through a different angle by highlighting issues with the evaluation procedure used by several recent methods. Chandrahas et al. (2018) analyze the geometry of KG embeddings and its correlation with task performance while Nayyeri et al. (2019) examine the effect of different loss functions on performance. However, their analysis is restricted to non-neural approaches. 0 2000 4000 6"
2020.acl-main.489,D15-1082,0,0.0265589,"ods use translation distance based (Bordes et al., 2013; Wang et al., 2014; Xiao et al., 2016; Sun et al., 2019) and semantic matching based (Nickel and Tresp, 2013; Yang et al., 2014; Nickel et al., 2016; Trouillon et al., 2016; ∗ Equal contribution. Liu et al., 2017) scoring functions which are easy to analyze. However, recently, a vast number of neural network-based methods have been proposed. They have complex score functions which utilize blackbox neural networks including Convolutional Neural Networks (CNNs) (Dettmers et al., 2018; Nguyen et al., 2018), Recurrent Neural Networks (RNNs) (Lin et al., 2015; Wang et al., 2018), Graph Neural Networks (GNNs) (Schlichtkrull et al., 2017; Shang et al., 2019), and Capsule Networks (Nguyen et al., 2019). While some of them report state-of-the-art performance on several benchmark datasets that are competitive to previous embedding-based approaches, a considerable portion of recent neural network-based papers report very high performance gains which are not consistent across different datasets. Moreover, most of these unusual behaviors are not at all analyzed. Such a pattern has become prominent and is misleading the whole community. In this paper, we i"
2020.acl-main.489,P19-1466,0,0.240123,"Missing"
2020.acl-main.489,N07-4000,0,0.238609,"Missing"
2020.acl-main.489,D14-1162,0,0.0916586,"Missing"
2020.acl-main.489,P16-1219,0,0.0436134,"Missing"
2020.acl-main.489,W15-4007,0,0.0272123,"e it penalizes the model for giving the same score to multiple triplets, i.e., if many triplets have the same score as the correct triple, the correct triplet gets the least rank possible. As a result, R ANDOM is the best evaluation technique which is both rigorous and fair to the model. It is in line with the situation we meet in the real world: given several same scored candidates, the only option is to select one of them randomly. Hence, we propose to use R ANDOM evaluation scheme for all model performance comparisons. 5 Experiments Datasets We evaluate the proposed protocols on FB15k-237 (Toutanova and Chen, 2015) dataset1 , which is a subset of FB15k (Bordes et al., 2013) with inverse relations deleted to prevent direct inference of test triples from training. 5.2 Methods Analyzed In our experiments, we categorize existing KGC methods into the following two categories: 1 • Affected: This category consists of recently proposed neural-network based methods whose performance is affected by different evaluation protocols. ConvKB, CapsE, TransGate2 , and KBAT are methods in this category. 5.3 We also report our results on WN18RR (Dettmers et al., 2018) dataset in the appendix. Evaluation Metrics For all th"
2020.acl-main.771,N18-1197,0,0.0193529,"t al., 2019; Oana-Maria et al., 2019b). Given these observations, measuring faithfulness, i.e., how well do the provided explanations correlate with the model’s decision making, is crucial. DeYoung et al. (2019) propose metrics to evaluate such faithfulness of rationales (supporting evidence) for NLP tasks. Through NILE, we propose a framework for generating faithful natural language explanations by requiring the model to condition on generated natural language explanations. The idea of using natural language strings as a latent space has been explored to capture compositional task structure (Andreas et al., 2018). Wu et al. (2019) explore improving 8731 visual question answering by learning to generate question-relevant captions. Rajani et al. (2019) aim to improve commonsense question answering by first generating commonsense explanations for multiple-choice questions, where the question and the choices are provided as the prompt. Similar to (Camburu et al., 2018), they learn by trying to generate human-provided explanations and subsequently conditioning on the generated explanation. In NILE, we instead aim to produce an explanation for each possible label and subsequently condition on the generated"
2020.acl-main.771,D15-1075,0,0.388997,"ion Processor S, which generates label scores using the evidence present in these explanations (see Figure 3 for the architectures used in this work). In addition to the explanations, NILE also utilizes the premise and hypothesis pair (See Section 4.4.2 for a discussion on the challenges in building such a system). Please see Section 4 for details. to these explanations. We choose NLI due to its importance as an NLP task, and the availability of e-SNLI, a large dataset annotated both with entailment relation labels and natural language human explanations of those labels (Camburu et al., 2018; Bowman et al., 2015). In summary, we make the following contributions in this work. 1. We propose NILE, an NLI system which generates and processes label-specific explanations to infer the task label, naturally providing explanations for its decisions. 2. We demonstrate the effectiveness of NILE compared to existing systems, in terms of label and explanation accuracy. 3. Through NILE, we provide a framework for generating falsifiable explanations. We propose ways to evaluate and improve the faithfulness of the system’s predictions to the generated explanations. We claim that task-specific probes of sensitivity ar"
2020.acl-main.771,N19-1423,0,0.0288311,": Labelspecific Candidate Explanation Generators first generate explanations supporting the respective labels (Section 4.3). 2. Explanation Processor: The Explanation Processor takes the explanations and also the premise and hypothesis pairs as input to produce the task label (Section 4.4). We also build NILE-PH, where the Explanation Processor has access only to the generated explanations (Section 4.4.1). Pretrained Language Models Transformer architectures (Vaswani et al., 2017) pre-trained on large corpora with self-supervision have shown significant improvements on various NLP benchmarks (Devlin et al., 2019; Radford Natural-language Inference over Label-specific Explanations (NILE) We note that NILE-PH more naturally fits the desiderata described in Section 1, while we design and evaluate NILE for the more general case 8732 where the Explanation Processor also accesses the premise and hypothesis pair. In Section 4.5, we describe comparable baseline architectures. 4.1 Notation We denote each data point by (p, h), where p is the premise and h the hypothesis sentence. G denotes a model trained to generate natural language explanations. Specifically, Gx denotes a model which generates natural langua"
2020.acl-main.771,D18-1407,0,0.0470386,"Missing"
2020.acl-main.771,P17-1015,0,0.044233,"fiable explanations. We propose ways to evaluate and improve the faithfulness of the system’s predictions to the generated explanations. We claim that task-specific probes of sensitivity are crucial for such evaluation. We have released the source code of NILE to aid reproducibility of the results. 2 Related Work Explainability of a model’s predictions has been studied from different perspectives, including feature importance based explanations (Ribeiro et al., 2016; Lundberg and Lee, 2017; Chen et al., 2018), or post-hoc natural language explanations (Huk Park et al., 2018; Kim et al., 2018; Ling et al., 2017). Hendricks et al. (2018) produce counterfactual natural language explanations for image classification given an image and a counter-class label. Camburu et al. (2018) propose a model for NLI to first generate a free-form natural language explanation and then infer the label from the explanation. However, as noted by Oana-Maria et al. (2019a), the system tends to generate inconsistent explanations. We reason that requiring a model to generate an explanation of the correct output requires it to first infer the output, and the system thus resembles post-hoc explanation generation methods. Given"
2020.acl-main.771,2021.ccl-1.108,0,0.101031,"Missing"
2020.acl-main.771,P19-1487,0,0.0903469,"te with the model’s decision making, is crucial. DeYoung et al. (2019) propose metrics to evaluate such faithfulness of rationales (supporting evidence) for NLP tasks. Through NILE, we propose a framework for generating faithful natural language explanations by requiring the model to condition on generated natural language explanations. The idea of using natural language strings as a latent space has been explored to capture compositional task structure (Andreas et al., 2018). Wu et al. (2019) explore improving 8731 visual question answering by learning to generate question-relevant captions. Rajani et al. (2019) aim to improve commonsense question answering by first generating commonsense explanations for multiple-choice questions, where the question and the choices are provided as the prompt. Similar to (Camburu et al., 2018), they learn by trying to generate human-provided explanations and subsequently conditioning on the generated explanation. In NILE, we instead aim to produce an explanation for each possible label and subsequently condition on the generated label-specific explanations to produce the final decision. 3 Background In this section, we discuss the datasets (Section 3.1) and pre-train"
2020.acl-main.771,N16-3020,0,0.135905,"iveness of NILE compared to existing systems, in terms of label and explanation accuracy. 3. Through NILE, we provide a framework for generating falsifiable explanations. We propose ways to evaluate and improve the faithfulness of the system’s predictions to the generated explanations. We claim that task-specific probes of sensitivity are crucial for such evaluation. We have released the source code of NILE to aid reproducibility of the results. 2 Related Work Explainability of a model’s predictions has been studied from different perspectives, including feature importance based explanations (Ribeiro et al., 2016; Lundberg and Lee, 2017; Chen et al., 2018), or post-hoc natural language explanations (Huk Park et al., 2018; Kim et al., 2018; Ling et al., 2017). Hendricks et al. (2018) produce counterfactual natural language explanations for image classification given an image and a counter-class label. Camburu et al. (2018) propose a model for NLI to first generate a free-form natural language explanation and then infer the label from the explanation. However, as noted by Oana-Maria et al. (2019a), the system tends to generate inconsistent explanations. We reason that requiring a model to generate an ex"
2020.acl-main.771,P19-1348,0,0.0313998,"et al., 2019b). Given these observations, measuring faithfulness, i.e., how well do the provided explanations correlate with the model’s decision making, is crucial. DeYoung et al. (2019) propose metrics to evaluate such faithfulness of rationales (supporting evidence) for NLP tasks. Through NILE, we propose a framework for generating faithful natural language explanations by requiring the model to condition on generated natural language explanations. The idea of using natural language strings as a latent space has been explored to capture compositional task structure (Andreas et al., 2018). Wu et al. (2019) explore improving 8731 visual question answering by learning to generate question-relevant captions. Rajani et al. (2019) aim to improve commonsense question answering by first generating commonsense explanations for multiple-choice questions, where the question and the choices are provided as the prompt. Similar to (Camburu et al., 2018), they learn by trying to generate human-provided explanations and subsequently conditioning on the generated explanation. In NILE, we instead aim to produce an explanation for each possible label and subsequently condition on the generated label-specific exp"
2020.acl-main.771,W18-5446,0,0.0635015,"Missing"
2020.acl-main.771,N18-1101,0,0.0286013,"uality of generated explanations. All variants of NILE produce more correct explanations (B, C) as well as a higher percentage of correct generated explanations among correct predictions (B/A, C/A). This demonstrates that NILE, through intermediate label-specific natural language explanations, provides a more general way for building systems which can produce natural language explanations for their decisions. Transfer to Out-of-domain NLI To test the generalization capability of NILE, we do training and model selection on the SNLI dataset (Section 5.1), and evaluate on the out-of-domain MNLI (Williams et al., 2018) development sets. Transfer without fine-tuning to out-of-domain NLI has been a challenging task with transfer learning 5.3 Evaluating Faithfulness using Sensitivity Analysis NILE and its variants allow a natural way to probe the sensitivity of their predictions to the generated explanations, which is by perturbing the explanations themselves. In this way, NILE resembles 8737 Model NILE-NS NILE Independent Aggregate Append Independent Aggregate I+ Exp 91.6 91.6 91.7 91.3 91.2 I only 33.8 33.8 91.2 33.8 33.8 Exp only 69.4 74.5 72.9 46.1 40.7 Table 3: Estimating the sensitivity of the system’s p"
2020.icon-main.8,P14-1090,0,0.101235,"Missing"
2020.icon-main.8,D15-1174,0,0.0351296,"cific similarity between head and tail entity vectors. Following the ideas of translation and projection, many different methods have been developed. These include TransH (Wang et al., 2014), TransR (Lin et al., 2015), STransE (Nguyen et al., 2016), ITransF (Xie et al., 2017), etc. These methods can only extract a restricted set of interactions from the vectors. 2.2 interactions they can extract. 2.3 Neural Models There are many models that use various neural network architectures for learning KG Embeddings. Some of these models are NTN (Socher et al., 2013), ER-MLP (Dong et al., 2014), CONV (Toutanova et al., 2015), ProjE (Shi and Weninger, 2017), R-GCN (Schlichtkrull et al., 2017), ConvE (Dettmers et al., 2018), R-MLP-2n (Ravishankar et al., 2017), KG-BERT (Yao et al., 2019), InteractE (Vashishth et al., 2020), etc. Unlike other methods, KG-BERT uses word embeddings for encoding entities and relations. Therefore, the interactions of entity and relation vectors are not directly clear. Among the rest of the models, ProjE can extract interactions only from entity or relation vectors but not both. ConvE can extract interactions from both, entity as well as relation vectors, but they are extracted using pre"
2020.icon-main.8,P17-1088,0,0.0209763,"1qxbusVO+vyrWHPI4CnMIZXIAH11CDO6hDAxgM4Ble4c0Rzovz7nwsWtecfOYE/sD5/AFxRo3z&lt;/latexit&gt; Interaction Layer Dot Product Score Score (b) FCE (a) FCConvE Figure 3: Architecture diagrams for FCConvE (left) and FCE (right). Please refer to Section 3.3 and Section 3.4 for more details. SE (Bordes et al., 2011) is another model that uses relation specific similarity between head and tail entity vectors. Following the ideas of translation and projection, many different methods have been developed. These include TransH (Wang et al., 2014), TransR (Lin et al., 2015), STransE (Nguyen et al., 2016), ITransF (Xie et al., 2017), etc. These methods can only extract a restricted set of interactions from the vectors. 2.2 interactions they can extract. 2.3 Neural Models There are many models that use various neural network architectures for learning KG Embeddings. Some of these models are NTN (Socher et al., 2013), ER-MLP (Dong et al., 2014), CONV (Toutanova et al., 2015), ProjE (Shi and Weninger, 2017), R-GCN (Schlichtkrull et al., 2017), ConvE (Dettmers et al., 2018), R-MLP-2n (Ravishankar et al., 2017), KG-BERT (Yao et al., 2019), InteractE (Vashishth et al., 2020), etc. Unlike other methods, KG-BERT uses word embedd"
2020.icon-main.9,P15-1144,0,0.0466043,"Missing"
2020.icon-main.9,W17-2609,0,0.0183878,"s textual triples, which we use for calculating PMI for entity pairs. where P is entity-pair PMI matrix and v(e) denote vector for entity e. This term can be used in the objective function defined in (7). 3.2 Objective The overall loss function can be written as follows 4 |  1 X − loss(t, t− o ) + loss(t, ts ) . |T | t∈T (6) Entity Model (Model-E) We use the Entity Model proposed in (Riedel et al., 2013) for learning KG embeddings. However, it should be noted that the proposed regularizer can be used along with any KG embedding model which represents entities as vectors. Also, as pointed in (Kadlec et al., 2017; Ruffinelli et al., 2020; Jain et al., 2020), various KG embedding models achieve similar performances when trained properly. Therefore, we select Model-E which is simple yet effective. This model assumes a vector v(e) for each entity and two vectors vs (r) and vo (r) for each relation of the KG. The score for the triple (es , r, eo ) is given by, 4.2 Experimental Setup We use the method proposed in (Riedel et al., 2013) as the baseline. Please refer to Section 3.2 for more details. For evaluating the learned embeddings, we test them on different tasks. All the hyper-parameters are tuned usin"
2020.icon-main.9,D19-1522,0,0.0170932,"ennis, United States etc while the edges represent relationships between them. These KGs have grown huge, but they are still not complete (Toutanova et al., 2015). Hence the task of inferring new facts becomes important. KG embeddings have been a popular approach for this task as they can perform the inference task efficiently. This task has achieved significant attention in the literature and many methods have been proposed, such as, (Bordes et al., 2013; Riedel et al., 2013; Yang et al., 2014; Toutanova et al., 2015; Trouillon et al., 2016; Schlichtkrull et al., 2017; Dettmers et al., 2018; Balazevic et al., 2019), etc. These methods learn representations for entities 2 Related Work Several methods have been proposed for learning KG embeddings. They differ on the modeling of entities and relations, usage of text data and interpretability of the learned embeddings. We summarize some of these methods in following sections. ∗ Contributed equally to the work. This research was conducted during the authors’ internships at Indian Institute of Science, Bangalore. † 70 Proceedings of the 17th International Conference on Natural Language Processing, pages 70–75 Patna, India, December 18 - 21, 2020. ©2020 NLP As"
2020.icon-main.9,E14-1056,0,0.338057,"nce in other KG tasks. 1 Although these methods have shown good performance in the end task, they do not address the interpretability, i.e., understanding semantics of individual dimensions of the KG embedding. Such representations enable a better understanding of the model and can be helpful for explaining a model’s decision on an end application. In this work, we focus on incorporating interpretability in KG embeddings. Specifically, we aim to learn interpretable embeddings for KG entities by incorporating additional entity co-occurrence statistics from text data. This work is motivated by (Lau et al., 2014) who presented automated methods for evaluating topics learned via topic modelling methods. We adapt these methods for KG embedding models and propose a method to directly maximize them while learning KG embedding. As demonstrated by the experiments, we find that such modeling significantly improves interpretability, supporting our choice of using topic coherence for embedding dimensions. To the best of our knowledge, this work presents the first regularization term which induces interpretability in KG embeddings. Introduction Knowledge Graphs such as Freebase (Bollacker et al., 2008) and NELL"
2020.icon-main.9,C12-1118,1,0.748592,"ollowing sections. 3.1 Coherence In topic models, coherence of a topic can be determined by semantic relatedness among top entities within the topic. This idea can also be used in vector space models by treating dimensions of the vector space as topics. With this assumption, we can use a measure of coherence defined in following section for evaluating interpretability of the embeddings. Interpretability of Embeddings While the KG embedding models perform well in many tasks, the semantics of learned representations are not directly clear. This problem for word embeddings has been addressed in (Murphy et al., 2012; Faruqui et al., 2015; Subramanian et al., 2018) where they apply a set of constraints inducing interpretability. A similar task of learning semantic features for entities and relations is KG was addressed in (Xiao et al., 2016). However, their approach is not applicable for the much popular KG embedding methods. The model proposed in (Xie et al., 2017) can generate interpretable embeddings for relations, but not entities. Another approach, as proposed in (Gusmao et al., 2018), is to generate weighted Horn rules as explanations for link prediction. We refer the reader to Section 4 of (Bianchi"
2020.icon-main.9,N13-1008,0,0.309955,"etc. They store a collection of facts in the form of a graph. The nodes in the graph represent real world entities such as Roger Federer, Tennis, United States etc while the edges represent relationships between them. These KGs have grown huge, but they are still not complete (Toutanova et al., 2015). Hence the task of inferring new facts becomes important. KG embeddings have been a popular approach for this task as they can perform the inference task efficiently. This task has achieved significant attention in the literature and many methods have been proposed, such as, (Bordes et al., 2013; Riedel et al., 2013; Yang et al., 2014; Toutanova et al., 2015; Trouillon et al., 2016; Schlichtkrull et al., 2017; Dettmers et al., 2018; Balazevic et al., 2019), etc. These methods learn representations for entities 2 Related Work Several methods have been proposed for learning KG embeddings. They differ on the modeling of entities and relations, usage of text data and interpretability of the learned embeddings. We summarize some of these methods in following sections. ∗ Contributed equally to the work. This research was conducted during the authors’ internships at Indian Institute of Science, Bangalore. † 70"
2020.icon-main.9,W15-4007,0,0.0118813,"high PMI score pij . This will result in an embedding matrix θe with a high value of Coherence@k since high PMI entity pairs are more likely to be among top k entities. This idea can be captured by following coherence term C(θe , P ) = n X i−1 X L(θe , θr , T ) = 3.3 kv(ei ) v(ej ) − pij k 2 L(θe , θr , T ) + λc C(θe , P ) + λr R(θe , θr ) (7)   where R(θe , θr ) = 12 kθe k2 + kθr k2 is the L2 regularization term and λc and λr are hyperparameters controlling the trade-off among different terms in the objective function. (4) 4.1 i=2 j=1 Experiments and Results Datasets We use the FB15k-237 (Toutanova and Chen, 2015) dataset, a factual KG, for experiments. It contains 14541 entities and 237 relations. The triples are split into training, validation and test set having 272115, 17535 and 20466 triples respectively. For extracting entity co-occurrences, we use the textual relations used in (Toutanova et al., 2015). It contains around 3.7 millions textual triples, which we use for calculating PMI for entity pairs. where P is entity-pair PMI matrix and v(e) denote vector for entity e. This term can be used in the objective function defined in (7). 3.2 Objective The overall loss function can be written as follo"
2020.icon-main.9,D15-1174,0,0.431321,"o the best of our knowledge, this work presents the first regularization term which induces interpretability in KG embeddings. Introduction Knowledge Graphs such as Freebase (Bollacker et al., 2008) and NELL (Mitchell et al., 2015) have become important resources for supporting many AI applications like web search, Q&A, etc. They store a collection of facts in the form of a graph. The nodes in the graph represent real world entities such as Roger Federer, Tennis, United States etc while the edges represent relationships between them. These KGs have grown huge, but they are still not complete (Toutanova et al., 2015). Hence the task of inferring new facts becomes important. KG embeddings have been a popular approach for this task as they can perform the inference task efficiently. This task has achieved significant attention in the literature and many methods have been proposed, such as, (Bordes et al., 2013; Riedel et al., 2013; Yang et al., 2014; Toutanova et al., 2015; Trouillon et al., 2016; Schlichtkrull et al., 2017; Dettmers et al., 2018; Balazevic et al., 2019), etc. These methods learn representations for entities 2 Related Work Several methods have been proposed for learning KG embeddings. They"
2020.tacl-1.22,K16-1002,0,0.137383,"based on latent variable probabilistic modeling, neural variational inference, and multi-task learning. This, in principle, is very similar to Chen et al. (2019b). As opposed to our model, which provides different levels of syntactic control of the exemplarbased generation, this approach is restrictive in terms of the flexibility it can offer. Also, as noted in Shi et al. (2016), an autoencoder-based approach might not offer rich enough syntactic information as offered by actual constituency parse trees. Additionally, VAEs (Kingma and Welling, 2014) are generally unstable and harder to train (Bowman et al., 2016; Gupta et al., 2018) than seq2seq-based approaches. 3 SGCP: Proposed Method In this section, we describe the inputs and various architectural components essential for building SGCP, an end-to-end trainable model. Our model, as shown in Figure 1, comprises a sentence encoder (3.2), syntactic tree encoder (3.3), and a syntactic-paraphrase-decoder (3.4). 3.1 Inputs Given an input sentence X and a syntactic exemplar Y , our goal is to generate a sentence Z that conforms to the syntax of Y while retaining the meaning of X . The semantic encoder (Section 3.2) works on sequence of input tokens, and"
2020.tacl-1.22,P18-1015,0,0.0219454,"neural models often tend to produce degenerate outputs and favor generic utterances (Vinyals and Le, 2015; Li et al., 2016). Although simple attributes are helpful in addressing what to say, they provide very little information about how to say it. Syntactic control over generation helps in filling this gap by providing that missing information. Incorporating complex syntactic information has shown promising results in neural machine translation (Stahlberg et al., 2016; Aharoni and Goldberg, 2017; Yang et al., 2019), data-to-text generation (Peng et al., 2019), abstractive textsummarization (Cao et al., 2018), and adversarial text generation (Iyyer et al., 2018). Additionally, recent work (Iyyer et al., 2018; Kumar et al., 2019) has shown that augmenting lexical and syntactical variations in the training set can help in building better performing and more robust models. In this paper, we focus on the task of syntactically controlled paraphrase generation, that is, given an input sentence and a syntactic exemplar, produce a sentence that conforms to the syntax of the exemplar while retaining the meaning of the original input sentence. While syntactically controlled generation of paraphrases finds a"
2020.tacl-1.22,P19-1599,0,0.122059,"ever, various other studies (Clark and Clark, 1968; Katz and Brent, 1968; Irwin, 1980) have suggested that for older school children, college students, and adults, comprehension is better for the cause-effect presentation, hence sentence S1. Thus, modifying a sentence, syntactically, would help in better comprehension based on literacy skills. Prior work in syntactically controlled paraphrase generation addressed this task by conditioning the semantic input on either the features learned from a linearized constituency-based parse tree (Iyyer et al., 2018), or the latent syntactic information (Chen et al., 2019a) learned from exemplars through variational auto-encoders. Linearizing parse trees typically results in loss of essential dependency information. On the other hand, as noted in Shi et al. (2016), an autoencoderbased approach might not offer rich enough syntactic information as guaranteed by actual constituency parse trees. Moreover, as noted in Chen et al. (2019a), SCPN (Iyyer et al., 2018), and CGEN (Chen et al., 2019a) tend to generate sentences of the same length as the exemplar. This is an undesirable characteristic because it often results in producing sentences that end abruptly, there"
2020.tacl-1.22,P17-2021,0,0.0176051,"providing information about what to say but also how to say it. Without any constraint, the ubiquitous sequence-to-sequence neural models often tend to produce degenerate outputs and favor generic utterances (Vinyals and Le, 2015; Li et al., 2016). Although simple attributes are helpful in addressing what to say, they provide very little information about how to say it. Syntactic control over generation helps in filling this gap by providing that missing information. Incorporating complex syntactic information has shown promising results in neural machine translation (Stahlberg et al., 2016; Aharoni and Goldberg, 2017; Yang et al., 2019), data-to-text generation (Peng et al., 2019), abstractive textsummarization (Cao et al., 2018), and adversarial text generation (Iyyer et al., 2018). Additionally, recent work (Iyyer et al., 2018; Kumar et al., 2019) has shown that augmenting lexical and syntactical variations in the training set can help in building better performing and more robust models. In this paper, we focus on the task of syntactically controlled paraphrase generation, that is, given an input sentence and a syntactic exemplar, produce a sentence that conforms to the syntax of the exemplar while ret"
2020.tacl-1.22,N19-1254,0,0.0692157,"ever, various other studies (Clark and Clark, 1968; Katz and Brent, 1968; Irwin, 1980) have suggested that for older school children, college students, and adults, comprehension is better for the cause-effect presentation, hence sentence S1. Thus, modifying a sentence, syntactically, would help in better comprehension based on literacy skills. Prior work in syntactically controlled paraphrase generation addressed this task by conditioning the semantic input on either the features learned from a linearized constituency-based parse tree (Iyyer et al., 2018), or the latent syntactic information (Chen et al., 2019a) learned from exemplars through variational auto-encoders. Linearizing parse trees typically results in loss of essential dependency information. On the other hand, as noted in Shi et al. (2016), an autoencoderbased approach might not offer rich enough syntactic information as guaranteed by actual constituency parse trees. Moreover, as noted in Chen et al. (2019a), SCPN (Iyyer et al., 2018), and CGEN (Chen et al., 2019a) tend to generate sentences of the same length as the exemplar. This is an undesirable characteristic because it often results in producing sentences that end abruptly, there"
2020.tacl-1.22,W05-0909,0,0.0240976,"tion set V. Step 1: For a given (X, Z ) ∈ Teval , construct an exemplar candidate set C = Teset − {X, Z }. |C |≈ 12, 000. (a) the generated and the syntactic exemplar in the test set - TED-E (b) the generated and the reference paraphrase in the test set - TED-R 4.3 Evaluation It should be noted that there is no single fully reliable metric for evaluating syntactic paraphrase generation. Therefore, we evaluate on the following metrics to showcase the efficacy of syntactic paraphrasing models. 1. Automated Evaluation. (i) Alignment based metrics: We compute BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), ROUGE-1, ROUGE-2, and ROUGE-L (Lin, 2004) scores between the generated and the reference paraphrases in the test set. (ii) Syntactic Transfer: We evaluate the syntactic transfer using Tree-edit distance (Zhang and Shasha, 1989) between the parse trees of: Step 2: Retain only those sentences C ∈ C whose sentence length (= number of tokens) differ by at most two when compared to the paraphrase Z . This is done since sentences with similar constituency-based parse tree structures tend to have similar token lengths. (iii) Model-based evaluation: Because our goal is to generate paraphrases of the"
2020.tacl-1.22,P18-1128,0,0.0131889,"lower H values for syntactic granularity. This can also be observed from the example given in Table 6 where H = 6 is more favorable than H = 7, because of better meaning retention. Although CGEN performs close to our model in terms of BLEU, ROUGE, and METEOR scores on ParaNMT-small dataset, its PDS is still much lower than that of our model, suggesting that our model is better at capturing the original meaning of the source sentence. In order to show that the results are not coincidental, we test the statistical significance of our model. We follow the nonparametric Pitman’s permutation test (Dror et al., 2018) and observe that our model is statistically significant when the significance level (α) is taken to be 0.05. Note that this holds true for all metric on both the datasets except ROUGE-2 on ParaNMT-small. SCPN CGEN SGCP-F QQP-Pos 1.63 2.47 2.70 2.99 ParaNMT-small 1.24 1.89 2.07 2.26 ones which differ a lot from the original sentence in terms of their syntax. A particularly interesting exemplar is what is chromosomal mutation ? what are some examples ?. Here, SGCP is able to generate a sentence with two question marks while preserving the essence of the source sentence. It should also be noted"
2020.tacl-1.22,N10-1000,0,0.0474668,"Missing"
2020.tacl-1.22,S07-1091,0,0.0162015,"hen et al., 2019a). These systems find applications in adversarial sample generation (Iyyer et al., 2018), 2 https://www.kaggle.com/c/quoraquestion-pairs. text summarization, and table-to-text generation (Peng et al., 2019). While achieving state-of-theart in their respective domains, these systems typically rely on a known finite set of attributes thereby making them quite restrictive in terms of the styles they can offer. Paraphrase Generation. While generation of paraphrases has been addressed in the past using traditional methods (McKeown, 1983; Barzilay and Lee, 2003; Quirk et al., 2004; Hassan et al., 2007; Zhao et al., 2008; Madnani and Dorr, 2010; Wubben et al., 2010), they have recently been superseded by deep learning-based approaches (Prakash et al., 2016; Gupta et al., 2018; Li et al., 2019, 2018; Kumar et al., 2019). The primary task of all these methods (Prakash et al., 2016; Gupta et al., 2018; Li et al., 2018) is to generate the most semantically similar sentence and they typically rely on beam search to obtain any kind of lexical diversity. Kumar et al. (2019) try to tackle the problem of achieving lexical, and limited syntactical diversity using submodular optimization but do not pr"
2020.tacl-1.22,N19-1363,1,0.879925,"016). Although simple attributes are helpful in addressing what to say, they provide very little information about how to say it. Syntactic control over generation helps in filling this gap by providing that missing information. Incorporating complex syntactic information has shown promising results in neural machine translation (Stahlberg et al., 2016; Aharoni and Goldberg, 2017; Yang et al., 2019), data-to-text generation (Peng et al., 2019), abstractive textsummarization (Cao et al., 2018), and adversarial text generation (Iyyer et al., 2018). Additionally, recent work (Iyyer et al., 2018; Kumar et al., 2019) has shown that augmenting lexical and syntactical variations in the training set can help in building better performing and more robust models. In this paper, we focus on the task of syntactically controlled paraphrase generation, that is, given an input sentence and a syntactic exemplar, produce a sentence that conforms to the syntax of the exemplar while retaining the meaning of the original input sentence. While syntactically controlled generation of paraphrases finds applications in multiple domains like dataaugmentation and text passivization, we highlight its importance in the particula"
2020.tacl-1.22,P82-1020,0,0.75866,"Missing"
2020.tacl-1.22,N16-1014,0,0.0395515,"alli3∗ Partha Talukdar1 1 Indian Institute of Science, Bangalore 2 Microsoft Research, Bangalore 3 Google, London ashutosh@iisc.ac.in, kabirahuja2431@gmail.com raghuram.4350@gmail.com, ppt@iisc.ac.in Abstract sentiment to positive, a controlled text generator is expected to produce the sentence ‘‘The movie is fantastic!’’. These constraints are important in not only providing information about what to say but also how to say it. Without any constraint, the ubiquitous sequence-to-sequence neural models often tend to produce degenerate outputs and favor generic utterances (Vinyals and Le, 2015; Li et al., 2016). Although simple attributes are helpful in addressing what to say, they provide very little information about how to say it. Syntactic control over generation helps in filling this gap by providing that missing information. Incorporating complex syntactic information has shown promising results in neural machine translation (Stahlberg et al., 2016; Aharoni and Goldberg, 2017; Yang et al., 2019), data-to-text generation (Peng et al., 2019), abstractive textsummarization (Cao et al., 2018), and adversarial text generation (Iyyer et al., 2018). Additionally, recent work (Iyyer et al., 2018; Kuma"
2020.tacl-1.22,D18-1421,0,0.125555,"wn finite set of attributes thereby making them quite restrictive in terms of the styles they can offer. Paraphrase Generation. While generation of paraphrases has been addressed in the past using traditional methods (McKeown, 1983; Barzilay and Lee, 2003; Quirk et al., 2004; Hassan et al., 2007; Zhao et al., 2008; Madnani and Dorr, 2010; Wubben et al., 2010), they have recently been superseded by deep learning-based approaches (Prakash et al., 2016; Gupta et al., 2018; Li et al., 2019, 2018; Kumar et al., 2019). The primary task of all these methods (Prakash et al., 2016; Gupta et al., 2018; Li et al., 2018) is to generate the most semantically similar sentence and they typically rely on beam search to obtain any kind of lexical diversity. Kumar et al. (2019) try to tackle the problem of achieving lexical, and limited syntactical diversity using submodular optimization but do not provide any syntactic control over the type of utterance that might be desired. These methods are therefore restrictive in terms of the syntactical diversity that they can offer. Controlled Paraphrase Generation. Our task is similar in spirit to Iyyer et al. (2018) and Chen et al. (2019a), which also deals with the task"
2020.tacl-1.22,N18-1170,0,0.301848,"uts and favor generic utterances (Vinyals and Le, 2015; Li et al., 2016). Although simple attributes are helpful in addressing what to say, they provide very little information about how to say it. Syntactic control over generation helps in filling this gap by providing that missing information. Incorporating complex syntactic information has shown promising results in neural machine translation (Stahlberg et al., 2016; Aharoni and Goldberg, 2017; Yang et al., 2019), data-to-text generation (Peng et al., 2019), abstractive textsummarization (Cao et al., 2018), and adversarial text generation (Iyyer et al., 2018). Additionally, recent work (Iyyer et al., 2018; Kumar et al., 2019) has shown that augmenting lexical and syntactical variations in the training set can help in building better performing and more robust models. In this paper, we focus on the task of syntactically controlled paraphrase generation, that is, given an input sentence and a syntactic exemplar, produce a sentence that conforms to the syntax of the exemplar while retaining the meaning of the original input sentence. While syntactically controlled generation of paraphrases finds applications in multiple domains like dataaugmentation"
2020.tacl-1.22,W04-1013,0,0.0271581,"t an exemplar candidate set C = Teset − {X, Z }. |C |≈ 12, 000. (a) the generated and the syntactic exemplar in the test set - TED-E (b) the generated and the reference paraphrase in the test set - TED-R 4.3 Evaluation It should be noted that there is no single fully reliable metric for evaluating syntactic paraphrase generation. Therefore, we evaluate on the following metrics to showcase the efficacy of syntactic paraphrasing models. 1. Automated Evaluation. (i) Alignment based metrics: We compute BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), ROUGE-1, ROUGE-2, and ROUGE-L (Lin, 2004) scores between the generated and the reference paraphrases in the test set. (ii) Syntactic Transfer: We evaluate the syntactic transfer using Tree-edit distance (Zhang and Shasha, 1989) between the parse trees of: Step 2: Retain only those sentences C ∈ C whose sentence length (= number of tokens) differ by at most two when compared to the paraphrase Z . This is done since sentences with similar constituency-based parse tree structures tend to have similar token lengths. (iii) Model-based evaluation: Because our goal is to generate paraphrases of the input sentences, we need some measure to d"
2020.tacl-1.22,W04-3219,0,0.257139,"yyer et al., 2018; Chen et al., 2019a). These systems find applications in adversarial sample generation (Iyyer et al., 2018), 2 https://www.kaggle.com/c/quoraquestion-pairs. text summarization, and table-to-text generation (Peng et al., 2019). While achieving state-of-theart in their respective domains, these systems typically rely on a known finite set of attributes thereby making them quite restrictive in terms of the styles they can offer. Paraphrase Generation. While generation of paraphrases has been addressed in the past using traditional methods (McKeown, 1983; Barzilay and Lee, 2003; Quirk et al., 2004; Hassan et al., 2007; Zhao et al., 2008; Madnani and Dorr, 2010; Wubben et al., 2010), they have recently been superseded by deep learning-based approaches (Prakash et al., 2016; Gupta et al., 2018; Li et al., 2019, 2018; Kumar et al., 2019). The primary task of all these methods (Prakash et al., 2016; Gupta et al., 2018; Li et al., 2018) is to generate the most semantically similar sentence and they typically rely on beam search to obtain any kind of lexical diversity. Kumar et al. (2019) try to tackle the problem of achieving lexical, and limited syntactical diversity using submodular optim"
2020.tacl-1.22,J18-3002,0,0.0371781,"Missing"
2020.tacl-1.22,J10-3003,0,0.0752176,"pplications in adversarial sample generation (Iyyer et al., 2018), 2 https://www.kaggle.com/c/quoraquestion-pairs. text summarization, and table-to-text generation (Peng et al., 2019). While achieving state-of-theart in their respective domains, these systems typically rely on a known finite set of attributes thereby making them quite restrictive in terms of the styles they can offer. Paraphrase Generation. While generation of paraphrases has been addressed in the past using traditional methods (McKeown, 1983; Barzilay and Lee, 2003; Quirk et al., 2004; Hassan et al., 2007; Zhao et al., 2008; Madnani and Dorr, 2010; Wubben et al., 2010), they have recently been superseded by deep learning-based approaches (Prakash et al., 2016; Gupta et al., 2018; Li et al., 2019, 2018; Kumar et al., 2019). The primary task of all these methods (Prakash et al., 2016; Gupta et al., 2018; Li et al., 2018) is to generate the most semantically similar sentence and they typically rely on beam search to obtain any kind of lexical diversity. Kumar et al. (2019) try to tackle the problem of achieving lexical, and limited syntactical diversity using submodular optimization but do not provide any syntactic control over the type o"
2020.tacl-1.22,P17-1099,0,0.145617,"Missing"
2020.tacl-1.22,P14-5010,0,0.0169121,"ce Z that conforms to the syntax of Y while retaining the meaning of X . The semantic encoder (Section 3.2) works on sequence of input tokens, and the syntactic encoder (Section 3.3) operates on constituencybased parse trees. We parse the syntactic exemplar Y 3 to obtain its constituency-based parse tree. The leaf nodes of the constituency-based parse tree consists of token for the sentence Y. These tokens, in some sense, carry the semantic information of sentence Y, which we do not need for generating paraphrases. In order to prevent any meaning 3 Obtained using the Stanford CoreNLP toolkit (Manning et al., 2014). 332 Figure 1: Architecture of SGCP (proposed method). SGCP aims to paraphrase an input sentence, while conforming to the syntax of an exemplar sentence (provided along with the input). The input sentence is encoded using the Sentence Encoder (Section 3.2) to obtain a semantic signal ct . The Syntactic Encoder (Section 3.3) takes a constituency parse tree (pruned at height H ) of the exemplar sentence as an input, and produces representations for all the nodes in the pruned tree. Once both of these are encoded, the Syntactic Paraphrase Decoder (Section 3.4) uses pointer-generator network, and"
2020.tacl-1.22,P16-1162,0,0.00888582,"+ bv ), (2) where e(yv ) is the embedding of the node label yv , and Wpa , Wv , bv are learnable parameters. This approach can be considered similar to TreeLSTM (Tai et al., 2015). We use GeLU activation function (Hendrycks and Gimpel, 2016) rather than the standard tanh or relu, because of superior empirical performance. As indicated in Section 3.1, syntactic encoder takes as input the height H , which governs the level of syntactic control. We randomly prune the (1) where e(xt ) represents the learnable embedding of the token xt and t ∈ {1, . . . , TX }. Note that we use byte-pair encoding (Sennrich et al., 2016) for word/token segmentation. 333 Figure 2: The constituency parse tree serves as an input to the syntactic encoder (Section 3.3). The first step is to remove the leaf nodes which contain meaning representative tokens (Here: What is the best language . . . ). H denotes the height to which the tree can be pruned and is an input to the model. Figure 2(a) shows the full constituency parse tree annotated with vector a for different heights. Figure 2(b) shows the same tree pruned at height H = 3 with its corresponding a vector. The vector a serves as an signalling vector (Section 3.4.2) which helps"
2020.tacl-1.22,J83-1001,0,0.698768,"as well as on syntactical templates (Iyyer et al., 2018; Chen et al., 2019a). These systems find applications in adversarial sample generation (Iyyer et al., 2018), 2 https://www.kaggle.com/c/quoraquestion-pairs. text summarization, and table-to-text generation (Peng et al., 2019). While achieving state-of-theart in their respective domains, these systems typically rely on a known finite set of attributes thereby making them quite restrictive in terms of the styles they can offer. Paraphrase Generation. While generation of paraphrases has been addressed in the past using traditional methods (McKeown, 1983; Barzilay and Lee, 2003; Quirk et al., 2004; Hassan et al., 2007; Zhao et al., 2008; Madnani and Dorr, 2010; Wubben et al., 2010), they have recently been superseded by deep learning-based approaches (Prakash et al., 2016; Gupta et al., 2018; Li et al., 2019, 2018; Kumar et al., 2019). The primary task of all these methods (Prakash et al., 2016; Gupta et al., 2018; Li et al., 2018) is to generate the most semantically similar sentence and they typically rely on beam search to obtain any kind of lexical diversity. Kumar et al. (2019) try to tackle the problem of achieving lexical, and limited"
2020.tacl-1.22,P02-1040,0,0.110591,"and remaining 3K for the validation set V. Step 1: For a given (X, Z ) ∈ Teval , construct an exemplar candidate set C = Teset − {X, Z }. |C |≈ 12, 000. (a) the generated and the syntactic exemplar in the test set - TED-E (b) the generated and the reference paraphrase in the test set - TED-R 4.3 Evaluation It should be noted that there is no single fully reliable metric for evaluating syntactic paraphrase generation. Therefore, we evaluate on the following metrics to showcase the efficacy of syntactic paraphrasing models. 1. Automated Evaluation. (i) Alignment based metrics: We compute BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), ROUGE-1, ROUGE-2, and ROUGE-L (Lin, 2004) scores between the generated and the reference paraphrases in the test set. (ii) Syntactic Transfer: We evaluate the syntactic transfer using Tree-edit distance (Zhang and Shasha, 1989) between the parse trees of: Step 2: Retain only those sentences C ∈ C whose sentence length (= number of tokens) differ by at most two when compared to the paraphrase Z . This is done since sentences with similar constituency-based parse tree structures tend to have similar token lengths. (iii) Model-based evaluation: Because our goa"
2020.tacl-1.22,D16-1159,0,0.0739105,"Missing"
2020.tacl-1.22,N19-1263,0,0.0383602,"Missing"
2020.tacl-1.22,P16-2049,0,0.0222866,"re important in not only providing information about what to say but also how to say it. Without any constraint, the ubiquitous sequence-to-sequence neural models often tend to produce degenerate outputs and favor generic utterances (Vinyals and Le, 2015; Li et al., 2016). Although simple attributes are helpful in addressing what to say, they provide very little information about how to say it. Syntactic control over generation helps in filling this gap by providing that missing information. Incorporating complex syntactic information has shown promising results in neural machine translation (Stahlberg et al., 2016; Aharoni and Goldberg, 2017; Yang et al., 2019), data-to-text generation (Peng et al., 2019), abstractive textsummarization (Cao et al., 2018), and adversarial text generation (Iyyer et al., 2018). Additionally, recent work (Iyyer et al., 2018; Kumar et al., 2019) has shown that augmenting lexical and syntactical variations in the training set can help in building better performing and more robust models. In this paper, we focus on the task of syntactically controlled paraphrase generation, that is, given an input sentence and a syntactic exemplar, produce a sentence that conforms to the synt"
2020.tacl-1.22,P15-1150,0,0.036485,"each node v ∈ V using the hidden-state representation of its parent node pa(v ) and the embedding associated with its label yv as follows: 3.2 Semantic Encoder The semantic encoder, a multilayered Gated Recurrent Unit (GRU), receives tokenized sentence X = {x1 , . . . , xTX } as input and computes the contextualized hidden state representation hX t for each token using: X hX t = GRU(ht−1 , e(xt )), hYv = GeLU(Wpa hYpa(v) + Wv e(yv ) + bv ), (2) where e(yv ) is the embedding of the node label yv , and Wpa , Wv , bv are learnable parameters. This approach can be considered similar to TreeLSTM (Tai et al., 2015). We use GeLU activation function (Hendrycks and Gimpel, 2016) rather than the standard tanh or relu, because of superior empirical performance. As indicated in Section 3.1, syntactic encoder takes as input the height H , which governs the level of syntactic control. We randomly prune the (1) where e(xt ) represents the learnable embedding of the token xt and t ∈ {1, . . . , TX }. Note that we use byte-pair encoding (Sennrich et al., 2016) for word/token segmentation. 333 Figure 2: The constituency parse tree serves as an input to the syntactic encoder (Section 3.3). The first step is to remov"
2020.tacl-1.22,D19-1072,0,0.0686855,"what to say but also how to say it. Without any constraint, the ubiquitous sequence-to-sequence neural models often tend to produce degenerate outputs and favor generic utterances (Vinyals and Le, 2015; Li et al., 2016). Although simple attributes are helpful in addressing what to say, they provide very little information about how to say it. Syntactic control over generation helps in filling this gap by providing that missing information. Incorporating complex syntactic information has shown promising results in neural machine translation (Stahlberg et al., 2016; Aharoni and Goldberg, 2017; Yang et al., 2019), data-to-text generation (Peng et al., 2019), abstractive textsummarization (Cao et al., 2018), and adversarial text generation (Iyyer et al., 2018). Additionally, recent work (Iyyer et al., 2018; Kumar et al., 2019) has shown that augmenting lexical and syntactical variations in the training set can help in building better performing and more robust models. In this paper, we focus on the task of syntactically controlled paraphrase generation, that is, given an input sentence and a syntactic exemplar, produce a sentence that conforms to the syntax of the exemplar while retaining the meaning o"
2020.tacl-1.22,N19-1131,0,0.0251461,"and other on ParaNMT + PAWS5 datasets (Classifier-2), Step 3: Remove those candidates C ∈ C, which are very similar to the source sentence X , that is, BLEU(X, C ) > 0.6. Step 4: From the remaining instances in C, choose that sentence C as the exemplar Y which has the least Tree-Edit distance with the paraphrase Z of the selected pair, namely, Y = argmin TED(Z, C ). This ensures that C ∈C the constituency-based parse tree of the exemplar Y is quite similar to that of Z , in terms of Tree-Edit distance. 5 Because the ParaNMT dataset only contains paraphrase pairs, we augment it with the PAWS (Zhang et al., 2019) dataset to acquire negative samples. Step 5: E := E ∪ (X, Y, Z ). 337 QQP-Pos Model BLEU↑ METEOR↑ ROUGE-1↑ ROUGE-2↑ ROUGE-L↑ Source-as-Output Exemplar-as-Output 17.2 16.8 31.1 17.6 51.9 38.2 26.2 20.5 52.9 43.2 16.2 4.8 TED-R↓ 16.6 0.0 TED-E↓ 99.8 10.7 PDS↑ SGCP (Iyyer et al., 2018) CGEN (Chen et al., 2019a) 15.6 34.9 19.6 37.4 40.6 62.6 20.5 42.7 44.6 65.4 9.1 6.7 8.0 6.0 27.0 65.4 SGCP-F SGCP-R 36.7 38.0 39.8 41.3 66.9 68.1 45.0 45.7 69.6 70.2 4.8 6.8 1.8 5.9 75.0 87.7 ParaNMT-small Source-as-Output Exemplar-as-Output 18.5 3.3 28.8 12.1 50.6 24.4 23.1 7.5 47.7 29.1 12.0 5.9 13.0 0.0 99.0 14"
2020.tacl-1.22,P18-1042,0,0.250416,"Missing"
2020.tacl-1.22,P08-1116,0,0.0503773,"hese systems find applications in adversarial sample generation (Iyyer et al., 2018), 2 https://www.kaggle.com/c/quoraquestion-pairs. text summarization, and table-to-text generation (Peng et al., 2019). While achieving state-of-theart in their respective domains, these systems typically rely on a known finite set of attributes thereby making them quite restrictive in terms of the styles they can offer. Paraphrase Generation. While generation of paraphrases has been addressed in the past using traditional methods (McKeown, 1983; Barzilay and Lee, 2003; Quirk et al., 2004; Hassan et al., 2007; Zhao et al., 2008; Madnani and Dorr, 2010; Wubben et al., 2010), they have recently been superseded by deep learning-based approaches (Prakash et al., 2016; Gupta et al., 2018; Li et al., 2019, 2018; Kumar et al., 2019). The primary task of all these methods (Prakash et al., 2016; Gupta et al., 2018; Li et al., 2018) is to generate the most semantically similar sentence and they typically rely on beam search to obtain any kind of lexical diversity. Kumar et al. (2019) try to tackle the problem of achieving lexical, and limited syntactical diversity using submodular optimization but do not provide any syntactic"
2020.tacl-1.22,W10-4223,0,0.461309,"Missing"
2020.tacl-1.22,N03-1003,0,\N,Missing
2020.tacl-1.22,C16-1275,0,\N,Missing
2020.tacl-1.22,P19-1332,0,\N,Missing
2021.acl-long.105,P19-1494,0,0.0179206,"for low webresource languages we cannot assume the presence of a well-trained translation system. We exploit the relatedness of the Indic languages to design a pseudo translation system that is motivated by two factors: • First, for most geographically proximal RPLLRL language pairs, word-level bilingual dictionaries have traditionally been available to enable communication. When they are not, crowd-sourcing creation of wordlevel dictionaries3 requires lower skill and resources than sentence level parallel data. Also, word-level lexicons can be created semiautomatically (Zhang et al., 2017) (Artetxe et al., 2019) (Xu et al., 2018). • Second, Indic languages exhibit common syntactic properties that control how words are composed to form a sentence. For example, they usually follow the Subject-ObjectVerb (SOV) order as against the Subject-VerbObject (SVO) order in English. We therefore create pseudo parallel data between R and L via a simple word-by-word translation using the bilingual lexicon. In a lexicon a word can be mapped to multiple words in another language. We choose a word with probability proportional to its frequency in the monolingual corpus DL . We experimented with a few other methods of"
2021.acl-long.105,2020.acl-main.421,0,0.0287269,"at adding a new target language to mBERT by simply extending the embedding layer with new weights results in better performing models when compared to bilingual-BERT pre-training with English as the second language. Pfeiffer et al. (2020c) adapt multilingual LMs to the LRLs and languages with scripts unseen during pre-training by learning new tokenizers for the unseen script and initializing their embedding matrix by leveraging the lexical overlap w.r.t. the languages seen during pre-training. Adapter (Pfeiffer et al., 2020a) based frameworks like (Pfeiffer et al., ¨ un et al., 2020) ad2020b; Artetxe et al., 2020; Ust¨ dress the lack of model’s capacity to accommodate multiple languages and establish the advantages of adding language-specific adapter modules in the BERT model for accommodating LRLs. These methods generally assume access to a fair amount of monolingual LRL data and do not exploit relatedness across languages explicitly. These methods provide complimentary gains to our method of directly exploiting language relatedness. LRL adaptation by utilizing parallel data When a parallel corpus of a high resource language and its translation into a LRL is available, Conneau and Lample (2019) show"
2021.acl-long.105,P19-4007,0,0.046573,"Missing"
2021.acl-long.105,N19-1423,0,0.118995,"r hypothesis that using a related language as pivot, along with transliteration and pseudo translation based data augmentation, can be an effective way to adapt LMs for LRLs, rather than direct training or pivoting through English. 1 Figure 1: Number of wikipedia articles for top-few Indian Languages and English. The height of the English bar is not to scale as indicated by the break. Number of English articles is roughly 400x more than articles in Oriya and 800x more than articles in Assamese. Introduction BERT-based pre-trained language models (LMs) have enabled significant advances in NLP (Devlin et al., 2019; Liu et al., 2019; Lan et al., 2020). Pretrained LMs have also been developed for the multilingual setting, where a single multilingual model is capable of handling inputs from many different ∗ Authors contributed equally languages. For example, the Multilingual BERT (mBERT) (Devlin et al., 2019) model was trained on 104 different languages. When fine-tuned for various downstream tasks, multilingual LMs have demonstrated significant success in generalizing across languages (Hu et al., 2020; Conneau et al., 2019). Thus, such models make it possible to transfer knowledge and resources from reso"
2021.acl-long.105,2020.findings-emnlp.319,0,0.0812766,"Missing"
2021.acl-long.105,P19-1433,0,0.0159394,"to our method of directly exploiting language relatedness. LRL adaptation by utilizing parallel data When a parallel corpus of a high resource language and its translation into a LRL is available, Conneau and Lample (2019) show that pre-training on concatenated parallel sentences results in improved cross-lingual transfer. Methods like Cao et al. (2020); Wu and Dredze (2020) discuss advantages of explicitly bringing together the contextual embeddings of aligned words in a translated pair. Language relatedness has been exploited in multilingual-NMT systems in various ways (Neubig and Hu, 2018; Goyal and Durrett, 2019; Song et al., 2020). These methods typically involve data augmentation for a LRL with help of a related high resource language (RPL) or to first learn the NMT model for a RPL followed by finetuning on the LRL. Wang et al. (2019) propose a soft-decoupled encoding approach for exploiting subword overlap between LRLs and HRLs to improve encoder representations for LRLs. Gao et al. (2020) address the issue of generating fluent 1314 LRL Punjabi Gujarati Bengali Percentage Overlap of Words Related Prominent Distant Prominent (Hindi) (English) 25.5 7.5 23.3 4.5 10.9 5.5 LRL (Target) Punjabi Gujarati"
2021.acl-long.105,2020.acl-srw.22,0,0.0877833,"Missing"
2021.acl-long.105,2020.findings-emnlp.445,0,0.0249745,"atasets. We demonstrate how RelateLM adapts mBERT to Oriya and Assamese, two low web-resource Indian languages by pivoting through Hindi. Via ablation studies on bilingual models we show that RelateLM is able to achieve accuracy of zero-shot transfer with limited data (20K documents) that is not surpassed even with four times as much data in existing methods. The source code for our experiments is available at https://github.com/yashkhem1/RelateLM. 2 Related Work Transformer (Vaswani et al., 2017) based language models like mBERT (Devlin et al., 2019), MuRIL (Khanuja et al., 2021), IndicBERT (Kakwani et al., 2020), and XLM-R (Conneau et al., 2019), trained on massive multilingual datasets have been shown to scale across a variety of tasks and languages. The zero-shot cross-lingual transferability offered by these models makes them promising for lowresource domains. Pires et al. (2019) find that cross-lingual transfer is even possible across languages of different scripts, but is more effective for typologically related languages. However, recent works (Lauscher et al., 2020; Pfeiffer et al., 2020b; Hu et al., 2020) have identified poor cross-lingual transfer to languages with limited data when jointly"
2021.acl-long.105,2020.emnlp-main.363,0,0.0416148,"Missing"
2021.acl-long.105,E17-2002,0,0.0507023,"Missing"
2021.acl-long.105,D18-1103,0,0.0181867,"complimentary gains to our method of directly exploiting language relatedness. LRL adaptation by utilizing parallel data When a parallel corpus of a high resource language and its translation into a LRL is available, Conneau and Lample (2019) show that pre-training on concatenated parallel sentences results in improved cross-lingual transfer. Methods like Cao et al. (2020); Wu and Dredze (2020) discuss advantages of explicitly bringing together the contextual embeddings of aligned words in a translated pair. Language relatedness has been exploited in multilingual-NMT systems in various ways (Neubig and Hu, 2018; Goyal and Durrett, 2019; Song et al., 2020). These methods typically involve data augmentation for a LRL with help of a related high resource language (RPL) or to first learn the NMT model for a RPL followed by finetuning on the LRL. Wang et al. (2019) propose a soft-decoupled encoding approach for exploiting subword overlap between LRLs and HRLs to improve encoder representations for LRLs. Gao et al. (2020) address the issue of generating fluent 1314 LRL Punjabi Gujarati Bengali Percentage Overlap of Words Related Prominent Distant Prominent (Hindi) (English) 25.5 7.5 23.3 4.5 10.9 5.5 LRL"
2021.acl-long.105,P17-1178,0,0.0286495,"9 77.5 71.7 78.7 Table 5: Different Adaptation Strategies evaluated for zero-shot transfer (F1-score) on NER, POS tagging and Text Classification after fine-tuning with the Prominent Language (English or Hindi). mBERT, which is trained with much larger datasets and more languages is not directly comparable, and is presented here just for reference. tuning on the RPL separately for three tasks: NER, POS and Text classification. Table 3 presents a summary of the training, validation data in RPL and test data in LRL on which we perform zero-shot evaluation. We obtained the NER data from WikiANN (Pan et al., 2017) and XTREME (Hu et al., 2020) and the POS and Text Classification data from the Technology Development for Indian Languages (TDIL)6 . We downsampled the TDIL data for each language to make them class-balanced. The POS tagset used was the BIS Tagset (Sardesai et al., 2012). For the English POS Dataset, we had to map the PENN tagset in to the BIS tagset. We have provided the mapping that we used in the Appendix (B) Methods compared We contrast RelateLM with three other adaptation techniques: (1) EBERT (Wang et al., 2020) that extends the vocabulary and tunes with MLM on DL as-is, (2) RelateLM wi"
2021.acl-long.105,2020.emnlp-demos.7,0,0.0692451,"Missing"
2021.acl-long.105,2020.emnlp-main.617,0,0.0509463,"Missing"
2021.acl-long.105,P19-1493,0,0.0248422,"ments) that is not surpassed even with four times as much data in existing methods. The source code for our experiments is available at https://github.com/yashkhem1/RelateLM. 2 Related Work Transformer (Vaswani et al., 2017) based language models like mBERT (Devlin et al., 2019), MuRIL (Khanuja et al., 2021), IndicBERT (Kakwani et al., 2020), and XLM-R (Conneau et al., 2019), trained on massive multilingual datasets have been shown to scale across a variety of tasks and languages. The zero-shot cross-lingual transferability offered by these models makes them promising for lowresource domains. Pires et al. (2019) find that cross-lingual transfer is even possible across languages of different scripts, but is more effective for typologically related languages. However, recent works (Lauscher et al., 2020; Pfeiffer et al., 2020b; Hu et al., 2020) have identified poor cross-lingual transfer to languages with limited data when jointly pre-trained. A primary reason behind poor transfer is the lack of model’s capacity to accommodate all languages simultaneously. This has led to increased interest in adapting multilingual LMs to LRLs and we discuss these in the following two settings. LRL adaptation using mon"
2021.acl-long.105,W12-5012,0,0.0210634,"ore languages is not directly comparable, and is presented here just for reference. tuning on the RPL separately for three tasks: NER, POS and Text classification. Table 3 presents a summary of the training, validation data in RPL and test data in LRL on which we perform zero-shot evaluation. We obtained the NER data from WikiANN (Pan et al., 2017) and XTREME (Hu et al., 2020) and the POS and Text Classification data from the Technology Development for Indian Languages (TDIL)6 . We downsampled the TDIL data for each language to make them class-balanced. The POS tagset used was the BIS Tagset (Sardesai et al., 2012). For the English POS Dataset, we had to map the PENN tagset in to the BIS tagset. We have provided the mapping that we used in the Appendix (B) Methods compared We contrast RelateLM with three other adaptation techniques: (1) EBERT (Wang et al., 2020) that extends the vocabulary and tunes with MLM on DL as-is, (2) RelateLM without pseudo translation loss, and (3) m-BERT when the language exists in m-BERT. Training Details For pre-training on MLM we chose batch size as 2048, learning rate as 3e-5 and maximum sequence length as 128. We used whole word masking for MLM and BertWordPieceTokenizer"
2021.acl-long.105,P17-1179,0,0.0145484,"udo parallel data but for low webresource languages we cannot assume the presence of a well-trained translation system. We exploit the relatedness of the Indic languages to design a pseudo translation system that is motivated by two factors: • First, for most geographically proximal RPLLRL language pairs, word-level bilingual dictionaries have traditionally been available to enable communication. When they are not, crowd-sourcing creation of wordlevel dictionaries3 requires lower skill and resources than sentence level parallel data. Also, word-level lexicons can be created semiautomatically (Zhang et al., 2017) (Artetxe et al., 2019) (Xu et al., 2018). • Second, Indic languages exhibit common syntactic properties that control how words are composed to form a sentence. For example, they usually follow the Subject-ObjectVerb (SOV) order as against the Subject-VerbObject (SVO) order in English. We therefore create pseudo parallel data between R and L via a simple word-by-word translation using the bilingual lexicon. In a lexicon a word can be mapped to multiple words in another language. We choose a word with probability proportional to its frequency in the monolingual corpus DL . We experimented with"
2021.acl-long.105,2020.acl-srw.37,0,0.0194708,"exploiting language relatedness. LRL adaptation by utilizing parallel data When a parallel corpus of a high resource language and its translation into a LRL is available, Conneau and Lample (2019) show that pre-training on concatenated parallel sentences results in improved cross-lingual transfer. Methods like Cao et al. (2020); Wu and Dredze (2020) discuss advantages of explicitly bringing together the contextual embeddings of aligned words in a translated pair. Language relatedness has been exploited in multilingual-NMT systems in various ways (Neubig and Hu, 2018; Goyal and Durrett, 2019; Song et al., 2020). These methods typically involve data augmentation for a LRL with help of a related high resource language (RPL) or to first learn the NMT model for a RPL followed by finetuning on the LRL. Wang et al. (2019) propose a soft-decoupled encoding approach for exploiting subword overlap between LRLs and HRLs to improve encoder representations for LRLs. Gao et al. (2020) address the issue of generating fluent 1314 LRL Punjabi Gujarati Bengali Percentage Overlap of Words Related Prominent Distant Prominent (Hindi) (English) 25.5 7.5 23.3 4.5 10.9 5.5 LRL (Target) Punjabi Gujarati Bengali Table 1: Mo"
2021.acl-long.105,2020.emnlp-main.180,0,0.0374132,"Missing"
2021.acl-long.105,2020.findings-emnlp.240,0,0.229345,"pt the given LM M for the target LRL L using a combination of Masked Language Model (MLM) and alignment losses. For notations and further details, please see Section 3. English, and yet Hindi is seven times larger than the O(20,000) documents of languages like Oriya and Assamese which are spoken by millions of people. This calls for the development of additional mechanisms for training multilingual LMs which are not exclusively reliant on large monolingual corpora. Recent methods of adapting a pre-trained multilingual LM to a LRL include fine-tuning the full model with an extended vocabulary (Wang et al., 2020), training a light-weight adapter layer while keeping the full model fixed (Pfeiffer et al., 2020b), and exploiting overlapping tokens to learn embeddings of the LRL (Pfeiffer et al., 2020c). These are general-purpose methods that do not sufficiently exploit the specific relatedness of languages within the same family. We propose RelateLM for this task. RelateLM exploits relatedness between the LRL of interest and a Related Prominent Language (RPL). We focus on Indic languages, and consider Hindi as the RPL. The languages we consider in this paper are related along several dimensions of lingui"
2021.acl-long.105,2020.emnlp-main.362,0,0.165677,"fic adapter modules in the BERT model for accommodating LRLs. These methods generally assume access to a fair amount of monolingual LRL data and do not exploit relatedness across languages explicitly. These methods provide complimentary gains to our method of directly exploiting language relatedness. LRL adaptation by utilizing parallel data When a parallel corpus of a high resource language and its translation into a LRL is available, Conneau and Lample (2019) show that pre-training on concatenated parallel sentences results in improved cross-lingual transfer. Methods like Cao et al. (2020); Wu and Dredze (2020) discuss advantages of explicitly bringing together the contextual embeddings of aligned words in a translated pair. Language relatedness has been exploited in multilingual-NMT systems in various ways (Neubig and Hu, 2018; Goyal and Durrett, 2019; Song et al., 2020). These methods typically involve data augmentation for a LRL with help of a related high resource language (RPL) or to first learn the NMT model for a RPL followed by finetuning on the LRL. Wang et al. (2019) propose a soft-decoupled encoding approach for exploiting subword overlap between LRLs and HRLs to improve encoder represent"
2021.acl-long.105,P19-1579,0,0.0574025,"Missing"
2021.acl-long.105,D18-1268,0,0.0249804,"nguages we cannot assume the presence of a well-trained translation system. We exploit the relatedness of the Indic languages to design a pseudo translation system that is motivated by two factors: • First, for most geographically proximal RPLLRL language pairs, word-level bilingual dictionaries have traditionally been available to enable communication. When they are not, crowd-sourcing creation of wordlevel dictionaries3 requires lower skill and resources than sentence level parallel data. Also, word-level lexicons can be created semiautomatically (Zhang et al., 2017) (Artetxe et al., 2019) (Xu et al., 2018). • Second, Indic languages exhibit common syntactic properties that control how words are composed to form a sentence. For example, they usually follow the Subject-ObjectVerb (SOV) order as against the Subject-VerbObject (SVO) order in English. We therefore create pseudo parallel data between R and L via a simple word-by-word translation using the bilingual lexicon. In a lexicon a word can be mapped to multiple words in another language. We choose a word with probability proportional to its frequency in the monolingual corpus DL . We experimented with a few other methods of selecting words th"
2021.acl-long.520,D18-1225,1,0.610586,"Missing"
2021.acl-long.520,N19-1423,0,0.0327819,"while creating this dataset: 1. The associated KG must provide temporal annotations. 2. Questions must involve an element of temporal reasoning. 3. The number of labeled instances must be large enough that it can be used for training models, rather than for evaluation alone. Guided by the above principles, we present a dataset consisting of a Temporal KG with 125k entities and 328k facts, along with a set of 410k natural language questions that require temporal reasoning. On this new dataset, we apply approaches based on deep language models (LM) alone, such as T5 (Raffel et al., 2020), BERT (Devlin et al., 2019), and KnowBERT (Peters et al., 2019), and also hybrid LM+KG embedding approaches, such as Entities-as-Experts (F´evry et al., 2020) and EmbedKGQA (Saxena et al., 2020). We find that these baselines are not suited to temporal reasoning. In response, we propose C RON KGQA, an enhancement of EmbedKGQA, which outperforms baselines across all question types. C RON KGQA achieves very high accuracy on simple temporal reasoning questions, but falls short when it comes to questions requiring more complex reasoning. Thus, although we get promising early results, C RON Q UESTIONS leaves ample scope to im"
2021.acl-long.520,2020.emnlp-main.400,0,0.162396,"Missing"
2021.acl-long.520,D18-1516,0,0.0314192,"Missing"
2021.acl-long.520,N19-1090,0,0.0280414,"Missing"
2021.acl-long.520,2020.emnlp-main.305,1,0.917122,"on Answering Over Temporal Knowledge Graphs Apoorv Saxena Indian Institute of Science Bangalore Soumen Chakrabarti Indian Institute of Technology Bombay Partha Talukdar Google Research India apoorvsaxena@iisc.ac.in soumen@cse.iitb.ac.in partha@google.com Abstract time intervals as well. These temporal scopes on facts can be either automatically estimated (Talukdar et al., 2012) or user contributed. Several such Temporal KGs have been proposed in the literature, where the focus is on KG completion (Dasgupta et al. 2018; Garc´ıa-Dur´an et al. 2018; Leetaru and Schrodt 2013; Lacroix et al. 2020; Jain et al. 2020). Temporal Knowledge Graphs (Temporal KGs) extend regular Knowledge Graphs by providing temporal scopes (e.g., start and end times) on each edge in the KG. While Question Answering over KG (KGQA) has received some attention from the research community, QA over Temporal KGs (Temporal KGQA) is a relatively unexplored area. Lack of broadcoverage datasets has been another factor limiting progress in this area. We address this challenge by presenting C RON Q UESTIONS, the largest known Temporal KGQA dataset, clearly stratified into buckets of structural complexity. C RON Q UESTIONS expands the only"
2021.acl-long.520,D16-1147,0,0.0282706,"trigger words (for example ‘before’, ‘after’) along with other constraints to filter out questions from these datasets that fell under this definition. However, this dataset contains only 1271 questions — useful only for evaluation — and the KG on which it is based (a subset of FreeBase (Bollacker et al., 2008)) is not a temporal KG. Another drawback is that FreeBase has not been under active development since 2015, therefore some information stored in it is outdated and this is a potential source of inaccuracy. 2.2 Temporal QA algorithms To the best of our knowledge, recent KGQA algorithms (Miller et al. 2016; Sun et al. 2019; Cohen et al. 2020; Sun et al. 2020) work with nontemporal KGs, i.e., KGs containing facts of the form (subject, relation, object). Extending these to temporal KGs containing facts of the form (subject, relation, object, start time, end time) is a non-trivial task. TEQUILA (Jia et al., 2018b) is one method aimed specifically at temporal KGQA. TEQUILA decomposes and rewrites the question into nontemporal sub-questions and temporal constraints. Answers to sub-questions are then retrieved using any KGQA engine. Finally, TEQUILA uses constraint reasoning on temporal intervals to"
2021.acl-long.520,2020.emnlp-main.88,0,0.0198421,"stions (Bordes et al., 2015) one needs to extract just a single fact from the KG to answer a question. MetaQA (Zhang et al., 2017) and WebQuestionsSP (Yih et al., 2015) require multi-hop reasoning, where one must traverse over multiple edges in the KG to reach the answer. ComplexWebQuestions (Talmor and Berant, 2018) contains both multi-hop and conjunction/comparison type questions. However, none of these are aimed at temporal reasoning, and the KG they are based on is non-temporal. Temporal QA datasets have mostly been studied in the area of reading comprehension. One such dataset is TORQUE (Ning et al., 2020), where the system is given a question along with some context (a text passage) and is asked to answer a multiple choice question with five choices. This is in contrast to KGQA, where there is no context, and the answer is one of potentially hundreds of thousands of entities. TempQuestions (Jia et al., 2018a) is a KGQA dataset specifically aimed at temporal QA. It consists of a subset of questions from WebQuestions, Free917 (Cai and Yates, 2013) and ComplexQuestions (Bao et al., 2016) that are temporal in 6664 Reasoning Simple time Simple entity Before/After First/Last Time join Example Templa"
2021.acl-long.520,D19-1005,0,0.0555952,"Missing"
2021.acl-long.520,D19-1250,0,0.0570378,"Missing"
2021.acl-long.520,D16-1264,0,0.0611667,"y over the next best performing method. Through extensive experiments, we give detailed insights into the workings of C RON KGQA, as well as situations where significant further improvements appear possible. In addition to the dataset, we have released our code as well. 1 The task of Knowledge Graph Question Answering (KGQA) is to answer natural language questions using a KG as the knowledge base. This is in contrast to reading comprehension-based question answering, where typically the question is accompanied by a context (e.g., text passage) and the answer is either one of multiple choices (Rajpurkar et al., 2016) or a piece of text from the context (Yang et al., 2018). In KGQA, the answer is usually an entity (node) in the KG, and the reasoning required to answer questions is either single-fact based (Bordes et al., 2015), multi-hop (Yih et al. 2015, Zhang et al. 2017) or conjunction/comparison based reasoning (Talmor and Berant, 2018). Temporal KGQA takes this a step further where: 1. The underlying KG is a Temporal KG. 2. The answer is either an entity or time duration. 3. Complex temporal reasoning might be needed. Introduction Temporal Knowledge Graphs (Temporal KGs) are multi-relational graph whe"
2021.acl-long.520,2020.acl-main.412,1,0.880395,"ed instances must be large enough that it can be used for training models, rather than for evaluation alone. Guided by the above principles, we present a dataset consisting of a Temporal KG with 125k entities and 328k facts, along with a set of 410k natural language questions that require temporal reasoning. On this new dataset, we apply approaches based on deep language models (LM) alone, such as T5 (Raffel et al., 2020), BERT (Devlin et al., 2019), and KnowBERT (Peters et al., 2019), and also hybrid LM+KG embedding approaches, such as Entities-as-Experts (F´evry et al., 2020) and EmbedKGQA (Saxena et al., 2020). We find that these baselines are not suited to temporal reasoning. In response, we propose C RON KGQA, an enhancement of EmbedKGQA, which outperforms baselines across all question types. C RON KGQA achieves very high accuracy on simple temporal reasoning questions, but falls short when it comes to questions requiring more complex reasoning. Thus, although we get promising early results, C RON Q UESTIONS leaves ample scope to improve complex Temporal KGQA. Our source code along with the C RON Q UESTIONS dataset can be found at https://github.com/apoorvumang/CronKGQA. 2 2.1 Related work Tempor"
2021.acl-long.520,D19-1242,0,0.0160818,"example ‘before’, ‘after’) along with other constraints to filter out questions from these datasets that fell under this definition. However, this dataset contains only 1271 questions — useful only for evaluation — and the KG on which it is based (a subset of FreeBase (Bollacker et al., 2008)) is not a temporal KG. Another drawback is that FreeBase has not been under active development since 2015, therefore some information stored in it is outdated and this is a potential source of inaccuracy. 2.2 Temporal QA algorithms To the best of our knowledge, recent KGQA algorithms (Miller et al. 2016; Sun et al. 2019; Cohen et al. 2020; Sun et al. 2020) work with nontemporal KGs, i.e., KGs containing facts of the form (subject, relation, object). Extending these to temporal KGs containing facts of the form (subject, relation, object, start time, end time) is a non-trivial task. TEQUILA (Jia et al., 2018b) is one method aimed specifically at temporal KGQA. TEQUILA decomposes and rewrites the question into nontemporal sub-questions and temporal constraints. Answers to sub-questions are then retrieved using any KGQA engine. Finally, TEQUILA uses constraint reasoning on temporal intervals to compute final ans"
2021.acl-long.520,D18-1259,0,0.0250131,"eriments, we give detailed insights into the workings of C RON KGQA, as well as situations where significant further improvements appear possible. In addition to the dataset, we have released our code as well. 1 The task of Knowledge Graph Question Answering (KGQA) is to answer natural language questions using a KG as the knowledge base. This is in contrast to reading comprehension-based question answering, where typically the question is accompanied by a context (e.g., text passage) and the answer is either one of multiple choices (Rajpurkar et al., 2016) or a piece of text from the context (Yang et al., 2018). In KGQA, the answer is usually an entity (node) in the KG, and the reasoning required to answer questions is either single-fact based (Bordes et al., 2015), multi-hop (Yih et al. 2015, Zhang et al. 2017) or conjunction/comparison based reasoning (Talmor and Berant, 2018). Temporal KGQA takes this a step further where: 1. The underlying KG is a Temporal KG. 2. The answer is either an entity or time duration. 3. Complex temporal reasoning might be needed. Introduction Temporal Knowledge Graphs (Temporal KGs) are multi-relational graph where each edge is associated with a time duration. This is"
2021.acl-long.520,P15-1128,0,0.262884,"sed our code as well. 1 The task of Knowledge Graph Question Answering (KGQA) is to answer natural language questions using a KG as the knowledge base. This is in contrast to reading comprehension-based question answering, where typically the question is accompanied by a context (e.g., text passage) and the answer is either one of multiple choices (Rajpurkar et al., 2016) or a piece of text from the context (Yang et al., 2018). In KGQA, the answer is usually an entity (node) in the KG, and the reasoning required to answer questions is either single-fact based (Bordes et al., 2015), multi-hop (Yih et al. 2015, Zhang et al. 2017) or conjunction/comparison based reasoning (Talmor and Berant, 2018). Temporal KGQA takes this a step further where: 1. The underlying KG is a Temporal KG. 2. The answer is either an entity or time duration. 3. Complex temporal reasoning might be needed. Introduction Temporal Knowledge Graphs (Temporal KGs) are multi-relational graph where each edge is associated with a time duration. This is in contrast to a regular KG where no time annotation is present. For example, a regular KG may contain a fact such as (Barack Obama, held position, President of USA), while a temporal"
2021.findings-acl.225,2020.acl-main.209,0,0.204946,"embeddings for OpenKGs has been a relatively under-explored area of research. Previous work using OpenKG embeddings has primarily focused on canonicalization. CESI (Vashishth et al., 2018) uses KG embedding models for the canonicalization of noun phrases in OpenKGs. The problem of incorporating canonicalization information into OpenKG embeddings was addressed by Gupta et al. (2019). Their method for OpenKG embeddings (i.e., CaRE) performs better than Ontological KG embedding baselines in terms of link prediction performance. The challenges in the link prediction for OpenKGs were discussed in Broscheit et al. (2020), and methods similar to CaRE were proposed. In spirit, CaRE (Gupta et al., 2019) comes closest to our model; however, they do not address the problem of type compatibility in the link prediction task. Entity Type: Entity typing is a popular problem where given a sentence and an entity mention, the goal is to predict explicit types of the entity. It has been an active area of research, and many models and datasets, such as (Mai et al., 2018), (Hovy et al., 2006), and (Choi et al., 2018), have been proposed. However, unlike this task, we aim to incorporate unsupervised implicit type information"
2021.findings-acl.225,D19-1036,1,0.681613,"multi-relational graph where the noun phrases (NPs) are the nodes, and the relation phrases (RPs) are the labeled edges between pairs of nodes. It is easy to bootstrap OpenKGs from a domain-specific corpus, making Although OpenKGs are structurally similar to Ontological KGs, they come with a different set of challenges. They are extremely sparse, NPs and RPs are not canonicalized, and no type information is present for NPs. There has been much work on learning embeddings for Ontological KGs in the past years. However, this task has not received much attention in the context of OpenKGs. CaRE (Gupta et al., 2019) is a recent method which addresses this problem. It learns embeddings for NPs and RPs in an OpenKG while incorporating NP canonicalization information. However, even after incorporating canonicalization, we find that CaRE struggles to predict NPs whose types are compati2546 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2546–2559 August 1–6, 2021. ©2021 Association for Computational Linguistics ble with given head NP and RP. As observed by Petroni et al. (2019), modern pretrained language representation models like BERT can store factual knowledge and can be"
2021.findings-acl.225,N06-2015,0,0.0549718,"embedding baselines in terms of link prediction performance. The challenges in the link prediction for OpenKGs were discussed in Broscheit et al. (2020), and methods similar to CaRE were proposed. In spirit, CaRE (Gupta et al., 2019) comes closest to our model; however, they do not address the problem of type compatibility in the link prediction task. Entity Type: Entity typing is a popular problem where given a sentence and an entity mention, the goal is to predict explicit types of the entity. It has been an active area of research, and many models and datasets, such as (Mai et al., 2018), (Hovy et al., 2006), and (Choi et al., 2018), have been proposed. However, unlike this task, we aim to incorporate unsupervised implicit type information present in the pre-trained BERT model into OpenKG embeddings, rather than predicting explicit entity types present in ontologies or corpora. For unsupervised cases, the problem of type compatibility in link prediction was addressed in (Jain et al., 2018). They employ a type compatibility score by learning a type vector for each NP and two type vectors (head and tail) for each relation. This score is multiplied with the triple score function, and the type vector"
2021.findings-acl.225,P18-2013,0,0.027387,"ular problem where given a sentence and an entity mention, the goal is to predict explicit types of the entity. It has been an active area of research, and many models and datasets, such as (Mai et al., 2018), (Hovy et al., 2006), and (Choi et al., 2018), have been proposed. However, unlike this task, we aim to incorporate unsupervised implicit type information present in the pre-trained BERT model into OpenKG embeddings, rather than predicting explicit entity types present in ontologies or corpora. For unsupervised cases, the problem of type compatibility in link prediction was addressed in (Jain et al., 2018). They employ a type compatibility score by learning a type vector for each NP and two type vectors (head and tail) for each relation. This score is multiplied with the triple score function, and the type vectors are trained jointly with embedding vectors. Although their method addresses the type compatibility issue, it is based on Ontological KG embedding models and shares the same limitations. In another work (Xie et al., 2016), hierarchical type information available in the dataset is incorporated while learning embeddings. However, their model is suitable only for Ontological KGs where the"
2021.findings-acl.225,2020.tacl-1.5,0,0.0161806,"loss term which forces the type vectors of correct triples to be closer in the type space. Similar to TripleLoss, we use the binary cross-entropy loss for type regularization as well. The type regularization term is shown below. Experiments Datasets: Following (Gupta et al., 2019), we use two subsets of English OpenKGs created using ReVerb (Fader et al., 2011), namely ReVerb20K and ReVerb45K. We follow the same train-validationtest split for these datasets. As noted in (Petroni et al., 2019), predicting multi-token NPs using BERT could be challenging and it might require special pre-training (Joshi et al., 2020). To understand this difference, we create filtered subsets of these datasets such that they contain only single token NPs 1 . Specifically, we create ReVerb20KF (ReVerb20K-Filtered) and ReVerb45KF (ReVerb45K-Filtered) which contain only single token NPs. More details about these datasets can be found in Table 2. Setup and hyperparameters: We use de = dr = 300 for NP and RP vectors. For other hyperparameters, we use grid-search and select the model based on MRR on validation split. For type vectors, we select dτ from {100, 300, 500}. The weight for type regularization term λ is selected from t"
2021.findings-acl.225,D16-1040,1,0.829833,"can see, both CaRE and BERT fail to predict the correct tail NP. However, BERT predictions are type compatible with the query. OKGIT predicts the correct NP while improving the type compatibility with the query. them suitable for newer domains. However, they are extremely sparse and may not be directly usable for an end task. Therefore, tasks such as NP canonicalization (merging mentions of the same entity) and link prediction (predicting new facts) become an important step in downstream applications. Some example applications are text comprehension (Mausam, 2016), relation schema induction (Nimishakavi et al., 2016), canonicalization (Vashishth et al., 2018), question answering (Yao and Van Durme, 2014), and web search query recommendation (Huang et al., 2016). In this work, we focus on improving OpenKG link prediction. Introduction An Open Knowledge Graph (OpenKG) is a set of factual triples extracted from a text corpus using Open Information Extraction (OpenIE) tools such as T EXT RUNNER (Banko et al., 2007) and ReVerb (Fader et al., 2011). These triples are of the form (noun phrase, relation phrase, noun phrase), e.g., (tesla, return to, new york). An OpenKG can be viewed as a multi-relational graph w"
2021.findings-acl.225,D19-1250,0,0.0839553,"Missing"
2021.findings-acl.254,2020.acl-main.421,0,0.0162589,"stic student LM. Introduction While current state-of-the-art multilingual language models (LMs) (Devlin et al., 2019; Conneau et al., 2020) aim to represent 100+ languages in a single model, efforts towards building monolingual (Martin et al., 2019; Kuratov and Arkhipov, 2019) or language-family based (Khanuja et al., 2021) models are only increasing with time (Rust et al., 2020). A single model is often incapable of effectively representing a diverse set of languages, evidence of which has been provided by works highlighting the importance of vocabulary curation and size (Chung et al., 2020; Artetxe et al., 2020), pre-training data volume (Liu et al., 2019a; Conneau et al., 2020), and the curse of multilinguality (Conneau et al., 2020). Language specific models alleviate these issues with a custom vocabulary which captures language subtleties1 and large magnitudes of pre-training data scraped from several domains (Virtanen et al., 2019; Antoun et al., 2020). However, building language specific LMs brings us back to where we started, preventing us from leveraging the benefits of multilinguality like zeroshot task transfer (Hu et al., 2020), positive transfer between related languages (Pires et al., 201"
2021.findings-acl.254,2020.lrec-1.677,0,0.0374893,"Missing"
2021.findings-acl.254,2020.coling-main.598,0,0.0622113,"Missing"
2021.findings-acl.254,2020.emnlp-main.367,0,0.0157235,"ltilingual task-agnostic student LM. Introduction While current state-of-the-art multilingual language models (LMs) (Devlin et al., 2019; Conneau et al., 2020) aim to represent 100+ languages in a single model, efforts towards building monolingual (Martin et al., 2019; Kuratov and Arkhipov, 2019) or language-family based (Khanuja et al., 2021) models are only increasing with time (Rust et al., 2020). A single model is often incapable of effectively representing a diverse set of languages, evidence of which has been provided by works highlighting the importance of vocabulary curation and size (Chung et al., 2020; Artetxe et al., 2020), pre-training data volume (Liu et al., 2019a; Conneau et al., 2020), and the curse of multilinguality (Conneau et al., 2020). Language specific models alleviate these issues with a custom vocabulary which captures language subtleties1 and large magnitudes of pre-training data scraped from several domains (Virtanen et al., 2019; Antoun et al., 2020). However, building language specific LMs brings us back to where we started, preventing us from leveraging the benefits of multilinguality like zeroshot task transfer (Hu et al., 2020), positive transfer between related langu"
2021.findings-acl.254,2020.tacl-1.30,0,0.0323119,"Missing"
2021.findings-acl.254,P19-1595,0,0.105846,"s: task-specific and task-agnostic. In task-specific distillation, the teacher LM is first fine-tuned for a specific task and is then distilled into a student model which can solve that task. Task-agnostic methods perform distillation on the pre-training objective like masked language modeling (MLM) in order to obtain a task-agnostic student model. Prior work has either used task-agnostic distillation to compress singlelanguage teachers (Sanh et al., 2019; Sun et al., 2020) or used task-specific distillation to combine multiple fine-tuned teachers into a multi-task student (Liu et al., 2019b; Clark et al., 2019). The former prevents positive language transfer while the latter restricts the student’s capabilities to the tasks and languages in the fine-tuned teacher LMs (as shown in Figure 1). We focus on the problem of merging multiple pre-trained LMs into a single multilingual student LM in the task-agnostic setting. To the best of our knowledge, this is the first effort of its kind, and makes the following contributions: 2875 • We propose M ERGE D ISTILL, a task-agnostic distillation approach to merge multiple teacher LMs at the pre-training stage, to train a strong multilingual student LM that can"
2021.findings-acl.254,2020.acl-main.747,0,0.0706604,"Missing"
2021.findings-acl.254,N19-1423,0,0.638573,"ely with or even outperform teacher LMs trained on several orders of magnitude more data and with a fixed model capacity. We also highlight the importance of teacher selection and its impact on student model performance. 1 Figure 1: Previous works (left) typically focus on combining fine-tuned models derived from a single pre-trained model using distillation. We propose M ERGE D ISTILL to combine pre-trained teacher LMs from multiple monolingual/multilingual LMs into a single multilingual task-agnostic student LM. Introduction While current state-of-the-art multilingual language models (LMs) (Devlin et al., 2019; Conneau et al., 2020) aim to represent 100+ languages in a single model, efforts towards building monolingual (Martin et al., 2019; Kuratov and Arkhipov, 2019) or language-family based (Khanuja et al., 2021) models are only increasing with time (Rust et al., 2020). A single model is often incapable of effectively representing a diverse set of languages, evidence of which has been provided by works highlighting the importance of vocabulary curation and size (Chung et al., 2020; Artetxe et al., 2020), pre-training data volume (Liu et al., 2019a; Conneau et al., 2020), and the curse of multilin"
2021.findings-acl.254,P19-1070,0,0.0389661,"Missing"
2021.findings-acl.254,2020.emnlp-main.363,0,0.0311988,"Missing"
2021.findings-acl.254,N18-1202,0,0.0324865,"Missing"
2021.findings-acl.254,P19-1493,0,0.0875145,"txe et al., 2020), pre-training data volume (Liu et al., 2019a; Conneau et al., 2020), and the curse of multilinguality (Conneau et al., 2020). Language specific models alleviate these issues with a custom vocabulary which captures language subtleties1 and large magnitudes of pre-training data scraped from several domains (Virtanen et al., 2019; Antoun et al., 2020). However, building language specific LMs brings us back to where we started, preventing us from leveraging the benefits of multilinguality like zeroshot task transfer (Hu et al., 2020), positive transfer between related languages (Pires et al., 2019; Lauscher et al., 2020) and an ability to handle codemixed text (Pires et al., 2019; Tsai et al., 2019). We need an approach that encompasses the best of both worlds, i.e., leverage the capabilities of the powerful language-specific LMs while still being multilingual and enabling positive language trans1 For example, in Arabic, (Antoun et al., 2020) argue that while the definite article “Al”, which is equivalent to “the” in English, is always prefixed to other words, it is not an intrinsic part of that word. While with a BERT-compatible tokenization tokens will appear twice, once with “Al-” a"
2021.findings-acl.254,P19-1015,0,0.0610777,"Missing"
2021.findings-acl.254,D16-1264,0,0.0631207,"Missing"
2021.findings-acl.395,2020.tacl-1.28,0,0.0473083,"in a prompt and show that searching over them can lead to significant gains in few-shot performance, without needing updates to the model parameters.3 Measuring task performance of language models without any parameter updates can be seen as a measure of the knowledge (either descriptive, or procedural) that is already contained in the pretrained language model. Probing knowledge contained in language models has been of interest, given the success of these models. Probing methods rely on creating cloze-style manual prompts (Petroni et al., 2019), or mining efficient natural language prompts (Jiang et al., 2020). Shin et al. (2020) rely on training examples to learn trigger tokens which when used as a prompt demonstrate the ability of language models to do sentiment analysis and NLI along with knowledge based completion, without needing any parameter updates. The learned trigger tokens however aren’t very meaningful leading to difficulty in interpreting these results. In this work, we instead focus on using natural language training examples as prompts. While being more interpretable, the prompts used in this work lead to significant gains in performance in the low-data regime. 3 Background: Genetic"
2021.findings-acl.395,2021.ccl-1.108,0,0.0348344,"Missing"
2021.findings-acl.395,marelli-etal-2014-sick,0,0.0133744,"citing knowledge from language models. Please see Section 5.3 for details and Appendix A.2.2 for more detailed results. Q3 What aspects of PERO are important for getting good performance? (Section 5.5) The experimental setup is described in Section 5.2, and the datasets are described in Section 5.1. 5.1 Datasets Sentiment Classification: We use SST2 (Socher et al., 2013), a binary sentiment classification task. The training data contains 67350 training, 873 validation and 1822 test examples. NLI: We use label-balanced 2-class NLI dataset created by Shin et al. (2020) using the SICK-E dataset (Marelli et al., 2014). This dataset has 1289 training, 143 validation and 1427 test examples. Fact Retrieval: We use the train, validation, and test splits created by Shin et al. (2020) (referred to as ‘original’ in the paper) for 41 relations. For our experiments, we use the manual prompts created by Petroni et al. (2019). Please see Appendix A.2.2 for relation wise prompts and training statistics. 5.2 Pretrained LM: We use RoBERTa-large (Liu et al., 2019) for all our experiments except for the fact retrieval task where we use the bert-large-cased model (Devlin et al., 2019) as this model has been shown to work b"
2021.findings-acl.395,D19-1250,0,0.025702,"Missing"
2021.findings-acl.395,2020.emnlp-main.346,0,0.0297761,"performance gains, often requiring large amounts of data. Brown et al. (2020) show that large pretrained language models (GPT3) are also efficient few-shot learners. Fewshot learning is achieved using task descriptions and labeled examples as prompts. Remarkably, with this priming-based approach and without needing any parameter updates, GPT3 often performs comparable to traditional finetuning-based supervised systems which use much larger datasets. One could argue that the task performance achieved in the priming-based approach measures what the pretrained language model has already learned. Shin et al. (2020), operating in the same setting, use automatically generated prompts to measure task specific knowledge in a pretrained language model. In this work, we further explore priming-based few-shot learning, while focusing on using examples as prompts. The training objective for a language model is typically the prediction of a token given a context. There is no clear incentive to treat a sequence of sentences in the context as equal and conveying examples of a concept. As a result, one could expect certain order of examples when used as a prompt to be more favorable at providing task 1 See the Intr"
2021.findings-acl.395,W18-5446,0,0.116107,"is a desirable property for Natural Language Processing (NLP) systems as well. It is critical in scenarios where collecting large Partha Talukdar Indian Institute of Science, Bangalore ppt@iisc.ac.in amounts of data is expensive. It is also important to enable a personalized Artificial Intelligence (AI) experience, where a single user is expected to use an AI agent to perform a task demonstrated through a handful of examples.1 Pretrained language models (Devlin et al., 2019; Liu et al., 2019; Raffel et al., 2020) have recently been shown to be exceedingly good at several benchmark NLP tasks (Wang et al., 2018, 2019). Traditionally the parameters of these language models have been finetuned on task specific datasets to achieve the aforementioned performance gains, often requiring large amounts of data. Brown et al. (2020) show that large pretrained language models (GPT3) are also efficient few-shot learners. Fewshot learning is achieved using task descriptions and labeled examples as prompts. Remarkably, with this priming-based approach and without needing any parameter updates, GPT3 often performs comparable to traditional finetuning-based supervised systems which use much larger datasets. One cou"
C12-1118,N09-1003,0,0.0202617,"oesio, 2004) test-set containing 402 nouns in a range of 21 concrete and abstract classes from WordNet (Miller et al., 1990). Both these tests were performed with the Cluto clustering package (Karypis, 2003) and cosine distances, and success was measured as percentage purity over clusters based on their plurality class. Two sets of similarity judgements were used: the Rubenstein 1938 Figure 2: Behavioral evaluation of SVD models with range of dimensionality (see Section 3.1.1 for more details). and Goodenough (1965) set of 65 concrete word pairs, and the strict-similarity subset of 203 pairs (Agirre et al., 2009) selected from the WordSim353 test-set (Finkelstein et al., 2002). Performance was evaluated with the Spearman correlation between the aggregate human judgements and pairwise cosine distances between word vectors in the model in question. Finally the TOEFL benchmark (Landauer and Dumais, 1997) consists of aggregate records from an examination task for learners of English, who have to identify a synonym among a set of distractors. Performance was measured as the percentage correct over 80 questions, if the cosine-distance of the target-synonym pair was smaller than the distance between the targ"
C12-1118,W04-3221,0,0.056996,"nts in this section will be available at http://www.cs.cmu.edu/~bmurphy/NNSE/. 3.1 Evaluating Model Performance 3.1.1 Behavioral Experiments The cognitive plausibility of computational models of word meaning has typically been tested using behavioural benchmarks, such as emulating elicited judgements of pairwise similarity or of category membership (Lund and Burgess, 1996; Rapp, 2003; Sahlgren, 2006). Here we used five such tests. The two categorization tests were the Battig (Battig and Montague, 1969) test-set consisting of 82 nouns, each assigned to one of 10 concrete classes; and the AAMP (Almuhareb and Poesio, 2004) test-set containing 402 nouns in a range of 21 concrete and abstract classes from WordNet (Miller et al., 1990). Both these tests were performed with the Cluto clustering package (Karypis, 2003) and cosine distances, and success was measured as percentage purity over clusters based on their plurality class. Two sets of similarity judgements were used: the Rubenstein 1938 Figure 2: Behavioral evaluation of SVD models with range of dimensionality (see Section 3.1.1 for more details). and Goodenough (1965) set of 65 concrete word pairs, and the strict-similarity subset of 203 pairs (Agirre et al"
C12-1118,J10-4006,0,0.0139912,"models of semantics, also termed vector-space models or word embeddings, derive word-representations in an unsupervised fashion from large corpora. They are primarily based on observed co-occurrence patterns, but are typically subsequently reduced in dimensionality using techniques such as Clustering, Latent Dirichlet Allocation (LDA) (Blei et al., 2003), or Singular Value Decomposition (SVD). They have proven effective as components of a wide range of NLP applications, and in the modelling of cognitive operations such as judgements of word similarity (Sahlgren, 2006; Turney and Pantel, 2010; Baroni and Lenci, 2010), and the brain activity elicited by particular concepts (Mitchell et al., 2008). However, with few exceptions (e.g. Baroni et al., 2010), the representations they derive from corpora are lacking in cognitive plausibility. For instance, one of the SVD-based models described in this paper models similarity very successfully, revealing the set of words mango, plum, cranberry, blueberry, melon as the cosine-distance nearest neighbours of pear. However, the latent SVD dimension for which pear has its largest weighting is hard to interpret – its most strongly positively associated tokens are action"
C12-1118,D07-1097,0,0.0841593,"Missing"
C12-1118,P12-1092,0,0.0502649,"by NNSE are considerably more interpretable compared to those estimated by SVD. We find that interpretability for both of them peak at k = 300. For qualitative comparison, top 5 words from 5 randomly selected dimensions each of SVD300 and NNSE1000 are presented in Table 2. From this, we get further anecdotal evidence about the higher interpretability of NNSE compared to SVD. 4 Related Work Corpus-derived models of semantics have been extensively studied in the NLP and machine learning communities (Collobert and Weston, 2008; Ratinov et al., 2010; Turney and Pantel, 2010; Socher et al., 2011; Huang et al., 2012). Additionally, dimensionality reduction techniques such as SVD, and topic distributions learned by probabilistic topic models such LDA (Blei et al., 2003) can also be used to induce word embeddings. Although the embeddings learned by these methods have many overlapping properties, to the best of our knowledge, none of these previous proposals satisfy the three desirable properties: effective in practice, sparse, and interpretable. We find that Non-Negative Sparse Encoding (NNSE), a variation on a matrix factorization technique previously studied in the machine learning community, can result i"
C12-1118,P98-2127,0,0.0415079,"Missing"
C12-1118,S12-1019,1,0.718734,"Missing"
C12-1118,J07-2002,0,0.0207571,"Missing"
C12-1118,2003.mtsummit-papers.42,0,0.0312786,"arious extrinsic evaluation tasks (Section 3.1)? How sparse are the SVD and NNSE representations (Section 3.2)? And how interpretable are these representations (Section 3.3)? All the NNSE representations used in the experiments in this section will be available at http://www.cs.cmu.edu/~bmurphy/NNSE/. 3.1 Evaluating Model Performance 3.1.1 Behavioral Experiments The cognitive plausibility of computational models of word meaning has typically been tested using behavioural benchmarks, such as emulating elicited judgements of pairwise similarity or of category membership (Lund and Burgess, 1996; Rapp, 2003; Sahlgren, 2006). Here we used five such tests. The two categorization tests were the Battig (Battig and Montague, 1969) test-set consisting of 82 nouns, each assigned to one of 10 concrete classes; and the AAMP (Almuhareb and Poesio, 2004) test-set containing 402 nouns in a range of 21 concrete and abstract classes from WordNet (Miller et al., 1990). Both these tests were performed with the Cluto clustering package (Karypis, 2003) and cosine distances, and success was measured as percentage purity over clusters based on their plurality class. Two sets of similarity judgements were used: the"
C12-1118,P10-1040,0,0.39138,"given by the i th row of matrix X . We note that overcomplete decomposition, i.e., k &gt; n, is possible in case of NNSE. For all experiments in this paper, we set λ = 0.05 and implement NNSE using the SPAMS package1 . 1 SPAMS Package: http://spams-devel.gforge.inria.fr/ 1936 3 Experiments In this section we try to answer two main questions: what broad categories of word embedding methods are most effective in modelling cognition; and which of the well-performing models are more cognitively plausible. Several of the models evaluated were already available, and were ˇ uˇrek and Sojka adopted from Ratinov et al. (2010) (Collobert & Weston, HLBL), and from Reh˚ (2010) (LDA topic model based on the English Wikipedia). The SVD and sparse non-negative models on which we concentrate, were constructed from scratch, based on both LSA-style word-document co-occurrence counts (i.e., word-region features) and HAL-style word-dependency co-occurrence counts (i.e., word-collocate features). Counts were computed from a large English web-corpus, Clueweb (Callan and Hoy, 2009), over a fixed 40,000 word vocabulary. These were the most frequent word-forms found in the American National Corpus (Nancy Ide and Keith Suderman, 2"
C12-1118,C98-2122,0,\N,Missing
C12-1118,ide-suderman-2004-american,0,\N,Missing
C12-2026,W06-1615,0,0.300726,"Missing"
C12-2026,D09-1052,1,0.864394,"Missing"
C12-2026,P07-1033,0,0.201476,"Missing"
C12-2026,C12-2026,1,0.0512688,"Missing"
C12-2026,W02-1011,0,0.0184953,"Missing"
D07-1112,W07-2416,0,0.0334886,"Missing"
D07-1112,W04-3111,0,0.0744338,"Missing"
D07-1112,J93-2004,0,0.0316121,"Missing"
D07-1112,W06-2932,1,0.426727,"Missing"
D07-1112,W97-0309,1,0.621417,"Missing"
D07-1112,D07-1096,0,\N,Missing
D08-1061,C92-2082,0,0.419541,"Missing"
D08-1061,A00-1040,0,0.0119261,"Angola, or active volcanoes like Etna and Kilauea. Note that zoonotic diseases, surgical procedures, African countries and active volcanoes serve as useful class labels that capture the semantics of the associated sets of class instances. Such interest in a wide variety of specific domains highlights the utility of constructing large collections of fine-grained classes. Comprehensive and accurate class-instance information is useful not only in search but also in a variety of other text processing tasks including co-reference resolution (McCarthy and Lehnert, 1995), named entity recognition (Stevenson and Gaizauskas, 2000) and seed-based information extraction (Riloff and Jones, 1999). 1.2 Contributions We study the acquisition of open-domain, labeled classes and their instances from both structured and unstructured textual data sources by combining and ranking individual extractions in a principled way with the Adsorption label-propagation algorithm (Baluja et al., 2008), reviewed in Section 3 below. A collection of labeled classes acquired from text (Van Durme and Pas¸ca, 2008) is extended in two ways: 1. Class label coverage is increased by identifying additional class labels (such as public agencies and gov"
D08-1061,C02-1144,0,\N,Missing
D13-1080,C92-2082,0,0.0751417,"na¨ıve addition of lexicalized edges may result in significant data sparsity, which can be overcome by mapping lexicalized edge labels to some latent embedding (e.g., (Alex Rodriguez, LatentFeat#5, NY Yankees) and running PRA over this augmented graph. Using latent embeddings, PRA could then use the following edge sequence as a feature in its prediction models: hLatentFeat#5, teamPlaysIni. We find this strategy to be very effective as described in Section 4. 2 Related Work There is a long history of methods using suface-level lexical patterns for extracting relational facts from text corpora (Hearst, 1992; Brin, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2004). Syntactic information in the form of dependency paths have been explored in (Snow et al., 2006; Suchanek et al., 2006). A method of latent embedding of relation instances for sentence-level relation extraction was shown in (Wang et al., 2011). However, none of this prior work makes explicit use of the background KBs as we explore in this paper. 834 Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) has been used previously to perform inference over graph-structured KBs (Lao et al., 2011), and to lear"
D13-1080,P03-1009,0,0.0130843,"els are either KB labels or at surface-level, in this paper we explore using latent edge labels in addition to surface-level labels in the graph over which PRA is applied. In particular, we focus on the problem of performing inference over a large KB and learn latent edge labels by mining dependency syntax statistics from a large text corpus. Though we use Principal Components Analysis (PCA) for dimensionality reduction for the experiments in this paper, this is by no means the only choice. Various other dimensionality reduction techniques, and in particular, other verb clustering techniques (Korhonen et al., 2003), may also be used. OpenIE systems such as Reverb (Etzioni et al., 2011) also extract verb-anchored dependency triples from large text corpus. In contrast to such approaches, we focus on how latent embedding of verbs in such triples can be combined with explicit background knowledge to improve coverage of existing KBs. This has the added capability of inferring facts which are not explicitly mentioned in text. The recently proposed Universal Schema (Riedel et al., 2013) also demonstrates the benefit of using latent features for increasing coverage of KBs. Key differences between that approach"
D13-1080,D11-1049,1,0.790658,"need to increase their coverage of facts to make them useful in practical applications. A strategy to increase coverage might be to perform inference directly over the KB represented as a graph. For example, if the KB contained the following facts, (Tiger Woods, participatesIn, PGA Tour)) and (Golf, sportOfTournament, PGA Tour), then by putting these two facts together, we could potentially infer that (Tiger Woods, playsSport, Golf ). The recently proposed Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) performs such inference by automatically learning semantic inference rules over the KB (Lao et al., 2011). PRA uses features based off of sequences of edge types, e.g., hplaysSport, sportOfTournamenti, to predict missing facts in the KB. PRA was extended by (Lao et al., 2012) to perform inference over a KB augmented with dependency parsed sentences. While this opens up the possibility of learning syntactic-semantic inference rules, the set of syntactic edge labels used are just the unlexicalized dependency role labels (e.g., nobj, dobj, etc., without the corresponding words), thereby limiting overall expressitivity of the learned inference rules. To overcome this limitation, in this paper we augm"
D13-1080,D12-1093,0,0.818427,"represented as a graph. For example, if the KB contained the following facts, (Tiger Woods, participatesIn, PGA Tour)) and (Golf, sportOfTournament, PGA Tour), then by putting these two facts together, we could potentially infer that (Tiger Woods, playsSport, Golf ). The recently proposed Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) performs such inference by automatically learning semantic inference rules over the KB (Lao et al., 2011). PRA uses features based off of sequences of edge types, e.g., hplaysSport, sportOfTournamenti, to predict missing facts in the KB. PRA was extended by (Lao et al., 2012) to perform inference over a KB augmented with dependency parsed sentences. While this opens up the possibility of learning syntactic-semantic inference rules, the set of syntactic edge labels used are just the unlexicalized dependency role labels (e.g., nobj, dobj, etc., without the corresponding words), thereby limiting overall expressitivity of the learned inference rules. To overcome this limitation, in this paper we augment the KB graph by adding edges with more expressive lexicalized syntactic labels (where the labels are words instead of dependen833 Proceedings of the 2013 Conference on"
D13-1080,P02-1006,0,0.0359393,"ificant data sparsity, which can be overcome by mapping lexicalized edge labels to some latent embedding (e.g., (Alex Rodriguez, LatentFeat#5, NY Yankees) and running PRA over this augmented graph. Using latent embeddings, PRA could then use the following edge sequence as a feature in its prediction models: hLatentFeat#5, teamPlaysIni. We find this strategy to be very effective as described in Section 4. 2 Related Work There is a long history of methods using suface-level lexical patterns for extracting relational facts from text corpora (Hearst, 1992; Brin, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2004). Syntactic information in the form of dependency paths have been explored in (Snow et al., 2006; Suchanek et al., 2006). A method of latent embedding of relation instances for sentence-level relation extraction was shown in (Wang et al., 2011). However, none of this prior work makes explicit use of the background KBs as we explore in this paper. 834 Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) has been used previously to perform inference over graph-structured KBs (Lao et al., 2011), and to learn formation of online communities (Settles and Dow, 2013). In (Lao et"
D13-1080,N13-1008,0,0.0279984,"ns the only choice. Various other dimensionality reduction techniques, and in particular, other verb clustering techniques (Korhonen et al., 2003), may also be used. OpenIE systems such as Reverb (Etzioni et al., 2011) also extract verb-anchored dependency triples from large text corpus. In contrast to such approaches, we focus on how latent embedding of verbs in such triples can be combined with explicit background knowledge to improve coverage of existing KBs. This has the added capability of inferring facts which are not explicitly mentioned in text. The recently proposed Universal Schema (Riedel et al., 2013) also demonstrates the benefit of using latent features for increasing coverage of KBs. Key differences between that approach and ours include our use of syntactic information as opposed to surface-level patterns in theirs, and also the ability of the proposed PRA-based method to generate useful inference rules which is beyond the capability of the matrix factorization approach in (Riedel et al., 2013). 3 3.1 Method Path Ranking Algorithm (PRA) In this section, we present a brief overview of the Path Ranking Algorithm (PRA) (Lao and Cohen, 2010), building on the notations in (Lao et al., 2012)"
D13-1080,P06-1101,0,0.0241815,"LatentFeat#5, NY Yankees) and running PRA over this augmented graph. Using latent embeddings, PRA could then use the following edge sequence as a feature in its prediction models: hLatentFeat#5, teamPlaysIni. We find this strategy to be very effective as described in Section 4. 2 Related Work There is a long history of methods using suface-level lexical patterns for extracting relational facts from text corpora (Hearst, 1992; Brin, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2004). Syntactic information in the form of dependency paths have been explored in (Snow et al., 2006; Suchanek et al., 2006). A method of latent embedding of relation instances for sentence-level relation extraction was shown in (Wang et al., 2011). However, none of this prior work makes explicit use of the background KBs as we explore in this paper. 834 Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) has been used previously to perform inference over graph-structured KBs (Lao et al., 2011), and to learn formation of online communities (Settles and Dow, 2013). In (Lao et al., 2012), PRA is extended to perform inference over a KB using syntactic information from parsed text. In contrast to"
D13-1080,D11-1132,0,0.0318212,"eature in its prediction models: hLatentFeat#5, teamPlaysIni. We find this strategy to be very effective as described in Section 4. 2 Related Work There is a long history of methods using suface-level lexical patterns for extracting relational facts from text corpora (Hearst, 1992; Brin, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2004). Syntactic information in the form of dependency paths have been explored in (Snow et al., 2006; Suchanek et al., 2006). A method of latent embedding of relation instances for sentence-level relation extraction was shown in (Wang et al., 2011). However, none of this prior work makes explicit use of the background KBs as we explore in this paper. 834 Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) has been used previously to perform inference over graph-structured KBs (Lao et al., 2011), and to learn formation of online communities (Settles and Dow, 2013). In (Lao et al., 2012), PRA is extended to perform inference over a KB using syntactic information from parsed text. In contrast to these previous PRA-based approaches where all edge labels are either KB labels or at surface-level, in this paper we explore using latent edge labe"
D14-1044,D13-1080,1,0.723034,"a logistic regression classifier that predicts new instances of the given relation. Each path can be viewed as a horn clause using knowledge base relations as predicates, and so PRA can be thought of as a kind of discriminatively trained logical inference. One major deficiency of random walk inference is the connectivity of the knowledge base graph— if there is no path connecting two nodes in the graph, PRA cannot predict any relation instance between them. Thus prior work has introduced the use of a text corpus to increase the connectivity of the graph used as input to PRA (Lao et al., 2012; Gardner et al., 2013). This approach is not without its own problems, however. Whereas knowledge base relations are semantically coherent and different relations have distinct meanings, this is not Much work in recent years has gone into the construction of large knowledge bases (KBs), such as Freebase, DBPedia, NELL, and YAGO. While these KBs are very large, they are still very incomplete, necessitating the use of inference to fill in gaps. Prior work has shown how to make use of a large text corpus to augment random walk inference over KBs. We present two improvements to the use of such large corpora to augment"
D14-1044,D12-1093,0,0.080969,"ed as features in a logistic regression classifier that predicts new instances of the given relation. Each path can be viewed as a horn clause using knowledge base relations as predicates, and so PRA can be thought of as a kind of discriminatively trained logical inference. One major deficiency of random walk inference is the connectivity of the knowledge base graph— if there is no path connecting two nodes in the graph, PRA cannot predict any relation instance between them. Thus prior work has introduced the use of a text corpus to increase the connectivity of the graph used as input to PRA (Lao et al., 2012; Gardner et al., 2013). This approach is not without its own problems, however. Whereas knowledge base relations are semantically coherent and different relations have distinct meanings, this is not Much work in recent years has gone into the construction of large knowledge bases (KBs), such as Freebase, DBPedia, NELL, and YAGO. While these KBs are very large, they are still very incomplete, necessitating the use of inference to fill in gaps. Prior work has shown how to make use of a large text corpus to augment random walk inference over KBs. We present two improvements to the use of such la"
D14-1044,D12-1048,0,0.00909139,"rity between the two edge types. This lets us combine notions of distributional similarity with symbolic logical inference, with the result of decreasing the sparsity of the feature space considered by PRA. We show with experiments using both the NELL and Freebase knowledge bases that this method gives significantly better performance than prior approaches to incorporating text data into random walk inference. 2 To create a graph from a corpus, we first preprocess the corpus to obtain a collection of surface relations, such as those extracted by open information extraction systems like OLLIE (Mausam et al., 2012). These surface relations consist of a pair of noun phrases in the corpus, and the verb-like connection between them (either an actual verb, as done by Talukdar et al. (2012), a dependency path, as done by Riedel et al. (2013), or OpenIE relations (Mausam et al., 2012)). The verb-like connections are naturally represented as edges in the graph, as they have a similar semantics to the knowledge base relations that are already represented as edges. We thus create a graph from these triples exactly as we do from a KB, with nodes corresponding to noun phrase types and edges corresponding to surfac"
D14-1044,mendes-etal-2012-dbpedia,0,0.0233272,"Missing"
D14-1044,W14-1609,0,0.00593772,"rge graphs that we use in this work. How best to incorporate the work presented in this paper with ProPPR is an open, and very interesting, question. Examples of other systems aimed at reasoning over common-sense knowledge are the CYC project (Lenat, 1995) and ConceptNet (Liu and Singh, 2004). These common-sense resources could easily be incorporated into the graphs we use for performing random walk inference. Lines of research that seek to incorporate distributional semantics into traditional natural language processing tasks, such as parsing (Socher et al., 2013a), named entity recognition (Passos et al., 2014), and sentiment analysis (Socher et al., 2013b), are also related to what we present in this paper. While our task is quite different from these prior works, we also aim to combine distributional semantics with more traditional methods (in our case, symbolic logical inference), and we take inspiration from these methods. 6 Entities Relation instances Total relation types Relation types tested Avg. instances/relation SVO triples used NELL 1.2M 3.4M 520 10 810 404k Freebase 20M 67M 4215 24 200 28M Table 1: Statistics of the data used in our experiments. steps of PRA, and spikiness and restart pa"
D14-1044,N13-1008,0,0.145809,"ments using both the NELL and Freebase knowledge bases that this method gives significantly better performance than prior approaches to incorporating text data into random walk inference. 2 To create a graph from a corpus, we first preprocess the corpus to obtain a collection of surface relations, such as those extracted by open information extraction systems like OLLIE (Mausam et al., 2012). These surface relations consist of a pair of noun phrases in the corpus, and the verb-like connection between them (either an actual verb, as done by Talukdar et al. (2012), a dependency path, as done by Riedel et al. (2013), or OpenIE relations (Mausam et al., 2012)). The verb-like connections are naturally represented as edges in the graph, as they have a similar semantics to the knowledge base relations that are already represented as edges. We thus create a graph from these triples exactly as we do from a KB, with nodes corresponding to noun phrase types and edges corresponding to surface relation triples. So far these two subgraphs we have created are entirely disconnected, with the KB graph containing nodes representing entities, and the surface relation graph containing nodes representing noun phrases, wit"
D14-1044,P13-1045,0,0.00644971,"yet appear to be scalable enough to handle the large graphs that we use in this work. How best to incorporate the work presented in this paper with ProPPR is an open, and very interesting, question. Examples of other systems aimed at reasoning over common-sense knowledge are the CYC project (Lenat, 1995) and ConceptNet (Liu and Singh, 2004). These common-sense resources could easily be incorporated into the graphs we use for performing random walk inference. Lines of research that seek to incorporate distributional semantics into traditional natural language processing tasks, such as parsing (Socher et al., 2013a), named entity recognition (Passos et al., 2014), and sentiment analysis (Socher et al., 2013b), are also related to what we present in this paper. While our task is quite different from these prior works, we also aim to combine distributional semantics with more traditional methods (in our case, symbolic logical inference), and we take inspiration from these methods. 6 Entities Relation instances Total relation types Relation types tested Avg. instances/relation SVO triples used NELL 1.2M 3.4M 520 10 810 404k Freebase 20M 67M 4215 24 200 28M Table 1: Statistics of the data used in our exper"
D14-1044,D13-1170,0,0.0021682,"yet appear to be scalable enough to handle the large graphs that we use in this work. How best to incorporate the work presented in this paper with ProPPR is an open, and very interesting, question. Examples of other systems aimed at reasoning over common-sense knowledge are the CYC project (Lenat, 1995) and ConceptNet (Liu and Singh, 2004). These common-sense resources could easily be incorporated into the graphs we use for performing random walk inference. Lines of research that seek to incorporate distributional semantics into traditional natural language processing tasks, such as parsing (Socher et al., 2013a), named entity recognition (Passos et al., 2014), and sentiment analysis (Socher et al., 2013b), are also related to what we present in this paper. While our task is quite different from these prior works, we also aim to combine distributional semantics with more traditional methods (in our case, symbolic logical inference), and we take inspiration from these methods. 6 Entities Relation instances Total relation types Relation types tested Avg. instances/relation SVO triples used NELL 1.2M 3.4M 520 10 810 404k Freebase 20M 67M 4215 24 200 28M Table 1: Statistics of the data used in our exper"
D15-1061,N07-4013,0,0.0855108,"Missing"
D15-1061,D11-1142,0,0.0667763,"Missing"
D15-1061,D13-1080,1,0.892335,"Missing"
D15-1061,D14-1044,1,0.884319,"Missing"
D15-1061,P14-5010,0,0.00335288,"se of ambiguous entities, a few keywords are also added to the query. For the experiments in this paper, the category is used as the keyword. For example, for the entity Albert Einstein from the scientist category, the query will be ”Albert Einstein scientist”. Top 20 documents returned by the search engine are downloaded and processed further. Text is extracted from the raw downloaded documents using regex patters, HTML tag matching, and by using the Boilerpipe tool2 . 3.2 Triple Extraction Text of each document obtained in the previous step is processed through the Stanford CoreNLP toolkit (Manning et al., 2014) for tokenization, coreference resolution, and dependency parsing. Tokenized and coreferenceresolved sentences are then passes through OpenIEv4 system 3 to extract (noun phrase, predicate, noun phrase) triples. Multiple and overlapping triples from the sentence was permitted. Length filter is applied on the noun phrase and the predicate of the triple extracted. This eliminates triples whose predicate is more than 6 tokens and noun phrase more than 7 tokens. 3.3 After this triple clustering step, the best representative triple from each cluster is selected based on a few rules. We consider the"
D15-1061,D12-1048,0,0.0629808,"Missing"
D15-1127,C08-1007,0,0.0669954,"Missing"
D15-1127,P14-5004,0,0.0226708,"Missing"
D15-1127,E14-1049,0,0.171658,"Missing"
D15-1127,N13-1092,0,0.0184137,"Missing"
D15-1127,P15-1119,0,0.0481052,"Missing"
D15-1241,D13-1080,1,0.734267,"2010), (Lao et al., 2011) performs such an inference by learning inference rules over the knowledge graph. If the knowledge graph is sparse, i.e., if there are a very few or no paths between source and target entities, then PRA is unable to predict the existence of a relation. To address this shortcoming, (Lao et al., 2012) augmented the knowledge graph with paths obtained from an external corpus. The added paths consisted of unlexicalized dependency labels obtained from a dependency parsed external corpus. To improve the expressivity of the added paths, instead of the unlexicalized labels, (Gardner et al., 2013) augmented the KB graph with verbs (surface relations) from a corpus containing over 600 million Subject-Verb-Object (SVO) triples. These verbs act as edges that connect previously unconnected entities thereby increasing the connectivity of the KB graph which can potentially improve PRA performance. However, na¨ıvely adding these edges increases the feature sparsity which degrades the discriminative ability of the logistic regression classifier 2038 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2038–2043, c Lisbon, Portugal, 17-21 September 2015."
D15-1241,D14-1044,1,0.893964,"r, na¨ıvely adding these edges increases the feature sparsity which degrades the discriminative ability of the logistic regression classifier 2038 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2038–2043, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. used in PRA. This can be addressed by adding latent relations obtained by clustering the surface relations, instead of directly adding the surface relations. This reduces feature sparsity and has been shown to improve PRA inference (Gardner et al., 2013) , (Gardner et al., 2014). In this article we propose a scheme for augmenting the KB using paths obtained by mining noun phrases that connect two SVO triples from an external corpus. We term these noun phrases as bridging entities since they bridge two KB relations to form a path. This is different from the scheme in (Gardner et al., 2013) and (Gardner et al., 2014), which adds edges between KB nodes by mining surface relations from an external corpus. We search for such bridging entities in the corpus by performing a limited depth DFS (depth first search) on the corpus graph in an on-demand fashion. We term this proc"
D15-1241,C92-2082,0,0.284544,"oes not increase feature sparsity which is responsible for performance degradation. Our experiments suggest that ODA provides better performance than (Gardner et al., 2013) and nearly the same prediction performance as provided by (Gardner et al., 2014), but in both cases with the added advantage of faster running time and greater flexibility due to its online and on-demand nature. The code along with the results can be obtained at https://github.com/malllabiisc/pra-oda. 2 Related Work Using surface level relations and noun phrases for extracting meaningful relational facts is not a new idea (Hearst, 1992),(Brin, 1999), (Etzioni et al., 2004). However, none of them make use of Knowledge Bases for improving information extraction. The Path Ranking Algorithm (PRA) first proposed in (Lao and Cohen, 2010) was used for performing inference over a KB in (Lao et al., 2011). It was extended by (Lao et al., 2012), to improve the inference by augmenting the KB with syntactic information obtained from a dependency parsed corpus. Augmenting the KB for improving PRA inference using surface relations mined from an external corpus and using latent edge labels obtained by performing PCA on the surface relation"
D15-1241,D11-1049,0,0.232596,"red in the form of a directed graph where links represent relations and nodes represent the entities. Although such KBs contain millions of entities, they are still very sparse, i.e., they are missing a large number of relations between existing entities (West et al., 2014). Performing inference over the knowledge graph for predicting relations between two entities is one way of densifying the KB graph. For example, from (Germany, playsinTournament, FIFA) and (FIFA, tournamentofSport, Soccer), we can infer (Germany, playsSport, Soccer). The Path Ranking Algorithm (PRA) (Lao and Cohen, 2010), (Lao et al., 2011) performs such an inference by learning inference rules over the knowledge graph. If the knowledge graph is sparse, i.e., if there are a very few or no paths between source and target entities, then PRA is unable to predict the existence of a relation. To address this shortcoming, (Lao et al., 2012) augmented the knowledge graph with paths obtained from an external corpus. The added paths consisted of unlexicalized dependency labels obtained from a dependency parsed external corpus. To improve the expressivity of the added paths, instead of the unlexicalized labels, (Gardner et al., 2013) augm"
D15-1241,D12-1093,0,0.0737223,"r the knowledge graph for predicting relations between two entities is one way of densifying the KB graph. For example, from (Germany, playsinTournament, FIFA) and (FIFA, tournamentofSport, Soccer), we can infer (Germany, playsSport, Soccer). The Path Ranking Algorithm (PRA) (Lao and Cohen, 2010), (Lao et al., 2011) performs such an inference by learning inference rules over the knowledge graph. If the knowledge graph is sparse, i.e., if there are a very few or no paths between source and target entities, then PRA is unable to predict the existence of a relation. To address this shortcoming, (Lao et al., 2012) augmented the knowledge graph with paths obtained from an external corpus. The added paths consisted of unlexicalized dependency labels obtained from a dependency parsed external corpus. To improve the expressivity of the added paths, instead of the unlexicalized labels, (Gardner et al., 2013) augmented the KB graph with verbs (surface relations) from a corpus containing over 600 million Subject-Verb-Object (SVO) triples. These verbs act as edges that connect previously unconnected entities thereby increasing the connectivity of the KB graph which can potentially improve PRA performance. Howe"
D16-1040,D14-1165,0,0.520977,"nsive human input. We refer to this problem as Relation Schema Induction (RSI). In contrast to ontology-guided KG construction techniques mentioned above, Open Information Extraction (OpenIE) techniques (Etzioni et al., 2011) aim to extract surface-level triples from unstructured text. Such OpenIE triples may provide a suitable starting point for the RSI problem. In fact, KB-LDA, 414 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 414–423, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics Target task Typed RESCAL (Chang et al., 2014a) Universal Schema (Singh et al., 2015) KB-LDA (MovshovitzAttias and Cohen, 2015) SICTF (this paper) Embedding Interpretable latent factors? No Can induce relation schema? No Can use NP side info? Yes Can use relation side info? No Link Prediction No No No No Ontology Induction Schema Induction Yes Yes Yes No Yes Yes Yes Yes Table 1: Comparison among SICTF (this paper) and other related methods. KB-LDA is the most related prior method which is extensively compared against SICTF in Section 4 a topic modeling-based method for inducing an ontology from SVO (Subject-Verb-Object) triples was recen"
D16-1040,P15-1047,0,0.102629,"methods are summarized in Table 12 . A method for inducing (binary) relations and the categories they connect was proposed by (Mohamed et al., 2011). However, in that work, categories and their instances were known a-priori. In contrast, in case of SICTF, both categories and relations are to be induced. A method for event schema induction, the task of learning high-level representations of complex events and their entity roles from unlabeled text, was proposed in (Chambers, 2013). This gives the schemas of slots per event, but our goal is to find schemas of relations. (Chen et al., 2013) and (Chen et al., 2015) deal with the problem of finding semantic slots for unsupervised spoken language understanding, but we are interested in finding schemas of relations relevant for a given domain. Methods for link prediction in the Universal Schema setting using matrix and a combination of matrix and tensor factorization are proposed in (Riedel et al., 2013) and (Singh et al., 2015), respectively. Instead of link prediction where relation schemas are assumed to be given, SICTF focuses on discovering such relation schemas. Moreover, in contrast to such 1 https://github.com/malllabiisc/sictf Please note that not"
D16-1040,W13-3203,0,0.0206677,"A PARAFAC (Harshman, 1970) based method for jointly factorizing a matrix and tensor for data fusion was proposed in (Acar et al., 2013). In such cases, the matrix is used to provide auxiliary information (Narita et al., 2012; Erdos and Miettinen, 2013). Similar PARAFAC-based ideas are explored in Rubik (Wang et al., 2015) to factorize structured electronic health records. In contrast to such structured data sources, SICTF aims at inducing relation schemas from unstructured text data. Propstore, a tensor-based model for distributional semantics, a problem different from RSI, was presented in (Goyal et al., 2013). Even though coupled factorization of tensor and matrices constructed out of unstructured text corpus provide a natural and 416 plausible approach for the RSI problem, they have not yet been explored – we fill this gap in this paper. Ontology Induction: Relation Schema Induction can be considered a sub problem of Ontology Induction (Velardi et al., 2013). Instead of building a full-fledged hierarchy over categories and relations as in ontology induction, we are particularly interested in finding relations and their schemas from unstructured text corpus. We consider KB-LDA3 (Movshovitz-Attias"
D16-1040,C92-2082,0,0.448056,"nables) Table 3: Examples of relation similarity side information in the form of automatically identified similar relation pairs. Please see Section 3.2 for details. score 0.8 involving relation undergo and induced categories A2 and A4 . In Section 3.2, we present details of the side information used by SICTF, and then in Section 3.3 present details of the optimization problem solved by SICTF. 3.2 Side Information • Noun Phrase Side Information: Through this type of side information, we would like to capture type information of as many noun phrases (NPs) as possible. We apply Hearst patterns (Hearst, 1992), e.g., ”<Hypernym> such as <NP>”, over the corpus to extract such (NP, Hypernym) pairs. Please note that neither hypernyms nor NPs are pre-specified, and they are all extracted from the data by the patterns. Examples of a few such pairs extracted from two different datasets are shown in Table 2. These extracted tuples are stored in a matrix Wn×h whose rows correspond to NPs and columns correspond to extracted hypernyms. We define, Wij =  1, 0, if NPi belongs to Hypernymj otherwise . Please note that we don’t expect W to be a fully specified matrix, i.e., we don’t assume that we know all poss"
D16-1040,P14-5010,0,0.00298612,"about 10-20 iterations. 4 Experiments In this section, we evaluate performance of different methods on the Relation Schema Induction (RSI) task. Specifically, we address the following questions. • Which method is most effective on the RSI task? (Section 4.3.1) • How important are the additional side information for RSI? (Section 4.3.2) • What is the importance of non-negativity in RSI with tensor factorization? (Section 4.3.3) 4.1 Experimental Setup Datasets: We used two datasets for the experiments in this paper, they are summarized in Table 4. For MEDLINE dataset, we used Stanford CoreNLP (Manning et al., 2014) for coreference resolution and Open IE v4.09 for triple extraction. Triples with Noun Phrases that have Hypernym information were retained. We obtained the StackOverflow triples directly from the authors of (Movshovitz-Attias and Cohen, 2015), which were also prepared using a very similar process. In both datasets, we use corpus frequency of triples for constructing the tensor. Side Information: Seven Hearst patterns such as ”<hypernym> such as <NP>”, ”<NP> or other <hypernym>” etc., given in (Hearst, 1992) were used to extract NP side information from the MEDLINE documents. NP side informati"
D16-1040,D11-1134,0,0.178099,"al side information for relation schema induction. • We compare SICTF against state-of-the-art baseline on various real-world datasets from diverse domains. We observe that SICTF is not only significantly more accurate than such 415 baselines, but also much faster. For example, SICTF achieves 14x speedup over KB-LDA (Movshovitz-Attias and Cohen, 2015). • We have made the data and code available 1 . 2 Related Work Schema Induction: Properties of SICTF and other related methods are summarized in Table 12 . A method for inducing (binary) relations and the categories they connect was proposed by (Mohamed et al., 2011). However, in that work, categories and their instances were known a-priori. In contrast, in case of SICTF, both categories and relations are to be induced. A method for event schema induction, the task of learning high-level representations of complex events and their entity roles from unlabeled text, was proposed in (Chambers, 2013). This gives the schemas of slots per event, but our goal is to find schemas of relations. (Chen et al., 2013) and (Chen et al., 2015) deal with the problem of finding semantic slots for unsupervised spoken language understanding, but we are interested in finding"
D16-1040,P15-1140,0,0.118977,"ma (Singh et al., 2015) KB-LDA (MovshovitzAttias and Cohen, 2015) SICTF (this paper) Embedding Interpretable latent factors? No Can induce relation schema? No Can use NP side info? Yes Can use relation side info? No Link Prediction No No No No Ontology Induction Schema Induction Yes Yes Yes No Yes Yes Yes Yes Table 1: Comparison among SICTF (this paper) and other related methods. KB-LDA is the most related prior method which is extensively compared against SICTF in Section 4 a topic modeling-based method for inducing an ontology from SVO (Subject-Verb-Object) triples was recently proposed in (Movshovitz-Attias and Cohen, 2015). We note that ontology induction (Velardi et al., 2013) is a more general problem than RSI, as we are primarily interested in identifying categories and relations from a domain corpus, and not necessarily any hierarchy over them. Nonetheless, KB-LDA maybe used for the RSI problem and we use it as a representative of the state-of-the-art of this area. Instead of a topic modeling approach, we take a tensor factorization-based approach for RSI in this paper. Tensors are a higher order generalization of matrices and they provide a natural way to represent OpenIE triples. Applying tensor factoriza"
D16-1040,C12-1118,1,0.816564,"ned by the fact that absence of non-negativity constraint results in an under constrained factorization problem where the model often overgenerates incorrect triples, and then compensates for this overgeneration by using negative latent factor weights. In contrast, imposition of non-negativity constraints restricts the model further forcing it to commit to specific semantics of the latent factors in A. This improved interpretability also results in better RSI accuracy as we have seen above. Similar benefits of non-negativity on interpretability have also been observed in matrix factorization (Murphy et al., 2012). 5 Conclusion Relation Schema Induction (RSI) is an important first step towards building a Knowledge Graph (KG) out of text corpus from a given domain. While human domain experts have traditionally prepared listing of relations and their schemas, this expert-mediated model poses significant challenges in terms of scalability and coverage. In order to overcome these challenges, in this paper, we present SICTF, a novel non-negative coupled tensor factorization method for relation schema induction. SICTF is flexible enough to incorporate various types of side information during factorization. T"
D16-1040,N13-1008,0,0.0812252,"the task of learning high-level representations of complex events and their entity roles from unlabeled text, was proposed in (Chambers, 2013). This gives the schemas of slots per event, but our goal is to find schemas of relations. (Chen et al., 2013) and (Chen et al., 2015) deal with the problem of finding semantic slots for unsupervised spoken language understanding, but we are interested in finding schemas of relations relevant for a given domain. Methods for link prediction in the Universal Schema setting using matrix and a combination of matrix and tensor factorization are proposed in (Riedel et al., 2013) and (Singh et al., 2015), respectively. Instead of link prediction where relation schemas are assumed to be given, SICTF focuses on discovering such relation schemas. Moreover, in contrast to such 1 https://github.com/malllabiisc/sictf Please note that not all methods mentioned in the table are directly comparable with SICTF, the table only illustrates the differences. KB-LDA is the only method which is directly comparable. 2 Figure 1: Relation Schema Induction (RSI) by SICTF, the proposed method. First, a tensor (X) is constructed to represent OpenIE triples extracted from a domain corpus. N"
D16-1040,W15-1519,0,0.228742,"Missing"
D16-1040,J13-3007,0,0.0723682,"TF (this paper) Embedding Interpretable latent factors? No Can induce relation schema? No Can use NP side info? Yes Can use relation side info? No Link Prediction No No No No Ontology Induction Schema Induction Yes Yes Yes No Yes Yes Yes Yes Table 1: Comparison among SICTF (this paper) and other related methods. KB-LDA is the most related prior method which is extensively compared against SICTF in Section 4 a topic modeling-based method for inducing an ontology from SVO (Subject-Verb-Object) triples was recently proposed in (Movshovitz-Attias and Cohen, 2015). We note that ontology induction (Velardi et al., 2013) is a more general problem than RSI, as we are primarily interested in identifying categories and relations from a domain corpus, and not necessarily any hierarchy over them. Nonetheless, KB-LDA maybe used for the RSI problem and we use it as a representative of the state-of-the-art of this area. Instead of a topic modeling approach, we take a tensor factorization-based approach for RSI in this paper. Tensors are a higher order generalization of matrices and they provide a natural way to represent OpenIE triples. Applying tensor factorization methods over OpenIE triples to identify relation sc"
D16-1040,D13-1185,0,\N,Missing
D17-1183,D11-1049,0,0.0426742,", θi )} t(h) ∈ {0, 1} l(h) ∈ {0, 1} Hi = Dom(Ci ) G = (H ∪ C, E) Q ⊆ H I G, Q ⊆ H Φ(Q) =P 1 |Q| h∈Q t(h) Description Set of all n Belief Evaluation Tasks (BETs) Cost of labeling h from crowd Coupling constraints C i with weights θi ∈ R+ True label of h Estimated label of h Hi ⊆ H which participate in C i Evaluation Coupling Graph, e ∈ E between Hj and C j denotes Hj ∈ Dom(Cj ) . BETs evaluated using crowd Inferable set for evidence Q: True accuracy of evaluated BETs Q ture of KGs. In this work, we derive coupling constraints C from the KG ontology and linkprediction algorithms, such as PRA (Lao et al., 2011) over NELL and AMIE (Gal´arraga et al., 2013) over Yago. These rules are jointly learned over entire KG with millions of facts and are assumed true. We use conjunction-form first-order-logic rules and refer to them as Horn clauses. Examples of a few such coupling constraints are shown below. C2 : (x, homeStadiumOf, y) → (y, isA, sportsTeam) C5 : (x, homeStadiumOf, y) ∧ (y, homeCity, z) → (x, stadiumLocatedInCity, z) Table 1: Summary of notations used (Section 2.2). Inference algorithm helps us work out evaluation labels of other BETs using constraints C. For a set of already evaluated  BETs Q"
D17-1281,W06-0901,0,0.0293718,"ng slowdown which is undesirable. We leverage recent advances in parallel RL training using asynchronous methods and propose RLIE-A3C. RLIEA3C trains multiple agents in parallel and is able to achieve upto 6x training speedup over RLIE-DQN, while suffering no loss in average accuracy. 1 Introduction Extracting information about an event (or entity) involves multiple decisions, as one first needs to identify documents relevant to the event, extract relevant information from those documents, and finally reconcile various values obtained for the same relation of the event from different sources (Ahn, 2006). Search based methods for Information Extraction have been increasingly investigated (West et al., 2014); (Hegde and Talukdar, 2015); (Zhang et al., 2016); (Bing et al., 2017). (Kanani and McCallum, 2012) combine search and Information Extraction (IE) using Reinforcement Learning (RL), with the goal of selecting good actions while staying within resource constraints, but don’t optimze for extraction accuracy. More recently, (Narasimhan et al., 2016) proposed a RL-based approach to model the IE process outlined above. We shall refer to this approach as RLIE-DQN in this paper. RLIE-DQN trains a"
D17-1281,D15-1061,1,0.845452,"ropose RLIE-A3C. RLIEA3C trains multiple agents in parallel and is able to achieve upto 6x training speedup over RLIE-DQN, while suffering no loss in average accuracy. 1 Introduction Extracting information about an event (or entity) involves multiple decisions, as one first needs to identify documents relevant to the event, extract relevant information from those documents, and finally reconcile various values obtained for the same relation of the event from different sources (Ahn, 2006). Search based methods for Information Extraction have been increasingly investigated (West et al., 2014); (Hegde and Talukdar, 2015); (Zhang et al., 2016); (Bing et al., 2017). (Kanani and McCallum, 2012) combine search and Information Extraction (IE) using Reinforcement Learning (RL), with the goal of selecting good actions while staying within resource constraints, but don’t optimze for extraction accuracy. More recently, (Narasimhan et al., 2016) proposed a RL-based approach to model the IE process outlined above. We shall refer to this approach as RLIE-DQN in this paper. RLIE-DQN trains an RL DQN trains a single agent sequentially, updating parameters based on one instance at a time. Each such instance is sampled from"
D17-1281,D16-1261,0,0.373894,"nt, extract relevant information from those documents, and finally reconcile various values obtained for the same relation of the event from different sources (Ahn, 2006). Search based methods for Information Extraction have been increasingly investigated (West et al., 2014); (Hegde and Talukdar, 2015); (Zhang et al., 2016); (Bing et al., 2017). (Kanani and McCallum, 2012) combine search and Information Extraction (IE) using Reinforcement Learning (RL), with the goal of selecting good actions while staying within resource constraints, but don’t optimze for extraction accuracy. More recently, (Narasimhan et al., 2016) proposed a RL-based approach to model the IE process outlined above. We shall refer to this approach as RLIE-DQN in this paper. RLIE-DQN trains an RL DQN trains a single agent sequentially, updating parameters based on one instance at a time. Each such instance is sampled from the entire training data, also called the experience replay. Such sequential experience replay-based training results in slow learning, while requiring high memory and computation resources. In order to overcome this challenge, A3C (Asynchronous Advantage Actor-Critic), an asynchronous deep RL training algorithm, has be"
D18-1157,P05-1045,0,0.0854488,"he probability distribution over the relations. p(y) = Softmax(W · Bˆ + b). 6 Experimental Setup 6.1 Datasets In our experiments, we evaluate the models on Riedel and Google Distant Supervision (GIDS) dataset. Statistics of the datasets is summarized in Table 1. Below we described each in detail1 . 1. Riedel: The dataset is developed by (Riedel et al., 2010) by aligning Freebase relations with New York Times (NYT) corpus, where sentences from the year 2005-2006 are used for creating the training set and from the year 2007 for the test set. The entity mentions are annotated using Stanford NER (Finkel et al., 2005) and are linked to Freebase. The dataset has been widely used for RE by (Hoffmann et al., 2011; Surdeanu et al., 2012) and more recently by (Lin et al., 2016; Feng et al.; He et al., 2018). 2. GIDS: Jat et al. (2018) created Google Distant Supervision (GIDS) dataset by extending the Google relation extraction corpus2 with additional instances for each entity pair. The dataset assures that the at-least-one assumption of multi-instance learning, holds. This makes automatic evaluation more reliable and thus removes the need for manual verification. 1 Data splits and hyperparameters are in supplem"
D18-1157,P15-1034,0,0.243754,"17), but such information is not available for all entities. Type information of entities has been used by Ling and Weld (2012); Liu et al. (2014) as features in their model. Yaghoobzadeh et al. (2017) also attempt to mitigate noise in DS through their joint entity typing and relation extraction model. However, KBs like Freebase readily provide reliable type information which could be directly utilized. In our work, we make principled use of entity type and relation alias information obtained from KB. We also use unsupervised Open Information Extraction (Open IE) methods (Mausam et al., 2012; Angeli et al., 2015), which automatically discover possible relations without the need of any predefined ontology, which is used as a side information as defined in Section 5.2. 3 Background: Graph Convolution Networks (GCN) u∈N (v) Here, Wluv ∈ Rd×d and bluv ∈ Rd are label dependent model parameters which are trained based on the downstream task. N (v) refers to the set of neighbors of v based on E 0 and f is any non-linear activation function. In order to capture multihop neighborhood, multiple GCN layers can be stacked. Hidden representation of node v in this case after k th GCN layer is given as:   hk+1 =f"
D18-1157,D17-1209,0,0.103624,"have demonstrated promising performance on RE. Zeng et al. (2014, 2015) employ Convolutional Neural Networks (CNN) to learn representations of instances. For alleviating noise in distant supervised datasets, attention has been utilized by (Lin et al., 2016; Jat et al., 2018). Syntactic information from dependency parses has been used by (Mintz et al., 2009; He et al., 2018) for capturing long-range dependencies between tokens. Recently proposed Graph Convolution Networks (GCN) (Defferrard et al., 2016) have been effectively employed for encoding this information (Marcheggiani and Titov, 2017; Bastings et al., 2017). However, all the above models rely only on the noisy instances from distant supervision for RE. Relevant side information can be effective for improving RE. For instance, in the sentence, Microsoft was started by Bill Gates., the type information of Bill Gates (person) and Microsoft (organization) can be helpful in predicting the correct relation founderOfCompany. This is because every relation constrains the type of its target en1257 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1257–1266 c Brussels, Belgium, October 31 - November 4, 2018. 201"
D18-1157,D14-1179,0,0.0325223,"Missing"
D18-1157,P11-1055,0,0.883297,"ation set, that exists between e1 and e2 . If no relation exists, we simply label it NA. Most supervised relation extraction methods require large labeled training data which is expensive to construct. Distant Supervision (DS) (Mintz et al., 2009) helps with the construction of this dataset automatically, under the assumption that if two entities have a relationship in a KB, then all sentences mentioning those entities express the same relation. While this approach works well in generating large amounts of training instances, the DS assumption does not hold in all cases. Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) propose multi-instance based learning to relax this assumption. However, they use NLP tools to extract features, which can be noisy. Recently, neural models have demonstrated promising performance on RE. Zeng et al. (2014, 2015) employ Convolutional Neural Networks (CNN) to learn representations of instances. For alleviating noise in distant supervised datasets, attention has been utilized by (Lin et al., 2016; Jat et al., 2018). Syntactic information from dependency parses has been used by (Mintz et al., 2009; He et al., 2018) for capturing long-range dependencies bet"
D18-1157,P16-1200,0,0.582447,"tion. While this approach works well in generating large amounts of training instances, the DS assumption does not hold in all cases. Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) propose multi-instance based learning to relax this assumption. However, they use NLP tools to extract features, which can be noisy. Recently, neural models have demonstrated promising performance on RE. Zeng et al. (2014, 2015) employ Convolutional Neural Networks (CNN) to learn representations of instances. For alleviating noise in distant supervised datasets, attention has been utilized by (Lin et al., 2016; Jat et al., 2018). Syntactic information from dependency parses has been used by (Mintz et al., 2009; He et al., 2018) for capturing long-range dependencies between tokens. Recently proposed Graph Convolution Networks (GCN) (Defferrard et al., 2016) have been effectively employed for encoding this information (Marcheggiani and Titov, 2017; Bastings et al., 2017). However, all the above models rely only on the noisy instances from distant supervision for RE. Relevant side information can be effective for improving RE. For instance, in the sentence, Microsoft was started by Bill Gates., the ty"
D18-1157,D17-1189,0,0.119192,"taset. The Precision-Recall curves on Riedel and GIDS are presented in Figure 3. Overall, we find that RESIDE achieves higher precision over the entire recall range on both the datasets. All the non-neural baselines could not perform well as the features used by them are mostly derived from NLP tools which can be erroneous. RESIDE outperforms PCNN+ATT and BGWA which indicates that incorporating side information helps in improving the performance of the model. The higher performance of BGWA and PCNN+ATT over PCNN shows that attention helps in distant supervised RE. Following (Lin et al., 2016; Liu et al., 2017), we also evaluate our method with different number of sentences. Results summarized in Table 2, show the improved precision of RESIDE in all test settings, as compared to the neural baselines, which demonstrates 1263 One PCNN PCNN+ATT BGWA RESIDE Two All P@100 P@200 P@300 P@100 P@200 P@300 P@100 P@200 P@300 73.3 73.3 78.0 80.0 64.8 69.2 71.0 75.5 56.8 60.8 63.3 69.3 70.3 77.2 81.0 83.0 67.2 71.6 73.0 73.5 63.1 66.1 64.0 70.6 72.3 76.2 82.0 84.0 69.7 73.1 75.0 78.5 64.1 67.4 72.0 75.6 Table 2: P@N for relation extraction using variable number of sentences in bags (with more than one sentence)"
D18-1157,C14-1199,0,0.0405433,"et al. (2018) use them for getting promising results through a recursive tree-GRU based model. In RESIDE, we make use of recently proposed Graph Convolution Networks (Defferrard et al., 2016; Kipf and Welling, 2017), which have been found to be quite effective for modelling syntactic information (Marcheggiani and Titov, 2017; Nguyen and Grishman, 2018; Vashishth et al., 2018a). Side Information in RE: Entity description from KB has been utilized for RE (Ji et al., 2017), but such information is not available for all entities. Type information of entities has been used by Ling and Weld (2012); Liu et al. (2014) as features in their model. Yaghoobzadeh et al. (2017) also attempt to mitigate noise in DS through their joint entity typing and relation extraction model. However, KBs like Freebase readily provide reliable type information which could be directly utilized. In our work, we make principled use of entity type and relation alias information obtained from KB. We also use unsupervised Open Information Extraction (Open IE) methods (Mausam et al., 2012; Angeli et al., 2015), which automatically discover possible relations without the need of any predefined ontology, which is used as a side informa"
D18-1157,P14-5010,0,0.00393016,"been found to be quite effective in encoding the context of tokens in several tasks (Sutskever et al., 2014; Graves et al., 2013). Although Bi-GRU is capable of capturing local context, it fails to capture long-range dependencies which can be captured through dependency edges. Prior works (Mintz et al., 2009; He et al., 2018) have exploited features from syntactic dependency trees for improving relation extraction. Motivated by their work, we employ Syntactic Graph Convolution Networks for encoding this information. For a given sentence, we generate its dependency tree using Stanford CoreNLP (Manning et al., 2014). We then run GCN over the dependency graph and use Equation 2 for updating the embeddings, taking Hgru as the input. Since dependency graph has 55 different edge labels, incorporating all of them overparameterizes the model significantly. Therefore, following (Marcheggiani and Titov, 2017; Nguyen and Grishman, 2018; Vashishth et al., 2018a) we use only three edge labels based on the direction of the edge {forward (→), backward (←), selfloop (&gt;)}. We define the new edge label Luv for an edge (u, v, luv ) as follows: Luv   → = ←   &gt; if edge exists in dependency parse if edge is an inverse"
D18-1157,D17-1159,0,0.334979,"oisy. Recently, neural models have demonstrated promising performance on RE. Zeng et al. (2014, 2015) employ Convolutional Neural Networks (CNN) to learn representations of instances. For alleviating noise in distant supervised datasets, attention has been utilized by (Lin et al., 2016; Jat et al., 2018). Syntactic information from dependency parses has been used by (Mintz et al., 2009; He et al., 2018) for capturing long-range dependencies between tokens. Recently proposed Graph Convolution Networks (GCN) (Defferrard et al., 2016) have been effectively employed for encoding this information (Marcheggiani and Titov, 2017; Bastings et al., 2017). However, all the above models rely only on the noisy instances from distant supervision for RE. Relevant side information can be effective for improving RE. For instance, in the sentence, Microsoft was started by Bill Gates., the type information of Bill Gates (person) and Microsoft (organization) can be helpful in predicting the correct relation founderOfCompany. This is because every relation constrains the type of its target en1257 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1257–1266 c Brussels, Belgium, October 31"
D18-1157,D12-1048,0,0.291377,"for RE (Ji et al., 2017), but such information is not available for all entities. Type information of entities has been used by Ling and Weld (2012); Liu et al. (2014) as features in their model. Yaghoobzadeh et al. (2017) also attempt to mitigate noise in DS through their joint entity typing and relation extraction model. However, KBs like Freebase readily provide reliable type information which could be directly utilized. In our work, we make principled use of entity type and relation alias information obtained from KB. We also use unsupervised Open Information Extraction (Open IE) methods (Mausam et al., 2012; Angeli et al., 2015), which automatically discover possible relations without the need of any predefined ontology, which is used as a side information as defined in Section 5.2. 3 Background: Graph Convolution Networks (GCN) u∈N (v) Here, Wluv ∈ Rd×d and bluv ∈ Rd are label dependent model parameters which are trained based on the downstream task. N (v) refers to the set of neighbors of v based on E 0 and f is any non-linear activation function. In order to capture multihop neighborhood, multiple GCN layers can be stacked. Hidden representation of node v in this case after k th GCN layer is"
D18-1157,P09-1113,0,0.872727,"ng semantic relationships between entity pairs from plain text. This task can be modeled as a simple classification problem after the entity pairs are specified. Formally, given an entity pair (e1 ,e2 ) from the KB and an entity annotated sentence (or instance), we aim to predict the ∗ Research internship at Indian Institute of Science. relation r, from a predefined relation set, that exists between e1 and e2 . If no relation exists, we simply label it NA. Most supervised relation extraction methods require large labeled training data which is expensive to construct. Distant Supervision (DS) (Mintz et al., 2009) helps with the construction of this dataset automatically, under the assumption that if two entities have a relationship in a KB, then all sentences mentioning those entities express the same relation. While this approach works well in generating large amounts of training instances, the DS assumption does not hold in all cases. Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) propose multi-instance based learning to relax this assumption. However, they use NLP tools to extract features, which can be noisy. Recently, neural models have demonstrated promising performance on"
D18-1157,P15-2070,0,0.0230025,"Missing"
D18-1157,D14-1162,0,0.0825152,"is concatenated with the matched relation embedding obtained from the previous step. Then, using attention over sentences, a representation for the entire bag is learned. This is then concatenated with entity type embedding before feeding into the softmax classifier for relation prediction. Please refer to Section 5.3 for more details. 5 RESIDE Details In this section, we provide the detailed description of the components of RESIDE. 5.1 Syntactic Sentence Encoding For each sentence in the bag si with m tokens {w1 , w2 , ...wm }, we first represent each token by k-dimensional GloVe embedding (Pennington et al., 2014). For incorporating relative position of tokens with respect to target entities, we use p-dimensional position embeddings, as used by (Zeng et al., 2014). The combined token embeddings are stacked together to get the sentence representation H ∈ Rm×(k+2p) . Then, using Bi-GRU (Cho et al., 2014) over H, we get the new sentence representation Hgru ∈ Rm×dgru , where dgru is the hidden state dimension. Bi-GRUs have been found to be quite effective in encoding the context of tokens in several tasks (Sutskever et al., 2014; Graves et al., 2013). Although Bi-GRU is capable of capturing local context,"
D18-1157,D12-1042,0,0.864844,"etween e1 and e2 . If no relation exists, we simply label it NA. Most supervised relation extraction methods require large labeled training data which is expensive to construct. Distant Supervision (DS) (Mintz et al., 2009) helps with the construction of this dataset automatically, under the assumption that if two entities have a relationship in a KB, then all sentences mentioning those entities express the same relation. While this approach works well in generating large amounts of training instances, the DS assumption does not hold in all cases. Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) propose multi-instance based learning to relax this assumption. However, they use NLP tools to extract features, which can be noisy. Recently, neural models have demonstrated promising performance on RE. Zeng et al. (2014, 2015) employ Convolutional Neural Networks (CNN) to learn representations of instances. For alleviating noise in distant supervised datasets, attention has been utilized by (Lin et al., 2016; Jat et al., 2018). Syntactic information from dependency parses has been used by (Mintz et al., 2009; He et al., 2018) for capturing long-range dependencies between tokens. Recently pr"
D18-1157,P18-1149,1,0.903296,"t al., 2014) for learning from multiple valid sentences. We also make use of attention for learning sentence and bag representations. Dependency tree based features have been found to be relevant for relation extraction (Mintz et al., 2009). He et al. (2018) use them for getting promising results through a recursive tree-GRU based model. In RESIDE, we make use of recently proposed Graph Convolution Networks (Defferrard et al., 2016; Kipf and Welling, 2017), which have been found to be quite effective for modelling syntactic information (Marcheggiani and Titov, 2017; Nguyen and Grishman, 2018; Vashishth et al., 2018a). Side Information in RE: Entity description from KB has been utilized for RE (Ji et al., 2017), but such information is not available for all entities. Type information of entities has been used by Ling and Weld (2012); Liu et al. (2014) as features in their model. Yaghoobzadeh et al. (2017) also attempt to mitigate noise in DS through their joint entity typing and relation extraction model. However, KBs like Freebase readily provide reliable type information which could be directly utilized. In our work, we make principled use of entity type and relation alias information obtained from KB."
D18-1157,E17-1111,0,0.147276,"Missing"
D18-1157,D15-1203,0,0.760896,"hortcoming, Riedel et al. (2010) relax distant supervision for multi-instance single-label learning. Subsequently, for handling overlapping relations between entities (Hoffmann et al., 2011; Surdeanu et al., 2012) propose multi-instance multi-label learning paradigm. Neural Relation Extraction: The performance of the above methods strongly rely on the quality of hand engineered features. Zeng et al. (2014) 1258 propose an end-to-end CNN based method which could automatically capture relevant lexical and sentence level features. This method is further improved through piecewise max-pooling by (Zeng et al., 2015). Lin et al. (2016); Nagarajan et al. (2017) use attention (Bahdanau et al., 2014) for learning from multiple valid sentences. We also make use of attention for learning sentence and bag representations. Dependency tree based features have been found to be relevant for relation extraction (Mintz et al., 2009). He et al. (2018) use them for getting promising results through a recursive tree-GRU based model. In RESIDE, we make use of recently proposed Graph Convolution Networks (Defferrard et al., 2016; Kipf and Welling, 2017), which have been found to be quite effective for modelling syntactic"
D18-1157,C14-1220,0,0.811541,"ps with the construction of this dataset automatically, under the assumption that if two entities have a relationship in a KB, then all sentences mentioning those entities express the same relation. While this approach works well in generating large amounts of training instances, the DS assumption does not hold in all cases. Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) propose multi-instance based learning to relax this assumption. However, they use NLP tools to extract features, which can be noisy. Recently, neural models have demonstrated promising performance on RE. Zeng et al. (2014, 2015) employ Convolutional Neural Networks (CNN) to learn representations of instances. For alleviating noise in distant supervised datasets, attention has been utilized by (Lin et al., 2016; Jat et al., 2018). Syntactic information from dependency parses has been used by (Mintz et al., 2009; He et al., 2018) for capturing long-range dependencies between tokens. Recently proposed Graph Convolution Networks (GCN) (Defferrard et al., 2016) have been effectively employed for encoding this information (Marcheggiani and Titov, 2017; Bastings et al., 2017). However, all the above models rely only"
D18-1213,D17-1209,0,0.0314369,"Missing"
D18-1213,P12-1011,0,0.347325,"ing of events and facts (Allan et al., 1998; Talukdar et al., 2012b), document summarization (Wan, 2007) and analysis (de Jong et al., 2005a) require precise and validated creation time of the documents. Most of the documents obtained from the Web either contain DCT that cannot be trusted or contain no DCT information at all (Kanhabua and Nørv˚ag, 2008). Thus, predicting the time of these documents based on their content is an important task, often referred to as Document Dating. A few generative approaches (de Jong et al., 2005b; Kanhabua and Nørv˚ag, 2008) as well as a discriminative model (Chambers, 2012) have been previously proposed for this task. Kotsakos et al. (2014) employs term-burstiness resulting in improved precision on this task. ppt@iisc.ac.in Recently proposed NeuralDater (Vashishth et al., 2018) uses a graph convolution network (GCN) based approach for document dating, outperforming all previous models by a significant margin. NeuralDater extensively uses the syntactic and temporal graph structure present within the document itself. Motivated by NeuralDater, we explicitly develop two different methods: a) Attentive Context Model, and b) Ordered Event Model. The first component tr"
D18-1213,P14-5010,0,0.0026023,"ow: 3.2.1 Context Embedding Bi-directional LSTM is employed for embedding each word with its context. The GloVe representation of the words X ∈ Rn×k is transformed to a context aware representation H cntx ∈ Rn×k to get the context embedding. This is essentially shown as the Bi-LSTM in Figure 1. 3.2.2 Syntactic Embedding In this step, the context embeddings are further processed using GCN over the dependency parse tree of the sentences in the document, in order to capture long range connection among words. The syntactic dependency structure is extracted by Stanford CoreNLP’s dependency parser (Manning et al., 2014). NeuralDater follows the same formulation of GCN for directed graph as described in Section 3.1, where additional edges are added to the graph to model the information flow. Again following (Marcheggiani and Titov, 2017), NeuralDater does not allocate separate weight matrices for different types of dependency edge labels, rather it considers only three type of edges: a) edges that exist originally, b) the reverse edges that are added explicitly, and c) self loops. The S-GCN portion of Figure 1 represents this component. More formally, H cntx ∈ Rn×k is transformed to H syn ∈ Rn×ksyn by applyin"
D18-1213,I17-1085,0,0.024163,"ual creation time, whereas the other one is a discriminative model trained on hand-crafted features. Kotsakos et al. (2014) propose a termburstiness (Lappas et al., 2009) based statistical method for the task. Vashishth et al. (2018) propose a deep learning based model which exploits the temporal and syntactic structure in documents using graph convolutional networks (GCN). Event Ordering System: The task of extracting temporally rich events and time expressions and ordering between them is introduced in the TempEval challenge (UzZaman et al., 2013; Verhagen et al., 2010). Various approaches (McDowell et al., 2017; Mirza and Tonelli, 2016) made for solving the task use sieve-based architectures, where multiple classifiers are ranked according to their precision and their predictions are weighted accordingly resulting in a temporal graph structure. A method to extract temporal ordering among relational facts was proposed in (Talukdar et al., 2012a). Graph Convolutional Network (GCN): GCN (Kipf and Welling, 2016) is the extension of convolutional networks over graphs. In different NLP tasks such as semantic-role labeling (Marcheggiani and Titov, 2017), neural machine translation (Bastings et al., 2017),"
D18-1213,C16-1007,0,0.288365,"eas the other one is a discriminative model trained on hand-crafted features. Kotsakos et al. (2014) propose a termburstiness (Lappas et al., 2009) based statistical method for the task. Vashishth et al. (2018) propose a deep learning based model which exploits the temporal and syntactic structure in documents using graph convolutional networks (GCN). Event Ordering System: The task of extracting temporally rich events and time expressions and ordering between them is introduced in the TempEval challenge (UzZaman et al., 2013; Verhagen et al., 2010). Various approaches (McDowell et al., 2017; Mirza and Tonelli, 2016) made for solving the task use sieve-based architectures, where multiple classifiers are ranked according to their precision and their predictions are weighted accordingly resulting in a temporal graph structure. A method to extract temporal ordering among relational facts was proposed in (Talukdar et al., 2012a). Graph Convolutional Network (GCN): GCN (Kipf and Welling, 2016) is the extension of convolutional networks over graphs. In different NLP tasks such as semantic-role labeling (Marcheggiani and Titov, 2017), neural machine translation (Bastings et al., 2017), and event detection (Nguye"
D18-1213,S13-2001,0,0.012689,"ns the probabilistic constraints between year mentions and the actual creation time, whereas the other one is a discriminative model trained on hand-crafted features. Kotsakos et al. (2014) propose a termburstiness (Lappas et al., 2009) based statistical method for the task. Vashishth et al. (2018) propose a deep learning based model which exploits the temporal and syntactic structure in documents using graph convolutional networks (GCN). Event Ordering System: The task of extracting temporally rich events and time expressions and ordering between them is introduced in the TempEval challenge (UzZaman et al., 2013; Verhagen et al., 2010). Various approaches (McDowell et al., 2017; Mirza and Tonelli, 2016) made for solving the task use sieve-based architectures, where multiple classifiers are ranked according to their precision and their predictions are weighted accordingly resulting in a temporal graph structure. A method to extract temporal ordering among relational facts was proposed in (Talukdar et al., 2012a). Graph Convolutional Network (GCN): GCN (Kipf and Welling, 2016) is the extension of convolutional networks over graphs. In different NLP tasks such as semantic-role labeling (Marcheggiani and"
D18-1213,P18-1149,1,0.308224,"ts. Most of the documents obtained from the Web either contain DCT that cannot be trusted or contain no DCT information at all (Kanhabua and Nørv˚ag, 2008). Thus, predicting the time of these documents based on their content is an important task, often referred to as Document Dating. A few generative approaches (de Jong et al., 2005b; Kanhabua and Nørv˚ag, 2008) as well as a discriminative model (Chambers, 2012) have been previously proposed for this task. Kotsakos et al. (2014) employs term-burstiness resulting in improved precision on this task. ppt@iisc.ac.in Recently proposed NeuralDater (Vashishth et al., 2018) uses a graph convolution network (GCN) based approach for document dating, outperforming all previous models by a significant margin. NeuralDater extensively uses the syntactic and temporal graph structure present within the document itself. Motivated by NeuralDater, we explicitly develop two different methods: a) Attentive Context Model, and b) Ordered Event Model. The first component tries to accumulate knowledge across documents, whereas the latter uses the temporal structure of the document for predicting its DCT. Motivated by the effectiveness of attention based models in different NLP t"
D18-1213,N16-1174,0,0.121228,"Missing"
D18-1225,P15-1026,0,0.0198226,"as well as temporal KG embedding methods. 1 Introduction Knowledge Graphs (KGs) are large multirelational graphs where nodes correspond to entities, and typed edges represent relationships among them. KGs encode factual beliefs in the form of triple (entity, relation, entity), e.g., (Brussels, isCapitalOf, Belgium). Examples of a few KGs include NELL (Mitchell et al., 2018), YAGO (Suchanek et al., 2007), and Freebase (Bollacker et al., 2008). KGs have been found to be useful for a variety of tasks, viz., Information Retrieval (Kotov and Zhai, 2012; Xiong and Callan, 2015), Question Answering (Dong et al., 2015; Bordes et al., 2015; Yao and Durme, 2014), among others. KG embedding has emerged as a very active area of research over the last few years, resulting Partha Talukdar Indian Institute of Science Bangalore, India ppt@iisc.ac.in in the development of several techniques (Bordes et al., 2013; Nickel et al., 2016b; Yang et al., 2014; Lin et al., 2015; Trouillon et al., 2016; Dettmers et al., 2018; Guo et al., 2018). These methods learn high-dimensional vectorial representations for nodes and relations in the KG, while preserving various graph and knowledge constraints. We note that KG beliefs are"
D18-1225,P15-1067,0,0.124664,"gh we try to attend to a similar problem, the method proposed in this paper is more related to relational embedding learning paradigm than scoping temporal facts from the web. Relational embedding learning methods: An enormous amount of research has been done in this field, especially for KG completion or link prediction task (Bordes et al., 2013). (Nickel et al., 2016a) provides a detailed review of the recent KG embedding learning methods. These can be broadly categorized into two different paradigms. TransE(Bordes et al., 2013), TransH(Wang et al., 2014), TransR (Lin et al., 2015), TransD (Ji et al., 2015) are the translational distance-based models. Here the main theme is to minimize the distance between two entity vectors where one of them is translated by a relation vector. The realm of matrix factorization based methods includes bilinear model RESCAL (Nickel et al., 2011), DistMult (Yang et al., 2014), HoIE (Nickel et al., 2016b). Some of the other notable models are Neural Tensor Networks(NTN) (Socher et al., 2013). We also provide some background on the traditional methods in section 3. However, the temporal dimension remains silent in all of these inference methods. Link prediction throu"
D18-1225,D16-1260,0,0.226665,"KG embedding methods ignore the availability or importance of such temporal scopes while learning embeddings of nodes and relations in the KGs. These methods treat the KG as a static graph with the assumption that the beliefs contained in them are universally true. This is clearly inadequate and it is quite conceivable that incorporating temporal scopes during representation learning is likely to yield better KG embeddings. In spite of its importance, temporally aware KG embeddings is a relatively unexplored area. Recently, a KG embedding method which utilizes temporal scopes was proposed in (Jiang et al., 2016). However, instead of directly incorporating time in the learned embeddings, the method proposed in (Jiang et al., 2016) first learns temporal order among relations (e.g., wasBorIn → wonPrize → diedIn). These relation orders are then incorporated as constraints during the KG embedding stage. Thus, the embedding learned by (Jiang et al., 2016) is not explicitly temporally aware. In order to overcome this challenge, in this paper, we propose Hyperplane-based Temporally aware KG Embedding (HyTE), a novel KG em2001 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Process"
D18-1225,I17-1085,0,0.0287863,"xpressions. On the other hand, systems like PRAVDA harvests temporal information from free text sources using label propagation. CoTS (Talukdar et al., 2012b) uses integer linear program based approach to model temporal constraints and proposes joint inference framework with few seed examples. A method for discovering temporal ordering among factual relations was proposed in (Talukdar et al., 2012a). The task of extracting temporally rich events and time expressions and ordering between them is introduced in TempEval challenge (UzZaman et al., 2013; Verhagen et al., 2010). Various approaches (McDowell et al., 2017; Mirza and Tonelli, 2016) made for solving the task proved to be effective in other temporal reasoning tasks. Although we try to attend to a similar problem, the method proposed in this paper is more related to relational embedding learning paradigm than scoping temporal facts from the web. Relational embedding learning methods: An enormous amount of research has been done in this field, especially for KG completion or link prediction task (Bordes et al., 2013). (Nickel et al., 2016a) provides a detailed review of the recent KG embedding learning methods. These can be broadly categorized into"
D18-1225,C16-1007,0,0.0195609,"r hand, systems like PRAVDA harvests temporal information from free text sources using label propagation. CoTS (Talukdar et al., 2012b) uses integer linear program based approach to model temporal constraints and proposes joint inference framework with few seed examples. A method for discovering temporal ordering among factual relations was proposed in (Talukdar et al., 2012a). The task of extracting temporally rich events and time expressions and ordering between them is introduced in TempEval challenge (UzZaman et al., 2013; Verhagen et al., 2010). Various approaches (McDowell et al., 2017; Mirza and Tonelli, 2016) made for solving the task proved to be effective in other temporal reasoning tasks. Although we try to attend to a similar problem, the method proposed in this paper is more related to relational embedding learning paradigm than scoping temporal facts from the web. Relational embedding learning methods: An enormous amount of research has been done in this field, especially for KG completion or link prediction task (Bordes et al., 2013). (Nickel et al., 2016a) provides a detailed review of the recent KG embedding learning methods. These can be broadly categorized into two different paradigms."
D18-1225,S13-2001,0,0.0331077,"data like Wikipedia Infoboxes, and categories using only regular expressions. On the other hand, systems like PRAVDA harvests temporal information from free text sources using label propagation. CoTS (Talukdar et al., 2012b) uses integer linear program based approach to model temporal constraints and proposes joint inference framework with few seed examples. A method for discovering temporal ordering among factual relations was proposed in (Talukdar et al., 2012a). The task of extracting temporally rich events and time expressions and ordering between them is introduced in TempEval challenge (UzZaman et al., 2013; Verhagen et al., 2010). Various approaches (McDowell et al., 2017; Mirza and Tonelli, 2016) made for solving the task proved to be effective in other temporal reasoning tasks. Although we try to attend to a similar problem, the method proposed in this paper is more related to relational embedding learning paradigm than scoping temporal facts from the web. Relational embedding learning methods: An enormous amount of research has been done in this field, especially for KG completion or link prediction task (Bordes et al., 2013). (Nickel et al., 2016a) provides a detailed review of the recent K"
D18-1225,P14-1090,0,0.0602746,"Missing"
deepa-etal-2004-automatic,W00-1219,0,\N,Missing
deepa-etal-2004-automatic,E03-1076,0,\N,Missing
deepa-etal-2004-automatic,P94-1033,0,\N,Missing
N15-1004,W13-3206,0,0.110838,"hrasal semantics, which we leverage to analyze performance on a behavioral task. 1 Introduction Vector Space Models (VSMs) are models of word semantics typically built with word usage statistics derived from corpora. VSMs have been shown to closely match human judgements of semantics (for an overview see Sahlgren (2006), Chapter 5), and can be used to study semantic composition (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Socher et al., 2012; Turney, 2012). Composition has been explored with different types of composition functions (Mitchell and Lapata, 2010; Mikolov et al., 2013; Dinu et al., 2013) including higher order functions (such as matrices) (Baroni and Zamparelli, 2010), and some have considered which corpus-derived information is most useful for semantic composition (Turney, 2012; Fyshe et al., 2013). Still, many VSMs act like a black box - it is unclear what VSM dimensions represent (save for broad classes of corpus statistic types) and what the application of a composition function to those dimensions entails. Neural network (NN) models are becoming increasingly popular (Socher et al., 2012; Hashimoto et al., 2014; Mikolov et al., 2013; Pennington et al., 2014), and some mod"
N15-1004,D14-1162,0,0.121474,"kolov et al., 2013; Dinu et al., 2013) including higher order functions (such as matrices) (Baroni and Zamparelli, 2010), and some have considered which corpus-derived information is most useful for semantic composition (Turney, 2012; Fyshe et al., 2013). Still, many VSMs act like a black box - it is unclear what VSM dimensions represent (save for broad classes of corpus statistic types) and what the application of a composition function to those dimensions entails. Neural network (NN) models are becoming increasingly popular (Socher et al., 2012; Hashimoto et al., 2014; Mikolov et al., 2013; Pennington et al., 2014), and some model introspection has been attempted: Levy and Goldberg (2014) examined connections between layers, Mikolov et al. (2013) and Pennington et al. (2014) explored how shifts in VSM space encodes semantic relationships. Still, interpreting NN VSM dimensions, or factors, remains elusive. This paper introduces a new method, Compositional Non-negative Sparse Embedding (CNNSE). In contrast to many other VSMs, our method learns an interpretable VSM that is tailored to suit the semantic composition function. Such interpretability allows for deeper exploration of semantic composition than pr"
N15-1004,W13-3510,1,0.817656,". VSMs have been shown to closely match human judgements of semantics (for an overview see Sahlgren (2006), Chapter 5), and can be used to study semantic composition (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Socher et al., 2012; Turney, 2012). Composition has been explored with different types of composition functions (Mitchell and Lapata, 2010; Mikolov et al., 2013; Dinu et al., 2013) including higher order functions (such as matrices) (Baroni and Zamparelli, 2010), and some have considered which corpus-derived information is most useful for semantic composition (Turney, 2012; Fyshe et al., 2013). Still, many VSMs act like a black box - it is unclear what VSM dimensions represent (save for broad classes of corpus statistic types) and what the application of a composition function to those dimensions entails. Neural network (NN) models are becoming increasingly popular (Socher et al., 2012; Hashimoto et al., 2014; Mikolov et al., 2013; Pennington et al., 2014), and some model introspection has been attempted: Levy and Goldberg (2014) examined connections between layers, Mikolov et al. (2013) and Pennington et al. (2014) explored how shifts in VSM space encodes semantic relationships. S"
N15-1004,D12-1110,0,\N,Missing
N15-1004,D14-1163,0,\N,Missing
N15-1004,C12-1118,1,\N,Missing
N18-1167,P16-1179,0,0.0936546,"effective graph densification method which may be applied to improve EL involving any KG. • By using pseudo entities and unambiguous mentions of entity in a corpus, we demonstrate how non-entity-linked corpus can be used to improve EL performance. • We have made ELDEN’s code and data publicly available4 . 2 Related Work Entity linking: Most EL systems use coherence among entities (Cheng and Roth, 2013) to link mentions. We studied coherence measures and datasets used in six recent5 EL systems (He et al., 2013; Huang et al., 2015; Sun et al., 2015; Yamada et al., 2016; Globerson et al., 2016; Barrena et al., 2016). We see that the two popular datasets used for evaluating EL (Chisholm and Hachey, 2015) on documents are CoNLL (Hoffart et al., 2011) and TAC2010 (Ji et al., 2010), here after TAC. The popular coherence measures used are (1) WLM, (2) Entity Embedding Similarity and (3) Jaccard Similarity (Chisholm and Hachey, 2015; Guo et al., 2013). WLM is widely acknowledged as most popular (Hoffart et al., 2012) with almost all the six approaches analyzed above using WLM or its variants. Entity embedding similarity (Yamada et al., 2016) is reported to give highest EL6 performance and is the baseline of EL"
N18-1167,D11-1098,0,0.0219611,"ng external corpus has been studied by Kotnis et al. (2015) and Hegde and Talukdar (2015). Densifying edge graph is also studied as ‘link prediction’ in literature (Mart´ınez et al., 2016). Kotnis et al. augment paths between KG nodes using ‘bridging entities’ which are noun phrases mined from an external corpus. ELDEN has a similar approach as it proposes densifying the KG edges of entities by adding edges from pseudo entities. However, densification is used for relation inference in the former methods whereas it is used for entity coherence measurement in ELDEN. Word co-occurrence measures: Chaudhari et al. (2011) survey several co-occurrence measures for word association including PMI, Jaccard (Dice, 1945) and Co-occurrence Significance Ratio (CSR). Damani (2013) proves that considering corpus level significant co-occurrences, PMI is better than others. Budiu et al. (2007) compare Latent Semantic Analysis (LSA), PMI and Generalized Latent Semantic Analysis (GLSA) and conclude that for large corpora like web corpus, PMI works best on word similarity tests. Hence, we chose PMI to refine co-occurring mentions of entities in web corpus. 3 Definitions and Problem Formulation In this section, we present a f"
N18-1167,D13-1184,0,0.0324844,"ased on number of common edges in KG. 3 This paper focuses on mention disambiguation. We assume mention and candidate entities are detected already. We make the following contributions: • ELDEN presents a simple yet effective graph densification method which may be applied to improve EL involving any KG. • By using pseudo entities and unambiguous mentions of entity in a corpus, we demonstrate how non-entity-linked corpus can be used to improve EL performance. • We have made ELDEN’s code and data publicly available4 . 2 Related Work Entity linking: Most EL systems use coherence among entities (Cheng and Roth, 2013) to link mentions. We studied coherence measures and datasets used in six recent5 EL systems (He et al., 2013; Huang et al., 2015; Sun et al., 2015; Yamada et al., 2016; Globerson et al., 2016; Barrena et al., 2016). We see that the two popular datasets used for evaluating EL (Chisholm and Hachey, 2015) on documents are CoNLL (Hoffart et al., 2011) and TAC2010 (Ji et al., 2010), here after TAC. The popular coherence measures used are (1) WLM, (2) Entity Embedding Similarity and (3) Jaccard Similarity (Chisholm and Hachey, 2015; Guo et al., 2013). WLM is widely acknowledged as most popular (Hof"
N18-1167,Q15-1011,0,0.382817,"mbiguation performance using KG densification: edges to WWW conference, a sparsely-connected entity in the KG, is increased by adding edges from pseudo entity Program Committee whose mention cooccurs with it in web corpus. ELDEN, the system proposed in this paper, uses such densified KG to successfully link ambiguous mention WWW to the correct entity WWW conference, instead of the more popular entity World Wide Web. Introduction Entity Linking (EL) is the task of mapping mentions of an entity in text to the corresponding entity in Knowledge Graph (KG) (Hoffart et al., 2011; Dong et al., 2014; Chisholm and Hachey, 2015). EL systems primarily exploit two types of information: (1) similarity of the mention to the candidate entity string, and (2) coherence between the candidate entity and other entities mentioned in the vicinity of the mention in text. Coherence essentially measures how well the candidate entity is connected, either directly or indirectly, with other KG entities mentioned in the vicinity (Milne and Witten, 2008; Globerson et al., 2016). In the stateof-the-art EL system by (Yamada et al., 2016), coherence is measured as distance between embeddings of entities. This system performs well on entiti"
N18-1167,P89-1010,0,0.0527623,"Missing"
N18-1167,W13-3503,0,0.0194452,"(Mart´ınez et al., 2016). Kotnis et al. augment paths between KG nodes using ‘bridging entities’ which are noun phrases mined from an external corpus. ELDEN has a similar approach as it proposes densifying the KG edges of entities by adding edges from pseudo entities. However, densification is used for relation inference in the former methods whereas it is used for entity coherence measurement in ELDEN. Word co-occurrence measures: Chaudhari et al. (2011) survey several co-occurrence measures for word association including PMI, Jaccard (Dice, 1945) and Co-occurrence Significance Ratio (CSR). Damani (2013) proves that considering corpus level significant co-occurrences, PMI is better than others. Budiu et al. (2007) compare Latent Semantic Analysis (LSA), PMI and Generalized Latent Semantic Analysis (GLSA) and conclude that for large corpora like web corpus, PMI works best on word similarity tests. Hence, we chose PMI to refine co-occurring mentions of entities in web corpus. 3 Definitions and Problem Formulation In this section, we present a few definitions and formulate the EL problem. Knowledge Graph (KG): A Knowledge Graph is defined as G = (E, F ) with entities E as nodes and F as edges. I"
N18-1167,K16-1026,0,0.0377315,"hod shows how unstructured data (web corpus about the entity to be linked) can be effectively used for entity disambiguation. Entity Embeddings: ELDEN presents a method to enhance embedding of entities and words in a 4 https://github.com/ priyaradhakrishnan0/ELDEN 5 (Shen et al., 2015) presents a survey of EL systems. 6 Named Entity Disambiguation (NED) and EL are synonymous terms in research (Hoffart et al., 2011) 1845 common vector space. Word embedding methods like word2vec (Mikolov et al., 2013) and Glove (Pennington et al., 2014) have been extended to entities in EL (Yamada et al., 2016; Fang et al., 2016; Zwicklbauer et al., 2016; Huang et al., 2015). These methods use data about entity-entity co-occurrences to improve the entity embeddings. In ELDEN, we improve it with web corpus cooccurrence statistics. Ganea and Hofmann (2017) present a very interesting neural model for jointly learning entity embedding along with mentions and contexts. KG densification with pseudo entities: KG densification using external corpus has been studied by Kotnis et al. (2015) and Hegde and Talukdar (2015). Densifying edge graph is also studied as ‘link prediction’ in literature (Mart´ınez et al., 2016). Kotnis e"
N18-1167,D17-1277,0,0.240235,"s://github.com/ priyaradhakrishnan0/ELDEN 5 (Shen et al., 2015) presents a survey of EL systems. 6 Named Entity Disambiguation (NED) and EL are synonymous terms in research (Hoffart et al., 2011) 1845 common vector space. Word embedding methods like word2vec (Mikolov et al., 2013) and Glove (Pennington et al., 2014) have been extended to entities in EL (Yamada et al., 2016; Fang et al., 2016; Zwicklbauer et al., 2016; Huang et al., 2015). These methods use data about entity-entity co-occurrences to improve the entity embeddings. In ELDEN, we improve it with web corpus cooccurrence statistics. Ganea and Hofmann (2017) present a very interesting neural model for jointly learning entity embedding along with mentions and contexts. KG densification with pseudo entities: KG densification using external corpus has been studied by Kotnis et al. (2015) and Hegde and Talukdar (2015). Densifying edge graph is also studied as ‘link prediction’ in literature (Mart´ınez et al., 2016). Kotnis et al. augment paths between KG nodes using ‘bridging entities’ which are noun phrases mined from an external corpus. ELDEN has a similar approach as it proposes densifying the KG edges of entities by adding edges from pseudo entit"
N18-1167,N13-1122,0,0.0314971,"ost EL systems use coherence among entities (Cheng and Roth, 2013) to link mentions. We studied coherence measures and datasets used in six recent5 EL systems (He et al., 2013; Huang et al., 2015; Sun et al., 2015; Yamada et al., 2016; Globerson et al., 2016; Barrena et al., 2016). We see that the two popular datasets used for evaluating EL (Chisholm and Hachey, 2015) on documents are CoNLL (Hoffart et al., 2011) and TAC2010 (Ji et al., 2010), here after TAC. The popular coherence measures used are (1) WLM, (2) Entity Embedding Similarity and (3) Jaccard Similarity (Chisholm and Hachey, 2015; Guo et al., 2013). WLM is widely acknowledged as most popular (Hoffart et al., 2012) with almost all the six approaches analyzed above using WLM or its variants. Entity embedding similarity (Yamada et al., 2016) is reported to give highest EL6 performance and is the baseline of ELDEN. Enhancing entity disambiguation: Among methods proposed in literature to enhance entity disambiguation utilizing KG (Bhattacharya and Getoor) uses additional relational information between database references; (Han and Zhao, 2010) uses semantic relatedness between entities in other KGs; and (Shen et al., 2018) uses paths consisti"
N18-1167,P10-1006,0,0.027817,"ed are (1) WLM, (2) Entity Embedding Similarity and (3) Jaccard Similarity (Chisholm and Hachey, 2015; Guo et al., 2013). WLM is widely acknowledged as most popular (Hoffart et al., 2012) with almost all the six approaches analyzed above using WLM or its variants. Entity embedding similarity (Yamada et al., 2016) is reported to give highest EL6 performance and is the baseline of ELDEN. Enhancing entity disambiguation: Among methods proposed in literature to enhance entity disambiguation utilizing KG (Bhattacharya and Getoor) uses additional relational information between database references; (Han and Zhao, 2010) uses semantic relatedness between entities in other KGs; and (Shen et al., 2018) uses paths consisting of defined relations between entities in the KG (IMDB and DBLP). All these methods utilize structured information, while our method shows how unstructured data (web corpus about the entity to be linked) can be effectively used for entity disambiguation. Entity Embeddings: ELDEN presents a method to enhance embedding of entities and words in a 4 https://github.com/ priyaradhakrishnan0/ELDEN 5 (Shen et al., 2015) presents a survey of EL systems. 6 Named Entity Disambiguation (NED) and EL are s"
N18-1167,D13-1041,0,0.172608,"te entities are detected already. We make the following contributions: • ELDEN presents a simple yet effective graph densification method which may be applied to improve EL involving any KG. • By using pseudo entities and unambiguous mentions of entity in a corpus, we demonstrate how non-entity-linked corpus can be used to improve EL performance. • We have made ELDEN’s code and data publicly available4 . 2 Related Work Entity linking: Most EL systems use coherence among entities (Cheng and Roth, 2013) to link mentions. We studied coherence measures and datasets used in six recent5 EL systems (He et al., 2013; Huang et al., 2015; Sun et al., 2015; Yamada et al., 2016; Globerson et al., 2016; Barrena et al., 2016). We see that the two popular datasets used for evaluating EL (Chisholm and Hachey, 2015) on documents are CoNLL (Hoffart et al., 2011) and TAC2010 (Ji et al., 2010), here after TAC. The popular coherence measures used are (1) WLM, (2) Entity Embedding Similarity and (3) Jaccard Similarity (Chisholm and Hachey, 2015; Guo et al., 2013). WLM is widely acknowledged as most popular (Hoffart et al., 2012) with almost all the six approaches analyzed above using WLM or its variants. Entity embedd"
N18-1167,D15-1241,1,0.531835,"g methods like word2vec (Mikolov et al., 2013) and Glove (Pennington et al., 2014) have been extended to entities in EL (Yamada et al., 2016; Fang et al., 2016; Zwicklbauer et al., 2016; Huang et al., 2015). These methods use data about entity-entity co-occurrences to improve the entity embeddings. In ELDEN, we improve it with web corpus cooccurrence statistics. Ganea and Hofmann (2017) present a very interesting neural model for jointly learning entity embedding along with mentions and contexts. KG densification with pseudo entities: KG densification using external corpus has been studied by Kotnis et al. (2015) and Hegde and Talukdar (2015). Densifying edge graph is also studied as ‘link prediction’ in literature (Mart´ınez et al., 2016). Kotnis et al. augment paths between KG nodes using ‘bridging entities’ which are noun phrases mined from an external corpus. ELDEN has a similar approach as it proposes densifying the KG edges of entities by adding edges from pseudo entities. However, densification is used for relation inference in the former methods whereas it is used for entity coherence measurement in ELDEN. Word co-occurrence measures: Chaudhari et al. (2011) survey several co-occurrence measur"
N18-1167,D14-1162,0,0.0874171,"Missing"
N18-1167,K16-1025,0,0.136506,"xt to the corresponding entity in Knowledge Graph (KG) (Hoffart et al., 2011; Dong et al., 2014; Chisholm and Hachey, 2015). EL systems primarily exploit two types of information: (1) similarity of the mention to the candidate entity string, and (2) coherence between the candidate entity and other entities mentioned in the vicinity of the mention in text. Coherence essentially measures how well the candidate entity is connected, either directly or indirectly, with other KG entities mentioned in the vicinity (Milne and Witten, 2008; Globerson et al., 2016). In the stateof-the-art EL system by (Yamada et al., 2016), coherence is measured as distance between embeddings of entities. This system performs well on entities which are densely-connected in KG, but not so well on sparsely-connected entities in the KG. We demonstrate this problem using the example sentence in Figure 1. This sentence has two mentions: Andrei Broder and WWW. The figure also shows mention-entity linkages, i.e., mentions and their candidate entities in KG. Using a conventional EL system, the first mention Andrei Broder1 can be easily linked to Andrei Broder using string similarity between the mention and candidate entity strings. Str"
N18-1167,Q15-1023,0,0.0879102,"aking this TAC candidate set publicly available. Baseline: Yamada16 Our baseline is the Yamada et al. system explained in Section 3. Entity embedding distance measured using ve trained on the 16 This is a tunable parameter. These recent datasets consist of other evaluations, e.g., mention detection, multilinguality etc. which is beyond the scope of the paper and hence we didnt focus on them in the paper. 18 https://github.com/masha-p/PPRforNED 19 http://lucene.apache.org/solr/ 17 Method CONLL CONLL TAC (P-micro) (P-micro) (P-macro) (Hoffart et al., 2011) 82.5 81.7 (He et al., 2013) 85.6 84.0 (Ling et al., 2015) 67.5 (Barrena et al., 2016) 88.32 (Chisholm and Hachey, 2015) - 81.0 - 88.7 - (Pershina et al., 2015) 91.8 89.9 91.7 - - (Globerson et al., 2016) (Yamada et al., 2016) 93.1 92.6 85.2 ELDEN 93.0 93.7 89.6 86.8 80.7 87.2 Table 4: Performance comparison with other recent EL approaches. ELDEN matches best results in CoNLL and outperforms the state-of-the-art in TAC dataset. (Please see Section 6.1 for details and ψELDEN++ row of Table 5 for ELDEN results.) input KG G is ψYamada . 6 6.1 Results Does ELDEN’s selective densification help in disambiguation in EL? In Table 4, we compare ELDEN’s EL per"
N19-1363,P13-1158,0,0.11204,"le. 1 B EAM S EARCH – how do i increase my height ? – how do i increase my body height ? – how do i increase the height ? – how would i increase my body height ? D I PS (O URS ) – how could i increase my height ? – what should i do to increase my height ? – what are the fastest ways to increase my height ? – is there any proven method to increase height ? cases desirable, to produce lexically diverse ones. Diversity in paraphrase generation finds applications in text simplification (Nisioi et al., 2017; Xu et al., 2015), document summarization (Li et al., 2009; Nema et al., 2017), QA systems (Fader et al., 2013; Bernhard and Gurevych, 2008), data augmentation (Zhang et al., 2015; Wang and Yang, 2015), conversational agents (Li et al., 2016) and information retrieval (Anick and Tipirneni, 1999). Paraphrasing is the task of rephrasing a given text in multiple ways such that the semantics of the generated sentences remain unaltered. Paraphrasing Quality can be attributed to two key characteristics - fidelity which measures the semantic similarity between the input text and generated text, and diversity, which measures the lexical dissimilarity between generated sentences. Many previous works (Prakash e"
N19-1363,W06-1673,0,0.0264359,"ash et al. (2016); Gupta et al. (2018); Li et al. (2018), we formulate the task of paraphrase generation as a sequence-to-sequence learning problem. Previous S EQ 2S EQ based approaches depend entirely on the standard crossentropy loss to produce semantically similar sentences and greedy decoding during generation. However, this does not guarantee lexical variety in the generated paraphrases. To incorporate some form of diversity, most prior approaches rely solely on top-k beam search sequences. The kbest list generated by standard beam search are a poor surrogate for the entire search space (Finkel et al., 2006). In fact, most of the sentences in the resulting set are structurally similar, differing only by punctuations or minor morphological variations. While being similar in the encoding scheme, our work adopts a different approach for the final decoding. We propose a framework which organi3611 Diversity Components Fidelity Components where , can , film , I , How , find that , that picture , .. I get , can I , Where can I : 3k Candidate Subsequences Where can I find that film? How can I get that picture? Rewards unique n-grams Source Sentence n-gram overlaps Where can I get that movie? Synonym (sim"
N19-1363,D13-1111,0,0.0287939,"ither been superseded by newer models or are not-directly applicable to our settings. However, most of these methods focus on the issue of generating semantically similar paraphrases, while paying little attention towards diversity. Diversity in paraphrasing models was first explored by (Gupta et al., 2018) where they propose to generate variations based on different samples from the latent space in a deep generative framework. Although diversity in paraphrasing models has not been explored extensively, methods have been proposed to address diversity in other NLP tasks (Li et al., 2016, 2015; Gimpel et al., 2013). Diverse beam search proposed by (Vijayakumar et al., 2018) generates k-diverse sequences by dividing the candidate subsequences at each time step into several groups and penalizing subsequences which are similar to prior groups. The most relevant to our approach is the method proposed by (Song et al., 2018) for neural conversation models where they incorporate diversity by using DPP to select diverse subsequences at each time step. Although their work is addressed in the scenario of neural conversation models, it could be naturally adapted to paraphrasing models and thus we use it as a basel"
N19-1363,N18-1170,0,0.0451162,"hich try to address the limitations of earlier traditional rule-based (McKeown, 1983) methods. Prakash et al. (2016) employ residual connections in LSTM to enhance the traditional sequence-to-sequence model. Gupta et al. (2018) provide a VAE (Kingma and Welling, 2013) based framework to improve the quality of generated paraphrases. Li et al. (2018) propose a reinforcement learning based model which uses pointer-generator (See et al., 2017) for generating paraphrases and an evaluator based on (Parikh et al., 2016) to penalize non-paraphrastic generations. Several other works (Cao et al., 2017; Iyyer et al., 2018) exist for paraphrasing, though they have either been superseded by newer models or are not-directly applicable to our settings. However, most of these methods focus on the issue of generating semantically similar paraphrases, while paying little attention towards diversity. Diversity in paraphrasing models was first explored by (Gupta et al., 2018) where they propose to generate variations based on different samples from the latent space in a deep generative framework. Although diversity in paraphrasing models has not been explored extensively, methods have been proposed to address diversity"
N19-1363,D14-1014,0,0.170364,"orate diversity by using DPP to select diverse subsequences at each time step. Although their work is addressed in the scenario of neural conversation models, it could be naturally adapted to paraphrasing models and thus we use it as a baseline. Submodular functions have been applied to a wide variety of problems in machine learning (Iyer and Bilmes, 2013; Jegelka and Bilmes, 2011; Krause and Guestrin, 2011; Kolmogorov and Zabih, 2002) and have recently attracted much attention in several NLP tasks including document summarization (Lin and Bilmes, 2011), data selection in machine translation (Kirchhoff and Bilmes, 2014) and goal-oriented chatbot training (Dimovski et al., 2018). However, their application to sequence generation is largely unexplored. Data augmentation is a technique for increasing the size of labeled training sets by leveraging task specific transformations which preserve class labels. While the technique is ubiquitous in the vision community (Krizhevsky et al., 2012; Ratner et al., 2017), data-augmentation in NLP is largely under-explored. Most current augmentation schemes involve thesaurus based synonym replacement (Zhang et al., 2015; Wang and Yang, 2015), and replacement by words with pa"
N19-1363,N18-2072,0,0.179317,"tbot training (Dimovski et al., 2018). However, their application to sequence generation is largely unexplored. Data augmentation is a technique for increasing the size of labeled training sets by leveraging task specific transformations which preserve class labels. While the technique is ubiquitous in the vision community (Krizhevsky et al., 2012; Ratner et al., 2017), data-augmentation in NLP is largely under-explored. Most current augmentation schemes involve thesaurus based synonym replacement (Zhang et al., 2015; Wang and Yang, 2015), and replacement by words with paradigmatic relations (Kobayashi, 2018). Both of these 3610 Algorithm 1: Greedy selection for submodular optiAlgorithm 2: DiPS Input: Input Sentence: Sin Max. decoding length: T Submodular objective: F No. of paraphrases required: k 1 Process Sin using the encoder of S EQ 2S EQ 2 Start the decoder with input symbol sos 3 t ← 0; P ← ∅ 4 while t < T do 5 Generate top 3k most probable subsequences 6 P ← Select k based on argmaxX⊆V (t) F(X) using Algorithm 1 7 t=t+1 8 end 9 return P mization (Cardinality constraint) Input: Ground Set: V Budget: k Submodular Function: F 1 S ← ∅ 2 N ← V 3 while |S |< k do 4 x∗ ← argmaxx∈N F(S ∪ {x}) 5 S"
N19-1363,D17-1126,0,0.0752547,"Missing"
N19-1363,N16-1014,0,0.474714,"i increase my body height ? D I PS (O URS ) – how could i increase my height ? – what should i do to increase my height ? – what are the fastest ways to increase my height ? – is there any proven method to increase height ? cases desirable, to produce lexically diverse ones. Diversity in paraphrase generation finds applications in text simplification (Nisioi et al., 2017; Xu et al., 2015), document summarization (Li et al., 2009; Nema et al., 2017), QA systems (Fader et al., 2013; Bernhard and Gurevych, 2008), data augmentation (Zhang et al., 2015; Wang and Yang, 2015), conversational agents (Li et al., 2016) and information retrieval (Anick and Tipirneni, 1999). Paraphrasing is the task of rephrasing a given text in multiple ways such that the semantics of the generated sentences remain unaltered. Paraphrasing Quality can be attributed to two key characteristics - fidelity which measures the semantic similarity between the input text and generated text, and diversity, which measures the lexical dissimilarity between generated sentences. Many previous works (Prakash et al., 2016; Gupta et al., 2018; Li et al., 2018) address the task of obtaining semantically similar paraphrases. While it is essent"
N19-1363,C02-1150,0,0.18845,"able to expect sentences that are structurally different to have lower degree of word/phrase alignment as compared to sentences with minor lexical variations. Edit distance (Levenshtein) is a widely accepted measure to determine such dissimilarities between two sentences. To incorporate this notion of diversity, a formulation in terms of edit distance seems like a natural fit for the problem. To do so, we use the coverage function which measures the similarity of the candidate sequences X with the ground set V (t) . The 3613 et al., 2018), Yahoo-L313 is used for intentclassification and TREC (Li and Roth, 2002) is used for question classification. Each dataset is balanced in terms of the number of samples per classes. coverage function is naturally monotone submodular and is defined as: D2 (X) = µ4 X xi ∈V (t) X R(xi , xj ) (9) xj ∈X where R(xi , xj ) is an alignment based similarity measure between two sequences xi and xj given by: R(xi , xj ) = 1 − EditDistance(xi , xj ) (10) |xi |+ |xj | Note that R(xi , xj ) will always lie in the range [0, 1]. Evidently, this method allows flexibility in terms of controlling diversity and fidelity. Our goal is to strike a balance between these two to obtain hig"
N19-1363,D18-1421,0,0.513957,"ata augmentation (Zhang et al., 2015; Wang and Yang, 2015), conversational agents (Li et al., 2016) and information retrieval (Anick and Tipirneni, 1999). Paraphrasing is the task of rephrasing a given text in multiple ways such that the semantics of the generated sentences remain unaltered. Paraphrasing Quality can be attributed to two key characteristics - fidelity which measures the semantic similarity between the input text and generated text, and diversity, which measures the lexical dissimilarity between generated sentences. Many previous works (Prakash et al., 2016; Gupta et al., 2018; Li et al., 2018) address the task of obtaining semantically similar paraphrases. While it is essential to produce paraphrases with high fidelity, it is equally important, and in many Equal Contribution This research was conducted during the author’s internship at the Indian Institute of Science, Bangalore. † – how do i increase body height ? – what do i do to increase my height ? Table 1: Sample paraphrases generated by beam search and DiPS (our method). DiPS offers lexically diverse paraphrases without compromising on fidelity. Introduction ∗ S OURCE R EFERENCE To obtain a set of multiple paraphrases, most o"
N19-1363,D11-1054,0,0.0628016,"Missing"
N19-1363,P11-1052,0,0.0356499,"et al., 2018) for neural conversation models where they incorporate diversity by using DPP to select diverse subsequences at each time step. Although their work is addressed in the scenario of neural conversation models, it could be naturally adapted to paraphrasing models and thus we use it as a baseline. Submodular functions have been applied to a wide variety of problems in machine learning (Iyer and Bilmes, 2013; Jegelka and Bilmes, 2011; Krause and Guestrin, 2011; Kolmogorov and Zabih, 2002) and have recently attracted much attention in several NLP tasks including document summarization (Lin and Bilmes, 2011), data selection in machine translation (Kirchhoff and Bilmes, 2014) and goal-oriented chatbot training (Dimovski et al., 2018). However, their application to sequence generation is largely unexplored. Data augmentation is a technique for increasing the size of labeled training sets by leveraging task specific transformations which preserve class labels. While the technique is ubiquitous in the vision community (Krizhevsky et al., 2012; Ratner et al., 2017), data-augmentation in NLP is largely under-explored. Most current augmentation schemes involve thesaurus based synonym replacement (Zhang"
N19-1363,J83-1001,0,0.796688,"We also compare against several possible diversity inducing schemes. 3. We demonstrate the utility of diverse paraphrases generated via DiPS as data augmentation schemes on multiple tasks such as intent and question classification. We have made DiPS’s source code available at https://github.com/malllabiisc/DiPS 2 Related Work Paraphrasing a given sentence is an important problem and numerous approaches have been proposed to address it. Recently sequence-tosequence based data-driven deep learning models have been proposed, which try to address the limitations of earlier traditional rule-based (McKeown, 1983) methods. Prakash et al. (2016) employ residual connections in LSTM to enhance the traditional sequence-to-sequence model. Gupta et al. (2018) provide a VAE (Kingma and Welling, 2013) based framework to improve the quality of generated paraphrases. Li et al. (2018) propose a reinforcement learning based model which uses pointer-generator (See et al., 2017) for generating paraphrases and an evaluator based on (Parikh et al., 2016) to penalize non-paraphrastic generations. Several other works (Cao et al., 2017; Iyyer et al., 2018) exist for paraphrasing, though they have either been superseded b"
N19-1363,P17-1098,0,0.0239871,"ave made the source code available. 1 B EAM S EARCH – how do i increase my height ? – how do i increase my body height ? – how do i increase the height ? – how would i increase my body height ? D I PS (O URS ) – how could i increase my height ? – what should i do to increase my height ? – what are the fastest ways to increase my height ? – is there any proven method to increase height ? cases desirable, to produce lexically diverse ones. Diversity in paraphrase generation finds applications in text simplification (Nisioi et al., 2017; Xu et al., 2015), document summarization (Li et al., 2009; Nema et al., 2017), QA systems (Fader et al., 2013; Bernhard and Gurevych, 2008), data augmentation (Zhang et al., 2015; Wang and Yang, 2015), conversational agents (Li et al., 2016) and information retrieval (Anick and Tipirneni, 1999). Paraphrasing is the task of rephrasing a given text in multiple ways such that the semantics of the generated sentences remain unaltered. Paraphrasing Quality can be attributed to two key characteristics - fidelity which measures the semantic similarity between the input text and generated text, and diversity, which measures the lexical dissimilarity between generated sentences"
N19-1363,P17-2014,0,0.0219167,"ssification and paraphrase recognition. In order to drive further research, we have made the source code available. 1 B EAM S EARCH – how do i increase my height ? – how do i increase my body height ? – how do i increase the height ? – how would i increase my body height ? D I PS (O URS ) – how could i increase my height ? – what should i do to increase my height ? – what are the fastest ways to increase my height ? – is there any proven method to increase height ? cases desirable, to produce lexically diverse ones. Diversity in paraphrase generation finds applications in text simplification (Nisioi et al., 2017; Xu et al., 2015), document summarization (Li et al., 2009; Nema et al., 2017), QA systems (Fader et al., 2013; Bernhard and Gurevych, 2008), data augmentation (Zhang et al., 2015; Wang and Yang, 2015), conversational agents (Li et al., 2016) and information retrieval (Anick and Tipirneni, 1999). Paraphrasing is the task of rephrasing a given text in multiple ways such that the semantics of the generated sentences remain unaltered. Paraphrasing Quality can be attributed to two key characteristics - fidelity which measures the semantic similarity between the input text and generated text, and"
N19-1363,P02-1040,0,0.10529,"Siamese adaptation of LSTM to measure quality between two sentences (Mueller and Thyagarajan, 2016) We also perform ablation testing to highlight the importance of each submodular component. Details can be found in the supplementary section. 5.4 Figure 2: Effect of varying the trade-off coefficient λ in DiPS on various diversity metrics on the Quora dataset. 5.3 Intrinsic Evaluation 1. Fidelity: To evaluate our method for fidelity of generated paraphrases, we use three machine translation metrics which have been shown to be suitable for paraphrase evaluation task (Wubben et al., 2010): BLEU (Papineni et al., 2002)(upto bigrams), METEOR (Banerjee and Lavie, 2005) and TERPlus (Snover et al., 2009). 2. Diversity: We report degree of diversity by calculating the number of distinct n-grams (n ∈ {1, 2, 3, 4}). The value is scaled by the number of generated tokens to avoid favoring long sequences. In addition to fidelity and diversity, we evaluate the efficacy of our method by using the generated paraphrases as augmented samples in the task of paraphrase recognition on the Quora-PR dataset. We perform experiments with multiple augmentation settings for the following classifiers: 1. LogReg: Simple Logistic Reg"
N19-1363,D16-1244,0,0.07257,"Missing"
N19-1363,C16-1275,0,0.695873,"al., 2013; Bernhard and Gurevych, 2008), data augmentation (Zhang et al., 2015; Wang and Yang, 2015), conversational agents (Li et al., 2016) and information retrieval (Anick and Tipirneni, 1999). Paraphrasing is the task of rephrasing a given text in multiple ways such that the semantics of the generated sentences remain unaltered. Paraphrasing Quality can be attributed to two key characteristics - fidelity which measures the semantic similarity between the input text and generated text, and diversity, which measures the lexical dissimilarity between generated sentences. Many previous works (Prakash et al., 2016; Gupta et al., 2018; Li et al., 2018) address the task of obtaining semantically similar paraphrases. While it is essential to produce paraphrases with high fidelity, it is equally important, and in many Equal Contribution This research was conducted during the author’s internship at the Indian Institute of Science, Bangalore. † – how do i increase body height ? – what do i do to increase my height ? Table 1: Sample paraphrases generated by beam search and DiPS (our method). DiPS offers lexically diverse paraphrases without compromising on fidelity. Introduction ∗ S OURCE R EFERENCE To obtain"
N19-1363,P17-1099,0,0.0525279,"ce is an important problem and numerous approaches have been proposed to address it. Recently sequence-tosequence based data-driven deep learning models have been proposed, which try to address the limitations of earlier traditional rule-based (McKeown, 1983) methods. Prakash et al. (2016) employ residual connections in LSTM to enhance the traditional sequence-to-sequence model. Gupta et al. (2018) provide a VAE (Kingma and Welling, 2013) based framework to improve the quality of generated paraphrases. Li et al. (2018) propose a reinforcement learning based model which uses pointer-generator (See et al., 2017) for generating paraphrases and an evaluator based on (Parikh et al., 2016) to penalize non-paraphrastic generations. Several other works (Cao et al., 2017; Iyyer et al., 2018) exist for paraphrasing, though they have either been superseded by newer models or are not-directly applicable to our settings. However, most of these methods focus on the issue of generating semantically similar paraphrases, while paying little attention towards diversity. Diversity in paraphrasing models was first explored by (Gupta et al., 2018) where they propose to generate variations based on different samples fro"
N19-1363,W09-0441,0,0.0212371,"agarajan, 2016) We also perform ablation testing to highlight the importance of each submodular component. Details can be found in the supplementary section. 5.4 Figure 2: Effect of varying the trade-off coefficient λ in DiPS on various diversity metrics on the Quora dataset. 5.3 Intrinsic Evaluation 1. Fidelity: To evaluate our method for fidelity of generated paraphrases, we use three machine translation metrics which have been shown to be suitable for paraphrase evaluation task (Wubben et al., 2010): BLEU (Papineni et al., 2002)(upto bigrams), METEOR (Banerjee and Lavie, 2005) and TERPlus (Snover et al., 2009). 2. Diversity: We report degree of diversity by calculating the number of distinct n-grams (n ∈ {1, 2, 3, 4}). The value is scaled by the number of generated tokens to avoid favoring long sequences. In addition to fidelity and diversity, we evaluate the efficacy of our method by using the generated paraphrases as augmented samples in the task of paraphrase recognition on the Quora-PR dataset. We perform experiments with multiple augmentation settings for the following classifiers: 1. LogReg: Simple Logistic Regression model. We use a set of hand-crafted features, the deData-Augmentation We ev"
N19-1363,D15-1306,0,0.103525,"– how do i increase the height ? – how would i increase my body height ? D I PS (O URS ) – how could i increase my height ? – what should i do to increase my height ? – what are the fastest ways to increase my height ? – is there any proven method to increase height ? cases desirable, to produce lexically diverse ones. Diversity in paraphrase generation finds applications in text simplification (Nisioi et al., 2017; Xu et al., 2015), document summarization (Li et al., 2009; Nema et al., 2017), QA systems (Fader et al., 2013; Bernhard and Gurevych, 2008), data augmentation (Zhang et al., 2015; Wang and Yang, 2015), conversational agents (Li et al., 2016) and information retrieval (Anick and Tipirneni, 1999). Paraphrasing is the task of rephrasing a given text in multiple ways such that the semantics of the generated sentences remain unaltered. Paraphrasing Quality can be attributed to two key characteristics - fidelity which measures the semantic similarity between the input text and generated text, and diversity, which measures the lexical dissimilarity between generated sentences. Many previous works (Prakash et al., 2016; Gupta et al., 2018; Li et al., 2018) address the task of obtaining semanticall"
N19-1363,W10-4223,0,0.545486,"Missing"
N19-1363,Q15-1021,0,0.0315735,"hrase recognition. In order to drive further research, we have made the source code available. 1 B EAM S EARCH – how do i increase my height ? – how do i increase my body height ? – how do i increase the height ? – how would i increase my body height ? D I PS (O URS ) – how could i increase my height ? – what should i do to increase my height ? – what are the fastest ways to increase my height ? – is there any proven method to increase height ? cases desirable, to produce lexically diverse ones. Diversity in paraphrase generation finds applications in text simplification (Nisioi et al., 2017; Xu et al., 2015), document summarization (Li et al., 2009; Nema et al., 2017), QA systems (Fader et al., 2013; Bernhard and Gurevych, 2008), data augmentation (Zhang et al., 2015; Wang and Yang, 2015), conversational agents (Li et al., 2016) and information retrieval (Anick and Tipirneni, 1999). Paraphrasing is the task of rephrasing a given text in multiple ways such that the semantics of the generated sentences remain unaltered. Paraphrasing Quality can be attributed to two key characteristics - fidelity which measures the semantic similarity between the input text and generated text, and diversity, which m"
P10-1149,C92-2082,0,0.142165,"Missing"
P10-1149,D09-1098,0,0.0267187,"similar class labels. In other words, the label smoothness assumption (see Section 2.2) holds on such graphs. We applied the same graph construction process on a subset of the Freebase dataset consisting of topics from 18 randomly selected domains: astronomy, automotive, biology, book, business, 1476 chemistry, comic books, computer, film, food, geography, location, people, religion, spaceflight, tennis, travel, and wine. The topics in this subset were further filtered so that only cell-value nodes with frequency 10 or more were retained. We call the resulting graph Freebase-1 (see Table 1). Pantel et al. (2009) have made available a set of gold class-instance pairs derived from Wikipedia, which is downloadable from http://ow.ly/13B57. From this set, we selected all classes which had more than 10 instances overlapping with the Freebase graph constructed above. This resulted in 23 classes, which along with their overlapping instances were used as the gold standard set for the experiments in this section. Experimental results with 2 and 10 seeds (labeled nodes) per class are shown in Figure 3. From the figure, we see that that LP-ZGL and Adsorption performed comparably on this dataset, with MAD signifi"
P10-1149,D09-1025,0,0.4392,"ars (Hearst, 1992; Riloff and Jones, 1999; Etzioni et al., 2005; Talukdar et al., 2006; Van Durme and Pas¸ca, 2008). Starting with a few seed instances for some classes, these methods, through analysis of unstructured text, extract new instances of the same class. This line of work has evolved to incorporate ideas from graph-based semi-supervised learning in extraction from semi-structured text (Wang and Cohen, 2007), and in combining extractions from free text and from structured sources (Talukdar et al., 2008). The benefits of combining multiple sources have also been demonstrated recently (Pennacchiotti and Pantel, 2009). We make the following contributions: Graph-based semi-supervised learning (SSL) algorithms have been successfully used to extract class-instance pairs from large unstructured and structured text collections. However, a careful comparison of different graph-based SSL algorithms on that task has been lacking. We compare three graph-based SSL algorithms for class-instance acquisition on a variety of graphs constructed from different domains. We find that the recently proposed MAD algorithm is the most effective. We also show that class-instance extraction can be significantly improved by adding"
P10-1149,W09-3209,0,0.0146352,"ised edge weights; and R is an n × m matrix of per-node label prior, if any, with Rl representing the lth column of R. As in Adsorption, MAD allows labels on seed nodes to change. In case of MAD, the three random-walk cont , and pabnd , defined by probabilities, pinj v , pv v Adsorption on each node are folded inside the ma0 trices S, L , and R, respectively. The optimization problem in (3) can be solved with an efficient iterative algorithm described in detail by Talukdar and Crammer (2009). These three algorithms are all easily parallelizable in a MapReduce framework (Talukdar et al., 2008; Rao and Yarowsky, 2009), which makes them suitable for SSL on large datasets. Additionally, all three algorithms have similar space and time complexity. Gender ··· Male Male Male ··· LP-ZGL Adsorption MAD 0.725 0.65 0.575 0.5 23 x 2 23 x 10 Amount of Supervision (# classes x seeds per class) Figure 3: Comparison of three graph transduction methods on a graph constructed from the Freebase dataset (see Section 3.1), with 23 classes. All results are averaged over 4 random trials. In each group, MAD is the rightmost bar. Freebase (Metaweb Technologies, 2009)2 is a large collaborative knowledge base. The knowledge base h"
P10-1149,W06-2919,1,0.405186,"Missing"
P10-1149,D08-1061,1,0.724626,"To overcome these difficulties, seed-based information extraction methods have been developed over the years (Hearst, 1992; Riloff and Jones, 1999; Etzioni et al., 2005; Talukdar et al., 2006; Van Durme and Pas¸ca, 2008). Starting with a few seed instances for some classes, these methods, through analysis of unstructured text, extract new instances of the same class. This line of work has evolved to incorporate ideas from graph-based semi-supervised learning in extraction from semi-structured text (Wang and Cohen, 2007), and in combining extractions from free text and from structured sources (Talukdar et al., 2008). The benefits of combining multiple sources have also been demonstrated recently (Pennacchiotti and Pantel, 2009). We make the following contributions: Graph-based semi-supervised learning (SSL) algorithms have been successfully used to extract class-instance pairs from large unstructured and structured text collections. However, a careful comparison of different graph-based SSL algorithms on that task has been lacking. We compare three graph-based SSL algorithms for class-instance acquisition on a variety of graphs constructed from different domains. We find that the recently proposed MAD al"
P10-2069,P07-1056,0,0.0179083,"Inference Driven Metric Learning (IDML) Input: instances X, training labels Y , training instance indicator S, label entropy threshold β, neighborhood size k Output: Mahalanobis distance parameter A ˆ ← Y , Sˆ ← S 1: Y 2: repeat ˆ Yˆ ) 3: A ← M ETRIC L EARNER(X, S, 4: W ← C ONSTRUCT K NN G RAPH(X, A, k) 0 ˆ Yˆ ) 5: Yˆ ← G RAPH L ABEL I NF(W, S, 0 ˆ β) 6: U ← S ELECT L OW E NT I NST(Yˆ , S, 0 ˆ ˆ ˆ 7: Y ← Y + UY 8: Sˆ ← Sˆ + U 9: until convergence (i.e., Uii = 0, ∀i) 10: return A first four datasets – Electronics, Books, Kitchen, and DVDs – are from the sentiment domain and previously used in (Blitzer et al., 2007). WebKB is a text classification dataset derived from (Subramanya and Bilmes, 2008). For details regarding features and data pre-processing, we refer the reader to the origin of these datasets cited above. One extra preprocessing that we did was that we only considered features which occurred more 20 times in the entire dataset to make the problem more computationally tractable and also since the infrequently occurring features usually contribute noise. We use classification error (lower is better) as the evaluation metric. We experiment with the following ways of estimating transformation mat"
P10-2069,D08-1114,0,0.168101,"Y , training instance indicator S, label entropy threshold β, neighborhood size k Output: Mahalanobis distance parameter A ˆ ← Y , Sˆ ← S 1: Y 2: repeat ˆ Yˆ ) 3: A ← M ETRIC L EARNER(X, S, 4: W ← C ONSTRUCT K NN G RAPH(X, A, k) 0 ˆ Yˆ ) 5: Yˆ ← G RAPH L ABEL I NF(W, S, 0 ˆ β) 6: U ← S ELECT L OW E NT I NST(Yˆ , S, 0 ˆ ˆ ˆ 7: Y ← Y + UY 8: Sˆ ← Sˆ + U 9: until convergence (i.e., Uii = 0, ∀i) 10: return A first four datasets – Electronics, Books, Kitchen, and DVDs – are from the sentiment domain and previously used in (Blitzer et al., 2007). WebKB is a text classification dataset derived from (Subramanya and Bilmes, 2008). For details regarding features and data pre-processing, we refer the reader to the origin of these datasets cited above. One extra preprocessing that we did was that we only considered features which occurred more 20 times in the entire dataset to make the problem more computationally tractable and also since the infrequently occurring features usually contribute noise. We use classification error (lower is better) as the evaluation metric. We experiment with the following ways of estimating transformation matrix P : Original2 : We set P = I, where I is the d × d identity matrix. Hence, the"
P12-4006,D08-1114,0,0.0323609,"ne of work, researchers have started to realize that graphs provide a natural way to represent data in a variety of domains. Graph-based SSL algorithms, which bring together these two lines of work, have been shown to outperform the state-ofthe-art in many applications in speech processing, computer vision and NLP. In particular, recent NLP research has successfully used graph-based SSL algorithms for PoS tagging (Subramanya et al., 2010), semantic parsing (Das and Smith, 2011), knowledge acquisition (Talukdar et al., 2008), sentiment analysis (Goldberg and Zhu, 2006) and text categorization (Subramanya and Bilmes, 2008). Familiarity with semi-supervised learning and graph-based methods will not be assumed, and the necessary background will be provided. Examples from NLP tasks will be used throughout the tutorial to convey the necessary concepts. At the end of this tutorial, the attendee will walk away with the following: • An in-depth knowledge of the current state-ofthe-art in graph-based SSL algorithms, and the ability to implement them. • The ability to decide on the suitability of graph-based SSL methods for a problem. • Familiarity with different NLP tasks where graph-based SSL methods have been success"
P12-4006,D10-1017,0,0.0745544,"Missing"
P12-4006,D08-1061,1,0.88386,"Missing"
P12-4006,P11-1144,0,\N,Missing
P12-4006,W06-3808,0,\N,Missing
P14-1046,D13-1202,0,0.198944,"Missing"
P14-1046,W11-2503,0,0.0388722,"Missing"
P14-1046,J90-1003,0,0.357969,"Missing"
P14-1046,W13-3510,1,0.837904,"data. 3.1 Related Work Perhaps the most well known related approach to joining data sources is Canonical Correlation Analysis (CCA) (Hotelling, 1936), which has been applied to brain activation data in the past (Rustandi et al., 2009). CCA seeks two linear transformations that maximally correlate two data sets in the transformed form. CCA requires that the data sources be paired (all rows in the corpus data must have a corresponding brain data), as correlation between points is integral to the objective. 4 Data 4.1 Corpus Data The corpus statistics used here are the downloadable vectors from Fyshe et al. (2013)3 . They are compiled from a 16 billion word subset of ClueWeb09 (Callan and Hoy, 2009) and contain two types of corpus features: dependency and document features, found to be complimentary for 3 http://www.cs.cmu.edu/˜afyshe/papers/ conll2013/ 492 each question for each word on a scale of 1-5. At least 3 respondents answered each question and the median score was used. This gives us a semantic representation of each of the 60 words in a 218-dimensional behavioral space. Because we required answers to each of the questions for all words, we do not have the problems of sparsity that exist for f"
P14-1046,C12-1118,1,0.753644,"g, the semantic content of each word will be necessarily activated in the mind, and so in patterns of activity over individual neurons. In principle then, brain activity could replace corpus data as input to a VSM, and contemporary imaging techniques allow us to attempt this. Functional Magnetic Resonance Imaging (fMRI) and Magnetoencephalography (MEG) are two brain activation recording technologies that measure neuronal activation in aggregate, and have been shown to have a predictive relationship with models of word meaning (Mitchell et al., 2008; Palatucci et al., 2009; Sudre et al., 2012; Murphy et al., 2012b).1 If brain activation data encodes semantics, we theorized that including brain data in a model of semantics could result in a model more consistent with semantic ground truth. However, the inclusion of brain data will only improve a text-based model if brain data contains semantic information not readily available in the corpus. In addition, if a semantic test involves another subject’s brain activation data, performance can improve only if the additional semantic information is consistent across brains. Of course, brains differ in shape, size and in connectivity, so additional information"
P14-1046,S12-1019,1,0.86172,"g, the semantic content of each word will be necessarily activated in the mind, and so in patterns of activity over individual neurons. In principle then, brain activity could replace corpus data as input to a VSM, and contemporary imaging techniques allow us to attempt this. Functional Magnetic Resonance Imaging (fMRI) and Magnetoencephalography (MEG) are two brain activation recording technologies that measure neuronal activation in aggregate, and have been shown to have a predictive relationship with models of word meaning (Mitchell et al., 2008; Palatucci et al., 2009; Sudre et al., 2012; Murphy et al., 2012b).1 If brain activation data encodes semantics, we theorized that including brain data in a model of semantics could result in a model more consistent with semantic ground truth. However, the inclusion of brain data will only improve a text-based model if brain data contains semantic information not readily available in the corpus. In addition, if a semantic test involves another subject’s brain activation data, performance can improve only if the additional semantic information is consistent across brains. Of course, brains differ in shape, size and in connectivity, so additional information"
P14-1046,D12-1130,0,0.0185333,"Missing"
P14-1046,P13-1056,0,0.0240867,"Missing"
P18-1012,N16-1054,0,0.493005,"lem of learning embeddings for Knowledge Graphs has received significant attention in recent years, with several methods being proposed (Bordes et al., 2013; Lin et al., 2015; Nguyen et al., 2016; Nickel et al., 2016; Trouillon et al., 2016). These methods represent entities and relations in a KG as vectors in high dimensional space. These vectors can then be used for various tasks, such as, link prediction, entity classification etc. Starting with TransE (Bordes et al., 2013), there have been many KG embedding methods such as TransH (Wang et al., 2014), TransR (Lin et al., 2015) and STransE (Nguyen et al., 2016) which represent relations as translation vectors from head entities to tail entities. These are additive models, as the vectors interact via addition and subtraction. Other KG embedding models, such as, DistMult (Yang et al., 2014), HolE (Nickel et al., 2016), and ComplEx (Trouillon et al., 2016) are multiplicative where entityrelation-entity triple likelihood is quantified by a multiplicative score function. All these methods employ a score function for distinguishing correct triples from incorrect ones. In spite of the existence of many KG embedding methods, our understanding of the geometr"
P18-1012,D15-1174,0,0.0569347,"ededges represent relationships among entities. Recent research in this area has resulted in the development of several large KGs, such as NELL (Mitchell et al., 2015), YAGO (Suchanek et al., 2007), and Freebase (Bollacker et al., 2008), among others. These KGs contain thousands of predicates (e.g., person, city, mayorOf(person, city), etc.), and millions of triples involving such 122 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 122–131 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics by (Toutanova et al., 2015). In this paper, we study the effect of the number of negative samples on KG embedding geometry as well as performance. In addition to the additive and multiplicative KG embedding methods already mentioned in Section 1, there is another set of methods where the entity and relation vectors interact via a neural network. Examples of methods in this category include NTN (Socher et al., 2013), CONV (Toutanova et al., 2015), ConvE (Dettmers et al., 2017), R-GCN (Schlichtkrull et al., 2017), ERMLP (Dong et al., 2014) and ER-MLP-2n (Ravishankar et al., 2017). Due to space limitations, in this paper w"
P18-1012,D17-1308,0,0.348093,"s to tail entities. These are additive models, as the vectors interact via addition and subtraction. Other KG embedding models, such as, DistMult (Yang et al., 2014), HolE (Nickel et al., 2016), and ComplEx (Trouillon et al., 2016) are multiplicative where entityrelation-entity triple likelihood is quantified by a multiplicative score function. All these methods employ a score function for distinguishing correct triples from incorrect ones. In spite of the existence of many KG embedding methods, our understanding of the geometry and structure of such embeddings is very shallow. A recent work (Mimno and Thompson, 2017) analyzed the geometry of word embeddings. However, the problem of analyzing geometry of KG embeddings is still unexplored – we fill this important gap. In this paper, we analyze the geometry of such vectors in terms of their lengths and conicity, which, as defined in Section 4, describes their positions and orientations in the vector space. We later study the effects of model type and training hyperparameters on the geometry of KG embeddings and correlate geometry with performance. Knowledge Graph (KG) embedding has emerged as a very active area of research over the last few years, resulting"
P18-1146,D13-1178,0,0.141901,"Attias and Cohen, 2015; Nimishakavi et al., 2016). McDonald et al. (2005) and Peng et al. (2017) extract n-ary relations from Biomedical documents, but do not induce the schema, i.e., type signature of the n-ary relations. There has been significant amount of work on Semantic Role Labeling (Lang and Lapata, 2011; Titov and Khoddam, 2015; Roth and Lapata, 2016), which can be considered as nary relation extraction. However, we are interested in inducing the schemata, i.e., the type signature of these relations. Event Schema Induction is the problem of inducing schemata for events in the corpus (Balasubramanian et al., 2013; Chambers, 2013; Nguyen et al., 2015). Recently, a model for event representations is proposed in (Weber et al., 2018). Cheung et al. (2013) propose a probabilistic model for inducing frames from text. Their notion of frame is closer to that of scripts (Schank and Abelson, 1977). Script learning is the process of automatically inferring sequence of events from text (Mooney and DeJong, 1985). There is a fair amount of recent work in statistical script learning (Pichotta and Mooney, 2016), (Pichotta and Mooney, 2014). While script learning deals with the sequence of events, we try to find the s"
P18-1146,D13-1185,0,0.16418,"akavi et al., 2016). McDonald et al. (2005) and Peng et al. (2017) extract n-ary relations from Biomedical documents, but do not induce the schema, i.e., type signature of the n-ary relations. There has been significant amount of work on Semantic Role Labeling (Lang and Lapata, 2011; Titov and Khoddam, 2015; Roth and Lapata, 2016), which can be considered as nary relation extraction. However, we are interested in inducing the schemata, i.e., the type signature of these relations. Event Schema Induction is the problem of inducing schemata for events in the corpus (Balasubramanian et al., 2013; Chambers, 2013; Nguyen et al., 2015). Recently, a model for event representations is proposed in (Weber et al., 2018). Cheung et al. (2013) propose a probabilistic model for inducing frames from text. Their notion of frame is closer to that of scripts (Schank and Abelson, 1977). Script learning is the process of automatically inferring sequence of events from text (Mooney and DeJong, 1985). There is a fair amount of recent work in statistical script learning (Pichotta and Mooney, 2016), (Pichotta and Mooney, 2014). While script learning deals with the sequence of events, we try to find the schemata of relat"
P18-1146,D14-1165,0,0.0283729,"odel for scripts, frames and events. Their model tries to capture all levels of Minsky Frame structure (Minsky, 1974), however we work with the surface semantic frames. Tensor and Matrix Factorizations: Matrix factorization and joint tensor-matrix factorizations have been used for the problem of predicting links in the Universal Schema setting (Riedel et al., 2013; Singh et al., 2015). Chen et al. (2015) use matrix factorizations for the problem of finding semantic slots for unsupervised spoken language understanding. Tensor factorization methods are also used in factorizing knowledge graphs (Chang et al., 2014; Nickel et al., 2012). Joint matrix and tensor factorization frameworks, where the matrix provides additional information, is proposed in (Acar et al., 2013) and (Wang et al., 2015). These models are based on PARAFAC (Harshman, 1970), a tensor factorization model which approximates the given tensor as a sum of rank1 tensors. A boolean Tucker decomposition for discovering facts is proposed in (Erdos and Miettinen, 2013). In this paper, we use a modified version (Tucker2) of Tucker decomposition (Tucker, 1963). RESCAL (Nickel et al., 2011) is a simplified Tucker model suitable for relational le"
P18-1146,P15-1047,0,0.0260306,"ney, 2016), (Pichotta and Mooney, 2014). While script learning deals with the sequence of events, we try to find the schemata of relations at a corpus level. Ferraro and Durme (2016) propose a unified Bayesian model for scripts, frames and events. Their model tries to capture all levels of Minsky Frame structure (Minsky, 1974), however we work with the surface semantic frames. Tensor and Matrix Factorizations: Matrix factorization and joint tensor-matrix factorizations have been used for the problem of predicting links in the Universal Schema setting (Riedel et al., 2013; Singh et al., 2015). Chen et al. (2015) use matrix factorizations for the problem of finding semantic slots for unsupervised spoken language understanding. Tensor factorization methods are also used in factorizing knowledge graphs (Chang et al., 2014; Nickel et al., 2012). Joint matrix and tensor factorization frameworks, where the matrix provides additional information, is proposed in (Acar et al., 2013) and (Wang et al., 2015). These models are based on PARAFAC (Harshman, 1970), a tensor factorization model which approximates the given tensor as a sum of rank1 tensors. A boolean Tucker decomposition for discovering facts is propo"
P18-1146,N13-1104,0,0.0239168,"ut do not induce the schema, i.e., type signature of the n-ary relations. There has been significant amount of work on Semantic Role Labeling (Lang and Lapata, 2011; Titov and Khoddam, 2015; Roth and Lapata, 2016), which can be considered as nary relation extraction. However, we are interested in inducing the schemata, i.e., the type signature of these relations. Event Schema Induction is the problem of inducing schemata for events in the corpus (Balasubramanian et al., 2013; Chambers, 2013; Nguyen et al., 2015). Recently, a model for event representations is proposed in (Weber et al., 2018). Cheung et al. (2013) propose a probabilistic model for inducing frames from text. Their notion of frame is closer to that of scripts (Schank and Abelson, 1977). Script learning is the process of automatically inferring sequence of events from text (Mooney and DeJong, 1985). There is a fair amount of recent work in statistical script learning (Pichotta and Mooney, 2016), (Pichotta and Mooney, 2014). While script learning deals with the sequence of events, we try to find the schemata of relations at a corpus level. Ferraro and Durme (2016) propose a unified Bayesian model for scripts, frames and events. Their model"
P18-1146,P05-1061,0,0.273408,"is needed for HRSI, rather than factorizing the higher-order tensor. Further, we discuss the proposed TFBA framework in Section 3.2. In Section 4, we demonstrate the effectiveness of the proposed approach using multiple real world datasets. We conclude with a brief summary in Section 5. 2 Related Work In this section, we discuss related works in two broad areas: schema induction, and tensor and matrix factorizations. Schema Induction: Most work on inducing schemata for relations has been in the binary setting (Mohamed et al., 2011; Movshovitz-Attias and Cohen, 2015; Nimishakavi et al., 2016). McDonald et al. (2005) and Peng et al. (2017) extract n-ary relations from Biomedical documents, but do not induce the schema, i.e., type signature of the n-ary relations. There has been significant amount of work on Semantic Role Labeling (Lang and Lapata, 2011; Titov and Khoddam, 2015; Roth and Lapata, 2016), which can be considered as nary relation extraction. However, we are interested in inducing the schemata, i.e., the type signature of these relations. Event Schema Induction is the problem of inducing schemata for events in the corpus (Balasubramanian et al., 2013; Chambers, 2013; Nguyen et al., 2015). Recen"
P18-1146,D11-1134,0,0.0469633,"Missing"
P18-1146,1985.tmi-1.17,0,0.386654,"lation extraction. However, we are interested in inducing the schemata, i.e., the type signature of these relations. Event Schema Induction is the problem of inducing schemata for events in the corpus (Balasubramanian et al., 2013; Chambers, 2013; Nguyen et al., 2015). Recently, a model for event representations is proposed in (Weber et al., 2018). Cheung et al. (2013) propose a probabilistic model for inducing frames from text. Their notion of frame is closer to that of scripts (Schank and Abelson, 1977). Script learning is the process of automatically inferring sequence of events from text (Mooney and DeJong, 1985). There is a fair amount of recent work in statistical script learning (Pichotta and Mooney, 2016), (Pichotta and Mooney, 2014). While script learning deals with the sequence of events, we try to find the schemata of relations at a corpus level. Ferraro and Durme (2016) propose a unified Bayesian model for scripts, frames and events. Their model tries to capture all levels of Minsky Frame structure (Minsky, 1974), however we work with the surface semantic frames. Tensor and Matrix Factorizations: Matrix factorization and joint tensor-matrix factorizations have been used for the problem of pred"
P18-1146,P15-1140,0,0.175394,"2. In Section 3.1, we first motivate why a back-off strategy is needed for HRSI, rather than factorizing the higher-order tensor. Further, we discuss the proposed TFBA framework in Section 3.2. In Section 4, we demonstrate the effectiveness of the proposed approach using multiple real world datasets. We conclude with a brief summary in Section 5. 2 Related Work In this section, we discuss related works in two broad areas: schema induction, and tensor and matrix factorizations. Schema Induction: Most work on inducing schemata for relations has been in the binary setting (Mohamed et al., 2011; Movshovitz-Attias and Cohen, 2015; Nimishakavi et al., 2016). McDonald et al. (2005) and Peng et al. (2017) extract n-ary relations from Biomedical documents, but do not induce the schema, i.e., type signature of the n-ary relations. There has been significant amount of work on Semantic Role Labeling (Lang and Lapata, 2011; Titov and Khoddam, 2015; Roth and Lapata, 2016), which can be considered as nary relation extraction. However, we are interested in inducing the schemata, i.e., the type signature of these relations. Event Schema Induction is the problem of inducing schemata for events in the corpus (Balasubramanian et al."
P18-1146,C12-1118,1,0.744491,"dal, Wimbledon), 10i and h(Federer, Win, Nadal, Australian Open), 5i will be aggregated to form a triple h(Federer, Win, Nadal), 15i. • X 2 ∈ Rn+1 ×n3 ×m is constructed out of the tuples in T by dropping the object argument + λa kAk2F + λb kBk2F + λc kCk2F , (1) where, 2 f (X i , G i , P, Q) = X i − G i ×1 P ×2 Q ×3 I F A ∈ Rn+1 ×r1 , B ∈ Rn+2 ×r2 , C ∈ Rn+3 ×r3 G 1 ∈ Rr+2 ×r3 ×m , G 2 ∈ Rr+1 ×r3 ×m , G 3 ∈ Rr+1 ×r2 ×m . We enforce non-negativity constraints on the matrices A, B, C and the core tensors G i (i ∈ {1, 2, 3}). Non-negativity is essential for learning interpretable latent factors (Murphy et al., 2012). 1578 Each slice of the core tensor G 3 corresponds to one of the m relations. Each cell in a slice corresponds to an induced schema in terms of the latent factors from matrices A and B. In other 3 words, Gi,j,k is an induced binary schema for relation k involving induced categories represented by columns Ai and Bj . Cells in G 1 and G 2 may be interpreted accordingly. We derive non-negative multiplicative updates for A, B and C following the NMF updating rules given in (Lee and Seung, 2000). For the update of A, we consider the mode-1 matricization of first and the second term in Equation 1"
P18-1146,P15-1019,0,0.033862,"Missing"
P18-1146,D16-1040,1,0.739244,"interest go beyond two entities. For example, in the sports domain, one may be interested in beliefs of the form win(Roger Federer, Nadal, Wimbledon, London), which is an instance of the high-order (or n-ary) relation win whose schema is given by win(WinningPlayer, OpponentPlayer, Tournament, Location). We refer to the problem of inducing such relation schemata involving multiple arguments as Higher-order Relation Schema Induction (HRSI). In spite of its importance, HRSI is mostly unexplored. Recently, tensor factorization-based methods have been proposed for binary relation schema induction (Nimishakavi et al., 2016), with gains in both speed and accuracy over previously proposed generative models. To the best of our knowledge, tensor factorization methods have not been used for HRSI. We address this gap in this paper. Due to data sparsity, straightforward adaptation of tensor factorization from (Nimishakavi et al., 2016) to HRSI is not feasible, as we shall see in Section 3.1. We overcome this challenge in this paper, and make the following contributions. Relation Schema Induction (RSI) is the problem of identifying type signatures of arguments of relations from unlabeled text. Most of the previous work"
P18-1146,Q17-1008,0,0.0139207,"than factorizing the higher-order tensor. Further, we discuss the proposed TFBA framework in Section 3.2. In Section 4, we demonstrate the effectiveness of the proposed approach using multiple real world datasets. We conclude with a brief summary in Section 5. 2 Related Work In this section, we discuss related works in two broad areas: schema induction, and tensor and matrix factorizations. Schema Induction: Most work on inducing schemata for relations has been in the binary setting (Mohamed et al., 2011; Movshovitz-Attias and Cohen, 2015; Nimishakavi et al., 2016). McDonald et al. (2005) and Peng et al. (2017) extract n-ary relations from Biomedical documents, but do not induce the schema, i.e., type signature of the n-ary relations. There has been significant amount of work on Semantic Role Labeling (Lang and Lapata, 2011; Titov and Khoddam, 2015; Roth and Lapata, 2016), which can be considered as nary relation extraction. However, we are interested in inducing the schemata, i.e., the type signature of these relations. Event Schema Induction is the problem of inducing schemata for events in the corpus (Balasubramanian et al., 2013; Chambers, 2013; Nguyen et al., 2015). Recently, a model for event"
P18-1146,E14-1024,0,0.0136702,"hema Induction is the problem of inducing schemata for events in the corpus (Balasubramanian et al., 2013; Chambers, 2013; Nguyen et al., 2015). Recently, a model for event representations is proposed in (Weber et al., 2018). Cheung et al. (2013) propose a probabilistic model for inducing frames from text. Their notion of frame is closer to that of scripts (Schank and Abelson, 1977). Script learning is the process of automatically inferring sequence of events from text (Mooney and DeJong, 1985). There is a fair amount of recent work in statistical script learning (Pichotta and Mooney, 2016), (Pichotta and Mooney, 2014). While script learning deals with the sequence of events, we try to find the schemata of relations at a corpus level. Ferraro and Durme (2016) propose a unified Bayesian model for scripts, frames and events. Their model tries to capture all levels of Minsky Frame structure (Minsky, 1974), however we work with the surface semantic frames. Tensor and Matrix Factorizations: Matrix factorization and joint tensor-matrix factorizations have been used for the problem of predicting links in the Universal Schema setting (Riedel et al., 2013; Singh et al., 2015). Chen et al. (2015) use matrix factoriza"
P18-1146,N13-1008,0,0.0389052,"tistical script learning (Pichotta and Mooney, 2016), (Pichotta and Mooney, 2014). While script learning deals with the sequence of events, we try to find the schemata of relations at a corpus level. Ferraro and Durme (2016) propose a unified Bayesian model for scripts, frames and events. Their model tries to capture all levels of Minsky Frame structure (Minsky, 1974), however we work with the surface semantic frames. Tensor and Matrix Factorizations: Matrix factorization and joint tensor-matrix factorizations have been used for the problem of predicting links in the Universal Schema setting (Riedel et al., 2013; Singh et al., 2015). Chen et al. (2015) use matrix factorizations for the problem of finding semantic slots for unsupervised spoken language understanding. Tensor factorization methods are also used in factorizing knowledge graphs (Chang et al., 2014; Nickel et al., 2012). Joint matrix and tensor factorization frameworks, where the matrix provides additional information, is proposed in (Acar et al., 2013) and (Wang et al., 2015). These models are based on PARAFAC (Harshman, 1970), a tensor factorization model which approximates the given tensor as a sum of rank1 tensors. A boolean Tucker dec"
P18-1146,P16-1113,0,0.015804,"Related Work In this section, we discuss related works in two broad areas: schema induction, and tensor and matrix factorizations. Schema Induction: Most work on inducing schemata for relations has been in the binary setting (Mohamed et al., 2011; Movshovitz-Attias and Cohen, 2015; Nimishakavi et al., 2016). McDonald et al. (2005) and Peng et al. (2017) extract n-ary relations from Biomedical documents, but do not induce the schema, i.e., type signature of the n-ary relations. There has been significant amount of work on Semantic Role Labeling (Lang and Lapata, 2011; Titov and Khoddam, 2015; Roth and Lapata, 2016), which can be considered as nary relation extraction. However, we are interested in inducing the schemata, i.e., the type signature of these relations. Event Schema Induction is the problem of inducing schemata for events in the corpus (Balasubramanian et al., 2013; Chambers, 2013; Nguyen et al., 2015). Recently, a model for event representations is proposed in (Weber et al., 2018). Cheung et al. (2013) propose a probabilistic model for inducing frames from text. Their notion of frame is closer to that of scripts (Schank and Abelson, 1977). Script learning is the process of automatically infe"
P18-1146,W15-1519,0,0.0409007,"Missing"
P18-1146,N15-1001,0,0.0157028,"f summary in Section 5. 2 Related Work In this section, we discuss related works in two broad areas: schema induction, and tensor and matrix factorizations. Schema Induction: Most work on inducing schemata for relations has been in the binary setting (Mohamed et al., 2011; Movshovitz-Attias and Cohen, 2015; Nimishakavi et al., 2016). McDonald et al. (2005) and Peng et al. (2017) extract n-ary relations from Biomedical documents, but do not induce the schema, i.e., type signature of the n-ary relations. There has been significant amount of work on Semantic Role Labeling (Lang and Lapata, 2011; Titov and Khoddam, 2015; Roth and Lapata, 2016), which can be considered as nary relation extraction. However, we are interested in inducing the schemata, i.e., the type signature of these relations. Event Schema Induction is the problem of inducing schemata for events in the corpus (Balasubramanian et al., 2013; Chambers, 2013; Nguyen et al., 2015). Recently, a model for event representations is proposed in (Weber et al., 2018). Cheung et al. (2013) propose a probabilistic model for inducing frames from text. Their notion of frame is closer to that of scripts (Schank and Abelson, 1977). Script learning is the proce"
P18-1149,P12-1011,0,0.301349,"nalysis of historical text (de Jong et al., 2005a), among others. In all such tasks, the document date is assumed to be available and also accurate – a strong assumption, especially for arbitrary documents from the Web. Thus, there is a need to automatically predict the date of a document based on its content. This problem is referred to as Document Dating. Initial attempts on automatic document dating started with generative models by (de Jong et al., 2005b). This model is later improved by (Kanhabua and Nørv˚ag, 2008a) who incorporate additional features such as POS tags, collocations, etc. Chambers (2012) shows significant improvement over these prior efforts through their discriminative models using handcrafted temporal features. Kotsakos et al. (2014) propose a statistical approach for document dating exploiting term bursti1605 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1605–1615 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Temporal Relation Extraction Dependency Parsing DCT Swiss subj adopted (e1) AFTER SAME 1995 (t1) SAME concession AFTER subj Classifier nmod govt BEFORE four yea"
P18-1149,Q14-1022,0,0.210213,"The existing approaches give higher confidence for timestamp immediate to the year mention 1995. NeuralDater exploits the syntactic and temporal structure of the document to predict the right timestamp (1999) for the document. With the exception of (Chambers, 2012), all prior works on the document dating problem ignore such informative temporal structure within the document. Research in document event extraction and ordering have made it possible to extract such temporal structures involving events, temporal expressions, and the (unknown) document date in a document (Mirza and Tonelli, 2016; Chambers et al., 2014). While methods to perform reasoning over such structures exist (Verhagen et al., 2007, 2010; UzZaman et al., 2013; Llorens et al., 2015; Pustejovsky et al., 2003), none of them have exploited advances in deep learning (Krizhevsky et al., 2012; Hinton et al., 2012; Goodfellow et al., 2016). In particular, recently proposed Graph Convolution Networks (GCN) (Defferrard et al., 2016; Kipf and Welling, 2017) have emerged as a way to learn graph representation while encoding structural information and constraints represented by the graph. We adapt GCNs for the document dating problem and make the f"
P18-1149,D08-1073,0,0.0358898,"s et al., 2009) of terms for dating documents. To the best of our knowledge, NeuralDater, our proposed method, is the first method to utilize deep learning techniques for the document dating problem. Event Ordering Systems: Temporal ordering of events is a vast research topic in NLP. The problem is posed as a temporal relation classification between two given temporal entities. Machine Learned classifiers and well crafted linguistic features for this task are used in (Chambers et al., 2007; Mirza and Tonelli, 2014). D’Souza and Ng (2013) use a hybrid approach by adding 437 hand-crafted rules. Chambers and Jurafsky (2008); Yoshikawa et al. (2009) try to classify with many more temporal constraints, while utilizing integer linear programming and Markov logic. CAEVO, a CAscading EVent Ordering architecture (Chambers et al., 2014) use sieve-based architecture (Lee et al., 2013) for temporal event ordering for the first time. They mix multiple learners according to their precision based ranks and use transitive closure for maintaining consistency of temporal graph. Mirza and Tonelli (2016) recently propose CATENA (CAusal and TEmporal relation extraction from NAtural language texts), the first integrated system for"
P18-1149,P07-2044,0,0.0126197,"ting. Kotsakos et al. (2014) propose a purely statistical method which considers lexical similarity alongside burstiness (Lappas et al., 2009) of terms for dating documents. To the best of our knowledge, NeuralDater, our proposed method, is the first method to utilize deep learning techniques for the document dating problem. Event Ordering Systems: Temporal ordering of events is a vast research topic in NLP. The problem is posed as a temporal relation classification between two given temporal entities. Machine Learned classifiers and well crafted linguistic features for this task are used in (Chambers et al., 2007; Mirza and Tonelli, 2014). D’Souza and Ng (2013) use a hybrid approach by adding 437 hand-crafted rules. Chambers and Jurafsky (2008); Yoshikawa et al. (2009) try to classify with many more temporal constraints, while utilizing integer linear programming and Markov logic. CAEVO, a CAscading EVent Ordering architecture (Chambers et al., 2014) use sieve-based architecture (Lee et al., 2013) for temporal event ordering for the first time. They mix multiple learners according to their precision based ranks and use transitive closure for maintaining consistency of temporal graph. Mirza and Tonelli"
P18-1149,chang-manning-2012-sutime,0,0.0877325,"Missing"
P18-1149,D17-1209,0,0.165394,"Missing"
P18-1149,N13-1112,0,0.0294228,"Missing"
P18-1149,D14-1181,0,0.0264301,"of input document with normalized time expression as its features. • NeuralDater: Our proposed method, refer Section 4. APW Table 2: Accuracies of different methods on APW and NYT datasets for the document dating problem (higher is better). NeuralDater significantly outperforms all other competitive baselines. This is our main result. Please see Section 7.1 for more details. • MaxEnt-Joint: Refers to MaxEnt-TimeNER combined with year mention classifier as described in (Chambers, 2012). • CNN: A Convolution Neural Network (CNN) (LeCun et al., 1999) based text classification model proposed by (Kim, 2014), which attained state-of-the-art results in several domains. Method Accuracy T-GCN S-GCN + T-GCN (K = 1) S-GCN + T-GCN (K = 2) S-GCN + T-GCN (K = 3) 57.3 57.8 58.8 59.1 Bi-LSTM Bi-LSTM + CNN Bi-LSTM + T-GCN Bi-LSTM + S-GCN + T-GCN (no gate) Bi-LSTM + S-GCN + T-GCN (K = 1) Bi-LSTM + S-GCN + T-GCN (K = 2) Bi-LSTM + S-GCN + T-GCN (K = 3) 58.6 59.0 60.5 62.7 64.1 63.8 63.3 Table 3: Accuracies of different ablated methods on the APW dataset. Overall, we observe that incorporation of context (Bi-LSTM), syntactic structure (S-GCN) and temporal structure (T-GCN) in NeuralDater achieves the best perfo"
P18-1149,J13-4004,0,0.0210933,"h topic in NLP. The problem is posed as a temporal relation classification between two given temporal entities. Machine Learned classifiers and well crafted linguistic features for this task are used in (Chambers et al., 2007; Mirza and Tonelli, 2014). D’Souza and Ng (2013) use a hybrid approach by adding 437 hand-crafted rules. Chambers and Jurafsky (2008); Yoshikawa et al. (2009) try to classify with many more temporal constraints, while utilizing integer linear programming and Markov logic. CAEVO, a CAscading EVent Ordering architecture (Chambers et al., 2014) use sieve-based architecture (Lee et al., 2013) for temporal event ordering for the first time. They mix multiple learners according to their precision based ranks and use transitive closure for maintaining consistency of temporal graph. Mirza and Tonelli (2016) recently propose CATENA (CAusal and TEmporal relation extraction from NAtural language texts), the first integrated system for the temporal and causal relations extraction between pre-annotated events and time expressions. They also incorporate sieve-based architecture which outperforms existing methods in temporal relation classification domain. We make use of CATENA for temporal"
P18-1149,S15-2134,0,0.0289491,"Missing"
P18-1149,P00-1010,0,0.193758,"structures is necessary. Bottom: Document date prediction by two state-of-the-art-baselines and NeuralDater, the method proposed in this paper. While the two previous methods are getting misled by the temporal expression (1995) in the document, NeuralDater is able to use the syntactic and temporal structure of the document to predict the right value (1999). Introduction Date of a document, also referred to as the Document Creation Time (DCT), is at the core of many important tasks, such as, information retrieval (Olson et al., 1999; Li and Croft, 2003; Dakka et al., 2008), temporal reasoning (Mani and Wilson, 2000; Llid´o et al., 2001), text summarization (Wan, 2007), event detection (Allan et al., 1998), and analysis of historical text (de Jong et al., 2005a), among others. In all such tasks, the document date is assumed to be available and also accurate – a strong assumption, especially for arbitrary documents from the Web. Thus, there is a need to automatically predict the date of a document based on its content. This problem is referred to as Document Dating. Initial attempts on automatic document dating started with generative models by (de Jong et al., 2005b). This model is later improved by (Kan"
P18-1149,P14-5010,0,0.00930284,"t, even though they are not immediate neighbors. A dependency parse may be used to capture such longer-range connections. In fact, similar features were exploited by (Chambers, 2012) for the document dating problem. NeuralDater captures such longerrange information by using another GCN run over the syntactic structure of the document. We describe this in detail below. The context embedding, Hcntx ∈ Rn×rcntx learned in the previous step is used as input to this layer. For a given document, we first extract its syntactic dependency structure by applying the Stanford CoreNLP’s dependency parser (Manning et al., 2014) on each sentence in the document individually. We now employ the Graph Convolution Network (GCN) over this dependency graph using the GCN formulation presented in Section 3.2. We call this GCN the Syntactic GCN or SGCN, as mentioned in Section 4. Since S-GCN operates over the dependency graph and uses Equation 2 for updating embeddings, the number of parameters in S-GCN is directly proportional to the number of dependency edge types. Stanford CoreNLP’s dependency parser returns 55 different dependency edge types. This large number of edge types is going to significantly over-parameterize S-GC"
P18-1149,D17-1159,0,0.44103,"methods in temporal relation classification domain. We make use of CATENA for temporal graph construction in our work. Graph Convolutional Networks (GCN): GCNs generalize Convolutional Neural Network (CNN) over graphs. GCN is introduced by (Bruna et al., 2014), and later extended by (Defferrard et al., 2016) with efficient localized filter approximation in spectral domain. Kipf and Welling (2017) propose a first-order approximation of localized filters through layer-wise propagation rule. GCNs over syntactic dependency trees have been recently exploited in the field of semantic-role labeling (Marcheggiani and Titov, 2017), neural machine translation (Bastings et al., 2017a), event detection (Bastings et al., 2017b). In our work, we successfully use GCNs for document dating. 3 Background: Graph Convolution Networks (GCN) In this section, we provide an overview of Graph Convolution Networks (GCN) (Kipf and Welling, 2017). GCN learns an embedding for each node of the graph it is applied over. We first present GCN for undirected graphs and then move onto GCN for directed graph setting. 3.1 GCN on Undirected Graph Let G = (V, E) be an undirected graph, where V is a set of n vertices and E the set of edges. The inpu"
P18-1149,E14-1033,0,0.0134002,"2014) propose a purely statistical method which considers lexical similarity alongside burstiness (Lappas et al., 2009) of terms for dating documents. To the best of our knowledge, NeuralDater, our proposed method, is the first method to utilize deep learning techniques for the document dating problem. Event Ordering Systems: Temporal ordering of events is a vast research topic in NLP. The problem is posed as a temporal relation classification between two given temporal entities. Machine Learned classifiers and well crafted linguistic features for this task are used in (Chambers et al., 2007; Mirza and Tonelli, 2014). D’Souza and Ng (2013) use a hybrid approach by adding 437 hand-crafted rules. Chambers and Jurafsky (2008); Yoshikawa et al. (2009) try to classify with many more temporal constraints, while utilizing integer linear programming and Markov logic. CAEVO, a CAscading EVent Ordering architecture (Chambers et al., 2014) use sieve-based architecture (Lee et al., 2013) for temporal event ordering for the first time. They mix multiple learners according to their precision based ranks and use transitive closure for maintaining consistency of temporal graph. Mirza and Tonelli (2016) recently propose C"
P18-1149,C16-1007,0,0.234377,"ion time of the document. The existing approaches give higher confidence for timestamp immediate to the year mention 1995. NeuralDater exploits the syntactic and temporal structure of the document to predict the right timestamp (1999) for the document. With the exception of (Chambers, 2012), all prior works on the document dating problem ignore such informative temporal structure within the document. Research in document event extraction and ordering have made it possible to extract such temporal structures involving events, temporal expressions, and the (unknown) document date in a document (Mirza and Tonelli, 2016; Chambers et al., 2014). While methods to perform reasoning over such structures exist (Verhagen et al., 2007, 2010; UzZaman et al., 2013; Llorens et al., 2015; Pustejovsky et al., 2003), none of them have exploited advances in deep learning (Krizhevsky et al., 2012; Hinton et al., 2012; Goodfellow et al., 2016). In particular, recently proposed Graph Convolution Networks (GCN) (Defferrard et al., 2016; Kipf and Welling, 2017) have emerged as a way to learn graph representation while encoding structural information and constraints represented by the graph. We adapt GCNs for the document datin"
P18-1149,D14-1162,0,0.0906187,"Missing"
P18-1149,S13-2001,0,0.0245174,"ts the syntactic and temporal structure of the document to predict the right timestamp (1999) for the document. With the exception of (Chambers, 2012), all prior works on the document dating problem ignore such informative temporal structure within the document. Research in document event extraction and ordering have made it possible to extract such temporal structures involving events, temporal expressions, and the (unknown) document date in a document (Mirza and Tonelli, 2016; Chambers et al., 2014). While methods to perform reasoning over such structures exist (Verhagen et al., 2007, 2010; UzZaman et al., 2013; Llorens et al., 2015; Pustejovsky et al., 2003), none of them have exploited advances in deep learning (Krizhevsky et al., 2012; Hinton et al., 2012; Goodfellow et al., 2016). In particular, recently proposed Graph Convolution Networks (GCN) (Defferrard et al., 2016; Kipf and Welling, 2017) have emerged as a way to learn graph representation while encoding structural information and constraints represented by the graph. We adapt GCNs for the document dating problem and make the following contributions: • We propose NeuralDater, a Graph Convolution Network (GCN)-based approach for document da"
P18-1149,S07-1014,0,0.0512105,"tion 1995. NeuralDater exploits the syntactic and temporal structure of the document to predict the right timestamp (1999) for the document. With the exception of (Chambers, 2012), all prior works on the document dating problem ignore such informative temporal structure within the document. Research in document event extraction and ordering have made it possible to extract such temporal structures involving events, temporal expressions, and the (unknown) document date in a document (Mirza and Tonelli, 2016; Chambers et al., 2014). While methods to perform reasoning over such structures exist (Verhagen et al., 2007, 2010; UzZaman et al., 2013; Llorens et al., 2015; Pustejovsky et al., 2003), none of them have exploited advances in deep learning (Krizhevsky et al., 2012; Hinton et al., 2012; Goodfellow et al., 2016). In particular, recently proposed Graph Convolution Networks (GCN) (Defferrard et al., 2016; Kipf and Welling, 2017) have emerged as a way to learn graph representation while encoding structural information and constraints represented by the graph. We adapt GCNs for the document dating problem and make the following contributions: • We propose NeuralDater, a Graph Convolution Network (GCN)-ba"
P18-1149,K17-1045,0,0.0377577,"pture nodes many hops away, multiple GCN layers may be stacked one on top of another. In particular, hk+1 v , representation of node v after k th GCN layer can be formulated as:  =f hk+1 v  X   W k hku + bk  , ∀v ∈ V. u∈N (v) where hku is the input to the k th layer. 1607 1 ReLU: f (x) = max(0, x) 3.2 GCN on Labeled and Directed Graph In this section, we consider GCN formulation over graphs where each edge is labeled as well as directed. In this setting, an edge from node u to v with label l(u, v) is denoted as (u, v, l(u, v)). While a few recent works focus on GCN over directed graphs (Yasunaga et al., 2017; Marcheggiani and Titov, 2017), none of them consider labeled edges. We handle both direction and label by incorporating label and direction specific filters. Based on the assumption that the information in a directed edge need not only propagate along its direction, following (Marcheggiani and Titov, 2017) we define an updated edge set E 0 which expands the original set E by incorporating inverse, as well self-loop edges. E 0 = E ∪ {(v, u, l(u, v)−1 ) |(u, v, l(u, v)) ∈ E} ∪ {(u, u, &gt;) |u ∈ V)}. (1) Here, l(u, v)−1 is the inverse edge label corresponding to label l(u, v), and &gt; is a special"
P18-1149,P09-1046,0,0.0338218,"ating documents. To the best of our knowledge, NeuralDater, our proposed method, is the first method to utilize deep learning techniques for the document dating problem. Event Ordering Systems: Temporal ordering of events is a vast research topic in NLP. The problem is posed as a temporal relation classification between two given temporal entities. Machine Learned classifiers and well crafted linguistic features for this task are used in (Chambers et al., 2007; Mirza and Tonelli, 2014). D’Souza and Ng (2013) use a hybrid approach by adding 437 hand-crafted rules. Chambers and Jurafsky (2008); Yoshikawa et al. (2009) try to classify with many more temporal constraints, while utilizing integer linear programming and Markov logic. CAEVO, a CAscading EVent Ordering architecture (Chambers et al., 2014) use sieve-based architecture (Lee et al., 2013) for temporal event ordering for the first time. They mix multiple learners according to their precision based ranks and use transitive closure for maintaining consistency of temporal graph. Mirza and Tonelli (2016) recently propose CATENA (CAusal and TEmporal relation extraction from NAtural language texts), the first integrated system for the temporal and causal"
P19-1320,P98-1013,0,0.104111,"c context by concatenating words with their dependency relations. For instance, in Figure 1 scientists_subj, water_obj, and mars_nmod needs to be included as a part of vocabulary for utilizing the dependency context of discover. This severely expands the vocabulary, thus limiting the scalability of models on large corpora. For instance, in Levy and Goldberg (2014) and Komninos and Manandhar (2016), the context vocabulary explodes to around 1.3 million for learning embeddings of 220k words. Incorporating relevant signals from semantic knowledge sources such as WordNet (Miller, 1995), FrameNet (Baker et al., 1998), and Paraphrase Database (PPDB) (Pavlick et al., 2015) has been shown to improve the quality of word embeddings. Recent works utilize these by incorporating them in a neural language modeling objective function (Yu and Dredze, 2014; Alsuhaibani et al., 2018), or as a post-processing step (Faruqui et al., 2014; Mrkši´c et al., 2016). Although existing approaches improve the quality of word embeddings, they require explicit modification for handling different types of semantic information. Recently proposed Graph Convolutional Networks (GCN) (Defferrard et al., 2016; Kipf and 3308 Proceedings o"
P19-1320,J10-4006,0,0.0305238,"methods, we compare them against the baselines on the following intrinsic and extrinsic tasks3 : • Intrinsic Tasks: Word Similarity is the task of evaluating closeness between semantically similar words. Following Komninos and Manandhar (2016); Pennington et al. (2014), we evaluate on Simlex999 (Hill et al., 2015), WS353 (Finkelstein et al., 2001), and RW (Luong et al., 2013) datasets. Concept Categorization involves grouping nominal concepts into natural categories. For instance, tiger and elephant should belong to mammal class. In our experiments, we evalute on AP (Almuhareb, 2006), Battig (Baroni and Lenci, 2010), BLESS (Baroni and Lenci, 2011), ESSLI (Baroni et al., 2008) datasets. Word Analogy task is to predict word b2 , given three words a1 , a2 , and b1 , such that the relation 3 Details of hyperparameters are in supplementary. b1 : b2 is same as the relation a1 : a2 . We compare methods on MSR (Mikolov et al., 2013c) and SemEval-2012 (Jurgens et al., 2012). • Extrinsic Tasks: Named Entity Recognition (NER) is the task of locating and classifying entity mentions into categories like person, organization etc. We use Lee et al. (2018)’s model on CoNLL-2003 dataset (Tjong Kim Sang and De Meulder, 20"
P19-1320,W11-2501,0,0.0156172,"the baselines on the following intrinsic and extrinsic tasks3 : • Intrinsic Tasks: Word Similarity is the task of evaluating closeness between semantically similar words. Following Komninos and Manandhar (2016); Pennington et al. (2014), we evaluate on Simlex999 (Hill et al., 2015), WS353 (Finkelstein et al., 2001), and RW (Luong et al., 2013) datasets. Concept Categorization involves grouping nominal concepts into natural categories. For instance, tiger and elephant should belong to mammal class. In our experiments, we evalute on AP (Almuhareb, 2006), Battig (Baroni and Lenci, 2010), BLESS (Baroni and Lenci, 2011), ESSLI (Baroni et al., 2008) datasets. Word Analogy task is to predict word b2 , given three words a1 , a2 , and b1 , such that the relation 3 Details of hyperparameters are in supplementary. b1 : b2 is same as the relation a1 : a2 . We compare methods on MSR (Mikolov et al., 2013c) and SemEval-2012 (Jurgens et al., 2012). • Extrinsic Tasks: Named Entity Recognition (NER) is the task of locating and classifying entity mentions into categories like person, organization etc. We use Lee et al. (2018)’s model on CoNLL-2003 dataset (Tjong Kim Sang and De Meulder, 2003) for evaluation. Question Ans"
P19-1320,D17-1209,0,0.205068,"se Mars Sentence Context Embedding GCN Embedding Output Layer SynGCN (Sentence-level) Figure 1: Overview of SynGCN: SynGCN employs Graph Convolution Network for utilizing dependency context for learning word embeddings. For each word in vocabulary, the model learns its representation by aiming to predict each word based on its dependency context encoded using GCNs. Please refer Section 5 for more details. Welling, 2016) have been found to be useful for encoding structural information in graphs. Even though GCNs have been successfully employed for several NLP tasks such as machine translation (Bastings et al., 2017), semantic role labeling (Marcheggiani and Titov, 2017), document dating (Vashishth et al., 2018a) and text classification (Yao et al., 2018), they have so far not been used for learning word embeddings, especially leveraging cues such as syntactic and semantic information. GCNs provide flexibility to represent diverse syntactic and semantic relationships between words all within one framework, without requiring relation-specific special handling as in previous methods. Recognizing these benefits, we make the following contributions in this paper. 1. We propose SynGCN, a Graph Convolution base"
P19-1320,P18-1078,0,0.0179273,"e relation a1 : a2 . We compare methods on MSR (Mikolov et al., 2013c) and SemEval-2012 (Jurgens et al., 2012). • Extrinsic Tasks: Named Entity Recognition (NER) is the task of locating and classifying entity mentions into categories like person, organization etc. We use Lee et al. (2018)’s model on CoNLL-2003 dataset (Tjong Kim Sang and De Meulder, 2003) for evaluation. Question Answering in Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) involves identifying answer to a question as a segment of text from a given passage. Following Peters et al. (2018), we evaluate using Clark and Gardner (2018)’s model. Part-of-speech (POS) tagging aims at associating with each word, a unique tag describing its syntactic role. For evaluating word embeddings, we use Lee et al. (2018)’s model on Penn Treebank POS dataset (Marcus et al., 1994). Co-reference Resolution (Coref) involves identifying all expressions that refer to the same entity in the text. To inspect the effect of embeddings, we use Lee et al. (2018)’s model on CoNLL2012 shared task dataset (Pradhan et al., 2012). 9 Results In this section, we attempt to answer the following questions. Q1. Does SynGCN learn better word embeddings than ex"
P19-1320,J15-4004,0,0.0328036,"ši´c et al., 2016), a method for injecting both antonym and synonym constraints into word embeddings. • JointReps (Alsuhaibani et al., 2018), a joint word representation learning method which simultaneously utilizes the corpus and KB. 8.3 Evaluation method To evaluate the effectiveness of our proposed methods, we compare them against the baselines on the following intrinsic and extrinsic tasks3 : • Intrinsic Tasks: Word Similarity is the task of evaluating closeness between semantically similar words. Following Komninos and Manandhar (2016); Pennington et al. (2014), we evaluate on Simlex999 (Hill et al., 2015), WS353 (Finkelstein et al., 2001), and RW (Luong et al., 2013) datasets. Concept Categorization involves grouping nominal concepts into natural categories. For instance, tiger and elephant should belong to mammal class. In our experiments, we evalute on AP (Almuhareb, 2006), Battig (Baroni and Lenci, 2010), BLESS (Baroni and Lenci, 2011), ESSLI (Baroni et al., 2008) datasets. Word Analogy task is to predict word b2 , given three words a1 , a2 , and b1 , such that the relation 3 Details of hyperparameters are in supplementary. b1 : b2 is same as the relation a1 : a2 . We compare methods on MSR"
P19-1320,S12-1047,0,0.0605669,"Missing"
P19-1320,D15-1242,0,0.0605092,"Missing"
P19-1320,N16-1175,0,0.361853,"rks has also been found to be quite useful (Collobert et al., 2011; Johnson et al., 2017; Strubell et al., 2018). Most popular methods for learning word embeddings are based on the distributional hypothesis, which utilizes the co-occurrence statistics ∗ Contributed equally to the work. from sequential context of words for learning word representations (Mikolov et al., 2013a; Pennington et al., 2014). More recently, this approach has been extended to include syntactic contexts (Levy and Goldberg, 2014) derived from dependency parse of text. Higher order dependencies have also been exploited by Komninos and Manandhar (2016); Li et al. (2018). Syntax-based embeddings encode functional similarity (in-place substitutable words) rather than topical similarity (topically related words) which provides an advantage on specific tasks like question classification (Komninos and Manandhar, 2016). However, current approaches incorporate syntactic context by concatenating words with their dependency relations. For instance, in Figure 1 scientists_subj, water_obj, and mars_nmod needs to be included as a part of vocabulary for utilizing the dependency context of discover. This severely expands the vocabulary, thus limiting the"
P19-1320,N18-2108,0,0.0576707,"Missing"
P19-1320,P14-2050,0,0.43243,"2013), and part-of-speech (POS) tagging (Ma and Hovy, 2016). Using word embeddings for initializing Deep Neural Networks has also been found to be quite useful (Collobert et al., 2011; Johnson et al., 2017; Strubell et al., 2018). Most popular methods for learning word embeddings are based on the distributional hypothesis, which utilizes the co-occurrence statistics ∗ Contributed equally to the work. from sequential context of words for learning word representations (Mikolov et al., 2013a; Pennington et al., 2014). More recently, this approach has been extended to include syntactic contexts (Levy and Goldberg, 2014) derived from dependency parse of text. Higher order dependencies have also been exploited by Komninos and Manandhar (2016); Li et al. (2018). Syntax-based embeddings encode functional similarity (in-place substitutable words) rather than topical similarity (topically related words) which provides an advantage on specific tasks like question classification (Komninos and Manandhar, 2016). However, current approaches incorporate syntactic context by concatenating words with their dependency relations. For instance, in Figure 1 scientists_subj, water_obj, and mars_nmod needs to be included as a p"
P19-1320,W13-3512,0,0.0500972,"nonym constraints into word embeddings. • JointReps (Alsuhaibani et al., 2018), a joint word representation learning method which simultaneously utilizes the corpus and KB. 8.3 Evaluation method To evaluate the effectiveness of our proposed methods, we compare them against the baselines on the following intrinsic and extrinsic tasks3 : • Intrinsic Tasks: Word Similarity is the task of evaluating closeness between semantically similar words. Following Komninos and Manandhar (2016); Pennington et al. (2014), we evaluate on Simlex999 (Hill et al., 2015), WS353 (Finkelstein et al., 2001), and RW (Luong et al., 2013) datasets. Concept Categorization involves grouping nominal concepts into natural categories. For instance, tiger and elephant should belong to mammal class. In our experiments, we evalute on AP (Almuhareb, 2006), Battig (Baroni and Lenci, 2010), BLESS (Baroni and Lenci, 2011), ESSLI (Baroni et al., 2008) datasets. Word Analogy task is to predict word b2 , given three words a1 , a2 , and b1 , such that the relation 3 Details of hyperparameters are in supplementary. b1 : b2 is same as the relation a1 : a2 . We compare methods on MSR (Mikolov et al., 2013c) and SemEval-2012 (Jurgens et al., 2012"
P19-1320,P16-1101,0,0.0233243,"semantic knowledge for further enhancing learned word representations. We make the source code of both models available to encourage reproducible research. 1 Introduction Representing words as real-valued vectors is an effective and widely adopted technique in NLP. Such representations capture properties of words based on their usage and allow them to generalize across tasks. Meaningful word embeddings have been shown to improve performance on several relevant tasks, such as named entity recognition (NER) (Bengio et al., 2013), parsing (Socher et al., 2013), and part-of-speech (POS) tagging (Ma and Hovy, 2016). Using word embeddings for initializing Deep Neural Networks has also been found to be quite useful (Collobert et al., 2011; Johnson et al., 2017; Strubell et al., 2018). Most popular methods for learning word embeddings are based on the distributional hypothesis, which utilizes the co-occurrence statistics ∗ Contributed equally to the work. from sequential context of words for learning word representations (Mikolov et al., 2013a; Pennington et al., 2014). More recently, this approach has been extended to include syntactic contexts (Levy and Goldberg, 2014) derived from dependency parse of te"
P19-1320,P14-5010,0,0.00410934,"rent edge types. SemGCN can be used as a post-processing method similar to Faruqui et al. (2014); Mrkši´c et al. (2016). We describe it in more detail in Section 6. 5 SynGCN In this section, we provide a detailed description of our proposed method, SynGCN. Following Mikolov et al. (2013b); Levy and Goldberg (2014); Komninos and Manandhar (2016), we separately define target and context embeddings for each word in the vocabulary as parameters in the model. For a given sentence s = (w1 , w2 , . . . , wn ), we first extract its dependency parse graph Gs = (Vs , Es ) using Stanford CoreNLP parser (Manning et al., 2014). Here, Vs = {w1 , w2 , . . . , wn } and Es denotes the labeled directed dependency edges of the form (wi , wj , lij ), where lij is the dependency relation of wi to wj . Similar to Mikolov et al. (2013b)’s continuousbag-of-words (CBOW) model, which defines the context of a word wi as Cwi = {wi+j : −c ≤ j ≤ c, j 6= 0} for a window of size c, we define the context as its neighbors in Gs , i.e., Cwi = N (wi ). Now, unlike CBOW which takes the sum of the context embedding of words in Cwi to predict wi , we apply directed Graph Convolution Network (as defined in Section 3) on Gs with context embed"
P19-1320,D17-1159,0,0.481221,"Output Layer SynGCN (Sentence-level) Figure 1: Overview of SynGCN: SynGCN employs Graph Convolution Network for utilizing dependency context for learning word embeddings. For each word in vocabulary, the model learns its representation by aiming to predict each word based on its dependency context encoded using GCNs. Please refer Section 5 for more details. Welling, 2016) have been found to be useful for encoding structural information in graphs. Even though GCNs have been successfully employed for several NLP tasks such as machine translation (Bastings et al., 2017), semantic role labeling (Marcheggiani and Titov, 2017), document dating (Vashishth et al., 2018a) and text classification (Yao et al., 2018), they have so far not been used for learning word embeddings, especially leveraging cues such as syntactic and semantic information. GCNs provide flexibility to represent diverse syntactic and semantic relationships between words all within one framework, without requiring relation-specific special handling as in previous methods. Recognizing these benefits, we make the following contributions in this paper. 1. We propose SynGCN, a Graph Convolution based method for learning word embeddings. Unlike previous"
P19-1320,H94-1020,0,0.185256,"person, organization etc. We use Lee et al. (2018)’s model on CoNLL-2003 dataset (Tjong Kim Sang and De Meulder, 2003) for evaluation. Question Answering in Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) involves identifying answer to a question as a segment of text from a given passage. Following Peters et al. (2018), we evaluate using Clark and Gardner (2018)’s model. Part-of-speech (POS) tagging aims at associating with each word, a unique tag describing its syntactic role. For evaluating word embeddings, we use Lee et al. (2018)’s model on Penn Treebank POS dataset (Marcus et al., 1994). Co-reference Resolution (Coref) involves identifying all expressions that refer to the same entity in the text. To inspect the effect of embeddings, we use Lee et al. (2018)’s model on CoNLL2012 shared task dataset (Pradhan et al., 2012). 9 Results In this section, we attempt to answer the following questions. Q1. Does SynGCN learn better word embeddings than existing approaches? (Section 9.1) Q2. Does SemGCN effectively handle diverse semantic information as compared to other methods? (Section 9.2) Q3. How does SemGCN perform compared to other methods when provided with the same semantic co"
P19-1320,N13-1090,0,0.130961,"ve performance on several relevant tasks, such as named entity recognition (NER) (Bengio et al., 2013), parsing (Socher et al., 2013), and part-of-speech (POS) tagging (Ma and Hovy, 2016). Using word embeddings for initializing Deep Neural Networks has also been found to be quite useful (Collobert et al., 2011; Johnson et al., 2017; Strubell et al., 2018). Most popular methods for learning word embeddings are based on the distributional hypothesis, which utilizes the co-occurrence statistics ∗ Contributed equally to the work. from sequential context of words for learning word representations (Mikolov et al., 2013a; Pennington et al., 2014). More recently, this approach has been extended to include syntactic contexts (Levy and Goldberg, 2014) derived from dependency parse of text. Higher order dependencies have also been exploited by Komninos and Manandhar (2016); Li et al. (2018). Syntax-based embeddings encode functional similarity (in-place substitutable words) rather than topical similarity (topically related words) which provides an advantage on specific tasks like question classification (Komninos and Manandhar, 2016). However, current approaches incorporate syntactic context by concatenating wor"
P19-1320,N16-1018,0,0.087938,"Missing"
P19-1320,P15-2070,0,0.0545974,"Missing"
P19-1320,D14-1162,0,0.124582,"al relevant tasks, such as named entity recognition (NER) (Bengio et al., 2013), parsing (Socher et al., 2013), and part-of-speech (POS) tagging (Ma and Hovy, 2016). Using word embeddings for initializing Deep Neural Networks has also been found to be quite useful (Collobert et al., 2011; Johnson et al., 2017; Strubell et al., 2018). Most popular methods for learning word embeddings are based on the distributional hypothesis, which utilizes the co-occurrence statistics ∗ Contributed equally to the work. from sequential context of words for learning word representations (Mikolov et al., 2013a; Pennington et al., 2014). More recently, this approach has been extended to include syntactic contexts (Levy and Goldberg, 2014) derived from dependency parse of text. Higher order dependencies have also been exploited by Komninos and Manandhar (2016); Li et al. (2018). Syntax-based embeddings encode functional similarity (in-place substitutable words) rather than topical similarity (topically related words) which provides an advantage on specific tasks like question classification (Komninos and Manandhar, 2016). However, current approaches incorporate syntactic context by concatenating words with their dependency re"
P19-1320,N18-1202,0,0.712638,"us methods, SynGCN utilizes syntactic context for learning word representations without increasing vocabulary size. 2. We also present SemGCN, a framework for incorporating diverse semantic knowledge (e.g., synonymy, antonymy, hyponymy, etc.) in learned word embeddings, without requiring relation-specific special handling as in previous methods. 3. Through experiments on multiple intrinsic and extrinsic tasks, we demonstrate that our proposed methods obtain substantial improvement over state-of-the-art approaches, and also yield an advantage when used in conjunction with methods such as ELMo (Peters et al., 2018). The source code of both the methods has been made available at http://github.com/ malllabiisc/WordGCN. 2 Related Work Word Embeddings: Recently, there has been much interest in learning meaningful word representations such as neural language modeling (Bengio et al., 2003) based continuous-bag-of-words (CBOW) and skip-gram (SG) models (Mikolov et al., 2013a). This is further extended by Pennington et al. (2014) which learns embeddings by factorizing word co-occurrence matrix to leverage global statistical information. Other formulations for learning word embeddings include multitask learning"
P19-1320,W12-4501,0,0.012505,"entifying answer to a question as a segment of text from a given passage. Following Peters et al. (2018), we evaluate using Clark and Gardner (2018)’s model. Part-of-speech (POS) tagging aims at associating with each word, a unique tag describing its syntactic role. For evaluating word embeddings, we use Lee et al. (2018)’s model on Penn Treebank POS dataset (Marcus et al., 1994). Co-reference Resolution (Coref) involves identifying all expressions that refer to the same entity in the text. To inspect the effect of embeddings, we use Lee et al. (2018)’s model on CoNLL2012 shared task dataset (Pradhan et al., 2012). 9 Results In this section, we attempt to answer the following questions. Q1. Does SynGCN learn better word embeddings than existing approaches? (Section 9.1) Q2. Does SemGCN effectively handle diverse semantic information as compared to other methods? (Section 9.2) Q3. How does SemGCN perform compared to other methods when provided with the same semantic constraints? (Section 9.3) Q4. Does dependency context based embedding encode complementary information compared to ELMo? (Section 9.4) 9.1 SynGCN Evaluation The evaluation results on intrinsic tasks – word similarity, concept categorization"
P19-1320,P18-1149,1,0.927344,"Overview of SynGCN: SynGCN employs Graph Convolution Network for utilizing dependency context for learning word embeddings. For each word in vocabulary, the model learns its representation by aiming to predict each word based on its dependency context encoded using GCNs. Please refer Section 5 for more details. Welling, 2016) have been found to be useful for encoding structural information in graphs. Even though GCNs have been successfully employed for several NLP tasks such as machine translation (Bastings et al., 2017), semantic role labeling (Marcheggiani and Titov, 2017), document dating (Vashishth et al., 2018a) and text classification (Yao et al., 2018), they have so far not been used for learning word embeddings, especially leveraging cues such as syntactic and semantic information. GCNs provide flexibility to represent diverse syntactic and semantic relationships between words all within one framework, without requiring relation-specific special handling as in previous methods. Recognizing these benefits, we make the following contributions in this paper. 1. We propose SynGCN, a Graph Convolution based method for learning word embeddings. Unlike previous methods, SynGCN utilizes syntactic contex"
P19-1320,D16-1264,0,0.0118223,"sk is to predict word b2 , given three words a1 , a2 , and b1 , such that the relation 3 Details of hyperparameters are in supplementary. b1 : b2 is same as the relation a1 : a2 . We compare methods on MSR (Mikolov et al., 2013c) and SemEval-2012 (Jurgens et al., 2012). • Extrinsic Tasks: Named Entity Recognition (NER) is the task of locating and classifying entity mentions into categories like person, organization etc. We use Lee et al. (2018)’s model on CoNLL-2003 dataset (Tjong Kim Sang and De Meulder, 2003) for evaluation. Question Answering in Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) involves identifying answer to a question as a segment of text from a given passage. Following Peters et al. (2018), we evaluate using Clark and Gardner (2018)’s model. Part-of-speech (POS) tagging aims at associating with each word, a unique tag describing its syntactic role. For evaluating word embeddings, we use Lee et al. (2018)’s model on Penn Treebank POS dataset (Marcus et al., 1994). Co-reference Resolution (Coref) involves identifying all expressions that refer to the same entity in the text. To inspect the effect of embeddings, we use Lee et al. (2018)’s model on CoNLL2012 shared ta"
P19-1320,P13-1045,0,0.0368315,"SemGCN, an effective framework for incorporating diverse semantic knowledge for further enhancing learned word representations. We make the source code of both models available to encourage reproducible research. 1 Introduction Representing words as real-valued vectors is an effective and widely adopted technique in NLP. Such representations capture properties of words based on their usage and allow them to generalize across tasks. Meaningful word embeddings have been shown to improve performance on several relevant tasks, such as named entity recognition (NER) (Bengio et al., 2013), parsing (Socher et al., 2013), and part-of-speech (POS) tagging (Ma and Hovy, 2016). Using word embeddings for initializing Deep Neural Networks has also been found to be quite useful (Collobert et al., 2011; Johnson et al., 2017; Strubell et al., 2018). Most popular methods for learning word embeddings are based on the distributional hypothesis, which utilizes the co-occurrence statistics ∗ Contributed equally to the work. from sequential context of words for learning word representations (Mikolov et al., 2013a; Pennington et al., 2014). More recently, this approach has been extended to include syntactic contexts (Levy a"
P19-1320,2020.coling-industry.22,0,0.101358,"Missing"
P19-1320,D18-1548,0,0.0437481,"Missing"
P19-1320,P14-2089,0,0.0445693,"s severely expands the vocabulary, thus limiting the scalability of models on large corpora. For instance, in Levy and Goldberg (2014) and Komninos and Manandhar (2016), the context vocabulary explodes to around 1.3 million for learning embeddings of 220k words. Incorporating relevant signals from semantic knowledge sources such as WordNet (Miller, 1995), FrameNet (Baker et al., 1998), and Paraphrase Database (PPDB) (Pavlick et al., 2015) has been shown to improve the quality of word embeddings. Recent works utilize these by incorporating them in a neural language modeling objective function (Yu and Dredze, 2014; Alsuhaibani et al., 2018), or as a post-processing step (Faruqui et al., 2014; Mrkši´c et al., 2016). Although existing approaches improve the quality of word embeddings, they require explicit modification for handling different types of semantic information. Recently proposed Graph Convolutional Networks (GCN) (Defferrard et al., 2016; Kipf and 3308 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3308–3318 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Scientists subj discover obj water WTarget chair"
P19-1320,P15-1150,0,0.127015,"Missing"
P19-1320,D18-1244,0,0.0271268,"sed methods (Levy and Goldberg, 2014; Komninos and Manandhar, 2016; Li et al., 2018) severely expand vocabulary size (as discussed in Section 1) which limits their scalability to a large corpus. To eliminate this drawback, we propose SynGCN which employs Graph Convolution Networks (Defferrard et al., 2016; Kipf and Welling, 3310 2016) to better encode syntactic information in embeddings. We prefer GCNs over other graph encoding architectures such as Tree LSTM (Tai et al., 2015) as GCNs do not restrict graphs to be trees and have been found to be more effective at capturing global information (Zhang et al., 2018). Moreover, they give substantial speedup as they do not involve recursive operations which are difficult to parallelize. The overall architecture is shown in Figure 1, for more details refer to Section 5. Enriching word embeddings with semantic knowledge helps to improve their quality for several NLP tasks (Faruqui et al., 2014; Mrkši´c et al., 2016). Existing approaches are either incapable of utilizing these diverse relations or need to be explicitly modeled for exploiting them. In this paper, we propose SemGCN which automatically learns to utilize multiple semantic constraints by modeling"
P19-1320,W03-0419,0,0.290605,"Missing"
P19-1507,D14-1162,0,0.0914539,"ccuracy results on macrocontext tests, therefore the layer aligned well with the whole brain activity. The choice of representation (deep neural network layer) to encode brain activity should be done carefully, as each representation may be good at encoding different parts of brain. A good criteria for representation selection requires further research. To demonstrate the efficacy of the synthetic dataset, we present the accuracy in predicting noun (or verb) stimuli from observed MEG activity with and without the additional synthetic MEG data. With linear ridge regression model (X2), a GloVe (Pennington et al., 2014) feature to brainactivity prediction models were trained to predict the MEG activity when a word is observed . To test the model performance, we calculate the accuracy of the predicted brain activity given the true brain activity during a word processing (Equation 1). All the experiments use 4-fold cross-validation. Figure 7 shows the increase in the noun/verb prediction accuracy with additional synthetically generated data. The statistical significance is calculated over 400 random label permutation tests. To summarize, these results show the utility of using previously trained regressor mode"
P19-1507,N18-1202,0,0.0490997,"of research by investigating the following three questions: (1) what is the relationship between sentence representations learned by deep learning networks and those encoded by the brain; (2) is there any correspondence between hidden layer activations in these deep models and brain regions; and (3) is it possible for deep recurrent models to synthesize brain data so that they can effectively be used for brain data augmentation. In order to evaluate these questions, we focus on representations of simple sentences. We employ various deep network architectures, including recently proposed ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) networks. We use MagnetoEncephaloGraphy (MEG) brain recording data of simple sentences as the target reference. We then correlate the representations learned by these various networks with the MEG recordings. Overall, we observe that BERT representations are the most predictive of MEG data. We also observe that the deep network models are effective at synthesizing brain data which are useful in overcoming data sparsity in stimuli decoding tasks involving brain data. In summary, in this paper we make the following contributions. ∗ This research was carried out du"
P19-1507,D14-1030,1,\N,Missing
P19-1507,N19-1423,0,\N,Missing
P19-1568,C14-1151,0,0.407895,"ses as discrete labels. Treating senses as discrete labels limits the generalization capability of these models for senses which occur infrequently in the training data. Further, for disambiguation of words not seen during training, these methods fall back on using a Most-Frequent-Sense (MFS) strategy, obtained from an external resource such as WordNet (Miller, 1995). To address these concerns, unsupervised knowledge-based (KB) approaches have been introduced, which rely solely on lexical resources (e.g., WordNet). KB methods include approaches based on context-definition overlap (Lesk, 1986; Basile et al., 2014), or on the structural properties of the lexical resource (Moro et al., 2014; Weissenborn et al., 2015; Chaplot et al., 2015; Chaplot and Salakhutdinov, 2018; 5670 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5670–5681 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Sense Embeddings BiLSTM link.n.02 necktie.n.01 cat.n.01 he wore a Sense Embedding SelfPrediction attention tie Natural Language Text Attentive Context Encoder Linear Context Embedding . . . neckwear consisting of Max Pooling fastener.n.02"
P19-1568,D18-1181,0,0.0388189,"hes learn context representations from unlabeled data, followed by a nearest neighbour classification (Melamud et al., 2016) or label propagation (Yuan et al., 2016). Recently, Raganato et al. (2017b) introduced neural sequence models for joint disambiguation of words in a sentence. All of these methods rely on sense-annotated data and, optionally, additional unlabeled corpora. Lexical resources provide an important source of knowledge about words and their meanings. Recent work has shown that neural networks can extract semantic information from dictionary definitions (Bahdanau et al., 2017; Bosc and Vincent, 2018). In this work, we use dictionary definitions to get representations of word meanings. Dictionary definitions have been used for WSD, motivated by the classical method of Lesk (Lesk, 1986). The original as well as subsequent modifications of the algorithm (Banerjee and Pedersen, 2003), including using word embeddings (Basile et al., 2014), operate on the hypothesis that the definition of the correct sense has a high overlap with the context in which a word is used. These methods tend to rely on heuristics based on insights about natural language text and their definitions. More recently, gloss"
P19-1568,D18-2029,0,0.0569638,"Missing"
P19-1568,D18-1170,0,0.487477,"Missing"
P19-1568,D17-1070,0,0.454454,"h integrate a module to score definition-context similarity (Luo et al., 2018b,a), and achieve state-ofthe-art results. We differ from these works in that we use the embeddings of definitions as the target space of a neural model, while learning in a supervised setup. Also, we don’t rely on any overlap heuristics, and use a single definition for a given sense as provided by WordNet. One approach for obtaining continuous representations for definitions is to use Universal Sentence Representations, which have been explored to allow transfer learning from large unlabeled as well as labeled data (Conneau et al., 2017; Cer et al., 2018). There has also been interest in learning deep contextualized word representations (Peters et al., 2018; Devlin et al., 2019). In this work, we evaluate definition embeddings obtained using these methods. Structural Knowledge available in lexical resources such as WordNet has motivated several unsupervised knowledge-based approaches for WSD. Graph based techniques have been used to match words to the most relevant sense (Navigli and Lapata, 2010; Sinha and Mihalcea, 2007; Agirre et al., 2014; Moro et al., 2014; Chaplot and Salakhutdinov, 2018). Our work differs from these m"
P19-1568,D15-1084,0,0.0306383,"done as a Research Assistant at Indian Institute of Science, Bangalore. It is clear that the implied sense of the word “tie” is very different in the two cases. The word is associated with “neckwear consisting of a long narrow piece of material” in the first example, and with “the finish of a contest in which the winner is undecided” in the second. The goal of WSD is to predict the right sense, given a word and its context. WSD has been shown to be useful for popular NLP tasks such as machine translation (Neale et al., 2016; Pu et al., 2018), information extraction (Zhong and Ng, 2012; Delli Bovi et al., 2015) and question answering (Ramakrishnan et al., 2003). The task of WSD can also be viewed as an intrinsic evaluation benchmark for the semantics learned by sentence comprehension models. WSD remains an open problem despite a long history of research. In this work, we study the all-words WSD task, where the goal is to disambiguate all ambiguous words in a corpus. Supervised (Zhong and Ng, 2010; Iacobacci et al., 2016; Melamud et al., 2016) and semisupervised approaches (Taghipour and Ng, 2015; Yuan et al., 2016) to WSD treat the target senses as discrete labels. Treating senses as discrete labels"
P19-1568,N19-1423,0,0.232388,"s in that we use the embeddings of definitions as the target space of a neural model, while learning in a supervised setup. Also, we don’t rely on any overlap heuristics, and use a single definition for a given sense as provided by WordNet. One approach for obtaining continuous representations for definitions is to use Universal Sentence Representations, which have been explored to allow transfer learning from large unlabeled as well as labeled data (Conneau et al., 2017; Cer et al., 2018). There has also been interest in learning deep contextualized word representations (Peters et al., 2018; Devlin et al., 2019). In this work, we evaluate definition embeddings obtained using these methods. Structural Knowledge available in lexical resources such as WordNet has motivated several unsupervised knowledge-based approaches for WSD. Graph based techniques have been used to match words to the most relevant sense (Navigli and Lapata, 2010; Sinha and Mihalcea, 2007; Agirre et al., 2014; Moro et al., 2014; Chaplot and Salakhutdinov, 2018). Our work differs from these methods in that we use structural knowledge to learn better representations of definitions, which are then used as targets for the WSD model. To l"
P19-1568,P16-1085,0,0.33595,"nd its context. WSD has been shown to be useful for popular NLP tasks such as machine translation (Neale et al., 2016; Pu et al., 2018), information extraction (Zhong and Ng, 2012; Delli Bovi et al., 2015) and question answering (Ramakrishnan et al., 2003). The task of WSD can also be viewed as an intrinsic evaluation benchmark for the semantics learned by sentence comprehension models. WSD remains an open problem despite a long history of research. In this work, we study the all-words WSD task, where the goal is to disambiguate all ambiguous words in a corpus. Supervised (Zhong and Ng, 2010; Iacobacci et al., 2016; Melamud et al., 2016) and semisupervised approaches (Taghipour and Ng, 2015; Yuan et al., 2016) to WSD treat the target senses as discrete labels. Treating senses as discrete labels limits the generalization capability of these models for senses which occur infrequently in the training data. Further, for disambiguation of words not seen during training, these methods fall back on using a Most-Frequent-Sense (MFS) strategy, obtained from an external resource such as WordNet (Miller, 1995). To address these concerns, unsupervised knowledge-based (KB) approaches have been introduced, which rely"
P19-1568,S15-1007,0,0.063286,"Missing"
P19-1568,P18-1230,0,0.427389,"s, supervised methods consistently outperform these methods in the general setting where inference is to be carried over both frequently occurring and rare words. Recently, Raganato et al. (2017b) posed WSD as a neural sequence labeling task, further improving the stateof-the-art. Yet, owing to an expensive annotation process (Lopez de Lacalle and Agirre, 2015), there is a scarcity of sense-annotated data thereby limiting the generalization ability of supervised methods. While there has been recent interest in incorporating definitions (glosses) to overcome the supervision bottleneck for WSD (Luo et al., 2018b,a), these methods are still limited due to their treatment of senses as discrete labels. Our hypothesis is that supervised methods can leverage lexical resources to improve on WSD for both observed and unobserved words and senses. We propose Extended WSD Incorporating Sense Embeddings (EWISE). Instead of learning a model to choose between discrete labels, EWISE learns a continuous space of sense embeddings as target. This enables generalized zero-shot learning, i.e., the ability to recognize instances of seen as well as unseen senses. EWISE utilizes sense definitions and additional informati"
P19-1568,K16-1006,0,0.419249,"been shown to be useful for popular NLP tasks such as machine translation (Neale et al., 2016; Pu et al., 2018), information extraction (Zhong and Ng, 2012; Delli Bovi et al., 2015) and question answering (Ramakrishnan et al., 2003). The task of WSD can also be viewed as an intrinsic evaluation benchmark for the semantics learned by sentence comprehension models. WSD remains an open problem despite a long history of research. In this work, we study the all-words WSD task, where the goal is to disambiguate all ambiguous words in a corpus. Supervised (Zhong and Ng, 2010; Iacobacci et al., 2016; Melamud et al., 2016) and semisupervised approaches (Taghipour and Ng, 2015; Yuan et al., 2016) to WSD treat the target senses as discrete labels. Treating senses as discrete labels limits the generalization capability of these models for senses which occur infrequently in the training data. Further, for disambiguation of words not seen during training, these methods fall back on using a Most-Frequent-Sense (MFS) strategy, obtained from an external resource such as WordNet (Miller, 1995). To address these concerns, unsupervised knowledge-based (KB) approaches have been introduced, which rely solely on lexical reso"
P19-1568,H93-1061,0,0.513771,"Missing"
P19-1568,S15-2049,0,0.686449,"Missing"
P19-1568,Q14-1019,0,0.431415,"ation capability of these models for senses which occur infrequently in the training data. Further, for disambiguation of words not seen during training, these methods fall back on using a Most-Frequent-Sense (MFS) strategy, obtained from an external resource such as WordNet (Miller, 1995). To address these concerns, unsupervised knowledge-based (KB) approaches have been introduced, which rely solely on lexical resources (e.g., WordNet). KB methods include approaches based on context-definition overlap (Lesk, 1986; Basile et al., 2014), or on the structural properties of the lexical resource (Moro et al., 2014; Weissenborn et al., 2015; Chaplot et al., 2015; Chaplot and Salakhutdinov, 2018; 5670 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5670–5681 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Sense Embeddings BiLSTM link.n.02 necktie.n.01 cat.n.01 he wore a Sense Embedding SelfPrediction attention tie Natural Language Text Attentive Context Encoder Linear Context Embedding . . . neckwear consisting of Max Pooling fastener.n.02 . Definition Encoder long narrow piece BiLSTM Tail Entities neckwear.n.01 0"
P19-1568,S13-2040,0,0.683632,"Missing"
P19-1568,S01-1005,0,0.19082,"Missing"
P19-1568,D14-1162,0,0.0837814,"Missing"
P19-1568,N18-1202,0,0.379309,"iffer from these works in that we use the embeddings of definitions as the target space of a neural model, while learning in a supervised setup. Also, we don’t rely on any overlap heuristics, and use a single definition for a given sense as provided by WordNet. One approach for obtaining continuous representations for definitions is to use Universal Sentence Representations, which have been explored to allow transfer learning from large unlabeled as well as labeled data (Conneau et al., 2017; Cer et al., 2018). There has also been interest in learning deep contextualized word representations (Peters et al., 2018; Devlin et al., 2019). In this work, we evaluate definition embeddings obtained using these methods. Structural Knowledge available in lexical resources such as WordNet has motivated several unsupervised knowledge-based approaches for WSD. Graph based techniques have been used to match words to the most relevant sense (Navigli and Lapata, 2010; Sinha and Mihalcea, 2007; Agirre et al., 2014; Moro et al., 2014; Chaplot and Salakhutdinov, 2018). Our work differs from these methods in that we use structural knowledge to learn better representations of definitions, which are then used as targets f"
P19-1568,D16-1174,0,0.0865078,"Missing"
P19-1568,C16-1330,0,0.22418,"Missing"
P19-1568,S07-1016,0,0.451113,"Missing"
P19-1568,E17-2025,0,0.0358468,". TransE (Bordes et al., 2013) models relations between entities as translations operating on the embeddings of the corresponding entities. ConvE (Dettmers et al., 2018), a more recent method, utilizes a multi-layer convolutional network, allowing it to learn more expressive features. Predicting in an embedding space is key to our methods, allowing generalized zero shot learning capability, as well as incorporating definitions and structural knowledge. The idea has been explored in the context of zero-shot learning (Xian et al., 2018). Tying the input and output embeddings of language models (Press and Wolf, 2017) resembles our approach. 3 Background In this work, we propose to use the training signal present in WordNet relations to learn encoders for definitions (Section 4.3.2). To learn from WordNet relations, we employ recently popular Knowledge Graph (KG) Embedding learning methods. In Section 3.1, we briefly introduce the framework for KG Embedding learning, and present the specific formulations for TransE and ConvE. 3.1 Knowledge Graph Embeddings Knowledge Graphs, a set of relations defined over a set of entities, provide an important field of research for representation learning. Methods for lea"
P19-1568,Q18-1044,0,0.0606082,"vest and tie” • “their record was 3 wins, 6 losses and a tie” ∗ Work done as a Research Assistant at Indian Institute of Science, Bangalore. It is clear that the implied sense of the word “tie” is very different in the two cases. The word is associated with “neckwear consisting of a long narrow piece of material” in the first example, and with “the finish of a contest in which the winner is undecided” in the second. The goal of WSD is to predict the right sense, given a word and its context. WSD has been shown to be useful for popular NLP tasks such as machine translation (Neale et al., 2016; Pu et al., 2018), information extraction (Zhong and Ng, 2012; Delli Bovi et al., 2015) and question answering (Ramakrishnan et al., 2003). The task of WSD can also be viewed as an intrinsic evaluation benchmark for the semantics learned by sentence comprehension models. WSD remains an open problem despite a long history of research. In this work, we study the all-words WSD task, where the goal is to disambiguate all ambiguous words in a corpus. Supervised (Zhong and Ng, 2010; Iacobacci et al., 2016; Melamud et al., 2016) and semisupervised approaches (Taghipour and Ng, 2015; Yuan et al., 2016) to WSD treat th"
P19-1568,E17-1010,0,0.641645,"e embedding for each sense in the inventory is generated using a BiLSTM-Max definition encoder. The encoder is learnt using the training signal present in WordNet Graph. An example signal with hypernym relation is depicted. Please see Section 4.3 for details on learning sense embeddings. Tripodi and Pelillo, 2017). While knowledge-based approaches offer a way to disambiguate rare and unseen words into potentially rare senses, supervised methods consistently outperform these methods in the general setting where inference is to be carried over both frequently occurring and rare words. Recently, Raganato et al. (2017b) posed WSD as a neural sequence labeling task, further improving the stateof-the-art. Yet, owing to an expensive annotation process (Lopez de Lacalle and Agirre, 2015), there is a scarcity of sense-annotated data thereby limiting the generalization ability of supervised methods. While there has been recent interest in incorporating definitions (glosses) to overcome the supervision bottleneck for WSD (Luo et al., 2018b,a), these methods are still limited due to their treatment of senses as discrete labels. Our hypothesis is that supervised methods can leverage lexical resources to improve on"
P19-1568,D17-1120,0,0.674859,"e embedding for each sense in the inventory is generated using a BiLSTM-Max definition encoder. The encoder is learnt using the training signal present in WordNet Graph. An example signal with hypernym relation is depicted. Please see Section 4.3 for details on learning sense embeddings. Tripodi and Pelillo, 2017). While knowledge-based approaches offer a way to disambiguate rare and unseen words into potentially rare senses, supervised methods consistently outperform these methods in the general setting where inference is to be carried over both frequently occurring and rare words. Recently, Raganato et al. (2017b) posed WSD as a neural sequence labeling task, further improving the stateof-the-art. Yet, owing to an expensive annotation process (Lopez de Lacalle and Agirre, 2015), there is a scarcity of sense-annotated data thereby limiting the generalization ability of supervised methods. While there has been recent interest in incorporating definitions (glosses) to overcome the supervision bottleneck for WSD (Luo et al., 2018b,a), these methods are still limited due to their treatment of senses as discrete labels. Our hypothesis is that supervised methods can leverage lexical resources to improve on"
P19-1568,W03-1201,0,0.0141591,"titute of Science, Bangalore. It is clear that the implied sense of the word “tie” is very different in the two cases. The word is associated with “neckwear consisting of a long narrow piece of material” in the first example, and with “the finish of a contest in which the winner is undecided” in the second. The goal of WSD is to predict the right sense, given a word and its context. WSD has been shown to be useful for popular NLP tasks such as machine translation (Neale et al., 2016; Pu et al., 2018), information extraction (Zhong and Ng, 2012; Delli Bovi et al., 2015) and question answering (Ramakrishnan et al., 2003). The task of WSD can also be viewed as an intrinsic evaluation benchmark for the semantics learned by sentence comprehension models. WSD remains an open problem despite a long history of research. In this work, we study the all-words WSD task, where the goal is to disambiguate all ambiguous words in a corpus. Supervised (Zhong and Ng, 2010; Iacobacci et al., 2016; Melamud et al., 2016) and semisupervised approaches (Taghipour and Ng, 2015; Yuan et al., 2016) to WSD treat the target senses as discrete labels. Treating senses as discrete labels limits the generalization capability of these mode"
P19-1568,W04-0811,0,0.211679,"Missing"
P19-1568,N15-1035,0,0.021892,"machine translation (Neale et al., 2016; Pu et al., 2018), information extraction (Zhong and Ng, 2012; Delli Bovi et al., 2015) and question answering (Ramakrishnan et al., 2003). The task of WSD can also be viewed as an intrinsic evaluation benchmark for the semantics learned by sentence comprehension models. WSD remains an open problem despite a long history of research. In this work, we study the all-words WSD task, where the goal is to disambiguate all ambiguous words in a corpus. Supervised (Zhong and Ng, 2010; Iacobacci et al., 2016; Melamud et al., 2016) and semisupervised approaches (Taghipour and Ng, 2015; Yuan et al., 2016) to WSD treat the target senses as discrete labels. Treating senses as discrete labels limits the generalization capability of these models for senses which occur infrequently in the training data. Further, for disambiguation of words not seen during training, these methods fall back on using a Most-Frequent-Sense (MFS) strategy, obtained from an external resource such as WordNet (Miller, 1995). To address these concerns, unsupervised knowledge-based (KB) approaches have been introduced, which rely solely on lexical resources (e.g., WordNet). KB methods include approaches b"
P19-1568,J17-1002,0,0.0131622,"then projected on to the space of sense embeddings. The score for each sense in the sense inventory is obtained using a dot product (indicated by ) of the sense embedding with the projected word embedding. Please see Section 4.2 for details on the context encoding and training of the context encoder. The sense embedding for each sense in the inventory is generated using a BiLSTM-Max definition encoder. The encoder is learnt using the training signal present in WordNet Graph. An example signal with hypernym relation is depicted. Please see Section 4.3 for details on learning sense embeddings. Tripodi and Pelillo, 2017). While knowledge-based approaches offer a way to disambiguate rare and unseen words into potentially rare senses, supervised methods consistently outperform these methods in the general setting where inference is to be carried over both frequently occurring and rare words. Recently, Raganato et al. (2017b) posed WSD as a neural sequence labeling task, further improving the stateof-the-art. Yet, owing to an expensive annotation process (Lopez de Lacalle and Agirre, 2015), there is a scarcity of sense-annotated data thereby limiting the generalization ability of supervised methods. While there"
P19-1568,P15-1058,0,0.0143754,"these models for senses which occur infrequently in the training data. Further, for disambiguation of words not seen during training, these methods fall back on using a Most-Frequent-Sense (MFS) strategy, obtained from an external resource such as WordNet (Miller, 1995). To address these concerns, unsupervised knowledge-based (KB) approaches have been introduced, which rely solely on lexical resources (e.g., WordNet). KB methods include approaches based on context-definition overlap (Lesk, 1986; Basile et al., 2014), or on the structural properties of the lexical resource (Moro et al., 2014; Weissenborn et al., 2015; Chaplot et al., 2015; Chaplot and Salakhutdinov, 2018; 5670 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5670–5681 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Sense Embeddings BiLSTM link.n.02 necktie.n.01 cat.n.01 he wore a Sense Embedding SelfPrediction attention tie Natural Language Text Attentive Context Encoder Linear Context Embedding . . . neckwear consisting of Max Pooling fastener.n.02 . Definition Encoder long narrow piece BiLSTM Tail Entities neckwear.n.01 0 1 0 a necktie.n.01 embeddi"
P19-1568,C16-1130,0,0.179932,"ale et al., 2016; Pu et al., 2018), information extraction (Zhong and Ng, 2012; Delli Bovi et al., 2015) and question answering (Ramakrishnan et al., 2003). The task of WSD can also be viewed as an intrinsic evaluation benchmark for the semantics learned by sentence comprehension models. WSD remains an open problem despite a long history of research. In this work, we study the all-words WSD task, where the goal is to disambiguate all ambiguous words in a corpus. Supervised (Zhong and Ng, 2010; Iacobacci et al., 2016; Melamud et al., 2016) and semisupervised approaches (Taghipour and Ng, 2015; Yuan et al., 2016) to WSD treat the target senses as discrete labels. Treating senses as discrete labels limits the generalization capability of these models for senses which occur infrequently in the training data. Further, for disambiguation of words not seen during training, these methods fall back on using a Most-Frequent-Sense (MFS) strategy, obtained from an external resource such as WordNet (Miller, 1995). To address these concerns, unsupervised knowledge-based (KB) approaches have been introduced, which rely solely on lexical resources (e.g., WordNet). KB methods include approaches based on context-defi"
S12-1019,W04-3221,0,0.381645,"Missing"
S12-1019,J10-4006,0,0.0716838,"Missing"
S12-1019,W10-0609,0,0.117011,"with a linear model which used training data to find neural basis images that correspond to the assumed semantic dimensions (for instance, one such basis image might be the activity of the brain for words representing animate concepts), and subsequently used these general patterns and known semantic dimensions to infer the fMRI activity that should be elicited by an unseen stimulus concept. Follow-on work has experimented with other neuroimaging modalities (Murphy et al., 2009), and with a range of semantic models including elicited property norms (Chang et al., 2011), corpus derived models (Devereux and Kelly, 2010; Pereira et al., 2011) and structured ontologies (Jelodar et al., 2010). The current state-of-the-art performance on this task is achieved using models that are handtailored in some respect, whether using manual annotation tasks (Palatucci et al., 2009), use of a domain-appropriate curated corpus (Pereira et al., 2011), or selection of particular collocates to suit the concepts to be described (Mitchell et al., 2008). While these approaches are clearly very successful, it is questionable whether they are a general solution to describe the various parts-of-speech and semantic domains that make"
S12-1019,D07-1097,0,0.0220474,"Missing"
S12-1019,W10-0603,0,0.122196,"hat correspond to the assumed semantic dimensions (for instance, one such basis image might be the activity of the brain for words representing animate concepts), and subsequently used these general patterns and known semantic dimensions to infer the fMRI activity that should be elicited by an unseen stimulus concept. Follow-on work has experimented with other neuroimaging modalities (Murphy et al., 2009), and with a range of semantic models including elicited property norms (Chang et al., 2011), corpus derived models (Devereux and Kelly, 2010; Pereira et al., 2011) and structured ontologies (Jelodar et al., 2010). The current state-of-the-art performance on this task is achieved using models that are handtailored in some respect, whether using manual annotation tasks (Palatucci et al., 2009), use of a domain-appropriate curated corpus (Pereira et al., 2011), or selection of particular collocates to suit the concepts to be described (Mitchell et al., 2008). While these approaches are clearly very successful, it is questionable whether they are a general solution to describe the various parts-of-speech and semantic domains that make up a speaker’s vocabulary. The Mitchell et al. (2008) 25-verb model wou"
S12-1019,W03-0208,0,0.0260834,"Missing"
S12-1019,P98-2127,0,0.0806289,"word- and documentlevel statistics, in raw and dimensionality reduced forms (Bullinaria and Levy, 2007; Turney and Pantel, 2010).2 ( PMIwf if PMIwf &gt; 0 PPMIwf = (1) 0 otherwise   p(w, f ) PMIwf = log (2) p(w)p(f ) A frequency threshold is commonly applied for three reasons: low-frequency co-occurrence counts are more noisy; PMI is positively biased towards hapax co-occurrences; and due to Zipfian distributions a cut-off dramatically reduces the amount of data to be processed. Many authors use a threshold of approximately 50-100 occurrences for word-collocate models (Lund and Burgess, 1996; Lin, 1998; Rapp, 2003). Since Bullinaria and Levy (2007) find improving performance with models using progressively lower cutoffs we explored two cut-offs of 20 and 50 which equate to low co-occurrences thresholds of 0.00125 or 0.003125 per million respectively; for the word-region model we chose a threshold of 2 occurrences of a target term in a document, to keep the input features to a reasonable dimensionality (Bradford, 2008). After applying these operations to the input data from each model, the resulting dimension2 Preliminary analyses confirmed that PPMI performed as well or better than alternat"
S12-1019,W02-0109,0,0.0201132,"Missing"
S12-1019,D09-1065,1,0.837138,"antics to learn the mapping between concepts and the neural activity which they elicit during neuroimaging experiments. This was achieved with a linear model which used training data to find neural basis images that correspond to the assumed semantic dimensions (for instance, one such basis image might be the activity of the brain for words representing animate concepts), and subsequently used these general patterns and known semantic dimensions to infer the fMRI activity that should be elicited by an unseen stimulus concept. Follow-on work has experimented with other neuroimaging modalities (Murphy et al., 2009), and with a range of semantic models including elicited property norms (Chang et al., 2011), corpus derived models (Devereux and Kelly, 2010; Pereira et al., 2011) and structured ontologies (Jelodar et al., 2010). The current state-of-the-art performance on this task is achieved using models that are handtailored in some respect, whether using manual annotation tasks (Palatucci et al., 2009), use of a domain-appropriate curated corpus (Pereira et al., 2011), or selection of particular collocates to suit the concepts to be described (Mitchell et al., 2008). While these approaches are clearly v"
S12-1019,J07-2002,0,0.340177,"Missing"
S12-1019,2003.mtsummit-papers.42,0,0.473495,"ocument co-occurrence, among others. Other parameters were kept fixed in a way that the literature suggests would be neutral to the various models, and so allow a fair comparison among them (Sahlgren, 2006; Bullinaria and Levy, 2007; Turney and Pantel, 2010). All textual statistics were gathered from a set of 50m English-language web-page documents consisting of 16 billion words. Where a fixed text window was used, we chose an extent of ±4 lower-case tokens either side of the target word of interest, which is in the mid-range of optimal values found by various authors (Lund and Burgess, 1996; Rapp, 2003; Sahlgren, 2006). Positive pointwise-mutual-information (1,2) was used as an association measure to normalize the observed co-occurrence frequency p(w, f ) for the varying frequency of the target word p(w) and its features p(f ). PPMI up-weights cooccurrences between rare words, yielding positive values for collocations that are more common than would be expected by chance (i.e. if word distributions were independent), and discards negative values that represent patterns of co-occurrences that are rarer than one would expect by chance. It has been shown to perform well generally, with both wo"
S12-1019,N03-1036,0,0.0336465,"Missing"
S12-1019,W02-0908,0,\N,Missing
S12-1019,P06-4018,0,\N,Missing
S12-1019,C98-2122,0,\N,Missing
S12-1019,ide-suderman-2004-american,0,\N,Missing
W06-2919,P05-1001,0,0.0144376,"Missing"
W06-2919,W03-0425,0,0.0139299,"Missing"
W06-2919,N04-1043,0,0.142536,"Missing"
W06-2919,W02-1028,0,0.106868,"d with these large corpora, the recent availability of entity lists in those domains has opened up interesting opportunities and challenges. Such lists are never complete and suffer from sampling biases, but we would like to exploit them, in combination with large unlabeled corpora, to speed up the creation of information extraction systems for different domains and languages. In this paper, we concentrate on exploring utility of such resources for named entity extraction. Previous approaches to context pattern induction were described by Riloff and Jones (1999), Agichtein and Gravano (2000), Thelen and Riloff (2002), Lin et al. (2003), and Etzioni et al. (2005), among others. The main advance in the present method is the combination of grammatical induction and statistical techniques to create high-precision patterns. The paper is organized as follows. Section 2 describes our pattern induction algorithm. Section 3 shows how to extend seed sets with entities extracted by the patterns from unlabeled data. Section 4 gives experimental results, and Section 5 compares our method with previous work. 1 For example, based on approximate matching, there is an overlap of only 22 organizations between the 2403 orga"
W06-2919,W03-0434,0,0.0325757,"Missing"
W07-1017,H05-1124,1,0.320413,"a feature indicating a short text field with the words in the field (“impression length=1 and ‘pneumonia’ ”) • A feature indicating each n-gram sequence that appears in both the impression and clinical history; the conjunction of certain terms where one appears in the history and the other in the impression (e.g. “cough in history and pneumonia in impression”). 3.1.2 Learning Technique Using these feature representations, we now learn a weight vector w that scores the correct labelings of the data higher than incorrect labelings. We used a k-best version of the MIRA algorithm (Crammer, 2004; McDonald et al., 2005). MIRA is an online learning algorithm that for each training document x updates the weight vector w according to the rule: wnew = arg min kw − wold k w s.t. ∀y ∈ Yk,wold (x) : w · f (x, y ∗ (x)) − w · f (x, y) ≥ L(y ∗ (x), y) where L(y ∗ (x), y) is a measure of the loss of labeling y with respect to the correct labeling y ∗ (x). For our experiments, we set k to 30 and iterated over the training data 10 times. Two standard modifications to this approach also helped. First, rather than using just the final weight vector, we average all weight vectors. This has a smoothing effect that improves p"
W12-2037,N07-4013,0,0.0113971,"lize rigorously, it seems intuitively plausible. For instance, user-generated content from Twitter and Facebook is mainly comprised of short, shallow snippets of information. Most current research in AI (and more broadly in computer science) does not seem likely to reverse this trend: e.g., work in crowdsourcing has concentrated on tasks that can be easily decomposed into small pieces, and much current NLP research aims at facilitating short-term “shallow” goals, such as answering well-formulated questions (e.g., (Kwok et al., 2001)) and extracting concrete facts (e.g., (Etzioni et al., 2006; Yates et al., 2007; Carlson et al., 2010)). This raises the question: what can AI do to facilitate deep, contemplative study? In this paper we address one aspect of this larger goal. Specifically, we consider automation of a novel task—using AI methods to facilitate the “deep comprehension” of complex technical material. We conjecture that the primary reason that technical documents are difficult to understand is lack of modularity: unlike a self-contained document written for a general reader, technical documents require certain background knowledge to comprehend—while that background knowledge may also be ava"
W12-2037,W09-3206,0,0.0170773,"ing for domain-independent prerequisite-structure prediction, as this suggests that for the prerequisite classification task, close to optimal (i.e., in-domain performance) is possible when the classifiers are trained in an out-of-domain setting. 3 Related Work We believe the task of prerequisite structure prediction to be novel; however, it is clearly related to a number of other well-studied research problems. In light of our emphasis on Wikipedia, a connection can be drawn between identifying prerequisites and measuring the semantic relatedness of concepts using Wikipedia’s link structure (Yeh et al., 2009). We consider here a related but narrower question, namely whether an inter-page link will improve comprehension for a specific reader. In the area of intelligent tutoring and educational data mining, recent research has looked at enriching textbooks with authoritative web content (Agrawal et al., 2010). Also, the problem of detecting prerequisite structure from differential student performance on tests has been considered (e.g., (Pavlik et al., 2008; Vuong et al., 2011)). Our proposal considers discovering prerequisite structure from text, rather than from exercises, and relies on different s"
W13-3510,N09-1003,0,0.0161568,"beautiful). The two feature sets can work together to up-weight the most suitable NNs (as in beautiful), or help to drown out noise (as in the NNs for bad publicity in the Document VSM). The remaining three tests use group judgements of similarity: the “Concrete Similarity” set of 65 concrete word pairs (Rubenstein and Goodenough, 1965); and two variations on the WordSim353 test-set (Finkelstein et al., 2002), partitioned into subsets corresponding to strict attributional similarity (“Mixed Similarity”, 203 noun pairs), and broader topical “relatedness” (“Mixed Relatedness”, 252 noun pairs) (Agirre et al., 2009). Performance on these benchmarks is Spearman correlation between the aggregate human judgements and pairwise cosine distances of word vectors in a VSM. The results in Figure 1 show that the Dependency VSM substantially outperforms the Document VSM when predicting human judgements of strict attributional (categorial) similarity (“Similarity” as opposed to “Relatedness”) for concrete nouns. Conversely the Document VSM is compet4.1 Judgements of Word Similarity As an initial test of the informativeness of Document and Dependency features, we evaluate the representation of single words. Behaviora"
W13-3510,D07-1097,0,0.0130624,"and use more expressive dependency-based features in our typebased VSM. A comparison of vector-space representations was recently published (Blacoe and Lapata, 2012), in which the authors compared several methods of combining single words vectors to create phrase vectors. They found that the best performance for adjective-noun composition used point-wise multiplication and a model based on type-based word co-occurrence patterns. 3 Creating a Vector-Space To create the Dependency vectors, a 16 billion word subset of ClueWeb09 (Callan and Hoy, 2009) was dependency parsed using the Malt parser (Hall et al., 2007). Dependency statistics were then collected for a predetermined list of target words and adjective-noun phrases, and for arbitrary adjective-noun phrases observed in the corpus. The list was composed of the 40 thousand most frequent single tokens in the American National Corpus (Ide and Suderman, 2006), and a small number of words and phrases used as stimuli in our brain imaging experiments. Additionally, we included any phrase found in the corpus whose maximal token span matched the PoS pattern J+N+, where J and N denote adjective and noun PoS tags respectively. For each unit (i.e., word or p"
W13-3510,W04-3221,0,0.0314299,"Missing"
W13-3510,J10-4006,0,0.0223793,"e a more type-based meaning (e.g. the noun judge might be modified by the adjective harsh, or be the subject of decide, as would related and substitutable words such as referee or conductor). Global patterns have been used in Latent Semantic Analysis (Landauer and Dumais, 1997) and LDA Topic models (Blei et al., 2003). Local patterns based on word co-occurrence in a fixed width window were used in Hyperspace Analogue to Language (Lund and Burgess, 1996). Subsequent models added increasing linguistic sophistication, up to full syntactic and dependency parses (Lin, 1998; Pad´o and Lapata, 2007; Baroni and Lenci, 2010). In most previous research on distributional semantics, Vector Space Models (VSMs) of words are built either from topical information (e.g., documents in which a word is present), or from syntactic/semantic types of words (e.g., dependency parse links of a word in sentences), but not both. In this paper, we explore the utility of combining these two representations to build VSM for the task of semantic composition of adjective-noun phrases. Through extensive experiments on benchmark datasets, we find that even though a type-based VSM is effective for semantic composition, it is often outperfo"
W13-3510,D10-1115,0,0.0326886,"fective for semantic composition, compared to a VSM built from Document and Dependency features alone. • We introduce a novel task: to predict the vector representation of a composed phrase from the brain activity of human subjects reading that phrase. • We explore two composition methods, addition and dilation, and find that while addition performs well on corpus-only tasks, dilation performs best on the brain activity task. • We build our VSMs, for both phrases and words, from a large syntactically parsed text corpus of 16 billion tokens. We also make the resulting VSM publicly available. 2 Baroni and Zamparelli (2010) extended the typical vector representation of words. Their model used matrices to represent adjectives, while nouns were represented with column vectors. The vectors for nouns and adjective-noun phrases were derived from local word co-occurrence statistics. The matrix to represent the adjective was estimated with partial least squares regression where the product of the learned adjective matrix and the observed noun vector should equal the observed adjective-noun vector. Socher et al. (2012) also extended word representations beyond simple vectors. Their model assigns each word a vector and a"
W13-3510,P09-1116,0,0.0352792,"omposition. Turney (2012) published an exploration of the impact of domain- and function-specific vector space models, analogous to the topic and type meanings encoded by our Document and Dependency models respectively. In Turney’s work, domain-specific information was represented by noun token co-occurrence statistics within a local window, and functional roles were represented by generalized token/part-of-speech cooccurrence patterns with verbs - both of which are relatively local and shallow when compared with this work. Similar local context-based features were used to cluster phrases in (Lin and Wu, 2009). Though the models discussed here are not entirely comparable to it, a recent comparison suggested that broader, deeper features such as ours may result in representations that are superior for tasks involving neural activation data (Murphy et al., 2012b). Related Work Mitchell and Lapata (2010) explored several methods of combining adjective and noun vectors to estimate phrase vectors, and compared the similarity judgements of humans to the similarity of their predicted phrase vectors. They found that for adjective-noun phrases, type-based models outperformed Latent Dirichlet Allocation (LDA"
W13-3510,D12-1050,0,0.130686,"ts both negative target-feature associations, and those that were not observed or fell below the frequency cut-off. To combine Document and Dependency information, we concatenate vectors. In contrast to the composite model in (Griffiths et al., 2005), in this paper we explore the complementarity of semantics captured by topical information and syntactic/semantic types. We focus on learning VSMs (involving both words and phrases) for semantic composition, and use more expressive dependency-based features in our typebased VSM. A comparison of vector-space representations was recently published (Blacoe and Lapata, 2012), in which the authors compared several methods of combining single words vectors to create phrase vectors. They found that the best performance for adjective-noun composition used point-wise multiplication and a model based on type-based word co-occurrence patterns. 3 Creating a Vector-Space To create the Dependency vectors, a 16 billion word subset of ClueWeb09 (Callan and Hoy, 2009) was dependency parsed using the Malt parser (Hall et al., 2007). Dependency statistics were then collected for a predetermined list of target words and adjective-noun phrases, and for arbitrary adjective-noun ph"
W13-3510,P98-2127,0,0.0221569,"erdict). Certain local patterns give a more type-based meaning (e.g. the noun judge might be modified by the adjective harsh, or be the subject of decide, as would related and substitutable words such as referee or conductor). Global patterns have been used in Latent Semantic Analysis (Landauer and Dumais, 1997) and LDA Topic models (Blei et al., 2003). Local patterns based on word co-occurrence in a fixed width window were used in Hyperspace Analogue to Language (Lund and Burgess, 1996). Subsequent models added increasing linguistic sophistication, up to full syntactic and dependency parses (Lin, 1998; Pad´o and Lapata, 2007; Baroni and Lenci, 2010). In most previous research on distributional semantics, Vector Space Models (VSMs) of words are built either from topical information (e.g., documents in which a word is present), or from syntactic/semantic types of words (e.g., dependency parse links of a word in sentences), but not both. In this paper, we explore the utility of combining these two representations to build VSM for the task of semantic composition of adjective-noun phrases. Through extensive experiments on benchmark datasets, we find that even though a type-based VSM is effecti"
W13-3510,P09-1072,1,0.797446,"mber of Dependency Dimensions Figure 3: The percentile rank of observed phrase vectors compared to vectors created using the addition composition function. and our predicted composed VSM for that phrase. We collected brain activity data using Magnetoencephalography (MEG). MEG is a brain imaging method with much higher temporal resolution (1 ms) than fMRI (∼2 sec). Since words are naturally read at a rate of about 2 per second, MEG is a better candidate for capturing the fast dynamics of semantic composition in the brain. Some previous work has explored adjective-noun composition in the brain (Chang et al., 2009), but used fMRI and corpus statistics based only on co-occurrence with 5 hand-selected verbs. (a) Addition composition function results. Our MEG data was collected while 9 participants viewed 38 phrases, each repeated 20 times (randomly interleaved). The stimulus nouns were chosen because previous research had shown them to be decodable from MEG recordings, and the adjectives were selected to modulate their most decodable semantic properties (e.g. edibility, manipulability) (Sudre et al., 2012). The 8 adjectives selected are (“big”, “small”, “ferocious”, “gentle”, “light”, “heavy”, “rotten”, “"
W13-3510,S12-1019,1,0.853656,"c information was represented by noun token co-occurrence statistics within a local window, and functional roles were represented by generalized token/part-of-speech cooccurrence patterns with verbs - both of which are relatively local and shallow when compared with this work. Similar local context-based features were used to cluster phrases in (Lin and Wu, 2009). Though the models discussed here are not entirely comparable to it, a recent comparison suggested that broader, deeper features such as ours may result in representations that are superior for tasks involving neural activation data (Murphy et al., 2012b). Related Work Mitchell and Lapata (2010) explored several methods of combining adjective and noun vectors to estimate phrase vectors, and compared the similarity judgements of humans to the similarity of their predicted phrase vectors. They found that for adjective-noun phrases, type-based models outperformed Latent Dirichlet Allocation (LDA) topic models. For the type-based models, multiplication performed the best, followed 85 the number of target word/phrase types they contained and choosing the top 10 million. A series of three additional filtering steps selected target words/phrases, a"
W13-3510,C12-1118,1,0.819117,"c information was represented by noun token co-occurrence statistics within a local window, and functional roles were represented by generalized token/part-of-speech cooccurrence patterns with verbs - both of which are relatively local and shallow when compared with this work. Similar local context-based features were used to cluster phrases in (Lin and Wu, 2009). Though the models discussed here are not entirely comparable to it, a recent comparison suggested that broader, deeper features such as ours may result in representations that are superior for tasks involving neural activation data (Murphy et al., 2012b). Related Work Mitchell and Lapata (2010) explored several methods of combining adjective and noun vectors to estimate phrase vectors, and compared the similarity judgements of humans to the similarity of their predicted phrase vectors. They found that for adjective-noun phrases, type-based models outperformed Latent Dirichlet Allocation (LDA) topic models. For the type-based models, multiplication performed the best, followed 85 the number of target word/phrase types they contained and choosing the top 10 million. A series of three additional filtering steps selected target words/phrases, a"
W13-3510,J07-2002,0,0.0918622,"Missing"
W13-3510,2003.mtsummit-papers.42,0,0.0710214,"Missing"
W13-3510,D12-1110,0,0.0660457,"parsed text corpus of 16 billion tokens. We also make the resulting VSM publicly available. 2 Baroni and Zamparelli (2010) extended the typical vector representation of words. Their model used matrices to represent adjectives, while nouns were represented with column vectors. The vectors for nouns and adjective-noun phrases were derived from local word co-occurrence statistics. The matrix to represent the adjective was estimated with partial least squares regression where the product of the learned adjective matrix and the observed noun vector should equal the observed adjective-noun vector. Socher et al. (2012) also extended word representations beyond simple vectors. Their model assigns each word a vector and a matrix, which are composed via an nonlinear function (e.g. tanh) to create phrase representations consisting of another vector/matrix pair. This process can proceed recursively, following a parse tree to produce a composite sentence meaning. Other general semantic composition frameworks have been suggested, e.g. (Sadrzadeh and Grefenstette, 2011) who focus on the operational nature of composition, rather than the representations that are supplied to the framework. Here we focus on creating w"
W13-3510,C98-2122,0,\N,Missing
W13-3510,ide-suderman-2004-american,0,\N,Missing
