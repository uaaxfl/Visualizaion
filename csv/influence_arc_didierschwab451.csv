2002.jeptalnrecital-long.10,2001.jeptalnrecital-long.21,1,0.606767,"Missing"
2002.jeptalnrecital-long.10,1995.mtsummit-1.1,0,0.234594,"Missing"
2003.jeptalnrecital-long.22,2001.jeptalnrecital-long.21,1,0.834084,"Missing"
2003.jeptalnrecital-long.22,2002.jeptalnrecital-long.10,1,0.799177,"Missing"
2003.jeptalnrecital-recital.8,P84-1004,0,0.340551,"Missing"
2003.jeptalnrecital-recital.8,2001.jeptalnrecital-long.21,0,0.0750267,"Missing"
2003.jeptalnrecital-recital.8,2001.jeptalnrecital-long.23,0,0.0245446,"e connaissance, c’est à dire l’ensemble des informations et des savoir-faire nécessaires à la réalisation de sa tâche. Les communications se font ici par envoi de messages entre agents et éventuellement tractations, négociations entre eux. Nous nous situons clairement ici dans la deuxième approche, au moins, à l’échelle globale du système (cf 3.2.1). Celle-ci est utilisée déjà depuis longtemps dans le domaine des langues naturelles. Dès le début des années 70, des travaux en IAD ont été effectués sur la compréhension automatique de la parole (HEARSAY-II (Erman et al., 1980)). Plus récemment, (Lebarbé, 2001) utilise cette approche pour l’analyse syntaxique et (Menézo et al., 1996) pour la détection d’erreurs. Parmis les autres travaux, certains se rapprochent de ce que nous voulons faire, une architecture modulaire permettant l’apprentissage de données et leur utilisation. On peut citer le système CARAMEL (Sabah, 1990) ou le système TALISMAN (Stefanini et al. 1992). Didier Schwab Les caractéristiques de notre système sont induites par les caractéristiques générales des agents et les moyens de communiquer entre eux, c’est à dire leur interaction. 3.2 Agents Nos agents ont des caractéristiques à la"
2003.jeptalnrecital-recital.8,1995.mtsummit-1.1,0,0.0199061,"Missing"
2003.jeptalnrecital-recital.8,C92-2075,0,0.105465,"Missing"
2003.jeptalnrecital-recital.8,2002.jeptalnrecital-long.10,1,0.881323,"Missing"
2005.jeptalnrecital-long.8,2002.jeptalnrecital-long.10,1,0.773132,"Missing"
2006.jeptalnrecital-poster.15,P84-1004,0,0.59594,"Missing"
2006.jeptalnrecital-poster.15,C92-2070,0,0.0330132,"Missing"
2007.jeptalnrecital-long.27,P06-1036,0,0.0676454,"Missing"
2007.jeptalnrecital-long.27,lafourcade-2006-conceptual,1,0.877035,"Missing"
2007.jeptalnrecital-long.27,1995.mtsummit-1.1,0,0.0607237,"Missing"
2007.jeptalnrecital-long.27,2002.jeptalnrecital-long.10,1,0.881296,"Missing"
2007.jeptalnrecital-long.27,C02-1038,0,0.0737216,"Missing"
2011.jeptalnrecital-long.11,H92-1046,0,0.405331,"Missing"
2011.jeptalnrecital-long.11,H92-1045,0,0.284182,"Missing"
2011.jeptalnrecital-long.11,S07-1006,0,0.0492998,"Missing"
2011.jeptalnrecital-long.11,P10-1154,0,0.0610231,"Missing"
2011.jeptalnrecital-long.11,vasilescu-etal-2004-evaluating,0,0.0765947,"Missing"
2011.jeptalnrecital-long.20,2003.mtsummit-papers.26,0,0.0783052,"Missing"
2011.jeptalnrecital-long.20,1995.mtsummit-1.1,0,0.0237433,"Missing"
2011.jeptalnrecital-long.20,W02-1118,1,0.843749,"Missing"
2015.jeptalnrecital-long.8,P91-1034,0,0.416111,"Missing"
2015.jeptalnrecital-long.8,S07-1054,0,0.0594784,"Missing"
2015.jeptalnrecital-long.8,P02-1033,0,0.126341,"Missing"
2015.jeptalnrecital-long.8,W08-0510,0,0.0387488,"Missing"
2015.jeptalnrecital-long.8,W02-1006,0,0.143672,"Missing"
2015.jeptalnrecital-long.8,J93-2004,0,0.0568506,"Missing"
2015.jeptalnrecital-long.8,H93-1061,0,0.237275,"Missing"
2015.jeptalnrecital-long.8,P96-1006,0,0.386883,"Missing"
2015.jeptalnrecital-long.8,F14-1005,0,0.0292103,"Missing"
2015.jeptalnrecital-long.8,Q14-1005,0,0.0259831,"Missing"
2015.jeptalnrecital-long.8,N01-1026,0,0.103393,"Missing"
2015.jeptalnrecital-long.8,H01-1035,0,0.056196,"Missing"
2016.jeptalnrecital-long.13,W10-4001,0,0.0420155,"Missing"
2016.jeptalnrecital-long.13,basile-etal-2012-developing,0,0.0293009,"Missing"
2016.jeptalnrecital-long.13,J06-1003,0,0.147647,"Missing"
2016.jeptalnrecital-long.13,S07-1054,0,0.729023,"Missing"
2016.jeptalnrecital-long.13,D14-1110,0,0.155337,"Missing"
2016.jeptalnrecital-long.13,H92-1046,0,0.901833,"Missing"
2016.jeptalnrecital-long.13,H93-1061,0,0.913693,"Missing"
2016.jeptalnrecital-long.13,C12-1109,0,0.286741,"Missing"
2016.jeptalnrecital-long.13,P96-1006,0,0.537279,"Missing"
2016.jeptalnrecital-long.13,C12-1146,1,0.588974,"Missing"
2016.jeptalnrecital-long.13,2004.jeptalnrecital-long.20,0,0.226401,"Missing"
2016.jeptalnrecital-poster.1,2012.iwslt-evaluation.16,0,0.023813,"Missing"
2016.jeptalnrecital-poster.1,W08-0510,0,0.0243148,"Missing"
2016.jeptalnrecital-poster.1,P07-2045,0,0.00480078,"Missing"
2016.jeptalnrecital-poster.1,W14-1614,0,0.0542726,"Missing"
2017.jeptalnrecital-court.14,N13-1090,0,0.164902,"Missing"
2017.jeptalnrecital-court.14,D14-1162,0,0.0940484,"Missing"
2017.jeptalnrecital-court.14,tiedemann-2012-parallel,0,0.035316,"Missing"
2017.jeptalnrecital-court.14,P10-1040,0,0.179072,"Missing"
2017.jeptalnrecital-court.14,S17-2007,0,0.0613229,"Missing"
2017.jeptalnrecital-court.18,P14-1023,0,0.0537753,"Missing"
2017.jeptalnrecital-court.18,D14-1110,0,0.0676671,"Missing"
2017.jeptalnrecital-court.18,S17-2012,1,0.860558,"Missing"
2017.jeptalnrecital-court.18,P14-2050,0,0.0874036,"Missing"
2017.jeptalnrecital-court.18,D14-1162,0,0.0778873,"Missing"
2017.jeptalnrecital-court.18,C16-1130,0,0.0376626,"Missing"
2017.jeptalnrecital-demo.9,P00-1064,0,0.0558796,"Missing"
2017.jeptalnrecital-demo.9,H93-1061,0,0.60827,"Missing"
2017.jeptalnrecital-demo.9,S15-2049,0,0.0681988,"Missing"
2017.jeptalnrecital-demo.9,ide-etal-2008-masc,0,0.0509677,"Missing"
2017.jeptalnrecital-demo.9,P10-1023,0,0.0605544,"Missing"
2017.jeptalnrecital-demo.9,P96-1006,0,0.620944,"Missing"
2017.jeptalnrecital-demo.9,E17-1010,0,0.0204335,"Missing"
2017.jeptalnrecital-demo.9,K15-1037,0,0.0366829,"Missing"
2018.jeptalnrecital-court.15,D07-1108,0,0.108092,"Missing"
2018.jeptalnrecital-court.15,S07-1054,0,0.0411897,"Missing"
2018.jeptalnrecital-court.15,P16-1085,0,0.0216973,"Missing"
2018.jeptalnrecital-court.15,P07-2045,0,0.0060887,"Missing"
2018.jeptalnrecital-court.15,2009.jeptalnrecital-long.19,0,0.0865709,"Missing"
2018.jeptalnrecital-court.15,2007.jeptalnrecital-long.9,0,0.0896225,"Missing"
2018.jeptalnrecital-court.15,S07-1047,0,0.0443775,"Missing"
2018.jeptalnrecital-court.15,2007.jeptalnrecital-long.37,0,0.0254618,"Missing"
2018.jeptalnrecital-demo.7,2009.jeptalnrecital-long.19,0,0.162179,"Missing"
2018.jeptalnrecital-demo.7,2007.jeptalnrecital-long.9,0,0.185196,"Missing"
2018.jeptalnrecital-demo.7,2007.jeptalnrecital-long.37,0,0.0839897,"Missing"
2018.jeptalnrecital-long.12,W14-4012,0,0.0651643,"Missing"
2018.jeptalnrecital-long.12,P16-1085,0,0.0321342,"Missing"
2018.jeptalnrecital-long.12,ide-etal-2008-masc,0,0.0194786,"Missing"
2018.jeptalnrecital-long.12,W16-5307,0,0.0542954,"Missing"
2018.jeptalnrecital-long.12,H93-1061,0,0.0974287,"Missing"
2018.jeptalnrecital-long.12,S15-2049,0,0.026349,"Missing"
2018.jeptalnrecital-long.12,D14-1162,0,0.0803252,"Missing"
2018.jeptalnrecital-long.12,S07-1016,0,0.11434,"Missing"
2018.jeptalnrecital-long.12,E17-1010,0,0.0286327,"Missing"
2018.jeptalnrecital-long.12,D17-1120,0,0.0241459,"Missing"
2018.jeptalnrecital-long.12,W04-0811,0,0.217008,"Missing"
2018.jeptalnrecital-long.12,K15-1037,0,0.0308682,"Missing"
2018.jeptalnrecital-long.12,C16-1130,0,0.0491544,"Missing"
2018.jeptalnrecital-long.12,P10-4014,0,0.0850606,"Missing"
2019.gwc-1.14,S01-1001,0,0.887612,"Missing"
2019.gwc-1.14,P16-1085,0,0.252731,"Missing"
2019.gwc-1.14,W02-0808,0,0.155709,"Missing"
2019.gwc-1.14,W16-5307,0,0.24442,"Missing"
2019.gwc-1.14,C18-1030,0,0.0216755,"Missing"
2019.gwc-1.14,D18-1170,0,0.185445,"Missing"
2019.gwc-1.14,P18-1230,0,0.12078,"Missing"
2019.gwc-1.14,H93-1061,0,0.920144,"Missing"
2019.gwc-1.14,S15-2049,0,0.748046,"Missing"
2019.gwc-1.14,Q14-1019,0,0.186664,"Missing"
2019.gwc-1.14,S07-1006,0,0.0918902,"Missing"
2019.gwc-1.14,S13-2040,0,0.809637,"Missing"
2019.gwc-1.14,D17-1008,0,0.0244557,"Missing"
2019.gwc-1.14,D14-1162,0,0.0819786,"Missing"
2019.gwc-1.14,N18-1202,0,0.0627995,"Missing"
2019.gwc-1.14,S07-1016,0,0.80523,"Missing"
2019.gwc-1.14,E17-1010,0,0.795791,"Missing"
2019.gwc-1.14,D17-1120,0,0.44339,"Missing"
2019.gwc-1.14,W04-0811,0,0.572166,"Missing"
2019.gwc-1.14,K15-1037,0,0.124404,"Missing"
2019.gwc-1.14,L18-1166,1,0.892281,"Missing"
2019.gwc-1.14,P10-4014,0,0.316001,"Missing"
2019.gwc-1.14,W06-1670,0,\N,Missing
2019.gwc-1.14,C16-1130,0,\N,Missing
2019.jeptalnrecital-long.4,P16-1085,0,0.0365024,"Missing"
2019.jeptalnrecital-long.4,W16-5307,0,0.0486658,"Missing"
2019.jeptalnrecital-long.4,C18-1030,0,0.0209002,"Missing"
2019.jeptalnrecital-long.4,D18-1170,0,0.0304913,"Missing"
2019.jeptalnrecital-long.4,P18-1230,0,0.0230245,"Missing"
2019.jeptalnrecital-long.4,H93-1061,0,0.452761,"Missing"
2019.jeptalnrecital-long.4,S15-2049,0,0.0369978,"Missing"
2019.jeptalnrecital-long.4,Q14-1019,0,0.069963,"Missing"
2019.jeptalnrecital-long.4,W18-6301,0,0.0422646,"Missing"
2019.jeptalnrecital-long.4,D17-1008,0,0.0229993,"Missing"
2019.jeptalnrecital-long.4,D14-1162,0,0.0816368,"Missing"
2019.jeptalnrecital-long.4,N18-1202,0,0.0267707,"Missing"
2019.jeptalnrecital-long.4,S07-1016,0,0.0667002,"Missing"
2019.jeptalnrecital-long.4,D17-1120,0,0.0232364,"Missing"
2019.jeptalnrecital-long.4,W04-0811,0,0.0788343,"Missing"
2019.jeptalnrecital-long.4,K15-1037,0,0.0267548,"Missing"
2019.jeptalnrecital-long.4,L18-1166,1,0.872858,"Missing"
2019.jeptalnrecital-long.4,C16-1130,0,0.0277121,"Missing"
2019.jeptalnrecital-long.4,P10-4014,0,0.0943769,"Missing"
2020.jeptalnrecital-taln.26,D19-1572,0,0.0323304,"Missing"
2020.jeptalnrecital-taln.26,P18-1031,0,0.0541897,"Missing"
2020.jeptalnrecital-taln.26,P19-1340,0,0.0367789,"Missing"
2020.jeptalnrecital-taln.26,P18-1249,0,0.0395933,"Missing"
2020.jeptalnrecital-taln.26,P07-2045,0,0.0108736,"Missing"
2020.jeptalnrecital-taln.26,P10-1023,0,0.119362,"Missing"
2020.jeptalnrecital-taln.26,D14-1162,0,0.0892081,"Missing"
2020.jeptalnrecital-taln.26,N18-1202,0,0.0990203,"Missing"
2020.jeptalnrecital-taln.26,P10-1114,0,0.0829177,"Missing"
2020.jeptalnrecital-taln.26,D17-1039,0,0.0479498,"Missing"
2020.jeptalnrecital-taln.26,W13-4917,0,0.0975746,"112 500 paires de phrases annotées avec les étiquettes entailment, contradiction ou neutre. FLUE intègre la partie française de ce corpus. Analyse syntaxique et étiquetage morphosyntaxique Nous considérons deux tâches d’analyse syntaxique : analyse en constituants et en dépendances, ainsi que l’étiquetage morphosyntaxique. Pour cela, nous utilisons le French Treebank (Abeillé et al., 2003), une collection de phrases du Monde annotées manuellement en constituants et dépendances syntaxiques. Nous utilisons la version de ce corpus de la campagne d’évaluation SPMRL 2014 décrite par Seddah et al. (2013), qui contient 14759, 1235 et 2541 phrases pour respectivement l’entraînement, le développement et l’évaluation. 271 Désambiguïsation lexicale des verbes et des noms La désambiguïsation lexicale consiste à assigner un sens, parmi un inventaire donné, à des mots d’une phrase. Pour la désambiguïsation lexicale de verbes, nous utilisons les données de FrenchSemEval (Segonne et al., 2019). Il s’agit d’un corpus d’évaluation dont les occurrences de verbes ont été annotées manuellement avec les sens de Wiktionary. 10 Pour la désambiguïsation lexicale des noms, nous utilisons la partie française de l"
2020.jeptalnrecital-taln.26,W19-0422,1,0.888597,"Missing"
2020.jeptalnrecital-taln.26,P16-1162,0,0.146858,"Missing"
2020.jeptalnrecital-taln.26,tiedemann-2012-parallel,0,0.0825572,"Missing"
2020.jeptalnrecital-taln.26,2019.gwc-1.14,1,0.893471,"Missing"
2020.jeptalnrecital-taln.26,W18-5446,0,0.0474061,"Missing"
2020.jeptalnrecital-taln.26,N18-1101,0,0.0610743,"Missing"
2020.jeptalnrecital-taln.26,D19-1382,0,0.0395373,"Missing"
2020.jeptalnrecital-taln.26,N19-1131,0,0.0313441,"Missing"
2020.lrec-1.21,H93-1061,0,0.763148,"then relied on the state-of-the-art WSD system proposed by (Vial et al., 2019), which is implemented in the open-source tool disambiguate2 . In their work, the authors only provide a model able to disambiguate English text, but their tool can be used to train a new disambiguation model in any language, given a set of sense annotated data used for training. We used the state of the art English to French Machine Translation system of the tool fairseq3 which also provide alignment between the source and the target words, and we translated the two corpora used by (Vial et al., 2019): the SemCor (Miller et al., 1993) and the WordNet Gloss Corpus4 . We then used these two corpora as training data for 2 https://github.com/getalp/disambiguate https://github.com/pytorch/fairseq 4 https://wordnetcode.princeton.edu/ glosstag-files/glosstag.shtml 3 the WSD system. Finally, once the WSD system trained, we used its predictions to facilitate the work of the mapping pictogramto-WordNet. For instance, if the annotator wanted to map a pictogram for “take a shower” (in French “prendre une douche”), the sense predicted by the WSD system on the word “take” could help to take a decision. We present in (Vaschalde et al., 2"
2020.lrec-1.21,2019.gwc-1.14,1,0.829306,"ive language, which gives the most probable WordNet sense to each word in a given French sentence. For the construction of this system, we exploited the method proposed by (Hadj Salah et al., 2018), which consists in automatically translating and aligning English sense annotated corpora into another language (in our case French), in order to have French sense annotated corpora. Indeed, manually sense annotated data are rare and almost nonexistent in non-English languages, but they are useful for building a good-quality WSD system. We then relied on the state-of-the-art WSD system proposed by (Vial et al., 2019), which is implemented in the open-source tool disambiguate2 . In their work, the authors only provide a model able to disambiguate English text, but their tool can be used to train a new disambiguation model in any language, given a set of sense annotated data used for training. We used the state of the art English to French Machine Translation system of the tool fairseq3 which also provide alignment between the source and the target words, and we translated the two corpora used by (Vial et al., 2019): the SemCor (Miller et al., 1993) and the WordNet Gloss Corpus4 . We then used these two cor"
2020.lrec-1.237,N19-1423,0,0.021088,"source toolkit to automatically build large-scale English information retrieval datasets based on Wikipedia. WIKIR is publicly available on GitHub. We also provide wikIR78k and wikIRS78k: two large-scale publicly available datasets that both contain 78,628 queries and 3,060,191 (query, relevant documents) pairs. Keywords: Information Retrieval, Open Source, Dataset, Deep Learning 1. Introduction Deep learning has been shown to be effective in various natural language processing (NLP) tasks such as language modeling, reading comprehension, question answering and natural language understanding (Devlin et al., 2019; Yang et al., 2019b). However, both large and public datasets are key factors for developing effective and reproducible deep learning models. Ad-hoc information retrieval (IR) consists in ranking a set of unstructured documents with respect to a query. Despite the progress in NLP using deep neural networks (DNNs), ad-hoc IR on text documents has not benefited as much as other fields of NLP from DNNs yet (Dehghani et al., 2017). The absence of significant success in ad-hoc IR using deep learning approaches is mainly due to the complexity of solving the ranking task using only unlabelled data ("
2020.lrec-1.237,W02-0109,0,0.413267,"&quot;: &quot;Anarchism

Anarchism is an &lt;a href=&quot;anti-authoritarian&quot;>antiauthoritarian&lt;/a> &lt;a href=&quot;political%20philosophy&quot;>political philosophy &lt;/a> that advocates ... &quot; } {&quot;id&quot;: &quot;25&quot;, &quot;url&quot;: &quot;https://en.wikipedia.org/wiki?curid=25&quot;, &quot;title&quot;: &quot;Autism&quot;, &quot;text&quot;: &quot;&quot;Autism

Autism is a &lt;a href=&quot;developmental%20disorder&quot;> developmental disorder&lt;/a> characterized by difficulties with ... &quot; } ... 1 2 3 4 5 6 7 8 9 10 11 Figure 2: json file extracted from English Wikipedia dump using WikiExtractor not in our GitHub repository. Because Rank-BM25 does not preprocess text, we used nltk Python library (Loper and Bird, 2002) to apply Porter stemmer (Porter, 2001) and stopword removal as commonly done in IR. It should be noted that we applied stemming and stopword removal only for BM25: the queries and documents in the dataset created by WIKIR are not stemmed and do contain stopwords. 3.3. WIKIR for neural re-ranking WIKIR can be used to train and evaluate DNNs on the dataset it created. As explained in Section 3.2., we perform neural re-ranking using BM25 as a first stage ranker. We used MatchZoo deep text matching library for training and evaluation of the models. We used MatchZoo because it has been accepted as"
2020.lrec-1.237,D14-1162,0,0.0926002,"Missing"
2020.lrec-1.237,N18-2073,0,0.0162351,"ical Existence, Describability and Identifiability. A simplified description of the construction process of an IR dataset using 2 articles from Wikipedia is displayed in Figure 1. Topical Existence. Every Wikipedia article is related to at least one topic: its main subject. Topical Identifiability. We assume that if an article a contains an internal link to another article at in its first sentence (denoted fa ), then the main subject of at is a topic of a. The intuition behind this assumption is that the first sentence of most Wikipedia articles is a good description of the article’s content (Sasaki et al., 2018) and if a link is present, it points to an important topic of the considered article. Therefore, we propose to define identify() as follows: o [n identify(a) = {sa } sat ∈ W ∃ fat −−→ a , (1) link where sa denotes the main subject of article a and fat −−→ link a designs an internal link in the first sentence of article at that points to article a. Thus, identify() considers the set of topics related to article a as the main subject of a: sa and the main subject of all articles that points to a in their first sentence. For example, the set of topics related to the article Developmental disorder"
2020.lrec-1.237,P14-2080,0,0.0300533,", 2017). However, this method can bias large models to rank similarly as the unsupervised ranker. Recently, Zheng et al. (2018) proposed Sogou-QCL, a publicly available dataset in Chinese with click relevance label. To the best of our knowledge, Sogou-QCL is the only public large-scale (≈500k queries) dataset for ad-hoc IR. The release of this dataset was the first step in reproducible research on neural ranking model applied to ad-hoc IR. Wikipedia has recently been used to build large-scale crosslingual information retrieval (CLIR) datasets to train effective neural learning-to-rank models (Schamoni et al., 2014). Leveraging this idea, we propose WIKIR: a toolkit to build a Wikipedia-based large-scale English IR dataset. WIKIR can also be used to train and evaluate several deep text matching models on the datasets it created. Moreover, we propose a general framework to build IR datasets automatically from any set of documents constrained by three topical properties that will be introduced further (see Section 2.1.). Our contributions are fourfold: • We provide WIKIR: a toolkit1 to build a Wikipediabased English Information Retrieval dataset; • We present a framework for creating IR datasets from a set"
2020.lrec-1.302,2020.osact-1.2,0,0.0214996,"QuAD (Rajpurkar et al., 2018), surpassing previous methods by a large margin. 2.2. Pre-trained Language Models Beyond English Given the impact of pre-trained language models on NLP downstream tasks in English, several works have recently released pre-trained models for other languages. For instance, ELMo exists for Portuguese, Japanese, German and Basque,5 while BERT and variants were specifically trained for simplified and traditional Chinese8 and German.6 A Portuguese version of MultiFiT is also available.7 Recently, more monolingual BERT-based models have been released, such as for Arabic (Antoun et al., 2020), Dutch (de Vries et al., 2019; Delobelle et al., 2020), Finnish (Virtanen et al., 2019), Italian (Polignano et al., 2019), Portuguese (Souza et al., 2019), Russian (Kuratov and Arkhipov, 2019), Spanish (Ca˜nete et al., 2020), and Vietnamese (Nguyen and Nguyen, 2020). For French, besides pre-trained language models using ULMFiT and MultiFiT configurations,7 CamemBERT (Martin et al., 2019) is a French BERT model concurrent to our work. Another trend considers one model estimated for several languages with a shared vocabulary. The release of multilingual BERT for 104 languages pioneered this app"
2020.lrec-1.302,Q19-1038,0,0.0214599,"19), Portuguese (Souza et al., 2019), Russian (Kuratov and Arkhipov, 2019), Spanish (Ca˜nete et al., 2020), and Vietnamese (Nguyen and Nguyen, 2020). For French, besides pre-trained language models using ULMFiT and MultiFiT configurations,7 CamemBERT (Martin et al., 2019) is a French BERT model concurrent to our work. Another trend considers one model estimated for several languages with a shared vocabulary. The release of multilingual BERT for 104 languages pioneered this approach.8 A recent extension of this work leverages parallel data to build a cross-lingual pre-trained version of LASER (Artetxe and Schwenk, 2019) for 93 languages, XLM (Lample and Conneau, 2019) and XLM-R (Conneau et al., 2019) for 100 languages. 2.3. Evaluation Protocol for French NLP Tasks The existence of a multi-task evaluation benchmark such as GLUE (Wang et al., 2018) for English is highly beneficial to facilitate research in the language of interest. The GLUE benchmark has become a prominent framework to evaluate the performance of NLP models in English. The recent contributions based on pre-trained language models have led to remarkable performance across a wide range of Natural Language Understanding (NLU) tasks. The authors o"
2020.lrec-1.302,W06-1615,0,0.0880109,"2 1 985 XNLI-FR Diverse genres 392 702 2 490 5 010 French Treebank Daily newspaper 14 759 1 235 2 541 FrenchSemEval Diverse genres 55 206 - 3 199 Noun Sense Disambiguation Diverse genres 818 262 - 1 445 Table 2: Descriptions of the datasets included in our FLUE benchmark. 4.1. Text Classification CLS The Cross Lingual Sentiment CLS (Prettenhofer and Stein, 2010) dataset consists of Amazon reviews for three product categories: books, DVD, and music in four languages: English, French, German, and Japanese. Each sample contains a review text and the associated rating from 1 to 5 stars. Following Blitzer et al. (2006) and Prettenhofer and Stein (2010), ratings with 3 stars are removed. Positive reviews have ratings higher than 3 and negative reviews are those rated lower than 3. There is one train and test set for each product category. The train and test sets are balanced, including around 1 000 positive and 1 000 negative reviews for a total of 2 000 reviews in each dataset. We take the French portion to create the binary text classification task in FLUE and report the accuracy on the test set. 4.2. Paraphrasing PAWS-X The Cross-lingual Adversarial Dataset for Paraphrase Identification PAWS-X (Yang et al"
2020.lrec-1.302,P17-1152,0,0.0617991,"Missing"
2020.lrec-1.302,D18-1269,0,0.0239485,"gement. The paraphrasing task is to identify whether the sentences in these pairs are semantically equivalent or not. Similar to previous approaches to create multilingual corpora, Yang et al. (2019a) used machine translation to create the training set for each target language in PAWS-X from the English training set in PAWS. The development and test sets for each language are translated by human translators. We take the related datasets for French to perform the paraphrasing task and report the accuracy on the test set. 4.3. Natural Language Inference XNLI The Cross-lingual NLI (XNLI) corpus (Conneau et al., 2018) extends the development and test sets of the Multi-Genre Natural Language Inference corpus (Williams et al., 2018, MultiNLI) to 15 languages. The development and test sets for each language consist of 7 500 humanannotated examples, making up a total of 112 500 sentence pairs annotated with the labels entailment, contradiction, or neutral. Each sentence pair includes a premise (p) and a hypothesis (h). The Natural Language Inference (NLI) task, also known as recognizing textual entailment (RTE), is to determine whether p entails, contradicts or neither entails nor contradicts h. We take the Fr"
2020.lrec-1.302,P19-4007,0,0.0544476,"Missing"
2020.lrec-1.302,W13-4905,0,0.054327,"shared task organizers. Our word representations are a concatenation of word embeddings and tag embeddings learned together with the model parameters on the French Treebank data itself, and at most one of (fastText, CamemBERT, FlauBERTBASE , FlauBERTBASE , mBERT) word vector. As Dozat and Manning (2016), we use word and tag dropout (d = 0.5) on word and tag embeddings but without dropout on BERT representations. We performed a fairly comprehensive grid search on hyperparameters for each model tested. Results The results are reported in Table 7. The best published results in this shared task (Constant et al., 2013) were involving an ensemble of parsers with additional resources for modelling multi word expressions (MWE), typical of the French treebank annotations. The monolingual French BERT models (CamemBERT, FlauBERT) perform better and set the new state of the art on this dataset with a single parser and without specific modelling for MWEs. One can observe that both FlauBERT models perform marginally better than CamemBERT, while all of them outperform mBERT by a large margin. 2484 Model UAS LAS Best published (Constant et al., 2013) 89.19 85.86 No pre-training fastText pre-training mBERT CamemBERT Fl"
2020.lrec-1.302,2020.findings-emnlp.292,0,0.0289963,"ethods by a large margin. 2.2. Pre-trained Language Models Beyond English Given the impact of pre-trained language models on NLP downstream tasks in English, several works have recently released pre-trained models for other languages. For instance, ELMo exists for Portuguese, Japanese, German and Basque,5 while BERT and variants were specifically trained for simplified and traditional Chinese8 and German.6 A Portuguese version of MultiFiT is also available.7 Recently, more monolingual BERT-based models have been released, such as for Arabic (Antoun et al., 2020), Dutch (de Vries et al., 2019; Delobelle et al., 2020), Finnish (Virtanen et al., 2019), Italian (Polignano et al., 2019), Portuguese (Souza et al., 2019), Russian (Kuratov and Arkhipov, 2019), Spanish (Ca˜nete et al., 2020), and Vietnamese (Nguyen and Nguyen, 2020). For French, besides pre-trained language models using ULMFiT and MultiFiT configurations,7 CamemBERT (Martin et al., 2019) is a French BERT model concurrent to our work. Another trend considers one model estimated for several languages with a shared vocabulary. The release of multilingual BERT for 104 languages pioneered this approach.8 A recent extension of this work leverages paral"
2020.lrec-1.302,N19-1423,0,0.620298,"is-diderot.fr, alexandre.allauzen@espci.fr Abstract Language models have become a key step to achieve state-of-the art results in many different Natural Language Processing (NLP) tasks. Leveraging the huge amount of unlabeled texts nowadays available, they provide an efficient way to pre-train continuous word representations that can be fine-tuned for a downstream task, along with their contextualization at the sentence level. This has been widely demonstrated for English using contextualized representations (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019b). In this paper, we introduce and share FlauBERT, a model learned on a very large and heterogeneous French corpus. Models of different sizes are trained using the new CNRS (French National Centre for Scientific Research) Jean Zay supercomputer. We apply our French language models to diverse NLP tasks (text classification, paraphrasing, natural language inference, parsing, word sense disambiguation) and show that most of the time they outperform other pre-training approaches. Different versions of FlauBERT as well as a unified evaluation protocol for the downstream tasks, c"
2020.lrec-1.302,eisele-chen-2010-multiun,0,0.103871,"Missing"
2020.lrec-1.302,D19-1572,0,0.0171919,"architecture, while ELMo adopts a bidirectional LSTM to build the final embedding for each input token from the concatenation of the left-to-right and rightto-left representations. Another fundamental difference lies in how each model can be tuned to different downstream tasks: ELMo delivers different word vectors that can be interpolated, whereas ULMFiT enables robust fine-tuning of the whole network w.r.t. the downstream tasks. The ability of fine-tuning was shown to significantly boost the performance, and thus this approach has been further developed in the recent works such as MultiFiT (Eisenschlos et al., 2019) or most prominently Transformer-based (Vaswani et al., 2017) architectures: GPT (Radford et al., 2018), BERT (Devlin et al., 2019), XLNet (Yang et al., 2019b), XLM (Lample and Conneau, 2019), RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2019), T5 (Raffel et al., 2019). These methods have one after the other established new state-ofthe-art results on various NLP benchmarks, such as GLUE (Wang et al., 2018) or SQuAD (Rajpurkar et al., 2018), surpassing previous methods by a large margin. 2.2. Pre-trained Language Models Beyond English Given the impact of pre-trained language models on NLP do"
2020.lrec-1.302,P18-1031,0,0.289639,"incent.segonne@etu, bcrabbe@linguist}.univ-paris-diderot.fr, alexandre.allauzen@espci.fr Abstract Language models have become a key step to achieve state-of-the art results in many different Natural Language Processing (NLP) tasks. Leveraging the huge amount of unlabeled texts nowadays available, they provide an efficient way to pre-train continuous word representations that can be fine-tuned for a downstream task, along with their contextualization at the sentence level. This has been widely demonstrated for English using contextualized representations (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019b). In this paper, we introduce and share FlauBERT, a model learned on a very large and heterogeneous French corpus. Models of different sizes are trained using the new CNRS (French National Centre for Scientific Research) Jean Zay supercomputer. We apply our French language models to diverse NLP tasks (text classification, paraphrasing, natural language inference, parsing, word sense disambiguation) and show that most of the time they outperform other pre-training approaches. Different versions of FlauBERT as well as a unified eval"
2020.lrec-1.302,P18-1249,0,0.141589,"Table 5. The results confirm the superiority of the French models compared to the multilingual model mBERT on this task. FlauBERTLARGE performs moderately better than CamemBERT. Both of them clearly outperform XLM-RBASE , while cannot surpass XLM-RLARGE . Model XLM-RLARGE † XLM-RBASE † mBERT‡ CamemBERT ‡ FlauBERTBASE FlauBERTLARGE † ‡ Accuracy 85.2 80.1 76.9 81.2 80.6 83.4 Results reported in (Conneau et al., 2019). Results reported in (Martin et al., 2019). Table 5: Results on the French XNLI dataset. 5.4. Constituency Parsing and POS Tagging Model description We use the parser described by Kitaev and Klein (2018) and Kitaev et al. (2019). It is an openly available19 chart parser based on a self-attentive encoder. We compare (i) a model without any pre-trained parameters, (ii) a model that additionally uses and fine-tunes fastText20 pre-trained embeddings, (iii) models based on pre-trained language models: mBERT, CamemBERT, and FlauBERT. We use the default hyperparameters from Kitaev and Klein (2018) for the first two settings and the hyperparameters from Kitaev et al. (2019) when using pretrained language models, except for FlauBERTLARGE . For this last model, we use a different learning rate (0.00001"
2020.lrec-1.302,P19-1340,0,0.0189308,"m the superiority of the French models compared to the multilingual model mBERT on this task. FlauBERTLARGE performs moderately better than CamemBERT. Both of them clearly outperform XLM-RBASE , while cannot surpass XLM-RLARGE . Model XLM-RLARGE † XLM-RBASE † mBERT‡ CamemBERT ‡ FlauBERTBASE FlauBERTLARGE † ‡ Accuracy 85.2 80.1 76.9 81.2 80.6 83.4 Results reported in (Conneau et al., 2019). Results reported in (Martin et al., 2019). Table 5: Results on the French XNLI dataset. 5.4. Constituency Parsing and POS Tagging Model description We use the parser described by Kitaev and Klein (2018) and Kitaev et al. (2019). It is an openly available19 chart parser based on a self-attentive encoder. We compare (i) a model without any pre-trained parameters, (ii) a model that additionally uses and fine-tunes fastText20 pre-trained embeddings, (iii) models based on pre-trained language models: mBERT, CamemBERT, and FlauBERT. We use the default hyperparameters from Kitaev and Klein (2018) for the first two settings and the hyperparameters from Kitaev et al. (2019) when using pretrained language models, except for FlauBERTLARGE . For this last model, we use a different learning rate (0.00001), batch size (8) and ign"
2020.lrec-1.302,P07-2045,0,0.0134554,"to extract the text or download them directly from their websites. The total size of the uncompressed text before preprocessing is 270 GB. More details can be found in Appendix A.1. Data preprocessing For all sub-corpora, we filtered out very short sentences as well as repetitive and nonmeaningful content such as telephone/fax numbers, email addresses, etc. For Common Crawl, which is our largest sub-corpus with 215 GB of raw text, we applied aggressive cleaning to reduce its size to 43.4 GB. All the data were Unicode-normalized in a consistent way before being tokenized using Moses tokenizer (Koehn et al., 2007). The resulting training corpus is 71 GB in size. Our code for downloading and preprocessing data is made publicly available.13 3.2. Models and Training Configurations Model architecture FlauBERT has the same model architecture as BERT (Devlin et al., 2019), which consists of a multi-layer bidirectional Transformer (Vaswani et al., 2017). Following Devlin et al. (2019), we propose two model sizes: • FlauBERTBASE : L = 12, H = 768, A = 12, • FlauBERTLARGE : L = 24, H = 1024, A = 16, where L, H and A respectively denote the number of Transformer blocks, the hidden size, and the number of selfatt"
2020.lrec-1.302,2005.mtsummit-papers.11,0,0.015061,"Missing"
2020.lrec-1.302,W19-5303,0,0.226301,"ding FlauBERT In this section, we describe the training corpus, the text preprocessing pipeline, the model architecture and training configurations to build FlauBERTBASE and FlauBERTLARGE . 3.1. Training Data Data collection Our French text corpus consists of 24 sub-corpora gathered from different sources, covering diverse topics and writing styles, ranging from formal and well-written text (e.g. Wikipedia and books)10 to random text crawled from the Internet (e.g. Common Crawl).11 The data were collected from three main sources: (1) monolingual data for French provided in WMT19 shared tasks (Li et al., 2019, 4 sub-corpora); (2) French text corpora offered in the OPUS collection (Tiedemann, 2012, 8 sub-corpora); and (3) datasets available in the Wikimedia projects (Meta, 2019, 8 sub-corpora). We used the WikiExtractor tool12 to extract the text from Wikipedia. For the other sub-corpora, we either used our 7 https://github.com/piegu/language-models https://github.com/google-research/bert 9 https://github.com/chineseGLUE/chineseGLUE 10 http://www.gutenberg.org 11 http://data.statmt.org/ngrams/deduped2017 12 https://github.com/attardi/wikiextractor 8 4 It should be noted that learning contextual emb"
2020.lrec-1.302,L16-1147,0,0.0701355,"Missing"
2020.lrec-1.302,H93-1061,0,0.0518853,"rt of the Multilingual WSD task of SemEval 2013 (Navigli et al., 2013), which targets nouns only. We adapted the task to use the WordNet 3.0 sense inventory (Miller, 1995) instead of BabelNet (Navigli and Ponzetto, 2010), by converting the sense keys to WordNet 3.0 if a mapping exists in BabelNet, and removing them otherwise. The result of the conversion process is an evaluation corpus composed of 306 sentences and 1 445 French nouns annotated with WordNet sense keys, and manually verified. For the training data, we followed the method proposed by Hadj Salah (2018), and translated the SemCor (Miller et al., 1993) and the WordNet Gloss Corpus16 into French, using the best English-French Machine Translation system of the fairseq toolkit17 (Ott et al., 2019). Finally, we aligned the WordNet sense annotation from the source English words to the the translated French words, using the alignment provided by the MT system. We rely on WordNet sense keys instead of the original BabelNet annotations for the following two reasons. First, WordNet is a resource that is entirely manually verified, and widely used in WSD research (Navigli, 2009). Second, there is already a large quantity of sense annotated data based"
2020.lrec-1.302,P10-1023,0,0.0572014,"(04-20-2018) openly available via Dbnary (S´erasset, 2012). For a given sense of a target key, the sense inventory offers a definition along with one or more examples. For this task, we considered the examples of the sense inventory as training examples and tested our model on the evaluation dataset. Noun Sense Disambiguation We propose a new challenging task for the WSD of French, based on the French part of the Multilingual WSD task of SemEval 2013 (Navigli et al., 2013), which targets nouns only. We adapted the task to use the WordNet 3.0 sense inventory (Miller, 1995) instead of BabelNet (Navigli and Ponzetto, 2010), by converting the sense keys to WordNet 3.0 if a mapping exists in BabelNet, and removing them otherwise. The result of the conversion process is an evaluation corpus composed of 306 sentences and 1 445 French nouns annotated with WordNet sense keys, and manually verified. For the training data, we followed the method proposed by Hadj Salah (2018), and translated the SemCor (Miller et al., 1993) and the WordNet Gloss Corpus16 into French, using the best English-French Machine Translation system of the fairseq toolkit17 (Ott et al., 2019). Finally, we aligned the WordNet sense annotation from"
2020.lrec-1.302,S13-2040,0,0.0327036,"ated version to French provided in XNLI. Following Conneau et al. (2018), we report the test accuracy. 4.4. Parsing and Part-of-Speech Tagging Syntactic parsing consists in assigning a tree structure to a sentence in natural language. We perform parsing on the French Treebank (Abeill´e et al., 2003), a collection of sentences extracted from French daily newspaper Le Monde, and manually annotated with both constituency and dependency syntactic trees and part-of-speech tags. Specifically, we use the version of the corpus instantiated for the SPMRL 2013 shared task and described by Seddah et al. (2013). This version is provided with a standard split representing 14 759 sentences for the training corpus, and respectively 1 235 and 2 541 sentences for the development and evaluation sets. 4.5. Word Sense Disambiguation Tasks Word Sense Disambiguation (WSD) is a classification task which aims to predict the sense of words in a given context according to a specific sense inventory. We used two French WSD tasks: the FrenchSemEval task (Segonne et al., 2019), which targets verbs only, and a modified version of the French part of the Multilingual WSD task of SemEval 2013 (Navigli et al., 2013), whi"
2020.lrec-1.302,2020.findings-emnlp.92,0,0.0305509,"ls for other languages. For instance, ELMo exists for Portuguese, Japanese, German and Basque,5 while BERT and variants were specifically trained for simplified and traditional Chinese8 and German.6 A Portuguese version of MultiFiT is also available.7 Recently, more monolingual BERT-based models have been released, such as for Arabic (Antoun et al., 2020), Dutch (de Vries et al., 2019; Delobelle et al., 2020), Finnish (Virtanen et al., 2019), Italian (Polignano et al., 2019), Portuguese (Souza et al., 2019), Russian (Kuratov and Arkhipov, 2019), Spanish (Ca˜nete et al., 2020), and Vietnamese (Nguyen and Nguyen, 2020). For French, besides pre-trained language models using ULMFiT and MultiFiT configurations,7 CamemBERT (Martin et al., 2019) is a French BERT model concurrent to our work. Another trend considers one model estimated for several languages with a shared vocabulary. The release of multilingual BERT for 104 languages pioneered this approach.8 A recent extension of this work leverages parallel data to build a cross-lingual pre-trained version of LASER (Artetxe and Schwenk, 2019) for 93 languages, XLM (Lample and Conneau, 2019) and XLM-R (Conneau et al., 2019) for 100 languages. 2.3. Evaluation Prot"
2020.lrec-1.302,N19-4009,0,0.0205028,"nventory (Miller, 1995) instead of BabelNet (Navigli and Ponzetto, 2010), by converting the sense keys to WordNet 3.0 if a mapping exists in BabelNet, and removing them otherwise. The result of the conversion process is an evaluation corpus composed of 306 sentences and 1 445 French nouns annotated with WordNet sense keys, and manually verified. For the training data, we followed the method proposed by Hadj Salah (2018), and translated the SemCor (Miller et al., 1993) and the WordNet Gloss Corpus16 into French, using the best English-French Machine Translation system of the fairseq toolkit17 (Ott et al., 2019). Finally, we aligned the WordNet sense annotation from the source English words to the the translated French words, using the alignment provided by the MT system. We rely on WordNet sense keys instead of the original BabelNet annotations for the following two reasons. First, WordNet is a resource that is entirely manually verified, and widely used in WSD research (Navigli, 2009). Second, there is already a large quantity of sense annotated data based on the sense inventory of WordNet (Vial et al., 2018) that we can use for the training of our system. We publicly release18 both our training da"
2020.lrec-1.302,D14-1162,0,0.0923659,"ding Evaluation), are shared to the research community for further reproducible experiments in French NLP. Keywords: FlauBERT, FLUE, BERT, Transformer, French, language model, pre-training, NLP benchmark, text classification, parsing, word sense disambiguation, natural language inference, paraphrase. 1. Introduction A recent game-changing contribution in Natural Language Processing (NLP) was the introduction of deep unsupervised language representations pre-trained using only plain text corpora. Previous word embedding pre-training approaches, such as word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014), learn a single vector for each wordform. By contrast, these new models are trained to produce contextual embeddings: the output representation depends on the entire input sequence (e.g. each token instance has a vector representation that depends on its left and right context). Initially based on recurrent neural networks (Dai and Le, 2015; Ramachandran et al., 2017; Howard and Ruder, 2018; Peters et al., 2018), these models quickly converged towards the use of the Transformer (Vaswani et al., 2017), such as GPT (Radford et al., 2018), BERT (Devlin et al., 2019), XLNet (Yang et al., 2019b),"
2020.lrec-1.302,N18-1202,0,0.806323,"-grenoble-alpes.fr {vincent.segonne@etu, bcrabbe@linguist}.univ-paris-diderot.fr, alexandre.allauzen@espci.fr Abstract Language models have become a key step to achieve state-of-the art results in many different Natural Language Processing (NLP) tasks. Leveraging the huge amount of unlabeled texts nowadays available, they provide an efficient way to pre-train continuous word representations that can be fine-tuned for a downstream task, along with their contextualization at the sentence level. This has been widely demonstrated for English using contextualized representations (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019b). In this paper, we introduce and share FlauBERT, a model learned on a very large and heterogeneous French corpus. Models of different sizes are trained using the new CNRS (French National Centre for Scientific Research) Jean Zay supercomputer. We apply our French language models to diverse NLP tasks (text classification, paraphrasing, natural language inference, parsing, word sense disambiguation) and show that most of the time they outperform other pre-training approaches. Different versions of FlauBERT a"
2020.lrec-1.302,P10-1114,0,0.0398144,"Missing"
2020.lrec-1.302,P18-2124,0,0.0233078,"f fine-tuning was shown to significantly boost the performance, and thus this approach has been further developed in the recent works such as MultiFiT (Eisenschlos et al., 2019) or most prominently Transformer-based (Vaswani et al., 2017) architectures: GPT (Radford et al., 2018), BERT (Devlin et al., 2019), XLNet (Yang et al., 2019b), XLM (Lample and Conneau, 2019), RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2019), T5 (Raffel et al., 2019). These methods have one after the other established new state-ofthe-art results on various NLP benchmarks, such as GLUE (Wang et al., 2018) or SQuAD (Rajpurkar et al., 2018), surpassing previous methods by a large margin. 2.2. Pre-trained Language Models Beyond English Given the impact of pre-trained language models on NLP downstream tasks in English, several works have recently released pre-trained models for other languages. For instance, ELMo exists for Portuguese, Japanese, German and Basque,5 while BERT and variants were specifically trained for simplified and traditional Chinese8 and German.6 A Portuguese version of MultiFiT is also available.7 Recently, more monolingual BERT-based models have been released, such as for Arabic (Antoun et al., 2020), Dutch ("
2020.lrec-1.302,D17-1039,0,0.213586,"uage Processing (NLP) was the introduction of deep unsupervised language representations pre-trained using only plain text corpora. Previous word embedding pre-training approaches, such as word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014), learn a single vector for each wordform. By contrast, these new models are trained to produce contextual embeddings: the output representation depends on the entire input sequence (e.g. each token instance has a vector representation that depends on its left and right context). Initially based on recurrent neural networks (Dai and Le, 2015; Ramachandran et al., 2017; Howard and Ruder, 2018; Peters et al., 2018), these models quickly converged towards the use of the Transformer (Vaswani et al., 2017), such as GPT (Radford et al., 2018), BERT (Devlin et al., 2019), XLNet (Yang et al., 2019b), RoBERTa (Liu et al., 2019). Using these pre-trained models in a transfer learning fashion has shown to yield striking improvements across a wide range of NLP tasks. One can easily build state-of-the-art NLP systems thanks to the publicly available pre-trained weights, saving time, energy, and resources. As a consequence, unsupervised language model pre-training has be"
2020.lrec-1.302,W13-4917,0,0.0460081,"Missing"
2020.lrec-1.302,W19-0422,1,0.895578,"Missing"
2020.lrec-1.302,P16-1162,0,0.0200296,"(attention) layers at each training step. Other techniques are also available such as progressive training (Gong et al., 2019), or improving initialization (Zhang et al., 2019a; Xu et al., 2019) and normalization (Nguyen and Salazar, 2019). For training FlauBERTLARGE , we employed pre-norm attention and stochastic depths for their simplicity. We found that these two techniques were sufficient for successful training. We set the rate of layer dropping to 0.2 in all the experiments. Other training details A vocabulary of 50K sub-word units is built using the Byte Pair Encoding (BPE) algorithm (Sennrich et al., 2016). The only difference between our work and RoBERTa is that the training data are preprocessed and tokenized using a basic tokenizer for French (Koehn et al., 2007, Moses), as in XLM (Lample and Conneau, 2019), before the application of BPE. We use fastBPE,15 a very efficient implementation to extract the BPE units and encode the corpora. 15 2481 https://github.com/glample/fastBPE FlauBERTBASE is trained on 32 GPUs Nvidia V100 in 410 hours and FlauBERTLARGE is trained on 128 GPUs in 390 hours, both with the effective batch size of 8192 sequences. Finally, we summarize the differences between Fl"
2020.lrec-1.302,serasset-2012-dbnary,0,0.036519,"Missing"
2020.lrec-1.302,skadins-etal-2014-billions,0,0.0456176,"Missing"
2020.lrec-1.302,tiedemann-2012-parallel,0,0.180846,"peline, the model architecture and training configurations to build FlauBERTBASE and FlauBERTLARGE . 3.1. Training Data Data collection Our French text corpus consists of 24 sub-corpora gathered from different sources, covering diverse topics and writing styles, ranging from formal and well-written text (e.g. Wikipedia and books)10 to random text crawled from the Internet (e.g. Common Crawl).11 The data were collected from three main sources: (1) monolingual data for French provided in WMT19 shared tasks (Li et al., 2019, 4 sub-corpora); (2) French text corpora offered in the OPUS collection (Tiedemann, 2012, 8 sub-corpora); and (3) datasets available in the Wikimedia projects (Meta, 2019, 8 sub-corpora). We used the WikiExtractor tool12 to extract the text from Wikipedia. For the other sub-corpora, we either used our 7 https://github.com/piegu/language-models https://github.com/google-research/bert 9 https://github.com/chineseGLUE/chineseGLUE 10 http://www.gutenberg.org 11 http://data.statmt.org/ngrams/deduped2017 12 https://github.com/attardi/wikiextractor 8 4 It should be noted that learning contextual embeddings was also proposed in (McCann et al., 2017), but in a supervised fashion as they u"
2020.lrec-1.302,W18-1819,0,0.025239,"RTLARGE : warmup steps of 30k, peak learning rate of 3e−4, β1 = 0.9, β2 = 0.98,  = 1e−6 and weight decay of 0.01. Training FlauBERTLARGE Training very deep Transformers is known to be susceptible to instability (Wang et al., 2019b; Nguyen and Salazar, 2019; Xu et al., 2019; Fan et al., 2019). Not surprisingly, we also observed this difficulty when training FlauBERTLARGE using the same configurations as BERTLARGE and RoBERTaLARGE , where divergence happened at an early stage. Several methods have been proposed to tackle this issue. For example, in an updated implementation of the Transformer (Vaswani et al., 2018), layer normalization is applied before each attention layer by default, rather than after each residual block as in the original implementation (Vaswani et al., 2017). These configurations are called pre-norm and post-norm, respectively. It was observed by Vaswani et al. (2018), and again confirmed by later works e.g. (Wang et al., 2019b; Xu et al., 2019; Nguyen and Salazar, 2019), that pre-norm helps stabilize training. Recently, a regularization technique called stochastic depths (Huang et al., 2016) has been demonstrated to be very effective for training deep Transformers, by e.g. Pham et"
2020.lrec-1.302,L18-1166,1,0.830909,"French, using the best English-French Machine Translation system of the fairseq toolkit17 (Ott et al., 2019). Finally, we aligned the WordNet sense annotation from the source English words to the the translated French words, using the alignment provided by the MT system. We rely on WordNet sense keys instead of the original BabelNet annotations for the following two reasons. First, WordNet is a resource that is entirely manually verified, and widely used in WSD research (Navigli, 2009). Second, there is already a large quantity of sense annotated data based on the sense inventory of WordNet (Vial et al., 2018) that we can use for the training of our system. We publicly release18 both our training data and the evaluation data in the UFSAC format (Vial et al., 2018). 5. Experiments and Results In this section, we present FlauBERT fine-tuning results on the FLUE benchmark. We compare the performance of FlauBERT with Multilingual BERT (Devlin et al., 2019, mBERT) and CamemBERT (Martin et al., 2019) on all tasks. In addition, for each task we also include the best non-BERT model for comparison. We made use of the open source libraries (Lample and Conneau, 2019, XLM) and (Wolf et al., 2019, Transformers)"
2020.lrec-1.302,2019.gwc-1.14,1,0.892947,"Missing"
2020.lrec-1.302,W18-5446,0,0.470157,"escribe our methodology to build FlauBERT – French Language Understanding via Bidirectional Encoder Representations from Transformers, a French BERT1 model that outperforms multilingual/cross-lingual models in several downstream NLP tasks, under similar configurations. FlauBERT relies on freely available datasets and is made publicly available in different versions.2 For further reproducible experiments, we also provide the complete processing and training pipeline as well as a general benchmark for evaluating French NLP systems. This evaluation setup is similar to the popular GLUE benchmark (Wang et al., 2018), and is named FLUE (French Language Understanding Evaluation). 2. 2.1. Related Work Pre-trained Language Models Self-supervised3 pre-training on unlabeled text data was first proposed in the task of neural language modeling (Bengio et al., 2003; Collobert and Weston, 2008), where it was shown that a neural network trained to predict next word from prior words can learn useful embedding representations, called word embeddings (each word is represented by a fixed vector). These representations were shown to play an important role in NLP, yielding state-of-the-art performance on multiple tasks ("
2020.lrec-1.302,P19-1176,0,0.104394,"2019) and XLM-R (Conneau et al., 2019) for 100 languages. 2.3. Evaluation Protocol for French NLP Tasks The existence of a multi-task evaluation benchmark such as GLUE (Wang et al., 2018) for English is highly beneficial to facilitate research in the language of interest. The GLUE benchmark has become a prominent framework to evaluate the performance of NLP models in English. The recent contributions based on pre-trained language models have led to remarkable performance across a wide range of Natural Language Understanding (NLU) tasks. The authors of GLUE have therefore introduced SuperGLUE (Wang et al., 2019a): a new benchmark built on the principles of GLUE, including more challenging and diverse set of tasks. A Chinese version of GLUE9 is also developed to evaluate model performance in Chinese NLP tasks. As of now, we have not learned of any such benchmark for French. 3. Building FlauBERT In this section, we describe the training corpus, the text preprocessing pipeline, the model architecture and training configurations to build FlauBERTBASE and FlauBERTLARGE . 3.1. Training Data Data collection Our French text corpus consists of 24 sub-corpora gathered from different sources, covering diverse"
2020.lrec-1.302,N18-1101,0,0.0161936,"ot. Similar to previous approaches to create multilingual corpora, Yang et al. (2019a) used machine translation to create the training set for each target language in PAWS-X from the English training set in PAWS. The development and test sets for each language are translated by human translators. We take the related datasets for French to perform the paraphrasing task and report the accuracy on the test set. 4.3. Natural Language Inference XNLI The Cross-lingual NLI (XNLI) corpus (Conneau et al., 2018) extends the development and test sets of the Multi-Genre Natural Language Inference corpus (Williams et al., 2018, MultiNLI) to 15 languages. The development and test sets for each language consist of 7 500 humanannotated examples, making up a total of 112 500 sentence pairs annotated with the labels entailment, contradiction, or neutral. Each sentence pair includes a premise (p) and a hypothesis (h). The Natural Language Inference (NLI) task, also known as recognizing textual entailment (RTE), is to determine whether p entails, contradicts or neither entails nor contradicts h. We take the French part of the XNLI corpus to form the development and test sets for the NLI task in FLUE. The train set is obta"
2020.lrec-1.302,D19-1382,0,0.0391611,"Missing"
2020.lrec-1.302,N19-1131,0,0.100143,"firmed by later works e.g. (Wang et al., 2019b; Xu et al., 2019; Nguyen and Salazar, 2019), that pre-norm helps stabilize training. Recently, a regularization technique called stochastic depths (Huang et al., 2016) has been demonstrated to be very effective for training deep Transformers, by e.g. Pham et al. (2019) and Fan et al. (2019) who successfully trained architectures of more than 40 layers. The idea is to randomly drop a number of (attention) layers at each training step. Other techniques are also available such as progressive training (Gong et al., 2019), or improving initialization (Zhang et al., 2019a; Xu et al., 2019) and normalization (Nguyen and Salazar, 2019). For training FlauBERTLARGE , we employed pre-norm attention and stochastic depths for their simplicity. We found that these two techniques were sufficient for successful training. We set the rate of layer dropping to 0.2 in all the experiments. Other training details A vocabulary of 50K sub-word units is built using the Byte Pair Encoding (BPE) algorithm (Sennrich et al., 2016). The only difference between our work and RoBERTa is that the training data are preprocessed and tokenized using a basic tokenizer for French (Koehn et a"
2021.acl-short.103,N19-1006,0,0.0243369,"be used to combine available pre-trained models to perform a multilingual ST task. In particular, we initialize the encoder using a pre-trained ASR encoder (on MuST-C)3 provided by Wang et al. (2020) and the decoder using mBART50, a multilingual denoising auto-encoder pre-trained on 50 languages (Tang et al., 2020). We tune language independent crossattention and language-specific adapters on top of these backbone models (using MuST-C as well). The results presented in Table 4 highlight that fine3 Pre-training on ASR data and then transferring to ST is not new but rather standard. See, e.g., Bansal et al. (2019). tuning cross-attention is crucial to transfer to multilingual ST (rows 3 and 5 show poor results without doing so). Adding adapters to the backbone decoder (row 4) or to both encoder and decoder (row 6) further boosts performance, demonstrating the ability of adapters to connect off-the-shelf models in a modular fashion. The best-performing model in this recipe (row 6) also outperforms bilingual systems (row 1) despite having fewer trainable parameters (190M vs. 248M). It is also important to mention that while we experiment on 8 target languages of MuST-C corpus, the multilingual ST model o"
2021.acl-short.103,D19-1165,0,0.397779,"ff between the aforementioned factors (and thus more generally between versatility and specialization) has often to be made, and depending on the application, one can be favored more than the other. One way to move along the spectrum between multilingual and bilingual models is to use adapter tuning which consists in freezing pre-trained parameters of a multilingual model and injecting lightweight modules between layers resulting in the addition of a small number of language-specific trainable parameters. While adapter tuning was investigated for multilingual neural machine translation (NMT) (Bapna and Firat, 2019), to our knowledge, this paper proposes the first comprehensive analysis of adapters for multilingual speech translation. Our contributions are the following: (1) we show that both versatility and specialization can be achieved by tuning language-specific adapter modules on top of a multilingual system. Bilingual models with higher accuracy than the original multilingual model are obtained, yet keeping a low maintenance complexity; (2) starting from a different initialization point, we show that adapters can also be 817 Proceedings of the 59th Annual Meeting of the Association for Computationa"
2021.acl-short.103,N19-1202,0,0.0418746,"Missing"
2021.acl-short.103,2020.acl-demos.34,0,0.0185407,"etails, together with extensive experimental results, in Section 5 and 6. In the next section, we present our experimental setup. 4 Adapters for Speech Translation In this section, we describe the integration of adapters into a given backbone model for speech translation. As the Transformer (Vaswani et al., 2017) has become increasingly common in speech 1 https://github.com/formiel/fairseq/tree/master/ examples/speech to text/docs/adapters.md 818 4.1 Experimental Setup Dataset MuST-C We evaluate our recipes on MuSTC (Di Gangi et al., 2019), a large-scale one-to-many 2 For speech applications (Inaguma et al., 2020; Wang et al., 2020), the embedding layer of the encoder is often a small convolutional neural network (Fukushima and Miyake, 1982; LeCun et al., 1989). Dict D d Adapter ENC DEC Finetune ENC DEC # params (M) trainable/total de es fr it nl pt ro ru Training data (hours) 408 504 492 465 442 385 432 489 avg mono multi - - - - - 8×31.1/8×31.1 32.1/32.1 22.16 22.37 30.42 30.40 27.92 27.49 22.92 22.79 24.10 24.42 27.19 27.32 21.51 20.78 14.36 14.54 23.82 23.76 3 4 multi multi 64 64 X X X - - 8×0.2/33.7 8×0.6/36.9 22.32 22.75 30.50 31.07 27.55 28.03 22.91 23.04 24.51 24.75 27.36 28.06 21.09 21.20 14."
2021.acl-short.103,2020.aacl-demo.6,1,0.922282,"uppose that our adapter layer is represented by a function g. The new “adapted” output is then given by: used as a glue to connect off-the-shelf systems (an automatic speech recognition (ASR) model and a multilingual denoising auto-encoder mBART (Liu et al., 2020; Tang et al., 2020)) to perform the multilingual ST task. Extensive experiments on the MuST-C dataset (Di Gangi et al., 2019) show that adapter-based fine-tuning can achieve very competitive results to full fine-tuning—while being much more parameter-efficient—in both standard and low-resource settings. Our code based on FAIRSEQ S2T (Wang et al., 2020) is publicly available.1 2 Related Work Adapter layers (or adapters for short) were first proposed in computer vision (Rebuffi et al., 2017), then explored for text classification tasks in NLP (Houlsby et al., 2019). Adapters are generally inserted between the layers of a pre-trained network and finetuned on the adaptation corpus. Bapna and Firat (2019) studied adapters in the context of NMT and evaluated them on two tasks: domain adaptation and massively multilingual NMT. Philip et al. (2020) later introduced monolingual adapters for zero-shot NMT. Other research groups made contributions on"
2021.codi-main.16,2020.autosimtrans-1.5,0,0.0149965,"language may prefer a certain discourse relation over the others that may not be the same as that of the original language. For example, in the case of Korean, which is an agglutinative language, the heavy usage of Korean postpositions (particles) dictates that the causal relation in the above example is revealed explicitly (regardless of the usage of explicit discourse connectives). Such a cross-lingual discourse analysis can provide some insights for improving the quality of (machine) translation (Meyer and Webber, 2013; Meyer and Poláková, 2013; Guzmán et al., 2014; Iruskieta et al., 2015; Chen et al., 2020). This paper presents an interactive system that visualizes the cross-lingual discourse relations using two human-annotated multilingual discourse datasets (Zeyrek et al., 2019; Long et al., 2020) derived from TED talks following the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) framework. Our interactive data dashboard provides users with an overview of relation preservation among all language pairs. We also display a graph network depicting the cross-lingual discourse relations between a pair of languages for a given TED talk, and provide a search function to find sentences with speci"
2021.codi-main.16,P14-1065,0,0.0320662,"language, the grammar and pragmatics of that language may prefer a certain discourse relation over the others that may not be the same as that of the original language. For example, in the case of Korean, which is an agglutinative language, the heavy usage of Korean postpositions (particles) dictates that the causal relation in the above example is revealed explicitly (regardless of the usage of explicit discourse connectives). Such a cross-lingual discourse analysis can provide some insights for improving the quality of (machine) translation (Meyer and Webber, 2013; Meyer and Poláková, 2013; Guzmán et al., 2014; Iruskieta et al., 2015; Chen et al., 2020). This paper presents an interactive system that visualizes the cross-lingual discourse relations using two human-annotated multilingual discourse datasets (Zeyrek et al., 2019; Long et al., 2020) derived from TED talks following the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) framework. Our interactive data dashboard provides users with an overview of relation preservation among all language pairs. We also display a graph network depicting the cross-lingual discourse relations between a pair of languages for a given TED talk, and provide a"
2021.codi-main.16,D19-1410,0,0.0640496,"Missing"
2021.codi-main.16,2020.emnlp-main.365,0,0.0233691,"Missing"
2021.codi-main.16,2020.lrec-1.129,0,0.0109067,"the heavy usage of Korean postpositions (particles) dictates that the causal relation in the above example is revealed explicitly (regardless of the usage of explicit discourse connectives). Such a cross-lingual discourse analysis can provide some insights for improving the quality of (machine) translation (Meyer and Webber, 2013; Meyer and Poláková, 2013; Guzmán et al., 2014; Iruskieta et al., 2015; Chen et al., 2020). This paper presents an interactive system that visualizes the cross-lingual discourse relations using two human-annotated multilingual discourse datasets (Zeyrek et al., 2019; Long et al., 2020) derived from TED talks following the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) framework. Our interactive data dashboard provides users with an overview of relation preservation among all language pairs. We also display a graph network depicting the cross-lingual discourse relations between a pair of languages for a given TED talk, and provide a search function to find sentences with specific query or relation types. We believe that, with this visualization tool, users can easily browse through the multilingual talks and discover which parts of the talks share similar discourse rel"
2021.codi-main.16,W13-3306,0,0.0214045,"se sentences into another language, the grammar and pragmatics of that language may prefer a certain discourse relation over the others that may not be the same as that of the original language. For example, in the case of Korean, which is an agglutinative language, the heavy usage of Korean postpositions (particles) dictates that the causal relation in the above example is revealed explicitly (regardless of the usage of explicit discourse connectives). Such a cross-lingual discourse analysis can provide some insights for improving the quality of (machine) translation (Meyer and Webber, 2013; Meyer and Poláková, 2013; Guzmán et al., 2014; Iruskieta et al., 2015; Chen et al., 2020). This paper presents an interactive system that visualizes the cross-lingual discourse relations using two human-annotated multilingual discourse datasets (Zeyrek et al., 2019; Long et al., 2020) derived from TED talks following the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) framework. Our interactive data dashboard provides users with an overview of relation preservation among all language pairs. We also display a graph network depicting the cross-lingual discourse relations between a pair of languages for a given TED"
2021.codi-main.16,W13-3303,0,0.0174348,"w, when we translate these sentences into another language, the grammar and pragmatics of that language may prefer a certain discourse relation over the others that may not be the same as that of the original language. For example, in the case of Korean, which is an agglutinative language, the heavy usage of Korean postpositions (particles) dictates that the causal relation in the above example is revealed explicitly (regardless of the usage of explicit discourse connectives). Such a cross-lingual discourse analysis can provide some insights for improving the quality of (machine) translation (Meyer and Webber, 2013; Meyer and Poláková, 2013; Guzmán et al., 2014; Iruskieta et al., 2015; Chen et al., 2020). This paper presents an interactive system that visualizes the cross-lingual discourse relations using two human-annotated multilingual discourse datasets (Zeyrek et al., 2019; Long et al., 2020) derived from TED talks following the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) framework. Our interactive data dashboard provides users with an overview of relation preservation among all language pairs. We also display a graph network depicting the cross-lingual discourse relations between a pair of"
2021.codi-main.16,W15-3049,0,0.0204122,"Missing"
2021.codi-main.16,prasad-etal-2008-penn,0,0.0969379,"the above example is revealed explicitly (regardless of the usage of explicit discourse connectives). Such a cross-lingual discourse analysis can provide some insights for improving the quality of (machine) translation (Meyer and Webber, 2013; Meyer and Poláková, 2013; Guzmán et al., 2014; Iruskieta et al., 2015; Chen et al., 2020). This paper presents an interactive system that visualizes the cross-lingual discourse relations using two human-annotated multilingual discourse datasets (Zeyrek et al., 2019; Long et al., 2020) derived from TED talks following the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) framework. Our interactive data dashboard provides users with an overview of relation preservation among all language pairs. We also display a graph network depicting the cross-lingual discourse relations between a pair of languages for a given TED talk, and provide a search function to find sentences with specific query or relation types. We believe that, with this visualization tool, users can easily browse through the multilingual talks and discover which parts of the talks share similar discourse relations, and where they diverge. 2 Preprocessing Datasets We abbreviate the two discourse-a"
2021.findings-acl.250,2021.eacl-main.189,0,0.0950811,"Missing"
2021.findings-acl.250,N19-4009,0,0.0599776,"Missing"
2021.findings-acl.250,P02-1040,0,0.124368,"the idea to the selfattentions in encoder as well. Methodology !2 As our goal was to identify “important” attention heads for different language pairs, we first needed to define a metric or a procedure that can capture the notion of “importance” of an attention head, and selected heads based on this importance. In Section 3.1, we present a set of metrics that quantify certain aspects of attention weights, which to some extent, can be considered as the importance. Section 3.2 illustrates a more direct approach where the importance of a head is defined as the extent of decrease in BLEU scores (Papineni et al., 2002) resulted in pruning the head. 1 3.1 either shared vocabulary or shared special tokens such as SEP , EOS , etc. cov(head) := X X j∈J αi,j i∈I More details on the metrics are provided in Appendix C. 3.2 Sequential Backward Selection of Heads Intuitively, a head can be considered as important if its removal results in a drastic decrease in the BLEU scores. As different combinations of heads can affect the performance differently, we followed the sequential backward selection (SBS) algorithm (Aha and Bankert, 1996), which is a top-down algorithm starting from a feature set of all features (in 2 A"
2021.iwslt-1.20,2020.emnlp-main.480,0,0.0339641,"Missing"
2021.iwslt-1.20,2020.iwslt-1.2,1,0.756555,"7 hours (it) to 189 hours (es). Translation data is part of the ASR talks for a given source language. Our experiments were performed in the constrained setting where only the provided data for the task is used. Model architecture Our system is based on the Dual-decoder Transformer (Le et al., 2020) which consists of an encoder and two decoders. This architecture jointly transcribes and translates an input speech. Each of the decoders is responsible for one task (ASR or ST) while interacting with each other. We refer the reader to the paper for further details. We initially followed Le et al. (2020) and used 12 encoder layers, 6 decoder layers, and a hidden dimension of d = 256. However, this model produced poor results. We hypothesize that with this configuration, the model capacity is too large for the dataset described in the previous section. In the end, we ended up using only 6 encoder layers and 3 decoder layers (with the same d = 256). In addition, we also trained a Transformer model having the same encoder of 6 layers but with only one decoder as the baseline (hereafter called singledecoder model). 3.3 Speech-to-text translation (ST) consists in translating a speech utterance in"
2021.iwslt-1.20,P82-1020,0,0.744682,"Missing"
2021.iwslt-1.20,D18-2012,0,0.0335627,"Missing"
2021.iwslt-1.20,2020.coling-main.314,1,0.844969,") on the test datasets of the low-resource task for the submitted system. et al., 2021), in which there are four source languages (Spanish (es), French (fr), Portuguese (pt), and Italian (it)) and five target languages (the aforementioned source languages plus English (en)). The sizes of the ASR talks range from 107 hours (it) to 189 hours (es). Translation data is part of the ASR talks for a given source language. Our experiments were performed in the constrained setting where only the provided data for the task is used. Model architecture Our system is based on the Dual-decoder Transformer (Le et al., 2020) which consists of an encoder and two decoders. This architecture jointly transcribes and translates an input speech. Each of the decoders is responsible for one task (ASR or ST) while interacting with each other. We refer the reader to the paper for further details. We initially followed Le et al. (2020) and used 12 encoder layers, 6 decoder layers, and a hidden dimension of d = 256. However, this model produced poor results. We hypothesize that with this configuration, the model capacity is too large for the dataset described in the previous section. In the end, we ended up using only 6 enco"
2021.iwslt-1.20,P12-3005,0,0.0177994,"Missing"
2021.iwslt-1.20,D15-1166,0,0.188486,"Missing"
2021.iwslt-1.20,P02-1040,0,0.109262,"Missing"
2021.iwslt-1.20,2020.aacl-demo.6,0,0.0637687,"Missing"
2021.jeptalnrecital-deft.6,W19-1909,0,0.0611273,"Missing"
2021.jeptalnrecital-deft.6,2020.bionlp-1.18,0,0.0301815,"Missing"
2021.jeptalnrecital-deft.6,2020.lrec-1.302,1,0.827169,"Missing"
2021.jeptalnrecital-deft.6,W19-5006,0,0.0207882,"Missing"
C02-1061,2001.jeptalnrecital-long.21,1,0.758298,"Missing"
C02-1061,1995.mtsummit-1.1,0,0.015342,"Missing"
C12-1146,C08-1009,0,0.122185,"ife scenarios as well as on analysis windows of more than a few words. Several approximation methods can be used in order to overcome the combinatorial explosion problem. On the one hand, complete approaches, try to reduce dimensionality using pruning techniques and sense selection heuristics. Some examples include: (Hirst and St-Onge, 1998), based on lexical chains that restrict the possible sense combinations by imposing constraints on the succession of relations in a taxonomy (e.g. WordNet); or (Gelbukh et al., 2005) that review general pruning techniques for Lesk-based algorithms; or yet (Brody and Lapata, 2008). On the other hand, incomplete approaches generally use stochastic sampling techniques to reach a local maximum by exploring as little as necessary of the search space. Our present work focuses on such approaches. Furthermore, we can distinguish two possible variants: • local neighbourhood-based approaches (new configurations are created from existing configurations) among which are some approaches from artificial intelligence such as genetic algorithms or optimization methods such as simulated annealing ; • constructive approaches (new configurations are generated by iteratively adding new e"
C12-1146,H92-1046,0,0.725909,"s a paragraph for instance, can not be handled. Thus, such approaches can not be used for applications where real time is a necessary constraint (image retrieval, machine translation, augmented reality). In order to overcome this problem and to perform WSD faster, we are interested in other methods. In this paper, we focus on three methods that globally propagate a local algorithm based on semantic relatedness to the span of a whole text. We consider two unsupervised algorithms from the state of the art, a Genetic Algorithm (GA) (Gelbukh et al., 2003) and a Simulated Annealing (SA) algorithm (Cowie et al., 1992), as well as an adaptation of an Ant Colony Algorithm (ACA) (Schwab et al., 2011). Our aim is to provide an empirical comparison of the ACA with the two other unsupervised algorithms, using the Semeval-2007, Task-7, Coarse grained corpus (Navigli et al., 2007) (both in terms of quality and execution time). Furthermore, we also evaluate the results after applying a majority vote strategy. After a brief review of the state-of-the-art of WSD, the algorithms are described. Subsequently, their implementations are discussed, as well as the estimation of the best parameters and the evaluation of the"
C12-1146,H92-1045,0,0.490009,"ods do for computational reasons). Indeed, in our opinion, using a context window smaller than that of the whole text raises two main issues: no guarantee on the consistency between two selected senses; contradictory sense assignments outside of the window range. For example in the following sentence, considering a window of 6 words: &quot;The two planes were parallel to each other. The pilot had parked them meticulously.&quot;, plane may be disambiguated wrongly due to pilot being outside the window of plane. Furthermore it can be detrimental to the semantic unity in the disambiguation, given that as (Gale et al., 1992) or (Hirst and St-Onge, 1998) pointed out, two words used several times in the same context tend to have the same sense. Therefore, some algorithms that are similar to our Ant Colony Algorithm but that use a context window have not been studied here (notably the adaptation (Mihalcea et al., 2004) of PageRank (Brin and Page, 1998) to WSD). Moreover, we are not interested in comparing these incomplete algorithm, which cannot pragmatically be used in a real-life context, to the optimal disambiguation (Brute Force). Even with a reduced windows of the context and weeks of execution time we were onl"
C12-1146,C04-1162,0,0.067379,"Missing"
C12-1146,S07-1006,0,0.464786,"aster, we are interested in other methods. In this paper, we focus on three methods that globally propagate a local algorithm based on semantic relatedness to the span of a whole text. We consider two unsupervised algorithms from the state of the art, a Genetic Algorithm (GA) (Gelbukh et al., 2003) and a Simulated Annealing (SA) algorithm (Cowie et al., 1992), as well as an adaptation of an Ant Colony Algorithm (ACA) (Schwab et al., 2011). Our aim is to provide an empirical comparison of the ACA with the two other unsupervised algorithms, using the Semeval-2007, Task-7, Coarse grained corpus (Navigli et al., 2007) (both in terms of quality and execution time). Furthermore, we also evaluate the results after applying a majority vote strategy. After a brief review of the state-of-the-art of WSD, the algorithms are described. Subsequently, their implementations are discussed, as well as the estimation of the best parameters and the evaluation of the tested algorithms. Finally, an analysis of the results is presented as well as a comparison to other systems on Semeval 2007 Task 7. Then, we conclude and propose some perspectives for future work. 2 Brief State of the art of Word Sense Disambiguation In simpl"
C12-1146,P10-1154,0,0.0407808,"Local and global WSD Algorithms 3.1 Our local algorithm : A variant of the Lesk Algorithm Our local algorithm is a variant of the Lesk Algorithm (Lesk, 1986). Proposed more than 25 years ago, it is simple, only requires a dictionary and no training. The score given to a sense pair is the number of common words (space separated strings) in the definition of the senses, without taking into account neither the word order in the definitions (bag-of-words approach), nor any syntactic or morphological information. Variants of this algorithm are still today among the best on English-language texts (Ponzetto and Navigli, 2010). Our local algorithm exploits the links provided by WordNet: it considers not only the definition of a sense but also the definitions of the linked senses (using all the semantic relations from WordNet) following (Banerjee and Pedersen, 2002), henceforth referred as E x t Lesk1 . Contrarily to Banerjee, however, we do not consider the sum of squared sub-string overlaps, but merely a bag-of-words overlap that allows us to generate a dictionary from WordNet, where each word contained in any of the word sense definitions is indexed by a unique integer and where each resulting definition is sorte"
C12-1146,J98-1001,0,\N,Missing
E17-2066,L16-1662,1,0.940445,"ulated as follows: The main idea of word embeddings is that their representation is obtained according to the context (the words around it). The words are projected on a continuous space and those with similar context should be close in this multi-dimensional space. A similarity between two word vectors can be measured by cosine similarity. So using wordembeddings for plagiarism detection is appealing since they can be used to calculate similarity between sentences in the same or in two different languages (they capture intrinsically synonymy and morphological closeness). We use the MultiVec (Berard et al., 2016) toolkit for computing and managing the continuous representations of the texts. It includes word2vec (Mikolov et al., 2013), paragraph vector (Le and Mikolov, 2014) and bilingual distributed representations (Luong et al., 2015) features. The corpus used to build the vectors is the News Commentary2 parallel corpus. For training our embeddings, we use CBOW model with a vector size of 100, a window size of 5, a negative sampling parameter of 5, and an alpha of 0.02. 3.1 V = (vector(ui )) (2) i=1 where ui is the ith word of the textual unit and vector is the function which gives the word embeddin"
E17-2066,D14-1082,0,0.0897818,"Missing"
E17-2066,L16-1657,1,0.604763,"Missing"
E17-2066,petrov-etal-2012-universal,0,0.162602,"Missing"
E17-2066,L16-1046,0,0.0676319,"Missing"
E17-2066,P16-1157,0,0.0576904,"Missing"
E17-2066,W15-1521,0,\N,Missing
L16-1657,C10-1005,0,0.0571791,"Missing"
L16-1657,F13-2001,0,0.0107288,"ch document in the corpus. Then, using this information, parallel English-Spanish pairs are extracted. The process led to nearly 3,000 document pairs. • Conference papers. So far, no corpus includes scientific texts, this is why we collected conference papers that were initially published in one language and then translated by their authors to be published in another language. For practical reasons, we focused exclusively on articles published first in French and then in English. The BibTeX file of French speaking conferences in NLP (the 1997-2014 TALN archives, made available in the works of Boudin (2013)6 and the 2006-2011 RNTI collection made available by the challenge of the EGC 2016 conference7 ) were parsed to extract the names of the authors of each article. Then, names were used as queries in Google Scholar and Google Search Engine. Papers in PDF format corresponding to the most relevant search results were downloaded. We detected the language of each downloaded file according to the Cavnar and Trenkle (1994) classification algorithm and each English candidate file was manually checked to see if a significant part of it was related to one of the French original documents cited in the Bi"
L16-1657,J93-2003,0,0.0530759,"in( |Vx ∩ Vy |− |Vx  Vy |, |Vy ∩ Vx |− |Vy  Vx |) (4) 2.4. Cross-Language Alignment-based Similarity Analysis (CL-ASA) CL-ASA is introduced for the first time by Barr´on-Cede˜no et al. (2008) and developed subsequently by Pinto et al. (2009). The aim of the method is to determinate how a textual unit d written in the language L is potentially the translation of an other textual unit d0 written in a language L0 . CL-ASA involves the creation of a bilingual unigram dictionary which contains the statistical probabilities of translations pairs determined from a parallel corpus. The IBM-1 model (Brown et al., 1993) can be adopted using only the lexical translations. Pinto et al. (2009) proposed a formula that factored the alignment function. Let x and y, two sentences, such as xj is the j th word of the sentence x and yi , the ith word of the sentence y. We want to know the probability p(x, y) that x is the translation of y. p(x, y) = |x| Y p(xj |y) (5) 1 p(xj |yi ) |y |+ 1 (6) j=1 where p(xj |y) = |y| X i=1 Improvements of the method were later proposed. For example, consider for each word x, only the best translations y, above a minimum probability (threshold of 0.4 according to the work of Barr´on-Ce"
L16-1657,2012.eamt-1.60,0,0.0843742,"Missing"
L16-1657,2005.mtsummit-papers.11,0,0.0111405,"w better results on monolingual textual comparisons (Barr´on-Cede˜no et al., 2009). Because machine translation tools can give too multiple translations (all correct but being substantially different) and therefore it is not advisable to make a monolingual alignment with lexical or syntactic methods (Barr´on-Cede˜no et al., 2010). 3. Dataset for the cross-language plagiarism detection task 3.1. Existing corpora There are many multi-language and cross-language dataset listed by OPUS2 website. One example of these most (8) 4164 2 http://opus.lingfil.uu.se/ used corpora is undoubtedly Europarl3 (Koehn, 2005). It is a widely used corpus in cross-language text analysis and machine translation. It is a parallel corpus consisting of the European Parliament exchanges transcriptions, about nearly 10,000 parallel documents in more than 21 languages spoken across the European Union. Similarly, JRC-Acquis4 is also often used in cross-language NLP or translation research. It is a parallel corpus, representing extracts of Acquis Communautaire (applicable laws in the European Union states), available in over 20 languages. As well, Wikipedia is often used as a comparable corpus in multiple languages. These la"
L16-1657,C10-2115,0,0.0905716,"Missing"
L16-1657,P10-1114,0,0.0293547,"00 parallel documents in more than 21 languages spoken across the European Union. Similarly, JRC-Acquis4 is also often used in cross-language NLP or translation research. It is a parallel corpus, representing extracts of Acquis Communautaire (applicable laws in the European Union states), available in over 20 languages. As well, Wikipedia is often used as a comparable corpus in multiple languages. These last two, i.e. JRC-Acquis and Wikipedia, were used by Potthast et al. (2011) for crosslanguage plagiarism detection. Finally, another interesting collection of documents is the one gathered by Prettenhofer and Stein (2010) who collected Amazon Product Reviews (books, DVD and music albums) for a cross-language sentiment analysis task (Google Translate was used to build the parallel corpus). 3.2. books freely available on the Gutenberg Project website5 . The extraction process involves analyzing XML files containing the metadata of each document in the corpus. Then, using this information, parallel English-Spanish pairs are extracted. The process led to nearly 3,000 document pairs. • Conference papers. So far, no corpus includes scientific texts, this is why we collected conference papers that were initially publ"
L18-1166,S07-1054,0,0.0144781,"rns from examples contained in the annotated corpus. For instance, the OMSTI (Taghipour and Ng, 2015) was created for this purpose. • Evaluate a WSD system by comparing its output to the annotations in the corpus, as it is the case for instance with corpora created as part of the evaluation campaigns SensEval-SemEval. After their distribution, there is no scientific reason not to use indistinctly these corpora either for building a WSD system, for estimating the distribution of senses or for evaluating a WSD system. Indeed, the SemCor is used since a long time for the learning of WSD systems (Chan et al., 2007; Navigli et al., 2007) or more recently for the evaluation of different methods (Yuan et al., 2016). This last usage is still very rare, since it is one of the first experiment that we found in the literature, along with (M`arquez et al., 2002). However, the format of the resources differs greatly depending on their original purpose. For the SemCor, a single file groups all the information, whereas in the case of the evaluation corpora, there are two files: one that contains the unannotated corpus, and the other that contains the sense annotations. In some corpora, like in the DSO and the OMS"
L18-1166,D14-1110,0,0.0548277,"Missing"
L18-1166,H92-1046,0,0.427313,"Missing"
L18-1166,P00-1064,0,0.0451383,"Missing"
L18-1166,H93-1061,0,0.906383,"with an identifier of sense from 1027 a specific lexical database. For example, all words in the corpus of the 7th task of the SemEval 2007 semantic evaluation campaign (Navigli et al., 2007) are annotated with sense identifiers from WordNet 2.1, whereas in the English corpus of the 13th task of SemEval 2015 (Moro and Navigli, 2015), all words are annotated with sense identifiers from WordNet 3.0, BabelNet 2.5 and Wikipedia pages. There are at least three reasons to create a sense annotated corpus: • Estimate the distribution of senses in the language. It is for this purpose that the SemCor (Miller et al., 1993) was annotated. Consequently, the senses in WordNet are, since version 1.7, sorted by this distribution of senses estimated on the SemCor. • Build a Word Sense Disambiguation system which learns from examples contained in the annotated corpus. For instance, the OMSTI (Taghipour and Ng, 2015) was created for this purpose. • Evaluate a WSD system by comparing its output to the annotations in the corpus, as it is the case for instance with corpora created as part of the evaluation campaigns SensEval-SemEval. After their distribution, there is no scientific reason not to use indistinctly these cor"
L18-1166,C12-1109,0,0.0622556,"Missing"
L18-1166,S15-2049,0,0.493012,"the learning process of a Word Sense Disambiguation (WSD) system, the importance of sense annotated corpora in Natural Language Processing (NLP) is considerable. On one hand, the evaluation in vivo, i.e. the evaluation of a WSD system as part of a larger task, has never been really exploited. On the other hand, the evaluation in vitro, which uses directly sense annotated corpora by comparing the output of a system to manual annotations, is predominant. Moreover, WSD systems exploiting examples from sense annotated corpora are generally far better than those which do not (Navigli et al., 2007; Moro and Navigli, 2015). At the time of its creation, WordNet (Miller, 1995) was undoubtedly the only lexical database freely available for English. Since the beginning of the 2000s, it has become the de facto standard for WSD in this language. Indeed, most of sense annotated corpora are either directly annotated with WordNet sense keys or they are annotated with a sense inventory linked to the senses of WordNet, such as BabelNet (Navigli and Ponzetto, 2010). However, it is not trivial to use these corpora, because most of them differ in their format and on the version of WordNet they use. As a consequence, very few"
L18-1166,P10-1023,0,0.0205213,"ions, is predominant. Moreover, WSD systems exploiting examples from sense annotated corpora are generally far better than those which do not (Navigli et al., 2007; Moro and Navigli, 2015). At the time of its creation, WordNet (Miller, 1995) was undoubtedly the only lexical database freely available for English. Since the beginning of the 2000s, it has become the de facto standard for WSD in this language. Indeed, most of sense annotated corpora are either directly annotated with WordNet sense keys or they are annotated with a sense inventory linked to the senses of WordNet, such as BabelNet (Navigli and Ponzetto, 2010). However, it is not trivial to use these corpora, because most of them differ in their format and on the version of WordNet they use. As a consequence, very few works in the literature of WSD are trained or evaluated on more than two annotated corpora. Also, WSD systems are systematically evaluated on corpora that have been initially created for the purpose of evaluation, and never on corpora that have been created for another purpose, such as training or for sense distribution estimation, whereas there is no scientific reason for that. This paper presents a work of unification of all existin"
L18-1166,S07-1006,0,0.081942,"Missing"
L18-1166,E17-1010,0,0.421311,"ing one is used for the evaluation, then switch the corpora and do this for every existing corpus. The language resource that we provide contains all English sense annotated corpora in UFSAC (Unified Format for Sense Annotated Corpora), the format that we propose, with sense annotations converted to the last version of WordNet (3.0), along with Java code to easily read, write and modify any corpus in this format, and scripts for converting a corpus from its original format to UFSAC. Our work is the continuity of the demonstration of (Vial et al., 2017), and it differs from the recent work of (Raganato et al., 2017) in several points. Their work is focused on the evaluation of WSD systems, whereas we provide a complete API for manipulating corpora in a new unified format (UFSAC), and conversion scripts allowing the full reconstruction of the corpora from the original data. We also propose five additional corpora in our resource among the most difficult to parse. In our resource, we provide a script for converting a corpus from our format to theirs, so existing WSD systems that rely on their format can be trained or evaluated on any of the corpus that we produced. We also provide a script for converting t"
L18-1166,C12-1146,1,0.883405,"Missing"
L18-1166,K15-1037,0,0.117918,"task of SemEval 2015 (Moro and Navigli, 2015), all words are annotated with sense identifiers from WordNet 3.0, BabelNet 2.5 and Wikipedia pages. There are at least three reasons to create a sense annotated corpus: • Estimate the distribution of senses in the language. It is for this purpose that the SemCor (Miller et al., 1993) was annotated. Consequently, the senses in WordNet are, since version 1.7, sorted by this distribution of senses estimated on the SemCor. • Build a Word Sense Disambiguation system which learns from examples contained in the annotated corpus. For instance, the OMSTI (Taghipour and Ng, 2015) was created for this purpose. • Evaluate a WSD system by comparing its output to the annotations in the corpus, as it is the case for instance with corpora created as part of the evaluation campaigns SensEval-SemEval. After their distribution, there is no scientific reason not to use indistinctly these corpora either for building a WSD system, for estimating the distribution of senses or for evaluating a WSD system. Indeed, the SemCor is used since a long time for the learning of WSD systems (Chan et al., 2007; Navigli et al., 2007) or more recently for the evaluation of different methods (Yu"
L18-1166,N03-1033,0,0.00904342,"ith annotations on different words, and creates a single sentence containing all annotations. Thus, this steps adds a crucial information for some WSD systems. For instance, a similarity-based WSD system can now “learn” that two word senses are often located in the same sentence. 3 http://wordnet.princeton.edu/glosstag.shtml 4.1.3. Lemma and POS tagging For the corpora that do not already contain these informations, we added the lemma for every word, when existing, using the WordNet’s morphy tool, and the part-of-speech tag from the Penn Treebank tag set using Stanford’s Loglinear POS tagger (Toutanova et al., 2003). 4.1.4. Cleaning Finally, this last step consists of trimming words, removing invisible characters and removing inconsistent annotations, for instance when the part of speech annotation differs from the part of speech of the sense annotation. 4.2. UFSAC File format Our approach for the unification of the different annotated corpora begins with a file format that is descriptive, easily understandable and readable by a human, and at the same time, efficient for a program to parse and create. Finally, it should be able to contain all the information contained in the original resources. These inf"
L18-1166,2016.jeptalnrecital-long.13,1,0.784695,"Missing"
L18-1166,C16-1130,0,0.251677,"5) was created for this purpose. • Evaluate a WSD system by comparing its output to the annotations in the corpus, as it is the case for instance with corpora created as part of the evaluation campaigns SensEval-SemEval. After their distribution, there is no scientific reason not to use indistinctly these corpora either for building a WSD system, for estimating the distribution of senses or for evaluating a WSD system. Indeed, the SemCor is used since a long time for the learning of WSD systems (Chan et al., 2007; Navigli et al., 2007) or more recently for the evaluation of different methods (Yuan et al., 2016). This last usage is still very rare, since it is one of the first experiment that we found in the literature, along with (M`arquez et al., 2002). However, the format of the resources differs greatly depending on their original purpose. For the SemCor, a single file groups all the information, whereas in the case of the evaluation corpora, there are two files: one that contains the unannotated corpus, and the other that contains the sense annotations. In some corpora, like in the DSO and the OMSTI, there is one file for every lemma in the dictionary, and each file contains thousands of example"
S13-2041,C08-1009,0,0.373973,"Missing"
S13-2041,S13-2040,0,0.184228,"Missing"
S13-2041,P10-1154,0,0.351195,"Missing"
S13-2041,C12-1146,1,0.913991,"Missing"
S13-2041,W12-6108,1,0.896185,"Missing"
S13-2041,P94-1019,0,\N,Missing
S17-2012,E17-2066,1,0.839973,"Missing"
S17-2012,S16-1081,0,0.194569,"track). Correlation results of all participants (including ours) on track 4b were much lower and we try to explain why (and question the validity of track 4b) in the last part of this paper. Introduction CompiLIG is a collaboration between Compilatio1 - a company particularly interested in crosslanguage plagiarism detection - and LIG research group on natural language processing (GETALP). Cross-language semantic textual similarity detection is an important step for cross-language plagiarism detection, and evaluation campaigns in this new domain are rare. For the first time, SemEval STS task (Agirre et al., 2016) was extended with a Spanish-English cross-lingual sub-task in 2016. This year, sub-task was renewed under track 4 (divided in two sub-corpora: track 4a and track 4b). Given a sentence in Spanish and a sentence in English, the objective is to compute their semantic textual similarity according to a score from 0 1 www.compilatio.net 109 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 109–114, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics 2 2.1 Cross-Language Textual Similarity Detection Methods available tran"
S17-2012,L16-1662,1,0.863744,"sub-task was renewed under track 4 (divided in two sub-corpora: track 4a and track 4b). Given a sentence in Spanish and a sentence in English, the objective is to compute their semantic textual similarity according to a score from 0 1 www.compilatio.net 109 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 109–114, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics 2 2.1 Cross-Language Textual Similarity Detection Methods available translations from DBNary to build the bag-of-words of a word. We use the MultiVec (Berard et al., 2016) toolkit for computing and managing word embeddings. The corpora used to build the embeddings are Europarl and Wikipedia sub-corpus, part of the dataset of Ferrero et al. (2016)2 . For training our embeddings, we use CBOW model with a vector size of 100, a window size of 5, a negative sampling parameter of 5, and an alpha of 0.02. So, the sets of words Sx0 and Sy0 are the conceptual representations in the same language of Sx and Sy respectively. To calculate the similarity between Sx and Sy , we use a syntactically and frequentially weighted augmentation of the Jaccard distance, defined as: Cr"
S17-2012,D15-1075,0,0.0595905,"Missing"
S17-2012,S16-1089,0,0.0613244,"tail them here. The lines in bold represent the methods that obtain the best mean score in each category of system (best method alone, unsupervised and supervised fusion). The scores for the supervised systems are obtained with 10-folds cross-validation. Translation + Monolingual Word Alignment (T+WA) The last method used is a two-step process. First, we translate the Spanish sentence into English with Google Translate (i.e. we are bringing the two sentences in the same language). Then, we align both utterances. We reuse the monolingual aligner4 of Sultan et al. (2015) with the improvement of Brychcin and Svoboda (2016), who won the cross-lingual sub-task in 2016 (Agirre et al., 2016). Because this improvement has not been released by the initial authors, we propose to share our re-implementation on GitHub5 . If Sx and Sy are two sentences in the same language, then we try to measure their similarity with the following formula: ω(Ax ) + ω(Ay ) ω(Sx ) + ω(Sy ) (7) where idf is the function which gives the inverse document frequency of a word. where wi is the ith word of the sentence S, vector is the function which gives the word embedding vector of a word, ϕ is the same that in formula (4), and . is the scala"
S17-2012,petrov-etal-2012-universal,0,0.0234669,"Missing"
S17-2012,S17-2001,0,0.0332907,"Missing"
S17-2012,L16-1657,1,0.879392,"Missing"
S17-2012,S15-2027,0,0.0983005,"Missing"
S17-2017,S15-2045,0,0.0292701,"s to support the identification of words that are highly descriptive in each sentence. LIM-LIG system achieves a Pearsons correlation of 0.74633, ranking 2nd among all participants in the Arabic monolingual pairs STS task organized within the SemEval 2017 evaluation campaign. 1 Didier Schwab LIG-GETALP Univ. Grenoble Alpes France Introduction 2 Semantic Textual Similarity (STS) is an important task in several application fields, such as information retrieval, machine translation, plagiarism detection and others. STS measures the degree of similarity between the meanings of two text sequences (Agirre et al., 2015). Since SemEval 2013, STS has been one of the official shared tasks. This is the first year in which SemEval has organized an Arabic monolingual pairs STS. The challenge in this task lies in the interpretation of the semantic similarity of two given Arabic sentences, with a continuous valued score ranging from 0 to 5. The Arabic STS measurement could be very useful for several areas, including: disguised plagiarism detection, word-sense disambiguation, laWord Embedding Models In the literature, several techniques are proposed to build word-embedding model. For instance, Collobert and Weston (2"
S17-2017,chen-eisele-2012-multiun,0,0.0281641,"en evaluated and compared, and they show that CBOW and SKIP-G are significantly faster to train with better accuracy compared to these techniques. For this reason, we have used the CBOW word representations for Arabic model1 proposed by Zahran et al. (2015). To train this model, they have used a large collection from different sources counting more than 5.8 billion words including: Arabic Wikipedia (WikiAr, 2006), BBC and CNN Arabic corpus (Saad and Ashour, 2010), Open parallel corpus (Tiedemann, 2012), Arabase Corpus (Raafat et al., 2013), Osac corpus (Saad and Ashour, 2010), MultiUN corpus (Chen and Eisele, 2012), KSU corpus (ksucorpus, 2012), Meedan Arabic corpus (Meedan, 2012) and other (see Zahran et al. 2015). 3.2  That means that, the words ” éJ ÊË@” (faculty) and  ” éªÓAm.Ì '@” (university) are semantically closer than  ” ZAÖÏ @ ” (evening) and ” éªÓAm.Ì '@” (university). System Description 3.1  sim( éJ ÊË@, éªÓAm.Ì '@) = cos(V ( éªÓAm.Ì '@), V ( éJ ÊË@)) = 0.72 3.3.1 No Weighting Method A simple way to compare two sentences, is to sum their words vectors. In addition, this method can be applied to any size of sentences. The similarity between S1 and S2 is obtained by calculating the co"
S17-2017,tiedemann-2012-parallel,0,0.0249122,"(Collobert and Weston, 2008), (Turian et al., 2010), (Mnih and Hinton, 2009), (Mikolov et al., 2013c) have been evaluated and compared, and they show that CBOW and SKIP-G are significantly faster to train with better accuracy compared to these techniques. For this reason, we have used the CBOW word representations for Arabic model1 proposed by Zahran et al. (2015). To train this model, they have used a large collection from different sources counting more than 5.8 billion words including: Arabic Wikipedia (WikiAr, 2006), BBC and CNN Arabic corpus (Saad and Ashour, 2010), Open parallel corpus (Tiedemann, 2012), Arabase Corpus (Raafat et al., 2013), Osac corpus (Saad and Ashour, 2010), MultiUN corpus (Chen and Eisele, 2012), KSU corpus (ksucorpus, 2012), Meedan Arabic corpus (Meedan, 2012) and other (see Zahran et al. 2015). 3.2  That means that, the words ” éJ ÊË@” (faculty) and  ” éªÓAm.Ì '@” (university) are semantically closer than  ” ZAÖÏ @ ” (evening) and ” éªÓAm.Ì '@” (university). System Description 3.1  sim( éJ ÊË@, éªÓAm.Ì '@) = cos(V ( éªÓAm.Ì '@), V ( éJ ÊË@)) = 0.72 3.3.1 No Weighting Method A simple way to compare two sentences, is to sum their words vectors. In addition, this"
S17-2017,E17-2066,1,0.663624,"Missing"
S17-2017,N13-1090,0,0.418284,"the literature, several techniques are proposed to build word-embedding model. For instance, Collobert and Weston (2008) have proposed a unified system based on a deep neural network architecture. Their word embedding model is stored in a matrix M ∈ Rd∗|D |, where D is a dictionary of all unique words in the training data, and each word is embedded into a d-dimensional vector. Mnih and Hinton (2009) have proposed the Hierarchical Log-Bilinear Model (HLBL). The HLBL Model concatenates the n − 1 first embedding words (w1 ..wn−1 ) and learns a neural linear model to predicate the last word wn . Mikolov et al. (2013a, 2013b) have proposed 134 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 134–138, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics (university), ” ZAÖÏ @” (evening) and ” éJ ÊË@” (faculty) be three words. The similarity between them is measured by computing the cosine similarity between their vectors as follows:   two other approaches to build a words representations in vector space. The first one named the continuous bag of word model CBOW (Mikolov et al., 2013a), predicts a pivot word according to the"
W08-1902,dutoit-nugues-2002-algorithm,0,0.643129,"onary (WordNet) and an encyclopedia (Wikipedia) (http://www.onelook.com/reverse-dictionary.shtml). 11 idea or concept,9 and the system will display all connected words. If the user can find the item he is looking for in this list, search stops, otherwise it will continue, the user giving other words of the list, or words evoked by them. Of course, remains the question of how to build this resource, in particular, how to populate the axis devoted to the trigger words, i.e. accesskeys. At present we consider three approaches: one, where we use the words occurring in word definitions (see also, (Dutoit and Nugues, 2002; Bilac et al., 2004)), the other is to mine a wellbalanced corpus, to find co-occurrences within a given window (Ferret and Zock, 2006), the size depending a bit on the text type (encyclopedia) or type of corpus. Still another solution would be to draw on the association lists produced by psychologists, see for example http://www.usf.edu/, or http://www.eat.rl.ac.uk. Of course, the idea of using matrices in linguistics is not new. There are at least two authors who have proposed its use: M. Gross (Gross, 1984) used it for coding the syntactic behavior of lexical items, hence the term lexicon-"
W08-1902,P06-1036,1,0.883456,"ll display all connected words. If the user can find the item he is looking for in this list, search stops, otherwise it will continue, the user giving other words of the list, or words evoked by them. Of course, remains the question of how to build this resource, in particular, how to populate the axis devoted to the trigger words, i.e. accesskeys. At present we consider three approaches: one, where we use the words occurring in word definitions (see also, (Dutoit and Nugues, 2002; Bilac et al., 2004)), the other is to mine a wellbalanced corpus, to find co-occurrences within a given window (Ferret and Zock, 2006), the size depending a bit on the text type (encyclopedia) or type of corpus. Still another solution would be to draw on the association lists produced by psychologists, see for example http://www.usf.edu/, or http://www.eat.rl.ac.uk. Of course, the idea of using matrices in linguistics is not new. There are at least two authors who have proposed its use: M. Gross (Gross, 1984) used it for coding the syntactic behavior of lexical items, hence the term lexicon-grammar, and G. Miller, the father of WN (Miller et al., 1990) suggested it to support lexical access. While the former work is not rele"
W08-1902,P98-2180,0,0.117161,"Missing"
W08-1902,P84-1058,0,0.290431,"s: one, where we use the words occurring in word definitions (see also, (Dutoit and Nugues, 2002; Bilac et al., 2004)), the other is to mine a wellbalanced corpus, to find co-occurrences within a given window (Ferret and Zock, 2006), the size depending a bit on the text type (encyclopedia) or type of corpus. Still another solution would be to draw on the association lists produced by psychologists, see for example http://www.usf.edu/, or http://www.eat.rl.ac.uk. Of course, the idea of using matrices in linguistics is not new. There are at least two authors who have proposed its use: M. Gross (Gross, 1984) used it for coding the syntactic behavior of lexical items, hence the term lexicon-grammar, and G. Miller, the father of WN (Miller et al., 1990) suggested it to support lexical access. While the former work is not relevant for us here, Miller’s proposal is. What are the differences between his proposal and ours? There are basically four main differences: chologists have gathered in their association experiments (Jung and Riklin, 1906; Deese, 1965; Schvaneveldt, 1989). Note, that instead of putting a boolean value at the intersection of the tw and the aw , we will put weights and the type of"
W08-1902,C96-1002,0,0.0501127,"different for the speaker and listener, even if both of them draw on the same resource. When speaking or writing we encounter basically either of the following two situations: one where everything works automatically, somehow like magic, words popping up one after another c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 9 Coling 2008: Proceedings of the workshop on Cognitive Aspects of the Lexicon (COGALEX 2008), pages 9–17 Manchester, August 2008 cording to Levelt (Levelt, 1996) the generation of words (synthesis) involves the following stages: conceptual preparation, lexical selection, phonological- and phonetic encoding, articulation. Bear in mind that having performed ’lexical selection’ does not imply access to the phonetic form (see the experiments on the tip-of-the-tongue phenomenon). Neither do we know how to represent them. Yet, there are ways around this problem as we will show. Whether concepts and words are organized and accessed differently is a question we cannot answer here. We can agree though on the fact that getting information concerning words is fa"
W08-1902,C00-1072,0,0.0776676,"s on underspecified, i.e. imperfect input. To achieve this goal we’ve started building an AM composed of form elements (the words and expressions of a given language) and aws . The role of the latter being to lead to or to evoke the tw . In the last part we’ve described briefly the results obtained by comparing two resources (WN and Wikipedia) and various inputs. Given the fact that the project is still quite young, only preliminary results can be shown at this point. Our next steps will be to take a closer look at the following work: clustering of similar words (Lin, 1998), topic signatures (Lin and Hovy, 2000) and Kilgariff’s sketch engine (Kilgarriff et al., 2004). We plan also to add other lexical functions to enrich our database with aws . We plan to experiment with corpora, trying to find out which ones are best for our purpose15 and we will certainly experiment with the window size 16 to see which size is best for which text type. Finally, we plan to insert in our AM the relations holding between the aw and the tw . As these links are contained in our corpus, we should be able to identify and type them. The question is, to what extent this can be done automatically. Obviously, the success of o"
W08-1902,P98-2127,0,0.0700144,"ical access based 15 References on underspecified, i.e. imperfect input. To achieve this goal we’ve started building an AM composed of form elements (the words and expressions of a given language) and aws . The role of the latter being to lead to or to evoke the tw . In the last part we’ve described briefly the results obtained by comparing two resources (WN and Wikipedia) and various inputs. Given the fact that the project is still quite young, only preliminary results can be shown at this point. Our next steps will be to take a closer look at the following work: clustering of similar words (Lin, 1998), topic signatures (Lin and Hovy, 2000) and Kilgariff’s sketch engine (Kilgarriff et al., 2004). We plan also to add other lexical functions to enrich our database with aws . We plan to experiment with corpora, trying to find out which ones are best for our purpose15 and we will certainly experiment with the window size 16 to see which size is best for which text type. Finally, we plan to insert in our AM the relations holding between the aw and the tw . As these links are contained in our corpus, we should be able to identify and type them. The question is, to what extent this can be done aut"
W08-1902,W04-2105,1,0.746284,"less for this kind of corpus: (a) size: though, all words are tagged, the corpus remains small as it contains only 63.941 different words; (b) in consequence, the corpus lacks many syntagmatic associations encoding encyclopedic knowledge. 12 http://www.wikipedia.org The optimal window-size depends probably on the text type (encyclopedia vs. unformatted text). Yet, in the absence of clear criteria, we consider the optimal window-size as an open, empirical question. 14 This latter aspect is not implemented yet, but will be added in the future, as it is a necessary component for easy navigation (Zock and Bilac, 2004; Zock, 2006; Zock, 2007). 13 14 5.3.3 Corpus Building Input We start arbitrarily from some page (for our experiment, we have chosen ”wine” as input), apply the algorithm outlined here above and pick then randomly a noun within this page to fetch with this input a new page on Wikipedia. This process is repeated until a given sample size is obtained (in our case 1000 pages). Of course, instead of picking randomly a noun, we could have decided to process all the nouns of a given page, and to add then incrementally the nouns of the next pages. Yet, doing this would have led us to privilege a spec"
W08-1902,C98-2175,0,\N,Missing
W08-1902,W06-1007,0,\N,Missing
W08-1902,C98-2122,0,\N,Missing
W10-3411,C00-1072,0,0.0107701,"tained in the material we use, since our resource will be based on this data. Hence, taking as corpus only the newspapers read by an elite (say, Le Monde, in France), will surely not sufﬁce to capture the information we need, as it will not relate information ordinary citizens, say sport fans, are familiar with or interested in. In sum, we need to take a wide variety of sources to extract then the needed information. While there is shortage of some document types needed, there are nevertheless quite a few sources one may consider to begin with: Wikipedia, domain taxonomies, topic signatures, (Lin and Hovy, 2000), a database like (http://openrdf.org), etc. 2.2 Building the resource There are two elements we need to get a clearer picture of: the nature of the resource (semantic map), and the search method i.e. the way to explore it. Concerning the resource, there are many possible sources (dictionary, thesaurus, corpora, or a mix of all this) and many ways of building it. Since our main goal is the building of an index based on the notion of word relations (triples composed of two terms and a link), the two prime candidates are of course corpora and association lists like the ones collected by psycholo"
W10-3411,P06-1036,1,0.802579,"m may be due to the distance between the source and the target word: the link may be mediated. • Topic sensitivity Weights are important, but they tend to change dynamically with time and the topic. Think of the word ’piano’ uttered in the contexts of a ’concert’ or ’household moving’. It is only in this latter case that this term evokes ideas like size or weight. The dynamic recompution of weights as a function of topic changes requires that the system be able to recognize the topic changes, as otherwise it might mislead the user by providing of inadequate weights. For some initial work see (Ferret and Zock, 2006). Association lists: Psychologists have built such lists already decades ago (Deese, 1965; Schvaneveldt, 1989). Similar lists are nowadays freely available on the web. For example, for English there is the Edinburgh Associative Thesaurus 6 and the compilation done by Nelson and his colleagues in Florida 7 . There are also some re6 7 77 http://www.eat.rl.ac.uk/ http://cyber.acomp.usf.edu/FreeAssociation/ sources for German (see 8 or 9 ), for Japanese,10 and probably many other languages. While association lists are generally built manually, one can also try to do so automatically or with the he"
W10-3411,W04-2105,1,0.525696,"ts have built such lists already decades ago (Deese, 1965; Schvaneveldt, 1989). Similar lists are nowadays freely available on the web. For example, for English there is the Edinburgh Associative Thesaurus 6 and the compilation done by Nelson and his colleagues in Florida 7 . There are also some re6 7 77 http://www.eat.rl.ac.uk/ http://cyber.acomp.usf.edu/FreeAssociation/ sources for German (see 8 or 9 ), for Japanese,10 and probably many other languages. While association lists are generally built manually, one can also try to do so automatically or with the help of people (see section 5 in (Zock and Bilac, 2004)). JeuxdeMot (JdM), a collectively built resource focusing on French being an example in case.11 2.3 Searching The goal of searching is more complex than one might think. Of course, ultimately one should ﬁnd the object one is looking for,12 but the very process should also be carried out quickly and naturally. In addition we want to allow for recovery in case of having taken the wrong turn, and we want to avoid looping, that is, walking in circles, without ever getting closer to the goal. Last, but not least we want to make sure that stored information can also be accessed. That this is less o"
W10-3411,W08-1902,1,0.848952,"on French being an example in case.11 2.3 Searching The goal of searching is more complex than one might think. Of course, ultimately one should ﬁnd the object one is looking for,12 but the very process should also be carried out quickly and naturally. In addition we want to allow for recovery in case of having taken the wrong turn, and we want to avoid looping, that is, walking in circles, without ever getting closer to the goal. Last, but not least we want to make sure that stored information can also be accessed. That this is less obvious than it might seem at ﬁrst sight has been shown by (Zock and Schwab, 2008). Taking two resources (WN and Wikipedia) that contain both a given target word, we wanted to see whether we could access it or not. The target word was vintage. In order to ﬁnd it we provided two access keys, i.e. trigger words: wine and harvest. Combining the two produced a list of 6 items in the case of WN and 45 in the case of Wikipedia, yet, while the latter displayed the target word, it was absent from the list produced by WN. This example illustrates the fact that our claim concerning storage and acess is well founded. Having stored something does by no means guarantee its access."
W10-4009,lavie-etal-2002-nespole,0,0.0174981,"uting the angular distance between two conceptual vectors (Schwab and Lafourcade, 2007). In our case, conceptual vectors are used for automatic disambiguation of texts. Using this method, we calculate confidence score for each UW hypothesis appearing in the Q-Graph. 5 Ontology driven content extraction The content extraction has to be leaded by a “knowledge base” containing the informations we want to retrieve. 5.1 Previous works in content extraction This approach has its roots in machine translation projects such as C-Star II (1993-1999) (Blanchon and Boitet, 2000) and Nespole! (2000-2002) (Metze et al., 2002), for on the fly translation of oral speech acts in the domain of tourism. In these projects, semantic transfer was achieved through an IF (Inter-exchange Format), that is a semantic pivot dedicated to the domain. This IF allows to store information extracted from texts but is although used to lead the content extraction process by giving a formal representation of the relevant informations to extract, according to the domain. The Nespole! IF consists of 123 concepts from the tourism domain, associated with several arguments and associable with speech acts markers. The extraction process is ba"
W12-6108,H92-1046,0,0.289749,"yet many of these methods have in common a sizeable complexity, reflected by a number of parameters that need to be tuned in order to obtain the best performance. Depending on the number of parameters, it can be very difficult for a human to directly estimate the optimal parameters. Even then, it remains a question of guesswork with no guarantee of success. This issue is all the more salient with algorithms and models that typically involve many parameters upon which the results greatly depend. Such algorithms for WSD include Neural Networks (Veronis and Ide, 1990), Simulated Annealing (SA) (Cowie et al., 1992), Genetic Algorithms (GA) (Gelbukh et al., 2003) and Ant Colony Algorithms (ACA) (Schwab and Guillaume, 2011), and many other machine learning methods. In such approaches, the values of the parameters are paramount. They are what make the algorithms work for specific applications such as WSD. Yet, the authors of the aforementioned articles give little insight as to how the parameters are determined. They only provide the “best” values. Although a trial and error approach is a good start when devising an algorithm or tailoring it to a new application, it makes the adaptation and the scalability"
W12-6108,W04-0827,0,0.0301578,"Missing"
W12-6108,C12-1146,1,0.875643,"ion can be expressed as P = T P+T and recall N as R = TP T The choice of the all words task in particular is based on the functioning of the algorithm that is more adapted to a globally coherent text, rather than independent sentences or words. As such, the metrics available for the evaluation are Precision, Recall and F-measure, which leads to some important restrictions on the methods or criteria available for parameter estimation as will be detailed in the subsequent sections. 3 Context: an ant colony algorithm for WSD The ant colony algorithm presented by (Schwab and Guillaume, 2011) and (Schwab et al., 2012) that our work focuses on, offers some interesting properties regarding the quality of the solutions, which are on par with state-of-the-art knowledge rich WSD systems, combined with a fast execution time compared to the latter systems. Furthermore, it is also well suited to working on full texts at a time with a full annotation coverage. Moreover, its stochastic nature leads to the generation of solutions with variable precision scores, which is usually not a very sought after property in algorithms. However, this variability enables the use of voting strategies that lead to results approachi"
W12-6108,C90-2067,0,\N,Missing
W14-6706,P10-1023,0,0.0808708,"Missing"
W17-1303,chen-eisele-2012-multiun,0,0.0289994,"at CBOW and SKIP-G are significantly faster to train with better accuracy compared to these techniques. For this reason, we have used the CBOW word representations for Arabic model1 proposed by Zahran et al. (Zahran et al., 2015). To train this model, they have used a large collection from different sources counting more than 5.8 billion words : • Arabic Wikipedia (WikiAr, 2006). • BBC and CNN Arabic corpus (Saad and Ashour, 2010). • The open parallel corpus (Tiedemann, 2012). • Arabase Corpus (Raafat et al., 2013). • Osac: Open source arabic corpora. (Saad and Ashour, 2010) • MultiUN corpus (Chen and Eisele, 2012) • AGC Arabic Gigaword Corpus. • King Saud University corpus (ksucorpus, 2012). • Meedan Arabic corpus (Meedan, 2012). • LDC Arabic newswire. • Raw Quran text (Quran, 2007). • KDE4 localization files (Tiedemann, 2009). • Khaleej and Watan 2004 (Khaleej, 2004). Training the Arabic CBOW model require choice of some parameters affecting the resulting vectors. All the parameters used by Zahran et al. (Zahran et al., 2015) are shown in Table 1. 1 19 https://sites.google.com/site/mohazahran/data 3.3 The Arabic CBOW Model Parameters Parameter Value Vector size Window Sample Hierarchical Softmax Negat"
W17-1303,tiedemann-2012-parallel,0,0.00472569,"eston, 2008), (Turian et al., 2010), (Mnih and Hinton, 2009), (Mikolov et al., 2013c) have been evaluated and compared, and they show that CBOW and SKIP-G are significantly faster to train with better accuracy compared to these techniques. For this reason, we have used the CBOW word representations for Arabic model1 proposed by Zahran et al. (Zahran et al., 2015). To train this model, they have used a large collection from different sources counting more than 5.8 billion words : • Arabic Wikipedia (WikiAr, 2006). • BBC and CNN Arabic corpus (Saad and Ashour, 2010). • The open parallel corpus (Tiedemann, 2012). • Arabase Corpus (Raafat et al., 2013). • Osac: Open source arabic corpora. (Saad and Ashour, 2010) • MultiUN corpus (Chen and Eisele, 2012) • AGC Arabic Gigaword Corpus. • King Saud University corpus (ksucorpus, 2012). • Meedan Arabic corpus (Meedan, 2012). • LDC Arabic newswire. • Raw Quran text (Quran, 2007). • KDE4 localization files (Tiedemann, 2009). • Khaleej and Watan 2004 (Khaleej, 2004). Training the Arabic CBOW model require choice of some parameters affecting the resulting vectors. All the parameters used by Zahran et al. (Zahran et al., 2015) are shown in Table 1. 1 19 https://s"
W17-1303,N13-1090,0,0.591641,"eman and Wunsch, 1970), LCS (Chvatal and Sankoff, 1975), JaroWinkler (Winkler, 1999), etc. The second type aims to quantify the degree to which two words are semantically related. As an example they can be, synonyms, represent the same thing or they are used in the same context. The classical way to measure this semantic similarity is by using linguistic resources, like WordNet (Miller, 1995), HowNet (Dong and Dong, 2003), BabelNet (Navigli and Ponzetto, 2012) or Dbnary (S´erasset, 2015). However, the word embedding techniques can be a more effective alternative to these linguistic databases (Mikolov et al., 2013a). In this article we focus our investigation on measuring the semantic similarity between short Arabic sentences using word embedding representations. We also consider the IDF weighting and Part-of-Speech tagging techniques in order to improve the identification of words that are highly descriptive in each sentence. The rest of this article is organized as follows, the next section describes work related to word representations in vector space. In Section 3, we present three variants of our proposed word embedding-based system. Section 4 describes the experimental results of this study. Fina"
W17-1303,P10-1040,0,0.495406,"ons. For instance, Collobert and Weston (2008) have proposed a unified system based on a deep neural network architecture, and trained jointly with many well known NLP tasks, including: Chunking, Part of Speech tagging, Named Entity Recognition and Semantic Role Labeling. Their word embedding model is stored in a matrix M ∈ Rd∗|D |, where D is a dictionary of all unique words in the training data, and each word is embedded into a d-dimensional vector. The sentences are represented using the embeddings of their forming words. A similar idea was independently proposed and used by Turian et al. (Turian et al., 2010). Mnih and Hinton (2009) have proposed another form to represent words in vector space, named Hierarchical Log-Bilinear Model (HLBL). Like virtually all neural language models, the HLBL model represents each word with a real-valued feature vector. For n-gram word-based, HLBL concatenates the n − 1 first embedding words (w1 ..wn−1 ) and learns a neural linear model to predicate the last word wn . Mikolov et al. (Mikolov et al., 2013c) have used a recurrent neural network (RNN) (Mikolov et al., 2010) to build a neural language model. The RNN encode the context word by word and predict the next w"
W17-1303,D14-1162,0,0.120226,"e order (Manning et al., 2008). There are 2 Word Embedding Models Words representations as vectors in a multidimensional space allows to capture the semantic and syntactic properties of the language (Mikolov et al., 2013a). These representations can serve as a fundamental building unit to many applications of 18 Proceedings of The Third Arabic Natural Language Processing Workshop (WANLP), pages 18–24, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics model SKIP-G, predicts surrounding words of the current pivot word wk (Mikolov et al., 2013b). Pennington et al. (Pennington et al., 2014) proposed a Global Vectors (GloVe) to build a words representations model, GloVe uses the global statistics of word-word co-occurrence to build cooccurrence matrix M . Then, M is used to calculate the probability of word wi to appear in the context of another word wj , this probability P (i/j) represents the relationship between words. Natural Language Processing (NLP). In the literature, several techniques are proposed to build vectorized space representations. For instance, Collobert and Weston (2008) have proposed a unified system based on a deep neural network architecture, and trained joi"
W17-2502,S16-1081,0,0.0607981,"s in replacing each word of one text by its most likely translations in the language of the other text, leading to a bags-of-words. We use DBNary (S´erasset, 2015) to get the translations. The metric used to compare two texts is a monolingual matching based on strict intersection of bags-of-words. 3 http://nlp.stanford.edu/software/ CRF-NER.shtml 4 https://wit3.fbk.eu/ 5 http://www.statmt.org/wmt13/ translation-task.html#download 8 tables, at chunk and sentence level, the overall F1 score over all sub-corpora of one method in one particular language pair is given. More recently, SemEval-2016 (Agirre et al., 2016) proposed a new subtask on evaluation of cross-lingual semantic textual similarity. Despite the fact that it was the first year that this subtask was attempted, there were 26 submissions from 10 teams. Most of the submissions relied on a machine translation step followed by a monolingual semantic similarity, but 4 teams tried to use learned vector representations (on words or sentences) combined with machine translation confidence (for instance the submission of Lo et al. (2016) or Ataman et al. (2016)). The method that achieved the best performance (Brychcin and Svoboda, 2016) was a supervise"
W17-2502,S16-1086,0,0.0436798,"Missing"
W17-2502,S16-1102,0,0.0623178,"Missing"
W17-2502,J93-2003,0,0.106592,": Number of aligned documents, sentences and noun chunks by sub-corpus. Syntax-Based Models Length Model (Pouliquen et al., 2003), CL-CnG (Potthast et al., 2011), Cognateness Dictionary-Based Models CL-VSM, CL-CTS (Pataki, 2012) Parallel Corpora-Based Models CL-ASA (Pinto et al., 2009), CL-LSI, CL-KCCA Comparable Corpora-Based Models CL-KGA, CL-ESA (Potthast et al., 2008) MT-Based Models Translation + Monolingual Analysis (Muhr et al., 2010) Figure 1: Taxonomy of Potthast et al. (2011), enriched by the study of Danilova (2013), of different approaches for cross-language similarity detection. (Brown et al., 1993) on the concatenation of TED4 (Cettolo et al., 2012) and News5 parallel corpora. We reuse the implementation of Pinto et al. (2009) that proposed a formula that factored the alignment function. document retrieval. Our implementation uses a part of Wikipedia, from which our test data was removed, to build the vector representations of the texts. Cross-Language Explicit Semantic Analysis (CL-ESA) is based on the explicit semantic analysis model introduced for the first time by Gabrilovich and Markovitch (2007), which represents the meaning of a document by a vector based on the vocabulary derive"
W17-2502,S16-1089,0,0.014298,"ecently, SemEval-2016 (Agirre et al., 2016) proposed a new subtask on evaluation of cross-lingual semantic textual similarity. Despite the fact that it was the first year that this subtask was attempted, there were 26 submissions from 10 teams. Most of the submissions relied on a machine translation step followed by a monolingual semantic similarity, but 4 teams tried to use learned vector representations (on words or sentences) combined with machine translation confidence (for instance the submission of Lo et al. (2016) or Ataman et al. (2016)). The method that achieved the best performance (Brychcin and Svoboda, 2016) was a supervised system built on a word alignment-based method proposed by Sultan et al. (2015). This very recent method is, however, not evaluated in this paper. 4 As a preliminary remark, one should note that CL-C3G and CL-ESA lead to the same results for a given language pair (same performance if we reverse source and target languages) due to their symmetrical property. Another remark we can make is that methods are consistent across language pairs: best performing methods are mostly the same, whatever the language pair considered. This is confirmed by the calculation of the Pearson correl"
W17-2502,S17-2001,0,0.0165888,"2 The previous sub-section has shown a consistent behavior of methods across language pairs (strongly consistent) and granularities (less strongly consistent). For this reason, we now propose a detailed analysis for different sub-corpora, for the English-French language pair - at chunk and sentence level - only. Providing these results for all language pairs and granularities would take too much space. Moreover, we also run those state-of-the-art methods on the dataset of the Spanish-English cross-lingual Semantic Textual Similarity task of SemEval-2016 (Agirre et al., 2016) and SemEval-2017 (Cer et al., 2017), and propose a shallower but equally rigorous analysis. However, all those results are also made available as supplementary material on our paper Web page. Table 8 shows the performances of methods on the EN→FR sub-corpora. As mentioned earlier, CL-C3G is in general the most effective method. CL-ESA seems to show better results on comparable corpora, like Wikipedia. In contrast, CL-ASA obtains better results on parallel corpora such as JRC or Europarl collections. CL-CTS and T+MA are pretty efficient and versatile too. It is also interesting to note that the results of the methods are well co"
W17-2502,2012.eamt-1.60,0,0.012308,"chunks by sub-corpus. Syntax-Based Models Length Model (Pouliquen et al., 2003), CL-CnG (Potthast et al., 2011), Cognateness Dictionary-Based Models CL-VSM, CL-CTS (Pataki, 2012) Parallel Corpora-Based Models CL-ASA (Pinto et al., 2009), CL-LSI, CL-KCCA Comparable Corpora-Based Models CL-KGA, CL-ESA (Potthast et al., 2008) MT-Based Models Translation + Monolingual Analysis (Muhr et al., 2010) Figure 1: Taxonomy of Potthast et al. (2011), enriched by the study of Danilova (2013), of different approaches for cross-language similarity detection. (Brown et al., 1993) on the concatenation of TED4 (Cettolo et al., 2012) and News5 parallel corpora. We reuse the implementation of Pinto et al. (2009) that proposed a formula that factored the alignment function. document retrieval. Our implementation uses a part of Wikipedia, from which our test data was removed, to build the vector representations of the texts. Cross-Language Explicit Semantic Analysis (CL-ESA) is based on the explicit semantic analysis model introduced for the first time by Gabrilovich and Markovitch (2007), which represents the meaning of a document by a vector based on the vocabulary derived from Wikipedia, to find a document within a corpus"
W17-2502,R13-2008,0,0.108283,"for Computational Linguistics tion of the methods across language pairs, while section 5.2 presents a detailed analysis on only English-French pair. Finally, section 6 concludes this work and gives a few perspectives. 2 of knowing why texts are similar and thus to assimilate these similarities to plagiarism. At the moment, there are five classes of approaches for cross-language plagiarism detection. The aim of each method is to estimate if two textual units in different languages express the same message or not. Figure 1 presents a taxonomy of Potthast et al. (2011), enriched by the study of Danilova (2013), of the different cross-language plagiarism detection methods grouped by class of approaches. We only describe below the state-of-the-art methods that we evaluate in the paper, one for each class of approaches (those in bold in the Figure 1). Dataset The reference dataset used during our study is the new dataset2 recently introduced by Ferrero et al. (2016). The dataset was specially designed for a rigorous evaluation of cross-language textual similarity detection. The different characteristics of the dataset are synthesized in Table 1, while Table 2 presents the number of aligned units by su"
W17-2502,L16-1657,1,0.892749,"Missing"
W17-2502,S15-2027,0,0.0410248,"Missing"
W17-6940,P14-1023,0,0.0654902,"cabulary size is about 3 million words and phrases, and the vectors have a dimension of 300. 2. The pre-trained Pennington et al. (2014)’s GloVe 3 , trained on 42 billion words from Common 2 3 https://code.google.com/archive/p/word2vec/ https://nlp.stanford.edu/projects/glove/ Crawl. The vocabulary size is about 2 million words, and the vectors have a dimension of 300. 3. The pre-trained Levy and Goldberg (2014)’s dependency-based word embeddings 4 . The training was done on Wikipedia. The vocabulary size is about 175,000 words, and the dimension is 300. 4. The best predict vectors created in Baroni et al. (2014) 5 . The vectors have a dimension of 400 and the vocabulary size is about 300,000. 5. Finally, the best reduced count vectors also created in Baroni et al. (2014) 5 , of dimension 500, and with the same vocabulary than the previous model. In the experiments section, we compare the performance of each of the embeddings model in our WSD system extension. All five sense embeddings models are publicly released on our GitHub6 . 3 Evaluation in Knowledge-Based WSD The generated vector representation of senses will be evaluated on a WSD task, as a supplementary resource for a knowledge-based method."
W17-6940,D14-1110,0,0.0237029,"reason, this paper will focus on a knowledge-based method, and on a novel approach that improves its performance in an unsupervised manner, by using word embeddings. Word embeddings are a set of methods which aim to represent words as vectors. Several recent state of the art methods such as Mikolov et al. (2013)’s Word2Vec, Pennington et al. (2014)’s GloVe and Levy and Goldberg (2014)’s dependency-based vectors have proven to be very useful in many NLP tasks, like Machine Translation, Word Similarity tasks, and even in WSD, where word embeddings are also parts of some recent methods, such as Chen et al. (2014), Iacobacci et al. (2016) or Yuan et al. (2016). In this paper, we are going to produce sense embeddings, i.e. vectors which represent senses present in the lexical database Princeton WordNet (Miller, 1995). The methodology used to create these vectors is described in section 2. Other methods exist for computing sense embeddings, such as in Iacobacci et al. 1 Less than ten different languages have at least one corpus sense annotated with WordNet, as listed in http:// globalwordnet.org/wordnet-annotated-corpora/ (2015) for example, who learn a distributional representation of senses through sen"
W17-6940,S17-2012,1,0.861005,"Missing"
W17-6940,P15-1010,0,0.0476432,"Missing"
W17-6940,P16-1085,0,0.0207521,"will focus on a knowledge-based method, and on a novel approach that improves its performance in an unsupervised manner, by using word embeddings. Word embeddings are a set of methods which aim to represent words as vectors. Several recent state of the art methods such as Mikolov et al. (2013)’s Word2Vec, Pennington et al. (2014)’s GloVe and Levy and Goldberg (2014)’s dependency-based vectors have proven to be very useful in many NLP tasks, like Machine Translation, Word Similarity tasks, and even in WSD, where word embeddings are also parts of some recent methods, such as Chen et al. (2014), Iacobacci et al. (2016) or Yuan et al. (2016). In this paper, we are going to produce sense embeddings, i.e. vectors which represent senses present in the lexical database Princeton WordNet (Miller, 1995). The methodology used to create these vectors is described in section 2. Other methods exist for computing sense embeddings, such as in Iacobacci et al. 1 Less than ten different languages have at least one corpus sense annotated with WordNet, as listed in http:// globalwordnet.org/wordnet-annotated-corpora/ (2015) for example, who learn a distributional representation of senses through sense-annotated corpora and"
W17-6940,P14-2050,0,0.413651,"enerally the only usable systems to work for disambiguating another language than English. Indeed, sense-annotated corpora are very expensive resources to produce, and almost practically inexisting for every other languages 1 . For this reason, this paper will focus on a knowledge-based method, and on a novel approach that improves its performance in an unsupervised manner, by using word embeddings. Word embeddings are a set of methods which aim to represent words as vectors. Several recent state of the art methods such as Mikolov et al. (2013)’s Word2Vec, Pennington et al. (2014)’s GloVe and Levy and Goldberg (2014)’s dependency-based vectors have proven to be very useful in many NLP tasks, like Machine Translation, Word Similarity tasks, and even in WSD, where word embeddings are also parts of some recent methods, such as Chen et al. (2014), Iacobacci et al. (2016) or Yuan et al. (2016). In this paper, we are going to produce sense embeddings, i.e. vectors which represent senses present in the lexical database Princeton WordNet (Miller, 1995). The methodology used to create these vectors is described in section 2. Other methods exist for computing sense embeddings, such as in Iacobacci et al. 1 Less tha"
W17-6940,S15-2049,0,0.0220209,"to see how our sense embeddings model performs as a semantic network, we use it for a lexical expansion of the dictionary’s glosses, for improving the Lesk local algorithm of our WSD system. Our expansion considers the related senses regarding two cosine similarity threshold: δ1 , filtering out senses based on their similarity with the target sense’s lemma vector, and δ2 , filtering out senses based on their similarity with the target sense vector. These two parameters have to be set in some way, so we chose two WSD tasks: SemEval 2007 task 7 (Navigli et al., 2007), and SemEval 2015 task 13 (Moro and Navigli, 2015), and we estimated the best set of parameters on a task, then tested this set of parameters on the other. The reason why we chose these two tasks is that they are of the same nature, i.e. both all-words WSD tasks, and that the task 7 of SemEval 2007 is largely used in most WSD articles, so it is easier to put the results in perspective. All five word embeddings models mentioned in section 2 are evaluated separately. And the parameters δ1 and δ2 have been estimated by testing every values in the range [0.5, 0.9] with steps of 0.1. The results of the best parameters estimation on SemEval 2007 ta"
W17-6940,D14-1162,0,0.0759212,"sed and knowledge-based methods are generally the only usable systems to work for disambiguating another language than English. Indeed, sense-annotated corpora are very expensive resources to produce, and almost practically inexisting for every other languages 1 . For this reason, this paper will focus on a knowledge-based method, and on a novel approach that improves its performance in an unsupervised manner, by using word embeddings. Word embeddings are a set of methods which aim to represent words as vectors. Several recent state of the art methods such as Mikolov et al. (2013)’s Word2Vec, Pennington et al. (2014)’s GloVe and Levy and Goldberg (2014)’s dependency-based vectors have proven to be very useful in many NLP tasks, like Machine Translation, Word Similarity tasks, and even in WSD, where word embeddings are also parts of some recent methods, such as Chen et al. (2014), Iacobacci et al. (2016) or Yuan et al. (2016). In this paper, we are going to produce sense embeddings, i.e. vectors which represent senses present in the lexical database Princeton WordNet (Miller, 1995). The methodology used to create these vectors is described in section 2. Other methods exist for computing sense embeddings, s"
W17-6940,C16-1130,0,0.0182188,"ased method, and on a novel approach that improves its performance in an unsupervised manner, by using word embeddings. Word embeddings are a set of methods which aim to represent words as vectors. Several recent state of the art methods such as Mikolov et al. (2013)’s Word2Vec, Pennington et al. (2014)’s GloVe and Levy and Goldberg (2014)’s dependency-based vectors have proven to be very useful in many NLP tasks, like Machine Translation, Word Similarity tasks, and even in WSD, where word embeddings are also parts of some recent methods, such as Chen et al. (2014), Iacobacci et al. (2016) or Yuan et al. (2016). In this paper, we are going to produce sense embeddings, i.e. vectors which represent senses present in the lexical database Princeton WordNet (Miller, 1995). The methodology used to create these vectors is described in section 2. Other methods exist for computing sense embeddings, such as in Iacobacci et al. 1 Less than ten different languages have at least one corpus sense annotated with WordNet, as listed in http:// globalwordnet.org/wordnet-annotated-corpora/ (2015) for example, who learn a distributional representation of senses through sense-annotated corpora and Word2Vec, but it presu"
W17-6940,S07-1006,0,\N,Missing
W19-4605,S16-1086,0,0.0293136,"Missing"
W19-4605,P14-1006,0,0.125575,"Missing"
W19-4605,R13-2008,0,0.0565617,"Missing"
W19-4605,2005.mtsummit-papers.11,0,0.0274595,"Missing"
W19-4605,S17-2001,0,0.0814577,"Missing"
W19-4605,2012.eamt-1.60,0,0.0574591,"Missing"
W19-4605,W15-1521,0,0.120445,"Missing"
W19-4605,Q18-1039,0,0.0478041,"Missing"
W19-4605,L18-1218,0,0.0277428,"Missing"
W19-4605,N13-1090,0,0.743604,", August 1, 2019. 2019 Association for Computational Linguistics ing the same architecture in a Wikipedia corpora of 104 different languages, requiring not a single alignment signal and realising, if not outperforming, state-of-the-art score in many NLP tasks such as Part Of Speech Tagging and Named Entity Recognition. However, BERT demands significantly more machine effort (Wu and Dredze, 2019). Table 1 summarises the cross-language embedding models mentioned above according to the architecture and used corpus, the target languages and the evaluation methods. 2010), (Mnih and Hinton, 2009), (Mikolov et al., 2013c,b) and (Peters et al., 2018). In the cross-lingual context, several word embedding models are proposed. Blunsom and Hermann (2014) introduced a Bilingual Compositional Model (BiCVM). Leveraging from the fact that aligned sentences have the same meaning. BiCVM is based on a sentence-aligned corpus to learn the bilingual word embedding vectors. Vuli´c and Moens (2015b) introduced a Bilingual Word Embedding Skip-Gram (BWESG), this model is constructed through three main steps: i) prepare a Skip-Gram Negative Sampling (Mikolov et al., 2013b) architecture that deals with document aligned comparab"
W19-4605,D18-1027,0,0.0203385,"ing Model Raki Lachraf Echahid Hamma Lakhdar University, El Oued, Algeria El Moatez Billah Nagoudi Echahid Hamma Lakhdar University, El Oued, Algeria LIM laboratory, Laghouat raki.lachraf@univ-eloued.dz moatez-nagoudi@univ-eloued.dz Didier Schwab Ahmed Abdelali Youcef Ayachi LIG-GETALP Echahid Hamma Lakhdar Hamad Bin Khalifa University Qatar Computing Research Institute Univ. Grenoble Alpes, University France Doha, Qatar El Oued, Algeria youcef.ayachi@univ-eloued.dz aabdelali@qf.org.qa Abstract more challenging task because the knowledge is transferred between two or more different languages (Doval et al., 2018). Recently, crosslingual word embeddings was used to address several issues, e.g. machine translation (Zou et al., 2013), cross-language information retrieval (Vuli´c and Moens, 2015a; Zhou et al., 2012), crosslanguage semantic similarity (Ataman et al., 2016; Nagoudi et al., 2017b) and plagiarism detection across multiple languages (Ferrero et al., 2017; Barr´on-Cede˜no et al., 2013). Many cross-lingual word embedding models in natural language have been developed, particularly for English, but Arabic did not get that much of interest. In this paper, we propose six Arabic-English cross-lingua"
W19-4605,E17-2066,1,0.816012,"esearch Institute Univ. Grenoble Alpes, University France Doha, Qatar El Oued, Algeria youcef.ayachi@univ-eloued.dz aabdelali@qf.org.qa Abstract more challenging task because the knowledge is transferred between two or more different languages (Doval et al., 2018). Recently, crosslingual word embeddings was used to address several issues, e.g. machine translation (Zou et al., 2013), cross-language information retrieval (Vuli´c and Moens, 2015a; Zhou et al., 2012), crosslanguage semantic similarity (Ataman et al., 2016; Nagoudi et al., 2017b) and plagiarism detection across multiple languages (Ferrero et al., 2017; Barr´on-Cede˜no et al., 2013). Many cross-lingual word embedding models in natural language have been developed, particularly for English, but Arabic did not get that much of interest. In this paper, we propose six Arabic-English cross-lingual word embedding models1 . To train these models, we have used a large collection with more than 93 million pairs of parallel ArabicEnglish sentences. The rest of this paper is organised as follows: in section 2 we provide a quick overview of work related to the cross-lingual word embedding models. We describe our dataset collection and the preprocessing"
W19-4605,S18-1055,1,0.821111,"Textual Similarity (STS), while the intrinsic evaluation is based on the Word Translation (WT) task. 1 didier.schwab@imag.fr Introduction Distributed word representations in vector space (Word Embeddings) are one of the most successful applications in deep learning for capturing the semantic and syntactic properties of words. Lately, many NLP tasks have been enriched using tools based on Mono and Cross-Lingual word embedding models. For instance, Mono-Lingual Word Embeddings (MLWE) have been widely used in information retrieval (Vuli´c and Moens, 2015a), sentiment analysis (Tang et al., 2014; Nagoudi, 2018) text classification (Lai et al., 2015), semantic textual similarity (Kenter and De Rijke, 2015; Nagoudi and Schwab, 2017) and plagiarism detection (Nagoudi et al., 2018). Cross-Lingual Word Embeddings (CLWE) is a 2 Related works While we focus on the cross-lingual word embedding models, the interested reader may refer to a number of research studies on the subject of mono-lingual word embeddings in general (Collobert and Weston, 2008), (Turian et al., 1 All models can be downloaded from : https://github.com/Raki22/ArbEngVec.git 40 Proceedings of the Fourth Arabic Natural Language Processing W"
W19-4605,S17-2017,1,0.923432,"GETALP Echahid Hamma Lakhdar Hamad Bin Khalifa University Qatar Computing Research Institute Univ. Grenoble Alpes, University France Doha, Qatar El Oued, Algeria youcef.ayachi@univ-eloued.dz aabdelali@qf.org.qa Abstract more challenging task because the knowledge is transferred between two or more different languages (Doval et al., 2018). Recently, crosslingual word embeddings was used to address several issues, e.g. machine translation (Zou et al., 2013), cross-language information retrieval (Vuli´c and Moens, 2015a; Zhou et al., 2012), crosslanguage semantic similarity (Ataman et al., 2016; Nagoudi et al., 2017b) and plagiarism detection across multiple languages (Ferrero et al., 2017; Barr´on-Cede˜no et al., 2013). Many cross-lingual word embedding models in natural language have been developed, particularly for English, but Arabic did not get that much of interest. In this paper, we propose six Arabic-English cross-lingual word embedding models1 . To train these models, we have used a large collection with more than 93 million pairs of parallel ArabicEnglish sentences. The rest of this paper is organised as follows: in section 2 we provide a quick overview of work related to the cross-lingual word"
W19-4605,D19-1077,0,0.0588195,"Missing"
W19-4605,W11-2162,0,0.147859,"Missing"
W19-4605,W17-1303,1,0.855417,"chwab@imag.fr Introduction Distributed word representations in vector space (Word Embeddings) are one of the most successful applications in deep learning for capturing the semantic and syntactic properties of words. Lately, many NLP tasks have been enriched using tools based on Mono and Cross-Lingual word embedding models. For instance, Mono-Lingual Word Embeddings (MLWE) have been widely used in information retrieval (Vuli´c and Moens, 2015a), sentiment analysis (Tang et al., 2014; Nagoudi, 2018) text classification (Lai et al., 2015), semantic textual similarity (Kenter and De Rijke, 2015; Nagoudi and Schwab, 2017) and plagiarism detection (Nagoudi et al., 2018). Cross-Lingual Word Embeddings (CLWE) is a 2 Related works While we focus on the cross-lingual word embedding models, the interested reader may refer to a number of research studies on the subject of mono-lingual word embeddings in general (Collobert and Weston, 2008), (Turian et al., 1 All models can be downloaded from : https://github.com/Raki22/ArbEngVec.git 40 Proceedings of the Fourth Arabic Natural Language Processing Workshop, pages 40–48 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics ing the same archit"
W19-4605,C12-1164,0,0.0306475,"oatez-nagoudi@univ-eloued.dz Didier Schwab Ahmed Abdelali Youcef Ayachi LIG-GETALP Echahid Hamma Lakhdar Hamad Bin Khalifa University Qatar Computing Research Institute Univ. Grenoble Alpes, University France Doha, Qatar El Oued, Algeria youcef.ayachi@univ-eloued.dz aabdelali@qf.org.qa Abstract more challenging task because the knowledge is transferred between two or more different languages (Doval et al., 2018). Recently, crosslingual word embeddings was used to address several issues, e.g. machine translation (Zou et al., 2013), cross-language information retrieval (Vuli´c and Moens, 2015a; Zhou et al., 2012), crosslanguage semantic similarity (Ataman et al., 2016; Nagoudi et al., 2017b) and plagiarism detection across multiple languages (Ferrero et al., 2017; Barr´on-Cede˜no et al., 2013). Many cross-lingual word embedding models in natural language have been developed, particularly for English, but Arabic did not get that much of interest. In this paper, we propose six Arabic-English cross-lingual word embedding models1 . To train these models, we have used a large collection with more than 93 million pairs of parallel ArabicEnglish sentences. The rest of this paper is organised as follows: in s"
W19-4605,N18-1202,0,0.172265,"Missing"
W19-4605,D13-1141,0,0.0151394,"niversity, El Oued, Algeria LIM laboratory, Laghouat raki.lachraf@univ-eloued.dz moatez-nagoudi@univ-eloued.dz Didier Schwab Ahmed Abdelali Youcef Ayachi LIG-GETALP Echahid Hamma Lakhdar Hamad Bin Khalifa University Qatar Computing Research Institute Univ. Grenoble Alpes, University France Doha, Qatar El Oued, Algeria youcef.ayachi@univ-eloued.dz aabdelali@qf.org.qa Abstract more challenging task because the knowledge is transferred between two or more different languages (Doval et al., 2018). Recently, crosslingual word embeddings was used to address several issues, e.g. machine translation (Zou et al., 2013), cross-language information retrieval (Vuli´c and Moens, 2015a; Zhou et al., 2012), crosslanguage semantic similarity (Ataman et al., 2016; Nagoudi et al., 2017b) and plagiarism detection across multiple languages (Ferrero et al., 2017; Barr´on-Cede˜no et al., 2013). Many cross-lingual word embedding models in natural language have been developed, particularly for English, but Arabic did not get that much of interest. In this paper, we propose six Arabic-English cross-lingual word embedding models1 . To train these models, we have used a large collection with more than 93 million pairs of par"
W19-4605,P14-1146,0,0.0438114,"-language Semantic Textual Similarity (STS), while the intrinsic evaluation is based on the Word Translation (WT) task. 1 didier.schwab@imag.fr Introduction Distributed word representations in vector space (Word Embeddings) are one of the most successful applications in deep learning for capturing the semantic and syntactic properties of words. Lately, many NLP tasks have been enriched using tools based on Mono and Cross-Lingual word embedding models. For instance, Mono-Lingual Word Embeddings (MLWE) have been widely used in information retrieval (Vuli´c and Moens, 2015a), sentiment analysis (Tang et al., 2014; Nagoudi, 2018) text classification (Lai et al., 2015), semantic textual similarity (Kenter and De Rijke, 2015; Nagoudi and Schwab, 2017) and plagiarism detection (Nagoudi et al., 2018). Cross-Lingual Word Embeddings (CLWE) is a 2 Related works While we focus on the cross-lingual word embedding models, the interested reader may refer to a number of research studies on the subject of mono-lingual word embeddings in general (Collobert and Weston, 2008), (Turian et al., 1 All models can be downloaded from : https://github.com/Raki22/ArbEngVec.git 40 Proceedings of the Fourth Arabic Natural Langu"
W19-4605,tiedemann-2012-parallel,0,0.0459589,"Missing"
W19-4605,P10-1040,0,0.0737633,"al., 2013a), as illustrated in figure 1.  The letters @ , @ , @ are replaced with @ while  the letter è is replaced with è. Also, The letter ø followed by Z replaced with ø . We converted elongated words back to their  original form, example : èYë@@@@@@AªÓ, which means treaty in English, and QK@ Qm .Ì '@ , which means Algeria will be converted to èYëAªÓ, QK@ Qm Ì '@. . 3. In addition, we remove the stop-words from Arabic and English sentences. 4 4.1 Building ArbEngVec Models Used Architectures In Mikolov et al. (2013a) all the word embedding models (Collobert and Weston, 2008), (Turian et al., 2010), (Mnih and Hinton, 2009), (Mikolov et al., 2010), (Mikolov et al., 2013c) and (Mikolov et al., 2013b) have been compared and evaluated, and they show that CBOW (Mikolov et al., 2013c) and Skip-Gram (Mikolov et al., 2013b) models are significantly faster to train with better accuracy. Accordingly, we used the CBOW and SkipGram to build our Arabic-English cross-lingual word embedding models. Figure 1: Architecture of CBOW and Skip-gram as described in (Mikolov et al., 2013b) 4.2 Proposed Models In this section, we present our proposed ArbEngVec models. In order to learn our models, we have reli"
