2020.coling-main.107,S07-1002,0,0.0605363,"Models are required to cluster all occurrences of each target word according to their meaning. Thus, the senses of all target words are discovered in an unsupervised fashion. For example, suppose that we have the following sentences with the target word bank: 1. He settled down on the river bank and contemplated the beauty of nature, 1248 2. They unloaded the tackle from the boat to the bank. 3. Grand River bank now offers a profitable mortgage. Sentences 1 and 2 shall be put in one cluster, while sentence 3 must be assigned to another. This task was proposed in several SemEval competitions (Agirre and Soroa, 2007; Manandhar et al., 2010; Jurgens and Klapaftis, 2013). The current state-of-the-art approach (Amrami and Goldberg, 2019) rely on substitute vectors, i.e., each word usage is represented as a substitute vector based on the most probable substitutes, then clustering is performed over these substitute vectors. Model (Amrami and Goldberg, 2018) (Amrami and Goldberg, 2019) C2V C2V+embs ELMo ELMo+embs BERT BERT+embs RoBERTa RoBERTa+embs XLNet XLNet+embs SemEval-2010 (AVG) – 53.6±1.2 38.9 28.5 41.8 45.3 52.0 53.8 49.6 51.4 52.2 54.2 SemEval-2013 (AVG) 25.43±0.48 37.0±0.5 18.2 21.7 27.6 28.2 34.5 36."
2020.coling-main.107,D18-1523,0,0.634132,"analyze the types of semantic relations between target words and their substitutes generated by different models or given by annotators. 1 Introduction Lexical substitution is the task of generating words that can replace a given word in a given textual context. For instance, in the sentence “My daughter purchased a new car” the word car can be substituted by its synonym automobile, but also with co-hyponym bike, or even hypernym motor vehicle while keeping the original sentence grammatical. Lexical substitution has been proven effective in various applications, such as word sense induction (Amrami and Goldberg, 2018), lexical relation extraction (Schick and Sch¨utze, 2020), paraphrase generation, text simplification, textual data augmentation, etc. Note that the preferable type (e.g., synonym, hypernym, co-hyponym, etc.) of generated substitutes depends on the task at hand. The new generation of language and masked language models (LMs/MLMs) based on deep neural networks enabled a profound breakthrough in almost all NLP tasks. These models are commonly used to perform pre-training of deep neural networks, which are then fine-tuned to some final task different from language modeling. However, in this paper"
2020.coling-main.107,R19-1008,1,0.7826,"β = 1, this formula can P (s)β be derived by applying the Bayes rule and assuming conditional independence of C and T given s. Other values of β can be used to penalize frequent words, more or less. Our current methods are limited to generating only substitutes from the vocabulary of the underlying LM/MLM. Thus, we take word or subword embeddings of the same model we apply the injection to. Other word embeddings like word2vec may perform better, but we leave these experiments for future work. Word probabilities P (s) are retrieved from wordfreq library2 for all models except ELMo. Following (Arefyev et al., 2019), for ELMo, we calculate word probabilities from word ranks in the ELMo vocabulary (which is ordered by word frequencies) based on Zipf-Mandelbrot distribution and found it performing better presumably due to better correspondence to the corpus ELMo was pre-trained on. Dynamic patterns Following the approach proposed in (Amrami and Goldberg, 2018), we replace the target word T by “T and ” (e.g. “Let me fly and away!”). Then some LM/MLM is employed to predict possible words at timestep “ ”. Thus, dynamic patterns provide information about the target word to the model via Hearst-like patterns. D"
2020.coling-main.107,biemann-2012-turk,0,0.624191,"hms, the mean and the standard deviation are reported. Our WSI algorithm is deterministic; hence, we report the results of a single run. Our best model achieves higher metrics than the previous SOTA on both datasets, however, the difference is within one standard deviation. Similarly to the intrinsic evaluation results, our +embs injection method substantially improves the performance of all models for WSI, except for the C2V+embs model on SemEval-2010, which probably used suboptimal hyperparameters. Hyperparameters The optimal hyperparameters for WSI models were selected on the TWSI dataset (Biemann, 2012). For both evaluation datasets we used β=2.0 for BERT+embs, XLNet+embs, RoBERTa+embs and ELMo+embs models, β=0.0 for C2V+embs, T =2.5 for BERT+embs, T =1.0 for XLNet+embs, T =10.0 for RoBERTa+embs, T =0.385 for ELMo+embs and T =1.0 for C2V+embs. 7 Analysis of Semantic Relation Types In this section we analyze the types of substitutes produced by different neural substitution models. 7.1 Experimental Setup For this analysis, the CoInCo lexical substitution dataset (Kremer et al., 2014) described above is used. We employ WordNet (Miller, 1995) to find the relationship between a target word and e"
2020.coling-main.107,N19-1423,0,0.486627,".) of generated substitutes depends on the task at hand. The new generation of language and masked language models (LMs/MLMs) based on deep neural networks enabled a profound breakthrough in almost all NLP tasks. These models are commonly used to perform pre-training of deep neural networks, which are then fine-tuned to some final task different from language modeling. However, in this paper we study how the progress in unsupervised pre-training over the last five years affected the quality of lexical substitution. We adapt context2vec (Melamud et al., 2016), ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019) and XLNet (Yang et al., 2019) to solve lexical substitution task without any fine-tuning, but using additional techniques to ensure similarity of substitutes to the target word, which we call target word injection techniques. We provide the first large-scale comparison of various neural LMs/MLMs with several target word injection methods on lexical substitution and WSI tasks. Our research questions are the following (i) which pre-trained models are the best for substitution in context, (ii) additionally to pre-training larger ∗ Left Samsung This work is licensed un"
2020.coling-main.107,P16-1012,0,0.0808754,"tes are integrated into the ranking metric to ensure minimal changes in the sentence meaning. This approach is very computationally expensive, requiring calculation of several forward passes of BERT for each input example, depending on the number of possible substitutes. We are not aware of any work applying XLNet for lexical substitution, but our experiments show that it outperforms BERT by a large margin. 1 The repository for this paper: https://github.com/bsheludko/lexical-substitution 1243 Supervised approaches to lexical substitution include (Szarvas et al., 2013a; Szarvas et al., 2013b; Hintz and Biemann, 2016). These approaches rely on manually curated lexical resources like WordNet, so they are not easily transferable to different languages, unlike those described above. Also, the latest unsupervised methods were shown to perform better (Zhou et al., 2019). 3 Neural Language Models for Lexical Substitution with Target Word Injection To generate substitutes, we introduce several substitute probability estimators, which are models taking a text fragment and a target word position in it as input and producing a list of substitutes with their probabilities. To build our substitute probability estimato"
2020.coling-main.107,S13-2049,0,0.0169217,"of each target word according to their meaning. Thus, the senses of all target words are discovered in an unsupervised fashion. For example, suppose that we have the following sentences with the target word bank: 1. He settled down on the river bank and contemplated the beauty of nature, 1248 2. They unloaded the tackle from the boat to the bank. 3. Grand River bank now offers a profitable mortgage. Sentences 1 and 2 shall be put in one cluster, while sentence 3 must be assigned to another. This task was proposed in several SemEval competitions (Agirre and Soroa, 2007; Manandhar et al., 2010; Jurgens and Klapaftis, 2013). The current state-of-the-art approach (Amrami and Goldberg, 2019) rely on substitute vectors, i.e., each word usage is represented as a substitute vector based on the most probable substitutes, then clustering is performed over these substitute vectors. Model (Amrami and Goldberg, 2018) (Amrami and Goldberg, 2019) C2V C2V+embs ELMo ELMo+embs BERT BERT+embs RoBERTa RoBERTa+embs XLNet XLNet+embs SemEval-2010 (AVG) – 53.6±1.2 38.9 28.5 41.8 45.3 52.0 53.8 49.6 51.4 52.2 54.2 SemEval-2013 (AVG) 25.43±0.48 37.0±0.5 18.2 21.7 27.6 28.2 34.5 36.8 34.0 34.5 33.4 37.3 Table 3: Extrinsic evaluation on"
2020.coling-main.107,E14-1057,0,0.0741367,"Missing"
2020.coling-main.107,P14-2050,0,0.0450684,"ppropriate in the given context and related to the target word in some sense (which may vary depending on the application of generated substitutes). To achieve this, unsupervised substitution models heavily rely on distributional similarity models of words (DSMs) and language models (LMs). Probably, the most commonly used DSM is word2vec model (Mikolov et al., 2013). It learns word embeddings and context embeddings to be similar when they tend to occur together, resulting in similar embeddings for distributionally similar words. Contexts are either nearby words or syntactically related words (Levy and Goldberg, 2014). In (Melamud et al., 2015b) several metrics for lexical substitution were proposed based on embedding similarity of substitutes both to the target word and to the words in the given context. Later (Roller and Erk, 2016) improved this approach by switching to dot-product instead of cosine similarity and applying an additional trainable transformation to context word embeddings. A more sophisticated context2vec model producing embeddings for a word in a particular context (contextualized word embeddings) was proposed in (Melamud et al., 2016) and was shown to outperform previous models in a ran"
2020.coling-main.107,S10-1011,0,0.0390623,"cluster all occurrences of each target word according to their meaning. Thus, the senses of all target words are discovered in an unsupervised fashion. For example, suppose that we have the following sentences with the target word bank: 1. He settled down on the river bank and contemplated the beauty of nature, 1248 2. They unloaded the tackle from the boat to the bank. 3. Grand River bank now offers a profitable mortgage. Sentences 1 and 2 shall be put in one cluster, while sentence 3 must be assigned to another. This task was proposed in several SemEval competitions (Agirre and Soroa, 2007; Manandhar et al., 2010; Jurgens and Klapaftis, 2013). The current state-of-the-art approach (Amrami and Goldberg, 2019) rely on substitute vectors, i.e., each word usage is represented as a substitute vector based on the most probable substitutes, then clustering is performed over these substitute vectors. Model (Amrami and Goldberg, 2018) (Amrami and Goldberg, 2019) C2V C2V+embs ELMo ELMo+embs BERT BERT+embs RoBERTa RoBERTa+embs XLNet XLNet+embs SemEval-2010 (AVG) – 53.6±1.2 38.9 28.5 41.8 45.3 52.0 53.8 49.6 51.4 52.2 54.2 SemEval-2013 (AVG) 25.43±0.48 37.0±0.5 18.2 21.7 27.6 28.2 34.5 36.8 34.0 34.5 33.4 37.3 Ta"
2020.coling-main.107,S07-1009,0,0.739818,"t: P nP IC(s|T, C) = P (s|T ) × Pn (s|C), where P (s|T ) ∝ exp(hembss , embsT i) and Pn (s|C) ∝ exp( hembss , embs0c i). Here embs and embs0 are c∈C dependency-based word and context embeddings, and C are those words that are directly connected to the target in the dependency tree. 5 Intrinsic Evaluation We perform an intrinsic evaluation of the proposed models on two lexical substitution datasets. 5.1 Experimental Setup Lexical substitution task is concerned with finding appropriate substitutes for a target word in a given context. This task was originally introduced in SemEval 2007 Task 10 (McCarthy and Navigli, 2007) to evaluate how distributional models handle polysemous words. In the lexical substitution task, annotators are provided with a target word and its context. Their task is to propose possible substitutes. Since there are several annotators, we have some weight for each possible substitute in each example, which is equal to the number of annotators provided this substitute. We rank substitutes for a target word in a context by acquiring probability distribution over vocabulary on the target position. Lexical substitution task comes with two variations: candidate ranking and allwords ranking. In"
2020.coling-main.107,N15-1050,0,0.3559,"t and related to the target word in some sense (which may vary depending on the application of generated substitutes). To achieve this, unsupervised substitution models heavily rely on distributional similarity models of words (DSMs) and language models (LMs). Probably, the most commonly used DSM is word2vec model (Mikolov et al., 2013). It learns word embeddings and context embeddings to be similar when they tend to occur together, resulting in similar embeddings for distributionally similar words. Contexts are either nearby words or syntactically related words (Levy and Goldberg, 2014). In (Melamud et al., 2015b) several metrics for lexical substitution were proposed based on embedding similarity of substitutes both to the target word and to the words in the given context. Later (Roller and Erk, 2016) improved this approach by switching to dot-product instead of cosine similarity and applying an additional trainable transformation to context word embeddings. A more sophisticated context2vec model producing embeddings for a word in a particular context (contextualized word embeddings) was proposed in (Melamud et al., 2016) and was shown to outperform previous models in a ranking scenario when candida"
2020.coling-main.107,W15-1501,0,0.667737,"t and related to the target word in some sense (which may vary depending on the application of generated substitutes). To achieve this, unsupervised substitution models heavily rely on distributional similarity models of words (DSMs) and language models (LMs). Probably, the most commonly used DSM is word2vec model (Mikolov et al., 2013). It learns word embeddings and context embeddings to be similar when they tend to occur together, resulting in similar embeddings for distributionally similar words. Contexts are either nearby words or syntactically related words (Levy and Goldberg, 2014). In (Melamud et al., 2015b) several metrics for lexical substitution were proposed based on embedding similarity of substitutes both to the target word and to the words in the given context. Later (Roller and Erk, 2016) improved this approach by switching to dot-product instead of cosine similarity and applying an additional trainable transformation to context word embeddings. A more sophisticated context2vec model producing embeddings for a word in a particular context (contextualized word embeddings) was proposed in (Melamud et al., 2016) and was shown to outperform previous models in a ranking scenario when candida"
2020.coling-main.107,K16-1006,0,0.318297,"preferable type (e.g., synonym, hypernym, co-hyponym, etc.) of generated substitutes depends on the task at hand. The new generation of language and masked language models (LMs/MLMs) based on deep neural networks enabled a profound breakthrough in almost all NLP tasks. These models are commonly used to perform pre-training of deep neural networks, which are then fine-tuned to some final task different from language modeling. However, in this paper we study how the progress in unsupervised pre-training over the last five years affected the quality of lexical substitution. We adapt context2vec (Melamud et al., 2016), ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019) and XLNet (Yang et al., 2019) to solve lexical substitution task without any fine-tuning, but using additional techniques to ensure similarity of substitutes to the target word, which we call target word injection techniques. We provide the first large-scale comparison of various neural LMs/MLMs with several target word injection methods on lexical substitution and WSI tasks. Our research questions are the following (i) which pre-trained models are the best for substitution in context, (ii) additionally to pre"
2020.coling-main.276,Q17-1010,0,0.188394,"Missing"
2020.coling-main.276,S15-2151,0,0.175007,"b.com/skoltech-nlp/diachronic-wordnets 3095 Proceedings of the 28th International Conference on Computational Linguistics, pages 3095–3106 Barcelona, Spain (Online), December 8-13, 2020 2 Related Work The existing studies on the taxonomies can be divided into three groups. The first one addresses the Hypernym Discovery problem (Camacho-Collados et al., 2018): given a word and a text corpus, the task is to identify hypernyms in the text. However, in this task, the participants are not given any predefined taxonomy to rely on. The second group of works deals with the Taxonomy Induction problem (Bordea et al., 2015; Bordea et al., 2016; Velardi et al., 2013), in other words, creation of a taxonomy from scratch. Finally, the third direction of research is the Taxonomy Enrichment task: the participants extend a given taxonomy with new words. Our methods tackle this task. Unlike the former two groups, the latter garners less attention. Until recently, the only dataset for this task was created under the scope of SemEval-2016. It contained definitions for new words, so the majority of models solving this task used the definitions. For instance, Tanev and Rotondi (2016) computed definition vector for the inp"
2020.coling-main.276,S16-1168,0,0.0933984,"achronic-wordnets 3095 Proceedings of the 28th International Conference on Computational Linguistics, pages 3095–3106 Barcelona, Spain (Online), December 8-13, 2020 2 Related Work The existing studies on the taxonomies can be divided into three groups. The first one addresses the Hypernym Discovery problem (Camacho-Collados et al., 2018): given a word and a text corpus, the task is to identify hypernyms in the text. However, in this task, the participants are not given any predefined taxonomy to rely on. The second group of works deals with the Taxonomy Induction problem (Bordea et al., 2015; Bordea et al., 2016; Velardi et al., 2013), in other words, creation of a taxonomy from scratch. Finally, the third direction of research is the Taxonomy Enrichment task: the participants extend a given taxonomy with new words. Our methods tackle this task. Unlike the former two groups, the latter garners less attention. Until recently, the only dataset for this task was created under the scope of SemEval-2016. It contained definitions for new words, so the majority of models solving this task used the definitions. For instance, Tanev and Rotondi (2016) computed definition vector for the input word, comparing it"
2020.coling-main.276,N19-1423,0,0.0157609,"Missing"
2020.coling-main.276,S16-1208,0,0.0209482,"arch is the Taxonomy Enrichment task: the participants extend a given taxonomy with new words. Our methods tackle this task. Unlike the former two groups, the latter garners less attention. Until recently, the only dataset for this task was created under the scope of SemEval-2016. It contained definitions for new words, so the majority of models solving this task used the definitions. For instance, Tanev and Rotondi (2016) computed definition vector for the input word, comparing it with the vector of the candidate definitions from WordNet using cosine similarity. Another example is TALN team (Espinosa-Anke et al., 2016) which also makes use of the definition by extracting noun and verb phrases for candidates generation. This scenario may be unrealistic for manual annotation because annotators are writing a definition for a new word and adding new words to the taxonomy simultaneously. Having a list of candidates would not only speed up the annotation process but also identify the range of possible senses. Moreover, it is possible that not yet included words may have no definition in any other sources: they could be very rare (“apparatchik”, “falanga”), relatively new (“selfie”, “hashtag”) or come from a narro"
2020.coling-main.276,W14-0142,0,0.0201251,"or the NLP community. In particular, enriching the most acknowledged lexical databases like WordNet (Miller, 1998) and its variants for almost 50 languages1 or collaboratively created lexical resources such as Wiktionary is crucial. Resources of this kind are widely used in multiple NLP tasks: Word Sense Disambiguation, Entity Linking (Moro and Navigli, 2015), Named Entity Recognition, Coreference Resolution (Ponzetto and Strube, 2006). There already exist several initiatives on WordNet extension, for example, the Open English WordNet with thousands of new manually added entries or plWordNet (Maziarz et al., 2014) which includes a mapping to an enlarged Princeton WordNet. However, the manual annotation process is too costly: it is time-consuming and requires language or domain experts. On the other hand, automatically created datasets and resources usually lag in quality compared to manually labelled ones. Therefore, it would be beneficial to assist manual work by introducing automatic annotation systems to keep valuable lexical resources up-to-date. In this paper, we analyse the approaches to automatic enrichment of wordnets. Formally, the goal of the Taxonomy Enrichment task is as follows: given word"
2020.coling-main.276,S15-2049,0,0.0902803,"training and evaluating taxonomy enrichment models and describe a technique of creating such datasets for other languages. 1 Introduction Nowadays, construction and maintenance of lexical resources (ontologies, knowledge bases, thesauri) have become essential for the NLP community. In particular, enriching the most acknowledged lexical databases like WordNet (Miller, 1998) and its variants for almost 50 languages1 or collaboratively created lexical resources such as Wiktionary is crucial. Resources of this kind are widely used in multiple NLP tasks: Word Sense Disambiguation, Entity Linking (Moro and Navigli, 2015), Named Entity Recognition, Coreference Resolution (Ponzetto and Strube, 2006). There already exist several initiatives on WordNet extension, for example, the Open English WordNet with thousands of new manually added entries or plWordNet (Maziarz et al., 2014) which includes a mapping to an enlarged Princeton WordNet. However, the manual annotation process is too costly: it is time-consuming and requires language or domain experts. On the other hand, automatically created datasets and resources usually lag in quality compared to manually labelled ones. Therefore, it would be beneficial to assi"
2020.coling-main.276,N06-1025,0,0.0492605,"e of creating such datasets for other languages. 1 Introduction Nowadays, construction and maintenance of lexical resources (ontologies, knowledge bases, thesauri) have become essential for the NLP community. In particular, enriching the most acknowledged lexical databases like WordNet (Miller, 1998) and its variants for almost 50 languages1 or collaboratively created lexical resources such as Wiktionary is crucial. Resources of this kind are widely used in multiple NLP tasks: Word Sense Disambiguation, Entity Linking (Moro and Navigli, 2015), Named Entity Recognition, Coreference Resolution (Ponzetto and Strube, 2006). There already exist several initiatives on WordNet extension, for example, the Open English WordNet with thousands of new manually added entries or plWordNet (Maziarz et al., 2014) which includes a mapping to an enlarged Princeton WordNet. However, the manual annotation process is too costly: it is time-consuming and requires language or domain experts. On the other hand, automatically created datasets and resources usually lag in quality compared to manually labelled ones. Therefore, it would be beneficial to assist manual work by introducing automatic annotation systems to keep valuable le"
2020.coling-main.276,K17-3009,0,0.0670342,"Missing"
2020.coling-main.276,K17-3001,0,0.0420954,"Missing"
2020.coling-main.276,S16-1210,0,0.162228,"deals with the Taxonomy Induction problem (Bordea et al., 2015; Bordea et al., 2016; Velardi et al., 2013), in other words, creation of a taxonomy from scratch. Finally, the third direction of research is the Taxonomy Enrichment task: the participants extend a given taxonomy with new words. Our methods tackle this task. Unlike the former two groups, the latter garners less attention. Until recently, the only dataset for this task was created under the scope of SemEval-2016. It contained definitions for new words, so the majority of models solving this task used the definitions. For instance, Tanev and Rotondi (2016) computed definition vector for the input word, comparing it with the vector of the candidate definitions from WordNet using cosine similarity. Another example is TALN team (Espinosa-Anke et al., 2016) which also makes use of the definition by extracting noun and verb phrases for candidates generation. This scenario may be unrealistic for manual annotation because annotators are writing a definition for a new word and adding new words to the taxonomy simultaneously. Having a list of candidates would not only speed up the annotation process but also identify the range of possible senses. Moreov"
2020.coling-main.276,J13-3007,0,0.181293,"5 Proceedings of the 28th International Conference on Computational Linguistics, pages 3095–3106 Barcelona, Spain (Online), December 8-13, 2020 2 Related Work The existing studies on the taxonomies can be divided into three groups. The first one addresses the Hypernym Discovery problem (Camacho-Collados et al., 2018): given a word and a text corpus, the task is to identify hypernyms in the text. However, in this task, the participants are not given any predefined taxonomy to rely on. The second group of works deals with the Taxonomy Induction problem (Bordea et al., 2015; Bordea et al., 2016; Velardi et al., 2013), in other words, creation of a taxonomy from scratch. Finally, the third direction of research is the Taxonomy Enrichment task: the participants extend a given taxonomy with new words. Our methods tackle this task. Unlike the former two groups, the latter garners less attention. Until recently, the only dataset for this task was created under the scope of SemEval-2016. It contained definitions for new words, so the majority of models solving this task used the definitions. For instance, Tanev and Rotondi (2016) computed definition vector for the input word, comparing it with the vector of the"
2020.lrec-1.728,D18-1523,0,0.0164621,"t al., 2016) or by PMI-like measures (Hope and Keller, 2013b). Word senses are induced via graph clustering algorithms, such as Chinese Whispers (Biemann, 2006) or MaxMax (Hope and Keller, 2013a). The technique suggested in our work belongs to this class of methods and is an extension of the method presented by Pelevina et al. (2016). Synonyms and substitute clustering approaches create vectors which represent synonyms or substitutes of polysemous words. Such vectors are created using synonymy dictionaries (Ustalov et al., 2019) or context-dependent substitutes obtained from a language model (Amrami and Goldberg, 2018). Analogously to previously described techniques, word senses are induced by clustering these vectors. 3. Algorithm for Word Sense Induction The majority of word vector models do not discriminate between multiple senses of individual words. However, a polysemous word can be identified via manual analysis of its nearest neighbours—they reflect different senses of the word. Table 1 shows manually sense-labelled most similar terms to the word Ruby according to the pre-trained fastText model (Grave et al., 2018). As it was suggested early by Widdows and Dorow (2002), the distributional properties"
2020.lrec-1.728,P18-1001,0,0.0606807,"Missing"
2020.lrec-1.728,L18-1618,0,0.022236,"the most appropriate sense (labelled by the centroid word of a corresponding cluster). by Ustalov et al. (2018), extending it with a back-end for multiple languages, language detection, and sense browsing capabilities. 5. Evaluation We first evaluate our converted embedding models on multi-language lexical similarity and relatedness tasks, as a sanity check, to make sure the word sense induction process did not hurt the general performance of the embeddings. Then, we test the sense embeddings on WSD task. 5.1. Lexical Similarity and Relatedness Experimental Setup We use the SemR-11 datasets4 (Barzegar et al., 2018), which contain word pairs with manually assigned similarity scores from 0 (words are not related) to 10 (words are fully interchangeable) for 12 languages: English (en), Arabic (ar), German (de), Spanish (es), Farsi (fa), French (fr), Italian (it), Dutch (nl), Portuguese (pt), Russian (ru), Swedish (sv), Chinese (zh). The task is to assign relatedness scores to these pairs so that the ranking of the pairs by this score is close to the ranking defined by the oracle score. The performance is measured with Pearson correlation of the rankings. Since one word can have several different senses in o"
2020.lrec-1.728,W06-3812,1,0.364448,"ess algorithm (Li and Jurafsky, 2015), AdaGram, a Bayesian extension of the Skip-Gram model (Bartunov et al., 2016), AutoSense, an extension of the LDA topic model (Amplayo et al., 2019), and other techniques. Word ego-network clustering is applied to semantic graphs. The nodes of a semantic graph are words, and edges between them denote semantic relatedness which is usually evaluated with cosine similarity of the corresponding embeddings (Pelevina et al., 2016) or by PMI-like measures (Hope and Keller, 2013b). Word senses are induced via graph clustering algorithms, such as Chinese Whispers (Biemann, 2006) or MaxMax (Hope and Keller, 2013a). The technique suggested in our work belongs to this class of methods and is an extension of the method presented by Pelevina et al. (2016). Synonyms and substitute clustering approaches create vectors which represent synonyms or substitutes of polysemous words. Such vectors are created using synonymy dictionaries (Ustalov et al., 2019) or context-dependent substitutes obtained from a language model (Amrami and Goldberg, 2018). Analogously to previously described techniques, word senses are induced by clustering these vectors. 3. Algorithm for Word Sense Ind"
2020.lrec-1.728,Q17-1010,0,0.301237,"ontexts, and diverse words do not share common contexts, is known as distributional hypothesis and underlies the technique of constructing word embeddings from unlabelled texts. The same intuition can be used to discriminate between different senses of individual words. There exist methods of training word embeddings that can detect polysemous words and assign them different vectors depending on their contexts (Athiwaratkun et al., 2018; Jain et al., 2019). Unfortunately, many widespread word embedding models, such as GloVe (Pennington et al., 2014), word2vec (Mikolov et al., 2013), fastText (Bojanowski et al., 2017), do not handle polysemous words. Words in these models are represented with single vectors, which were constructed from diverse sets of contexts corresponding to different senses. In such cases, their disam? Currently at Yandex. biguation needs knowledge-rich approaches. We tackle this problem by suggesting a method of posthoc unsupervised WSD. It does not require any external knowledge and can separate different senses of a polysemous word using only the information encoded in pretrained word embeddings. We construct a semantic similarity graph for words and partition it into densely connect"
2020.lrec-1.728,N19-1423,0,0.00893264,"ification problem. Knowledge-based approaches construct sense embeddings, i.e. embeddings that separate various word senses. SupWSD (Papandrea et al., 2017) is a state-of-the-art system for supervised WSD. It makes use of linear classifiers and a number of features such as POS tags, surrounding words, local collocations, word embeddings, and syntactic relations. GlossBERT model (Huang et al., 2019), which is another implementation of supervised WSD, achieves a significant improvement by leveraging gloss information. This model benefits from sentence-pair classification approach, introduced by Devlin et al. (2019) in their BERT contextualized embedding model. The input to the model consists of a context (a sentence which contains an ambiguous word) and a gloss (sense definition) from WordNet. The contextgloss pair is concatenated through a special token ([SEP]) and classified as positive or negative. On the other hand, sense embeddings are an alternative to traditional word vector models such as word2vec, fastText or GloVe, which represent monosemous words well but fail for ambiguous words. Sense embeddings represent individual senses of polysemous words as separate vectors. They can be linked to an ex"
2020.lrec-1.728,L18-1550,0,0.16799,"pment of completely unsupervised and knowledge-free approaches to word sense disambiguation (WSD). They are particularly useful for under-resourced languages which do not have any resources for building either supervised and/or knowledge-based models. In this paper, we present a method that takes as input a standard pre-trained word embedding model and induces a fully-fledged word sense inventory, which can be used for disambiguation in context. We use this method to induce a collection of sense inventories for 158 languages on the basis of the original pre-trained fastText word embeddings by Grave et al. (2018), enabling WSD in these languages. Models and system are available online. Keywords: word sense induction, word sense disambiguation, word embeddings, sense embeddings, graph clustering 1. Introduction There are many polysemous words in virtually any language. If not treated as such, they can hamper the performance of all semantic NLP tasks (Resnik, 2006). Therefore, the task of resolving the polysemy and choosing the most appropriate meaning of a word in context has been an important NLP task for a long time. It is usually referred to as Word Sense Disambiguation (WSD) and aims at assigning m"
2020.lrec-1.728,S13-2113,0,0.0199433,"s senses directly (Kutuzov, 2018), or employed further to learn sense embeddings via Chinese Restaurant Process algorithm (Li and Jurafsky, 2015), AdaGram, a Bayesian extension of the Skip-Gram model (Bartunov et al., 2016), AutoSense, an extension of the LDA topic model (Amplayo et al., 2019), and other techniques. Word ego-network clustering is applied to semantic graphs. The nodes of a semantic graph are words, and edges between them denote semantic relatedness which is usually evaluated with cosine similarity of the corresponding embeddings (Pelevina et al., 2016) or by PMI-like measures (Hope and Keller, 2013b). Word senses are induced via graph clustering algorithms, such as Chinese Whispers (Biemann, 2006) or MaxMax (Hope and Keller, 2013a). The technique suggested in our work belongs to this class of methods and is an extension of the method presented by Pelevina et al. (2016). Synonyms and substitute clustering approaches create vectors which represent synonyms or substitutes of polysemous words. Such vectors are created using synonymy dictionaries (Ustalov et al., 2019) or context-dependent substitutes obtained from a language model (Amrami and Goldberg, 2018). Analogously to previously descr"
2020.lrec-1.728,D19-1355,0,0.0605952,"Missing"
2020.lrec-1.728,P19-1165,0,0.0213523,"(a sentence which contains an ambiguous word) and a gloss (sense definition) from WordNet. The contextgloss pair is concatenated through a special token ([SEP]) and classified as positive or negative. On the other hand, sense embeddings are an alternative to traditional word vector models such as word2vec, fastText or GloVe, which represent monosemous words well but fail for ambiguous words. Sense embeddings represent individual senses of polysemous words as separate vectors. They can be linked to an explicit inventory (Iacobacci et al., 2015) or induce a sense inventory from unlabelled data (Iacobacci and Navigli, 2019). LSTMEmbed (Iacobacci and Navigli, 2019) aims at learning sense embeddings linked to BabelNet (Navigli and Ponzetto, 2012), at the same time handling word ordering, and using pre-trained embeddings as an objective. Although it was tested only on English, the approach can be easily adapted to other languages present in BabelNet. However, manually labelled datasets as well as knowledge bases exist only for a small number of wellresourced languages. Thus, to disambiguate polysemous words in other languages one has to resort to fully unsupervised techniques. The task of Word Sense Induction (WSI)"
2020.lrec-1.728,P15-1010,0,0.0808199,"Missing"
2020.lrec-1.728,W19-7405,0,0.01615,"lysemous word occur in very diverse contexts and can potentially be discriminated with their help. The fact that semantically related words occur in similar contexts, and diverse words do not share common contexts, is known as distributional hypothesis and underlies the technique of constructing word embeddings from unlabelled texts. The same intuition can be used to discriminate between different senses of individual words. There exist methods of training word embeddings that can detect polysemous words and assign them different vectors depending on their contexts (Athiwaratkun et al., 2018; Jain et al., 2019). Unfortunately, many widespread word embedding models, such as GloVe (Pennington et al., 2014), word2vec (Mikolov et al., 2013), fastText (Bojanowski et al., 2017), do not handle polysemous words. Words in these models are represented with single vectors, which were constructed from diverse sets of contexts corresponding to different senses. In such cases, their disam? Currently at Yandex. biguation needs knowledge-rich approaches. We tackle this problem by suggesting a method of posthoc unsupervised WSD. It does not require any external knowledge and can separate different senses of a polyse"
2020.lrec-1.728,S13-2049,0,0.151837,"ge in Pearson correlation score when switching from the baseline fastText embeddings to our sense vectors. The new vectors significantly improve the relatedness detection for German, Farsi, Russian, and Chinese, whereas for Italian, Dutch, and Swedish the score slightly falls behind the baseline. For other languages, the performance of sense vectors is on par with regular fastText. 5.2. Word Sense Disambiguation The purpose of our sense vectors is disambiguation of polysemous words. Therefore, we test the inventories constructed with egvi on the Task 13 of SemEval-2013 — Word Sense Induction (Jurgens and Klapaftis, 2013). The task is to identify the different senses of a target word in context in a fully unsupervised manner. 5947 lar to state-of-the-art word sense disambiguation and word sense induction models. In particular, we can see that it outperforms SenseGram on the majority of metrics. We should note that this comparison is not fully rigorous, because SenseGram induces sense inventories from word2vec as opposed to fastText vectors used in our work. 5.3. Figure 3: Absolute improvement of Pearson correlation scores of our embeddings compared to fastText. This is the averaged difference of the scores for"
2020.lrec-1.728,D15-1200,0,0.0219916,"ems, Rubyist, Rubyists, Rubys, Sadie, Sapphire, Sypro, Violet, jRuby, ruby, rubyists Table 1: Top nearest neighbours of the fastText vector of the word Ruby are clustered according to various senses of this word: programming language, gem, first name, color, but also its spelling variations (typeset in black color). vectors. Here, the definition of context may vary from window-based context to latent topic-alike context. Afterwards, the resulting clusters are either used as senses directly (Kutuzov, 2018), or employed further to learn sense embeddings via Chinese Restaurant Process algorithm (Li and Jurafsky, 2015), AdaGram, a Bayesian extension of the Skip-Gram model (Bartunov et al., 2016), AutoSense, an extension of the LDA topic model (Amplayo et al., 2019), and other techniques. Word ego-network clustering is applied to semantic graphs. The nodes of a semantic graph are words, and edges between them denote semantic relatedness which is usually evaluated with cosine similarity of the corresponding embeddings (Pelevina et al., 2016) or by PMI-like measures (Hope and Keller, 2013b). Word senses are induced via graph clustering algorithms, such as Chinese Whispers (Biemann, 2006) or MaxMax (Hope and Ke"
2020.lrec-1.728,W14-0121,0,0.0241505,"= K. arated into two clusters. Interestingly, fastText handles typos, code-switching, and emojis by correctly associating all non-standard variants to the word they refer, and our method is able to cluster them appropriately. Both inventories were produced with K = 200, which ensures stronger connectivity of graph. However, we see that this setting still produces too many clusters. We computed the average numbers of clusters produced by our model with K = 200 for words from the word relatedness datasets and compared these numbers with the number of senses in WordNet for English and RuWordNet (Loukachevitch and Dobrov, 2014) for Russian (see Table 3). We can see that the number of senses extracted by our method is consistently higher than the real number of senses. We also compute the average number of senses per word for all the languages and different values of K (see Figure 5). The average across languages does not change much as we increase K. However, for larger K the average exceed the median value, indicating that more languages have lower number of senses per word. At the same time, while at smaller K the maximum average number of senses per word does not exceed 6, larger values of K produce outliers, e.g"
2020.lrec-1.728,C14-3003,0,0.031943,"ddings, graph clustering 1. Introduction There are many polysemous words in virtually any language. If not treated as such, they can hamper the performance of all semantic NLP tasks (Resnik, 2006). Therefore, the task of resolving the polysemy and choosing the most appropriate meaning of a word in context has been an important NLP task for a long time. It is usually referred to as Word Sense Disambiguation (WSD) and aims at assigning meaning to a word in context. The majority of approaches to WSD are based on the use of knowledge bases, taxonomies, and other external manually built resources (Moro et al., 2014; Upadhyay et al., 2018). However, different senses of a polysemous word occur in very diverse contexts and can potentially be discriminated with their help. The fact that semantically related words occur in similar contexts, and diverse words do not share common contexts, is known as distributional hypothesis and underlies the technique of constructing word embeddings from unlabelled texts. The same intuition can be used to discriminate between different senses of individual words. There exist methods of training word embeddings that can detect polysemous words and assign them different vecto"
2020.lrec-1.728,D17-2016,1,0.833565,"5 clustered, in the method presented in this section we sparsify the graph by removing 13 nodes which were not in the set of the “anti-edges” i.e. pairs of most dissimilar terms out of these 50 neighbours. Examples of anti-edges i.e. pairs of most dissimilar terms for this graph include: (Haskell, Sapphire), (Garnet, Rails), (Opal, Rubyist), (Hazel, RubyOnRails), and (Coffeescript, Opal). Labelling of Induced Senses We label each word cluster representing a sense to make them and the WSD results interpretable by humans. Prior systems used hypernyms to label the clusters (Ruppert et al., 2015; Panchenko et al., 2017), e.g. “animal” in the “python (animal)”. However, neither hypernyms nor rules for their automatic extraction are available for all 158 languages. Therefore, we use a simpler method to select a keyword which would help to interpret each cluster. For each graph node v ∈ V we count the number of anti-edges it belongs to: count(v) = |{(wi , wi ) : (wi , wi ) ∈ E ∧ (v = wi ∨ v = wi )}|. A graph clustering yields a partition of V into n clusters: V = {V1 , V2 , ..., Vn }. For each cluster Vi we define a keyword wikey as the word with the largest number of antiedges count(·) among words in this clus"
2020.lrec-1.728,D17-2018,0,0.0605054,"Missing"
2020.lrec-1.728,W16-1620,1,0.576845,"rwards, the resulting clusters are either used as senses directly (Kutuzov, 2018), or employed further to learn sense embeddings via Chinese Restaurant Process algorithm (Li and Jurafsky, 2015), AdaGram, a Bayesian extension of the Skip-Gram model (Bartunov et al., 2016), AutoSense, an extension of the LDA topic model (Amplayo et al., 2019), and other techniques. Word ego-network clustering is applied to semantic graphs. The nodes of a semantic graph are words, and edges between them denote semantic relatedness which is usually evaluated with cosine similarity of the corresponding embeddings (Pelevina et al., 2016) or by PMI-like measures (Hope and Keller, 2013b). Word senses are induced via graph clustering algorithms, such as Chinese Whispers (Biemann, 2006) or MaxMax (Hope and Keller, 2013a). The technique suggested in our work belongs to this class of methods and is an extension of the method presented by Pelevina et al. (2016). Synonyms and substitute clustering approaches create vectors which represent synonyms or substitutes of polysemous words. Such vectors are created using synonymy dictionaries (Ustalov et al., 2019) or context-dependent substitutes obtained from a language model (Amrami and G"
2020.lrec-1.728,D14-1162,0,0.0837391,"Missing"
2020.lrec-1.728,L18-1167,1,0.8605,"pairs with manually assigned similarity scores from 0 (words are not related) to 10 (words are fully interchangeable) for 12 languages: English (en), Arabic (ar), German (de), Spanish (es), Farsi (fa), French (fr), Italian (it), Dutch (nl), Portuguese (pt), Russian (ru), Swedish (sv), Chinese (zh). The task is to assign relatedness scores to these pairs so that the ranking of the pairs by this score is close to the ranking defined by the oracle score. The performance is measured with Pearson correlation of the rankings. Since one word can have several different senses in our setup, we follow Remus and Biemann (2018) and define the relatedness score for a pair of words as the maximum cosine similarity between any of their sense vectors. We extract the sense inventories from fastText embedding vectors. We set N = K for all our experiments, i.e. the number of vertices in the graph and the maximum number of vertices’ nearest neighbours match. We conduct experiments with N = K set to 50, 100, and 200. For each cluster Vi we create a sense vector si by averaging vectors that belong to this cluster. We rely on the methodology of (Remus and Biemann, 2018) shifting the generated sense vector to the direction of t"
2020.lrec-1.728,P15-4018,1,0.847544,"e all 50 terms are 5945 clustered, in the method presented in this section we sparsify the graph by removing 13 nodes which were not in the set of the “anti-edges” i.e. pairs of most dissimilar terms out of these 50 neighbours. Examples of anti-edges i.e. pairs of most dissimilar terms for this graph include: (Haskell, Sapphire), (Garnet, Rails), (Opal, Rubyist), (Hazel, RubyOnRails), and (Coffeescript, Opal). Labelling of Induced Senses We label each word cluster representing a sense to make them and the WSD results interpretable by humans. Prior systems used hypernyms to label the clusters (Ruppert et al., 2015; Panchenko et al., 2017), e.g. “animal” in the “python (animal)”. However, neither hypernyms nor rules for their automatic extraction are available for all 158 languages. Therefore, we use a simpler method to select a keyword which would help to interpret each cluster. For each graph node v ∈ V we count the number of anti-edges it belongs to: count(v) = |{(wi , wi ) : (wi , wi ) ∈ E ∧ (v = wi ∨ v = wi )}|. A graph clustering yields a partition of V into n clusters: V = {V1 , V2 , ..., Vn }. For each cluster Vi we define a keyword wikey as the word with the largest number of antiedges count(·)"
2020.lrec-1.728,I05-3027,0,0.0267775,"rs of GPU-accelerated computations. We release the constructed sense inventories for all the available languages. They contain all the necessary information for using them in the proposed WSD system or in other downstream tasks. 4.2. Word Sense Disambiguation System The first text pre-processing step is language identification, for which we use the fastText language identification models by Bojanowski et al. (2017). Then the input is tokenised. For languages which use Latin, Cyrillic, Hebrew, or Greek scripts, we employ the Europarl tokeniser.2 For Chinese, we use the Stanford Word Segmenter (Tseng et al., 2005). For Japanese, we use Mecab (Kudo, 2006). We tokenise Vietnamese with UETsegmenter (Nguyen and Le, 2016). All other languages are processed with the ICU tokeniser, as implemented in the PyICU project.3 After the tokenisation, the system analyses all the input words with pre-extracted sense inventories and defines the most appropriate sense for polysemous words. Figure 2 shows the interface of the system. It has a textual input form. The automatically identified language of text is shown above. A click on any of the words displays a prompt (shown in black) with the most appropriate sense of a"
2020.lrec-1.728,D18-1270,0,0.0263365,"ering 1. Introduction There are many polysemous words in virtually any language. If not treated as such, they can hamper the performance of all semantic NLP tasks (Resnik, 2006). Therefore, the task of resolving the polysemy and choosing the most appropriate meaning of a word in context has been an important NLP task for a long time. It is usually referred to as Word Sense Disambiguation (WSD) and aims at assigning meaning to a word in context. The majority of approaches to WSD are based on the use of knowledge bases, taxonomies, and other external manually built resources (Moro et al., 2014; Upadhyay et al., 2018). However, different senses of a polysemous word occur in very diverse contexts and can potentially be discriminated with their help. The fact that semantically related words occur in similar contexts, and diverse words do not share common contexts, is known as distributional hypothesis and underlies the technique of constructing word embeddings from unlabelled texts. The same intuition can be used to discriminate between different senses of individual words. There exist methods of training word embeddings that can detect polysemous words and assign them different vectors depending on their co"
2020.lrec-1.728,L18-1164,1,0.90455,"Missing"
2020.lrec-1.728,C02-1114,0,0.018722,"tained from a language model (Amrami and Goldberg, 2018). Analogously to previously described techniques, word senses are induced by clustering these vectors. 3. Algorithm for Word Sense Induction The majority of word vector models do not discriminate between multiple senses of individual words. However, a polysemous word can be identified via manual analysis of its nearest neighbours—they reflect different senses of the word. Table 1 shows manually sense-labelled most similar terms to the word Ruby according to the pre-trained fastText model (Grave et al., 2018). As it was suggested early by Widdows and Dorow (2002), the distributional properties of a word can be used to construct a graph of words that are semantically related to it, and if a word is polysemous, such graph can easily be partitioned into a number of densely connected subgraphs corresponding to different senses of this word. Our algorithm is based on the same principle. 5944 3.1. SenseGram: A Baseline Graph-based Word Sense Induction Algorithm nodes – those which should not be connected: E = {(w1 , w1 ), (w2 , w2 ), ..., (wN , wN )}. To clarify this, consider the target (ego) word w = python, its top similar term w1 = Java and the resultin"
2020.lrec-1.728,J19-3002,1,\N,Missing
2020.pam-1.13,S19-2004,1,0.82754,"hod for inducing frame-semantic resources based on a few frame-annotated sentences using lexical substitution, and (ii) an evaluation of various distributional semantic models and lexical substitution methods on the ground truth from FrameNet. WordNet-based methods to automatically induce new LUs and reported their results on FrameNet. Our method is inspired by the recent work of Amrami and Goldberg (2018). They suggest to predict the substitutes vectors for target words using pre-trained ELMo (Peters et al., 2018) and dynamic symmetric patterns, then induced the word senses using clustering. Arefyev et al. (2019) takes the idea of substitute vectors from (Amrami and Goldberg, 2018) for the SemEval 2019 (QasemiZadeh et al., 2019) frame induction task and replaces ELMo with BERT (Devlin et al., 2019) for improved performance. Zhou et al. (2019) show the utility of BERT for the lexical substitution task. Lexical substitution has been used for a range of NLP tasks such as paraphrasing or text simplification, but here, we are employing it, as far as we are aware, for the first time to perform expansion of frame-semantic resources. 2 We experimented with two groups of lexical substitution methods. The first"
2020.pam-1.13,P98-1013,0,0.836384,"er, the latter show comparable performance on the task of LU expansion. Substitutes for Assistance: assist, aid Substitutes for Helper: she, I, he, you, we, someone, they, it, lori, hannah, paul, sarah, melanie, pam, riley Substitutes for Benefited party: me, him, folk, her, everyone, people Substitutes for Time: tomorrow, now, shortly, sooner, tonight, today, later Table 1: An example of the induced lexical representation (roles and LUs) of the Assistance FrameNet frame using lexical substitutes from a single seed sentence. annotated resources. Some publicly available resources are FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005), yet for many languages and domains, specialized resources do not exist. Besides, due to the inherent vagueness of frame definitions, the annotation task is challenging and requires semanticists or very complex crowd-sourcing setups (Fossati et al., 2013). We suggest a different perspective on the problem: expanding the FrameNet resource automatically by using lexical substitution. Given a small set of seed sentences with their frame annotations, we can expand it by substituting the targets (words corresponding to lexical units of the respective frame) and a"
2020.pam-1.13,P13-2130,0,0.0246292,"ple Substitutes for Time: tomorrow, now, shortly, sooner, tonight, today, later Table 1: An example of the induced lexical representation (roles and LUs) of the Assistance FrameNet frame using lexical substitutes from a single seed sentence. annotated resources. Some publicly available resources are FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005), yet for many languages and domains, specialized resources do not exist. Besides, due to the inherent vagueness of frame definitions, the annotation task is challenging and requires semanticists or very complex crowd-sourcing setups (Fossati et al., 2013). We suggest a different perspective on the problem: expanding the FrameNet resource automatically by using lexical substitution. Given a small set of seed sentences with their frame annotations, we can expand it by substituting the targets (words corresponding to lexical units of the respective frame) and arguments (words corresponding to roles of the respective frame) of those sentences and aggregating possible substitutions into an induced frame-semantic resource. Table 1 shows one such induced example. For this purpose, we have experimented with state-of-the-art noncontextualized (static)"
2020.pam-1.13,P14-1133,0,0.0234777,"e (Pennington et al., 2014), and word2vec (Mikolov et al., 2013); and distributional thesaurus, i.e. JoBimText (BieIntroduction The goal of lexical substitution (McCarthy and Navigli, 2009) is to replace a given target word in its context with meaning-preserving alternatives. In this paper, we show how lexical substitution can be used for semantic frame expansion. A semantic frame is a linguistic structure used to describe the formal meaning of a situation or event (Fillmore, 1982). Semantic frames have witnessed a wide range of applications; such as question answering (Shen and Lapata, 2007; Berant and Liang, 2014; Khashabi et al., 2018), machine translation (Gao and Vogel, 2011; Zhai et al., 2013), and semantic role labelling (Do et al., 2017; Swayamdipta et al., 2018). The impact, however, is limited by the scarce availability of 95 Proceedings of the Probability and Meaning Conference (PaM 2020), pages 95–103 c October 14-15 2020 2020 Association for Computational Linguistics mann and Riedl, 2013); and compared their results with contextualized word representations of the state-of-the-art BERT model (Devlin et al., 2019), which has set a new benchmark performance on many downstream NLP applications."
2020.pam-1.13,L16-1630,0,0.0404709,"Missing"
2020.pam-1.13,P14-1136,0,0.01767,"d embedding model BERT (Devlin et al., 2019) and the lexical substitution model of Melamud et al. (2015). 3 Related Work Approaches to semantic frame parsing with respect to a pre-defined semantic frame resource, such as FrameNet, have received much attention in the literature (Das et al., 2010; Oepen et al., 2016; Yang and Mitchell, 2017; Peng et al., 2018), with SEMAFOR (Das et al., 2014) being a most widely known system to extract complete frame structure including target identification. Some works focus on identifying partial structures such as frame identification (Hartmann et al., 2017; Hermann et al., 2014), role labelling with frame identification (Swayamdipta et al., 2017; Yang and Mitchell, 2017), and simple role labelling (Kshirsagar et al., 2015; Roth and Lapata, 2015; Swayamdipta et al., 2018), which is considered very similar to standard PropBank (Palmer et al., 2005) style semantic role labelling, albeit more challenging because of the high granularity of frame roles. These supervised models rely on a dataset of frame-annotated sentences such as FrameNet. FrameNet-like resources are available only for very few languages and cover only a few domains. In this paper, we venture into the inv"
2020.pam-1.13,padro-stanilovsky-2012-freeling,0,0.100483,"Missing"
2020.pam-1.13,J05-1004,0,0.426396,"formance on the task of LU expansion. Substitutes for Assistance: assist, aid Substitutes for Helper: she, I, he, you, we, someone, they, it, lori, hannah, paul, sarah, melanie, pam, riley Substitutes for Benefited party: me, him, folk, her, everyone, people Substitutes for Time: tomorrow, now, shortly, sooner, tonight, today, later Table 1: An example of the induced lexical representation (roles and LUs) of the Assistance FrameNet frame using lexical substitutes from a single seed sentence. annotated resources. Some publicly available resources are FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005), yet for many languages and domains, specialized resources do not exist. Besides, due to the inherent vagueness of frame definitions, the annotation task is challenging and requires semanticists or very complex crowd-sourcing setups (Fossati et al., 2013). We suggest a different perspective on the problem: expanding the FrameNet resource automatically by using lexical substitution. Given a small set of seed sentences with their frame annotations, we can expand it by substituting the targets (words corresponding to lexical units of the respective frame) and arguments (words corresponding to ro"
2020.pam-1.13,P15-2036,0,0.0218338,"frame parsing with respect to a pre-defined semantic frame resource, such as FrameNet, have received much attention in the literature (Das et al., 2010; Oepen et al., 2016; Yang and Mitchell, 2017; Peng et al., 2018), with SEMAFOR (Das et al., 2014) being a most widely known system to extract complete frame structure including target identification. Some works focus on identifying partial structures such as frame identification (Hartmann et al., 2017; Hermann et al., 2014), role labelling with frame identification (Swayamdipta et al., 2017; Yang and Mitchell, 2017), and simple role labelling (Kshirsagar et al., 2015; Roth and Lapata, 2015; Swayamdipta et al., 2018), which is considered very similar to standard PropBank (Palmer et al., 2005) style semantic role labelling, albeit more challenging because of the high granularity of frame roles. These supervised models rely on a dataset of frame-annotated sentences such as FrameNet. FrameNet-like resources are available only for very few languages and cover only a few domains. In this paper, we venture into the inverse problem, the case where the number of annotations is insufficient, similar to the idea of Pennacchiotti et al. (2008) who investigated the ut"
2020.pam-1.13,N18-1135,0,0.011093,"7), GloVe (Pennington et al., 2014), and word2vec (Mikolov et al., 2013), as well as distributional thesaurus based models in the form of JoBimText (Biemann and Riedl, 2013). The second group of methods does use the context: here, we tried contextualized word embedding model BERT (Devlin et al., 2019) and the lexical substitution model of Melamud et al. (2015). 3 Related Work Approaches to semantic frame parsing with respect to a pre-defined semantic frame resource, such as FrameNet, have received much attention in the literature (Das et al., 2010; Oepen et al., 2016; Yang and Mitchell, 2017; Peng et al., 2018), with SEMAFOR (Das et al., 2014) being a most widely known system to extract complete frame structure including target identification. Some works focus on identifying partial structures such as frame identification (Hartmann et al., 2017; Hermann et al., 2014), role labelling with frame identification (Swayamdipta et al., 2017; Yang and Mitchell, 2017), and simple role labelling (Kshirsagar et al., 2015; Roth and Lapata, 2015; Swayamdipta et al., 2018), which is considered very similar to standard PropBank (Palmer et al., 2005) style semantic role labelling, albeit more challenging because of"
2020.pam-1.13,P14-2050,0,0.0304354,"rd annotations from the rest of the corpus marked with the same role and related to the same frame as ground truth substitutes. The final datasets for experiments contain 79, 584 records for lexical unit expansion and 191, 252 records for role expansion (cf. Tables 4 and 5). Contextualized Models Static word representations fail to handle polysemic words. This paves the way for contextaware word representation models, which can generate diverse word-probability distributions for a target word based on its context. Melamud et al. (2015) This simple model uses syntax-based skip-gram embeddings (Levy and Goldberg, 2014) of a word and its context to produce context-sensitive lexical substitutes, where the context of the word is represented by the dependency relations of the word. We use the original word and context embeddings of Melamud et al. (2015), trained on the ukWaC (Ferraresi et al., 2008) corpus. To find dependency relations, we use Stanford Parser (Chen and Manning, 2014) and collapsed the dependencies that include prepositions. Top k substitutes are produced if both the word and its context are present in the model’s vocabulary. Melamud et al. (2015) proposed four measures of contextual similarity"
2020.pam-1.13,D08-1048,0,0.0924979,"Missing"
2020.pam-1.13,P98-2127,0,0.149749,"ent words as vectors of continuous numbers, where words with similar meanings are expected to have similar vectors. Thus, to produce substitutes, we extracted the k nearest neighbors using a cosine similarity measure. We use pre-trained embeddings by authors models: fastText trained on the Common Crawl corpus, GloVe trained on Common Crawl corpus with 840 billion words, word2vec trained on Google News. All these models produce 300-dimension vectors. 96 Distributional Thesaurus (DT) In this approach, word similarities are computed using complex linguistic features such as dependency relations (Lin, 1998). The representations provided by DTs are sparser, but similarity scores based on them can be better. JoBimText (Biemann and Riedl, 2013) is a framework that offers many DTs computed on a range of different corpora. Context features for each word are ranked using the lexicographer’s mutual information (LMI) score and used to compute word similarity by feature overlap. We extract the k nearest neighbors for the target word. We use two JoBimText DTs: (i) DT built on Wikipedia with n-grams as contexts and (ii) DT built on a 59G corpus (Wikipedia, Gigaword, ukWaC, and LCC corpora combined) using d"
2020.pam-1.13,D14-1162,0,0.0836952,"exical substitution. Given a small set of seed sentences with their frame annotations, we can expand it by substituting the targets (words corresponding to lexical units of the respective frame) and arguments (words corresponding to roles of the respective frame) of those sentences and aggregating possible substitutions into an induced frame-semantic resource. Table 1 shows one such induced example. For this purpose, we have experimented with state-of-the-art noncontextualized (static) word representation models including neural word embeddings, i.e. fastText (Bojanowski et al., 2017), GloVe (Pennington et al., 2014), and word2vec (Mikolov et al., 2013); and distributional thesaurus, i.e. JoBimText (BieIntroduction The goal of lexical substitution (McCarthy and Navigli, 2009) is to replace a given target word in its context with meaning-preserving alternatives. In this paper, we show how lexical substitution can be used for semantic frame expansion. A semantic frame is a linguistic structure used to describe the formal meaning of a situation or event (Fillmore, 1982). Semantic frames have witnessed a wide range of applications; such as question answering (Shen and Lapata, 2007; Berant and Liang, 2014; Kha"
2020.pam-1.13,N18-1202,0,0.0391628,"to generate meaningpreserving substitutes for this argument. Contributions of our work are (i) a method for inducing frame-semantic resources based on a few frame-annotated sentences using lexical substitution, and (ii) an evaluation of various distributional semantic models and lexical substitution methods on the ground truth from FrameNet. WordNet-based methods to automatically induce new LUs and reported their results on FrameNet. Our method is inspired by the recent work of Amrami and Goldberg (2018). They suggest to predict the substitutes vectors for target words using pre-trained ELMo (Peters et al., 2018) and dynamic symmetric patterns, then induced the word senses using clustering. Arefyev et al. (2019) takes the idea of substitute vectors from (Amrami and Goldberg, 2018) for the SemEval 2019 (QasemiZadeh et al., 2019) frame induction task and replaces ELMo with BERT (Devlin et al., 2019) for improved performance. Zhou et al. (2019) show the utility of BERT for the lexical substitution task. Lexical substitution has been used for a range of NLP tasks such as paraphrasing or text simplification, but here, we are employing it, as far as we are aware, for the first time to perform expansion of f"
2020.pam-1.13,S19-2003,0,0.0149138,"(ii) an evaluation of various distributional semantic models and lexical substitution methods on the ground truth from FrameNet. WordNet-based methods to automatically induce new LUs and reported their results on FrameNet. Our method is inspired by the recent work of Amrami and Goldberg (2018). They suggest to predict the substitutes vectors for target words using pre-trained ELMo (Peters et al., 2018) and dynamic symmetric patterns, then induced the word senses using clustering. Arefyev et al. (2019) takes the idea of substitute vectors from (Amrami and Goldberg, 2018) for the SemEval 2019 (QasemiZadeh et al., 2019) frame induction task and replaces ELMo with BERT (Devlin et al., 2019) for improved performance. Zhou et al. (2019) show the utility of BERT for the lexical substitution task. Lexical substitution has been used for a range of NLP tasks such as paraphrasing or text simplification, but here, we are employing it, as far as we are aware, for the first time to perform expansion of frame-semantic resources. 2 We experimented with two groups of lexical substitution methods. The first one use no context: non-contextualized neural word embedding models, i.e. fastText (Bojanowski et al., 2017), GloVe ("
2020.pam-1.13,Q15-1032,0,0.0190172,"t to a pre-defined semantic frame resource, such as FrameNet, have received much attention in the literature (Das et al., 2010; Oepen et al., 2016; Yang and Mitchell, 2017; Peng et al., 2018), with SEMAFOR (Das et al., 2014) being a most widely known system to extract complete frame structure including target identification. Some works focus on identifying partial structures such as frame identification (Hartmann et al., 2017; Hermann et al., 2014), role labelling with frame identification (Swayamdipta et al., 2017; Yang and Mitchell, 2017), and simple role labelling (Kshirsagar et al., 2015; Roth and Lapata, 2015; Swayamdipta et al., 2018), which is considered very similar to standard PropBank (Palmer et al., 2005) style semantic role labelling, albeit more challenging because of the high granularity of frame roles. These supervised models rely on a dataset of frame-annotated sentences such as FrameNet. FrameNet-like resources are available only for very few languages and cover only a few domains. In this paper, we venture into the inverse problem, the case where the number of annotations is insufficient, similar to the idea of Pennacchiotti et al. (2008) who investigated the utility of semantic space"
2020.pam-1.13,D07-1002,0,0.0865619,"ski et al., 2017), GloVe (Pennington et al., 2014), and word2vec (Mikolov et al., 2013); and distributional thesaurus, i.e. JoBimText (BieIntroduction The goal of lexical substitution (McCarthy and Navigli, 2009) is to replace a given target word in its context with meaning-preserving alternatives. In this paper, we show how lexical substitution can be used for semantic frame expansion. A semantic frame is a linguistic structure used to describe the formal meaning of a situation or event (Fillmore, 1982). Semantic frames have witnessed a wide range of applications; such as question answering (Shen and Lapata, 2007; Berant and Liang, 2014; Khashabi et al., 2018), machine translation (Gao and Vogel, 2011; Zhai et al., 2013), and semantic role labelling (Do et al., 2017; Swayamdipta et al., 2018). The impact, however, is limited by the scarce availability of 95 Proceedings of the Probability and Meaning Conference (PaM 2020), pages 95–103 c October 14-15 2020 2020 Association for Computational Linguistics mann and Riedl, 2013); and compared their results with contextualized word representations of the state-of-the-art BERT model (Devlin et al., 2019), which has set a new benchmark performance on many down"
2020.pam-1.13,D18-1412,0,0.0703975,"tion (McCarthy and Navigli, 2009) is to replace a given target word in its context with meaning-preserving alternatives. In this paper, we show how lexical substitution can be used for semantic frame expansion. A semantic frame is a linguistic structure used to describe the formal meaning of a situation or event (Fillmore, 1982). Semantic frames have witnessed a wide range of applications; such as question answering (Shen and Lapata, 2007; Berant and Liang, 2014; Khashabi et al., 2018), machine translation (Gao and Vogel, 2011; Zhai et al., 2013), and semantic role labelling (Do et al., 2017; Swayamdipta et al., 2018). The impact, however, is limited by the scarce availability of 95 Proceedings of the Probability and Meaning Conference (PaM 2020), pages 95–103 c October 14-15 2020 2020 Association for Computational Linguistics mann and Riedl, 2013); and compared their results with contextualized word representations of the state-of-the-art BERT model (Devlin et al., 2019), which has set a new benchmark performance on many downstream NLP applications. To complete the comparison, we also include the lexical substitution model of Melamud et al. (2015), which uses dependency-based word and context embeddings a"
2020.pam-1.13,D17-1128,0,0.0129786,"t (Bojanowski et al., 2017), GloVe (Pennington et al., 2014), and word2vec (Mikolov et al., 2013), as well as distributional thesaurus based models in the form of JoBimText (Biemann and Riedl, 2013). The second group of methods does use the context: here, we tried contextualized word embedding model BERT (Devlin et al., 2019) and the lexical substitution model of Melamud et al. (2015). 3 Related Work Approaches to semantic frame parsing with respect to a pre-defined semantic frame resource, such as FrameNet, have received much attention in the literature (Das et al., 2010; Oepen et al., 2016; Yang and Mitchell, 2017; Peng et al., 2018), with SEMAFOR (Das et al., 2014) being a most widely known system to extract complete frame structure including target identification. Some works focus on identifying partial structures such as frame identification (Hartmann et al., 2017; Hermann et al., 2014), role labelling with frame identification (Swayamdipta et al., 2017; Yang and Mitchell, 2017), and simple role labelling (Kshirsagar et al., 2015; Roth and Lapata, 2015; Swayamdipta et al., 2018), which is considered very similar to standard PropBank (Palmer et al., 2005) style semantic role labelling, albeit more ch"
2020.pam-1.13,P13-1111,0,0.0107154,"urus, i.e. JoBimText (BieIntroduction The goal of lexical substitution (McCarthy and Navigli, 2009) is to replace a given target word in its context with meaning-preserving alternatives. In this paper, we show how lexical substitution can be used for semantic frame expansion. A semantic frame is a linguistic structure used to describe the formal meaning of a situation or event (Fillmore, 1982). Semantic frames have witnessed a wide range of applications; such as question answering (Shen and Lapata, 2007; Berant and Liang, 2014; Khashabi et al., 2018), machine translation (Gao and Vogel, 2011; Zhai et al., 2013), and semantic role labelling (Do et al., 2017; Swayamdipta et al., 2018). The impact, however, is limited by the scarce availability of 95 Proceedings of the Probability and Meaning Conference (PaM 2020), pages 95–103 c October 14-15 2020 2020 Association for Computational Linguistics mann and Riedl, 2013); and compared their results with contextualized word representations of the state-of-the-art BERT model (Devlin et al., 2019), which has set a new benchmark performance on many downstream NLP applications. To complete the comparison, we also include the lexical substitution model of Melamud"
2020.semeval-1.234,Q17-1010,0,0.0261931,"le 2: Example of our sentence augmentation method based on different (1) models for word embeddings, e.g. GloVe or BERT; (2) word POS used for that substitution, e.g. “n” for noun expansion. Red color denotes replaced nouns, green is adjectives, violet is adverbs, and blue color denotes replaced verbs. Finally, yellow box denotes the target propaganda span annotation. • Substitution model. In order to find a replacement for the word, we used the search for the nearest word vector representations. In this research we decided to investigate research on GloVe (Pennington et al., 2014), fastText (Bojanowski et al., 2017) and BERT (Devlin et al., 2019) word embeddings. • Choice of words to replace. We chose candidates for replacement based on their parts of speech (POS). At the same time, we did not replace stop words, as well as words with a high frequency of occurrence in the language. This was done not to replace the pronouns, common nouns (everything, nothing), numerals, common adverbs (very), etc. As a results, we combine several strategies for substitution: only nouns (n); nouns and adjectives (n, adj); nouns, adjectives, and adverbs (n, adj, adv); nouns, adjectives, adverbs and verbs (n, adj, adv, v). •"
2020.semeval-1.234,P19-3031,1,0.827819,"erts char-level markup into token-level markup. At the post-processing step, we did the reverse transformation of the markup. The pipeline of our final solution is presented in Figure 1. Input in char-level markup Token-level markup; Combining sentences in context. BERT Language Model Linear Classiﬁer Token classes Output in char-level markup Figure 1: The pipeline of our final solution of the Span Identification (SI) task (BERT-Linear). 3.1 Model During the competition, we conducted experiments with different models. The first one was BiLSTMCNN-CRF model (Ma and Hovy, 2016) as implemented by Chernodub et al. (2019). This is a commonly used approach for NER and sequence labelling task: it uses both word-level and char-level embeddings that are fed to BiLSTM-CNN-CRF module. The second one, denoted as BERT-Linear, relies on a linear classifier on the top of BERT-based token representations. The implementation of this sequence tagger is based on BertForTokenClassification class from the transformers library2 as done by Shelmanov et al. (2019). Finally, we also tried BERT-CRF model3 : after BERT classifier a Viterbi decoder is used for better tags sequence approximation. Our final submission is based on the"
2020.semeval-1.234,D19-5024,0,0.0131867,". Our main contribution is an investigation of several unsupervised data augmentation techniques based on distributional semantics expanding the original small training dataset as applied to this BERT-based sequence tagger. We explore various expansion strategies and show that they can substantially shift the balance between precision and recall, while maintaining comparable levels of the F1 score. 1 Introduction Propaganda is one of the primary indicators of false news. Therefore, the task of detecting and classifying propaganda is an important one in the field of fake news detection. Da San Martino et al. (2019) presented a new dataset for propaganda detection. The main advantage of this dataset is the markup at the level of individual text fragments. In addition to binary markup (is/is not propaganda), there is also a multiclass markup, which includes 18 different propaganda classes. Using this dataset, the Span Identification (SI) task of Detection of Propaganda Techniques in News Articles task requires to determine specific text fragments which contain at least one propaganda technique. For instance, in the sentence “Manchin says Democrats acted like babies at the SOTU (video) Personal Liberty Pol"
2020.semeval-1.234,N19-1423,0,0.496153,"task requires to determine specific text fragments which contain at least one propaganda technique. For instance, in the sentence “Manchin says Democrats acted like babies at the SOTU (video) Personal Liberty PollExercise your right to vote” the part from the 34-th through the 40-th character (i.e., word “babies”) belongs to the Name-calling and Labeling class, so it should be marked as propaganda. This paper describes the solution by “SkoltechNLP” team which took part in the SI task of the competition and achieved a score F 1 = 0.34 on the test set evaluation. Our solution is based on BERT (Devlin et al., 2019) that is specially pretrained for the sequence tagging task. However, such architectures usually require large datasets for fine-tuning: for instance, CoNLL-2003 (Tjong Kim Sang and De Meulder, 2003) consists of 976 articles in the training set, in comparison with 371 articles in the training set for the propaganda detection task. Therefore, we conducted a study of the dataset expansion with several augmentation techniques. We built a number of strategies, performing investigation of various combinations of such parameters as: the size of the enlarged dataset, models for word representations,"
2020.semeval-1.234,D19-5023,0,0.0206404,"nts with propaganda. The difference is that the markup was at the level of whole sentences. As a result, the authors solved the problem of determining the sentence to one of the 19 classes (without propaganda or one of the 18 types of propaganda). To solve this problem they used model based on BERT Language Model with linear classification head for token classification. They also tried several techniques to overcome the lack of data and classes imbalance: 1) weighting rarer classes with higher probability; 2) sample propaganda sentences with a higher probability than non-propaganda sentences. Ek and Ghanimifard (2019) describe solution also for the same competition at the NLP4IF’19 workshop. As a classification model they use BiLSTM. In addition to the model development, the authors investigate different augmentation techniques for balancing classes. They used synthetic-minority over-sampling (Chawla et al., 2002) algorithm to generate token embeddnings for the minority classes in the dataset. They used three models for contextual embeddings – ELMo (Peters et al., 2018), BERT (Devlin et al., 2019) and GROVER (Zellers et al., 2019). Out of these models, ELMo showed the overall best F1-score for classes in t"
2020.semeval-1.234,P16-1101,0,0.0369538,"e made a preprocessing step that converts char-level markup into token-level markup. At the post-processing step, we did the reverse transformation of the markup. The pipeline of our final solution is presented in Figure 1. Input in char-level markup Token-level markup; Combining sentences in context. BERT Language Model Linear Classiﬁer Token classes Output in char-level markup Figure 1: The pipeline of our final solution of the Span Identification (SI) task (BERT-Linear). 3.1 Model During the competition, we conducted experiments with different models. The first one was BiLSTMCNN-CRF model (Ma and Hovy, 2016) as implemented by Chernodub et al. (2019). This is a commonly used approach for NER and sequence labelling task: it uses both word-level and char-level embeddings that are fed to BiLSTM-CNN-CRF module. The second one, denoted as BERT-Linear, relies on a linear classifier on the top of BERT-based token representations. The implementation of this sequence tagger is based on BertForTokenClassification class from the transformers library2 as done by Shelmanov et al. (2019). Finally, we also tried BERT-CRF model3 : after BERT classifier a Viterbi decoder is used for better tags sequence approximat"
2020.semeval-1.234,D14-1162,0,0.0881465,"pidemic will soon fully be over. Table 2: Example of our sentence augmentation method based on different (1) models for word embeddings, e.g. GloVe or BERT; (2) word POS used for that substitution, e.g. “n” for noun expansion. Red color denotes replaced nouns, green is adjectives, violet is adverbs, and blue color denotes replaced verbs. Finally, yellow box denotes the target propaganda span annotation. • Substitution model. In order to find a replacement for the word, we used the search for the nearest word vector representations. In this research we decided to investigate research on GloVe (Pennington et al., 2014), fastText (Bojanowski et al., 2017) and BERT (Devlin et al., 2019) word embeddings. • Choice of words to replace. We chose candidates for replacement based on their parts of speech (POS). At the same time, we did not replace stop words, as well as words with a high frequency of occurrence in the language. This was done not to replace the pronouns, common nouns (everything, nothing), numerals, common adverbs (very), etc. As a results, we combine several strategies for substitution: only nouns (n); nouns and adjectives (n, adj); nouns, adjectives, and adverbs (n, adj, adv); nouns, adjectives, a"
2020.semeval-1.234,N18-1202,0,0.013119,"ghting rarer classes with higher probability; 2) sample propaganda sentences with a higher probability than non-propaganda sentences. Ek and Ghanimifard (2019) describe solution also for the same competition at the NLP4IF’19 workshop. As a classification model they use BiLSTM. In addition to the model development, the authors investigate different augmentation techniques for balancing classes. They used synthetic-minority over-sampling (Chawla et al., 2002) algorithm to generate token embeddnings for the minority classes in the dataset. They used three models for contextual embeddings – ELMo (Peters et al., 2018), BERT (Devlin et al., 2019) and GROVER (Zellers et al., 2019). Out of these models, ELMo showed the overall best F1-score for classes in the FLC task. However, for individual classes, the best model varies. Since in the previous competition the participant with successful solutions focused more on pre-trained contextualized models, we also decided in our approach to focus on such models, BERT in particular. Moreover, as data augmentation applications were used in previous works in propaganda detection (Ek and Ghanimifard, 2019) and also have shown significant results in other fields like comp"
2020.semeval-1.234,D19-5011,0,0.0177003,"presents our final solution and describes model architecture used, preprocessing, and implementation details. Section 4 provides the description and the results of the dataset expansion experiments. Finally, Section 5 concludes this report. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 https://github.com/skoltech-nlp/semeval2020-propaganda 1786 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 1786–1792 Barcelona, Spain (Online), December 12, 2020. 2 Related Work Yoosuf and Yang (2019) proposed a solution for Fragment Level Classification (FLC) task in the Fine Grained Propaganda Detection competition at the NLP4IF’19 workshop. The participants had a similar task as in “Detection of Propaganda Techniques in News Articles” competition of SemEval 2020 to detect text fragments with propaganda. The difference is that the markup was at the level of whole sentences. As a result, the authors solved the problem of determining the sentence to one of the 19 classes (without propaganda or one of the 18 types of propaganda). To solve this problem they used model based on BERT Language"
2021.acl-srw.32,karadzhov-etal-2017-fully,0,0.0677273,"Missing"
2021.acl-srw.32,2020.coling-main.165,0,0.043136,"a et al., 2020) different linguistic features extracted from news texts were used. In (Ghanem et al., 2020) the perspective of the usage of emotional signals extracted from the news text for detecting fakes was shown. In addition to internal features, a set of external features can add more confidence in fake news detection model decision reasoning. For instance, user interaction signals were explored in (Nguyen et al., 2020; Cui et al., 2019). Another quite strong signal can be additional information extracted from the Web. In (Popat et al., 2017; Karadzhov et al., 2017; Ghanem et al., 2018; Li and Zhou, 2020) the authors referred to the Web search (Google or Bing) to collect relevant articles and use such scraped information as an external feature to build a fake news classifier. Seeking information via some search engine to find evidence is a quite natural feature motivated by real users’ behaviour. Several studies tried to figure out how users authenticate the information from the Web. Jr. et al. (2018) showed that individuals rely on both their judgment of the source and the message, and when this does not adequately provide a definitive answer, they turn to external resources to authenticate n"
2021.acl-srw.32,C18-1287,0,0.0620024,"Missing"
2021.bsnlp-1.4,2020.alw-1.16,0,0.0340579,"models: Xia et al. (2020) train their model to detect two objectives: toxicity and presence of the toxicity-provoking topic, Zhang et al. (2020) perform re-weighing of instances, Park et al. (2018) create pseudo-data to level off the balance of examples. Unlike our research, these works often deal with one topic and use topic-specific methods. The main drawback of topic-based toxicity detection in the existing research is the ad-hoc choice of topics: the authors select a small number of popular topics manually or based on the topics which emerge in the data often, as Ousidhoum et al. (2019). Banko et al. (2020) suggest a taxonomy of harmful online behaviour. It contains toxic topics, but they are mixed with other parameters of toxicity (e.g. direction or severity). The work by Salminen et al. (2020) is the only example of an extensive list of toxicity-provoking topics. This is similar to sensitive topics we deal with. However, our definition is broader — sensitive topics are not only topics that attract toxicity, but they can also create unwanted dialogues of multiple types (e.g. incitement to law violation or to cause harm to oneself or others). • We define the notions of sensitive topics and inapp"
2021.bsnlp-1.4,S19-2007,0,0.0237845,"emphasize that our definition of sensitive topics does not imply that any conversation concerning them need to be banned. Sensitive topics are just topics that should be considered with extra care and tend to often flame/catalyze toxicity. The contributions of our work are three-fold: 2019). Offenses can be directed towards an individual, a group, or undirected (Zampieri et al., 2019), explicit or implicit (Waseem et al., 2017). Insults do not necessarily have a topic, but there certainly exist toxic topics, such as sexism, racism, xenophobia. Waseem and Hovy (2016) tackle sexism and racism, Basile et al. (2019) collect texts which contain sexism and aggression towards immigrants. Besides directly classifying toxic messages for a topic, the notion of the topic in toxicity is also indirectly used to collect the data: Zampieri et al. (2019) pre-select messages for toxicity labeling based on their topic. Similarly, Hessel and Lee (2019) use topics to find controversial (potentially toxic) discussions. Such a topic-based view of toxicity causes unintended bias in toxicity detection – a false association of toxicity with a particular topic (LGBT, Islam, feminism, etc.) (Dixon et al., 2018; Vaidya et al.,"
2021.bsnlp-1.4,D19-1176,0,0.0615398,"Missing"
2021.bsnlp-1.4,D19-1474,0,0.0135536,"for debiasing the trained models: Xia et al. (2020) train their model to detect two objectives: toxicity and presence of the toxicity-provoking topic, Zhang et al. (2020) perform re-weighing of instances, Park et al. (2018) create pseudo-data to level off the balance of examples. Unlike our research, these works often deal with one topic and use topic-specific methods. The main drawback of topic-based toxicity detection in the existing research is the ad-hoc choice of topics: the authors select a small number of popular topics manually or based on the topics which emerge in the data often, as Ousidhoum et al. (2019). Banko et al. (2020) suggest a taxonomy of harmful online behaviour. It contains toxic topics, but they are mixed with other parameters of toxicity (e.g. direction or severity). The work by Salminen et al. (2020) is the only example of an extensive list of toxicity-provoking topics. This is similar to sensitive topics we deal with. However, our definition is broader — sensitive topics are not only topics that attract toxicity, but they can also create unwanted dialogues of multiple types (e.g. incitement to law violation or to cause harm to oneself or others). • We define the notions of sensi"
2021.bsnlp-1.4,D18-1302,0,0.019698,"a topic-based view of toxicity causes unintended bias in toxicity detection – a false association of toxicity with a particular topic (LGBT, Islam, feminism, etc.) (Dixon et al., 2018; Vaidya et al., 2020). This is in line with our work since we also acknowledge that there exist acceptable and unacceptable messages within toxicity-provoking topics. The existing work suggests algorithmic ways for debiasing the trained models: Xia et al. (2020) train their model to detect two objectives: toxicity and presence of the toxicity-provoking topic, Zhang et al. (2020) perform re-weighing of instances, Park et al. (2018) create pseudo-data to level off the balance of examples. Unlike our research, these works often deal with one topic and use topic-specific methods. The main drawback of topic-based toxicity detection in the existing research is the ad-hoc choice of topics: the authors select a small number of popular topics manually or based on the topics which emerge in the data often, as Ousidhoum et al. (2019). Banko et al. (2020) suggest a taxonomy of harmful online behaviour. It contains toxic topics, but they are mixed with other parameters of toxicity (e.g. direction or severity). The work by Salminen"
2021.bsnlp-1.4,D17-1117,0,0.0288632,"in and release models which define a topic of a text and define its appropriateness. We open the access to the produced datasets, code, and pre-trained models for the research use.3 2 Related Work There exist a large number of English textual corpora labeled for the presence or absence of toxicity; some resources indicate the degree of toxicity and its topic. However, the definition of the term “toxicity” itself is not agreed among the research community, so each research deals with different texts. Some works refer to any unwanted behaviour as toxicity and do not make any further separation (Pavlopoulos et al., 2017). However, the majority of researchers use more fine-grained labeling. The Wikipedia Toxic comment datasets by Jigsaw (Jigsaw, 2018, 2019, 2020) are the largest English toxicity datasets available to date operate with multiple types of toxicity (toxic, obscene, threat, insult, identity hate, etc). Toxicity differs across multiple axes. Some works concentrate solely on major offence (hate speech) (Davidson et al., 2017), others research more subtle assaults (Breitfeller et al., 3 3 https://github.com/skoltech-nlp/ inappropriate-sensitive-topics Inappropriateness and Sensitive Topics Consider th"
2021.bsnlp-1.4,N19-1166,0,0.02824,"an individual, a group, or undirected (Zampieri et al., 2019), explicit or implicit (Waseem et al., 2017). Insults do not necessarily have a topic, but there certainly exist toxic topics, such as sexism, racism, xenophobia. Waseem and Hovy (2016) tackle sexism and racism, Basile et al. (2019) collect texts which contain sexism and aggression towards immigrants. Besides directly classifying toxic messages for a topic, the notion of the topic in toxicity is also indirectly used to collect the data: Zampieri et al. (2019) pre-select messages for toxicity labeling based on their topic. Similarly, Hessel and Lee (2019) use topics to find controversial (potentially toxic) discussions. Such a topic-based view of toxicity causes unintended bias in toxicity detection – a false association of toxicity with a particular topic (LGBT, Islam, feminism, etc.) (Dixon et al., 2018; Vaidya et al., 2020). This is in line with our work since we also acknowledge that there exist acceptable and unacceptable messages within toxicity-provoking topics. The existing work suggests algorithmic ways for debiasing the trained models: Xia et al. (2020) train their model to detect two objectives: toxicity and presence of the toxicity"
2021.bsnlp-1.4,2020.socialnlp-1.2,0,0.0284037,"9) pre-select messages for toxicity labeling based on their topic. Similarly, Hessel and Lee (2019) use topics to find controversial (potentially toxic) discussions. Such a topic-based view of toxicity causes unintended bias in toxicity detection – a false association of toxicity with a particular topic (LGBT, Islam, feminism, etc.) (Dixon et al., 2018; Vaidya et al., 2020). This is in line with our work since we also acknowledge that there exist acceptable and unacceptable messages within toxicity-provoking topics. The existing work suggests algorithmic ways for debiasing the trained models: Xia et al. (2020) train their model to detect two objectives: toxicity and presence of the toxicity-provoking topic, Zhang et al. (2020) perform re-weighing of instances, Park et al. (2018) create pseudo-data to level off the balance of examples. Unlike our research, these works often deal with one topic and use topic-specific methods. The main drawback of topic-based toxicity detection in the existing research is the ad-hoc choice of topics: the authors select a small number of popular topics manually or based on the topics which emerge in the data often, as Ousidhoum et al. (2019). Banko et al. (2020) sugges"
2021.bsnlp-1.4,N19-1144,0,0.028125,"he Internet, we suggest that it has many use-cases which serve the common good and do not limit free speech. Such applications are parental control or sustaining of respectful tone in conversations online, inter alia. We would like to emphasize that our definition of sensitive topics does not imply that any conversation concerning them need to be banned. Sensitive topics are just topics that should be considered with extra care and tend to often flame/catalyze toxicity. The contributions of our work are three-fold: 2019). Offenses can be directed towards an individual, a group, or undirected (Zampieri et al., 2019), explicit or implicit (Waseem et al., 2017). Insults do not necessarily have a topic, but there certainly exist toxic topics, such as sexism, racism, xenophobia. Waseem and Hovy (2016) tackle sexism and racism, Basile et al. (2019) collect texts which contain sexism and aggression towards immigrants. Besides directly classifying toxic messages for a topic, the notion of the topic in toxicity is also indirectly used to collect the data: Zampieri et al. (2019) pre-select messages for toxicity labeling based on their topic. Similarly, Hessel and Lee (2019) use topics to find controversial (poten"
2021.bsnlp-1.4,2020.acl-main.380,0,0.0226589,"d controversial (potentially toxic) discussions. Such a topic-based view of toxicity causes unintended bias in toxicity detection – a false association of toxicity with a particular topic (LGBT, Islam, feminism, etc.) (Dixon et al., 2018; Vaidya et al., 2020). This is in line with our work since we also acknowledge that there exist acceptable and unacceptable messages within toxicity-provoking topics. The existing work suggests algorithmic ways for debiasing the trained models: Xia et al. (2020) train their model to detect two objectives: toxicity and presence of the toxicity-provoking topic, Zhang et al. (2020) perform re-weighing of instances, Park et al. (2018) create pseudo-data to level off the balance of examples. Unlike our research, these works often deal with one topic and use topic-specific methods. The main drawback of topic-based toxicity detection in the existing research is the ad-hoc choice of topics: the authors select a small number of popular topics manually or based on the topics which emerge in the data often, as Ousidhoum et al. (2019). Banko et al. (2020) suggest a taxonomy of harmful online behaviour. It contains toxic topics, but they are mixed with other parameters of toxicit"
2021.eacl-demos.36,D19-5405,0,0.0329609,"Missing"
2021.eacl-demos.36,D19-6606,0,0.0217934,"ject, and (4) entering extracted comparative structures in templates. 5.1 most representative sentences and display it in the proper form. To create an answer, CAM: Bullet points mentions a “winner” defined by CAM with respect to aspects if they exist. It also takes the top-3 sentences supporting each of the objects and produces a list for highlighting the advantages and disadvantages of each object in comparison. Generation Methods Pre-trained Language Models Pre-trained language models have been shown to contain commonsense knowledge, so they can be successfully used for question answering (Andrews and Witteveen, 2019) and for generating sensible and coherent continuation of text. Therefore, we use Transformersbased CTRL (Keskar et al., 2019) models for answering comparative questions. CTRL allows explicit control codes to vary the domain and the content of the text. We use the Links control code, which forces the model to produce text similar to online news and reports. We feed into CTRL phrase “Links Which is better in respect to the aspect: object1 or object2 ?” and a row question from the input. We also vary the maximum number of tokens generated by CTRL. We experiment with different length set, includi"
2021.eacl-demos.36,N19-1423,0,0.0136652,"t question. We test several common baselines starting with simple one-layer bidirectional LSTM described by (Arora et al., 2017) where the input is encoded with GloVe (Pennington et al., 2014) embeddings. For some further improvements, we add Conditional Random Field (Sutton and McCallum, 2012) Objects Aspects Predicates 0.925 0.829 0.654 0.631 0.582 0.685 0.563 0.487 0.475 0.328 0.894 0.869 0.825 0.766 0.730 to LSTM and use context-based ELMO (Peters et al., 2018) embeddings for token representations. We also experiment with Transformers (Vaswani et al., 2017) using a pre-trained BERT model (Devlin et al., 2019) and RoBERTa (Liu et al., 2019), which is its modification yielding better performance. For every classifier, during training, we tune hyperparameters by varying a batch size (from 16 to 100) and a learning rate (from 10−6 to 10−2 ). To find a proper converge of the training process, we apply two types of learning rate schedulers: Linear With Warmup and Slanted Triangular. For the model with the highest achieved F1 (RoBERTa), we employ stochastic weight ensembling (Goodfellow et al., 2015; Garipov et al., 2018), i.e., we interpolate between the weights obtained by training a certain model with"
2021.eacl-demos.36,W16-3622,0,0.0499051,"Missing"
2021.eacl-demos.36,W06-3812,1,0.222676,"f edges and sets the node weights to be proportional to the importance of adjacent edges. To make the graph sparse, we remove the edges with a score below average. We create separate graphs for sentences supporting each of the objects. We apply TextRank to each of them and then cluster them. Clustering divides the nodes in graphs by semantic similarity and thus allows identifying groups of sentences supporting a particular idea. Then, we apply TextRank again to each of the clusters separately and select the three most characteristic sentences from each cluster as produced by Chinese Whispers (Biemann, 2006), an iterative clustering algorithm, which assigns vertices to the most common class among their neighbors. Argumentative sentences selected in this way are displayed as a bullet-list after declaring the “winner” object of comparison. Document-Retrieval-Based Method To compose an answer, CAM: First snippets takes the first sentence related to the “winner” object in CAM output. Then it finds a document corresponding to this sentence and extracts the surrounding context. The obtained context consists of 3 sentences and is considered to be a system answer. 306 Table 3: Evaluation of generation me"
2021.eacl-demos.36,D18-1426,0,0.0198934,"template-based generation to Transformers-based language models. The main contributions of our work are threefold: (i) we design an evaluation framework for comparative QA, featuring a dataset based on Yahoo! Answers; (ii) we test several strategies for identification of comparative structures and for answer generation; (iii) we develop an online demo using three answer generation approaches. A demo of the system is available online.1 Besides, we release our code and data. 2 Related Work Text Generation Most of the current text natural language generation tasks (Duˇsek and Jurˇc´ıcˇ ek, 2016; Freitag and Roy, 2018) are based on se1 https://skoltech-nlp.github.io/coqas 302 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations, pages 302–311 April 19 - 23, 2021. ©2021 Association for Computational Linguistics quence to sequence model architecture (Sutskever et al., 2014). The existing generation methods are developed by employing attention mechanism (Bahdanau et al., 2015) and pointer-generator network (See et al., 2017). More recent work on text generation focus on generating natural language using multitask learning from multi-"
2021.eacl-demos.36,C08-1031,0,0.0748995,", 2019; Coavoux et al., 2019). A common approach to summarization is based on the TextRank graph algorithm (Mihalcea, 2004; Fan and Fang, 2017). Comparative QA According to Li and Roth (2006), questions can be divided into 6 coarse and 50 fine-grained categories, such as factoid questions, list questions, or definition questions: we focus on comparative questions. Sun et al. (2006) proposed one of the first works on automatic comparative web search, where each object was submitted as a separate query, then obtained results were compared. Opinion mining of comparative sentences is discussed by Ganapathibhotla and Liu (2008) and Jindal and Liu (2006), yet with no connection to argumentation mining. Instead, comparative information needs are partially satisfied by several kinds of industrial systems mentioned above. Schildw¨achter et al. (2019) proposed Comparative Argumentative Machine (CAM)2 , which a comparison system based on extracting and ranking arguments from the web. The authors have conducted a user study on 34 comparison topics, showing that the system is faster and more confident at finding arguments when answering comparative questions in contrast to a keyword-based search. Wachsmuth et al. (2017) pre"
2021.eacl-demos.36,P18-1013,0,0.013115,"nlp.github.io/coqas 302 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations, pages 302–311 April 19 - 23, 2021. ©2021 Association for Computational Linguistics quence to sequence model architecture (Sutskever et al., 2014). The existing generation methods are developed by employing attention mechanism (Bahdanau et al., 2015) and pointer-generator network (See et al., 2017). More recent work on text generation focus on generating natural language using multitask learning from multi-document or multi-passage sources (Hsu et al., 2018; Nishida et al., 2019). However, in our generation task, we have a list of arguments used to build the final answer. This makes our task similar to unsupervised summarization. There exist several approaches for tackling the latter task, e.g. graph-based (Litvak and Last, 2008) and neural models (Isonuma et al., 2019; Coavoux et al., 2019). A common approach to summarization is based on the TextRank graph algorithm (Mihalcea, 2004; Fan and Fang, 2017). Comparative QA According to Li and Roth (2006), questions can be divided into 6 coarse and 50 fine-grained categories, such as factoid question"
2021.eacl-demos.36,P19-1206,0,0.0144812,"g generation methods are developed by employing attention mechanism (Bahdanau et al., 2015) and pointer-generator network (See et al., 2017). More recent work on text generation focus on generating natural language using multitask learning from multi-document or multi-passage sources (Hsu et al., 2018; Nishida et al., 2019). However, in our generation task, we have a list of arguments used to build the final answer. This makes our task similar to unsupervised summarization. There exist several approaches for tackling the latter task, e.g. graph-based (Litvak and Last, 2008) and neural models (Isonuma et al., 2019; Coavoux et al., 2019). A common approach to summarization is based on the TextRank graph algorithm (Mihalcea, 2004; Fan and Fang, 2017). Comparative QA According to Li and Roth (2006), questions can be divided into 6 coarse and 50 fine-grained categories, such as factoid questions, list questions, or definition questions: we focus on comparative questions. Sun et al. (2006) proposed one of the first works on automatic comparative web search, where each object was submitted as a separate query, then obtained results were compared. Opinion mining of comparative sentences is discussed by Ganapa"
2021.eacl-demos.36,kessler-kuhn-2014-corpus,0,0.0490592,"Missing"
2021.eacl-demos.36,W08-1404,0,0.0565599,"ture (Sutskever et al., 2014). The existing generation methods are developed by employing attention mechanism (Bahdanau et al., 2015) and pointer-generator network (See et al., 2017). More recent work on text generation focus on generating natural language using multitask learning from multi-document or multi-passage sources (Hsu et al., 2018; Nishida et al., 2019). However, in our generation task, we have a list of arguments used to build the final answer. This makes our task similar to unsupervised summarization. There exist several approaches for tackling the latter task, e.g. graph-based (Litvak and Last, 2008) and neural models (Isonuma et al., 2019; Coavoux et al., 2019). A common approach to summarization is based on the TextRank graph algorithm (Mihalcea, 2004; Fan and Fang, 2017). Comparative QA According to Li and Roth (2006), questions can be divided into 6 coarse and 50 fine-grained categories, such as factoid questions, list questions, or definition questions: we focus on comparative questions. Sun et al. (2006) proposed one of the first works on automatic comparative web search, where each object was submitted as a separate query, then obtained results were compared. Opinion mining of comp"
2021.eacl-demos.36,2021.ccl-1.108,0,0.0522553,"Missing"
2021.eacl-demos.36,P04-3020,0,0.0233581,"(See et al., 2017). More recent work on text generation focus on generating natural language using multitask learning from multi-document or multi-passage sources (Hsu et al., 2018; Nishida et al., 2019). However, in our generation task, we have a list of arguments used to build the final answer. This makes our task similar to unsupervised summarization. There exist several approaches for tackling the latter task, e.g. graph-based (Litvak and Last, 2008) and neural models (Isonuma et al., 2019; Coavoux et al., 2019). A common approach to summarization is based on the TextRank graph algorithm (Mihalcea, 2004; Fan and Fang, 2017). Comparative QA According to Li and Roth (2006), questions can be divided into 6 coarse and 50 fine-grained categories, such as factoid questions, list questions, or definition questions: we focus on comparative questions. Sun et al. (2006) proposed one of the first works on automatic comparative web search, where each object was submitted as a separate query, then obtained results were compared. Opinion mining of comparative sentences is discussed by Ganapathibhotla and Liu (2008) and Jindal and Liu (2006), yet with no connection to argumentation mining. Instead, compara"
2021.eacl-demos.36,P19-1220,0,0.0116277,"s 302 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations, pages 302–311 April 19 - 23, 2021. ©2021 Association for Computational Linguistics quence to sequence model architecture (Sutskever et al., 2014). The existing generation methods are developed by employing attention mechanism (Bahdanau et al., 2015) and pointer-generator network (See et al., 2017). More recent work on text generation focus on generating natural language using multitask learning from multi-document or multi-passage sources (Hsu et al., 2018; Nishida et al., 2019). However, in our generation task, we have a list of arguments used to build the final answer. This makes our task similar to unsupervised summarization. There exist several approaches for tackling the latter task, e.g. graph-based (Litvak and Last, 2008) and neural models (Isonuma et al., 2019; Coavoux et al., 2019). A common approach to summarization is based on the TextRank graph algorithm (Mihalcea, 2004; Fan and Fang, 2017). Comparative QA According to Li and Roth (2006), questions can be divided into 6 coarse and 50 fine-grained categories, such as factoid questions, list questions, or d"
2021.eacl-demos.36,W19-4516,1,0.918932,"y in order to get good results. Figure 2: The interface of the Comparative Question Answering System (CoQAS). with respect to an aspect specified by the user. First, using the Elasticsearch BM25, CAM retrieves sentences containing the two compared objects and the comparison aspect from the Common Crawl-based corpus featuring 14.3 billion sentences (Panchenko et al., 2018). Then, CAM classifies the sentences as comparative or not and identifies the “winner” of the two compared objects in the sentence context. Besides, it extracts aspects and predicates from the retrieved comparative sentences (Panchenko et al., 2019). Finally, CAM outputs a list of argumentative pro/con sentences and shows the “winner” of the comparison along with the comparison aspects. web interface in action. In the NLG module, we use several approaches to response generation: an information retrievalbased approach and an approach built upon pretrained language models. These techniques provide different answers: the first is more structured, and the second one is based on experience and opinions. Therefore, we allow a user to choose a generation model from different types: CAM, CTRL, and Snippets (cf. Figure 2). Finally, for integratio"
2021.eacl-demos.36,L18-1286,1,0.865385,"ions with Python OBJ very easy.The downside is, that you have to be careful when using it. If you’re not careful, you’ll end up writing code which will crash your computer if something goes wrong. You also need to know how to use libraries like numpy in order to get good results. Figure 2: The interface of the Comparative Question Answering System (CoQAS). with respect to an aspect specified by the user. First, using the Elasticsearch BM25, CAM retrieves sentences containing the two compared objects and the comparison aspect from the Common Crawl-based corpus featuring 14.3 billion sentences (Panchenko et al., 2018). Then, CAM classifies the sentences as comparative or not and identifies the “winner” of the two compared objects in the sentence context. Besides, it extracts aspects and predicates from the retrieved comparative sentences (Panchenko et al., 2019). Finally, CAM outputs a list of argumentative pro/con sentences and shows the “winner” of the comparison along with the comparison aspects. web interface in action. In the NLG module, we use several approaches to response generation: an information retrievalbased approach and an approach built upon pretrained language models. These techniques provi"
2021.eacl-demos.36,D14-1162,0,0.0862706,"Missing"
2021.eacl-demos.36,N18-1202,0,0.0466749,"e question components (objects, aspects, predicates, or none) is a sequence-labeling task, where the classifier should tag respective tokens in an input question. We test several common baselines starting with simple one-layer bidirectional LSTM described by (Arora et al., 2017) where the input is encoded with GloVe (Pennington et al., 2014) embeddings. For some further improvements, we add Conditional Random Field (Sutton and McCallum, 2012) Objects Aspects Predicates 0.925 0.829 0.654 0.631 0.582 0.685 0.563 0.487 0.475 0.328 0.894 0.869 0.825 0.766 0.730 to LSTM and use context-based ELMO (Peters et al., 2018) embeddings for token representations. We also experiment with Transformers (Vaswani et al., 2017) using a pre-trained BERT model (Devlin et al., 2019) and RoBERTa (Liu et al., 2019), which is its modification yielding better performance. For every classifier, during training, we tune hyperparameters by varying a batch size (from 16 to 100) and a learning rate (from 10−6 to 10−2 ). To find a proper converge of the training process, we apply two types of learning rate schedulers: Linear With Warmup and Slanted Triangular. For the model with the highest achieved F1 (RoBERTa), we employ stochasti"
2021.eacl-demos.36,P17-1099,0,0.0340107,"ext Generation Most of the current text natural language generation tasks (Duˇsek and Jurˇc´ıcˇ ek, 2016; Freitag and Roy, 2018) are based on se1 https://skoltech-nlp.github.io/coqas 302 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations, pages 302–311 April 19 - 23, 2021. ©2021 Association for Computational Linguistics quence to sequence model architecture (Sutskever et al., 2014). The existing generation methods are developed by employing attention mechanism (Bahdanau et al., 2015) and pointer-generator network (See et al., 2017). More recent work on text generation focus on generating natural language using multitask learning from multi-document or multi-passage sources (Hsu et al., 2018; Nishida et al., 2019). However, in our generation task, we have a list of arguments used to build the final answer. This makes our task similar to unsupervised summarization. There exist several approaches for tackling the latter task, e.g. graph-based (Litvak and Last, 2008) and neural models (Isonuma et al., 2019; Coavoux et al., 2019). A common approach to summarization is based on the TextRank graph algorithm (Mihalcea, 2004; Fa"
2021.eacl-demos.36,W17-5106,0,0.0201308,"apathibhotla and Liu (2008) and Jindal and Liu (2006), yet with no connection to argumentation mining. Instead, comparative information needs are partially satisfied by several kinds of industrial systems mentioned above. Schildw¨achter et al. (2019) proposed Comparative Argumentative Machine (CAM)2 , which a comparison system based on extracting and ranking arguments from the web. The authors have conducted a user study on 34 comparison topics, showing that the system is faster and more confident at finding arguments when answering comparative questions in contrast to a keyword-based search. Wachsmuth et al. (2017) presented args.me, a search engine for retrieving pro and con arguments given for a given controversial topic. The input to this system is not structured but rather a query in a free textual form. The Touch´e shared task on argument retrieval at CLEF (Bondarenko et al., 2020b, 2021) featured a related track. The task was to retrieve from a large web corpus documents answering comparative question queries like “What IDE is better for Java: NetBeans or Eclipse?”. Objects: - Python - MATLAB Python Pros: - ... - ... Cons: - ... CAM / args.me / Touché ... Aspects: - Deep Learning MATLAB Pros: - .."
2021.eacl-demos.36,D18-1259,0,0.0723619,"Missing"
2021.eacl-demos.36,P13-4001,1,0.8493,"Missing"
2021.eacl-main.145,C18-1139,0,0.0540508,"Missing"
2021.eacl-main.145,2020.acl-main.189,0,0.0730709,"Missing"
2021.eacl-main.145,Q16-1026,0,0.0187227,"chreiter and Schmidhuber, 1997). BiLSTM processes sequences in two passes: from left-to-right and from right-to-left producing a contextualized token vector in each pass. These vectors are concatenated and are used as features in a CRF layer that performs the final scoring of tags. We experiment with two versions of the BiLSTM-CRF model. The first one uses GloVe (Pennington et al., 2014) word embeddings pretrained on English Wikipedia and the 5-th edition of the Gigaword corpus, and a convolutional character encoder (Ma and Hovy, 2016), which helps to deal with out-of-vocabulary words. As in (Chiu and Nichols, 2016), the model additionally leverages the basic capitalization features, which has been shown to be useful for achieving good performance with this model. We will refer to it as CNN-BiLSTM-CRF. We consider this model as another baseline that does not exploit deep pretraining. The second version of the BiLSTM-CRF model uses pre-trained medium-size ELMo (Peters et al., 2018) to produce contextualized word representations. ELMo is a BiLSTM language model enhanced with a CNN character encoder. This model does not rely on feature-engineering at all. We will refer to it as ELMo-BiLSTM-CRF. 3.3 Transfor"
2021.eacl-main.145,N19-1423,0,0.238902,"s. The model predictions are used by the AL query strategy to sample the informative objects, which are then further demonstrated to the expert annotators. When the annotators provide labels for these objects, the next iteration begins. The collected data can be used for training a final successor model that is used in a target application. During AL, acquisition models have to be trained on very small amounts of the labeled data, especially during the early iterations. Recently, this problem has been tackled by transfer learning with deep pre-trained models: ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), ELECTRA (Clark et al., 2020), and others. Pre-trained on a large amount of unlabeled data, they are capable of demonstrating remarkable performance when only hundreds or even dozens of labeled training instances are available. This trait suits the AL framework but poses the question about the usefulness of the biased sampling provided by the AL query strategies. In this work, we investigate AL with the aforementioned deep pre-trained models and compare the results of this combination to the outcome of 1698 Proceedings of the 16th Conference of the European Chapter of the Association for Comp"
2021.eacl-main.145,2020.emnlp-main.638,0,0.0428992,"models. Other notable works on deep active learning include (Erdmann et al., 2019), which proposes an AL algorithm based on a bootstrapping approach (Jones et al., 1999) and (Lowell et al., 2019), which concerns the problem of the mismatch between a model used to construct a training dataset via AL (acquisition model) and a final model that is trained on it (successor model). Deep pre-trained models are evaluated in the AL setting for NER by Shelmanov et al. (2019). However, they perform the evaluation only on the specific biomedical datasets and do not consider the Bayesian query strategies. Ein-Dor et al. (2020) conduct an empirical study of AL with pre-trained BERT but only on the text classification task. Brantley et al. (2020) use pre-trained BERT in experiments with NER, but they do not fine-tune it, which results in suboptimal performance. In this work, we try to fill the gap by evaluating deep pre-trained models: ELMo and various Transformers, in the AL setting with practical query strategies, including Bayesian, and on the widely-used benchmarks in this area. 3 Sequence Tagging Models We use a tagger based on the Conditional Random Field model (Lafferty et al., 2001), two BiLSTM1699 CRF tagger"
2021.eacl-main.145,N19-1231,0,0.0602417,"Missing"
2021.eacl-main.145,D17-1063,0,0.0475075,"Missing"
2021.eacl-main.145,P82-1020,0,0.718203,"Missing"
2021.eacl-main.145,N18-1202,0,0.192829,"e pool of unannotated objects. The model predictions are used by the AL query strategy to sample the informative objects, which are then further demonstrated to the expert annotators. When the annotators provide labels for these objects, the next iteration begins. The collected data can be used for training a final successor model that is used in a target application. During AL, acquisition models have to be trained on very small amounts of the labeled data, especially during the early iterations. Recently, this problem has been tackled by transfer learning with deep pre-trained models: ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), ELECTRA (Clark et al., 2020), and others. Pre-trained on a large amount of unlabeled data, they are capable of demonstrating remarkable performance when only hundreds or even dozens of labeled training instances are available. This trait suits the AL framework but poses the question about the usefulness of the biased sampling provided by the AL query strategies. In this work, we investigate AL with the aforementioned deep pre-trained models and compare the results of this combination to the outcome of 1698 Proceedings of the 16th Conference of the European Chapter"
2021.eacl-main.145,W13-3516,0,0.0401604,"Missing"
2021.eacl-main.145,P19-1401,0,0.020149,"ulating the uncertainty estimates: Bayes-by-Backprop (Blundell et al., 2015) and the MC dropout (Gal and Ghahramani, 2016a). The experiments show that the variation ratio (Freeman, 1965) has substantial improvements over MNLP. In contrast to them, we additionally experiment with the Bayesian active learning by disagreement (BALD) query strategy proposed by Houlsby et al. (2011) and perform a comparison with variation ratio. There is a series of works that tackle AL with a trainable policy model that serves as a query strategy. For this purpose, imitation learning is used in Liu et al. (2018); Vu et al. (2019); Brantley et al. (2020), while in (Fang et al., 2017), the authors use deep reinforcement learning. Although the proposed solutions are shown to outperform other heuristic algorithms with comparably weak models (basic CRF or BERT without fine-tuning) in experiments with a small number of AL iterations, they can be not very practical due to the high computational costs of collecting training data for policy models. Other notable works on deep active learning include (Erdmann et al., 2019), which proposes an AL algorithm based on a bootstrapping approach (Jones et al., 1999) and (Lowell et al.,"
2021.eacl-main.157,2020.emnlp-main.667,0,0.0330043,"he significant margin in the binary misclassification detection task. We also demonstrate that replacing the last dropout layer with the DPP dropout can yield significant improvements over the baseline in some cases, but less than the usage of the MC dropout on all dropout layers. Despite being inferior compared to the latter, the DPP dropout can provide a better trade-off between computation time and performance of error detection, which can be important for practical use cases. In future work, we are seeking to improve UEs quality obtained using the DPP dropout with the help of calibration (Safavi et al., 2020) and conduct experiments on sequence tagging tasks. Acknowledgments We thank the reviewers for their valuable feedback. The development of uncertainty estimation algorithms for Transformer models (Section 3) was supported by the joint MTS-Skoltech lab. The development of a software system for the experimental study of uncertainty estimation methods and its application to NLP tasks (Section 4) was supported by the Russian Science Foundation grant 20-7110135. The Zhores supercomputer (Zacharov et al., 2019) was used for computations. 1837 References Ali C ¸ ivril and Malik Magdon-Ismail. 2009. O"
2021.eacl-main.157,2021.eacl-main.145,1,0.814305,"Missing"
2021.eacl-main.157,D18-1318,0,0.0255534,"h ∼ Bernoulli(1−p), where p ∈ [0; 1] is the dropout rate. This theoretical explanation enables the use of the dropout not only at the training stage but also at the inference stage via sampling of multiple (t) masks Mh , t = 1, . . . , T for each dropout layer of the network h and subsequently providing an ensemble of models parameterized by these masks:  (t)  ft (x) = f x |Mh . The obtained UEs are relatively fast, convenient, and applicable to various tasks, such as regression (Tsymbalov et al., 2018), image classification (Gal and Ghahramani, 2016), and active learning (Gal et al., 2017; Siddhant and Lipton, 2018). Monte Carlo Dropout with Determinantal Point Processes The models obtained from the standard dropout masks usually show a high degree of correlation in predictions between them, limiting the power of the resulting ensemble. Recently, it was proposed to improve the diversity of predictions by considering the correlations between neurons and sampling the diverse neurons via the mechanism of Determinantal Point Processes (DPP; Kulesza and Taskar (2012)), an approach for sampling diverse elements from a set of points. This setup was proposed by (Tsymbalov et al., 2020) and evaluated for the simp"
2021.eacl-main.157,D13-1170,0,0.00770894,"Missing"
2021.eacl-main.157,Q19-1040,0,0.0609682,"Missing"
2021.eacl-main.157,2020.emnlp-demos.6,0,0.0961796,"Missing"
2021.eacl-main.157,N19-1316,0,0.0255929,"he European Chapter of the Association for Computational Linguistics, pages 1833–1840 April 19 - 23, 2021. ©2021 Association for Computational Linguistics between models’ predictions are interpreted as a sample variance (Lakshminarayanan et al., 2017); (iii) Bayesian neural networks (Teye et al., 2018), which have a built-in mechanism to capture uncertainty via a single model. There are a few recent works that investigate uncertainty quantification for NLP models and use MC dropout techniques. Dong et al. (2018) use Bayesian UEs for the analysis of semantic parser predictions for correctness. Zhang et al. (2019) propose an additional training loss component that facilitates smaller inter-class and bigger intra-class distances in the vector space of the output layer. Experiments with convolutional NNs on text classification datasets show that this modification helps to improve error detection using MC dropout UEs. For quantifying model data uncertainty, Xiao and Wang (2019) use NNs to parameterize a probability distribution (mean and variance) instead of making a prediction directly. For quantifying model uncertainty, the authors leverage the MC dropout. Modeling both types of uncertainties in convolu"
2021.emnlp-main.629,2021.bsnlp-1.4,1,0.660642,"uld be used for suggesting detoxifying edits rather than performing them automaticallly. At the same time, detoxification models can make chatbots safer by detoxifying (if necessary) their answers before sending them to users. An automatically generated toxic comment by a neural chatbot may be the result of pre-training on the biased textual data – a problem which is currently unsolved completely (Gehman et al., 2020). Therefore, a detoxification of automatically generated content might be a valid use-case for minimizing reputational losses for the company created such an unmoderated chatbot (Babakov et al., 2021). Toxification of Texts Detoxification task implies the possibility to perform the opposite transformation, i.e. to rewrite a neutral text into a toxic one. Various style transfer models, including ours, could in principle be used to complete this task. However, in case of CondBERT, the quality of such transformation would be bad, and it would be almost impossible to pass the results of this “toxification” off as real toxic sentences. The reason for that is the structure of toxic data. One of the main properties of toxic style is the Acknowledgements presence of lexical markers of this style ("
2021.emnlp-main.629,N19-1423,0,0.0251514,"substitutes using multitoken BERT live in (3) rerank the substitutes by similarity and toxicity disagree with Figure 2: The overview of the CondBERT model. For each word in a sentence, we compute the toxicity score and then define toxic words as the words with the score above a threshold t = max(tmin , max(s1 , s2 , ..., sn )/2), where s1 , s2 , ..., sn are scores of all words in a sentence and tmin = 0.2 is a minimum toxicity score. This adaptive threshold allows balancing the percentage of toxic words in a sentence so that we avoid cases when too many or no words are marked as toxic. BERT (Devlin et al., 2019) has been trained on To preserve the meaning of the replaced word, the task of filling in gaps (“masked LM”), we can we employ the content preservation heuristics suguse it to insert non-toxic words instead of the toxic gested by Arefyev et al. (2020): (i) Preserve the ones. This approach has been suggested by Wu et al. (2019a) as a method of data augmentation. original tokens instead of masking them before the replacement; (ii) Rerank the replacement words The authors identify words belonging to the source suggested by BERT by the similarity of their emstyle, replace them with the [MASK] toke"
2021.emnlp-main.629,2020.trac-1.4,0,0.0793668,"Missing"
2021.emnlp-main.629,2020.findings-emnlp.301,0,0.0263645,"is policy by rewriting toxic messages instead of removing them altogether. Last but not least, we suggest that user messages should not be modified without user consent. The detoxificaiton models should be used for suggesting detoxifying edits rather than performing them automaticallly. At the same time, detoxification models can make chatbots safer by detoxifying (if necessary) their answers before sending them to users. An automatically generated toxic comment by a neural chatbot may be the result of pre-training on the biased textual data – a problem which is currently unsolved completely (Gehman et al., 2020). Therefore, a detoxification of automatically generated content might be a valid use-case for minimizing reputational losses for the company created such an unmoderated chatbot (Babakov et al., 2021). Toxification of Texts Detoxification task implies the possibility to perform the opposite transformation, i.e. to rewrite a neutral text into a toxic one. Various style transfer models, including ours, could in principle be used to complete this task. However, in case of CondBERT, the quality of such transformation would be bad, and it would be almost impossible to pass the results of this “toxi"
2021.emnlp-main.629,2020.emnlp-main.622,0,0.0342604,"ed any class-conditional pre-training to successfully change the text style from toxic to normal. In addition, we perform a large-scale evaluation of style transfer models on detoxification task, comparing our new models with baselines and state-ofthe-art approaches. We release our code and data.3 Our contributions are as follows: • We propose two novel detoxification methods based on pre-trained neural language models: ParaGeDi (paraphrasing GeDi) and CondBERT (conditional BERT). Identification of toxicity in user texts is an active area of research (Zampieri et al., 2020; D’Sa et al., 2020; Han and Tsvetkov, 2020). The task of automatic rewriting of offensive content attracted less attention, yet it may find various useful applications such as making online world a better place by suggesting to a user posting a more neutral version of an emotional comment. The existing works on text detoxification (dos Santos et al., 2018; Tran et al., 2020; Laugier et al., 2021) cast this task as style transfer. The style transfer task is generally understood as rewriting of text with the same content and with altering of one or several attributes which con1 stitute the “style”, such as authorship (Voigt et al., For e"
2021.emnlp-main.629,2021.ccl-1.108,0,0.0757712,"Missing"
2021.emnlp-main.629,2020.acl-main.169,0,0.0229097,"al comment. The existing works on text detoxification (dos Santos et al., 2018; Tran et al., 2020; Laugier et al., 2021) cast this task as style transfer. The style transfer task is generally understood as rewriting of text with the same content and with altering of one or several attributes which con1 stitute the “style”, such as authorship (Voigt et al., For example, Lample et al. (2019) provide the following sentence as an example of transfer from male to female writing: 2018), sentiment (Shen et al., 2017), or degree of Gotta say that beard makes you look like a Viking → Gotta politeness (Madaan et al., 2020). Despite the goal say that hair makes you look like a Mermaid. 2 of preserving the content, in many cases changing A formal task definition is presented in Appendix A. 3 the style attributes changes the meaning of a senhttps://github.com/skoltech-nlp/detox 7979 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7979–7996 c November 7–11, 2021. 2021 Association for Computational Linguistics • We conduct an evaluation of these models and their comparison with a number of state-of-theart models for text detoxification and sentiment transfer and release"
2021.emnlp-main.629,2020.emnlp-main.699,0,0.0398124,"Missing"
2021.emnlp-main.629,D19-5614,0,0.0200631,"e 10,000 sentences with the highest toxicity score according to our classifier. 5.3 Metrics racy (ACC) is measured with a pre-trained toxicity classifier described in Section 5.1. Content preservation (SIM) is evaluated as the similarity of sentence-level embeddings of the original and transformed texts computed by the model of Wieting et al. (2019). Fluency (FL) measured with the classifier of linguistic acceptability trained on the CoLA dataset (Warstadt et al., 2019). J is computed as the average of their sentence-level product. In addition to that, we tried a similar aggregated metric GM (Pang and Gimpel, 2019; Laugier et al., 2021) which uses perplexity as the measure of fluency and employs a different aggregation method. Our preliminary experiments showed that J and GM are strongly correlated, so we keep only J for further evaluation. 5.4 Implementation Details For ParaGeDi, we use a pre-trained T5-based (Raffel et al., 2020) paraphraser,4 fine-tuned on a random subsample of the ParaNMT dataset (Wieting and Gimpel, 2018). As a discriminator, we finetune the gpt2-medium model (Radford et al., 2019) on the training part of the Jigsaw-1 dataset using two control codes for toxic and polite texts. Bef"
2021.emnlp-main.629,2020.coling-main.518,0,0.0107619,"hods outperform other other models in terms of J. As before, the other state-of-the-art style transfer models on the tasks models fail to generate fluent texts because they re- of detoxification and sentiment transfer. 7987 Ethical Statement Toxicity is a sensitive topic where the unexpected results and byproducts of research can cause harm. Therefore, we would like to consider some ethical concerns related to our work. On Definition of Toxicity Toxicity is an umbrella term for almost any undesirable behaviour on the Internet. It ranges from “mild” phenomena like condescending language (Perez Almendros et al., 2020) to grave insults or oppression based on racial or other social-demographic characteristics. While annotators agree when labelling serious cases of toxicity such as hate speech (Fortuna and Nunes, 2018), the labelling of less severe toxicity is subjective and depends on the annotator’s background (Al Kuwatly et al., 2020). This can cause the underestimation of certain types of toxicity. To define the toxicity in the most objective feasible way, we adopt a data-driven approach as presented in detail formally in Appendix A. Both models we propose recognise toxicity based on a toxicitylabelled da"
2021.emnlp-main.629,P18-1080,0,0.0141875,"ply the classifier from section 5.1 to select the least toxic candidate from the 10 resulting paraphrases. 5.5 Competing Methods We compare our models with state-of-the-art methods described in Section 2: DRG-TemplateBased, DRG-RetrieveOnly, Mask&Infill, DLSM, STRAP, and SST. We also implement three other baselines: Machine Translation, Detoxifying GPT-2, and Paraphraser. We do not directly compare our models with GeDi, because it is a language model and was not explicitly trained to transform texts. Machine Translation There is evidence that automatic translation tends to eliminate toxicity (Prabhumoye et al., 2018). Thus, we use a chain There is no parallel test set available for the detoxification task, so we cannot use BLEU, METEOR or ROUGE metrics and resort to referenceless evaluation. Style transfer models need to change the style, preserve content and produce a fluent text. These parameters are often inversely correlated, so we need a compound metric to find a balance between them. We follow the evaluation strategy of Krishna et al. (2020) and use the metric J, which is the multiplication of sentence-level style accuracy, content 4 preservation, and fluency. The system-level J is https://huggingfa"
2021.emnlp-main.629,2020.coling-main.190,0,0.120032,"ropose two novel detoxification methods based on pre-trained neural language models: ParaGeDi (paraphrasing GeDi) and CondBERT (conditional BERT). Identification of toxicity in user texts is an active area of research (Zampieri et al., 2020; D’Sa et al., 2020; Han and Tsvetkov, 2020). The task of automatic rewriting of offensive content attracted less attention, yet it may find various useful applications such as making online world a better place by suggesting to a user posting a more neutral version of an emotional comment. The existing works on text detoxification (dos Santos et al., 2018; Tran et al., 2020; Laugier et al., 2021) cast this task as style transfer. The style transfer task is generally understood as rewriting of text with the same content and with altering of one or several attributes which con1 stitute the “style”, such as authorship (Voigt et al., For example, Lample et al. (2019) provide the following sentence as an example of transfer from male to female writing: 2018), sentiment (Shen et al., 2017), or degree of Gotta say that beard makes you look like a Viking → Gotta politeness (Madaan et al., 2020). Despite the goal say that hair makes you look like a Mermaid. 2 of preservi"
2021.emnlp-main.629,Q19-1040,0,0.0236804,"f non-toxic sentences from the sentence-separated Jigsaw data. The test set is prepared analogously to the test set of the Jigsaw competition: we use 10,000 sentences with the highest toxicity score according to our classifier. 5.3 Metrics racy (ACC) is measured with a pre-trained toxicity classifier described in Section 5.1. Content preservation (SIM) is evaluated as the similarity of sentence-level embeddings of the original and transformed texts computed by the model of Wieting et al. (2019). Fluency (FL) measured with the classifier of linguistic acceptability trained on the CoLA dataset (Warstadt et al., 2019). J is computed as the average of their sentence-level product. In addition to that, we tried a similar aggregated metric GM (Pang and Gimpel, 2019; Laugier et al., 2021) which uses perplexity as the measure of fluency and employs a different aggregation method. Our preliminary experiments showed that J and GM are strongly correlated, so we keep only J for further evaluation. 5.4 Implementation Details For ParaGeDi, we use a pre-trained T5-based (Raffel et al., 2020) paraphraser,4 fine-tuned on a random subsample of the ParaNMT dataset (Wieting and Gimpel, 2018). As a discriminator, we finetun"
2021.emnlp-main.629,P19-1427,0,0.0243776,"rt of the dataset (we find 154,771 of them). To select the neutral part of the dataset, we randomly pick the same number of non-toxic sentences from the sentence-separated Jigsaw data. The test set is prepared analogously to the test set of the Jigsaw competition: we use 10,000 sentences with the highest toxicity score according to our classifier. 5.3 Metrics racy (ACC) is measured with a pre-trained toxicity classifier described in Section 5.1. Content preservation (SIM) is evaluated as the similarity of sentence-level embeddings of the original and transformed texts computed by the model of Wieting et al. (2019). Fluency (FL) measured with the classifier of linguistic acceptability trained on the CoLA dataset (Warstadt et al., 2019). J is computed as the average of their sentence-level product. In addition to that, we tried a similar aggregated metric GM (Pang and Gimpel, 2019; Laugier et al., 2021) which uses perplexity as the measure of fluency and employs a different aggregation method. Our preliminary experiments showed that J and GM are strongly correlated, so we keep only J for further evaluation. 5.4 Implementation Details For ParaGeDi, we use a pre-trained T5-based (Raffel et al., 2020) parap"
2021.emnlp-main.629,P18-1042,0,0.35976,"ndix A. 3 the style attributes changes the meaning of a senhttps://github.com/skoltech-nlp/detox 7979 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7979–7996 c November 7–11, 2021. 2021 Association for Computational Linguistics • We conduct an evaluation of these models and their comparison with a number of state-of-theart models for text detoxification and sentiment transfer and release the detoxification dataset. • We create an English parallel corpus for the detoxification task by retrieving toxic/safe sentence pairs from the ParaNMT dataset (Wieting and Gimpel, 2018). We show that it can further improve our best-performing models. 2 Related Work One of the most straightforward ways of solving style transfer task is to “translate” a source sentence into the target style using a supervised encoderdecoder model (Rao and Tetreault, 2018). Since the source and the target are in the same language, pretrained LMs such as GPT-2 (Radford et al., 2019) can be applied for this task — fine-tuning them on relatively small parallel corpora gives a good result (Wang et al., 2019). However, this method is used quite rarely because of the lack of sufficiently large parall"
2021.findings-acl.146,Q16-1028,0,0.0180807,"text embedding model based on spherical geometry. Tensor decomposition models have been applied to many tasks in the NLP. Particularly, Van de Cruys et al. (2013) proposed the Tucker model for decomposing subject-verb-object co-occurance tensor for computation of compositionality. The most similar to our task is the word embedding problem. In this direction, Sharan and Valiant (2017) explored the Canonical Polyadic Decomposition (CPD) of word triplet tensor. Bailey and Aeron (2017) used symmetric CPD of pointwise mutual information tensor. Frandsen and Ge (2019) extended the RAND-WALK model (Arora et al., 2016) to word triplets. The main drawback of existing approaches is that they can not preserve word order information of long n-grams properly. For example, in the case of the CPD, we need to use separate parameters for each word based on its position in the text. This restriction does not allow us to efficiently use the linguistic meaning of tensor modes. The symmetric CPD completely loses word order information and the Tucker model suffers from exponentially increasing parameter size in the case of long length n-grams. Our approach eliminates these disadvantages. 3 Problem and model description I"
2021.findings-acl.146,W17-2621,0,0.0118595,"this method goes from realization of the Frege’s principle of compositionality through order-preserving property of matrix operations. As shown by Rudolph and Giesbrecht (2010), this type of models can internally combine various properties from statistical and symbolic models of natural language and therefore it is more flexible than vector space models. In spite of that fact, such models are usually hard to optimize on the real data. To this end, Yessenalina and Cardie (2011) took attention to the needs of nontrivial initialization and proposed to learn the weights by the bag-of-words model. Asaadi and Rudolph (2017) used complex multi-stage initialization based on unigrams and bigrams scoring. Both approaches try to solve the sentiment analysis task. Recently Mai et al. (2019) considered the problem of self-supervised continuous representation of words via matrix-space models. They optimized a modified word2vec objective function (Mikolov et al., 2013) and proposed a novel initialization by adding small isotropic Gaussian noise to the identity matrix. In this paper, we use a similarity function between matrices similar to Mai et al. (2019), but instead of neural network type of learning, we implement the"
2021.findings-acl.146,D14-1179,0,0.0182539,"Missing"
2021.findings-acl.146,N13-1134,0,0.0348811,"Missing"
2021.findings-acl.146,N19-1423,0,0.00568773,"he fairness of comparison we use fixed embedding dimension, number of negative samples, and number of epochs for all models including ours. These values try to mimic the usual values of these hyperparameters in practice. We compare our model not only with vector models but also with neural network models. We use 5.5B ELMo (Peters et al., 2018) version which is pre-trained on Wikipedia (1.9B) and all of the monolingual news crawl data from WMT 20082012 (3.6B). ELMo embedding dimension is equal to 1024. Also we use 768-dimensional embedding vectors from BERT model ”bert-base-uncased”. Following Devlin et al. (2019) we take the last layer hidden state corresponding to the [CLS] token as the aggregate document representation. If length of document is bigger than 512 we cut document on 512-length parts and average representation of this parts. Finally, we add Sentence BERT (Reimers and Gurevych, 2019) to the baseline models. This model is fine-tuned on SNLI and MultiNLI datasets for sentence embedding generation. We use 768dimensional embedding vectors from model ”bertbase-nli-mean-tokens”. We do not perform fine-tuning of BERT and ELMo models for our datasets, because in our experiments it doesn’t give an"
2021.findings-acl.146,W18-1708,0,0.0211621,"idean embedding models (Mikolov et al., 2013; Pennington et al., 2014) based on implicit word-context co-occurance matrix factorization (Levy and Goldberg, 2014) are an important framework for current NLP tasks. Proposed models achieve relatively high performance in various NLP tasks like text classification (Kim, 2014), named entity recognition (Lample et al., 2016), machine translation (Cho et al., 2014). Riemannian embedding models have shown promising results by expanding embedding methods beyond Euclidean geometry. There are several models with negative sectional curvature like Poincare (Dhingra et al., 2018; Nickel and Kiela, 2017) and Lorentz models (Nickel and Kiela, 2018). Furthermore, Meng et al. (2019) proposed a text embedding model based on spherical geometry. Tensor decomposition models have been applied to many tasks in the NLP. Particularly, Van de Cruys et al. (2013) proposed the Tucker model for decomposing subject-verb-object co-occurance tensor for computation of compositionality. The most similar to our task is the word embedding problem. In this direction, Sharan and Valiant (2017) explored the Canonical Polyadic Decomposition (CPD) of word triplet tensor. Bailey and Aeron (2017)"
2021.findings-acl.146,P18-1128,0,0.0126681,"age length. This implies that statistics of long n-grams differ between these datasets too and in the case of the ArXiv dataset statistics of n-grams are significantly better converged than in the case of 20 newsgroups. This fact allow us to hypothesize that matrix-space models should less overfit on the ArXiv dataset. We fix the document embeddings and optimize multinomial logistic regression with SAGA optimizer and l2 -norm regularization. Instead of test set we use nested 10-fold cross-validation to estimate statistical significance using Wilcoxon signedrank test (Japkowicz and Shah, 2011; Dror et al., 2018). For each fold we estimate the hyperparameter of l2 -regularization on 10 point logarithmic grid from 0.01 to 100 by using additional 10-fold cross-validation with macro-averaged F1 score. For text preprocessing, we use CountVectorizer from the Scikit-learn package. Additionally we remove words which occur in the NLTK stopwords list or 1679 occur only in 1 document. In the case of ArXiv dataset we use half of this dataset and use only documents in the range from 1000 to 5000 words (smaller documents are removed and bigger documents are reduced to the first 5000 words). Dataset 20 Newsgroups A"
2021.findings-acl.146,D14-1181,0,0.00294562,"nt in comparison with neural network models. • Our model achieves state-of-the-art performance on the task of representation learning for multiclass classification both on short and long document datasets. Our implementation of the proposed model is available online1 . 2 Related work Euclidean embedding models (Mikolov et al., 2013; Pennington et al., 2014) based on implicit word-context co-occurance matrix factorization (Levy and Goldberg, 2014) are an important framework for current NLP tasks. Proposed models achieve relatively high performance in various NLP tasks like text classification (Kim, 2014), named entity recognition (Lample et al., 2016), machine translation (Cho et al., 2014). Riemannian embedding models have shown promising results by expanding embedding methods beyond Euclidean geometry. There are several models with negative sectional curvature like Poincare (Dhingra et al., 2018; Nickel and Kiela, 2017) and Lorentz models (Nickel and Kiela, 2018). Furthermore, Meng et al. (2019) proposed a text embedding model based on spherical geometry. Tensor decomposition models have been applied to many tasks in the NLP. Particularly, Van de Cruys et al. (2013) proposed the Tucker mode"
2021.findings-acl.146,N16-1030,0,0.0152204,"odels. • Our model achieves state-of-the-art performance on the task of representation learning for multiclass classification both on short and long document datasets. Our implementation of the proposed model is available online1 . 2 Related work Euclidean embedding models (Mikolov et al., 2013; Pennington et al., 2014) based on implicit word-context co-occurance matrix factorization (Levy and Goldberg, 2014) are an important framework for current NLP tasks. Proposed models achieve relatively high performance in various NLP tasks like text classification (Kim, 2014), named entity recognition (Lample et al., 2016), machine translation (Cho et al., 2014). Riemannian embedding models have shown promising results by expanding embedding methods beyond Euclidean geometry. There are several models with negative sectional curvature like Poincare (Dhingra et al., 2018; Nickel and Kiela, 2017) and Lorentz models (Nickel and Kiela, 2018). Furthermore, Meng et al. (2019) proposed a text embedding model based on spherical geometry. Tensor decomposition models have been applied to many tasks in the NLP. Particularly, Van de Cruys et al. (2013) proposed the Tucker model for decomposing subject-verb-object co-occuran"
2021.findings-acl.146,W16-1609,0,0.020394,"7)), Doc2vecC (Chen, 2017), sent2vec (Pagliardini et al., 2018) and recently proposed JoSe (Meng et al., 2019). The comparison with the last two of these models seems to be more informative than with others because of some similarity of these models to our model (sent2vec can use n-gram information and JoSe also based on the spherical type of embedding geometry). Due to the large number of possible values of hyperparameters for each model, we used the default values proposed by the authors of these models or proposed in subsequent studies of these models like in the case of paragraph vectors (Lau and Baldwin, 2016). We modify only the min count to 1 and window size to be equal to the number of negative samples for paragraph2vec and word2vec models because it gives better results for these methods in our experiments. We choose n-grams number equal to 1 for sent2vec, because other values doesn’t improve results. To preserve the fairness of comparison we use fixed embedding dimension, number of negative samples, and number of epochs for all models including ours. These values try to mimic the usual values of these hyperparameters in practice. We compare our model not only with vector models but also with n"
2021.findings-acl.146,D18-1405,0,0.0155843,"s becomes rotation. 3.5 Noise-contrastive estimation In practice we do not need to construct set of ten¯ (n) }N explicitly. Instead, since each X ¯ (n) sors {X n=1 represents a higher-order frequency table, we can 1677 optimize the sum of MLE tasks: (1) (N ) Ew,d∼X xwd ) + · · · + Ew,d∼X xwd ). ¯ (1) log(ˆ ¯ (N ) log(ˆ Usually, we have huge amount of data which make problem of computing partition function for each (n) x ˆwd intractable for many current computing architectures. We can avoid this problem by using noise-contrastive estimation (Gutmann and Hyv¨arinen, 2012) for conditional model (Ma and Collins, 2018). Similar to Chen et al. (2017), we construct negative samples from our batch by connecting non-linked n-grams and documents. Finally, for parameter set |W| |D| Θ = {Uw }w=1 ∪ {Vd }d=1 ∪ {κ(n) }N n=1 , Stiefel manifold we can swap two columns for each parameter matrix if the determinant of the this matrix is -1. However, this initialization can be below optimal way, because these rotation matrices can be far away from each other and due to the non-trivial structure of the loss function on this manifold we can stuck in local minima. To overcome this problem, we can fix particular point on the m"
2021.findings-acl.146,N18-1049,0,0.0199084,"ocuments are removed and bigger documents are reduced to the first 5000 words). Dataset 20 Newsgroups ArXiv #cls 20 6 |W| 75752 251108 |D| 18846 16371 #w 180 3829 Table 3: Datasets considered in the paper, where #cls - number of classes, #w - average number of words in documents, |W |- vocabulary size, |D |- number of documents. Baselines. We compare our model with different vector space models: paragraph vectors (Le and Mikolov, 2014), weighted combinations of the word2vec skipgram vectors (Mikolov et al., 2013) (average, TF-IDF and SIF (Arora et al., 2017)), Doc2vecC (Chen, 2017), sent2vec (Pagliardini et al., 2018) and recently proposed JoSe (Meng et al., 2019). The comparison with the last two of these models seems to be more informative than with others because of some similarity of these models to our model (sent2vec can use n-gram information and JoSe also based on the spherical type of embedding geometry). Due to the large number of possible values of hyperparameters for each model, we used the default values proposed by the authors of these models or proposed in subsequent studies of these models like in the case of paragraph vectors (Lau and Baldwin, 2016). We modify only the min count to 1 and w"
2021.findings-acl.146,D14-1162,0,0.0867928,"earning method based on the Riemannian geometry of matrix groups. • We show that our approach to model the compositionality and word order allows us to increase the quality of document embedding on downstream tasks. Moreover, it is also more computationally efficient in comparison with neural network models. • Our model achieves state-of-the-art performance on the task of representation learning for multiclass classification both on short and long document datasets. Our implementation of the proposed model is available online1 . 2 Related work Euclidean embedding models (Mikolov et al., 2013; Pennington et al., 2014) based on implicit word-context co-occurance matrix factorization (Levy and Goldberg, 2014) are an important framework for current NLP tasks. Proposed models achieve relatively high performance in various NLP tasks like text classification (Kim, 2014), named entity recognition (Lample et al., 2016), machine translation (Cho et al., 2014). Riemannian embedding models have shown promising results by expanding embedding methods beyond Euclidean geometry. There are several models with negative sectional curvature like Poincare (Dhingra et al., 2018; Nickel and Kiela, 2017) and Lorentz models (Nick"
2021.findings-acl.146,N18-1202,0,0.0396387,"window size to be equal to the number of negative samples for paragraph2vec and word2vec models because it gives better results for these methods in our experiments. We choose n-grams number equal to 1 for sent2vec, because other values doesn’t improve results. To preserve the fairness of comparison we use fixed embedding dimension, number of negative samples, and number of epochs for all models including ours. These values try to mimic the usual values of these hyperparameters in practice. We compare our model not only with vector models but also with neural network models. We use 5.5B ELMo (Peters et al., 2018) version which is pre-trained on Wikipedia (1.9B) and all of the monolingual news crawl data from WMT 20082012 (3.6B). ELMo embedding dimension is equal to 1024. Also we use 768-dimensional embedding vectors from BERT model ”bert-base-uncased”. Following Devlin et al. (2019) we take the last layer hidden state corresponding to the [CLS] token as the aggregate document representation. If length of document is bigger than 512 we cut document on 512-length parts and average representation of this parts. Finally, we add Sentence BERT (Reimers and Gurevych, 2019) to the baseline models. This model"
2021.findings-acl.146,D19-1410,0,0.0203469,"h neural network models. We use 5.5B ELMo (Peters et al., 2018) version which is pre-trained on Wikipedia (1.9B) and all of the monolingual news crawl data from WMT 20082012 (3.6B). ELMo embedding dimension is equal to 1024. Also we use 768-dimensional embedding vectors from BERT model ”bert-base-uncased”. Following Devlin et al. (2019) we take the last layer hidden state corresponding to the [CLS] token as the aggregate document representation. If length of document is bigger than 512 we cut document on 512-length parts and average representation of this parts. Finally, we add Sentence BERT (Reimers and Gurevych, 2019) to the baseline models. This model is fine-tuned on SNLI and MultiNLI datasets for sentence embedding generation. We use 768dimensional embedding vectors from model ”bertbase-nli-mean-tokens”. We do not perform fine-tuning of BERT and ELMo models for our datasets, because in our experiments it doesn’t give any positive effect on the final performance of these models. However, this is not true for Sentence BERT. Fine-tuning slightly improve performance of this model on the 20 newsgroup (50 epoch with maximum margin triplet loss). We should notice that this experimental design gives some benefi"
2021.findings-acl.146,D11-1016,0,0.039508,"cularly, we investigate the matrix-space model of language, in which semantic space consists of square matrices of real values. The key idea behind this method goes from realization of the Frege’s principle of compositionality through order-preserving property of matrix operations. As shown by Rudolph and Giesbrecht (2010), this type of models can internally combine various properties from statistical and symbolic models of natural language and therefore it is more flexible than vector space models. In spite of that fact, such models are usually hard to optimize on the real data. To this end, Yessenalina and Cardie (2011) took attention to the needs of nontrivial initialization and proposed to learn the weights by the bag-of-words model. Asaadi and Rudolph (2017) used complex multi-stage initialization based on unigrams and bigrams scoring. Both approaches try to solve the sentiment analysis task. Recently Mai et al. (2019) considered the problem of self-supervised continuous representation of words via matrix-space models. They optimized a modified word2vec objective function (Mikolov et al., 2013) and proposed a novel initialization by adding small isotropic Gaussian noise to the identity matrix. In this pap"
2021.findings-acl.146,P10-1093,0,0.0211745,"me. However, there exist other ways to utilize higherorder interactions and still preserve the efficiency of linear models. In this research, we focus on more data-oriented, i.e., more linguistic grounded, and better interpretable methods which still can achieve high results in the practical tasks. Particularly, we investigate the matrix-space model of language, in which semantic space consists of square matrices of real values. The key idea behind this method goes from realization of the Frege’s principle of compositionality through order-preserving property of matrix operations. As shown by Rudolph and Giesbrecht (2010), this type of models can internally combine various properties from statistical and symbolic models of natural language and therefore it is more flexible than vector space models. In spite of that fact, such models are usually hard to optimize on the real data. To this end, Yessenalina and Cardie (2011) took attention to the needs of nontrivial initialization and proposed to learn the weights by the bag-of-words model. Asaadi and Rudolph (2017) used complex multi-stage initialization based on unigrams and bigrams scoring. Both approaches try to solve the sentiment analysis task. Recently Mai"
2021.gwc-1.15,P19-1474,1,0.89603,"Missing"
2021.gwc-1.15,S18-1152,0,0.0324629,"Missing"
2021.gwc-1.15,S18-1116,0,0.0484942,"Missing"
2021.gwc-1.15,Q17-1010,0,0.050124,"resentations are complementary to the distributional word embeddings, as they capture the hypo-hypernymy relations from graphs. We expect that models using graph representations could be beneficial for the taxonomy enrichment task in combination with distributed word vector representations or on their own. We check our hypothesis on several models which make use of graph structures: node2vec (Grover and Leskovec, 2016), Poincar´e embeddings (Nickel and Kiela, 2017) and GCN autoencoder (Kipf and Welling, 2016a) and compare it with an approach of Nikishina et al. (2020b) which applies fastText (Bojanowski et al., 2017) and features from Wiktionary. All in all, our contribution is the exploration of graph-based representation for the taxonomy enrichment task and its combination with the word distributed representations. 2 Related Work The existing studies on the taxonomies can be divided into three groups. The first one addresses the Hypernym Discovery problem (CamachoCollados et al., 2018): given a word and a text corpus, the task is to identify hypernyms in the text. However, in this task the participants are not given any predefined taxonomy to rely on. The second group of works tackles Taxonomy Induction"
2021.gwc-1.15,S15-2151,0,0.0260509,"res from Wiktionary. All in all, our contribution is the exploration of graph-based representation for the taxonomy enrichment task and its combination with the word distributed representations. 2 Related Work The existing studies on the taxonomies can be divided into three groups. The first one addresses the Hypernym Discovery problem (CamachoCollados et al., 2018): given a word and a text corpus, the task is to identify hypernyms in the text. However, in this task the participants are not given any predefined taxonomy to rely on. The second group of works tackles Taxonomy Induction problem (Bordea et al., 2015; Bordea et al., 2016; Velardi et al., 2013), where the goal is to create a taxonomy automatically from scratch. The third group deals with the Taxonomy Enrichment task: the participants need to extend a given taxonomy with new words (Jurgens and Pilehvar, 2016; Nikishina et al., 2020a). Both word and graph representations can be applied to any of these tasks. 2.1 Approaches using word vector representations Approaches using word vector representations are the most popular choice for all tasks related to taxonomies. When solving the Hypernym Discovery problem in SemEval-2018 Task 9 (CamachoCol"
2021.gwc-1.15,S16-1168,0,0.019997,"All in all, our contribution is the exploration of graph-based representation for the taxonomy enrichment task and its combination with the word distributed representations. 2 Related Work The existing studies on the taxonomies can be divided into three groups. The first one addresses the Hypernym Discovery problem (CamachoCollados et al., 2018): given a word and a text corpus, the task is to identify hypernyms in the text. However, in this task the participants are not given any predefined taxonomy to rely on. The second group of works tackles Taxonomy Induction problem (Bordea et al., 2015; Bordea et al., 2016; Velardi et al., 2013), where the goal is to create a taxonomy automatically from scratch. The third group deals with the Taxonomy Enrichment task: the participants need to extend a given taxonomy with new words (Jurgens and Pilehvar, 2016; Nikishina et al., 2020a). Both word and graph representations can be applied to any of these tasks. 2.1 Approaches using word vector representations Approaches using word vector representations are the most popular choice for all tasks related to taxonomies. When solving the Hypernym Discovery problem in SemEval-2018 Task 9 (CamachoCollados et al., 2018) w"
2021.gwc-1.15,2020.acl-main.747,0,0.0185525,"Missing"
2021.gwc-1.15,S16-1169,0,0.0500915,"Missing"
2021.gwc-1.15,S18-1151,0,0.0377813,"Missing"
2021.gwc-1.15,S15-2049,0,0.031679,"resources, taxonomies are graph structures. Combining word embeddings with graph structure of taxonomy could be of use for predicting taxonomic relations. In this paper we compare several approaches for attaching new words to the existing taxonomy which are based on the graph representations with the one that relies on fastText embeddings. We test all methods on Russian and English datasets, but they could be also applied to other wordnets and languages. 1 Introduction Taxonomic structures are often used for the downstream tasks like lexical entailment (Herrera et al., 2005), entity linking (Moro and Navigli, 2015), named entity recognition (Negri and Magnini, 2004). Therefore, they always need to be up-todate and to keep up with the language change. Moreover, with the rapid growth of lexical resources for specific domains it becomes more and more important to develop systems that could automatically enrich the existing knowledge bases with new words or at least facilitate the manual taxonomy extension process. In this paper we tackle the taxonomy enrichment task which aims at associating new words (words not present in a taxonomy) with the appropriate hypernym synsets from the taxonomy. For instance, t"
2021.gwc-1.15,2020.coling-main.276,1,0.820805,"h representations. We assume that graph-based representations are complementary to the distributional word embeddings, as they capture the hypo-hypernymy relations from graphs. We expect that models using graph representations could be beneficial for the taxonomy enrichment task in combination with distributed word vector representations or on their own. We check our hypothesis on several models which make use of graph structures: node2vec (Grover and Leskovec, 2016), Poincar´e embeddings (Nickel and Kiela, 2017) and GCN autoencoder (Kipf and Welling, 2016a) and compare it with an approach of Nikishina et al. (2020b) which applies fastText (Bojanowski et al., 2017) and features from Wiktionary. All in all, our contribution is the exploration of graph-based representation for the taxonomy enrichment task and its combination with the word distributed representations. 2 Related Work The existing studies on the taxonomies can be divided into three groups. The first one addresses the Hypernym Discovery problem (CamachoCollados et al., 2018): given a word and a text corpus, the task is to identify hypernyms in the text. However, in this task the participants are not given any predefined taxonomy to rely on. T"
2021.gwc-1.15,D14-1162,0,0.0885801,"ted to taxonomies. When solving the Hypernym Discovery problem in SemEval-2018 Task 9 (CamachoCollados et al., 2018) word embeddings are used by most of participants. Bernier-Colborne and Barri`ere (2018) predict the likelihood of the relationship between an input word and a candidate using word2vec (Mikolov et al., 2013) embeddings. Word2vec is used by Berend et al. (2018) to compute features to train a logistic regression classifier. Maldonado and Klubiˇcka (2018) simply consider top-10 closest associates from the Skipgram word2vec model as hypernym candidates. Pre-trained GloVe embeddings (Pennington et al., 2014) are also used by Shwartz et al. (2016) to initialize embeddings for their LSTM-based Hypernymy Detection model. Pocostales (2016) also solve the SemEval-2016 Task 13 on taxonomy induction with word embeddings: they compute the vector offset as the average offset of all the pairs generated and exploit it to predict hypernyms for the new data. Afterwards, Aly et al. (2019) apply word2vec embeddings similarity to improve the approaches of the SemEval-2016 Task 13 participants. The vast majority of participants of SemEval2016 task 14 (Jurgens and Pilehvar, 2016) and RUSSE’2020 (Nikishina et al.,"
2021.gwc-1.15,S16-1202,0,0.0175458,"used by most of participants. Bernier-Colborne and Barri`ere (2018) predict the likelihood of the relationship between an input word and a candidate using word2vec (Mikolov et al., 2013) embeddings. Word2vec is used by Berend et al. (2018) to compute features to train a logistic regression classifier. Maldonado and Klubiˇcka (2018) simply consider top-10 closest associates from the Skipgram word2vec model as hypernym candidates. Pre-trained GloVe embeddings (Pennington et al., 2014) are also used by Shwartz et al. (2016) to initialize embeddings for their LSTM-based Hypernymy Detection model. Pocostales (2016) also solve the SemEval-2016 Task 13 on taxonomy induction with word embeddings: they compute the vector offset as the average offset of all the pairs generated and exploit it to predict hypernyms for the new data. Afterwards, Aly et al. (2019) apply word2vec embeddings similarity to improve the approaches of the SemEval-2016 Task 13 participants. The vast majority of participants of SemEval2016 task 14 (Jurgens and Pilehvar, 2016) and RUSSE’2020 (Nikishina et al., 2020a) also apply word embeddings to find the correct hypernyms in the existing taxonomy. For instance, Tanev and Rotondi (2016) c"
2021.gwc-1.15,2020.emnlp-main.594,0,0.0815842,"Missing"
2021.gwc-1.15,P16-1226,0,0.0319294,"Missing"
2021.gwc-1.15,S16-1210,0,0.0138621,"n model. Pocostales (2016) also solve the SemEval-2016 Task 13 on taxonomy induction with word embeddings: they compute the vector offset as the average offset of all the pairs generated and exploit it to predict hypernyms for the new data. Afterwards, Aly et al. (2019) apply word2vec embeddings similarity to improve the approaches of the SemEval-2016 Task 13 participants. The vast majority of participants of SemEval2016 task 14 (Jurgens and Pilehvar, 2016) and RUSSE’2020 (Nikishina et al., 2020a) also apply word embeddings to find the correct hypernyms in the existing taxonomy. For instance, Tanev and Rotondi (2016) compute a definition vector for the input word by comparing it with the definition vectors of the candidates from a wordnet using cosine similarity. Kunilovskaya et al. (2020) train word2vec embeddings from scratch and cast the task as a classification problem. Arefyev et al. (2020) compare the approach based on XLM-R model (Conneau et al., 2020) with the word2vec “hypernyms of co-hyponyms” method. It considers nearest neighbours as co-hyponyms and takes their hypernyms as candidate synsets. Summing up, the usage of distributed word vector representations is a simple yet efficient approach to"
2021.gwc-1.15,J13-3007,0,0.0380202,"ibution is the exploration of graph-based representation for the taxonomy enrichment task and its combination with the word distributed representations. 2 Related Work The existing studies on the taxonomies can be divided into three groups. The first one addresses the Hypernym Discovery problem (CamachoCollados et al., 2018): given a word and a text corpus, the task is to identify hypernyms in the text. However, in this task the participants are not given any predefined taxonomy to rely on. The second group of works tackles Taxonomy Induction problem (Bordea et al., 2015; Bordea et al., 2016; Velardi et al., 2013), where the goal is to create a taxonomy automatically from scratch. The third group deals with the Taxonomy Enrichment task: the participants need to extend a given taxonomy with new words (Jurgens and Pilehvar, 2016; Nikishina et al., 2020a). Both word and graph representations can be applied to any of these tasks. 2.1 Approaches using word vector representations Approaches using word vector representations are the most popular choice for all tasks related to taxonomies. When solving the Hypernym Discovery problem in SemEval-2018 Task 9 (CamachoCollados et al., 2018) word embeddings are used"
2021.semeval-1.126,N19-1423,0,0.0331432,"tection, namely, SemEval-2021 Task 51 (Pavlopoulos et al., 2021). It provides training, development, and test data for English. As far as we know, it is the first attempt to explicitly formulate toxicity detection as sequence labeling instead of classification of sentences. Multiple NLP tasks recently benefited from transfer learning — transfer of probability distributions learned on some task to another model solving a different task. The most common example of transfer learning is the use of embeddings and language models pre-trained on unlabeled data (e.g. ELMo (Peters et al., 2018), BERT (Devlin et al., 2019) and its variations, T5 (Raffel et al., 2020), etc.) on other tasks (e.g. He et al. (2020); Wang et al. (2020) inter alia use pre-trained BERT models to perform tasks from the GLUE benchmark (Wang et al., 2018)). Word-level toxicity classification can be formulated as a sequence labeling task, which also actively uses the pre-trained models mentioned above. BERT comprises the versatile information on words and their context, which allows to successfully use it for sequence labeling tasks of different levels: part-of-speech tagging and syntactic parsing (Koto et al., 2020), named entity recogni"
2021.semeval-1.126,D19-5709,0,0.0282829,"ts variations, T5 (Raffel et al., 2020), etc.) on other tasks (e.g. He et al. (2020); Wang et al. (2020) inter alia use pre-trained BERT models to perform tasks from the GLUE benchmark (Wang et al., 2018)). Word-level toxicity classification can be formulated as a sequence labeling task, which also actively uses the pre-trained models mentioned above. BERT comprises the versatile information on words and their context, which allows to successfully use it for sequence labeling tasks of different levels: part-of-speech tagging and syntactic parsing (Koto et al., 2020), named entity recognition (Hakala and Pyysalo, 2019), semantic role labeling (He et al., 2019), detection of Machine Translation errors (Moura et al., 2020). This diversity of applications suggests that wordlevel toxicity detection can also benefit from pretrained models. Besides that, toxicity itself has been successfully tackled with BERT-based models. Research on sentence-level toxicity extensively used BERT and other pre-trained models. Both language-specific and multilingual BERT models were used to fine-tune toxicity classifiers (Leite et al., 2020; Ozler et al., 2020). This shows that BERT has information on toxicity. 1 https://competiti"
2021.semeval-1.126,D19-1538,0,0.0208235,"ther tasks (e.g. He et al. (2020); Wang et al. (2020) inter alia use pre-trained BERT models to perform tasks from the GLUE benchmark (Wang et al., 2018)). Word-level toxicity classification can be formulated as a sequence labeling task, which also actively uses the pre-trained models mentioned above. BERT comprises the versatile information on words and their context, which allows to successfully use it for sequence labeling tasks of different levels: part-of-speech tagging and syntactic parsing (Koto et al., 2020), named entity recognition (Hakala and Pyysalo, 2019), semantic role labeling (He et al., 2019), detection of Machine Translation errors (Moura et al., 2020). This diversity of applications suggests that wordlevel toxicity detection can also benefit from pretrained models. Besides that, toxicity itself has been successfully tackled with BERT-based models. Research on sentence-level toxicity extensively used BERT and other pre-trained models. Both language-specific and multilingual BERT models were used to fine-tune toxicity classifiers (Leite et al., 2020; Ozler et al., 2020). This shows that BERT has information on toxicity. 1 https://competitions.codalab.org/ competitions/25623 927 Pr"
2021.semeval-1.126,2020.coling-main.66,0,0.0355912,"t al., 2018), BERT (Devlin et al., 2019) and its variations, T5 (Raffel et al., 2020), etc.) on other tasks (e.g. He et al. (2020); Wang et al. (2020) inter alia use pre-trained BERT models to perform tasks from the GLUE benchmark (Wang et al., 2018)). Word-level toxicity classification can be formulated as a sequence labeling task, which also actively uses the pre-trained models mentioned above. BERT comprises the versatile information on words and their context, which allows to successfully use it for sequence labeling tasks of different levels: part-of-speech tagging and syntactic parsing (Koto et al., 2020), named entity recognition (Hakala and Pyysalo, 2019), semantic role labeling (He et al., 2019), detection of Machine Translation errors (Moura et al., 2020). This diversity of applications suggests that wordlevel toxicity detection can also benefit from pretrained models. Besides that, toxicity itself has been successfully tackled with BERT-based models. Research on sentence-level toxicity extensively used BERT and other pre-trained models. Both language-specific and multilingual BERT models were used to fine-tune toxicity classifiers (Leite et al., 2020; Ozler et al., 2020). This shows that"
2021.semeval-1.126,2021.ccl-1.108,0,0.0642304,"Missing"
2021.semeval-1.126,2020.wmt-1.119,0,0.0594305,"Missing"
2021.semeval-1.126,2020.alw-1.4,0,0.032362,"syntactic parsing (Koto et al., 2020), named entity recognition (Hakala and Pyysalo, 2019), semantic role labeling (He et al., 2019), detection of Machine Translation errors (Moura et al., 2020). This diversity of applications suggests that wordlevel toxicity detection can also benefit from pretrained models. Besides that, toxicity itself has been successfully tackled with BERT-based models. Research on sentence-level toxicity extensively used BERT and other pre-trained models. Both language-specific and multilingual BERT models were used to fine-tune toxicity classifiers (Leite et al., 2020; Ozler et al., 2020). This shows that BERT has information on toxicity. 1 https://competitions.codalab.org/ competitions/25623 927 Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021), pages 927–934 Bangkok, Thailand (online), August 5–6, 2021. ©2021 Association for Computational Linguistics Thus, we follow this line of work. Namely, we fine-tune a RoBERTa model (Liu et al., 2019) to perform a sequence labeling task. Besides that, we train a model for sentence classification on the Jigsaw dataset of toxic comments and use the information from this model to detect toxicity at the su"
2021.semeval-1.126,2021.semeval-1.6,0,0.0219794,"hat localizing toxicity within a sentence is also useful. If we know which words of a sentence are toxic, it is easier to “fix” this sentence by removing or replacing them with non-toxic synonyms. Mathew et al. (2020) make human labelers annotate the spans as rationales for classifying a comment as hateful, offensive, or normal. They show that using such spans when training a toxicity classifier improves its accuracy and explainability and reduces unintended bias towards toxicity targets. This year the SemEval hosts the first competition on toxic spans detection, namely, SemEval-2021 Task 51 (Pavlopoulos et al., 2021). It provides training, development, and test data for English. As far as we know, it is the first attempt to explicitly formulate toxicity detection as sequence labeling instead of classification of sentences. Multiple NLP tasks recently benefited from transfer learning — transfer of probability distributions learned on some task to another model solving a different task. The most common example of transfer learning is the use of embeddings and language models pre-trained on unlabeled data (e.g. ELMo (Peters et al., 2018), BERT (Devlin et al., 2019) and its variations, T5 (Raffel et al., 2020"
2021.semeval-1.126,D14-1162,0,0.0893727,"ds. The model performs closely to the attention-based classifier. Sequence labeling with LSTM Finally, we experiment with the LSTM architecture (Hochreiter and Schmidhuber, 1997). We implement a BiLSTM network and also train an LSTM tagger from the AllenNLP library.3 We do not use any pre. In both methods, we label a space character as toxic only if the characters both to the right and to the left of it are toxic. 3 929 https://github.com/allenai/allennlp trained embeddings in the Bi-LSTM model and use two versions of the AllenNLP LSTM: without pretrained embeddings and with GloVe embeddings (Pennington et al., 2014). 5 Model Top-5 participants HITSZ-HLT S-NLP hitmi&t L YNU-HPCC Evaluation 5.1 Experimental Setting For each transfer learning model, we use twostage fine-tuning. We first train only the output layers of the models with the learning rate of 10−3 , and then the whole models with the learning rate of 10−5 . In both cases, we use linear learning rate warm-up for 3000 steps. We use the AdamW optimizer (Loshchilov and Hutter, 2019) and the batch size of 8, and early stopping to determine the number of training steps. We use the transformers4 library for training. For all the models which use the ad"
2021.semeval-1.126,N18-1202,0,0.022414,"ompetition on toxic spans detection, namely, SemEval-2021 Task 51 (Pavlopoulos et al., 2021). It provides training, development, and test data for English. As far as we know, it is the first attempt to explicitly formulate toxicity detection as sequence labeling instead of classification of sentences. Multiple NLP tasks recently benefited from transfer learning — transfer of probability distributions learned on some task to another model solving a different task. The most common example of transfer learning is the use of embeddings and language models pre-trained on unlabeled data (e.g. ELMo (Peters et al., 2018), BERT (Devlin et al., 2019) and its variations, T5 (Raffel et al., 2020), etc.) on other tasks (e.g. He et al. (2020); Wang et al. (2020) inter alia use pre-trained BERT models to perform tasks from the GLUE benchmark (Wang et al., 2018)). Word-level toxicity classification can be formulated as a sequence labeling task, which also actively uses the pre-trained models mentioned above. BERT comprises the versatile information on words and their context, which allows to successfully use it for sequence labeling tasks of different levels: part-of-speech tagging and syntactic parsing (Koto et al.,"
2021.semeval-1.126,2020.aacl-main.91,0,0.0368619,"-speech tagging and syntactic parsing (Koto et al., 2020), named entity recognition (Hakala and Pyysalo, 2019), semantic role labeling (He et al., 2019), detection of Machine Translation errors (Moura et al., 2020). This diversity of applications suggests that wordlevel toxicity detection can also benefit from pretrained models. Besides that, toxicity itself has been successfully tackled with BERT-based models. Research on sentence-level toxicity extensively used BERT and other pre-trained models. Both language-specific and multilingual BERT models were used to fine-tune toxicity classifiers (Leite et al., 2020; Ozler et al., 2020). This shows that BERT has information on toxicity. 1 https://competitions.codalab.org/ competitions/25623 927 Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021), pages 927–934 Bangkok, Thailand (online), August 5–6, 2021. ©2021 Association for Computational Linguistics Thus, we follow this line of work. Namely, we fine-tune a RoBERTa model (Liu et al., 2019) to perform a sequence labeling task. Besides that, we train a model for sentence classification on the Jigsaw dataset of toxic comments and use the information from this model to dete"
2021.semeval-1.126,N18-1027,0,0.0225017,", 2021. ©2021 Association for Computational Linguistics Thus, we follow this line of work. Namely, we fine-tune a RoBERTa model (Liu et al., 2019) to perform a sequence labeling task. Besides that, we train a model for sentence classification on the Jigsaw dataset of toxic comments and use the information from this model to detect toxicity at the subsentential level. This helps us overcome the insufficient data size. This is in line with previous work, which has shown that sentence-level labels can be used in combination with token labels (Rei and Søgaard, 2019) or completely substitute them (Rei and Søgaard, 2018; Schmaltz, 2019). In our experiments, we test the hypothesis that the sentence-level toxicity labeling can be used for a sequence labeler that recognizes toxic spans in text. We suggest three ways of incorporating this data: as a corpus for pre-training, pseudo-labeling, and for joint training of sentence-level and tokenlevel toxicity detection models. Our experiments show that the latter method yields the best result. Moreover, we show that using sentence-level labels can dramatically improve toxic span prediction when the dataset with token-level labels is small. The contributions of this w"
2021.semeval-1.126,D17-1035,0,0.025593,"Missing"
2021.semeval-1.126,P18-2031,0,0.0166611,"g models, it is only 2 points below the best result. This suggests the viability of our approach. 1 Introduction Toxicity and offensive content is a major concern for many platforms on the Internet. Therefore, the task of toxicity detection has attracted much attention in the NLP community (Wulczyn et al., 2017; Hosseini et al., 2017; Dixon et al., 2018). Until recently, the majority of research on toxicity focused on classifying entire user messages as toxic or safe. However, the surge of work on text detoxification, i.e., editing of text to keep its content and remove toxicity (Nogueira dos Santos et al., 2018; Tran et al., 2020), suggests that localizing toxicity within a sentence is also useful. If we know which words of a sentence are toxic, it is easier to “fix” this sentence by removing or replacing them with non-toxic synonyms. Mathew et al. (2020) make human labelers annotate the spans as rationales for classifying a comment as hateful, offensive, or normal. They show that using such spans when training a toxicity classifier improves its accuracy and explainability and reduces unintended bias towards toxicity targets. This year the SemEval hosts the first competition on toxic spans detection"
2021.semeval-1.126,2020.coling-main.190,0,0.0257407,"2 points below the best result. This suggests the viability of our approach. 1 Introduction Toxicity and offensive content is a major concern for many platforms on the Internet. Therefore, the task of toxicity detection has attracted much attention in the NLP community (Wulczyn et al., 2017; Hosseini et al., 2017; Dixon et al., 2018). Until recently, the majority of research on toxicity focused on classifying entire user messages as toxic or safe. However, the surge of work on text detoxification, i.e., editing of text to keep its content and remove toxicity (Nogueira dos Santos et al., 2018; Tran et al., 2020), suggests that localizing toxicity within a sentence is also useful. If we know which words of a sentence are toxic, it is easier to “fix” this sentence by removing or replacing them with non-toxic synonyms. Mathew et al. (2020) make human labelers annotate the spans as rationales for classifying a comment as hateful, offensive, or normal. They show that using such spans when training a toxicity classifier improves its accuracy and explainability and reduces unintended bias towards toxicity targets. This year the SemEval hosts the first competition on toxic spans detection, namely, SemEval-20"
2021.semeval-1.126,W18-5446,0,0.0554307,"Missing"
2021.semeval-1.16,2020.acl-main.747,0,0.0764367,"Missing"
2021.semeval-1.16,N19-1423,0,0.0155103,"age (en-en) context pairs and validation sets of 1 000 context pairs for English-English (en-en), FrenchFrench (fr-fr), Russian-Russian (ru-ru), ArabicArabic (ar-ar), and Chinese-Chinese (zh-zh) languages. Since no cross-lingual training data were provided, except for a very small trial set barely usable for training, we decided to venture into generating such data automatically. Essentially, the given task is a binary classification problem. The first question was which supervised model to use for the classification of context pairs. Recently, pre-trained masked language models such as BERT (Devlin et al., 2019) and XLMR (Conneau et al., 2020) have been used to reach promising results in a variety of similar NLU classification tasks. Thus, we decided to make use of contextualized embeddings from XLM-R, which provides multilingual-lingual embeddings for more than 100 languages, covering all language pairs of interest in the shared task. In all our experiments, this model is used as the backbone. A straightforward way of solving tasks where two contexts are to be compared, as the word-incontext tasks, is to use deep contextualized embeddings and train a classifier over these embeddings as has been expl"
2021.semeval-1.16,2020.emnlp-main.363,0,0.0202564,"Missing"
2021.semeval-1.16,2021.semeval-1.3,0,0.028154,"ontext of this shared task. In our experiments, we used a neural system based on the XLM-R (Conneau et al., 2020), a pre-trained transformer-based masked language model, as a baseline. We show the effectiveness of the proposed approach as it allows to substantially improve the performance of this strong neural baseline model. In addition, in this study, we present multiple types of the XLM-R based classifier, experimenting with various ways of mixing information from the first and second occurrences of the target word in two samples. 1 Introduction The goal of the second task of SemEval-2021 (Martelli et al., 2021) is to perform multilingual and cross-lingual word-in-context disambiguation. More specifically, participants are asked to distinguish whether the meanings of a target word in two provided contexts are the same or not. Organizers provided a training set of 8 000 English language (en-en) context pairs and validation sets of 1 000 context pairs for English-English (en-en), FrenchFrench (fr-fr), Russian-Russian (ru-ru), ArabicArabic (ar-ar), and Chinese-Chinese (zh-zh) languages. Since no cross-lingual training data were provided, except for a very small trial set barely usable for training, we d"
2021.semeval-1.16,N19-1128,0,0.0194174,"h promising results in a variety of similar NLU classification tasks. Thus, we decided to make use of contextualized embeddings from XLM-R, which provides multilingual-lingual embeddings for more than 100 languages, covering all language pairs of interest in the shared task. In all our experiments, this model is used as the backbone. A straightforward way of solving tasks where two contexts are to be compared, as the word-incontext tasks, is to use deep contextualized embeddings and train a classifier over these embeddings as has been explored in the original monolingual word-in-context task (Pilehvar and CamachoCollados, 2019). Note that commonly embeddings of two contexts are simply concatenated (Ma et al., 2019) and this operation is asymmetric. In our work, we explored various symmetric ways of aggregating embeddings from two contexts. The contributions of our work are two-fold. First, we present a simple yet effective method for the generation of cross-lingual training data, showing that it can substantially improve the performance compared to the model trained using monolingual data. Second, we test various ways of encoding two input target word occurrences contexts using the XLM-R model. 2 Baseline Supervised"
D17-2016,J13-3008,0,0.00623738,"Missing"
D17-2016,D12-1129,1,0.834113,"o the semantic class as features. Super senses based on context word features. This model relies on the same semantic classes as the previous one but, instead, sense representations are obtained by averaging vectors of words sharing the same class. 3.2 an ambiguous word and its context. The output of the system is a ranked list of all word senses of the ambiguous word ordered by relevance to the input context. By default, only the best matching sense is displayed. The user can quickly understand the meaning of each induced sense by looking at the hypernym and the image representing the sense. Faralli and Navigli (2012) showed that Web search engines can be used to acquire information about word senses. We assign an image to each word in the cluster by querying an image search API9 using a query composed of the ambiguous word and its hypernym, e.g. “jaguar animal”. The first hit of this query is selected to represent the induced word sense. Interpretability of each sense is further ensured by providing to the user the list of related senses, the list of the most salient context clues, and the sense usage examples (cf. Figure 2). Note that all these elements are obtained without manual intervention. Finally,"
D17-2016,C92-2082,0,0.232587,"ollections. Induction of a WSD model consists of several steps. First, a graph of semantically related words, i.e. a distributional thesaurus, is extracted. Second, word senses are induced by clustering of an ego-network of related words (Biemann, 2006). Each discovered word sense is represented as a cluster of words. Next, the induced sense inventory is used as a pivot to generate sense representations by aggregation of the context clues of cluster words. To improve interpretability of the sense clusters they are labeled with hypernyms, which are in turn extracted from the input corpus using Hearst (1992) patterns. Finally, the obtained WSD model is used to retrieve a list of sentences that characterize each sense. Sentences that mention a given word are disambiguated and then ranked by prediction confidence. Top sentences are used as sense usage examples. For more details about the model induction process refer to (Panchenko et al., 2017). Currently, the following WSD models induced from a text corpus are available: Word senses based on cluster word features. This model uses the cluster words from the induced word sense inventory as sparse features that represent the sense. Word senses based"
D17-2016,S13-2049,0,0.0191872,"ical context clues. Each of these elements is extracted automatically. The reasons of the predictions are provided in terms of common sparse features of the input sentence and a sense representation (E). The induced senses are linked to BabelNet using the method of Faralli et al. (2016) (F). Figure 3: All words disambiguation mode: results of disambiguation of all nouns in a sentence. 94 # Words # Senses Avg. Polysemy # Contexts 863 2,708 3.13 11,712 WSD Model Inventory Features Table 1: Evaluation dataset based on BabelNet. methods for WSD, including participants of the SemEval 2013 Task 13 (Jurgens and Klapaftis, 2013) and two unsupervised knowledge-free WSD systems based on word sense embeddings (Bartunov et al., 2016; Pelevina et al., 2016). These evaluations were based on the “lexical sample” setting, where the system is expected to predict a sense identifier of the ambiguous word. In this section, we perform an extra evaluation that assesses how well hypernyms of ambiguous words are assigned in context by our system. Namely, the task is to assign a correct hypernym of an ambiguous word, e.g. “animal” for the word “Jaguar” in the context “Jaguar is a large spotted predator of tropical America”. This task"
D17-2016,P13-4007,0,0.351132,"Missing"
D17-2016,Q14-1019,0,0.219564,"retable vectors. Therefore, the meaning of a sense can be interpreted only on the basis of a list of related senses. We present a system that brings interpretability of the knowledge-based sense representations into the world of unsupervised knowledge-free WSD models. The contribution of this paper is the first system for word sense induction and disambiguation, which is unsupervised, knowledge-free, and interpretable at the same time. The system is based on the WSD approach of Panchenko et al. (2017) and is designed to reach interpretability level of knowledge-based systems, such as Babelfy (Moro et al., 2014), within an unsupervised knowledgefree framework. Implementation of the system is open source.1 A live demo featuring several disambiguation models is available online.2 Interpretability of a predictive model is a powerful feature that gains the trust of users in the correctness of the predictions. In word sense disambiguation (WSD), knowledge-based systems tend to be much more interpretable than knowledge-free counterparts as they rely on the wealth of manually-encoded elements representing word senses, such as hypernyms, usage examples, and images. We present a WSD system that bridges the ga"
D17-2016,D14-1113,0,0.0192822,"nces are used as sense usage examples. For more details about the model induction process refer to (Panchenko et al., 2017). Currently, the following WSD models induced from a text corpus are available: Word senses based on cluster word features. This model uses the cluster words from the induced word sense inventory as sparse features that represent the sense. Word senses based on context word features. This representation is based on a sum of word vectors of all cluster words in the induced sense inventory weighted by distributional similarity scores. Knowledge-Free and Unsupervised Systems Neelakantan et al. (2014) proposed a multi-sense extension of the Skip-gram model that features an open implementation. AdaGram (Bartunov et al., 2016) is a system that learns sense embeddings using a Bayesian extension of the Skip-gram model and provides WSD functionality based on the induced sense inventory. SenseGram (Pelevina et al., 2016) is a system that transforms word embeddings to sense embeddings via graph clustering and uses them for WSD. Other methods to learn sense embeddings were proposed, but these do not feature open implementations for WSD. Among all listed systems, only Babelfy implements a user inte"
D17-2016,E17-1009,1,0.647167,"ioned features enabling interpretability. For instance, systems based on sense embeddings are based on dense uninterpretable vectors. Therefore, the meaning of a sense can be interpreted only on the basis of a list of related senses. We present a system that brings interpretability of the knowledge-based sense representations into the world of unsupervised knowledge-free WSD models. The contribution of this paper is the first system for word sense induction and disambiguation, which is unsupervised, knowledge-free, and interpretable at the same time. The system is based on the WSD approach of Panchenko et al. (2017) and is designed to reach interpretability level of knowledge-based systems, such as Babelfy (Moro et al., 2014), within an unsupervised knowledgefree framework. Implementation of the system is open source.1 A live demo featuring several disambiguation models is available online.2 Interpretability of a predictive model is a powerful feature that gains the trust of users in the correctness of the predictions. In word sense disambiguation (WSD), knowledge-based systems tend to be much more interpretable than knowledge-free counterparts as they rely on the wealth of manually-encoded elements repr"
D17-2016,W16-1620,1,0.761499,"parse features that represent the sense. Word senses based on context word features. This representation is based on a sum of word vectors of all cluster words in the induced sense inventory weighted by distributional similarity scores. Knowledge-Free and Unsupervised Systems Neelakantan et al. (2014) proposed a multi-sense extension of the Skip-gram model that features an open implementation. AdaGram (Bartunov et al., 2016) is a system that learns sense embeddings using a Bayesian extension of the Skip-gram model and provides WSD functionality based on the induced sense inventory. SenseGram (Pelevina et al., 2016) is a system that transforms word embeddings to sense embeddings via graph clustering and uses them for WSD. Other methods to learn sense embeddings were proposed, but these do not feature open implementations for WSD. Among all listed systems, only Babelfy implements a user interface supporting interpretable visualization of the disambiguation results. 3 Unsupervised Knowledge-Free Interpretable WSD This section describes (1) how WSD models are learned in an unsupervised way from text and (2) how the system uses these models to enable human interpretable disambiguation in context. 3 Induction"
D17-2016,W06-3812,1,0.134336,"tecture of the WSD system. As one may observe, no human labor is used to learn interpretable sense representations and the corresponding disambiguation models. Instead, these are induced from the input text corpus using the JoBimText approach (Biemann and Riedl, 2013) implemented using the Apache Spark framework4 , enabling seamless processing of large text collections. Induction of a WSD model consists of several steps. First, a graph of semantically related words, i.e. a distributional thesaurus, is extracted. Second, word senses are induced by clustering of an ego-network of related words (Biemann, 2006). Each discovered word sense is represented as a cluster of words. Next, the induced sense inventory is used as a pivot to generate sense representations by aggregation of the context clues of cluster words. To improve interpretability of the sense clusters they are labeled with hypernyms, which are in turn extracted from the input corpus using Hearst (1992) patterns. Finally, the obtained WSD model is used to retrieve a list of sentences that characterize each sense. Sentences that mention a given word are disambiguated and then ranked by prediction confidence. Top sentences are used as sense"
D17-2016,P15-1173,0,0.0206782,"Missing"
D17-2016,biemann-2012-turk,1,0.85795,"Missing"
D17-2016,P17-1145,1,0.786442,"dge-Free Interpretable WSD This section describes (1) how WSD models are learned in an unsupervised way from text and (2) how the system uses these models to enable human interpretable disambiguation in context. 3 Induction of the WSD Models 4 https://github.com/alvations/pywsd 92 http://spark.apache.org Super senses based on cluster word features. To build this model, induced word senses are first globally clustered using the Chinese Whispers graph clustering algorithm (Biemann, 2006). The edges in this sense graph are established by disambiguation of the related words (Faralli et al., 2016; Ustalov et al., 2017). The resulting clusters represent semantic classes grouping words sharing a common hypernym, e.g. “animal”. This set of semantic classes is used as an automatically learned inventory of super senses: There is only one global sense inventory shared among all words in contrast to the two previous traditional “per word” models. Each semantic class is labeled with hypernyms. This model uses words belonging to the semantic class as features. Super senses based on context word features. This model relies on the same semantic classes as the previous one but, instead, sense representations are obtain"
D17-2016,P10-4014,0,0.042323,"ntations. Introduction The notion of word sense is central to computational lexical semantics. Word senses can be either encoded manually in lexical resources or induced automatically from text. The former knowledgebased sense representations, such as those found in the BabelNet lexical semantic network (Navigli and Ponzetto, 2012), are easily interpretable by humans due to the presence of definitions, usage examples, taxonomic relations, related words, and images. The cost of such interpretability is that every element mentioned above is encoded Knowledge-Based and/or Supervised Systems IMS (Zhong and Ng, 2010) is a supervised allwords WSD system that allows users to integrate additional features and different classifiers. By default, the system relies on the linear support vector machines with multiple features. The AutoExtend (Rothe and Sch¨utze, 2015) approach can be used to learn embeddings for lexemes and synsets 1 2 91 https://github.com/uhh-lt/wsd http://jobimtext.org/wsd Proceedings of the 2017 EMNLP System Demonstrations, pages 91–96 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Induction of the WSD Models (Scala/Spark): §3.1 WSD Model Index RES"
E17-1009,D14-1110,0,0.0391811,"e amounts of senselabeled examples per target word. Knowledge-based approaches rely on a lexical resource that provides a sense inventory and features for disambiguation and vary from the classical Lesk (1986) algorithm that uses word definitions to the Babelfy (Moro et al., 2014) system that uses harnesses a multilingual lexicalsemantic network. Classical examples of such approaches include (Banerjee and Pedersen, 2002; Pedersen et al., 2005; Miller et al., 2012). More recently, several methods were proposed to learn sense embeddings on the basis of the sense inventory of a lexical resource (Chen et al., 2014; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2015; Iacobacci et al., 2015; Nieto Pi˜na and Johansson, 2016). Unsupervised knowledge-free approaches use neither handcrafted lexical resources nor handannotated sense-labeled corpora. Instead, they induce word sense inventories automatically from corpora. Unsupervised WSD methods fall into two main categories: context clustering and word ego-network clustering. Context clustering approaches, e.g. (Pedersen and Bruce, 1997; Sch¨utze, 1998), represent an instance usually by a vector that characterizes its context, where the definition of con"
E17-1009,de-marneffe-etal-2006-generating,0,0.0297452,"Missing"
E17-1009,J13-3008,0,0.232296,"Missing"
E17-1009,S13-2050,0,0.0233307,"del Precision Recall F-score Precision Recall F-score Dependencies Dependencies Exp. 0.728 0.687 0.343 0.633 0.466 0.659 0.432 0.414 0.190 0.379 0.263 0.396 Dependencies + LM Dependencies Exp. + LM 0.689 0.684 0.681 0.676 0.685 0.680 0.426 0.412 0.422 0.408 0.424 0.410 Table 2: Effect of the feature expansion: performance on the full (on the left) and the sense-balanced (on the right) TWSI datasets. The models were trained on the Wikipedia corpus using the coarse inventory (1.96 senses per word). The best results overall are underlined. 4.2 generated using word embeddings6 . The AI-KU system (Baskaya et al., 2013) directly clusters test contexts using the k-means algorithm based on lexical substitution features. The Unimelb system (Lau et al., 2013) uses one hierarchical topic model to induce and disambiguate senses of one word. The UoS system (Hope and Keller, 2013) induces senses by building an ego-network of a word using dependency relations, which is subsequently clustered using the MaxMax clustering algorithm. The La Sapienza system (Jurgens and Klapaftis, 2013), relies on WordNet for the sense inventory and disambiguation. In contrast to the TWSI evaluation, the most fine-grained model yields the"
E17-1009,J93-1003,0,0.524247,"syntactic dependency such as “nn(•,writing)” or “prep at(sit,•)”, extracted from the Stanford Dependencies (De Marneffe et al., 2006) obtained with the the PCFG model of the Stanford parser (Klein and Manning, 2003). Weights are computed using the Local Mutual Information (LMI) (Evert, 2005). One word is represented with 1000 most significant features. Co-occurrence Features. This type of features represents a word by another word. We extract the list of words that significantly co-occur in a sentence with the target word in the input corpus based on the log-likelihood as word-feature weight (Dunning, 1993). Language Model Feature. This type of features are based on a trigram model with Kneser-Ney smoothing (Kneser and Ney, 1995). In particular, a word is represented by (1) right and left context words, e.g. “office • and”, (2) two preceding words “new office •”, and (3) two succeeding words, e.g. “• and chairs”. We use the conditional probabilities of the resulting trigrams as word-feature weights. Our approach was inspired by the knowledgebased system Babelfy (Moro et al., 2014). While the inventory of Babelfy is interpretable as it relies on BabelNet, the system provides no underlying reasons"
E17-1009,D12-1129,1,0.811774,"scores: Pthe hypernym relevance score, calculated as w∈cluster sim(t, w)f req(w, h), and Pthe hypernym coverage score, calculated Here the as w∈cluster min(f req(w, h), 1). sim(t, w) is the relatedness of the cluster word w to the target word t, and the f req(w, h) is the frequency of the hypernymy relation (w, h) as extracted via patterns. Thus, a high-ranked hypernym h has high relevance, but also is confirmed by several cluster words. This stage results in a ranked list of labels that specify the word sense, for which we here show the first one, e.g. “table (furniture)” or “table (data)”. Faralli and Navigli (2012) showed that web search engines can be used to bootstrap senserelated information. To further improve interpretability of induced senses, we assign an image to each word in the cluster (see Figure 2) by queryWord Sense Induction We induce a sense inventory by clustering of egonetwork of similar words. In our case, an inventory represents senses by a word cluster, such as “chair, bed, bench, stool, sofa, desk, cabinet” for the “furniture” sense of the word “table”. The sense induction processes one word t of the distributional thesaurus T per iteration. First, we retrieve nodes of the ego-netwo"
E17-1009,W06-3812,1,0.735982,"s, e.g. (Pedersen and Bruce, 1997; Sch¨utze, 1998), represent an instance usually by a vector that characterizes its context, where the definition of context can vary greatly. These vectors of each instance are then clustered. Multi-prototype extensions of the skipgram model (Mikolov et al., 2013) that use no predefined sense inventory learn one embedding word vector per one word sense and are commonly fitted with a disambiguation mechanism (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., Word ego-network clustering methods (Lin, 1998; Pantel and Lin, 2002; Widdows and Dorow, 2002; Biemann, 2006; Hope and Keller, 2013) cluster graphs of words semantically related to the ambiguous word. An ego network consists of a single node (ego) together with the nodes they are connected to (alters) and all the edges among those alters (Everett and Borgatti, 2005). In our case, such a network is a local neighborhood of one word. Nodes of the ego-network can be (1) words semantically similar to the target word, as in our approach, or (2) context words relevant to the target, as in the UoS system (Hope and Keller, 2013). Graph edges represent semantic relations between words derived using corpus-bas"
E17-1009,biemann-2012-turk,1,0.869612,"sociated with the senses are retrieved using a search engine:“table data” and “table furniture”. 4 2. Sense feature representation. Each sense in our model is characterized by a list of sparse features ordered by relevance to the sense. Figure 2 (2) shows most salient dependency features to senses of the word “table”. These feature representations are obtained by aggregating features of sense cluster words. Experiments We use two lexical sample collections suitable for evaluation of unsupervised WSD systems. The first one is the Turk Bootstrap Word Sense Inventory (TWSI) dataset introduced by Biemann (2012). It is used for testing different configurations of our approach. The second collection, the SemEval 2013 word sense induction dataset by Jurgens and Klapaftis (2013), is used to compare our approach to existing systems. In both datasets, to measure WSD performance, induced senses are mapped to gold standard senses. In experiments with the TWSI dataset, the models were trained on the Wikipedia corpus4 while in experiments with the SemEval datasets models are trained on the ukWaC corpus (Baroni et al., 2009) for a fair comparison with other participants. In systems based on dense vector repres"
E17-1009,C92-2082,0,0.102793,"y a pruning step in which only the 200 most similar terms are kept to every word. The resulting word similarities are browsable online.2 Note that while words can be characterized with distributions over features, features can vice versa be characterized by a distribution over words. We use this duality to compute feature similarities using the same mechanism and explore their use in disambiguation below. 3.3 3.4 Labeling Induced Senses with Hypernyms and Images Each sense cluster is automatically labeled to improve its interpretability. First, we extract hypernyms from the input corpus using Hearst (1992) patterns. Second, we rank hypernyms relevant to the cluster by a product of two scores: Pthe hypernym relevance score, calculated as w∈cluster sim(t, w)f req(w, h), and Pthe hypernym coverage score, calculated Here the as w∈cluster min(f req(w, h), 1). sim(t, w) is the relatedness of the cluster word w to the target word t, and the f req(w, h) is the frequency of the hypernymy relation (w, h) as extracted via patterns. Thus, a high-ranked hypernym h has high relevance, but also is confirmed by several cluster words. This stage results in a ranked list of labels that specify the word sense, fo"
E17-1009,N15-1059,0,0.0197223,"word. Knowledge-based approaches rely on a lexical resource that provides a sense inventory and features for disambiguation and vary from the classical Lesk (1986) algorithm that uses word definitions to the Babelfy (Moro et al., 2014) system that uses harnesses a multilingual lexicalsemantic network. Classical examples of such approaches include (Banerjee and Pedersen, 2002; Pedersen et al., 2005; Miller et al., 2012). More recently, several methods were proposed to learn sense embeddings on the basis of the sense inventory of a lexical resource (Chen et al., 2014; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2015; Iacobacci et al., 2015; Nieto Pi˜na and Johansson, 2016). Unsupervised knowledge-free approaches use neither handcrafted lexical resources nor handannotated sense-labeled corpora. Instead, they induce word sense inventories automatically from corpora. Unsupervised WSD methods fall into two main categories: context clustering and word ego-network clustering. Context clustering approaches, e.g. (Pedersen and Bruce, 1997; Sch¨utze, 1998), represent an instance usually by a vector that characterizes its context, where the definition of context can vary greatly. These vectors of each instance are"
E17-1009,P12-1092,0,0.0637845,"lly from corpora. Unsupervised WSD methods fall into two main categories: context clustering and word ego-network clustering. Context clustering approaches, e.g. (Pedersen and Bruce, 1997; Sch¨utze, 1998), represent an instance usually by a vector that characterizes its context, where the definition of context can vary greatly. These vectors of each instance are then clustered. Multi-prototype extensions of the skipgram model (Mikolov et al., 2013) that use no predefined sense inventory learn one embedding word vector per one word sense and are commonly fitted with a disambiguation mechanism (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., Word ego-network clustering methods (Lin, 1998; Pantel and Lin, 2002; Widdows and Dorow, 2002; Biemann, 2006; Hope and Keller, 2013) cluster graphs of words semantically related to the ambiguous word. An ego network consists of a single node (ego) together with the nodes they are connected to (alters) and all the edges among those alters (Everett and Borgatti, 2005). In our case, such a network is a local neighborhood of one word. Nodes of the ego-network can be (1) words semantically similar to the target word, as in our approach, or (2) context words"
E17-1009,P15-1010,0,0.0282997,"es rely on a lexical resource that provides a sense inventory and features for disambiguation and vary from the classical Lesk (1986) algorithm that uses word definitions to the Babelfy (Moro et al., 2014) system that uses harnesses a multilingual lexicalsemantic network. Classical examples of such approaches include (Banerjee and Pedersen, 2002; Pedersen et al., 2005; Miller et al., 2012). More recently, several methods were proposed to learn sense embeddings on the basis of the sense inventory of a lexical resource (Chen et al., 2014; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2015; Iacobacci et al., 2015; Nieto Pi˜na and Johansson, 2016). Unsupervised knowledge-free approaches use neither handcrafted lexical resources nor handannotated sense-labeled corpora. Instead, they induce word sense inventories automatically from corpora. Unsupervised WSD methods fall into two main categories: context clustering and word ego-network clustering. Context clustering approaches, e.g. (Pedersen and Bruce, 1997; Sch¨utze, 1998), represent an instance usually by a vector that characterizes its context, where the definition of context can vary greatly. These vectors of each instance are then clustered. Multi-p"
E17-1009,P14-2050,0,0.0238238,"rameters and its comparable performance on the WSI task to the state-of-theart (Di Marco and Navigli, 2013). 2013) as it yields comparable performance on semantic similarity to state-of-the-art dense representations (Mikolov et al., 2013) compared on the WordNet as gold standard (Riedl, 2016), but is interpretable as word are represented by sparse interpretable features. Namely we use dependencybased features as, according to prior evaluations, this kind of features provides state-of-the-art semantic relatedness scores (Pad´o and Lapata, 2007; Van de Cruys, 2010; Panchenko and Morozova, 2012; Levy and Goldberg, 2014). First, features of each word are ranked using the LMI metric (Evert, 2005). Second, the word representations are pruned keeping 1000 most salient features per word and 1000 most salient words per feature. The pruning reduces computational complexity and noise. Finally, word similarities are computed as a number of common features for two words. This is again followed by a pruning step in which only the 200 most similar terms are kept to every word. The resulting word similarities are browsable online.2 Note that while words can be characterized with distributions over features, features can"
E17-1009,J98-1001,0,0.0520264,"Missing"
E17-1009,D15-1200,0,0.0789216,"for Word Sense Induction and Disambiguation Alexander Panchenko‡ , Eugen Ruppert‡ , Stefano Faralli† , Simone Paolo Ponzetto† and Chris Biemann‡ ‡ † Language Technology Group, Computer Science Dept., University of Hamburg, Germany Web and Data Science Group, Computer Science Dept., University of Mannheim, Germany {panchenko,ruppert,biemann}@informatik.uni-hamburg.de {faralli,simone}@informatik.uni-mannheim.de Abstract Word sense induction from domain-specific corpora is a supposed to solve this problem. However, most approaches to word sense induction and disambiguation, e.g. (Sch¨utze, 1998; Li and Jurafsky, 2015; Bartunov et al., 2016), rely on clustering methods and dense vector representations that make a WSD model uninterpretable as compared to knowledge-based WSD methods. Interpretability of a statistical model is important as it lets us understand the reasons behind its predictions (Vellido et al., 2011; Freitas, 2014; Li et al., 2016). Interpretability of WSD models (1) lets a user understand why in the given context one observed a given sense (e.g., for educational applications); (2) performs a comprehensive analysis of correct and erroneous predictions, giving rise to improved disambiguation"
E17-1009,S13-2049,0,0.14673,"el is characterized by a list of sparse features ordered by relevance to the sense. Figure 2 (2) shows most salient dependency features to senses of the word “table”. These feature representations are obtained by aggregating features of sense cluster words. Experiments We use two lexical sample collections suitable for evaluation of unsupervised WSD systems. The first one is the Turk Bootstrap Word Sense Inventory (TWSI) dataset introduced by Biemann (2012). It is used for testing different configurations of our approach. The second collection, the SemEval 2013 word sense induction dataset by Jurgens and Klapaftis (2013), is used to compare our approach to existing systems. In both datasets, to measure WSD performance, induced senses are mapped to gold standard senses. In experiments with the TWSI dataset, the models were trained on the Wikipedia corpus4 while in experiments with the SemEval datasets models are trained on the ukWaC corpus (Baroni et al., 2009) for a fair comparison with other participants. In systems based on dense vector representations, there is no straightforward way to get the most salient features of a sense, which makes the analysis of learned representations problematic. 3. Disambiguat"
E17-1009,N16-1082,0,0.0684958,"n}@informatik.uni-hamburg.de {faralli,simone}@informatik.uni-mannheim.de Abstract Word sense induction from domain-specific corpora is a supposed to solve this problem. However, most approaches to word sense induction and disambiguation, e.g. (Sch¨utze, 1998; Li and Jurafsky, 2015; Bartunov et al., 2016), rely on clustering methods and dense vector representations that make a WSD model uninterpretable as compared to knowledge-based WSD methods. Interpretability of a statistical model is important as it lets us understand the reasons behind its predictions (Vellido et al., 2011; Freitas, 2014; Li et al., 2016). Interpretability of WSD models (1) lets a user understand why in the given context one observed a given sense (e.g., for educational applications); (2) performs a comprehensive analysis of correct and erroneous predictions, giving rise to improved disambiguation models. The contribution of this paper is an interpretable unsupervised knowledge-free WSD method. The novelty of our method is in (1) a technique to disambiguation that relies on induced inventories as a pivot for learning sense feature representations, (2) a technique for making induced sense representations interpretable by labeli"
E17-1009,P03-1054,0,0.0189642,"g them much less interpretable as compared to the knowledge-based models. Ruppert et al. (2015) proposed a system for visualising sense inventories derived in an unsupervised way using graph-based distributional semantics. Panchenko (2016) proposed a method for making sense inventory of word sense embeddings interpretable by mapping it to BabelNet. Dependency Features. These feature represents a word by a syntactic dependency such as “nn(•,writing)” or “prep at(sit,•)”, extracted from the Stanford Dependencies (De Marneffe et al., 2006) obtained with the the PCFG model of the Stanford parser (Klein and Manning, 2003). Weights are computed using the Local Mutual Information (LMI) (Evert, 2005). One word is represented with 1000 most significant features. Co-occurrence Features. This type of features represents a word by another word. We extract the list of words that significantly co-occur in a sentence with the target word in the input corpus based on the log-likelihood as word-feature weight (Dunning, 1993). Language Model Feature. This type of features are based on a trigram model with Kneser-Ney smoothing (Kneser and Ney, 1995). In particular, a word is represented by (1) right and left context words,"
E17-1009,W02-0811,0,0.0372423,"resting is that this approach is similar to ours, but instead of sparse representations the authors rely on word embeddings, making their approach less interpretable. Multiple designs of WSD systems were proposed (Agirre and Edmonds, 2007; Navigli, 2009). They vary according to the level of supervision and the amount of external knowledge used. Most current systems either make use of lexical resources and/or rely on an explicitly annotated sense corpus. Supervised approaches use a sense-labeled corpus to train a model, usually building one submodel per target word (Ng, 1997; Lee and Ng, 2002; Klein et al., 2002; Wee, 2010). The IMS system by Zhong and Ng (2010) provides an implementation of the supervised approach to WSD that yields state-of-the-art results. While supervised approaches demonstrate top performance in competitions, they require large amounts of senselabeled examples per target word. Knowledge-based approaches rely on a lexical resource that provides a sense inventory and features for disambiguation and vary from the classical Lesk (1986) algorithm that uses word definitions to the Babelfy (Moro et al., 2014) system that uses harnesses a multilingual lexicalsemantic network. Classical"
E17-1009,C12-1109,1,0.808148,"ervised approach to WSD that yields state-of-the-art results. While supervised approaches demonstrate top performance in competitions, they require large amounts of senselabeled examples per target word. Knowledge-based approaches rely on a lexical resource that provides a sense inventory and features for disambiguation and vary from the classical Lesk (1986) algorithm that uses word definitions to the Babelfy (Moro et al., 2014) system that uses harnesses a multilingual lexicalsemantic network. Classical examples of such approaches include (Banerjee and Pedersen, 2002; Pedersen et al., 2005; Miller et al., 2012). More recently, several methods were proposed to learn sense embeddings on the basis of the sense inventory of a lexical resource (Chen et al., 2014; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2015; Iacobacci et al., 2015; Nieto Pi˜na and Johansson, 2016). Unsupervised knowledge-free approaches use neither handcrafted lexical resources nor handannotated sense-labeled corpora. Instead, they induce word sense inventories automatically from corpora. Unsupervised WSD methods fall into two main categories: context clustering and word ego-network clustering. Context clustering approaches, e"
E17-1009,S13-2051,0,0.175444,"med by assigning the sense with the highest overlap between the instance’s context words and the words of the sense cluster. V´eronis (2004) compiles a corpus with contexts of polysemous nouns using a search engine. A word graph is built by drawing edges between co-occurring words in the gathered corpus, where edges below a certain similarity threshold were discarded. His HyperLex algorithm detects hubs of this graph, which are interpreted as word senses. Disambiguation is this experiment is performed by computing the distance between context words and hubs in this graph. Di Marco and Navigli (2013) presents a comprehensive study of several graph-based WSI methods including Chinese Whispers, HyperLex, curvature clustering (Dorow et al., 2005). Besides, 87 3 authors propose two novel algorithms: Balanced Maximum Spanning Tree Clustering and Squares (B-MST), Triangles and Diamonds (SquaT++). To construct graphs, authors use first-order and second-order relations extracted from a background corpus as well as keywords from snippets. This research goes beyond intrinsic evaluations of induced senses and measures impact of the WSI in the context of an information retrieval via clustering and di"
E17-1009,S15-2049,0,0.00630219,"re. Experiments show that our model performs on par with state-of-the-art word sense embeddings and other unsupervised systems while offering the possibility to justify its decisions in human-readable form. 1 Introduction A word sense disambiguation (WSD) system takes as input a target word t and its context C. The system returns an identifier of a word sense si from the word sense inventory {s1 , ..., sn } of t, where the senses are typically defined manually in advance. Despite significant progress in methodology during the two last decades (Ide and V´eronis, 1998; Agirre and Edmonds, 2007; Moro and Navigli, 2015), WSD is still not widespread in applications (Navigli, 2009), which indicates the need for further progress. The difficulty of the problem largely stems from the lack of domain-specific training data. A fixed sense inventory, such as the one of WordNet (Miller, 1995), may contain irrelevant senses for the given application and at the same time lack relevant domain-specific senses. 1 http://www.jobimtext.org/wsd 86 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 86–98, c Valencia, Spain, April 3-7, 2017."
E17-1009,W02-1006,0,0.0721617,"later method interesting is that this approach is similar to ours, but instead of sparse representations the authors rely on word embeddings, making their approach less interpretable. Multiple designs of WSD systems were proposed (Agirre and Edmonds, 2007; Navigli, 2009). They vary according to the level of supervision and the amount of external knowledge used. Most current systems either make use of lexical resources and/or rely on an explicitly annotated sense corpus. Supervised approaches use a sense-labeled corpus to train a model, usually building one submodel per target word (Ng, 1997; Lee and Ng, 2002; Klein et al., 2002; Wee, 2010). The IMS system by Zhong and Ng (2010) provides an implementation of the supervised approach to WSD that yields state-of-the-art results. While supervised approaches demonstrate top performance in competitions, they require large amounts of senselabeled examples per target word. Knowledge-based approaches rely on a lexical resource that provides a sense inventory and features for disambiguation and vary from the classical Lesk (1986) algorithm that uses word definitions to the Babelfy (Moro et al., 2014) system that uses harnesses a multilingual lexicalsemantic"
E17-1009,Q14-1019,0,0.377872,"l, usually building one submodel per target word (Ng, 1997; Lee and Ng, 2002; Klein et al., 2002; Wee, 2010). The IMS system by Zhong and Ng (2010) provides an implementation of the supervised approach to WSD that yields state-of-the-art results. While supervised approaches demonstrate top performance in competitions, they require large amounts of senselabeled examples per target word. Knowledge-based approaches rely on a lexical resource that provides a sense inventory and features for disambiguation and vary from the classical Lesk (1986) algorithm that uses word definitions to the Babelfy (Moro et al., 2014) system that uses harnesses a multilingual lexicalsemantic network. Classical examples of such approaches include (Banerjee and Pedersen, 2002; Pedersen et al., 2005; Miller et al., 2012). More recently, several methods were proposed to learn sense embeddings on the basis of the sense inventory of a lexical resource (Chen et al., 2014; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2015; Iacobacci et al., 2015; Nieto Pi˜na and Johansson, 2016). Unsupervised knowledge-free approaches use neither handcrafted lexical resources nor handannotated sense-labeled corpora. Instead, they induce word"
E17-1009,P10-1023,1,0.451602,"lar, we extract three types of features: Interpretable approaches. The need in methods that interpret results of opaque statistical models is widely recognised (Vellido et al., 2011; Vellido et al., 2012; Freitas, 2014; Li et al., 2016; Park et al., 2016). An interpretable WSD system is expected to provide (1) a human-readable sense inventory, (2) human-readable reasons why in a given context c a given sense si was detected. Lexical resources, such as WordNet, solve the first problem by providing manually-crafted definitions of senses, examples of usage, hypernyms, and synonyms. The BabelNet (Navigli and Ponzetto, 2010) integrates all these sense representations, adding to them links to external resources, such as Wikipedia, topical category labels, and images representing the sense. The unsupervised models listed above do not feature any of these representations making them much less interpretable as compared to the knowledge-based models. Ruppert et al. (2015) proposed a system for visualising sense inventories derived in an unsupervised way using graph-based distributional semantics. Panchenko (2016) proposed a method for making sense inventory of word sense embeddings interpretable by mapping it to Babel"
E17-1009,W16-1620,1,0.723784,"difficulty of the problem largely stems from the lack of domain-specific training data. A fixed sense inventory, such as the one of WordNet (Miller, 1995), may contain irrelevant senses for the given application and at the same time lack relevant domain-specific senses. 1 http://www.jobimtext.org/wsd 86 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 86–98, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics 2 Related Work 2014; Bartunov et al., 2016; Li and Jurafsky, 2015; Pelevina et al., 2016). Comparisons of the AdaGram (Bartunov et al., 2016) to (Neelakantan et al., 2014) on three SemEval word sense induction and disambiguation datasets show the advantage of the former. For this reason, we use AdaGram as a representative of the state-of-the-art word sense embeddings in our experiments. In addition, we compare to SenseGram, an alternative sense embedding based approach by Pelevina et al. (2016). What makes the comparison to the later method interesting is that this approach is similar to ours, but instead of sparse representations the authors rely on word embeddings, making their"
E17-1009,D14-1113,0,0.0333137,"ng data. A fixed sense inventory, such as the one of WordNet (Miller, 1995), may contain irrelevant senses for the given application and at the same time lack relevant domain-specific senses. 1 http://www.jobimtext.org/wsd 86 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 86–98, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics 2 Related Work 2014; Bartunov et al., 2016; Li and Jurafsky, 2015; Pelevina et al., 2016). Comparisons of the AdaGram (Bartunov et al., 2016) to (Neelakantan et al., 2014) on three SemEval word sense induction and disambiguation datasets show the advantage of the former. For this reason, we use AdaGram as a representative of the state-of-the-art word sense embeddings in our experiments. In addition, we compare to SenseGram, an alternative sense embedding based approach by Pelevina et al. (2016). What makes the comparison to the later method interesting is that this approach is similar to ours, but instead of sparse representations the authors rely on word embeddings, making their approach less interpretable. Multiple designs of WSD systems were proposed (Agirre"
E17-1009,W97-0323,0,0.288821,"son to the later method interesting is that this approach is similar to ours, but instead of sparse representations the authors rely on word embeddings, making their approach less interpretable. Multiple designs of WSD systems were proposed (Agirre and Edmonds, 2007; Navigli, 2009). They vary according to the level of supervision and the amount of external knowledge used. Most current systems either make use of lexical resources and/or rely on an explicitly annotated sense corpus. Supervised approaches use a sense-labeled corpus to train a model, usually building one submodel per target word (Ng, 1997; Lee and Ng, 2002; Klein et al., 2002; Wee, 2010). The IMS system by Zhong and Ng (2010) provides an implementation of the supervised approach to WSD that yields state-of-the-art results. While supervised approaches demonstrate top performance in competitions, they require large amounts of senselabeled examples per target word. Knowledge-based approaches rely on a lexical resource that provides a sense inventory and features for disambiguation and vary from the classical Lesk (1986) algorithm that uses word definitions to the Babelfy (Moro et al., 2014) system that uses harnesses a multilingu"
E17-1009,P15-1173,0,0.186255,"Missing"
E17-1009,W16-1401,0,0.0436871,"Missing"
E17-1009,P15-4018,1,0.84569,"-readable reasons why in a given context c a given sense si was detected. Lexical resources, such as WordNet, solve the first problem by providing manually-crafted definitions of senses, examples of usage, hypernyms, and synonyms. The BabelNet (Navigli and Ponzetto, 2010) integrates all these sense representations, adding to them links to external resources, such as Wikipedia, topical category labels, and images representing the sense. The unsupervised models listed above do not feature any of these representations making them much less interpretable as compared to the knowledge-based models. Ruppert et al. (2015) proposed a system for visualising sense inventories derived in an unsupervised way using graph-based distributional semantics. Panchenko (2016) proposed a method for making sense inventory of word sense embeddings interpretable by mapping it to BabelNet. Dependency Features. These feature represents a word by a syntactic dependency such as “nn(•,writing)” or “prep at(sit,•)”, extracted from the Stanford Dependencies (De Marneffe et al., 2006) obtained with the the PCFG model of the Stanford parser (Klein and Manning, 2003). Weights are computed using the Local Mutual Information (LMI) (Evert,"
E17-1009,J07-2002,0,0.101588,"Missing"
E17-1009,W12-0502,1,0.771556,"ated by the absence of meta-parameters and its comparable performance on the WSI task to the state-of-theart (Di Marco and Navigli, 2013). 2013) as it yields comparable performance on semantic similarity to state-of-the-art dense representations (Mikolov et al., 2013) compared on the WordNet as gold standard (Riedl, 2016), but is interpretable as word are represented by sparse interpretable features. Namely we use dependencybased features as, according to prior evaluations, this kind of features provides state-of-the-art semantic relatedness scores (Pad´o and Lapata, 2007; Van de Cruys, 2010; Panchenko and Morozova, 2012; Levy and Goldberg, 2014). First, features of each word are ranked using the LMI metric (Evert, 2005). Second, the word representations are pruned keeping 1000 most salient features per word and 1000 most salient words per feature. The pruning reduces computational complexity and noise. Finally, word similarities are computed as a number of common features for two words. This is again followed by a pruning step in which only the 200 most similar terms are kept to every word. The resulting word similarities are browsable online.2 Note that while words can be characterized with distributions ov"
E17-1009,J98-1004,0,0.194444,"Missing"
E17-1009,L16-1421,1,0.84598,"nually-crafted definitions of senses, examples of usage, hypernyms, and synonyms. The BabelNet (Navigli and Ponzetto, 2010) integrates all these sense representations, adding to them links to external resources, such as Wikipedia, topical category labels, and images representing the sense. The unsupervised models listed above do not feature any of these representations making them much less interpretable as compared to the knowledge-based models. Ruppert et al. (2015) proposed a system for visualising sense inventories derived in an unsupervised way using graph-based distributional semantics. Panchenko (2016) proposed a method for making sense inventory of word sense embeddings interpretable by mapping it to BabelNet. Dependency Features. These feature represents a word by a syntactic dependency such as “nn(•,writing)” or “prep at(sit,•)”, extracted from the Stanford Dependencies (De Marneffe et al., 2006) obtained with the the PCFG model of the Stanford parser (Klein and Manning, 2003). Weights are computed using the Local Mutual Information (LMI) (Evert, 2005). One word is represented with 1000 most significant features. Co-occurrence Features. This type of features represents a word by another"
E17-1009,C14-1016,0,0.0529438,"supervised WSD methods fall into two main categories: context clustering and word ego-network clustering. Context clustering approaches, e.g. (Pedersen and Bruce, 1997; Sch¨utze, 1998), represent an instance usually by a vector that characterizes its context, where the definition of context can vary greatly. These vectors of each instance are then clustered. Multi-prototype extensions of the skipgram model (Mikolov et al., 2013) that use no predefined sense inventory learn one embedding word vector per one word sense and are commonly fitted with a disambiguation mechanism (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., Word ego-network clustering methods (Lin, 1998; Pantel and Lin, 2002; Widdows and Dorow, 2002; Biemann, 2006; Hope and Keller, 2013) cluster graphs of words semantically related to the ambiguous word. An ego network consists of a single node (ego) together with the nodes they are connected to (alters) and all the edges among those alters (Everett and Borgatti, 2005). In our case, such a network is a local neighborhood of one word. Nodes of the ego-network can be (1) words semantically similar to the target word, as in our approach, or (2) context words relevant to the tar"
E17-1009,C02-1114,0,0.0171122,"text clustering approaches, e.g. (Pedersen and Bruce, 1997; Sch¨utze, 1998), represent an instance usually by a vector that characterizes its context, where the definition of context can vary greatly. These vectors of each instance are then clustered. Multi-prototype extensions of the skipgram model (Mikolov et al., 2013) that use no predefined sense inventory learn one embedding word vector per one word sense and are commonly fitted with a disambiguation mechanism (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., Word ego-network clustering methods (Lin, 1998; Pantel and Lin, 2002; Widdows and Dorow, 2002; Biemann, 2006; Hope and Keller, 2013) cluster graphs of words semantically related to the ambiguous word. An ego network consists of a single node (ego) together with the nodes they are connected to (alters) and all the edges among those alters (Everett and Borgatti, 2005). In our case, such a network is a local neighborhood of one word. Nodes of the ego-network can be (1) words semantically similar to the target word, as in our approach, or (2) context words relevant to the target, as in the UoS system (Hope and Keller, 2013). Graph edges represent semantic relations between words derived u"
E17-1009,P10-4014,0,0.0897347,"but instead of sparse representations the authors rely on word embeddings, making their approach less interpretable. Multiple designs of WSD systems were proposed (Agirre and Edmonds, 2007; Navigli, 2009). They vary according to the level of supervision and the amount of external knowledge used. Most current systems either make use of lexical resources and/or rely on an explicitly annotated sense corpus. Supervised approaches use a sense-labeled corpus to train a model, usually building one submodel per target word (Ng, 1997; Lee and Ng, 2002; Klein et al., 2002; Wee, 2010). The IMS system by Zhong and Ng (2010) provides an implementation of the supervised approach to WSD that yields state-of-the-art results. While supervised approaches demonstrate top performance in competitions, they require large amounts of senselabeled examples per target word. Knowledge-based approaches rely on a lexical resource that provides a sense inventory and features for disambiguation and vary from the classical Lesk (1986) algorithm that uses word definitions to the Babelfy (Moro et al., 2014) system that uses harnesses a multilingual lexicalsemantic network. Classical examples of such approaches include (Banerjee and P"
E17-1009,W97-0322,0,\N,Missing
E17-1056,D13-1018,1,0.87749,"ntrastMedium Algorithm: Taxonomy Induction From Noisy Knowledge Graphs With Just a Few Links Stefano Faralli1 , Alexander Panchenko2 , Chris Biemann2 and Simone Paolo Ponzetto1 1 Data and Web Science Group, University of Mannheim, Germany Language Technology Group, University of Hamburg, Germany {stefano,simone}@informatik.uni-mannheim.de {panchenko,biemann}@informatik.uni-hamburg.de 2 Abstract closed information extraction approaches (Dutta et al., 2014). The use of an encyclopedia-centric (e.g., Wikipedia-based) dictionary of entities leads to poor coverage of domain-specific terminologies (Faralli and Navigli, 2013). This can be alleviated by constructing knowledge bases of ever increasing coverage and complexity from the Web (Wu et al., 2012; Gupta et al., 2014; Dong et al., 2014) or by community efforts (Bollacker et al., 2008). However, the focus on large size and wide coverage at entity level has led all these resources to avoid the complementary problem of curating and maintaining a clean taxonomic backbone with as minimal supervision as possible. That is, no resource, to date, integrates structured information from existing wide-coverage knowledge graphs with empirical evidence from text for the ex"
E17-1056,S15-2151,1,0.925043,"Missing"
E17-1056,P99-1016,0,0.48042,"cit goal of building full-fledged taxonomies consisting of a clean and fully-connected directed acyclic graph (DAG). This is despite the fact that taxonomies have been known for a long time to provide valid tools to represent domain-specific knowledge with dozens of scientific, industrial and social applications (Glass and Vessey, 1995). In taxonomy induction, the required domain knowledge can be acquired with many different methods for hypernym extraction, ranging from simple lexical patterns (Hearst, 1992; Oakes, 2005; Kozareva and Hovy, 2010) to statistical and machine learning techniques (Caraballo, 1999; Agirre et al., 2000; Ritter et al., 2009; Velardi et al., 2013). Recent efforts, such as Microsoft’s Probase (Wu et al., 2012) or the WebIsaDB (Seitner et al., 2016) similarly focus on ‘local’ extraction of single hypernym relations, and do not address the problem of how to combine these single relations into a coherent taxonomy. When taxonomies are automatically acquired, their cleaning (also called “pruning”) becomes a mandatory step (Velardi et al., 2013). In this paper, we present ContrastMedium, an algorithm that transforms noisy semantic networks into full-fledged, clean taxonomies. Co"
E17-1056,N15-1151,0,0.0311339,"Mintz et al., 2009; Aprosio et al., 2013) or other KBs (Wang et al., 2012; Bryl and Bizer, 2014) for acquiring additional knowledge. Alternative approaches, in contrast, primarily rely on existing information from the KB itself (Socher et al., 2013; Nickel et al., 2016b) used as ground-truth to simultaneously learn continuous representations of KB concepts and relations, which are used to infer additional KB relations. Finally, Open Information Extraction methods looked at ways to extract large amounts of facts from Web-scale corpora in order to acquire open-domain KBs (Etzioni et al., 2011; Faruqui and Kumar, 2015, inter alia); In this paper, we focus on a different, yet complementary task, which is a necessary step when inducing novel KBs from scratch, namely extracting clean taxonomies from noisy knowlSome aspects of the proposed approach – namely, the propagation of the nodes’ weights through the graph, which we metaphorically represent as the flow of a contrast medium across nodes (Section 3.3) – are somewhat similar in spirit to spreading activation (Collins and Loftus, 1975) and random walks on graphs (Lov´asz, 1993) approaches. However, in contrast to spreading activation approaches we leverage"
E17-1056,S16-1206,1,0.913086,"oisy edges, the wrongly-acquired relations between unrelated concepts or out-of-domain relations, e.g., Jaguar Cars isa Feline; iii) cycles of hypernymy relations, such as those derived from counts over very large corpora (Seitner et al., 2016), e.g., jaguar (Panthera onca) → feline → animal → jaguar (Panthera onca). We accordingly define the task of extracting a clean taxonomy from a NKG as that of pruning the cycles, as well as the noisy edges and nodes, from the hypernymy subgraph T of G. 3.2 vised methods. To this end, we use the linked disambiguated distributional KBs from Faralli et al. (2016)1 , which are built in three steps: 1) Learning a JoBimText model. Initially, a sense inventory is created from a large text collection using the pipeline of the JoBimText project (Biemann and Riedl, 2013).2 The resulting structure contains disambiguated protoconcepts (i.e., senses), their similar and related terms, as well as aggregated contextual clues per proto-concept. 2) Disambiguation of related terms. Similar terms and hypernyms associated with a protoconcept are fully disambiguated based on the partial disambiguation from step (1). The result is a proto-conceptualization (PCZ), where a"
E17-1056,C92-2082,0,0.315937,"rmation from existing wide-coverage knowledge graphs with empirical evidence from text for the explicit goal of building full-fledged taxonomies consisting of a clean and fully-connected directed acyclic graph (DAG). This is despite the fact that taxonomies have been known for a long time to provide valid tools to represent domain-specific knowledge with dozens of scientific, industrial and social applications (Glass and Vessey, 1995). In taxonomy induction, the required domain knowledge can be acquired with many different methods for hypernym extraction, ranging from simple lexical patterns (Hearst, 1992; Oakes, 2005; Kozareva and Hovy, 2010) to statistical and machine learning techniques (Caraballo, 1999; Agirre et al., 2000; Ritter et al., 2009; Velardi et al., 2013). Recent efforts, such as Microsoft’s Probase (Wu et al., 2012) or the WebIsaDB (Seitner et al., 2016) similarly focus on ‘local’ extraction of single hypernym relations, and do not address the problem of how to combine these single relations into a coherent taxonomy. When taxonomies are automatically acquired, their cleaning (also called “pruning”) becomes a mandatory step (Velardi et al., 2013). In this paper, we present Contr"
E17-1056,L16-1056,1,0.855973,"e been known for a long time to provide valid tools to represent domain-specific knowledge with dozens of scientific, industrial and social applications (Glass and Vessey, 1995). In taxonomy induction, the required domain knowledge can be acquired with many different methods for hypernym extraction, ranging from simple lexical patterns (Hearst, 1992; Oakes, 2005; Kozareva and Hovy, 2010) to statistical and machine learning techniques (Caraballo, 1999; Agirre et al., 2000; Ritter et al., 2009; Velardi et al., 2013). Recent efforts, such as Microsoft’s Probase (Wu et al., 2012) or the WebIsaDB (Seitner et al., 2016) similarly focus on ‘local’ extraction of single hypernym relations, and do not address the problem of how to combine these single relations into a coherent taxonomy. When taxonomies are automatically acquired, their cleaning (also called “pruning”) becomes a mandatory step (Velardi et al., 2013). In this paper, we present ContrastMedium, an algorithm that transforms noisy semantic networks into full-fledged, clean taxonomies. ContrastMedium is able to identify the embedded taxonomy structure from a noisy knowledge graph without explicit human supervision such as, for instance, a set of manual"
E17-1056,P06-1101,0,0.34143,"mapping (Navigli and Ponzetto, 2012; Faralli et al., 2016, inter alia) or by relying on ground truth information from the Linguistic Linked Open Data cloud (Chiarcos et al., 2012). Knowledge Bases (KBs) can be created in many different ways depending on the availability of external resources and specific application needs. Recently, much work in Natural Language Processing focused on Knowledge Base Completion (Nickel et al., 2016a, KBC), the task of enriching and refining existing KBs. Many different methods have been explored for KBC, including exploitation of resources such as text corpora (Snow et al., 2006; Mintz et al., 2009; Aprosio et al., 2013) or other KBs (Wang et al., 2012; Bryl and Bizer, 2014) for acquiring additional knowledge. Alternative approaches, in contrast, primarily rely on existing information from the KB itself (Socher et al., 2013; Nickel et al., 2016b) used as ground-truth to simultaneously learn continuous representations of KB concepts and relations, which are used to infer additional KB relations. Finally, Open Information Extraction methods looked at ways to extract large amounts of facts from Web-scale corpora in order to acquire open-domain KBs (Etzioni et al., 2011;"
E17-1056,D10-1108,0,0.212701,"coverage knowledge graphs with empirical evidence from text for the explicit goal of building full-fledged taxonomies consisting of a clean and fully-connected directed acyclic graph (DAG). This is despite the fact that taxonomies have been known for a long time to provide valid tools to represent domain-specific knowledge with dozens of scientific, industrial and social applications (Glass and Vessey, 1995). In taxonomy induction, the required domain knowledge can be acquired with many different methods for hypernym extraction, ranging from simple lexical patterns (Hearst, 1992; Oakes, 2005; Kozareva and Hovy, 2010) to statistical and machine learning techniques (Caraballo, 1999; Agirre et al., 2000; Ritter et al., 2009; Velardi et al., 2013). Recent efforts, such as Microsoft’s Probase (Wu et al., 2012) or the WebIsaDB (Seitner et al., 2016) similarly focus on ‘local’ extraction of single hypernym relations, and do not address the problem of how to combine these single relations into a coherent taxonomy. When taxonomies are automatically acquired, their cleaning (also called “pruning”) becomes a mandatory step (Velardi et al., 2013). In this paper, we present ContrastMedium, an algorithm that transforms"
E17-1056,P09-1113,0,0.091549,"d Ponzetto, 2012; Faralli et al., 2016, inter alia) or by relying on ground truth information from the Linguistic Linked Open Data cloud (Chiarcos et al., 2012). Knowledge Bases (KBs) can be created in many different ways depending on the availability of external resources and specific application needs. Recently, much work in Natural Language Processing focused on Knowledge Base Completion (Nickel et al., 2016a, KBC), the task of enriching and refining existing KBs. Many different methods have been explored for KBC, including exploitation of resources such as text corpora (Snow et al., 2006; Mintz et al., 2009; Aprosio et al., 2013) or other KBs (Wang et al., 2012; Bryl and Bizer, 2014) for acquiring additional knowledge. Alternative approaches, in contrast, primarily rely on existing information from the KB itself (Socher et al., 2013; Nickel et al., 2016b) used as ground-truth to simultaneously learn continuous representations of KB concepts and relations, which are used to infer additional KB relations. Finally, Open Information Extraction methods looked at ways to extract large amounts of facts from Web-scale corpora in order to acquire open-domain KBs (Etzioni et al., 2011; Faruqui and Kumar,"
E17-1056,velardi-etal-2012-new,1,0.878279,"Missing"
E17-1056,J13-3007,1,0.916583,"a clean and fully-connected directed acyclic graph (DAG). This is despite the fact that taxonomies have been known for a long time to provide valid tools to represent domain-specific knowledge with dozens of scientific, industrial and social applications (Glass and Vessey, 1995). In taxonomy induction, the required domain knowledge can be acquired with many different methods for hypernym extraction, ranging from simple lexical patterns (Hearst, 1992; Oakes, 2005; Kozareva and Hovy, 2010) to statistical and machine learning techniques (Caraballo, 1999; Agirre et al., 2000; Ritter et al., 2009; Velardi et al., 2013). Recent efforts, such as Microsoft’s Probase (Wu et al., 2012) or the WebIsaDB (Seitner et al., 2016) similarly focus on ‘local’ extraction of single hypernym relations, and do not address the problem of how to combine these single relations into a coherent taxonomy. When taxonomies are automatically acquired, their cleaning (also called “pruning”) becomes a mandatory step (Velardi et al., 2013). In this paper, we present ContrastMedium, an algorithm that transforms noisy semantic networks into full-fledged, clean taxonomies. ContrastMedium is able to identify the embedded taxonomy structure"
E17-1056,D11-1142,0,\N,Missing
E17-2087,W11-2501,0,0.385952,"; Weeds et al., 2014; Levy et al., 2015; Vylomova et al., 2016). HypeNET (Shwartz et al., 2016) is a hybrid approach which is also based on a classifier, but in addition to two word embeddings a third vector is used. It represents path-based syntactic information encoded using an LSTM model (Hochreiter and Schmidhuber, 1997). Their results significantly outperform the ones from previous pathbased work of Snow et al. (2004). An inherent limitation of classification-based approaches is that they require a list of candidate words pairs. While these are given in evaluation datasets such as BLESS (Baroni and Lenci, 2011), a corpus-wide classification of relations would need to classify all possible word pairs, which is computationally expensive for large vocabularies. Besides, Levy et al. (2015) discovered a tendency to lexical memorization of such approaches hampering the generalization. Methods based on projection learning take one hyponym word vector as an input and output a word vector in a topological vicinity of hypernym word vectors. Scaling this to the vocabulary, there is only one such operation per word. Mikolov et al. (2013a) used projection learning for bilingual word translation. Vuli´c and Korho"
E17-2087,S12-1012,0,0.0418583,"on the model of Fu et al. (2014), our regularizers can be straightforwardly integrated into the model of Yamane et al. (2016). Two branches of methods relying on distributional representations emerged so far. Methods based on word pair classification take an ordered pair of word embeddings (a candidate hyponym-hypernym pair) as an input and output a binary label indicating a presence of the hypernymy relation between the words. Typically, a binary classifier is trained on concatenation or subtraction of the input embeddings, cf. (Roller et al., 2014). Further examples of such methods include (Lenci and Benotto, 2012; Weeds et al., 2014; Levy et al., 2015; Vylomova et al., 2016). HypeNET (Shwartz et al., 2016) is a hybrid approach which is also based on a classifier, but in addition to two word embeddings a third vector is used. It represents path-based syntactic information encoded using an LSTM model (Hochreiter and Schmidhuber, 1997). Their results significantly outperform the ones from previous pathbased work of Snow et al. (2004). An inherent limitation of classification-based approaches is that they require a list of candidate words pairs. While these are given in evaluation datasets such as BLESS ("
E17-2087,P14-1113,0,0.405509,"ry Ustalov† , Nikolay Arefyev§ , Chris Biemann‡ , and Alexander Panchenko‡ † Ural Federal University, Institute of Natural Sciences and Mathematics, Russia Moscow State University, Faculty of Computational Mathematics and Cybernetics, Russia ‡ University of Hamburg, Deptartment of Informatics, Language Technology Group, Germany dmitry.ustalov@urfu.ru, narefjev@cs.msu.ru {biemann,panchenko}@informatik.uni-hamburg.de § Abstract The contribution of this paper is a novel approach for hypernymy extraction based on projection learning. Namely, we present an improved version of the model proposed by Fu et al. (2014), which makes use of both positive and negative training instances enforcing the asymmetry of the projection. The proposed model is generic and could be straightforwardly used in other relation extraction tasks where both positive and negative training samples are available. Finally, we are the first to successfully apply projection learning for hypernymy extraction in a morphologically rich language. An implementation of our approach and the pre-trained models are available online.1 We present a new approach to extraction of hypernyms based on projection learning and word embeddings. In contr"
E17-2087,N15-1098,1,0.924597,"l language processing tasks ranging from construction of taxonomies (Snow et al., 2006; Panchenko et al., 2016a) to query expansion (Gong et al., 2005) and question answering (Zhou et al., 2013). Automatic extraction of hypernyms from text has been an active area of research since manually constructed high-quality resources featuring hypernyms, such as WordNet (Miller, 1995), are not available for many domain-language pairs. The drawback of pattern-based approaches to hypernymy extraction (Hearst, 1992) is their sparsity. Approaches that rely on the classification of pairs of word embeddings (Levy et al., 2015) aim to tackle this shortcoming, but they require candidate hyponym-hypernym pairs. We explore a hypernymy extraction approach that requires no candidate pairs. Instead, the method performs prediction of a hypernym embedding on the basis of a hyponym embedding. 1 http://github.com/nlpub/projlearn 543 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 543–550, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics experiments. Nayak (2015) performed evaluations of distributional h"
E17-2087,goldhahn-etal-2012-building,0,0.0174841,"distinct vocabulary to avoid the lexical overfitting. This results in 25 067 training, 8 192 validation, and 8 310 test examples. The validation and test sets contain hypernyms from Wiktionary, while the training set is composed of hypernyms and synonyms coming from both sources. 4.3 4 Experiment 2: The English Language We performed the evaluation on two datasets. EVALution Dataset. In this evaluation, word embeddings were trained on a 6.3 billion token text collection composed of Wikipedia, ukWaC (Ferraresi et al., 2008), Gigaword (Graff, 2003), and news corpora from the Leipzig Collection (Goldhahn et al., 2012). We used the skipgram model with the context window size of 8 tokens and 300-dimensional vectors. We use the EVALution dataset (Santus et al., 2015) for training and testing the model, composed of 1 449 hypernyms and 520 synonyms, where hypernyms are split into 944 training, 65 validation and 440 test pairs. Similarly to the first experiment, we extracted extra training hypernyms using the Hearst patterns, but in contrast to Russian, they did not improve the results significantly, so we left them out for English. A reason for such difference could be the more complex morphological system of R"
E17-2087,C92-2082,0,0.235916,"it is natural to use both positive and negative training examples in supervised relation extraction, the impact of negative examples on hypernym prediction was not studied so far. In this paper, we show that explicit negative examples used for regularization of the model significantly improve performance compared to the stateof-the-art approach of Fu et al. (2014) on three datasets from different languages. 1 2 Related Work Path-based methods for hypernymy extraction rely on sentences where both hyponym and hypernym co-occur in characteristic contexts, e.g., “such cars as Mercedes and Audi”. Hearst (1992) proposed to use hand-crafted lexical-syntactic patterns to extract hypernyms from such contexts. Snow et al. (2004) introduced a method for learning patterns automatically based on a set of seed hyponym-hypernym pairs. Further examples of path-based approaches include (Tjong Kim Sang and Hofmann, 2009) and (Navigli and Velardi, 2010). The inherent limitation of the path-based methods leading to sparsity issues is that hyponym and hypernym have to co-occur in the same sentence. Methods based on distributional vectors, such as those generated using the word2vec toolkit (Mikolov et al., 2013b),"
E17-2087,P10-1134,0,0.127561,"stateof-the-art approach of Fu et al. (2014) on three datasets from different languages. 1 2 Related Work Path-based methods for hypernymy extraction rely on sentences where both hyponym and hypernym co-occur in characteristic contexts, e.g., “such cars as Mercedes and Audi”. Hearst (1992) proposed to use hand-crafted lexical-syntactic patterns to extract hypernyms from such contexts. Snow et al. (2004) introduced a method for learning patterns automatically based on a set of seed hyponym-hypernym pairs. Further examples of path-based approaches include (Tjong Kim Sang and Hofmann, 2009) and (Navigli and Velardi, 2010). The inherent limitation of the path-based methods leading to sparsity issues is that hyponym and hypernym have to co-occur in the same sentence. Methods based on distributional vectors, such as those generated using the word2vec toolkit (Mikolov et al., 2013b), aim to overcome this sparsity issue as they require no hyponymhypernym co-occurrence in a sentence. Such methods take representations of individual words as an input to predict relations between them. Introduction Hypernyms are useful in many natural language processing tasks ranging from construction of taxonomies (Snow et al., 2006;"
E17-2087,heylen-etal-2008-modelling,0,0.110215,"e-projection of the hyponym vector (xΦΦ), we also tested two regularizers without re-projection, denoted as xΦ. The neighbor regularizer in this variation is defined as follows: X 1 R= (xΦ · z)2 . |N | (x,z)∈N Linguistic Constraints via Regularization In our case, this regularizer penalizes relatedness of the predicted hypernym xΦ to the synonym z. The asymmetric regularizer without re-projection is defined in a similar way. The nearest neighbors generated using distributional word vectors tend to contain a mixture of synonyms, hypernyms, co-hyponyms and other related words (Wandmacher, 2005; Heylen et al., 2008; Panchenko, 2011). In order to explicitly provide examples of undesired relations to the model, we propose two improved versions of the baseline model: asymmetric regularization that uses inverted relations as negative examples, and neighbor regularization that uses relations of other types as negative examples. For that, we add a regularization term to the loss function: 1 X Φ∗ = arg min kxΦ − yk2 + λR, Φ |P| 3.3 Training of the Models To learn parameters of the considered models we used the Adam method (Kingma and Ba, 2014) with the default meta-parameters as implemented in the TensorFlow f"
E17-2087,S15-1021,0,0.0385093,"neighbor regularization requires explicit negative examples, while asymmetric regularization does not. to English. Therefore, extra training samples are needed for Russian (embeddings of Russian were trained on a non-lemmatized corpus). Combined Dataset. To show the robustness of our approach across configurations, this dataset has more training instances, different embeddings, and both synonyms and co-hyponyms as negative samples. We used hypernyms, synonyms and cohyponyms from the four commonly used datasets: EVALution, BLESS (Baroni and Lenci, 2011), ROOT09 (Santus et al., 2016) and K&H+N (Necsulescu et al., 2015).The obtained 14 528 relations were split into 9 959 training, 1 631 validation and 1 625 test hypernyms; 1 313 synonyms and cohyponyms were used as negative samples. We used the standard 300-dimensional embeddings trained on the 100 billion tokens Google News corpus (Mikolov et al., 2013b). 5 Conclusion In this study, we presented a new model for extraction of hypernymy relations based on the projection of distributional word vectors. The model incorporates information about explicit negative training instances represented by relations of other types, such as synonyms and co-hyponyms, and enf"
E17-2087,P16-1226,0,0.287855,"Missing"
E17-2087,S16-1206,1,0.871651,"Missing"
E17-2087,P06-1101,0,0.0605713,"and Velardi, 2010). The inherent limitation of the path-based methods leading to sparsity issues is that hyponym and hypernym have to co-occur in the same sentence. Methods based on distributional vectors, such as those generated using the word2vec toolkit (Mikolov et al., 2013b), aim to overcome this sparsity issue as they require no hyponymhypernym co-occurrence in a sentence. Such methods take representations of individual words as an input to predict relations between them. Introduction Hypernyms are useful in many natural language processing tasks ranging from construction of taxonomies (Snow et al., 2006; Panchenko et al., 2016a) to query expansion (Gong et al., 2005) and question answering (Zhou et al., 2013). Automatic extraction of hypernyms from text has been an active area of research since manually constructed high-quality resources featuring hypernyms, such as WordNet (Miller, 1995), are not available for many domain-language pairs. The drawback of pattern-based approaches to hypernymy extraction (Hearst, 1992) is their sparsity. Approaches that rely on the classification of pairs of word embeddings (Levy et al., 2015) aim to tackle this shortcoming, but they require candidate hyponym-"
E17-2087,W09-1122,0,0.0140739,"performance compared to the stateof-the-art approach of Fu et al. (2014) on three datasets from different languages. 1 2 Related Work Path-based methods for hypernymy extraction rely on sentences where both hyponym and hypernym co-occur in characteristic contexts, e.g., “such cars as Mercedes and Audi”. Hearst (1992) proposed to use hand-crafted lexical-syntactic patterns to extract hypernyms from such contexts. Snow et al. (2004) introduced a method for learning patterns automatically based on a set of seed hyponym-hypernym pairs. Further examples of path-based approaches include (Tjong Kim Sang and Hofmann, 2009) and (Navigli and Velardi, 2010). The inherent limitation of the path-based methods leading to sparsity issues is that hyponym and hypernym have to co-occur in the same sentence. Methods based on distributional vectors, such as those generated using the word2vec toolkit (Mikolov et al., 2013b), aim to overcome this sparsity issue as they require no hyponymhypernym co-occurrence in a sentence. Such methods take representations of individual words as an input to predict relations between them. Introduction Hypernyms are useful in many natural language processing tasks ranging from construction o"
E17-2087,P16-1024,0,0.0636052,"Missing"
E17-2087,W11-2502,1,0.771569,"yponym vector (xΦΦ), we also tested two regularizers without re-projection, denoted as xΦ. The neighbor regularizer in this variation is defined as follows: X 1 R= (xΦ · z)2 . |N | (x,z)∈N Linguistic Constraints via Regularization In our case, this regularizer penalizes relatedness of the predicted hypernym xΦ to the synonym z. The asymmetric regularizer without re-projection is defined in a similar way. The nearest neighbors generated using distributional word vectors tend to contain a mixture of synonyms, hypernyms, co-hyponyms and other related words (Wandmacher, 2005; Heylen et al., 2008; Panchenko, 2011). In order to explicitly provide examples of undesired relations to the model, we propose two improved versions of the baseline model: asymmetric regularization that uses inverted relations as negative examples, and neighbor regularization that uses relations of other types as negative examples. For that, we add a regularization term to the loss function: 1 X Φ∗ = arg min kxΦ − yk2 + λR, Φ |P| 3.3 Training of the Models To learn parameters of the considered models we used the Adam method (Kingma and Ba, 2014) with the default meta-parameters as implemented in the TensorFlow framework (Abadi et"
E17-2087,P16-1158,0,0.141805,"htforwardly integrated into the model of Yamane et al. (2016). Two branches of methods relying on distributional representations emerged so far. Methods based on word pair classification take an ordered pair of word embeddings (a candidate hyponym-hypernym pair) as an input and output a binary label indicating a presence of the hypernymy relation between the words. Typically, a binary classifier is trained on concatenation or subtraction of the input embeddings, cf. (Roller et al., 2014). Further examples of such methods include (Lenci and Benotto, 2012; Weeds et al., 2014; Levy et al., 2015; Vylomova et al., 2016). HypeNET (Shwartz et al., 2016) is a hybrid approach which is also based on a classifier, but in addition to two word embeddings a third vector is used. It represents path-based syntactic information encoded using an LSTM model (Hochreiter and Schmidhuber, 1997). Their results significantly outperform the ones from previous pathbased work of Snow et al. (2004). An inherent limitation of classification-based approaches is that they require a list of candidate words pairs. While these are given in evaluation datasets such as BLESS (Baroni and Lenci, 2011), a corpus-wide classification of relati"
E17-2087,C14-1097,0,0.324294,"Missing"
E17-2087,2005.jeptalnrecital-recital.1,0,0.178527,"ve, that rely on re-projection of the hyponym vector (xΦΦ), we also tested two regularizers without re-projection, denoted as xΦ. The neighbor regularizer in this variation is defined as follows: X 1 R= (xΦ · z)2 . |N | (x,z)∈N Linguistic Constraints via Regularization In our case, this regularizer penalizes relatedness of the predicted hypernym xΦ to the synonym z. The asymmetric regularizer without re-projection is defined in a similar way. The nearest neighbors generated using distributional word vectors tend to contain a mixture of synonyms, hypernyms, co-hyponyms and other related words (Wandmacher, 2005; Heylen et al., 2008; Panchenko, 2011). In order to explicitly provide examples of undesired relations to the model, we propose two improved versions of the baseline model: asymmetric regularization that uses inverted relations as negative examples, and neighbor regularization that uses relations of other types as negative examples. For that, we add a regularization term to the loss function: 1 X Φ∗ = arg min kxΦ − yk2 + λR, Φ |P| 3.3 Training of the Models To learn parameters of the considered models we used the Adam method (Kingma and Ba, 2014) with the default meta-parameters as implemente"
E17-2087,W15-4208,0,0.0426051,"est sets contain hypernyms from Wiktionary, while the training set is composed of hypernyms and synonyms coming from both sources. 4.3 4 Experiment 2: The English Language We performed the evaluation on two datasets. EVALution Dataset. In this evaluation, word embeddings were trained on a 6.3 billion token text collection composed of Wikipedia, ukWaC (Ferraresi et al., 2008), Gigaword (Graff, 2003), and news corpora from the Leipzig Collection (Goldhahn et al., 2012). We used the skipgram model with the context window size of 8 tokens and 300-dimensional vectors. We use the EVALution dataset (Santus et al., 2015) for training and testing the model, composed of 1 449 hypernyms and 520 synonyms, where hypernyms are split into 944 training, 65 validation and 440 test pairs. Similarly to the first experiment, we extracted extra training hypernyms using the Hearst patterns, but in contrast to Russian, they did not improve the results significantly, so we left them out for English. A reason for such difference could be the more complex morphological system of Russian, where each word has more morphological variants compared Discussion of Results. Figure 1 (left) shows performance of the three projection lea"
E17-2087,C14-1212,0,0.312339,"(2014), our regularizers can be straightforwardly integrated into the model of Yamane et al. (2016). Two branches of methods relying on distributional representations emerged so far. Methods based on word pair classification take an ordered pair of word embeddings (a candidate hyponym-hypernym pair) as an input and output a binary label indicating a presence of the hypernymy relation between the words. Typically, a binary classifier is trained on concatenation or subtraction of the input embeddings, cf. (Roller et al., 2014). Further examples of such methods include (Lenci and Benotto, 2012; Weeds et al., 2014; Levy et al., 2015; Vylomova et al., 2016). HypeNET (Shwartz et al., 2016) is a hybrid approach which is also based on a classifier, but in addition to two word embeddings a third vector is used. It represents path-based syntactic information encoded using an LSTM model (Hochreiter and Schmidhuber, 1997). Their results significantly outperform the ones from previous pathbased work of Snow et al. (2004). An inherent limitation of classification-based approaches is that they require a list of candidate words pairs. While these are given in evaluation datasets such as BLESS (Baroni and Lenci, 20"
E17-2087,L16-1722,0,\N,Missing
E17-2087,C16-1176,0,\N,Missing
F12-3003,N09-1003,0,0.0648678,"Missing"
F12-3003,W11-2501,0,0.0421236,"Missing"
F12-3003,J06-1003,0,0.184015,"Missing"
F12-3003,W03-0415,0,0.0589458,"Missing"
F12-3003,W02-1029,0,0.0936964,"Missing"
F12-3003,W02-0908,0,0.0968406,"Missing"
F12-3003,C92-2082,0,0.0706533,"Missing"
F12-3003,heylen-etal-2008-modelling,0,0.0367334,"Missing"
F12-3003,P08-1048,0,0.036131,"Missing"
F12-3003,P98-2127,0,0.0641931,"Missing"
F12-3003,H93-1061,0,0.0164735,"Missing"
F12-3003,W11-2502,1,0.813782,"Missing"
F12-3003,W06-2501,0,0.0991191,"Missing"
F12-3003,N07-2052,0,0.0449324,"Missing"
F12-3003,zesch-etal-2008-extracting,0,0.0600265,"Missing"
F12-3003,O97-1002,0,\N,Missing
F12-3003,P94-1019,0,\N,Missing
F12-3003,C98-2122,0,\N,Missing
F13-2031,C92-2082,0,0.0547496,"Missing"
F13-2031,P98-2127,0,0.416226,"Missing"
F13-2031,zesch-etal-2008-extracting,0,0.0881463,"Missing"
J19-3002,2016.gwc-1.10,1,0.89418,"where nodes are words and edges are synonymy relationships. Synsets represent word senses and are building blocks of thesauri and lexical ontologies, such as WordNet (Fellbaum 1998). These resources are crucial for many NLP applications that require common sense reasoning, such as information retrieval (Gong, Cheang, and Hou U 2005), sentiment analysis (Montejo-R´aez et al. 2014), and question answering (Kwok, Etzioni, and Weld 2001; Zhou et al. 2013). For most languages, no manually constructed resource is available that is comparable to the English WordNet in terms of coverage and quality (Braslavski et al. 2016). For instance, Kiselev, Porshnev, and Mukhin (2015) present a comparative analysis of lexical resources available for the Russian language, concluding that there is no resource compared with WordNet in terms of completeness and availability for Russian. This lack of linguistic resources for many languages strongly motivates the development of new methods for automatic construction of WordNet-like resources. In this section, we apply WATSET for unsupervised synset induction from a synonymy graph and compare it with state-of-the-art graph clustering algorithms run on the same task. 441 Computat"
J19-3002,E17-2036,0,0.0177047,"an observe a Zipfian-like power-law (Zipf 1949) distribution with a few clusters, such as artifact and person, accounting for a large fraction of all nouns in the resource. Overall, in this experiment we decided to focus on nouns, as the input distributional thesauri used in this experiment (as presented in Section 6.2) are most studied for modeling of noun semantics (Panchenko et al. 2016b). The WordNet supersenses were applied later also for word sense disambiguation as a system of broad sense labels (Flekova and Gurevych 2016). For BabelNet, there is a similar data set called BabelDomains (Camacho-Collados and Navigli 2017) produced by automatically labeling BabelNet synsets with 32 different domains based on the topics of Wikipedia featured articles. Despite the larger size, however, BabelDomains provides only a silver standard (being semi-automatically created). We thus opt in the following to use WordNet supersenses only, because they provide instead a gold standard created by human experts. 6.1.2 Flat Cuts of the WordNet Taxonomy. The second type of semantic classes used in our study are more semantically specific and defined as subtrees of WordNet at some fixed 462 Ustalov et al. WATSET: Local-Global Graph"
J19-3002,N13-1104,0,0.067555,"Missing"
J19-3002,W03-1022,0,0.139544,"tly. Examples of concrete semantic classes include sets of animals (dog, cat, . . . ), vehicles (car, motorcycle, . . . ), and fruit trees (apple tree, peach tree, . . . ). In this experiment, we use a gold standard derived from a reference lexicographical database, namely, WordNet (Fellbaum 1998). 35 The examples are from the file triw2v-watset-n30-top-top-triples.txt is available in the “Downloads” section of our GitHub repository at https://github.com/uhh-lt/triframes. 461 Computational Linguistics Volume 45, Number 3 Figure 14 A summary of the noun semantic classes in WordNet supersenses (Ciaramita and Johnson 2003). This allows us to benchmark the ability of WATSET to reconstruct the semantic lexicon of such a reliable reference resource that has been widely used in NLP for many decades. 6.1.1 WordNet Supersenses. The first data set used in our experiments consists of 26 broad semantic classes, also known as supersenses in the literature (Ciaramita and Johnson 2003): person, communication, artifact, act, group, food, cognition, possession, location, substance, state, time, attribute, object, process, tops, phenomenon, event, quantity, motive, animal, body, feeling, shape, plant, and relation. This syste"
J19-3002,N16-1172,0,0.0539531,"Missing"
J19-3002,E17-2028,0,0.0465007,"Missing"
J19-3002,J14-1002,0,0.075833,"Missing"
J19-3002,de-marneffe-etal-2006-generating,0,0.0378146,"Missing"
J19-3002,J13-3008,0,0.072588,"Missing"
J19-3002,E03-1020,0,0.152312,"of networks of linguistic data, such as word co-occurrence graphs, especially those that were used for induction of linguistic structures such as word senses. Markov Clustering (MCL; van Dongen 2000) is a hard clustering algorithm, that is, a method that partitions nodes of the graph in a set of disjoint clusters. This method is based on simulation of stochastic flow in graphs. MCL simulates random walks within a graph by the alternation of two operators, called expansion and inflation, which recompute the class labels. Notably, it has been successfully used for the word sense induction task (Dorow and Widdows 2003). Chinese Whispers (CW; Biemann 2006, 2012) is a hard clustering algorithm for weighted graphs, which can be considered as a special case of MCL with a simplified class update step. At each iteration, the labels of all the nodes are updated according to the majority of labels among the neighboring nodes. The algorithm has a hyperparameter that controls graph weights, which can be set to three values: (1) CWtop sums 425 Computational Linguistics Volume 45, Number 3 over the neighborhood’s classes; (2) CWlin downgrades the influence of a neighboring node by its degree; or (3) CWlog by the logari"
J19-3002,P18-1128,0,0.152993,"443 Computational Linguistics Volume 45, Number 3 Table 7 Statistics of the gold standard data sets used in our experiments. Resource Language # words # synsets # pairs WordNet BabelNet English 148,730 11,710,137 117,659 6,667,855 152,254 28,822,400 RuWordNet YARN Russian 110,242 9,141 49,492 2,210 278,381 48,291 toolkit (Seabold and Perktold 2010).19 Since the hypothesis tested by the McNemar’s test is whether the results from both algorithms are similar against the alternative that they are not, we use the p-value of this test to assess the significance of the difference between F1 -scores (Dror et al. 2018). We consider the performance of one algorithm to be higher than the performance of another if its F1 -score is larger and the corresponding p-value is smaller than a significance level of 0.01. Gold Standards. We conduct our evaluation on four lexical semantic resources for two different natural languages. Statistics of the gold standard data sets are present in Table 7. We report the number of lexical units (# words), synsets (# synsets), and the generated synonymy pairs (# pairs). We use WordNet,20 a popular English lexical database constructed by expert lexicographers (Fellbaum 1998). Word"
J19-3002,N15-1184,0,0.0489558,"eights based on the cosine of the angle between Skip-Gram word vectors (Mikolov et al. 2013), we should note that such an approach assigns high values of similarity not just to synonymous words, but to antonymous and generally any lexically related words. This is a common problem with lexical embedding spaces, which we tried to evade by explicitly using a synonymy dictionary as an input. For example, “audio play” and “radio play,” or “accusative” and “oblique,” are semantically related expressions, but really not synonyms. Such a problem can be addressed using techniques such as retrofitting (Faruqui et al. 2015) and contextualization (Peters et al. 2018). However, one limitation of all the approaches considered in this section is the dependence on the completeness of the input dictionary of synonyms. In some parts of the input synonymy graph, important bridges between words can be missing, leading to smaller-than-desired synsets. A promising extension of the present methodology is using distributional models to enhance connectivity of the graph by cautiously adding extra relationships (Ustalov et al. 2017). Cross-Resource Evaluation. In order to estimate the upper bound of precision, recall, and F1 -"
J19-3002,P16-1191,0,0.120597,"Missing"
J19-3002,J02-3001,0,0.346615,"pairs over a certain threshold are added to output synsets. We compare this approach to five other state-of-the-art graph clustering algorithms described in Section 2.1 as the baselines. 2.3 Semantic Frame Induction Frame Semantics was originally introduced by Fillmore (1982) and further developed in the FrameNet project (Baker, Fillmore, and Lowe 1998). FrameNet is a lexical resource composed of a collection of semantic frames, relationships between them, and a corpus of frame occurrences in text. This annotated corpus gave rise to the development of frame parsers using supervised learning (Gildea and Jurafsky 2002; Erk and Pado´ 2006; Das et al. 2014, inter alia), as well as its application to a wide range of tasks, ranging from answer extraction in Question Answering (Shen and Lapata 2007) and Textual Entailment (Burchardt et al. 2009; Ben Aharon, Szpektor, and Dagan 2010). However, frame-semantic resources are arguably expensive and time-consuming to build because of difficulties in defining the frames, their granularity and domain, as well as the complexity of the construction and annotation tasks. Consequently, such resources exist only for a few languages (Boas 2009) and even English is lacking do"
J19-3002,goldhahn-etal-2012-building,0,0.0267265,"Missing"
J19-3002,E12-1059,0,0.186249,"Manning 2014), there are several approaches that produce word sense embeddings. Multi-prototype extensions of the Skip-Gram model (Mikolov et al. 2013) that use no predefined sense inventory learn one embedding word vector per one word sense and are commonly fitted with a disambiguation mechanism (Huang et al. 2012; Apidianaki and Sagot 2014; 426 Ustalov et al. WATSET: Local-Global Graph Clustering with Applications Neelakantan et al. 2014; Tian et al. 2014; Li and Jurafsky 2015; Bartunov et al. 2016; Cocos and Callison-Burch 2016; Pelevina et al. 2016; Thomason and Mooney 2017). Huang et al. (2012) introduced multiple word prototypes for dense vector representations (embeddings). Their approach is based on a neural network architecture; during training, all contexts of the word are clustered. Apidianaki and Sagot (2014) use an aligned parallel corpus and WordNet for English to perform cross-lingual word sense disambiguation to produce French synsets. However, Cocos and Callison-Burch (2016) showed that it is possible to successfully perform a monolingual word sense induction using only such a paraphrase corpus as Paraphrase Database (Pavlick et al. 2015). Tian et al. (2014) introduced a"
J19-3002,Q16-1015,0,0.0324022,"Missing"
J19-3002,C92-2082,0,0.235054,"Information about semantic classes, in turn, has been shown to benefit such high-level NLP tasks as coreference (Ng 2007). Induction of semantic classes as a research direction in the field of NLP starts, to the best of our knowledge, with Lin and Pantel (2001), where sets of similar words are clustered into concepts. This approach performs a hard clustering and does not label clusters, but these drawbacks are addressed by Pantel and Lin (2002), where words can belong to several clusters, thus representing senses. Pantel and Ravichandran (2004) aggregate hypernyms per cluster, which come from Hearst (1992) patterns. Pattern-based approaches were further developed using 3 https://tac.nist.gov/2010/Summarization. 429 Computational Linguistics Volume 45, Number 3 graph-based methods using a PageRank-based weighting (Kozareva, Riloff, and Hovy 2008), random walks (Talukdar et al. 2008), or heuristic scoring (Qadir et al. 2015). Other approaches use probabilistic graphical models, such as the ones proposed by Ritter, Mausam, and Etzioni (2010) and Hovy et al. (2011). To ensure the overall quality of extraction pattern with minimal supervision, Thelen and Riloff (2002) explored a bootstrapping approa"
J19-3002,S13-2113,0,0.0987206,"mann 2006, 2012) is a hard clustering algorithm for weighted graphs, which can be considered as a special case of MCL with a simplified class update step. At each iteration, the labels of all the nodes are updated according to the majority of labels among the neighboring nodes. The algorithm has a hyperparameter that controls graph weights, which can be set to three values: (1) CWtop sums 425 Computational Linguistics Volume 45, Number 3 over the neighborhood’s classes; (2) CWlin downgrades the influence of a neighboring node by its degree; or (3) CWlog by the logarithm of its degree. MaxMax (Hope and Keller 2013a) is a fuzzy clustering algorithm particularly designed for the word sense induction task. In a nutshell, pairs of nodes are grouped if they have a maximal mutual affinity. The algorithm starts by converting the undirected input graph into a directed graph by keeping the maximal affinity nodes of each node. Next, all nodes are marked as root nodes. Finally, for each root node, the following procedure is repeated: All transitive children of this root form a cluster and the roots are marked as non-root nodes; a root node together with all its transitive children form a fuzzy cluster. Clique Per"
J19-3002,P11-1147,0,0.0258674,"Missing"
J19-3002,P12-1092,0,0.0191533,"arse tf–idf vectors are clustered, using a parametric method fixing the same number of senses for all words. Sense vectors are centroids of the clusters. Whereas most dense word vector models represent a word with a single vector and thus conflate senses (Mikolov et al. 2013; Pennington, Socher, and Manning 2014), there are several approaches that produce word sense embeddings. Multi-prototype extensions of the Skip-Gram model (Mikolov et al. 2013) that use no predefined sense inventory learn one embedding word vector per one word sense and are commonly fitted with a disambiguation mechanism (Huang et al. 2012; Apidianaki and Sagot 2014; 426 Ustalov et al. WATSET: Local-Global Graph Clustering with Applications Neelakantan et al. 2014; Tian et al. 2014; Li and Jurafsky 2015; Bartunov et al. 2016; Cocos and Callison-Burch 2016; Pelevina et al. 2016; Thomason and Mooney 2017). Huang et al. (2012) introduced multiple word prototypes for dense vector representations (embeddings). Their approach is based on a neural network architecture; during training, all contexts of the word are clustered. Apidianaki and Sagot (2014) use an aligned parallel corpus and WordNet for English to perform cross-lingual wor"
J19-3002,S17-1025,0,0.346463,"Missing"
J19-3002,S13-2049,0,0.285653,"p between the instance’s context words and the words of the sense cluster. V´eronis (2004) compiles a corpus with contexts of polysemous nouns using a search engine. 427 Computational Linguistics Volume 45, Number 3 A word graph is built by drawing edges between co-occurring words in the gathered corpus, where edges below a certain similarity threshold were discarded. His HyperLex algorithm detects hubs of this graph, which are interpreted as word senses. Disambiguation in this experiment is performed by computing the distance between context words and hubs in this graph. Di Marco and Navigli (2013) present a comprehensive study of several graph-based WSI methods, including CW, HyperLex, and curvature clustering (Dorow et al. 2005). Additionally, the authors propose two novel algorithms: Balanced Maximum Spanning Tree Clustering and Squares (B-MST), and Triangles and Diamonds (SquaT++). To construct graphs, authors use first-order and second-order relationships extracted from a background corpus as well as keywords from snippets. This research goes beyond intrinsic evaluations of induced senses and measures the impact of the WSI in the context of an information retrieval via clustering a"
J19-3002,S18-2016,0,0.036049,"Missing"
J19-3002,P14-1097,0,0.0305612,"Missing"
J19-3002,P03-1009,0,0.239319,"Missing"
J19-3002,Q13-1015,0,0.175147,"s (e.g., words), WATSET induces a set of unambiguous overlapping clusters (communities) by disambiguating and grouping the ambiguous objects. WATSET is a meta-algorithm that uses existing hard clustering algorithms for graphs to obtain a fuzzy clustering (e.g., soft clustering). In computational linguistics, graph clustering is used for addressing problems such as word sense induction (Biemann 2006), lexical chain computing (Medelyan 2007), Web search results diversification (Di Marco and Navigli 2013), sentiment analysis (Pang and Lee 2004), and cross-lingual semantic relationship induction (Lewis and Steedman 2013b); more applications can be found in the book by Mihalcea and Radev (2011). Definitions. Let G = (V, E) be an undirected simple graph,5 where V is a set of nodes and E ⊆ V 2 is a set of undirected edges. We denote a subset of nodes Ci ⊆ V as a cluster. S A graph clustering algorithm then is a function C LUSTER : (V, E) → C such that V = Ci ∈C Ci . We distinguish two classes of graph clustering algorithms: hard clustering algorithms (partitionings) produce non-overlapping clusters, that is, Ci ∩ Cj = ∅ ⇐⇒ i 6= j, ∀Ci , Cj ∈ C, whereas fuzzy clustering algorithms permit cluster overlapping, tha"
J19-3002,D13-1064,0,0.0354668,"Missing"
J19-3002,D15-1200,0,0.369011,"st dense word vector models represent a word with a single vector and thus conflate senses (Mikolov et al. 2013; Pennington, Socher, and Manning 2014), there are several approaches that produce word sense embeddings. Multi-prototype extensions of the Skip-Gram model (Mikolov et al. 2013) that use no predefined sense inventory learn one embedding word vector per one word sense and are commonly fitted with a disambiguation mechanism (Huang et al. 2012; Apidianaki and Sagot 2014; 426 Ustalov et al. WATSET: Local-Global Graph Clustering with Applications Neelakantan et al. 2014; Tian et al. 2014; Li and Jurafsky 2015; Bartunov et al. 2016; Cocos and Callison-Burch 2016; Pelevina et al. 2016; Thomason and Mooney 2017). Huang et al. (2012) introduced multiple word prototypes for dense vector representations (embeddings). Their approach is based on a neural network architecture; during training, all contexts of the word are clustered. Apidianaki and Sagot (2014) use an aligned parallel corpus and WordNet for English to perform cross-lingual word sense disambiguation to produce French synsets. However, Cocos and Callison-Burch (2016) showed that it is possible to successfully perform a monolingual word sense"
J19-3002,S10-1011,0,0.187061,"ailable implementations have been used. During the evaluation, we delete clusters equal to or larger than the threshold of 150 words, as they can hardly represent any meaningful synset. Only the clusters produced by the MaxMax algorithm were actually affected by this threshold. Quality Measure. To evaluate the quality of the induced synsets, we transform them into synonymy pairs and computed precision, recall, and F1 -score on the basis of the overlap of these synonymy pairs with the synonymy pairs from the gold standard data sets. The F1 -score calculated this way is known as paired F-score (Manandhar et al. 2010; Hope and Keller 2013a). Let C be the set of obtained synsets and CG be the set of gold synsets. Given a synset containing n &gt; 1 words, we generate n(n2−1) pairs of synonyms, so we transform C into a set of pairs P and CG into a set of gold pairs PG . We then compute the numbers of positive and negative answers as follows: TP = |P ∪ PG | (9) FP = |P  PG | (10) FN = |PG  P| (11) where TP is the number of true positives, FP is the number of false positives, and FN is the number of false negatives. As a result, we use the standard definitions of precision 2·Pr·Re TP as Pr = TPTP +FP , recall a"
J19-3002,N13-1051,0,0.418879,"nsformed into a triframe, which is a triple that is composed of the subjects fs ⊆ V, the verbs fv ⊆ V, and the objects fo ⊆ V. For example, the triples shown in Figure 9 will form a triframe ({man, people, woman}, {make, earn}, {profit, money} ). 5.2 Evaluation Currently, there is no universally accepted approach for evaluating unsupervised frame induction methods. All the previously developed methods were evaluated on completely different incomparable setups and used different input corpora (Titov and 454 Ustalov et al. WATSET: Local-Global Graph Clustering with Applications Klementiev 2012; Materna 2013; O’Connor 2013, etc.). We propose a unified methodology by treating the complex multi-stage frame induction task as a straightforward triple clustering task. 5.2.1 Experimental Setup. We compare our method, Triframes WATSET, to several available state-of-the-art baselines applicable to our data set of triples (Section 2.3). LDAFrames by Materna (2012, 2013) is a frame induction method based on topic modeling. Higher-Order Skip-Gram (HOSG) by Cotterell et al. (2017) generalizes the Skip-Gram model (Mikolov et al. 2013) by extending it from word-context co-occurrence matrices to tensors factori"
J19-3002,P09-1045,0,0.0378997,"ed approaches were further developed using 3 https://tac.nist.gov/2010/Summarization. 429 Computational Linguistics Volume 45, Number 3 graph-based methods using a PageRank-based weighting (Kozareva, Riloff, and Hovy 2008), random walks (Talukdar et al. 2008), or heuristic scoring (Qadir et al. 2015). Other approaches use probabilistic graphical models, such as the ones proposed by Ritter, Mausam, and Etzioni (2010) and Hovy et al. (2011). To ensure the overall quality of extraction pattern with minimal supervision, Thelen and Riloff (2002) explored a bootstrapping approach, later extended by McIntosh and Curran (2009) with bagging and distributional similarity to minimize the semantic drift problem of iterative bootstrapping algorithms. As an alternative to pattern-based methods, Panchenko et al. (2018b) show how to apply semantic classes to improve hypernymy extraction and taxonomy induction. Like in our experiments in Section 6, it uses a distributional thesaurus as input, as well as multiple pre- and post-processing stages to filter the input graph and disambiguate individual nodes. In contrast to Pachenko et al., here we directly apply the WATSET algorithm to obtain the resulting distributional semanti"
J19-3002,P07-3015,0,0.0440453,"Algorithm for Fuzzy Graph Clustering In this section, we present WATSET, a meta-algorithm for fuzzy graph clustering. Given a graph connecting potentially ambiguous objects (e.g., words), WATSET induces a set of unambiguous overlapping clusters (communities) by disambiguating and grouping the ambiguous objects. WATSET is a meta-algorithm that uses existing hard clustering algorithms for graphs to obtain a fuzzy clustering (e.g., soft clustering). In computational linguistics, graph clustering is used for addressing problems such as word sense induction (Biemann 2006), lexical chain computing (Medelyan 2007), Web search results diversification (Di Marco and Navigli 2013), sentiment analysis (Pang and Lee 2004), and cross-lingual semantic relationship induction (Lewis and Steedman 2013b); more applications can be found in the book by Mihalcea and Radev (2011). Definitions. Let G = (V, E) be an undirected simple graph,5 where V is a set of nodes and E ⊆ V 2 is a set of undirected edges. We denote a subset of nodes Ci ⊆ V as a cluster. S A graph clustering algorithm then is a function C LUSTER : (V, E) → C such that V = Ci ∈C Ci . We distinguish two classes of graph clustering algorithms: hard clust"
J19-3002,C14-2023,0,0.0313933,"Missing"
J19-3002,W12-1901,0,0.0610861,"Missing"
J19-3002,D14-1113,0,0.136393,"s are centroids of the clusters. Whereas most dense word vector models represent a word with a single vector and thus conflate senses (Mikolov et al. 2013; Pennington, Socher, and Manning 2014), there are several approaches that produce word sense embeddings. Multi-prototype extensions of the Skip-Gram model (Mikolov et al. 2013) that use no predefined sense inventory learn one embedding word vector per one word sense and are commonly fitted with a disambiguation mechanism (Huang et al. 2012; Apidianaki and Sagot 2014; 426 Ustalov et al. WATSET: Local-Global Graph Clustering with Applications Neelakantan et al. 2014; Tian et al. 2014; Li and Jurafsky 2015; Bartunov et al. 2016; Cocos and Callison-Burch 2016; Pelevina et al. 2016; Thomason and Mooney 2017). Huang et al. (2012) introduced multiple word prototypes for dense vector representations (embeddings). Their approach is based on a neural network architecture; during training, all contexts of the word are clustered. Apidianaki and Sagot (2014) use an aligned parallel corpus and WordNet for English to perform cross-lingual word sense disambiguation to produce French synsets. However, Cocos and Callison-Burch (2016) showed that it is possible to succes"
J19-3002,P07-1068,0,0.0417874,"ng semantic classes from text, also known as semantic lexicon induction, has also been extensively explored in previous works. This is because inducing semantic classes directly from text has the potential to avoid the limited coverage problems of knowledge bases like Freebase, DBpedia (Bizer et al. 2009), or BabelNet (Navigli and Ponzetto 2012), which rely on Wikipedia (Hovy, Navigli, and Ponzetto 2013), as well as to allow for resource induction across domains (Hovy et al. 2011). Information about semantic classes, in turn, has been shown to benefit such high-level NLP tasks as coreference (Ng 2007). Induction of semantic classes as a research direction in the field of NLP starts, to the best of our knowledge, with Lin and Pantel (2001), where sets of similar words are clustered into concepts. This approach performs a hard clustering and does not label clusters, but these drawbacks are addressed by Pantel and Lin (2002), where words can belong to several clusters, thus representing senses. Pantel and Ravichandran (2004) aggregate hypernyms per cluster, which come from Hearst (1992) patterns. Pattern-based approaches were further developed using 3 https://tac.nist.gov/2010/Summarization."
J19-3002,R15-1061,0,0.0343153,"Missing"
J19-3002,nivre-etal-2006-maltparser,0,0.0381336,"Missing"
J19-3002,S16-1206,1,0.914841,"ure 14 shows the distribution of the 82,115 noun synsets from WordNet 3.1 across the supersenses. In our experiments in this section, these classes are used as gold standard clustering of word senses as recorded in WordNet. One can observe a Zipfian-like power-law (Zipf 1949) distribution with a few clusters, such as artifact and person, accounting for a large fraction of all nouns in the resource. Overall, in this experiment we decided to focus on nouns, as the input distributional thesauri used in this experiment (as presented in Section 6.2) are most studied for modeling of noun semantics (Panchenko et al. 2016b). The WordNet supersenses were applied later also for word sense disambiguation as a system of broad sense labels (Flekova and Gurevych 2016). For BabelNet, there is a similar data set called BabelDomains (Camacho-Collados and Navigli 2017) produced by automatically labeling BabelNet synsets with 32 different domains based on the topics of Wikipedia featured articles. Despite the larger size, however, BabelDomains provides only a silver standard (being semi-automatically created). We thus opt in the following to use WordNet supersenses only, because they provide instead a gold standard creat"
J19-3002,L18-1286,1,0.835468,"zareva, Riloff, and Hovy 2008), random walks (Talukdar et al. 2008), or heuristic scoring (Qadir et al. 2015). Other approaches use probabilistic graphical models, such as the ones proposed by Ritter, Mausam, and Etzioni (2010) and Hovy et al. (2011). To ensure the overall quality of extraction pattern with minimal supervision, Thelen and Riloff (2002) explored a bootstrapping approach, later extended by McIntosh and Curran (2009) with bagging and distributional similarity to minimize the semantic drift problem of iterative bootstrapping algorithms. As an alternative to pattern-based methods, Panchenko et al. (2018b) show how to apply semantic classes to improve hypernymy extraction and taxonomy induction. Like in our experiments in Section 6, it uses a distributional thesaurus as input, as well as multiple pre- and post-processing stages to filter the input graph and disambiguate individual nodes. In contrast to Pachenko et al., here we directly apply the WATSET algorithm to obtain the resulting distributional semantic classes instead of using a sophisticated parametric pipeline that performs a sequence of clustering and pruning steps. Another related strain of research to semantic class induction is d"
J19-3002,L18-1244,1,0.90656,"zareva, Riloff, and Hovy 2008), random walks (Talukdar et al. 2008), or heuristic scoring (Qadir et al. 2015). Other approaches use probabilistic graphical models, such as the ones proposed by Ritter, Mausam, and Etzioni (2010) and Hovy et al. (2011). To ensure the overall quality of extraction pattern with minimal supervision, Thelen and Riloff (2002) explored a bootstrapping approach, later extended by McIntosh and Curran (2009) with bagging and distributional similarity to minimize the semantic drift problem of iterative bootstrapping algorithms. As an alternative to pattern-based methods, Panchenko et al. (2018b) show how to apply semantic classes to improve hypernymy extraction and taxonomy induction. Like in our experiments in Section 6, it uses a distributional thesaurus as input, as well as multiple pre- and post-processing stages to filter the input graph and disambiguate individual nodes. In contrast to Pachenko et al., here we directly apply the WATSET algorithm to obtain the resulting distributional semantic classes instead of using a sophisticated parametric pipeline that performs a sequence of clustering and pruning steps. Another related strain of research to semantic class induction is d"
J19-3002,P04-1035,0,0.0483492,"graph clustering. Given a graph connecting potentially ambiguous objects (e.g., words), WATSET induces a set of unambiguous overlapping clusters (communities) by disambiguating and grouping the ambiguous objects. WATSET is a meta-algorithm that uses existing hard clustering algorithms for graphs to obtain a fuzzy clustering (e.g., soft clustering). In computational linguistics, graph clustering is used for addressing problems such as word sense induction (Biemann 2006), lexical chain computing (Medelyan 2007), Web search results diversification (Di Marco and Navigli 2013), sentiment analysis (Pang and Lee 2004), and cross-lingual semantic relationship induction (Lewis and Steedman 2013b); more applications can be found in the book by Mihalcea and Radev (2011). Definitions. Let G = (V, E) be an undirected simple graph,5 where V is a set of nodes and E ⊆ V 2 is a set of undirected edges. We denote a subset of nodes Ci ⊆ V as a cluster. S A graph clustering algorithm then is a function C LUSTER : (V, E) → C such that V = Ci ∈C Ci . We distinguish two classes of graph clustering algorithms: hard clustering algorithms (partitionings) produce non-overlapping clusters, that is, Ci ∩ Cj = ∅ ⇐⇒ i 6= j, ∀Ci ,"
J19-3002,N04-1041,0,0.0348718,", as well as to allow for resource induction across domains (Hovy et al. 2011). Information about semantic classes, in turn, has been shown to benefit such high-level NLP tasks as coreference (Ng 2007). Induction of semantic classes as a research direction in the field of NLP starts, to the best of our knowledge, with Lin and Pantel (2001), where sets of similar words are clustered into concepts. This approach performs a hard clustering and does not label clusters, but these drawbacks are addressed by Pantel and Lin (2002), where words can belong to several clusters, thus representing senses. Pantel and Ravichandran (2004) aggregate hypernyms per cluster, which come from Hearst (1992) patterns. Pattern-based approaches were further developed using 3 https://tac.nist.gov/2010/Summarization. 429 Computational Linguistics Volume 45, Number 3 graph-based methods using a PageRank-based weighting (Kozareva, Riloff, and Hovy 2008), random walks (Talukdar et al. 2008), or heuristic scoring (Qadir et al. 2015). Other approaches use probabilistic graphical models, such as the ones proposed by Ritter, Mausam, and Etzioni (2010) and Hovy et al. (2011). To ensure the overall quality of extraction pattern with minimal superv"
J19-3002,P15-2070,0,0.0389199,"Missing"
J19-3002,W97-0322,0,0.281913,"es belonging to the same community and builds a new network whose nodes are the communities. These steps are repeated to maximize modularity of the clustering result. 2.2 Word Sense Induction Word Sense Induction is an unsupervised knowledge-free approach to Word Sense Disambiguation (WSD): It uses neither handcrafted lexical resources nor hand-annotated sense-labeled corpora. Instead, it induces word sense inventories automatically from corpora. Unsupervised WSD methods fall into two main categories: context clustering and word ego network clustering. ¨ Context clustering approaches, such as Pedersen and Bruce (1997) and Schutze (1998), represent an instance usually by a vector that characterizes its context, where the definition of context can vary greatly. These vectors of each instance are then clustered. ¨ Schutze (1998) induced sparse sense vectors by clustering context vectors, using the expectation-maximization algorithm. This approach is fitted with a similarity-based WSD mechanism. Pantel and Lin (2002) used a two-staged Clustering by Committee algorithm. In the first stage, it uses average-link clustering to find small and tight clusters, which are used to iteratively identify committees from th"
J19-3002,W16-1620,1,0.894029,"conflate senses (Mikolov et al. 2013; Pennington, Socher, and Manning 2014), there are several approaches that produce word sense embeddings. Multi-prototype extensions of the Skip-Gram model (Mikolov et al. 2013) that use no predefined sense inventory learn one embedding word vector per one word sense and are commonly fitted with a disambiguation mechanism (Huang et al. 2012; Apidianaki and Sagot 2014; 426 Ustalov et al. WATSET: Local-Global Graph Clustering with Applications Neelakantan et al. 2014; Tian et al. 2014; Li and Jurafsky 2015; Bartunov et al. 2016; Cocos and Callison-Burch 2016; Pelevina et al. 2016; Thomason and Mooney 2017). Huang et al. (2012) introduced multiple word prototypes for dense vector representations (embeddings). Their approach is based on a neural network architecture; during training, all contexts of the word are clustered. Apidianaki and Sagot (2014) use an aligned parallel corpus and WordNet for English to perform cross-lingual word sense disambiguation to produce French synsets. However, Cocos and Callison-Burch (2016) showed that it is possible to successfully perform a monolingual word sense induction using only such a paraphrase corpus as Paraphrase Database (Pavli"
J19-3002,D14-1162,0,0.0797303,"Missing"
J19-3002,N18-1202,0,0.0153532,"een Skip-Gram word vectors (Mikolov et al. 2013), we should note that such an approach assigns high values of similarity not just to synonymous words, but to antonymous and generally any lexically related words. This is a common problem with lexical embedding spaces, which we tried to evade by explicitly using a synonymy dictionary as an input. For example, “audio play” and “radio play,” or “accusative” and “oblique,” are semantically related expressions, but really not synonyms. Such a problem can be addressed using techniques such as retrofitting (Faruqui et al. 2015) and contextualization (Peters et al. 2018). However, one limitation of all the approaches considered in this section is the dependence on the completeness of the input dictionary of synonyms. In some parts of the input synonymy graph, important bridges between words can be missing, leading to smaller-than-desired synsets. A promising extension of the present methodology is using distributional models to enhance connectivity of the graph by cautiously adding extra relationships (Ustalov et al. 2017). Cross-Resource Evaluation. In order to estimate the upper bound of precision, recall, and F1 -score in our synset induction experiments,"
J19-3002,N10-1013,0,0.0170646,"1998), represent an instance usually by a vector that characterizes its context, where the definition of context can vary greatly. These vectors of each instance are then clustered. ¨ Schutze (1998) induced sparse sense vectors by clustering context vectors, using the expectation-maximization algorithm. This approach is fitted with a similarity-based WSD mechanism. Pantel and Lin (2002) used a two-staged Clustering by Committee algorithm. In the first stage, it uses average-link clustering to find small and tight clusters, which are used to iteratively identify committees from these clusters. Reisinger and Mooney (2010) presented a multi-prototype vector space. Sparse tf–idf vectors are clustered, using a parametric method fixing the same number of senses for all words. Sense vectors are centroids of the clusters. Whereas most dense word vector models represent a word with a single vector and thus conflate senses (Mikolov et al. 2013; Pennington, Socher, and Manning 2014), there are several approaches that produce word sense embeddings. Multi-prototype extensions of the Skip-Gram model (Mikolov et al. 2013) that use no predefined sense inventory learn one embedding word vector per one word sense and are comm"
J19-3002,W17-6933,1,0.844526,"n.01, x-axis.n.01, y-axis.n.01, z-axis.n.01, major axis.n.01, minor axis.n.01, optic axis.n.01, principal axis.n.01, semimajor axis.n.01, semiminor axis.n.01 specifically, the dimensions of the vector space represent salient syntactic dependencies of each word extracted using a dependency parser. For this, we use the JoBimText framework for computation of count-based distributional models from raw text collections (Biemann and Riedl 2013).36 Although similar graphs could be derived also from neural distributional models, such as Word2Vec (Mikolov et al. 2013), it was shown in Riedl (2016) and Riedl and Biemann (2017) that the quality of syntactically-based graphs is generally superior. The JoBimText framework involves several steps. First, it takes an unlabeled input text corpus and performs dependency parsing so as to extract features representing each word. Each word is represented by a bag of syntactic dependencies such as conj and(Ruby, · ) or prep in(code, · ), extracted from the dependencies of MaltParser (Nivre, Hall, and Nilsson 2006), which are further collapsed using the tool by Ruppert et al. (2015) in the notation of Stanford Dependencies (de Marneffe, MacCartney, and Manning 2006). Next, sema"
J19-3002,P10-1044,0,0.102279,"Missing"
J19-3002,N16-1173,0,0.0331879,"Missing"
J19-3002,J98-1004,0,0.710235,"nity and builds a new network whose nodes are the communities. These steps are repeated to maximize modularity of the clustering result. 2.2 Word Sense Induction Word Sense Induction is an unsupervised knowledge-free approach to Word Sense Disambiguation (WSD): It uses neither handcrafted lexical resources nor hand-annotated sense-labeled corpora. Instead, it induces word sense inventories automatically from corpora. Unsupervised WSD methods fall into two main categories: context clustering and word ego network clustering. ¨ Context clustering approaches, such as Pedersen and Bruce (1997) and Schutze (1998), represent an instance usually by a vector that characterizes its context, where the definition of context can vary greatly. These vectors of each instance are then clustered. ¨ Schutze (1998) induced sparse sense vectors by clustering context vectors, using the expectation-maximization algorithm. This approach is fitted with a similarity-based WSD mechanism. Pantel and Lin (2002) used a two-staged Clustering by Committee algorithm. In the first stage, it uses average-link clustering to find small and tight clusters, which are used to iteratively identify committees from these clusters. Reisi"
J19-3002,D07-1002,0,0.0693088,"nes. 2.3 Semantic Frame Induction Frame Semantics was originally introduced by Fillmore (1982) and further developed in the FrameNet project (Baker, Fillmore, and Lowe 1998). FrameNet is a lexical resource composed of a collection of semantic frames, relationships between them, and a corpus of frame occurrences in text. This annotated corpus gave rise to the development of frame parsers using supervised learning (Gildea and Jurafsky 2002; Erk and Pado´ 2006; Das et al. 2014, inter alia), as well as its application to a wide range of tasks, ranging from answer extraction in Question Answering (Shen and Lapata 2007) and Textual Entailment (Burchardt et al. 2009; Ben Aharon, Szpektor, and Dagan 2010). However, frame-semantic resources are arguably expensive and time-consuming to build because of difficulties in defining the frames, their granularity and domain, as well as the complexity of the construction and annotation tasks. Consequently, such resources exist only for a few languages (Boas 2009) and even English is lacking domainspecific frame-based resources. Possible inroads are cross-lingual semantic annotation 2 http://ontopt.dei.uc.pt. 428 Ustalov et al. WATSET: Local-Global Graph Clustering with"
J19-3002,M92-1001,0,0.293429,"rdan (2003) for generating semantic frames and their respective frame-specific semantic roles at the same time. The authors evaluated their approach against the CPA corpus (Hanks and Pustejovsky 2005). Although Ritter, Mausam, and Etzioni (2010) have applied LDA for inducing structures similar to frames, their study is focused on the extraction of mutually related frame arguments. ProFinder (Cheung, Poon, and Vanderwende 2013) is another generative approach that also models both frames and roles as latent topics. The evaluation was performed on the in-domain information extraction task MUC-4 (Sundheim 1992) and on the text summarization task TAC-2010.3 Modi, Titov, and Klementiev (2012) build on top of an unsupervised semantic role labeling model (Titov and Klementiev 2012). The raw text of sentences from the FrameNet data is used for training. The FrameNet gold annotations are then used to evaluate the labeling of the obtained frames and roles, effectively clustering instances known during induction. Kawahara, Peterson, and Palmer (2014) harvest a huge collection of verbal predicates along with their argument instances and then apply the Chinese Restaurant Process clustering algorithm to group"
J19-3002,D08-1061,0,0.0384586,"of similar words are clustered into concepts. This approach performs a hard clustering and does not label clusters, but these drawbacks are addressed by Pantel and Lin (2002), where words can belong to several clusters, thus representing senses. Pantel and Ravichandran (2004) aggregate hypernyms per cluster, which come from Hearst (1992) patterns. Pattern-based approaches were further developed using 3 https://tac.nist.gov/2010/Summarization. 429 Computational Linguistics Volume 45, Number 3 graph-based methods using a PageRank-based weighting (Kozareva, Riloff, and Hovy 2008), random walks (Talukdar et al. 2008), or heuristic scoring (Qadir et al. 2015). Other approaches use probabilistic graphical models, such as the ones proposed by Ritter, Mausam, and Etzioni (2010) and Hovy et al. (2011). To ensure the overall quality of extraction pattern with minimal supervision, Thelen and Riloff (2002) explored a bootstrapping approach, later extended by McIntosh and Curran (2009) with bagging and distributional similarity to minimize the semantic drift problem of iterative bootstrapping algorithms. As an alternative to pattern-based methods, Panchenko et al. (2018b) show how to apply semantic classes to impr"
J19-3002,W02-1028,0,0.251425,"ate hypernyms per cluster, which come from Hearst (1992) patterns. Pattern-based approaches were further developed using 3 https://tac.nist.gov/2010/Summarization. 429 Computational Linguistics Volume 45, Number 3 graph-based methods using a PageRank-based weighting (Kozareva, Riloff, and Hovy 2008), random walks (Talukdar et al. 2008), or heuristic scoring (Qadir et al. 2015). Other approaches use probabilistic graphical models, such as the ones proposed by Ritter, Mausam, and Etzioni (2010) and Hovy et al. (2011). To ensure the overall quality of extraction pattern with minimal supervision, Thelen and Riloff (2002) explored a bootstrapping approach, later extended by McIntosh and Curran (2009) with bagging and distributional similarity to minimize the semantic drift problem of iterative bootstrapping algorithms. As an alternative to pattern-based methods, Panchenko et al. (2018b) show how to apply semantic classes to improve hypernymy extraction and taxonomy induction. Like in our experiments in Section 6, it uses a distributional thesaurus as input, as well as multiple pre- and post-processing stages to filter the input graph and disambiguate individual nodes. In contrast to Pachenko et al., here we di"
J19-3002,C14-1016,0,0.0229688,"usters. Whereas most dense word vector models represent a word with a single vector and thus conflate senses (Mikolov et al. 2013; Pennington, Socher, and Manning 2014), there are several approaches that produce word sense embeddings. Multi-prototype extensions of the Skip-Gram model (Mikolov et al. 2013) that use no predefined sense inventory learn one embedding word vector per one word sense and are commonly fitted with a disambiguation mechanism (Huang et al. 2012; Apidianaki and Sagot 2014; 426 Ustalov et al. WATSET: Local-Global Graph Clustering with Applications Neelakantan et al. 2014; Tian et al. 2014; Li and Jurafsky 2015; Bartunov et al. 2016; Cocos and Callison-Burch 2016; Pelevina et al. 2016; Thomason and Mooney 2017). Huang et al. (2012) introduced multiple word prototypes for dense vector representations (embeddings). Their approach is based on a neural network architecture; during training, all contexts of the word are clustered. Apidianaki and Sagot (2014) use an aligned parallel corpus and WordNet for English to perform cross-lingual word sense disambiguation to produce French synsets. However, Cocos and Callison-Burch (2016) showed that it is possible to successfully perform a m"
J19-3002,P11-1145,0,0.0800752,"Missing"
J19-3002,E12-1003,0,0.0220272,"he CPA corpus (Hanks and Pustejovsky 2005). Although Ritter, Mausam, and Etzioni (2010) have applied LDA for inducing structures similar to frames, their study is focused on the extraction of mutually related frame arguments. ProFinder (Cheung, Poon, and Vanderwende 2013) is another generative approach that also models both frames and roles as latent topics. The evaluation was performed on the in-domain information extraction task MUC-4 (Sundheim 1992) and on the text summarization task TAC-2010.3 Modi, Titov, and Klementiev (2012) build on top of an unsupervised semantic role labeling model (Titov and Klementiev 2012). The raw text of sentences from the FrameNet data is used for training. The FrameNet gold annotations are then used to evaluate the labeling of the obtained frames and roles, effectively clustering instances known during induction. Kawahara, Peterson, and Palmer (2014) harvest a huge collection of verbal predicates along with their argument instances and then apply the Chinese Restaurant Process clustering algorithm to group predicates with similar arguments. The approach was evaluated on the verb cluster data set of Korhonen, Krymolowski, and Marx (2003). These and some other related approac"
J19-3002,W09-1127,0,0.152335,"ifficulties in defining the frames, their granularity and domain, as well as the complexity of the construction and annotation tasks. Consequently, such resources exist only for a few languages (Boas 2009) and even English is lacking domainspecific frame-based resources. Possible inroads are cross-lingual semantic annotation 2 http://ontopt.dei.uc.pt. 428 Ustalov et al. WATSET: Local-Global Graph Clustering with Applications transfer (Pado´ and Lapata 2009; Hartmann, Eckle-Kohler, and Gurevych 2016) or linking FrameNet to other lexical-semantic or ontological resources (Narayanan et al. 2003; Tonelli and Pighin 2009; Laparra and Rigau 2010; Gurevych et al. 2012, inter alia). One inroad for overcoming these issues is automatizing the process of FrameNet construction through unsupervised frame induction techniques, as investigated by the systems described next. LDA-Frames (Materna 2012, 2013) is an approach to inducing semantic frames using a latent Dirichlet allocation (LDA) by Blei, Ng, and Jordan (2003) for generating semantic frames and their respective frame-specific semantic roles at the same time. The authors evaluated their approach against the CPA corpus (Hanks and Pustejovsky 2005). Although Ritt"
J19-3002,P17-1145,1,0.797757,"but really not synonyms. Such a problem can be addressed using techniques such as retrofitting (Faruqui et al. 2015) and contextualization (Peters et al. 2018). However, one limitation of all the approaches considered in this section is the dependence on the completeness of the input dictionary of synonyms. In some parts of the input synonymy graph, important bridges between words can be missing, leading to smaller-than-desired synsets. A promising extension of the present methodology is using distributional models to enhance connectivity of the graph by cautiously adding extra relationships (Ustalov et al. 2017). Cross-Resource Evaluation. In order to estimate the upper bound of precision, recall, and F1 -score in our synset induction experiments, we conducted a cross-resource evaluation between the used gold-standard data sets (Table 12). Similarly to the experimental setup described in Section 4.2.1, we transformed synsets from every data set into sets of synonymy pairs. Then, for every pair of gold standard data sets, we computed the pairwise precision, recall, and F1 -score by assessing synset-induced synonymy pairs of one data set on the pairs of another data set. As a result, we see that the lo"
J19-3002,P18-2010,1,0.815639,", Panchenko, and Biemann (2017), including an analysis of its computational complexity and run-time. We also describe a simplified version of WATSET that does not use the context similarity measure for propagating links in the original graph to the appropriate senses in the disambiguated graph. Three subsequent sections present different applications of the algorithm. Section 4 applies WATSET for unsupervised synset induction, referencing results by Ustalov, Panchenko, and Biemann. Section 5 shows frame induction with WATSET on the basis of a triclustering approach, as previously described by Ustalov et al. (2018). Section 6 presents new experiments on semantic class induction with WATSET. Section 7 concludes with the final remarks and pointers for future work. Table 1 shows several examples of linguistic structures on which we conduct experiments described in this article. With the exception of the type of input graph and the hyper-parameters of the WATSET algorithm, the overall pipeline remains similar in every described application. For instance, in Section 4 the input of the clustering algorithm is a graph of ambiguous synonyms and the output is an induced linguistic 1 This article builds upon and"
J19-3002,C02-1114,0,0.164904,"n, and semantic relatedness tasks, but yield no improvement for named entity recognition and sentiment analysis. Thomason and Mooney (2017) performed multi-modal word sense induction by combining both language and vision signals. In this approach, word embeddings are learned from the ImageNet corpus (Deng et al. 2009) and visual features are obtained from a deep neural network. Running a k-means algorithm on the joint feature set produces WordNet-like synsets. Word ego network clustering methods cluster graphs of words semantically related to the ambiguous word (Lin 1998; Pantel and Lin 2002; Widdows and Dorow 2002; Biemann 2006; Hope and Keller 2013a). An ego network consists of a single node (ego), together with the nodes they are connected to (alters), and all the edges among those alters (Everett and Borgatti 2005). In our case, such a network is a local neighborhood of one word. Nodes of the ego network can be (1) words semantically similar to the target word, as in our approach, or (2) context words relevant to the target, as in the UoS system (Hope and Keller 2013b). Graph edges represent semantic relationships between words derived using corpus-based methods (e.g., distributional semantics) or g"
J19-3002,P13-1086,0,0.0803084,"Missing"
J19-3002,zesch-etal-2008-extracting,0,0.076042,"Missing"
J19-3002,erk-pado-2006-shalmaneser,0,\N,Missing
J19-3002,bauer-etal-2012-dependency,0,\N,Missing
J19-3002,N10-1137,0,\N,Missing
J19-3002,W06-3812,1,\N,Missing
J19-3002,P10-2045,0,\N,Missing
J19-3002,P98-1013,0,\N,Missing
J19-3002,C98-1013,0,\N,Missing
J19-3002,P08-1119,0,\N,Missing
J19-3002,D09-1098,0,\N,Missing
L16-1421,W06-3814,0,0.193877,"Missing"
L16-1421,S12-1059,0,0.0745251,"Missing"
L16-1421,J10-4006,0,0.0321651,"y in the form of synsets (i.e. sets of synonyms), typed relations between synsets and sense definitions. A prominent example of this approach is the Princeton WordNet (Miller, 1995). The second approach makes use of text corpora to extract relations between words and feature representations of words and senses. These methods are trying to avoid manual work as much as possible. Whereas lexical resources are manually created, in the second approach most methods extract the information from text without human intervention. Examples of the second group of methods include ”classical” vector-based (Baroni and Lenci, 2010) and symbolic (Biemann and Riedl, 2013) distributional models, as well as word embeddings (Mikolov et al., 2013; Pennington et al., 2014). One of the strongest sides of lexical-semantic resources is their interpretability – they are entirely human-readable and drawn distinctions are motivated by lexicographic or psychological considerations. On the downside, these WordNet-like resources are expensive to create, and it is not easy to adapt them to a given domain of interest or language. Besides, sense inventories of lexical resources are often too fine grained to be useful in downstream applica"
L16-1421,W10-2309,0,0.0251842,"ach, based on word characteristics as yielded by the Sketch Engine corpus analysis tool (Kilgarriff et al., 2014). The need of corpus-based adaptation of lexical resources is discussed by McCarthy et al. (2004), who define a method to find the dominant sense of a word with respect to a text collection, in order to inform the most frequent sense baseline in word sense disambiguation. In (Agirre et al., 2006), automatically induced senses are mapped to WordNet via hand-labelled instances in the training set. Automatically induced sense inventories were used in word sense disambiguation tasks by Biemann (2010), yet as features and without explicit mapping to WordNet senses. While most word embedding approaches represent a term with a single vector and thus conflate senses, there are few approaches to produce word sense embeddings from corpora (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014; Bartunov et al., 2015; Li and Jurafsky, 2015). However, these representations have, to our knowledge, not been directly mapped to a lexicographic resource. Approaches that compute embeddings directly on knowledge bases are presented by Bordes et al. (2011) and Camacho-Collados et al. (2015). Rot"
L16-1421,P08-2063,0,0.0277702,"mbolic (Biemann and Riedl, 2013) distributional models, as well as word embeddings (Mikolov et al., 2013; Pennington et al., 2014). One of the strongest sides of lexical-semantic resources is their interpretability – they are entirely human-readable and drawn distinctions are motivated by lexicographic or psychological considerations. On the downside, these WordNet-like resources are expensive to create, and it is not easy to adapt them to a given domain of interest or language. Besides, sense inventories of lexical resources are often too fine grained to be useful in downstream applications (Brown, 2008). At the same time, corpus-driven approaches are strong at adaptivity – they can be re-trained on a new corpus, thus naturally adapting to the domain at hand. If fitted with a word sense induction algorithm, corpus-driven approaches can also discover new senses (Erk et al., 2009). However, the representations they deliver are often not matching the standards of lexicography, and they rather distinguish word usages than senses. Moreover, dense numeric vector representations as present in latent vector spaces (Sch¨utze, 1998) and word embeddings are barely interpretable. Word sense embeddings (H"
L16-1421,P15-1072,0,0.0430512,"ambiguation tasks by Biemann (2010), yet as features and without explicit mapping to WordNet senses. While most word embedding approaches represent a term with a single vector and thus conflate senses, there are few approaches to produce word sense embeddings from corpora (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014; Bartunov et al., 2015; Li and Jurafsky, 2015). However, these representations have, to our knowledge, not been directly mapped to a lexicographic resource. Approaches that compute embeddings directly on knowledge bases are presented by Bordes et al. (2011) and Camacho-Collados et al. (2015). Rothe and Sch¨utze (2015) combine un-disambiguated embeddings to WordNet synset to obtain synset representations in the embeddings space. The approach is evaluated on lexical sample tasks by adding synset embeddings as features to an existing WSD system. While this setup is flexible with respect to the kinds of embeddings used, it requires a large number of training instances per lexeme and is not able to find new senses in the underlying corpora. Our approach is different as we do not try to learn embeddings for all synsets in a lexical resource, but instead retrieve synsets that correspond"
L16-1421,P09-1002,0,0.029064,"e motivated by lexicographic or psychological considerations. On the downside, these WordNet-like resources are expensive to create, and it is not easy to adapt them to a given domain of interest or language. Besides, sense inventories of lexical resources are often too fine grained to be useful in downstream applications (Brown, 2008). At the same time, corpus-driven approaches are strong at adaptivity – they can be re-trained on a new corpus, thus naturally adapting to the domain at hand. If fitted with a word sense induction algorithm, corpus-driven approaches can also discover new senses (Erk et al., 2009). However, the representations they deliver are often not matching the standards of lexicography, and they rather distinguish word usages than senses. Moreover, dense numeric vector representations as present in latent vector spaces (Sch¨utze, 1998) and word embeddings are barely interpretable. Word sense embeddings (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014) extend word embeddings so that a word is represented by several vectors corresponding to meanings of the word. Li and Jurafsky (2015) show that sense embeddings can significantly improve performance of part-of-speech"
L16-1421,E12-1059,0,0.0208554,"available online.1 To our knowledge, this is the first attempt to tag sense embeddings with interpretable synsets from a lexical resource. While other approaches exist that use distributional information for enriching lexical resources (c.f. the next section), we are not aware of any other approach that utilizes corpus-induced senses in the form of sense embeddings for this purpose. 2. Related Work Aligning senses across several lexicographic resources has been sought as a means to achieve more comprehensive sense inventories. Recent approaches include methods used to build BabelNet and UBY (Gurevych et al., 2012). Both of these lexical resources automatically interlink word senses across multiple dictionaries and encyclopaedias, such as Wiktionary2 , Wikipedia3 and Omega Wiki4 . This line of research is focused on interlinking manually created lexical resources. However, they not attempt to align any corpus-driven sense inventory. While sense coverage and disambiguation coverage is increased through more and richer sense representations, 2649 1 http://tudarmstadt-lt.github.io/vec2synset http://www.wiktionary.org 3 http://www.wikipedia.org 4 http://www.omegawiki.org 2 these extended resources suffer fr"
L16-1421,P12-1092,0,0.852251,"). At the same time, corpus-driven approaches are strong at adaptivity – they can be re-trained on a new corpus, thus naturally adapting to the domain at hand. If fitted with a word sense induction algorithm, corpus-driven approaches can also discover new senses (Erk et al., 2009). However, the representations they deliver are often not matching the standards of lexicography, and they rather distinguish word usages than senses. Moreover, dense numeric vector representations as present in latent vector spaces (Sch¨utze, 1998) and word embeddings are barely interpretable. Word sense embeddings (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014) extend word embeddings so that a word is represented by several vectors corresponding to meanings of the word. Li and Jurafsky (2015) show that sense embeddings can significantly improve performance of part-of-speech tagging, semantic relation identification and semantic relatedness tasks, but yield no improvement for named entity recognition and sentiment analysis. Sense embeddings suffer the same interpretability limitations as other dense vector representations. The contribution of the paper is a technique that links word sense embeddings to a"
L16-1421,S13-2049,0,0.0321354,"t of these similarities, but in the second strategy, one vector is linked to at most one synset. 5. Evaluation We evaluate our linking techniques with respect to a manual mapping of senses. In particular, we built an evaluation dataset for 50 ambiguous words presented in Table 2. More specifically, we selected words with homonymous senses i.e. senses with unrelated meanings, such as “python” in the animal and the programming language senses. Some of these words, such as “bank” and “plant” are commonly used in word sense disambiguation evaluations (Navigli et al., 2007; Manandhar et al., 2010; Jurgens and Klapaftis, 2013); others, like “delphi” or “python” may refer to both nouns and named entities. For each of these words, we retrieved all BabelNet and AdaGram senses. Next, we generated all 3795 possible matching combinations for these 50 words and annotated them binarily. As mentioned above, BabelNet is very fine grained and contains more senses than AdaGram. Word sense embeddings used in our experiments are on the countrary coarsegrained with at most five senses per word (this is tunable by the α parameter). Therefore, the corpus-based model cannot learn a fine-grained polysemic inventory featuring tens of"
L16-1421,D15-1200,0,0.351281,"word sense induction algorithm, corpus-driven approaches can also discover new senses (Erk et al., 2009). However, the representations they deliver are often not matching the standards of lexicography, and they rather distinguish word usages than senses. Moreover, dense numeric vector representations as present in latent vector spaces (Sch¨utze, 1998) and word embeddings are barely interpretable. Word sense embeddings (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014) extend word embeddings so that a word is represented by several vectors corresponding to meanings of the word. Li and Jurafsky (2015) show that sense embeddings can significantly improve performance of part-of-speech tagging, semantic relation identification and semantic relatedness tasks, but yield no improvement for named entity recognition and sentiment analysis. Sense embeddings suffer the same interpretability limitations as other dense vector representations. The contribution of the paper is a technique that links word sense embeddings to a lexical resource, making them more interpretable. The main motivation of the technique is to close the gap between interpretability and adaptivity of lexical-semantic models. We de"
L16-1421,S10-1011,0,0.0314328,"on the global threshold t of these similarities, but in the second strategy, one vector is linked to at most one synset. 5. Evaluation We evaluate our linking techniques with respect to a manual mapping of senses. In particular, we built an evaluation dataset for 50 ambiguous words presented in Table 2. More specifically, we selected words with homonymous senses i.e. senses with unrelated meanings, such as “python” in the animal and the programming language senses. Some of these words, such as “bank” and “plant” are commonly used in word sense disambiguation evaluations (Navigli et al., 2007; Manandhar et al., 2010; Jurgens and Klapaftis, 2013); others, like “delphi” or “python” may refer to both nouns and named entities. For each of these words, we retrieved all BabelNet and AdaGram senses. Next, we generated all 3795 possible matching combinations for these 50 words and annotated them binarily. As mentioned above, BabelNet is very fine grained and contains more senses than AdaGram. Word sense embeddings used in our experiments are on the countrary coarsegrained with at most five senses per word (this is tunable by the α parameter). Therefore, the corpus-based model cannot learn a fine-grained polysemi"
L16-1421,P04-1036,0,0.0782112,"stadt-lt.github.io/vec2synset http://www.wiktionary.org 3 http://www.wikipedia.org 4 http://www.omegawiki.org 2 these extended resources suffer from alignment errors, as well as the disadvantages of lexicographic resources as discussed in the introduction. While lexicographic work mostly relies on corpus-based, yet hand-picked evidence, Hanks (2013) presents an approach to systematize and formalize this approach, based on word characteristics as yielded by the Sketch Engine corpus analysis tool (Kilgarriff et al., 2014). The need of corpus-based adaptation of lexical resources is discussed by McCarthy et al. (2004), who define a method to find the dominant sense of a word with respect to a text collection, in order to inform the most frequent sense baseline in word sense disambiguation. In (Agirre et al., 2006), automatically induced senses are mapped to WordNet via hand-labelled instances in the training set. Automatically induced sense inventories were used in word sense disambiguation tasks by Biemann (2010), yet as features and without explicit mapping to WordNet senses. While most word embedding approaches represent a term with a single vector and thus conflate senses, there are few approaches to p"
L16-1421,P10-1023,0,0.0387752,"ic relatedness tasks, but yield no improvement for named entity recognition and sentiment analysis. Sense embeddings suffer the same interpretability limitations as other dense vector representations. The contribution of the paper is a technique that links word sense embeddings to a lexical resource, making them more interpretable. The main motivation of the technique is to close the gap between interpretability and adaptivity of lexical-semantic models. We demonstrate the performance of our method by linking AdaGram sense embeddings, proposed by Bartunov et al. (2015) to synsets of BabelNet (Navigli and Ponzetto, 2010). However, the approach can be straightforwardly applied to any combination of a WordNet-like resource and a word sense embeddings model. Scripts and datasets related to this experiment are available online.1 To our knowledge, this is the first attempt to tag sense embeddings with interpretable synsets from a lexical resource. While other approaches exist that use distributional information for enriching lexical resources (c.f. the next section), we are not aware of any other approach that utilizes corpus-induced senses in the form of sense embeddings for this purpose. 2. Related Work Aligning"
L16-1421,S07-1006,0,0.088075,"Missing"
L16-1421,D14-1113,0,0.540026,"oaches are strong at adaptivity – they can be re-trained on a new corpus, thus naturally adapting to the domain at hand. If fitted with a word sense induction algorithm, corpus-driven approaches can also discover new senses (Erk et al., 2009). However, the representations they deliver are often not matching the standards of lexicography, and they rather distinguish word usages than senses. Moreover, dense numeric vector representations as present in latent vector spaces (Sch¨utze, 1998) and word embeddings are barely interpretable. Word sense embeddings (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014) extend word embeddings so that a word is represented by several vectors corresponding to meanings of the word. Li and Jurafsky (2015) show that sense embeddings can significantly improve performance of part-of-speech tagging, semantic relation identification and semantic relatedness tasks, but yield no improvement for named entity recognition and sentiment analysis. Sense embeddings suffer the same interpretability limitations as other dense vector representations. The contribution of the paper is a technique that links word sense embeddings to a lexical resource, making them more interpretab"
L16-1421,D14-1162,0,0.0854852,"Missing"
L16-1421,P15-1173,0,0.138964,"Missing"
L16-1421,J98-1004,0,0.788209,"Missing"
L16-1421,C14-1016,0,0.494534,"corpus-driven approaches are strong at adaptivity – they can be re-trained on a new corpus, thus naturally adapting to the domain at hand. If fitted with a word sense induction algorithm, corpus-driven approaches can also discover new senses (Erk et al., 2009). However, the representations they deliver are often not matching the standards of lexicography, and they rather distinguish word usages than senses. Moreover, dense numeric vector representations as present in latent vector spaces (Sch¨utze, 1998) and word embeddings are barely interpretable. Word sense embeddings (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014) extend word embeddings so that a word is represented by several vectors corresponding to meanings of the word. Li and Jurafsky (2015) show that sense embeddings can significantly improve performance of part-of-speech tagging, semantic relation identification and semantic relatedness tasks, but yield no improvement for named entity recognition and sentiment analysis. Sense embeddings suffer the same interpretability limitations as other dense vector representations. The contribution of the paper is a technique that links word sense embeddings to a lexical resource, m"
L18-1093,P98-1013,0,0.595481,"(bs) ∪ Bf (F )) and A.c(w) equals the number of occurrences of the word w in A. For instance, with respect to the excerpts of Tables 3 and 4 we obtain w(Communication, bn:00085007v) = 15.0. 3. Using the Enriched Representations for Word Frame Disambiguation We evaluate our extensions of Framester profiles following the experimental setting of Gangemi et al. (2016b), and compare the extended and the original profiles in a task of Word Frame Disambiguation (WFD). 3.1. Dataset: FrameNet Full Text Documents To create a silver standard we processed all 108 documents from the FrameNet 1.7 dataset (Baker et al., 1998) with BabelFy (Moro et al., 2014)3 . By combining the original frame annotations with the automatically generated entity links we collected a total of 81,706 annotations, which we use in our experimental setting as a silver standard. 3.2. Word Frame Disambiguation Following the WFD approach described in Gangemi et al. (2016b) we implemented a simple word frame disambiguator, where for each provided annotation in our silver standard we try to predict a frame label only on the basis of the BabelNet synsets generated through BabelFy. In order to provide the most suitable frame label F for the pro"
L18-1093,E17-1056,1,0.906433,"usually spontaneous – the rules by which we interpret meaning. Here, the reference to the ‘Reading aloud’ frame from FrameNet1 could be triggered on the basis of the occurrences of the verbs in the sentence: hear, understanding and interpret. Such connections, in turn, could be provided by a hybrid resource where distributional representations from text have been explicitly linked to semantic knowledge repositories, since hybrid resources of this kind have been shown in the past to improve performance on lexical understanding (Panchenko et al., 2017) as well as taxonomy learning and cleaning (Faralli et al., 2017). In this paper, we bridge the gap between distributional and frame semantics by linking distributional semantic representations to Framester, a knowledge graph that acts as a hub between resources like FrameNet, BabelNet and DBpedia, among others. As a result of this, we introduce a new lexical resource that enriches the Framester knowledge graph with distributional features extracted from text, and show how this hybrid resource yields better results on the task of recognizing frames in running text. Joining distributional and frame semantics builds upon and 1 587 https://framenet.icsi.berkel"
L18-1093,Q14-1019,0,0.0297903,"the number of occurrences of the word w in A. For instance, with respect to the excerpts of Tables 3 and 4 we obtain w(Communication, bn:00085007v) = 15.0. 3. Using the Enriched Representations for Word Frame Disambiguation We evaluate our extensions of Framester profiles following the experimental setting of Gangemi et al. (2016b), and compare the extended and the original profiles in a task of Word Frame Disambiguation (WFD). 3.1. Dataset: FrameNet Full Text Documents To create a silver standard we processed all 108 documents from the FrameNet 1.7 dataset (Baker et al., 1998) with BabelFy (Moro et al., 2014)3 . By combining the original frame annotations with the automatically generated entity links we collected a total of 81,706 annotations, which we use in our experimental setting as a silver standard. 3.2. Word Frame Disambiguation Following the WFD approach described in Gangemi et al. (2016b) we implemented a simple word frame disambiguator, where for each provided annotation in our silver standard we try to predict a frame label only on the basis of the BabelNet synsets generated through BabelFy. In order to provide the most suitable frame label F for the provided BabelNet synset label bs: 1"
L18-1093,W17-1909,1,0.92743,"we hear someone READ a text, our understanding of what we hear is usually spontaneous – the rules by which we interpret meaning. Here, the reference to the ‘Reading aloud’ frame from FrameNet1 could be triggered on the basis of the occurrences of the verbs in the sentence: hear, understanding and interpret. Such connections, in turn, could be provided by a hybrid resource where distributional representations from text have been explicitly linked to semantic knowledge repositories, since hybrid resources of this kind have been shown in the past to improve performance on lexical understanding (Panchenko et al., 2017) as well as taxonomy learning and cleaning (Faralli et al., 2017). In this paper, we bridge the gap between distributional and frame semantics by linking distributional semantic representations to Framester, a knowledge graph that acts as a hub between resources like FrameNet, BabelNet and DBpedia, among others. As a result of this, we introduce a new lexical resource that enriches the Framester knowledge graph with distributional features extracted from text, and show how this hybrid resource yields better results on the task of recognizing frames in running text. Joining distributional and f"
L18-1093,D17-1270,0,0.0292348,"Missing"
L18-1093,C98-1013,0,\N,Missing
L18-1164,S10-1011,0,0.460828,"any language for which a tokenizer, part-of-speech tagger, lemmatizer, and a sense inventory are available. The rest of the paper is organized as follows. Section 2 reviews related work. Section 3 presents the Watasense word sense disambiguation system, presents its architecture, and describes the unsupervised word sense disambiguation methods bundled with it. Section 4 evaluates the system on a gold standard for Russian. Section 5 concludes with final remarks. 2. Related Work Although the problem of WSD has been addressed in many SemEval campaigns (Navigli et al., 2007; Agirre et al., 2010; Manandhar et al., 2010, inter alia), we focus here on word sense disambiguation systems rather than on the research methodologies. Among the freely available systems, IMS (“It Makes Sense”) is a supervised WSD system designed initially for the English language (Zhong and Ng, 2010). The system uses a support vector machine classifier to infer the particular sense of a word in the sentence given its contextual sentence-level features. Pywsd is an implementation of several popular WSD algorithms implemented in a library for the Python programming language.1 It offers both the classical Lesk algorithm for WSD and path-"
L18-1164,P13-4007,0,0.0247526,"arch methodologies. Among the freely available systems, IMS (“It Makes Sense”) is a supervised WSD system designed initially for the English language (Zhong and Ng, 2010). The system uses a support vector machine classifier to infer the particular sense of a word in the sentence given its contextual sentence-level features. Pywsd is an implementation of several popular WSD algorithms implemented in a library for the Python programming language.1 It offers both the classical Lesk algorithm for WSD and path-based algorithms that heavily use the WordNet and similar lexical ontologies. DKPro WSD (Miller et al., 2013) is a generalpurpose framework for WSD that uses a lexical ontology as the sense inventory and offers the variety of WordNet-based algorithms. Babelfy (Moro et al., 2014) is a WSD system that uses BabelNet, a large-scale multilingual lexical ontology available for most natural languages. Due to the broad coverage of BabelNet, Babelfy offers entity linking as part of the WSD functionality. Panchenko et al. (2017b) present an unsupervised WSD system that is also knowledge-free: its sense inventory is induced based on the JoBimText framework, and disambiguation is performed by computing the seman"
L18-1164,Q14-1019,0,0.0545051,"The system uses a support vector machine classifier to infer the particular sense of a word in the sentence given its contextual sentence-level features. Pywsd is an implementation of several popular WSD algorithms implemented in a library for the Python programming language.1 It offers both the classical Lesk algorithm for WSD and path-based algorithms that heavily use the WordNet and similar lexical ontologies. DKPro WSD (Miller et al., 2013) is a generalpurpose framework for WSD that uses a lexical ontology as the sense inventory and offers the variety of WordNet-based algorithms. Babelfy (Moro et al., 2014) is a WSD system that uses BabelNet, a large-scale multilingual lexical ontology available for most natural languages. Due to the broad coverage of BabelNet, Babelfy offers entity linking as part of the WSD functionality. Panchenko et al. (2017b) present an unsupervised WSD system that is also knowledge-free: its sense inventory is induced based on the JoBimText framework, and disambiguation is performed by computing the semantic similarity between the context and the candidate senses (Biemann and Riedl, 2013). Pelevina et al. (2016) proposed a similar approach to WSD, but based on dense vecto"
L18-1164,S07-1006,0,0.112434,"Missing"
L18-1164,D17-2016,1,0.945865,"for the Python programming language.1 It offers both the classical Lesk algorithm for WSD and path-based algorithms that heavily use the WordNet and similar lexical ontologies. DKPro WSD (Miller et al., 2013) is a generalpurpose framework for WSD that uses a lexical ontology as the sense inventory and offers the variety of WordNet-based algorithms. Babelfy (Moro et al., 2014) is a WSD system that uses BabelNet, a large-scale multilingual lexical ontology available for most natural languages. Due to the broad coverage of BabelNet, Babelfy offers entity linking as part of the WSD functionality. Panchenko et al. (2017b) present an unsupervised WSD system that is also knowledge-free: its sense inventory is induced based on the JoBimText framework, and disambiguation is performed by computing the semantic similarity between the context and the candidate senses (Biemann and Riedl, 2013). Pelevina et al. (2016) proposed a similar approach to WSD, but based on dense vector representations (word embeddings), called SenseGram. Similarly to SenseGram, our WSD system is based on averaging of word embeddings on the basis of an automatically induced sense inventory. A crucial difference, however, is that we induce ou"
L18-1164,W16-1620,1,0.943397,"ory and offers the variety of WordNet-based algorithms. Babelfy (Moro et al., 2014) is a WSD system that uses BabelNet, a large-scale multilingual lexical ontology available for most natural languages. Due to the broad coverage of BabelNet, Babelfy offers entity linking as part of the WSD functionality. Panchenko et al. (2017b) present an unsupervised WSD system that is also knowledge-free: its sense inventory is induced based on the JoBimText framework, and disambiguation is performed by computing the semantic similarity between the context and the candidate senses (Biemann and Riedl, 2013). Pelevina et al. (2016) proposed a similar approach to WSD, but based on dense vector representations (word embeddings), called SenseGram. Similarly to SenseGram, our WSD system is based on averaging of word embeddings on the basis of an automatically induced sense inventory. A crucial difference, however, is that we induce our sense inventory from synonymy dictionaries and not distributional word vectors. While this requires more manually created resources, a potential advantage of our approach is that the resulting inventory contains less noise. 3. Watasense, an Unsupervised System for Word Sense Disambiguation Wa"
L18-1164,P17-1145,1,0.925562,"present system and also conduct its evaluation on three different lexical semantic resources for Russian. We found that the dense mode substantially outperforms the sparse one on all datasets according to the adjusted Rand index. Keywords: word sense disambiguation, system, synset induction 1. Introduction Word sense disambiguation (WSD) is a natural language processing task of identifying the particular word senses of polysemous words used in a sentence. Recently, a lot of attention was paid to the problem of WSD for the Russian language (Lopukhin and Lopukhina, 2016; Lopukhin et al., 2017; Ustalov et al., 2017). This problem is especially difficult because of both linguistic issues – namely, the rich morphology of Russian and other Slavic languages in general – and technical challenges like the lack of software and language resources required for addressing the problem. To address these issues, we present Watasense, an unsupervised system for word sense disambiguation. We describe its architecture and conduct an evaluation on three datasets for Russian. The choice of an unsupervised system is motivated by the absence of resources that would enable a supervised system for under-resourced languages. W"
L18-1164,P10-4014,0,0.0428215,"s its architecture, and describes the unsupervised word sense disambiguation methods bundled with it. Section 4 evaluates the system on a gold standard for Russian. Section 5 concludes with final remarks. 2. Related Work Although the problem of WSD has been addressed in many SemEval campaigns (Navigli et al., 2007; Agirre et al., 2010; Manandhar et al., 2010, inter alia), we focus here on word sense disambiguation systems rather than on the research methodologies. Among the freely available systems, IMS (“It Makes Sense”) is a supervised WSD system designed initially for the English language (Zhong and Ng, 2010). The system uses a support vector machine classifier to infer the particular sense of a word in the sentence given its contextual sentence-level features. Pywsd is an implementation of several popular WSD algorithms implemented in a library for the Python programming language.1 It offers both the classical Lesk algorithm for WSD and path-based algorithms that heavily use the WordNet and similar lexical ontologies. DKPro WSD (Miller et al., 2013) is a generalpurpose framework for WSD that uses a lexical ontology as the sense inventory and offers the variety of WordNet-based algorithms. Babelfy"
L18-1244,P11-1062,0,0.0300176,"arge-scale repository of automatically extracted hypernyms to date. 2.2. Taxonomy and Ontology Learning Most relevant in the context of automatic construction of lexical resource are methods for building resources from text (Caraballo, 1999; Biemann, 2005; Cimiano, 2006; Bordea et al., 2015; Velardi et al., 2013) as opposed to methods that automatically construct resources from semistructured data (Auer et al., 2007; Navigli and Ponzetto, 2012) or using crowdsourcing (Biemann, 2013; Braslavski et al., 2016). Our representation differs from the global hierarchy of words as constructed e.g. by (Berant et al., 2011; Faralli et al., 2016), as we are grouping many lexical items into a labeled sense cluster as opposed to organizing them in deep hierarchies. Kozareva and Hovy (2013) proposed a taxonomy induction method based on extraction of hypernyms using the doubly-anchored lexical patterns. Graph 2 http://www.commoncrawl.org algorithms are used to induce a proper tree from the binary relations harvested from text. 2.3. Induction of Semantic Classes This line of research starts with (Lin and Pantel, 2001), where sets of similar words are clustered into concepts. While this approach performs a hard cluste"
L18-1244,W06-3812,1,0.812843,"age” semantic classes. Similarly to the induced word senses, the semantic classes are labeled with hypernyms. In contrast to the induced word senses, which represent a local clustering of word senses (related to a given word) semantic classes represent a global sense clustering of word senses. One sense c, such as “apple#0”, can appear only in a single cluster. of related ambiguous terms (Biemann and Riedl, 2013); (2) word sense induction via clustering of ego networks (Widdows and Dorow, 2002; Everett and Borgatti, 2005) of related words using the Chinese Whispers graph clustering algorithm (Biemann, 2006); (3) disambiguation of related words and hypernyms. The word sense inventory used in our experiment3 was extracted from a 9.3 billion tokens corpus, which is a concatenation of Wikipedia4 , ukWac (Ferraresi et al., 2008), LCC (Richter et al., 2006) and Gigaword (Graff and Cieri, 2003). Note that analogous graphs of senses can be obtained using word sense embeddings, see (Neelakantan et al., 2014; Bartunov et al., 2016). Similarly to any other distributional word graph, the induced sense inventory sense network is scale-free, cf. (Steyvers and Tenenbaum, 2005). Our experiments show that a glob"
L18-1244,S15-2151,1,0.859596,"ypernyms, in contrast to our method, they do not take into account word senses and global distributional structure. Seitner et al. (2016) performed extraction of hypernyms from the web-scale Common Crawl2 text corpus to ensure high lexical coverage. In our experiments, we use this webscale database of noisy hypernyms, as the large-scale repository of automatically extracted hypernyms to date. 2.2. Taxonomy and Ontology Learning Most relevant in the context of automatic construction of lexical resource are methods for building resources from text (Caraballo, 1999; Biemann, 2005; Cimiano, 2006; Bordea et al., 2015; Velardi et al., 2013) as opposed to methods that automatically construct resources from semistructured data (Auer et al., 2007; Navigli and Ponzetto, 2012) or using crowdsourcing (Biemann, 2013; Braslavski et al., 2016). Our representation differs from the global hierarchy of words as constructed e.g. by (Berant et al., 2011; Faralli et al., 2016), as we are grouping many lexical items into a labeled sense cluster as opposed to organizing them in deep hierarchies. Kozareva and Hovy (2013) proposed a taxonomy induction method based on extraction of hypernyms using the doubly-anchored lexical"
L18-1244,S16-1168,0,0.0609794,"thermore, we show the utility of our method in the domain taxonomy induction task, achieving the state-of-the-art results on a SemEval’16 task on taxonomy induction. Keywords: semantic classes, distributional semantics, hypernyms, co-hyponyms, word sense induction 1. Introduction Hypernyms are useful in various applications, such as question answering (Zhou et al., 2013), query expansion (Gong et al., 2005), and semantic role labelling (Shi and Mihalcea, 2005) as they can help to overcome sparsity of statistical models. Hypernyms are also the building blocks for learning taxonomies from text (Bordea et al., 2016). Consider the following sentence: “This caf´e serves fresh mangosteen juice”. Here the infrequent word “mangosteen” may be poorly represented or even absent in the vocabulary of a statistical model, yet it can be substituted by lexical items with better representations, which carry close meaning, such as its hypernym “fruit” or one of its close co-hyponyms, e.g. “mango”. Currently available approaches to hypernymy extraction focus on the acquisition of individual binary hypernymy relations (Hearst, 1992; Snow et al., 2004; Weeds et al., 2014; Shwartz et al., 2016; Glavaˇs and Ponzetto, 2017)."
L18-1244,2016.gwc-1.10,1,0.335932,"to ensure high lexical coverage. In our experiments, we use this webscale database of noisy hypernyms, as the large-scale repository of automatically extracted hypernyms to date. 2.2. Taxonomy and Ontology Learning Most relevant in the context of automatic construction of lexical resource are methods for building resources from text (Caraballo, 1999; Biemann, 2005; Cimiano, 2006; Bordea et al., 2015; Velardi et al., 2013) as opposed to methods that automatically construct resources from semistructured data (Auer et al., 2007; Navigli and Ponzetto, 2012) or using crowdsourcing (Biemann, 2013; Braslavski et al., 2016). Our representation differs from the global hierarchy of words as constructed e.g. by (Berant et al., 2011; Faralli et al., 2016), as we are grouping many lexical items into a labeled sense cluster as opposed to organizing them in deep hierarchies. Kozareva and Hovy (2013) proposed a taxonomy induction method based on extraction of hypernyms using the doubly-anchored lexical patterns. Graph 2 http://www.commoncrawl.org algorithms are used to induce a proper tree from the binary relations harvested from text. 2.3. Induction of Semantic Classes This line of research starts with (Lin and Pantel,"
L18-1244,P99-1016,0,0.383204,"use distributional features for extraction of hypernyms, in contrast to our method, they do not take into account word senses and global distributional structure. Seitner et al. (2016) performed extraction of hypernyms from the web-scale Common Crawl2 text corpus to ensure high lexical coverage. In our experiments, we use this webscale database of noisy hypernyms, as the large-scale repository of automatically extracted hypernyms to date. 2.2. Taxonomy and Ontology Learning Most relevant in the context of automatic construction of lexical resource are methods for building resources from text (Caraballo, 1999; Biemann, 2005; Cimiano, 2006; Bordea et al., 2015; Velardi et al., 2013) as opposed to methods that automatically construct resources from semistructured data (Auer et al., 2007; Navigli and Ponzetto, 2012) or using crowdsourcing (Biemann, 2013; Braslavski et al., 2016). Our representation differs from the global hierarchy of words as constructed e.g. by (Berant et al., 2011; Faralli et al., 2016), as we are grouping many lexical items into a labeled sense cluster as opposed to organizing them in deep hierarchies. Kozareva and Hovy (2013) proposed a taxonomy induction method based on extract"
L18-1244,P14-1113,0,0.0239862,"e-aware distributional semantic classes are induced from a text corpus and then used to filter noisy hypernyms database (e.g. extracted by an external method from a text corpus). hypernyms and co-hyponyms include (Roller et al., 2014; Weeds et al., 2014; Necsulescu et al., 2015; Vylomova et al., 2016). They rely on two distributional vectors to characterize a relation between two words, e.g. on the basis of the difference of such vectors or their concatenation. Levy et al. (2015) discovered a tendency to lexical memorization of such approaches, hampering their generalization to other domains. Fu et al. (2014) relied on an alternative approach where a projection matrix is learned, which transforms a distributional vector of a hyponym to the vector of its hypernym. Ustalov et al. (2017a) improved this method by adding regularizers in the model that take into account negative training samples and the asymmetric nature of the hypernyms. Recent approaches to hypernym extraction focused on learning supervised models based on a combination of syntactic patterns and distributional features (Shwartz et al., 2016). Note that while methods, such as (Mirkin et al., 2006) and (Shwartz et al., 2016) use distrib"
L18-1244,D17-1185,1,0.891904,"Missing"
L18-1244,C92-2082,0,0.731648,"models. Hypernyms are also the building blocks for learning taxonomies from text (Bordea et al., 2016). Consider the following sentence: “This caf´e serves fresh mangosteen juice”. Here the infrequent word “mangosteen” may be poorly represented or even absent in the vocabulary of a statistical model, yet it can be substituted by lexical items with better representations, which carry close meaning, such as its hypernym “fruit” or one of its close co-hyponyms, e.g. “mango”. Currently available approaches to hypernymy extraction focus on the acquisition of individual binary hypernymy relations (Hearst, 1992; Snow et al., 2004; Weeds et al., 2014; Shwartz et al., 2016; Glavaˇs and Ponzetto, 2017). Frequencies of the extracted relations usually follow a power-law, with a long tail of noisy extractions containing rare words. We propose a method that performs post-processing of such noisy binary hypernyms using distributional semantics, cf. Figure 1. Namely, we use the observation that distributionally related words are often are co-hyponyms (Wandmacher, 2005; Heylen et al., 2008) and operationalize it to perform filtering of noisy relations by finding dense graphs composed of both hypernyms and co-"
L18-1244,heylen-etal-2008-modelling,0,0.0368325,"ango”. Currently available approaches to hypernymy extraction focus on the acquisition of individual binary hypernymy relations (Hearst, 1992; Snow et al., 2004; Weeds et al., 2014; Shwartz et al., 2016; Glavaˇs and Ponzetto, 2017). Frequencies of the extracted relations usually follow a power-law, with a long tail of noisy extractions containing rare words. We propose a method that performs post-processing of such noisy binary hypernyms using distributional semantics, cf. Figure 1. Namely, we use the observation that distributionally related words are often are co-hyponyms (Wandmacher, 2005; Heylen et al., 2008) and operationalize it to perform filtering of noisy relations by finding dense graphs composed of both hypernyms and co-hyponyms. The contribution of the paper is an unsupervised method for post-processing of noisy hypernymy relations based on clustering of graphs of word senses induced from text. The idea to use distributional semantics to find hypernyms seems natural and has been widely used. However, the existing methods used distributional, yet sense-unaware and local features. We are the first to use global sense-aware distributional structure via the induced semantic classes to improve"
L18-1244,N15-1098,1,0.894317,"Missing"
L18-1244,C14-2023,0,0.0820281,"of 4,870 relations using lexical split by hyponyms. All relations from Hcluster and Horig of one hyponym were included in the sample. These relations were subsequently annotated by human judges using crowdsourcing. We asked crowdworkers to provide a binary judgment about the correctness of each hypernymy relation as illustrated in Figure 7. 6.2.3. Results Overall, 298 annotators completed 4,870 unique tasks each labeled 6.9 times on average, resulting in a total of 33,719 binary human judgments about hypernyms. We obtained a fair agreement among annotators of 0.548 in terms of the Randolph κ (Meyer et al., 2014). Since CrowdFlower reports a confidence for each answer, we selected N = 3 most confident answers per pair and aggregated them using weighted majority voting. The ties were broken pessimistically, i.e. by treating a hypernym as irrelevant. Results for N ∈ 3, 5, 6 varied less than by 0.002 in terms of F-score. The task received the rating of a 4.4 out of 5.0 according to the annotator’s feedback mechanism. Table 5 presents results of the experiment. Since each pair received a binary score, we calculated Precision, Recall, and F-measure of two compared methods. Our denoising method improves the"
L18-1244,P06-2075,0,0.0444314,"ing their generalization to other domains. Fu et al. (2014) relied on an alternative approach where a projection matrix is learned, which transforms a distributional vector of a hyponym to the vector of its hypernym. Ustalov et al. (2017a) improved this method by adding regularizers in the model that take into account negative training samples and the asymmetric nature of the hypernyms. Recent approaches to hypernym extraction focused on learning supervised models based on a combination of syntactic patterns and distributional features (Shwartz et al., 2016). Note that while methods, such as (Mirkin et al., 2006) and (Shwartz et al., 2016) use distributional features for extraction of hypernyms, in contrast to our method, they do not take into account word senses and global distributional structure. Seitner et al. (2016) performed extraction of hypernyms from the web-scale Common Crawl2 text corpus to ensure high lexical coverage. In our experiments, we use this webscale database of noisy hypernyms, as the large-scale repository of automatically extracted hypernyms to date. 2.2. Taxonomy and Ontology Learning Most relevant in the context of automatic construction of lexical resource are methods for bu"
L18-1244,S15-1021,0,0.0148946,"obal Sense Graph §3.4 Induced Word Senses §3.2 §3.1 Representing Senses Sense Graph Word Sense Induction Clustering of with Ego Networks Construction from Text Corpus Word Senes Global Sense Clusters §4 Labeling Sense Clusters with Hypernyms Noisy Hypernyms Text Corpus Cleansed Hypernyms Semantic Classes Figure 2: Outline of our approach: sense-aware distributional semantic classes are induced from a text corpus and then used to filter noisy hypernyms database (e.g. extracted by an external method from a text corpus). hypernyms and co-hyponyms include (Roller et al., 2014; Weeds et al., 2014; Necsulescu et al., 2015; Vylomova et al., 2016). They rely on two distributional vectors to characterize a relation between two words, e.g. on the basis of the difference of such vectors or their concatenation. Levy et al. (2015) discovered a tendency to lexical memorization of such approaches, hampering their generalization to other domains. Fu et al. (2014) relied on an alternative approach where a projection matrix is learned, which transforms a distributional vector of a hyponym to the vector of its hypernym. Ustalov et al. (2017a) improved this method by adding regularizers in the model that take into account n"
L18-1244,D14-1113,0,0.099783,"s (Biemann and Riedl, 2013); (2) word sense induction via clustering of ego networks (Widdows and Dorow, 2002; Everett and Borgatti, 2005) of related words using the Chinese Whispers graph clustering algorithm (Biemann, 2006); (3) disambiguation of related words and hypernyms. The word sense inventory used in our experiment3 was extracted from a 9.3 billion tokens corpus, which is a concatenation of Wikipedia4 , ukWac (Ferraresi et al., 2008), LCC (Richter et al., 2006) and Gigaword (Graff and Cieri, 2003). Note that analogous graphs of senses can be obtained using word sense embeddings, see (Neelakantan et al., 2014; Bartunov et al., 2016). Similarly to any other distributional word graph, the induced sense inventory sense network is scale-free, cf. (Steyvers and Tenenbaum, 2005). Our experiments show that a global clustering of this network can lead to a discovery of giant components, which are useless in our context as they represent no semantic class. To overcome this problem, we re-build the sense network as described below. 3.2. Representing Senses with Ego Networks To perform a global clustering of senses, we represent each induced sense s by a second-order ego network (Everett and Borgatti, 2005)."
L18-1244,S16-1206,1,0.906671,"Missing"
L18-1244,D17-2016,1,0.832821,"aluate our approach, we performed three experiments. A large-scale crowdsourcing study indicated a high plausibility of extracted semantic classes according to human judgment. Besides, we demonstrated that our approach helps to improve precision and recall of a hypernymy extraction method. Finally, we showed how the proposed semantic classes can be used to improve domain taxonomy induction from text. While we have demonstrated the utility of our approach for hypernym extraction and taxonomy induction, we believe that the induced semantic classes can be useful in other tasks. For instance, in (Panchenko et al., 2017) these semantic classes were used as an inventory for word sense disambiguation to deal with out of vocabulary words. 8. Acknowledgements This research was supported by Deutscher Akademischer Austauschdienst (DAAD), Deutsche Forschungsgemeinschaft (DFG) under the project ”Joining Ontologies and Semantics Induced from Text” (JOIN-T), and by the Ministry of Education and Science of the Russian Federation Agreement no. 02.A03.21.0006. We are grateful to three anonymous reviewers for their helpful comments. Finally, we are grateful to Dirk Johannßen for providing feedback on an early version of th"
L18-1244,P06-1015,0,0.0996737,"rnymy relations using distributionally induced semantic classes, represented by clusters of induced word senses labeled with noisy hypernyms. The word postfix, such as #1, is an ID of an induced sense. The wrong hypernyms outside the cluster labels are removed, while the missing ones not present in the noisy database of hypernyms are added. 2. 2.1. Related Work Extraction of Hypernyms In her pioneering work, Hearst (1992) proposed to extract hypernyms based on lexical-syntactic patterns from text. Snow et al. (2004) learned such patterns automatically based on a set of hyponym-hypernym pairs. Pantel and Pennacchiotti (2006) presented another approach for weakly supervised extraction of similar extraction patterns. These approaches use some training pairs of hypernyms to bootstrap the pattern discovery process. For instance, Tjong Kim Sang (2007) used web snippets as a corpus for extraction of hypernyms. More recent approaches exploring the use of distributional word representations for extraction of 1541 1 https://github.com/uhh-lt/mangosteen §3 Induction of Semantic Classes Sense Ego-Networks §3.3 Global Sense Graph §3.4 Induced Word Senses §3.2 §3.1 Representing Senses Sense Graph Word Sense Induction Clusteri"
L18-1244,N04-1041,0,0.578714,"roposed a taxonomy induction method based on extraction of hypernyms using the doubly-anchored lexical patterns. Graph 2 http://www.commoncrawl.org algorithms are used to induce a proper tree from the binary relations harvested from text. 2.3. Induction of Semantic Classes This line of research starts with (Lin and Pantel, 2001), where sets of similar words are clustered into concepts. While this approach performs a hard clustering and does not label clusters, these drawbacks are addressed in (Pantel and Lin, 2002), where words can belong to several clusters, thus representing senses, and in (Pantel and Ravichandran, 2004), where authors aggregate hypernyms per cluster, which come from Hearst patterns. The main difference to our approach is that we explicitly represent senses both in clusters and in their hypernym labels, which enables us to connect our sense clusters into a global taxonomic structure. Consequently, we are the first to use semantic classes to improve hypernymy extraction. Ustalov et al. (2017b) proposed a synset induction approach based on global clustering of word senses. The authors used the graph constructed of dictionary synonyms, while we use distributionally-induced graphs of senses. 3. U"
L18-1244,C14-1097,0,0.161869,"mantic Classes Sense Ego-Networks §3.3 Global Sense Graph §3.4 Induced Word Senses §3.2 §3.1 Representing Senses Sense Graph Word Sense Induction Clustering of with Ego Networks Construction from Text Corpus Word Senes Global Sense Clusters §4 Labeling Sense Clusters with Hypernyms Noisy Hypernyms Text Corpus Cleansed Hypernyms Semantic Classes Figure 2: Outline of our approach: sense-aware distributional semantic classes are induced from a text corpus and then used to filter noisy hypernyms database (e.g. extracted by an external method from a text corpus). hypernyms and co-hyponyms include (Roller et al., 2014; Weeds et al., 2014; Necsulescu et al., 2015; Vylomova et al., 2016). They rely on two distributional vectors to characterize a relation between two words, e.g. on the basis of the difference of such vectors or their concatenation. Levy et al. (2015) discovered a tendency to lexical memorization of such approaches, hampering their generalization to other domains. Fu et al. (2014) relied on an alternative approach where a projection matrix is learned, which transforms a distributional vector of a hyponym to the vector of its hypernym. Ustalov et al. (2017a) improved this method by adding regul"
L18-1244,L16-1056,1,0.932779,"rnym. Ustalov et al. (2017a) improved this method by adding regularizers in the model that take into account negative training samples and the asymmetric nature of the hypernyms. Recent approaches to hypernym extraction focused on learning supervised models based on a combination of syntactic patterns and distributional features (Shwartz et al., 2016). Note that while methods, such as (Mirkin et al., 2006) and (Shwartz et al., 2016) use distributional features for extraction of hypernyms, in contrast to our method, they do not take into account word senses and global distributional structure. Seitner et al. (2016) performed extraction of hypernyms from the web-scale Common Crawl2 text corpus to ensure high lexical coverage. In our experiments, we use this webscale database of noisy hypernyms, as the large-scale repository of automatically extracted hypernyms to date. 2.2. Taxonomy and Ontology Learning Most relevant in the context of automatic construction of lexical resource are methods for building resources from text (Caraballo, 1999; Biemann, 2005; Cimiano, 2006; Bordea et al., 2015; Velardi et al., 2013) as opposed to methods that automatically construct resources from semistructured data (Auer et"
L18-1244,P16-1226,0,0.239693,"earning taxonomies from text (Bordea et al., 2016). Consider the following sentence: “This caf´e serves fresh mangosteen juice”. Here the infrequent word “mangosteen” may be poorly represented or even absent in the vocabulary of a statistical model, yet it can be substituted by lexical items with better representations, which carry close meaning, such as its hypernym “fruit” or one of its close co-hyponyms, e.g. “mango”. Currently available approaches to hypernymy extraction focus on the acquisition of individual binary hypernymy relations (Hearst, 1992; Snow et al., 2004; Weeds et al., 2014; Shwartz et al., 2016; Glavaˇs and Ponzetto, 2017). Frequencies of the extracted relations usually follow a power-law, with a long tail of noisy extractions containing rare words. We propose a method that performs post-processing of such noisy binary hypernyms using distributional semantics, cf. Figure 1. Namely, we use the observation that distributionally related words are often are co-hyponyms (Wandmacher, 2005; Heylen et al., 2008) and operationalize it to perform filtering of noisy relations by finding dense graphs composed of both hypernyms and co-hyponyms. The contribution of the paper is an unsupervised me"
L18-1244,steinberger-etal-2006-jrc,0,0.0164921,"used to improve results of other state-of-the-art hypernymy extraction approaches, such as HypeNET (Shwartz et al., 2016). 6.3. Experiment 3: Improving Domain Taxonomy Induction In this section, we show how the labeled semantic classes can be used for induction of domain taxonomies. 6.3.1. SemEval 2016 Task 13 We use the taxonomy extraction evaluation dataset by Bordea et al. (2016), featuring gold standard taxonomies for three domains (Food, Science, Environment) and four languages (English, Dutch, French, and Italian) on the basis of existing lexical resources, such as WordNet and Eurovoc (Steinberger et al., 2006).7 Participants were supposed to build a taxonomy provided a vocabulary of a domain. Since our other experiments were conducted on English, we used the English part of the task. The evaluation is 1547 7 http://eurovoc.europa.eu System / Domain, Dataset Food, WordNet Science, WordNet Food, Combined Science, Combined Science, Eurovoc Environment, Eurovoc WordNet 1.0000 1.0000 0.5870 0.5760 0.6243 n.a. Baseline JUNLP NUIG-UNLP QASSIT TAXI USAAR 0.0022 0.1925 n.a. n.a. 0.3260 0.0021 0.0016 0.0494 0.0027 0.2255 0.2255 0.0008 0.0019 0.2608 n.a. n.a. 0.2021 0.0000 0.0163 0.1774 0.0090 0.5757 0.3634 0"
L18-1244,P07-2042,0,0.0798013,"Missing"
L18-1244,E17-2087,1,0.887253,"us). hypernyms and co-hyponyms include (Roller et al., 2014; Weeds et al., 2014; Necsulescu et al., 2015; Vylomova et al., 2016). They rely on two distributional vectors to characterize a relation between two words, e.g. on the basis of the difference of such vectors or their concatenation. Levy et al. (2015) discovered a tendency to lexical memorization of such approaches, hampering their generalization to other domains. Fu et al. (2014) relied on an alternative approach where a projection matrix is learned, which transforms a distributional vector of a hyponym to the vector of its hypernym. Ustalov et al. (2017a) improved this method by adding regularizers in the model that take into account negative training samples and the asymmetric nature of the hypernyms. Recent approaches to hypernym extraction focused on learning supervised models based on a combination of syntactic patterns and distributional features (Shwartz et al., 2016). Note that while methods, such as (Mirkin et al., 2006) and (Shwartz et al., 2016) use distributional features for extraction of hypernyms, in contrast to our method, they do not take into account word senses and global distributional structure. Seitner et al. (2016) perf"
L18-1244,P17-1145,1,0.586116,"Missing"
L18-1244,J13-3007,1,0.933854,"to our method, they do not take into account word senses and global distributional structure. Seitner et al. (2016) performed extraction of hypernyms from the web-scale Common Crawl2 text corpus to ensure high lexical coverage. In our experiments, we use this webscale database of noisy hypernyms, as the large-scale repository of automatically extracted hypernyms to date. 2.2. Taxonomy and Ontology Learning Most relevant in the context of automatic construction of lexical resource are methods for building resources from text (Caraballo, 1999; Biemann, 2005; Cimiano, 2006; Bordea et al., 2015; Velardi et al., 2013) as opposed to methods that automatically construct resources from semistructured data (Auer et al., 2007; Navigli and Ponzetto, 2012) or using crowdsourcing (Biemann, 2013; Braslavski et al., 2016). Our representation differs from the global hierarchy of words as constructed e.g. by (Berant et al., 2011; Faralli et al., 2016), as we are grouping many lexical items into a labeled sense cluster as opposed to organizing them in deep hierarchies. Kozareva and Hovy (2013) proposed a taxonomy induction method based on extraction of hypernyms using the doubly-anchored lexical patterns. Graph 2 http:"
L18-1244,P16-1158,0,0.0305763,"uced Word Senses §3.2 §3.1 Representing Senses Sense Graph Word Sense Induction Clustering of with Ego Networks Construction from Text Corpus Word Senes Global Sense Clusters §4 Labeling Sense Clusters with Hypernyms Noisy Hypernyms Text Corpus Cleansed Hypernyms Semantic Classes Figure 2: Outline of our approach: sense-aware distributional semantic classes are induced from a text corpus and then used to filter noisy hypernyms database (e.g. extracted by an external method from a text corpus). hypernyms and co-hyponyms include (Roller et al., 2014; Weeds et al., 2014; Necsulescu et al., 2015; Vylomova et al., 2016). They rely on two distributional vectors to characterize a relation between two words, e.g. on the basis of the difference of such vectors or their concatenation. Levy et al. (2015) discovered a tendency to lexical memorization of such approaches, hampering their generalization to other domains. Fu et al. (2014) relied on an alternative approach where a projection matrix is learned, which transforms a distributional vector of a hyponym to the vector of its hypernym. Ustalov et al. (2017a) improved this method by adding regularizers in the model that take into account negative training samples"
L18-1244,2005.jeptalnrecital-recital.1,0,0.0247916,"-hyponyms, e.g. “mango”. Currently available approaches to hypernymy extraction focus on the acquisition of individual binary hypernymy relations (Hearst, 1992; Snow et al., 2004; Weeds et al., 2014; Shwartz et al., 2016; Glavaˇs and Ponzetto, 2017). Frequencies of the extracted relations usually follow a power-law, with a long tail of noisy extractions containing rare words. We propose a method that performs post-processing of such noisy binary hypernyms using distributional semantics, cf. Figure 1. Namely, we use the observation that distributionally related words are often are co-hyponyms (Wandmacher, 2005; Heylen et al., 2008) and operationalize it to perform filtering of noisy relations by finding dense graphs composed of both hypernyms and co-hyponyms. The contribution of the paper is an unsupervised method for post-processing of noisy hypernymy relations based on clustering of graphs of word senses induced from text. The idea to use distributional semantics to find hypernyms seems natural and has been widely used. However, the existing methods used distributional, yet sense-unaware and local features. We are the first to use global sense-aware distributional structure via the induced semant"
L18-1244,C14-1212,0,0.297993,"uilding blocks for learning taxonomies from text (Bordea et al., 2016). Consider the following sentence: “This caf´e serves fresh mangosteen juice”. Here the infrequent word “mangosteen” may be poorly represented or even absent in the vocabulary of a statistical model, yet it can be substituted by lexical items with better representations, which carry close meaning, such as its hypernym “fruit” or one of its close co-hyponyms, e.g. “mango”. Currently available approaches to hypernymy extraction focus on the acquisition of individual binary hypernymy relations (Hearst, 1992; Snow et al., 2004; Weeds et al., 2014; Shwartz et al., 2016; Glavaˇs and Ponzetto, 2017). Frequencies of the extracted relations usually follow a power-law, with a long tail of noisy extractions containing rare words. We propose a method that performs post-processing of such noisy binary hypernyms using distributional semantics, cf. Figure 1. Namely, we use the observation that distributionally related words are often are co-hyponyms (Wandmacher, 2005; Heylen et al., 2008) and operationalize it to perform filtering of noisy relations by finding dense graphs composed of both hypernyms and co-hyponyms. The contribution of the paper"
L18-1244,C02-1114,0,0.354924,"ogy#0, language#0, format#2, app#0 Table 2: Sample of the induced sense clusters representing “fruits” and “programming language” semantic classes. Similarly to the induced word senses, the semantic classes are labeled with hypernyms. In contrast to the induced word senses, which represent a local clustering of word senses (related to a given word) semantic classes represent a global sense clustering of word senses. One sense c, such as “apple#0”, can appear only in a single cluster. of related ambiguous terms (Biemann and Riedl, 2013); (2) word sense induction via clustering of ego networks (Widdows and Dorow, 2002; Everett and Borgatti, 2005) of related words using the Chinese Whispers graph clustering algorithm (Biemann, 2006); (3) disambiguation of related words and hypernyms. The word sense inventory used in our experiment3 was extracted from a 9.3 billion tokens corpus, which is a concatenation of Wikipedia4 , ukWac (Ferraresi et al., 2008), LCC (Richter et al., 2006) and Gigaword (Graff and Cieri, 2003). Note that analogous graphs of senses can be obtained using word sense embeddings, see (Neelakantan et al., 2014; Bartunov et al., 2016). Similarly to any other distributional word graph, the induc"
L18-1286,C14-1076,0,0.0222431,"identified in the 251.92 billion tokens output corpus. 3.3.3. Dependency Parsing To make large-scale parsing of texts possible, a parser needs to be not only reasonably accurate but also fast. Unfortunately, the most accurate parsers, such as Stanford parser based on the PCFG grammar (De Marneffe et al., 2006), according to our experiments, take up to 60 minutes to process 1 Mb of text on a single core, which was prohibitively slow for our use-case (details of the hardware configuration are available in Section 3.5.). We tested all versions of the Stanford, Malt (Hall et al., 2010), and Mate (Ballesteros and Bohnet, 2014) parsers for English available via the DKPro Core framework. To dependency-parse texts, we selected the Malt parser, due to an optimal ratio of efficiency and effectiveness (parsing of 1 Mb of text per core in 1–4 minutes). This parser was successfully used in the past for the construction of linguistically analyzed web corpora, such as P UK WAC (Baroni et al., 2009) and ENCOW16 (Sch¨afer, 2015). While more accurate parsers exist, e.g. the Stanford parser, according to our experiments, even the neural-based version of this parser is substantially slower. On the other hand, as shown by Chen and"
L18-1286,P01-1005,0,0.0320709,"is a combination of Wikipedia with two We compute syntactic count-based distributional representations of words using the JoBimText framework (Biemann 1821 30 https://github.com/uhh-lt/josimtext other corpora, we can reach the even better result by training the model (with exactly the same parameters) on the dependency-based features extracted from the full D EP CC corpus. This model substantially outperforms also the prior state of the art models, e.g. (Baroni et al., 2014) and (Gerz et al., 2016), on the SimVerb dataset, through the sheer size of the input corpus, as previously shown, e.g. (Banko and Brill, 2001) inter alia. 5.3.3. Differences in Performance for Test/Train Sets For the SimVerb dataset, the absolute performance on the test part (SimVerb500) is higher than the absolute performance on the train part (SimVerb300) for almost all models, including the baselines. We attribute this to a specific split of the data in the dataset: our models do not use the training data to learn verb representations. 6. Conclusion In this paper, we introduced a new web-scale corpus of English texts extracted from the C OMMON C RAWL, the largest openly available linguistically analyzed corpus to date, according"
L18-1286,P14-1023,0,0.148916,"e any copyrights as the authors of this derivative resource, but while using the D EP CC corpus you need to make sure to respect the Terms of Use of the original C OMMON C RAWL dataset it is based on.29 22 https://aws.amazon.com https://aws.amazon.com/ec2 24 https://aws.amazon.com/emr 25 https://www.elastic.co/guide/en/kibana/ current/lucene-query.html 26 23 https://www.elastic.co https://www.elastic.co/products/kibana 28 https://github.com/uhh-lt/josimtext 29 http://commoncrawl.org/terms-of-use 27 1820 Model SimVerb3500 SimVerb3000 SimVerb500 SimLex222 Wikipedia+ukWaC+BNC: Count SVD 500-dim (Baroni et al., 2014) PolyglotWikipedia: SGNS BOW 300-dim (Gerz et al., 2016) 8B: SGNS BOW 500-dim (Gerz et al., 2016) 8B: SGNS DEPS 500-dim (Gerz et al., 2016) PolyglotWikipedia:SGNS DEPS 300-dim (Gerz et al., 2016) 0.196 0.274 0.348 0.356 0.313 0.186 0.333 0.350 0.351 0.304 0.259 0.265 0.378 0.389 0.401 0.200 0.328 0.307 0.385 0.390 Wikipedia: LMI DEPS wpf-1000 fpw-2000 Wikipedia+ukWac+GigaWord: LMI DEPS wpf-1000 fpw-2000 D EP CC: LMI DEPS wpf-1000 fpw-1000 D EP CC: LMI DEPS wpf-1000 fpw-2000 D EP CC: LMI DEPS wpf-2000 fpw-2000 D EP CC: LMI DEPS wpf-5000 fpw-5000 0.283 0.376 0.400 0.404 0.399 0.382 0.284 0.368 0"
L18-1286,D14-1082,0,0.0495504,"t, 2014) parsers for English available via the DKPro Core framework. To dependency-parse texts, we selected the Malt parser, due to an optimal ratio of efficiency and effectiveness (parsing of 1 Mb of text per core in 1–4 minutes). This parser was successfully used in the past for the construction of linguistically analyzed web corpora, such as P UK WAC (Baroni et al., 2009) and ENCOW16 (Sch¨afer, 2015). While more accurate parsers exist, e.g. the Stanford parser, according to our experiments, even the neural-based version of this parser is substantially slower. On the other hand, as shown by Chen and Manning (2014), the performance of the Malt parser is only about 1.5–2.5 points below the neural-based Stanford parser. In particular, we used the stack model based on the projective transition system with the Malt.19 11 http://commoncrawl.org/2016/02/ http://index.commoncrawl.org/ CC-MAIN-2016-07 13 s3://commoncrawl/crawl-data/ CC-MAIN-2016-07 14 s3://commoncrawl/contrib/c4corpus/ CC-MAIN-2016-07 12 15 https://hadoop.apache.org https://uima.apache.org 17 https://github.com/uhh-lt/lefex 18 stanfordnlp-model-ner-en-all.3class.distsim.crf, 20.04.2015 19 The used model is de.tudarmstadt.ukp.dkpro.core.maltpars"
L18-1286,de-marneffe-etal-2006-generating,0,0.284992,"Missing"
L18-1286,W14-5201,0,0.0353294,"Missing"
L18-1286,P05-1045,0,0.0307133,"r the CC-BY license. 3.3. Linguistic Analysis of Texts Linguistic analysis consists of four stages presented in Figure 1 and is implemented using the Apache Hadoop framework15 for parallelization and the Apache UIMA framework16 for integration of linguistic analysers via the DKPro Core library (Eckart de Castilho and Gurevych, 2014).17 3.3.1. POS Tagging and Lemmatization For morphological analysis of texts, we used OpenNLP part-of-speech tagger and Stanford lemmatizer. 3.3.2. Named Entity Recognition To detect occurrences of persons, locations, and organizations we use the Stanford NER tool (Finkel et al., 2005).18 Overall, 7.48 billion occurrences of named entities were identified in the 251.92 billion tokens output corpus. 3.3.3. Dependency Parsing To make large-scale parsing of texts possible, a parser needs to be not only reasonably accurate but also fast. Unfortunately, the most accurate parsers, such as Stanford parser based on the PCFG grammar (De Marneffe et al., 2006), according to our experiments, take up to 60 minutes to process 1 Mb of text on a single core, which was prohibitively slow for our use-case (details of the hardware configuration are available in Section 3.5.). We tested all v"
L18-1286,P14-1097,0,0.38862,"Missing"
L18-1286,P14-2050,0,0.0413078,"art largescale experiments with syntax-aware models without the need of long and resource-intensive preprocessing. We built an index of sentences and their linguistic meta-data accessible though an interactive web-based search interface or via a RESTful API. In our experiments on the verb similarity task, a distributional model trained on the new corpus outperformed models trained on the smaller corpora, like Wikipedia, reaching new state of the art of verb similarity on the SimVerb3500 dataset. The corpus can be used in various contexts, ranging from training of syntax-based word embeddings (Levy and Goldberg, 2014) to unsupervised induction of word senses (Biemann et al., 2018) and frame structures (Kawahara et al., 2014). A promising direction of future work is using the proposed technology for building corpora in multiple languages. 7. Acknowledgements This research was supported by the Deutsche Forschungsgemeinschaft (DFG) under the project ”Joining Ontologies and Semantics Induced from Text” (JOIN-T). We are grateful to Amazon for providing required computational resources though the “AWS Cloud Credits for Research” program. Finally, we thank Kiril Gashteovski and three anonymous reviewers for their"
L18-1286,D14-1162,0,0.0799156,"Missing"
L18-1286,D14-1101,0,0.0153647,"Ginter, 2014). The texts were morphologically and syntactically analyzed. In addition, distributional vector space representations of the words were obtained using the word2vec toolkit (Mikolov et al., 2013). The resources were made available under an open license. GloVe (Pennington et al., 2014) is an unsupervised model for learning distributional word representations similar to word2vec. The authors distribute10 two models trained on the English part of a C OMMON C RAWL corpus (comprising respectively 42 and 820 billion of tokens), which are often used to build neural NLP systems, such as (Tsuboi, 2014). The models were trained on the C OMMON C RAWL documents texts tokenized with the Stanford tokenizer. In addition, the smaller training corpus was lowercased. 10 1817 https://nlp.stanford.edu/projects/glove The Web Term Vectors, Distributional Thesaurus §3.3 WARC web crawls §3.2 Filtered preprocessed documents §3.1 Linguistic Analysis: Preprocessing: Crawling Web Pages: lefex (Apache Hadoop) C4Corpus (Apache Hadoop) CCBot (Apache Nutch) POS Tagging (OpenNLP) §5.2 Comp. of Distributional Model: JoBimText (Apache Spark) Lemmatization (Stanford) DepCC: Dependency Parsed Corpus Named Entity Recog"
L18-1286,L16-1146,0,0.0555073,"Missing"
L18-1286,J15-4004,0,0.010721,"tes models based on bag-of-word features, while “DEPS” denotes syntax-based models. SimVerb3000 and SimVerb500 are train and test partitions of the SimVerb3500, while the SimLex222 dataset is composed of verb pairs from the SimLex999 dataset. The best results in a section are boldfaced, the best results overall are underlined. 5. Evaluation: Verb Similarity Task As an example of potential use-case, we demonstrate the utility of the corpus and the overall methodology on a verb similarity task. This task structurally is the same as the word similarity tasks based on such datasets as SimLex-999 (Hill et al., 2015). Namely, a system is given two words as input and needs to predict a scalar value which characterizes semantic similarity of the input words. While in the word similarity task the input pairs are words of various parts of speeches (nouns, adjectives, etc.), in this paper we only consider verb pairs. We chose this task since verb meaning is largely defined by the meaning of its arguments (Fillmore, 1982), therefore dependency-based features seem relevant for building distributional representations of verbs. 5.1. Datasets: SimVerb3500 and SimLex222 Recently a new challenging dataset for verb re"
L18-1286,W13-3520,0,0.087056,"Missing"
L18-1286,D16-1235,0,0.0315627,"Missing"
P16-4028,P03-1020,0,0.150902,"Missing"
P17-1145,W06-3812,1,0.924091,"zy Graph Clustering Disambiguated Ambiguous Sense Inventory Weighted Graph Weighted Graph Local Clustering: Global Clustering: Disambiguation of Graph Construction Word Sense Induction Synset Induction Neighbors Background Corpus Word Similarities Synonymy Dictionary Synsets Figure 1: Outline of the WATSET method for synset induction. MCL simulates random walks within a graph by alternation of two operators called expansion and inflation, which recompute the class labels. Notably, it has been successfully used for the word sense induction task (Dorow and Widdows, 2003). Chinese Whispers (CW) (Biemann, 2006) is a hard clustering algorithm for weighted graphs that can be considered as a special case of MCL with a simplified class update step. At each iteration, the labels of all the nodes are updated according to the majority labels among the neighboring nodes. The algorithm has a meta-parameter that controls graph weights that can be set to three values: (1) top sums over the neighborhood’s classes; (2) nolog downgrades the influence of a neighboring node by its degree or by (3) log of its degree. Clique Percolation Method (CPM) (Palla et al., 2005) is a fuzzy clustering algorithm for unweighted"
P17-1145,E03-1020,0,0.556916,"of WordNet and Wikipedia. UBY (Gurevych et al., 2012) is a general-purpose specification for the representation of lexical-semantic resources and links between them. The main advantage of our approach compared to the lexical resources is that no manual synset encoding is required. Methods based on word sense induction try to induce sense representations without the need for any initial lexical resource by extracting semantic relations from text. In particular, word sense induction (WSI) based on word ego networks clusters graphs of semantically related words (Lin, 1998; Pantel and Lin, 2002; Dorow and Widdows, 2003; V´eronis, 2004; Hope and Keller, 2013; Pelevina et al., 2016; Panchenko et al., 2017a), where each cluster corresponds to a word sense. An ego network consists of a single node (ego) together with the nodes they are connected to (alters) and all the edges among those alters (Everett and Borgatti, 2005). In the case of WSI, such a network is a local neighborhood of one word. Nodes of the ego network are the words which are semantically similar to the target word. Such approaches are able to discover homonymous senses of words, e.g., “bank” as slope versus “bank” as organisation (Di Marco and"
P17-1145,E12-1059,0,0.116633,"t induction. The induced resource eliminates the need in manual synset construction and can be used to build WordNet-like semantic networks for under-resourced languages. An implementation of our method along with induced lexical resources is available online.6 2 Related Work Methods based on resource linking surveyed by Gurevych et al. (2016) gather various existing lexical resources and perform their linking to obtain a machine-readable repository of lexical semantic knowledge. For instance, BabelNet (Navigli and Ponzetto, 2012) relies in its core on a linking of WordNet and Wikipedia. UBY (Gurevych et al., 2012) is a general-purpose specification for the representation of lexical-semantic resources and links between them. The main advantage of our approach compared to the lexical resources is that no manual synset encoding is required. Methods based on word sense induction try to induce sense representations without the need for any initial lexical resource by extracting semantic relations from text. In particular, word sense induction (WSI) based on word ego networks clusters graphs of semantically related words (Lin, 1998; Pantel and Lin, 2002; Dorow and Widdows, 2003; V´eronis, 2004; Hope and Kell"
P17-1145,heylen-etal-2008-modelling,0,0.114052,"are the words which are semantically similar to the target word. Such approaches are able to discover homonymous senses of words, e.g., “bank” as slope versus “bank” as organisation (Di Marco and Navigli, 2012). However, as the graphs are usually composed of semantically related words obtained using distributional methods (Baroni and Lenci, 2010; Biemann and Riedl, 2013), the resulting clusters by no means can be considered synsets. Namely, (1) they contain words related not only via synonymy relation, but via a mixture of relations such as synonymy, hypernymy, 6 co-hyponymy, antonymy, etc. (Heylen et al., 2008; Panchenko, 2011); (2) clusters are not unique, i.e., one word can occur in clusters of different ego networks referring to the same sense, while in WordNet a word sense occurs only in a single synset. In our synset induction method, we use word ego network clustering similarly as in word sense induction approaches, but apply them to a graph of semantically clean synonyms. Methods based on clustering of synonyms, such as our approach, induce the resource from an ambiguous graph of synonyms where edges a extracted from manually-created resources. According to the best of our knowledge, most ex"
P17-1145,S13-2049,0,0.115655,"the gold standard datasets. 4.2 Evaluation Metrics To evaluate the quality of the induced synsets, we transformed them into binary synonymy relations and computed precision, recall, and F-score on the basis of the overlap of these binary relations with the binary relations from the gold standard datasets. Given a synset containing n words, we generate a set of n(n−1) pairs of syn2 onyms. The F-score calculated this way is known as Paired F-score (Manandhar et al., 2010; Hope and Keller, 2013). The advantage of this measure compared to other cluster evaluation measures, such as Fuzzy B-Cubed (Jurgens and Klapaftis, 2013), is its straightforward interpretability. 4.3 Language English Russian Input Dictionary of Synonyms For each language, we constructed a synonymy graph using openly available language resources. The statistics of the graphs used as the input in the further experiments are shown in Table 2. https://code.google.com/p/word2vec 13 http://www.dialog-21.ru/en/ evaluation/2015/semantic_similarity 14 http://russe.nlpub.ru/downloads # words 243 840 83 092 # synonyms 212 163 211 986 Table 2: Statistics of the input datasets. Russian. We use the 500-dimensional word embeddings trained using the skip-gram"
P17-1145,S10-1011,0,0.110618,"words 148 730 11 710 137 110 242 9 141 # synsets 117 659 6 667 855 49 492 2 210 # synonyms 152 254 28 822 400 278 381 48 291 Table 1: Statistics of the gold standard datasets. 4.2 Evaluation Metrics To evaluate the quality of the induced synsets, we transformed them into binary synonymy relations and computed precision, recall, and F-score on the basis of the overlap of these binary relations with the binary relations from the gold standard datasets. Given a synset containing n words, we generate a set of n(n−1) pairs of syn2 onyms. The F-score calculated this way is known as Paired F-score (Manandhar et al., 2010; Hope and Keller, 2013). The advantage of this measure compared to other cluster evaluation measures, such as Fuzzy B-Cubed (Jurgens and Klapaftis, 2013), is its straightforward interpretability. 4.3 Language English Russian Input Dictionary of Synonyms For each language, we constructed a synonymy graph using openly available language resources. The statistics of the graphs used as the input in the further experiments are shown in Table 2. https://code.google.com/p/word2vec 13 http://www.dialog-21.ru/en/ evaluation/2015/semantic_similarity 14 http://russe.nlpub.ru/downloads # words 243 840 83"
P17-1145,W11-2502,1,0.770689,"are semantically similar to the target word. Such approaches are able to discover homonymous senses of words, e.g., “bank” as slope versus “bank” as organisation (Di Marco and Navigli, 2012). However, as the graphs are usually composed of semantically related words obtained using distributional methods (Baroni and Lenci, 2010; Biemann and Riedl, 2013), the resulting clusters by no means can be considered synsets. Namely, (1) they contain words related not only via synonymy relation, but via a mixture of relations such as synonymy, hypernymy, 6 co-hyponymy, antonymy, etc. (Heylen et al., 2008; Panchenko, 2011); (2) clusters are not unique, i.e., one word can occur in clusters of different ego networks referring to the same sense, while in WordNet a word sense occurs only in a single synset. In our synset induction method, we use word ego network clustering similarly as in word sense induction approaches, but apply them to a graph of semantically clean synonyms. Methods based on clustering of synonyms, such as our approach, induce the resource from an ambiguous graph of synonyms where edges a extracted from manually-created resources. According to the best of our knowledge, most experiments either e"
P17-1145,E17-1009,1,0.881247,"on for the representation of lexical-semantic resources and links between them. The main advantage of our approach compared to the lexical resources is that no manual synset encoding is required. Methods based on word sense induction try to induce sense representations without the need for any initial lexical resource by extracting semantic relations from text. In particular, word sense induction (WSI) based on word ego networks clusters graphs of semantically related words (Lin, 1998; Pantel and Lin, 2002; Dorow and Widdows, 2003; V´eronis, 2004; Hope and Keller, 2013; Pelevina et al., 2016; Panchenko et al., 2017a), where each cluster corresponds to a word sense. An ego network consists of a single node (ego) together with the nodes they are connected to (alters) and all the edges among those alters (Everett and Borgatti, 2005). In the case of WSI, such a network is a local neighborhood of one word. Nodes of the ego network are the words which are semantically similar to the target word. Such approaches are able to discover homonymous senses of words, e.g., “bank” as slope versus “bank” as organisation (Di Marco and Navigli, 2012). However, as the graphs are usually composed of semantically related wo"
P17-1145,W16-1620,1,0.802218,"ral-purpose specification for the representation of lexical-semantic resources and links between them. The main advantage of our approach compared to the lexical resources is that no manual synset encoding is required. Methods based on word sense induction try to induce sense representations without the need for any initial lexical resource by extracting semantic relations from text. In particular, word sense induction (WSI) based on word ego networks clusters graphs of semantically related words (Lin, 1998; Pantel and Lin, 2002; Dorow and Widdows, 2003; V´eronis, 2004; Hope and Keller, 2013; Pelevina et al., 2016; Panchenko et al., 2017a), where each cluster corresponds to a word sense. An ego network consists of a single node (ego) together with the nodes they are connected to (alters) and all the edges among those alters (Everett and Borgatti, 2005). In the case of WSI, such a network is a local neighborhood of one word. Nodes of the ego network are the words which are semantically similar to the target word. Such approaches are able to discover homonymous senses of words, e.g., “bank” as slope versus “bank” as organisation (Di Marco and Navigli, 2012). However, as the graphs are usually composed of"
P17-1145,zesch-etal-2008-extracting,0,0.0541351,"Missing"
P17-1145,J13-3008,0,\N,Missing
P18-2010,P98-1013,0,0.679639,"ully unsupervised frame-based semantic annotation exhibits far more challenges, starting with the preliminary step of automatically inducing a set of semantic frame definitions that would drive a subsequent text annotation. In this work, we aim at overcoming these issues by automatizing the process of FrameNet construction through unsupervised frame induction techniques. Introduction Recent years have seen much work on Frame Semantics (Fillmore, 1982), enabled by the availability of a large set of frame definitions, as well as a manually annotated text corpus provided by the FrameNet project (Baker et al., 1998). FrameNet data enabled the development of wide-coverage frame parsers using supervised learning (Gildea and Jurafsky, 2002; Erk and Pad´o, 2006; Das et al., 2014, inter alia), as well as its application to a wide range of tasks, ranging from answer extraction in Question Answering (Shen and Lapata, 2007) and Textual Entailment (Burchardt et al., 2009; Ben Aharon et al., 2010). However, frame-semantic resources are arguably expensive and time-consuming to build due to difficulties in defining the frames, their granularity and domain, as well as the complexity of the construction and annotation"
P18-2010,bauer-etal-2012-dependency,0,0.336218,"Missing"
P18-2010,P10-2045,0,0.591263,"Missing"
P18-2010,W06-3812,1,0.865724,"56 Algorithm 1 Triframes frame induction Input: an embedding model v ∈ V → ~v ∈ Rd , a set of SVO triples T ⊆ V 3 , the number of nearest neighbors k ∈ N, a graph clustering algorithm C LUSTER. Output: a set of triframes F . 1: S ← {t → ~ t ∈ R3d : t ∈ T } 0 0 ~ 2: E ← {(t, t ) ∈ T 2 : t0 ∈ NNS k (t), t 6= t } 3: F ← ∅ 4: for all C ∈ C LUSTER(T, E) do 5: fs ← {s ∈ V : (s, v, o) ∈ C} 6: fv ← {v ∈ V : (s, v, o) ∈ C} 7: fo ← {o ∈ V : (s, v, o) ∈ C} 8: F ← F ∪ {(fs , fv , fo )} 9: return F ate sense-aware representation that is clustered using the Chinese Whispers (CW) hard clustering algorithm (Biemann, 2006). We chose WATSET due to its performance on the related synset induction task, its fuzzy nature, and the ability to find the number of frames automatically. 3 Evaluation Input Corpus. In our evaluation, we use triple frequencies from the DepCC dataset (Panchenko et al., 2018) , which is a dependency-parsed version of the Common Crawl corpus, and the standard 300-dimensional word embeddings model trained on the Google News corpus (Mikolov et al., 2013). All evaluated algorithms are executed on the same set of triples, eliminating variations due to different corpora or pre-processing. G = (T, E)"
P18-2010,J02-3001,0,0.262426,"automatically inducing a set of semantic frame definitions that would drive a subsequent text annotation. In this work, we aim at overcoming these issues by automatizing the process of FrameNet construction through unsupervised frame induction techniques. Introduction Recent years have seen much work on Frame Semantics (Fillmore, 1982), enabled by the availability of a large set of frame definitions, as well as a manually annotated text corpus provided by the FrameNet project (Baker et al., 1998). FrameNet data enabled the development of wide-coverage frame parsers using supervised learning (Gildea and Jurafsky, 2002; Erk and Pad´o, 2006; Das et al., 2014, inter alia), as well as its application to a wide range of tasks, ranging from answer extraction in Question Answering (Shen and Lapata, 2007) and Textual Entailment (Burchardt et al., 2009; Ben Aharon et al., 2010). However, frame-semantic resources are arguably expensive and time-consuming to build due to difficulties in defining the frames, their granularity and domain, as well as the complexity of the construction and annotation tasks requiring expertise in the underlying knowledge. Consequently, such resources exist only for a few languages (Boas,"
P18-2010,E12-1059,0,0.0392429,"chmarks demonstrate that the proposed graph-based approach, Triframes, shows state-of-the art results on this task on a FrameNet-derived dataset and performing on par with competitive methods on a verb class clustering task. 1 Role Subject Verb Object Lexical Units (LU) kidnapper, alien, militant snatch, kidnap, abduct son, people, soldier, child Table 1: Example of a LU tricluster corresponding to the “Kidnapping” frame from FrameNet. et al., 2016) or linking FrameNet to other lexicalsemantic or ontological resources (Narayanan et al., 2003; Tonelli and Pighin, 2009; Laparra and Rigau, 2010; Gurevych et al., 2012, inter alia). But while the arguably simpler task of PropBankbased Semantic Role Labeling has been successfully addressed by unsupervised approaches (Lang and Lapata, 2010; Titov and Klementiev, 2011), fully unsupervised frame-based semantic annotation exhibits far more challenges, starting with the preliminary step of automatically inducing a set of semantic frame definitions that would drive a subsequent text annotation. In this work, we aim at overcoming these issues by automatizing the process of FrameNet construction through unsupervised frame induction techniques. Introduction Recent ye"
P18-2010,Q16-1015,0,0.188294,"Missing"
P18-2010,S17-1025,0,0.132917,"ional Linguistics The contributions of this paper are three-fold: (1) we are the first to apply triclustering algorithms for unsupervised frame induction, (2) we propose a new approach to triclustering, achieving state-of-the-art performance on the frame induction task, (3) we propose a new method for the evaluation of frame induction enabling straightforward comparison of approaches. In this paper, we focus on the simplest setup with subject-verbobject (SVO) triples and two roles, but our evaluation framework can be extended to more roles. In contrast to the recent approaches like the one by Jauhar and Hovy (2017), our approach induces semantic frames without any supervision, yet capturing only two core roles: the subject and the object of a frame triggered by verbal predicates. Note that it is not generally correct to expect that the SVO triples obtained by a dependency parser are necessarily the core arguments of a predicate. Such roles can be implicit, i.e., unexpressed in a given context (Schenk and Chiarcos, 2016). Keeping this limitation in mind, we assume that the triples obtained from a Web-scale corpus cover most core arguments sufficiently. cluster dataset of Korhonen et al. (2003). A major i"
P18-2010,P14-1097,0,0.532208,"r a few languages (Boas, 2009) and even English is lacking domain-specific frame-based resources. Possible inroads are cross-lingual semantic annotation transfer (Pad´o and Lapata, 2009; Hartmann Triclustering. In this work, we cast the frame induction problem as a triclustering task (Zhao and Zaki, 2005; Ignatov et al., 2015), namely a generalization of standard clustering and biclustering (Cheng and Church, 2000), aiming at simultaneously clustering objects along three dimensions (cf. Table 1). First, using triclustering allows to avoid sequential nature of frame induction approaches, e.g. (Kawahara et al., 2014), where two independent clusterings are needed. Second, benchmarking frame induction as triclustering against other methods on dependency triples allows to abstract away the evaluation of the frame induction algorithm from other factors, e.g., the input corpus or pre-processing steps, thus allowing a fair comparison of different induction models. 55 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 55–62 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics The contributions of this paper are three"
P18-2010,P03-1009,0,0.454771,"the one by Jauhar and Hovy (2017), our approach induces semantic frames without any supervision, yet capturing only two core roles: the subject and the object of a frame triggered by verbal predicates. Note that it is not generally correct to expect that the SVO triples obtained by a dependency parser are necessarily the core arguments of a predicate. Such roles can be implicit, i.e., unexpressed in a given context (Schenk and Chiarcos, 2016). Keeping this limitation in mind, we assume that the triples obtained from a Web-scale corpus cover most core arguments sufficiently. cluster dataset of Korhonen et al. (2003). A major issue with unsupervised frame induction task is that these and some other related approaches, e.g., (O’Connor, 2013), were all evaluated in completely different incomparable settings, and used different input corpora. In this paper, we propose a methodology to resolve this issue. 2 The Triframes Algorithm Our approach to frame induction relies on graph clustering. We focused on a simple setup using two roles and the SVO triples, arguing that it still can be useful, as frame roles are primarily expressed by subjects and objects, giving rise to semantic structures extracted in an unsup"
P18-2010,L18-1286,1,0.87147,"E ← {(t, t ) ∈ T 2 : t0 ∈ NNS k (t), t 6= t } 3: F ← ∅ 4: for all C ∈ C LUSTER(T, E) do 5: fs ← {s ∈ V : (s, v, o) ∈ C} 6: fv ← {v ∈ V : (s, v, o) ∈ C} 7: fo ← {o ∈ V : (s, v, o) ∈ C} 8: F ← F ∪ {(fs , fv , fo )} 9: return F ate sense-aware representation that is clustered using the Chinese Whispers (CW) hard clustering algorithm (Biemann, 2006). We chose WATSET due to its performance on the related synset induction task, its fuzzy nature, and the ability to find the number of frames automatically. 3 Evaluation Input Corpus. In our evaluation, we use triple frequencies from the DepCC dataset (Panchenko et al., 2018) , which is a dependency-parsed version of the Common Crawl corpus, and the standard 300-dimensional word embeddings model trained on the Google News corpus (Mikolov et al., 2013). All evaluated algorithms are executed on the same set of triples, eliminating variations due to different corpora or pre-processing. G = (T, E) by constructing the edge set E ⊆ T 2 . For that, we compute k ∈ N nearest neighbors of each triple vector ~t ∈ R3d and establish cosine similarity-weighted edges between the corresponding triples. Then, we assume that the triples representing similar contexts appear in simil"
P18-2010,N16-1173,0,0.0731203,"us on the simplest setup with subject-verbobject (SVO) triples and two roles, but our evaluation framework can be extended to more roles. In contrast to the recent approaches like the one by Jauhar and Hovy (2017), our approach induces semantic frames without any supervision, yet capturing only two core roles: the subject and the object of a frame triggered by verbal predicates. Note that it is not generally correct to expect that the SVO triples obtained by a dependency parser are necessarily the core arguments of a predicate. Such roles can be implicit, i.e., unexpressed in a given context (Schenk and Chiarcos, 2016). Keeping this limitation in mind, we assume that the triples obtained from a Web-scale corpus cover most core arguments sufficiently. cluster dataset of Korhonen et al. (2003). A major issue with unsupervised frame induction task is that these and some other related approaches, e.g., (O’Connor, 2013), were all evaluated in completely different incomparable settings, and used different input corpora. In this paper, we propose a methodology to resolve this issue. 2 The Triframes Algorithm Our approach to frame induction relies on graph clustering. We focused on a simple setup using two roles an"
P18-2010,laparra-rigau-2010-extended,0,0.0592218,"data. Our replicable benchmarks demonstrate that the proposed graph-based approach, Triframes, shows state-of-the art results on this task on a FrameNet-derived dataset and performing on par with competitive methods on a verb class clustering task. 1 Role Subject Verb Object Lexical Units (LU) kidnapper, alien, militant snatch, kidnap, abduct son, people, soldier, child Table 1: Example of a LU tricluster corresponding to the “Kidnapping” frame from FrameNet. et al., 2016) or linking FrameNet to other lexicalsemantic or ontological resources (Narayanan et al., 2003; Tonelli and Pighin, 2009; Laparra and Rigau, 2010; Gurevych et al., 2012, inter alia). But while the arguably simpler task of PropBankbased Semantic Role Labeling has been successfully addressed by unsupervised approaches (Lang and Lapata, 2010; Titov and Klementiev, 2011), fully unsupervised frame-based semantic annotation exhibits far more challenges, starting with the preliminary step of automatically inducing a set of semantic frame definitions that would drive a subsequent text annotation. In this work, we aim at overcoming these issues by automatizing the process of FrameNet construction through unsupervised frame induction techniques."
P18-2010,D07-1002,0,0.442772,"FrameNet construction through unsupervised frame induction techniques. Introduction Recent years have seen much work on Frame Semantics (Fillmore, 1982), enabled by the availability of a large set of frame definitions, as well as a manually annotated text corpus provided by the FrameNet project (Baker et al., 1998). FrameNet data enabled the development of wide-coverage frame parsers using supervised learning (Gildea and Jurafsky, 2002; Erk and Pad´o, 2006; Das et al., 2014, inter alia), as well as its application to a wide range of tasks, ranging from answer extraction in Question Answering (Shen and Lapata, 2007) and Textual Entailment (Burchardt et al., 2009; Ben Aharon et al., 2010). However, frame-semantic resources are arguably expensive and time-consuming to build due to difficulties in defining the frames, their granularity and domain, as well as the complexity of the construction and annotation tasks requiring expertise in the underlying knowledge. Consequently, such resources exist only for a few languages (Boas, 2009) and even English is lacking domain-specific frame-based resources. Possible inroads are cross-lingual semantic annotation transfer (Pad´o and Lapata, 2009; Hartmann Triclusterin"
P18-2010,M92-1001,0,0.788705,"Missing"
P18-2010,N13-1051,0,0.267798,"Missing"
P18-2010,P11-1145,0,0.0980018,"Missing"
P18-2010,E12-1003,0,0.34924,"Missing"
P18-2010,W09-1127,0,0.141963,"of clustering for triadic data. Our replicable benchmarks demonstrate that the proposed graph-based approach, Triframes, shows state-of-the art results on this task on a FrameNet-derived dataset and performing on par with competitive methods on a verb class clustering task. 1 Role Subject Verb Object Lexical Units (LU) kidnapper, alien, militant snatch, kidnap, abduct son, people, soldier, child Table 1: Example of a LU tricluster corresponding to the “Kidnapping” frame from FrameNet. et al., 2016) or linking FrameNet to other lexicalsemantic or ontological resources (Narayanan et al., 2003; Tonelli and Pighin, 2009; Laparra and Rigau, 2010; Gurevych et al., 2012, inter alia). But while the arguably simpler task of PropBankbased Semantic Role Labeling has been successfully addressed by unsupervised approaches (Lang and Lapata, 2010; Titov and Klementiev, 2011), fully unsupervised frame-based semantic annotation exhibits far more challenges, starting with the preliminary step of automatically inducing a set of semantic frame definitions that would drive a subsequent text annotation. In this work, we aim at overcoming these issues by automatizing the process of FrameNet construction through unsupervised fr"
P18-2010,P17-1145,1,0.73043,"frames F as presented in Algorithm 1. The hyper-parameters of the algorithm are the number of nearest neighbors for establishing edges (k) and the graph clustering algorithm C LUSTER. During the concatenation of the vectors for words forming triples, the (|T |× 3d)-dimensional vector space S is created. Thus, given the triple t ∈ T , we denote the k nearest neighbors extraction procedure of its concatenated embedding from S as NNSk (~t) ⊆ T . We used k = 10 nearest neighbors per triple. To cluster the nearest neighbor graph of SVO triples G, we use the WATSET fuzzy graph clustering algorithm (Ustalov et al., 2017). It treats the vertices T of the input graph G as the SVO triples, induces their senses, and constructs an intermediDatasets. We cast the complex multi-stage frame induction task as a straightforward triple clustering task. We constructed a gold standard set of triclusters, each corresponding to a FrameNet frame, similarly to the one illustrated in Table 1. To construct the evaluation dataset, we extracted frame annotations from the over 150 thousand sentences from the FrameNet 1.7 (Baker et al., 1998). Each sentence contains data about the frame, FEE, and its arguments, which were used to ge"
P18-2010,erk-pado-2006-shalmaneser,0,\N,Missing
P18-2010,J14-1002,0,\N,Missing
P18-2010,E17-2028,0,\N,Missing
P19-1316,P19-1474,1,0.87498,"Missing"
P19-1316,W03-1812,0,0.478278,"meaning of the phrase can be derived from the meanings of its constituent words. To motivate its importance, e.g., in machine translation, noncompositional phrases must be translated as a unit; in word sense disambiguation, assigning one of the constituent word’s senses to the whole phrase should be avoided for idiomatic phrases; semantic parsing also requires to correctly identify complex predicates and their arguments in this way. A significant amount of effort has gone into operationalizing dense-vector distributional semantic models (DSMs) of different flavors such as count-based models (Baldwin et al. (2003); Venkatapathy and Joshi (2005); McCarthy et al. (2007)), word embeddings based on word2vec (both CBOW and SkipGram) and similar (Reddy et al. (2011); Salehi et al. (2014); Cordeiro et al. (2016, 2019)), and multi-sense skip-gram models for compositionality prediction (Salehi et al., 2015). All these attempts are based on the hypothesis that the composition of the representation of constituent words will be closer to the representation of the entire phrase in case of compositional phrases as compared to the non-compositional ones (Choueka, 1988). Observing that the distributional information 3"
P19-1316,W11-1304,1,0.776988,"tion more effectively than the commonly used additive and multiplicative functions. Kiela and Clark (2013) detect non-compositionality using concepts of mutual information. Lioma et al. (2015) replace the context vectors with language models and compute their Kullback–Leibler divergence to approximate their semantic distance. In another stream, researchers have also attempted to classify idiomatic vs. non-idiomatic expressions in different languages considering the context of the expressions (Flor and Klebanov, 2018; Bizzoni et al., 2018; Peng et al., 2018), see also a respective shared task (Biemann and Giesbrecht, 2011). In one of the recent attempts, Cordeiro et al. (2016) conduct an analysis of several DSMs (word2vec, GloVe, PPMI) with variations of hyper-parameters and produce the state-of-the-art results in the compositionality prediction task, which is extended further for different languages by Cordeiro et al. (2019). We take their work as our baseline and carry forward our investigation to improve the state-of-the-art performance by introducing the 3264 hyponymy-hypernymy information in the form of Poincar´e embeddings. Le et al. (2019) and Aly et al. (2019) also showed usefulness the use of Poincar´e"
P19-1316,W07-1106,0,0.0524878,"mprovements on benchmark datasets in unsupervised and supervised settings. 3. We publicly release our Poincar´e embeddings trained on pattern extractions on a very large corpus. 2 Related Work Some of the initial efforts on compositionality prediction were undertaken by Baldwin et al. (2003), who use LSA to calculate the similarity between a phrase and its components, whereas Venkatapathy and Joshi (2005) extend this idea with collocation features (e.g., phrase frequency, point-wise mutual information). Researchers also tried to identify non-compositionality in verb-noun phrases using syntax (Cook et al., 2007) and selectional preferences (McCarthy et al., 2007). Attempts to examine the possibility to derive the semantics of a compound or multiword expression from its parts have been researched extensively (McCarthy et al., 2003; Mitchell and Lapata, 2008; Tratz and Hovy, 2010). Reddy et al. (2011) define a compositionality score and use different vector operations to estimate the semantic distance between a phrase and its individual components. Some of the investigations are made for compositionality detection using representation learning of word embeddings (Socher et al., 2012; Salehi et al., 201"
P19-1316,P13-4006,0,0.017524,"ls as provided, with the vector dimension size of 750 (PPMI-SVD, W2V) and 500 (GloVe)2 . PPMI-SVD baseline: For each word, its neighboring nouns and verbs in a symmetric sliding window of w words in both directions, using a linear decay weighting scheme with respect to its distance d to the target (Levy et al., 2015) are extracted. The representation of a word is a vector containing the positive pointwise mutual information (PPMI) association scores between the word and its contexts. Note that, for each target word, contexts that appear less than 1000 times are discarded. The Dissect toolkit (Dinu et al., 2013) is then used in order to build a PPMI matrix and its dimensionality is reduced using singular value decomposition (SVD) to factorize the matrix. word2vec baseline: This DSM is prepared using the well-known word2vec (Mikolov et al., 2013) in both variants CBOW (W2V-CBOW) and Skip-Gram (W2V-SG), using default configurations except for the following: no hierarchical softmax; negative sampling of 25; frequent-word downsampling weight of 10−6 ; runs 15 training iterations; minimum word count threshold of 5. GloVe baseline: The count-based DSM of Pennington et al. (2014), implementing a factorizati"
P19-1316,W15-0904,0,0.0132577,"s a recently introduced resource created for evaluation (Ramisch et al., 2016) that extends the Reddy dataset with an additional 90 English nominal compounds, amounting to a total of 180 nominal compounds. Consistent with RD, the scores range from 0 (idiomatic) to 5 (compositional) and are annotated through Mechanical Turk and averaged over the annotators. The additional 90 entries are adjective-noun pairs, balanced with respect to compositionality. Farahmand (FD): This dataset contains 1042 English compounds extracted from Wikipedia with binary non-compositionality judgments by four experts (Farahmand et al., 2015). In evaluations we use the sum of all the judgments to have a single numeral compositionality score, ranging from 0 (compositional) to 4 (idiomatic). We optimize our method on subsets of the datasets for pairs and constituents with available Poincar´e embeddings in order to measure the direct impact of our method, which comprises 79, 146 and 780 datapoints for the three sets RD-R, RD++-R and FD-R, respectively. We subsequently report scores on the full datasets RD-F (90), RD++-F (180) and FD-F (1042) for the sake of fair comparison to previous works. In cases where no Poincar´e embeddings are"
P19-1316,W18-0905,0,0.0128343,"ds that complex functions such as polynomial projection and neural networks can model semantic composition more effectively than the commonly used additive and multiplicative functions. Kiela and Clark (2013) detect non-compositionality using concepts of mutual information. Lioma et al. (2015) replace the context vectors with language models and compute their Kullback–Leibler divergence to approximate their semantic distance. In another stream, researchers have also attempted to classify idiomatic vs. non-idiomatic expressions in different languages considering the context of the expressions (Flor and Klebanov, 2018; Bizzoni et al., 2018; Peng et al., 2018), see also a respective shared task (Biemann and Giesbrecht, 2011). In one of the recent attempts, Cordeiro et al. (2016) conduct an analysis of several DSMs (word2vec, GloVe, PPMI) with variations of hyper-parameters and produce the state-of-the-art results in the compositionality prediction task, which is extended further for different languages by Cordeiro et al. (2019). We take their work as our baseline and carry forward our investigation to improve the state-of-the-art performance by introducing the 3264 hyponymy-hypernymy information in the form"
P19-1316,goldhahn-etal-2012-building,0,0.0259866,"ous representations of symbolic data by simultaneously capturing hierarchy and similarity. As per this proposed Poincar´e ball model, let β d = {x ∈ R : kxk &lt; 1} (3) be the open d-dimensional unit ball, where k.k denotes the Euclidean norm. The list of hyponym-hypernym pairs was obtained by applying lexical-syntactic patterns described by Hearst (1992) on the corpus prepared by Panchenko et al. (2016). This corpus is a concatenation of the English Wikipedia (2016 dump), Gigaword (Parker et al., 2009), ukWaC (Ferraresi et al., 2008) and English news corpora from the Leipzig Corpora Collection (Goldhahn et al., 2012). The lexical-syntactic patterns proposed by Hearst (1992) and further extended and implemented in the form of FSTs by Panchenko et al. (2012)1 for extracting (noisy) hyponym-hypernym pairs are given as follows – (i) such NP as NP, NP[,] and/or NP; (ii) NP such as NP, NP[,] and/or NP; (iii) NP, NP [,] or other NP; (iv) NP, NP [,] and other NP; (v) NP, including NP, NP [,] and/or NP; (vi) NP, especially NP, NP [,] and/or NP. Pattern extraction on the corpus yields a list of 27.6 million hyponym-hypernym pairs along with the frequency of their occurrence in the corpus. We normalize the frequency"
P19-1316,C92-2082,0,0.530466,"example, ‘art school’ and ‘school’ have one common hypernym ‘educational institution’ whereas ‘hot dog’ has no common hypernym with ‘hot’ or ‘dog’, apart from very abstract concepts such as ‘physical entity’. Of course, this only holds for noun phrases, where taxonomic relations between nouns apply. To represent hypernymy information we use Poincar´e embeddings (Nickel and Kiela, 2017) for learning hierarchical representations of symbolic data by embedding them into a hyperbolic space. To this end, we extract hyponym-hypernym pairs by applying well-known lexical-syntactic patterns proposed by Hearst (1992) on a large corpus and train Poincar´e embeddings on a list of hyponymhypernym pairs. Relying on two types of representations, i.e., dense vectors in the Euclidean space and the novel hyperbolic Poincar´e embeddings, we interpolate their similarity predictions in a novel compositionality score metric that takes both distributional and hypernymy information into account. We evaluate our proposed metric on three well-accepted English datasets, i.e., Reddy (Reddy et al., 2011), Reddy++ (Ramisch et al., 2016) and Farahmand (Farahmand et al., 2015), demonstrating a performance boost when including"
P19-1316,P16-1187,0,0.0999607,"in word sense disambiguation, assigning one of the constituent word’s senses to the whole phrase should be avoided for idiomatic phrases; semantic parsing also requires to correctly identify complex predicates and their arguments in this way. A significant amount of effort has gone into operationalizing dense-vector distributional semantic models (DSMs) of different flavors such as count-based models (Baldwin et al. (2003); Venkatapathy and Joshi (2005); McCarthy et al. (2007)), word embeddings based on word2vec (both CBOW and SkipGram) and similar (Reddy et al. (2011); Salehi et al. (2014); Cordeiro et al. (2016, 2019)), and multi-sense skip-gram models for compositionality prediction (Salehi et al., 2015). All these attempts are based on the hypothesis that the composition of the representation of constituent words will be closer to the representation of the entire phrase in case of compositional phrases as compared to the non-compositional ones (Choueka, 1988). Observing that the distributional information 3263 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3263–3274 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Lingu"
P19-1316,D13-1147,0,0.0203407,"ual components. Some of the investigations are made for compositionality detection using representation learning of word embeddings (Socher et al., 2012; Salehi et al., 2015). Salehi et al. (2014) also show that distributional similarity over multiple languages can help in improving the quality of compositionality prediction. In a recent attempt, Yazdani et al. (2015) tries to learn semantic composition and finds that complex functions such as polynomial projection and neural networks can model semantic composition more effectively than the commonly used additive and multiplicative functions. Kiela and Clark (2013) detect non-compositionality using concepts of mutual information. Lioma et al. (2015) replace the context vectors with language models and compute their Kullback–Leibler divergence to approximate their semantic distance. In another stream, researchers have also attempted to classify idiomatic vs. non-idiomatic expressions in different languages considering the context of the expressions (Flor and Klebanov, 2018; Bizzoni et al., 2018; Peng et al., 2018), see also a respective shared task (Biemann and Giesbrecht, 2011). In one of the recent attempts, Cordeiro et al. (2016) conduct an analysis o"
P19-1316,J19-1001,0,0.0209654,"c distance. In another stream, researchers have also attempted to classify idiomatic vs. non-idiomatic expressions in different languages considering the context of the expressions (Flor and Klebanov, 2018; Bizzoni et al., 2018; Peng et al., 2018), see also a respective shared task (Biemann and Giesbrecht, 2011). In one of the recent attempts, Cordeiro et al. (2016) conduct an analysis of several DSMs (word2vec, GloVe, PPMI) with variations of hyper-parameters and produce the state-of-the-art results in the compositionality prediction task, which is extended further for different languages by Cordeiro et al. (2019). We take their work as our baseline and carry forward our investigation to improve the state-of-the-art performance by introducing the 3264 hyponymy-hypernymy information in the form of Poincar´e embeddings. Le et al. (2019) and Aly et al. (2019) also showed usefulness the use of Poincar´e embeddings: in their case for inducing taxonomies from the text. In both works, hyperbolic embeddings are trained using relations harvested using Hearst patterns, like in our work. The usefulness of hyperbolic embeddings was also shown beyond text processing: Khrulkov et al. (2019) successfully applied them"
P19-1316,P19-1313,0,0.0240964,"ng et al., 2018), see also a respective shared task (Biemann and Giesbrecht, 2011). In one of the recent attempts, Cordeiro et al. (2016) conduct an analysis of several DSMs (word2vec, GloVe, PPMI) with variations of hyper-parameters and produce the state-of-the-art results in the compositionality prediction task, which is extended further for different languages by Cordeiro et al. (2019). We take their work as our baseline and carry forward our investigation to improve the state-of-the-art performance by introducing the 3264 hyponymy-hypernymy information in the form of Poincar´e embeddings. Le et al. (2019) and Aly et al. (2019) also showed usefulness the use of Poincar´e embeddings: in their case for inducing taxonomies from the text. In both works, hyperbolic embeddings are trained using relations harvested using Hearst patterns, like in our work. The usefulness of hyperbolic embeddings was also shown beyond text processing: Khrulkov et al. (2019) successfully applied them for hierarchical relations in image classification tasks. 3 Methodology Our aim is to produce a compositionality score for a given two-word noun phrase w1 w2 . As per our hypothesis, the proposed compositionality score metri"
P19-1316,Q15-1016,0,0.0481565,"seline, where authors apply several distributional semantic models and their variants by tuning hyperparameters like the dimension of vectors, the window-size during training and others. We resort to PPMI-SVD, two variants of word2vec (CBOW and SkipGram) and GloVe as our baselines. We use these models as provided, with the vector dimension size of 750 (PPMI-SVD, W2V) and 500 (GloVe)2 . PPMI-SVD baseline: For each word, its neighboring nouns and verbs in a symmetric sliding window of w words in both directions, using a linear decay weighting scheme with respect to its distance d to the target (Levy et al., 2015) are extracted. The representation of a word is a vector containing the positive pointwise mutual information (PPMI) association scores between the word and its contexts. Note that, for each target word, contexts that appear less than 1000 times are discarded. The Dissect toolkit (Dinu et al., 2013) is then used in order to build a PPMI matrix and its dimensionality is reduced using singular value decomposition (SVD) to factorize the matrix. word2vec baseline: This DSM is prepared using the well-known word2vec (Mikolov et al., 2013) in both variants CBOW (W2V-CBOW) and Skip-Gram (W2V-SG), usin"
P19-1316,W03-1810,0,0.149849,"s on compositionality prediction were undertaken by Baldwin et al. (2003), who use LSA to calculate the similarity between a phrase and its components, whereas Venkatapathy and Joshi (2005) extend this idea with collocation features (e.g., phrase frequency, point-wise mutual information). Researchers also tried to identify non-compositionality in verb-noun phrases using syntax (Cook et al., 2007) and selectional preferences (McCarthy et al., 2007). Attempts to examine the possibility to derive the semantics of a compound or multiword expression from its parts have been researched extensively (McCarthy et al., 2003; Mitchell and Lapata, 2008; Tratz and Hovy, 2010). Reddy et al. (2011) define a compositionality score and use different vector operations to estimate the semantic distance between a phrase and its individual components. Some of the investigations are made for compositionality detection using representation learning of word embeddings (Socher et al., 2012; Salehi et al., 2015). Salehi et al. (2014) also show that distributional similarity over multiple languages can help in improving the quality of compositionality prediction. In a recent attempt, Yazdani et al. (2015) tries to learn semantic"
P19-1316,D07-1039,0,0.108452,"Missing"
P19-1316,P08-1028,0,0.0877299,"rediction were undertaken by Baldwin et al. (2003), who use LSA to calculate the similarity between a phrase and its components, whereas Venkatapathy and Joshi (2005) extend this idea with collocation features (e.g., phrase frequency, point-wise mutual information). Researchers also tried to identify non-compositionality in verb-noun phrases using syntax (Cook et al., 2007) and selectional preferences (McCarthy et al., 2007). Attempts to examine the possibility to derive the semantics of a compound or multiword expression from its parts have been researched extensively (McCarthy et al., 2003; Mitchell and Lapata, 2008; Tratz and Hovy, 2010). Reddy et al. (2011) define a compositionality score and use different vector operations to estimate the semantic distance between a phrase and its individual components. Some of the investigations are made for compositionality detection using representation learning of word embeddings (Socher et al., 2012; Salehi et al., 2015). Salehi et al. (2014) also show that distributional similarity over multiple languages can help in improving the quality of compositionality prediction. In a recent attempt, Yazdani et al. (2015) tries to learn semantic composition and finds that"
P19-1316,S16-1206,1,0.884605,"Missing"
P19-1316,D14-1162,0,0.0900465,"e discarded. The Dissect toolkit (Dinu et al., 2013) is then used in order to build a PPMI matrix and its dimensionality is reduced using singular value decomposition (SVD) to factorize the matrix. word2vec baseline: This DSM is prepared using the well-known word2vec (Mikolov et al., 2013) in both variants CBOW (W2V-CBOW) and Skip-Gram (W2V-SG), using default configurations except for the following: no hierarchical softmax; negative sampling of 25; frequent-word downsampling weight of 10−6 ; runs 15 training iterations; minimum word count threshold of 5. GloVe baseline: The count-based DSM of Pennington et al. (2014), implementing a factorization of the co-occurrence count matrix is used for the task. The configurations are the default ones, except for the following: internal cutoff parameter xmax = 75; builds co-occurrence matrix in 15 iterations; minimum word count threshold of 5. Other baseline models proposed by Reddy et al. (2011), Salehi et al. (2014), Salehi et al. (2015) report results only on Reddy dataset (since the other two datasets have been introduced later) whereas Yazdani et al. (2015) perform their evaluation only on the Farahmand dataset for their supervised model. In addition, this supe"
P19-1316,P16-2026,0,0.0142342,"Datasets To evaluate our proposed models (both supervised and unsupervised) we use three gold standard datasets for English on compositionality detection and describe them in the following. 3266 Reddy (RD): This dataset contains compositionality judgments for 90 compounds in a scale of literality from 0 (idiomatic) to 5 (compositional), obtained by averaging crowdsourced judgments on these pairs (Reddy et al., 2011). For evaluation, we use only the global compositionality score, ignoring individual word judgments. Reddy++ (RD++): This is a recently introduced resource created for evaluation (Ramisch et al., 2016) that extends the Reddy dataset with an additional 90 English nominal compounds, amounting to a total of 180 nominal compounds. Consistent with RD, the scores range from 0 (idiomatic) to 5 (compositional) and are annotated through Mechanical Turk and averaged over the annotators. The additional 90 entries are adjective-noun pairs, balanced with respect to compositionality. Farahmand (FD): This dataset contains 1042 English compounds extracted from Wikipedia with binary non-compositionality judgments by four experts (Farahmand et al., 2015). In evaluations we use the sum of all the judgments to"
P19-1316,I11-1024,0,0.337019,"ional phrases must be translated as a unit; in word sense disambiguation, assigning one of the constituent word’s senses to the whole phrase should be avoided for idiomatic phrases; semantic parsing also requires to correctly identify complex predicates and their arguments in this way. A significant amount of effort has gone into operationalizing dense-vector distributional semantic models (DSMs) of different flavors such as count-based models (Baldwin et al. (2003); Venkatapathy and Joshi (2005); McCarthy et al. (2007)), word embeddings based on word2vec (both CBOW and SkipGram) and similar (Reddy et al. (2011); Salehi et al. (2014); Cordeiro et al. (2016, 2019)), and multi-sense skip-gram models for compositionality prediction (Salehi et al., 2015). All these attempts are based on the hypothesis that the composition of the representation of constituent words will be closer to the representation of the entire phrase in case of compositional phrases as compared to the non-compositional ones (Choueka, 1988). Observing that the distributional information 3263 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3263–3274 c Florence, Italy, July 28 - August 2, 2"
P19-1316,E14-1050,0,0.261156,"translated as a unit; in word sense disambiguation, assigning one of the constituent word’s senses to the whole phrase should be avoided for idiomatic phrases; semantic parsing also requires to correctly identify complex predicates and their arguments in this way. A significant amount of effort has gone into operationalizing dense-vector distributional semantic models (DSMs) of different flavors such as count-based models (Baldwin et al. (2003); Venkatapathy and Joshi (2005); McCarthy et al. (2007)), word embeddings based on word2vec (both CBOW and SkipGram) and similar (Reddy et al. (2011); Salehi et al. (2014); Cordeiro et al. (2016, 2019)), and multi-sense skip-gram models for compositionality prediction (Salehi et al., 2015). All these attempts are based on the hypothesis that the composition of the representation of constituent words will be closer to the representation of the entire phrase in case of compositional phrases as compared to the non-compositional ones (Choueka, 1988). Observing that the distributional information 3263 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3263–3274 c Florence, Italy, July 28 - August 2, 2019. 2019 Association"
P19-1316,N15-1099,0,0.0363598,"Missing"
P19-1316,D12-1110,0,0.0529303,"hrases using syntax (Cook et al., 2007) and selectional preferences (McCarthy et al., 2007). Attempts to examine the possibility to derive the semantics of a compound or multiword expression from its parts have been researched extensively (McCarthy et al., 2003; Mitchell and Lapata, 2008; Tratz and Hovy, 2010). Reddy et al. (2011) define a compositionality score and use different vector operations to estimate the semantic distance between a phrase and its individual components. Some of the investigations are made for compositionality detection using representation learning of word embeddings (Socher et al., 2012; Salehi et al., 2015). Salehi et al. (2014) also show that distributional similarity over multiple languages can help in improving the quality of compositionality prediction. In a recent attempt, Yazdani et al. (2015) tries to learn semantic composition and finds that complex functions such as polynomial projection and neural networks can model semantic composition more effectively than the commonly used additive and multiplicative functions. Kiela and Clark (2013) detect non-compositionality using concepts of mutual information. Lioma et al. (2015) replace the context vectors with language m"
P19-1316,S10-1049,0,0.0130872,"y Baldwin et al. (2003), who use LSA to calculate the similarity between a phrase and its components, whereas Venkatapathy and Joshi (2005) extend this idea with collocation features (e.g., phrase frequency, point-wise mutual information). Researchers also tried to identify non-compositionality in verb-noun phrases using syntax (Cook et al., 2007) and selectional preferences (McCarthy et al., 2007). Attempts to examine the possibility to derive the semantics of a compound or multiword expression from its parts have been researched extensively (McCarthy et al., 2003; Mitchell and Lapata, 2008; Tratz and Hovy, 2010). Reddy et al. (2011) define a compositionality score and use different vector operations to estimate the semantic distance between a phrase and its individual components. Some of the investigations are made for compositionality detection using representation learning of word embeddings (Socher et al., 2012; Salehi et al., 2015). Salehi et al. (2014) also show that distributional similarity over multiple languages can help in improving the quality of compositionality prediction. In a recent attempt, Yazdani et al. (2015) tries to learn semantic composition and finds that complex functions such"
P19-1316,H05-1113,0,0.269682,"can be derived from the meanings of its constituent words. To motivate its importance, e.g., in machine translation, noncompositional phrases must be translated as a unit; in word sense disambiguation, assigning one of the constituent word’s senses to the whole phrase should be avoided for idiomatic phrases; semantic parsing also requires to correctly identify complex predicates and their arguments in this way. A significant amount of effort has gone into operationalizing dense-vector distributional semantic models (DSMs) of different flavors such as count-based models (Baldwin et al. (2003); Venkatapathy and Joshi (2005); McCarthy et al. (2007)), word embeddings based on word2vec (both CBOW and SkipGram) and similar (Reddy et al. (2011); Salehi et al. (2014); Cordeiro et al. (2016, 2019)), and multi-sense skip-gram models for compositionality prediction (Salehi et al., 2015). All these attempts are based on the hypothesis that the composition of the representation of constituent words will be closer to the representation of the entire phrase in case of compositional phrases as compared to the non-compositional ones (Choueka, 1988). Observing that the distributional information 3263 Proceedings of the 57th Ann"
P19-1316,D15-1201,0,0.200919,"researched extensively (McCarthy et al., 2003; Mitchell and Lapata, 2008; Tratz and Hovy, 2010). Reddy et al. (2011) define a compositionality score and use different vector operations to estimate the semantic distance between a phrase and its individual components. Some of the investigations are made for compositionality detection using representation learning of word embeddings (Socher et al., 2012; Salehi et al., 2015). Salehi et al. (2014) also show that distributional similarity over multiple languages can help in improving the quality of compositionality prediction. In a recent attempt, Yazdani et al. (2015) tries to learn semantic composition and finds that complex functions such as polynomial projection and neural networks can model semantic composition more effectively than the commonly used additive and multiplicative functions. Kiela and Clark (2013) detect non-compositionality using concepts of mutual information. Lioma et al. (2015) replace the context vectors with language models and compute their Kullback–Leibler divergence to approximate their semantic distance. In another stream, researchers have also attempted to classify idiomatic vs. non-idiomatic expressions in different languages"
P19-1325,P18-1011,0,0.031699,"follow them while outperforming other graph embedding baselines: We indicate the differences with respect to the original with a subscript number. 5 5.1 Evaluation on Knowledge Base Graphs Experimental Settings To show the utility of our model besides the WordNet graph, we also applied it to two graphs derived from knowledge bases (KBs). More specifically, we base our experiments on two publicly available standard samples from these two resources: the FB15k-237 (Toutanova and Chen, 2015) dataset contains 14,951 entities/nodes and is derived from Freebase (Bollacker et al., 2008); the DB100k (Ding et al., 2018) dataset contains 99,604 entities/nodes and is derived from DBPe3352 dia (Auer et al., 2007). It is important to note that both datasets were used to evaluate approaches that learn knowledge graph embeddings, e.g. (Lin et al., 2015; Xie et al., 2016; Joulin et al., 2017) on the task on knowledge base completion (KBC), to predict missing KB edges/relations between nodes/entities. The specificity of our model is that it learns a given graph similarity metric, which is not provided in these datasets. Therefore, we use only the graphs from these datasets, computing the shortest path distances betw"
P19-1325,J15-4004,0,0.0950214,"Missing"
P19-1325,S19-1014,1,0.822582,"ning metric embeddings for three types of graphs (WordNet, FreeBase, and DBPedia), based on several similarity measures. Second, in an extrinsic evaluation on the Word Sense Disambiguation (WSD) task (Navigli, 2009) we replace several original measures with their vectorized counterparts in a known graph-based WSD algorithm by Sinha and Mihalcea (2007), reaching comparable levels of performance with the graph-based algorithms while maintaining computational gains. The main contribution of this paper is the demonstration of the effectiveness and efficiency of the path2vec node embedding method (Kutuzov et al., 2019). This method learns dense vector embeddings of nodes V based on a user-defined custom similarity measure sim, e.g. the shortest path distance or any other similarity measure. While our method is able to closely approximate quite different similarity measures as we show 1 https://github.com/uhh-lt/path2vec 3349 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3349–3355 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics on WordNet-based measures and therefore can be used in lieu of these measures in NLP compo"
P19-1325,W04-0807,0,0.0870945,"LCH (WordNet) LCH (path2vec) 0.547↓0.000 0.527↓0.020 0.494↓0.000 0.472↓0.022 0.550↓0.000 0.536↓0.014 ShP (WordNet) ShP (path2vec) 0.548↓0.000 0.534↓0.014 0.495↓0.000 0.489↓0.006 0.550↓0.000 0.563↑0.013 WuP (WordNet) WuP (path2vec) 0.547↓0.000 0.543↓0.004 0.487↓0.000 0.489↑0.002 0.542↓0.000 0.545↑0.003 Various baseline graph embeddings trained on WordNet TransR node2vec DeepWalk FSE 0.540 0.503 0.528 0.536 0.466 0.467 0.476 0.476 0.536 0.489 0.552 0.523 Table 2: F1 scores of a graph-based WSD algorithm on WordNet versus its vectorized counterparts. Senseval-2 (Palmer et al., 2001), Senseval-3 (Mihalcea et al., 2004), and SemEval-15 Task 13 (Moro and Navigli, 2015). The raw WordNet similarities have a small edge over their vector approximations in the majority of the cases yet the path2vec models consistently closely follow them while outperforming other graph embedding baselines: We indicate the differences with respect to the original with a subscript number. 5 5.1 Evaluation on Knowledge Base Graphs Experimental Settings To show the utility of our model besides the WordNet graph, we also applied it to two graphs derived from knowledge bases (KBs). More specifically, we base our experiments on two publi"
P19-1325,S15-2049,0,0.0141756,".020 0.494↓0.000 0.472↓0.022 0.550↓0.000 0.536↓0.014 ShP (WordNet) ShP (path2vec) 0.548↓0.000 0.534↓0.014 0.495↓0.000 0.489↓0.006 0.550↓0.000 0.563↑0.013 WuP (WordNet) WuP (path2vec) 0.547↓0.000 0.543↓0.004 0.487↓0.000 0.489↑0.002 0.542↓0.000 0.545↑0.003 Various baseline graph embeddings trained on WordNet TransR node2vec DeepWalk FSE 0.540 0.503 0.528 0.536 0.466 0.467 0.476 0.476 0.536 0.489 0.552 0.523 Table 2: F1 scores of a graph-based WSD algorithm on WordNet versus its vectorized counterparts. Senseval-2 (Palmer et al., 2001), Senseval-3 (Mihalcea et al., 2004), and SemEval-15 Task 13 (Moro and Navigli, 2015). The raw WordNet similarities have a small edge over their vector approximations in the majority of the cases yet the path2vec models consistently closely follow them while outperforming other graph embedding baselines: We indicate the differences with respect to the original with a subscript number. 5 5.1 Evaluation on Knowledge Base Graphs Experimental Settings To show the utility of our model besides the WordNet graph, we also applied it to two graphs derived from knowledge bases (KBs). More specifically, we base our experiments on two publicly available standard samples from these two res"
P19-1325,S01-1005,0,0.0243681,"ph-based vs vector-based measures LCH (WordNet) LCH (path2vec) 0.547↓0.000 0.527↓0.020 0.494↓0.000 0.472↓0.022 0.550↓0.000 0.536↓0.014 ShP (WordNet) ShP (path2vec) 0.548↓0.000 0.534↓0.014 0.495↓0.000 0.489↓0.006 0.550↓0.000 0.563↑0.013 WuP (WordNet) WuP (path2vec) 0.547↓0.000 0.543↓0.004 0.487↓0.000 0.489↑0.002 0.542↓0.000 0.545↑0.003 Various baseline graph embeddings trained on WordNet TransR node2vec DeepWalk FSE 0.540 0.503 0.528 0.536 0.466 0.467 0.476 0.476 0.536 0.489 0.552 0.523 Table 2: F1 scores of a graph-based WSD algorithm on WordNet versus its vectorized counterparts. Senseval-2 (Palmer et al., 2001), Senseval-3 (Mihalcea et al., 2004), and SemEval-15 Task 13 (Moro and Navigli, 2015). The raw WordNet similarities have a small edge over their vector approximations in the majority of the cases yet the path2vec models consistently closely follow them while outperforming other graph embedding baselines: We indicate the differences with respect to the original with a subscript number. 5 5.1 Evaluation on Knowledge Base Graphs Experimental Settings To show the utility of our model besides the WordNet graph, we also applied it to two graphs derived from knowledge bases (KBs). More specifically,"
P19-1325,D14-1162,0,0.110934,"vi ,vj )∈Bn log σ(−vi · v batch of positive training samples, Bn is the batch of the generated negative samples, and σ is the sigmoid function. At this, Skip-gram uses only local information, never creating the full co-occurrence count matrix. In our path2vec model, the target dot product values sij are not binary, but can take arbitrary values in the [0...1] range, as given by the custom distance metric. Further, we use only a single embedding matrix with vector representations of the graph nodes, not needing to distinguish target and context. Another related model is Global Vectors (GloVe) (Pennington et al., 2014), which learns co-occurrence probabilities in a given corpus. The objective function to be minimized in GloVe model is L = P f (s )(v · v ˜ − log sij + bi + bj )2 , ij i j (vi ,vj )∈B where sij counts the co-occurrences of words vi and vj , bi and bj are additional biases for each word, and f (sij ) is a weighting function handling rare co-occurrences. Like the Skip-gram, GloVe also uses two embedding matrices, but it relies only on global information, pre-aggregating global word co-occurrence counts. Computing Training Similarities In general case, our model requires computing pairwise node s"
P19-1325,P15-2002,0,0.174842,"he depth of the two nodes in the taxonomy and the depth of their most specific ancestor node. For instance, for LCH this procedure took about 30 hours on an Intel Xeon E5-2603v4@1.70GHz CPU using 10 threads. We pruned similarities to the first 50 most similar ‘neighbors’ of each synset and trained path2vec on this dataset. Discussion of Results Figure 1 presents computation times for pairwise similarities between one synset and all other 82,115 WordNet noun synsets. We compare running times of calculating two original graph-based metrics to Hamming distance between 128D FSE binary embeddings (Subercaze et al., 2015) and to dot product between their dense vectorized 300D counterparts (using CPU). Using float vectors (path2vec) is 4 orders of magnitude faster than operating directly on graphs, and 2 orders faster than Hamming distance. The dot product computation is much faster as compared to shortest path computation (and other complex walks) on a large graph. Also, lowdimensional vector representations of nodes take much less space than the pairwise similarities between all the nodes. The time complexity of calculating the shortest path between graph nodes (as in ShP or LCH) is in the best case linear in"
P19-1325,W15-4007,0,0.0139358,"arities have a small edge over their vector approximations in the majority of the cases yet the path2vec models consistently closely follow them while outperforming other graph embedding baselines: We indicate the differences with respect to the original with a subscript number. 5 5.1 Evaluation on Knowledge Base Graphs Experimental Settings To show the utility of our model besides the WordNet graph, we also applied it to two graphs derived from knowledge bases (KBs). More specifically, we base our experiments on two publicly available standard samples from these two resources: the FB15k-237 (Toutanova and Chen, 2015) dataset contains 14,951 entities/nodes and is derived from Freebase (Bollacker et al., 2008); the DB100k (Ding et al., 2018) dataset contains 99,604 entities/nodes and is derived from DBPe3352 dia (Auer et al., 2007). It is important to note that both datasets were used to evaluate approaches that learn knowledge graph embeddings, e.g. (Lin et al., 2015; Xie et al., 2016; Joulin et al., 2017) on the task on knowledge base completion (KBC), to predict missing KB edges/relations between nodes/entities. The specificity of our model is that it learns a given graph similarity metric, which is not"
P19-1474,baroni-bernardini-2004-bootcat,0,0.0267867,"cally applicable to (noisy) taxonomies, yielding an improved taxonomy extraction system overall. 3.1 Domain-specific Poincar´e Embedding Training Dataset Construction To create domain-specific Poincar´e embeddings, we use noisy hypernym relationships extracted from a combination of general and domain-specific corpora. For the general domain, we extracted 59.2 GB of text from English Wikipedia, Gigaword (Parker et al., 2009), ukWac (Ferraresi et al., 2008) and LCC news corpora (Goldhahn et al., 2012). The domain-specific corpora consist of web pages, selected by using a combination of BootCat (Baroni and Bernardini, 2004) and focused crawling (Remus and Biemann, 2016). Noisy IS-A relations are extracted with lexicalsyntactic patterns from all corpora by applying PattaMaika2 , PatternSim (Panchenko et al., 2012), and WebISA (Seitner et al., 2016), following (Panchenko et al., 2016).3 The extracted noisy relationships of the common and domain-specific corpora are further processed separately and combined afterward. To limit the number of terms and relationships, we restrict the IS-A relationships on pairs for which both entities are part of the taxonomy’s vocabulary. Relations with a frequency of less than three"
P19-1474,S15-2151,0,0.272488,"IS-A relation can be improved by studying how pattern-based and distributional approaches complement each other; ii) there is only limited success of pure deep learn1 https://github.com/uhh-lt/Taxonomy_ Refinement_Embeddings 4811 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4811–4817 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics ing paradigms here, mostly because it is difficult to design a single objective function for this task. On the two recent TExEval tasks at SemEval for taxonomy extraction (Bordea et al., 2015, 2016), attracting a total of 10 participating teams, attempts to primarily use a distributional representation failed. This might seem counterintuitive, as taxonomies are surely modeling semantics and thus their extraction should benefit from semantic representations. The 2015 winner INRIASAC (Grefenstette, 2015) performed relation discovery using substring inclusion, lexicalsyntactic patterns and co-occurrence information based on sentences and documents from Wikipedia. The winner in 2016, TAXI (Panchenko et al., 2016), harvests hypernyms with substring inclusion and Hearst-style lexical-sy"
P19-1474,S16-1168,0,0.175656,"onym-hypernym relations – called taxonomy – from text corpora. Compared to many other domains of natural language processing that make use of pre-trained dense representations, state-ofthe-art taxonomy learning is still highly relying on traditional approaches like extraction of lexicalsyntactic patterns (Hearst, 1992) or co-occurrence information (Grefenstette, 2015). Despite the success of pattern-based approaches, most taxonomy induction systems suffer from a significant number of disconnected terms, since the extracted relationships are too specific to cover most words (Wang et al., 2017; Bordea et al., 2016). The use of distributional semantics for hypernym identification and relation representation has thus received increasing attention (Shwartz et al., 2016). However, Levy et al. (2015) observe that many proposed supervised approaches instead learn prototypical hypernyms (that are hypernyms to many other terms), not taking into account the relation between both terms in classification. Therefore, past applications of distributional semantics appear to be rather unsuitable to be directly applied to taxonomy induction as the sole signal (Tan et al., 2015; Pocostales, 2016). We address that issue"
P19-1474,P14-1113,0,0.151137,"ll model is defined as:   ||u − v||2 d(u, v) = arcosh 1 + 2 . (1 − ||u||2 )(1 − ||v||2 ) This Poincar´e distance enables us to capture the hierarchy and similarity between words simultaneously. It increases exponentially with the depth of the hierarchy. So while the distance of a leaf node to most other nodes in the hierarchy is very high, nodes on abstract levels, such as the root, have a comparably small distance to all nodes in the hierarchy. The word2vec embeddings have no notion of hierarchy and hierarchical relationships cannot be represented with vector offsets across the vocabulary (Fu et al., 2014). When applying word2vec, we use the observation that distributionally similar words are often co-hyponyms (Heylen et al., 2008; Weeds et al., 2014). 3.2 Relocation of Outlier Terms Poincar´e embeddings are used to compute and store a rank rank(x, y) between every child and parent of the existing taxonomy, defined as the index of y in the list of sorted Poincar´e distances of all entities of the taxonomy to x. Hypernymhyponym relationships with a rank larger than the mean of all ranks are removed, chosen on the basis of tests on the 2015 TExEval data (Bordea et al., 2015). Disconnected compone"
P19-1474,P19-1313,0,0.155064,"web crawling. The only submission to the TExEval 2016 task that relied exclusively on distributional semantics to induce hypernyms by adding a vector offset to the corresponding hyponym (Pocostales, 2016) achieved only modest results. A more refined approach to applying distributional semantics by Zhang et al. (2018) generates a hierarchical clustering of terms with each node consisting of several terms. They find concepts that should stay in the same cluster using embedding similarity – whereas, similar to the TExEval task, we are interested in making distinctions between all terms. Finally, Le et al. (2019) also explore using Poincar´e embeddings for taxonomy induction, evaluating their method on hypernymy detection and reconstructing WordNet. However, in contrast to our approach that filters and attaches terms, they perform inference. 3 Taxonomy Refinement using Hyperbolic Word Embeddings We employ embeddings using distributional semantics (i.e. word2vec CBOW) and Poincar´e embeddings (Nickel and Kiela, 2017) to alleviate the largest error classes in taxonomy extraction: the existence of orphans – disconnected nodes that have an overall connectivity degree of zero and outliers – a child node th"
P19-1474,N15-1098,1,0.935588,"he-art taxonomy learning is still highly relying on traditional approaches like extraction of lexicalsyntactic patterns (Hearst, 1992) or co-occurrence information (Grefenstette, 2015). Despite the success of pattern-based approaches, most taxonomy induction systems suffer from a significant number of disconnected terms, since the extracted relationships are too specific to cover most words (Wang et al., 2017; Bordea et al., 2016). The use of distributional semantics for hypernym identification and relation representation has thus received increasing attention (Shwartz et al., 2016). However, Levy et al. (2015) observe that many proposed supervised approaches instead learn prototypical hypernyms (that are hypernyms to many other terms), not taking into account the relation between both terms in classification. Therefore, past applications of distributional semantics appear to be rather unsuitable to be directly applied to taxonomy induction as the sole signal (Tan et al., 2015; Pocostales, 2016). We address that issue by introducing a series of simple and parameter-free refinement steps that employ word embeddings in order to improve existing domain-specific taxonomies, induced from text using tradi"
P19-1474,S16-1204,0,0.0196188,"17 16.5 15.6 15.6 16.3 16.2 15.6 Environment 16.2 16.4 15.8 Science Food JUNLP Figure 2: F1 results for the systems on all domains. Vocabulary sizes: environment (|V |= 261), science (|V |= 453), food (|V |= 1556). Bold numbers are significantly different results to the original system with p &lt; 0.05. 4 Evaluation Proposed methods are evaluated on the data of SemEval2016 TExEval (Bordea et al., 2016) for submitted systems that created taxonomies for all domains of the task4 , namely the task-winning system TAXI (Panchenko et al., 2016) as well as the systems USAAR (Tan et al., 2016) and JUNLP (Maitra and Das, 2016). TAXI harvests 4 http://alt.qcri.org/semeval2016/ task13/index.php Comparison to Baselines Figure 2 shows comparative results for all datasets and measures for every system. The Root method, which connects all orphans to the root of the taxonomy, has the highest connectivity but falls behind in scores significantly. Word2vec CBOW embeddings partly increase the scores, however, the effect appears to be inconsistent. Word2vec embeddings connect more orphans to the taxonomy (cf. Table 2), albeit with mixed quality, thus the interpretation of word similarity as co-hyponymy does not seem to be app"
P19-1474,goldhahn-etal-2012-building,0,0.0408558,"refinement pipeline. In our experiments, we use the output of three different systems. The refinement method is generically applicable to (noisy) taxonomies, yielding an improved taxonomy extraction system overall. 3.1 Domain-specific Poincar´e Embedding Training Dataset Construction To create domain-specific Poincar´e embeddings, we use noisy hypernym relationships extracted from a combination of general and domain-specific corpora. For the general domain, we extracted 59.2 GB of text from English Wikipedia, Gigaword (Parker et al., 2009), ukWac (Ferraresi et al., 2008) and LCC news corpora (Goldhahn et al., 2012). The domain-specific corpora consist of web pages, selected by using a combination of BootCat (Baroni and Bernardini, 2004) and focused crawling (Remus and Biemann, 2016). Noisy IS-A relations are extracted with lexicalsyntactic patterns from all corpora by applying PattaMaika2 , PatternSim (Panchenko et al., 2012), and WebISA (Seitner et al., 2016), following (Panchenko et al., 2016).3 The extracted noisy relationships of the common and domain-specific corpora are further processed separately and combined afterward. To limit the number of terms and relationships, we restrict the IS-A relatio"
P19-1474,S15-2152,0,0.139878,", supporting the hypothesis that they can better capture hierarchical lexical-semantic relationships than embeddings in the Euclidean space. 1 Introduction The task of taxonomy induction aims at creating a semantic hierarchy of entities by using hyponym-hypernym relations – called taxonomy – from text corpora. Compared to many other domains of natural language processing that make use of pre-trained dense representations, state-ofthe-art taxonomy learning is still highly relying on traditional approaches like extraction of lexicalsyntactic patterns (Hearst, 1992) or co-occurrence information (Grefenstette, 2015). Despite the success of pattern-based approaches, most taxonomy induction systems suffer from a significant number of disconnected terms, since the extracted relationships are too specific to cover most words (Wang et al., 2017; Bordea et al., 2016). The use of distributional semantics for hypernym identification and relation representation has thus received increasing attention (Shwartz et al., 2016). However, Levy et al. (2015) observe that many proposed supervised approaches instead learn prototypical hypernyms (that are hypernyms to many other terms), not taking into account the relation"
P19-1474,C92-2082,0,0.7644,"over distributional semantic representations, supporting the hypothesis that they can better capture hierarchical lexical-semantic relationships than embeddings in the Euclidean space. 1 Introduction The task of taxonomy induction aims at creating a semantic hierarchy of entities by using hyponym-hypernym relations – called taxonomy – from text corpora. Compared to many other domains of natural language processing that make use of pre-trained dense representations, state-ofthe-art taxonomy learning is still highly relying on traditional approaches like extraction of lexicalsyntactic patterns (Hearst, 1992) or co-occurrence information (Grefenstette, 2015). Despite the success of pattern-based approaches, most taxonomy induction systems suffer from a significant number of disconnected terms, since the extracted relationships are too specific to cover most words (Wang et al., 2017; Bordea et al., 2016). The use of distributional semantics for hypernym identification and relation representation has thus received increasing attention (Shwartz et al., 2016). However, Levy et al. (2015) observe that many proposed supervised approaches instead learn prototypical hypernyms (that are hypernyms to many o"
P19-1474,heylen-etal-2008-modelling,0,0.0250853,"s to capture the hierarchy and similarity between words simultaneously. It increases exponentially with the depth of the hierarchy. So while the distance of a leaf node to most other nodes in the hierarchy is very high, nodes on abstract levels, such as the root, have a comparably small distance to all nodes in the hierarchy. The word2vec embeddings have no notion of hierarchy and hierarchical relationships cannot be represented with vector offsets across the vocabulary (Fu et al., 2014). When applying word2vec, we use the observation that distributionally similar words are often co-hyponyms (Heylen et al., 2008; Weeds et al., 2014). 3.2 Relocation of Outlier Terms Poincar´e embeddings are used to compute and store a rank rank(x, y) between every child and parent of the existing taxonomy, defined as the index of y in the list of sorted Poincar´e distances of all entities of the taxonomy to x. Hypernymhyponym relationships with a rank larger than the mean of all ranks are removed, chosen on the basis of tests on the 2015 TExEval data (Bordea et al., 2015). Disconnected components that have children are re-connected to the most similar parent in the taxonomy or to the taxonomy root if no distributed re"
P19-1474,S16-1202,0,0.190789,"(Wang et al., 2017; Bordea et al., 2016). The use of distributional semantics for hypernym identification and relation representation has thus received increasing attention (Shwartz et al., 2016). However, Levy et al. (2015) observe that many proposed supervised approaches instead learn prototypical hypernyms (that are hypernyms to many other terms), not taking into account the relation between both terms in classification. Therefore, past applications of distributional semantics appear to be rather unsuitable to be directly applied to taxonomy induction as the sole signal (Tan et al., 2015; Pocostales, 2016). We address that issue by introducing a series of simple and parameter-free refinement steps that employ word embeddings in order to improve existing domain-specific taxonomies, induced from text using traditional approaches in an unsupervised fashion. We compare two types of dense vector embeddings: the standard word2vec CBOW model (Mikolov et al., 2013a,b), that embeds terms in Euclidean space based on distributional similarity, and the more recent Poincar´e embeddings (Nickel and Kiela, 2017), which capture similarity as well as hierarchical relationships in a hyperbolic space. The source"
P19-1474,L16-1572,1,0.835287,"improved taxonomy extraction system overall. 3.1 Domain-specific Poincar´e Embedding Training Dataset Construction To create domain-specific Poincar´e embeddings, we use noisy hypernym relationships extracted from a combination of general and domain-specific corpora. For the general domain, we extracted 59.2 GB of text from English Wikipedia, Gigaword (Parker et al., 2009), ukWac (Ferraresi et al., 2008) and LCC news corpora (Goldhahn et al., 2012). The domain-specific corpora consist of web pages, selected by using a combination of BootCat (Baroni and Bernardini, 2004) and focused crawling (Remus and Biemann, 2016). Noisy IS-A relations are extracted with lexicalsyntactic patterns from all corpora by applying PattaMaika2 , PatternSim (Panchenko et al., 2012), and WebISA (Seitner et al., 2016), following (Panchenko et al., 2016).3 The extracted noisy relationships of the common and domain-specific corpora are further processed separately and combined afterward. To limit the number of terms and relationships, we restrict the IS-A relationships on pairs for which both entities are part of the taxonomy’s vocabulary. Relations with a frequency of less than three are removed to filter noise. Besides further r"
P19-1474,L16-1056,0,0.0200324,"m relationships extracted from a combination of general and domain-specific corpora. For the general domain, we extracted 59.2 GB of text from English Wikipedia, Gigaword (Parker et al., 2009), ukWac (Ferraresi et al., 2008) and LCC news corpora (Goldhahn et al., 2012). The domain-specific corpora consist of web pages, selected by using a combination of BootCat (Baroni and Bernardini, 2004) and focused crawling (Remus and Biemann, 2016). Noisy IS-A relations are extracted with lexicalsyntactic patterns from all corpora by applying PattaMaika2 , PatternSim (Panchenko et al., 2012), and WebISA (Seitner et al., 2016), following (Panchenko et al., 2016).3 The extracted noisy relationships of the common and domain-specific corpora are further processed separately and combined afterward. To limit the number of terms and relationships, we restrict the IS-A relationships on pairs for which both entities are part of the taxonomy’s vocabulary. Relations with a frequency of less than three are removed to filter noise. Besides further removing every reflexive relationship, only the more frequent pair of a symmetric relationship is kept. Hence, the set of cleaned relationships is transformed into being antisymmetri"
P19-1474,P16-1226,0,0.201626,"Missing"
P19-1474,S16-1203,0,0.0333196,"Missing"
P19-1474,S15-2155,0,0.0474667,"Missing"
P19-1474,D17-1123,0,0.32268,"Missing"
P19-1474,C14-1212,0,0.029562,"archy and similarity between words simultaneously. It increases exponentially with the depth of the hierarchy. So while the distance of a leaf node to most other nodes in the hierarchy is very high, nodes on abstract levels, such as the root, have a comparably small distance to all nodes in the hierarchy. The word2vec embeddings have no notion of hierarchy and hierarchical relationships cannot be represented with vector offsets across the vocabulary (Fu et al., 2014). When applying word2vec, we use the observation that distributionally similar words are often co-hyponyms (Heylen et al., 2008; Weeds et al., 2014). 3.2 Relocation of Outlier Terms Poincar´e embeddings are used to compute and store a rank rank(x, y) between every child and parent of the existing taxonomy, defined as the index of y in the list of sorted Poincar´e distances of all entities of the taxonomy to x. Hypernymhyponym relationships with a rank larger than the mean of all ranks are removed, chosen on the basis of tests on the 2015 TExEval data (Bordea et al., 2015). Disconnected components that have children are re-connected to the most similar parent in the taxonomy or to the taxonomy root if no distributed representation exists."
P19-2044,E06-1002,0,0.167272,"Missing"
P19-2044,K18-1050,0,0.43043,"¨at Hamburg, Hamburg, Germany Skolkovo Institute of Science and Technology, Moscow, Russia ? Diffbot Inc., Menlo Park, CA, USA {sevgili,panchenko,biemann}@informatik.uni-hamburg.de Abstract the entity disambiguation task. The goal of an ED system is resolving the ambiguity of entity mentions, such as Mars, Galaxy, and Bounty are all delicious. It is hard for an algorithm to identify whether the entity is an astronomical structure1 or a brand of milk chocolate2 . Current neural approaches to EL/ED attempt to use context and word embeddings (and sometimes entity embeddings on mentions in text) (Kolitsas et al., 2018; Sun et al., 2015). Whereas these and most other previous approaches employ embeddings trained from text, we aim to create entity embeddings based on structured data (i.e. hyperlinks) using graph embeddings and integrate them into the ED models. Graph embeddings aim at representing nodes in a graph, or subgraph structure, by finding a mapping between a graph structure and the points in a low-dimensional vector space (Hamilton et al., 2017). The goal is to preserve the features of the graph structure and map these features to the geometric relationships, such as distances between different nod"
P19-2044,D07-1074,0,0.112723,"Missing"
P19-2044,K16-1026,0,0.0139628,"entity (Mihalcea and Csomai, 2007; Strube and Ponzetto, 2006; Bunescu and Pas¸ca, 2006). Similarly, Milne and Witten (2008) define a measurement of entityentity relatedness. Current state-of-the-art approaches are based on neural networks (Huang et al., 2015; Ganea and Hofmann, 2017; Kolitsas et al., 2018; Sun et al., 2015), where are based on character, word and/or entity embeddings created by a neural network with a motivation of their capability to automatically induce features, as opposed to hand-crafting them. Then, they all use these embeddings in neural EL/ED. Yamada et al. (2016) and Fang et al. (2016) utilize structured data modelling entities and words in the same space and mapping spans to entities based on the similarity in this space. They expand the objective function of word2vec (Mikolov et al., 2013a,b) and use both text and structured information. Radhakrishnan et al. (2018) extend the work of Yamada et al. (2016) by creating their own graph based on co-occurrences statistics instead of using the knowledge graph directly. Contrary to them, our model learns a mapping of spans and entities, which reside in different spaces and use graph embeddings trained on the knowledge graph for r"
P19-2044,D17-1277,0,0.132996,"Missing"
P19-2044,P10-1154,0,0.10386,"Missing"
P19-2044,N18-1167,0,0.0222984,", 2017; Kolitsas et al., 2018; Sun et al., 2015), where are based on character, word and/or entity embeddings created by a neural network with a motivation of their capability to automatically induce features, as opposed to hand-crafting them. Then, they all use these embeddings in neural EL/ED. Yamada et al. (2016) and Fang et al. (2016) utilize structured data modelling entities and words in the same space and mapping spans to entities based on the similarity in this space. They expand the objective function of word2vec (Mikolov et al., 2013a,b) and use both text and structured information. Radhakrishnan et al. (2018) extend the work of Yamada et al. (2016) by creating their own graph based on co-occurrences statistics instead of using the knowledge graph directly. Contrary to them, our model learns a mapping of spans and entities, which reside in different spaces and use graph embeddings trained on the knowledge graph for representing structured information. 3 Learning Graph-based Entity Vectors In order to make information from a semantic graph available for an entity linking system, we make use of graph embeddings. We use DeepWalk (Perozzi et al., 2014) to create the representation of entities in the DB"
P19-2044,P11-1138,0,0.225398,"Missing"
P19-2044,roder-etal-2014-n3,0,0.14207,"Missing"
P19-2044,K16-1025,0,0.0501088,"a mention and a candidate entity (Mihalcea and Csomai, 2007; Strube and Ponzetto, 2006; Bunescu and Pas¸ca, 2006). Similarly, Milne and Witten (2008) define a measurement of entityentity relatedness. Current state-of-the-art approaches are based on neural networks (Huang et al., 2015; Ganea and Hofmann, 2017; Kolitsas et al., 2018; Sun et al., 2015), where are based on character, word and/or entity embeddings created by a neural network with a motivation of their capability to automatically induce features, as opposed to hand-crafting them. Then, they all use these embeddings in neural EL/ED. Yamada et al. (2016) and Fang et al. (2016) utilize structured data modelling entities and words in the same space and mapping spans to entities based on the similarity in this space. They expand the objective function of word2vec (Mikolov et al., 2013a,b) and use both text and structured information. Radhakrishnan et al. (2018) extend the work of Yamada et al. (2016) by creating their own graph based on co-occurrences statistics instead of using the knowledge graph directly. Contrary to them, our model learns a mapping of spans and entities, which reside in different spaces and use graph embeddings trained on th"
P19-3031,P17-1002,0,0.19588,"e tagged with argument information or it routes keyword-based queries to the index to retrieve sentences in which the query terms match argument units. 3.1 Neural Sequence Tagger We implement a BiLSTM-CNN-CRF neural tagger (Ma and Hovy, 2016) for identifying argumentative units and for classifying them as claims or premises. The BiLSTM-CNN-CRF method is a popular sequence tagging approach and achieves (near) state-of-the-art performance for tasks like named entity recognition and part-of-speech tagging (Ma and Hovy, 2016; Lample et al., 2016); it has also been used for argument mining before (Eger et al., 2017). The general method relies on pre-computed word embeddings, a single bidirectional-LSTM/GRU recurrent layer, convolutional character-level embeddings to capture out-of-vocabulary words, and a first-order Condi3 github.com/explosion/displacy-ent Claims Premises Major Claims Backing Refutations Rebuttals None Combined Essays WebD IBM 22,443 67,157 10,966 47,619 3,670 20,906 10,775 867 2,247 46,352 8,073,589 35,349,501 3,710,839 148,185 84,817 47,133,929 Table 1: Number of tokens per category in the training datasets. Note that the IBM data contains many duplicate claims; it was originally publi"
P19-3031,J17-1004,0,0.168622,"laims. tional Random Field (Lafferty et al., 2001) to capture dependencies between adjacent tags. Besides the existing BiLSTM-CNN-CRF implementation of Reimers and Gurevych (2017), we also use an own Python 3.6 / PyTorch 1.0 implementation that does not contain any third-party dependencies, has native vectorized code for efficient training and evaluation, and supports several input data formats as well as evaluation functions. The different argument tagging models currently usable through TARGER’s API are trained on the persuasive essays (Essays) (Eger et al., 2017), the web discourse (WebD) (Habernal and Gurevych, 2017), and the IBM Debater (IBM) (Levy et al., 2018) datasets (characteristics in Table 1). The models use GloVe (Pennington et al., 2014), fastText (Mikolov et al., 2018), or dependency-based embeddings (Levy and Goldberg, 2014) (overview in Table 2). For training, the following variations were used for hyperparameter tuning: optimizer [SGD, Adam], learning rate [0.001, 0.05, 0.01], Data Embeddings Essays Tagger Essays Essays Essays fastText Dependency GloVe (Reimers and Gurevych, 2017) (Reimers and Gurevych, 2017) Ours WebD WebD WebD fastText Dependency GloVe (Reimers and Gurevych, 2017) (Reimers"
P19-3031,N16-1030,0,0.0448872,".3 The API routes free text inputs to the respective selected model to be tagged with argument information or it routes keyword-based queries to the index to retrieve sentences in which the query terms match argument units. 3.1 Neural Sequence Tagger We implement a BiLSTM-CNN-CRF neural tagger (Ma and Hovy, 2016) for identifying argumentative units and for classifying them as claims or premises. The BiLSTM-CNN-CRF method is a popular sequence tagging approach and achieves (near) state-of-the-art performance for tasks like named entity recognition and part-of-speech tagging (Ma and Hovy, 2016; Lample et al., 2016); it has also been used for argument mining before (Eger et al., 2017). The general method relies on pre-computed word embeddings, a single bidirectional-LSTM/GRU recurrent layer, convolutional character-level embeddings to capture out-of-vocabulary words, and a first-order Condi3 github.com/explosion/displacy-ent Claims Premises Major Claims Backing Refutations Rebuttals None Combined Essays WebD IBM 22,443 67,157 10,966 47,619 3,670 20,906 10,775 867 2,247 46,352 8,073,589 35,349,501 3,710,839 148,185 84,817 47,133,929 Table 1: Number of tokens per category in the training datasets. Note tha"
P19-3031,P14-2050,0,0.0111321,"lementation that does not contain any third-party dependencies, has native vectorized code for efficient training and evaluation, and supports several input data formats as well as evaluation functions. The different argument tagging models currently usable through TARGER’s API are trained on the persuasive essays (Essays) (Eger et al., 2017), the web discourse (WebD) (Habernal and Gurevych, 2017), and the IBM Debater (IBM) (Levy et al., 2018) datasets (characteristics in Table 1). The models use GloVe (Pennington et al., 2014), fastText (Mikolov et al., 2018), or dependency-based embeddings (Levy and Goldberg, 2014) (overview in Table 2). For training, the following variations were used for hyperparameter tuning: optimizer [SGD, Adam], learning rate [0.001, 0.05, 0.01], Data Embeddings Essays Tagger Essays Essays Essays fastText Dependency GloVe (Reimers and Gurevych, 2017) (Reimers and Gurevych, 2017) Ours WebD WebD WebD fastText Dependency GloVe (Reimers and Gurevych, 2017) (Reimers and Gurevych, 2017) Ours IBM IBM fastText GloVe (Reimers and Gurevych, 2017) Ours Approach STagBLCC TARGER (GloVe) F1 64.74 64.54 Web Discourse Approach SVMhmm TARGER (GloVe) F1 22.90 24.20 Table 3: Comparison of TARGER’s p"
P19-3031,C18-1176,0,0.397908,"ture dependencies between adjacent tags. Besides the existing BiLSTM-CNN-CRF implementation of Reimers and Gurevych (2017), we also use an own Python 3.6 / PyTorch 1.0 implementation that does not contain any third-party dependencies, has native vectorized code for efficient training and evaluation, and supports several input data formats as well as evaluation functions. The different argument tagging models currently usable through TARGER’s API are trained on the persuasive essays (Essays) (Eger et al., 2017), the web discourse (WebD) (Habernal and Gurevych, 2017), and the IBM Debater (IBM) (Levy et al., 2018) datasets (characteristics in Table 1). The models use GloVe (Pennington et al., 2014), fastText (Mikolov et al., 2018), or dependency-based embeddings (Levy and Goldberg, 2014) (overview in Table 2). For training, the following variations were used for hyperparameter tuning: optimizer [SGD, Adam], learning rate [0.001, 0.05, 0.01], Data Embeddings Essays Tagger Essays Essays Essays fastText Dependency GloVe (Reimers and Gurevych, 2017) (Reimers and Gurevych, 2017) Ours WebD WebD WebD fastText Dependency GloVe (Reimers and Gurevych, 2017) (Reimers and Gurevych, 2017) Ours IBM IBM fastText GloV"
P19-3031,P16-1101,0,0.0180999,"n an Elasticsearch BM25F-index of the DepCC (details in Section 3.2). The online usage is handled via a Flask-based web app whose API accepts AJAX requests from the Web UI component or via API calls (details in Sections 3.3 and 3.4). The web interface is based on the named entity visualiser displaCy ENT.3 The API routes free text inputs to the respective selected model to be tagged with argument information or it routes keyword-based queries to the index to retrieve sentences in which the query terms match argument units. 3.1 Neural Sequence Tagger We implement a BiLSTM-CNN-CRF neural tagger (Ma and Hovy, 2016) for identifying argumentative units and for classifying them as claims or premises. The BiLSTM-CNN-CRF method is a popular sequence tagging approach and achieves (near) state-of-the-art performance for tasks like named entity recognition and part-of-speech tagging (Ma and Hovy, 2016; Lample et al., 2016); it has also been used for argument mining before (Eger et al., 2017). The general method relies on pre-computed word embeddings, a single bidirectional-LSTM/GRU recurrent layer, convolutional character-level embeddings to capture out-of-vocabulary words, and a first-order Condi3 github.com/e"
P19-3031,L18-1008,0,0.0157992,"(2017), we also use an own Python 3.6 / PyTorch 1.0 implementation that does not contain any third-party dependencies, has native vectorized code for efficient training and evaluation, and supports several input data formats as well as evaluation functions. The different argument tagging models currently usable through TARGER’s API are trained on the persuasive essays (Essays) (Eger et al., 2017), the web discourse (WebD) (Habernal and Gurevych, 2017), and the IBM Debater (IBM) (Levy et al., 2018) datasets (characteristics in Table 1). The models use GloVe (Pennington et al., 2014), fastText (Mikolov et al., 2018), or dependency-based embeddings (Levy and Goldberg, 2014) (overview in Table 2). For training, the following variations were used for hyperparameter tuning: optimizer [SGD, Adam], learning rate [0.001, 0.05, 0.01], Data Embeddings Essays Tagger Essays Essays Essays fastText Dependency GloVe (Reimers and Gurevych, 2017) (Reimers and Gurevych, 2017) Ours WebD WebD WebD fastText Dependency GloVe (Reimers and Gurevych, 2017) (Reimers and Gurevych, 2017) Ours IBM IBM fastText GloVe (Reimers and Gurevych, 2017) Ours Approach STagBLCC TARGER (GloVe) F1 64.74 64.54 Web Discourse Approach SVMhmm TARGE"
P19-3031,L18-1286,1,0.859356,"lack of freely available tools that enable users, especially non-experts, to make use of the field’s recent advances. In this paper, we close this gap by introducing TARGER: a system with a userfriendly web interface1 that can extract argumentative units in free input texts in real-time using 1 ltdemos.informatik.uni-hamburg.de/targer models trained on recent argument mining corpora with a highly configurable and efficient neural sequence tagger. TARGER’s web interface and API also allow for very fast keyword-based argument retrieval from a pre-tagged version of the Common Crawl-based DepCC (Panchenko et al., 2018). The native PyTorch implementation underlying TARGER has no external depencies and is available as open source software:2 it can easily be incorporated into any existing NLP pipeline. 2 Related Work There are three publicly available systems offering some functionality similar to TARGER. ArgumenText (Stab et al., 2018) is an argument search engine that retrieves argumentative sentences from the Common Crawl and labels them as pro or con given a keyword-based user query. Similarly, args.me (Wachsmuth et al., 2017) retrieves pro and con arguments from 300,000 arguments crawled from debating por"
P19-3031,D14-1162,0,0.0822942,"lementation of Reimers and Gurevych (2017), we also use an own Python 3.6 / PyTorch 1.0 implementation that does not contain any third-party dependencies, has native vectorized code for efficient training and evaluation, and supports several input data formats as well as evaluation functions. The different argument tagging models currently usable through TARGER’s API are trained on the persuasive essays (Essays) (Eger et al., 2017), the web discourse (WebD) (Habernal and Gurevych, 2017), and the IBM Debater (IBM) (Levy et al., 2018) datasets (characteristics in Table 1). The models use GloVe (Pennington et al., 2014), fastText (Mikolov et al., 2018), or dependency-based embeddings (Levy and Goldberg, 2014) (overview in Table 2). For training, the following variations were used for hyperparameter tuning: optimizer [SGD, Adam], learning rate [0.001, 0.05, 0.01], Data Embeddings Essays Tagger Essays Essays Essays fastText Dependency GloVe (Reimers and Gurevych, 2017) (Reimers and Gurevych, 2017) Ours WebD WebD WebD fastText Dependency GloVe (Reimers and Gurevych, 2017) (Reimers and Gurevych, 2017) Ours IBM IBM fastText GloVe (Reimers and Gurevych, 2017) Ours Approach STagBLCC TARGER (GloVe) F1 64.74 64.54 We"
P19-3031,N18-1202,0,0.0112408,"ntative (e.g., punctuation) while the vast majority are tokens in claims and premises (but the only 150 different claims are heavily duplicated). Not surprisingly—given the class imbalance and duplication—, the resulting trained TARGER models “optimistically” identify some argumentative units in almost every input text. We still provide the models as a starting point with the intention to de-duplicate the data and to add more non-argumentative text passages for a more balanced / realistic training scenario. work, we plan to integrate contextualized embeddings with ELMo- and BERT-based models (Peters et al., 2018; Devlin et al., 2018). Finally, by looking at our experimental results as well as tagging examples for free input texts or the DepCC web data, we noticed that despite the recent advances in argument mining, there is still considerable headroom to improve in-domain, but especially out-of-domain argument tagging performance. Free input texts of different styles or genres taken from the web are tagged very inconsistently by the different models. More research on domain adaptation and transfer learning (Ruder, 2019) in the scenario of argument mining needs to address this issue. 4.2 TARGER @ TREC"
P19-3031,D17-1035,0,0.0240147,"i3 github.com/explosion/displacy-ent Claims Premises Major Claims Backing Refutations Rebuttals None Combined Essays WebD IBM 22,443 67,157 10,966 47,619 3,670 20,906 10,775 867 2,247 46,352 8,073,589 35,349,501 3,710,839 148,185 84,817 47,133,929 Table 1: Number of tokens per category in the training datasets. Note that the IBM data contains many duplicate claims; it was originally published as a dataset to identify relevant premises for 150 claims. tional Random Field (Lafferty et al., 2001) to capture dependencies between adjacent tags. Besides the existing BiLSTM-CNN-CRF implementation of Reimers and Gurevych (2017), we also use an own Python 3.6 / PyTorch 1.0 implementation that does not contain any third-party dependencies, has native vectorized code for efficient training and evaluation, and supports several input data formats as well as evaluation functions. The different argument tagging models currently usable through TARGER’s API are trained on the persuasive essays (Essays) (Eger et al., 2017), the web discourse (WebD) (Habernal and Gurevych, 2017), and the IBM Debater (IBM) (Levy et al., 2018) datasets (characteristics in Table 1). The models use GloVe (Pennington et al., 2014), fastText (Mikolo"
P19-3031,D15-1050,0,0.0322126,"mains and use cases, such as an application to search engine ranking that we also describe shortly. 1 Introduction Argumentation is a multi-disciplinary field that extends from philosophy and psychology to linguistics as well as to artificial intelligence. Recent developments in argument mining apply natural language processing (NLP) methods to argumentation (Palau and Moens, 2011; Lippi and Torroni, 2016a) and are mostly focused on training classifiers on annotated text fragments to identify argumentative text units, such as claims and premises (Biran and Rambow, 2011; Habernal et al., 2014; Rinott et al., 2015). More specifically, current approaches mainly focus on three tasks: (1) detection of sentences containing argumentative units, (2) detection of the argumentative units’ boundaries inside sentences, and (3) identifying relations between argumentative units. Despite vital research in argument mining, there is a lack of freely available tools that enable users, especially non-experts, to make use of the field’s recent advances. In this paper, we close this gap by introducing TARGER: a system with a userfriendly web interface1 that can extract argumentative units in free input texts in real-time"
P19-3031,N19-5004,0,0.0139533,"n to integrate contextualized embeddings with ELMo- and BERT-based models (Peters et al., 2018; Devlin et al., 2018). Finally, by looking at our experimental results as well as tagging examples for free input texts or the DepCC web data, we noticed that despite the recent advances in argument mining, there is still considerable headroom to improve in-domain, but especially out-of-domain argument tagging performance. Free input texts of different styles or genres taken from the web are tagged very inconsistently by the different models. More research on domain adaptation and transfer learning (Ruder, 2019) in the scenario of argument mining needs to address this issue. 4.2 TARGER @ TREC Common Core Track Acknowledgments As a proof of concept, we used TARGER’s model pre-trained on essays with dependencybased embeddings in a TREC 2018 Common Core track submission (Bondarenko et al., 2018). The TARGER API served as a subroutine in a pipeline axiomatically re-ranking (Hagen et al., 2016) BM25F retrieval results with respect to their argumentativeness (presence/absence of arguments). For the Washington Post corpus used in the track, the dependency-based essays model best tagged argumentative units i"
P19-3031,N18-5005,0,0.215224,"g.de/targer models trained on recent argument mining corpora with a highly configurable and efficient neural sequence tagger. TARGER’s web interface and API also allow for very fast keyword-based argument retrieval from a pre-tagged version of the Common Crawl-based DepCC (Panchenko et al., 2018). The native PyTorch implementation underlying TARGER has no external depencies and is available as open source software:2 it can easily be incorporated into any existing NLP pipeline. 2 Related Work There are three publicly available systems offering some functionality similar to TARGER. ArgumenText (Stab et al., 2018) is an argument search engine that retrieves argumentative sentences from the Common Crawl and labels them as pro or con given a keyword-based user query. Similarly, args.me (Wachsmuth et al., 2017) retrieves pro and con arguments from 300,000 arguments crawled from debating portals. Finally, MARGOT (Lippi and Torroni, 2016b) provides argument tagging for free-text inputs. However, answer times of MARGOT are rather slow for single input sentences (&gt;5 seconds) and the F1 scores of 17.5 for claim detection and 16.7 for evidence detection are slightly worse compared to our approach (see Section 4"
P19-3031,W17-5106,0,0.134112,"ed argument retrieval from a pre-tagged version of the Common Crawl-based DepCC (Panchenko et al., 2018). The native PyTorch implementation underlying TARGER has no external depencies and is available as open source software:2 it can easily be incorporated into any existing NLP pipeline. 2 Related Work There are three publicly available systems offering some functionality similar to TARGER. ArgumenText (Stab et al., 2018) is an argument search engine that retrieves argumentative sentences from the Common Crawl and labels them as pro or con given a keyword-based user query. Similarly, args.me (Wachsmuth et al., 2017) retrieves pro and con arguments from 300,000 arguments crawled from debating portals. Finally, MARGOT (Lippi and Torroni, 2016b) provides argument tagging for free-text inputs. However, answer times of MARGOT are rather slow for single input sentences (&gt;5 seconds) and the F1 scores of 17.5 for claim detection and 16.7 for evidence detection are slightly worse compared to our approach (see Section 4.1). TARGER offers a real-time retrieval functionality similar to ArgumenText and fast real-time freetext argument tagging with the option of switching between different pre-trained state-of-the-art"
R19-1008,S07-1002,0,0.189878,"an average of embeddings of lexical substitutes (also combination of both measures is tested). Finally, Amrami and Goldberg (2018) proposed using neural language models and dynamic symmetric patterns establishing a new best result on this dataset. Their approach is described in details in Section 3 as a starting point for our method. Related Work The first methods to word sense induction were proposed already in the late 90s (Pedersen and Bruce, 1997; Sch¨utze, 1998; Lin, 1998) with several competitions being organized to systematically evaluate various methods, including SemEval 2007 task 2 (Agirre and Soroa, 2007), SemEval 2010 task 14 (Manandhar et al., 2010) and SemEval 2013 task 13 (Jurgens and Klapaftis, 2013) for the English language, and RUSSE 2018 (Panchenko et al., 2018) for the Russian language.1 Navigli (2012) provides a survey of word sense induction and related approaches. Methods for word sense induction can be broadly classified into three groups: context clustering approaches, word (ego-network) clustering, and latent variable models. We discuss these approaches below. Also, note that methods for learning word sense embedding (Camacho-Collados and Pilehvar, 2018) can be used to induce ve"
R19-1008,D18-1523,0,0.444157,"Missing"
R19-1008,S13-2113,0,0.0188617,"from text. 2.1 2.2 This group of methods cluster word ego-networks consisting of a single node (ego) together with the nodes they are connected to (alters) and all the edges among those alters. Nodes of an egonetwork can be words semantically similar to the target word or context features relevant to the target. This line of work starts from the seminal work of (Widdows and Dorow, 2002) who used graph-based methods for unsupervised lexical acquisition. In this work, senses of the word were defined as connected components in a graph which excludes the ego. V´eronis (2004), Biemann (2006), and Hope and Keller (2013) further developed this idea by performing clustering of nodes instead of the simple search for connected components. Pelevina et al. (2016) proposed to transform word embeddings to sense embeddings using graph clustering (Biemann, 2006). The obtained sense embeddings were used to solve the WSI task based on similarity computations between the context and the induced sense. Context/Vector Clustering Methods This methods from this group represent a word instance by a vector that characterizes its context, where the definition of context can vary greatly. These vectors are subsequently clustered"
R19-1008,S19-2018,1,0.834716,"e. Context/Vector Clustering Methods This methods from this group represent a word instance by a vector that characterizes its context, where the definition of context can vary greatly. These vectors are subsequently clustered. Early approaches, such as (Pedersen and Bruce, 1997; Sch¨utze, 1998; Reisinger and Mooney, 2010) used sparse vector representations. Later approaches dense vector representations were adopted, e.g. Arefyev et al. (2018) and Kutuzov (2018) used weighted word embeddings (Mikolov et al., 2013) pre-trained on a large corpus to represent context of an ambiguous target word. Anwar et al. (2019) used contextualized (Peters et al., 2018) and non-contextualized (Mikolov et al., 2013) word embeddings to cluster occurrences of ambiguous occurrences of verbs according to their semantic frames. 1 Word/Graph Clustering Methods 2.3 Latent Variable Methods Methods from this group, define a generative process of the documents which include word senses as a latent variable and then perform estimation https://russe.nlpub.org 63 is done S times resulting in S representatives of the original example consisting of 2L substitutes each. Then TF-IDF BoW vectors for all representatives of all examples"
R19-1008,S13-2049,0,0.569392,"ead of an ambiguous word in a particular context, also known as lexical contextual substitutes, are very helpful for WSI because possible substitutes strongly depend on the expressed meaning of the ambiguous word. For instance, for the word build possible substitutes are manufacture, make, assemble, ship, export if it is used in the manufacturing some goods sense and erect, rebuild, open for the constructing a building sense. Baskaya et al. (2013) proposed generating substitutes using n-gram language models and had shown one of the best results at the SemEval-2013 WSI shared task for English (Jurgens and Klapaftis, 2013). Later Amrami and Goldberg (2018) proposed generating contextual substitutes with a bidirectional neural language model (biLM) ELMo (Peters et al., 2018). With several other improvements, they had achieved new state-of-the-art results on the same dataset. However, their method simply unites substitutes generated independently from probability distributions P (wi |wi−1 ...w1 ) and P (wi |wi+1 ...wT ) estimated by the forward and the backward ELMo LM independently, each given only one-sided context. This results in noisy substitutes when either left or right context is short or non-informative."
R19-1008,S13-2051,0,0.0781502,"Missing"
R19-1008,S13-2050,0,0.250766,"Missing"
R19-1008,S10-1011,0,0.0987039,"(also combination of both measures is tested). Finally, Amrami and Goldberg (2018) proposed using neural language models and dynamic symmetric patterns establishing a new best result on this dataset. Their approach is described in details in Section 3 as a starting point for our method. Related Work The first methods to word sense induction were proposed already in the late 90s (Pedersen and Bruce, 1997; Sch¨utze, 1998; Lin, 1998) with several competitions being organized to systematically evaluate various methods, including SemEval 2007 task 2 (Agirre and Soroa, 2007), SemEval 2010 task 14 (Manandhar et al., 2010) and SemEval 2013 task 13 (Jurgens and Klapaftis, 2013) for the English language, and RUSSE 2018 (Panchenko et al., 2018) for the Russian language.1 Navigli (2012) provides a survey of word sense induction and related approaches. Methods for word sense induction can be broadly classified into three groups: context clustering approaches, word (ego-network) clustering, and latent variable models. We discuss these approaches below. Also, note that methods for learning word sense embedding (Camacho-Collados and Pilehvar, 2018) can be used to induce vector representations of senses from text. 2.1 2"
R19-1008,W06-3812,0,0.0119399,"sentations of senses from text. 2.1 2.2 This group of methods cluster word ego-networks consisting of a single node (ego) together with the nodes they are connected to (alters) and all the edges among those alters. Nodes of an egonetwork can be words semantically similar to the target word or context features relevant to the target. This line of work starts from the seminal work of (Widdows and Dorow, 2002) who used graph-based methods for unsupervised lexical acquisition. In this work, senses of the word were defined as connected components in a graph which excludes the ego. V´eronis (2004), Biemann (2006), and Hope and Keller (2013) further developed this idea by performing clustering of nodes instead of the simple search for connected components. Pelevina et al. (2016) proposed to transform word embeddings to sense embeddings using graph clustering (Biemann, 2006). The obtained sense embeddings were used to solve the WSI task based on similarity computations between the context and the induced sense. Context/Vector Clustering Methods This methods from this group represent a word instance by a vector that characterizes its context, where the definition of context can vary greatly. These vector"
R19-1008,W97-0322,0,0.475443,"cosine similarities between pre-trained word embeddings by Mikolov et al. (2013). One measure relies on an average of embeddings of context words. Another one relies on an average of embeddings of lexical substitutes (also combination of both measures is tested). Finally, Amrami and Goldberg (2018) proposed using neural language models and dynamic symmetric patterns establishing a new best result on this dataset. Their approach is described in details in Section 3 as a starting point for our method. Related Work The first methods to word sense induction were proposed already in the late 90s (Pedersen and Bruce, 1997; Sch¨utze, 1998; Lin, 1998) with several competitions being organized to systematically evaluate various methods, including SemEval 2007 task 2 (Agirre and Soroa, 2007), SemEval 2010 task 14 (Manandhar et al., 2010) and SemEval 2013 task 13 (Jurgens and Klapaftis, 2013) for the English language, and RUSSE 2018 (Panchenko et al., 2018) for the Russian language.1 Navigli (2012) provides a survey of word sense induction and related approaches. Methods for word sense induction can be broadly classified into three groups: context clustering approaches, word (ego-network) clustering, and latent var"
R19-1008,W16-1620,1,0.817547,"cted to (alters) and all the edges among those alters. Nodes of an egonetwork can be words semantically similar to the target word or context features relevant to the target. This line of work starts from the seminal work of (Widdows and Dorow, 2002) who used graph-based methods for unsupervised lexical acquisition. In this work, senses of the word were defined as connected components in a graph which excludes the ego. V´eronis (2004), Biemann (2006), and Hope and Keller (2013) further developed this idea by performing clustering of nodes instead of the simple search for connected components. Pelevina et al. (2016) proposed to transform word embeddings to sense embeddings using graph clustering (Biemann, 2006). The obtained sense embeddings were used to solve the WSI task based on similarity computations between the context and the induced sense. Context/Vector Clustering Methods This methods from this group represent a word instance by a vector that characterizes its context, where the definition of context can vary greatly. These vectors are subsequently clustered. Early approaches, such as (Pedersen and Bruce, 1997; Sch¨utze, 1998; Reisinger and Mooney, 2010) used sparse vector representations. Later"
R19-1008,N18-1202,0,0.122085,"end on the expressed meaning of the ambiguous word. For instance, for the word build possible substitutes are manufacture, make, assemble, ship, export if it is used in the manufacturing some goods sense and erect, rebuild, open for the constructing a building sense. Baskaya et al. (2013) proposed generating substitutes using n-gram language models and had shown one of the best results at the SemEval-2013 WSI shared task for English (Jurgens and Klapaftis, 2013). Later Amrami and Goldberg (2018) proposed generating contextual substitutes with a bidirectional neural language model (biLM) ELMo (Peters et al., 2018). With several other improvements, they had achieved new state-of-the-art results on the same dataset. However, their method simply unites substitutes generated independently from probability distributions P (wi |wi−1 ...w1 ) and P (wi |wi+1 ...wT ) estimated by the forward and the backward ELMo LM independently, each given only one-sided context. This results in noisy substitutes when either left or right context is short or non-informative. The main contribution of this paper is an approach that combines the forward and the backward distributions into a single distribution and fuses the simi"
R19-1008,N10-1013,0,0.0448451,"of the simple search for connected components. Pelevina et al. (2016) proposed to transform word embeddings to sense embeddings using graph clustering (Biemann, 2006). The obtained sense embeddings were used to solve the WSI task based on similarity computations between the context and the induced sense. Context/Vector Clustering Methods This methods from this group represent a word instance by a vector that characterizes its context, where the definition of context can vary greatly. These vectors are subsequently clustered. Early approaches, such as (Pedersen and Bruce, 1997; Sch¨utze, 1998; Reisinger and Mooney, 2010) used sparse vector representations. Later approaches dense vector representations were adopted, e.g. Arefyev et al. (2018) and Kutuzov (2018) used weighted word embeddings (Mikolov et al., 2013) pre-trained on a large corpus to represent context of an ambiguous target word. Anwar et al. (2019) used contextualized (Peters et al., 2018) and non-contextualized (Mikolov et al., 2013) word embeddings to cluster occurrences of ambiguous occurrences of verbs according to their semantic frames. 1 Word/Graph Clustering Methods 2.3 Latent Variable Methods Methods from this group, define a generative pr"
R19-1008,J98-1004,0,0.801715,"Missing"
R19-1008,C02-1114,0,0.209575,"network) clustering, and latent variable models. We discuss these approaches below. Also, note that methods for learning word sense embedding (Camacho-Collados and Pilehvar, 2018) can be used to induce vector representations of senses from text. 2.1 2.2 This group of methods cluster word ego-networks consisting of a single node (ego) together with the nodes they are connected to (alters) and all the edges among those alters. Nodes of an egonetwork can be words semantically similar to the target word or context features relevant to the target. This line of work starts from the seminal work of (Widdows and Dorow, 2002) who used graph-based methods for unsupervised lexical acquisition. In this work, senses of the word were defined as connected components in a graph which excludes the ego. V´eronis (2004), Biemann (2006), and Hope and Keller (2013) further developed this idea by performing clustering of nodes instead of the simple search for connected components. Pelevina et al. (2016) proposed to transform word embeddings to sense embeddings using graph clustering (Biemann, 2006). The obtained sense embeddings were used to solve the WSI task based on similarity computations between the context and the induce"
S16-1206,baroni-bernardini-2004-bootcat,0,0.0434616,"awl Food FocusedCrawl Environment FocusedCrawl Science EN 11.0 59.2 168000.0 ‡ 22.8 23.9 8.8 FR 3.2 – – 7.9 8.9 5.4 NL 1.4 – – 3.4 2.0 6.6 IT 3.0 – – 3.6 7.1 5.1 Table 1: Corpora sizes used in our system in GB, where ‡is the size of the crawl archive. The second corpus is a concatenation of the English Wikipedia, Gigaword (Parker et al., 2009), ukWaC (Ferraresi et al., 2008) and a news corpora from the Leipzig Collection (Goldhahn et al., 2012). Domain-Specific Corpora. Lefever (2015) showed the usefulness for taxonomy extraction of domain dependent corpora crawled from the Web using BootCat (Baroni and Bernardini, 2004). This method takes terms as input, which are randomly combined into sequences of a pre-defined length, and sent to a Web search engine. The search results, i.e. the returned URLs, compose a domaindependent corpus. The number of input terms, the number of queries and the amount of desired URLs impact the size of the corpus. With 1,000 web queries and 10 URLs per query, the expected size of the resulting corpus is around 300 MB. While Lefever (2015) shows that such small in-domain corpora can be already useful for taxonomy extraction, we assumed that better results can be obtained if bigger dom"
S16-1206,S16-1168,0,0.255599,"Missing"
S16-1206,S16-1205,0,0.309212,"and languages (NL, FR, IT) for the multilingual setting. The BestComp lists the respective best scores across four our competitors. The best scores excluding the baseline are set in boldface. Definitions of the measures are available in Section 4. that improves structure of the resource. These united mechanisms are not used in other submissions to the challenge. The NUIG-UNLP team (Pocostales, 2016) relies on vector directionality in dense word embedding spaces. Such approximation of patterns based on distributional similarity provided good recall, but attained low precision. The QASSIT team (Cleuziou and Moreno, 2016), who ranked second in the competition, uses patterns to extract hypernym candidates, but they rely solely on the Wikipedia. Subsequently, an optimization technique based on genetic algorithms is used to learn the parametrization of a so-called pretopological space, which leads to desired structural properties of the resulting taxonomy. While we use simpler optimization procedure based on supervised learning, TAXI outperforms QASSIT in terms of comparisons with the gold standard. Possible reasons why our method performs better are (1) QASSIT use no substring features, (2) this team relies on s"
S16-1206,goldhahn-etal-2012-building,0,0.0150556,"e use three general purpose corpora in our approach presented in Table 1: Wikipedia, 59G and CommonCrawl1 . 1 https://commoncrawl.org 1321 Wikipedia 59G CommonCrawl FocusedCrawl Food FocusedCrawl Environment FocusedCrawl Science EN 11.0 59.2 168000.0 ‡ 22.8 23.9 8.8 FR 3.2 – – 7.9 8.9 5.4 NL 1.4 – – 3.4 2.0 6.6 IT 3.0 – – 3.6 7.1 5.1 Table 1: Corpora sizes used in our system in GB, where ‡is the size of the crawl archive. The second corpus is a concatenation of the English Wikipedia, Gigaword (Parker et al., 2009), ukWaC (Ferraresi et al., 2008) and a news corpora from the Leipzig Collection (Goldhahn et al., 2012). Domain-Specific Corpora. Lefever (2015) showed the usefulness for taxonomy extraction of domain dependent corpora crawled from the Web using BootCat (Baroni and Bernardini, 2004). This method takes terms as input, which are randomly combined into sequences of a pre-defined length, and sent to a Web search engine. The search results, i.e. the returned URLs, compose a domaindependent corpus. The number of input terms, the number of queries and the amount of desired URLs impact the size of the corpus. With 1,000 web queries and 10 URLs per query, the expected size of the resulting corpus is aro"
S16-1206,S15-2152,0,0.0318236,"co-syntactic patterns to harvest hypernyms from the Web. The extracted hypernym relation graph is subsequently pruned. Veraldi et al. (2013) proposed a graph-based algorithm to learn a taxonomy from textual definitions, extracted from a corpus and the Web. An optimal branching algorithm is used to induce a taxonomy. Finally, Bordea et al. (2015) introduced the first shared task on Taxonomy Extraction Evaluation to provide a common ground for evaluation. Six systems participated in the competition. The top system in this challange used features based on substrings and co-occurrence statistics (Grefenstette, 2015). Lefever et al. (2015) reached the second place gathered hypernyms from patterns, substrings and WordNet. Tan et al. (2015) used word embeddings, reaching the third place. 3 Taxonomy Induction Method Our approach is characterized by scalability and simplicity, assuming that being able to process larger input data is more important than the sophisticated extraction inference. Our approach to taxonomy induction takes as input a set of domain terms and general-domain text corpora and outputs a taxonomy. It consists of four steps. Firstly, we crawl domain-specific corpora based on terminology of"
S16-1206,C92-2082,0,0.718669,"some labeled examples might be utilized to tune the extraction and induction process, we avoid relying on structured lexical resources such as WordNet (Miller, 1995) or BabelNet (Navigli and Ponzetto, 2010). We rather envision a situation where a taxonomy shall be induced in a new domain or a new language for which such resources do not Related Work The extraction of taxonomic relationships from text is a long-standing challenge in ontology learning, see e.g. Biemann (2005) for a survey. The literature on hypernym extraction offers a high variability of methods, from simple lexical patterns (Hearst, 1992; Oakes, 2005), similar to those used in our method, to complex statistical techniques (Agirre et al., 2000; Ritter et al., 2009). Snow et al. (2004) use sentences that contain two terms which are known to be hypernyms. They parse sentences and extract patterns from the parse trees. Finally, they train a hypernym classifier based on these features and applied to text corpora. Yang and Callan (2009) presented a semisupervised taxonomy induction framework that integrates co-occurrence, syntactic dependencies, lexical-syntactic patterns and other features to learn an ontology metric, calculated i"
S16-1206,R11-2017,0,0.141238,"Missing"
S16-1206,P10-1150,0,0.0134154,"ncrementally clustered on the basis of their ontology metric scores. Snow et al. (2006) perform incremental construction of taxonomies using a probabilistic model. They combine evidence from multiple supervised classifiers trained on large training datasets of hyponymy and co-hyponymy relations. The taxonomy learning task is defined as the problem of finding the taxonomy that maximizes the probability of individ1320 Proceedings of SemEval-2016, pages 1320–1327, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics ual relations extracted by the classifiers. Kozareva and Hovy (2010) start from a set of root terms and use Hearst-like lexico-syntactic patterns to harvest hypernyms from the Web. The extracted hypernym relation graph is subsequently pruned. Veraldi et al. (2013) proposed a graph-based algorithm to learn a taxonomy from textual definitions, extracted from a corpus and the Web. An optimal branching algorithm is used to induce a taxonomy. Finally, Bordea et al. (2015) introduced the first shared task on Taxonomy Extraction Evaluation to provide a common ground for evaluation. Six systems participated in the competition. The top system in this challange used fea"
S16-1206,S15-2157,0,0.0549555,"ch presented in Table 1: Wikipedia, 59G and CommonCrawl1 . 1 https://commoncrawl.org 1321 Wikipedia 59G CommonCrawl FocusedCrawl Food FocusedCrawl Environment FocusedCrawl Science EN 11.0 59.2 168000.0 ‡ 22.8 23.9 8.8 FR 3.2 – – 7.9 8.9 5.4 NL 1.4 – – 3.4 2.0 6.6 IT 3.0 – – 3.6 7.1 5.1 Table 1: Corpora sizes used in our system in GB, where ‡is the size of the crawl archive. The second corpus is a concatenation of the English Wikipedia, Gigaword (Parker et al., 2009), ukWaC (Ferraresi et al., 2008) and a news corpora from the Leipzig Collection (Goldhahn et al., 2012). Domain-Specific Corpora. Lefever (2015) showed the usefulness for taxonomy extraction of domain dependent corpora crawled from the Web using BootCat (Baroni and Bernardini, 2004). This method takes terms as input, which are randomly combined into sequences of a pre-defined length, and sent to a Web search engine. The search results, i.e. the returned URLs, compose a domaindependent corpus. The number of input terms, the number of queries and the amount of desired URLs impact the size of the corpus. With 1,000 web queries and 10 URLs per query, the expected size of the resulting corpus is around 300 MB. While Lefever (2015) shows th"
S16-1206,P10-1023,1,0.702747,", to adapt the method to a new domain or language, only a small amount of manual labour is needed. 2 1 Introduction In this paper, we describe TAXI – a taxonomy induction method first presented at the SemEval 2016 challenge on Taxonomy Extraction Evaluation (Bordea et al., 2016). We consider taxonomy induction as a process that should – as much as possible – be driven solely on the basis of raw text processing. While some labeled examples might be utilized to tune the extraction and induction process, we avoid relying on structured lexical resources such as WordNet (Miller, 1995) or BabelNet (Navigli and Ponzetto, 2010). We rather envision a situation where a taxonomy shall be induced in a new domain or a new language for which such resources do not Related Work The extraction of taxonomic relationships from text is a long-standing challenge in ontology learning, see e.g. Biemann (2005) for a survey. The literature on hypernym extraction offers a high variability of methods, from simple lexical patterns (Hearst, 1992; Oakes, 2005), similar to those used in our method, to complex statistical techniques (Agirre et al., 2000; Ritter et al., 2009). Snow et al. (2004) use sentences that contain two terms which ar"
S16-1206,S16-1202,0,0.035602,"eline BestComp TAXI 0 0 0 0.009 0.016 0.189 64.28 178.22 64.94 40.50 34.89 1.00 0.009 0.016 0.189 n.a. 0.298 0.625 Table 3: Overall scores obtained by averaging the results over domains (Environment, Science, Food) and languages (NL, FR, IT) for the multilingual setting. The BestComp lists the respective best scores across four our competitors. The best scores excluding the baseline are set in boldface. Definitions of the measures are available in Section 4. that improves structure of the resource. These united mechanisms are not used in other submissions to the challenge. The NUIG-UNLP team (Pocostales, 2016) relies on vector directionality in dense word embedding spaces. Such approximation of patterns based on distributional similarity provided good recall, but attained low precision. The QASSIT team (Cleuziou and Moreno, 2016), who ranked second in the competition, uses patterns to extract hypernym candidates, but they rely solely on the Wikipedia. Subsequently, an optimization technique based on genetic algorithms is used to learn the parametrization of a so-called pretopological space, which leads to desired structural properties of the resulting taxonomy. While we use simpler optimization pro"
S16-1206,L16-1572,1,0.831085,"already useful for taxonomy extraction, we assumed that better results can be obtained if bigger domain-specific corpora are used. We therefore follow a different approach based on focused crawling, where BootCat is used only for initialization of seed URLs. We use the provided taxonomy terms as input for the BootCat method, generate 1,000 random triples, and use the retrieved URLs as a starting point for further crawling. Focused crawling is an extension to standard web crawling where URLs, expected to point to relevant web documents, are prioritized for download (Chakrabarti et al., 1999). Remus and Biemann (2016) introduced a focused crawling approach based on language modeling. The idea is that relevant web documents refer to other relevant web documents, where the relevance of a web document is computed by considering a statistical n-gram language model of a small, initially provided, domain-defining corpus. We provide a domain-defining corpus for each category by using Wikipedia articles, that are directly contained in the matching Wikipedia category. For example, for the the Food domain we used the Wikipedia articles of Category:Foods to build a language model of the Food domain. The language mode"
S16-1206,L16-1056,1,0.0775824,"PatternSim. This system was used to process English and French corpora. It encodes patterns in the form of finite state transducers implemented with the Unitex corpus processor.2 PatternSim relies on 10 English patterns yielding average precision of top 5 extracted semantic relations per word of 0.69 (Panchenko et al., 2012). For French, 9 hypernym extraction patterns are used providing precision at top 5 of 0.63 (Panchenko et al., 2013). WebISA. In addition to PattaMaika and PatternSim, we used a publicly available database of English hypernym relations extracted from the CommonCrawl corpus (Seitner et al., 2016). We used 108 million hypernym relations with frequency above one. This collection of relations was harvested using a regexp-based implementation of 59 patterns collected from the literature. Combination of hypernyms. Result of the extraction are 18 collections of hypernym relations listed in Table 2. Even the huge WebISA collection extracted from tens of terabytes of text does not provide hypernyms for all rare taxonomic terms, such as “ground and whole bean coffee” and “black sesame rice cake”. On the other hand, most of the collections contain many noisy relations. For instance, frequent re"
S16-1206,P06-1101,0,0.455322,"se sentences that contain two terms which are known to be hypernyms. They parse sentences and extract patterns from the parse trees. Finally, they train a hypernym classifier based on these features and applied to text corpora. Yang and Callan (2009) presented a semisupervised taxonomy induction framework that integrates co-occurrence, syntactic dependencies, lexical-syntactic patterns and other features to learn an ontology metric, calculated in terms of the semantic distance for each pair of terms in a taxonomy. Terms are incrementally clustered on the basis of their ontology metric scores. Snow et al. (2006) perform incremental construction of taxonomies using a probabilistic model. They combine evidence from multiple supervised classifiers trained on large training datasets of hyponymy and co-hyponymy relations. The taxonomy learning task is defined as the problem of finding the taxonomy that maximizes the probability of individ1320 Proceedings of SemEval-2016, pages 1320–1327, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics ual relations extracted by the classifiers. Kozareva and Hovy (2010) start from a set of root terms and use Hearst-like lexico-synt"
S16-1206,S15-2155,0,0.132777,"Missing"
S16-1206,S16-1203,0,0.384121,"r taxonomy extraction, we assumed that better results can be obtained if bigger domain-specific corpora are used. We therefore follow a different approach based on focused crawling, where BootCat is used only for initialization of seed URLs. We use the provided taxonomy terms as input for the BootCat method, generate 1,000 random triples, and use the retrieved URLs as a starting point for further crawling. Focused crawling is an extension to standard web crawling where URLs, expected to point to relevant web documents, are prioritized for download (Chakrabarti et al., 1999). Remus and Biemann (2016) introduced a focused crawling approach based on language modeling. The idea is that relevant web documents refer to other relevant web documents, where the relevance of a web document is computed by considering a statistical n-gram language model of a small, initially provided, domain-defining corpus. We provide a domain-defining corpus for each category by using Wikipedia articles, that are directly contained in the matching Wikipedia category. For example, for the the Food domain we used the Wikipedia articles of Category:Foods to build a language model of the Food domain. The language mode"
S16-1206,J13-3007,1,0.79304,"= j; (ti , tj ) ∈ T × T }. The pairs classified using the positive class are added to the taxonomy. nodes with out degree equal to zero, and the presence of cycles. Second, system outputs were compared against the corresponding domain gold standards and performances are evaluated in terms of Fscore. Here precision and recall are based on the number of edges in common with the gold standard taxonomy over the number of system edges and over the number of gold standard edges respectively. To better compare against gold standard taxonomies the task included the evaluation of a cumulative measure (Velardi et al., 2013), namely Cumulative Fowlkes & Mallows Measure (F&M), where the similarity between the system and the reference taxonomies are measured as the combination of the hierarchical cluster similarities. Finally, the organizers performed manual quality assessment to estimate the precision of the hypernyms. To compute this measure, annotators labeled a sample of 100 hypernym relations as correct or wrong. The taxonomy extraction was evaluated on four languages, namely English, Dutch, French and Italian, and three different domains (Food, Science and Environment). A detailed description of the evaluatio"
S16-1206,P09-1031,0,0.0656634,"ips from text is a long-standing challenge in ontology learning, see e.g. Biemann (2005) for a survey. The literature on hypernym extraction offers a high variability of methods, from simple lexical patterns (Hearst, 1992; Oakes, 2005), similar to those used in our method, to complex statistical techniques (Agirre et al., 2000; Ritter et al., 2009). Snow et al. (2004) use sentences that contain two terms which are known to be hypernyms. They parse sentences and extract patterns from the parse trees. Finally, they train a hypernym classifier based on these features and applied to text corpora. Yang and Callan (2009) presented a semisupervised taxonomy induction framework that integrates co-occurrence, syntactic dependencies, lexical-syntactic patterns and other features to learn an ontology metric, calculated in terms of the semantic distance for each pair of terms in a taxonomy. Terms are incrementally clustered on the basis of their ontology metric scores. Snow et al. (2006) perform incremental construction of taxonomies using a probabilistic model. They combine evidence from multiple supervised classifiers trained on large training datasets of hyponymy and co-hyponymy relations. The taxonomy learning"
S16-1206,S15-2151,1,\N,Missing
S19-1014,J15-4004,0,0.0726856,"anguage data: the overwhelming part of these unique pairs are synsets not related to each other at all. For most tasks, it is useless to ‘know’ that, e.g., ‘ambulance’ and ‘general’ are less similar than ‘ambulance’ and ‘president’. While the distances between these node pairs are indeed different on the WordNet graph, we find it much more important for the model to be able to robustly tell really similar pairs from the unrelated ones so that they could benefit applications. As a more balanced and relevant test set, we use noun pairs (666 total) from the SimLex999 semantic similarity dataset (Hill et al., 2015). SimLex999 contains lemmas; as some lemmas may map to several WordNet synsets, for each word pair we choose the synset pair maximizing the WordNet similarity, following (Resnik, 1999). Then, we measure the Spearman rank correlation between these ‘gold’ scores and the similarities produced by the graph embedding models trained on the WordNet. Further on, we call this evaluation score the ‘correlation with WordNet similarities’. This evaluation method directly measures how well the model fits the training objective4 . We also would like to check whether our models generalize to extrinsic tasks."
S19-1014,O97-1002,0,0.941242,"consist of pairs of noun synsets and their ‘ground truth’ similarity values. There exist several methods to calculate synset similarities on the WordNet (Budanitsky and Hirst, 2006). We compile four datasets, with different similarity functions: LeacockChodorow similarities (LCH); Jiang-Conrath similarities calculated over the SemCor corpus ( JCNS); Wu-Palmer similarities (WuP); and Shortest path similarities (ShP). LCH similarity (Leacock and Chodorow, 1998) is based on the shortest path between two synsets in the WordNet hypernym/hyponym taxonomy and its maximum depth, while JCN similarity (Jiang and Conrath, 1997) uses the lowest common parent of two synsets in the same taxonomy. JCN is significantly faster but additionally requires a corpus as a source of probabilistic data about the distributions of synsets (‘information content’). We employed the SemCor subset of the Brown corpus, manually annotated with word senses (Kucera and Francis, 1982). WuP similarities (Wu and Palmer, 1994) are based on the depth of the two nodes in the taxonomy and the depth of their most specific ancesRelation to Similar Models Our model is similar to the Skip-gram model (Mikolov et al., 2013), where pairs of words (vi , v"
S19-1014,J06-1003,0,0.269283,"ng Pairwise Similarities Selection of the Similarity Measures Our aim is to produce node embeddings that capture given similarities between nodes in a graph. In our case, the graph is WordNet, and the nodes are its 82,115 noun synsets. We focused on nouns since in WordNet and SimLex999 they are represented better than other parts of speech. Embeddings for synsets of different part of speech can be generated analogously. The training datasets consist of pairs of noun synsets and their ‘ground truth’ similarity values. There exist several methods to calculate synset similarities on the WordNet (Budanitsky and Hirst, 2006). We compile four datasets, with different similarity functions: LeacockChodorow similarities (LCH); Jiang-Conrath similarities calculated over the SemCor corpus ( JCNS); Wu-Palmer similarities (WuP); and Shortest path similarities (ShP). LCH similarity (Leacock and Chodorow, 1998) is based on the shortest path between two synsets in the WordNet hypernym/hyponym taxonomy and its maximum depth, while JCN similarity (Jiang and Conrath, 1997) uses the lowest common parent of two synsets in the same taxonomy. JCN is significantly faster but additionally requires a corpus as a source of probabilist"
S19-1014,W04-0807,0,0.382845,"al., 2014) FSE (Subercaze et al., 2015) 0.386 0.462 0.533 0.556 Raw WordNet JCN-S Raw WordNet LCH Raw WordNet ShP Raw WordNet WuP 0.487 0.513 0.513 0.474 path2vec JCN-S path2vec LCH path2vec ShP path2vec WuP 0.533 0.532 0.555 0.555 Table 3: Spearman correlations with human SimLex999 noun similarities (model synset selection). Discussion of Results Table 4 presents the WSD micro-F1 scores using raw WordNet similarities, 300D path2vec, Deepwalk and node2vec models, and the 128D FSE model. We evaluate on the following all-words English WSD test sets: Senseval-2 (Palmer et al., 2001), Senseval-3 (Mihalcea et al., 2004), and SemEval-15 Task 13 (Raganato et al., 2017). Raw WordNet similarities are still the best, but the path2vec models are consistently the second after them (and orders of magnitude faster), outperforming other graph embedding baselines. The largest drop between the original and vector-based measures is for JCN-S, which is also the only one which relies not only on graph but also on external information from a corpus, making it more difficult to approximate (see also Figure 2, where this measure distribution seems to be the most difficult to reproduce). Note that both the original graph-based"
S19-1014,Q17-1022,0,0.0279129,"Missing"
S19-1014,N09-2060,0,0.0444916,"Missing"
S19-1014,P15-1173,0,0.0428543,"Missing"
S19-1014,S01-1005,0,0.0859796,"kovec, 2016) Deepwalk (Perozzi et al., 2014) FSE (Subercaze et al., 2015) 0.386 0.462 0.533 0.556 Raw WordNet JCN-S Raw WordNet LCH Raw WordNet ShP Raw WordNet WuP 0.487 0.513 0.513 0.474 path2vec JCN-S path2vec LCH path2vec ShP path2vec WuP 0.533 0.532 0.555 0.555 Table 3: Spearman correlations with human SimLex999 noun similarities (model synset selection). Discussion of Results Table 4 presents the WSD micro-F1 scores using raw WordNet similarities, 300D path2vec, Deepwalk and node2vec models, and the 128D FSE model. We evaluate on the following all-words English WSD test sets: Senseval-2 (Palmer et al., 2001), Senseval-3 (Mihalcea et al., 2004), and SemEval-15 Task 13 (Raganato et al., 2017). Raw WordNet similarities are still the best, but the path2vec models are consistently the second after them (and orders of magnitude faster), outperforming other graph embedding baselines. The largest drop between the original and vector-based measures is for JCN-S, which is also the only one which relies not only on graph but also on external information from a corpus, making it more difficult to approximate (see also Figure 2, where this measure distribution seems to be the most difficult to reproduce). Not"
S19-1014,D14-1162,0,0.115764,"vk and vl are randomly sampled nodes from V . Embeddings are initialized randomly and trained with the Adam optimizer (Kingma and Ba, 2014) with early stopping.2 Once the model is trained, the computation of node similarities is approximated with the dot product of the learned node vectors, making the computations efficient: sˆij = vi · vj . embedding matrix, with the number of rows equal to the number of nodes and column width set to the desired embedding dimensionality. Finally, unlike the Skip-gram, we do not use any non-linearities. Another closely related model is Global Vectors (GloVe) (Pennington et al., 2014), which approximates the co-occurrence probabilities in a given corpus. The objective function to be minimized in GloVe model is L = P ˜j − log sij + bi + bj )2 , (vi ,vj )∈B f (sij )(vi · v where sij counts the number of co-occurrence of words vi and vj , bi and bj are additional biases for each word, and f (sij ) is a weighting function to give appropriate weight for rare co-occurrences. Like the Skip-gram, GloVe also uses two embedding matrices, but it relies on global information. 4 4.1 Computing Pairwise Similarities Selection of the Similarity Measures Our aim is to produce node embeddin"
S19-1014,P15-2002,0,0.381203,"tor space with a much lower dimensionality than the number of nodes. The method described in this paper falls into the category of ‘shallow embeddings’, meaning that we do not attempt to embed entire communities or neighborhoods: our aim is to approximate distances or similarities between (single) nodes. Existing approaches to solving this task mostly use either factorization of the graph adjacency matrix (Cao et al., 2015; Ou et al., 2016) or random walks over the graph as in Deepwalk (Perozzi et al., 2014) and node2vec (Grover and Leskovec, 2016). A completely different approach is taken by Subercaze et al. (2015), who directly embed the WordNet tree graph into Hamming hypercube binary representations. Their model is dubbed ‘Fast similarity embedding’ (FSE) and also optimizes one of our objectives, i.e. to provide a much quicker way of calculating semantic similarities based on WordNet knowledge. However, the FSE embeddings are not differentiable, limiting their use in many deep neural architectures, especially if fine-tuning is needed. TransE (Bordes et al., 2013) interprets entities as vectors in the low-dimensional embeddings space and relations as a translation operation between two entity vectors."
S19-1014,D16-1174,0,0.0379846,"Missing"
S19-1014,P94-1019,0,0.199846,"and Shortest path similarities (ShP). LCH similarity (Leacock and Chodorow, 1998) is based on the shortest path between two synsets in the WordNet hypernym/hyponym taxonomy and its maximum depth, while JCN similarity (Jiang and Conrath, 1997) uses the lowest common parent of two synsets in the same taxonomy. JCN is significantly faster but additionally requires a corpus as a source of probabilistic data about the distributions of synsets (‘information content’). We employed the SemCor subset of the Brown corpus, manually annotated with word senses (Kucera and Francis, 1982). WuP similarities (Wu and Palmer, 1994) are based on the depth of the two nodes in the taxonomy and the depth of their most specific ancesRelation to Similar Models Our model is similar to the Skip-gram model (Mikolov et al., 2013), where pairs of words (vi , vj ) from a training corpus are optimized to have their corresponding vectors dot product vi · v ˜j close to 1, while randomly generated pairs (‘negative samples’) are optimized to have their dot product close to 0. In the Skipgram model, the target is to minimize the log likelihood of the conditional probabilities of context words vj given current words vi , which is in the c"
S19-1014,E17-1010,0,0.0261646,"462 0.533 0.556 Raw WordNet JCN-S Raw WordNet LCH Raw WordNet ShP Raw WordNet WuP 0.487 0.513 0.513 0.474 path2vec JCN-S path2vec LCH path2vec ShP path2vec WuP 0.533 0.532 0.555 0.555 Table 3: Spearman correlations with human SimLex999 noun similarities (model synset selection). Discussion of Results Table 4 presents the WSD micro-F1 scores using raw WordNet similarities, 300D path2vec, Deepwalk and node2vec models, and the 128D FSE model. We evaluate on the following all-words English WSD test sets: Senseval-2 (Palmer et al., 2001), Senseval-3 (Mihalcea et al., 2004), and SemEval-15 Task 13 (Raganato et al., 2017). Raw WordNet similarities are still the best, but the path2vec models are consistently the second after them (and orders of magnitude faster), outperforming other graph embedding baselines. The largest drop between the original and vector-based measures is for JCN-S, which is also the only one which relies not only on graph but also on external information from a corpus, making it more difficult to approximate (see also Figure 2, where this measure distribution seems to be the most difficult to reproduce). Note that both the original graph-based measures and graph embeddings do not outperform"
S19-1014,W08-2006,0,0.0165187,"trained on the WordNet graph alone. ing it encodes paths (or other similarities) between graph nodes into dense vectors. Our first contribution is an effective and efficient approach to learn graph embeddings based on a user-defined custom similarity measure sim on a set of nodes V , e.g. the shortest path distance. The second contribution is an application of state-of-the-art graph embeddings to word sense disambiguation task. 2 Related Work Various methods have been employed in NLP to derive lexical similarity directly from geometrical properties of the WordNet graph, from random walks in (Rao et al., 2008) to kernels in ´ S´eaghdha, 2009). More recently, representa(O tion learning on graphs (Bordes et al., 2011) received much attention in various research communities; see (Hamilton et al., 2017a) for a thorough survey on the existing methods. All of them (including ours) are based on the idea of projecting graph nodes into a latent vector space with a much lower dimensionality than the number of nodes. The method described in this paper falls into the category of ‘shallow embeddings’, meaning that we do not attempt to embed entire communities or neighborhoods: our aim is to approximate distance"
S19-2004,S13-2050,0,0.0628273,"Missing"
S19-2004,C92-2082,0,0.527064,"model was utilized. It is likely that the large model could generate better substitutes, but we left it for future work. Nonlemmatized lowercased text was passed through all the layers of the model. We didn’t add biases of the last linear layer to obtain less frequent but more contextually suitable subwords. We took K most probable substitutes to represent each example (K=40 was selected on the development set), lemmatized them to get rid of grammatical bias, and then built TF-IDF bag-of-words vectors. To improve results we employ symmetric patterns. Symmetric patterns were first proposed in Hearst (1992) and then used in many cases, including Widdows and Dorow (2002), Panchenko et al. (2012), Schwartz et al. (2015), to extract lexical relations like hyponymy, hypernymy, cohyponymy, etc. from texts, and to augment lexical resources. However, we were not aware of any Hearst-like patterns designed specifically for verbs. Along with “T and ” pattern and trivial “T” and “ ” patterns we proposed and experimented with “T and then ”, “T and will ” and “T and then will ” patterns. We suppose that the meaning of a verb is better described not by its hypernyms or coBERT Hidden Representations In the pre"
S19-2004,C02-1114,0,0.164614,"l could generate better substitutes, but we left it for future work. Nonlemmatized lowercased text was passed through all the layers of the model. We didn’t add biases of the last linear layer to obtain less frequent but more contextually suitable subwords. We took K most probable substitutes to represent each example (K=40 was selected on the development set), lemmatized them to get rid of grammatical bias, and then built TF-IDF bag-of-words vectors. To improve results we employ symmetric patterns. Symmetric patterns were first proposed in Hearst (1992) and then used in many cases, including Widdows and Dorow (2002), Panchenko et al. (2012), Schwartz et al. (2015), to extract lexical relations like hyponymy, hypernymy, cohyponymy, etc. from texts, and to augment lexical resources. However, we were not aware of any Hearst-like patterns designed specifically for verbs. Along with “T and ” pattern and trivial “T” and “ ” patterns we proposed and experimented with “T and then ”, “T and will ” and “T and then will ” patterns. We suppose that the meaning of a verb is better described not by its hypernyms or coBERT Hidden Representations In the preliminary experiments, we compared dense representations from dif"
S19-2004,S13-2049,0,0.0211877,"an ambiguous word according to their meaning which is similar to Frame Induction. One of the major differences from Frame Induction is that WSI doesn’t require grouping together different words with similar meanings, however, we adopt some ideas from WSI in this work. Instead of graph or vector representation of word cooccurrence information traditionally used to solve WSI task, Baskaya et al. (2013) proposed exploiting n-gram language model (LM) to generate possible substitutes for an ambiguous word in a particular context. Their approach was one of the best in SemEval-2013 WSI shared task (Jurgens and Klapaftis, 2013). Struyanskiy and Arefyev (2018) proposed pretraining SOTA neural machine translation model built from Transformer blocks (Vaswani et al., 2017) to restore target words hidden from its input (replaced with a special token CENTERWORD). After pretraining, they exploited both predicted output embeddings to represent ambiguous words and attention weights to better weigh relevant context words in word2vec weighted average representation. A combination of these representations achieved SOTA results on one of the datasets from RUSSE’2018 Word Sense Induction for the Russian language shared task (Panc"
S19-2018,P98-1013,0,0.915854,"mEval 2019 Task 2 on unsupervised semantic frame induction (QasemiZadeh et al., 2019). Our approach separates this task into two independent steps: verb clustering using word and their context embeddings and role labeling by combining these embeddings with syntactical features. A simple combination of these steps shows very competitive results and can be extended to process other datasets and languages. 1 Introduction Recent years have seen a lot of interest in computational models of frame semantics, with the availability of annotated sources like PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998). Unfortunately, such annotated resources are very scarce due to their language and domain specificity. Consequently, there has been work that investigated methods for unsupervised frame acquisition and parsing (Lang and Lapata, 2010; Modi et al., 2012; Kallmeyer et al., 2018; Ustalov et al., 2018). Researchers have used different approaches to induce frames, including clustering verb-specific arguments as per their roles (Lang and Lapata, 2010), subject-verb-object triples (Ustalov et al., 2018), syntactic dependency representation using dependency formats like CoNLL (Modi et al., 2012; Titov"
S19-2018,J05-1004,0,0.383238,"he runner-up in Subtask A of the SemEval 2019 Task 2 on unsupervised semantic frame induction (QasemiZadeh et al., 2019). Our approach separates this task into two independent steps: verb clustering using word and their context embeddings and role labeling by combining these embeddings with syntactical features. A simple combination of these steps shows very competitive results and can be extended to process other datasets and languages. 1 Introduction Recent years have seen a lot of interest in computational models of frame semantics, with the availability of annotated sources like PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998). Unfortunately, such annotated resources are very scarce due to their language and domain specificity. Consequently, there has been work that investigated methods for unsupervised frame acquisition and parsing (Lang and Lapata, 2010; Modi et al., 2012; Kallmeyer et al., 2018; Ustalov et al., 2018). Researchers have used different approaches to induce frames, including clustering verb-specific arguments as per their roles (Lang and Lapata, 2010), subject-verb-object triples (Ustalov et al., 2018), syntactic dependency representation using dependency formats li"
S19-2018,Q17-1010,0,0.0444771,"e frame evoked by the sentence. The gold standard for this subtask is based on the FrameNet (Baker et al., 1998) definitions for frames. 2.1 Method on TensorFlow Hub.3 Among all the layers of this model, we used the mean-pooling layer for word and context embeddings. Method Since sentences evoking the same frame should receive the same labels, we used a verb clustering approach and experimented with a number of pre-trained word and sentence embeddings models, namely Word2Vec (Mikolov et al., 2013), ELMo (Peters et al., 2018), Universal Sentence Embeddings (Conneau et al., 2017), and fastText (Bojanowski et al., 2017). This setup is similar to treating the frame induction task as a word sense disambiguation task (Brown et al., 2011). We experimented with embedding different lexical units, such as verb (V), its sentence (context, C ), subject-verb-object (SVO) triples, and verb arguments. Combination of context and word representations (C+W) from Word2Vec and ELMo turned out to be the best combination in our case. We used the standard Google News Word2Vec embedding model by Mikolov et al. (2013). Since this model is trained on individual words only and the SemEval dataset contained phrasal verbs, such as fa"
S19-2018,W11-0110,0,0.0286894,"ns for frames. 2.1 Method on TensorFlow Hub.3 Among all the layers of this model, we used the mean-pooling layer for word and context embeddings. Method Since sentences evoking the same frame should receive the same labels, we used a verb clustering approach and experimented with a number of pre-trained word and sentence embeddings models, namely Word2Vec (Mikolov et al., 2013), ELMo (Peters et al., 2018), Universal Sentence Embeddings (Conneau et al., 2017), and fastText (Bojanowski et al., 2017). This setup is similar to treating the frame induction task as a word sense disambiguation task (Brown et al., 2011). We experimented with embedding different lexical units, such as verb (V), its sentence (context, C ), subject-verb-object (SVO) triples, and verb arguments. Combination of context and word representations (C+W) from Word2Vec and ELMo turned out to be the best combination in our case. We used the standard Google News Word2Vec embedding model by Mikolov et al. (2013). Since this model is trained on individual words only and the SemEval dataset contained phrasal verbs, such as fall back and buy out, we have considered only the first word in the phrase. If this word is not present in the model v"
S19-2018,D17-1070,0,0.0377806,"each highlighted verb according to the frame evoked by the sentence. The gold standard for this subtask is based on the FrameNet (Baker et al., 1998) definitions for frames. 2.1 Method on TensorFlow Hub.3 Among all the layers of this model, we used the mean-pooling layer for word and context embeddings. Method Since sentences evoking the same frame should receive the same labels, we used a verb clustering approach and experimented with a number of pre-trained word and sentence embeddings models, namely Word2Vec (Mikolov et al., 2013), ELMo (Peters et al., 2018), Universal Sentence Embeddings (Conneau et al., 2017), and fastText (Bojanowski et al., 2017). This setup is similar to treating the frame induction task as a word sense disambiguation task (Brown et al., 2011). We experimented with embedding different lexical units, such as verb (V), its sentence (context, C ), subject-verb-object (SVO) triples, and verb arguments. Combination of context and word representations (C+W) from Word2Vec and ELMo turned out to be the best combination in our case. We used the standard Google News Word2Vec embedding model by Mikolov et al. (2013). Since this model is trained on individual words only and the SemEval dat"
S19-2018,N18-1202,0,0.0333785,"which is usually the predicate. The goal is to label each highlighted verb according to the frame evoked by the sentence. The gold standard for this subtask is based on the FrameNet (Baker et al., 1998) definitions for frames. 2.1 Method on TensorFlow Hub.3 Among all the layers of this model, we used the mean-pooling layer for word and context embeddings. Method Since sentences evoking the same frame should receive the same labels, we used a verb clustering approach and experimented with a number of pre-trained word and sentence embeddings models, namely Word2Vec (Mikolov et al., 2013), ELMo (Peters et al., 2018), Universal Sentence Embeddings (Conneau et al., 2017), and fastText (Bojanowski et al., 2017). This setup is similar to treating the frame induction task as a word sense disambiguation task (Brown et al., 2011). We experimented with embedding different lexical units, such as verb (V), its sentence (context, C ), subject-verb-object (SVO) triples, and verb arguments. Combination of context and word representations (C+W) from Word2Vec and ELMo turned out to be the best combination in our case. We used the standard Google News Word2Vec embedding model by Mikolov et al. (2013). Since this model i"
S19-2018,S18-2016,0,0.0742481,"ures. A simple combination of these steps shows very competitive results and can be extended to process other datasets and languages. 1 Introduction Recent years have seen a lot of interest in computational models of frame semantics, with the availability of annotated sources like PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998). Unfortunately, such annotated resources are very scarce due to their language and domain specificity. Consequently, there has been work that investigated methods for unsupervised frame acquisition and parsing (Lang and Lapata, 2010; Modi et al., 2012; Kallmeyer et al., 2018; Ustalov et al., 2018). Researchers have used different approaches to induce frames, including clustering verb-specific arguments as per their roles (Lang and Lapata, 2010), subject-verb-object triples (Ustalov et al., 2018), syntactic dependency representation using dependency formats like CoNLL (Modi et al., 2012; Titov and Klementiev, 2012), and latent-variable PCFG models (Kallmeyer et al., 2018). The SemEval 2019 task of semantic frame and role induction consists of three subtasks: (A) 1. a method that uses contextualized distributional word representations (embeddings) for grouping verb"
S19-2018,S19-2003,0,0.200367,"e induction is similar to the word sense induction approach by Arefyev et al. (2018), which uses tf–idf-weighted context word embeddings for a shared task on word sense induction by Panchenko et al. (2018). In this unsupervised task, our approach for clustering mainly consists of exploring the effectiveness of already available pre-trained models.1 Main contributions of this paper are: We present our system for semantic frame induction that showed the best performance in Subtask B.1 and finished as the runner-up in Subtask A of the SemEval 2019 Task 2 on unsupervised semantic frame induction (QasemiZadeh et al., 2019). Our approach separates this task into two independent steps: verb clustering using word and their context embeddings and role labeling by combining these embeddings with syntactical features. A simple combination of these steps shows very competitive results and can be extended to process other datasets and languages. 1 Introduction Recent years have seen a lot of interest in computational models of frame semantics, with the availability of annotated sources like PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998). Unfortunately, such annotated resources are very scarce due to t"
S19-2018,E12-1003,0,0.059716,"Missing"
S19-2018,J93-2004,0,0.0648084,"Missing"
S19-2018,P18-2010,1,0.635096,"on of these steps shows very competitive results and can be extended to process other datasets and languages. 1 Introduction Recent years have seen a lot of interest in computational models of frame semantics, with the availability of annotated sources like PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998). Unfortunately, such annotated resources are very scarce due to their language and domain specificity. Consequently, there has been work that investigated methods for unsupervised frame acquisition and parsing (Lang and Lapata, 2010; Modi et al., 2012; Kallmeyer et al., 2018; Ustalov et al., 2018). Researchers have used different approaches to induce frames, including clustering verb-specific arguments as per their roles (Lang and Lapata, 2010), subject-verb-object triples (Ustalov et al., 2018), syntactic dependency representation using dependency formats like CoNLL (Modi et al., 2012; Titov and Klementiev, 2012), and latent-variable PCFG models (Kallmeyer et al., 2018). The SemEval 2019 task of semantic frame and role induction consists of three subtasks: (A) 1. a method that uses contextualized distributional word representations (embeddings) for grouping verbs to frame type cluster"
S19-2018,W12-1901,0,0.0329549,"th syntactical features. A simple combination of these steps shows very competitive results and can be extended to process other datasets and languages. 1 Introduction Recent years have seen a lot of interest in computational models of frame semantics, with the availability of annotated sources like PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998). Unfortunately, such annotated resources are very scarce due to their language and domain specificity. Consequently, there has been work that investigated methods for unsupervised frame acquisition and parsing (Lang and Lapata, 2010; Modi et al., 2012; Kallmeyer et al., 2018; Ustalov et al., 2018). Researchers have used different approaches to induce frames, including clustering verb-specific arguments as per their roles (Lang and Lapata, 2010), subject-verb-object triples (Ustalov et al., 2018), syntactic dependency representation using dependency formats like CoNLL (Modi et al., 2012; Titov and Klementiev, 2012), and latent-variable PCFG models (Kallmeyer et al., 2018). The SemEval 2019 task of semantic frame and role induction consists of three subtasks: (A) 1. a method that uses contextualized distributional word representations (embed"
W11-2502,C92-2082,0,0.192595,"ci , t, cj i linking two concepts ci , cj ∈ C with a semantic relation of type t ∈ T . We are dealing with six types of semantic relations: hyperonymy, co-hyponymy, meronymy, event (associative), attributes, and random: T = {hyper, coord, mero, event, attri, random}. We describe analytically and compare experimentally methods, which discover set of semantic relations ˆ for a given set of concepts C. A semantic relation R ˆ ∼ R. extraction algorithm aims to discover R One approach for semantic relations extraction is based on the lexico-syntactic patterns which are constructed either manually (Hearst, 1992) or semiautomatically (Snow et al., 2004). The alternative approach, adopted in this paper, is unsupervised (see e.g. Lin (1998a) or Sahlgren (2006)). It relies on a similarity measure between lexical units. Various measures are available. We compare 21 baseline measures: 8 knowledge-based, 4 corpus-based, and 9 web-based. We would like to answer on two questions: “What metric is most suitable for the unsupervised relation extraction?”, and “Does various metrics capture the same semantic relations?”. The second question is particularly interesting for developing of a meta-measure combining sev"
W11-2502,O97-1002,0,0.750355,"Missing"
W11-2502,P98-2127,0,0.0324083,"lations: hyperonymy, co-hyponymy, meronymy, event (associative), attributes, and random: T = {hyper, coord, mero, event, attri, random}. We describe analytically and compare experimentally methods, which discover set of semantic relations ˆ for a given set of concepts C. A semantic relation R ˆ ∼ R. extraction algorithm aims to discover R One approach for semantic relations extraction is based on the lexico-syntactic patterns which are constructed either manually (Hearst, 1992) or semiautomatically (Snow et al., 2004). The alternative approach, adopted in this paper, is unsupervised (see e.g. Lin (1998a) or Sahlgren (2006)). It relies on a similarity measure between lexical units. Various measures are available. We compare 21 baseline measures: 8 knowledge-based, 4 corpus-based, and 9 web-based. We would like to answer on two questions: “What metric is most suitable for the unsupervised relation extraction?”, and “Does various metrics capture the same semantic relations?”. The second question is particularly interesting for developing of a meta-measure combining several metrics. This information may also help us choose a measure well-suited for a concrete application. We extend existing sur"
W11-2502,W06-2501,0,0.596739,"ngth of the shortest path in the network between concepts; cij is a lowest common subsumer of concepts ci and cj ; P (c) is the probability of the concept, estimated from a corpus (see below). Then, the Inverted Edge Count measure (Jurafsky and Martin, 2009, p. 687) is Knowledge-based Measures The knowledge-based metrics use a hierarchical semantic network in order to calculate similarities. Some of the metrics also use counts derived from 12 where simg is a gloss-based similarity measure, and set Ci includes concept ci and all concepts which are directly related to it. Gloss Vectors measure (Patwardhan and Pedersen, 2006) is calculated as a cosine (9) between context vectors vi and vj of concepts ci and cj . A context vector calculated as following: X vi = fj . (8) ∀j:cj ∈Gi Here fj is a first-order co-occurrence vector, derived from the corpus of all glosses, and Gi is concatenation of glosses of the concept ci and all concepts which are directly related to it. We experiment with measures relying on the W ORD N ET 3.0 (Miller, 1995) as a semantic network and S EM C OR as a corpus (Miller et al., 1993). 2.3 Corpus-based measures We use four measures, which rely on the bagof-word distributional analysis (BDA) ("
W11-2502,2005.jeptalnrecital-recital.1,0,0.633756,"Missing"
W11-2502,J00-4006,0,\N,Missing
W11-2502,H93-1061,0,\N,Missing
W11-2502,P94-1019,0,\N,Missing
W11-2502,N04-3012,0,\N,Missing
W11-2502,C98-2122,0,\N,Missing
W12-0502,N09-1003,0,0.0620504,"Missing"
W12-0502,W11-2501,0,0.0163965,"ormance and direct assessment of 6 We used Mean as a hybrid measure and the following criteria: MAP(20), MAP(50), P(10), P(20) and P(50). We kept measures which were selected by most of the criteria. 7 An evaluation script is available at http://cental. extraction quality with the measure. Each of these datasets consists of a set of semantic relations R, such as ⟨agitator, syn, activist⟩, ⟨hawk , hyper, predator⟩, ⟨gun, syn,weapon⟩, and ⟨dishwasher, cohypo, reezer⟩. Each “target” term has roughly the same number of meaningful and random relations. We use two semantic relation datasets: BLESS (Baroni and Lenci, 2011) and SN. The first is used to assess hypernyms and cohyponyms extraction. BLESS relates 200 target terms (100 animate and 100 inanimate nouns) to 8625 relatum terms with 26554 semantic relations (14440 are meaningful and 12154 are random). Every relation has one of the following types: hypernym, co-hyponym, meronym, attribute, event, or random. We use the second dataset to evaluate synonymy extraction. SN relates 462 target terms (nouns) to 5910 relatum terms with 14682 semantic relations (7341 are meaningful and 7341 are random). We built SN from WordNet, Roget’s thesaurus, and a synonyms dat"
W12-0502,W03-0415,0,0.0521964,"Missing"
W12-0502,W02-0908,0,0.0938424,"Missing"
W12-0502,W02-1029,0,0.0521745,"Missing"
W12-0502,C92-2082,0,0.0745105,"culated with a cosine between their respective feature vectors. Pattern-based Measure We developed a novel similarity measure PatternWiki (13), which relies on 10 lexico-syntactic patterns. 2 First, we apply the patterns to the WA C YPEDIA corpus and get as a result a list of concordances (see below). Next, we select the concordances which contain at least two terms from the input vocabulary C. The semantic similarity sij between each two terms ci , cj ∈ C is equal to the number of their co-occurences in the same concordance. The set of the patterns we used is a compilation of the 6 classical Hearst (1992) patterns, aiming at the extraction of hypernymic relations, as well as 3 patterns retrieving some other hypernyms and co-hyponyms and 1 synonym extraction pattern, which we found in accordance with Hearst’s pattern discovery algorithm. The patterns are encoded in a form of finite-state transducers with the help of a corpus processing tool U NITEX 3 (Paumier, 2003). The main graph is a cascade of the subgraphs, each of which encodes one of the patterns. For example, Figure 2 presents the graph which extracts, e. g.: • such diverse {[occupations]} as {[doctors]}, {[engineers]} and {[scientists]"
W12-0502,heylen-etal-2008-modelling,0,0.0132842,"> 0.05, otherwise p ≤ 0.05. they only can determine relations between a limited number of terms. On the other hand, measures based on web and corpora are nearly unlimited in their coverage, but provide less precise results. Combination of the measures enables keeping high precision for frequent terms (e. g., “disease”) present in WordNet and dictionaries, and empowers calculation of relations between rare terms unlisted in the handcrafted resources (e. g., “bronchocele”) with web and corpus measures. Second, combinations work well because, as it was found in previous research (Sahlgren, 2006; Heylen et al., 2008), different measures provide complementary types of semantic relations. For instance, WordNet-based measures score higher hypernyms than associative relations; distributional analysis score high co-hyponyms and synonyms, etc. In that respect, a combination helps to recall more different relations. For example, a WordNet-based measure may return a hypernym ⟨salmon, seafood⟩, while a corpus-based measure would extract a co-hyponym ⟨salmon, mackerel⟩. Finally, the supervised combination method works better than unsupervised ones because of two reasons. First, the measures generate scores which ha"
W12-0502,P98-2127,0,0.121575,"tween them. Distributional Measures These measures are based on a distributional analysis of a 800M tokens corpus WAC YPE DIA (Baroni et al., 2009) tagged with T REE TAG GER and dependency-parsed with M ALT PARSER . We rely on our own implementation of two distributional measures. The distributional measure (9) performs Bag-of-words Distributional Analysis (BDA) (Sahlgren, 2006). We use as features the 5000 most frequent lemmas (nouns, adjectives, and verbs) from a context window of 3 words, excluding stopwords. The distributional measure (10) performs Syntactic Distributional Analysis (SDA) (Lin, 1998b). For this one, we use as features the 100.000 most frequent dependencylemma pairs. In our implementation of SDA a term ci is represented with a feature ⟨dtj , wk ⟩, if wk is not in a stoplist and dtj has one of the following dependency types: NMOD, P, PMOD, ADV, SBJ, OBJ, VMOD, COORD, CC, VC, DEP, PRD, AMOD, PRN, PRT, LGS, IOBJ, EXP, CLF, GAP . For both BDA and SDA: the feature matrix is normalized with Pointwise Mutual Information; similarities between terms are calculated with a cosine between their respective feature vectors. Pattern-based Measure We developed a novel similarity measure"
W12-0502,P09-1031,0,0.0434968,"Missing"
W12-0502,zesch-etal-2008-extracting,0,0.018801,"Missing"
W12-0502,W06-2501,0,\N,Missing
W12-0502,O97-1002,0,\N,Missing
W12-0502,P94-1019,0,\N,Missing
W12-0502,N04-3012,0,\N,Missing
W12-0502,C98-2122,0,\N,Missing
W13-5011,W10-3506,0,0.1985,"Missing"
W13-5011,W09-3206,0,0.0126826,"y constructed network, as opposed to manually crafted networks such as WordNet (Miller, 1995). Gabrilovich and Markovitch (2007) introduced Explicit Semantic Analysis (ESA), where the words of a document are represented as mixtures of concepts, i.e. Wikipedia pages, according to their occurence in the body texts of the pages. The experimental results show that this strategy works very well and outranks, for example, LSA (Landauer and Dumais, 1997) in the task of measuring document similarity. ESA was later extended by taking into account the graph structure provided by the links in Wikipedia (Yeh et al., 2009). The authors of this work used a PageRank-based algorithm on the graph for measuring word and document similarity. This approach was coined WikiWalk. Associating the elements of a text document under analysis with Wikipedia pages involves itself already many problems often encountered in NLP. The process where certain words and multiword expressions are associated with a certain Wikipedia page has been called Wikification (Mihalcea and Csomai, 2007). In our work, we take a more general approach, and try to associate the full input text to a set of Wikipedia pages according to different vector"
W16-1620,E09-1005,0,0.0100572,"systems were trained on the ukWaC corpus. ing the top-ranked SemEval submissions and AdaGram, were able to beat the most frequent sense baselines of the respective datasets (with the exception of the balanced version of TWSI). Similar results are observed for other unsupervised WSD methods (Nieto Pi˜na and Johansson, 2016). ambiguate word senses. The UoS system (Hope and Keller, 2013) is most similar to our approach: to induce senses it builds an ego-network of a word using dependency relations, which is subsequently clustered using a simple graph clustering algorithm. The La Sapienza system (Agirre and Soroa, 2009), relies on WordNet to get word senses and perform disambiguation. Table 5 shows a comparative evaluation of our method on the SemEval dataset. Like above, dependency-based (JBT) word similarities yield slightly better results than word embedding similarity (w2v) for inventory induction. In addition to these two configurations, we also built a model based on the TWSI sense inventory (only for nouns as the TWSI contains nouns only). This model significantly outperforms both JBT- and w2v-based models, thus precise sense inventories greatly influence WSD performance. As one may observe, performan"
W16-1620,P15-1072,0,0.0300438,"Missing"
W16-1620,J90-1003,0,0.346085,"ilarity of their respective vectors. For scalability reasons, we perform similarity computations via block matrix multiplications, using blocks of 1000 vectors. Similarities using JoBimText (JBT). In this unsupervised approach, every word is represented 1 https://code.google.com/p/word2vec We used an English Wikipedia dump of October 2015: http://panchenko.me/data/joint/ corpora/en59g/wikipedia.txt.gz 2 176 as a bag of sparse dependency-based features extracted using the Malt parser and collapsed using an approach similar to (Ruppert et al., 2015). Features are normalized using the LMI score (Church and Hanks, 1990) and further pruned down according to the recommended defaults: we keep 1000 features per word and 1000 words per feature. Similarity of two words is equal to the number of common features. Multiple alternatives exist for computation of semantic relatedness (Zhang et al., 2013). JBT has two advantages in our case: (1) accurate estimation of word similarities based on dependency features; (2) efficient computation of nearest neighbours for all words in a corpus. Besides, we observed that nearest neighbours of word embeddings often tend to belong to the dominant sense, even if minor senses have"
W16-1620,J10-4006,0,0.010358,"se to downstream applications. Experiments show that the performance of our method is comparable to state-of-the-art unsupervised WSD systems. 1 Introduction 2 Term representations in the form of dense vectors are useful for many natural language processing applications. First of all, they enable the computation of semantically related words. Besides, they can be used to represent other linguistic units, such as phrases and short texts, reducing the inherent sparsity of traditional vector-space representations (Salton et al., 1975). One limitation of most word vector models, including sparse (Baroni and Lenci, 2010) and dense (Mikolov et al., 2013) representations, is that they conflate all senses of a word into a single vector. Several architectures for learning multiprototype embeddings were proposed that try to address this shortcoming (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014; Nieto Pi˜na and Johansson, 2015; Bartunov et al., 2016). Li and Jurafsky (2015) provide indications that such sense vectors improve the performance of text proRelated Work Our method learns multi-prototype word embeddings and applies them to WSD. Below we briefly review both strains of research. 2.1 Multi"
W16-1620,P14-1023,0,0.201214,"on 175 the word similarity graph, which relies on dependency features and is expected to provide more accurate similarities (therefore, the stage (2) is changed). Second, we use a sense inventory constructed using crowdsourcing (thus, stages (2) and (3) are skipped). Below we describe each of the stages of our method in detail. 3.1 To learn word vectors, we use the word2vec toolkit (Mikolov et al., 2013), namely we train CBOW word embeddings with 100 or 300 dimensions, context window size of 3 and minimum word frequency of 5. We selected these parameters according to prior evaluations, e.g. (Baroni et al., 2014), and tested them on the development dataset (see Section 5.1). Initial experiments showed that this configuration is superior to others, e.g. the Skip-gram model, with respect to WSD performance. For training, we modified the standard implementation of word2vec1 so that it also saves context vectors needed for one of our WSD approaches. For experiments, we use two commonly used corpora for training distributional models: Wikipedia2 and ukWaC (Ferraresi et al., 2008). Figure 2: Visualization of the ego-network of “table” with furniture and data sense clusters. Note that the target “table” is e"
W16-1620,P12-1092,0,0.0216553,"ssing applications. First of all, they enable the computation of semantically related words. Besides, they can be used to represent other linguistic units, such as phrases and short texts, reducing the inherent sparsity of traditional vector-space representations (Salton et al., 1975). One limitation of most word vector models, including sparse (Baroni and Lenci, 2010) and dense (Mikolov et al., 2013) representations, is that they conflate all senses of a word into a single vector. Several architectures for learning multiprototype embeddings were proposed that try to address this shortcoming (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014; Nieto Pi˜na and Johansson, 2015; Bartunov et al., 2016). Li and Jurafsky (2015) provide indications that such sense vectors improve the performance of text proRelated Work Our method learns multi-prototype word embeddings and applies them to WSD. Below we briefly review both strains of research. 2.1 Multi-Prototype Word Vector Spaces In his pioneering work, Sch¨utze (1998) induced sparse sense vectors by clustering context vectors using the EM algorithm. This approach is fitted with a similarity-based WSD mechanism. Later, Reisinger and Mooney (20"
W16-1620,S13-2050,0,0.0250181,"gher performance (0.410 vs 0.390) on the balanced TWSI, indicating the importance of a precise sense inventory. Finally, using the ”gold” TWSI inventory significantly improves the performance on the balanced dataset outperforming models based on induced inventories. 5.2 Evaluation metrics. Performance is measured with three measures that require a mapping of sense inventories (Jaccard Index, Tau and WNDCG) and two cluster comparison measures (Fuzzy NMI and Fuzzy B-Cubed). Discussion of results. We compare our approach to SemEval participants and the AdaGram sense embeddings. The AI-KU system (Baskaya et al., 2013) directly clusters test contexts using the k-means algorithm based on lexical substitution features. The Unimelb system (Lau et al., 2013) uses a hierarchical topic model to induce and disEvaluation on SemEval-2013 Task 13 The goal of this evaluation is to compare the performance of our method to state-of-the-art unsupervised WSD systems. 180 Supervised Evaluation Jacc. Ind. Tau WNDCG Model Clustering Evaluation F.NMI F.B-Cubed Baselines One sense for all One sense per instance Most Frequent Sense (MFS) 0.171 0.000 0.579 0.627 0.953 0.583 0.302 0.000 0.431 0.000 0.072 – 0.631 0.000 – SemEval A"
W16-1620,S13-2049,0,0.0332809,"38 0.305 0.275 0.366 0.383 0.360 0.338 0.305 0.275 Table 3: Upper-bound and actual value of the WSD performance on the sense-balanced TWSI dataset, function of sense inventory used for unweighted pooling of word vectors. Figure 3: WSD performance of our method trained on the Wikipedia corpus on the full (on the left) and on the sense-balanced (on the right) TWSI dataset. The w2v models are based on the CBOW with 300 dimensions and context window size 3. The JBT models are computed using the Malt parser. Dataset. The SemEval-2013 task 13 “Word Sense Induction for Graded and Non-Graded Senses” (Jurgens and Klapaftis, 2013) provides 20 nouns, 20 verbs and 10 adjectives in WordNetsense-tagged contexts. It contains 20-100 contexts per word, and 4,664 contexts in total, which were drawn from the Open American National Corpus. Participants were asked to cluster these 4,664 instances into groups, with each group corresponding to a distinct word sense. to the mechanism based on probabilities. Indeed, cosine similarity between embeddings proved to be useful for semantic relatedness, yielding stateof-the-art results (Baroni et al., 2014), while there is less evidence about successful use-cases of the CBOW as a language"
W16-1620,W02-0811,0,0.0984888,"6) introduced AdaGram, a non-parametric method for learning sense embeddings based on a Bayesian extension of the Skipgram model. The granularity of learned sense embeddings is controlled by the parameter α. Comparisons of their approach to (Neelakantan et al., 2014) on three SemEval word sense induction and 2.2 Word Sense Disambiguation (WSD) Many different designs of WSD systems were proposed, see (Agirre and Edmonds, 2007; Navigli, 2009). Supervised approaches use an explicitly sense-labeled training corpus to construct a model, usually building one model per target word (Lee and Ng, 2002; Klein et al., 2002). These approaches demonstrate top performance in competitions, but require considerable amounts of senselabeled examples. Knowledge-based approaches do not learn a model per target, but rather derive sense representation from information available in a lexical resource, such as WordNet. Examples of such system include (Lesk, 1986; Banerjee and Pedersen, 2002; Pedersen et al., 2005; Moro et al., 2014) Unsupervised WSD approaches rely neither on hand-annotated sense-labeled corpora, nor on 175 the word similarity graph, which relies on dependency features and is expected to provide more accurat"
W16-1620,W06-3812,1,0.718622,"aC (Ferraresi et al., 2008). Figure 2: Visualization of the ego-network of “table” with furniture and data sense clusters. Note that the target “table” is excluded from clustering. handcrafted lexical resources. Instead, they automatically induce a sense inventory from raw corpora. Such unsupervised sense induction methods fall into two categories: context clustering, such as (Pedersen and Bruce, 1997; Sch¨utze, 1998; Reisinger and Mooney, 2010; Neelakantan et al., 2014; Bartunov et al., 2016) and word (egonetwork) clustering, such as (Lin, 1998; Pantel and Lin, 2002; Widdows and Dorow, 2002; Biemann, 2006; Hope and Keller, 2013). Unsupervised methods use disambiguation clues from the induced sense inventory for word disambiguation. Usually, the WSD procedure is determined by the design of sense inventory. It might be the highest overlap between the instance’s context words and the words of the sense cluster, as in (Hope and Keller, 2013) or the smallest distance between context words and sense hubs in graph sense representation, as in (V´eronis, 2004). 3 Learning Word Vectors 3.2 Calculating Word Similarity Graph At this step, we build a graph of word similarities, such as (table, desk, 0.78)."
W16-1620,S13-2051,0,0.108765,"Missing"
W16-1620,W10-2309,1,0.835524,"ustered with the Chinese Whispers algorithm (Biemann, 2006). This method is parameter free, thus we make no assumptions about the number of word senses. The sense induction algorithm has three metaparameters: the ego-network size (N ) of the target ego word t; the ego-network connectivity (n) is the maximum number of connections the neighbour v is allowed to have within the ego-network; the minimum size of the cluster k. The n parameter regulates the granularity of the inventory. In our experiments, we set the N to 200, n to 50, 100 or 200 and k to 5 or 15 to obtain different granulates, cf. (Biemann, 2010). Each word in a sense cluster has a weight which is equal to the similarity score between this word and the ambiguous word t. Algorithm 1: Word sense induction. input : T – word similarity graph, N – ego-network size, n – ego-network connectivity, k – minimum cluster size output: for each term t ∈ T , a clustering St of its N most similar terms foreach t ∈ T do V ← N most similar terms of t from T G ← graph with V as nodes and no edges E foreach v ∈ V do V 0 ← n most similar terms of v from T foreach v 0 ∈ V 0 do if v 0 ∈ V then add edge (v, v 0 ) to E end end St ← ChineseWhispers(G) St ← {s"
W16-1620,W02-1006,0,0.0174832,"rtunov et al. (2016) introduced AdaGram, a non-parametric method for learning sense embeddings based on a Bayesian extension of the Skipgram model. The granularity of learned sense embeddings is controlled by the parameter α. Comparisons of their approach to (Neelakantan et al., 2014) on three SemEval word sense induction and 2.2 Word Sense Disambiguation (WSD) Many different designs of WSD systems were proposed, see (Agirre and Edmonds, 2007; Navigli, 2009). Supervised approaches use an explicitly sense-labeled training corpus to construct a model, usually building one model per target word (Lee and Ng, 2002; Klein et al., 2002). These approaches demonstrate top performance in competitions, but require considerable amounts of senselabeled examples. Knowledge-based approaches do not learn a model per target, but rather derive sense representation from information available in a lexical resource, such as WordNet. Examples of such system include (Lesk, 1986; Banerjee and Pedersen, 2002; Pedersen et al., 2005; Moro et al., 2014) Unsupervised WSD approaches rely neither on hand-annotated sense-labeled corpora, nor on 175 the word similarity graph, which relies on dependency features and is expected to"
W16-1620,biemann-2012-turk,1,0.907374,"Missing"
W16-1620,D14-1162,0,0.109945,"Missing"
W16-1620,D15-1200,0,0.272475,"represent other linguistic units, such as phrases and short texts, reducing the inherent sparsity of traditional vector-space representations (Salton et al., 1975). One limitation of most word vector models, including sparse (Baroni and Lenci, 2010) and dense (Mikolov et al., 2013) representations, is that they conflate all senses of a word into a single vector. Several architectures for learning multiprototype embeddings were proposed that try to address this shortcoming (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014; Nieto Pi˜na and Johansson, 2015; Bartunov et al., 2016). Li and Jurafsky (2015) provide indications that such sense vectors improve the performance of text proRelated Work Our method learns multi-prototype word embeddings and applies them to WSD. Below we briefly review both strains of research. 2.1 Multi-Prototype Word Vector Spaces In his pioneering work, Sch¨utze (1998) induced sparse sense vectors by clustering context vectors using the EM algorithm. This approach is fitted with a similarity-based WSD mechanism. Later, Reisinger and Mooney (2010) presented a multiprototype vector space. Sparse TF-IDF vectors are clustered using a parametric method fixing the same num"
W16-1620,N10-1013,0,0.157269,"012; Tian et al., 2014; Neelakantan et al., 2014; Nieto Pi˜na and Johansson, 2015; Bartunov et al., 2016). Li and Jurafsky (2015) provide indications that such sense vectors improve the performance of text proRelated Work Our method learns multi-prototype word embeddings and applies them to WSD. Below we briefly review both strains of research. 2.1 Multi-Prototype Word Vector Spaces In his pioneering work, Sch¨utze (1998) induced sparse sense vectors by clustering context vectors using the EM algorithm. This approach is fitted with a similarity-based WSD mechanism. Later, Reisinger and Mooney (2010) presented a multiprototype vector space. Sparse TF-IDF vectors are clustered using a parametric method fixing the same number of senses for all words. Sense vectors are centroids of the clusters. While most dense word vector models represent a word with a single vector and thus conflate senses (Mikolov et al., 2013; Pennington et al., 2014), there are several approaches that produce word sense embeddings. Huang et al. (2012) learn 174 Proceedings of the 1st Workshop on Representation Learning for NLP, pages 174–183, c Berlin, Germany, August 11th, 2016. 2016 Association for Computational Ling"
W16-1620,P15-1173,0,0.0368148,"Missing"
W16-1620,Q14-1019,0,0.0655666,"girre and Edmonds, 2007; Navigli, 2009). Supervised approaches use an explicitly sense-labeled training corpus to construct a model, usually building one model per target word (Lee and Ng, 2002; Klein et al., 2002). These approaches demonstrate top performance in competitions, but require considerable amounts of senselabeled examples. Knowledge-based approaches do not learn a model per target, but rather derive sense representation from information available in a lexical resource, such as WordNet. Examples of such system include (Lesk, 1986; Banerjee and Pedersen, 2002; Pedersen et al., 2005; Moro et al., 2014) Unsupervised WSD approaches rely neither on hand-annotated sense-labeled corpora, nor on 175 the word similarity graph, which relies on dependency features and is expected to provide more accurate similarities (therefore, the stage (2) is changed). Second, we use a sense inventory constructed using crowdsourcing (thus, stages (2) and (3) are skipped). Below we describe each of the stages of our method in detail. 3.1 To learn word vectors, we use the word2vec toolkit (Mikolov et al., 2013), namely we train CBOW word embeddings with 100 or 300 dimensions, context window size of 3 and minimum wo"
W16-1620,D14-1113,0,0.156749,"enable the computation of semantically related words. Besides, they can be used to represent other linguistic units, such as phrases and short texts, reducing the inherent sparsity of traditional vector-space representations (Salton et al., 1975). One limitation of most word vector models, including sparse (Baroni and Lenci, 2010) and dense (Mikolov et al., 2013) representations, is that they conflate all senses of a word into a single vector. Several architectures for learning multiprototype embeddings were proposed that try to address this shortcoming (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014; Nieto Pi˜na and Johansson, 2015; Bartunov et al., 2016). Li and Jurafsky (2015) provide indications that such sense vectors improve the performance of text proRelated Work Our method learns multi-prototype word embeddings and applies them to WSD. Below we briefly review both strains of research. 2.1 Multi-Prototype Word Vector Spaces In his pioneering work, Sch¨utze (1998) induced sparse sense vectors by clustering context vectors using the EM algorithm. This approach is fitted with a similarity-based WSD mechanism. Later, Reisinger and Mooney (2010) presented a multiprototype vector space."
W16-1620,J98-1004,0,0.241594,"Missing"
W16-1620,R15-1061,0,0.0688527,"Missing"
W16-1620,C14-1016,0,0.131499,"First of all, they enable the computation of semantically related words. Besides, they can be used to represent other linguistic units, such as phrases and short texts, reducing the inherent sparsity of traditional vector-space representations (Salton et al., 1975). One limitation of most word vector models, including sparse (Baroni and Lenci, 2010) and dense (Mikolov et al., 2013) representations, is that they conflate all senses of a word into a single vector. Several architectures for learning multiprototype embeddings were proposed that try to address this shortcoming (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014; Nieto Pi˜na and Johansson, 2015; Bartunov et al., 2016). Li and Jurafsky (2015) provide indications that such sense vectors improve the performance of text proRelated Work Our method learns multi-prototype word embeddings and applies them to WSD. Below we briefly review both strains of research. 2.1 Multi-Prototype Word Vector Spaces In his pioneering work, Sch¨utze (1998) induced sparse sense vectors by clustering context vectors using the EM algorithm. This approach is fitted with a similarity-based WSD mechanism. Later, Reisinger and Mooney (2010) presented a mul"
W16-1620,W16-1401,0,0.0673895,"Missing"
W16-1620,C02-1114,0,0.211562,"odels: Wikipedia2 and ukWaC (Ferraresi et al., 2008). Figure 2: Visualization of the ego-network of “table” with furniture and data sense clusters. Note that the target “table” is excluded from clustering. handcrafted lexical resources. Instead, they automatically induce a sense inventory from raw corpora. Such unsupervised sense induction methods fall into two categories: context clustering, such as (Pedersen and Bruce, 1997; Sch¨utze, 1998; Reisinger and Mooney, 2010; Neelakantan et al., 2014; Bartunov et al., 2016) and word (egonetwork) clustering, such as (Lin, 1998; Pantel and Lin, 2002; Widdows and Dorow, 2002; Biemann, 2006; Hope and Keller, 2013). Unsupervised methods use disambiguation clues from the induced sense inventory for word disambiguation. Usually, the WSD procedure is determined by the design of sense inventory. It might be the highest overlap between the instance’s context words and the words of the sense cluster, as in (Hope and Keller, 2013) or the smallest distance between context words and sense hubs in graph sense representation, as in (V´eronis, 2004). 3 Learning Word Vectors 3.2 Calculating Word Similarity Graph At this step, we build a graph of word similarities, such as (tabl"
W16-1620,W97-0322,0,0.563393,"he standard implementation of word2vec1 so that it also saves context vectors needed for one of our WSD approaches. For experiments, we use two commonly used corpora for training distributional models: Wikipedia2 and ukWaC (Ferraresi et al., 2008). Figure 2: Visualization of the ego-network of “table” with furniture and data sense clusters. Note that the target “table” is excluded from clustering. handcrafted lexical resources. Instead, they automatically induce a sense inventory from raw corpora. Such unsupervised sense induction methods fall into two categories: context clustering, such as (Pedersen and Bruce, 1997; Sch¨utze, 1998; Reisinger and Mooney, 2010; Neelakantan et al., 2014; Bartunov et al., 2016) and word (egonetwork) clustering, such as (Lin, 1998; Pantel and Lin, 2002; Widdows and Dorow, 2002; Biemann, 2006; Hope and Keller, 2013). Unsupervised methods use disambiguation clues from the induced sense inventory for word disambiguation. Usually, the WSD procedure is determined by the design of sense inventory. It might be the highest overlap between the instance’s context words and the words of the sense cluster, as in (Hope and Keller, 2013) or the smallest distance between context words and"
W16-1620,S01-1021,0,\N,Missing
W17-1909,E03-1020,0,0.0844048,"hese hybrid methods, our approach does not require access to web search engines, texts mapped to a sense inventory, or machine translation systems. 1 Introduction The representation of word senses and the disambiguation of lexical items in context is an ongoing long-established branch of research (Agirre and Edmonds, 2007; Navigli, 2009). Traditionally, word senses are defined and represented in lexical resources, such as WordNet (Fellbaum, 1998), while more recently, there is an increased interest in approaches that induce word senses from corpora using graph-based distributional approaches (Dorow and Widdows, 2003; Biemann, 2006; Hope and Keller, 2013), word sense embeddings (Neelakantan et al., 2014; Bartunov et al., 2 Related Work Several prior approaches combined distributional information extracted from text (Turney and Pantel, 2010) from text with information available in lexical resources, such as WordNet. Yu and Dredze (2014) proposed a model to learn word embeddings based on lexical relations of words from WordNet and PPDB (Ganitkevitch et al., 2013). The objective function of their model 72 Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications, pag"
W17-1909,W06-3812,1,0.424857,"approach does not require access to web search engines, texts mapped to a sense inventory, or machine translation systems. 1 Introduction The representation of word senses and the disambiguation of lexical items in context is an ongoing long-established branch of research (Agirre and Edmonds, 2007; Navigli, 2009). Traditionally, word senses are defined and represented in lexical resources, such as WordNet (Fellbaum, 1998), while more recently, there is an increased interest in approaches that induce word senses from corpora using graph-based distributional approaches (Dorow and Widdows, 2003; Biemann, 2006; Hope and Keller, 2013), word sense embeddings (Neelakantan et al., 2014; Bartunov et al., 2 Related Work Several prior approaches combined distributional information extracted from text (Turney and Pantel, 2010) from text with information available in lexical resources, such as WordNet. Yu and Dredze (2014) proposed a model to learn word embeddings based on lexical relations of words from WordNet and PPDB (Ganitkevitch et al., 2013). The objective function of their model 72 Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications, pages 72–78, c Val"
W17-1909,N15-1184,0,0.0163787,"based on lexical relations of words from WordNet and PPDB (Ganitkevitch et al., 2013). The objective function of their model 72 Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications, pages 72–78, c Valencia, Spain, April 4 2017. 2017 Association for Computational Linguistics All these diverse contributions indicate the benefits of hybrid knowledge sources for learning word and sense representations. combines the objective function of the skip-gram model (Mikolov et al., 2013) with a term that takes into account lexical relations of a target word. Faruqui et al. (2015) proposed a related approach that performs a post-processing of word embeddings on the basis of lexical relations from the same resources. Pham et al. (2015) introduced another model that also aim at improving word vector representations by using lexical relations from WordNet. The method makes representations of synonyms closer than representations of antonyms of the given word. While these three models improve the performance on word relatedness evaluations, they do not model word senses. Jauhar et al. (2015) proposed two models that tackle this shortcoming, learning sense embeddings using t"
W17-1909,N15-1059,0,0.154874,"D) task. In particular, the contribution of this paper is a new unsupervised knowledge-based approach to WSD based on the hybrid aligned resource (HAR) introduced by Faralli et al. (2016). The key difference of our approach from prior hybrid methods based on sense embeddings, e.g. (Rothe and Sch¨utze, 2015), is that we rely on sparse lexical representations that make the sense representation readable and allow to straightforwardly use this representation for word sense disambiguation, as will be shown below. In contrast to hybrid approaches based on sparse interpretable representations, e.g. (Camacho-Collados et al., 2015a), our method requires no mapping of texts to a sense inventory and thus can be applied to larger text collections. By linking symbolic distributional sense representations to lexical resources, we are able to improve representations of senses, leading to performance gains in word sense disambiguation. We introduce a new method for unsupervised knowledge-based word sense disambiguation (WSD) based on a resource that links two types of sense-aware lexical networks: one is induced from a corpus using distributional semantics, the other is manually constructed. The combination of two networks re"
W17-1909,N13-1092,0,0.0846982,"Missing"
W17-1909,P15-1072,0,0.0758306,"D) task. In particular, the contribution of this paper is a new unsupervised knowledge-based approach to WSD based on the hybrid aligned resource (HAR) introduced by Faralli et al. (2016). The key difference of our approach from prior hybrid methods based on sense embeddings, e.g. (Rothe and Sch¨utze, 2015), is that we rely on sparse lexical representations that make the sense representation readable and allow to straightforwardly use this representation for word sense disambiguation, as will be shown below. In contrast to hybrid approaches based on sparse interpretable representations, e.g. (Camacho-Collados et al., 2015a), our method requires no mapping of texts to a sense inventory and thus can be applied to larger text collections. By linking symbolic distributional sense representations to lexical resources, we are able to improve representations of senses, leading to performance gains in word sense disambiguation. We introduce a new method for unsupervised knowledge-based word sense disambiguation (WSD) based on a resource that links two types of sense-aware lexical networks: one is induced from a corpus using distributional semantics, the other is manually constructed. The combination of two networks re"
W17-1909,N15-1165,0,0.0236298,"l, 2013). Word Sense Induction. In DTs, entries of polysemous terms are mixed, i.e. they contain related terms of several senses. The Chinese Whispers (Biemann, 2006) graph clustering is applied to the ego-network (Everett and Borgatti, 2005) of the each term, as defined by its related terms and connections between then observed in the DT to derive word sense clusters. Rothe and Sch¨utze (2015) proposed a method that learns sense embedding using word embeddings and the sense inventory of WordNet. The approach was evaluated on the WSD tasks using features based on the learned sense embeddings. Goikoetxea et al. (2015) proposed a method for learning word embeddings using random walks on a graph of a lexical resource. Nieto Pi˜na and Johansson (2016) used a similar approach based on random walks on a WordNet to learn sense embeddings. Labeling Word Senses with Hypernyms. Hearst (1992) patterns are used to extract hypernyms from the corpus. These hypernyms are assigned to senses by aggregating hypernym 73 PCZ ID mouse:0 mouse:1 keyboard:0 keyboard:1 WordNet ID mouse:1 mouse:4 keyboard:1 keyboard:1 Related Terms rat:0, rodent:0, monkey:0, ... keyboard:1, computer:0, printer:0 ... piano:1, synthesizer:2, organ:"
W17-1909,P15-2003,0,0.0258444,"pproach is to train the standard skipgram model on a pre-disambiguated corpus using the Babelfy WSD system (Moro et al., 2014). NASARI (Camacho-Collados et al., 2015a) relies on Wikipedia and WordNet to produce vector representations of senses. In this approach, a sense is represented in lexical or sense-based feature spaces. The links between WordNet and Wikipedia are retrieved from BabelNet. MUFFIN (Camacho-Collados et al., 2015b) adapts several ideas from NASARI, extending the method to the multi-lingual case by using BabelNet synsets instead of monolingual WordNet synsets. The approach of Chen et al. (2015) to learning sense embeddings starts from initialization of sense vectors using WordNet glosses. It proceeds by performing a more conventional context clustering, similar what is found to unsupervised methods such as (Neelakantan et al., 2014; Bartunov et al., 2016). Building a Distributional Thesaurus (DT). At this stage, a similarity graph over terms is induced from a corpus, where each entry consists of the most similar 200 terms for a given term using the JoBimText method (Biemann and Riedl, 2013). Word Sense Induction. In DTs, entries of polysemous terms are mixed, i.e. they contain relat"
W17-1909,C92-2082,0,0.455908,"lated terms and connections between then observed in the DT to derive word sense clusters. Rothe and Sch¨utze (2015) proposed a method that learns sense embedding using word embeddings and the sense inventory of WordNet. The approach was evaluated on the WSD tasks using features based on the learned sense embeddings. Goikoetxea et al. (2015) proposed a method for learning word embeddings using random walks on a graph of a lexical resource. Nieto Pi˜na and Johansson (2016) used a similar approach based on random walks on a WordNet to learn sense embeddings. Labeling Word Senses with Hypernyms. Hearst (1992) patterns are used to extract hypernyms from the corpus. These hypernyms are assigned to senses by aggregating hypernym 73 PCZ ID mouse:0 mouse:1 keyboard:0 keyboard:1 WordNet ID mouse:1 mouse:4 keyboard:1 keyboard:1 Related Terms rat:0, rodent:0, monkey:0, ... keyboard:1, computer:0, printer:0 ... piano:1, synthesizer:2, organ:0 ... keypad:0, mouse:1, screen:1 ... Hypernyms animal:0, species:1, ... device:1, equipment:3, ... instrument:2, device:3, ... device:1, technology:0 ... Context Clues rat:conj and, white-footed:amod, ... click:-prep of, click:-nn, .... play:-dobj, electric:amod, .. co"
W17-1909,S07-1015,0,0.22111,"n context. For each test instance consisting of a target word and its context, we select the sense whose corresponding sense representation has the highest cosine similarity with the target word’s context. improvements in the results. Further expansion of the sense representation with context clues (cf. Table 1) provide a modest further improvement on the SemEval-2007 dataset and yield no further improvement on the case of the Senseval-3 dataset. 4 Comparison to the state-of-the-art. We compare our approach to four state-of-the-art systems: KnowNet (Cuadros and Rigau, 2008), BabelNet, WN+XWN (Cuadros and Rigau, 2007), and NASARI. KnowNet builds sense representations based on snippets retrieved with a web search engine. We use the best configuration reported in the original paper (KnowNet-20), which extends each sense with 20 keywords. BabelNet in its core relies on a mapping of WordNet synsets and Wikipedia articles to obtain enriched sense representations. The WN+XWN system is the topranked unsupervised knowledge-based system of Senseval-3 and SemEval-2007 datasets from the original competition (Cuadros and Rigau, 2007). It alleviates sparsity by combining WordNet with the eXtended WordNet (Mihalcea and"
W17-1909,C08-1021,0,0.0324758,"from the hybrid aligned resource. form WSD in context. For each test instance consisting of a target word and its context, we select the sense whose corresponding sense representation has the highest cosine similarity with the target word’s context. improvements in the results. Further expansion of the sense representation with context clues (cf. Table 1) provide a modest further improvement on the SemEval-2007 dataset and yield no further improvement on the case of the Senseval-3 dataset. 4 Comparison to the state-of-the-art. We compare our approach to four state-of-the-art systems: KnowNet (Cuadros and Rigau, 2008), BabelNet, WN+XWN (Cuadros and Rigau, 2007), and NASARI. KnowNet builds sense representations based on snippets retrieved with a web search engine. We use the best configuration reported in the original paper (KnowNet-20), which extends each sense with 20 keywords. BabelNet in its core relies on a mapping of WordNet synsets and Wikipedia articles to obtain enriched sense representations. The WN+XWN system is the topranked unsupervised knowledge-based system of Senseval-3 and SemEval-2007 datasets from the original competition (Cuadros and Rigau, 2007). It alleviates sparsity by combining Word"
W17-1909,W16-1401,0,0.0467827,"Missing"
W17-1909,N15-1070,0,0.0148651,"al., 2013) with a term that takes into account lexical relations of a target word. Faruqui et al. (2015) proposed a related approach that performs a post-processing of word embeddings on the basis of lexical relations from the same resources. Pham et al. (2015) introduced another model that also aim at improving word vector representations by using lexical relations from WordNet. The method makes representations of synonyms closer than representations of antonyms of the given word. While these three models improve the performance on word relatedness evaluations, they do not model word senses. Jauhar et al. (2015) proposed two models that tackle this shortcoming, learning sense embeddings using the word sense inventory of WordNet. 3 Unsupervised Knowledge-based WSD using Hybrid Aligned Resource We rely on the hybrid aligned lexical semantic resource proposed by Faralli et al. (2016) to perform WSD. We start with a short description of this resource and then discuss how it is used for WSD. 3.1 Construction of the Hybrid Aligned Resource (HAR) The hybrid aligned resource links two lexical semantic networks using the method of Faralli et al. (2016): a corpus-based distributionallyinduced network and a man"
W17-1909,W16-1620,1,0.89163,"Missing"
W17-1909,S10-1011,0,0.0341299,"original publications. However, as NASARI was not evaluated on the datasets used in our study, we used the following procedure to obtain NASARI-based sense representations: Each WordNet-based sense representation was extended with all features from the lexical vectors of NASARI.2 Thus, we compare our method to three hybrid systems that induce sense representations on the Evaluation We perform an extrinsic evaluation and show the impact of the hybrid aligned resource on word sense disambiguation performance. While there exist many datasets for WSD (Mihalcea et al., 2004; Pradhan et al., 2007; Manandhar et al., 2010, inter alia), we follow Navigli and Ponzetto (2012) and use the SemEval-2007 Task 16 on the “Evaluation of wide-coverage knowledge resources” (Cuadros and Rigau, 2007). This task is specifically designed for evaluating the impact of lexical resources on WSD performance. The SemEval-2007 Task 16 is, in turn, based on two “lexical sample” datasets, from the Senseval-3 (Mihalcea et al., 2004) and SemEval-2007 Task 17 (Pradhan et al., 2007) evaluation campaigns. The first dataset has coarse- and fine-grained annotations, while the second contains only fine-grained sense annotations. In all experi"
W17-1909,P15-2004,0,0.0159052,"on Sense, Concept and Entity Representations and their Applications, pages 72–78, c Valencia, Spain, April 4 2017. 2017 Association for Computational Linguistics All these diverse contributions indicate the benefits of hybrid knowledge sources for learning word and sense representations. combines the objective function of the skip-gram model (Mikolov et al., 2013) with a term that takes into account lexical relations of a target word. Faruqui et al. (2015) proposed a related approach that performs a post-processing of word embeddings on the basis of lexical relations from the same resources. Pham et al. (2015) introduced another model that also aim at improving word vector representations by using lexical relations from WordNet. The method makes representations of synonyms closer than representations of antonyms of the given word. While these three models improve the performance on word relatedness evaluations, they do not model word senses. Jauhar et al. (2015) proposed two models that tackle this shortcoming, learning sense embeddings using the word sense inventory of WordNet. 3 Unsupervised Knowledge-based WSD using Hybrid Aligned Resource We rely on the hybrid aligned lexical semantic resource"
W17-1909,W04-0807,0,0.198258,"we use the scores reported in the respective original publications. However, as NASARI was not evaluated on the datasets used in our study, we used the following procedure to obtain NASARI-based sense representations: Each WordNet-based sense representation was extended with all features from the lexical vectors of NASARI.2 Thus, we compare our method to three hybrid systems that induce sense representations on the Evaluation We perform an extrinsic evaluation and show the impact of the hybrid aligned resource on word sense disambiguation performance. While there exist many datasets for WSD (Mihalcea et al., 2004; Pradhan et al., 2007; Manandhar et al., 2010, inter alia), we follow Navigli and Ponzetto (2012) and use the SemEval-2007 Task 16 on the “Evaluation of wide-coverage knowledge resources” (Cuadros and Rigau, 2007). This task is specifically designed for evaluating the impact of lexical resources on WSD performance. The SemEval-2007 Task 16 is, in turn, based on two “lexical sample” datasets, from the Senseval-3 (Mihalcea et al., 2004) and SemEval-2007 Task 17 (Pradhan et al., 2007) evaluation campaigns. The first dataset has coarse- and fine-grained annotations, while the second contains only"
W17-1909,S07-1016,0,0.019055,"rted in the respective original publications. However, as NASARI was not evaluated on the datasets used in our study, we used the following procedure to obtain NASARI-based sense representations: Each WordNet-based sense representation was extended with all features from the lexical vectors of NASARI.2 Thus, we compare our method to three hybrid systems that induce sense representations on the Evaluation We perform an extrinsic evaluation and show the impact of the hybrid aligned resource on word sense disambiguation performance. While there exist many datasets for WSD (Mihalcea et al., 2004; Pradhan et al., 2007; Manandhar et al., 2010, inter alia), we follow Navigli and Ponzetto (2012) and use the SemEval-2007 Task 16 on the “Evaluation of wide-coverage knowledge resources” (Cuadros and Rigau, 2007). This task is specifically designed for evaluating the impact of lexical resources on WSD performance. The SemEval-2007 Task 16 is, in turn, based on two “lexical sample” datasets, from the Senseval-3 (Mihalcea et al., 2004) and SemEval-2007 Task 17 (Pradhan et al., 2007) evaluation campaigns. The first dataset has coarse- and fine-grained annotations, while the second contains only fine-grained sense an"
W17-1909,Q14-1019,0,0.0758595,"s of the lexical resource and relations between them (WordNet ID). Each sense in the PCZ network is subsequently linked to a sense of the knowledgebased network based on their similarity calculated on the basis of lexical representations of senses and their neighbors. The construction of the PCZ involves the following steps (Faralli et al., 2016): Iacobacci et al. (2015) proposed to learn sense embeddings on the basis of the BabelNet lexical ontology (Navigli and Ponzetto, 2012). Their approach is to train the standard skipgram model on a pre-disambiguated corpus using the Babelfy WSD system (Moro et al., 2014). NASARI (Camacho-Collados et al., 2015a) relies on Wikipedia and WordNet to produce vector representations of senses. In this approach, a sense is represented in lexical or sense-based feature spaces. The links between WordNet and Wikipedia are retrieved from BabelNet. MUFFIN (Camacho-Collados et al., 2015b) adapts several ideas from NASARI, extending the method to the multi-lingual case by using BabelNet synsets instead of monolingual WordNet synsets. The approach of Chen et al. (2015) to learning sense embeddings starts from initialization of sense vectors using WordNet glosses. It proceeds"
W17-1909,P15-1173,0,0.0342251,"Missing"
W17-1909,P14-2089,0,0.0231136,"igli, 2009). Traditionally, word senses are defined and represented in lexical resources, such as WordNet (Fellbaum, 1998), while more recently, there is an increased interest in approaches that induce word senses from corpora using graph-based distributional approaches (Dorow and Widdows, 2003; Biemann, 2006; Hope and Keller, 2013), word sense embeddings (Neelakantan et al., 2014; Bartunov et al., 2 Related Work Several prior approaches combined distributional information extracted from text (Turney and Pantel, 2010) from text with information available in lexical resources, such as WordNet. Yu and Dredze (2014) proposed a model to learn word embeddings based on lexical relations of words from WordNet and PPDB (Ganitkevitch et al., 2013). The objective function of their model 72 Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications, pages 72–78, c Valencia, Spain, April 4 2017. 2017 Association for Computational Linguistics All these diverse contributions indicate the benefits of hybrid knowledge sources for learning word and sense representations. combines the objective function of the skip-gram model (Mikolov et al., 2013) with a term that takes into ac"
W17-1909,D14-1113,0,0.0724977,"mapped to a sense inventory, or machine translation systems. 1 Introduction The representation of word senses and the disambiguation of lexical items in context is an ongoing long-established branch of research (Agirre and Edmonds, 2007; Navigli, 2009). Traditionally, word senses are defined and represented in lexical resources, such as WordNet (Fellbaum, 1998), while more recently, there is an increased interest in approaches that induce word senses from corpora using graph-based distributional approaches (Dorow and Widdows, 2003; Biemann, 2006; Hope and Keller, 2013), word sense embeddings (Neelakantan et al., 2014; Bartunov et al., 2 Related Work Several prior approaches combined distributional information extracted from text (Turney and Pantel, 2010) from text with information available in lexical resources, such as WordNet. Yu and Dredze (2014) proposed a model to learn word embeddings based on lexical relations of words from WordNet and PPDB (Ganitkevitch et al., 2013). The objective function of their model 72 Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications, pages 72–78, c Valencia, Spain, April 4 2017. 2017 Association for Computational Linguistic"
W17-1909,P15-1010,0,\N,Missing
W19-3708,W03-1810,0,0.142641,", 2015). The construction of datasets presenting compositionality can be traced back to as early as the 2000s: Baldwin and Villavicencio (2002) proposed chunk-based extraction methods for English verb-prepositional combinations and gave some binary judgments on the subject of considering them as phrasal verbs. In the follow-up paper, Baldwin et al. (2003) used the same framework to retrieve 1,710 Noun-Noun compounds from 1996 Wall Street Journal corpus. The authors use LSA to calculate the similarity between a phrase and its components as one of the early compositionality prediction attempts. McCarthy et al. (2003) evaluated 116 candidates of English phrasal verbs using three annotators’ predictions on a scale from 0 to 10. Venkatapathy and Joshi (2005) used 800 verbobject collocations obtained from British National Corpus to give annotations from 1 to 6 where one stands for total non-compositionality and 6 for complete compositionality. The dataset developed by Reddy et al. (2011) contained 90 English noun compounds and used an average of 30 judgments to give each phrase compositionality scores. This work provided compositionality assessments for both the phrase and its constituents enabling the use of"
W19-3708,C18-1225,0,0.0607053,"Missing"
W19-3708,P08-1028,0,0.0686246,"uilding various distributional semantic models (DSMs), and on methods for combining vector representations of atomic elements like words into representations of bigger fragments: phrases, sentences, texts. A simple but strong baseline for this task suggests averaging word embeddings of a text fragment (sometimes weighted, e.g., according to IDF). Although the result vector representation is rough compared to results could be achieved by more elaborate neural network encoding methods, it was shown that this baseline has high performance in many tasks (Weston et al., 2013; Mikolov et al., 2013; Mitchell and Lapata, 2008; Anke and Schockaert, 2018). 1. We present the first gold-standard dataset for Russian annotated with compositionality information of noun compounds.1 2. We provide an experimental evaluation of models and methods for predicting compositionality of noun compounds. We show that the methods from the previous work trained on the proposed Russian-language resource achieve the performance comparable with results on English corpora. 1 https://github.com/slangtech/ru-comps 56 Proceedings of the 7th Workshop on Balto-Slavic Natural Language Processing, pages 56–62, c Florence, Italy, 2 August 2019. 2"
W19-3708,L16-1262,0,0.0942524,"Missing"
W19-3708,W03-1812,0,0.112128,"riments on noun compositionality were conducted for the English language and to the best of our knowledge, to date, there are no datasets for compositionality detection task for any Slavic language structurally similar to (Reddy et al., 2011) and (Farahmand et al., 2015). The construction of datasets presenting compositionality can be traced back to as early as the 2000s: Baldwin and Villavicencio (2002) proposed chunk-based extraction methods for English verb-prepositional combinations and gave some binary judgments on the subject of considering them as phrasal verbs. In the follow-up paper, Baldwin et al. (2003) used the same framework to retrieve 1,710 Noun-Noun compounds from 1996 Wall Street Journal corpus. The authors use LSA to calculate the similarity between a phrase and its components as one of the early compositionality prediction attempts. McCarthy et al. (2003) evaluated 116 candidates of English phrasal verbs using three annotators’ predictions on a scale from 0 to 10. Venkatapathy and Joshi (2005) used 800 verbobject collocations obtained from British National Corpus to give annotations from 1 to 6 where one stands for total non-compositionality and 6 for complete compositionality. The d"
W19-3708,W02-2001,0,0.0485173,"is dataet includes annotated verbal MWEs for several Slavic languages Jana et al. (2019) explored the use of hyperbolic embeddings for noun compositionality detection comparing it to the Euclidian embeddings. Most of the experiments on noun compositionality were conducted for the English language and to the best of our knowledge, to date, there are no datasets for compositionality detection task for any Slavic language structurally similar to (Reddy et al., 2011) and (Farahmand et al., 2015). The construction of datasets presenting compositionality can be traced back to as early as the 2000s: Baldwin and Villavicencio (2002) proposed chunk-based extraction methods for English verb-prepositional combinations and gave some binary judgments on the subject of considering them as phrasal verbs. In the follow-up paper, Baldwin et al. (2003) used the same framework to retrieve 1,710 Noun-Noun compounds from 1996 Wall Street Journal corpus. The authors use LSA to calculate the similarity between a phrase and its components as one of the early compositionality prediction attempts. McCarthy et al. (2003) evaluated 116 candidates of English phrasal verbs using three annotators’ predictions on a scale from 0 to 10. Venkatapa"
W19-3708,P16-2026,0,0.0178382,"obtained from British National Corpus to give annotations from 1 to 6 where one stands for total non-compositionality and 6 for complete compositionality. The dataset developed by Reddy et al. (2011) contained 90 English noun compounds and used an average of 30 judgments to give each phrase compositionality scores. This work provided compositionality assessments for both the phrase and its constituents enabling the use of various operations with corresponding embeddings of a compound and its distinctive parts in the context of linking human validations with measurements of semantic distance. Ramisch et al. (2016) extended this dataset to 180 phrases presenting two parallel sets for French and Portuguese languages. English NounNoun compounds were mapped with Noun-PrepNoun and Noun-Adj constructions according to the grammar equivalents. Farahmand et al. (2015) presented considerably larger dataset, which has 1,042 Noun-Noun compounds annotated with the help of 4 experts. We also should note some works on compositionality detection datasets for non-English languages. Gurrutxaga and Alegria (2013) studied 1,200 Basque Noun-Verb collocations and resolve classification task into three classes: idiom, colloc"
W19-3708,P16-1187,0,0.0319367,"Missing"
W19-3708,I11-1024,0,0.0233488,"ated Work trary length and syntactical structure. By design, PARSEME is more suited for MWE extraction tasks rather than compositionality evaluation. This dataet includes annotated verbal MWEs for several Slavic languages Jana et al. (2019) explored the use of hyperbolic embeddings for noun compositionality detection comparing it to the Euclidian embeddings. Most of the experiments on noun compositionality were conducted for the English language and to the best of our knowledge, to date, there are no datasets for compositionality detection task for any Slavic language structurally similar to (Reddy et al., 2011) and (Farahmand et al., 2015). The construction of datasets presenting compositionality can be traced back to as early as the 2000s: Baldwin and Villavicencio (2002) proposed chunk-based extraction methods for English verb-prepositional combinations and gave some binary judgments on the subject of considering them as phrasal verbs. In the follow-up paper, Baldwin et al. (2003) used the same framework to retrieve 1,710 Noun-Noun compounds from 1996 Wall Street Journal corpus. The authors use LSA to calculate the similarity between a phrase and its components as one of the early compositionality"
W19-3708,W15-0904,0,0.108878,"d syntactical structure. By design, PARSEME is more suited for MWE extraction tasks rather than compositionality evaluation. This dataet includes annotated verbal MWEs for several Slavic languages Jana et al. (2019) explored the use of hyperbolic embeddings for noun compositionality detection comparing it to the Euclidian embeddings. Most of the experiments on noun compositionality were conducted for the English language and to the best of our knowledge, to date, there are no datasets for compositionality detection task for any Slavic language structurally similar to (Reddy et al., 2011) and (Farahmand et al., 2015). The construction of datasets presenting compositionality can be traced back to as early as the 2000s: Baldwin and Villavicencio (2002) proposed chunk-based extraction methods for English verb-prepositional combinations and gave some binary judgments on the subject of considering them as phrasal verbs. In the follow-up paper, Baldwin et al. (2003) used the same framework to retrieve 1,710 Noun-Noun compounds from 1996 Wall Street Journal corpus. The authors use LSA to calculate the similarity between a phrase and its components as one of the early compositionality prediction attempts. McCarth"
W19-3708,W13-1017,0,0.0204567,"ompound and its distinctive parts in the context of linking human validations with measurements of semantic distance. Ramisch et al. (2016) extended this dataset to 180 phrases presenting two parallel sets for French and Portuguese languages. English NounNoun compounds were mapped with Noun-PrepNoun and Noun-Adj constructions according to the grammar equivalents. Farahmand et al. (2015) presented considerably larger dataset, which has 1,042 Noun-Noun compounds annotated with the help of 4 experts. We also should note some works on compositionality detection datasets for non-English languages. Gurrutxaga and Alegria (2013) studied 1,200 Basque Noun-Verb collocations and resolve classification task into three classes: idiom, collocation, and free combination. Roller et al. (2013) provides 244 German compounds with compositionality scores assigned from 1 to 7 as an average from 30 validations. PARSEME project (Savary et al., 2015) is devoted to the multilingual annotation of multiword expressions (MWE) of arbiAgreement Metric Pearson’s correlation Cronbach’s alpha Value 0.541 0.700 Table 1: Annotation agreement metrics for our dataset. 3 3.1 Noun Compound Dataset Data Collection The compound phrases are collected"
W19-3708,W13-1005,0,0.0162779,"phrases presenting two parallel sets for French and Portuguese languages. English NounNoun compounds were mapped with Noun-PrepNoun and Noun-Adj constructions according to the grammar equivalents. Farahmand et al. (2015) presented considerably larger dataset, which has 1,042 Noun-Noun compounds annotated with the help of 4 experts. We also should note some works on compositionality detection datasets for non-English languages. Gurrutxaga and Alegria (2013) studied 1,200 Basque Noun-Verb collocations and resolve classification task into three classes: idiom, collocation, and free combination. Roller et al. (2013) provides 244 German compounds with compositionality scores assigned from 1 to 7 as an average from 30 validations. PARSEME project (Savary et al., 2015) is devoted to the multilingual annotation of multiword expressions (MWE) of arbiAgreement Metric Pearson’s correlation Cronbach’s alpha Value 0.541 0.700 Table 1: Annotation agreement metrics for our dataset. 3 3.1 Noun Compound Dataset Data Collection The compound phrases are collected from the Russian Universal Dependency (UD) treebanks (Nivre et al., 2016) according to part of speech patterns, such as adjectives (ADJ) + noun (NOUN) or noun"
W19-3708,H05-1113,0,0.0690484,"io (2002) proposed chunk-based extraction methods for English verb-prepositional combinations and gave some binary judgments on the subject of considering them as phrasal verbs. In the follow-up paper, Baldwin et al. (2003) used the same framework to retrieve 1,710 Noun-Noun compounds from 1996 Wall Street Journal corpus. The authors use LSA to calculate the similarity between a phrase and its components as one of the early compositionality prediction attempts. McCarthy et al. (2003) evaluated 116 candidates of English phrasal verbs using three annotators’ predictions on a scale from 0 to 10. Venkatapathy and Joshi (2005) used 800 verbobject collocations obtained from British National Corpus to give annotations from 1 to 6 where one stands for total non-compositionality and 6 for complete compositionality. The dataset developed by Reddy et al. (2011) contained 90 English noun compounds and used an average of 30 judgments to give each phrase compositionality scores. This work provided compositionality assessments for both the phrase and its constituents enabling the use of various operations with corresponding embeddings of a compound and its distinctive parts in the context of linking human validations with me"
W19-3708,D13-1136,0,0.023034,"ne NLP research encompasses many works on building various distributional semantic models (DSMs), and on methods for combining vector representations of atomic elements like words into representations of bigger fragments: phrases, sentences, texts. A simple but strong baseline for this task suggests averaging word embeddings of a text fragment (sometimes weighted, e.g., according to IDF). Although the result vector representation is rough compared to results could be achieved by more elaborate neural network encoding methods, it was shown that this baseline has high performance in many tasks (Weston et al., 2013; Mikolov et al., 2013; Mitchell and Lapata, 2008; Anke and Schockaert, 2018). 1. We present the first gold-standard dataset for Russian annotated with compositionality information of noun compounds.1 2. We provide an experimental evaluation of models and methods for predicting compositionality of noun compounds. We show that the methods from the previous work trained on the proposed Russian-language resource achieve the performance comparable with results on English corpora. 1 https://github.com/slangtech/ru-comps 56 Proceedings of the 7th Workshop on Balto-Slavic Natural Language Processing,"
W19-4516,D17-1070,0,0.0290688,"ll word vectors of a sentence, representing it by kind of a centroid word—a simple method shown to be effective for several tasks (Wieting et al., 2016). Sentence Embeddings Bags of words and average word embeddings lose sequence information, which intuitively should help for (directed) comparison extraction. Sentence embeddings aim to learn representations for spans of text instead of single words by taking sequence information into account. Several methods like FastSent (Hill et al., 2016) or SkipTought (Kiros et al., 2015) have been proposed to create sentence embeddings. We use InferSent (Conneau et al., 2017) that learns sentence embeddings similar to word embeddings. A neural network is trained on the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015) containing 570,000 English sentence pairs (each labelled as entailment, contradiction, or neutral). InferSent combines the embeddings u and v of the two sentences from a sentence pair into one feature vector (containing the concatenation, the element-wise product, and the elementwise difference of u and v), that is then fed into a fully connected layer and a softmax layer. We use the pre-trained embeddings in our experiments.7"
W19-4516,D17-1218,0,0.0347067,"Missing"
W19-4516,D17-1245,0,0.0216943,"domain-specific vocabulary) but also paths between comparison targets in dependency parses. More recently, Gupta et al. (2017) described a system for the biomedical domain that also combines manually collected patterns for lexical matches and dependency parses in order to identify comparison targets and comparison type using the as gradable, non-gradable, superlativetaxonomy of Jindal and Liu (2006). Developing a system for mining comparative sentences (with potential argumentation support for a preference) from the web might utilize specialized jargon like hashtags for argumentative tweets (Dusmanu et al., 2017) but at the same time faces the challenges recognized for general web ˇ argument mining (Snajder, 2017): web text is typically not well formulated, misses argument structures, and contains poorly formulated claims. In contrast to the use of dependency parses for mining comparative sentences in the biomedical domain, such syntactic features are often impossible to derive for noisy web text and were even shown to not really help in identifying argument structures from well-formulated texts like persuasive essays or Wikipedia articles (Aker et al., 2017; Stab and Gurevych, 2014); simpler structur"
W19-4516,W17-5112,0,0.018989,"like hashtags for argumentative tweets (Dusmanu et al., 2017) but at the same time faces the challenges recognized for general web ˇ argument mining (Snajder, 2017): web text is typically not well formulated, misses argument structures, and contains poorly formulated claims. In contrast to the use of dependency parses for mining comparative sentences in the biomedical domain, such syntactic features are often impossible to derive for noisy web text and were even shown to not really help in identifying argument structures from well-formulated texts like persuasive essays or Wikipedia articles (Aker et al., 2017; Stab and Gurevych, 2014); simpler structural features such as punctuation subsumed syntactic features in the above studies. The role of discourse markers in the identification of claims and premises was discussed by Eckle-Kohler et al. (2015), who found such markers to be moderately useful for identifying argumentative sentences. Also Daxenberger et al. (2017) noted that claims share lexical clues across different datasets. They also concluded from their experiments that typical argumentation mining datasets were too small to unleash the power of recent DNN-based classifiers; methods based o"
W19-4516,W07-1018,0,0.0547236,"o! Answers that contain a lot of “How does X compare to Y?”-questions with human answers but the web itself is an even larger source of textual comparisons. Mining and categorizing comparative sentences from the web could support search engines in answering comparative queries (with potential argumentation justifying the preference in the mined sentence itself or in its context) but also has opinion mining (Ganapathibhotla and Liu, 2008) as another important application. Still, previous work on recognizing comparative sentences has mostly been conducted in the biomedical domain. For instance, Fiszman et al. (2007) identify sentences explicitly comparing elements of drug therapy via manually developed comparative and direction patterns informed by a lot of domain knowledge. Later, Park and Blake (2012) trained a highprecision Bayesian Network classifier for toxicol1 2 3 Dataset As there is no large publicly available crossdomain dataset for comparative argument mining, we create one composed of sentences annotated with markers BETTER (the first item is better or “wins”) / WORSE (the first item is worse or “looses”) or NONE (the sentence does not contain a comparison of the target items). The BETTERsente"
W19-4516,D15-1075,0,0.0171193,"ings Bags of words and average word embeddings lose sequence information, which intuitively should help for (directed) comparison extraction. Sentence embeddings aim to learn representations for spans of text instead of single words by taking sequence information into account. Several methods like FastSent (Hill et al., 2016) or SkipTought (Kiros et al., 2015) have been proposed to create sentence embeddings. We use InferSent (Conneau et al., 2017) that learns sentence embeddings similar to word embeddings. A neural network is trained on the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015) containing 570,000 English sentence pairs (each labelled as entailment, contradiction, or neutral). InferSent combines the embeddings u and v of the two sentences from a sentence pair into one feature vector (containing the concatenation, the element-wise product, and the elementwise difference of u and v), that is then fed into a fully connected layer and a softmax layer. We use the pre-trained embeddings in our experiments.7 LexNet (customized) To overcome the LexNet (original) coverage issue, we relaxed the restriction by extending the maximal path length to 16 and ignoring edge directions"
W19-4516,W17-2326,0,0.0719825,"m at providing comparative functionality across domains. Still, such pages and systems usually suffer from coverage issues relying on structured databases as the only source of information ignoring the rich textual content available on the web. 136 Proceedings of the 6th Workshop on Argument Mining, pages 136–145 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics Our main contributions are two-fold: ogy publications that used lexical clues (comparatives and domain-specific vocabulary) but also paths between comparison targets in dependency parses. More recently, Gupta et al. (2017) described a system for the biomedical domain that also combines manually collected patterns for lexical matches and dependency parses in order to identify comparison targets and comparison type using the as gradable, non-gradable, superlativetaxonomy of Jindal and Liu (2006). Developing a system for mining comparative sentences (with potential argumentation support for a preference) from the web might utilize specialized jargon like hashtags for argumentative tweets (Dusmanu et al., 2017) but at the same time faces the challenges recognized for general web ˇ argument mining (Snajder, 2017): w"
W19-4516,D14-1162,0,0.0820342,"0 neurons, batch size of 128, RMSprop with learning rate 0.01 and 150 epochs, and max pooling with a pool size of 2. A Keras embedding layer is used to create word embeddings of length 100 for the string path components. In the original study, paths were restricted to a length of four with the first comparison target having to be reachable from the lowest common head of the two targets by following left edges only, the second one by following right edges. With this LexNet (original) restriction, a path was found for only 1,519 of our 5,759 training sentences. Word Embeddings We rely on GloVe (Pennington et al., 2014) embeddings of size 300 to create a dense, low-dimension vector representation of a sentence.6 We average all word vectors of a sentence, representing it by kind of a centroid word—a simple method shown to be effective for several tasks (Wieting et al., 2016). Sentence Embeddings Bags of words and average word embeddings lose sequence information, which intuitively should help for (directed) comparison extraction. Sentence embeddings aim to learn representations for spans of text instead of single words by taking sequence information into account. Several methods like FastSent (Hill et al., 20"
W19-4516,N16-1162,0,0.0197701,"n et al., 2014) embeddings of size 300 to create a dense, low-dimension vector representation of a sentence.6 We average all word vectors of a sentence, representing it by kind of a centroid word—a simple method shown to be effective for several tasks (Wieting et al., 2016). Sentence Embeddings Bags of words and average word embeddings lose sequence information, which intuitively should help for (directed) comparison extraction. Sentence embeddings aim to learn representations for spans of text instead of single words by taking sequence information into account. Several methods like FastSent (Hill et al., 2016) or SkipTought (Kiros et al., 2015) have been proposed to create sentence embeddings. We use InferSent (Conneau et al., 2017) that learns sentence embeddings similar to word embeddings. A neural network is trained on the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015) containing 570,000 English sentence pairs (each labelled as entailment, contradiction, or neutral). InferSent combines the embeddings u and v of the two sentences from a sentence pair into one feature vector (containing the concatenation, the element-wise product, and the elementwise difference of u and v"
W19-4516,N18-1202,0,0.0493126,"Missing"
W19-4516,W16-5304,0,0.0146014,"ion experiments using several machine learning approaches and representations and analyse the results. We use common performance metrics: precision, recall and F1 per each class and micro-averaged when reporting overall results. Dependency-based Features The HypeNet method to detect hypernym relations between words (Shwartz et al., 2016) combines distributional and dependency path-based methods to create a vector representation for word pairs. The LexNet generalization of HypeNet encodes tries to capture multiple semantic relationships between two words also using dependency path information (Shwartz and Dagan, 2016). Since dependency paths have been one of the major sources for comparison extraction in related work from the biomedical domain (see Section 2), we also include two LexNet-based features in our experiments. 5.1 Using spaCy’s en core web lg spacy.io/models/en#section-en core web lg. 7 github.com/facebookresearch/InferSent Impact of Classification Models To identify the best classification algorithm, we used a fixed baseline set of feature representations: a sparse bag-of-words model with binary weights computed on the whole sentence (see Section 4.3). We used F1 score to measure the models per"
W19-4516,P16-1226,0,0.0387854,"Missing"
W19-4516,L18-1286,1,0.850117,"Missing"
W19-4516,D14-1006,0,0.0218616,"rgumentative tweets (Dusmanu et al., 2017) but at the same time faces the challenges recognized for general web ˇ argument mining (Snajder, 2017): web text is typically not well formulated, misses argument structures, and contains poorly formulated claims. In contrast to the use of dependency parses for mining comparative sentences in the biomedical domain, such syntactic features are often impossible to derive for noisy web text and were even shown to not really help in identifying argument structures from well-formulated texts like persuasive essays or Wikipedia articles (Aker et al., 2017; Stab and Gurevych, 2014); simpler structural features such as punctuation subsumed syntactic features in the above studies. The role of discourse markers in the identification of claims and premises was discussed by Eckle-Kohler et al. (2015), who found such markers to be moderately useful for identifying argumentative sentences. Also Daxenberger et al. (2017) noted that claims share lexical clues across different datasets. They also concluded from their experiments that typical argumentation mining datasets were too small to unleash the power of recent DNN-based classifiers; methods based on feature engineering stil"
W19-4516,W12-4301,0,0.38074,"e sentences from the web could support search engines in answering comparative queries (with potential argumentation justifying the preference in the mined sentence itself or in its context) but also has opinion mining (Ganapathibhotla and Liu, 2008) as another important application. Still, previous work on recognizing comparative sentences has mostly been conducted in the biomedical domain. For instance, Fiszman et al. (2007) identify sentences explicitly comparing elements of drug therapy via manually developed comparative and direction patterns informed by a lot of domain knowledge. Later, Park and Blake (2012) trained a highprecision Bayesian Network classifier for toxicol1 2 3 Dataset As there is no large publicly available crossdomain dataset for comparative argument mining, we create one composed of sentences annotated with markers BETTER (the first item is better or “wins”) / WORSE (the first item is worse or “looses”) or NONE (the sentence does not contain a comparison of the target items). The BETTERsentences represent a pro argument in favor of the first compared item (or a con argument for the seczenodo.org/record/3237552 github.com/uhh-lt/comparative 137 ond item) while the roles are excha"
yimam-etal-2017-entity,P14-5016,1,\N,Missing
