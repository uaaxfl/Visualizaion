2006.eamt-1.31,baroni-bernardini-2004-bootcat,1,0.938019,"corpus query tool, for further exploration. Reference corpora are used to identify the key terms in the specialist domain. 1 Introduction Where should a translator look if they want to find the terminology of a specialist area? Regular dictionaries will not cover it, specialist dictionaries, if they exist, will be hard to find and expensive and are likely to be out of date. The obvious answer is the web. In 2006, this is probably what every working translator and terminologist does as a matter of course. The question, then, is how to do it effectively and efficiently. 1 Baroni and Bernardini ([2004]) responded to the challenge with the BootCaT tools. The basic method is • Select a few “seed terms”. • Send queries with the seed terms to Google. • Collect the pages that the Google hits page points to. This is then a first-pass specialist corpus. The vocabulary in this corpus can be compared with a reference corpus and terms can be automatically extracted. The process can 1 For early accounts of using the web in this way see Varantola ([2000]) and Jones and Ghani ([2000]). For an overview of the use of the web as a source of linguistic data see Kilgarriff and Grefenstette ([2003]). also be"
2006.eamt-1.31,E06-2001,1,0.622881,"ecialist corpus. The vocabulary in this corpus can be compared with a reference corpus and terms can be automatically extracted. The process can 1 For early accounts of using the web in this way see Varantola ([2000]) and Jones and Ghani ([2000]). For an overview of the use of the web as a source of linguistic data see Kilgarriff and Grefenstette ([2003]). also be iterated with the new terms as seeds to give a “purer” specialist corpus. The software is freely available for download and has been widely used, both to produce specialist corpus for technical term extraction (see, e.g., Fantinuoli [2006]) and to produce large general-language corpora (Sharoff [2005], Baroni and Kilgarriff [2006]). However, the software must be downloaded and installed, and this presents a barrier for people without computer systems skills. 2 WebBootCaT In this paper we present a web-service version of the BootCaT tools, WebBootCaT. The user no longer needs to download or install software, as they use a copy of the software which is already installed on our webserver. Our webserver also holds the corpus and loads it into a corpus query tool, the Sketch Engine (Kilgarriff et al [2004]) for further investigation"
2006.eamt-1.31,J03-3001,1,0.723111,"ernardini ([2004]) responded to the challenge with the BootCaT tools. The basic method is • Select a few “seed terms”. • Send queries with the seed terms to Google. • Collect the pages that the Google hits page points to. This is then a first-pass specialist corpus. The vocabulary in this corpus can be compared with a reference corpus and terms can be automatically extracted. The process can 1 For early accounts of using the web in this way see Varantola ([2000]) and Jones and Ghani ([2000]). For an overview of the use of the web as a source of linguistic data see Kilgarriff and Grefenstette ([2003]). also be iterated with the new terms as seeds to give a “purer” specialist corpus. The software is freely available for download and has been widely used, both to produce specialist corpus for technical term extraction (see, e.g., Fantinuoli [2006]) and to produce large general-language corpora (Sharoff [2005], Baroni and Kilgarriff [2006]). However, the software must be downloaded and installed, and this presents a barrier for people without computer systems skills. 2 WebBootCaT In this paper we present a web-service version of the BootCaT tools, WebBootCaT. The user no longer needs to down"
2014.lilt-9.5,P98-1013,0,0.0943933,"margin remains high. Its bounds are given by the accuracy of the structures the parser has learned from. Better structures in the learning sample should lead to better parsing across the whole corpus to be parsed, and it is possible that in the future incorporating DS measures in parsing preferences might lead to better results, perhaps to the point of modeling human garden-path e↵ects. See Manning (2011) for similar considerations with respect to part-of-speech tagging. 5 An alternative approach is to rely on lexical resources that contain rich semantic information about content words (e.g., Baker et al. 1998; Fellbaum 1998; Kipper et al. 2008). We find the corpus route more appealing because it is not a priori Frege in Space / 247 meaning representations should be objects that compose together to form more complex meanings, while accounting for how composition causes more or less sytematic shifts in word meaning, as in the cocomposition, co-predication and coercion examples above. Moreover, the meaning of content words as we can extract them from a corpus should be able to combine with the meaning of grammatical words, formal semantics’ special focus, in ways that account for the importance of st"
2014.lilt-9.5,E12-1004,1,0.180531,"Ps, involving nouns from the same domain and mostly the same or a related quantifying determiner. Note moreover how the neighbours of the count usage of some in some cats are, consistently, other expressions involving counting of distinct individuals. The mass usage with co↵ee, on the other hand, tends to attract other constructions involving quantifying amounts of substances. It should be possible to learn, by regressing on training examples of this sort, that some has a di↵erent meaning when modifying a count or a mass noun, as illustrated in the toy SOME matrix in the Section 3.4 above. In Baroni et al. (2012), we have shown that corpus-harvested distributional vectors for DPs with a quantifying determiner contain enough information for a statistical algorithm to correctly learn and generalize the entailment status of pairs of DPs represented distributionally. For example, if we extract from the corpus distributional vectors for a few thousand entailing (each dog|=some dog) and non-entailing (many cats6|=all cats) pairs, and we feed them as labeled training data to a ma286 / Marco Baroni, Raffaella Bernardi and Roberto Zamparelli chine learning program, the program is then able, given an arbitrary"
2014.lilt-9.5,J10-4006,1,0.852432,"ot an informative quantity. For example, since it is a function of how frequently the word represented by the vector was encountered in the corpus (modulo possible statistical transformations of the input values), it is a measure of the reliability of the distributional evidence encoded in the vector. Finally, note that Euclidean distance and cosine are in a bijective functional relation if Euclidean distance is computed on vectors that have been normalized to length 1. Frege in Space / 253 intuitions about the thematic fit of verb arguments and even spotting the alternation classes of verbs (Baroni and Lenci 2010; Baroni et al. 2010; Landauer and Dumais 1997; Lenci 2011; Lund and Burgess 1996; McDonald and Brew 2004; Pad´o and Lapata 2007; Pad´o et al. 2007, and references there). For example, starting with the classic work of Landauer and Dumais (1997), researchers have shown that cosines in distributional space predict which word, among a set of candidates, is the synonym of a target item (e.g., DSMs pick pinnacle as synonym of zenith over the foils completion, outset and decline). DSM performance on this task approximates that of native English speakers with a college education (Rapp 2004). Pad´ o"
2014.lilt-9.5,D10-1115,1,0.119557,"essions, distributional semantics must find ways to extract from finite evidence an estimate of how their distributional profile would look if we had an infinite corpus available. For words, a large but finite corpus provides a sample of possible contexts of sufficient size to constitute a decent surrogate of infinity; for most phrases and sentences, it does not (given that there is an infinite number of possible phrases and sentences), and we need a di↵erent, compositional strategy to come up with indirect estimates of their distributional profiles. Building on what we originally proposed in Baroni and Zamparelli (2010), we present an approach to compositional distributional semantics that relies on Frege’s (1892) distinction between “complete” and “incomplete” expressions. Specifically, we distinguish between words whose meaning is directly determined by their distributional behaviour, e.g. nouns, and words that act as functions transforming the distributional profile of other words (e.g., verbs). As discussed in Section 2, representations for the former can be directly induced from their patterns of co-occurrence in a corpus. We add to this standard practice a new view on the incomplete expressions and tre"
2014.lilt-9.5,basile-etal-2012-developing,0,0.00721642,"Ps, involving nouns from the same domain and mostly the same or a related quantifying determiner. Note moreover how the neighbours of the count usage of some in some cats are, consistently, other expressions involving counting of distinct individuals. The mass usage with co↵ee, on the other hand, tends to attract other constructions involving quantifying amounts of substances. It should be possible to learn, by regressing on training examples of this sort, that some has a di↵erent meaning when modifying a count or a mass noun, as illustrated in the toy SOME matrix in the Section 3.4 above. In Baroni et al. (2012), we have shown that corpus-harvested distributional vectors for DPs with a quantifying determiner contain enough information for a statistical algorithm to correctly learn and generalize the entailment status of pairs of DPs represented distributionally. For example, if we extract from the corpus distributional vectors for a few thousand entailing (each dog|=some dog) and non-entailing (many cats6|=all cats) pairs, and we feed them as labeled training data to a ma286 / Marco Baroni, Raffaella Bernardi and Roberto Zamparelli chine learning program, the program is then able, given an arbitrary"
2014.lilt-9.5,W11-1304,0,0.00751678,"nstructions with a specific verb. These matrices are estimated from corpus-extracted examples of &lt;subject, subject verb object&gt; vector pairs (picking, of course, subject-verb-object structures that occur with a certain frequency in the corpus, in order to be able to extract meaningful distributional vectors for them). After estimating a suitable number of such matrices for a variety of objects of the same verb (e.g., “eat cake”, “eat meat”, “eat snacks”), we use pairs of corpus-derived object vectors and the has recently been proposed as a benchmarking task for distributional semantic models (Biemann and Giesbrecht 2011). 53 Georgiana Dinu (p.c.) has developed a method to estimate higher-order tensors in just one step: However, the method requires the same training data as the multistep method, that is conceptually simpler. 54 In the conclusion, we will come back to some important issues pertaining to this treatment of verbs, such as how to handle changes in argument structure. 55 Like Grefenstette et al. (2013), we ignore for purposes of all examples discussed in this subsection the inflection of the verb and number of nouns and DPs. 290 / Marco Baroni, Raffaella Bernardi and Roberto Zamparelli STEP 1: ESTIM"
2014.lilt-9.5,D12-1050,0,0.0783256,"successful model of Mitchell and Lapata (2010), namely the dilation model, can be seen as a special way to estimate the weights of the weighted additive model, and we consider it as a special case of the latter here. Frege in Space / 265 Other studies have confirmed that the ML methods, in particular the multiplicative model, are very competitive in various composition tasks that involve simple phrases and do not test for word order or di↵erent syntactic structures (Erk and Pad´o 2008; Grefenstette and Sadrzadeh 2011a; Vecchi et al. 2011; Boleda et al. 2012b). Interestingly and surprisingly, Blacoe and Lapata (2012) recently found that the ML models reach performance close to the one of knowledge-intensive state-of-the-art systems on a full-sentence paraphrasing task. Given the weaknesses of the models we will present below, we can only conjecture that the sentences in this data set fail to test for some crucial syntactic aspects of language (a suspicion that is strengthened by the fact that Blacoe and Lapata obtain excellent results with versions of the additive and multiplicative models that ignore, if we understand correctly, all function words – determiners, negation, etc. – in the test sentences). T"
2014.lilt-9.5,S12-1023,0,0.075745,"nly) content words. On the face of it, standard DSMs do not address the issue of polysemy, since they represent each word with a single distributional vector. In the common case in which a word has more than one facet of meaning (ranging from full-fledged instances of homonymy such as river bank vs. central bank to subtler alternations such as chicken in the farm vs. chicken in the oven or the cases of co-predication and coercion discussed in the introduction), the distributional vector will be a summary of these facets. There has however been a lot of work on handling polysemy in DSMs (e.g., Boleda et al. 2012a; Erk 2010; Pantel and Lin 2002; Sch¨ utze 1998) showing that these models are actually very well-suited to capture various kinds of polysemy on a large scale. Note that polysemy is naturally modeled in 254 / Marco Baroni, Raffaella Bernardi and Roberto Zamparelli terms of the contexts in which a word appears: In a sentence containing words such as farm, free-range and outdoors, the word chicken is more likely to mean the animal than its meat (albeit an animal with a clear culinary destiny). Consequently, a large subset of work on polysemy in DSMs (e.g., Dinu and Lapata 2010; Erk and Pad´o 20"
2014.lilt-9.5,D12-1112,0,0.0645636,"nly) content words. On the face of it, standard DSMs do not address the issue of polysemy, since they represent each word with a single distributional vector. In the common case in which a word has more than one facet of meaning (ranging from full-fledged instances of homonymy such as river bank vs. central bank to subtler alternations such as chicken in the farm vs. chicken in the oven or the cases of co-predication and coercion discussed in the introduction), the distributional vector will be a summary of these facets. There has however been a lot of work on handling polysemy in DSMs (e.g., Boleda et al. 2012a; Erk 2010; Pantel and Lin 2002; Sch¨ utze 1998) showing that these models are actually very well-suited to capture various kinds of polysemy on a large scale. Note that polysemy is naturally modeled in 254 / Marco Baroni, Raffaella Bernardi and Roberto Zamparelli terms of the contexts in which a word appears: In a sentence containing words such as farm, free-range and outdoors, the word chicken is more likely to mean the animal than its meat (albeit an animal with a clear culinary destiny). Consequently, a large subset of work on polysemy in DSMs (e.g., Dinu and Lapata 2010; Erk and Pad´o 20"
2014.lilt-9.5,W11-2503,1,0.744028,"ling that DSMs have been applied with some success to the Voynich Manuscript, a 15th century text written in an unreadable script (Reddy and Knight 2011)—a case of ‘semantic analysis’ on a document of unknown content. We believe that the current limitations of DSMs to linguistic contexts are more practical than theoretical. Indeed, by exploiting recent advances in image analysis, a new generation of DSMs integrates text data with visual features automatically extracted from pictures that cooccur with the target words, to attain a more perceptually grounded view of distributional word meaning (Bruni et al. 2011, 2012; Feng and 258 / Marco Baroni, Raffaella Bernardi and Roberto Zamparelli Lapata 2010; Leong and Mihalcea 2011; Silberer and Lapata 2012). With research continuing in this direction, DSMs might be the first symbolic semantic models (or even more generally the first fully implemented large-scale computational semantic models) to truly address the symbol grounding problem. 2.6 Meaning and reference The symbol grounding challenge raised by philosophers and cognitive scientists pertains to the perceptual underpinnings of our generic knowledge of concepts (you need to have seen a dog to truly"
2014.lilt-9.5,J06-1003,0,0.00467225,"ion when developing a DSM pertains to defining what is a context for purposes of counting cooccurrences. Definitions of context range from simple ones (such as documents or the occurrence of another word inside a fixed window from the target word) to more linguistically sophisticated ones (such as the occurrence of words of certain syntactic categories connected to the target by special syntactic relations) (Curran and Moens 2002a; Grefenstette 1994; Pad´o and Lapata 2007; Sahlgren 2006; Turney and Pantel 2010). Di↵erent contexts capture di↵erent kinds of semantic similarity or “relatedness” (Budanitsky and Hirst 2006). At the two extremes, counting documents as contexts captures “topical” relations (the words war and Afghanistan will have a high cosine, because they often co-occur in documents), whereas DSMs based on word cooccurrence within narrow windows or constrained by syntactic relations tend to capture tighter “taxonomic” relations (such as the one between dog and hyena). Unsurprisingly, no single definition of context is appropriate for all tasks, and the jury on the “best” context model is still out (Sahlgren 2008). Next, raw target-context counts are typically transformed into association scores"
2014.lilt-9.5,J07-4004,0,0.135481,"Natural language parsers, which automatically assign a syntactic structure to sentences, have made great advances in recent years by exploiting probabilistic information about parts of speech (POS tags) and syntactic attachment preferences. This in turn has been made possible by the availability of medium-sized corpora annotated for POS and syntactic information, such as the Penn Treebank (Marcus et al. 1993), that serve as the basis for extracting probabilistic information from. Today’s state-of-the-art parsers can process dozens of unannotated, possibly noisy real-life sentences per second (Clark and Curran 2007; Nivre 2003).4 Learning from pre-annotated data has been less directly applicable to the goal of providing a semantic representation for sentences because there are few learning samples marked for meaning (but see Basile et al. 2012). Moreover, the range, variety and often ‘fuzzy’ nature of semantic phenomena makes the prospect of manual semantic markup of text data a lot less appealing than for syntax. As a consequence, data-driven semantics—which would in principle be a way to address the vastness of lexical meanings—has not advanced as rapidly as datadriven syntax. What sort of data-driven"
2014.lilt-9.5,P07-2009,0,0.179443,"on; assign to the lexical entries syntactic categories that correspond to their semantic types. This procedure allows one to proceed in parallel in the composition of the syntactic and semantic constructions. Besides this theoretical advantage, employing a CG framework has practical benefits, because of the existence of a fast and wide-coverage syntactic parser, namely C&C parser(Clark and Curran 2007), based on (Combinatory) Categorial Grammar(Steedman 2000). This parser is also integrated with Boxer, a system that builds a referential semantic tier using Discourse Representation Structures (Curran et al. 2007), thus allowing us to maintain the same large-scale approach that characterizes lexical DSMs in our compositional component, and providing a concrete infrastructure for the possibility of parallel distributional/referential representations built from the same semantic structures. Of course, other lexicalized formal grammars could also be considered; CG is just the one that might allow the integration in the most 294 / Marco Baroni, Raffaella Bernardi and Roberto Zamparelli straightforward way.58 In denotational semantics the semantic types give the type of the domain of denotation (e.g., the d"
2014.lilt-9.5,W02-0908,0,0.158862,"2.2), and then turn to some theoretical issues pertaining to them (Sections from 2.3 to 2.6). More thorough recent introductions to DSMs are provided by Clark (2013b), Erk (2012) and Turney and Pantel (2010). 2.1 Parameters of DSMs Most research on DSMs focuses on the many parameters of the pipeline to extract distributional vectors from corpora.8 Surprisingly, there is relatively little research on how the nature of the source corpus a↵ects the quality of the resulting vectors, but, as in many other areas of computational linguistics, the general consensus is that “more data is better data” (Curran and Moens 2002b). The most popular data source 8 Bullinaria and Levy (2007, 2012) provide a sysstematic evaluation of how some of the pipeline parameters a↵ect DSM quality. Frege in Space / 251 is the British National Corpus,9 a 100 million word corpus attempting to provide a “balanced” sample of various registers and genres of both written and spoken English. More recently, larger corpora (in the order of a few billion words), often made up of Web documents (including Wikipedia pages), are also widely used.10 Probably the most important decision when developing a DSM pertains to defining what is a context"
2014.lilt-9.5,P02-1030,0,0.289837,"2.2), and then turn to some theoretical issues pertaining to them (Sections from 2.3 to 2.6). More thorough recent introductions to DSMs are provided by Clark (2013b), Erk (2012) and Turney and Pantel (2010). 2.1 Parameters of DSMs Most research on DSMs focuses on the many parameters of the pipeline to extract distributional vectors from corpora.8 Surprisingly, there is relatively little research on how the nature of the source corpus a↵ects the quality of the resulting vectors, but, as in many other areas of computational linguistics, the general consensus is that “more data is better data” (Curran and Moens 2002b). The most popular data source 8 Bullinaria and Levy (2007, 2012) provide a sysstematic evaluation of how some of the pipeline parameters a↵ect DSM quality. Frege in Space / 251 is the British National Corpus,9 a 100 million word corpus attempting to provide a “balanced” sample of various registers and genres of both written and spoken English. More recently, larger corpora (in the order of a few billion words), often made up of Web documents (including Wikipedia pages), are also widely used.10 Probably the most important decision when developing a DSM pertains to defining what is a context"
2014.lilt-9.5,D10-1113,0,0.188269,"nted in a lower-dimensionality space whose components (deriving from the original ones via a statis9 http://www.natcorp.ox.ac.uk/ 10 See for example http://wacky.sslmit.unibo.it/. A potential problem with Web corpora is their systematic skewness, as in the probable overassociation of page with home. This can presumably be addressed with better sampling and filtering techniques (see Fletcher 2004, 2012). 252 / Marco Baroni, Raffaella Bernardi and Roberto Zamparelli tical process that considers their correlation patterns) should capture more robust “latent” aspects of meaning (Blei et al. 2003; Dinu and Lapata 2010; Griffiths et al. 2007; Landauer and Dumais 1997; Sahlgren 2005; Sch¨ utze 1997). Although it is not strictly a parameter in the construction of DSMs, researchers measure distributional (and thus semantic) similarity of pairs of target words with di↵erent similarity functions. The already introduced cosine of the angle formed by vectors is the most natural, geometrically justified and widely used of these functions.11 We conclude this short survey of DSM engineering by observing that, while in our discussion below we assume that the components of a semantic space can be interpreted as a distr"
2014.lilt-9.5,W10-2803,0,0.0126382,"n the face of it, standard DSMs do not address the issue of polysemy, since they represent each word with a single distributional vector. In the common case in which a word has more than one facet of meaning (ranging from full-fledged instances of homonymy such as river bank vs. central bank to subtler alternations such as chicken in the farm vs. chicken in the oven or the cases of co-predication and coercion discussed in the introduction), the distributional vector will be a summary of these facets. There has however been a lot of work on handling polysemy in DSMs (e.g., Boleda et al. 2012a; Erk 2010; Pantel and Lin 2002; Sch¨ utze 1998) showing that these models are actually very well-suited to capture various kinds of polysemy on a large scale. Note that polysemy is naturally modeled in 254 / Marco Baroni, Raffaella Bernardi and Roberto Zamparelli terms of the contexts in which a word appears: In a sentence containing words such as farm, free-range and outdoors, the word chicken is more likely to mean the animal than its meat (albeit an animal with a clear culinary destiny). Consequently, a large subset of work on polysemy in DSMs (e.g., Dinu and Lapata 2010; Erk and Pad´o 2008, 2010; K"
2014.lilt-9.5,D08-1094,0,0.507855,"Missing"
2014.lilt-9.5,P10-2017,0,0.0643032,"Missing"
2014.lilt-9.5,N10-1011,0,0.078307,"Missing"
2014.lilt-9.5,N10-3005,0,0.00967894,"ns that are di↵erent from the classic logic-based ones, but are not distributional in our sense. This line of research includes the corpus-based induction of semantic parsers suitable for question answering (Liang et al. 2011), role labeling (Titov and Klementiev 2012) and modeling semantic and syntactic acquisition (Kwiatkowski et al. 2012). The output of such semantic parsers could be tried as an alternative to the purely syntactic CG input used in our current work. Recently, there also has been much interest in higher-order tensors for distributional semantics (e.g., Baroni and Lenci 2010; Giesbrecht 2010; Turney 2007; Van de Cruys 2010; Widdows 2008). However, even when this line of work tackles the issue of compositionality, it looks at tensors as a way to represent larger structures that result from composition, rather than taking the view we propose here of using tensors to encode composition functions. Our view on the syntax-semantics relations traces back to the traditional type-logical approach introduced by van Benthem in 1986, following which we have exploited the Curry-Howard correspondence between logical rules and lambda-calculus rules to obtain a proof term for a parsed structure."
2014.lilt-9.5,W13-0112,0,0.193813,"efine a composition function generating the distributional vector of some co↵ee from that of co↵ee, it stands to reason that we define a function that approximates the actual distributional vector of some co↵ee. Of course, not many corpus-extracted phrases (and very few sentences) are common enough to find enough occurrences of them in a corpus to extract meaningful distributional vectors (that’s why we want composition in the first place). However, we only need a few, reasonably frequent examples for each composition function to be learned by regression. In the transitive verb experiments of Grefenstette et al. (2013), good results were obtained with as little as 10 training examples per verb. Corpus-extracted phrase vectors as targets of learning Given the centrality of learning from phrase examples for our approach, we have collected various forms of empirical evidence that, at least for adjective-noun constructions (ANs) and DPs, phrase vectors directly extracted from the corpus make good semantic sense. It is thus reasonable to use them as our target of learning. In Baroni and Zamparelli (2010), we have presented qualitative ev284 / Marco Baroni, Raffaella Bernardi and Roberto Zamparelli TABLE 4: The 3"
2014.lilt-9.5,D11-1129,0,0.062427,"hus, the (cosine of the) angle of the composed vector with any other vector will stay the same. 22 The other successful model of Mitchell and Lapata (2010), namely the dilation model, can be seen as a special way to estimate the weights of the weighted additive model, and we consider it as a special case of the latter here. Frege in Space / 265 Other studies have confirmed that the ML methods, in particular the multiplicative model, are very competitive in various composition tasks that involve simple phrases and do not test for word order or di↵erent syntactic structures (Erk and Pad´o 2008; Grefenstette and Sadrzadeh 2011a; Vecchi et al. 2011; Boleda et al. 2012b). Interestingly and surprisingly, Blacoe and Lapata (2012) recently found that the ML models reach performance close to the one of knowledge-intensive state-of-the-art systems on a full-sentence paraphrasing task. Given the weaknesses of the models we will present below, we can only conjecture that the sentences in this data set fail to test for some crucial syntactic aspects of language (a suspicion that is strengthened by the fact that Blacoe and Lapata obtain excellent results with versions of the additive and multiplicative models that ignore, if"
2014.lilt-9.5,W11-2507,0,0.0720733,"hus, the (cosine of the) angle of the composed vector with any other vector will stay the same. 22 The other successful model of Mitchell and Lapata (2010), namely the dilation model, can be seen as a special way to estimate the weights of the weighted additive model, and we consider it as a special case of the latter here. Frege in Space / 265 Other studies have confirmed that the ML methods, in particular the multiplicative model, are very competitive in various composition tasks that involve simple phrases and do not test for word order or di↵erent syntactic structures (Erk and Pad´o 2008; Grefenstette and Sadrzadeh 2011a; Vecchi et al. 2011; Boleda et al. 2012b). Interestingly and surprisingly, Blacoe and Lapata (2012) recently found that the ML models reach performance close to the one of knowledge-intensive state-of-the-art systems on a full-sentence paraphrasing task. Given the weaknesses of the models we will present below, we can only conjecture that the sentences in this data set fail to test for some crucial syntactic aspects of language (a suspicion that is strengthened by the fact that Blacoe and Lapata obtain excellent results with versions of the additive and multiplicative models that ignore, if"
2014.lilt-9.5,W10-2805,0,0.10123,"ity. For example, since it is a function of how frequently the word represented by the vector was encountered in the corpus (modulo possible statistical transformations of the input values), it is a measure of the reliability of the distributional evidence encoded in the vector. Finally, note that Euclidean distance and cosine are in a bijective functional relation if Euclidean distance is computed on vectors that have been normalized to length 1. Frege in Space / 253 intuitions about the thematic fit of verb arguments and even spotting the alternation classes of verbs (Baroni and Lenci 2010; Baroni et al. 2010; Landauer and Dumais 1997; Lenci 2011; Lund and Burgess 1996; McDonald and Brew 2004; Pad´o and Lapata 2007; Pad´o et al. 2007, and references there). For example, starting with the classic work of Landauer and Dumais (1997), researchers have shown that cosines in distributional space predict which word, among a set of candidates, is the synonym of a target item (e.g., DSMs pick pinnacle as synonym of zenith over the foils completion, outset and decline). DSM performance on this task approximates that of native English speakers with a college education (Rapp 2004). Pad´ o and Lapata (2007) an"
2014.lilt-9.5,W12-1702,0,0.0096488,"ns, bats, lionesses), which makes them difficult to handle in straightforward logical quantification terms. Two aspects of generics suggest that distributional semantics might have something to contribute to this task. First, sentences such as “lions have a mane” express a facet of our general knowledge about a concept – in this case, that of a lion (Carlson 2009, draws an explicit connection between generics and conceptual knowledge). Not surprisingly, DSMs derived from large corpora are good at extracting general world knowledge about concepts (Almuhareb and Poesio 2005; Baroni et al. 2010; Kelly et al. 2012), so it is reasonable to expect their compositional extension to capture valid statements about properties of concepts. Second, acceptability judgments about generic statements are not sharp. “Lions live in Africa” is perfect, “lions live in Europe” sounds funny but it is not nearly as bizarre and obviously false as “lions live on the moon”. Again, it is easier to model this sort of gradience in the geometric framework of distributional semantics than in truth-functional terms. In particular, there might be a relation between the way in which we just proposed to handle semantic acceptability a"
2014.lilt-9.5,E12-1024,0,0.0123192,"ork that share our goal and that uses approaches either very close or radically di↵erent from ours. Before going into the details of this work, it is worth mentioning that there is a rich tradition of corpus-based statistical semantics methods producing compositional representations that are di↵erent from the classic logic-based ones, but are not distributional in our sense. This line of research includes the corpus-based induction of semantic parsers suitable for question answering (Liang et al. 2011), role labeling (Titov and Klementiev 2012) and modeling semantic and syntactic acquisition (Kwiatkowski et al. 2012). The output of such semantic parsers could be tried as an alternative to the purely syntactic CG input used in our current work. Recently, there also has been much interest in higher-order tensors for distributional semantics (e.g., Baroni and Lenci 2010; Giesbrecht 2010; Turney 2007; Van de Cruys 2010; Widdows 2008). However, even when this line of work tackles the issue of compositionality, it looks at tensors as a way to represent larger structures that result from composition, rather than taking the view we propose here of using tensors to encode composition functions. Our view on the syn"
2014.lilt-9.5,W11-0607,0,0.0122606,"w frequently the word represented by the vector was encountered in the corpus (modulo possible statistical transformations of the input values), it is a measure of the reliability of the distributional evidence encoded in the vector. Finally, note that Euclidean distance and cosine are in a bijective functional relation if Euclidean distance is computed on vectors that have been normalized to length 1. Frege in Space / 253 intuitions about the thematic fit of verb arguments and even spotting the alternation classes of verbs (Baroni and Lenci 2010; Baroni et al. 2010; Landauer and Dumais 1997; Lenci 2011; Lund and Burgess 1996; McDonald and Brew 2004; Pad´o and Lapata 2007; Pad´o et al. 2007, and references there). For example, starting with the classic work of Landauer and Dumais (1997), researchers have shown that cosines in distributional space predict which word, among a set of candidates, is the synonym of a target item (e.g., DSMs pick pinnacle as synonym of zenith over the foils completion, outset and decline). DSM performance on this task approximates that of native English speakers with a college education (Rapp 2004). Pad´ o and Lapata (2007) and others have shown how the cosines be"
2014.lilt-9.5,I11-1162,0,0.0100776,"an unreadable script (Reddy and Knight 2011)—a case of ‘semantic analysis’ on a document of unknown content. We believe that the current limitations of DSMs to linguistic contexts are more practical than theoretical. Indeed, by exploiting recent advances in image analysis, a new generation of DSMs integrates text data with visual features automatically extracted from pictures that cooccur with the target words, to attain a more perceptually grounded view of distributional word meaning (Bruni et al. 2011, 2012; Feng and 258 / Marco Baroni, Raffaella Bernardi and Roberto Zamparelli Lapata 2010; Leong and Mihalcea 2011; Silberer and Lapata 2012). With research continuing in this direction, DSMs might be the first symbolic semantic models (or even more generally the first fully implemented large-scale computational semantic models) to truly address the symbol grounding problem. 2.6 Meaning and reference The symbol grounding challenge raised by philosophers and cognitive scientists pertains to the perceptual underpinnings of our generic knowledge of concepts (you need to have seen a dog to truly grasp the meaning of the word dog). The dominant tradition in formal semantics stresses instead another type of rel"
2014.lilt-9.5,P11-1060,0,0.00973588,"2008, and Section 5.1 for our view of Garrette et al. 2013 and Turney 2012). Here, we limit our discussion to work that share our goal and that uses approaches either very close or radically di↵erent from ours. Before going into the details of this work, it is worth mentioning that there is a rich tradition of corpus-based statistical semantics methods producing compositional representations that are di↵erent from the classic logic-based ones, but are not distributional in our sense. This line of research includes the corpus-based induction of semantic parsers suitable for question answering (Liang et al. 2011), role labeling (Titov and Klementiev 2012) and modeling semantic and syntactic acquisition (Kwiatkowski et al. 2012). The output of such semantic parsers could be tried as an alternative to the purely syntactic CG input used in our current work. Recently, there also has been much interest in higher-order tensors for distributional semantics (e.g., Baroni and Lenci 2010; Giesbrecht 2010; Turney 2007; Van de Cruys 2010; Widdows 2008). However, even when this line of work tackles the issue of compositionality, it looks at tensors as a way to represent larger structures that result from compositi"
2014.lilt-9.5,J93-2004,0,0.0471575,"he problem of assigning reasonable (if not exhaustive) syntactic structures to arbitrary, real-life sentences is perhaps equally hard. Here, however, technology has provided an important part of the answer: Natural language parsers, which automatically assign a syntactic structure to sentences, have made great advances in recent years by exploiting probabilistic information about parts of speech (POS tags) and syntactic attachment preferences. This in turn has been made possible by the availability of medium-sized corpora annotated for POS and syntactic information, such as the Penn Treebank (Marcus et al. 1993), that serve as the basis for extracting probabilistic information from. Today’s state-of-the-art parsers can process dozens of unannotated, possibly noisy real-life sentences per second (Clark and Curran 2007; Nivre 2003).4 Learning from pre-annotated data has been less directly applicable to the goal of providing a semantic representation for sentences because there are few learning samples marked for meaning (but see Basile et al. 2012). Moreover, the range, variety and often ‘fuzzy’ nature of semantic phenomena makes the prospect of manual semantic markup of text data a lot less appealing"
2014.lilt-9.5,P04-1003,0,0.0342518,"y the vector was encountered in the corpus (modulo possible statistical transformations of the input values), it is a measure of the reliability of the distributional evidence encoded in the vector. Finally, note that Euclidean distance and cosine are in a bijective functional relation if Euclidean distance is computed on vectors that have been normalized to length 1. Frege in Space / 253 intuitions about the thematic fit of verb arguments and even spotting the alternation classes of verbs (Baroni and Lenci 2010; Baroni et al. 2010; Landauer and Dumais 1997; Lenci 2011; Lund and Burgess 1996; McDonald and Brew 2004; Pad´o and Lapata 2007; Pad´o et al. 2007, and references there). For example, starting with the classic work of Landauer and Dumais (1997), researchers have shown that cosines in distributional space predict which word, among a set of candidates, is the synonym of a target item (e.g., DSMs pick pinnacle as synonym of zenith over the foils completion, outset and decline). DSM performance on this task approximates that of native English speakers with a college education (Rapp 2004). Pad´ o and Lapata (2007) and others have shown how the cosines between vectors of word pairs can predict whether"
2014.lilt-9.5,P08-1028,0,0.416123,"Lin 2002; Sch¨ utze 1998) showing that these models are actually very well-suited to capture various kinds of polysemy on a large scale. Note that polysemy is naturally modeled in 254 / Marco Baroni, Raffaella Bernardi and Roberto Zamparelli terms of the contexts in which a word appears: In a sentence containing words such as farm, free-range and outdoors, the word chicken is more likely to mean the animal than its meat (albeit an animal with a clear culinary destiny). Consequently, a large subset of work on polysemy in DSMs (e.g., Dinu and Lapata 2010; Erk and Pad´o 2008, 2010; Kintsch 2001; Mitchell and Lapata 2008; Reisinger and Mooney 2010; Thater et al. 2009) has focused on the goal of modeling word meaning in context. There is a clear connection between distributional models of word meaning in context and distributional models of compositionality, which is the main topic of this article. For example, Mitchell and Lapata (2008) discriminate between the senses of running in water runs vs. horse runs by composing vectors representing the two phrases, whereas Erk and Pad´ o (2008) and others approach the same task in terms of how the runs vector changes due to contextual e↵ects triggered by the presence"
2014.lilt-9.5,D09-1045,0,0.0139036,"aspects of language (a suspicion that is strengthened by the fact that Blacoe and Lapata obtain excellent results with versions of the additive and multiplicative models that ignore, if we understand correctly, all function words – determiners, negation, etc. – in the test sentences). The ML models are also very well-suited (and empirically e↵ective) for tasks that we will not consider here under the rubric of compositionality but which do involve looking at sentences and larger passages, such as measuring textual coherence (Foltz et al. 1998) or predicting the next word that will be uttered (Mitchell and Lapata 2009). Besides their good empirical performance, the ML models are extremely easy to implement, which makes them, undoubtedly, the best current choice for practical applications. Criticism of vector-mixture models There are principled reasons, however, to believe that the ML models can only account for the simple phrases made of content words (nouns, verbs, adjectives) that they have been generally tested on, and that they will not scale up to represent the meanings of sentences, or even sub-sentential constituents with more complex internal structure. One important limitation stems from the fact t"
2014.lilt-9.5,S12-1019,0,0.00951421,"idence that statistical patterns of co-occurrence influence subjects’ intuitions about the meaning of nonce words just as they do in DSMs (McDonald and Ramscar 2001).16 Fourth and last, in neuroscience there is strong support for the view that concepts are represented in the brain as patterns of neural activation over broad areas (Haxby et al. 2001), and vectors are a natural way to encode such patterns (Huth et al. 2012); this suggests intriguing similarities between neural and distributional representations of meaning. Indeed, recent work in brain-computer interaction (Mitchell et al. 2008; Murphy et al. 2012) has shown that corpus-based distributional vectors are good predictors of the patterns of brain activation recorded in subjects thinking of a concept. This suggests, albeit in a very speculative way, that there might be a direct link between the distributional information encoded in DSMs and the way in which concepts are evoked in the brain. Of course, we do not want to suggest that DSMs are models of 14 Bloom and others emphasize the role of interaction and attention in language acquisition. Rather than providing extra cues to meaning, however, these cognitive functions help learners to focu"
2014.lilt-9.5,P10-1142,0,0.0125153,"review of their approach. 260 / Marco Baroni, Raffaella Bernardi and Roberto Zamparelli ambiguous than descriptive ones, and this causes a proliferation of apparent inconsistencies. There are di↵erent strategies to cope with this problem. One is to abandon the attempt to distinguish one John from another and focus on what descriptive content remains to names and deictics, for instance the fact that John will appear in contexts suitable for an anglophone male human being. At the other end of the spectrum, one could preprocess the input corpus with a (cross-document) anaphora resolution system (Ng 2010; Poesio et al. 2010) to try to unify those names and deictics that are likely to be coreferential.18 At least for the time being, we will just treat denotational semantics and DSMs as covering complementary aspects of meaning. To exemplify, suppose we hear the sentence A dog is barking. Our distributionalfeature-based representation of its constituents will provide us with a sketch of typical contexts in which it can be uttered truthfully, which can orient our perceptual system to pick up the relevant cues to determine if a dog is indeed barking right now, so that we can evaluate the referent"
2014.lilt-9.5,W03-3017,0,0.0429029,"s, which automatically assign a syntactic structure to sentences, have made great advances in recent years by exploiting probabilistic information about parts of speech (POS tags) and syntactic attachment preferences. This in turn has been made possible by the availability of medium-sized corpora annotated for POS and syntactic information, such as the Penn Treebank (Marcus et al. 1993), that serve as the basis for extracting probabilistic information from. Today’s state-of-the-art parsers can process dozens of unannotated, possibly noisy real-life sentences per second (Clark and Curran 2007; Nivre 2003).4 Learning from pre-annotated data has been less directly applicable to the goal of providing a semantic representation for sentences because there are few learning samples marked for meaning (but see Basile et al. 2012). Moreover, the range, variety and often ‘fuzzy’ nature of semantic phenomena makes the prospect of manual semantic markup of text data a lot less appealing than for syntax. As a consequence, data-driven semantics—which would in principle be a way to address the vastness of lexical meanings—has not advanced as rapidly as datadriven syntax. What sort of data-driven methods coul"
2014.lilt-9.5,J07-2002,0,0.216393,"Missing"
2014.lilt-9.5,D07-1042,0,0.0350195,"Missing"
2014.lilt-9.5,rapp-2004-freely,0,0.0231441,"roni and Lenci 2010; Baroni et al. 2010; Landauer and Dumais 1997; Lenci 2011; Lund and Burgess 1996; McDonald and Brew 2004; Pad´o and Lapata 2007; Pad´o et al. 2007, and references there). For example, starting with the classic work of Landauer and Dumais (1997), researchers have shown that cosines in distributional space predict which word, among a set of candidates, is the synonym of a target item (e.g., DSMs pick pinnacle as synonym of zenith over the foils completion, outset and decline). DSM performance on this task approximates that of native English speakers with a college education (Rapp 2004). Pad´ o and Lapata (2007) and others have shown how the cosines between vectors of word pairs can predict whether the corresponding words will “prime” each other or not (that is, whether a subject will recognize the second word faster when the first one has just been presented). Kotlerman et al. (2010) and others use DSMs to predict lexical entailment (discovering whether the concept denoted by one word implies the one denoted by another; for example, dog entails animal ). Pad´ o et al. (2007) show that (simplifying somewhat) the cosine between a vector representing the typical subject or obj"
2014.lilt-9.5,W11-1511,0,0.0106477,"e most similar, or accurately predict which sentences could contain the word daisy. Still, since the DSM has never seen a daisy and so it has never experienced its color, its shape, etc., we might be reluctant to admit that the model truly “knows” the meaning of the word daisy (references for the grounding debate in relation to DSMs include Andrews et al. 2009; Burgess 2000; Glenberg and Robertson 2000; Louwerse 2011; Riordan and Jones 2011). It is indeed quite telling that DSMs have been applied with some success to the Voynich Manuscript, a 15th century text written in an unreadable script (Reddy and Knight 2011)—a case of ‘semantic analysis’ on a document of unknown content. We believe that the current limitations of DSMs to linguistic contexts are more practical than theoretical. Indeed, by exploiting recent advances in image analysis, a new generation of DSMs integrates text data with visual features automatically extracted from pictures that cooccur with the target words, to attain a more perceptually grounded view of distributional word meaning (Bruni et al. 2011, 2012; Feng and 258 / Marco Baroni, Raffaella Bernardi and Roberto Zamparelli Lapata 2010; Leong and Mihalcea 2011; Silberer and Lapata"
2014.lilt-9.5,N10-1013,0,0.0867913,"ity. For example, since it is a function of how frequently the word represented by the vector was encountered in the corpus (modulo possible statistical transformations of the input values), it is a measure of the reliability of the distributional evidence encoded in the vector. Finally, note that Euclidean distance and cosine are in a bijective functional relation if Euclidean distance is computed on vectors that have been normalized to length 1. Frege in Space / 253 intuitions about the thematic fit of verb arguments and even spotting the alternation classes of verbs (Baroni and Lenci 2010; Baroni et al. 2010; Landauer and Dumais 1997; Lenci 2011; Lund and Burgess 1996; McDonald and Brew 2004; Pad´o and Lapata 2007; Pad´o et al. 2007, and references there). For example, starting with the classic work of Landauer and Dumais (1997), researchers have shown that cosines in distributional space predict which word, among a set of candidates, is the synonym of a target item (e.g., DSMs pick pinnacle as synonym of zenith over the foils completion, outset and decline). DSM performance on this task approximates that of native English speakers with a college education (Rapp 2004). Pad´ o and Lapata (2007) an"
2014.lilt-9.5,D12-1137,0,0.00913497,"sense and reference distinction (e.g., Dummett 1981) that see the sense of a linguistic expression as the manner in which we determine the referent (this would be the job of its distributional representation), whereas the denotational meaning is the referent itself. Bridging denotational and distributional semantics to account for the semantic interpretation of episodic statements is an exciting research program, but it is probably too early to pursue it. However, 18 An exciting new development that might be useful for these purposes is that of distributional methods to geo-locate documents (Roller et al. 2012): The John Smith referred to in a document from Bakersfield is relatively unlikely to be the same John Smith mentioned in an article from Hyderabad. Frege in Space / 261 there are many other aspects of semantics that can be captured independently of the ability to pick up reference from the current state of the world. We reviewed above the many lexical semantic tasks where DSMs representing single words have been very e↵ective despite their lack of direct real-world referencing capabilities (spotting synonyms and other semantic relations, measuring verb-argument plausibility, etc.), and we wil"
2014.lilt-9.5,W03-0902,0,0.014883,"ill-understood topic. DSMs, on the other hand, are extracted from large corpora where proper names, common nouns and other predicates refer to states of the world and events spanning a large chunk of time and space, reflecting di↵erent points of view, etc. So, if they are able to extract any factual information at all, this is very likely to take the form of generic knowledge. Indeed, a typical application of corpus-based semantics is the extraction of commonsense-knowledge “factoids” that are generally useful while not universally true: bananas are yellow, birds fly, etc. (e.g., Eslick 2006; Schubert and Tong 2003). Statistical notions are simFrege in Space / 259 ply not native to the formal semantics setup, but they are at the heart of the DSM approach. We will return to the issue of generic knowledge in Section 5.2 below. Two things should be noted here. First, it is perfectly possible to give a reference-based interpretation of DSMs, the question is what it tells us.17 Suppose we characterize the meaning of a word w in the corpus C in terms of its sentential context, expressed via a vector of binary values: for every word g, vectorw (g) = 1 if g appears along with w in some sentence in C, 0 otherwise"
2014.lilt-9.5,J98-1004,0,0.746674,"Missing"
2014.lilt-9.5,D12-1130,0,0.00874156,"dy and Knight 2011)—a case of ‘semantic analysis’ on a document of unknown content. We believe that the current limitations of DSMs to linguistic contexts are more practical than theoretical. Indeed, by exploiting recent advances in image analysis, a new generation of DSMs integrates text data with visual features automatically extracted from pictures that cooccur with the target words, to attain a more perceptually grounded view of distributional word meaning (Bruni et al. 2011, 2012; Feng and 258 / Marco Baroni, Raffaella Bernardi and Roberto Zamparelli Lapata 2010; Leong and Mihalcea 2011; Silberer and Lapata 2012). With research continuing in this direction, DSMs might be the first symbolic semantic models (or even more generally the first fully implemented large-scale computational semantic models) to truly address the symbol grounding problem. 2.6 Meaning and reference The symbol grounding challenge raised by philosophers and cognitive scientists pertains to the perceptual underpinnings of our generic knowledge of concepts (you need to have seen a dog to truly grasp the meaning of the word dog). The dominant tradition in formal semantics stresses instead another type of relation between linguistic si"
2014.lilt-9.5,D12-1110,0,0.0932519,"Missing"
2014.lilt-9.5,W09-2506,0,0.0131424,"re actually very well-suited to capture various kinds of polysemy on a large scale. Note that polysemy is naturally modeled in 254 / Marco Baroni, Raffaella Bernardi and Roberto Zamparelli terms of the contexts in which a word appears: In a sentence containing words such as farm, free-range and outdoors, the word chicken is more likely to mean the animal than its meat (albeit an animal with a clear culinary destiny). Consequently, a large subset of work on polysemy in DSMs (e.g., Dinu and Lapata 2010; Erk and Pad´o 2008, 2010; Kintsch 2001; Mitchell and Lapata 2008; Reisinger and Mooney 2010; Thater et al. 2009) has focused on the goal of modeling word meaning in context. There is a clear connection between distributional models of word meaning in context and distributional models of compositionality, which is the main topic of this article. For example, Mitchell and Lapata (2008) discriminate between the senses of running in water runs vs. horse runs by composing vectors representing the two phrases, whereas Erk and Pad´ o (2008) and others approach the same task in terms of how the runs vector changes due to contextual e↵ects triggered by the presence of water vs. horse. Mitchell and Lapata constru"
2014.lilt-9.5,E12-1003,0,0.0091094,"of Garrette et al. 2013 and Turney 2012). Here, we limit our discussion to work that share our goal and that uses approaches either very close or radically di↵erent from ours. Before going into the details of this work, it is worth mentioning that there is a rich tradition of corpus-based statistical semantics methods producing compositional representations that are di↵erent from the classic logic-based ones, but are not distributional in our sense. This line of research includes the corpus-based induction of semantic parsers suitable for question answering (Liang et al. 2011), role labeling (Titov and Klementiev 2012) and modeling semantic and syntactic acquisition (Kwiatkowski et al. 2012). The output of such semantic parsers could be tried as an alternative to the purely syntactic CG input used in our current work. Recently, there also has been much interest in higher-order tensors for distributional semantics (e.g., Baroni and Lenci 2010; Giesbrecht 2010; Turney 2007; Van de Cruys 2010; Widdows 2008). However, even when this line of work tackles the issue of compositionality, it looks at tensors as a way to represent larger structures that result from composition, rather than taking the view we propose"
2014.lilt-9.5,W11-1301,1,0.455046,"the composed vector with any other vector will stay the same. 22 The other successful model of Mitchell and Lapata (2010), namely the dilation model, can be seen as a special way to estimate the weights of the weighted additive model, and we consider it as a special case of the latter here. Frege in Space / 265 Other studies have confirmed that the ML methods, in particular the multiplicative model, are very competitive in various composition tasks that involve simple phrases and do not test for word order or di↵erent syntactic structures (Erk and Pad´o 2008; Grefenstette and Sadrzadeh 2011a; Vecchi et al. 2011; Boleda et al. 2012b). Interestingly and surprisingly, Blacoe and Lapata (2012) recently found that the ML models reach performance close to the one of knowledge-intensive state-of-the-art systems on a full-sentence paraphrasing task. Given the weaknesses of the models we will present below, we can only conjecture that the sentences in this data set fail to test for some crucial syntactic aspects of language (a suspicion that is strengthened by the fact that Blacoe and Lapata obtain excellent results with versions of the additive and multiplicative models that ignore, if we understand correct"
2014.lilt-9.5,C10-1142,0,0.0298247,"cke et al. formalism with ours by adopting the regression-based learning method we explained in this article (the empirical results of Grefenstette et al. 2013, are reviewed in Section 4.2 above). We share the idea of learning composition functions by regression on corpus-extracted examples of their inputs and outputs with Guevara (2010), who, however, treats all linguistic expressions as vectors without distinguishing them into atomic and functional types. The importance of exploiting input and output training data for building compositional distributional semantic models is also stressed by Zanzotto et al. (2010), who present a model similar to the one of Guevara, but cleverly exploit dictionary definitions to extract both positive and negative training examples. Another model that is closely related to ours is that of Socher et al. (2012), who also implement function application in terms of operations on matrices and vectors. However, di↵erently from us, they treat each word equally, as both a vector and a matrix. Distributional composition is doubled – each word matrix is composed with the lexical vector of the other word in a phrase – and the result is still a pair of a vector and a matrix. Since S"
2014.lilt-9.5,J12-1002,0,\N,Missing
2014.lilt-9.5,C98-1013,0,\N,Missing
2020.acl-main.407,D19-3010,1,\N,Missing
2020.acl-main.407,2020.blackboxnlp-1.2,1,\N,Missing
2020.blackboxnlp-1.2,D19-3010,1,0.831825,"iency in downstream tasks, as had been previously argued. This resonates with our results below. 3 Communication Game We base our experimental study on a one-episode one-direction communication game, as commonly done in the relevant literature (Lazaridou et al., 2016, 2018; Havrylov and Titov, 2017; Chaabouni et al., 2019). In this setup, we have two agents, Sender and Receiver. An input i is fed into Sender, in turn Sender produces a message m, which is conˆ sumed by Receiver. Receiver produces its output o. Comparing the output oˆ with the ground-truth output o provides a loss. We used EGG (Kharitonov et al., 2019) to implement the experiments.3 In contrast to the language emergence scenario, we use a hard-coded Sender agent that produces a fixed, pre-defined language. This allows us to easily control the (na¨ıve) compositionality of the language and measure how it affects Receiver’s performance. This setup is akin to the motivating example of Li and Bowling (2019). We study two Receiver’s characteristics: (i) acquisition speed, measured as the number of epoch needed to achieve a fixed level of performance on training set, and (ii) generalization performance on held-out data. 4 Experimental setup To dem"
2020.blackboxnlp-1.2,D17-1321,0,0.539755,"r better, generalization performance and acquisition speed than compositional ones. Further research in the area should be clearer about what benefits are expected from compositionality, and how the latter would lead to them. 1 Introduction There is a recent spike of interest in studying the languages that emerge when artificial neural agents communicate to solve a common task (Foerster et al., 2016; Lazaridou et al., 2016; Havrylov and Titov, 2017). A good portion of such studies looks for traces of compositional structure in those languages, or even tries to inject such structure into them (Kottur et al., 2017; Choi et al., 2018; Lazaridou et al., 2018; Mordatch and Abbeel, 2018; Andreas, 2019; Cogswell et al., 2019; Li and Bowling, 2019; Resnick et al., 2019; Chaabouni et al., 2020). Besides possibly providing insights on how compositionality emerged in natural language (Townsend et al., 2018), this emphasis is justified by the idea that a compositional language has various desirable properties. In particular, compositional languages are expected to help agents to better generalize to new (composite) inputs (Kottur et al., 2017; Lazaridou et al., 2018), and to be faster to acquire (Cogswell et al."
2020.blackboxnlp-1.2,2020.acl-main.407,1,0.93579,"ositionality, and how the latter would lead to them. 1 Introduction There is a recent spike of interest in studying the languages that emerge when artificial neural agents communicate to solve a common task (Foerster et al., 2016; Lazaridou et al., 2016; Havrylov and Titov, 2017). A good portion of such studies looks for traces of compositional structure in those languages, or even tries to inject such structure into them (Kottur et al., 2017; Choi et al., 2018; Lazaridou et al., 2018; Mordatch and Abbeel, 2018; Andreas, 2019; Cogswell et al., 2019; Li and Bowling, 2019; Resnick et al., 2019; Chaabouni et al., 2020). Besides possibly providing insights on how compositionality emerged in natural language (Townsend et al., 2018), this emphasis is justified by the idea that a compositional language has various desirable properties. In particular, compositional languages are expected to help agents to better generalize to new (composite) inputs (Kottur et al., 2017; Lazaridou et al., 2018), and to be faster to acquire (Cogswell et al., 2019; Li and Bowling, 2019; Ren et al., 2019). 2 Operationalizing compositionality Before we proceed, let us clarify our definition of compositionality. Linguists and philosop"
2021.blackboxnlp-1.37,2020.acl-main.385,0,0.0207045,"ational Linguistics Figure 1: Schematic description of our tasks. Different tasks require different interpretations of the query. The answer is positive for all these instances. 2 Related Work The current study belongs in the newly developed area of interpretability and explainability of computational linguistics, which seeks to understand how models capture natural language phenomena (Alishahi et al., 2019; Belinkov and Glass, 2019; Rogers et al., 2021). Much of this literature has focused on model behaviour in complex NLP tasks (Linzen et al., 2016; Conneau et al., 2018; Voita et al., 2019; Abnar and Zuidema, 2020; Sinha et al., 2021; Lakretz et al., 2021). Some other work has used controlled tasks to analyze neural networks regarding specific aspects, such as reasoning skills (Weston et al., 2015), compositionality (Lake and Baroni, 2018; Hupkes et al., 2020), PoS tagging (Hewitt and Liang, 2019), inductive biases (White and Cotterell, 2021) or hierarchical structure (Hupkes et al., 2018; Chrupała and Alishahi, 2019). We follow this latter methodology, and design tasks that highlight one of the core abilities that a model needs to possess in order to process natural language, namely detecting one or m"
2021.blackboxnlp-1.37,Q19-1004,0,0.0253412,"/DiscreteSeq 468 Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 468–478 Online, November 11, 2021. ©2021 Association for Computational Linguistics Figure 1: Schematic description of our tasks. Different tasks require different interpretations of the query. The answer is positive for all these instances. 2 Related Work The current study belongs in the newly developed area of interpretability and explainability of computational linguistics, which seeks to understand how models capture natural language phenomena (Alishahi et al., 2019; Belinkov and Glass, 2019; Rogers et al., 2021). Much of this literature has focused on model behaviour in complex NLP tasks (Linzen et al., 2016; Conneau et al., 2018; Voita et al., 2019; Abnar and Zuidema, 2020; Sinha et al., 2021; Lakretz et al., 2021). Some other work has used controlled tasks to analyze neural networks regarding specific aspects, such as reasoning skills (Weston et al., 2015), compositionality (Lake and Baroni, 2018; Hupkes et al., 2020), PoS tagging (Hewitt and Liang, 2019), inductive biases (White and Cotterell, 2021) or hierarchical structure (Hupkes et al., 2018; Chrupała and Alishahi, 2019)."
2021.blackboxnlp-1.37,P19-1283,0,0.0160308,"019; Belinkov and Glass, 2019; Rogers et al., 2021). Much of this literature has focused on model behaviour in complex NLP tasks (Linzen et al., 2016; Conneau et al., 2018; Voita et al., 2019; Abnar and Zuidema, 2020; Sinha et al., 2021; Lakretz et al., 2021). Some other work has used controlled tasks to analyze neural networks regarding specific aspects, such as reasoning skills (Weston et al., 2015), compositionality (Lake and Baroni, 2018; Hupkes et al., 2020), PoS tagging (Hewitt and Liang, 2019), inductive biases (White and Cotterell, 2021) or hierarchical structure (Hupkes et al., 2018; Chrupała and Alishahi, 2019). We follow this latter methodology, and design tasks that highlight one of the core abilities that a model needs to possess in order to process natural language, namely detecting one or more features in a sequence of tokens, possibly in a context- and/or order-sensitive way. 3 3.1 Description of the tasks Task description it has 2 activated features. The tasks are meant to be incremental, in the sense that, when processing the input sequence, the model does not know what information it will need to retrieve from it. Some uses of Transformers in the NLP literature instead give access to all th"
2021.blackboxnlp-1.37,P18-1198,1,0.825581,"ber 11, 2021. ©2021 Association for Computational Linguistics Figure 1: Schematic description of our tasks. Different tasks require different interpretations of the query. The answer is positive for all these instances. 2 Related Work The current study belongs in the newly developed area of interpretability and explainability of computational linguistics, which seeks to understand how models capture natural language phenomena (Alishahi et al., 2019; Belinkov and Glass, 2019; Rogers et al., 2021). Much of this literature has focused on model behaviour in complex NLP tasks (Linzen et al., 2016; Conneau et al., 2018; Voita et al., 2019; Abnar and Zuidema, 2020; Sinha et al., 2021; Lakretz et al., 2021). Some other work has used controlled tasks to analyze neural networks regarding specific aspects, such as reasoning skills (Weston et al., 2015), compositionality (Lake and Baroni, 2018; Hupkes et al., 2020), PoS tagging (Hewitt and Liang, 2019), inductive biases (White and Cotterell, 2021) or hierarchical structure (Hupkes et al., 2018; Chrupała and Alishahi, 2019). We follow this latter methodology, and design tasks that highlight one of the core abilities that a model needs to possess in order to proces"
2021.blackboxnlp-1.37,P19-1285,0,0.063215,"Missing"
2021.blackboxnlp-1.37,N19-1423,0,0.0335728,"tasks that highlight one of the core abilities that a model needs to possess in order to process natural language, namely detecting one or more features in a sequence of tokens, possibly in a context- and/or order-sensitive way. 3 3.1 Description of the tasks Task description it has 2 activated features. The tasks are meant to be incremental, in the sense that, when processing the input sequence, the model does not know what information it will need to retrieve from it. Some uses of Transformers in the NLP literature instead give access to all the information from the start, such as in BERT (Devlin et al., 2019) and its variants. Arguably, incrementality is necessary in most instances of language understanding “in the wild”: For some applications, like Machine Translation for documents, it is realistic to give access to all the input at once, but as we move towards real-time applications such as virtual assistants, models will need to act incrementally. T1: one-feature detection. T1 asks whether the active feature of the query is present in some token in the sequence. A linguistic example, relevant for syntactic processing, would be: did the plural feature occur in a span of tokens? In the example in"
2021.blackboxnlp-1.37,D19-1275,0,0.0730818,"r into account, with deelling made it harder to understand which model coder attention performing most of the work in components are relevant and how they work. This other cases. Positional encoding plays a counterinhas motivated the NLP community to develop new tuitive role, helping in tasks that do not involve ormethods for model analysis. der by providing beneficial noise, and harming perGiven the difficulties analyzing large models, formance when it is redundant with self-attention. some of this work (Hupkes et al., 2018; Lake and The LSTM only compares favorably to the TransBaroni, 2018; Hewitt and Liang, 2019; Chrupała former in short sequence lengths for simple tasks, and Alishahi, 2019; White and Cotterell, 2021) and decoder attention is highly beneficial for this has focused on controlled tasks that emulate spe- model. cific aspects of language. We propose a new set of 1 such controlled tasks to explore a crucial aspect of The associated dataset and code can be downloaded at: natural language processing that has not received https://github.com/sorodoc/DiscreteSeq 468 Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 468–478 Online, Novem"
2021.blackboxnlp-1.37,Q16-1037,0,0.0345191,"468–478 Online, November 11, 2021. ©2021 Association for Computational Linguistics Figure 1: Schematic description of our tasks. Different tasks require different interpretations of the query. The answer is positive for all these instances. 2 Related Work The current study belongs in the newly developed area of interpretability and explainability of computational linguistics, which seeks to understand how models capture natural language phenomena (Alishahi et al., 2019; Belinkov and Glass, 2019; Rogers et al., 2021). Much of this literature has focused on model behaviour in complex NLP tasks (Linzen et al., 2016; Conneau et al., 2018; Voita et al., 2019; Abnar and Zuidema, 2020; Sinha et al., 2021; Lakretz et al., 2021). Some other work has used controlled tasks to analyze neural networks regarding specific aspects, such as reasoning skills (Weston et al., 2015), compositionality (Lake and Baroni, 2018; Hupkes et al., 2020), PoS tagging (Hewitt and Liang, 2019), inductive biases (White and Cotterell, 2021) or hierarchical structure (Hupkes et al., 2018; Chrupała and Alishahi, 2019). We follow this latter methodology, and design tasks that highlight one of the core abilities that a model needs to poss"
2021.blackboxnlp-1.37,2021.emnlp-main.230,0,0.0931003,"Missing"
2021.blackboxnlp-1.37,E17-1001,0,0.070068,"Missing"
2021.blackboxnlp-1.37,P19-1580,0,0.0230843,"sociation for Computational Linguistics Figure 1: Schematic description of our tasks. Different tasks require different interpretations of the query. The answer is positive for all these instances. 2 Related Work The current study belongs in the newly developed area of interpretability and explainability of computational linguistics, which seeks to understand how models capture natural language phenomena (Alishahi et al., 2019; Belinkov and Glass, 2019; Rogers et al., 2021). Much of this literature has focused on model behaviour in complex NLP tasks (Linzen et al., 2016; Conneau et al., 2018; Voita et al., 2019; Abnar and Zuidema, 2020; Sinha et al., 2021; Lakretz et al., 2021). Some other work has used controlled tasks to analyze neural networks regarding specific aspects, such as reasoning skills (Weston et al., 2015), compositionality (Lake and Baroni, 2018; Hupkes et al., 2020), PoS tagging (Hewitt and Liang, 2019), inductive biases (White and Cotterell, 2021) or hierarchical structure (Hupkes et al., 2018; Chrupała and Alishahi, 2019). We follow this latter methodology, and design tasks that highlight one of the core abilities that a model needs to possess in order to process natural language,"
2021.blackboxnlp-1.37,2021.acl-long.38,0,0.180181,"f the work in components are relevant and how they work. This other cases. Positional encoding plays a counterinhas motivated the NLP community to develop new tuitive role, helping in tasks that do not involve ormethods for model analysis. der by providing beneficial noise, and harming perGiven the difficulties analyzing large models, formance when it is redundant with self-attention. some of this work (Hupkes et al., 2018; Lake and The LSTM only compares favorably to the TransBaroni, 2018; Hewitt and Liang, 2019; Chrupała former in short sequence lengths for simple tasks, and Alishahi, 2019; White and Cotterell, 2021) and decoder attention is highly beneficial for this has focused on controlled tasks that emulate spe- model. cific aspects of language. We propose a new set of 1 such controlled tasks to explore a crucial aspect of The associated dataset and code can be downloaded at: natural language processing that has not received https://github.com/sorodoc/DiscreteSeq 468 Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 468–478 Online, November 11, 2021. ©2021 Association for Computational Linguistics Figure 1: Schematic description of our tasks."
baroni-bernardini-2004-bootcat,W00-0901,0,\N,Missing
baroni-bernardini-2004-bootcat,J03-3001,0,\N,Missing
baroni-bisi-2004-using,N03-1032,0,\N,Missing
baroni-bisi-2004-using,J92-4003,0,\N,Missing
baroni-bisi-2004-using,P89-1010,0,\N,Missing
baroni-bisi-2004-using,baroni-bernardini-2004-bootcat,1,\N,Missing
baroni-etal-2004-introducing,J01-2002,0,\N,Missing
baroni-etal-2004-introducing,W02-1011,0,\N,Missing
baroni-etal-2008-cleaneval,W06-1421,1,\N,Missing
baroni-etal-2008-cleaneval,J03-3001,1,\N,Missing
C02-1096,W97-0506,0,0.0195373,"ompound prediction model, focusing on the new class-based head prediction component. In section 5, we report the results of simulations run with the enhanced compound prediction model. In section 6, we report about our preliminary experiments with the integration of compound and simple word prediction. Finally, in section 7, we summarize the main results we obtained and indicate directions for further work. 2 Word prediction for AAC Word prediction systems based on n-gram statistics are an important component of AAC devices, i.e., software and possibly hardware typing aids for disabled users (Copestake, 1997; Carlberger, 1998). Word predictors provide the user with a prediction window, i.e. a menu that, at any time, lists the most likely next word candidates, given the input that the user has typed until the current character. If the word that the user intends to type next is in the prediction window, the user can select it from there. Otherwise, the user will keep typing letters, until the target word appears in the prediction window (or until she finishes typing the word). The (percentage) keystroke savings rate (ksr) is a standard measure used in AAC research to evaluate word predictors. The k"
C02-1096,P01-1025,0,0.013408,"emantic classes is certainly not a trivial job, and, if a large input lexicon must be partitioned, it is not a task that could be accomplished by a human expert. Drawing inspiration from Brown et al. (1990), we constructed instead semantic classes using a clustering algorithm extracting them from a corpus, on the basis of the average mutual information (MI) between pairs of words (Rosenfeld, 1996).5 4 Even if the model handled other compound types, very few POS combinations are attested within compounds. 5 We are aware of the fact that other measures of lexical association have been proposed (Evert and Krenn, 2001, and MI values were computed using Adam Berger’s trigger toolkit (Berger, 1997).6 The same training corpus of about 25.5M words (and with N+N compounds split) that we describe below was used to collect MI values for noun pairs. All modifiers and heads of N+N compounds and all corpus words that were parsed as nouns by the Xerox morphological analyzer (Karttunen et al., 1997) were counted as nouns for this purpose. MI was computed only for pairs that co-occurred at least three times in the corpus (thus, only a subset of the input nouns appears in the output list). Valid co-occurrences were boun"
C02-1096,J92-4003,0,\N,Missing
D09-1065,W02-0908,0,0.0339623,"r via preposition-mediated paths (e.g., tagliare con forbici “to cut with scissors”), and where the paths were among the top 30 most frequent in the corpus. In the repubblica-depfilter model, we record co-occurrence with verbs that are linked to the nouns by one of the top 30 paths, but we do not preserve the paths themselves in the features. This is analogous to the model proposed by Pad´o and Lapata (2007). In the repubblica-deppath model, we preserve the paths as part of the features (so that subj-uccidere “subj-kill” and objuccidere count as different features), analogously to Lin (1998), Curran and Moens (2002) and others. For all models, following standard practice in computational linguistics (Evert, 2005), we transform raw co-occurrence counts into log-likelihood ratios. repubblica-window abbattere “demolish” afferrare “seize” impugnare “grasp” tagliare “cut” trovare “find” repubblica-depfilter abbattere “demolish” correre “run” parlare “speak” saltare “jump” tagliare “cut” repubblica-position X-ferire “X-wound” X-usare “X-use” dipingere-X “paint-X” munire-X “supply-X” tagliare-X “cut-X” repubblica-deppath con+tagliare “with+cut” obj+abbattere “obj+demolish” obj+uccidere “obj+kill” intr-subj+vive"
D09-1065,P98-2127,0,0.0179111,"nd object) or via preposition-mediated paths (e.g., tagliare con forbici “to cut with scissors”), and where the paths were among the top 30 most frequent in the corpus. In the repubblica-depfilter model, we record co-occurrence with verbs that are linked to the nouns by one of the top 30 paths, but we do not preserve the paths themselves in the features. This is analogous to the model proposed by Pad´o and Lapata (2007). In the repubblica-deppath model, we preserve the paths as part of the features (so that subj-uccidere “subj-kill” and objuccidere count as different features), analogously to Lin (1998), Curran and Moens (2002) and others. For all models, following standard practice in computational linguistics (Evert, 2005), we transform raw co-occurrence counts into log-likelihood ratios. repubblica-window abbattere “demolish” afferrare “seize” impugnare “grasp” tagliare “cut” trovare “find” repubblica-depfilter abbattere “demolish” correre “run” parlare “speak” saltare “jump” tagliare “cut” repubblica-position X-ferire “X-wound” X-usare “X-use” dipingere-X “paint-X” munire-X “supply-X” tagliare-X “cut-X” repubblica-deppath con+tagliare “with+cut” obj+abbattere “obj+demolish” obj+uccidere"
D09-1065,J07-2002,0,0.0268598,"Missing"
D09-1065,sahlgren-2006-towards,0,\N,Missing
D09-1065,C98-2122,0,\N,Missing
D10-1115,D08-1094,0,0.866238,"Missing"
D10-1115,W09-0208,0,0.0114108,"Missing"
D10-1115,W10-2805,0,0.377805,"(along similar lines, Kintsch (2001) also focused on composite input vectors, within an additive framework). Again, the multiplicative model works best in Erk and Pad´o’s experiments. The above-mentioned researchers do not exploit corpus evidence about the p vectors that result from composition, despite the fact that it is straightforward (at least for short constructions) to extract direct distributional evidence about the composite items from the corpus (just collect co-occurrence information for the composite item from windows around the contexts in which it occurs). The main innovation of Guevara (2010), who focuses on adjective-noun combinations (AN), is to use the cooccurrence vectors of observed ANs to train a supervised composition model (we became aware of Guevara’s approach after we had developed our own model, that also exploits observed ANs for training). Guevara adopts the full additive composition form from Equation (1) and he estimates the A and B weights using partial least squares regression. The training data are pairs of adjective-noun vector concatenations, as input, and corpus-derived AN vectors, as output. Guevara compares his model to the simplified additive and multiplica"
D10-1115,P08-1028,0,0.786238,"ure on compositionality in vector-based semantics encompasses various related topics, some of them not of direct interest here, such as how to 1184 encode word order information in context vectors (Jones and Mewhort, 2007; Sahlgren et al., 2008) or sophisticated composition methods based on tensor products, quantum logic, etc., that have not yet been empirically tested on large-scale corpus-based semantic space tasks (Clark and Pulman, 2007; Rudolph and Giesbrecht, 2010; Smolensky, 1990; Widdows, 2008). Closer to our current purposes is the general framework for vector composition proposed by Mitchell and Lapata (2008), subsuming various earlier proposals. Given two vectors u and v, they identify two general classes of composition models, (linear) additive models: p = Au + Bv (1) where A and B are weight matrices, and multiplicative models: p = Cuv where C is a weight tensor projecting the uv tensor product onto the space of p. Mitchell and Lapata derive two simplified models from these general forms. Their simplified additive model p = αu + βv was a common approach to composition in the earlier literature, typically with the scalar weights set to 1 or to normalizing constants (Foltz et al., 1998; Kintsch,"
D10-1115,D09-1045,0,0.0986452,", whereas component-wise multiplication has an intersective effect. They also evaluate a weighted combination of the simplified additive and multiplicative functions. The best results on the task of paraphrasing noun-verb combinations with ambiguous verbs (sales slump is more like declining than slouching) are obtained using the multiplicative approach, and by weighted combination of addition and multiplication (we do not test model combinations in our current experiments). The multiplicative approach also performs best (but only by a small margin) in a later application to language modeling (Mitchell and Lapata, 2009). Erk and Pad´o (2008; 2009) adopt the same formalism but focus on the nature of input vectors, suggesting that when a verb is composed with a noun, the noun component is given by an average of verbs that the noun is typically object of (along similar lines, Kintsch (2001) also focused on composite input vectors, within an additive framework). Again, the multiplicative model works best in Erk and Pad´o’s experiments. The above-mentioned researchers do not exploit corpus evidence about the p vectors that result from composition, despite the fact that it is straightforward (at least for short co"
D10-1115,2003.mtsummit-papers.42,0,0.101234,"ods differently. Dimensionality reduction Since, for each test set adjective, we need to estimate a regression model for each dimension, we want a compact space with relatively few, dense dimensions. A natural way to do this is to apply the Singular Value Decomposition (SVD) to the co-occurrence matrix, and represent the items of interest with their coordinates in the space spanned by the first n right singular vectors. Applying SVD is independently justified because, besides mitigating the dimensionality problem, it often improves the quality of the semantic space (Landauer and Dumais, 1997; Rapp, 2003; Sch¨utze, 1997). To avoid bias in favour of dimensions that capture variance in the test set ANs, we applied SVD to the core vocabulary subset of the co-occurrence matrix (containing only adjective and noun rows). The core 12K×10K matrix was reduced using SVD to a 12K×300 matrix. The other row vectors of the full co-occurrence matrix (including the ANs) were projected onto the same reduced space by multiplying them by a matrix containing the first n right singular vectors as columns. Merging the items used to compute the SVD and those projected onto the resulting space, we obtain a 40,999×30"
D10-1115,P10-1093,0,0.0667617,"and evaluate this representation in an adjective clustering task. Section 8 concludes by sketching directions for further work. 2 Related work The literature on compositionality in vector-based semantics encompasses various related topics, some of them not of direct interest here, such as how to 1184 encode word order information in context vectors (Jones and Mewhort, 2007; Sahlgren et al., 2008) or sophisticated composition methods based on tensor products, quantum logic, etc., that have not yet been empirically tested on large-scale corpus-based semantic space tasks (Clark and Pulman, 2007; Rudolph and Giesbrecht, 2010; Smolensky, 1990; Widdows, 2008). Closer to our current purposes is the general framework for vector composition proposed by Mitchell and Lapata (2008), subsuming various earlier proposals. Given two vectors u and v, they identify two general classes of composition models, (linear) additive models: p = Au + Bv (1) where A and B are weight matrices, and multiplicative models: p = Cuv where C is a weight tensor projecting the uv tensor product onto the space of p. Mitchell and Lapata derive two simplified models from these general forms. Their simplified additive model p = αu + βv was a common"
D13-1015,D10-1115,1,0.921697,"sometimes called distributional semantics) scales well to large lexicons and does not require words to be manually disambiguated (Sch¨utze, 1997). Until recently, however, this method had been almost exclusively limited to the level of single content words (nouns, adjectives, verbs), and had not directly addressed the problem of compositionality (Frege, 1892; Montague, 1970; Partee, 2004), Several recent proposals have strived to extend distributional semantics with a component that also generates vectors for complex linguistic constituents, using compositional operations in the vector space (Baroni and Zamparelli, 2010; Guevara, 2010; Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012). All of these approaches construct distributional representations for novel phrases starting from the corpusderived vectors for their lexical constituents and exploiting the geometric quality of the representation. Such methods are able to capture complex semantic information of adjective-noun (AN) phrases, such as characterizing modification (Boleda et al., 2012; Boleda et al., 2013), and can detect semantic deviance in novel phrases (Vecchi et al., 2011). Furthermore, these methods are naturall"
D13-1015,D12-1050,0,0.0235107,"onsisting of a matrix where each row represents the meaning of an adjective, noun, AN or AAN as a distributional vector, each column a semantic dimension of meaning. We first introduce the source corpus, then the vocabulary of words and phrases that we represent in the space, and finally the procedure adopted to build the vectors representing the vocabulary items from corpus statistics, and obtain the semantic space matrix. We work here with a traditional, window-based semantic space, since our focus is on the effect of different composition methods given a common semantic space. In addition, Blacoe and Lapata (2012) found that a vanilla space of this sort performed best in their composition experiments, when compared to a syntax-aware space and to neural language model vectors such as those used for composition by Socher et al. (2011). Semantic space vocabulary The words/phrases in the semantic space must of course include the items that we need for our experiments (adjectives, nouns, ANs and AANs used for model training, as input to composition and for evaluation). Therefore, we first populate our semantic space with a core vocabulary containing the 8K most frequent nouns and the 4K most frequent adject"
D13-1015,W13-0104,1,0.886915,"generates vectors for complex linguistic constituents, using compositional operations in the vector space (Baroni and Zamparelli, 2010; Guevara, 2010; Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012). All of these approaches construct distributional representations for novel phrases starting from the corpusderived vectors for their lexical constituents and exploiting the geometric quality of the representation. Such methods are able to capture complex semantic information of adjective-noun (AN) phrases, such as characterizing modification (Boleda et al., 2012; Boleda et al., 2013), and can detect semantic deviance in novel phrases (Vecchi et al., 2011). Furthermore, these methods are naturally recursive: they can derive a representation not only for, e.g., red car, but also for new red car, fast new red car, etc. This aspect is appealing since trying to extract meaningful representations for all recursive phrases directly from a corpus will result in a problem of sparsity, since most large phrases will never occur in any finite sample. Once we start seriously looking into recursive modification, however, the issue of modifier ordering restrictions naturally arises. Suc"
D13-1015,W10-0701,0,0.0128477,"ociation strengths for both Ax N and Ay N, thus we expect ∆PMI to be closer to 0 than for rigid order AANs. 2.4 Gold standard To our knowledge, this is the first study to use distributional representations of recursive modification; therefore we must first determine if the composed AAN vector representations are semantically coherent objects. Thus, for vector analysis, a gold standard of 320 corpus-extracted AAN vectors were selected and their quality was established by inspecting their nearest neighbors. In order to create the gold standard, we ran a crowdsourcing experiment on CrowdFlower6 (Callison-Burch and Dredze, 2010; Munro et al., 2010), as follows. First, we gathered a randomly selected set of 600 corpus-extracted AANs, containing 300 flexible order and 300 attested rigid order AANs. We then extracted the top 3 nearest neighbors to the corpusextracted AAN vectors as represented in the semantic space7 . Each AAN was then presented with each of the nearest neighbors, and participants were asked to judge “how strongly related are the two phrases?” on a scale of 1-7. The rationale was that if we obtained a good distributional representation of the AAN, its nearest neighbors should be closely related words a"
D13-1015,J90-1003,0,0.34922,"Lapata (2010), as well as best reported results from Mitchell & Lapata (M&L). Semantic vector construction For each of the items in our vocabulary, we first build 10Kdimensional vectors by recording the item’s sentence-internal co-occurrence with the top 10K most frequent content lemmas (nouns, adjectives, verbs or adverbs) in the corpus. We built a rank of these co-occurrence counts, and excluded as stop words from the dimensions any element of any POS whose rank was from 0 to 300. The raw co-occurrence counts were then transformed into (positive) Pointwise Mutual Information (pPMI) scores (Church and Hanks, 1990). Next, we reduce the full co-occurrence matrix to 300 dimensions applying the Non-negative Matrix Factorization (NMF) operation (Lin, 2007). We did not tune the semantic vector construction parameters, since we found them to work best in a number of independent earlier experiments. Corpus-extracted vectors (corp) were computed for the ANs and for the flexible order and attested rigid order AANs, and then mapped onto the 300dimension NMF-reduced semantic space. As a sanity check, the first row of Table 1 reports the correlation between the AN phrase similarity ratings collected in Mitchell and"
D13-1015,W11-2707,0,0.314006,"der of adjectives, to be tested also in cases where neither order is found in the corpus, so direct corpus evidence cannot help. For a full account of adjectival ordering, non-semantic factors should also be taken into account. As shown by the effectiveness in our experiments of PMI, which is a classic measure used to harvest idioms and other multiword expressions (Church and Hanks, 1990), ordering is affected by arbitrary lexicalization patterns. Metrical effects are also likely to play a role, like they do in the wellstudied case of “binomials” such as salt and pepper (Benor and Levy, 2006; Copestake and Herbelot, 2011). In a pilot study, we found that indeed word length (roughly quantified by number of letters) is a significant factor in predicting adjective ordering (the shorter adjective being more likely to occur first), but its effect is not nearly as strong as that of the semantic measures we considered here. In our future work, we would like to develop an order model that exploits semantic, metrical and lexicalization features jointly for maximal classification accuracy. Adjectival ordering information could be useful in parsing: in English, it could tell whether an AANN sequence should be parsed as A"
D13-1015,P13-4006,1,0.806552,"del and M&L’s (cf. Table 1) is due to the fact that we use log-transformed pPMI scores, making their multiplicative model more akin to our additive approach. 144 by multiplying a function matrix U with a component vector: p~ = U~v . Given a weight matrix, A, for each adjective in the phrase, we apply the functions in sequence recursively as shown in Equation (4). p~ = Ax (Ay ~n) (4) Composition model estimation Parameters for W. ADD , F. ADD and LFM were estimated following the strategy proposed by Guevara (2010) and Baroni and Zamparelli (2010), recently extended to all composition models by Dinu et al. (2013b). Specifically, we learn parameter values that optimize the mapping from the noun to the AN as seen in examples of corpus-extracted N-AN vector pairs, using least-squares methods. All parameter estimations and phrase compositions were implemented using the DISSECT toolkit4 (Dinu et al., 2013a), with a training set of 74,767 corpus-extracted NAN vector pairs, ranging from 100 to over 1K items across the 663 adjectives. Importantly, while below we report experimental results on capturing various properties of recursive AAN constructions, no AAN was seen during training, which was based entirel"
D13-1015,W13-3206,1,0.771691,"del and M&L’s (cf. Table 1) is due to the fact that we use log-transformed pPMI scores, making their multiplicative model more akin to our additive approach. 144 by multiplying a function matrix U with a component vector: p~ = U~v . Given a weight matrix, A, for each adjective in the phrase, we apply the functions in sequence recursively as shown in Equation (4). p~ = Ax (Ay ~n) (4) Composition model estimation Parameters for W. ADD , F. ADD and LFM were estimated following the strategy proposed by Guevara (2010) and Baroni and Zamparelli (2010), recently extended to all composition models by Dinu et al. (2013b). Specifically, we learn parameter values that optimize the mapping from the noun to the AN as seen in examples of corpus-extracted N-AN vector pairs, using least-squares methods. All parameter estimations and phrase compositions were implemented using the DISSECT toolkit4 (Dinu et al., 2013a), with a training set of 74,767 corpus-extracted NAN vector pairs, ranging from 100 to over 1K items across the 663 adjectives. Importantly, while below we report experimental results on capturing various properties of recursive AAN constructions, no AAN was seen during training, which was based entirel"
D13-1015,D11-1129,0,0.0146407,"s and does not require words to be manually disambiguated (Sch¨utze, 1997). Until recently, however, this method had been almost exclusively limited to the level of single content words (nouns, adjectives, verbs), and had not directly addressed the problem of compositionality (Frege, 1892; Montague, 1970; Partee, 2004), Several recent proposals have strived to extend distributional semantics with a component that also generates vectors for complex linguistic constituents, using compositional operations in the vector space (Baroni and Zamparelli, 2010; Guevara, 2010; Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012). All of these approaches construct distributional representations for novel phrases starting from the corpusderived vectors for their lexical constituents and exploiting the geometric quality of the representation. Such methods are able to capture complex semantic information of adjective-noun (AN) phrases, such as characterizing modification (Boleda et al., 2012; Boleda et al., 2013), and can detect semantic deviance in novel phrases (Vecchi et al., 2011). Furthermore, these methods are naturally recursive: they can derive a representation not only for, e.g., red car, b"
D13-1015,W10-2805,0,0.408747,"al semantics) scales well to large lexicons and does not require words to be manually disambiguated (Sch¨utze, 1997). Until recently, however, this method had been almost exclusively limited to the level of single content words (nouns, adjectives, verbs), and had not directly addressed the problem of compositionality (Frege, 1892; Montague, 1970; Partee, 2004), Several recent proposals have strived to extend distributional semantics with a component that also generates vectors for complex linguistic constituents, using compositional operations in the vector space (Baroni and Zamparelli, 2010; Guevara, 2010; Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012). All of these approaches construct distributional representations for novel phrases starting from the corpusderived vectors for their lexical constituents and exploiting the geometric quality of the representation. Such methods are able to capture complex semantic information of adjective-noun (AN) phrases, such as characterizing modification (Boleda et al., 2012; Boleda et al., 2013), and can detect semantic deviance in novel phrases (Vecchi et al., 2011). Furthermore, these methods are naturally recursive: th"
D13-1015,P00-1012,0,0.103369,"f the semantic measures we considered here. In our future work, we would like to develop an order model that exploits semantic, metrical and lexicalization features jointly for maximal classification accuracy. Adjectival ordering information could be useful in parsing: in English, it could tell whether an AANN sequence should be parsed as A[[AN]N] or A[A[NN]]; in languages with pre- and postN adjectives, like Italian or Spanish, it could tell whether ANA sequences should be parsed as A[NA] or [AN]A. The ability to detect ordering restrictions could also help Natural Language Generation tasks (Malouf, 2000), especially for the generation of unattested combinations of As and Ns. From a theoretical point of view, we would like to extend our analysis to adjective coordination (what’s the difference between new and creative idea and new creative idea?). Additionally, we could go more granular, looking at whether compositional models can help us to understand why certain classes of adjectives are more likely to precede or follow others (why is size more likely to take scope over color, so that big red car sounds more natural than red big car?) or studying the behaviour of specific adjectives (can our"
D13-1015,W10-0719,0,0.023657,"and Ay N, thus we expect ∆PMI to be closer to 0 than for rigid order AANs. 2.4 Gold standard To our knowledge, this is the first study to use distributional representations of recursive modification; therefore we must first determine if the composed AAN vector representations are semantically coherent objects. Thus, for vector analysis, a gold standard of 320 corpus-extracted AAN vectors were selected and their quality was established by inspecting their nearest neighbors. In order to create the gold standard, we ran a crowdsourcing experiment on CrowdFlower6 (Callison-Burch and Dredze, 2010; Munro et al., 2010), as follows. First, we gathered a randomly selected set of 600 corpus-extracted AANs, containing 300 flexible order and 300 attested rigid order AANs. We then extracted the top 3 nearest neighbors to the corpusextracted AAN vectors as represented in the semantic space7 . Each AAN was then presented with each of the nearest neighbors, and participants were asked to judge “how strongly related are the two phrases?” on a scale of 1-7. The rationale was that if we obtained a good distributional representation of the AAN, its nearest neighbors should be closely related words and phrases. Each pair"
D13-1015,D12-1110,0,0.0380204,"manually disambiguated (Sch¨utze, 1997). Until recently, however, this method had been almost exclusively limited to the level of single content words (nouns, adjectives, verbs), and had not directly addressed the problem of compositionality (Frege, 1892; Montague, 1970; Partee, 2004), Several recent proposals have strived to extend distributional semantics with a component that also generates vectors for complex linguistic constituents, using compositional operations in the vector space (Baroni and Zamparelli, 2010; Guevara, 2010; Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012). All of these approaches construct distributional representations for novel phrases starting from the corpusderived vectors for their lexical constituents and exploiting the geometric quality of the representation. Such methods are able to capture complex semantic information of adjective-noun (AN) phrases, such as characterizing modification (Boleda et al., 2012; Boleda et al., 2013), and can detect semantic deviance in novel phrases (Vecchi et al., 2011). Furthermore, these methods are naturally recursive: they can derive a representation not only for, e.g., red car, but also for new red ca"
D13-1015,W11-1301,1,0.930517,"l operations in the vector space (Baroni and Zamparelli, 2010; Guevara, 2010; Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012). All of these approaches construct distributional representations for novel phrases starting from the corpusderived vectors for their lexical constituents and exploiting the geometric quality of the representation. Such methods are able to capture complex semantic information of adjective-noun (AN) phrases, such as characterizing modification (Boleda et al., 2012; Boleda et al., 2013), and can detect semantic deviance in novel phrases (Vecchi et al., 2011). Furthermore, these methods are naturally recursive: they can derive a representation not only for, e.g., red car, but also for new red car, fast new red car, etc. This aspect is appealing since trying to extract meaningful representations for all recursive phrases directly from a corpus will result in a problem of sparsity, since most large phrases will never occur in any finite sample. Once we start seriously looking into recursive modification, however, the issue of modifier ordering restrictions naturally arises. Such restrictions have often been discussed in the theoretical linguistic li"
D13-1015,C10-1142,0,0.0334352,"cursive setting of this experiment, as seen in Equation (1). p~ = ~ax ~ay ~n (1) This composition method is order-insensitive, the formula above corresponding to the representation of both Ax Ay N and Ay Ax N. In the weighted additive model (W. ADD), we obtain the composed vector as a weighted sum of the two component vectors: p~ = α~u + β~v , where α and β are scalars. Again, we can easily apply this function recursively, as in Equation (2). p~ = α~ax + β(α~ay + β~n) = α~ax + αβ~ay + β 2~n (2) We also consider the full extension of the additive model (F. ADD), presented in Guevara (2010) and Zanzotto et al. (2010), such that the component vectors are pre-multiplied by weight matrices before being added: p~ = W1 ~u + W2~v . Similarly to the W. ADD model, Equation (3) describes how we apply this function recursively. p~ = W1~ax + W2 (W1~ay + W2~n) = W1~ax + W2 W1~ay + (3) W22~n Finally, we consider the lexical function model (LFM), first introduced in Baroni and Zamparelli (2010), in which attributive adjectives are treated as functions from noun meanings to noun meanings. This is a standard approach in Montague semantics (Thomason, 1974), except noun meanings here are distributional vectors, not denotat"
D13-1015,D12-1112,1,\N,Missing
D13-1196,D10-1115,1,0.937841,"t is not dead? While our intuition, based on the meaning of this phrase, prefers the former interpretation, the Stanford parser, which lacks semantic features, incorrectly predicts the latter as the correct parse.1 The correct syntactic parsing of sentences is clearly steered by semantic information (as formal syntacticians have pointed out at least since Fillmore (1968)), and consequently the semantic plausibility of alternative parses can provide crucial evidence about their validity. An emerging line of parsing research capitalizes on the advances of compositional distributional semantics (Baroni and Zamparelli, 2010; Guevara, 2010; Mitchell and Lapata, 2010; Socher et al., 2012). Information related to compositionallyderived distributional representations of phrases is 1 http://nlp.stanford.edu:8080/parser/ index.jsp integrated at various stages of the parsing process to improve overall performance.2 We are aware of two very recent studies exploiting the semantic information provided by distributional models to resolve syntactic ambiguity: Socher et al. (2013) and Le et al. (2013). Socher et al. (2013) present a recursive neural network architecture which jointly learns semantic representations and synta"
D13-1196,J03-4003,0,0.141336,"Missing"
D13-1196,P13-4006,1,0.152725,"ly (non-negative) Pointwise Mutual Information as weighting scheme and dimensionality reduction using Non-negative Matrix Factorization, setting the number of reduced-space dimensions to 300.5 Composition functions We experiment with various composition functions, chosen among those sensitive to internal structure (Baroni and Zamparelli, 2010; Guevara, 2010; Mitchell and Lapata, 2010), namely dilation (dil), weighted additive (wadd), lexical function (lexfunc) and full additive (fulladd).6 For model implementation and (unsupervised) estimation, we rely on the freely available DISSECT toolkit (Dinu et al., 2013).7 For all methods, vectors were normalized before composing, both in training and in generation. Table 2 presents a summary description of the composition methods we used. Following previous literature (Mitchell and Lapata, 2010), and the general intuition that adjectival modification is quite a different process from noun combination (Gagn´e and Spalding, 2009; McNally, 2013), we learn different parameters for noun-noun (NN) and adjective-noun (AN) phrases. As an example of the learned parameters, for the wadd model the ratio of parameters w1 and w2 is 1:2 for ANs, whereas for NNs it is almo"
D13-1196,W10-2805,0,0.0407893,"ition, based on the meaning of this phrase, prefers the former interpretation, the Stanford parser, which lacks semantic features, incorrectly predicts the latter as the correct parse.1 The correct syntactic parsing of sentences is clearly steered by semantic information (as formal syntacticians have pointed out at least since Fillmore (1968)), and consequently the semantic plausibility of alternative parses can provide crucial evidence about their validity. An emerging line of parsing research capitalizes on the advances of compositional distributional semantics (Baroni and Zamparelli, 2010; Guevara, 2010; Mitchell and Lapata, 2010; Socher et al., 2012). Information related to compositionallyderived distributional representations of phrases is 1 http://nlp.stanford.edu:8080/parser/ index.jsp integrated at various stages of the parsing process to improve overall performance.2 We are aware of two very recent studies exploiting the semantic information provided by distributional models to resolve syntactic ambiguity: Socher et al. (2013) and Le et al. (2013). Socher et al. (2013) present a recursive neural network architecture which jointly learns semantic representations and syntactic categories"
D13-1196,P03-1054,0,0.0301383,"Missing"
D13-1196,P95-1007,0,0.0545006,"lausibility Accuracy 65.6 77.3 74.6 74.0 76.2 75.9 78.2 78.7 81.2 82.9 85.6 Table 3: Evaluation of feature sets from Section 3 (blood pressure) medicine and blood (pressure medicine). • fplausibility concatenates fbasic and frec . • pmi contains the PMI scores extracted from our corpus for blood pressure and pressure medicine.9 • pmi + fplausibility concatenates pmi and fplausibility . Baseline Model Given the skewed bracketing distribution in our dataset, we implement the following majority baselines: a) right classifies all phrases as right-bracketed; b) pos classifies NNN as leftbracketed (Lauer, 1995), ANN as right-bracketed. 4 Results and Discussion Table 3 omits results for dil and fulladd since they were outperformed by the right baseline. That wadd- and lexfunc-based plausibility features perform well above this baseline is encouraging, since it represents the typical default behaviour of parsers for NPs, although note that these features perform comparably to the pos baseline, which would be quite simple to embed in a parser (for English, at least). For both models, using both basic and recursive features leads to a boost in performance over basic features alone. Note that recursive f"
D13-1196,W13-3202,0,0.0275169,"Missing"
D13-1196,J93-2004,0,0.0495618,"against features based on n-gram statistics, which can arguably also capture some semantic information in terms of frequent occurrences of meaningful phrases. Inspired by previous literature demonstrating the power of metrics based on Pointwise Mutual Information (PMI) in NP bracketing (Nakov and Hearst, 2005; Pitler et al., 2010; Vadas and Curran, 2011), we test an approach exploiting PMI features, and show that plausibility features relying on composed representations can significantly boost accuracy over PMI. 2 Setup Noun phrase dataset To construct our dataset, we used the Penn TreeBank (Marcus et al., 1993), which we enriched with the annotation provided by Vadas and Curran (2007a), since the original treebank does not distinguish different structures inside the NPs and always marks them as right bracketed, e.g., local (phone company) but also blood (pressure medicine). We focus on NPs formed by three elements, where the first can be an adjective (A) or a noun (N), the other two are nouns. Table 1 summarizes the characteristics of the dataset.3 Distributional semantic space As our source corpus we use the concatenation of ukWaC, the English Wikipedia (2009 dump) and the BNC, with a total of 3 Th"
D13-1196,W05-0603,0,0.0124186,"n to correlate with semantic plausibility (Vecchi et al., 2011). We develop a controlled experimental setup, focusing on a single syntactic category, that is, noun phrases (NP), where our task can be formalized as (left or right) bracketing. Unlike previous work, we compare our compositional semantic component against features based on n-gram statistics, which can arguably also capture some semantic information in terms of frequent occurrences of meaningful phrases. Inspired by previous literature demonstrating the power of metrics based on Pointwise Mutual Information (PMI) in NP bracketing (Nakov and Hearst, 2005; Pitler et al., 2010; Vadas and Curran, 2011), we test an approach exploiting PMI features, and show that plausibility features relying on composed representations can significantly boost accuracy over PMI. 2 Setup Noun phrase dataset To construct our dataset, we used the Penn TreeBank (Marcus et al., 1993), which we enriched with the annotation provided by Vadas and Curran (2007a), since the original treebank does not distinguish different structures inside the NPs and always marks them as right bracketed, e.g., local (phone company) but also blood (pressure medicine). We focus on NPs formed"
D13-1196,P06-1055,0,0.0191339,"Missing"
D13-1196,C10-1100,0,0.0122184,"ntic plausibility (Vecchi et al., 2011). We develop a controlled experimental setup, focusing on a single syntactic category, that is, noun phrases (NP), where our task can be formalized as (left or right) bracketing. Unlike previous work, we compare our compositional semantic component against features based on n-gram statistics, which can arguably also capture some semantic information in terms of frequent occurrences of meaningful phrases. Inspired by previous literature demonstrating the power of metrics based on Pointwise Mutual Information (PMI) in NP bracketing (Nakov and Hearst, 2005; Pitler et al., 2010; Vadas and Curran, 2011), we test an approach exploiting PMI features, and show that plausibility features relying on composed representations can significantly boost accuracy over PMI. 2 Setup Noun phrase dataset To construct our dataset, we used the Penn TreeBank (Marcus et al., 1993), which we enriched with the annotation provided by Vadas and Curran (2007a), since the original treebank does not distinguish different structures inside the NPs and always marks them as right bracketed, e.g., local (phone company) but also blood (pressure medicine). We focus on NPs formed by three elements, w"
D13-1196,D12-1110,0,0.0185987,", prefers the former interpretation, the Stanford parser, which lacks semantic features, incorrectly predicts the latter as the correct parse.1 The correct syntactic parsing of sentences is clearly steered by semantic information (as formal syntacticians have pointed out at least since Fillmore (1968)), and consequently the semantic plausibility of alternative parses can provide crucial evidence about their validity. An emerging line of parsing research capitalizes on the advances of compositional distributional semantics (Baroni and Zamparelli, 2010; Guevara, 2010; Mitchell and Lapata, 2010; Socher et al., 2012). Information related to compositionallyderived distributional representations of phrases is 1 http://nlp.stanford.edu:8080/parser/ index.jsp integrated at various stages of the parsing process to improve overall performance.2 We are aware of two very recent studies exploiting the semantic information provided by distributional models to resolve syntactic ambiguity: Socher et al. (2013) and Le et al. (2013). Socher et al. (2013) present a recursive neural network architecture which jointly learns semantic representations and syntactic categories of phrases. By annotating syntactic categories w"
D13-1196,P13-1045,0,0.0479174,"Missing"
D13-1196,P07-1031,0,0.103849,"pture some semantic information in terms of frequent occurrences of meaningful phrases. Inspired by previous literature demonstrating the power of metrics based on Pointwise Mutual Information (PMI) in NP bracketing (Nakov and Hearst, 2005; Pitler et al., 2010; Vadas and Curran, 2011), we test an approach exploiting PMI features, and show that plausibility features relying on composed representations can significantly boost accuracy over PMI. 2 Setup Noun phrase dataset To construct our dataset, we used the Penn TreeBank (Marcus et al., 1993), which we enriched with the annotation provided by Vadas and Curran (2007a), since the original treebank does not distinguish different structures inside the NPs and always marks them as right bracketed, e.g., local (phone company) but also blood (pressure medicine). We focus on NPs formed by three elements, where the first can be an adjective (A) or a noun (N), the other two are nouns. Table 1 summarizes the characteristics of the dataset.3 Distributional semantic space As our source corpus we use the concatenation of ukWaC, the English Wikipedia (2009 dump) and the BNC, with a total of 3 The dataset is available from: http://clic.cimec. unitn.it/composes 1909 abo"
D13-1196,J11-4006,0,0.015475,"cchi et al., 2011). We develop a controlled experimental setup, focusing on a single syntactic category, that is, noun phrases (NP), where our task can be formalized as (left or right) bracketing. Unlike previous work, we compare our compositional semantic component against features based on n-gram statistics, which can arguably also capture some semantic information in terms of frequent occurrences of meaningful phrases. Inspired by previous literature demonstrating the power of metrics based on Pointwise Mutual Information (PMI) in NP bracketing (Nakov and Hearst, 2005; Pitler et al., 2010; Vadas and Curran, 2011), we test an approach exploiting PMI features, and show that plausibility features relying on composed representations can significantly boost accuracy over PMI. 2 Setup Noun phrase dataset To construct our dataset, we used the Penn TreeBank (Marcus et al., 1993), which we enriched with the annotation provided by Vadas and Curran (2007a), since the original treebank does not distinguish different structures inside the NPs and always marks them as right bracketed, e.g., local (phone company) but also blood (pressure medicine). We focus on NPs formed by three elements, where the first can be an"
D13-1196,W11-1301,1,0.313446,"ikely to appear in corpora; for a review see, e.g., Turney and Pantel (2010). 1908 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1908–1913, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics Type of NP A (N N) (A N) N N (N N) (N N) N Total # 1296 343 164 424 2227 Example local phone company crude oil sector miracle home run blood pressure medicine - Table 1: NP dataset tracted from the compositional distributional representations of phrases, that have been shown to correlate with semantic plausibility (Vecchi et al., 2011). We develop a controlled experimental setup, focusing on a single syntactic category, that is, noun phrases (NP), where our task can be formalized as (left or right) bracketing. Unlike previous work, we compare our compositional semantic component against features based on n-gram statistics, which can arguably also capture some semantic information in terms of frequent occurrences of meaningful phrases. Inspired by previous literature demonstrating the power of metrics based on Pointwise Mutual Information (PMI) in NP bracketing (Nakov and Hearst, 2005; Pitler et al., 2010; Vadas and Curran,"
D13-1202,P13-4032,1,0.838757,"Missing"
D13-1202,J90-1003,0,0.540752,"Missing"
D13-1202,N10-1011,0,0.0328261,"y et al. (2012), who showed that purely corpus-based distributional models are at least as good at brain signal prediction tasks as earlier models that made use of manually-generated or controlled knowledge sources (Chang et al., 2011; Palatucci et al., 2009; Pereira et al., 2011), and we evaluate a very recent type of distributional model, namely one that is not extracted from textual data but from image collections through automated visual feature extraction techniques. It has been argued that this new generation of image-based distributional models (Bruni et al., 2011; Bruni et al., 2012b; Feng and Lapata, 2010; Leong and Mihalcea, 2011) provides a more realistic view of meaning, since humans obviously acquire a large proportion of their semantic knowledge from perceptual data. The first question that we ask, thus, is whether the more “grounded” image-based models can help us in interpreting conceptual representations in the brain. More specifically, we will compare the performance of different image-based representations, and we will test whether text- and image-based representations are complementary, so that when used together they can better account for patterns in neural data. Finally, we will"
D13-1202,I11-1162,0,0.0210728,"owed that purely corpus-based distributional models are at least as good at brain signal prediction tasks as earlier models that made use of manually-generated or controlled knowledge sources (Chang et al., 2011; Palatucci et al., 2009; Pereira et al., 2011), and we evaluate a very recent type of distributional model, namely one that is not extracted from textual data but from image collections through automated visual feature extraction techniques. It has been argued that this new generation of image-based distributional models (Bruni et al., 2011; Bruni et al., 2012b; Feng and Lapata, 2010; Leong and Mihalcea, 2011) provides a more realistic view of meaning, since humans obviously acquire a large proportion of their semantic knowledge from perceptual data. The first question that we ask, thus, is whether the more “grounded” image-based models can help us in interpreting conceptual representations in the brain. More specifically, we will compare the performance of different image-based representations, and we will test whether text- and image-based representations are complementary, so that when used together they can better account for patterns in neural data. Finally, we will check for differences betwe"
D13-1202,S12-1019,0,0.144058,"hmarks). If we found that a corpus-based model of meaning can make nontrivial predictions about the structure of the semantic space in the brain, that would make a pretty strong case for the intriguing idea that the model is approximating, in interesting ways, the way in which humans acquire and represent semantic knowledge. 1960 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1960–1970, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics We take as our starting point the extensive experiments reported in Murphy et al. (2012), who showed that purely corpus-based distributional models are at least as good at brain signal prediction tasks as earlier models that made use of manually-generated or controlled knowledge sources (Chang et al., 2011; Palatucci et al., 2009; Pereira et al., 2011), and we evaluate a very recent type of distributional model, namely one that is not extracted from textual data but from image collections through automated visual feature extraction techniques. It has been argued that this new generation of image-based distributional models (Bruni et al., 2011; Bruni et al., 2012b; Feng and Lapata"
D13-1202,P13-1056,0,0.0735555,"Missing"
D13-1202,P12-1015,1,\N,Missing
D13-1202,W11-2503,1,\N,Missing
D13-1202,R11-1055,0,\N,Missing
D15-1002,J10-4006,1,0.736996,"Missing"
D15-1002,J10-4007,1,0.625165,"Missing"
D15-1002,P09-1113,0,0.00429198,"-of::organization with the value world bank results in a binary attribute Referential Representations As our source of referential attributes, we use FreeBase (see footnote 1), a knowledge base of structured information on a wide range of entities of different semantic types (people, geographical entities, etc.). The information in FreeBase comes from various sources, including Wikipedia and domainspecific databases, plus user content generation and correction. FreeBase currently records at least 2 attributes for over 47 million entities, and it has been used fairly extensively in NLP before (Mintz et al., 2009; Socher et al., 2013a, among others). For each entity, FreeBase contains a list of attribute-value tuples (where values can in turn be entities, allowing a graph view of the data that we do not exploit here). Table 1 shows a sample of the attributes that FreeBase records for countries. Note that some attributes are simple (e.g., date founded), while other can be called complex, in the sense that they are attributes of attributes (e.g., geolocation::latitude). We use a double-colon notation to refer to complex attributes. The values of all attributes can be either numeric or categorical. The n"
D15-1002,W15-0107,0,0.0504358,"Missing"
D15-1002,S12-1019,0,0.0125682,"th Herbelot (2015). However, the specific approach is different, as she constructs vectors for individual entities (literary characters) by contextualizing generic noun vectors with distributional properties of those entities. Finally, we share our methodology with work on mapping between corpus-based word representations and other representational spaces, such as subject-generated concept properties (Johns and Jones, 2012; Hill et al., 2014; F˘ag˘ar˘as¸an et al., 2015), visual features (Frome et al., 2013; Socher et al., 2013b; Lazaridou et al., 2014) or brain signals (Mitchell et al., 2008; Murphy et al., 2012). In all these settings, the focus is entirely on predicting numerical attributes, whereas we treat both numerical and binary attributes. Rubinstein et al. (2015) use distributional vectors to predict binary conceptual attributes of common nouns, as well as a continuous score measuring saliency of such attributes. Our target features are conceptually very different from those of all these studies. Related Work There is a large literature on exploiting corpus evidence, sometimes through distributional semantic methods, in order to construct and populate structured knowledge bases (KBs) (e.g., B"
D15-1002,J07-2002,1,0.247538,"Missing"
D15-1002,P15-2119,0,0.0494805,"neric noun vectors with distributional properties of those entities. Finally, we share our methodology with work on mapping between corpus-based word representations and other representational spaces, such as subject-generated concept properties (Johns and Jones, 2012; Hill et al., 2014; F˘ag˘ar˘as¸an et al., 2015), visual features (Frome et al., 2013; Socher et al., 2013b; Lazaridou et al., 2014) or brain signals (Mitchell et al., 2008; Murphy et al., 2012). In all these settings, the focus is entirely on predicting numerical attributes, whereas we treat both numerical and binary attributes. Rubinstein et al. (2015) use distributional vectors to predict binary conceptual attributes of common nouns, as well as a continuous score measuring saliency of such attributes. Our target features are conceptually very different from those of all these studies. Related Work There is a large literature on exploiting corpus evidence, sometimes through distributional semantic methods, in order to construct and populate structured knowledge bases (KBs) (e.g., Buitelaar and Cimiano (2008) and references therein). This line of work, however, does not attempt to connect entity representations extracted from corpora and fro"
D15-1002,W15-0120,0,0.0479495,"e rely on the same architecture to learn discrete features denoting relations with entities and numerical features, to induce full attribute-based descriptions of entities. Our proposal is only distantly related to methods to embed words tokens and KB entities and relationships in a vector space, e.g., for better relation extraction (see Weston et al. (2013) and references therein). This line of work does not use distributional semantics to induce word vectors, and ignores numerical attributes. The broader goal of getting at referential information with distributional semantics is shared with Herbelot (2015). However, the specific approach is different, as she constructs vectors for individual entities (literary characters) by contextualizing generic noun vectors with distributional properties of those entities. Finally, we share our methodology with work on mapping between corpus-based word representations and other representational spaces, such as subject-generated concept properties (Johns and Jones, 2012; Hill et al., 2014; F˘ag˘ar˘as¸an et al., 2015), visual features (Frome et al., 2013; Socher et al., 2013b; Lazaridou et al., 2014) or brain signals (Mitchell et al., 2008; Murphy et al., 201"
D15-1002,Q14-1023,0,0.0142536,"tional semantics to induce word vectors, and ignores numerical attributes. The broader goal of getting at referential information with distributional semantics is shared with Herbelot (2015). However, the specific approach is different, as she constructs vectors for individual entities (literary characters) by contextualizing generic noun vectors with distributional properties of those entities. Finally, we share our methodology with work on mapping between corpus-based word representations and other representational spaces, such as subject-generated concept properties (Johns and Jones, 2012; Hill et al., 2014; F˘ag˘ar˘as¸an et al., 2015), visual features (Frome et al., 2013; Socher et al., 2013b; Lazaridou et al., 2014) or brain signals (Mitchell et al., 2008; Murphy et al., 2012). In all these settings, the focus is entirely on predicting numerical attributes, whereas we treat both numerical and binary attributes. Rubinstein et al. (2015) use distributional vectors to predict binary conceptual attributes of common nouns, as well as a continuous score measuring saliency of such attributes. Our target features are conceptually very different from those of all these studies. Related Work There is a"
D15-1002,P14-1132,1,0.435548,"eferential information with distributional semantics is shared with Herbelot (2015). However, the specific approach is different, as she constructs vectors for individual entities (literary characters) by contextualizing generic noun vectors with distributional properties of those entities. Finally, we share our methodology with work on mapping between corpus-based word representations and other representational spaces, such as subject-generated concept properties (Johns and Jones, 2012; Hill et al., 2014; F˘ag˘ar˘as¸an et al., 2015), visual features (Frome et al., 2013; Socher et al., 2013b; Lazaridou et al., 2014) or brain signals (Mitchell et al., 2008; Murphy et al., 2012). In all these settings, the focus is entirely on predicting numerical attributes, whereas we treat both numerical and binary attributes. Rubinstein et al. (2015) use distributional vectors to predict binary conceptual attributes of common nouns, as well as a continuous score measuring saliency of such attributes. Our target features are conceptually very different from those of all these studies. Related Work There is a large literature on exploiting corpus evidence, sometimes through distributional semantic methods, in order to co"
D15-1002,D13-1136,0,0.0119385,"ose in spirit to what we do, except that, given an entity1-relation-entity2 tuple, we treat relation-entity2 as a binary attribute of entity1, and we try to induce such attributes on a larger scale (Socher et al. consider seven relations in total). Moreover, we rely on the same architecture to learn discrete features denoting relations with entities and numerical features, to induce full attribute-based descriptions of entities. Our proposal is only distantly related to methods to embed words tokens and KB entities and relationships in a vector space, e.g., for better relation extraction (see Weston et al. (2013) and references therein). This line of work does not use distributional semantics to induce word vectors, and ignores numerical attributes. The broader goal of getting at referential information with distributional semantics is shared with Herbelot (2015). However, the specific approach is different, as she constructs vectors for individual entities (literary characters) by contextualizing generic noun vectors with distributional properties of those entities. Finally, we share our methodology with work on mapping between corpus-based word representations and other representational spaces, such"
D15-1002,N13-1090,0,0.154617,"Missing"
D17-1030,W13-3512,0,0.769753,"Missing"
D17-1030,C16-1122,1,0.914223,"Missing"
D17-1030,P12-1092,0,0.126114,"to its standard architecture to learn new terms from tiny data, using background knowledge from a previously learnt semantic space. We test our model on word definitions and on a nonce task involving 2-6 sentences’ worth of context, showing a large increase in performance over state-of-the-art models on the definitional task. 1 Marco Baroni Center for Mind/Brain Sciences University of Trento marco.baroni@unitn.it Introduction Distributional models (DS: Turney and Pantel (2010); Clark (2012); Erk (2012)), and in particular neural network approaches (Bengio et al., 2003; Collobert et al., 2011; Huang et al., 2012; Mikolov et al., 2013), do not fare well in the absence of large corpora. That is, for a DS model to learn a word vector, it must have seen that word a sufficient number of times. This is in sharp contrast with the human ability to perform fast mapping, i.e. the acquisition of a new concept from a single exposure to information (Lake et al., 2011; Trueswell et al., 2013; Lake et al., 2016). There are at least two reasons for wanting to acquire vectors from very small data. First, some words are simply rare in corpora, but potentially crucial to some applications (consider, for instance, the p"
D17-1030,W15-0108,0,0.0714061,"Missing"
D17-1030,W16-3643,0,0.0311532,"ers in a non-monotonic way, to take into account the quality of the information it is processing. This point seems to be an important general requirement for any architecture that claims incrementality: our results indicate very strongly that a notion of informativeness must play a role in the learning decisions of the system. This conclusion is in line with work in other domains, e.g. interactive word learning using dialogue, where performance is linked to the ability of the system to measure its own confidence in particular pieces of knowledge and ask questions with a high information gain (Yu et al., 2016). It also meets with general considerations on language acquisition, which accounts for the ability of young children to learn from limited ‘primary linguistic data’ by restricting explanatory models to those that provide such efficiency (Clark and Lappin, 2010). Acknowledgments We are grateful to Katrin Erk for inspiring conversations about tiny data and fast-mapping, and to Raffaella Bernardi and Sandro Pezzelle for comments on an early draft of this paper. We also thank the anonymous reviewers for their time and valuable comments. We acknowledge ERC 2011 Starting Independent Research Grant"
D17-1030,P13-1149,1,\N,Missing
D19-3010,D18-1119,1,0.8185,"symbol or variable-length communication (with vanilla RNNs (Elman, 1990), GRUs (Cho et al., 2014), LSTMs (Hochreiter and Schmidhuber, 1997));1 (b) Training with optimization of the communication channel through REINFORCE or Gumbel-Softmax relaxation via a common interface; (c) Simplified configuration of the general components, such as check-pointing, optimization, Tensorboard support,2 etc.; (d) Studying the languages that emerge when neural agents interact with each other recently became a vibrant area of research (Havrylov and Titov, 2017; Lazaridou et al., 2016, 2018; Kottur et al., 2017; Bouchacourt and Baroni, 2018; Lowe et al., 2019). Interest in this scenario is fueled by the hypothesis that the ability to interact through a human-like language is a prerequisite for genuine AI (Mikolov et al., 2016; Chevalier-Boisvert et al., 2019). Furthermore, such simulations might lead to a better understanding of both standard NLP models (Chaabouni et al., 2019b) and the evolution of human language itself (Kirby, 2002). For all its promise, research in this domain is technically very challenging, due to the discrete nature of communication. The latter pre1 EGG also provides an experimental support of Transformers"
D19-3010,P19-1509,1,0.749117,"ing, optimization, Tensorboard support,2 etc.; (d) Studying the languages that emerge when neural agents interact with each other recently became a vibrant area of research (Havrylov and Titov, 2017; Lazaridou et al., 2016, 2018; Kottur et al., 2017; Bouchacourt and Baroni, 2018; Lowe et al., 2019). Interest in this scenario is fueled by the hypothesis that the ability to interact through a human-like language is a prerequisite for genuine AI (Mikolov et al., 2016; Chevalier-Boisvert et al., 2019). Furthermore, such simulations might lead to a better understanding of both standard NLP models (Chaabouni et al., 2019b) and the evolution of human language itself (Kirby, 2002). For all its promise, research in this domain is technically very challenging, due to the discrete nature of communication. The latter pre1 EGG also provides an experimental support of Transformers (Vaswani et al., 2017). 2 https://www.tensorflow.org/ tensorboard A screencast demonstration of EGG is available at https://vimeo.com/345470060 55 Proceedings of the 2019 EMNLP and the 9th IJCNLP (System Demonstrations), pages 55–60 c Hong Kong, China, November 3 – 7, 2019. 2019 Association for Computational Linguistics SymbolGameReinforce"
D19-3010,D17-1321,0,0.150104,"implementing single-symbol or variable-length communication (with vanilla RNNs (Elman, 1990), GRUs (Cho et al., 2014), LSTMs (Hochreiter and Schmidhuber, 1997));1 (b) Training with optimization of the communication channel through REINFORCE or Gumbel-Softmax relaxation via a common interface; (c) Simplified configuration of the general components, such as check-pointing, optimization, Tensorboard support,2 etc.; (d) Studying the languages that emerge when neural agents interact with each other recently became a vibrant area of research (Havrylov and Titov, 2017; Lazaridou et al., 2016, 2018; Kottur et al., 2017; Bouchacourt and Baroni, 2018; Lowe et al., 2019). Interest in this scenario is fueled by the hypothesis that the ability to interact through a human-like language is a prerequisite for genuine AI (Mikolov et al., 2016; Chevalier-Boisvert et al., 2019). Furthermore, such simulations might lead to a better understanding of both standard NLP models (Chaabouni et al., 2019b) and the evolution of human language itself (Kirby, 2002). For all its promise, research in this domain is technically very challenging, due to the discrete nature of communication. The latter pre1 EGG also provides an experi"
E06-1028,J03-3001,0,0.0762028,"ions. The method is based on the comparison of the word frequency distribution of the target corpus to word frequency distributions from corpora built in deliberately biased ways. We apply the method to the task of building a corpus via queries to Google. Our results indicate that this approach can be used, reliably, to discriminate biased and unbiased document collections and to choose the most appropriate query terms. 1 Introduction The Web is a very rich source of linguistic data, and in the last few years it has been used intensively by linguists and language technologists for many tasks (Kilgarriff and Grefenstette, 2003). Among other uses, the Web allows fast and inexpensive construction of “general purpose” corpora, i.e., corpora that are not meant to represent a specific sub-language, but a language as a whole. There are several recent studies on the extent to which Web-derived corpora are comparable, in terms of variety of topics and styles, to traditional “balanced” corpora (Fletcher, 2004; Sharoff, 2006). Our contribution, in this paper, is to present an automated, quantitative method to evaluate the “variety” or “randomness” (with respect to a number of non-random partitions) of a Web corpus. The more r"
E06-1028,magnini-cavaglia-2000-integrating,0,0.0118302,". The BNC.af list is sampled randomly from the whole BNC and, because of the Zipfian properties of word types, coupled with the large size of the BNC, it is mostly characterized by very low frequency words. In this case, we might expect data sparseness problems. Finally, we expect the spoken demographic sample to be a “mildly biased” set, as it samples only words used in spoken conversational English. In order to build biased queries, hopefully leading to the retrieval of topically related documents, we defined a set of specialized categories using the WordNet (Fellbaum, 1998) “domain” lists (Magnini and Cavaglia, 2000). We selected 200 words at random from each of the following domains: administration, commerce, computer science, fashion, gastronomy, geography, law, military, music, sociology. These domains were chosen since they look “general” enough that they should be very well-represented on the Web, but not so general as to be virtually unbiased (cf. the WordNet domain person). We selected words only among those that did not belong to more than 3 http://wordlist.sourceforge.net/ 12dicts-readme.html one WordNet domain, and we avoided multi-word terms. It is important to realize that a balanced corpus is"
E06-1028,baroni-bernardini-2004-bootcat,1,\N,Missing
E06-2001,J03-3001,1,0.681055,"lternative is to crawl the Web ourselves, which also allows us to remove duplicates and nearduplicates, navigational material, and a range of other kinds of non-linguistic matter. We can also tokenize, lemmatise and part-of-speech tag the corpus, and load the data into a corpus query tool which supports sophisticated linguistic queries. We have now done this for German and Italian, with corpus sizes of over 1 billion words in each case. We provide Web access to the corpora in our query tool, the Sketch Engine. 1 Introduction The Web contains vast amounts of linguistic data for many languages (Kilgarriff and Grefenstette, 2003). One key issue for linguists and language technologists is how to access it. The drawbacks of using commercial search engines are presented in Kilgarriff (2003). An alternative is to crawl the Web ourselves.1 We have done this for two languages, German and Italian, and here we report on the pipeline of processes which give us reasonably well-behaved, ‘clean’ corpora for each language. 1 Another Web access option is Alexa (http://pages. alexa.com/company/index.html), who allow the user (for a modest fee) to access their cached Web directly. Using Alexa would mean one did not need to crawl; how"
E12-1004,W03-1812,0,0.0177654,"rds tend to share similar contexts, DS has been very successful in tasks that require quantifying semantic similarity among words, such as synonym detection and concept clustering (Turney and Pantel, 2010). Recently, there has been a flurry of interest in DS to model meaning composition: How can we derive the DS representation of a composite phrase from that of its constituents? Although the general focus in the area is to perform algebraic operations on word semantic vectors (Mitchell and Lapata, 2010), some researchers have also directly examined the corpus contexts of phrases. For example, Baldwin et al. (2003) studied vector extraction for phrases because they were interested in the decomposability of multiword expressions. Baroni and Zamparelli (2010) and Guevara (2010) look at corpus-harvested phrase vectors to learn composition functions that should derive such composite vectors automatically. Baroni and Zamparelli, in particular, showed qualitatively that directly corpus-harvested vectors for AN constructions are meaningful; for example, the vector of young husband has nearest neighbors small son, small daughter and mistress. Following up on this approach, we show here quantitatively that corpu"
E12-1004,W11-2501,1,0.303507,"N usually does not entail another N, we can create negative examples (AN1 6|= N2 ) just by randomly permuting the Ns. Of course, such unsupervised data would be slightly noisy, especially because some of the most frequent adjectives are not restrictive. To collect cleaner data and to be sure that we are really examining the phenomenon of entailment, we took a mere few moments of manual effort to select the 256 restrictive adjectives from the most frequent 300 adjectives in the corpus. We then took the Cartesian product of these 256 adjectives with the 200 concrete nouns in the BLESS data set (Baroni and Lenci, 2011). Those nouns were chosen to avoid highly polysemous words. From the Cartesian product, we obtain a total of 1246 AN sequences, such as big cat, that occur more than 100 times in the corpus. These AN sequences encompass 190 of the 256 adjec26 tives and 128 of the 200 nouns. The process results in 1246 positive instances of AN |= N entailment, which we use as training data. To create a comparable amount of negative data, we randomly permuted the nouns in the positive instances to obtain pairs of AN1 6|= N2 (e.g., big cat 6|= dog). We manually double-checked that all positive and negative exampl"
E12-1004,D10-1115,1,0.92743,"a, but it has been largely limited to the lexical domain. On the other hand, FS has provided sophisticated models of sentence meaning, but it has been largely limited to hand-coded models that do not scale up to real-life challenges by learning from data. Given these complementary strengths, we naturally ask if DS and FS can address each other’s limitations. Two recent strands of research are bringing DS closer to meeting core FS challenges. One strand attempts to model compositionality with DS methods, representing both primitive and composed linguistic expressions as distributional vectors (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Guevara, 2010; Mitchell and Lapata, 2010). The other strand attempts to reformulate FS’s notion of logical inference in terms that DS can capture (Erk, 2009; Geffet and Dagan, 2005; Kotlerman et al., 2010; Zhitomirsky-Geffet and Dagan, 2010). In keeping with the lexical emphasis of DS, this strand has focused on inference at the word level, or lexical entailment, that is, discovering from distributional vectors of hyponyms (dog) that they entail their hypernyms (animal). This paper brings these two strands of research together by demonstrating two ways in wh"
E12-1004,W07-1427,0,0.0641273,"Missing"
E12-1004,J90-1003,0,0.0700654,"ord phrases and single words, and more precisely to: those AN and QN sequences that are in the data sets (see next subsections), the adjectives, quantifiers and nouns contained in those sequences, and the most frequent (9.8K) nouns and (8.1K) adjectives in the corpus. The first step is to count the content words (more precisely, the most frequent 9.8K nouns, 8.1K adjectives, and 9.6K verbs in the corpus) that occur in the same sentence as phrases of interest. In the second step, following standard practice, the co-occurrence counts are converted into pointwise mutual information (PMI) scores (Church and Hanks, 1990). The result of this step is a sparse matrix (with both positive and negative entries) with 48K rows (one per phrase of interest) and 27K columns (one per content word). 3.2 The AN |= N data set To characterize entailment between nouns using their semantic vectors, we need data exemplifying which noun entails which. This section introduces one cheap way to collect such a training data set exploiting semantic vectors for composed expressions, namely AN sequences. We rely on the linguistic fact that ANs share a syntactic category and semantic type with plain common nouns (big cat shares syntacti"
E12-1004,W09-3711,0,0.101211,"o real-life challenges by learning from data. Given these complementary strengths, we naturally ask if DS and FS can address each other’s limitations. Two recent strands of research are bringing DS closer to meeting core FS challenges. One strand attempts to model compositionality with DS methods, representing both primitive and composed linguistic expressions as distributional vectors (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Guevara, 2010; Mitchell and Lapata, 2010). The other strand attempts to reformulate FS’s notion of logical inference in terms that DS can capture (Erk, 2009; Geffet and Dagan, 2005; Kotlerman et al., 2010; Zhitomirsky-Geffet and Dagan, 2010). In keeping with the lexical emphasis of DS, this strand has focused on inference at the word level, or lexical entailment, that is, discovering from distributional vectors of hyponyms (dog) that they entail their hypernyms (animal). This paper brings these two strands of research together by demonstrating two ways in which the distributional vectors of composite expressions bear on inference. Here we focus on phrasal vectors harvested directly from the corpus rather than obtained compositionally. In a first"
E12-1004,P05-1014,0,0.850495,"challenges by learning from data. Given these complementary strengths, we naturally ask if DS and FS can address each other’s limitations. Two recent strands of research are bringing DS closer to meeting core FS challenges. One strand attempts to model compositionality with DS methods, representing both primitive and composed linguistic expressions as distributional vectors (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Guevara, 2010; Mitchell and Lapata, 2010). The other strand attempts to reformulate FS’s notion of logical inference in terms that DS can capture (Erk, 2009; Geffet and Dagan, 2005; Kotlerman et al., 2010; Zhitomirsky-Geffet and Dagan, 2010). In keeping with the lexical emphasis of DS, this strand has focused on inference at the word level, or lexical entailment, that is, discovering from distributional vectors of hyponyms (dog) that they entail their hypernyms (animal). This paper brings these two strands of research together by demonstrating two ways in which the distributional vectors of composite expressions bear on inference. Here we focus on phrasal vectors harvested directly from the corpus rather than obtained compositionally. In a first experiment, we exploit t"
E12-1004,D11-1129,0,0.0134247,"mited to the lexical domain. On the other hand, FS has provided sophisticated models of sentence meaning, but it has been largely limited to hand-coded models that do not scale up to real-life challenges by learning from data. Given these complementary strengths, we naturally ask if DS and FS can address each other’s limitations. Two recent strands of research are bringing DS closer to meeting core FS challenges. One strand attempts to model compositionality with DS methods, representing both primitive and composed linguistic expressions as distributional vectors (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Guevara, 2010; Mitchell and Lapata, 2010). The other strand attempts to reformulate FS’s notion of logical inference in terms that DS can capture (Erk, 2009; Geffet and Dagan, 2005; Kotlerman et al., 2010; Zhitomirsky-Geffet and Dagan, 2010). In keeping with the lexical emphasis of DS, this strand has focused on inference at the word level, or lexical entailment, that is, discovering from distributional vectors of hyponyms (dog) that they entail their hypernyms (animal). This paper brings these two strands of research together by demonstrating two ways in which the distributional vectors of"
E12-1004,W10-2805,0,0.0908867,"e other hand, FS has provided sophisticated models of sentence meaning, but it has been largely limited to hand-coded models that do not scale up to real-life challenges by learning from data. Given these complementary strengths, we naturally ask if DS and FS can address each other’s limitations. Two recent strands of research are bringing DS closer to meeting core FS challenges. One strand attempts to model compositionality with DS methods, representing both primitive and composed linguistic expressions as distributional vectors (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Guevara, 2010; Mitchell and Lapata, 2010). The other strand attempts to reformulate FS’s notion of logical inference in terms that DS can capture (Erk, 2009; Geffet and Dagan, 2005; Kotlerman et al., 2010; Zhitomirsky-Geffet and Dagan, 2010). In keeping with the lexical emphasis of DS, this strand has focused on inference at the word level, or lexical entailment, that is, discovering from distributional vectors of hyponyms (dog) that they entail their hypernyms (animal). This paper brings these two strands of research together by demonstrating two ways in which the distributional vectors of composite expre"
E12-1004,C92-2082,0,0.612514,"notion just described, whereas the entailment relations |=N and |=QP among nouns and quantifier phrases are the inclusion relations among sets of entities and sets of sets of entities respectively. Our results in Section 5 show that DS needs to treat |=N and |=QP differently as well. Empirical, corpus-based perspectives on entailment Until recently, the corpus-based research tradition has studied entailment mostly at the word level, with applied goals such as classifying lexical relations and building taxonomic WordNet-like resources automatically. The most popular approach, first adopted by Hearst (1992), extracts lexical relations from patterns in large corpora. For instance, from the pattern N1 such as N2 one learns that N2 |= N1 (from insects such as beetles, derive beetles |= insects). Several studies have refined and extended this approach (Pantel and Ravichandran, 2004; Snow et al., 2005; Snow et al., 2006; Turney, 2008). While empirically very successful, the patternbased method is mostly limited to single content words (or frequent content-word phrases). We are interested in entailment between phrases, where it is not obvious how to use lexico-syntactic patterns and cope with data spa"
E12-1004,N04-1041,0,0.0124637,"|=QP differently as well. Empirical, corpus-based perspectives on entailment Until recently, the corpus-based research tradition has studied entailment mostly at the word level, with applied goals such as classifying lexical relations and building taxonomic WordNet-like resources automatically. The most popular approach, first adopted by Hearst (1992), extracts lexical relations from patterns in large corpora. For instance, from the pattern N1 such as N2 one learns that N2 |= N1 (from insects such as beetles, derive beetles |= insects). Several studies have refined and extended this approach (Pantel and Ravichandran, 2004; Snow et al., 2005; Snow et al., 2006; Turney, 2008). While empirically very successful, the patternbased method is mostly limited to single content words (or frequent content-word phrases). We are interested in entailment between phrases, where it is not obvious how to use lexico-syntactic patterns and cope with data sparsity. For instance, it seems hard to find a pattern that frequently connects one QP to another it entails, as in all beetles PATTERN many beetles. Hence, we aim to find a more general method and investigate whether DS vectors (whether corpus-harvested or compositionally deri"
E12-1004,2003.mtsummit-papers.42,0,0.0525299,"Missing"
E12-1004,P06-1101,0,0.122522,"perspectives on entailment Until recently, the corpus-based research tradition has studied entailment mostly at the word level, with applied goals such as classifying lexical relations and building taxonomic WordNet-like resources automatically. The most popular approach, first adopted by Hearst (1992), extracts lexical relations from patterns in large corpora. For instance, from the pattern N1 such as N2 one learns that N2 |= N1 (from insects such as beetles, derive beetles |= insects). Several studies have refined and extended this approach (Pantel and Ravichandran, 2004; Snow et al., 2005; Snow et al., 2006; Turney, 2008). While empirically very successful, the patternbased method is mostly limited to single content words (or frequent content-word phrases). We are interested in entailment between phrases, where it is not obvious how to use lexico-syntactic patterns and cope with data sparsity. For instance, it seems hard to find a pattern that frequently connects one QP to another it entails, as in all beetles PATTERN many beetles. Hence, we aim to find a more general method and investigate whether DS vectors (whether corpus-harvested or compositionally derived) encode the information needed to"
E12-1004,C08-1114,0,0.0120661,"ailment Until recently, the corpus-based research tradition has studied entailment mostly at the word level, with applied goals such as classifying lexical relations and building taxonomic WordNet-like resources automatically. The most popular approach, first adopted by Hearst (1992), extracts lexical relations from patterns in large corpora. For instance, from the pattern N1 such as N2 one learns that N2 |= N1 (from insects such as beetles, derive beetles |= insects). Several studies have refined and extended this approach (Pantel and Ravichandran, 2004; Snow et al., 2005; Snow et al., 2006; Turney, 2008). While empirically very successful, the patternbased method is mostly limited to single content words (or frequent content-word phrases). We are interested in entailment between phrases, where it is not obvious how to use lexico-syntactic patterns and cope with data sparsity. For instance, it seems hard to find a pattern that frequently connects one QP to another it entails, as in all beetles PATTERN many beetles. Hence, we aim to find a more general method and investigate whether DS vectors (whether corpus-harvested or compositionally derived) encode the information needed to account for phr"
E12-1004,C04-1146,0,0.885377,"Missing"
E12-1004,W07-1412,0,0.0147927,"ing common sense into account. Instead of a relation from the truth of the consequent to the truth of the antecedent in any circumstance, the applied view looks at entailment in terms of plausibility: φ |= ψ if a human who reads (and trusts) φ would most likely infer that ψ is also true. Entailment systems have been compared under this new perspective in various evaluation campaigns, the best known being the Recognizing Textual Entailment (RTE) initiative (Dagan et al., 2009). Most RTE systems are based on advanced NLP components, machine learning techniques, and/or syntactic transformations (Zanzotto et al., 2007; Kouleykov and Magnini, 2005). A few systems exploit deep FS analysis (Bos and Markert, 2006; Chambers et al., 2007). In particular, the FS results about QP properties that affect entailment have been exploited by Chambers et al, who complement a core broad-coverage system with a Natural Logic module to trade lower recall for higher precision. For instance, they exploit the monotonicity properties of no that cause the following reversal in entailment direction: some beetles |= some insects but no insects |= no beetles. To investigate entailment step by step, we address here a much simpler and"
E12-1004,W07-1400,0,\N,Missing
E12-1004,J09-3004,0,\N,Missing
E14-1046,D10-1115,1,0.965284,"sentences. In order to achieve this goal, the principle of compositionality of linguistic structures, which states that complex linguistic structures can be formed through composition of simple elements, is applied to distributional vectors. Therefore, in recent years, the problem of composition within distributional models has caught many researchers’ attention (Clark, 2013; Erk, 2012). A number of compositional frameworks have been proposed and tested. Mitchell and Lapata (2008) propose a set of simple component-wise operations, such as multiplication and addition. Later, Guevara (2010) and Baroni and Zamparelli (2010) proposed more elaborate methods, in which composition is modeled as matrix-vector multiplication operations. Particularly new to their approach is the proposal to estimate model parameters by minimizing the distance of the composed vectors to corpus-observed phrase vectors. For example, Baroni and Zamparelli (2010) consider the case of Adjective-Noun composition and model it as matrix-vector multiplication: adjective matrices are parameters to be estimated and nouns are cooccurrence vectors. The model parameter estimation procedure becomes a multiple response multivariate regression problem."
E14-1046,P13-2010,1,0.716204,"th noun-noun compounds and adjective-noun phrases (ANs). We focus on the latter, and we frame the task as in Dinu et al. (2013). The dataset contains 620 ANs, each paired with a single-noun paraphrase. Examples include: upper side/upside, false belief/fallacy and electric refrigerator/fridge. We evaluate a model by computing the cosine of all 20K nouns in our semantic space with the target AN, and looking at the rank of the correct paraphrase in this list. The lower the rank, the better the model. We report median rank across the test items. Determiner phrases The third dataset, introduced in Bernardi et al. (2013), focuses on a class of determiner words. It is a multiplechoice test where target nouns (e.g., omniscience) must be matched with the most closely related determiner(-noun) phrases (DPs) (e.g., all knowledge). There are 173 target nouns in total, each paired with one correct DP response, as well as 5 foils, namely the determiner (all) and noun (knowledge) from the correct response and three more DPs, two of which contain the same noun as the correct phrase (much knowledge, some knowledge), the third the same determiner (all preliminaries). Other examples of targets/related-phrases are quatrain"
E14-1046,P04-1003,0,0.093174,"Missing"
E14-1046,P08-1028,0,0.265504,"only deal with isolated words, and there is a strong need to construct representations for longer linguistic structures such as phrases and sentences. In order to achieve this goal, the principle of compositionality of linguistic structures, which states that complex linguistic structures can be formed through composition of simple elements, is applied to distributional vectors. Therefore, in recent years, the problem of composition within distributional models has caught many researchers’ attention (Clark, 2013; Erk, 2012). A number of compositional frameworks have been proposed and tested. Mitchell and Lapata (2008) propose a set of simple component-wise operations, such as multiplication and addition. Later, Guevara (2010) and Baroni and Zamparelli (2010) proposed more elaborate methods, in which composition is modeled as matrix-vector multiplication operations. Particularly new to their approach is the proposal to estimate model parameters by minimizing the distance of the composed vectors to corpus-observed phrase vectors. For example, Baroni and Zamparelli (2010) consider the case of Adjective-Noun composition and model it as matrix-vector multiplication: adjective matrices are parameters to be estim"
E14-1046,W13-3206,1,0.774374,"ods, in which composition is modeled as matrix-vector multiplication operations. Particularly new to their approach is the proposal to estimate model parameters by minimizing the distance of the composed vectors to corpus-observed phrase vectors. For example, Baroni and Zamparelli (2010) consider the case of Adjective-Noun composition and model it as matrix-vector multiplication: adjective matrices are parameters to be estimated and nouns are cooccurrence vectors. The model parameter estimation procedure becomes a multiple response multivariate regression problem. This method, that, following Dinu et al. (2013) and others, we term the lexical function composition model, can also be generalized to more complex structures such as 3rd order tensors for modeling transitive verbs (Grefenstette et al., 2013). Socher et al. (2012) proposed a more complex and flexible framework based on matrix-vector representations. Each word or lexical node in a parsing tree is assigned a vector (representing inherent meaning of the constituent) and a matrix (controlling the behavior to modify the meaning of In this paper, we show that the lexical function model for composition of distributional semantic vectors can be im"
E14-1046,D12-1110,0,0.484582,"corpus-observed phrase vectors. For example, Baroni and Zamparelli (2010) consider the case of Adjective-Noun composition and model it as matrix-vector multiplication: adjective matrices are parameters to be estimated and nouns are cooccurrence vectors. The model parameter estimation procedure becomes a multiple response multivariate regression problem. This method, that, following Dinu et al. (2013) and others, we term the lexical function composition model, can also be generalized to more complex structures such as 3rd order tensors for modeling transitive verbs (Grefenstette et al., 2013). Socher et al. (2012) proposed a more complex and flexible framework based on matrix-vector representations. Each word or lexical node in a parsing tree is assigned a vector (representing inherent meaning of the constituent) and a matrix (controlling the behavior to modify the meaning of In this paper, we show that the lexical function model for composition of distributional semantic vectors can be improved by adopting a more advanced regression technique. We use the pathwise coordinate-descent optimized elastic-net regression method to estimate the composition parameters, and compare the resulting model with seve"
E14-1046,C10-1142,0,0.0245499,"odel for prediction. Besides better prediction accuracy, the elastic-net method also brings interpretability to the composition procedure through sparsity constraints on the model. The rest of this paper is organized as follows: In Section 2, we give details on the above-mentioned composition models, which will be used for comparison in our experiments. In Section 3, we describe the pathwise optimized elastic-net regression algorithm. Experimental evaluation on three composition tasks is provided in Section 4. In Section 5 we conclude and suggest directions for future work. Guevara (2010) and Zanzotto et al. (2010) explore a full form of the additive model (Fulladd), where the two vectors entering a composition process are pre-multiplied by weight matrices before being added, so that each output component is a weighted sum of all input components: p = W1 u + W2v . Baroni and Zamparelli (2010) and Coecke et al. (2010), taking inspiration from formal semantics, characterize composition as function application. For example, Baroni and Zamparelli model adjective-noun phrases by treating the adjective as a regression function from nouns onto (modified) nouns. Given that linear functions can be expressed b"
E14-1046,W13-0112,0,0.101338,"of the composed vectors to corpus-observed phrase vectors. For example, Baroni and Zamparelli (2010) consider the case of Adjective-Noun composition and model it as matrix-vector multiplication: adjective matrices are parameters to be estimated and nouns are cooccurrence vectors. The model parameter estimation procedure becomes a multiple response multivariate regression problem. This method, that, following Dinu et al. (2013) and others, we term the lexical function composition model, can also be generalized to more complex structures such as 3rd order tensors for modeling transitive verbs (Grefenstette et al., 2013). Socher et al. (2012) proposed a more complex and flexible framework based on matrix-vector representations. Each word or lexical node in a parsing tree is assigned a vector (representing inherent meaning of the constituent) and a matrix (controlling the behavior to modify the meaning of In this paper, we show that the lexical function model for composition of distributional semantic vectors can be improved by adopting a more advanced regression technique. We use the pathwise coordinate-descent optimized elastic-net regression method to estimate the composition parameters, and compare the res"
E14-1046,W10-2805,0,0.284435,"uch as phrases and sentences. In order to achieve this goal, the principle of compositionality of linguistic structures, which states that complex linguistic structures can be formed through composition of simple elements, is applied to distributional vectors. Therefore, in recent years, the problem of composition within distributional models has caught many researchers’ attention (Clark, 2013; Erk, 2012). A number of compositional frameworks have been proposed and tested. Mitchell and Lapata (2008) propose a set of simple component-wise operations, such as multiplication and addition. Later, Guevara (2010) and Baroni and Zamparelli (2010) proposed more elaborate methods, in which composition is modeled as matrix-vector multiplication operations. Particularly new to their approach is the proposal to estimate model parameters by minimizing the distance of the composed vectors to corpus-observed phrase vectors. For example, Baroni and Zamparelli (2010) consider the case of Adjective-Noun composition and model it as matrix-vector multiplication: adjective matrices are parameters to be estimated and nouns are cooccurrence vectors. The model parameter estimation procedure becomes a multiple response"
J10-4006,W04-3221,0,0.0241765,"ss 1996; Schutze 1997; Bullinaria and Levy 2007; Pado´ and Lapata 2007). These collocates are seen as proxies for various attributes of the concepts that the words denote. Words that share many collocates denote concepts that share many attributes. Both dog and puppy may occur near owner, leash, and bark, because these words denote properties that are shared by dogs and puppies. The attributional similarity between dog and puppy, as approximated by their contextual similarity, will be very high. DSMs succeed in tasks like synonym detection (Landauer and Dumais 1997) or concept categorization (Almuhareb and Poesio 2004) because such tasks require a measure of attributional similarity that favors concepts that share many properties, such as synonyms and co-hyponyms. However, many other tasks require detecting different kinds of semantic similarity. Turney (2006b) deﬁnes relational similarity as the property shared by pairs of words (e.g, dog–animal and car–vehicle) linked by similar semantic relations (e.g., hypernymy), despite the fact that the words in one pair might not be attributionally similar to those in the other pair (e.g., dog is not attributionally similar to car, nor is animal to vehicle). Turney"
J10-4006,J09-2001,0,0.00850195,"Missing"
J10-4006,W09-0201,1,0.731884,"Missing"
J10-4006,P06-1127,0,0.0061999,"Missing"
J10-4006,J90-1003,0,0.292435,"tive subspace with the example pairs n, qr  as unique non-zero dimensions, and a negative subspace with nonzero dimensions corresponding to all w1 , w2  pairs such that w1 is one of the training nominal concepts, and w2 is not a quale qr in the example pairs. We then measure the length of each link in both subspaces. For example, we measure the length of the obj link in a subspace characterized by n, qtelic  example pairs, and the length of obj in a subspace characterized by n, w2  pairs that are probably not Telic examples. We compute the pointwise mutual information (PMI) statistic (Church and Hanks 1990) on these lengths to ﬁnd the links that are most typical of the positive subspace corresponding to each qualia role. PMI, with respect to other association measures, ﬁnds more speciﬁc links, which is good for our purposes. However, it is also notoriously 709 Computational Linguistics Volume 36, Number 4 ¨ prone to over-estimating the importance of rare items (Manning and Schutze 1999, Chapter 5). Thus, before selecting the top 20 links ranked by PMI, we ﬁlter out those links that do not have at least 10 non-zero dimensions in the positive subspace. Many parameters here should be tuned more sys"
J10-4006,P07-1112,0,0.0657599,"istributional approaches. For instance, the notion of property plays a key role in cognitive science and linguistics, which both typically represent concepts as clusters of properties (Jackendoff 1990; Murphy 2002). In this case, the task is not to ﬁnd out that dog is similar to puppy or cat, but that it has a tail, it is used for hunting, and so on. Almuhareb (2006), Baroni and Lenci (2008), and Baroni et al. (2010) use the words co-occurring with a noun to approximate its most prototypical properties and correlate distributionally derived data with the properties produced by human subjects. Cimiano and Wenderoth (2007) instead focus on that subset of noun properties known in lexical semantics as qualia roles (Pustejovsky 1995), and use lexical patterns to identify, for example, the constitutive parts of a concept or its function (this is in turn analogous to the problem of relation extraction). The distributional semantics methodology also extends to more complex aspects of word meaning, addressing issues such as verb selectional preferences (Erk 2007), argument alternations (Merlo and Stevenson 2001; Joanis, Stevenson, and James 2008), event types (Zarcone and Lenci 2008), and so forth. Finally, some DSMs"
J10-4006,W02-0908,0,0.10217,"mes 2008), event types (Zarcone and Lenci 2008), and so forth. Finally, some DSMs capture 674 Baroni and Lenci Distributional Memory a sort of “topical” relatedness between words: They might ﬁnd, for example, a relation between dog and ﬁdelity. Topical relatedness, addressed by DSMs based on document distributions such as LSA (Landauer and Dumais 1997) and Topic Models (Grifﬁths, Steyvers, and Tenenbaum 2007), is not further discussed in this article. DSMs have found wide applications in computational lexicography, especially for automatic thesaurus construction (Grefenstette 1994; Lin 1998a; Curran and Moens 2002; Kilgarriff et al. 2004; Rapp 2004). Corpus-based semantic models have also attracted the attention of lexical semanticists as a way to provide the notion of synonymy with a more robust empirical foundation (Geeraerts 2010; Heylen et al. 2008). Moreover, DSMs for attributional and relational similarity are widely used for the semi-automatic bootstrapping or extension of terminological repositories, computational lexicons (e.g., WordNet), and ontologies (Buitelaar, Cimiano, and Magnini 2005; Lenci 2010). Innovative applications of corpus-based semantics are also being explored in linguistics,"
J10-4006,P08-1027,0,0.0416155,"Missing"
J10-4006,P08-1079,0,0.0243438,"Missing"
J10-4006,J93-1003,0,0.0930495,"Missing"
J10-4006,P07-1028,0,0.0923357,"o approximate its most prototypical properties and correlate distributionally derived data with the properties produced by human subjects. Cimiano and Wenderoth (2007) instead focus on that subset of noun properties known in lexical semantics as qualia roles (Pustejovsky 1995), and use lexical patterns to identify, for example, the constitutive parts of a concept or its function (this is in turn analogous to the problem of relation extraction). The distributional semantics methodology also extends to more complex aspects of word meaning, addressing issues such as verb selectional preferences (Erk 2007), argument alternations (Merlo and Stevenson 2001; Joanis, Stevenson, and James 2008), event types (Zarcone and Lenci 2008), and so forth. Finally, some DSMs capture 674 Baroni and Lenci Distributional Memory a sort of “topical” relatedness between words: They might ﬁnd, for example, a relation between dog and ﬁdelity. Topical relatedness, addressed by DSMs based on document distributions such as LSA (Landauer and Dumais 1997) and Topic Models (Grifﬁths, Steyvers, and Tenenbaum 2007), is not further discussed in this article. DSMs have found wide applications in computational lexicography, esp"
J10-4006,J06-1005,0,0.00554731,"Missing"
J10-4006,S07-1003,0,0.00910646,"Missing"
J10-4006,C92-2082,0,0.506399,"t them in the corpus. Pairs of words that are connected by similar patterns probably hold similar relations, that is, they are relationally similar. For example, we can hypothesize that dog–tail is more similar to car–wheel than to dog–animal, because the patterns connecting dog and tail (of, have, etc.) are more like those of car–wheel than like those of dog–animal (is a, such as, etc.). Turney uses the relational space to implement tasks such as solving analogies and harvesting instances of relations. Although they are not explicitly expressed in these terms, relation extraction algorithms (Hearst 1992, 1998; Girju, Badulescu, and Moldovan 2006; Pantel and Pennacchiotti 2006) also rely on relational similarity, and focus on learning one relation type at a time (e.g., ﬁnding parts). Although semantic similarity, either attributional or relational, has the lion’s share in DSMs, similarity is not the only aspect of meaning that is addressed by distributional approaches. For instance, the notion of property plays a key role in cognitive science and linguistics, which both typically represent concepts as clusters of properties (Jackendoff 1990; Murphy 2002). In this case, the task is not to ﬁnd"
J10-4006,W09-0205,1,0.793259,"rney’s model. For instance, language is full of productive semantic phenomena, such as the selectional preferences of verbs with respect to unseen arguments (eating topinambur vs. eating sympathy). Predicting the plausibility of unseen pairs cannot, by deﬁnition, be tackled by the current version of PairClass, which will have to be expanded to deal with such cases, perhaps adopting ideas similar to those we present (that are, in turn, inspired by Turney’s own work on attributional and relational similarity). A ﬁrst step in this direction, within a framework ˇ similar to Turney’s, was taken by Herdagdelen and Baroni (2009). Turney (2007) explicitly formalizes the set of corpus-extracted word–link–word triples as a tensor, and was our primary source of inspiration in formalizing DM in these terms. The focus of Turney’s article, however, is on dimensionality reduction techniques applied to tensors, and the application to corpora is only brieﬂy discussed. Moreover, Turney only derives the W1×LW2 space from the tensor, and does not discuss the possibility of using the tensor-based formalization to unify different views of semantic data, which is instead our main point. The higher-order tensor dimensionality reducti"
J10-4006,W09-3207,1,0.583088,"Missing"
J10-4006,heylen-etal-2008-modelling,0,0.00902465,"opical relatedness, addressed by DSMs based on document distributions such as LSA (Landauer and Dumais 1997) and Topic Models (Grifﬁths, Steyvers, and Tenenbaum 2007), is not further discussed in this article. DSMs have found wide applications in computational lexicography, especially for automatic thesaurus construction (Grefenstette 1994; Lin 1998a; Curran and Moens 2002; Kilgarriff et al. 2004; Rapp 2004). Corpus-based semantic models have also attracted the attention of lexical semanticists as a way to provide the notion of synonymy with a more robust empirical foundation (Geeraerts 2010; Heylen et al. 2008). Moreover, DSMs for attributional and relational similarity are widely used for the semi-automatic bootstrapping or extension of terminological repositories, computational lexicons (e.g., WordNet), and ontologies (Buitelaar, Cimiano, and Magnini 2005; Lenci 2010). Innovative applications of corpus-based semantics are also being explored in linguistics, for instance in the study of semantic change (Sagi, Kaufmann, and Clark 2009), lexical variation (Peirsman and Speelman 2009), and for the analysis of multiword expressions (Alishahi and Stevenson 2008). The wealth and variety of semantic issue"
J10-4006,P98-2127,0,0.334913,"son, and James 2008), event types (Zarcone and Lenci 2008), and so forth. Finally, some DSMs capture 674 Baroni and Lenci Distributional Memory a sort of “topical” relatedness between words: They might ﬁnd, for example, a relation between dog and ﬁdelity. Topical relatedness, addressed by DSMs based on document distributions such as LSA (Landauer and Dumais 1997) and Topic Models (Grifﬁths, Steyvers, and Tenenbaum 2007), is not further discussed in this article. DSMs have found wide applications in computational lexicography, especially for automatic thesaurus construction (Grefenstette 1994; Lin 1998a; Curran and Moens 2002; Kilgarriff et al. 2004; Rapp 2004). Corpus-based semantic models have also attracted the attention of lexical semanticists as a way to provide the notion of synonymy with a more robust empirical foundation (Geeraerts 2010; Heylen et al. 2008). Moreover, DSMs for attributional and relational similarity are widely used for the semi-automatic bootstrapping or extension of terminological repositories, computational lexicons (e.g., WordNet), and ontologies (Buitelaar, Cimiano, and Magnini 2005; Lenci 2010). Innovative applications of corpus-based semantics are also being e"
J10-4006,J01-3003,0,0.025609,"l properties and correlate distributionally derived data with the properties produced by human subjects. Cimiano and Wenderoth (2007) instead focus on that subset of noun properties known in lexical semantics as qualia roles (Pustejovsky 1995), and use lexical patterns to identify, for example, the constitutive parts of a concept or its function (this is in turn analogous to the problem of relation extraction). The distributional semantics methodology also extends to more complex aspects of word meaning, addressing issues such as verb selectional preferences (Erk 2007), argument alternations (Merlo and Stevenson 2001; Joanis, Stevenson, and James 2008), event types (Zarcone and Lenci 2008), and so forth. Finally, some DSMs capture 674 Baroni and Lenci Distributional Memory a sort of “topical” relatedness between words: They might ﬁnd, for example, a relation between dog and ﬁdelity. Topical relatedness, addressed by DSMs based on document distributions such as LSA (Landauer and Dumais 1997) and Topic Models (Grifﬁths, Steyvers, and Tenenbaum 2007), is not further discussed in this article. DSMs have found wide applications in computational lexicography, especially for automatic thesaurus construction (Gre"
J10-4006,D08-1095,0,0.00529955,"Missing"
J10-4006,P08-1028,0,0.65488,"onality and similar issues in DSMs lie beyond the scope of this paper. However, there is nothing in DM that prevents it from interacting with any of the research directions we have mentioned here. Indeed, we believe that the generalized nature of DM represents a precondition for distributional semantics to be able to satisfactorily address these more advanced challenges. A multi-purpose, distributional semantic resource like DM can allow researchers to focus on the next steps of semantic modeling. These include compositionality, but also modulating word meaning in context (Erk and Pado´ 2008; Mitchell and Lapata 2008) and ﬁnding ways to embed the distributional memory in complex NLP systems (e.g., for question answering or textual entailment) or even embodied agents and robots. DM-style triples predicating a relation between two entities are common currency in many semantic representation models (e.g., semantic networks) and knowledgeexchange formalisms such as RDF. This might also pave the way to the integration of corpus-based information with other knowledge sources. It is hard to see how such integration could be pursued within generalized systems, such as PairClass (Turney 2008), that require keeping"
J10-4006,E09-1071,0,0.0662303,"on unfolds the tensor into a matrix with the n-th index indexing the rows of the matrix and a column for each pair of elements from the other two tensor indices. For example, the mode-1 matricization of the tensor in Table 2 results in a matrix with the entries vertically arranged as they are in the table, but replacing the second and third indices with a single index ranging from 1 to 6 (cf. matrix A of Table 3). More explicitly, in mode-n matricization we map each tensor entry (i1 , i2 , ..., iN ) to matrix entry (in , j), where j is computed as in Equation (1), adapted from Kolda and Bader (2009). N  j=1+ k =1 k=n ((ik − 1) k −1 Dm ) (1) m= 1 m=n For example, if we apply mode-1 matricization to the tensor of dimensionality 3 × 2 × 3 in Table 2, we obtain the matrix A3×6 in Table 3 (ignore the labels for now). The tensor entry x3,1,1 is mapped to the matrix cell a3,1 ; x3,2,3 is mapped to a3,6 ; and x1,2,2 is mapped to a1,4 . Observe that each column of the matrix is a mode-1 ﬁber of the tensor: The ﬁrst column is the x∗11 ﬁber; the second column is the x∗21 ﬁber, and so on. Matricization has various mathematically interesting properties and practical applications in computations i"
J10-4006,J07-2002,0,0.461166,"Missing"
J10-4006,D07-1042,0,0.00556845,"Missing"
J10-4006,P06-1015,0,0.0533181,"by similar patterns probably hold similar relations, that is, they are relationally similar. For example, we can hypothesize that dog–tail is more similar to car–wheel than to dog–animal, because the patterns connecting dog and tail (of, have, etc.) are more like those of car–wheel than like those of dog–animal (is a, such as, etc.). Turney uses the relational space to implement tasks such as solving analogies and harvesting instances of relations. Although they are not explicitly expressed in these terms, relation extraction algorithms (Hearst 1992, 1998; Girju, Badulescu, and Moldovan 2006; Pantel and Pennacchiotti 2006) also rely on relational similarity, and focus on learning one relation type at a time (e.g., ﬁnding parts). Although semantic similarity, either attributional or relational, has the lion’s share in DSMs, similarity is not the only aspect of meaning that is addressed by distributional approaches. For instance, the notion of property plays a key role in cognitive science and linguistics, which both typically represent concepts as clusters of properties (Jackendoff 1990; Murphy 2002). In this case, the task is not to ﬁnd out that dog is similar to puppy or cat, but that it has a tail, it is used"
J10-4006,W09-0202,0,0.0117113,"on of lexical semanticists as a way to provide the notion of synonymy with a more robust empirical foundation (Geeraerts 2010; Heylen et al. 2008). Moreover, DSMs for attributional and relational similarity are widely used for the semi-automatic bootstrapping or extension of terminological repositories, computational lexicons (e.g., WordNet), and ontologies (Buitelaar, Cimiano, and Magnini 2005; Lenci 2010). Innovative applications of corpus-based semantics are also being explored in linguistics, for instance in the study of semantic change (Sagi, Kaufmann, and Clark 2009), lexical variation (Peirsman and Speelman 2009), and for the analysis of multiword expressions (Alishahi and Stevenson 2008). The wealth and variety of semantic issues that DSMs are able to tackle conﬁrms the importance of looking at distributional data to explore meaning, as well as the maturity of this research ﬁeld. However, if we looked from a distance at the whole ﬁeld of DSMs we would see that, besides the general assumption shared by all models that information about the context of a word is an important key in grasping its meaning, the elements of difference overcome the commonalities. For instance, DSMs geared towards attributiona"
J10-4006,2003.mtsummit-papers.42,0,0.0856492,"rovided by Pado´ and Lapata (2007) is built around the notion of a matrix M|B|×|T |, with B the set of basis elements representing the contexts used to compare the distributional similarity of the target elements T. This binary structure is inherently suitable for approaches that represent distributional data in terms of unstructured co-occurrence relations between an element and a context. The latter can be either documents (Landauer and Dumais 1997; Grifﬁths, Steyvers, and Tenenbaum 2007) or lexical collocates within a certain distance from the ¨ target (Lund and Burgess 1996; Schutze 1997; Rapp 2003; Bullinaria and Levy 2007). We will refer to such models as unstructured DSMs, because they do not use the linguistic structure of texts to compute co-occurrences, and only record whether the target occurs 676 Baroni and Lenci Distributional Memory in or close to the context element, without considering the type of this relation. For instance, an unstructured DSM might derive from a sentence like The teacher eats a red apple that eat is a feature shared by apple and red, just because they appear in the same context window, without considering the fact that there is no real linguistic relation"
J10-4006,rapp-2004-freely,0,0.0104959,", and so forth. Finally, some DSMs capture 674 Baroni and Lenci Distributional Memory a sort of “topical” relatedness between words: They might ﬁnd, for example, a relation between dog and ﬁdelity. Topical relatedness, addressed by DSMs based on document distributions such as LSA (Landauer and Dumais 1997) and Topic Models (Grifﬁths, Steyvers, and Tenenbaum 2007), is not further discussed in this article. DSMs have found wide applications in computational lexicography, especially for automatic thesaurus construction (Grefenstette 1994; Lin 1998a; Curran and Moens 2002; Kilgarriff et al. 2004; Rapp 2004). Corpus-based semantic models have also attracted the attention of lexical semanticists as a way to provide the notion of synonymy with a more robust empirical foundation (Geeraerts 2010; Heylen et al. 2008). Moreover, DSMs for attributional and relational similarity are widely used for the semi-automatic bootstrapping or extension of terminological repositories, computational lexicons (e.g., WordNet), and ontologies (Buitelaar, Cimiano, and Magnini 2005; Lenci 2010). Innovative applications of corpus-based semantics are also being explored in linguistics, for instance in the study of semanti"
J10-4006,W09-0203,0,0.0984847,"Missing"
J10-4006,W09-0214,0,0.0677036,"Missing"
J10-4006,J06-2001,0,0.00767065,"Missing"
J10-4006,N03-1032,0,0.0154253,"Missing"
J10-4006,P06-1040,0,0.0818364,"li, University of Pisa, Via Santa Maria 36, 56126 Pisa (PI), Italy. E-mail: alessandro.lenci@ling.unipi.it. Submission received: 11 January 2010; revised submission received: 15 April 2010; accepted for publication: 1 June 2010. © 2010 Association for Computational Linguistics Computational Linguistics Volume 36, Number 4 as a function of the degree of overlap among their linguistic contexts. Conversely, the format of distributional representations greatly varies depending on the speciﬁc aspects of meaning they are designed to model. The most straightforward phenomenon tackled by DSMs is what Turney (2006b) calls attributional similarity, which encompasses standard taxonomic semantic relations such as synonymy, co-hyponymy, and hypernymy. Words like dog and puppy, for example, are attributionally similar in the sense that their meanings share a large number of attributes: They are animals, they bark, and so on. Attributional similarity is typically addressed by DSMs based on word collocates (Grefenstette 1994; Lund and ¨ Burgess 1996; Schutze 1997; Bullinaria and Levy 2007; Pado´ and Lapata 2007). These collocates are seen as proxies for various attributes of the concepts that the words denote"
J10-4006,J06-3003,0,0.626941,"of the target word pairs that are provided with the test set. The second data set (NS) comes from Nastase and Szpakowicz (2003). It pertains to the classiﬁcation of 600 modiﬁer–noun pairs and it is of interest because it proposes a very ﬁne-grained categorization into 30 semantic classes, such as Cause (cloud–storm), Purpose (album–picture), Location-At (pain–chest), Location-From (visitor–country), Frequency (superstition–occasional), Time-At (snack–midnight), and so on. The modiﬁers can be nouns, adjectives, or adverbs. Because the data set is not split into training and test data we follow Turney (2006b) and perform leave-one-out cross-validation. The data set also comes with a coarser ﬁve-way classiﬁcation. Our unreported results on it are comparable, in terms of relative performance, to the ones for the 30-way classiﬁcation. ´ The last data set (OC) contains 1,443 noun–noun compounds classiﬁed by O S´eaghdha and Copestake (2009) into 6 relations: Be (celebrity–winner), Have (door–latch), In (air–disaster), Actor (university–scholarship), Instrument ( freight–train), and About 701 Computational Linguistics Volume 36, Number 4 ´ S´eaghdha and Copestake (2009) and references there. We use th"
J10-4006,C08-1114,0,0.0445781,"of our tensor, with better tuple extraction and weighting techniques on one side, and better matrix manipulation and similarity measurement on the other. As long as the former operations result in data that can be arranged into a weighted tuple structure, and the latter procedures act on vectors, such innovations ﬁt into the DM framework and can be used to improve performance on tasks deﬁned on any space derivable from the DM tensor. Whereas the model proposed by Pado´ and Lapata (2007) is designed only to address tasks involving the measurement of the attributional similarity between words, Turney (2008) shares with DM the goal of unifying attributional and relational similarity under the same distributional model. He observes that tasks that are traditionally solved with an attributional similarity approach can be recast as relational similarity tasks. Instead of determining whether two words are, for example, synonymous by looking at the features they share, we can learn what the typical patterns are that connect synonym pairs when they co-occur (also known as, sometimes called, etc.), and make a decision about a potential synonym pair based on their occurrence in similar contexts. Given a"
J10-4006,W09-0211,0,0.0394864,"Missing"
J10-4006,veale-hao-2008-acquiring,0,0.0244473,"Missing"
J10-4006,C02-1114,0,0.00854843,"Missing"
J10-4006,zarcone-lenci-2008-computational,1,0.375361,"s produced by human subjects. Cimiano and Wenderoth (2007) instead focus on that subset of noun properties known in lexical semantics as qualia roles (Pustejovsky 1995), and use lexical patterns to identify, for example, the constitutive parts of a concept or its function (this is in turn analogous to the problem of relation extraction). The distributional semantics methodology also extends to more complex aspects of word meaning, addressing issues such as verb selectional preferences (Erk 2007), argument alternations (Merlo and Stevenson 2001; Joanis, Stevenson, and James 2008), event types (Zarcone and Lenci 2008), and so forth. Finally, some DSMs capture 674 Baroni and Lenci Distributional Memory a sort of “topical” relatedness between words: They might ﬁnd, for example, a relation between dog and ﬁdelity. Topical relatedness, addressed by DSMs based on document distributions such as LSA (Landauer and Dumais 1997) and Topic Models (Grifﬁths, Steyvers, and Tenenbaum 2007), is not further discussed in this article. DSMs have found wide applications in computational lexicography, especially for automatic thesaurus construction (Grefenstette 1994; Lin 1998a; Curran and Moens 2002; Kilgarriff et al. 2004;"
J10-4006,J09-3004,0,0.309913,"hat closely resembles our W1 W2×L space, whereas we tackle this task under the more standard W1×LW2 view. It is an open question whether there are principled ways to select the optimal space conﬁguration for a given semantic task. In this article, we limit ourselves to proving that each space derived through tensor matricization is semantically interesting in the sense that it provides the proper ground to address some semantic task. Feature selection/reweighting and dimensionality reduction have been shown to improve DSM performance. For instance, the feature bootstrapping method proposed by Zhitomirsky-Geffet and Dagan (2009) boosts the precision of a DSM in lexical entailment recognition. Even if these methods can be applied to DM as well, we did not use them in our experiments. The results presented subsequently should be regarded as a “baseline” performance that could be enhanced in future work by exploring various task-speciﬁc parameters (we will come back in the conclusion to the role of parameter tuning in DM). This is consistent with our current aim of focusing on the generality and adaptivity of DM, rather than on task-speciﬁc optimization. As a ﬁrst, important step in this latter direction, however, we co"
J10-4006,D08-1094,0,\N,Missing
J10-4006,W09-0208,0,\N,Missing
J10-4006,J13-3006,0,\N,Missing
J10-4006,C98-2122,0,\N,Missing
J15-1010,W13-0112,0,0.179789,"= TALL boy, T T  cat  F = g(K1 ( f (tall), f ( )), K2 ( f (boy), f (cat))) = TALL I, boy (19) Dual Space Model. We have until now applied the CC to linear CDSMs with the dot product as the final comparison operator (what we called K). The CC also holds for the effective Dual Space model of Turney (2012), which assumes that each word has two distributional representations, wd in “domain” space and wf in “function” space. The similarity of two phrases is directly computed as the geometric average of the separate similarities between the first and second words in both spaces. Even though 2 Grefenstette et al. (2013) first framed the Lexical Function in terms of tensor contraction. 171 Computational Linguistics Volume 41, Number 1 there is no explicit composition step, it is still possible to put the model in CC form. Take x = (x1 , x2 ) and its trivial decomposition. Define, for a word w with  vector repT resentations wd and wf : f (w) = wd w f . Define also K1 ( f (x1 ), f (y1 )) =  f (x1 ), f (y1 )F , √  K2 ( f (x2 ), f (y2 )) =  f (x2 ), f (y2 )F and g(a, b) to be ab. Then g(K 1 ( f (x1 ), f (y1 )), K2 ( f (x2 ), f (y2 ))) =   = xd1 xf 1 T , yd1 yf 1 T F · xd2 xf 2 T , yd2 yf 2 T"
J15-1010,W10-2805,0,0.0383526,"Missing"
J15-1010,N10-1146,1,0.755896,"Missing"
J15-1010,P08-1028,0,0.52287,"Missing"
J15-1010,C10-1142,1,0.901092,"Missing"
J16-2006,J90-1003,0,0.36722,"he degree of lexicalization of the phrase). We use PMI(ab) to refer P(ab|a) P(ab) to this “phrase cohesion” PMI. PMI(ab) = log( P(a) ·P(b) ) = log( P(b) ) depends on how often words a and b form a phrase of the relevant kind, e.g., how often adjective a modifies noun b, while PMI(a, b) is based on the probability of two words co-occurring in the relevant co-occurrence context (e.g., within an n-word window). Quantifying phrase cohesion with PMI(ab) has been the most common usage of PMI in computational linguistics outside distributional semantics ever since the seminal work on collocations by Church and Hanks (1990). Submission received: 18 June 2015; revised submission received: 16 November 2015; accepted for publication: 3 February 2016. doi:10.1162/COLI a 00250 © 2016 Association for Computational Linguistics Computational Linguistics Volume 42, Number 2 Table 1 Composition methods. is pointwise multiplication. α and β are scalar parameters, matrices X and Y represent syntactic relation slots (e.g., Adjective and Noun), matrix A stands for a functional word (e.g., the adjective in an adjective-noun construction), and ~c is a constant vector. model additive multiplicative weighted additive phrase vecto"
J16-2006,W13-3206,1,0.891006,"Missing"
J16-4003,2014.lilt-9.5,1,0.893455,"Missing"
J16-4003,P14-1023,1,0.773388,"Missing"
J16-4003,W11-2501,1,0.792201,"sibly evoke the alternative This is a cat?,” we ask the more intuitive question: “How plausible is the sentence This is not a dog, it is a cat?” The corresponding THERE context is There is no dog here, but there is a cat, with but added as it makes the sentence more natural. 4.2 Selecting Potential Alternatives As just discussed, our stimuli contain exactly two content words—a noun either in a predicative or in an existential negated position, and its potential alternative in a comparable position. To construct the noun pairs, we took as negated elements 50 randomly selected items from BLESS (Baroni and Lenci 2011), and paired them with potential alternatives from several sources. We picked alternatives from different sources in order to take a variety of relations that might affect alternativehood into account. However, we make no claim about the exhaustiveness of the phenomena we are considering, and we do not present theoretical conjectures about how lexical relations affect the likelihood of being a plausible alternative. One of the sources was WordNet,2 from which we extracted lists of cohyponyms, hyponyms, and hypernyms. For the 50 negated items, there are almost 4K WordNet cohyponyms, many of the"
J16-4003,D10-1115,1,0.875636,"Missing"
J16-4003,W13-3206,1,0.907722,"Missing"
J16-4003,W10-2803,0,0.0254291,"y symmetric (There is no cat here, but there is an animal is plausible, but ?There is no animal here, but there is a cat sounds odd). Inverted pairs will obviously be a challenge to the symmetric cosine measure. We expect that, once we enrich the data set with such cases, plain DS similarity will no longer suffice, and we will have to rely on supervised approaches that currently seem almost superfluous. Similarly, compositional methods were only very moderately useful to account for our current skeletal sentential contexts. Compositional, or, more generally, wordmeaning-in-context approaches (Erk 2010) will have a better chance to prove their worth if we extend the empirical base to include more informative contexts, and let longer constituents be under the scope of negation and/or in the alternative set (cf., There is no bachelor here, but there is a. . . married man, ??unmarried man). From an empirical point of view, it will of course also be important to study conversational negations in more natural set-ups than the highly controlled experiment we designed. From a theoretical point of view, the most pressing and interesting issue pertains to integrating what we observed empirically into"
J16-4003,D08-1094,0,0.0759667,"Missing"
J16-4003,J10-4007,0,0.0402758,"Missing"
J16-4003,S13-1001,0,0.259002,"defined a priori, attempting to mimic the logical properties of negation, rather than being induced from distributional data. Clark, Coecke, and Sadrzadeh (2008) explore the idea that sentences live in a space spanned by a single “truth-denoting” basis (~1), with the origin (~0) corresponding to “false.” A sentence like John likes Mary is represented by ~1 if the sentence is true, ~0 otherwise. In this framework, further elaborated by Coecke, Sadrzadeh, and Clark (2010), negation is elegantly modeled as a swap matrix. Related approaches have been presented by Preller and Sadrzadeh (2011) and Grefenstette (2013). All this work, however, is purely theoretical, and it is not clear how the proposed models would be implemented in practice. Moreover, treating negation as a swapping operator only makes sense in the abstract scenario of a vector space representing truth values. If vectors of sentences and other expressions are instead distributional in nature (e.g., representing distributions over possible contexts), it is far from clear that swapped vectors would capture linguistic negation. Indeed, Hermann, Grefenstette, and Blunsom (2013) argue that negation should not affect all dimensions in a vector,"
J16-4003,W10-2805,0,0.0245924,"y. We considered two other approaches to composition that we ruled out based on poor development set performance. These were: ignoring negation (e.g., using the same composition function for this/it is a hawk and this/it is not a hawk), and modeling negation as a separate composition step (e.g., deriving this is not a hawk from the application of two composition functions: not(this is(hawk))). We did not attempt to model here and, more importantly, but in the THERE context (there is no X here, but there is a Y). To estimate the function parameters (matrix weights), we followed the approach of Guevara (2010), Baroni and Zamparelli (2010), and Dinu, Pham, and Baroni (2013), as implemented in the DISSECT toolkit.11 Specifically, we extracted from our corpus distributional vectors for sufficiently frequent phrases matching the target template (e.g., this/it is cat), and estimated the corresponding function by optimizing (in the leastsquares sense) the mapping from the vectors of the nouns in these phrases (cat) onto the corpus-extracted phrase vectors. Theoretical and empirical justifications for this method can be found in Baroni, Bernardi, and Zamparelli (2014), or any of the articles mentioned he"
J16-4003,W13-3209,0,0.0677149,"Missing"
J16-4003,W10-3109,0,0.0755906,"Missing"
J16-4003,N15-1097,1,0.843147,". We can conclude, then, that word and phrase vectors co-exist in the same space and, at the same time, that they have, sensibly, different distributions: phrase vectors, because of their shared semantics, tend to be more similar to each other than word vectors are. 5.1.3 Supervised Regression. The distributional vectors of negated items and their alternatives (either noun or composed phrase vectors) were also fed to a supervised regression algorithm, in order to tune similarity to the specific factors that determine felicitous alternativehood. We explored all the neural-network techniques of Kruszewski and Baroni (2015) across a wide range of hyperparameters, but found, on the development set, that it was best to use support vector regression (Drucker et al. 1996) with an RBF kernel. We also exploited the development set to tune the hyperparameters of this model through a broad grid search, conducted separately for noun vs. phrase inputs and IT vs. THERE ratings. We had to decide how to merge the vectors representing a negated item and its candidate alternative in order to feed them together to the supervised regression algorithm. Following recent work that addressed the same problem in the context of superv"
J16-4003,Q15-1016,0,0.0385939,"Missing"
J16-4003,Q13-1015,0,0.0316184,"the broader context (e.g., emphasizing the container components of cup) by means of standard word-meaning-in-context methods (Erk and Pado´ 2008). 6. Discussion and Future Work This special issue addresses the integration of formal and distributional models. This is a challenging task, because the two strands of research rely on different research cultures, methodologies, and simplifying assumptions inevitable in any scientific work. Some studies on finding a unifying perspective have been carried out on both empirical and theoretical fronts (Garrette, Erk, and Mooney 2013; Grefenstette 2013; Lewis and Steedman 2013; McNally and Boleda, to appear). We started our work on this article with the firm conviction that distributional models can do more, inspiring and helping to tackle new tasks that are both theoretically interesting and empirically challenging. The pragmatic contribution of negation is just such a task. Our success at modeling alternatives under conversational negation shows that distributional models are immediately applicable to theoretically interesting problems beyond arguably trivial ones, such as identifying near-synonyms. Cosine in a distributional semantic space turned out to be a ver"
J16-4003,N13-1090,0,0.0977992,"Missing"
J16-4003,C14-1097,0,0.0551762,"Missing"
J16-4003,D12-1130,0,0.0501069,"Missing"
J16-4003,C14-1212,0,0.0178886,"as best to use support vector regression (Drucker et al. 1996) with an RBF kernel. We also exploited the development set to tune the hyperparameters of this model through a broad grid search, conducted separately for noun vs. phrase inputs and IT vs. THERE ratings. We had to decide how to merge the vectors representing a negated item and its candidate alternative in order to feed them together to the supervised regression algorithm. Following recent work that addressed the same problem in the context of supervised entailment detection with distributional vectors (Roller, Erk, and Boleda 2014; Weeds et al. 2014), we explored the options of concatenating and subtracting the input vectors. We picked the second strategy as it consistently produced better development set results across all settings. Note that subtraction, unlike concatenation, explicitly encodes the knowledge that we are comparing two feature vectors living in the same space, and what matters should be how the two vectors differ on a feature-by-feature basis. Furthermore, because supervision should zero in on the information that is not already captured by direct distributional similarity, we explored the possibility to train supervised"
J16-4003,P03-1018,0,0.0966414,"and cold). A survey of the relevant DS literature is provided by Mohammad et al. (2013). The consensus view is that contrasting words tend to occur in similar contexts (Mohammad et al. even 639 Computational Linguistics Volume 42, Number 4 propose a “distributional hypothesis of highly contrasting pairs” stating that highly contrasting pairs occur in similar contexts more often than non-contrasting word pairs). Thus, it is impossible to distinguish them from non-contrasting related words (e.g., synonyms) using standard distributional similarity measures, and ad hoc strategies must be devised. Widdows (2003) presented a pioneering study of explicit negation in DS. The assumption of that work is that negated word meanings should be orthogonal, which is to say that they should not share any common feature. Specifically, Widdows proposes a binary negation operator, NOT(A, B), which projects the vector representing A onto the orthogonal space of the B vector. In logical terms, this can be seen as conjunction with a negated predicate (A ∧ ¬B). The orthogonality assumption makes perfect sense for the information retrieval applications envisioned by Widdows (web NOT internet), but it is too strict to ch"
J16-4003,J13-3004,0,\N,Missing
marelli-etal-2014-sick,D11-1129,0,\N,Missing
marelli-etal-2014-sick,D10-1115,1,\N,Missing
marelli-etal-2014-sick,D12-1110,0,\N,Missing
marelli-etal-2014-sick,D08-1027,0,\N,Missing
marelli-etal-2014-sick,W07-1431,0,\N,Missing
marelli-etal-2014-sick,P08-1028,0,\N,Missing
marelli-etal-2014-sick,S12-1051,0,\N,Missing
marelli-etal-2014-sick,S14-2001,1,\N,Missing
marelli-etal-2014-sick,S13-2005,1,\N,Missing
marelli-etal-2014-sick,S12-1053,1,\N,Missing
N15-1016,P14-1023,1,0.0792012,"GRAM) in two tasks, and achieving the best absolute performance on the visual-coverage subset of Simlex-999. Regarding multimodal fusion (that is, focusing on the visual-coverage subsets), both MMS KIP - GRAM models perform very well, at the top or just below it on all tasks, with comparable results for the two variants. Their performance is also good on the full data sets, where they consistently outperform S KIP - GRAM and SVD (that is much more strongly affected by lack of complete visual information). They’re just a few points below the state-of-the-art MEN correlation (0.8), achieved by Baroni et al. (2014) with a corpus 3 larger than ours and extensive tuning. MMS KIP - GRAM -B is close to the state of the art for Simlex-999, reported by the resource creators to be at 0.41 (Hill et al., 2014). Most impressively, MMS KIP - GRAM -A reaches the performance level of the Silberer and Lapata (2014) model on their SemSim and VisSim data sets, despite the fact that the latter has full visual-data coverage and uses attribute-based image representations, requiring supervised learning of attribute classifiers, that achieve performance in the semantic tasks comparable or higher than that of our CNN feature"
N15-1016,P12-1015,1,0.836853,"of meaning from patterns of word co-occurrence in corpora. DSMs have been very effectively applied to a variety of semantic tasks (Clark, 2015; Mikolov et al., 2013b; Turney and Pantel, 2010). However, compared to human semantic knowledge, these purely textual models, just like traditional symbolic AI systems (Harnad, 1990; Searle, 1984), are severely impoverished, suffering of lack of grounding in extra-linguistic modalities (Glenberg and Robertson, 2000). This observaMDSMs outperform state-of-the-art text-based approaches, not only in tasks that directly require access to visual knowledge (Bruni et al., 2012), but also on general semantic benchmarks (Bruni et al., 2014; Silberer and Lapata, 2014). However, current MDSMs still have a number of drawbacks. First, they are generally constructed by first separately building linguistic and visual representations of the same concepts, and then merging them. This is obviously very different from how humans learn about concepts, by hearing words in a situated perceptual context. Second, MDSMs assume that both linguistic and visual information is available for all words, with no generalization of knowledge across modalities. Third, because of this latter as"
N15-1016,N10-1011,0,0.0122674,"ying on images annotated with highlevel “visual attributes”, and a multimodal fusion strategy based on stacked autoencoders. Kiela and Bottou (2014) adopt instead a simple concatenation strategy, but obtain empirical improvements by using state-of-the-art convolutional neural networks to extract visual features, and the skip-gram model for text. These and related systems take a twostage approach to derive multimodal spaces (unimodal induction followed by fusion), and they are only tested on concepts for which both textual and visual labeled training data are available (the pioneering model of Feng and Lapata (2010) did learn from text and images jointly using Topic Models, but was shown to be empirically weak by Bruni et al. (2014)). Howell et al. (2005) propose an incremental multimodal model based on simple recurrent networks (Elman, 1990), focusing on grounding propagation 154 from early-acquired concrete words to a larger vocabulary. However, they use subject-generated features as surrogate for realistic perceptual information, and only test the model in small-scale simulations of word learning. Hill and Korhonen (2014), whose evaluation focuses on how perceptual information affects different word c"
N15-1016,P14-1046,0,0.022779,"of subject-generated features and text from image annotations into a skipgram model. They inject perceptual information by merging words expressing perceptual features with corpus contexts, which amounts to linguisticcontext re-weighting, thus making it impossible to separate linguistic and perceptual aspects of the induced representation, and to extend the model with non-linguistic features. We use instead authentic image analysis as proxy to perceptual information, and we design a robust way to incorporate it, easily extendible to other signals, such as feature norm or brain signal vectors (Fyshe et al., 2014). The recent work on so-called zero-shot learning to address the annotation bottleneck in image labeling (Frome et al., 2013; Lazaridou et al., 2014; Socher et al., 2013) looks at image- and text-based vectors from a different perspective. Instead of combining visual and linguistic information in a common space, it aims at learning a mapping from image- to text-based vectors. The mapping, induced from annotated data, is then used to project images of objects that were not seen during training onto linguistic space, in order to retrieve the nearest word vectors as labels. Multimodal word vector"
N15-1016,D14-1032,0,0.587615,"th textual and visual labeled training data are available (the pioneering model of Feng and Lapata (2010) did learn from text and images jointly using Topic Models, but was shown to be empirically weak by Bruni et al. (2014)). Howell et al. (2005) propose an incremental multimodal model based on simple recurrent networks (Elman, 1990), focusing on grounding propagation 154 from early-acquired concrete words to a larger vocabulary. However, they use subject-generated features as surrogate for realistic perceptual information, and only test the model in small-scale simulations of word learning. Hill and Korhonen (2014), whose evaluation focuses on how perceptual information affects different word classes more or less effectively, similarly to Howell et al., integrate perceptual information in the form of subject-generated features and text from image annotations into a skipgram model. They inject perceptual information by merging words expressing perceptual features with corpus contexts, which amounts to linguisticcontext re-weighting, thus making it impossible to separate linguistic and perceptual aspects of the induced representation, and to extend the model with non-linguistic features. We use instead au"
N15-1016,D14-1005,0,0.807987,"terature on multimodal distributional semantic models. We focus here on a few representative systems. Bruni et al. (2014) propose a straightforward approach to MDSM induction, where text- and image-based vectors for the same words are constructed independently, and then “mixed” by applying the Singular Value Decomposition to their concatenation. An empirically superior model has been proposed by Silberer and Lapata (2014), who use more advanced visual representations relying on images annotated with highlevel “visual attributes”, and a multimodal fusion strategy based on stacked autoencoders. Kiela and Bottou (2014) adopt instead a simple concatenation strategy, but obtain empirical improvements by using state-of-the-art convolutional neural networks to extract visual features, and the skip-gram model for text. These and related systems take a twostage approach to derive multimodal spaces (unimodal induction followed by fusion), and they are only tested on concepts for which both textual and visual labeled training data are available (the pioneering model of Feng and Lapata (2010) did learn from text and images jointly using Topic Models, but was shown to be empirically weak by Bruni et al. (2014)). Howe"
N15-1016,P14-2135,0,0.661682,"ence that abstract concepts are also grounded in the senses (Barsalou, 2008; Lakoff and Johnson, 1999). Since the word representations produced by MMS KIP - GRAM -A, including those pertaining to abstract concepts, can be directly used to search for near images in visual space, we decided to verify, experimentally, if these near images (of concrete things) are relevant not only for concrete words, as expected, but also for abstract ones, as predicted by embodied views of meaning. More precisely, we focused on the set of 200 words that were sampled across the USF norms concreteness spectrum by Kiela et al. (2014) (2 words had to be excluded for technical reasons). This set includes not only concrete (meat) and abstract (thought) nouns, but also adjectives (boring), verbs (teach), and even grammatical terms (how). Some words in the set have relatively high concreteness ratings, but are not particularly imageable, e.g.: hot, smell, pain, sweet. For each word in the set, we extracted the nearest neighbour picture of its MMS KIP - GRAM -A representation, and matched it with a random picture. The pictures were selected from a set of 5,100, all labeled with distinct words (the picture set includes, for each"
N15-1016,J99-4009,0,0.379733,"Missing"
N15-1016,P14-1132,1,0.803681,"Missing"
N15-1016,N13-1090,0,0.0707236,"over, since they propagate visual information to all words, we use them to improve image labeling and retrieval in the zero-shot setup, where the test concepts are never seen during model training. Finally, the MMS KIP - GRAM models discover intriguing visual properties of abstract words, paving the way to realistic implementations of embodied theories of meaning. 1 Introduction Distributional semantic models (DSMs) derive vector-based representations of meaning from patterns of word co-occurrence in corpora. DSMs have been very effectively applied to a variety of semantic tasks (Clark, 2015; Mikolov et al., 2013b; Turney and Pantel, 2010). However, compared to human semantic knowledge, these purely textual models, just like traditional symbolic AI systems (Harnad, 1990; Searle, 1984), are severely impoverished, suffering of lack of grounding in extra-linguistic modalities (Glenberg and Robertson, 2000). This observaMDSMs outperform state-of-the-art text-based approaches, not only in tasks that directly require access to visual knowledge (Bruni et al., 2012), but also on general semantic benchmarks (Bruni et al., 2014; Silberer and Lapata, 2014). However, current MDSMs still have a number of drawbacks"
N15-1016,P14-1068,0,0.583759,"fectively applied to a variety of semantic tasks (Clark, 2015; Mikolov et al., 2013b; Turney and Pantel, 2010). However, compared to human semantic knowledge, these purely textual models, just like traditional symbolic AI systems (Harnad, 1990; Searle, 1984), are severely impoverished, suffering of lack of grounding in extra-linguistic modalities (Glenberg and Robertson, 2000). This observaMDSMs outperform state-of-the-art text-based approaches, not only in tasks that directly require access to visual knowledge (Bruni et al., 2012), but also on general semantic benchmarks (Bruni et al., 2014; Silberer and Lapata, 2014). However, current MDSMs still have a number of drawbacks. First, they are generally constructed by first separately building linguistic and visual representations of the same concepts, and then merging them. This is obviously very different from how humans learn about concepts, by hearing words in a situated perceptual context. Second, MDSMs assume that both linguistic and visual information is available for all words, with no generalization of knowledge across modalities. Third, because of this latter assumption of full linguistic and visual coverage, current MDSMs, paradoxically, cannot be"
N15-1016,Q14-1017,0,0.175571,"1, is given by softmax: e u0wt+j T uwt p(wt+j |wt ) = P W w0 =1 0 euw0 T uwt (2) where uw and u0w are the context and target vector representations of word w respectively, and W is the size of the vocabulary. Due to the normalization term, Equation 2 requires O(|W |) time complexity. A considerable speedup to O(log |W |), is achieved by using the hierarchical version of Equation 2 (Morin and Bengio, 2005), adopted here. 3.2 maximize context prediction CAT maximize similarity map to visual space cat Multimodal Skip-gram Architecture T 1 X T = the cute little sat on the mat + Mao et al., 2014; Socher et al., 2014). These methods rely on necessarily limited collections of captioned images as sources of multimodal evidence, whereas we automatically enrich a very large corpus with images to induce general-purpose multimodal word representations, that could be used as input embeddings in systems specifically tuned to caption processing. Thus, our work is complementary to this line of research. Injecting visual knowledge We now assume that word learning takes place in a situated context, in which, for a subset of the target words, the corpus contexts are accompanied by a 155 Figure 1: “Cartoon” of MMS KIP -"
N15-1016,D11-1063,0,0.0140082,"Missing"
N15-1097,D14-1151,0,0.130328,"Missing"
N15-1097,P14-1023,1,0.881648,"es distributional embeddings of words as input and learns alternative embeddings that perform the compatibility detection task quite well. 1 Introduction Vectors encoding distributional information extracted from large text corpora provide very effective estimates of semantic similarity or, more generally, relatedness between words (Clark, 2015; Erk, 2012; Turney and Pantel, 2010). Semantic relatedness is undoubtedly a core property of word understanding, and indeed current vector-based distributional semantic models (DSMs) provide an impressive approximation to human judgments in many tasks (Baroni et al., 2014). However, relatedness alone is too general a notion to truly capture the nuances of human conceptual knowledge. The terms animal, puppy, and cat are all closely related to dog, but the nature of their relation is very different, each affording different inferences: If you tell me that Fido is a dog, I will also conclude that he’s an animal, that he is not a cat, and that he might or might not be a puppy. The previous examples hint at a fundamental semantic property that is only partially linked to relatThe notions of compatibility and incompatibility have been introduced in theoretical semant"
N15-1097,W13-3209,0,0.106337,"on answering, document summarization or even machine translation (the violinist also played the drum might corefer with the drummer also played the violin, whereas the dog was killed and the cat was killed must refer to different events). Other applications could include modeling semantic plausibility of a nominal phrase (Vecchi et al., 2011; Lynott and Connell, 2009), where the goal is to accept expressions like coastal mosquito, but reject parlamentary tomato. Finally, the notion of incompatibility relates to (certain kinds of) negation. Negation is notoriously difficult to model with DSMs (Hermann et al., 2013), and compatibility might offer a new angle into it. In this paper, we introduce a new, large benchmark to evaluate computational models on compatibility detection. We then present a supervised 965 neural-network based model that takes distributional semantic vectors as input and embeds them into a space that is optimized for compatibility detection. The model performs significantly better than direct DSM relatedness, and achieves high scores in absolute terms. 2 The compatibility benchmark We started the benchmark construction by manually assembling a list of 299 words including mostly concre"
N15-1097,W11-1301,1,0.837878,"or the inferences that can be drawn (antonyms are just the tip of the incompatibility iceberg: dog and cat are not antonyms, but one still contradicts the other). Knowing what’s compatible might also help in tasks that require recognizing (distant) paraphrases, such as question answering, document summarization or even machine translation (the violinist also played the drum might corefer with the drummer also played the violin, whereas the dog was killed and the cat was killed must refer to different events). Other applications could include modeling semantic plausibility of a nominal phrase (Vecchi et al., 2011; Lynott and Connell, 2009), where the goal is to accept expressions like coastal mosquito, but reject parlamentary tomato. Finally, the notion of incompatibility relates to (certain kinds of) negation. Negation is notoriously difficult to model with DSMs (Hermann et al., 2013), and compatibility might offer a new angle into it. In this paper, we introduce a new, large benchmark to evaluate computational models on compatibility detection. We then present a supervised 965 neural-network based model that takes distributional semantic vectors as input and embeds them into a space that is optimize"
N15-1097,J13-3004,0,\N,Missing
N16-1043,W07-0607,1,0.711698,"eing able to associate a word with a visual extension, our model is simultaneously learning word representations that allow us to deal with a variety of other tasks—for example, as mentioned above, guessing the appearance of the object denoted by a new word from a purely verbal description, grouping concepts into categories by their similarity, or having both abstract and concrete words represented in the same space. 6 Related Work While there is work on learning from multimodal data (Roy, 2000; Yu, 2005, a.o.) as well as work on learning distributed representations from childdirected speech (Baroni et al., 2007; Kievit-Kylar and Jones, 2011, a.o.), to the best of our knowledge ours is the first method which learns distributed representations from multimodal child-directed data. For example, in comparison to Yu (2005)’s model, our approach (1) induces distributed representations for words, based on linguistic and visual context, and (2) operates entirely on distributed representations through similarity measures without positing a categorical level on which to learn wordsymbol/category-symbol associations. This leads to rich multimodal conceptual representations of words in terms of distributed multi"
N16-1043,P15-1029,0,0.0199639,"d multimodal features, while in Yu’s approach words are simply distributions over categories. It is therefore not clear how Yu’s approach could capture phenomena such as predicting appearance from a verbal description or representing abstract words–all tasks that our model is at least in principle well-suited for. Note also that Frank et al. (2007)’s Bayesian model we compare against could be extended to include realistic visual data in a similar vein to Yu’s, but it would then have the same limitations. Our work is also related to research on reference resolution in dialogue systems, such as Kennington and Schlangen (2015). However, unlike Kennington 391 Conclusion Our very encouraging results suggest that multimodal distributed models are well-suited to simulating human word learning. We think the most pressing issue to move ahead in this direction is to construct larger corpora recording the linguistic and visual environment in which children acquire language, in line with the efforts of the Human Speechome Project (Roy, 2009; Roy et al., 2015). Having access to such data will enable us to design agents that acquire semantic knowledge by leveraging all available cues present in multimodal communicative setups"
N16-1043,N15-1016,1,0.851398,"e objects present in a communicative episode. Inspired by recent computational models of meaning (Bruni et al., 2014; Kiros et al., 2014; Silberer 1 See K´ad´ar et al. (2015) for a recent review of this line of work, and another learning model using, like ours, real visual input. 2 Attentive Social MSG Model Like the original MSG, our model learns multimodal word embeddings by reading an utterance sequentially and making, for each word, two sets of predictions: (a) the preceding and following words, and (b) the visual representations of objects co-occurring with the utterance. However, unlike Lazaridou et al. (2015), we do not assume we know the right object to be associated with a word. We consider instead a more realistic scenario where multiple words in an utterance co-occur with multiple objects in the corresponding scene. Under this referential uncertainty, the model needs to induce word-object associations as part of learning, relying on current knowledge about word-object affinities as well as on any social clues present in the scene. Similar to the standard skipgram, the model’s parameters are context word embeddings W0 and tar387 Proceedings of NAACL-HLT 2016, pages 387–392, c San Diego, Califor"
N16-1043,P14-1068,0,0.221684,"Missing"
N18-1108,P17-1080,0,0.0665188,"d controlled artificial languages, in which complex hierarchical phenomena were often overrepresented compared to natural languages. Our work, which is based on naturally occurring data, is most closely related to that of Linzen et al. (2016) and Bernardy and Lappin (2017), which we discussed in the introduction. Other recent work has focused on the morphological and grammatical knowledge that RNN-based machine-translation systems and sentence embeddings encode, typically by training classifiers to decode various linguistic properties from hidden states of the network (e.g., Adi et al., 2017; Belinkov et al., 2017; Shi et al., 2016), or looking at whether the end-to-end system correctly translates sentences with challenging constructions (Sennrich, 2017). Previous work in neurolinguistics and psycholinguistics used jabberwocky, or pseudo-word, sentences to probe how speakers process syntactic information (Friederici et al., 2000; Moro et al., Linzen’s dataset was recently reported by Yogatama et al. (2018). 1202 2001; Johnson and Goldberg, 2013). Such sentences are obtained by substituting original words with morphologically and phonologically acceptable nonce forms. We are not aware of work that used"
N18-1108,2017.lilt-15.3,0,0.141251,"rtantly, their analysis did not rule out the possibility that RNNs might be relying on semantic or collocational/frequency-based information, rather than purely on syntactic structure. In “dogs dogs bark an RNN might in the neighbourhood often bark bark”, get the right agreement by encoding information 1195 Proceedings of NAACL-HLT 2018, pages 1195–1205 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics about what typically barks (dogs, not neighbourhoods), without relying on more abstract structural cues. In a follow-up study to Linzen and colleagues’, Bernardy and Lappin (2017) observed that RNNs are better at long-distance agreement when they construct rich lexical representations of words, which suggests effects of this sort might indeed be at play. We introduce a method to probe the syntactic abilities of RNNs that abstracts away from potential lexical, semantic and frequency-based confounds. Inspired by Chomsky’s (1957) insight that “grammaticalness cannot be identified with meaningfulness” (p. 106), we test long-distance agreement both in standard corpus-extracted examples and in comparable nonce sentences that are grammatical but completely meaningless, e.g.,"
N18-1108,P16-2006,0,0.0233094,"ext-free languages (Elman, 1991). More recently, RNNs have ∗ The work was conducted during the internship at Facebook AI Research, Paris. achieved impressive results in large-scale tasks such as language modeling for speech recognition and machine translation, and are by now standard tools for sequential natural language tasks (e.g., Mikolov et al., 2010; Graves, 2012; Wu et al., 2016). This suggests that RNNs may learn to track grammatical structure even when trained on noisier natural data. The conjecture is supported by the success of RNNs as feature extractors for syntactic parsing (e.g., Cross and Huang, 2016; Kiperwasser and Goldberg, 2016; Zhang et al., 2017). Linzen et al. (2016) directly evaluated the extent to which RNNs can approximate hierarchical structure in corpus-extracted natural language data. They tested whether RNNs can learn to predict English subject-verb agreement, a task thought to require hierarchical structure in the general case (“the girl are?”). girl the boys like. . . is is or are are Their experiments confirmed that RNNs can, in principle, handle such constructions. However, in their study RNNs could only succeed when provided with explicit supervision on the target task."
N18-1108,K17-1003,1,0.730838,"Missing"
N18-1108,W11-2123,0,0.0268527,"ts for sRNNs can be found in the SM. 8 https://www.mturk.com/ HE RU #constructions #original 8 119 2 41 18 373 21 442 Unigram Original Nonce 54.6 54.1 65.9 42.5 67.8 63.1 60.2 54.0 5-gram KN Original Nonce 63.9 52.8 63.4 43.4 72.1 61.7 73.5 56.8 Perplexity Baselines. We consider three baselines: first, a unigram baseline, which picks the most frequent form in the training corpus out of the two candidate target forms (singular or plural); second, a 5-gram model with Kneser-Ney smoothing (KN, Kneser and Ney, 1995) trained using the IRSTLM package (Federico et al., 2008) and queried using KenLM (Heafield, 2011); and third, a 5-gram LSTM, which only had access to windows of five tokens (Chelba et al., 2017). Compared to KN, the 5-gram LSTM can generalize to unseen ngrams thanks to its embedding layer and recurrent connections. However, it cannot discover longdistance dependency patterns that span more than five words. See SM for details on the hyperparameters of this baseline. EN 5-gram LSTM Original Nonce Perplexity LSTM Original Nonce Perplexity 147.8 168.9 122.0 166.6 81.8 70.2 90.9 91.5 ±3.2 ±5.8 ±1.2 ±0.4 78.0 58.2 77.5 85.7 ±1.3 ±2.1 ±0.8 ±0.7 62.6 71.6 59.9 61.1 ±0.2 ±0.3 ±0.2 ±0.4 92.1 81.0 9"
N18-1108,P82-1020,0,0.869559,"Missing"
N18-1108,Q16-1023,0,0.0450461,"an, 1991). More recently, RNNs have ∗ The work was conducted during the internship at Facebook AI Research, Paris. achieved impressive results in large-scale tasks such as language modeling for speech recognition and machine translation, and are by now standard tools for sequential natural language tasks (e.g., Mikolov et al., 2010; Graves, 2012; Wu et al., 2016). This suggests that RNNs may learn to track grammatical structure even when trained on noisier natural data. The conjecture is supported by the success of RNNs as feature extractors for syntactic parsing (e.g., Cross and Huang, 2016; Kiperwasser and Goldberg, 2016; Zhang et al., 2017). Linzen et al. (2016) directly evaluated the extent to which RNNs can approximate hierarchical structure in corpus-extracted natural language data. They tested whether RNNs can learn to predict English subject-verb agreement, a task thought to require hierarchical structure in the general case (“the girl are?”). girl the boys like. . . is is or are are Their experiments confirmed that RNNs can, in principle, handle such constructions. However, in their study RNNs could only succeed when provided with explicit supervision on the target task. Linzen and colleagues argued th"
N18-1108,Q16-1037,1,0.557788,"nducted during the internship at Facebook AI Research, Paris. achieved impressive results in large-scale tasks such as language modeling for speech recognition and machine translation, and are by now standard tools for sequential natural language tasks (e.g., Mikolov et al., 2010; Graves, 2012; Wu et al., 2016). This suggests that RNNs may learn to track grammatical structure even when trained on noisier natural data. The conjecture is supported by the success of RNNs as feature extractors for syntactic parsing (e.g., Cross and Huang, 2016; Kiperwasser and Goldberg, 2016; Zhang et al., 2017). Linzen et al. (2016) directly evaluated the extent to which RNNs can approximate hierarchical structure in corpus-extracted natural language data. They tested whether RNNs can learn to predict English subject-verb agreement, a task thought to require hierarchical structure in the general case (“the girl are?”). girl the boys like. . . is is or are are Their experiments confirmed that RNNs can, in principle, handle such constructions. However, in their study RNNs could only succeed when provided with explicit supervision on the target task. Linzen and colleagues argued that the unsupervised language modeling objec"
N18-1108,N13-1090,0,0.106847,"Missing"
N18-1108,E17-2060,0,0.0400373,"is based on naturally occurring data, is most closely related to that of Linzen et al. (2016) and Bernardy and Lappin (2017), which we discussed in the introduction. Other recent work has focused on the morphological and grammatical knowledge that RNN-based machine-translation systems and sentence embeddings encode, typically by training classifiers to decode various linguistic properties from hidden states of the network (e.g., Adi et al., 2017; Belinkov et al., 2017; Shi et al., 2016), or looking at whether the end-to-end system correctly translates sentences with challenging constructions (Sennrich, 2017). Previous work in neurolinguistics and psycholinguistics used jabberwocky, or pseudo-word, sentences to probe how speakers process syntactic information (Friederici et al., 2000; Moro et al., Linzen’s dataset was recently reported by Yogatama et al. (2018). 1202 2001; Johnson and Goldberg, 2013). Such sentences are obtained by substituting original words with morphologically and phonologically acceptable nonce forms. We are not aware of work that used nonce sentences made of real words to evaluate the syntactic abilities of models or human subjects. As a proof of concept, Pereira (2000) and,"
N18-1108,D16-1159,0,0.0897948,"languages, in which complex hierarchical phenomena were often overrepresented compared to natural languages. Our work, which is based on naturally occurring data, is most closely related to that of Linzen et al. (2016) and Bernardy and Lappin (2017), which we discussed in the introduction. Other recent work has focused on the morphological and grammatical knowledge that RNN-based machine-translation systems and sentence embeddings encode, typically by training classifiers to decode various linguistic properties from hidden states of the network (e.g., Adi et al., 2017; Belinkov et al., 2017; Shi et al., 2016), or looking at whether the end-to-end system correctly translates sentences with challenging constructions (Sennrich, 2017). Previous work in neurolinguistics and psycholinguistics used jabberwocky, or pseudo-word, sentences to probe how speakers process syntactic information (Friederici et al., 2000; Moro et al., Linzen’s dataset was recently reported by Yogatama et al. (2018). 1202 2001; Johnson and Goldberg, 2013). Such sentences are obtained by substituting original words with morphologically and phonologically acceptable nonce forms. We are not aware of work that used nonce sentences mad"
N18-1108,E17-1063,0,0.0154163,"ave ∗ The work was conducted during the internship at Facebook AI Research, Paris. achieved impressive results in large-scale tasks such as language modeling for speech recognition and machine translation, and are by now standard tools for sequential natural language tasks (e.g., Mikolov et al., 2010; Graves, 2012; Wu et al., 2016). This suggests that RNNs may learn to track grammatical structure even when trained on noisier natural data. The conjecture is supported by the success of RNNs as feature extractors for syntactic parsing (e.g., Cross and Huang, 2016; Kiperwasser and Goldberg, 2016; Zhang et al., 2017). Linzen et al. (2016) directly evaluated the extent to which RNNs can approximate hierarchical structure in corpus-extracted natural language data. They tested whether RNNs can learn to predict English subject-verb agreement, a task thought to require hierarchical structure in the general case (“the girl are?”). girl the boys like. . . is is or are are Their experiments confirmed that RNNs can, in principle, handle such constructions. However, in their study RNNs could only succeed when provided with explicit supervision on the target task. Linzen and colleagues argued that the unsupervised l"
N18-1108,L16-1262,0,\N,Missing
N19-1002,2017.lilt-15.3,0,0.0201929,"may suffice to trigger the induction of non-trivial grammatical rules. 2 Related Work Starting with the seminal work of Linzen et al. (2016), a long-distance number agreement task has emerged as a standard way to probe the syntactic capabilities of neural language models. In the number agreement task, a model is asked to predict the verb in a sentence where the subject and main verb are separated by one or more intervening nouns (“the boy near the cars greets. . . ”) and evaluated based on how often it predicts the right verb form. Following mixed initial results by Linzen and colleagues and Bernardy and Lappin (2017), Gulordava et al. (2018) and Kuncoro et al. (2018b) have robustly established that LSTM language models achieve near-human performance on the agreement task. While Gulordava and colleagues provided some evidence that the LSTMs are relying on genuine syntactic generalizations, Kuncoro et al. (2018a) and Linzen and Leonard (2018) suggested that the LSTM achievements can, at least in part, be accounted by superficial heuristics (e.g., “percolate the number of the first noun in a sentence”). Other recent work has extended syntax probing to other phenomena such as negative polarity items and islan"
N19-1002,C18-1012,0,0.0261741,"al. (2018) and Kuncoro et al. (2018b) have robustly established that LSTM language models achieve near-human performance on the agreement task. While Gulordava and colleagues provided some evidence that the LSTMs are relying on genuine syntactic generalizations, Kuncoro et al. (2018a) and Linzen and Leonard (2018) suggested that the LSTM achievements can, at least in part, be accounted by superficial heuristics (e.g., “percolate the number of the first noun in a sentence”). Other recent work has extended syntax probing to other phenomena such as negative polarity items and island constraints (Chowdhury and Zamparelli, 2018; Jumelet and Hupkes, 2018; Marvin and Linzen, 2018; Wilcox et al., 2018). While Linzen et al. (2016) presented intrigu3 Setup Language Model We study the pretrained LSTM language model made available by Gulordava et al. (2018). This model is composed of a 650-dimensional embedding layer, two 650dimensional hidden layers, and an output layer with vocabulary size 50,000. The model was trained on Wikipedia data, without fine-tuning for number agreement, and obtained perplexity close to state of the art in the experiments of Gulordava et al.2 Number-Agreement Tasks We complement analysis of the n"
N19-1002,P18-1132,0,0.0480903,"Missing"
N19-1002,C16-1124,0,0.0277268,"9, pages 11–20 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics ing qualitative data showing cells that track grammatical number in a network directly trained on the agreement task, most of the following work focused on testing the network output behaviour, rather than on understanding how the latter follows from the inner representations of the network. Another research line studied linguistic processing in neural networks through ‘diagnostic classifiers’, that is, classifiers trained to predict a certain property from network activations (e.g., Gelderloos and Chrupała, 2016; Adi et al., 2017; Alain and Bengio, 2017; Hupkes et al., 2018). This approach may give insight into which information is encoded by the network in different layers or at different time points, but it only provides indirect evidence about the specific mechanics of linguistic processing in the network. Other studies are closer to our approach in that they attempt to attribute function to specific network cells, often by means of visualization (Karpathy et al., 2016; Li et al., 2016; Tang et al., 2017). Radford et al. (2017), for example, detected a “sentiment” grandmother cell in a language-mo"
N19-1002,W18-5426,1,0.786011,"y units, 651 and 1300 , and the syntax unit 1150 to be described below (Section 4.3) do not have segregated efferent weights to verb forms, as expected. 4.2 Short-Range Number Information Performance on the easier NA-tasks (Simple, Adv, 2Adv) was not impaired by single-unit ablations. This suggests that number may be encoded also elsewhere in the network, perhaps via a more distributed code. To verify this, we tested whether subject number can be decoded from the whole pattern of activities in the network (excluding the two LR-number units) and whether this decoding is stable across time (see Giulianelli et al., 2018, for similar observations and related methods). We expected this distributed activity to track number in a small time window after the subject, but, unlike the LR-number units, to be affected by incongruent intervening nouns. We trained a linear model to predict the grammatical number of the subject from network activity in response to the presentation of the subject, and tested its prediction on test sets from all time points (King and Dehaene, 2014), in incongruent conditions only of the nounPP task. We used Area under of Curve (AUC) to evaluate model performance. Figure 2 shows decoding ac"
N19-1002,N16-1082,0,0.0280068,"ifiers’, that is, classifiers trained to predict a certain property from network activations (e.g., Gelderloos and Chrupała, 2016; Adi et al., 2017; Alain and Bengio, 2017; Hupkes et al., 2018). This approach may give insight into which information is encoded by the network in different layers or at different time points, but it only provides indirect evidence about the specific mechanics of linguistic processing in the network. Other studies are closer to our approach in that they attempt to attribute function to specific network cells, often by means of visualization (Karpathy et al., 2016; Li et al., 2016; Tang et al., 2017). Radford et al. (2017), for example, detected a “sentiment” grandmother cell in a language-model-trained network. Kementchedjhieva and Lopez (2018) recently found a character-level RNN to track morpheme boundaries in a single cell. We are however not aware of others studies systematically characterizing the processing of a linguistic phenomenon at the level of RNN cell dynamics, as is the attempt in the study hereby presented. possesses a more distributed mechanism to predict number when subject and verb are close, with the grandmother number cells only playing a crucial r"
N19-1002,N18-1108,1,0.931238,"induction of non-trivial grammatical rules. 2 Related Work Starting with the seminal work of Linzen et al. (2016), a long-distance number agreement task has emerged as a standard way to probe the syntactic capabilities of neural language models. In the number agreement task, a model is asked to predict the verb in a sentence where the subject and main verb are separated by one or more intervening nouns (“the boy near the cars greets. . . ”) and evaluated based on how often it predicts the right verb form. Following mixed initial results by Linzen and colleagues and Bernardy and Lappin (2017), Gulordava et al. (2018) and Kuncoro et al. (2018b) have robustly established that LSTM language models achieve near-human performance on the agreement task. While Gulordava and colleagues provided some evidence that the LSTMs are relying on genuine syntactic generalizations, Kuncoro et al. (2018a) and Linzen and Leonard (2018) suggested that the LSTM achievements can, at least in part, be accounted by superficial heuristics (e.g., “percolate the number of the first noun in a sentence”). Other recent work has extended syntax probing to other phenomena such as negative polarity items and island constraints (Chowdhury"
N19-1002,Q16-1037,0,0.447667,"ctions to the long-distance number cells, suggesting that the network relies on genuine syntactic information to regulate agreement-feature percolation. Our analysis thus provides direct evidence for the claim that LSTMs trained on unannotated corpus data, despite lacking significant linguistic priors, learn to perform structure-dependent linguistic operations. In turn, this suggests that raw linguistic input and generic memory mechanisms, such as those implemented in LSTMs, may suffice to trigger the induction of non-trivial grammatical rules. 2 Related Work Starting with the seminal work of Linzen et al. (2016), a long-distance number agreement task has emerged as a standard way to probe the syntactic capabilities of neural language models. In the number agreement task, a model is asked to predict the verb in a sentence where the subject and main verb are separated by one or more intervening nouns (“the boy near the cars greets. . . ”) and evaluated based on how often it predicts the right verb form. Following mixed initial results by Linzen and colleagues and Bernardy and Lappin (2017), Gulordava et al. (2018) and Kuncoro et al. (2018b) have robustly established that LSTM language models achieve ne"
N19-1002,P82-1020,0,0.707941,"Missing"
N19-1002,D18-1151,0,0.0921161,"ished that LSTM language models achieve near-human performance on the agreement task. While Gulordava and colleagues provided some evidence that the LSTMs are relying on genuine syntactic generalizations, Kuncoro et al. (2018a) and Linzen and Leonard (2018) suggested that the LSTM achievements can, at least in part, be accounted by superficial heuristics (e.g., “percolate the number of the first noun in a sentence”). Other recent work has extended syntax probing to other phenomena such as negative polarity items and island constraints (Chowdhury and Zamparelli, 2018; Jumelet and Hupkes, 2018; Marvin and Linzen, 2018; Wilcox et al., 2018). While Linzen et al. (2016) presented intrigu3 Setup Language Model We study the pretrained LSTM language model made available by Gulordava et al. (2018). This model is composed of a 650-dimensional embedding layer, two 650dimensional hidden layers, and an output layer with vocabulary size 50,000. The model was trained on Wikipedia data, without fine-tuning for number agreement, and obtained perplexity close to state of the art in the experiments of Gulordava et al.2 Number-Agreement Tasks We complement analysis of the naturalistic, corpus-derived number-agreement test s"
N19-1002,W18-5424,1,0.755523,"018b) have robustly established that LSTM language models achieve near-human performance on the agreement task. While Gulordava and colleagues provided some evidence that the LSTMs are relying on genuine syntactic generalizations, Kuncoro et al. (2018a) and Linzen and Leonard (2018) suggested that the LSTM achievements can, at least in part, be accounted by superficial heuristics (e.g., “percolate the number of the first noun in a sentence”). Other recent work has extended syntax probing to other phenomena such as negative polarity items and island constraints (Chowdhury and Zamparelli, 2018; Jumelet and Hupkes, 2018; Marvin and Linzen, 2018; Wilcox et al., 2018). While Linzen et al. (2016) presented intrigu3 Setup Language Model We study the pretrained LSTM language model made available by Gulordava et al. (2018). This model is composed of a 650-dimensional embedding layer, two 650dimensional hidden layers, and an output layer with vocabulary size 50,000. The model was trained on Wikipedia data, without fine-tuning for number agreement, and obtained perplexity close to state of the art in the experiments of Gulordava et al.2 Number-Agreement Tasks We complement analysis of the naturalistic, corpus-derive"
N19-1002,W18-5417,0,0.0300206,"lain and Bengio, 2017; Hupkes et al., 2018). This approach may give insight into which information is encoded by the network in different layers or at different time points, but it only provides indirect evidence about the specific mechanics of linguistic processing in the network. Other studies are closer to our approach in that they attempt to attribute function to specific network cells, often by means of visualization (Karpathy et al., 2016; Li et al., 2016; Tang et al., 2017). Radford et al. (2017), for example, detected a “sentiment” grandmother cell in a language-model-trained network. Kementchedjhieva and Lopez (2018) recently found a character-level RNN to track morpheme boundaries in a single cell. We are however not aware of others studies systematically characterizing the processing of a linguistic phenomenon at the level of RNN cell dynamics, as is the attempt in the study hereby presented. possesses a more distributed mechanism to predict number when subject and verb are close, with the grandmother number cells only playing a crucial role in more difficult long-distance cases. Crucially, we independently identified a set of cells tracking syntactic structure, and found that one of them encodes the pr"
N19-1002,W18-5423,0,0.0881232,"models achieve near-human performance on the agreement task. While Gulordava and colleagues provided some evidence that the LSTMs are relying on genuine syntactic generalizations, Kuncoro et al. (2018a) and Linzen and Leonard (2018) suggested that the LSTM achievements can, at least in part, be accounted by superficial heuristics (e.g., “percolate the number of the first noun in a sentence”). Other recent work has extended syntax probing to other phenomena such as negative polarity items and island constraints (Chowdhury and Zamparelli, 2018; Jumelet and Hupkes, 2018; Marvin and Linzen, 2018; Wilcox et al., 2018). While Linzen et al. (2016) presented intrigu3 Setup Language Model We study the pretrained LSTM language model made available by Gulordava et al. (2018). This model is composed of a 650-dimensional embedding layer, two 650dimensional hidden layers, and an output layer with vocabulary size 50,000. The model was trained on Wikipedia data, without fine-tuning for number agreement, and obtained perplexity close to state of the art in the experiments of Gulordava et al.2 Number-Agreement Tasks We complement analysis of the naturalistic, corpus-derived number-agreement test set of Linzen et al. (2"
P07-1114,C00-1027,0,\N,Missing
P12-1015,J10-4006,1,0.271139,"ounts by Pointwise Mutual Information, and it is a close approximation to the LogLikelihood Ratio (Evert, 2005). It counteracts the tendency of PMI to favour extremely rare events. 2 137 30K target words across the 30K documents in the concatenated corpus that have the largest cumulative LMI mass. This model is thus akin to traditional Latent Semantic Analysis (Landauer and Dumais, 1997), without dimensionality reduction. We add to the models we constructed the freely available Distributional Memory (DM) model,3 that has been shown to reach state-of-the-art performance in many semantic tasks (Baroni and Lenci, 2010). DM is an example of a more complex textbased model that exploits lexico-syntactic and dependency relations between words (see Baroni and Lenci’s article for details), and we use it as an instance of a grammar-based model. DM is based on the same corpora we used plus the 100M-word British National Corpus,4 and it also uses LMI scores. 2.2 Visual models The visual models use information extracted from images instead of textual corpora. We use image data where each image is associated with one or more words or tags (we use “tag” for each word associated to the image, and “label” for the set of"
P12-1015,R11-1055,0,0.0485659,"ext corpora (Turney and Pantel, 2010). These models (as well as virtually all work in computational lexical semantics) rely on verbal information only, while human semantic knowledge also relies on non-verbal experience and representation (Louwerse, 2011), crucially on the information gathered through perception. Recent developments in computer vision make it possible to computationally model one vital human perceptual channel: vision (Mooney, 2008). A few studies have begun to use visual information extracted from images as part of distributional semantic models (Bergsma and Van Durme, 2011; Bergsma and Goebel, 2011; Bruni et al., 2011; Feng and Lapata, 2010; Leong and Mihalcea, 2011). These preliminary studies all focus on how vision may help text-based models in general terms, by evaluating performance on, for instance, word similarity datasets such as WordSim353. This paper contributes to connecting language and perception, focusing on how to exploit visual information to build better models of word meaning, in three ways: (1) We carry out a systematic comparison of models using textual, visual, and both types of information. (2) We evaluate the models on general semantic relatedness tasks and on two"
P12-1015,W11-2503,1,0.920391,"ntel, 2010). These models (as well as virtually all work in computational lexical semantics) rely on verbal information only, while human semantic knowledge also relies on non-verbal experience and representation (Louwerse, 2011), crucially on the information gathered through perception. Recent developments in computer vision make it possible to computationally model one vital human perceptual channel: vision (Mooney, 2008). A few studies have begun to use visual information extracted from images as part of distributional semantic models (Bergsma and Van Durme, 2011; Bergsma and Goebel, 2011; Bruni et al., 2011; Feng and Lapata, 2010; Leong and Mihalcea, 2011). These preliminary studies all focus on how vision may help text-based models in general terms, by evaluating performance on, for instance, word similarity datasets such as WordSim353. This paper contributes to connecting language and perception, focusing on how to exploit visual information to build better models of word meaning, in three ways: (1) We carry out a systematic comparison of models using textual, visual, and both types of information. (2) We evaluate the models on general semantic relatedness tasks and on two specific tasks where"
P12-1015,N10-1011,0,0.589516,"odels (as well as virtually all work in computational lexical semantics) rely on verbal information only, while human semantic knowledge also relies on non-verbal experience and representation (Louwerse, 2011), crucially on the information gathered through perception. Recent developments in computer vision make it possible to computationally model one vital human perceptual channel: vision (Mooney, 2008). A few studies have begun to use visual information extracted from images as part of distributional semantic models (Bergsma and Van Durme, 2011; Bergsma and Goebel, 2011; Bruni et al., 2011; Feng and Lapata, 2010; Leong and Mihalcea, 2011). These preliminary studies all focus on how vision may help text-based models in general terms, by evaluating performance on, for instance, word similarity datasets such as WordSim353. This paper contributes to connecting language and perception, focusing on how to exploit visual information to build better models of word meaning, in three ways: (1) We carry out a systematic comparison of models using textual, visual, and both types of information. (2) We evaluate the models on general semantic relatedness tasks and on two specific tasks where visual information is"
P12-1015,I11-1162,0,0.0842035,"ally all work in computational lexical semantics) rely on verbal information only, while human semantic knowledge also relies on non-verbal experience and representation (Louwerse, 2011), crucially on the information gathered through perception. Recent developments in computer vision make it possible to computationally model one vital human perceptual channel: vision (Mooney, 2008). A few studies have begun to use visual information extracted from images as part of distributional semantic models (Bergsma and Van Durme, 2011; Bergsma and Goebel, 2011; Bruni et al., 2011; Feng and Lapata, 2010; Leong and Mihalcea, 2011). These preliminary studies all focus on how vision may help text-based models in general terms, by evaluating performance on, for instance, word similarity datasets such as WordSim353. This paper contributes to connecting language and perception, focusing on how to exploit visual information to build better models of word meaning, in three ways: (1) We carry out a systematic comparison of models using textual, visual, and both types of information. (2) We evaluate the models on general semantic relatedness tasks and on two specific tasks where visual information is highly relevant, as they fo"
P12-1015,W11-0611,0,0.0285915,"Missing"
P12-1015,P10-1071,0,0.00642982,"0.3 Vision: green ● L 0.2 Vision: blue 0.30 0.30 Vision: black ● N L N 0.00 0.00 0.00 0.0 0.0 ● L L N L N L N Figure 2: Discrimination of literal (L) vs. nonliteral (N) uses by the best visual and textual models. termining the color of an object by the nearness of the noun denoting the object to the color term. In other words, we are trying to model the meaning of color terms and how they relate to other words, and not to directly extract the color of an object from pictures depicting them. Our second experiment is connected to the literature on the automated detection of figurative language (Shutova, 2010). There is in particular some similarity with the tasks studied by Turney et al. (2011). Turney and colleagues try, among other things, to distinguish literal and metaphorical usages of adjectives when combined with nouns, including the highly visual adjective dark (dark hair vs. dark humour). Their method, based on automatically quantifying the degree of abstractness of the noun, is complementary to ours. Future work could combine our approach and theirs. 7 Conclusion We have presented evidence that distributional semantic models based on text, while providing a good general semantic represen"
P12-1015,D11-1063,0,0.0167834,"0.00 0.0 0.0 ● L L N L N L N Figure 2: Discrimination of literal (L) vs. nonliteral (N) uses by the best visual and textual models. termining the color of an object by the nearness of the noun denoting the object to the color term. In other words, we are trying to model the meaning of color terms and how they relate to other words, and not to directly extract the color of an object from pictures depicting them. Our second experiment is connected to the literature on the automated detection of figurative language (Shutova, 2010). There is in particular some similarity with the tasks studied by Turney et al. (2011). Turney and colleagues try, among other things, to distinguish literal and metaphorical usages of adjectives when combined with nouns, including the highly visual adjective dark (dark hair vs. dark humour). Their method, based on automatically quantifying the degree of abstractness of the noun, is complementary to ours. Future work could combine our approach and theirs. 7 Conclusion We have presented evidence that distributional semantic models based on text, while providing a good general semantic representation of word meaning, can be outperformed by models using visual information for sema"
P13-1149,D10-1115,1,0.90381,"derived form, the stem was not fully appropriate), and sometimes the jump in meaning can be quite dramatic (resourceless and resource mean very different things!). In the past few years there has been much interest in how DSMs can scale up to represent the meaning of larger chunks of text such as phrases or even sentences. Trying to represent the meaning of arbitrarily long constructions by directly collecting co-occurrence statistics is obviously ineffective and thus methods have been developed to derive the meaning of larger constructions as a function of the meaning of their constituents (Baroni and Zamparelli, 2010; Coecke et al., 2010; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Socher et al., 2012). Compositional distributional semantic models (cDSMs) of word units aim at handling, compositionally, the high productivity of phrases and consequent data sparseness. It is natural to hypothesize that the same methods can be applied to morphology to derive the meaning of complex words from the meaning of their parts: For example, instead of harvesting a rebuild vector directly from the corpus, the latter could be constructed from the distributional representations of re- and build. Besides allevia"
P13-1149,W13-0104,1,0.738191,"k in lemma format, plus any item from the morphological dataset described above that was below this rank. The top 20K content words also constitute our context elements. We use a standard bag-of-words approach, counting collocates in a narrow 2-word before-and-after window. We apply (non-negative) Pointwise Mutual Information as weighting scheme and dimensionality reduction by Non-negative Matrix Factorization, setting the number of reduced-space dimensions to 350. These settings are chosen without tuning, and are based on previous experiments where they produced high-quality semantic spaces (Boleda et al., 2013; Bullinaria and Levy, 2007). 4.3 Implementation of composition methods All composition methods except mult and stem have weights to be estimated (e.g., the λ parameter of dilation or the affix matrices of lexfunc). We adopt the estimation strategy proposed by Guevara (2010) and Baroni and Zamparelli (2010), namely we pick parameter values that optimize the mapping between stem and derived vectors directly extracted from the corpus. To learn, say, a lexfunc matrix representing the prefix re-, we extract vectors of V/reV pairs that occur with sufficient frequency (visit/revisit, think/rethink."
P13-1149,D11-1057,0,0.00515604,"oxy to derived-form meaning. Our 2 Of course, spotting and segmenting complex words is a big research topic unto itself (Beesley and Karttunen, 2000; Black et al., 1991; Sproat, 1992), and one we completely sidestep here. results suggest that exploiting morphology could improve the quality of DSMs in general, extend the range of tasks that cDSMs can successfully model and support the development of new ways to test their performance. 2 Related work Morphological induction systems use corpusbased methods to decide if two words are morphologically related and/or to segment words into morphemes (Dreyer and Eisner, 2011; Goldsmith, 2001; Goldwater and McClosky, 2005; Goldwater, 2006; Naradowsky and Goldwater, 2009; Wicentowski, 2004). Morphological induction has recently received considerable attention since morphological analysis can mitigate data sparseness in domains such as parsing and machine translation (Goldberg and Tsarfaty, 2008; Lee, 2004). Among the cues that have been exploited there is distributional similarity among morphologically related words (Schone and Jurafsky, 2000; Yarowsky and Wicentowski, 2000). Our work, however, differs substantially from this track of research. We do not aim at seg"
P13-1149,P08-1043,0,0.0119783,"e range of tasks that cDSMs can successfully model and support the development of new ways to test their performance. 2 Related work Morphological induction systems use corpusbased methods to decide if two words are morphologically related and/or to segment words into morphemes (Dreyer and Eisner, 2011; Goldsmith, 2001; Goldwater and McClosky, 2005; Goldwater, 2006; Naradowsky and Goldwater, 2009; Wicentowski, 2004). Morphological induction has recently received considerable attention since morphological analysis can mitigate data sparseness in domains such as parsing and machine translation (Goldberg and Tsarfaty, 2008; Lee, 2004). Among the cues that have been exploited there is distributional similarity among morphologically related words (Schone and Jurafsky, 2000; Yarowsky and Wicentowski, 2000). Our work, however, differs substantially from this track of research. We do not aim at segmenting morphological complex words or identifying paradigms. Our goal is to automatically construct, given distributional representations of stems and affixes, semantic representations for the derived words containing those stems and affixes. A morphological induction system, given rebuild, will segment it into re- and bu"
P13-1149,J01-2001,0,0.0133775,"ng. Our 2 Of course, spotting and segmenting complex words is a big research topic unto itself (Beesley and Karttunen, 2000; Black et al., 1991; Sproat, 1992), and one we completely sidestep here. results suggest that exploiting morphology could improve the quality of DSMs in general, extend the range of tasks that cDSMs can successfully model and support the development of new ways to test their performance. 2 Related work Morphological induction systems use corpusbased methods to decide if two words are morphologically related and/or to segment words into morphemes (Dreyer and Eisner, 2011; Goldsmith, 2001; Goldwater and McClosky, 2005; Goldwater, 2006; Naradowsky and Goldwater, 2009; Wicentowski, 2004). Morphological induction has recently received considerable attention since morphological analysis can mitigate data sparseness in domains such as parsing and machine translation (Goldberg and Tsarfaty, 2008; Lee, 2004). Among the cues that have been exploited there is distributional similarity among morphologically related words (Schone and Jurafsky, 2000; Yarowsky and Wicentowski, 2000). Our work, however, differs substantially from this track of research. We do not aim at segmenting morpholog"
P13-1149,H05-1085,0,0.0210253,"se, spotting and segmenting complex words is a big research topic unto itself (Beesley and Karttunen, 2000; Black et al., 1991; Sproat, 1992), and one we completely sidestep here. results suggest that exploiting morphology could improve the quality of DSMs in general, extend the range of tasks that cDSMs can successfully model and support the development of new ways to test their performance. 2 Related work Morphological induction systems use corpusbased methods to decide if two words are morphologically related and/or to segment words into morphemes (Dreyer and Eisner, 2011; Goldsmith, 2001; Goldwater and McClosky, 2005; Goldwater, 2006; Naradowsky and Goldwater, 2009; Wicentowski, 2004). Morphological induction has recently received considerable attention since morphological analysis can mitigate data sparseness in domains such as parsing and machine translation (Goldberg and Tsarfaty, 2008; Lee, 2004). Among the cues that have been exploited there is distributional similarity among morphologically related words (Schone and Jurafsky, 2000; Yarowsky and Wicentowski, 2000). Our work, however, differs substantially from this track of research. We do not aim at segmenting morphological complex words or identify"
P13-1149,W10-2805,0,0.0930218,"component-wise operations on the constituent vectors. Given input vectors u and v, the multiplicative model (mult) returns a composed vector c with: ci = ui vi . In the weighted additive model (wadd), the composed vector is a weighted sum of the two input vectors: c = αu + βv, where α and β are two scalars. In the dilation model, the output vector is obtained by first decomposing one of the input vectors, say v, into a vector parallel to u and an orthogonal vector. Following this, the parallel vector is dilated by a factor λ before re-combining. This results in: c = (λ − 1)hu, viu + hu, uiv. Guevara (2010) and Zanzotto et al. (2010) propose the full additive model (fulladd), where the two vectors to be added are pre-multiplied by weight matrices: c = Au + Bv Since the Mitchell and Lapata and fulladd models were developed for phrase composition, the two input vectors were taken to be, very straightforwardly, the vectors of the two words to be composed into the phrase of interest. In morphological derivation, at least one of the items to be composed (the affix) is a bound morpheme. In our adaptation of these composition models, we build bound morpheme vectors by accumulating the contexts in which"
P13-1149,N04-4015,0,0.019356,"an successfully model and support the development of new ways to test their performance. 2 Related work Morphological induction systems use corpusbased methods to decide if two words are morphologically related and/or to segment words into morphemes (Dreyer and Eisner, 2011; Goldsmith, 2001; Goldwater and McClosky, 2005; Goldwater, 2006; Naradowsky and Goldwater, 2009; Wicentowski, 2004). Morphological induction has recently received considerable attention since morphological analysis can mitigate data sparseness in domains such as parsing and machine translation (Goldberg and Tsarfaty, 2008; Lee, 2004). Among the cues that have been exploited there is distributional similarity among morphologically related words (Schone and Jurafsky, 2000; Yarowsky and Wicentowski, 2000). Our work, however, differs substantially from this track of research. We do not aim at segmenting morphological complex words or identifying paradigms. Our goal is to automatically construct, given distributional representations of stems and affixes, semantic representations for the derived words containing those stems and affixes. A morphological induction system, given rebuild, will segment it into re- and build (possibl"
P13-1149,P08-1028,0,0.246754,", and sometimes the jump in meaning can be quite dramatic (resourceless and resource mean very different things!). In the past few years there has been much interest in how DSMs can scale up to represent the meaning of larger chunks of text such as phrases or even sentences. Trying to represent the meaning of arbitrarily long constructions by directly collecting co-occurrence statistics is obviously ineffective and thus methods have been developed to derive the meaning of larger constructions as a function of the meaning of their constituents (Baroni and Zamparelli, 2010; Coecke et al., 2010; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Socher et al., 2012). Compositional distributional semantic models (cDSMs) of word units aim at handling, compositionally, the high productivity of phrases and consequent data sparseness. It is natural to hypothesize that the same methods can be applied to morphology to derive the meaning of complex words from the meaning of their parts: For example, instead of harvesting a rebuild vector directly from the corpus, the latter could be constructed from the distributional representations of re- and build. Besides alleviating data sparseness problems, a system of this"
P13-1149,D09-1045,0,0.0384815,"Missing"
P13-1149,C10-1142,0,0.0304842,"rations on the constituent vectors. Given input vectors u and v, the multiplicative model (mult) returns a composed vector c with: ci = ui vi . In the weighted additive model (wadd), the composed vector is a weighted sum of the two input vectors: c = αu + βv, where α and β are two scalars. In the dilation model, the output vector is obtained by first decomposing one of the input vectors, say v, into a vector parallel to u and an orthogonal vector. Following this, the parallel vector is dilated by a factor λ before re-combining. This results in: c = (λ − 1)hu, viu + hu, uiv. Guevara (2010) and Zanzotto et al. (2010) propose the full additive model (fulladd), where the two vectors to be added are pre-multiplied by weight matrices: c = Au + Bv Since the Mitchell and Lapata and fulladd models were developed for phrase composition, the two input vectors were taken to be, very straightforwardly, the vectors of the two words to be composed into the phrase of interest. In morphological derivation, at least one of the items to be composed (the affix) is a bound morpheme. In our adaptation of these composition models, we build bound morpheme vectors by accumulating the contexts in which a set of derived words con"
P13-1149,W00-0712,0,0.121683,"ion systems use corpusbased methods to decide if two words are morphologically related and/or to segment words into morphemes (Dreyer and Eisner, 2011; Goldsmith, 2001; Goldwater and McClosky, 2005; Goldwater, 2006; Naradowsky and Goldwater, 2009; Wicentowski, 2004). Morphological induction has recently received considerable attention since morphological analysis can mitigate data sparseness in domains such as parsing and machine translation (Goldberg and Tsarfaty, 2008; Lee, 2004). Among the cues that have been exploited there is distributional similarity among morphologically related words (Schone and Jurafsky, 2000; Yarowsky and Wicentowski, 2000). Our work, however, differs substantially from this track of research. We do not aim at segmenting morphological complex words or identifying paradigms. Our goal is to automatically construct, given distributional representations of stems and affixes, semantic representations for the derived words containing those stems and affixes. A morphological induction system, given rebuild, will segment it into re- and build (possibly using distributional similarity between the words as a cue). Our system, given re- and build, predicts the (distributional semantic) mean"
P13-1149,D12-1110,0,0.0381794,"tic (resourceless and resource mean very different things!). In the past few years there has been much interest in how DSMs can scale up to represent the meaning of larger chunks of text such as phrases or even sentences. Trying to represent the meaning of arbitrarily long constructions by directly collecting co-occurrence statistics is obviously ineffective and thus methods have been developed to derive the meaning of larger constructions as a function of the meaning of their constituents (Baroni and Zamparelli, 2010; Coecke et al., 2010; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Socher et al., 2012). Compositional distributional semantic models (cDSMs) of word units aim at handling, compositionally, the high productivity of phrases and consequent data sparseness. It is natural to hypothesize that the same methods can be applied to morphology to derive the meaning of complex words from the meaning of their parts: For example, instead of harvesting a rebuild vector directly from the corpus, the latter could be constructed from the distributional representations of re- and build. Besides alleviating data sparseness problems, a system of this sort, that automatically induces the semantic con"
P13-1149,W04-0109,0,0.0177323,"Beesley and Karttunen, 2000; Black et al., 1991; Sproat, 1992), and one we completely sidestep here. results suggest that exploiting morphology could improve the quality of DSMs in general, extend the range of tasks that cDSMs can successfully model and support the development of new ways to test their performance. 2 Related work Morphological induction systems use corpusbased methods to decide if two words are morphologically related and/or to segment words into morphemes (Dreyer and Eisner, 2011; Goldsmith, 2001; Goldwater and McClosky, 2005; Goldwater, 2006; Naradowsky and Goldwater, 2009; Wicentowski, 2004). Morphological induction has recently received considerable attention since morphological analysis can mitigate data sparseness in domains such as parsing and machine translation (Goldberg and Tsarfaty, 2008; Lee, 2004). Among the cues that have been exploited there is distributional similarity among morphologically related words (Schone and Jurafsky, 2000; Yarowsky and Wicentowski, 2000). Our work, however, differs substantially from this track of research. We do not aim at segmenting morphological complex words or identifying paradigms. Our goal is to automatically construct, given distribu"
P13-1149,P00-1027,0,0.107347,"methods to decide if two words are morphologically related and/or to segment words into morphemes (Dreyer and Eisner, 2011; Goldsmith, 2001; Goldwater and McClosky, 2005; Goldwater, 2006; Naradowsky and Goldwater, 2009; Wicentowski, 2004). Morphological induction has recently received considerable attention since morphological analysis can mitigate data sparseness in domains such as parsing and machine translation (Goldberg and Tsarfaty, 2008; Lee, 2004). Among the cues that have been exploited there is distributional similarity among morphologically related words (Schone and Jurafsky, 2000; Yarowsky and Wicentowski, 2000). Our work, however, differs substantially from this track of research. We do not aim at segmenting morphological complex words or identifying paradigms. Our goal is to automatically construct, given distributional representations of stems and affixes, semantic representations for the derived words containing those stems and affixes. A morphological induction system, given rebuild, will segment it into re- and build (possibly using distributional similarity between the words as a cue). Our system, given re- and build, predicts the (distributional semantic) meaning of rebuild. Another emerging"
P13-1149,J13-3004,0,\N,Missing
P13-2010,D10-1115,1,0.949648,"u + βv, where α and β are two scalars. Finally, in the dilation model, the output vector is obtained by first decomposing one of the input vectors, say v, into a vector parallel to u and an orthogonal vector. Following this, the parallel vector is dilated by a factor λ before re-combining. This results in: p = (λ − 1)hu, viu + hu, uiv. A more general form of the additive model (fulladd) has been proposed by Guevara (2010) (see also Zanzotto et al. (2010)). In this approach, the two vectors to be added are pre-multiplied by weight matrices estimated from corpus-extracted examples: p = Au + Bv. Baroni and Zamparelli (2010) and Coecke et al. (2010) take inspiration from formal semantics to characterize composition in terms of function application. The former model adjective-noun phrases by treating the adjective as a function from nouns onto modified nouns. Given that linear functions can be expressed by matrices and their application by matrix-by-vector multiplication, a 4 Other approaches to composition in DSMs have been recently proposed by Socher et al. (2012) and Turney (2012). We leave their empirical evaluation on DPs to further work, in the first case because it is not trivial to adapt their complex arch"
P13-2010,E12-1004,1,0.876171,"cation. The former model adjective-noun phrases by treating the adjective as a function from nouns onto modified nouns. Given that linear functions can be expressed by matrices and their application by matrix-by-vector multiplication, a 4 Other approaches to composition in DSMs have been recently proposed by Socher et al. (2012) and Turney (2012). We leave their empirical evaluation on DPs to further work, in the first case because it is not trivial to adapt their complex architecture to our setting; in the other because it is not clear how Turney would extend his approach to represent DPs. 2 Baroni et al. (2012), like us, study determiner phrases with distributional methods, but they do not model them compositionally. 3 Dataset and code available from clic.cimec. unitn.it/composes. 54 noun duel homeless polygamy opulence target DP two opponents no home several wives too many goods same-N foil 1 various opponents too few homes most wives some goods same-N foil 2 three opponents one home fewer wives no goods same-D foil two engineers no incision several negotiators too many abductions D foil two no several too many N foil opponents home wives goods Table 1: Examples from the noun-DP relatedness benchma"
P13-2010,C10-1142,0,0.412805,"e make our new dataset publicly available, and we hope that it will stimulate further work on the distributional semantics of grammatical elements.3 2 functor (such as the adjective) is represented by a matrix U to be multiplied with the argument vector v (e.g., the noun vector): p = Uv. Adjective matrices are estimated from corpus-extracted examples of noun vectors and corresponding output adjective-noun phrase vectors, similarly to Guevara’s approach.4 3 The noun-DP relatedness benchmark Paraphrasing a single word with a phrase is a natural task for models of compositionality (Turney, 2012; Zanzotto et al., 2010) and determiners sometimes play a crucial role in defining the meaning of a noun. For example a trilogy is composed of three works, an assemblage includes several things and an orchestra is made of many musicians. These examples are particularly interesting, since they point to a “conceptual” use of determiners, as components of the stable and generic meaning of a content word (as opposed to situation-dependent deictic and anaphoric usages): for these determiners the boundary between content and grammatical word is somewhat blurred, and they thus provide a good entry point for testing DSM repr"
P13-2010,P12-1015,1,0.766679,"eparate words, since number clearly plays an important role in DP semantics. Finally, for each of the target determiners we added to the space the 2K most frequent DPs containing that determiner and a target noun. Co-occurrence statistics were collected from the concatenation of ukWaC, a mid-2009 dump of the English Wikipedia and the British National Corpus,5 with a total of 2.8 billion tokens. We use a bag-of-words approach, counting co-occurrence with all context words in the same sentence with a target item. We tuned a number of parameters on the independent MEN word-relatedness benchmark (Bruni et al., 2012). This led us to pick the top 20K most frequent content word lemmas as context items, Pointwise Mutual Information as weighting scheme, and dimensionality reduction by Non-negative Matrix Factorization. Except for the parameter-free mult method, parameters of the composition methods are estimated by minimizing the average Euclidean distance between the model-generated and corpusextracted vectors of the 20K DPs we consider.6 For the lexfunc model, we assume that the determiner is the functor and the noun is the argument, accuracy 39.3 34.7 34.1 31.8 23.1 method noun random mult determiner accur"
P13-2010,D11-1129,0,0.267774,"Missing"
P13-2010,W10-2805,0,0.347546,"cus almost entirely on phrases made of two or more content words (e.g., adjective-noun or verb-noun combinations) and completely ignore grammatical words, to the point that even the test set of transitive sentences proposed by Grefenstette and Sadrzadeh (2011) contains only 1 Some linguists refer to what we call DPs as noun phrases or NPs. We say DPs simply to emphasize our focus on determiners. 53 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 53–57, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics elli (2010), Guevara (2010) and Mitchell and Lapata (2010). Thus, we can straightforwardly extend the methods already proposed for adjective-noun phrases to DPs. We introduce a new task, a similarity-based challenge, where we consider nouns that are strongly conceptually related to certain DPs and test whether cDSMs can pick the most appropriate related DP (e.g., monarchy is more related to one ruler than many rulers).2 We make our new dataset publicly available, and we hope that it will stimulate further work on the distributional semantics of grammatical elements.3 2 functor (such as the adjective) is represented by a"
P13-2010,P08-1028,0,0.700392,"“the table shows no results” since the two sentences contain the same content words, or that “to kill many rats” and “to kill few rats” are equally good paraphrases of “to exterminate rats”. We focus here on how cDSMs handle determiners and the phrases they form with nouns (determiner phrases, or DPs).1 While determiners are only a subset of grammatical words, they are a large and important subset, constituting the natural stepping stone towards sentential distributional semantics: Compositional methods have already been successfully applied to simple noun-verb and noun-verb-noun structures (Mitchell and Lapata, 2008; Grefenstette and Sadrzadeh, 2011), and determiners are just what is missing to turn these skeletal constructions into full-fledged sentences. Moreover, determiner-noun phrases are, in superficial syntactic terms, similar to the adjective-noun phrases that have already been extensively studied from a cDSM perspective by Baroni and ZamparDistributional models of semantics capture word meaning very effectively, and they have been recently extended to account for compositionally-obtained representations of phrases made of content words. We explore whether compositional distributional semantic mo"
P13-2010,D09-1045,0,0.0588046,"Missing"
P13-2010,D12-1110,0,\N,Missing
P13-4006,W13-0112,0,0.0158943,"and structures to index its rows and columns, and supports similarity queries and transformations. Transformations take one semantic space as input to return another, transformed, space. Composition functions take one or more input spaces and yield a composed-elements space, which can further undergo transformations and be used for similarity queries. In fact, DISSECT semantic spaces also support higher-order tensor representations, not just vectors. Higher-order representations are used, for example, to represent transitive verbs and other multi-argument functors by Coecke et al. (2010) and Grefenstette et al. (2013). See http://clic.cimec.unitn.it/ 5 Future extensions We implemented and are currently testing DISSECT functions supporting other composition methods, including the one proposed by Socher et al. (2012). Adding further methods is our toppriority goal. In particular, several distributional models of word meaning in context share important similarities with composition models, and we plan to add them to DISSECT. Dinu et al. (2012) show, for example, that well-performing, simplified variants of the method in Thater et al. (2010), Thater et al. (2011) and Erk and Pad´o (2008) can be reduced to rela"
P13-4006,W10-2805,0,0.22325,". The first step is highly language-, task- and corpus-annotation-dependent. We do not attempt to implement all the corpus pre-processing and co-occurrence extraction routines that it would require to be of general use, and expect instead as input a matrix of raw target-context co-occurrence counts.2 DISSECT provides various methods to re-weight the counts with association measures, dimensionality reduction methods as well as the composition functions proposed by Mitchell and Lapata (2010) (Additive, Multiplicative and Dilation), Baroni and Zamparelli (2010)/Coecke et al. (2010) (Lexfunc) and Guevara (2010)/Zanzotto et al. (2010) (Fulladd). In DISSECT we define and implement these in a unified framework and in a computationally efficient manner. The focus of DISSECT is to provide an intuitive interface for researchers and to allow easy extension by adding other composition methods. 3 Semantic spaces and transformations The concept of a semantic space (composes.semantic space) is at the core of the DISSECT toolkit. A semantic space consists of co-occurrence values, stored as a matrix, together with strings associated to the rows of this matrix (by design, the target linguistic elements) and a (po"
P13-4006,P10-4006,0,0.0375053,"s is a major inconvenience. For example, a typical symmetric co-occurrence matrix extracted from a corpus of several billion words, defining context in terms of 5-word windows and considering the top 100K×100K most frequent words, contains ≈ 250 million entries and requires only 2GB of memory for (double precision) storage. Comparison to existing software In terms of design choices, DISSECT most resembles the ˇ uˇrek and Sojka, 2010). HowGensim toolkit (Reh˚ ever Gensim is intended for topic modeling, and therefore diverges considerably from DISSECT in its functionality. The SSpace package of Jurgens and Stevens (2010) also overlaps to some degree with DISSECT in terms of its intended use, however, like Gensim, it does not support compositional operations that, as far as we know, are an unique feature of DISSECT. Simple design We have opted for a very simple and intuitive design as the classes interact in very natural ways: A semantic space stores the actual data matrix and structures to index its rows and columns, and supports similarity queries and transformations. Transformations take one semantic space as input to return another, transformed, space. Composition functions take one or more input spaces an"
P13-4006,P13-1149,1,0.118589,"y is provided by numpy.array and scipy.sparse. Table 3: Compositional models for morphology. Top 3 neighbours of florist using its (lowfrequency) corpus-extracted vector, and when the vector is obtained through composition of flora and -ist with Fulladd, Lexfunc and Additive. neighbours of false belief obtained through composition with the Fulladd, Lexfunc and Additive models. In Table 3, we exemplify a less typical application of compositional models to derivational morphology, namely obtaining a representation of florist compositionally from distributional representations of flora and -ist (Lazaridou et al., 2013). 4 Efficient computations DISSECT is optimized for speed since most operations are cast as matrix operations, that are very efficiently implemented in Python’s numpy and scipy modules5 . Tables 4 and 5 show running times for typical DISSECT operations: application of the ppmi weighting scheme, nearest neighbour queries and estimation of composition function parameters (on a 2.1 Main features Support for dense and sparse representations Co-occurrence matrices, as extracted from text, tend to be very sparse structures, especially when using detailed context features which include syntactic info"
P13-4006,D10-1115,1,0.866272,"to measure similarity among the resulting word or phrase vectors. The first step is highly language-, task- and corpus-annotation-dependent. We do not attempt to implement all the corpus pre-processing and co-occurrence extraction routines that it would require to be of general use, and expect instead as input a matrix of raw target-context co-occurrence counts.2 DISSECT provides various methods to re-weight the counts with association measures, dimensionality reduction methods as well as the composition functions proposed by Mitchell and Lapata (2010) (Additive, Multiplicative and Dilation), Baroni and Zamparelli (2010)/Coecke et al. (2010) (Lexfunc) and Guevara (2010)/Zanzotto et al. (2010) (Fulladd). In DISSECT we define and implement these in a unified framework and in a computationally efficient manner. The focus of DISSECT is to provide an intuitive interface for researchers and to allow easy extension by adding other composition methods. 3 Semantic spaces and transformations The concept of a semantic space (composes.semantic space) is at the core of the DISSECT toolkit. A semantic space consists of co-occurrence values, stored as a matrix, together with strings associated to the rows of this matrix (by"
P13-4006,D12-1110,0,0.0458667,"ns take one or more input spaces and yield a composed-elements space, which can further undergo transformations and be used for similarity queries. In fact, DISSECT semantic spaces also support higher-order tensor representations, not just vectors. Higher-order representations are used, for example, to represent transitive verbs and other multi-argument functors by Coecke et al. (2010) and Grefenstette et al. (2013). See http://clic.cimec.unitn.it/ 5 Future extensions We implemented and are currently testing DISSECT functions supporting other composition methods, including the one proposed by Socher et al. (2012). Adding further methods is our toppriority goal. In particular, several distributional models of word meaning in context share important similarities with composition models, and we plan to add them to DISSECT. Dinu et al. (2012) show, for example, that well-performing, simplified variants of the method in Thater et al. (2010), Thater et al. (2011) and Erk and Pad´o (2008) can be reduced to relatively simple matrix operations, making them particularly suitable for a DISSECT implementation. 35 DISSECT is currently optimized for the composition of many phrases of the same type. This is in line"
P13-4006,P10-1097,0,0.0667701,"Missing"
P13-4006,N12-1076,1,0.837357,"ot just vectors. Higher-order representations are used, for example, to represent transitive verbs and other multi-argument functors by Coecke et al. (2010) and Grefenstette et al. (2013). See http://clic.cimec.unitn.it/ 5 Future extensions We implemented and are currently testing DISSECT functions supporting other composition methods, including the one proposed by Socher et al. (2012). Adding further methods is our toppriority goal. In particular, several distributional models of word meaning in context share important similarities with composition models, and we plan to add them to DISSECT. Dinu et al. (2012) show, for example, that well-performing, simplified variants of the method in Thater et al. (2010), Thater et al. (2011) and Erk and Pad´o (2008) can be reduced to relatively simple matrix operations, making them particularly suitable for a DISSECT implementation. 35 DISSECT is currently optimized for the composition of many phrases of the same type. This is in line with most of the current evaluations of compositional models, which focus on specific phenomena, such as adjectival modification, nounnoun compounds or intransitive verbs, to name a few. In the future we plan to provide a module f"
P13-4006,I11-1127,0,0.0413307,"Missing"
P13-4006,C10-1142,0,0.127904,"p is highly language-, task- and corpus-annotation-dependent. We do not attempt to implement all the corpus pre-processing and co-occurrence extraction routines that it would require to be of general use, and expect instead as input a matrix of raw target-context co-occurrence counts.2 DISSECT provides various methods to re-weight the counts with association measures, dimensionality reduction methods as well as the composition functions proposed by Mitchell and Lapata (2010) (Additive, Multiplicative and Dilation), Baroni and Zamparelli (2010)/Coecke et al. (2010) (Lexfunc) and Guevara (2010)/Zanzotto et al. (2010) (Fulladd). In DISSECT we define and implement these in a unified framework and in a computationally efficient manner. The focus of DISSECT is to provide an intuitive interface for researchers and to allow easy extension by adding other composition methods. 3 Semantic spaces and transformations The concept of a semantic space (composes.semantic space) is at the core of the DISSECT toolkit. A semantic space consists of co-occurrence values, stored as a matrix, together with strings associated to the rows of this matrix (by design, the target linguistic elements) and a (potentially empty) list o"
P13-4006,D08-1094,0,0.176674,"Missing"
P14-1009,D11-1129,0,0.043314,"centage Spearman coefficients for anvan1 and anvan2, t-standardized average difference between mean cosines with paraphrases and with foils for tfds, percentage Pearson coefficients for msrvid and onwn. Stateof-the-art (soa) references: anvan1: Kartsaklis and Sadrzadeh (2013); anvan2: Grefenstette (2013); tfds: The Pham et al. (2013); msrvid: B¨ar et al. (2012); onwn: Han et al. (2013). Baselines: anvan1/anvan2: verb vectors only; tfds: word overlap; msrvid/onwn: word overlap + sentence length. reported by Grefenstette (2013) (an implementation of the lexical function ideas along the lines of Grefenstette and Sadrzadeh (2011a; 2011b)). And lf is, again, the only model, besides plf, that performs better than the baseline. In the tfds task, not surprisingly the add and mult models, lacking determiner representations and being order-insensitive, fail to distinguish between true paraphrases and foils (indeed, for the mult model foils are significantly closer to the targets than the paraphrases, probably because the latter have lower content word overlap than the foils, that often differ in word order and determiners only). Our plf approach is able to handle determiners and word order correctly, as demonstrated by a h"
P14-1009,W11-2507,0,0.0221217,"centage Spearman coefficients for anvan1 and anvan2, t-standardized average difference between mean cosines with paraphrases and with foils for tfds, percentage Pearson coefficients for msrvid and onwn. Stateof-the-art (soa) references: anvan1: Kartsaklis and Sadrzadeh (2013); anvan2: Grefenstette (2013); tfds: The Pham et al. (2013); msrvid: B¨ar et al. (2012); onwn: Han et al. (2013). Baselines: anvan1/anvan2: verb vectors only; tfds: word overlap; msrvid/onwn: word overlap + sentence length. reported by Grefenstette (2013) (an implementation of the lexical function ideas along the lines of Grefenstette and Sadrzadeh (2011a; 2011b)). And lf is, again, the only model, besides plf, that performs better than the baseline. In the tfds task, not surprisingly the add and mult models, lacking determiner representations and being order-insensitive, fail to distinguish between true paraphrases and foils (indeed, for the mult model foils are significantly closer to the targets than the paraphrases, probably because the latter have lower content word overlap than the foils, that often differ in word order and determiners only). Our plf approach is able to handle determiners and word order correctly, as demonstrated by a h"
P14-1009,S12-1051,0,0.00942525,"he difference between mean cosine with paraphrases and mean cosine with foils (Pham and colleagues, equivalently, reported non-standardized average and standard deviations). 3.2 Semantic space construction and composition model implementation The two remaining data sets are larger and more ‘natural’, as they were not constructed by linguists under controlled conditions to focus on specific phenomena. They are aimed at evaluating systems on the sort of free-form sentences one encounters in real-life applications. The msrvid data set from the SemEval-2012 Semantic Textual Similarity (STS) task (Agirre et al., 2012) consists of 750 sentence pairs that describe brief videos. Sentence pairs were scored for similarity by 5 subjects each. Following standard practice in paraphrase detection studies (e.g., Blacoe and Lapata (2012)), we use cosine similarity between sentence pairs as computed by one of our systems together with two shallow similarity cues: word overlap between the two sentences and difference in sentence length. We obtain a final similarity score by weighted addition of the 3 cues, with the optimal weights determined by linear regression on separate msrvid train data that were also provided by"
P14-1009,W13-0112,0,0.578452,"ies on automatically extracted phrase vectors and on the analytical solution of the least-squares-error problem. The same method was later applied to matrix representations of intransitive verbs and determiners (Bernardi et al., 2013; Dinu et al., 2013), always with good empirical results. The full range of semantic types required for natural language processing, including those of adverbs and transitive verbs, has to include, however, tensors of greater rank. The estimation method originally proposed by Baroni and Zamparelli has been extended to 3-way tensors representing transitive verbs by Grefenstette et al. (2013) with preliminary success. Grefenstette et al.’s method works in two steps. First, one estimates matrices of verb-object phrases from subject and subject-verb-object vectors; next, transitive verb tensors are estimated from verb-object matrices and object vectors. cake, eat fruits), each attested in combination with a certain number of subject nouns with sufficient frequency to extract sensible vectors. It is not feasible to obtain enough data points for all verbs in such a training design. 1.2 In all those cases, the same word has to be mapped to tensors of different orders. Since each of the"
P14-1009,S13-1004,0,0.00613223,". Similarly, mult uses component-wise multiplication of vectors for composition. While these models are very simple, a long experimental tradition has proven their effectiveness (Landauer and Dumais, 1997; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Blacoe and Lapata, 2012). For the lf (lexical function) model, we construct functional matrix representations of adjectives, determiners and intransitive verbs. These are trained using Ridge regression with generalized crossvalidation from corpus-extracted vectors of nouns, The final set we use is onwn, from the *SEM2013 STS shared task (Agirre et al., 2013). This set contains 561 pairs of glosses (from the WordNet and OntoNotes databases), rated by 5 judges for similarity. Our main interest in this set stems from the fact that glosses are rarely well-formed full sentences (consider, e.g., cause something to pass or lead somewhere; coerce by violence, fill with terror). For this reason, they are very challenging for standard parsers. Indeed, we estimated from a sample of 40 onwn glosses that the C&C 3 We did not evaluate on other STS benchmarks since they have characteristics, such as high density of named entities, that would require embedding o"
P14-1009,S12-1059,0,0.00743949,"Missing"
P14-1009,W10-2805,0,0.113927,"Missing"
P14-1009,D10-1115,1,0.923239,"mpirical evaluation, this approach is hardly practical for generalpurpose semantic language processing, since it requires computationally expensive approximate parameter optimization techniques, and it assumes task-specific parameter learning whose results are not meant to generalize across tasks. Distributional semantic methods to approximate word meaning with context vectors have been very successful empirically, and the last years have seen a surge of interest in their compositional extension to phrases and sentences. We present here a new model that, like those of Coecke et al. (2010) and Baroni and Zamparelli (2010), closely mimics the standard Montagovian semantic treatment of composition in distributional terms. However, our approach avoids a number of issues that have prevented the application of the earlier linguistically-motivated models to full-fledged, real-life sentences. We test the model on a variety of empirical tasks, showing that it consistently outperforms a set of competitive rivals. 1 Compositional distributional semantics 1.1 The research of the last two decades has established empirically that distributional vectors for words obtained from corpus statistics can be used to represent word"
P14-1009,S13-1005,0,0.0255263,"22 -4 30 36 27 22 tfds -0.2 -2.3 5.90 2.7 11.4 7.9 msr vid 78 77 NA 79 87 77 onwn 66 55 NA 67 75 55 Table 5: Performance of composition models on all evaluation sets. Figures of merit follow previous art on each set and are: percentage Spearman coefficients for anvan1 and anvan2, t-standardized average difference between mean cosines with paraphrases and with foils for tfds, percentage Pearson coefficients for msrvid and onwn. Stateof-the-art (soa) references: anvan1: Kartsaklis and Sadrzadeh (2013); anvan2: Grefenstette (2013); tfds: The Pham et al. (2013); msrvid: B¨ar et al. (2012); onwn: Han et al. (2013). Baselines: anvan1/anvan2: verb vectors only; tfds: word overlap; msrvid/onwn: word overlap + sentence length. reported by Grefenstette (2013) (an implementation of the lexical function ideas along the lines of Grefenstette and Sadrzadeh (2011a; 2011b)). And lf is, again, the only model, besides plf, that performs better than the baseline. In the tfds task, not surprisingly the add and mult models, lacking determiner representations and being order-insensitive, fail to distinguish between true paraphrases and foils (indeed, for the mult model foils are significantly closer to the targets than"
P14-1009,D13-1166,0,0.141272,"Missing"
P14-1009,P13-2010,1,0.858605,"se a practical and empirically effective way to estimate matrices representing adjectival modifiers of nouns by linear regression from corpus-extracted examples of noun and adjective-noun vectors. Unlike the neural network approach of Socher et al. (2011; 2012), the Baroni and Zamparelli method does not require manually labeled data nor costly iterative estimation procedures, as it relies on automatically extracted phrase vectors and on the analytical solution of the least-squares-error problem. The same method was later applied to matrix representations of intransitive verbs and determiners (Bernardi et al., 2013; Dinu et al., 2013), always with good empirical results. The full range of semantic types required for natural language processing, including those of adverbs and transitive verbs, has to include, however, tensors of greater rank. The estimation method originally proposed by Baroni and Zamparelli has been extended to 3-way tensors representing transitive verbs by Grefenstette et al. (2013) with preliminary success. Grefenstette et al.’s method works in two steps. First, one estimates matrices of verb-object phrases from subject and subject-verb-object vectors; next, transitive verb tensors ar"
P14-1009,W13-3513,0,0.657392,"y attested as transitive in the training corpus, we can simply omit the object matrix to obtain a type-appropriate representation. On the other hand, if the verb occurs with more arguments than usual in testing materials, we can add a default diagonal identity matrix to its representation, signaling agnosticism about how the verb relates to the unexpected argu3 3.1 Evaluation Evaluation materials We consider 5 different benchmarks that focus on different aspects of sentence-level semantic composition. The first data set, created by Edward Grefenstette and Mehrnoosh Sadrzadeh and introduced in Kartsaklis et al. (2013), features 200 sentence pairs that were rated for similarity by 43 annotators. In this data set, sentences have fixed adjective-noun-verb-adjective-noun (anvan) structure, and they were built in order to crucially require context-based verb disambiguation (e.g., young woman filed long nails is paired with both young woman smoothed long nails and young woman registered long nails). We also consider a similar data set introduced by Grefenstette (2013), comprising 200 sentence pairs rated by 50 annotators. We will call these benchmarks anvan1 and anvan2, respectively. Evaluation is carried out by"
P14-1009,D12-1050,0,0.559066,"sent word meaning in a variety of tasks (Turney and Pantel, 2010). If distributional vectors encode certain aspects of word meaning, it is natural to expect that similar aspects of sentence meaning can also receive vector representations, obtained compositionally from word vectors. Developing a practical model of compositionality is still an open issue, which we address in this paper. One approach is to use simple, parameterfree models that perform operations such as pointwise multiplication or summing (Mitchell and Lapata, 2008). Such models turn out to be surprisingly effective in practice (Blacoe and Lapata, 2012), but they have obvious limitations. For instance, symmetric operations like vector addition are insensitive to syntactic structure, therefore meaning differences encoded in word order The lexical function model None of the proposals mentioned above, from simple to elaborate, incorporates in its architecture the intuitive idea (standard in theoretical linguistics) that semantic composition is more than a weighted combination of words. Generally one of the components of a phrase, e.g., an adjective, acts as a function affecting the other component (e.g., a noun). This underlying intuition, adop"
P14-1009,J07-4004,0,0.069633,"bject matrices. Since syntax guides lf and plf composition, we supplied all test sentences with categorial grammar parses. Every sentence in the anvan1 and anvan2 datasets has the form (subject) Adjective + Noun + Transitive Verb + (object) Adjective + Noun, so parsing them is trivial. All sentences in tfds have a predictable structure that allows perfect parsing with simple finite state rules. In all these cases, applying a general-purpose parser to the data would have, at best, had no impact and, at worst, introduced parsing errors. For msrvid and onwn, we used the output of the C&C parser (Clark and Curran, 2007). 3.3 add mult lf plf soa baseline anvan 2 22 -4 30 36 27 22 tfds -0.2 -2.3 5.90 2.7 11.4 7.9 msr vid 78 77 NA 79 87 77 onwn 66 55 NA 67 75 55 Table 5: Performance of composition models on all evaluation sets. Figures of merit follow previous art on each set and are: percentage Spearman coefficients for anvan1 and anvan2, t-standardized average difference between mean cosines with paraphrases and with foils for tfds, percentage Pearson coefficients for msrvid and onwn. Stateof-the-art (soa) references: anvan1: Kartsaklis and Sadrzadeh (2013); anvan2: Grefenstette (2013); tfds: The Pham et al."
P14-1009,P08-1028,0,0.683877,"that distributional vectors for words obtained from corpus statistics can be used to represent word meaning in a variety of tasks (Turney and Pantel, 2010). If distributional vectors encode certain aspects of word meaning, it is natural to expect that similar aspects of sentence meaning can also receive vector representations, obtained compositionally from word vectors. Developing a practical model of compositionality is still an open issue, which we address in this paper. One approach is to use simple, parameterfree models that perform operations such as pointwise multiplication or summing (Mitchell and Lapata, 2008). Such models turn out to be surprisingly effective in practice (Blacoe and Lapata, 2012), but they have obvious limitations. For instance, symmetric operations like vector addition are insensitive to syntactic structure, therefore meaning differences encoded in word order The lexical function model None of the proposals mentioned above, from simple to elaborate, incorporates in its architecture the intuitive idea (standard in theoretical linguistics) that semantic composition is more than a weighted combination of words. Generally one of the components of a phrase, e.g., an adjective, acts as"
P14-1009,D12-1110,0,0.185626,"o compositional distributional semantics Denis Paperno and Nghia The Pham and Marco Baroni Center for Mind/Brain Sciences (University of Trento, Italy) (denis.paperno|thenghia.pham|marco.baroni)@unitn.it Abstract are lost in composition: pandas eat bamboo is identical to bamboo eats pandas. Guevara (2010), Mitchell and Lapata (2010), Socher et al. (2011) and Zanzotto et al. (2010) generalize the simple additive model by applying structure-encoding operators to the vectors of two sister nodes before addition, thus breaking the inherent symmetry of the simple additive model. A related approach (Socher et al., 2012) assumes richer lexical representations where each word is represented with a vector and a matrix that encodes its interaction with its syntactic sister. The training proposed in this model estimates the parameters in a supervised setting. Despite positive empirical evaluation, this approach is hardly practical for generalpurpose semantic language processing, since it requires computationally expensive approximate parameter optimization techniques, and it assumes task-specific parameter learning whose results are not meant to generalize across tasks. Distributional semantic methods to approxim"
P14-1009,W13-0603,1,0.924288,"urran, 2007). 3.3 add mult lf plf soa baseline anvan 2 22 -4 30 36 27 22 tfds -0.2 -2.3 5.90 2.7 11.4 7.9 msr vid 78 77 NA 79 87 77 onwn 66 55 NA 67 75 55 Table 5: Performance of composition models on all evaluation sets. Figures of merit follow previous art on each set and are: percentage Spearman coefficients for anvan1 and anvan2, t-standardized average difference between mean cosines with paraphrases and with foils for tfds, percentage Pearson coefficients for msrvid and onwn. Stateof-the-art (soa) references: anvan1: Kartsaklis and Sadrzadeh (2013); anvan2: Grefenstette (2013); tfds: The Pham et al. (2013); msrvid: B¨ar et al. (2012); onwn: Han et al. (2013). Baselines: anvan1/anvan2: verb vectors only; tfds: word overlap; msrvid/onwn: word overlap + sentence length. reported by Grefenstette (2013) (an implementation of the lexical function ideas along the lines of Grefenstette and Sadrzadeh (2011a; 2011b)). And lf is, again, the only model, besides plf, that performs better than the baseline. In the tfds task, not surprisingly the add and mult models, lacking determiner representations and being order-insensitive, fail to distinguish between true paraphrases and foils (indeed, for the mult mod"
P14-1009,C10-1142,0,0.0959222,"Missing"
P14-1009,W13-3206,1,\N,Missing
P14-1009,2014.lilt-9.5,1,\N,Missing
P14-1023,J10-4006,1,0.351788,"predict DSMs across many parameter settings and on a large variety of mostly standard lexical semantics benchmarks. Our title already gave away what we discovered. We will refer to DSMs built in the traditional way as count models (since they initialize vectors with co-occurrence counts), and to their trainingbased alternative as predict(ive) models.2 Now, the most natural question to ask, of course, is which of the two approaches is best in empirical terms. Surprisingly, despite the long tradition of extensive evaluations of alternative count DSMs on standard benchmarks (Agirre et al., 2009; Baroni and Lenci, 2010; Bullinaria and Levy, 2007; Bullinaria and Levy, 2012; Sahlgren, 2006; Pad´o and Lapata, 2007), the existing literature contains very little in terms of direct comparison of count vs. predictive DSMs. This is in part due to the fact that context-predicting vectors were first developed as an approach to language modeling and/or as a way to initialize feature vectors in neuralnetwork-based “deep learning” NLP architectures, so their effectiveness as semantic representations was initially seen as little more than an interesting side effect. Sociological reasons might also be partly responsible f"
P14-1023,D12-1050,0,0.150709,"eloped within the neural-network community, with little or no awareness of recent DSM work in computational linguistics. Whatever the reasons, we know of just three works reporting direct comparisons, all limited in their scope. Huang et al. (2012) compare, in passing, one count model and several predict DSMs on the standard WordSim353 benchmark (Table 3 of their paper). In this experiment, the count model actually outperforms the best predictive approach. Instead, in a word-similarity-in-context task (Table 5), the best predict model outperforms the count model, albeit not by a large margin. Blacoe and Lapata (2012) compare count and predict representations as input to composition functions. Count vectors make for better inputs in a phrase similarity task, whereas the two representations are comparable in a paraphrase classification experiment.3 2 Distributional semantic models Both count and predict models are extracted from a corpus of about 2.8 billion tokens constructed by concatenating ukWaC,5 the English Wikipedia6 and the British National Corpus.7 For both model types, we consider the top 300K most frequent words in the corpus both as target and context elements. 2.1 Count models We prepared the c"
P14-1023,W09-0205,1,0.205087,"Missing"
P14-1023,P12-1092,0,0.953909,"timizes the amount of preserved variance, etc.). Occasionally, some kind of indirect supervision is used: Several parameter settings are tried, and the best setting is chosen based on performance on a semantic task that has been selected for tuning. The last few years have seen the development of a new generation of DSMs that frame the vector estimation problem directly as a supervised task, where the weights in a word vector are set to maximize the probability of the contexts in which the word is observed in the corpus (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011; Huang et al., 2012; Mikolov et al., 2013a; Turian et al., 2010). The traditional construction of context vectors is turned on its head: Instead of first collecting context vectors and then reweighting these vectors based on various criteria, the vector weights are directly set to optimally predict the contexts in which the corresponding words tend to appear. Since similar words occur in similar contexts, the system naturally learns to assign similar vectors to similar words. This new way to train DSMs is attractive because it replaces the essentially heuristic stacking of vector transforms in earlier models wit"
P14-1023,D12-1110,0,0.682142,"Missing"
P14-1023,P10-1040,0,0.496123,"tc.). Occasionally, some kind of indirect supervision is used: Several parameter settings are tried, and the best setting is chosen based on performance on a semantic task that has been selected for tuning. The last few years have seen the development of a new generation of DSMs that frame the vector estimation problem directly as a supervised task, where the weights in a word vector are set to maximize the probability of the contexts in which the word is observed in the corpus (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011; Huang et al., 2012; Mikolov et al., 2013a; Turian et al., 2010). The traditional construction of context vectors is turned on its head: Instead of first collecting context vectors and then reweighting these vectors based on various criteria, the vector weights are directly set to optimally predict the contexts in which the corresponding words tend to appear. Since similar words occur in similar contexts, the system naturally learns to assign similar vectors to similar words. This new way to train DSMs is attractive because it replaces the essentially heuristic stacking of vector transforms in earlier models with a single, well-defined supervised learning"
P14-1023,N13-1090,0,0.661471,"f preserved variance, etc.). Occasionally, some kind of indirect supervision is used: Several parameter settings are tried, and the best setting is chosen based on performance on a semantic task that has been selected for tuning. The last few years have seen the development of a new generation of DSMs that frame the vector estimation problem directly as a supervised task, where the weights in a word vector are set to maximize the probability of the contexts in which the word is observed in the corpus (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011; Huang et al., 2012; Mikolov et al., 2013a; Turian et al., 2010). The traditional construction of context vectors is turned on its head: Instead of first collecting context vectors and then reweighting these vectors based on various criteria, the vector weights are directly set to optimally predict the contexts in which the corresponding words tend to appear. Since similar words occur in similar contexts, the system naturally learns to assign similar vectors to similar words. This new way to train DSMs is attractive because it replaces the essentially heuristic stacking of vector transforms in earlier models with a single, well-defin"
P14-1023,J07-2002,0,0.0147606,"Missing"
P14-1023,W09-0203,0,0.0418112,"Missing"
P14-1023,N09-1003,0,\N,Missing
P14-1059,W13-3211,0,0.0152393,"rmally, similarly to Mitchell and Lapata (2008), we define a syntaxdependent composition function yielding a phrase vector p~: p~ = fcompR (~u, ~v ) Related work To the best of our knowledge, we are the first to explicitly and systematically pursue the generation problem in distributional semantics. Kalchbrenner and Blunsom (2013) use top-level, composed distributed representations of sentences to guide generation in a machine translation setting. More precisely, they condition the target language model on the composed representation (addition of word vectors) of the source language sentence. Andreas and Ghahramani (2013) discuss the the issue of generating language from vectors and present a probabilistic generative model for distributional vectors. However, their emphasis is on reversing the generative story in order to derive composed meaning representations from word sequences. The theoretical generating capabilities of the methods they propose are briefly exemplified, but not fully explored or tested. Socher et al. (2011) come closest to our target problem. They introduce a bidirectional languageto-meaning model for compositional distributional semantics that is similar in spirit to ours. However, we pres"
P14-1059,D10-1115,1,0.410068,"ge vectors for the cross-lingual experiments of Section 6 were trained on 1.6 billion tokens from itWaC.5 A word token is a wordform + POS-tag string. We extract both word vectors and the observed phrase vectors which are Composition function performance Since the experiments below also use composed vectors as input to the generation process, it is important to provide independent evidence that the composition model is of high quality. This is indeed the case: We tested our composition approach on the task of retrieving observed AN and PN vectors, based on their composed vectors (similarly to Baroni and Zamparelli (2010), we want to retrieve the observed red.car vector using fcompAN (red, car)). We obtain excellent results, with minimum accuracy of 0.23 (chance level <0.0001). We also test on the AN-N paraphrasing test set used in Dinu et al. (2013) (in turn adapting Turney (2012)). The dataset contains 620 ANs, each paired with a single-noun paraphrase (e.g., false belief/fallacy, personal appeal/charisma). The task is to rank all nouns in the lexicon by their similarity to the phrase, and return the rank of the correct paraphrase. Results are reported in the first row of Table 1. To facilitate comparison, w"
P14-1059,P14-1023,1,0.674192,"k of the correct paraphrase. Results are reported in the first row of Table 1. To facilitate comparison, we search, like Dinu et al., through a vocabulary containing the 20K most frequent nouns. The count vectors results are similar to those reported by Dinu and colleagues for the same model, and with cbow vec3 Available at https://code.google.com/p/ word2vec/ 4 The parameters of both models have been chosen without specific tuning, based on their observed stable performance in previous independent experiments. 5 Corpus sources: http://wacky.sslmit.unibo. it, http://www.natcorp.ox.ac.uk 6 See Baroni et al. (2014) for an extensive comparison of the two types of vector representations. 7 For PNs, we ignore determiners and we collapse, for example, in.the.car and in.car occurrences. 627 Input A◦N Output N A, N N cbow 11 67,29 count 171 204,168 Table 1: Median rank on the AN-N set of Dinu et al. (2013) (e.g., personal appeal/charisma). First row: the A and N are composed and the closest N is returned as a paraphrase. Second row: the N vector is decomposed into A and N vectors and their nearest (POS-tag consistent) neighbours are returned. 5.1 Output A.N A, N P.N P, N A◦N A, N P◦N P, N cbow 0.36,0.61 0.93,"
P14-1059,D12-1050,0,0.00499183,"60,0.57 0.86,0.99 1.00,1.00 Table 2: Accuracy of generation models at retrieving (at rank 1) the constituent words of adjective-noun (AN) and preposition-noun (PN) phrases. Observed (A.N) and composed representations (A◦N) are decomposed with observed(eq. 2) and composed-trained (eq. 3) functions respectively. tors we obtain a median rank that is considerably higher than that of the methods they test. 5 Input paraphrase-by-generation task we tackle here and in the next experiments. Compositional distributional semantic systems are often evaluated on phrase and sentence paraphrasing data sets (Blacoe and Lapata, 2012; Mitchell and Lapata, 2010; Socher et al., 2011; Turney, 2012). However, these experiments assume a pre-compiled list of candidate paraphrases, and the task is to rank correct paraphrases above foils (paraphrase ranking) or to decide, for a given pair, if the two phrases/sentences are mutual paraphrases (paraphrase detection). Here, instead, we do not assume a given set of candidates: For example, in N→AN paraphrasing, any of 20K2 possible combinations of adjectives and nouns from the lexicon could be generated. This is a much more challenging task and it paves the way to more realistic appli"
P14-1059,P12-1015,1,0.210937,"s: min 0 ∈R2d×d WR k[U ; V ] − WR0 WR [U ; V ]k Recursive (de)composition 2 Note that in terms of computational efficiency, cosinebased nearest neighbour searches reduce to vector-matrix multiplications, for which many efficient implementations exist. Methods such as locality sensitive hashing can be used for further speedups when working with particularly large vocabularies (Andoni and Indyk, 2008). (3) where the matrix WR is a given composition function for the same relation R. Training with 626 4 Evaluation setting required for the training procedures. We sanitycheck the two spaces on MEN (Bruni et al., 2012), a 3,000 items word similarity data set. cbow significantly outperforms count (0.80 vs. 0.72 Spearman correlations with human judgments). count performance is consistent with previously reported results.6 In our empirical part, we focus on noun phrase generation. A noun phrase can be a single noun or a noun with one or more modifiers, where a modifier can be an adjective or a prepositional phrase. A prepositional phrase is in turn composed of a preposition and a noun phrase. We learn two composition (and corresponding decomposition) functions: one for modifier-noun phrases, trained on adjecti"
P14-1059,W13-3206,1,0.895786,"t modality or a different language), then the objective in eq. (3) is more motivated. (1) We use the approximation of observed phrase vectors as objective because these vectors can provide direct evidence of the polysemous behaviour of words: For example, the corpus-observed vectors of green jacket and green politician reflect how the meaning of green is affected by its occurrence with different nouns. Moreover, it has been shown that for two-word phrases, despite their relatively low frequency, such corpus-observed representations are still difficult to outperform in phrase similarity tasks (Dinu et al., 2013; Turney, 2012). 3.2 Nearest neighbour search We retrieve the nearest neighbours of each constituent vector ~u obtained by decomposition by applying a search function s: Generation NN~u = s(~u, Lv , t) Generation of a two-word sequence from a vector proceeds in two steps: decomposition of the phrase vectors into two constituent vectors, and search for the nearest neighbours of each constituent vector in Lv (the lexical matrix) in order to retrieve the corresponding words from Lw . where NN~u is a list containing the t nearest neighours of ~u from Lv , the lexical vectors. Depending on the task"
P14-1059,W10-2805,0,0.0171046,"be able to control the generation of different output structures. Finally, the potential for generation is only addressed in passing, by presenting a few cases where the generated sequence has the same syntactic structure of the input sequence. 3 Synthesis where ~u and ~v are the vector representations associated to words u and v. fcompR : Rd × Rd → Rd (for d the dimensionality of vectors) is a composition function specific to the syntactic relation R holding between the two words.1 Although we are not bound to a specific composition model, throughout this paper we use the method proposed by Guevara (2010) and Zanzotto et al. (2010) which defines composition as application of linear transformations to the two constituents followed by summing the resulting vectors: fcompR (~u, ~v ) = W1 ~u + W2~v . We will further use the following equivalent formulation: fcompR (~u, ~v ) = WR [~u; ~v ] where WR ∈ Rd×2d and [~u; ~v ] is the vertical concatenation of the two vectors (using Matlab notation). Following Guevara, we learn WR using examples of word and phrase vectors directly extracted from the corpus (for the rest of the paper, we refer to these phrase vectors extracted non-compositionally from the c"
P14-1059,P08-1088,0,0.0171297,"esser degree, this might be desirable as a disambiguation-in-context effect as, for example, in underground cavern, in secret would not be a context-appropriate paraphrase of underground. 6 Noun phrase translation This section describes preliminary experiments performed in a cross-lingual setting on the task of composing English AN phrases and generating Italian translations. Creation of cross-lingual vector spaces A common semantic space is required in order to map words and phrases across languages. This problem has been extensively addressed in the bilingual lexicon acquisition literature (Haghighi et al., 2008; Koehn and Knight, 2002). We opt for a very simple yet accurate method (Klementiev et al., 2012; Rapp, 1999) in which a bilingual dictionary is used to identify a set of shared dimensions across spaces and the vectors of both languages are projected into the subspace defined by these (Subspace Projection - SP). This method is applicable to count-type vector spaces, for which the dimen8 This dataset is available at http://clic.cimec. unitn.it/composes 629 Input A◦N A◦N Output N, P, N N, P, N Training observed composed cbow 0.98(1),0.08(5.5),0.13(20.5) 0.99(1),0.02(12), 0.12(24) count 0.82(1),0"
P14-1059,D13-1176,0,0.0352537,"quence. The generated vectors can then be efficiently matched against those in the lexicon or fed to the decomposition system again to produce longer phrases recursively. 2 3.1 To construct the vector representing a two-word phrase, we must compose the vectors associated to the input words. More formally, similarly to Mitchell and Lapata (2008), we define a syntaxdependent composition function yielding a phrase vector p~: p~ = fcompR (~u, ~v ) Related work To the best of our knowledge, we are the first to explicitly and systematically pursue the generation problem in distributional semantics. Kalchbrenner and Blunsom (2013) use top-level, composed distributed representations of sentences to guide generation in a machine translation setting. More precisely, they condition the target language model on the composed representation (addition of word vectors) of the source language sentence. Andreas and Ghahramani (2013) discuss the the issue of generating language from vectors and present a probabilistic generative model for distributional vectors. However, their emphasis is on reversing the generative story in order to derive composed meaning representations from word sequences. The theoretical generating capabiliti"
P14-1059,W12-1702,0,0.0238712,"andard (for example vitriol and folk in Table 3). Other interesting errors consist of decomposing a noun into two words which both have the same meaning as the noun, generating for example religion → religious religions. We observe moreover that sometimes the decomposition reflects selectional preference effects, by generating adjectives that denote typical properties of the noun to be paraphrased (e.g., animosity is a (political, personal,...) hostility or a fridge is a (big, large, small,...) refrigerator). This effect could be exploited for tasks such as property-based concept description (Kelly et al., 2012). 5.2 Recursive decomposition We continue by testing generation through recursive decomposition on the task of generating nounpreposition-noun (NPN) paraphrases of adjectivenouns (AN) phrases. We introduce a dataset containing 192 AN-NPN pairs (such as pre-election promises→ promises before election), which was created by the second author and additionally corrected by an English native speaker. The data set was created by analyzing a list of randomly selected frequent ANs. 49 further ANs (with adjectives such as amazing and great) were judged not NPN-paraphrasable and were used for the experi"
P14-1059,tiedemann-2012-parallel,0,0.014934,"Missing"
P14-1059,E12-1014,0,0.0533947,"n underground cavern, in secret would not be a context-appropriate paraphrase of underground. 6 Noun phrase translation This section describes preliminary experiments performed in a cross-lingual setting on the task of composing English AN phrases and generating Italian translations. Creation of cross-lingual vector spaces A common semantic space is required in order to map words and phrases across languages. This problem has been extensively addressed in the bilingual lexicon acquisition literature (Haghighi et al., 2008; Koehn and Knight, 2002). We opt for a very simple yet accurate method (Klementiev et al., 2012; Rapp, 1999) in which a bilingual dictionary is used to identify a set of shared dimensions across spaces and the vectors of both languages are projected into the subspace defined by these (Subspace Projection - SP). This method is applicable to count-type vector spaces, for which the dimen8 This dataset is available at http://clic.cimec. unitn.it/composes 629 Input A◦N A◦N Output N, P, N N, P, N Training observed composed cbow 0.98(1),0.08(5.5),0.13(20.5) 0.99(1),0.02(12), 0.12(24) count 0.82(1),0.17(4.5),0.05(71.5) 0.99(1),0.06(10), 0.05(150.5) Table 4: Top 1 accuracy (median rank) on the A"
P14-1059,W02-0902,0,0.0348771,"t be desirable as a disambiguation-in-context effect as, for example, in underground cavern, in secret would not be a context-appropriate paraphrase of underground. 6 Noun phrase translation This section describes preliminary experiments performed in a cross-lingual setting on the task of composing English AN phrases and generating Italian translations. Creation of cross-lingual vector spaces A common semantic space is required in order to map words and phrases across languages. This problem has been extensively addressed in the bilingual lexicon acquisition literature (Haghighi et al., 2008; Koehn and Knight, 2002). We opt for a very simple yet accurate method (Klementiev et al., 2012; Rapp, 1999) in which a bilingual dictionary is used to identify a set of shared dimensions across spaces and the vectors of both languages are projected into the subspace defined by these (Subspace Projection - SP). This method is applicable to count-type vector spaces, for which the dimen8 This dataset is available at http://clic.cimec. unitn.it/composes 629 Input A◦N A◦N Output N, P, N N, P, N Training observed composed cbow 0.98(1),0.08(5.5),0.13(20.5) 0.99(1),0.02(12), 0.12(24) count 0.82(1),0.17(4.5),0.05(71.5) 0.99("
P14-1059,C10-1142,0,0.0355241,"the generation of different output structures. Finally, the potential for generation is only addressed in passing, by presenting a few cases where the generated sequence has the same syntactic structure of the input sequence. 3 Synthesis where ~u and ~v are the vector representations associated to words u and v. fcompR : Rd × Rd → Rd (for d the dimensionality of vectors) is a composition function specific to the syntactic relation R holding between the two words.1 Although we are not bound to a specific composition model, throughout this paper we use the method proposed by Guevara (2010) and Zanzotto et al. (2010) which defines composition as application of linear transformations to the two constituents followed by summing the resulting vectors: fcompR (~u, ~v ) = W1 ~u + W2~v . We will further use the following equivalent formulation: fcompR (~u, ~v ) = WR [~u; ~v ] where WR ∈ Rd×2d and [~u; ~v ] is the vertical concatenation of the two vectors (using Matlab notation). Following Guevara, we learn WR using examples of word and phrase vectors directly extracted from the corpus (for the rest of the paper, we refer to these phrase vectors extracted non-compositionally from the corpus as observed vectors)."
P14-1059,P08-1028,0,0.043531,"ntroduce a more direct approach to phrase generation, inspired by the work in compositional distributional semantics. In short, we revert the composition process and we propose a framework of data-induced, syntax-dependent functions that decompose a single vector into a vector sequence. The generated vectors can then be efficiently matched against those in the lexicon or fed to the decomposition system again to produce longer phrases recursively. 2 3.1 To construct the vector representing a two-word phrase, we must compose the vectors associated to the input words. More formally, similarly to Mitchell and Lapata (2008), we define a syntaxdependent composition function yielding a phrase vector p~: p~ = fcompR (~u, ~v ) Related work To the best of our knowledge, we are the first to explicitly and systematically pursue the generation problem in distributional semantics. Kalchbrenner and Blunsom (2013) use top-level, composed distributed representations of sentences to guide generation in a machine translation setting. More precisely, they condition the target language model on the composed representation (addition of word vectors) of the source language sentence. Andreas and Ghahramani (2013) discuss the the i"
P14-1059,S12-1019,0,0.0217804,"Missing"
P14-1059,P99-1067,0,0.0379864,"secret would not be a context-appropriate paraphrase of underground. 6 Noun phrase translation This section describes preliminary experiments performed in a cross-lingual setting on the task of composing English AN phrases and generating Italian translations. Creation of cross-lingual vector spaces A common semantic space is required in order to map words and phrases across languages. This problem has been extensively addressed in the bilingual lexicon acquisition literature (Haghighi et al., 2008; Koehn and Knight, 2002). We opt for a very simple yet accurate method (Klementiev et al., 2012; Rapp, 1999) in which a bilingual dictionary is used to identify a set of shared dimensions across spaces and the vectors of both languages are projected into the subspace defined by these (Subspace Projection - SP). This method is applicable to count-type vector spaces, for which the dimen8 This dataset is available at http://clic.cimec. unitn.it/composes 629 Input A◦N A◦N Output N, P, N N, P, N Training observed composed cbow 0.98(1),0.08(5.5),0.13(20.5) 0.99(1),0.02(12), 0.12(24) count 0.82(1),0.17(4.5),0.05(71.5) 0.99(1),0.06(10), 0.05(150.5) Table 4: Top 1 accuracy (median rank) on the AN→NPN paraphr"
P14-1059,Q14-1017,0,\N,Missing
P14-1132,P09-1010,0,0.0196631,"children can learn to associate a word to an object or property by a single exposure to it (Bloom, 2000; Carey, 1978; Carey and Bartlett, 1978; Heibeck and Markman, 1987). But lack of reference is not 1403 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1403–1414, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics only a theoretical weakness: Without the ability to refer to the outside world, context vectors are arguably useless for practical goals such as learning to execute natural language instructions (Branavan et al., 2009; Chen and Mooney, 2011), that could greatly benefit from the rich network of lexical meaning such vectors encode, in order to scale up to real-life challenges. Very recently, a number of papers have exploited advances in automated feature extraction form images and videos to enrich context vectors with visual information (Bruni et al., 2014; Feng and Lapata, 2010; Leong and Mihalcea, 2011; Regneri et al., 2013; Silberer et al., 2013). This line of research tackles the grounding problem: Word representations are no longer limited to their linguistic contexts but also encode visual information"
P14-1132,P13-4032,1,0.831444,"lion, wolf bridge, castle, house, road cloud, mountain, plain, sea camel, cattle, chimpanzee, kangaroo fox, porcupine, possum, skunk crab, snail, spider, worm baby, girl, man, woman crocodile, dinosaur, snake, turtle hamster, mouse, rabbit, shrew bicycle, motorcycle, train rocket, tank, tractor Unseen (Test) Concepts dolphin shark rose cup orange clock bed butterfly tiger skyscraper forest elephant raccoon lobster boy lizard squirrel bus streetcar Table 1: Concepts in our version of the CIFAR-100 data set We implement the entire visual pipeline with VSEM, an open library for visual semantics (Bruni et al., 2013).3 4.3 Linguistic Semantic Spaces For constructing the text-based vectors, we follow a standard pipeline in distributional semantics (Turney and Pantel, 2010) without tuning its parameters and collect co-occurrence statistics from the concatenation of ukWaC4 and the Wikipedia, amounting to 2.7 billion tokens in total. Semantic vectors are constructed for a set of 30K target words (lemmas), namely the top 20K most frequent nouns, 5K most frequent adjectives and 5K most frequent verbs, and the same 30K lemmas are also employed as contextual elements. We collect co-occurrences in a symmetric cont"
P14-1132,N10-1011,0,0.08915,"ciation for Computational Linguistics only a theoretical weakness: Without the ability to refer to the outside world, context vectors are arguably useless for practical goals such as learning to execute natural language instructions (Branavan et al., 2009; Chen and Mooney, 2011), that could greatly benefit from the rich network of lexical meaning such vectors encode, in order to scale up to real-life challenges. Very recently, a number of papers have exploited advances in automated feature extraction form images and videos to enrich context vectors with visual information (Bruni et al., 2014; Feng and Lapata, 2010; Leong and Mihalcea, 2011; Regneri et al., 2013; Silberer et al., 2013). This line of research tackles the grounding problem: Word representations are no longer limited to their linguistic contexts but also encode visual information present in images associated with the corresponding objects. In this paper, we rely on the same image analysis techniques but instead focus on the reference problem: We do not aim at enriching word representations with visual information, although this might be a side effect of our approach, but we address the issue of automatically mapping objects, as depicted in"
P14-1132,W12-1702,0,0.0167378,"lakhutdinov, 2012). Furthermore, we intend to adopt visual attributes (Farhadi et al., 2009; Silberer et al., 2013) as visual representations, since they should allow a better understanding of how crossmodal mapping works, thanks to their linguistic interpretability. The error analysis in Section 5.3 suggests that automated localization techniques (van de Sande et al., 2011), distinguishing an object from its surroundings, might drastically improve mapping accuracy. Similarly, in the textual domain, models that extract collocates of a word that are more likely to denote conceptual properties (Kelly et al., 2012) might lead to more informative and discriminative linguistic vectors. Finally, the lack of large child-directed speech corpora constrained the experimental design of fast mapping simulations; we plan to run more realistic experiments with true nonce words and using source corpora (e.g., the Simple Wikipedia, child stories, portions of CHILDES) that contain sentences more akin to those a child might effectively hear or read in her word-learning years. Acknowledgments We thank Adam Liˇska for helpful discussions and the 3 anonymous reviewers for useful comments. This work was supported by ERC 2"
P14-1132,I11-1162,0,0.0350997,"al Linguistics only a theoretical weakness: Without the ability to refer to the outside world, context vectors are arguably useless for practical goals such as learning to execute natural language instructions (Branavan et al., 2009; Chen and Mooney, 2011), that could greatly benefit from the rich network of lexical meaning such vectors encode, in order to scale up to real-life challenges. Very recently, a number of papers have exploited advances in automated feature extraction form images and videos to enrich context vectors with visual information (Bruni et al., 2014; Feng and Lapata, 2010; Leong and Mihalcea, 2011; Regneri et al., 2013; Silberer et al., 2013). This line of research tackles the grounding problem: Word representations are no longer limited to their linguistic contexts but also encode visual information present in images associated with the corresponding objects. In this paper, we rely on the same image analysis techniques but instead focus on the reference problem: We do not aim at enriching word representations with visual information, although this might be a side effect of our approach, but we address the issue of automatically mapping objects, as depicted in images, to the context ve"
P14-1132,Q13-1003,0,0.0226669,"retical weakness: Without the ability to refer to the outside world, context vectors are arguably useless for practical goals such as learning to execute natural language instructions (Branavan et al., 2009; Chen and Mooney, 2011), that could greatly benefit from the rich network of lexical meaning such vectors encode, in order to scale up to real-life challenges. Very recently, a number of papers have exploited advances in automated feature extraction form images and videos to enrich context vectors with visual information (Bruni et al., 2014; Feng and Lapata, 2010; Leong and Mihalcea, 2011; Regneri et al., 2013; Silberer et al., 2013). This line of research tackles the grounding problem: Word representations are no longer limited to their linguistic contexts but also encode visual information present in images associated with the corresponding objects. In this paper, we rely on the same image analysis techniques but instead focus on the reference problem: We do not aim at enriching word representations with visual information, although this might be a side effect of our approach, but we address the issue of automatically mapping objects, as depicted in images, to the context vectors representing the"
P14-1132,D12-1130,0,0.0500082,"tion of regions (Socher and Fei-Fei, 2010) and complete images (Hardoon et al., 2006; Hodosh et al., 2013). Given two paired observation matrices, in our case Vs and Ws , CCA aims at capturing the linear relationship that exists between these variables. This is achieved by finding a pair of matrices, in our 1407 case CV ∈ Rdv ×d and CW ∈ Rdw ×d , such that the correlation between the projections of the two multidimensional variables into a common, lower-rank space is maximized. The resulting multimodal space has been shown to provide a good approximation to human concept similarity judgments (Silberer and Lapata, 2012). In our setup, after applying CCA on the two spaces Vs and Ws , we obtain the two projection mappings onto the common space and thus our projection function can be derived as: fprojv→w = CV CW −1 (2) Singular Value Decomposition (SVD) SVD is the most widely used dimensionality reduction technique in distributional semantics (Turney and Pantel, 2010), and it has recently been exploited to combine visual and linguistic dimensions in the multimodal distributional semantic model of Bruni et al. (2014). SVD smoothing is also a way to infer values of unseen dimensions in partially incomplete matric"
P14-1132,P13-1056,0,0.435269,"out the ability to refer to the outside world, context vectors are arguably useless for practical goals such as learning to execute natural language instructions (Branavan et al., 2009; Chen and Mooney, 2011), that could greatly benefit from the rich network of lexical meaning such vectors encode, in order to scale up to real-life challenges. Very recently, a number of papers have exploited advances in automated feature extraction form images and videos to enrich context vectors with visual information (Bruni et al., 2014; Feng and Lapata, 2010; Leong and Mihalcea, 2011; Regneri et al., 2013; Silberer et al., 2013). This line of research tackles the grounding problem: Word representations are no longer limited to their linguistic contexts but also encode visual information present in images associated with the corresponding objects. In this paper, we rely on the same image analysis techniques but instead focus on the reference problem: We do not aim at enriching word representations with visual information, although this might be a side effect of our approach, but we address the issue of automatically mapping objects, as depicted in images, to the context vectors representing the corresponding words. Th"
P14-1132,N13-1090,0,0.675553,"out words and their objects, as well as exploiting the limited new evidence available, the learner must learn to associate new objects with words. Our results on this task pave the way to realistic simulations of how children or robots could use existing knowledge to bootstrap grounded semantic knowledge about new concepts. 1 Introduction Computational models of meaning that rely on corpus-extracted context vectors, such as LSA (Landauer and Dumais, 1997), HAL (Lund and Burgess, 1996), Topic Models (Griffiths et al., 2007) and more recent neural-network approaches (Collobert and Weston, 2008; Mikolov et al., 2013b) have successfully tackled a number of lexical semantics tasks, where context vector similarity highly correlates with various indices of semantic relatedness (Turney and Pantel, 2010). Given that these models are learned from naturally occurring data using simple associative techniques, various authors have advanced the claim that they might be also capturing some crucial aspects of how humans acquire and use language (Landauer and Dumais, 1997; Lenci, 2008). However, the models induce the meaning of words entirely from their co-occurrence with other words, without links to the external wor"
P14-1132,P13-1006,0,0.0245048,"ulate a fast mapping scenario, thus strengthening the claims of this approach as a full-fledged, fully inductive theory of meaning acquisition. 2 Related Work The problem of establishing word reference has been extensively explored in computational simulations of cross-situational learning (see Fazly et al. (2010) for a recent proposal and extended review of previous work). This line of research has traditionally assumed artificial models of the external world, typically a set of linguistic or logical labels for objects, actions and possibly other aspects of a scene (Siskind, 1996). Recently, Yu and Siskind (2013) presented a system that induces word-object mappings from features extracted from short videos paired with sentences. Our work complements theirs in two ways. First, unlike Yu and Siskind (2013) who considered a limited lexicon of 15 items with only 4 nouns, we conduct experiments in a large search space containing a highly ambiguous set of potential target words for every object (see Section 4.1). Most importantly, by projecting visual representations of objects into a shared semantic space, we do not limit ourselves to establishing a link between ob1404 jects and words. We induce a rich sem"
P15-1027,P14-1023,1,0.867683,"hods (Palatucci et al., 2009) address the scalability problem by building on the observation that the labels of interest are often words (or longer linguistic expressions), which stand in a semantic similarity relation to each other. Moreover, distributional approaches allow us to estimate very large semantic word spaces in an efficient and unsupervised manner, using just unannotated text corpora as input (Turney and Pantel, 2010). Extensive evidence has shown that the similarity estimates obtained by representing words as vectors in such corpus-induced semantic spaces are extremely accurate (Baroni et al., 2014). Under the assumption that the domain of interest (e.g., objects in pictures, words in a source language) exhibits comparable similarity structure to that manifested in language, we can rephrase the learning task, from inducing multiple functions from the source feature space onto independent atomic labels, to that of estimating a single crossspace mapping function from vectors in the source feature space onto vectors for the corresponding word labels in distributional semantic space. The induced function can then also be applied to a data-point whose label was not used for training. The word"
P15-1027,N06-1020,0,0.0163373,"ven that the latter has been addressed as a domain adaptation problem (Gong et al., 2012; Donahue et al., 2013), we adopt here a similar view. Self-training has been successfully used for domain adaptation in NLP, e.g., in syntactic parsing. Given the limited amount of syntactically annotated data coming from monotonous sources (e.g., the Wall Street Journal), parsers show a big drop in performance when applied to different domains (e.g., reviews), since training and test domains differ dramatically, thus affecting their generalization performance. In a nutshell, the idea behind selftraining (McClosky et al., 2006; Reichart and Rappoport, 2007) is to use manually annotated data A A A (xA i , .., xN , yi , .., yN ) from domain A to train a B parser, feed the trained parser with data xB i , .., xK from domain B in order to obtain their automated B and then retrain the parser annotations y ˆiB , .., y ˆK Pollution The quantitative results and post-hoc analysis of hubs in Section 3 suggest that cross-modal mapping is facing a serious generalization problem. To get a better grasp of the phenomenon, we define a binary measure of (training data) pollution for a queried item x and parameterized by k, such that"
P15-1027,N13-1090,0,0.484173,"rl-derived dictionary (Tiedemann, 2012).4 The 5K most frequent translation pairs were used for training, while the test set includes 1.5K English words equally split into 5 frequency bins. The search for the correct translation is performed in a semantic space of 200K 1 https://code.google.com/p/word2vec/ Other hyperparameters, which we adopted without further tuning, include a context window size of 5 words to either side of the target, setting the sub-sampling option to 1e-05 and estimating the probability of target words by negative sampling, drawing 10 samples from the noise distribution (Mikolov et al., 2013b). 3 Corpus sources: http://wacky.sslmit.unibo. it, http://www.natcorp.ox.ac.uk 4 http://opus.lingfil.uu.se/ 2 271 Italian words.5 (ridge): ˆ = argmin kXW − Yk + λkWk, W Cross-modal experiments In the cross-modal experiments, we induce a mapping from visual to linguistic space. Specifically, given an image, we apply the mapping to its visual vector representation to obtain an estimate of its representation in linguistic space, where the word associated to the nearest neighbour is retrieved as the image label. Similarly to translation pairs in the crosslinguistic setup, we create a list of “vi"
P15-1027,P14-1059,1,0.280631,"bels in distributional semantic space. The induced function can then also be applied to a data-point whose label was not used for training. The word corresponding to the nearest neighbour of the mapped vector in the latter space is used as the label of the data point. Zero-shot learning using distributional semantic spaces was originally proposed for brain signal decoding (Mitchell et al., 2008), but it has since been extensively applied in other domains, including image labeling (Frome et al., 2013; Lazaridou et al., 2014; Socher et al., 2013) and bilingual dictionary/phrase table induction (Dinu and Baroni, 2014; Mikolov et al., Introduction In many supervised problems, the parameters of a classification function are estimated on (x, y) pairs, where x is a vector representing a training instance in some feature space, and y is the label assigned to the instance. For example, in image labeling x contains visual features extracted from a picture and y is the name of the object depicted in the picture (Grauman and Leibe, 2011). Since each label is treated as an unanalyzed primitive, 270 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Join"
P15-1027,P07-1078,0,0.0101714,"been addressed as a domain adaptation problem (Gong et al., 2012; Donahue et al., 2013), we adopt here a similar view. Self-training has been successfully used for domain adaptation in NLP, e.g., in syntactic parsing. Given the limited amount of syntactically annotated data coming from monotonous sources (e.g., the Wall Street Journal), parsers show a big drop in performance when applied to different domains (e.g., reviews), since training and test domains differ dramatically, thus affecting their generalization performance. In a nutshell, the idea behind selftraining (McClosky et al., 2006; Reichart and Rappoport, 2007) is to use manually annotated data A A A (xA i , .., xN , yi , .., yN ) from domain A to train a B parser, feed the trained parser with data xB i , .., xK from domain B in order to obtain their automated B and then retrain the parser annotations y ˆiB , .., y ˆK Pollution The quantitative results and post-hoc analysis of hubs in Section 3 suggest that cross-modal mapping is facing a serious generalization problem. To get a better grasp of the phenomenon, we define a binary measure of (training data) pollution for a queried item x and parameterized by k, such that pollution is 1 if x has a (tar"
P15-1027,P05-1044,0,0.0211648,"data augmentation technique. In this case, we augment the 5K training elements with 11.5K chimeras, for the 1.5K test elements and 10K randomly sampled distractors. For these 11.5K elements, we associate their Italian (target space) label yi with a 5 Picking informative negative examples An interesting feature of the ranking max-margin objective lies in its active use of negative examples. While previous work in cross-space mapping has paid little attention to the properties that negative samples should possess, this has not gone unnoticed in the NLP literature on structured prediction tasks. Smith and Eisner (2005) propose a contrastive estimation framework in the context of POS-tagging, in which positive evidence derived from gold sentence annotations is extended with negative evidence derived by various neighbourhood functions that corrupt the data in particular ways (e.g., by deleting 1 word). Having shown the effectiveness of max-margin estimation in the previous sections, we now take 277 P@1 P@5 P@10 Cross-modal intruder random intruder 38.4 54.2 60.4 40.2 55.5 61.8 3.7 10.9 15.8 5.6 12.4 17.8 0.45 intruder random 0.4 0.35 Precision@1 Cross-linguistic random Table 11: Random vs. intruding negative"
P15-1027,Q14-1017,0,0.0146228,"result holds for a pure least-squares solution, without the ridge L2 regularization term. Whether it also applies to ridge-based estimates will depend on the relative impact of the least-squares and L2 terms on the final solution (and it is not excluded that the L2 term might also independently reduce variance, of course). Empirically, we find that, indeed, lower variance also characterizes test vectors mapped with a ridge-estimated function. Interestingly, in the literature on cross-space mapping we find that authors choose a different cost function than ridge, without motivating the choice. Socher et al. (2014) mention in passing that max-margin outperforms a least-squarederror cost for cross-modal mapping. Table 4 confirms that the improvement brought about by max-margin is indeed (at least partially) due to hubness reduction. A large proportion of vectors retrieved as top-1 predictions (translations/labels) are hubs when mapping is trained with ridge, but the proportion drops dramatically with max-margin. Still, more than 1/5 top predictions for cross-modal mapping with max-margin are hubs (vs. less than 1/10 for the original vectors). Now, the mathematical properties we reviewed above suggest tha"
P15-1027,P14-1132,1,0.381789,"g function from vectors in the source feature space onto vectors for the corresponding word labels in distributional semantic space. The induced function can then also be applied to a data-point whose label was not used for training. The word corresponding to the nearest neighbour of the mapped vector in the latter space is used as the label of the data point. Zero-shot learning using distributional semantic spaces was originally proposed for brain signal decoding (Mitchell et al., 2008), but it has since been extensively applied in other domains, including image labeling (Frome et al., 2013; Lazaridou et al., 2014; Socher et al., 2013) and bilingual dictionary/phrase table induction (Dinu and Baroni, 2014; Mikolov et al., Introduction In many supervised problems, the parameters of a classification function are estimated on (x, y) pairs, where x is a vector representing a training instance in some feature space, and y is the label assigned to the instance. For example, in image labeling x contains visual features extracted from a picture and y is the name of the object depicted in the picture (Grauman and Leibe, 2011). Since each label is treated as an unanalyzed primitive, 270 Proceedings of the 53rd A"
P15-1027,tiedemann-2012-parallel,0,0.0418989,"from English to Italian and adopt the setup (word vectors, training and test data) of Dinu et al. (2015). For a set of 200K words, 300-dimensional vectors were built using the word2vec toolkit,1 choosing the CBOW method.2 CBOW, which learns to predict a target word from the ones surrounding it, produces state-of-the-art results in many linguistic tasks (Baroni et al., 2014). The word vectors were induced from corpora of 2.8 and 1.6 billion tokens, respectively, for English and Italian.3 The train and test English-to-Italian translation pairs were extracted from a Europarl-derived dictionary (Tiedemann, 2012).4 The 5K most frequent translation pairs were used for training, while the test set includes 1.5K English words equally split into 5 frequency bins. The search for the correct translation is performed in a semantic space of 200K 1 https://code.google.com/p/word2vec/ Other hyperparameters, which we adopted without further tuning, include a context window size of 5 words to either side of the target, setting the sub-sampling option to 1e-05 and estimating the probability of target words by negative sampling, drawing 10 samples from the noise distribution (Mikolov et al., 2013b). 3 Corpus source"
P15-1027,D11-1063,0,0.0210494,"Missing"
P15-1094,N09-1003,0,0.0122934,"Missing"
P15-1094,S13-1004,0,0.0432643,"we report it between human scores and sentence vector cosine similarities computed by the models. SICK (Marelli et al., 2014) (sick-r) was created specifically for the purpose of evaluating compositional models, focusing on linguistic phenomena such as lexical variation and word order. Here we report performance of the systems on the test part of the data set, which contains 5K sentence pairs. The top performance (from the SICK SemEval shared task) was reached by Zhao et al. (2014) using a heterogeneous set of features that include WordNet and extra training corpora. Agirre et al. (2012) and Agirre et al. (2013) created two collections of sentential similarities consisting of subsets coming from different sources. From these, we pick the Microsoft Research video description dataset (msrvid), where near paraphrases are descriptions of the same short video, and the OnWN 2012 (onwn1) and OnWN 2013 (onwn2) data sets (each of these sets contains 750 pairs). The latter are quite different from other sentence relatedness benchmarks, since they compare definitions for the same or different words taken from WordNet and OntoNotes: these glosses often are syntactic fragments (“cause something to pass or lead so"
P15-1094,W13-3206,1,0.712325,"Missing"
P15-1094,J10-4006,1,0.558721,"cal learning objective 973 Lapata, 2010),3 but C-PHRASE is both more effective in compositional tasks (see evaluation below), and it has the further advantage that it learns its own word vectors, thus reducing the number of arbitrary choices to be made in modeling. Comparison with traditional syntax-sensitive word representations Syntax has often been exploited in distributional semantics for a richer characterization of context. By relying on a syntactic parse of the input corpus, a distributional model can take more informative contexts such as subject-of-eat vs. object-of-eat into account (Baroni and Lenci, 2010; Curran and Moens, 2002; Grefenstette, 1994; Erk and Pad´o, 2008; Levy and Goldberg, 2014a; Pad´o and Lapata, 2007; Rothenh¨ausler and Sch¨utze, 2009). In this approach, syntactic information serves to select and/or enrich the contexts that are used to build representations of target units. On the other hand, we use syntax to determine the target units that we build representations for (in the sense that we jointly learn representations of their constituents). The focus is thus on unrelated aspects of model induction, and we could indeed use syntax-mediated contexts together with our phrasing"
P15-1094,D08-1094,0,0.138456,"Missing"
P15-1094,D10-1115,1,0.740891,"Missing"
P15-1094,W10-2805,0,0.0308355,"arge unannotated corpora and efficient pre-trained parsers are available for many languages, making the CPHRASE knowledge demands feasible for practical purposes. There is no need to parse the sentences we want to build representations for at test time, since the component word vectors are simply added. The only parameters of the model are the word vectors; specifically, no extra parameters are needed for composition (composition models such as the one presented in Socher et al. (2012) require an extra parameter matrix for each word in the vocabulary, and even leaner models such as the one of Guevara (2010) must estimate a parameter matrix for each composition rule in the grammar). This makes C-PHRASE as simple as additive and multiplicative composition (Mitchell and Joint optimization of word and phrase vectors The C-PHRASE hierarchical learning objective 973 Lapata, 2010),3 but C-PHRASE is both more effective in compositional tasks (see evaluation below), and it has the further advantage that it learns its own word vectors, thus reducing the number of arbitrary choices to be made in modeling. Comparison with traditional syntax-sensitive word representations Syntax has often been exploited in d"
P15-1094,2014.lilt-9.5,1,0.592876,"mple, given training sentence “A sad dog is howling in the park”, C-PHRASE will optimize context prediction for dog, sad dog, a sad dog, a sad dog is howling, etc., but not, for example, for howling in, as these two words do not form a syntactic constituent by themselves. Introduction Distributional semantic models, that induce vector-based meaning representations from patterns of co-occurrence of words in corpora, have proven very successful at modeling many lexical relations, such as synonymy, co-hyponomy and analogy (Mikolov et al., 2013c; Turney and Pantel, 2010). The recent evaluation of Baroni et al. (2014b) suggests that the C-BOW model introduced by Mikolov et al. (2013a) is, consistently, the best across many tasks.1 Interestingly, C-BOW vectors are estimated with a simple compositional approach: The weights of adjacent words are jointly optimized so that their sum will predict the distribution of their contexts. This is reminiscent of how the parameters of some compositional distributional semanC-PHRASE word representations outperform C-BOW on several word-level benchmarks. In addition, because they are estimated in a compositional way, C-PHRASE word vectors, when combined through simple ad"
P15-1094,P14-1023,1,0.471174,"mple, given training sentence “A sad dog is howling in the park”, C-PHRASE will optimize context prediction for dog, sad dog, a sad dog, a sad dog is howling, etc., but not, for example, for howling in, as these two words do not form a syntactic constituent by themselves. Introduction Distributional semantic models, that induce vector-based meaning representations from patterns of co-occurrence of words in corpora, have proven very successful at modeling many lexical relations, such as synonymy, co-hyponomy and analogy (Mikolov et al., 2013c; Turney and Pantel, 2010). The recent evaluation of Baroni et al. (2014b) suggests that the C-BOW model introduced by Mikolov et al. (2013a) is, consistently, the best across many tasks.1 Interestingly, C-BOW vectors are estimated with a simple compositional approach: The weights of adjacent words are jointly optimized so that their sum will predict the distribution of their contexts. This is reminiscent of how the parameters of some compositional distributional semanC-PHRASE word representations outperform C-BOW on several word-level benchmarks. In addition, because they are estimated in a compositional way, C-PHRASE word vectors, when combined through simple ad"
P15-1094,S14-2141,0,0.0210173,"Missing"
P15-1094,W13-3214,0,0.0177365,"lect and/or enrich the contexts that are used to build representations of target units. On the other hand, we use syntax to determine the target units that we build representations for (in the sense that we jointly learn representations of their constituents). The focus is thus on unrelated aspects of model induction, and we could indeed use syntax-mediated contexts together with our phrasing strategy. Currently, given eat (red apples), we treat eat as window-based context of red apples, but we could also take the context to be object-of-eat. Supervision Unlike many recent composition models (Kalchbrenner and Blunsom, 2013; Kalchbrenner et al., 2014; Socher et al., 2012; Socher et al., 2013, among others), the context-prediction objective of C-PHRASE does not require annotated data, and it is meant to provide generalpurpose representations that can serve in different tasks. C-PHRASE vectors can also be used as initialization parameters for fully supervised, task-specific systems. Alternatively, the current unsupervised objective could be combined with task-specific supervised objectives to fine-tune CPHRASE to specific purposes. Sensitivity to syntactic structure During training, C-PHRASE is sensitive to syntac"
P15-1094,D12-1050,0,0.0187354,"ve that C-PHRASE is providing excellent word representations, (nearly) as good or better than the C-BOW vectors of Baroni and colleagues in all cases, except for ap. Whenever C-PHRASE is not close to the state of the art results, the latter relied on richer knowledge sources and/or much larger corpora (ap, esslli, an). Turning to the sentential tasks (Table 2), we first remark that using high-quality word vectors (such as C-BOW) and summing them leads to good results in all tasks, competitive with those obtained with more sophisticated composition models. This confirms the observation made by Blacoe and Lapata (2012) that simple-minded composition models are not necessarily worse than advanced approaches. Still, C-PHRASE is consistently better than C-BOW in all tasks, except sst, where the two models reach the same performance level. C-PHRASE is outperforming PV on all tasks except sick-e, where the two models have the same performance, and onwn2, where PV is slightly Comparing vector lengths of C-BOW and CPHRASE We gather some insight into how the C-PHRASE objective might adjust word representations for composition with respect to C-BOW by looking at how the length of word vectors changes across the two"
P15-1094,P14-1062,0,0.107055,"y because C-PHRASE is trained to predict how the contexts of a word change based on its phrasal collocates (cup will have very different contexts in world cup vs. coffee cup ). At the same time, because the vectors are optimized based on their occurrence in phrases of different syntactic complexity, they produce good sentence representations when they are combined. To the best of our knowledge, C-PHRASE is the first model that is jointly optimized for lexical and compositional tasks. C-BOW uses shallow composition information to learn word vectors. Conversely, some compositional models –e.g., Kalchbrenner et al. (2014), Socher et al. (2013)– induce word representations, that are only optimized for a compositional task and are not tested at the lexical level. Somewhat relatedly to what we do, Hill et al. (2014) evaluated representations learned in a sentence translation task on wordlevel benchmarks. Some a priori justification for treating word and sentence learning as joint problems comes from human language acquisition, as it is obvious that children learn word and phrase meanings in parallel and interactively, not sequentially (Tomasello, 2003). Figure 1: C-PHRASE context prediction objective for the phra"
P15-1094,W02-0908,0,0.00893521,"73 Lapata, 2010),3 but C-PHRASE is both more effective in compositional tasks (see evaluation below), and it has the further advantage that it learns its own word vectors, thus reducing the number of arbitrary choices to be made in modeling. Comparison with traditional syntax-sensitive word representations Syntax has often been exploited in distributional semantics for a richer characterization of context. By relying on a syntactic parse of the input corpus, a distributional model can take more informative contexts such as subject-of-eat vs. object-of-eat into account (Baroni and Lenci, 2010; Curran and Moens, 2002; Grefenstette, 1994; Erk and Pad´o, 2008; Levy and Goldberg, 2014a; Pad´o and Lapata, 2007; Rothenh¨ausler and Sch¨utze, 2009). In this approach, syntactic information serves to select and/or enrich the contexts that are used to build representations of target units. On the other hand, we use syntax to determine the target units that we build representations for (in the sense that we jointly learn representations of their constituents). The focus is thus on unrelated aspects of model induction, and we could indeed use syntax-mediated contexts together with our phrasing strategy. Currently, gi"
P15-1094,P14-1009,1,0.906632,"dy mentioned, sentence vectors are built by summing the vectors of the words in them. In lexical tasks, we compare our model to the best C-BOW model from Baroni et al. (2014b),7 and to a Skip-gram model built using the same hyperparameters as C-PHRASE (that also led to the best MEN-train results for Skip-gram). In sentential tasks, we compare our model against adding the best C-BOW vectors pretrained by Baroni and colleagues,8 and adding our Skip-gram vectors. We compare the additive approaches to two sophisticated composition models. The first is the Practical Lexical Function (PLF) model of Paperno et al. (2014). This is a linguistically motivated model in the tradition of the “functional composition” approaches of Coecke et al. (2010) and Baroni et al. (2014a), and the only model in this line of research that has been shown to empirically scale up to real-life sentence challenges. In short, in the PLF model all words are represented by vectors. Words acting as argument-taking functions (such as verbs or adjectives) are also associated to one matrix for each argument they take (e.g., each transitive verb has a subject and an object matrix). Vector representations of arguments are recursively multipli"
P15-1094,P03-1054,0,0.015708,"vectors composed by a model as features, and report accuracy on the test set. State of the art is obtained by Le and Mikolov (2014) with the Paragraph Vector approach we describe below. 3.2 Model implementation The source corpus we use to build the lexical vectors is created by concatenating three sources: ukWaC,4 a mid-2009 dump of the English Wikipedia5 and the British National Corpus6 (about 2.8B words in total). We build vectors for the 180K words occurring at least 100 times in the corpus. Since our training procedure requires parsed trees, we parse the corpus using the Stanford parser (Klein and Manning, 2003). C-PHRASE has two hyperparameters (see Section 2 above), namely basic window size (c1 ) and height-dependent window enlargement factor (c2 ). 7 For fairness, we report their results when all tasks were evaluated with the same set of parameters, tuned on rg: this is row 8 of their Table 2. 8 http://clic.cimec.unitn.it/composes/ semantic-vectors.html 4 http://wacky.sslmit.unibo.it http://en.wikipedia.org 6 http://www.natcorp.ox.ac.uk 5 976 better. C-PHRASE is outperforming PLF by a large margin on the SICK sets, whereas the two models are equal on msrvid, and PLF better on onwn2. Recall, howeve"
P15-1094,S14-2055,0,0.018685,"f people are playing football”), CONTRADICTING (“The brown horse is near a red barrel at the rodeo”/“The brown horse is far from a red barrel at the rodeo”) or NEUTRAL (“A man in a black jacket is doing tricks on a motorbike”/”A person is riding the bicycle on one wheel”). For each model, we train a simple SVM classifier based on 2 features: cosine similarity between the two sentence vectors, as given by the models, and whether the sentence pair contains a negation word (the latter has been shown to be a very informative feature for SICK entailment). The current state-of-the-art is reached by Lai and Hockenmaier (2014), using a much richer set of features, that include WordNet, the denotation graph of Young et al. (2014) and extra training data from other resources. Moreover, following Mikolov et al. (2013b), during training we sub-sample less informative, very frequent words: this option is controlled by a parameter t, resulting in aggressive subsampling of words with relative frequency above it. We tune on MEN-train, obtaining c1 = 5, c2 = 2 and t = 10−5 . As already mentioned, sentence vectors are built by summing the vectors of the words in them. In lexical tasks, we compare our model to the best C-BOW"
P15-1094,D14-1162,0,0.0949051,"are estimated in a compositional way, C-PHRASE word vectors, when combined through simple addition, produce sentence representations that are better than those obtained when adding other kinds of vectors, and competitive against ad-hoc compositional methods on various sentence meaning benchmarks. 2 1 We refer here not only to the results reported in Baroni et al. (2014b), but also to the more extensive evaluation that Baroni and colleagues present in the companion website (http://clic.cimec.unitn. it/composes/semantic-vectors.html). The experiments there suggest that only the Glove vectors of Pennington et al. (2014) are competitive with C-BOW, and only when trained on a corpus several orders of magnitude larger than the one used for C-BOW. The C-PHRASE model We start with a brief overview of the models proposed by Mikolov et al. (2013a), as C-PHRASE builds on them. The Skip-gram model derives the vector of a target word by setting its weights to predict the words surrounding it in the corpus. 971 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 971–981, c Beijing, China, July 26-31, 201"
P15-1094,W09-0203,0,0.0207922,"Missing"
P15-1094,P14-2050,0,0.0170664,"tional tasks (see evaluation below), and it has the further advantage that it learns its own word vectors, thus reducing the number of arbitrary choices to be made in modeling. Comparison with traditional syntax-sensitive word representations Syntax has often been exploited in distributional semantics for a richer characterization of context. By relying on a syntactic parse of the input corpus, a distributional model can take more informative contexts such as subject-of-eat vs. object-of-eat into account (Baroni and Lenci, 2010; Curran and Moens, 2002; Grefenstette, 1994; Erk and Pad´o, 2008; Levy and Goldberg, 2014a; Pad´o and Lapata, 2007; Rothenh¨ausler and Sch¨utze, 2009). In this approach, syntactic information serves to select and/or enrich the contexts that are used to build representations of target units. On the other hand, we use syntax to determine the target units that we build representations for (in the sense that we jointly learn representations of their constituents). The focus is thus on unrelated aspects of model induction, and we could indeed use syntax-mediated contexts together with our phrasing strategy. Currently, given eat (red apples), we treat eat as window-based context of red"
P15-1094,D12-1110,0,0.572472,"us (more precisely, it only requires the constituent structure assigned by the parser, as it is blind to syntactic labels). Both large unannotated corpora and efficient pre-trained parsers are available for many languages, making the CPHRASE knowledge demands feasible for practical purposes. There is no need to parse the sentences we want to build representations for at test time, since the component word vectors are simply added. The only parameters of the model are the word vectors; specifically, no extra parameters are needed for composition (composition models such as the one presented in Socher et al. (2012) require an extra parameter matrix for each word in the vocabulary, and even leaner models such as the one of Guevara (2010) must estimate a parameter matrix for each composition rule in the grammar). This makes C-PHRASE as simple as additive and multiplicative composition (Mitchell and Joint optimization of word and phrase vectors The C-PHRASE hierarchical learning objective 973 Lapata, 2010),3 but C-PHRASE is both more effective in compositional tasks (see evaluation below), and it has the further advantage that it learns its own word vectors, thus reducing the number of arbitrary choices to"
P15-1094,W14-1618,0,0.0159209,"tional tasks (see evaluation below), and it has the further advantage that it learns its own word vectors, thus reducing the number of arbitrary choices to be made in modeling. Comparison with traditional syntax-sensitive word representations Syntax has often been exploited in distributional semantics for a richer characterization of context. By relying on a syntactic parse of the input corpus, a distributional model can take more informative contexts such as subject-of-eat vs. object-of-eat into account (Baroni and Lenci, 2010; Curran and Moens, 2002; Grefenstette, 1994; Erk and Pad´o, 2008; Levy and Goldberg, 2014a; Pad´o and Lapata, 2007; Rothenh¨ausler and Sch¨utze, 2009). In this approach, syntactic information serves to select and/or enrich the contexts that are used to build representations of target units. On the other hand, we use syntax to determine the target units that we build representations for (in the sense that we jointly learn representations of their constituents). The focus is thus on unrelated aspects of model induction, and we could indeed use syntax-mediated contexts together with our phrasing strategy. Currently, given eat (red apples), we treat eat as window-based context of red"
P15-1094,D13-1170,0,0.130106,"d to predict how the contexts of a word change based on its phrasal collocates (cup will have very different contexts in world cup vs. coffee cup ). At the same time, because the vectors are optimized based on their occurrence in phrases of different syntactic complexity, they produce good sentence representations when they are combined. To the best of our knowledge, C-PHRASE is the first model that is jointly optimized for lexical and compositional tasks. C-BOW uses shallow composition information to learn word vectors. Conversely, some compositional models –e.g., Kalchbrenner et al. (2014), Socher et al. (2013)– induce word representations, that are only optimized for a compositional task and are not tested at the lexical level. Somewhat relatedly to what we do, Hill et al. (2014) evaluated representations learned in a sentence translation task on wordlevel benchmarks. Some a priori justification for treating word and sentence learning as joint problems comes from human language acquisition, as it is obvious that children learn word and phrase meanings in parallel and interactively, not sequentially (Tomasello, 2003). Figure 1: C-PHRASE context prediction objective for the phrase small cat and its c"
P15-1094,S14-2001,1,0.944885,"al semantic relatedness Similarly to word relatedness, composed sentence representations can be evaluated against benchmarks where humans provided relatedness/similarity scores for sentence pairs (sentences with high scores, such as “A person in a black jacket is doing tricks on a motorbike”/“A man in a black jacket is doing tricks on a motorbike” from the SICK data-set, tend to be near-paraphrases). Following previous work on these data sets, Pearson correlation is our figure of merit, and we report it between human scores and sentence vector cosine similarities computed by the models. SICK (Marelli et al., 2014) (sick-r) was created specifically for the purpose of evaluating compositional models, focusing on linguistic phenomena such as lexical variation and word order. Here we report performance of the systems on the test part of the data set, which contains 5K sentence pairs. The top performance (from the SICK SemEval shared task) was reached by Zhao et al. (2014) using a heterogeneous set of features that include WordNet and extra training corpora. Agirre et al. (2012) and Agirre et al. (2013) created two collections of sentential similarities consisting of subsets coming from different sources. F"
P15-1094,W14-1619,0,0.0137214,"p-gram learns each word representation separately, the C-BOW model takes their combination into account. More precisely, it tries to predict a context word from the combination of the previous and following words, where the combination method is vector addition. The objective function is: X T 1X log p(wt |wt−c ..wt−1 , wt+1 ..wt+c ) T (2) X  log p(wl−j |C[wl , · · · , wr ]) C[wl ,··· ,wr ]∈T 1≤j≤cC  + log p(wr+j |C[wl , · · · , wr ]) (3) t=1 While other distributional models consider sequences of words jointly as context when estimating the parameters for a single word (Agirre et al., 2009; Melamud et al., 2014), C-BOW is unique in that it estimates the weights of a sequence of words jointly, based on their shared context. In this respect, C-BOW extends the distributional hypothesis (Harris, 1954) that words with similar context distributions should have similar meanings to longer sequences. However, the word combinations of C-BOW are not natural linguistic constituents, but arbitrary n-grams (e.g., sequences of 5 words with a gap in the middle). Moreover, the model does not attempt to capture the hierarchical nature of syntactic phrasing, such that big brown dog is a meaningful phrase, but so are it"
P15-1094,Q14-1006,0,0.00380654,"horse is far from a red barrel at the rodeo”) or NEUTRAL (“A man in a black jacket is doing tricks on a motorbike”/”A person is riding the bicycle on one wheel”). For each model, we train a simple SVM classifier based on 2 features: cosine similarity between the two sentence vectors, as given by the models, and whether the sentence pair contains a negation word (the latter has been shown to be a very informative feature for SICK entailment). The current state-of-the-art is reached by Lai and Hockenmaier (2014), using a much richer set of features, that include WordNet, the denotation graph of Young et al. (2014) and extra training data from other resources. Moreover, following Mikolov et al. (2013b), during training we sub-sample less informative, very frequent words: this option is controlled by a parameter t, resulting in aggressive subsampling of words with relative frequency above it. We tune on MEN-train, obtaining c1 = 5, c2 = 2 and t = 10−5 . As already mentioned, sentence vectors are built by summing the vectors of the words in them. In lexical tasks, we compare our model to the best C-BOW model from Baroni et al. (2014b),7 and to a Skip-gram model built using the same hyperparameters as C-PH"
P15-1094,S14-2044,0,0.0118281,"data-set, tend to be near-paraphrases). Following previous work on these data sets, Pearson correlation is our figure of merit, and we report it between human scores and sentence vector cosine similarities computed by the models. SICK (Marelli et al., 2014) (sick-r) was created specifically for the purpose of evaluating compositional models, focusing on linguistic phenomena such as lexical variation and word order. Here we report performance of the systems on the test part of the data set, which contains 5K sentence pairs. The top performance (from the SICK SemEval shared task) was reached by Zhao et al. (2014) using a heterogeneous set of features that include WordNet and extra training corpora. Agirre et al. (2012) and Agirre et al. (2013) created two collections of sentential similarities consisting of subsets coming from different sources. From these, we pick the Microsoft Research video description dataset (msrvid), where near paraphrases are descriptions of the same short video, and the OnWN 2012 (onwn1) and OnWN 2013 (onwn2) data sets (each of these sets contains 750 pairs). The latter are quite different from other sentence relatedness benchmarks, since they compare definitions for the same"
P15-1094,N13-1090,0,0.648132,"ontext-predictions at different levels of the syntactic hierarchy. For example, given training sentence “A sad dog is howling in the park”, C-PHRASE will optimize context prediction for dog, sad dog, a sad dog, a sad dog is howling, etc., but not, for example, for howling in, as these two words do not form a syntactic constituent by themselves. Introduction Distributional semantic models, that induce vector-based meaning representations from patterns of co-occurrence of words in corpora, have proven very successful at modeling many lexical relations, such as synonymy, co-hyponomy and analogy (Mikolov et al., 2013c; Turney and Pantel, 2010). The recent evaluation of Baroni et al. (2014b) suggests that the C-BOW model introduced by Mikolov et al. (2013a) is, consistently, the best across many tasks.1 Interestingly, C-BOW vectors are estimated with a simple compositional approach: The weights of adjacent words are jointly optimized so that their sum will predict the distribution of their contexts. This is reminiscent of how the parameters of some compositional distributional semanC-PHRASE word representations outperform C-BOW on several word-level benchmarks. In addition, because they are estimated in a"
P15-1094,J07-2002,0,0.0358854,"Missing"
P15-1094,S12-1051,0,\N,Missing
P15-2004,N13-1090,0,0.305596,"contrast into word embeddings, but performance on a general semantic relatedness task decreased dramati21 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 21–26, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 0 are respectively the target and where vw and vw context vector representations of word w, and W is the number of words in the vocabulary. To avoid the O(|W |) time complexity of the normalization term in Equation (2), Mikolov et al. (2013) use either hierarchical softmax or negative sampling. Here, we adopt the negative sampling method. effect on semantic vectors, leading to state-of-theart results in a challenging similarity task, and enabling better learning of a compositional negation function. Our work is also closely related to Faruqui et al. (2015), who propose an algorithm to adapt pretrained DSM representations using semantic resources such as WordNet. This post-processing approach, while extremely effective, has the disadvantage that changes only affect words that are present in the resource, without propagating to the"
P15-2004,2014.lilt-9.5,1,0.217464,"ectives in Skip-gram vs mLCM spaces 5 Learning Negation Skip-gram mLCM Having shown that injecting lexical contrast information into word embeddings is beneficial for lexical tasks, we further explore if it can also help composition. Since mLCM makes contrasting and compatible words more distinguishable from each other, we conjecture that it would be easier for compositional DSMs to capture negation in mLCM space. We perform a proof-of-concept experiment where we represent not as a function that is trained to map an adjective to its antonym (good to bad). That is, by adopting the framework of Baroni et al. (2014), we take not to be a matrix that, when multiplied with an adjectiverepresenting vector, returns the vector of an adjective with the opposite meaning. We realize that this is capturing only a tiny fraction of the linguistic uses of negation, but it is at least a concrete starting point. First, we select a list of adjectives and antonyms from WordNet; for each adjective, we only pick the antonym of its first sense. This yields a total of around 4,000 antonym pairs. Then, we induce the not matrix with least-squares regression on training pairs. Finally, we assess the learned negation function by"
P15-2004,N15-1100,0,0.166856,"Missing"
P15-2004,N15-1184,0,0.0792554,"6-31, 2015. 2015 Association for Computational Linguistics 0 are respectively the target and where vw and vw context vector representations of word w, and W is the number of words in the vocabulary. To avoid the O(|W |) time complexity of the normalization term in Equation (2), Mikolov et al. (2013) use either hierarchical softmax or negative sampling. Here, we adopt the negative sampling method. effect on semantic vectors, leading to state-of-theart results in a challenging similarity task, and enabling better learning of a compositional negation function. Our work is also closely related to Faruqui et al. (2015), who propose an algorithm to adapt pretrained DSM representations using semantic resources such as WordNet. This post-processing approach, while extremely effective, has the disadvantage that changes only affect words that are present in the resource, without propagating to the whole lexicon. Other recent work has instead adopted multitask objectives similar to ours in order to directly plug in knowledge from structured resources at DSM induction time (Fried and Duh, 2015; Xu et al., 2014; Yu and Dredze, 2014). Our main novelties with respect to these proposals are the focus on capturing sema"
P15-2004,Y14-1018,0,0.214665,"ndamental phenomenon of negation at the phrasal and sentential levels (the distributional vectors for good and not good are nearly identical) (Hermann et al., 2013; Preller and Sadrzadeh, 2011). Mohammad and colleagues concluded that DSMs alone cannot detect semantic contrast, and proposed an approach that couples them with other resources. Pure-DSM solutions include isolating contexts that are expected to be more discriminative of contrast, tuning the similarity measure to make it more sensitive to contrast or training a supervised contrast classifier on DSM vectors (Adel and Sch¨utze, 2014; Santus et al., 2014; Schulte im Walde and K¨oper, 2013; Turney, 2008). We propose instead to induce word vectors using a multitask cost function combining a traditional DSM context-prediction objective with a term forcing words to be closer to their WordNet synonyms than to their antonyms. In this way, we make the model aware that contrasting words such as hot and cold, while still semantically related, should not be nearest neighbours in the space. In a similar spirit, Yih et al. (2012) devise a DSM in which the embeddings of the antonyms of a word are pushed to be the vectors that are farthest away from its re"
P15-2004,W13-3209,0,0.0149064,"into Distributional Semantics Nghia The Pham Angeliki Lazaridou Marco Baroni Center for Mind/Brain Sciences University of Trento {thenghia.pham|angeliki.lazaridou|marco.baroni}@unitn.it Abstract (black is very close to both dark and white in distributional semantic space, but it implies the former while contradicting the latter). Beyond wordlevel relations, the same difficulties make it challenging for compositional extensions of DSMs to capture the fundamental phenomenon of negation at the phrasal and sentential levels (the distributional vectors for good and not good are nearly identical) (Hermann et al., 2013; Preller and Sadrzadeh, 2011). Mohammad and colleagues concluded that DSMs alone cannot detect semantic contrast, and proposed an approach that couples them with other resources. Pure-DSM solutions include isolating contexts that are expected to be more discriminative of contrast, tuning the similarity measure to make it more sensitive to contrast or training a supervised contrast classifier on DSM vectors (Adel and Sch¨utze, 2014; Santus et al., 2014; Schulte im Walde and K¨oper, 2013; Turney, 2008). We propose instead to induce word vectors using a multitask cost function combining a tradit"
P15-2004,J15-4004,0,0.200772,"Missing"
P15-2004,C08-1114,0,0.0127986,"ential levels (the distributional vectors for good and not good are nearly identical) (Hermann et al., 2013; Preller and Sadrzadeh, 2011). Mohammad and colleagues concluded that DSMs alone cannot detect semantic contrast, and proposed an approach that couples them with other resources. Pure-DSM solutions include isolating contexts that are expected to be more discriminative of contrast, tuning the similarity measure to make it more sensitive to contrast or training a supervised contrast classifier on DSM vectors (Adel and Sch¨utze, 2014; Santus et al., 2014; Schulte im Walde and K¨oper, 2013; Turney, 2008). We propose instead to induce word vectors using a multitask cost function combining a traditional DSM context-prediction objective with a term forcing words to be closer to their WordNet synonyms than to their antonyms. In this way, we make the model aware that contrasting words such as hot and cold, while still semantically related, should not be nearest neighbours in the space. In a similar spirit, Yih et al. (2012) devise a DSM in which the embeddings of the antonyms of a word are pushed to be the vectors that are farthest away from its representation. While their model is able to correct"
P15-2004,D13-1169,0,0.0787458,"Missing"
P15-2004,D12-1111,0,0.188086,"Missing"
P15-2004,P14-2089,0,0.0438408,"learning of a compositional negation function. Our work is also closely related to Faruqui et al. (2015), who propose an algorithm to adapt pretrained DSM representations using semantic resources such as WordNet. This post-processing approach, while extremely effective, has the disadvantage that changes only affect words that are present in the resource, without propagating to the whole lexicon. Other recent work has instead adopted multitask objectives similar to ours in order to directly plug in knowledge from structured resources at DSM induction time (Fried and Duh, 2015; Xu et al., 2014; Yu and Dredze, 2014). Our main novelties with respect to these proposals are the focus on capturing semantic contrast, and explicitly testing the hypothesis that the multitask objective is also beneficial to words that are not directly exposed to WordNet evidence during training.2 2 Injecting lexical contrast information We account for lexical contrast by implementing a 2-task strategy, combining the Skip-gram context prediction objective with a new term: T 1X (Jskipgram (wt ) + Jlc (wt )) (3) T t=1 The lexical contrast objective Jlc (wt ) tries to enforce the constraint that contrasting pairs should have lower s"
P15-2004,J13-3004,0,\N,Missing
P15-2004,D14-1151,0,\N,Missing
P15-2004,N15-1097,1,\N,Missing
P15-2004,N15-1098,0,\N,Missing
P16-1144,D15-1075,0,0.0561594,"n about one fifth of the cases, the annotators could not guess the word even when the broader context was given. Thus, only a small portion of the CBT passages are really probing the model’s ability to understand the broader context, which is instead the focus of LAMBADA. The idea of a book excerpt completion task was originally introduced in the MSRCC dataset (Zweig and Burges, 2011). However, the latter limited context to single sentences, not attempting to measure broader passage understanding. Of course, text understanding can be tested through other tasks, including entailment detection (Bowman et al., 2015), answering questions about a text (Richardson et al., 2013; Weston et al., 2015) and measuring inter-clause coherence (Yin and Sch¨utze, 2015). While different tasks can provide complementary insights into the models’ abilities, we find word prediction particularly attractive because of its naturalness (it’s easy to norm the data with non-expert humans) and simplicity. Models just need to be trained to predict the most likely word given the previous context, following the classic language modeling paradigm, which is a much simpler setup than the one required, say, to determine whether two sen"
P16-1144,P82-1020,0,0.82838,"Missing"
P16-1144,C14-3002,0,0.0424602,"Missing"
P16-1144,P15-1007,0,0.0212447,"Missing"
P16-1144,D13-1020,0,0.058654,"guess the word even when the broader context was given. Thus, only a small portion of the CBT passages are really probing the model’s ability to understand the broader context, which is instead the focus of LAMBADA. The idea of a book excerpt completion task was originally introduced in the MSRCC dataset (Zweig and Burges, 2011). However, the latter limited context to single sentences, not attempting to measure broader passage understanding. Of course, text understanding can be tested through other tasks, including entailment detection (Bowman et al., 2015), answering questions about a text (Richardson et al., 2013; Weston et al., 2015) and measuring inter-clause coherence (Yin and Sch¨utze, 2015). While different tasks can provide complementary insights into the models’ abilities, we find word prediction particularly attractive because of its naturalness (it’s easy to norm the data with non-expert humans) and simplicity. Models just need to be trained to predict the most likely word given the previous context, following the classic language modeling paradigm, which is a much simpler setup than the one required, say, to determine whether two sentences entail each other. Moreover, models can have access"
P16-1144,N15-1020,0,0.0259137,"ately, the system responses are appropriate for the respective questions. However, when taken together, they are incoherent. The system behaviour is somewhat parrot-like. It can locally produce perfectly sensible language fragments, but it fails to take the meaning of the broader discourse context into account. Much research effort has consequently focused on designing systems able to keep information from the broader context into memory, and possibly even perform simple forms of reasoning about it (Hermann et al., 2015; Hochreiter and Schmidhuber, 1997; Ji et al., 2015; Mikolov et al., 2015; Sordoni et al., 2015; Sukhbaatar et al., 2015; Wang and Cho, 2015, a.o.). In this paper, we introduce the LAMBADA dataset (LAnguage Modeling Broadened to Account for Discourse Aspects). LAMBADA proposes a word prediction task where the target item is difficult to guess (for English speakers) when only the sentence in which it appears is available, but becomes easy when a broader context is presented. Consider Example (1) in Figure 1. The sentence Do you honestly think that I would want you to have a ? has a multitude of possible continuations, but the broad context clearly indicates that the missing word is misca"
P16-2035,D10-1040,0,0.0768403,"Missing"
P16-2035,D14-1086,0,0.134532,"discriminative attributes, i.e., properties that distinguish them (has_tail). Moreover, although supervision is only provided in terms of discriminativeness of attributes for pairs, the model learns to assign plausible attributes to specific objects (SOFA-has_cushion). Finally, we present a preliminary experiment confirming the referential success of the predicted discriminative attributes. 1 made_of_wood is_green is_round Figure 1: Discriminative attributes predicted by our model. Can you identify the intended referent? See Section 6 for more information Reiter, 1995; Mitchell et al., 2010; Kazemzadeh et al., 2014). A necessary condition for achieving successful reference is that referring expressions (REs) accurately distinguish the intended referent from any other object in the context (Dale and Haddock, 1991). Along these lines, we present here a model that, given an intended referent and a context object, predicts the attributes that discriminate between the two. Some examples of the behaviour of the model are presented in Figure 1. Importantly, and distinguishing our work from earlier literature on generating REs (Krahmer and Van Deemter, 2012): (i) the input objects are represented by natural imag"
P16-2035,J12-1006,0,0.0513171,"Missing"
P16-2035,W10-4210,0,0.0664488,"Missing"
P16-2035,P13-1056,0,0.122586,"Missing"
P16-2035,N13-1127,0,0.0132907,"tion There has recently been renewed interest in developing systems capable of genuine language understanding (Hermann et al., 2015; Hill et al., 2015). In this perspective, it is important to think of an appropriate general framework for teaching language to machines. Since we use language primarily for communication, a reasonable approach is to develop systems within a genuine communicative setup (Steels, 2003; Mikolov et al., 2015). Out long-term goal is thus to develop communities of computational agents that learn how to use language efficiently in order to achieve communicative success (Vogel et al., 2013; Foerster et al., 2016). Within this general picture, one fundamental aspect of meaning where communication is indeed crucial is the act of reference (Searle, 1969; Abbott, 2010), the ability to successfully talk to others about things in the external world. A specific instantiation of reference studied in this paper is that of referring expression generation (Dale and 2 Discriminative Attribute Dataset We generated the Discriminative Attribute Dataset, consisting of pairs of (intended) referents and contexts, with respect to which the referents should be identified by their distinctive attri"
P18-1198,P17-1080,0,0.270596,"Tense and TopConst. It is not clear that they controlled for the same factors we considered (in particular, lexical overlap and 2133 Figure 2: Spearman correlation matrix between probing and downstream tasks. Correlations based on all sentence embeddings we investigated (more than 40). Cells in gray denote task pairs that are not significantly correlated (after correcting for multiple comparisons). sentence length), and they use much smaller training sets, limiting classifier-based evaluation to logistic regression. Moreover, they test a smaller set of models, focusing on machine translation. Belinkov et al. (2017a), Belinkov et al. (2017b) and Dalvi et al. (2017) are also interested in understanding the type of linguistic knowledge encoded in sentence and word embeddings, but their focus is on word-level morphosyntax and lexical semantics, and specifically on NMT encoders and decoders. Sennrich (2017) also focuses on NMT systems, and proposes a contrastive test to assess how they handle various linguistic phenomena. Other work explores the linguistic behaviour of recurrent networks and related models by using visualization, input/hidden representation deletion techniques or by looking at the word-by-w"
P18-1198,I17-1001,0,0.0335861,"Tense and TopConst. It is not clear that they controlled for the same factors we considered (in particular, lexical overlap and 2133 Figure 2: Spearman correlation matrix between probing and downstream tasks. Correlations based on all sentence embeddings we investigated (more than 40). Cells in gray denote task pairs that are not significantly correlated (after correcting for multiple comparisons). sentence length), and they use much smaller training sets, limiting classifier-based evaluation to logistic regression. Moreover, they test a smaller set of models, focusing on machine translation. Belinkov et al. (2017a), Belinkov et al. (2017b) and Dalvi et al. (2017) are also interested in understanding the type of linguistic knowledge encoded in sentence and word embeddings, but their focus is on word-level morphosyntax and lexical semantics, and specifically on NMT encoders and decoders. Sennrich (2017) also focuses on NMT systems, and proposes a contrastive test to assess how they handle various linguistic phenomena. Other work explores the linguistic behaviour of recurrent networks and related models by using visualization, input/hidden representation deletion techniques or by looking at the word-by-w"
P18-1198,D15-1075,0,0.013006,"sentences. Following Vinyals et al. (2015), we train a seq2seq architecture to generate linearized grammatical parse trees (see Table 1) from source sentences (Seq2Tree). We use the Stanford parser to generate trees for Europarl source English sentences. We train SkipThought vectors (Kiros et al., 2015) by predicting the next sentence given the current one (Tang et al., 2017), on 30M sentences from the Toronto Book Corpus, excluding those in the probing sets. Finally, following Conneau et al. (2017), we train sentence encoders on Natural Language Inference using the concatenation of the SNLI (Bowman et al., 2015) and MultiNLI (Bowman et al., 2015) data sets (about 1M sentence pairs). In this task, a sentence encoder is trained to encode two sentences, which are fed to a classifier and whose role is to distinguish whether the sentences are contradictory, neutral or entailed. Finally, as in Conneau et al. (2017), we also include Untrained encoders with random weights, which act as random projections of pre-trained word embeddings. 3.3 Training details BiLSTM encoders use 2 layers of 512 hidden units (∼4M parameters), Gated ConvNet has 8 convolutional layers of 512 hidden units, kernel size 3 (∼12M param"
P18-1198,D17-1070,1,0.640675,"=1,...,T , a bidirectional LSTM computes a set of T vectors {ht }t . For t ∈ [1, . . . , T ], ht is the concatenation of a forward LSTM and a backward LSTM that read the sentences in two opposite directions. We experiment with two ways of combining the varying number of (h1 , . . . , hT ) to form a fixed-size vector, either by selecting the last hidden state of hT or by selecting the maximum value over each dimension of the hidden units. The choice of these models are motivated by their demonstrated efficiency in seq2seq (Sutskever et al., 2014) and universal sentence representation learning (Conneau et al., 2017), respectively.2 Gated ConvNet We also consider the nonrecurrent convolutional equivalent of LSTMs, based on stacked gated temporal convolutions. Gated convolutional networks were shown to perform well as neural machine translation encoders (Gehring et al., 2017) and language modeling decoders (Dauphin et al., 2017). The encoder is composed of an input word embedding table that is augmented with positional encodings (Sukhbaatar et al., 2015), followed by a stack of temporal convolutions with small kernel size. The output of each convolutional layer is filtered by a gating mechanism, similar to"
P18-1198,I17-1015,0,0.0114727,"led for the same factors we considered (in particular, lexical overlap and 2133 Figure 2: Spearman correlation matrix between probing and downstream tasks. Correlations based on all sentence embeddings we investigated (more than 40). Cells in gray denote task pairs that are not significantly correlated (after correcting for multiple comparisons). sentence length), and they use much smaller training sets, limiting classifier-based evaluation to logistic regression. Moreover, they test a smaller set of models, focusing on machine translation. Belinkov et al. (2017a), Belinkov et al. (2017b) and Dalvi et al. (2017) are also interested in understanding the type of linguistic knowledge encoded in sentence and word embeddings, but their focus is on word-level morphosyntax and lexical semantics, and specifically on NMT encoders and decoders. Sennrich (2017) also focuses on NMT systems, and proposes a contrastive test to assess how they handle various linguistic phenomena. Other work explores the linguistic behaviour of recurrent networks and related models by using visualization, input/hidden representation deletion techniques or by looking at the word-by-word behaviour of the network (e.g., Nagamine et al."
P18-1198,P03-1054,0,0.0358665,"y contain to their hierarchical structure to subtle facets of semantic acceptability. We think the current task set is reasonably representative of different linguistic domains, but we are not claiming that it is exhaustive. We expect future work to extend it. The sentences for all our tasks are extracted from the Toronto Book Corpus (Zhu et al., 2015), more specifically from the random pre-processed portion made available by Paperno et al. (2016). We only sample sentences in the 5-to-28 word range. We parse them with the Stanford Parser (2017-06-09 version), using the pre-trained PCFG model (Klein and Manning, 2003), and we rely on the part-of-speech, constituency and dependency parsing information provided by this tool where needed. For each task, we construct training sets containing 100k sentences, and 10k-sentence val1 https://github.com/facebookresearch/ SentEval/tree/master/data/probing idation and test sets. All sets are balanced, having an equal number of instances of each target class. Surface information These tasks test the extent to which sentence embeddings are preserving surface properties of the sentences they encode. One can solve the surface tasks by simply looking at tokens in the input"
P18-1198,2005.mtsummit-papers.11,0,0.00574583,"irectional LSTM, with consistently poorer results. max-pooling along the temporal dimension is performed on the output feature maps of the last convolution (Collobert and Weston, 2008). 3.2 Training tasks Seq2seq systems have shown strong results in machine translation (Zhou et al., 2016). They consist of an encoder that encodes a source sentence into a fixed-size representation, and a decoder which acts as a conditional language model and that generates the target sentence. We train Neural Machine Translation systems on three language pairs using about 2M sentences from the Europarl corpora (Koehn, 2005). We pick English-French, which involves two similar languages, English-German, involving larger syntactic differences, and English-Finnish, a distant pair. We also train with an AutoEncoder objective (Socher et al., 2011) on Europarl source English sentences. Following Vinyals et al. (2015), we train a seq2seq architecture to generate linearized grammatical parse trees (see Table 1) from source sentences (Seq2Tree). We use the Stanford parser to generate trees for Europarl source English sentences. We train SkipThought vectors (Kiros et al., 2015) by predicting the next sentence given the cur"
P18-1198,P07-2045,0,0.0125781,"Missing"
P18-1198,S14-2055,0,0.00993521,"s require complex forms of inference, making it difficult to pinpoint the information a model is relying upon. Impressive as it might be that a system can tell that the sentence “A movie that doesn’t aim too high, but it doesn’t need to” (Pang and Lee, 2004) expresses a subjective viewpoint, it is Guillaume Lample Facebook AI Research Sorbonne Universités glample@fb.com Marco Baroni Facebook AI Research mbaroni@fb.com hard to tell how the system (or even a human) comes to this conclusion. Complex tasks can also carry hidden biases that models might lock onto (Jabri et al., 2016). For example, Lai and Hockenmaier (2014) show that the simple heuristic of checking for explicit negation words leads to good accuracy in the SICK sentence entailment task. Model introspection techniques have been applied to sentence encoders in order to gain a better understanding of which properties of the input sentences their embeddings retain (see Section 5). However, these techniques often depend on the specifics of an encoder architecture, and consequently cannot be used to compare different methods. Shi et al. (2016) and Adi et al. (2017) introduced a more general approach, relying on the notion of what we will call probing"
P18-1198,N16-1082,0,0.017319,"standing the type of linguistic knowledge encoded in sentence and word embeddings, but their focus is on word-level morphosyntax and lexical semantics, and specifically on NMT encoders and decoders. Sennrich (2017) also focuses on NMT systems, and proposes a contrastive test to assess how they handle various linguistic phenomena. Other work explores the linguistic behaviour of recurrent networks and related models by using visualization, input/hidden representation deletion techniques or by looking at the word-by-word behaviour of the network (e.g., Nagamine et al., 2015; Hupkes et al., 2017; Li et al., 2016; Linzen et al., 2016; Kàdàr et al., 2017; Li et al., 2017). These methods, complementary to ours, are not agnostic to encoder architecture, and cannot be used for general-purpose cross-model evaluation. Finally, Conneau et al. (2017) propose a largescale, multi-task evaluation of sentence embeddings, focusing entirely on downstream tasks. 6 Conclusion We introduced a set of tasks probing the linguistic knowledge of sentence embedding methods. Their purpose is not to encourage the development of ad-hoc models that attain top performance on them, but to help exploring what information is captur"
P18-1198,Q16-1037,0,0.140678,"of linguistic knowledge encoded in sentence and word embeddings, but their focus is on word-level morphosyntax and lexical semantics, and specifically on NMT encoders and decoders. Sennrich (2017) also focuses on NMT systems, and proposes a contrastive test to assess how they handle various linguistic phenomena. Other work explores the linguistic behaviour of recurrent networks and related models by using visualization, input/hidden representation deletion techniques or by looking at the word-by-word behaviour of the network (e.g., Nagamine et al., 2015; Hupkes et al., 2017; Li et al., 2016; Linzen et al., 2016; Kàdàr et al., 2017; Li et al., 2017). These methods, complementary to ours, are not agnostic to encoder architecture, and cannot be used for general-purpose cross-model evaluation. Finally, Conneau et al. (2017) propose a largescale, multi-task evaluation of sentence embeddings, focusing entirely on downstream tasks. 6 Conclusion We introduced a set of tasks probing the linguistic knowledge of sentence embedding methods. Their purpose is not to encourage the development of ad-hoc models that attain top performance on them, but to help exploring what information is captured by different pre-t"
P18-1198,marelli-etal-2014-sick,1,0.538324,"downstream tasks after WC. We observe intriguing asymmetries: SOMO correlates with the SICK-E sentence entailment test, but not with SICK-R, which is about modeling sentence relatedness intuitions. Indeed, logical entailment requires deeper semantic analysis than modeling similarity judgments. TopConst and the number tasks negatively correlate with various similarity and sentiment data sets (SST, STS, SICK-R). This might expose biases in these tasks: SICK-R, for example, deliberately contains sentence pairs with opposite voice, that will have different constituent structure but equal meaning (Marelli et al., 2014). It might also mirrors genuine factors affecting similarity judgments (e.g., two sentences differing only in object number are very similar). Remarkably, TREC question type classification is the downstream task correlating with most probing tasks. Question classification is certainly an outlier among our downstream tasks, but we must leave a full understanding of this behaviour to future work (this is exactly the sort of analysis our probing tasks should stimulate). 5 Related work Adi et al. (2017) introduced SentLen, WC and a word order test, focusing on a bag-of-vectors baseline, an autoenc"
P18-1198,L18-1008,0,0.0218728,"pairs). In this task, a sentence encoder is trained to encode two sentences, which are fed to a classifier and whose role is to distinguish whether the sentences are contradictory, neutral or entailed. Finally, as in Conneau et al. (2017), we also include Untrained encoders with random weights, which act as random projections of pre-trained word embeddings. 3.3 Training details BiLSTM encoders use 2 layers of 512 hidden units (∼4M parameters), Gated ConvNet has 8 convolutional layers of 512 hidden units, kernel size 3 (∼12M parameters). We use pre-trained fastText word embeddings of size 300 (Mikolov et al., 2018) without fine-tuning, to isolate the impact of encoder architectures and to handle words outside the training sets. Training task performance and further details are in Appendix. 2129 task source I myself was out on an island in the AutoEncoder Swedish archipelago , at Sandhamn . I myself was out on an island in the NMT En-Fr Swedish archipelago , at Sandhamn . We really need to up our particular conNMT En-De tribution in that regard . It is too early to see one system as a uniNMT En-Fi versal panacea and dismiss another . SkipThought the old sami was gone , and he was a different person now ."
P18-1198,N13-1090,0,0.18687,"Missing"
P18-1198,P04-1035,0,0.0348809,"Missing"
P18-1198,P16-1144,1,0.804551,"up with a set of tasks that, while respecting the previous constraints, probe a wide range of phenomena, from superficial properties of sentences such as which words they contain to their hierarchical structure to subtle facets of semantic acceptability. We think the current task set is reasonably representative of different linguistic domains, but we are not claiming that it is exhaustive. We expect future work to extend it. The sentences for all our tasks are extracted from the Toronto Book Corpus (Zhu et al., 2015), more specifically from the random pre-processed portion made available by Paperno et al. (2016). We only sample sentences in the 5-to-28 word range. We parse them with the Stanford Parser (2017-06-09 version), using the pre-trained PCFG model (Klein and Manning, 2003), and we rely on the part-of-speech, constituency and dependency parsing information provided by this tool where needed. For each task, we construct training sets containing 100k sentences, and 10k-sentence val1 https://github.com/facebookresearch/ SentEval/tree/master/data/probing idation and test sets. All sets are balanced, having an equal number of instances of each target class. Surface information These tasks test the"
P18-1198,N18-1101,0,0.0165034,"Missing"
P18-1198,D14-1162,0,0.123956,"Missing"
P18-1198,P15-1094,1,0.909464,"Missing"
P18-1198,Q16-1027,0,0.0181797,"composed of an input word embedding table that is augmented with positional encodings (Sukhbaatar et al., 2015), followed by a stack of temporal convolutions with small kernel size. The output of each convolutional layer is filtered by a gating mechanism, similar to the one of LSTMs. Finally, 2 We also experimented with a unidirectional LSTM, with consistently poorer results. max-pooling along the temporal dimension is performed on the output feature maps of the last convolution (Collobert and Weston, 2008). 3.2 Training tasks Seq2seq systems have shown strong results in machine translation (Zhou et al., 2016). They consist of an encoder that encodes a source sentence into a fixed-size representation, and a decoder which acts as a conditional language model and that generates the target sentence. We train Neural Machine Translation systems on three language pairs using about 2M sentences from the Europarl corpora (Koehn, 2005). We pick English-French, which involves two similar languages, English-German, involving larger syntactic differences, and English-Finnish, a distant pair. We also train with an AutoEncoder objective (Socher et al., 2011) on Europarl source English sentences. Following Vinyal"
P18-1198,E17-2060,0,0.00815249,"denote task pairs that are not significantly correlated (after correcting for multiple comparisons). sentence length), and they use much smaller training sets, limiting classifier-based evaluation to logistic regression. Moreover, they test a smaller set of models, focusing on machine translation. Belinkov et al. (2017a), Belinkov et al. (2017b) and Dalvi et al. (2017) are also interested in understanding the type of linguistic knowledge encoded in sentence and word embeddings, but their focus is on word-level morphosyntax and lexical semantics, and specifically on NMT encoders and decoders. Sennrich (2017) also focuses on NMT systems, and proposes a contrastive test to assess how they handle various linguistic phenomena. Other work explores the linguistic behaviour of recurrent networks and related models by using visualization, input/hidden representation deletion techniques or by looking at the word-by-word behaviour of the network (e.g., Nagamine et al., 2015; Hupkes et al., 2017; Li et al., 2016; Linzen et al., 2016; Kàdàr et al., 2017; Li et al., 2017). These methods, complementary to ours, are not agnostic to encoder architecture, and cannot be used for general-purpose cross-model evaluat"
P18-1198,D16-1159,0,0.18153,"ex tasks can also carry hidden biases that models might lock onto (Jabri et al., 2016). For example, Lai and Hockenmaier (2014) show that the simple heuristic of checking for explicit negation words leads to good accuracy in the SICK sentence entailment task. Model introspection techniques have been applied to sentence encoders in order to gain a better understanding of which properties of the input sentences their embeddings retain (see Section 5). However, these techniques often depend on the specifics of an encoder architecture, and consequently cannot be used to compare different methods. Shi et al. (2016) and Adi et al. (2017) introduced a more general approach, relying on the notion of what we will call probing tasks. A probing task is a classification problem that focuses on simple linguistic properties of sentences. For example, one such task might require to categorize sentences by the tense of their main verb. Given an encoder (e.g., an LSTM) pre-trained on a certain task (e.g., machine translation), we use the sentence embeddings it produces to train the tense classifier (without further embedding tuning). If the classifier succeeds, it means that the pre-trained encoder is storing reada"
P18-1198,J17-4003,0,\N,Missing
P18-1198,L18-1269,1,\N,Missing
P19-1380,D18-1119,1,0.836973,"as to what extent comparable functional pressures could have shaped human language, and whether deep learning models can develop human-like linguistic skills. For such inquiries to be meaningful, the designed setup should reflect as many aspects of human communication as possible. Moreover, appropriate tools should be applied to the analysis of emergent communication, since, as several recent studies have shown, agents might succeed at a task without truly relying on their communicative channel, or by means of ad-hoc communication techniques overfitting their environment (Kottur et al., 2017; Bouchacourt and Baroni, 2018; Lowe et al., 2019). We contribute on both fronts. We introduce a game meeting many desiderata for a natural communication environment. We further propose a two-pronged analysis of emerging communication, at the pragmatic and semantic levels. At the pragmatic level, we study communicative acts from a functional perspective, measuring whether the messages produced by an agent have an impact on the subsequent behaviour of the other. At the semantic level, we decode which aspects of the extra-linguistic context the agents refer to, and how such reference acts differ between agents. Some of our c"
P19-1380,D19-1384,0,0.0112769,"aracterized as dialogues, in the sense that the behaviour of each agent is affected by what the other agent says. Moreover, they use language, at least in part, to denote the objects in their environment, showing primitive hallmarks of a referential semantics. We also find, however, that agent pairs trained together in fully symmetrical conditions develop their own idiolects, such that an agent won’t (fully) understand itself in self play. As convergence to a shared code is another basic property of human language, in future research we will explore ways to make it emerge. First, we note that Graesser et al. (2019), who study a simple signaling game, similarly conclude that training single pairs of agents does not lead to the emergence of a common language, which requires diffusion in larger communities. We intend to verify if a similar trend emerges if we extend our game to larger agent 3916 groups. Conversely, equipping the agents with a feedback loop in which they also receive their own messages as input might encourage shared codes across speaker and listener roles. In the current paper, we limited ourselves to one-symbol messages, facilitating analysis but greatly reducing the spectrum of potential"
P19-1380,D17-1321,0,0.307373,"e participants. By conducting a thorough pragmatic and semantic analysis of the emergent protocol, we show that the agents solve the shared task through genuine bilateral, referential communication. However, the agents develop multiple idiolects, which makes us conclude that full symmetry is not a sufficient condition for a common language to emerge. 1 Introduction The advent of powerful deep learning architectures has revived research in simulations of language emergence among computational agents that must communicate to accomplish a task (e.g., Jorge et al., 2016; Havrylov and Titov, 2017; Kottur et al., 2017; Lazaridou et al., 2017; Lee et al., 2017; Choi et al., 2018; Evtimova et al., 2018; Lazaridou et al., 2018). The nature of the emergent communication code should provide insights on questions such as to what extent comparable functional pressures could have shaped human language, and whether deep learning models can develop human-like linguistic skills. For such inquiries to be meaningful, the designed setup should reflect as many aspects of human communication as possible. Moreover, appropriate tools should be applied to the analysis of emergent communication, since, as several recent studi"
P19-1380,D17-1259,0,0.0311474,"et al., 2017; Havrylov and Titov, 2017; Lazaridou et al., 2018). Evtimova et al. (2018) extend the referential game presenting the sender and receiver with referent views in different modalities, and allowing multiple message rounds. Still, reward is given directly for referential success, and the roles and turns of the agents are fixed. Das et al. (2017) generalize Lewis’ signaling game (Lewis, 1969) and propose a cooperative image guessing game between two agents, a question bot and an answer bot. They find that grounded language emerges without supervision. Cao et al. (2018) (expanding on Lewis et al., 2017) propose a setup where two agents see the same set of items, and each is provided with arbitrary, episode-specific utility functions for the object. The agents must converge in multi-turn conversation to a decision about how to split the items. The fundamental novelty of our game with respect to theirs is that our rewards depend on consistent, realistic commonsense knowledge that is stable across episodes (hammers are good to break hard-shell fruits, etc.). Mordatch and Abbeel (2018) (see also Lowe et al., 2017) study emergent communication among multiple (&gt; 2) agents pursuing their respective"
P19-1380,P13-1056,0,0.0302241,"to be cut, a knife is intrinsically more useful than a spoon. Note that the agents do not have any a priori knowledge of the tools utility. Yet, baseline agents are able to discover context-independent tool affordances and already reach high performance. We believe that this scenario, in which communication-transmitted information complements knowledge that can be directly inferred by observing the world, is more interesting than typical games in which language is the only information carrier. Game ingredients and utility We picked 16 tool and 31 fruit categories from McRae et al. (2005) and Silberer et al. (2013), who provide subject-elicited property-based commonsense descriptions of objects, with some extensions. We used 11 fruit and 15 tool features from these databases to represent the categories. We rescaled the elicitation-frequency-based property values provided in the norms to lie in the [0, 1] range, and manually changed some counter-intuitive values. An object instance is a property vector sampled from the corresponding category as follows. For binary properties such as has a pit, we use Bernoulli sampling with p equaling the category value. For continuous properties such as is small, we sam"
P19-1381,W18-5407,1,0.658957,"rules, suggesting that the difference between compositional and noncompositional behaviour is not clear-cut. 1 Marco Baroni ICREA Facebook AI Research mbaroni@fb.com Introduction 2 Recent deep neural network successes rekindled classic debates on their natural language processing abilities (e.g., Kirov and Cotterell, 2018; McCoy et al., 2018; Pater, 2018). Lake and Baroni (2018) and Loula et al. (2018) proposed the SCAN challenge to directly assess the ability of sequence-to-sequence networks to perform systematic, compositional generalization of linguistic rules. Their results, and those of Bastings et al. (2018), have shown that modern recurrent networks (gated RNNs, such as LSTMs and GRUs) generalize well to new sequences that resemble those encountered in training, but achieve very low performance when generalization must be supported by a systematic compositional rule, such as “to X twice you X and X” (e.g., to jump twice, you jump and jump again). Non-recurrent models, such as convolutional neural networks (CNNs, Kalchbrenner et al., 2016; Gehring et al., 2016, 2017) and selfattentive models (Vaswani et al., 2017; Chen et al., 2018) have recently reached comparable or better performance than RNNs"
P19-1381,P18-1008,0,0.0512242,"Missing"
P19-1381,D18-1458,0,0.055861,"Missing"
P19-1381,P17-1012,0,\N,Missing
P19-1381,Q18-1045,0,\N,Missing
P19-1381,W18-5413,1,\N,Missing
P19-1384,W18-6404,0,0.0147066,"entence length is accounted for? The answer to these questions promises insights into the nature of constraints on the human parser, opening new research avenues on the computational complexity of human language. 2 Language Arabic Bulgarian Catalan Danish Dutch French German Greek Hebrew Italian Latvian Portuguese Romanian Russian Slovenian Spanish Swedish Data Corpora We focus on 17 languages, selected based on data and tool availability. We annotated 2 types of large, publicly available corpora: Wikipedia dumps from March 2017 and, where available, the WMT News Crawl corpora from 2007-2017 (Bei et al., 2018). Table 1 provides basic statistics of the annotated corpora. Parsing Each corpus was tokenized using UDPipe’s (Straka et al., 2016) pre-trained UDv2.2 models (Straka, 2018) and then parsed as follows: We trained Dozat et al. (2017)’s parsing model, a state-of-the-art graph-based neural dependency parser, on the Universal Dependencies 2.2 dataset (Nivre et al., 2018). We used the hyperparameter configuration described in Dozat et al. (2017), and pre-trained FastText word embeddings for frequent words (Bojanowski et al., 2016). We are aiming to make the parsed corpora available as soon as possi"
P19-1384,L16-1680,0,0.0273389,"Missing"
P19-1509,P82-1020,0,0.780629,"Missing"
P19-1509,D17-1321,0,0.236338,"ntext, researchers have looked at how trained models solve different NLP tasks characterizing their outputs and internal representation. We instead focus directly on uncovering their “innate” biases while learning a task. We study whether LSTM-based Seq2Seq models deployed as communicating agents are subject to some of the natural pressures that characterize the typology and evolution of human languages. In this respect, we connect to the recent research line on language emergence in deep network agents that communicate to accomplish a task (e.g., Jorge et al., 2016; Havrylov and Titov, 2017; Kottur et al., 2017; Lazaridou et al., 2017; Choi et al., 2018; Evtimova et al., 2018; Lazaridou et al., 2018; Mordatch and Abbeel, 2018). Most of this work provides the agents with a basic communication channel, and evaluates task success and the emerging communication protocol in an entirely bottom-up fashion. We train instead our agents to communicate with simple languages possessing the properties we want to study, and look at whether such properties make the languages easier or harder to learn. Other studies (Lee et al., 2017b,a) had also seeded their agents with (real) languages, but for different purposes"
P19-1509,D17-1259,0,0.0334593,"f a trade-off between different ways to encode constituent roles, with redundant solutions often being preferred. The research direction we introduced might lead to a better understanding of the biases that affect the linguistic behaviour of LSTMs and simi5173 lar models. This could help current efforts towards the development of artificial agents that communicate to solve a task, with the ultimate goal of developing AIs that can talk with humans. It has been observed that the communication protocol emerging in such simulations is very different from human language (e.g., Kottur et al., 2017; Lewis et al., 2017; Bouchacourt and Baroni, 2018). A better understanding of what are the “innate” biases of standard models in highly controlled setups, such as the one studied here, should complement large-scale simulations, as part of the effort to develop new methods to encourage the emergence of more human-like language. For example, our results suggest that current neural networks, as they are not subject to human-like least-effort constraints, might not display the same trend towards efficient communication that we encounter in natural languages. How to incorporate “effort”-based pressures in neural netw"
P19-1509,D18-1119,1,\N,Missing
poesio-etal-2010-babyexp,J07-2002,0,\N,Missing
poesio-etal-2010-babyexp,P98-2127,0,\N,Missing
poesio-etal-2010-babyexp,C98-2122,0,\N,Missing
Q15-1014,D07-1090,0,0.011612,"ect, as induced from text, independently of whether they are depicted in an image. Although the gold annotation of our dataset should already penalize this imageindependent labeling strategy (see Section 2.4), we control for this behaviour by comparing against three models that have access to the gold noun annotations of the image and favor adjectives that are typical modifiers of the nouns. LM We build a bigram Language Model by using the Berkeley LM toolkit (Pauls and Klein, 2012)12 on the one-trillion-token Google Web1T corpus13 and smooth probabilities with the “Stupid” backoff technique (Brants et al., 2007). Given an image with object-noun annotation, we score all attributes-adjectives based on the language-modelderived conditional probability p(adjective|noun). All images of the same object produce identical rankings. As an example, among the top attributes of cocktail we find heady, creamy and fruity. V LM LM does not exploit visual information about the image to be annotated. A natural way to enhance it is to combine it with D I R O , our crossmodal mapping adjective retrieval method. In the visually-enriched Language Model, we interpolate (using equal weights) the ranks produced by the two m"
Q15-1014,P14-1059,1,0.479602,"representations that significantly improve performance in supervised object recognition. 1 ∗ Current affiliation: Thomas J. Watson Research Center, IBM, gdinu@us.ibm.com In this paper, we assume that, just like nouns are the linguistic counterpart of visual objects, visual attributes are expressed by adjectives. An informal survey of the relevant literature suggests that, when attributes have linguistic labels, they are indeed mostly expressed by adjectives. There are some attributes, such as parts, that are more naturally expressed by prepositional phrases (PPs: with a tail). Interestingly, Dinu and Baroni (2014) showed that the decomposition function we will adopt here can derive both adjective-noun and noun-PP phrases, suggesting that our approach could be seamlessly extended to visual attributes expressed by noun-modifying PPs. 183 Transactions of the Association for Computational Linguistics, vol. 3, pp. 183–196, 2015. Action Editor: Patrick Pantel. Submission batch: 9/2014; Revision batch 1/2015; Revision batch 3/2015; Published 3/2015. c 2015 Association for Computational Linguistics. Distributed under a CC-BY-NC-SA 4.0 license. mantic intuition (Silberer et al., 2013). Moreover, automated attri"
Q15-1014,J10-4007,0,0.0350205,"Missing"
Q15-1014,P14-2118,0,0.0146249,"et ramekin, wooden meat). We thus have solid quantitative support that the superiority of D EC is partially due to how it learns to jointly account for adjective and noun semantics, producing phrases that are linguistically more meaningful. Adjective concreteness We can gain further insight into the nature of the adjectives chosen by the models by considering the fact that phrases that are meant to describe an object in a picture should mostly contain concrete adjectives, and thus the degree of concreteness of the adjectives produced by a model is an indirect measure of its quality. Following Hill and Korhonen (2014), we define the concreteness of an adjective as the average concreteness score of the nouns it modifies in our text corpus. Noun concreteness scores are taken, in turn, from Turney et al. (2011). For each test image and model, we obtain a concreteness score by averaging the concreteness of the top 5 adjectives that the model selected for the image. Figure 4 reports the distributions of the resulting scores across models. We confirm that the purely language-based models (LM, SP) are producing generic abstract adjectives that are not appropriate to describe images (e.g., cryptographic key, homem"
Q15-1014,P14-1132,1,0.900652,"Missing"
Q15-1014,E12-1076,0,0.0267251,"ectives through Decompositional Distributional Semantics Angeliki Lazaridou Georgiana Dinu∗ Adam Liska Marco Baroni Center for Mind/Brain Sciences University of Trento {angeliki.lazaridou|georgiana.dinu|adam.liska|marco.baroni}@unitn.it Abstract 1 Introduction As the quality of image analysis algorithms improves, there is increasing interest in annotating images with linguistic descriptions ranging from single words describing the depicted objects and their properties (Farhadi et al., 2009; Lampert et al., 2009) to richer expressions such as full-fledged image captions (Kulkarni et al., 2011; Mitchell et al., 2012). This trend has generated wide interest in linguistic annotations beyond concrete nouns, with the role of adjectives in image descriptions receiving, in particular, much attention. Adjectives are of special interest because of their central role in so-called attribute-centric image representations. This framework views objects as bundles of properties, or attributes, commonly expressed by adjectives (e.g., furry, brown), and uses the latter as features to learn higher-level, semantically richer representations of objects (Farhadi et al., 2009).1 Attribute-based methods achieve better generali"
Q15-1014,P12-1101,0,0.0130632,"els A cross-modal function trained exclusively on object-noun data might be able to capture only prototypical characteristics of an object, as induced from text, independently of whether they are depicted in an image. Although the gold annotation of our dataset should already penalize this imageindependent labeling strategy (see Section 2.4), we control for this behaviour by comparing against three models that have access to the gold noun annotations of the image and favor adjectives that are typical modifiers of the nouns. LM We build a bigram Language Model by using the Berkeley LM toolkit (Pauls and Klein, 2012)12 on the one-trillion-token Google Web1T corpus13 and smooth probabilities with the “Stupid” backoff technique (Brants et al., 2007). Given an image with object-noun annotation, we score all attributes-adjectives based on the language-modelderived conditional probability p(adjective|noun). All images of the same object produce identical rankings. As an example, among the top attributes of cocktail we find heady, creamy and fruity. V LM LM does not exploit visual information about the image to be annotated. A natural way to enhance it is to combine it with D I R O , our crossmodal mapping adje"
Q15-1014,P13-1056,0,0.031647,"th a tail). Interestingly, Dinu and Baroni (2014) showed that the decomposition function we will adopt here can derive both adjective-noun and noun-PP phrases, suggesting that our approach could be seamlessly extended to visual attributes expressed by noun-modifying PPs. 183 Transactions of the Association for Computational Linguistics, vol. 3, pp. 183–196, 2015. Action Editor: Patrick Pantel. Submission batch: 9/2014; Revision batch 1/2015; Revision batch 3/2015; Published 3/2015. c 2015 Association for Computational Linguistics. Distributed under a CC-BY-NC-SA 4.0 license. mantic intuition (Silberer et al., 2013). Moreover, automated attribute annotation can facilitate finergrained image retrieval (e.g., searching for a rocky beach rather than a sandy beach) and provide the basis for more accurate image search (for example in cases of visual sense disambiguation (Divvala et al., 2014), where a user disambiguates their query by searching for images of wooden cabinet as furniture and not just cabinet, which can also mean council). Classic attribute-centric image analysis requires, however, extensive manual and often domainspecific annotation of attributes (Vedaldi et al., 2014), or, at best, complex uns"
Q15-1014,D11-1063,0,0.0262802,"s that are linguistically more meaningful. Adjective concreteness We can gain further insight into the nature of the adjectives chosen by the models by considering the fact that phrases that are meant to describe an object in a picture should mostly contain concrete adjectives, and thus the degree of concreteness of the adjectives produced by a model is an indirect measure of its quality. Following Hill and Korhonen (2014), we define the concreteness of an adjective as the average concreteness score of the nouns it modifies in our text corpus. Noun concreteness scores are taken, in turn, from Turney et al. (2011). For each test image and model, we obtain a concreteness score by averaging the concreteness of the top 5 adjectives that the model selected for the image. Figure 4 reports the distributions of the resulting scores across models. We confirm that the purely language-based models (LM, SP) are producing generic abstract adjectives that are not appropriate to describe images (e.g., cryptographic key, homemade bread, Greek salad, beaten yolk). The image-informed V LM and D I R O models produce considerably more concrete adjectives. Not surprisingly, D I R A , that was directly trained on concrete"
Q15-1027,J10-4006,1,0.827321,"age performance (among comparable alternatives) on a variety of semantic relatedness/similarity tasks, such as synonymy detection, concept categorization and analogy solving. If the same vectors turn out to also serve as good inputs for constructing Boolean representations, we are thus getting the best of both worlds: distributional vectors with proven high performance on relatedness/similarity tasks which can 3 http://clic.cimec.unitn.it/composes/ semantic-vectors.html 379 be mapped into a Boolean space to tackle logicrelated tasks. We also experiment with the pretrained vectors from TypeDM (Baroni and Lenci, 2010),4 which are built by exploiting syntactic information, and should have different qualitative properties from the window-based approaches. The count vectors of Baroni and colleagues are built from a 2-word-window co-occurrence matrix of 300k lower-cased words extracted from a 2.8 billion tokens corpus. The matrix is weighted using positive Pointwise Mutual Information (Church and Hanks, 1990). We use the full 300k×300k positive PMI matrix to compute the asymmetric similarity measures discussed in the next section, since the latter are designed for non-negative, sparse, full-rank representation"
Q15-1027,W11-2501,1,0.630635,"imal elephant → herbivore Negative Positive ape 9 book animal 9 elephant elephant 9 hippo elephant 9 trunk A man is slowly trekking A group of scouts are in the woods → The man camping in the grass 9 is hiking in the woods A group of scouts are hiking through the grass Table 2: SICK sentence entailment examples. Table 1: Lexical entailment examples. very strong baseline here. We thus explore a more challenging setup, LEDS-dir, where we replace the negative examples of LEDS-core by positive pairs in reverse order, thus focusing on entailment direction. We derive two more benchmarks from BLESS (Baroni and Lenci, 2011). BLESS lists pairs of concepts linked by one of 5 possible relations: coordinates, hypernymy, meronymy, attributes and events. We employed this resource to construct BLESScoord, which –unlike LEDS, where entailing pairs have to be distinguished from pairs of words that, mostly, bear no relation– is composed of 1,236 super-subordinate pairs (which we treat as positive examples) to be distinguished from 3,526 coordinate pairs. BLESS-mero has the same positive examples, but 2,943 holo-meronyms pairs as negatives. Examples of all lexical benchmarks are given in Table 1. Sentence entailment To eva"
Q15-1027,E12-1004,1,0.933935,"rs boils down to contextual inclusion, and there is no reason to think that a hypernym should occur in all the contexts in which its hyponyms appear. For example, bark can be a typical context for dog, but we don’t expect to find it a significant number of times with mammal even in a very large corpus. In practice distributional inclusion turns out to be a weak tool for recognizing the entailment relation (Erk, 2009; Santus et al., 2014) because denotational and distributional inclusion are independent properties. More recently, several authors have explored supervised methods. In particular, Baroni et al. (2012), Roller et al. (2014) and Weeds et al. (2014) show that a Support Vector Machine trained on the distributional vectors of entailing or non-entailing pairs outperform the distributional inclusion measures. In our experiments, we will use this method as the main comparison point. The similarly supervised approach of Turney and Mohammad (2014) assumes the representational framework of Turney (2012), and we do not attempt to re-implement it here. Very recently, other properties of distributional vectors, such as entropy (Santus et al., 2014) and topical coherence (Rimell, 2014), have been propose"
Q15-1027,2014.lilt-9.5,1,0.193961,"the papers with concrete implementations reviewed above tries, like us, to learn a Boolean structure where entailment corresponds to inclusion. A paper that does attempt to exploit a similar idea is Young et al. (2014), which also uses the notion of model from Formal Semantics to recognize entailment based on denotations of words and phrases. However, since the denotations in their approach are ultimately derived from humangenerated captions of images, the method does not generalize to concepts that are not exemplified in the training database. Finally, a number of studies, both theoretical (Baroni et al., 2014a; Coecke et al., 2010) and empirical (Paperno et al., 2014; Polajnar et al., 2014), adapt compositional methods from formal semantics to distributional vectors, in order to derive representa377 tions of phrases and sentences. This line of research applies formal operations to distributional representations, whereas we derive formal-semantics-like representations from distributional ones. Below, we apply our method to input sentence vectors constructed with the composition algorithm of Paperno et al. (2014). 3 The Boolean Distributional Semantic Model We build the Boolean Distributional Semant"
Q15-1027,P14-1023,1,0.836365,"the papers with concrete implementations reviewed above tries, like us, to learn a Boolean structure where entailment corresponds to inclusion. A paper that does attempt to exploit a similar idea is Young et al. (2014), which also uses the notion of model from Formal Semantics to recognize entailment based on denotations of words and phrases. However, since the denotations in their approach are ultimately derived from humangenerated captions of images, the method does not generalize to concepts that are not exemplified in the training database. Finally, a number of studies, both theoretical (Baroni et al., 2014a; Coecke et al., 2010) and empirical (Paperno et al., 2014; Polajnar et al., 2014), adapt compositional methods from formal semantics to distributional vectors, in order to derive representa377 tions of phrases and sentences. This line of research applies formal operations to distributional representations, whereas we derive formal-semantics-like representations from distributional ones. Below, we apply our method to input sentence vectors constructed with the composition algorithm of Paperno et al. (2014). 3 The Boolean Distributional Semantic Model We build the Boolean Distributional Semant"
Q15-1027,S13-1002,0,0.0347246,"Rimell, 2014), have been proposed as entailment cues. Since they are not based on feature inclusion, we see them as complementary, rather than alternative to our proposal. Formal and distributional semantic models We try to derive a structured representation inspired by formal semantic theories from data-driven distributional semantic models. Combining the two approaches has proven a hard task. Some systems adopt logic-based representations but use distributional evidence for predicate disambiguation (Lewis and Steedman, 2013) or to weight probabilistic inference rules (Garrette et al., 2013; Beltagy et al., 2013). Other authors propose ways to encode aspects of logic-based representations such as logical connectives and truth values (Grefenstette, 2013) or predicate-argument structure (Clark and Pulman, 2007) in a vector-based framework. These studies are, however, entirely theoretical. Rockt¨aschel et al. (2015) expand on the first, allowing for some generalization to unseen knowledge, by introducing some degree of fuzziness into the representations of predicates and terms. Still, this work does not attempt to map concepts to a logic-based representation nor tries to exploit the wealth of information"
Q15-1027,D12-1050,0,0.0241057,"s nonentailing. The Word Overlap method (WO) calculates the number of words in common between two sentences and classifies them as entailing whenever the ratio is above a certain threshold (calibrated on SICK-train). Results are given in Table 4. Because of class unbalance, F1 is more informative than accuracy (the Majority baseline reaches the best accuracy with 0 precision and recall), so we focus on the former for analysis. We observe first that sentence vectors obtained with the additive model are consistently outperforming the more sophisticated plf approach. This confirms the results of Blacoe and Lapata (2012) on the effectiveness of simple composition methods. We leave it to further studies to determine to what extent this can be attributed to specific characteristics of SICK that make word order information redundant, and to what extent it indicates that plf is not exploiting syntactic information adequately (note that Paperno et al. (2014) report minimal performance differences between additive and plf for their msrvid benchmark, that is the closest to SICK). Coming now to the crucial comparison of BDSM against SVM (focusing on the results obtained with the additive method), BDSM emerges as the"
Q15-1027,J90-1003,0,0.33702,"ty tasks which can 3 http://clic.cimec.unitn.it/composes/ semantic-vectors.html 379 be mapped into a Boolean space to tackle logicrelated tasks. We also experiment with the pretrained vectors from TypeDM (Baroni and Lenci, 2010),4 which are built by exploiting syntactic information, and should have different qualitative properties from the window-based approaches. The count vectors of Baroni and colleagues are built from a 2-word-window co-occurrence matrix of 300k lower-cased words extracted from a 2.8 billion tokens corpus. The matrix is weighted using positive Pointwise Mutual Information (Church and Hanks, 1990). We use the full 300k×300k positive PMI matrix to compute the asymmetric similarity measures discussed in the next section, since the latter are designed for non-negative, sparse, full-rank representations. Due to efficiency constraints, for BDSM and SVM (also presented next), the matrix is reduced to 300 dimensions by Singular Value Decomposition (Sch¨utze, 1997). The experiments of Baroni et al. (2014b) with these very same vectors suggest that SVD is lowering performance somewhat. So we are, if anything, giving an advantage to the simple asymmetric measures. The predict vectors are built w"
Q15-1027,J12-1002,0,0.00988092,"on. This is based on the intuitive idea – the so-called distributional inclusion hypothesis – that the features (vector dimensions) of a hypernym and a hyponym should be in a superset-subset relation, analogously to what we are trying to achieve in the Boolean space we induce, but directly applied to distributional vectors (Geffet and Dagan, 2005; Kotlerman et al., 2010; Lenci and Benotto, 2012; Weeds et al., 2004). It has been noticed that distributional context inclusion defines a Boolean structure on vectors just as entailment defines a Boolean structure on formal semantic representations (Clarke, 2012). However, the match between context inclusion and entailment is far from perfect. 376 First, distributional vectors are real-valued and contain way more nuanced information than simply inclusion or exclusion of certain features. Second, and more fundamentally, the information encoded in distributional vectors is simply not of the right kind since “feature inclusion” for distributional vectors boils down to contextual inclusion, and there is no reason to think that a hypernym should occur in all the contexts in which its hyponyms appear. For example, bark can be a typical context for dog, but"
Q15-1027,W09-3711,0,0.0274895,"sion of certain features. Second, and more fundamentally, the information encoded in distributional vectors is simply not of the right kind since “feature inclusion” for distributional vectors boils down to contextual inclusion, and there is no reason to think that a hypernym should occur in all the contexts in which its hyponyms appear. For example, bark can be a typical context for dog, but we don’t expect to find it a significant number of times with mammal even in a very large corpus. In practice distributional inclusion turns out to be a weak tool for recognizing the entailment relation (Erk, 2009; Santus et al., 2014) because denotational and distributional inclusion are independent properties. More recently, several authors have explored supervised methods. In particular, Baroni et al. (2012), Roller et al. (2014) and Weeds et al. (2014) show that a Support Vector Machine trained on the distributional vectors of entailing or non-entailing pairs outperform the distributional inclusion measures. In our experiments, we will use this method as the main comparison point. The similarly supervised approach of Turney and Mohammad (2014) assumes the representational framework of Turney (2012)"
Q15-1027,W13-0109,0,0.0397104,"fic words are pushed to the bottom. Interestingly, while vehicle and organization were present in the training data, that was not the case for media or press. Moreover, the training data did not contain any specific type of car (like volvo or suv) or newspaper (tribune or tabloid). 7.2 Similarity in Boolean space From the model-theoretical point of view, wordto-BDSM mapping provides an interpretation function in the logical sense, mapping linguistic expressions to elements of the model domain (Boolean dimensions). If distributional vectors relate to concepts in a (hyper)intensional construal (Erk, 2013), Boolean vectors could encode their (possible) extensions along the lines suggested by Grefenstette (2013), with vector dimensions corresponding to entities in the domain of discourse.13 Under the exten12 Due to tagging errors, the neighbors also include some verbs like parked or adjectives like weekly. 13 In fact everything we say here applies equally well to certain intensional interpretations of Boolean vectors. For example, the atoms of the Boolean algebra could correspond not to entities in the actual world but to classes of individuals across 385 sional interpretation, the Boolean vecto"
Q15-1027,P05-1014,0,0.25771,"to automatically recognize entailment between words or sentences (Dagan et al., 2009). On the other hand, some researchers relying on distributional representations of meaning have attempted to apply various versions of the notion of feature inclusion to entailment detection. This is based on the intuitive idea – the so-called distributional inclusion hypothesis – that the features (vector dimensions) of a hypernym and a hyponym should be in a superset-subset relation, analogously to what we are trying to achieve in the Boolean space we induce, but directly applied to distributional vectors (Geffet and Dagan, 2005; Kotlerman et al., 2010; Lenci and Benotto, 2012; Weeds et al., 2004). It has been noticed that distributional context inclusion defines a Boolean structure on vectors just as entailment defines a Boolean structure on formal semantic representations (Clarke, 2012). However, the match between context inclusion and entailment is far from perfect. 376 First, distributional vectors are real-valued and contain way more nuanced information than simply inclusion or exclusion of certain features. Second, and more fundamentally, the information encoded in distributional vectors is simply not of the ri"
Q15-1027,S13-1001,0,0.491886,"ternative to our proposal. Formal and distributional semantic models We try to derive a structured representation inspired by formal semantic theories from data-driven distributional semantic models. Combining the two approaches has proven a hard task. Some systems adopt logic-based representations but use distributional evidence for predicate disambiguation (Lewis and Steedman, 2013) or to weight probabilistic inference rules (Garrette et al., 2013; Beltagy et al., 2013). Other authors propose ways to encode aspects of logic-based representations such as logical connectives and truth values (Grefenstette, 2013) or predicate-argument structure (Clark and Pulman, 2007) in a vector-based framework. These studies are, however, entirely theoretical. Rockt¨aschel et al. (2015) expand on the first, allowing for some generalization to unseen knowledge, by introducing some degree of fuzziness into the representations of predicates and terms. Still, this work does not attempt to map concepts to a logic-based representation nor tries to exploit the wealth of information contained in distributional vectors. Socher et al. (2013), Bordes et al. (2012) and Jenatton et al. (2012) try to discover unseen facts from a"
Q15-1027,P14-2135,0,0.0127098,"chieved by distributional vector L1 norm (which, surprisingly, has positive correlations: ρ=13 with depth, ρ=21 with concreteness) and word frequency (ρ=-2 with depth, ρ=4 with concreteness). We visualize how Boolean activation correlates with generality in Figure 4. We plot the two example words car and newspaper together with their 30 nearest nominal neighbours in distributional 10 With count input representations, our tuning favoured relatively dense 100-dimensional vectors (see Section 4.4). 11 Automatically determining the degree of abstractness of concepts is a lively topic of research (Kiela et al., 2014; Turney et al., 2011). Figure 4: Boolean activation (percentage of positive dimensions) of the 30 nearest distributional neighbours of car and newspaper. space,12 sorting them from most to least activated. More general words do indeed cluster towards the top, while more specific words are pushed to the bottom. Interestingly, while vehicle and organization were present in the training data, that was not the case for media or press. Moreover, the training data did not contain any specific type of car (like volvo or suv) or newspaper (tribune or tabloid). 7.2 Similarity in Boolean space From the"
Q15-1027,S12-1012,0,0.63038,"ords or sentences (Dagan et al., 2009). On the other hand, some researchers relying on distributional representations of meaning have attempted to apply various versions of the notion of feature inclusion to entailment detection. This is based on the intuitive idea – the so-called distributional inclusion hypothesis – that the features (vector dimensions) of a hypernym and a hyponym should be in a superset-subset relation, analogously to what we are trying to achieve in the Boolean space we induce, but directly applied to distributional vectors (Geffet and Dagan, 2005; Kotlerman et al., 2010; Lenci and Benotto, 2012; Weeds et al., 2004). It has been noticed that distributional context inclusion defines a Boolean structure on vectors just as entailment defines a Boolean structure on formal semantic representations (Clarke, 2012). However, the match between context inclusion and entailment is far from perfect. 376 First, distributional vectors are real-valued and contain way more nuanced information than simply inclusion or exclusion of certain features. Second, and more fundamentally, the information encoded in distributional vectors is simply not of the right kind since “feature inclusion” for distributi"
Q15-1027,Q13-1015,0,0.0818183,"ties of distributional vectors, such as entropy (Santus et al., 2014) and topical coherence (Rimell, 2014), have been proposed as entailment cues. Since they are not based on feature inclusion, we see them as complementary, rather than alternative to our proposal. Formal and distributional semantic models We try to derive a structured representation inspired by formal semantic theories from data-driven distributional semantic models. Combining the two approaches has proven a hard task. Some systems adopt logic-based representations but use distributional evidence for predicate disambiguation (Lewis and Steedman, 2013) or to weight probabilistic inference rules (Garrette et al., 2013; Beltagy et al., 2013). Other authors propose ways to encode aspects of logic-based representations such as logical connectives and truth values (Grefenstette, 2013) or predicate-argument structure (Clark and Pulman, 2007) in a vector-based framework. These studies are, however, entirely theoretical. Rockt¨aschel et al. (2015) expand on the first, allowing for some generalization to unseen knowledge, by introducing some degree of fuzziness into the representations of predicates and terms. Still, this work does not attempt to ma"
Q15-1027,S14-2001,1,0.83889,"ymy, attributes and events. We employed this resource to construct BLESScoord, which –unlike LEDS, where entailing pairs have to be distinguished from pairs of words that, mostly, bear no relation– is composed of 1,236 super-subordinate pairs (which we treat as positive examples) to be distinguished from 3,526 coordinate pairs. BLESS-mero has the same positive examples, but 2,943 holo-meronyms pairs as negatives. Examples of all lexical benchmarks are given in Table 1. Sentence entailment To evaluate the models on recognizing entailment between sentences, we use a benchmark derived from SICK (Marelli et al., 2014b). The original data set contains pairs of sentences in entailment, contradiction and neutral relations. We focus on recognizing entailment, treating both contradictory and neutral pairs as negative examples (as in the classic RTE shared tasks up to 2008).7 Data are divided into a development set (SICK-dev) with 500 sentence pairs (144 positive, 356 negative), a training set (SICK-train) with 4,500 pairs (1,299 positive, 3,201 negative) and a test set (SICK-test) with 4,927 pairs (1,414 positive, 3,513 negative). Examples from SICK are given in 7 This prevents a direct comparison with the res"
Q15-1027,marelli-etal-2014-sick,1,0.690543,"ymy, attributes and events. We employed this resource to construct BLESScoord, which –unlike LEDS, where entailing pairs have to be distinguished from pairs of words that, mostly, bear no relation– is composed of 1,236 super-subordinate pairs (which we treat as positive examples) to be distinguished from 3,526 coordinate pairs. BLESS-mero has the same positive examples, but 2,943 holo-meronyms pairs as negatives. Examples of all lexical benchmarks are given in Table 1. Sentence entailment To evaluate the models on recognizing entailment between sentences, we use a benchmark derived from SICK (Marelli et al., 2014b). The original data set contains pairs of sentences in entailment, contradiction and neutral relations. We focus on recognizing entailment, treating both contradictory and neutral pairs as negative examples (as in the classic RTE shared tasks up to 2008).7 Data are divided into a development set (SICK-dev) with 500 sentence pairs (144 positive, 356 negative), a training set (SICK-train) with 4,500 pairs (1,299 positive, 3,201 negative) and a test set (SICK-test) with 4,927 pairs (1,414 positive, 3,513 negative). Examples from SICK are given in 7 This prevents a direct comparison with the res"
Q15-1027,P14-1009,1,0.94305,"ries, like us, to learn a Boolean structure where entailment corresponds to inclusion. A paper that does attempt to exploit a similar idea is Young et al. (2014), which also uses the notion of model from Formal Semantics to recognize entailment based on denotations of words and phrases. However, since the denotations in their approach are ultimately derived from humangenerated captions of images, the method does not generalize to concepts that are not exemplified in the training database. Finally, a number of studies, both theoretical (Baroni et al., 2014a; Coecke et al., 2010) and empirical (Paperno et al., 2014; Polajnar et al., 2014), adapt compositional methods from formal semantics to distributional vectors, in order to derive representa377 tions of phrases and sentences. This line of research applies formal operations to distributional representations, whereas we derive formal-semantics-like representations from distributional ones. Below, we apply our method to input sentence vectors constructed with the composition algorithm of Paperno et al. (2014). 3 The Boolean Distributional Semantic Model We build the Boolean Distributional Semantic Model (BDSM) by mapping real-valued vectors from a distr"
Q15-1027,D14-1111,0,0.0203038,"n a Boolean structure where entailment corresponds to inclusion. A paper that does attempt to exploit a similar idea is Young et al. (2014), which also uses the notion of model from Formal Semantics to recognize entailment based on denotations of words and phrases. However, since the denotations in their approach are ultimately derived from humangenerated captions of images, the method does not generalize to concepts that are not exemplified in the training database. Finally, a number of studies, both theoretical (Baroni et al., 2014a; Coecke et al., 2010) and empirical (Paperno et al., 2014; Polajnar et al., 2014), adapt compositional methods from formal semantics to distributional vectors, in order to derive representa377 tions of phrases and sentences. This line of research applies formal operations to distributional representations, whereas we derive formal-semantics-like representations from distributional ones. Below, we apply our method to input sentence vectors constructed with the composition algorithm of Paperno et al. (2014). 3 The Boolean Distributional Semantic Model We build the Boolean Distributional Semantic Model (BDSM) by mapping real-valued vectors from a distributional semantic model"
Q15-1027,E14-1054,0,0.0115508,"particular, Baroni et al. (2012), Roller et al. (2014) and Weeds et al. (2014) show that a Support Vector Machine trained on the distributional vectors of entailing or non-entailing pairs outperform the distributional inclusion measures. In our experiments, we will use this method as the main comparison point. The similarly supervised approach of Turney and Mohammad (2014) assumes the representational framework of Turney (2012), and we do not attempt to re-implement it here. Very recently, other properties of distributional vectors, such as entropy (Santus et al., 2014) and topical coherence (Rimell, 2014), have been proposed as entailment cues. Since they are not based on feature inclusion, we see them as complementary, rather than alternative to our proposal. Formal and distributional semantic models We try to derive a structured representation inspired by formal semantic theories from data-driven distributional semantic models. Combining the two approaches has proven a hard task. Some systems adopt logic-based representations but use distributional evidence for predicate disambiguation (Lewis and Steedman, 2013) or to weight probabilistic inference rules (Garrette et al., 2013; Beltagy et al"
Q15-1027,N15-1118,0,0.115781,"Missing"
Q15-1027,C14-1097,0,0.392405,"Missing"
Q15-1027,E14-4008,0,0.0180339,"tain features. Second, and more fundamentally, the information encoded in distributional vectors is simply not of the right kind since “feature inclusion” for distributional vectors boils down to contextual inclusion, and there is no reason to think that a hypernym should occur in all the contexts in which its hyponyms appear. For example, bark can be a typical context for dog, but we don’t expect to find it a significant number of times with mammal even in a very large corpus. In practice distributional inclusion turns out to be a weak tool for recognizing the entailment relation (Erk, 2009; Santus et al., 2014) because denotational and distributional inclusion are independent properties. More recently, several authors have explored supervised methods. In particular, Baroni et al. (2012), Roller et al. (2014) and Weeds et al. (2014) show that a Support Vector Machine trained on the distributional vectors of entailing or non-entailing pairs outperform the distributional inclusion measures. In our experiments, we will use this method as the main comparison point. The similarly supervised approach of Turney and Mohammad (2014) assumes the representational framework of Turney (2012), and we do not attemp"
Q15-1027,D11-1063,0,0.0147333,"ional vector L1 norm (which, surprisingly, has positive correlations: ρ=13 with depth, ρ=21 with concreteness) and word frequency (ρ=-2 with depth, ρ=4 with concreteness). We visualize how Boolean activation correlates with generality in Figure 4. We plot the two example words car and newspaper together with their 30 nearest nominal neighbours in distributional 10 With count input representations, our tuning favoured relatively dense 100-dimensional vectors (see Section 4.4). 11 Automatically determining the degree of abstractness of concepts is a lively topic of research (Kiela et al., 2014; Turney et al., 2011). Figure 4: Boolean activation (percentage of positive dimensions) of the 30 nearest distributional neighbours of car and newspaper. space,12 sorting them from most to least activated. More general words do indeed cluster towards the top, while more specific words are pushed to the bottom. Interestingly, while vehicle and organization were present in the training data, that was not the case for media or press. Moreover, the training data did not contain any specific type of car (like volvo or suv) or newspaper (tribune or tabloid). 7.2 Similarity in Boolean space From the model-theoretical poi"
Q15-1027,W11-1301,1,0.0540939,"egrades more gracefully when less training data are available and displays interesting qualitative properties. 1 Introduction Different aspects of natural language semantics have been studied from different perspectives. Distributional semantic models (Turney and Pantel, 2010) induce large-scale vector-based lexical semantic representations from statistical patterns of word usage. These models have proven successful in tasks relying on meaning relatedness, such as synonymy detection (Landauer and Dumais, 1997), word sense discrimination (Sch¨utze, 1997), or even measuring phrase plausibility (Vecchi et al., 2011). On the other hand, logical relations and operations, such as entailment, contradiction, conjunction and negation, receive an elegant treatment in formal semantic models. The latter lack, however, general procedures to learn from data, and consequently have problems scaling up to real-life problems. Formal semantics captures fundamental aspects of meaning in set-theoretic terms: Entailment, for example, is captured as the inclusion relation between the sets (of the relevant type) denoted by words or other linguistic expressions, e.g., sets of possible worlds that two propositions hold of (Chi"
Q15-1027,C04-1146,0,0.59809,"Missing"
Q15-1027,C14-1212,0,0.376547,"e is no reason to think that a hypernym should occur in all the contexts in which its hyponyms appear. For example, bark can be a typical context for dog, but we don’t expect to find it a significant number of times with mammal even in a very large corpus. In practice distributional inclusion turns out to be a weak tool for recognizing the entailment relation (Erk, 2009; Santus et al., 2014) because denotational and distributional inclusion are independent properties. More recently, several authors have explored supervised methods. In particular, Baroni et al. (2012), Roller et al. (2014) and Weeds et al. (2014) show that a Support Vector Machine trained on the distributional vectors of entailing or non-entailing pairs outperform the distributional inclusion measures. In our experiments, we will use this method as the main comparison point. The similarly supervised approach of Turney and Mohammad (2014) assumes the representational framework of Turney (2012), and we do not attempt to re-implement it here. Very recently, other properties of distributional vectors, such as entropy (Santus et al., 2014) and topical coherence (Rimell, 2014), have been proposed as entailment cues. Since they are not based"
Q15-1027,Q14-1006,0,0.032197,"ding entailment), Bowman (2013) applies a softmax classifier to the combined distributional representation of two given statements, which are in turn learned compositionally in a supervised fashion in order to guess the relation between them. The paper, however, only evaluates the model on a small restricted dataset, and it is unclear whether the method would scale to realworld challenges. None of the papers with concrete implementations reviewed above tries, like us, to learn a Boolean structure where entailment corresponds to inclusion. A paper that does attempt to exploit a similar idea is Young et al. (2014), which also uses the notion of model from Formal Semantics to recognize entailment based on denotations of words and phrases. However, since the denotations in their approach are ultimately derived from humangenerated captions of images, the method does not generalize to concepts that are not exemplified in the training database. Finally, a number of studies, both theoretical (Baroni et al., 2014a; Coecke et al., 2010) and empirical (Paperno et al., 2014; Polajnar et al., 2014), adapt compositional methods from formal semantics to distributional vectors, in order to derive representa377 tions"
Q19-1033,K17-1037,0,0.0217703,"bword units. Still, his character-based systems lagged only marginally behind the subword architectures on grammatical tasks such as handling agreement and negation. Radford et al. (2017) focused on CNLMs deployed in the domain of sentiment analysis, where they found the network to specialize a unit for sentiment tracking. We will discuss below how our CNLMs also show single-unit specialization, but for boundary tracking. Godin et al. (2018) investigated the rules implicitly used by supervised character-aware neural morphological segmentation methods, finding linguistically sensible patterns. Alishahi et al. (2017) probed the linguistic knowledge induced by a neural network that receives unsegmented acoustic input. Focusing on phonology, they found that the lower layers of the model process finer-grained information, whereas higher layers are sensitive to more abstract patterns. Kementchedjhieva and Lopez (2018) recently probed the linguistic knowledge of an English CNLM trained with whitespace in the input. Their results are aligned with ours. The model is sensitive to lexical and morphological structure, and it captures morphosyntactic categories as well as constraints on possible morpheme combination"
Q19-1033,P17-1080,0,0.0247203,"/morpheme boundaries through a single specialized unit, suggesting that such boundaries are salient (at least when marked by whitespace, as in their experiments) and informative enough that it is worthwhile for the network to devote a special mechanism to process them. We replicated this finding for our networks trained on whitespacefree text, as discussed in Section 4.4, where we discuss it in the context of our other results. Probing linguistic knowledge of neural language models is currently a popular research topic (Li et al., 2016; Linzen et al., 2016; Shi et al., 2016; Adi et al., 2017; Belinkov et al., 2017; K`ad`ar et al., 2017; Hupkes et al., 2018; Conneau et al., 2018; Ettinger et al., 2018; Linzen et al., 2018). Among studies focusing on character-level models, Elman (1990) already reported a proofof-concept experiment on implicit learning of word segmentation. Christiansen et al. (1998) trained a RNN on phoneme-level language modeling of transcribed child-directed speech with tokens marking utterance boundaries, and found that the network learned to segment the input by predicting the utterance boundary symbol also at word edges. More recently, Sennrich (2017) 3 Experimental Set-up We extra"
Q19-1033,P18-1198,1,0.844481,"that such boundaries are salient (at least when marked by whitespace, as in their experiments) and informative enough that it is worthwhile for the network to devote a special mechanism to process them. We replicated this finding for our networks trained on whitespacefree text, as discussed in Section 4.4, where we discuss it in the context of our other results. Probing linguistic knowledge of neural language models is currently a popular research topic (Li et al., 2016; Linzen et al., 2016; Shi et al., 2016; Adi et al., 2017; Belinkov et al., 2017; K`ad`ar et al., 2017; Hupkes et al., 2018; Conneau et al., 2018; Ettinger et al., 2018; Linzen et al., 2018). Among studies focusing on character-level models, Elman (1990) already reported a proofof-concept experiment on implicit learning of word segmentation. Christiansen et al. (1998) trained a RNN on phoneme-level language modeling of transcribed child-directed speech with tokens marking utterance boundaries, and found that the network learned to segment the input by predicting the utterance boundary symbol also at word edges. More recently, Sennrich (2017) 3 Experimental Set-up We extracted plain text from full English, German, and Italian Wikipedia"
Q19-1033,N18-2085,0,0.018088,"e, we applied random hyperparameter search. We terminated training after 72 hours.6 None of the models had overfitted, as measured by performance on the validation set.7 Language modeling performance on the test partitions is shown in Table 1. Recall that we removed whitespace, which is both easy to predict, and aids prediction of other characters. Consequently, the fact that our character-level models are below the state of the art is expected.8 For example, the best model of Merity et al. (2018) achieved 1.23 English bits per character (BPC) on a Wikipedia-derived dataset. On EuroParl data, Cotterell et al. (2018) report 0.85 for English, 0.90 for German, and 0.82 for Italian. Still, our English BPC is comparable to that reported by Graves English German Italian LSTM RNN WordNLM 1.62 1.51 1.47 2.08 1.83 1.97 48.99 37.96 42.02 Table 1: Performance of language models. For CNLMs, we report bits-per-character (BPC). For WordNLMs, we report perplexity. (2014) for his static character-level LSTM trained on space-delimited Wikipedia data, suggesting that we are achieving reasonable performance. The perplexity of the word-level model might not be comparable to that of highly optimized stateof-the-art architect"
Q19-1033,D18-1461,0,0.0225469,"associated embeddings in a standard word-based NLM. Intriguingly, our CNLMs captured a range of lexical phenomena without anything resembling a word dictionary. Any information a CNLM might acquire about units larger than characters must be stored in its recurrent weights. This suggests a radically different and possibly more neurally plausible view of the lexicon as implicitly encoded in a distributed memory, that we intend to characterize more precisely and test in future work (similar ideas are being explored in a more applied NLP perspective, e.g., Gillick et al., 2016; Lee et al., 2017; Cherry et al., 2018). Concerning the model input, we would like to study whether the CNLM successes crucially depend on the huge amount of training data it receives. Are word priors more important when learning from smaller corpora? In terms of comparison with human learning, the Wikipedia text we fed our CNLMs is far from what children acquiring a language would hear. Future work should explore character/phoneme-level learning from child-directed speech corpora. Still, by feeding our networks ‘‘grown-up’’ prose, we are arguably making the job of identifying basic constituents harder than it might be when process"
Q19-1033,N18-1108,1,0.887209,"Missing"
Q19-1033,C18-1152,0,0.0202922,"are salient (at least when marked by whitespace, as in their experiments) and informative enough that it is worthwhile for the network to devote a special mechanism to process them. We replicated this finding for our networks trained on whitespacefree text, as discussed in Section 4.4, where we discuss it in the context of our other results. Probing linguistic knowledge of neural language models is currently a popular research topic (Li et al., 2016; Linzen et al., 2016; Shi et al., 2016; Adi et al., 2017; Belinkov et al., 2017; K`ad`ar et al., 2017; Hupkes et al., 2018; Conneau et al., 2018; Ettinger et al., 2018; Linzen et al., 2018). Among studies focusing on character-level models, Elman (1990) already reported a proofof-concept experiment on implicit learning of word segmentation. Christiansen et al. (1998) trained a RNN on phoneme-level language modeling of transcribed child-directed speech with tokens marking utterance boundaries, and found that the network learned to segment the input by predicting the utterance boundary symbol also at word edges. More recently, Sennrich (2017) 3 Experimental Set-up We extracted plain text from full English, German, and Italian Wikipedia dumps with WikiExtracto"
Q19-1033,P82-1020,0,0.668928,"Missing"
Q19-1033,Q18-1032,0,0.141324,"Missing"
Q19-1033,N16-1155,0,0.0205116,"units, not unlike the list of words and associated embeddings in a standard word-based NLM. Intriguingly, our CNLMs captured a range of lexical phenomena without anything resembling a word dictionary. Any information a CNLM might acquire about units larger than characters must be stored in its recurrent weights. This suggests a radically different and possibly more neurally plausible view of the lexicon as implicitly encoded in a distributed memory, that we intend to characterize more precisely and test in future work (similar ideas are being explored in a more applied NLP perspective, e.g., Gillick et al., 2016; Lee et al., 2017; Cherry et al., 2018). Concerning the model input, we would like to study whether the CNLM successes crucially depend on the huge amount of training data it receives. Are word priors more important when learning from smaller corpora? In terms of comparison with human learning, the Wikipedia text we fed our CNLMs is far from what children acquiring a language would hear. Future work should explore character/phoneme-level learning from child-directed speech corpora. Still, by feeding our networks ‘‘grown-up’’ prose, we are arguably making the job of identifying basic constitue"
Q19-1033,D18-1365,0,0.0414875,"Missing"
Q19-1033,D16-1097,0,0.0639974,"Missing"
Q19-1033,W18-5417,0,0.303801,"ting point to test nonword-centric models, compared with agglutinative or polysynthetic languages, where the very notion of what counts as a word is problematic. Our tasks require models to develop the latent ability to parse characters into word-like items associated to morphological, syntactic, and broadly semantic features. The RNNs pass most of the tests, suggesting that they are in some way able to construct and manipulate the right lexical objects. In a final experiment, we look more directly into how the models are handling word-like units. We find, confirming an earlier observation by Kementchedjhieva and Lopez (2018), that the RNNs specialized some cells to the task of detecting word boundaries (or, more generally, salient linguistic boundaries, in a sense to be further discussed below). Taken together, our results suggest that character-level RNNs capture forms of linguistic knowledge that are traditionally thought to be word-based, without being exposed to an explicit segmentation of their input and, more importantly, without possessing an explicit word lexicon. We will discuss the implications of these findings in the Discussion.2 2 Related Work On the primacy of words Several linguistic studies sugges"
Q19-1033,P13-2017,0,0.0183174,"Missing"
Q19-1033,K16-1006,0,0.0344863,"RNN is not successful even when trained in-domain, 476 48.0 58.9 61.4 65.1 Table 5: Results on MSR Sentence Completion. For our models (top), we show accuracies for Wikipedia (left) and in-domain (right) training. We compare with language models from prior work (left): KneserNey 5-gram model (Mikolov, 2012), Word RNN (Zweig et al., 2012), Word LSTM and LdTreeLSTM (Zhang et al., 2016). We further report models incorporating distributional encodings of semantics (right): Skipgram(+RNNs) from Mikolov et al. (2013a), the PMI-based model of Woods (2016), and the Context-Embedding-based approach of Melamud et al. (2016). contrasting with the word-based vanilla RNN from the literature, whose performance, while still below LSTMs, is much stronger. Once more, this suggests that capturing word-level generalizations with a word-lexicon-less character model requires the long-span processing abilities of an LSTM. 4.4 Boundary tracking in CNLMs The good performance of CNLMs on most tasks above suggests that, although they lack a hardcoded word vocabulary and they were trained on unsegmented input, there is enough pressure from the language modeling task for them to learn to track word-like items, and associate them"
Q19-1033,Q17-1026,0,0.0293929,"list of words and associated embeddings in a standard word-based NLM. Intriguingly, our CNLMs captured a range of lexical phenomena without anything resembling a word dictionary. Any information a CNLM might acquire about units larger than characters must be stored in its recurrent weights. This suggests a radically different and possibly more neurally plausible view of the lexicon as implicitly encoded in a distributed memory, that we intend to characterize more precisely and test in future work (similar ideas are being explored in a more applied NLP perspective, e.g., Gillick et al., 2016; Lee et al., 2017; Cherry et al., 2018). Concerning the model input, we would like to study whether the CNLM successes crucially depend on the huge amount of training data it receives. Are word priors more important when learning from smaller corpora? In terms of comparison with human learning, the Wikipedia text we fed our CNLMs is far from what children acquiring a language would hear. Future work should explore character/phoneme-level learning from child-directed speech corpora. Still, by feeding our networks ‘‘grown-up’’ prose, we are arguably making the job of identifying basic constituents harder than it"
Q19-1033,N16-1082,0,0.0366527,"nts on possible morpheme combinations. Intriguingly, the model tracks word/morpheme boundaries through a single specialized unit, suggesting that such boundaries are salient (at least when marked by whitespace, as in their experiments) and informative enough that it is worthwhile for the network to devote a special mechanism to process them. We replicated this finding for our networks trained on whitespacefree text, as discussed in Section 4.4, where we discuss it in the context of our other results. Probing linguistic knowledge of neural language models is currently a popular research topic (Li et al., 2016; Linzen et al., 2016; Shi et al., 2016; Adi et al., 2017; Belinkov et al., 2017; K`ad`ar et al., 2017; Hupkes et al., 2018; Conneau et al., 2018; Ettinger et al., 2018; Linzen et al., 2018). Among studies focusing on character-level models, Elman (1990) already reported a proofof-concept experiment on implicit learning of word segmentation. Christiansen et al. (1998) trained a RNN on phoneme-level language modeling of transcribed child-directed speech with tokens marking utterance boundaries, and found that the network learned to segment the input by predicting the utterance boundary symbol a"
Q19-1033,N13-1090,0,0.0125791,"LSTM CNLM outperforms many earlier word-level neural models, and is only slightly below our WordNLM. The RNN is not successful even when trained in-domain, 476 48.0 58.9 61.4 65.1 Table 5: Results on MSR Sentence Completion. For our models (top), we show accuracies for Wikipedia (left) and in-domain (right) training. We compare with language models from prior work (left): KneserNey 5-gram model (Mikolov, 2012), Word RNN (Zweig et al., 2012), Word LSTM and LdTreeLSTM (Zhang et al., 2016). We further report models incorporating distributional encodings of semantics (right): Skipgram(+RNNs) from Mikolov et al. (2013a), the PMI-based model of Woods (2016), and the Context-Embedding-based approach of Melamud et al. (2016). contrasting with the word-based vanilla RNN from the literature, whose performance, while still below LSTMs, is much stronger. Once more, this suggests that capturing word-level generalizations with a word-lexicon-less character model requires the long-span processing abilities of an LSTM. 4.4 Boundary tracking in CNLMs The good performance of CNLMs on most tasks above suggests that, although they lack a hardcoded word vocabulary and they were trained on unsegmented input, there is enoug"
Q19-1033,Q16-1037,0,0.0299114,"orpheme combinations. Intriguingly, the model tracks word/morpheme boundaries through a single specialized unit, suggesting that such boundaries are salient (at least when marked by whitespace, as in their experiments) and informative enough that it is worthwhile for the network to devote a special mechanism to process them. We replicated this finding for our networks trained on whitespacefree text, as discussed in Section 4.4, where we discuss it in the context of our other results. Probing linguistic knowledge of neural language models is currently a popular research topic (Li et al., 2016; Linzen et al., 2016; Shi et al., 2016; Adi et al., 2017; Belinkov et al., 2017; K`ad`ar et al., 2017; Hupkes et al., 2018; Conneau et al., 2018; Ettinger et al., 2018; Linzen et al., 2018). Among studies focusing on character-level models, Elman (1990) already reported a proofof-concept experiment on implicit learning of word segmentation. Christiansen et al. (1998) trained a RNN on phoneme-level language modeling of transcribed child-directed speech with tokens marking utterance boundaries, and found that the network learned to segment the input by predicting the utterance boundary symbol also at word edges. Mo"
Q19-1033,E17-1074,0,0.056492,"Missing"
Q19-1033,P16-2071,0,0.0211948,"eural models, and is only slightly below our WordNLM. The RNN is not successful even when trained in-domain, 476 48.0 58.9 61.4 65.1 Table 5: Results on MSR Sentence Completion. For our models (top), we show accuracies for Wikipedia (left) and in-domain (right) training. We compare with language models from prior work (left): KneserNey 5-gram model (Mikolov, 2012), Word RNN (Zweig et al., 2012), Word LSTM and LdTreeLSTM (Zhang et al., 2016). We further report models incorporating distributional encodings of semantics (right): Skipgram(+RNNs) from Mikolov et al. (2013a), the PMI-based model of Woods (2016), and the Context-Embedding-based approach of Melamud et al. (2016). contrasting with the word-based vanilla RNN from the literature, whose performance, while still below LSTMs, is much stronger. Once more, this suggests that capturing word-level generalizations with a word-lexicon-less character model requires the long-span processing abilities of an LSTM. 4.4 Boundary tracking in CNLMs The good performance of CNLMs on most tasks above suggests that, although they lack a hardcoded word vocabulary and they were trained on unsegmented input, there is enough pressure from the language modeling t"
Q19-1033,E17-2060,0,0.0247772,"2016; Adi et al., 2017; Belinkov et al., 2017; K`ad`ar et al., 2017; Hupkes et al., 2018; Conneau et al., 2018; Ettinger et al., 2018; Linzen et al., 2018). Among studies focusing on character-level models, Elman (1990) already reported a proofof-concept experiment on implicit learning of word segmentation. Christiansen et al. (1998) trained a RNN on phoneme-level language modeling of transcribed child-directed speech with tokens marking utterance boundaries, and found that the network learned to segment the input by predicting the utterance boundary symbol also at word edges. More recently, Sennrich (2017) 3 Experimental Set-up We extracted plain text from full English, German, and Italian Wikipedia dumps with WikiExtractor.4 We randomly selected test and validation sections consisting of 50,000 paragraphs each, and used the remainder for training. The training sets contained 16M (German), 9M (Italian), and 41M (English) paragraphs, corresponding to 819M, 4 469 https://github.com/attardi/wikiextractor. 463M, and 2,333M words, respectively. Paragraph order was shuffled for training, without attempting to split by sentences. All characters were lower-cased. For benchmark construction and word-bas"
Q19-1033,N16-1035,0,0.0303875,"Missing"
Q19-1033,D16-1159,0,0.0297241,"Intriguingly, the model tracks word/morpheme boundaries through a single specialized unit, suggesting that such boundaries are salient (at least when marked by whitespace, as in their experiments) and informative enough that it is worthwhile for the network to devote a special mechanism to process them. We replicated this finding for our networks trained on whitespacefree text, as discussed in Section 4.4, where we discuss it in the context of our other results. Probing linguistic knowledge of neural language models is currently a popular research topic (Li et al., 2016; Linzen et al., 2016; Shi et al., 2016; Adi et al., 2017; Belinkov et al., 2017; K`ad`ar et al., 2017; Hupkes et al., 2018; Conneau et al., 2018; Ettinger et al., 2018; Linzen et al., 2018). Among studies focusing on character-level models, Elman (1990) already reported a proofof-concept experiment on implicit learning of word segmentation. Christiansen et al. (1998) trained a RNN on phoneme-level language modeling of transcribed child-directed speech with tokens marking utterance boundaries, and found that the network learned to segment the input by predicting the utterance boundary symbol also at word edges. More recently, Sennr"
Q19-1033,P12-1063,0,0.024138,"Missing"
Q19-1033,J17-4003,0,\N,Missing
Q19-1033,Q18-1045,0,\N,Missing
S14-1021,W11-2501,1,0.825511,"hly comparable tuples that do not feature such effects. An alternative approach would have been to rate phrases that were randomly selected from a corpus. This would have led to a dataset reflecting a more realistic distribution of modification effects, but it would not have guaranteed, for the same number of pairs, a fair amount of distorted tuples and comparable controls. We leave the study of the natural distribution of modification strength in text to further work. To find inspiration for the tuples, we looked into various databases containing concepts organized by category, namely BLESS (Baroni and Lenci, 2011), ConceptNet (Speer and Havasi, 2013) and WordNet (Fellbaum, 1998). We insured that all words in our tuples occurred at least 200 times in the large corpus we describe below (phrases were not filtered by frequency, due to data sparseness). Finally, when looking for tuples matching the distorted ones, we made sure that the mh phrases in the new tuples have similar Pointwise Mutual Information to the corresponding phrases in the distorted tuple (or, where the latter were not attested in the corpus, similar m and h frequencies). Finding meaningful combinations among unattested or infrequent phras"
S14-1021,D10-1115,1,0.804727,"taining to different word senses of the head term. One might argue, for example, that egg has a food sense and a reproductive vessel sense. The human modifier picks the second sense, and so, obviously, human eggs are judged as bad instances of food. While we see the point of this objection, we think it’s impossible to draw a clear-cut distinction between discrete word senses (even in the rather extreme egg case, the eggs we eat are reproductive vessels from a chicken point of view!). This has been long recognized in the linguistic and cognitive literature (Kilgarriff, 1997; Murphy, 2002), 174 Baroni and Zamparelli (2010) and Coecke et al. (2010) takes inspiration from formal semantics to characterize composition as function application. In particular, in modifier-head phrases, the modifier is treated as a linear function operating on the head vector. Given that linear functions can be expressed by matrices and their application by matrix-by-vector multiplication, the modifier is represented by a matrix A to be multiplied with the modifier vector ~b, so that: p~ = A~b. We use the DISSECT toolkit7 to estimate the parameters of the composition methods and derive phrase vectors. In particular, DISSECT finds optim"
S14-1021,D12-1050,0,0.0267575,"sanity check, since they are clearly failing to capture properties pertaining to the instance-class relation (if a combination is not able to tell that it is a parrot that is a pet, and not vice versa, there is no point in asking if the same combination is able to model how typical a dead parrot is as a pet). Looking at composition methods, there is no evidence that the more complex, matrix-based fulladd and lexfunc approaches are performing any better than the simple multiplicative and additive methods. Indeed, mult shows the most consistent overall performance, confirming the conclusion of Blacoe and Lapata (2012) that, at the present time, when it comes to composition, “simpler is better”. A related point emerges from the comparison of the low- and full-rank results for mult and wadd. The smoothing process due to dimensionality reduction is quite disruptive for the current asymmetric measures, that are based on feature inclusion. This is a further reason to stick to simpler composition methods, that can be applied directly in the full-rank spaces. Modeling typicality ratings of mh → c pairs Next, for each of the remaining spaces, we first performed composition as described in Section 3.1 above to buil"
S14-1021,W10-2805,0,0.0338869,"= w1~a + w2~b. In the dilation model (dil), the output vector is obtained by decomposing one of the input vectors, say ~b, into a vector parallel to ~a and its orthogonal counterpart, and then dilating only the parallel vector by a factor λ before re-combining. The corresponding formula is: (~a ·~a)~b + (λ − 1)(~a ·~b)~a. In our experiments, we stretch the head vector in the direction of the modifier (i.e., ~a is the modifier, ~b is the head). In the multiplicative model (mult), vectors are combined by component-wise multiplication, such that each phrase component pi is given by: pi = ai bi . Guevara (2010) and Zanzotto et al. (2010) propose a full form of the additive model (fulladd), where the two constituent vectors are multiplied by weight matrices before being added, so that each phrase component is a weighted sum of all constituent components: p~ = W1~a + W2~b. Finally, the lexical function (lexfunc) model of 7 http://clic.cimec.unitn.it/composes/ toolkit/ 8 We speak of “instance-class relations” in a very broad and loose sense, to encompass classic relations such as hyponymy but also the fuzzier notion of lexical entailment. 9 SVM classifiers have also been shown by Baroni et al. (2012) t"
S14-1021,D12-1112,0,0.0140071,"t modifiers have on heads should thus have a positive effect on important tasks such as recognizing textual entailment, paraphrasing and anaphora resolution (Androutsopoulos and Malakasiotis, 2010; Dagan et 1 http://en.wikipedia.org/wiki/Dead_ Parrot_sketch 171 Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 171–181, Dublin, Ireland, August 23-24 2014. 2 al., 2009; Poesio et al., 2010). Despite their theoretical and practical import, modification effects have been largely overlooked in computational linguistics, with the notable exception of Boleda et al. (2012; 2013), who only focused on the extreme case of intensional adjectives, studied a limited number of modifiers, and did not attempt to capture the graded nature of modification (a dead parrot is not a prototypical animal, but a toy parrot is not an animal at all). This paper aims to stimulate computational research into modifier effects on phrase meaning in two ways. First, we introduce a new, large, publicly available data set of modifier-head phrases annotated with four kinds of modification-related subject ratings: whether the concept denoted by the phrase is an instance of the concept deno"
S14-1021,W13-0104,1,0.805209,"Missing"
S14-1021,S12-1012,0,0.112238,"parrot? and as a pet?). Second, we present a first attempt to model the collected judgments computationally. We choose distributional semantics (Erk, 2012) as our frame of reference, as it produces continuous similarity scores, in line with the graded nature of the modification effects we are investigating. In particular, we look at the compositional extension of distributional semantics (Baroni, 2013), because we need representations not only for words, but also phrases, and we adopt the asymmetric similarity measures developed in the literature on lexical entailment (Kotlerman et al., 2010; Lenci and Benotto, 2012), because we are interested in an asymmetric relation (to what extent the concept denoted by the phrase is a good instance of the target class, and not vice versa). As far as we know, this is the first time these asymmetric measures are applied to composed representations (Baroni et al. (2012) experimented with entailment measures applied to phrase representations directly harvested from corpora, and not derived compositionally). We are thus also providing a novel evaluation of compositional models and asymmetric measures on a challenging task where they could potentially be very useful.2 The"
S14-1021,W13-3206,1,0.812913,". In particular, in modifier-head phrases, the modifier is treated as a linear function operating on the head vector. Given that linear functions can be expressed by matrices and their application by matrix-by-vector multiplication, the modifier is represented by a matrix A to be multiplied with the modifier vector ~b, so that: p~ = A~b. We use the DISSECT toolkit7 to estimate the parameters of the composition methods and derive phrase vectors. In particular, DISSECT finds optimal parameter settings by learning to approximate corpus-extracted phrase vector examples with least-squares methods (Dinu et al., 2013). We use as training examples all the modifier-head phrases that contain a modifier of interest and occur at least 50 times in our source corpus (see Section 3.3 below). and even by the computational word sense disambiguation community, that is currently addressing the continuous nature of polysemy by shifting to the lexical-substitution-in-context task (McCarthy and Navigli, 2009). Context provides fundamental cues to disambiguating polysemous words, and noun modifiers typically act as important disambiguating contexts for the nouns. Thus, we think that it is more productive for computational"
S14-1021,C04-1146,0,0.122264,"Missing"
S14-1021,C10-1142,0,0.0233329,"he dilation model (dil), the output vector is obtained by decomposing one of the input vectors, say ~b, into a vector parallel to ~a and its orthogonal counterpart, and then dilating only the parallel vector by a factor λ before re-combining. The corresponding formula is: (~a ·~a)~b + (λ − 1)(~a ·~b)~a. In our experiments, we stretch the head vector in the direction of the modifier (i.e., ~a is the modifier, ~b is the head). In the multiplicative model (mult), vectors are combined by component-wise multiplication, such that each phrase component pi is given by: pi = ai bi . Guevara (2010) and Zanzotto et al. (2010) propose a full form of the additive model (fulladd), where the two constituent vectors are multiplied by weight matrices before being added, so that each phrase component is a weighted sum of all constituent components: p~ = W1~a + W2~b. Finally, the lexical function (lexfunc) model of 7 http://clic.cimec.unitn.it/composes/ toolkit/ 8 We speak of “instance-class relations” in a very broad and loose sense, to encompass classic relations such as hyponymy but also the fuzzier notion of lexical entailment. 9 SVM classifiers have also been shown by Baroni et al. (2012) to be well-suited for entail"
S14-1021,E12-1004,1,\N,Missing
S14-2001,D10-1115,1,0.672361,"ss and (ii) entailment. The task attracted 21 teams, most of which participated in both subtasks. We received 17 submissions in the relatedness subtask (for a total of 66 runs) and 18 in the entailment subtask (65 runs). 1 Introduction Distributional Semantic Models (DSMs) approximate the meaning of words with vectors summarizing their patterns of co-occurrence in corpora. Recently, several compositional extensions of DSMs (CDSMs) have been proposed, with the purpose of representing the meaning of phrases and sentences by composing the distributional representations of the words they contain (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Mitchell and Lapata, 2010; Socher et al., 2012). Despite the ever increasing interest in the field, the development of adequate benchmarks for CDSMs, especially at the sentence level, is still lagging. Existing data sets, such as those introduced by Mitchell and Lapata (2008) and Grefenstette and Sadrzadeh (2011), are limited to a few hundred instances of very short sentences with a fixed structure. In the last ten years, several large 2 The Task The Task involved two subtasks. (i) Relatedness: predicting the degree of semantic similarity between two sentenc"
S14-2001,S12-1051,0,0.383512,"Missing"
S14-2001,S14-2013,0,0.0692411,"Missing"
S14-2001,S14-2141,0,0.0602478,"78.5 S UIO-Lien run1 77.1 77.0 FBK-TR run3 P 75.4 StanfordNLP run5 S 74.5 UTexas run1 P/S 73.2 Yamraj run1 70.7 asjai run5 S 69.8 haLF run2 S 69.4 RTM-DCU run1 UANLPCourse run2 67.2 S 48.7 Table 6: Primary run results for the entailment subtask. The table also shows whether a system exploits composition information at either the phrase (P) or sentence (S) level. Approaches A summary of the approaches used by the systems to address the task is presented in Table 8. In the table, systems in bold are those for which the authors submitted a paper (Ferrone and Zanzotto, 2014; Bjerva et al., 2014; Beltagy et al., 2014; Lai and Hockenmaier, 2014; Alves et al., 2014; Le´on et al., 2014; Bestgen, 2014; Zhao et al., 2014; Vo et al., 2014; Bic¸ici and Way, 2014; Lien and Kouylekov, 2014; Jimenez et al., 2014; Proisl and Evert, 2014; Gupta et al., 2014). For the others, we used the brief description sent with the system’s results, double-checking the information with the authors. In the table, “E” and “R” refer to the entailment and relatedness task respectively, and “B” to both. Almost all systems combine several kinds of features. To highlight the role played by composition, we draw a distinction between compo"
S14-2001,S14-2125,0,0.0284327,"NLPCourse run2 67.2 S 48.7 Table 6: Primary run results for the entailment subtask. The table also shows whether a system exploits composition information at either the phrase (P) or sentence (S) level. Approaches A summary of the approaches used by the systems to address the task is presented in Table 8. In the table, systems in bold are those for which the authors submitted a paper (Ferrone and Zanzotto, 2014; Bjerva et al., 2014; Beltagy et al., 2014; Lai and Hockenmaier, 2014; Alves et al., 2014; Le´on et al., 2014; Bestgen, 2014; Zhao et al., 2014; Vo et al., 2014; Bic¸ici and Way, 2014; Lien and Kouylekov, 2014; Jimenez et al., 2014; Proisl and Evert, 2014; Gupta et al., 2014). For the others, we used the brief description sent with the system’s results, double-checking the information with the authors. In the table, “E” and “R” refer to the entailment and relatedness task respectively, and “B” to both. Almost all systems combine several kinds of features. To highlight the role played by composition, we draw a distinction between compositional and non-compositional features, and divide the former into ‘fully compositional’ (systems that compositionally computed the meaning of the full sentences, tho"
S14-2001,S14-2024,0,0.0394666,"S 73.2 Yamraj run1 70.7 asjai run5 S 69.8 haLF run2 S 69.4 RTM-DCU run1 UANLPCourse run2 67.2 S 48.7 Table 6: Primary run results for the entailment subtask. The table also shows whether a system exploits composition information at either the phrase (P) or sentence (S) level. Approaches A summary of the approaches used by the systems to address the task is presented in Table 8. In the table, systems in bold are those for which the authors submitted a paper (Ferrone and Zanzotto, 2014; Bjerva et al., 2014; Beltagy et al., 2014; Lai and Hockenmaier, 2014; Alves et al., 2014; Le´on et al., 2014; Bestgen, 2014; Zhao et al., 2014; Vo et al., 2014; Bic¸ici and Way, 2014; Lien and Kouylekov, 2014; Jimenez et al., 2014; Proisl and Evert, 2014; Gupta et al., 2014). For the others, we used the brief description sent with the system’s results, double-checking the information with the authors. In the table, “E” and “R” refer to the entailment and relatedness task respectively, and “B” to both. Almost all systems combine several kinds of features. To highlight the role played by composition, we draw a distinction between compositional and non-compositional features, and divide the former into ‘fully composi"
S14-2001,marelli-etal-2014-sick,1,0.808802,"re also welcome. Besides being of intrinsic interest, the latter systems’ performance will serve to situate CDSM performance within the broader landscape of computational semantics. 3 Data Set Creation The SICK Data Set The SICK data set, consisting of about 10,000 English sentence pairs annotated for relatedness in meaning and entailment, was used to evaluate the systems participating in the task. The data set creation methodology is outlined in the following subsections, while all the details about data generation and annotation, quality control, and interannotator agreement can be found in Marelli et al. (2014). 1 http://nlp.cs.illinois.edu/HockenmaierGroup/data.html http://www.cs.york.ac.uk/semeval2012/task6/index.php?id=data 2 2 Relatedness score Example 1.6 A: “A man is jumping into an empty pool” B: “There is no biker jumping in the air” 2.9 A: “Two children are lying in the snow and are making snow angels” B: “Two angels are making snow on the lying children” 3.6 A: “The young boys are playing outdoors and the man is smiling nearby” B: “There is no boy playing outdoors and there is no man smiling” 4.9 A: “A person in a black jacket is doing tricks on a motorbike” B: “A man in a black jacket is"
S14-2001,S14-2085,0,0.0831811,"Missing"
S14-2001,P08-1028,0,0.15047,"ectors summarizing their patterns of co-occurrence in corpora. Recently, several compositional extensions of DSMs (CDSMs) have been proposed, with the purpose of representing the meaning of phrases and sentences by composing the distributional representations of the words they contain (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Mitchell and Lapata, 2010; Socher et al., 2012). Despite the ever increasing interest in the field, the development of adequate benchmarks for CDSMs, especially at the sentence level, is still lagging. Existing data sets, such as those introduced by Mitchell and Lapata (2008) and Grefenstette and Sadrzadeh (2011), are limited to a few hundred instances of very short sentences with a fixed structure. In the last ten years, several large 2 The Task The Task involved two subtasks. (i) Relatedness: predicting the degree of semantic similarity between two sentences, and (ii) Entailment: detecting the entailment relation holding between them This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 1 Proceedings of the"
S14-2001,S14-2114,0,0.327729,"Missing"
S14-2001,S14-2093,0,0.0223856,"results for the entailment subtask. The table also shows whether a system exploits composition information at either the phrase (P) or sentence (S) level. Approaches A summary of the approaches used by the systems to address the task is presented in Table 8. In the table, systems in bold are those for which the authors submitted a paper (Ferrone and Zanzotto, 2014; Bjerva et al., 2014; Beltagy et al., 2014; Lai and Hockenmaier, 2014; Alves et al., 2014; Le´on et al., 2014; Bestgen, 2014; Zhao et al., 2014; Vo et al., 2014; Bic¸ici and Way, 2014; Lien and Kouylekov, 2014; Jimenez et al., 2014; Proisl and Evert, 2014; Gupta et al., 2014). For the others, we used the brief description sent with the system’s results, double-checking the information with the authors. In the table, “E” and “R” refer to the entailment and relatedness task respectively, and “B” to both. Almost all systems combine several kinds of features. To highlight the role played by composition, we draw a distinction between compositional and non-compositional features, and divide the former into ‘fully compositional’ (systems that compositionally computed the meaning of the full sentences, though not necessarily by assigning meanings to i"
S14-2001,D12-1110,0,0.111915,"btasks. We received 17 submissions in the relatedness subtask (for a total of 66 runs) and 18 in the entailment subtask (65 runs). 1 Introduction Distributional Semantic Models (DSMs) approximate the meaning of words with vectors summarizing their patterns of co-occurrence in corpora. Recently, several compositional extensions of DSMs (CDSMs) have been proposed, with the purpose of representing the meaning of phrases and sentences by composing the distributional representations of the words they contain (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Mitchell and Lapata, 2010; Socher et al., 2012). Despite the ever increasing interest in the field, the development of adequate benchmarks for CDSMs, especially at the sentence level, is still lagging. Existing data sets, such as those introduced by Mitchell and Lapata (2008) and Grefenstette and Sadrzadeh (2011), are limited to a few hundred instances of very short sentences with a fixed structure. In the last ten years, several large 2 The Task The Task involved two subtasks. (i) Relatedness: predicting the degree of semantic similarity between two sentences, and (ii) Entailment: detecting the entailment relation holding between them Thi"
S14-2001,S14-2047,0,0.0241507,"S 69.8 haLF run2 S 69.4 RTM-DCU run1 UANLPCourse run2 67.2 S 48.7 Table 6: Primary run results for the entailment subtask. The table also shows whether a system exploits composition information at either the phrase (P) or sentence (S) level. Approaches A summary of the approaches used by the systems to address the task is presented in Table 8. In the table, systems in bold are those for which the authors submitted a paper (Ferrone and Zanzotto, 2014; Bjerva et al., 2014; Beltagy et al., 2014; Lai and Hockenmaier, 2014; Alves et al., 2014; Le´on et al., 2014; Bestgen, 2014; Zhao et al., 2014; Vo et al., 2014; Bic¸ici and Way, 2014; Lien and Kouylekov, 2014; Jimenez et al., 2014; Proisl and Evert, 2014; Gupta et al., 2014). For the others, we used the brief description sent with the system’s results, double-checking the information with the authors. In the table, “E” and “R” refer to the entailment and relatedness task respectively, and “B” to both. Almost all systems combine several kinds of features. To highlight the role played by composition, we draw a distinction between compositional and non-compositional features, and divide the former into ‘fully compositional’ (systems that compositionall"
S14-2001,D11-1129,0,0.0154758,"Missing"
S14-2001,S14-2044,0,0.216165,"Missing"
S14-2001,S14-2139,0,0.0534159,"Missing"
S14-2001,S14-2131,0,0.156692,"Table 6: Primary run results for the entailment subtask. The table also shows whether a system exploits composition information at either the phrase (P) or sentence (S) level. Approaches A summary of the approaches used by the systems to address the task is presented in Table 8. In the table, systems in bold are those for which the authors submitted a paper (Ferrone and Zanzotto, 2014; Bjerva et al., 2014; Beltagy et al., 2014; Lai and Hockenmaier, 2014; Alves et al., 2014; Le´on et al., 2014; Bestgen, 2014; Zhao et al., 2014; Vo et al., 2014; Bic¸ici and Way, 2014; Lien and Kouylekov, 2014; Jimenez et al., 2014; Proisl and Evert, 2014; Gupta et al., 2014). For the others, we used the brief description sent with the system’s results, double-checking the information with the authors. In the table, “E” and “R” refer to the entailment and relatedness task respectively, and “B” to both. Almost all systems combine several kinds of features. To highlight the role played by composition, we draw a distinction between compositional and non-compositional features, and divide the former into ‘fully compositional’ (systems that compositionally computed the meaning of the full sentences, though not necessarily by"
S14-2001,S14-2055,0,0.250575,"77.1 77.0 FBK-TR run3 P 75.4 StanfordNLP run5 S 74.5 UTexas run1 P/S 73.2 Yamraj run1 70.7 asjai run5 S 69.8 haLF run2 S 69.4 RTM-DCU run1 UANLPCourse run2 67.2 S 48.7 Table 6: Primary run results for the entailment subtask. The table also shows whether a system exploits composition information at either the phrase (P) or sentence (S) level. Approaches A summary of the approaches used by the systems to address the task is presented in Table 8. In the table, systems in bold are those for which the authors submitted a paper (Ferrone and Zanzotto, 2014; Bjerva et al., 2014; Beltagy et al., 2014; Lai and Hockenmaier, 2014; Alves et al., 2014; Le´on et al., 2014; Bestgen, 2014; Zhao et al., 2014; Vo et al., 2014; Bic¸ici and Way, 2014; Lien and Kouylekov, 2014; Jimenez et al., 2014; Proisl and Evert, 2014; Gupta et al., 2014). For the others, we used the brief description sent with the system’s results, double-checking the information with the authors. In the table, “E” and “R” refer to the entailment and relatedness task respectively, and “B” to both. Almost all systems combine several kinds of features. To highlight the role played by composition, we draw a distinction between compositional and non-compositio"
S14-2001,S14-2021,0,0.0355515,"Missing"
S14-2001,W07-1401,0,\N,Missing
S15-1023,2014.lilt-9.5,1,0.850058,"sentence pairs is not carefully controlled, e.g., the datasets of Dolan and Brockett (2005) and Agirre 199 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 199–204, Denver, Colorado, June 4–5, 2015. et al. (2014). One such study obtained the influential result that on such a dataset, simple composition models such as vector addition perform comparably to a state-of-the-art composition model (Blacoe and Lapata, 2012). The success of these simplistic models led to the conjecture that these data sets fail to assess critical aspects of language (Baroni et al., 2014a) and leaves open the question of whether CDSM would outperform simplistic models in a setting in which lexical cues are uninformative. In the present study, we develop a method for removing the confound of lexical cues from CDSM assessment. The method is to create a set of sentences where each sentence fits into a semantic category and where a sentence’s semantic category cannot be determined based on any individual word in the sentence. CDSM are then challenged to create a vector space in which the representations for sentences in a given category cluster together, even though the individua"
S15-1023,P14-1023,1,0.930829,"sentence pairs is not carefully controlled, e.g., the datasets of Dolan and Brockett (2005) and Agirre 199 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 199–204, Denver, Colorado, June 4–5, 2015. et al. (2014). One such study obtained the influential result that on such a dataset, simple composition models such as vector addition perform comparably to a state-of-the-art composition model (Blacoe and Lapata, 2012). The success of these simplistic models led to the conjecture that these data sets fail to assess critical aspects of language (Baroni et al., 2014a) and leaves open the question of whether CDSM would outperform simplistic models in a setting in which lexical cues are uninformative. In the present study, we develop a method for removing the confound of lexical cues from CDSM assessment. The method is to create a set of sentences where each sentence fits into a semantic category and where a sentence’s semantic category cannot be determined based on any individual word in the sentence. CDSM are then challenged to create a vector space in which the representations for sentences in a given category cluster together, even though the individua"
S15-1023,D12-1050,0,0.0220877,"ty of pairs of sentences with similarity ratings given by humans. Many of these studies used datasets in which the amount of lexical overlap between the sentence pairs is not carefully controlled, e.g., the datasets of Dolan and Brockett (2005) and Agirre 199 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 199–204, Denver, Colorado, June 4–5, 2015. et al. (2014). One such study obtained the influential result that on such a dataset, simple composition models such as vector addition perform comparably to a state-of-the-art composition model (Blacoe and Lapata, 2012). The success of these simplistic models led to the conjecture that these data sets fail to assess critical aspects of language (Baroni et al., 2014a) and leaves open the question of whether CDSM would outperform simplistic models in a setting in which lexical cues are uninformative. In the present study, we develop a method for removing the confound of lexical cues from CDSM assessment. The method is to create a set of sentences where each sentence fits into a semantic category and where a sentence’s semantic category cannot be determined based on any individual word in the sentence. CDSM are"
S15-1023,I05-5002,0,0.0605481,"nings of the following phrases: “red apple,” “red hair,” and “red state.” The meaning of the word “red” in each of these examples interacts with the meaning of the noun it modifies, applying ∗ Please address correspondence to the first author at swritter@princeton.edu CDSM Assessment Past studies have tested how well CDSM adhere to this principle by comparing the vector similarity of pairs of sentences with similarity ratings given by humans. Many of these studies used datasets in which the amount of lexical overlap between the sentence pairs is not carefully controlled, e.g., the datasets of Dolan and Brockett (2005) and Agirre 199 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 199–204, Denver, Colorado, June 4–5, 2015. et al. (2014). One such study obtained the influential result that on such a dataset, simple composition models such as vector addition perform comparably to a state-of-the-art composition model (Blacoe and Lapata, 2012). The success of these simplistic models led to the conjecture that these data sets fail to assess critical aspects of language (Baroni et al., 2014a) and leaves open the question of whether CDSM would outperform simplis"
S15-1023,D11-1129,0,0.0565147,"Missing"
S15-1023,W13-0112,0,0.0146821,"to the word overlap baseline. CW refers to the vectors from Collobert and Weston (2008) . 3.1 Compositional Distributional Models We compared six models that are currently prominent in the CDSM literature: addition, multiplication (Mitchell and Lapata, 2008), lexical function (LF) (Coecke et al., 2010), practical lexical function (PLF) (Paperno et al., 2014), full additive (FA) (Guevara, 2010; Zanzotto et al., 2010), and the recursive auto-encoder (RAE) (Socher et al., 2011). The training data for LF, PLF, and FA was the UKWAC+Wikipedia+BNC 2.8 billion word corpus. In training LF, we followed Grefenstette et al. (2013), employing a two-step training regime using corpus-extracted vectors for noun–preposition–noun combinations to estimate matrices of corresponding prepositional phrases, which were in turn used to estimate a three-way tensor of each preposition. For PLF and FA, we learned separate matrices for combining prepositions with each of the two nouns in the construction, using corpus-based vectors of prepositional phrases for training preposition–noun combination. For training composition of the head noun with the prepositional phrase, we used corpusextracted noun+preposition (for lexical matrices in"
S15-1023,W10-2805,0,0.0242312,"0.7 0.6 0.5 0.4 0.3 0.2 rlap Ove net d Wor RAE LF PLF FA Add t Mul cat Con Figure 1: Naive Bayes accuracy scores for count and predict variants of several CDSM. Chance performance on this task was 0.2. Overlap refers to the word overlap baseline. CW refers to the vectors from Collobert and Weston (2008) . 3.1 Compositional Distributional Models We compared six models that are currently prominent in the CDSM literature: addition, multiplication (Mitchell and Lapata, 2008), lexical function (LF) (Coecke et al., 2010), practical lexical function (PLF) (Paperno et al., 2014), full additive (FA) (Guevara, 2010; Zanzotto et al., 2010), and the recursive auto-encoder (RAE) (Socher et al., 2011). The training data for LF, PLF, and FA was the UKWAC+Wikipedia+BNC 2.8 billion word corpus. In training LF, we followed Grefenstette et al. (2013), employing a two-step training regime using corpus-extracted vectors for noun–preposition–noun combinations to estimate matrices of corresponding prepositional phrases, which were in turn used to estimate a three-way tensor of each preposition. For PLF and FA, we learned separate matrices for combining prepositions with each of the two nouns in the construction, usi"
S15-1023,W09-2415,0,0.0280875,"Missing"
S15-1023,C12-2054,0,0.0154531,"tive or intransitive verb phrases. These phrases are then matched with “landmark” verbs that are either similar or not similar in meaning to the full phrase. CDSM are then challenged to create representations of the phrases from which classifiers can determine whether or not a phrase is similar to its landmark verb (Kintsch, 2001; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011). Another notable CDSM assessment task involves matching a phrase with a word with a similar meaning, for example, matching a short dictionary definition with the word it defines (Kartsaklis et al., 2012; Turney, 2014). While these methods are applicable only to simple phrases that can be mapped reasonably to a single word, the present method can, in principle, be applied to any type of phrase. This allowed us to build a dataset that extends the current landmark word and word matching datasets in at least two important ways. First, it includes function words, specifically prepositions. Second, it requires the characterization of interactions among three words in each expression, whereas previous datasets had two words per expression, or subsets of the words did not interact in complex ways. O"
S15-1023,S14-2001,1,0.939691,"entists have pointed out this complexity and proposed various models for accommodating it (Kintsch, 2001; Mitchell and Lapata, 2010; Socher et al., 2013). A dominant modeling approach seeks to learn functions that combine word representations derived from the distributional structure of large natural language corpora (Deerwester et al., 1990; Landauer and Dumais, 1997). Because the word representations to be combined and the compositional functions are generated based on the distributions of words in corpora, these models have been dubbed compositional distributional semantic models, or CDSM (Marelli et al., 2014). CDSM produce fixed-dimensional vector representations of arbitrary sentences and phrases, and the foundational principle of these models is, stated simply, that semantically similar phrases should have vector representations that are close together in the vector space. Complex interactions among the meanings of words are important factors in the function that maps word meanings to phrase meanings. Recently, compositional distributional semantics models (CDSM) have been designed with the goal of emulating these complex interactions; however, experimental results on the effectiveness of CDSM h"
S15-1023,P08-1028,0,0.609006,"information is uninformative, and models must rely on compositionality to score well in classification. 1.2 Relation to Past Work This approach to CDSM assessment is similar to a previous method wherein polysemous verbs are paired with disambiguating nouns in transitive or intransitive verb phrases. These phrases are then matched with “landmark” verbs that are either similar or not similar in meaning to the full phrase. CDSM are then challenged to create representations of the phrases from which classifiers can determine whether or not a phrase is similar to its landmark verb (Kintsch, 2001; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011). Another notable CDSM assessment task involves matching a phrase with a word with a similar meaning, for example, matching a short dictionary definition with the word it defines (Kartsaklis et al., 2012; Turney, 2014). While these methods are applicable only to simple phrases that can be mapped reasonably to a single word, the present method can, in principle, be applied to any type of phrase. This allowed us to build a dataset that extends the current landmark word and word matching datasets in at least two important ways. First,"
S15-1023,P14-1009,1,0.802166,"d-Based Predict Count CW Proportion Correct 0.7 0.6 0.5 0.4 0.3 0.2 rlap Ove net d Wor RAE LF PLF FA Add t Mul cat Con Figure 1: Naive Bayes accuracy scores for count and predict variants of several CDSM. Chance performance on this task was 0.2. Overlap refers to the word overlap baseline. CW refers to the vectors from Collobert and Weston (2008) . 3.1 Compositional Distributional Models We compared six models that are currently prominent in the CDSM literature: addition, multiplication (Mitchell and Lapata, 2008), lexical function (LF) (Coecke et al., 2010), practical lexical function (PLF) (Paperno et al., 2014), full additive (FA) (Guevara, 2010; Zanzotto et al., 2010), and the recursive auto-encoder (RAE) (Socher et al., 2011). The training data for LF, PLF, and FA was the UKWAC+Wikipedia+BNC 2.8 billion word corpus. In training LF, we followed Grefenstette et al. (2013), employing a two-step training regime using corpus-extracted vectors for noun–preposition–noun combinations to estimate matrices of corresponding prepositional phrases, which were in turn used to estimate a three-way tensor of each preposition. For PLF and FA, we learned separate matrices for combining prepositions with each of the"
S15-1023,D13-1170,0,0.034618,"Paperno University of Trento Marco Baroni University of Trento Matthew Botvinick Princeton University Adele Goldberg Princeton University Abstract a different color to the first two and a political affiliation to the third. This is an example of a common phenomenon in natural language in which the meaning of a whole expression is not derived from a simple concatenation of its parts, but is composed by interactions among their meanings. Cognitive and computer scientists have pointed out this complexity and proposed various models for accommodating it (Kintsch, 2001; Mitchell and Lapata, 2010; Socher et al., 2013). A dominant modeling approach seeks to learn functions that combine word representations derived from the distributional structure of large natural language corpora (Deerwester et al., 1990; Landauer and Dumais, 1997). Because the word representations to be combined and the compositional functions are generated based on the distributions of words in corpora, these models have been dubbed compositional distributional semantic models, or CDSM (Marelli et al., 2014). CDSM produce fixed-dimensional vector representations of arbitrary sentences and phrases, and the foundational principle of these"
S15-1023,Q13-1019,0,0.0964628,"and a set of 420 sentences with at least one informative noun. By this method, we ensured that no component of the models’ classification accuracy on the test set is due to the recognition of individual nouns. In addition to the CDSM, we included two nondistributional models for comparison. The first, referred to as word overlap, consists of a binary feature vector containing one feature per vocabulary item. This model’s performance provides an upper-bound on the performance that a model can achieve given only the distribution of word tokens in the training set. The second model, inspired by Srikumar and Roth (2013), contains binary features for Wordnet hypernyms (up to 4 levels) of each sense of the noun and a binary feature for each preposition. This model’s score provides an indication of the amount of task-relevant information contained in the taxonomic features of individual words. We compared CDSM to a further control that consisted of the concatenation of the word vectors. The concatenated vectors contain a complete representation of all of the individual word information, so that any performance the CDSM can achieve above the concatenation score can be attributed to semantic interaction informati"
S15-1023,P10-1040,0,0.041775,"s respectively. Both word models were trained on the same corpus as those used to train the compositional models. Count was based on a 5 word window weighted with positive PMI and was reduced to 300 dimensions via SVD, while predict was based on a 5 word window using Mikolov’s continuous bag of words approach with negative sampling (Mikolov et al., 2013). These parameters were based on their strong performance in the systematic evaluation by Baroni et al. (2014b). Socher et al.’s RAE implementation composes neural language model vectors described by Collobert and Weston (2008) and supplied by Turian et al. (2010). For comparison with the RAE, we report results for addition, multiplication, and concatenation of these same embeddings. 4 Results The naive Bayes accuracy scores for all models are displayed in Figure 1. Addition, PLF, and the RAE each substantially outperformed concatenation, indicating that these models’ vectors contain information about the semantic interactions between phrase constituents. Addition scored higher than PLF, while the RAE achieved comparable performance to its additive counterpart. In all cases except FA in which predict and count vectors were compared, predict achieved a"
S15-1023,C10-1142,0,0.0675034,"Missing"
S15-1023,S14-2010,0,\N,Missing
W02-0606,C02-1096,1,0.832368,"Missing"
W02-0606,P89-1010,0,0.0258947,"9 characters, the other 9 or 8 characters, and the two differ in only one character.4 For the above reasons, it is crucial that orthographic similarity is combined with an independent measure that allows us to distinguish between similarity due to morphological relatedness vs. similarity due to chance or other reasons. 3.3 Scoring the semantic similarity of word pairs Measuring the semantic similarity of words on the basis of raw corpus data is obviously a much harder task than measuring the orthographic similarity of words. Mutual information (first introduced to computational linguistics by Church and Hanks (1989)) is one of many measures that seems to be roughly correlated to the degree of semantic relatedness between words. The mutual information between two words A and B is given by: I(A, B) = log P r(A, B) P r(A)P r(B) (1) Intuitively, the larger the deviation between the empirical frequency of co-occurrence of two words and the expected frequency of co-occurrence if they were independent, the more likely it is that the occurrence of one of the two words is not independent from the occurrence of the other. Brown et alii (1990) observed that when mutual information is computed in a bi-directional fa"
W02-0606,J01-2001,0,0.683668,"t) that would probably be harder to discover using affix-based approaches. The remainder of the paper is organized as follows: In section 2, we shortly review related work. In section 3, we present our model. In section 4, we discuss the results of experiments with German and English input. Finally, in section 5 we summarize our main results, we sketch possible directions that our current work could take, and we discuss some potential uses for the output of our algorithm. 2 Related work For space reason, we discuss here only three approaches that are closely related to ours. See, for example, Goldsmith (2001) for a very different (possibly complementary) approach, and for a review of other relevant work. 2.1 Jacquemin (1997) Jacquemin (1997) presents a model that automatically extracts morphologically related forms from a list of English two-word medical terms and a corpus from the medical domain. The algorithm looks for correspondences between two-word terms and orthographically similar pairs of words that are adjacent in the corpus. For example, the list contains the term artificial ventilation, and the corpus contains the phrase artificially ventilated. Jacquemin’s algorithm thus postulates the"
W02-0606,W00-0712,0,0.300046,"suffixation patterns. Again, we will not describe here how this is accomplished. Our basic idea is related to that of Jacquemin, but we propose an approach that is more general both in terms of orthography and in terms of semantics. In terms of orthography, we do not require that two strings share the left (or right) substring in order to constitute a candidate pair. Thus, we are not limited to affixal morphological patterns. Moreover, our algorithm extracts semantic information directly from the input corpus, and thus it does not require a precompiled list of semantically related pairs. 2.2 Schone and Jurafsky (2000) Schone and Jurafsky (2000) present a knowledgefree unsupervised model in which orthographybased distributional cues are combined with semantic information automatically extracted from word co-occurrence patterns in the input corpus. They first look for potential suffixes by searching for frequent word-final substrings. Then, they look for potentially morphologically related pairs, i.e., pairs that end in potential suffixes and share the left substring preceding those suffixes. Finally, they look, among those pairs, for those whose semantic vectors (computed using latent semantic analysis) are"
W02-0606,P01-1063,0,0.549471,"han in terms of affixation rules, and are thus well suited to identify patterns involving non-concatenative morphology and/or morphophonological changes. A list of related words constitutes a more suitable input for them than a list of words segmented into morphemes. Rules extracted in this way would have a number of practical uses – for example, they could be used to construct stemmers for information retrieval applications, or they could be integrated into morphological analyzers. Our procedure could also be used to replace the first step of algorithms, such as those of Goldsmith (2001) and Snover and Brent (2001), where heuristic methods are employed to generate morphological hypotheses, and then an informationtheoretically/probabilistically motivated measure is used to evaluate or improve such hypotheses. More in general, our algorithm can help reduce the size of the search space that all morphological discovery procedures must explore. Last but not least, the ranked output of (an improved version of) our algorithm can be of use to the linguist analyzing the morphology of a language, who can treat it as a way to pre-process her/his data, while still relying on her/his analytical skills to extract the"
W02-0606,P00-1027,0,0.430985,"Missing"
W02-0606,J00-4006,0,\N,Missing
W02-0606,J90-1003,0,\N,Missing
W02-0606,J92-4003,0,\N,Missing
W03-2501,J90-1003,0,\N,Missing
W03-2501,W02-0606,1,\N,Missing
W03-2501,C00-1027,0,\N,Missing
W03-2501,J92-4003,0,\N,Missing
W03-2501,C02-1096,1,\N,Missing
W09-0201,S07-1003,0,0.0353938,"Missing"
W09-0201,P98-2127,0,0.262846,"eling noun-to-noun and noun-to-verb connections, we selected the 20,000 most frequent nouns and 5,000 most frequent verbs as target concepts (minus stop lists of very frequent items). We selected as target links the top 30 most frequent direct verbnoun dependency paths (e.g., kill+obj+victim), the top 30 preposition-mediated noun-to-noun or While our unified framework is, as far as we know, novel, the specific ways in which we tackle the different tasks are standard. Concept similarity is often measured by vectors of co-occurrence with context words that are typed with dependency information (Lin, 1998; Curran and Moens, 2002). Our approach to selectional preference is nearly identical to the one of Pad´o et al. (2007). We solve SAT analogies with a simplified version of the method of Turney (2006). Detecting whether a pair expresses a target relation by looking at shared connector patterns with model pairs is a common strategy in relation extraction (Pantel and Pennacchiotti, 2008). Finally, our method to detect verb slot similarity is analogous to the “slot overlap” of Joanis et al. (2008) and others. Since we aim at a unified approach, the lack of originality of our task-specific methods"
W09-0201,J07-2002,0,0.144773,"Missing"
W09-0201,D07-1042,0,0.0635675,"Missing"
W09-0201,J06-3003,0,0.436413,"cs, inspired by cognitive science, in which different semantic tasks are tackled using the same underlying repository of distributional information, collected once and for all from the source corpus. Task-specific semantic spaces are then built on demand from the repository. A straightforward implementation of our proposal achieves state-of-the-art performance on a number of unrelated tasks. 1 Introduction Corpus-derived distributional semantic spaces have proved valuable in tackling a variety of tasks, ranging from concept categorization to relation extraction to many others (Sahlgren, 2006; Turney, 2006; Pad´o and Lapata, 2007). The typical approach in the field has been a “local” one, in which each semantic task (or set of closely related tasks) is treated as a separate problem, that requires its own corpus-derived model and algorithms. Its successes notwithstanding, the “one task – one model” approach has also some drawbacks. From a cognitive angle, corpus-based models hold promise as simulations of how humans acquire and use conceptual and linguistic information from their environment (Landauer and Dumais, 1997). However, the common view in cognitive (neuro)science is that humans resort t"
W09-0201,W02-0908,0,0.123723,"to-noun and noun-to-verb connections, we selected the 20,000 most frequent nouns and 5,000 most frequent verbs as target concepts (minus stop lists of very frequent items). We selected as target links the top 30 most frequent direct verbnoun dependency paths (e.g., kill+obj+victim), the top 30 preposition-mediated noun-to-noun or While our unified framework is, as far as we know, novel, the specific ways in which we tackle the different tasks are standard. Concept similarity is often measured by vectors of co-occurrence with context words that are typed with dependency information (Lin, 1998; Curran and Moens, 2002). Our approach to selectional preference is nearly identical to the one of Pad´o et al. (2007). We solve SAT analogies with a simplified version of the method of Turney (2006). Detecting whether a pair expresses a target relation by looking at shared connector patterns with model pairs is a common strategy in relation extraction (Pantel and Pennacchiotti, 2008). Finally, our method to detect verb slot similarity is analogous to the “slot overlap” of Joanis et al. (2008) and others. Since we aim at a unified approach, the lack of originality of our task-specific methods should be regarded as a"
W09-0201,C08-1114,0,0.182485,"eir environment (Landauer and Dumais, 1997). However, the common view in cognitive (neuro)science is that humans resort to a multipurpose semantic memory, i.e., a database of interconnected concepts and properties (Rogers and McClelland, 2004), adapting the information stored there to the task at hand. From an engineering perspective, going back to the corpus to train a different model for each application is inefficient and it runs the risk of overfitting the model to a specific task, while losing sight of its adaptivity – a highly desirable feature for any intelligent system. 2 Related work Turney (2008) recently advocated the need for a uniform approach to corpus-based semantic tasks. Turney recasts a number of semantic challenges in terms of relational or analogical similarity. Thus, if an algorithm is able to tackle the latter, it can Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 1–8, c Athens, Greece, 31 March 2009. 2009 Association for Computational Linguistics 1 3 also be used to address the former. Turney tests his system in a variety of tasks, obtaining good results across the board. His approach amounts to picking a task (analog"
W09-0201,D08-1094,0,\N,Missing
W09-0201,C98-2122,0,\N,Missing
W09-0205,P06-2075,0,0.0281987,"ion by comparing the contexts in which they co-occur to the contexts of known part-of pairs, we can guess that cars and automobiles are synonyms by comparing the contexts in which they co-occur to the contexts linking known synonym pairs. Here, we build on Turney’s work, adding two main methodological innovations that allow us further generalization. First, merging classic approaches to taxonomic and relational similarity, we represent concept pairs by a vector that concatenates information about the contexts in which the two words occur independently, and the contexts in which they co-occur (Mirkin et al. 2006 also integrate information from the lexical patterns in which two words co-occur and similarity of the contexts in which each word occurs on its own, to improve performance in lexical entailment acquisition). Second, we represent contexts as bag of words and bigrams, rather than strings of words (“patterns”) of arbitrary length: we leave it to the machine learning algorithm to zero in on the most interesting words/bigrams. Thanks to the concatenated vector, we can tackle tasks in which the two words are not expected to co-occur even in very large corpora (such as selectional preference). Conc"
W09-0205,D07-1042,0,0.0709968,"veloper.yahoo.com/search/ boss/ 3 http://nodebox.net/code/index.php/ Linguistics 4 http://www.ims.uni-stuttgart.de/ projekte/corplex/TreeTagger/ We carried out our tests on two different corpora: ukWaC, a Web-derived, POS-tagged and lemmatized collection of about 2 billion tokens,1 and the Yahoo! 1 Model implementation http://wacky.sslmit.unibo.it 35 3 3.1 Tasks 3.3 Linguists have long been interested in the semantic constraints that verbs impose on their arguments, a broad area that has also attracted computational modeling, with increasing interest in purely corpus-based methods (Erk, 2007; Padó et al., 2007). This task is of particular interest to us as an example of a broader class of linguistic problems that involve productive constraints on composition. As has been stressed at least since Chomsky’s early work (Chomsky, 1957), no matter how large a corpus is, if a phenomenon is productive there will always be new well-formed instances that are not in the corpus. In the domain of selectional restrictions this is particularly obvious: we would not say that an algorithm learned the constraints on the possible objects/patients of eating simply by producing the list of all the attested objects of th"
W09-0205,2003.mtsummit-papers.42,0,0.0680861,"Missing"
W09-0205,P07-1028,0,0.0548295,"2 http://developer.yahoo.com/search/ boss/ 3 http://nodebox.net/code/index.php/ Linguistics 4 http://www.ims.uni-stuttgart.de/ projekte/corplex/TreeTagger/ We carried out our tests on two different corpora: ukWaC, a Web-derived, POS-tagged and lemmatized collection of about 2 billion tokens,1 and the Yahoo! 1 Model implementation http://wacky.sslmit.unibo.it 35 3 3.1 Tasks 3.3 Linguists have long been interested in the semantic constraints that verbs impose on their arguments, a broad area that has also attracted computational modeling, with increasing interest in purely corpus-based methods (Erk, 2007; Padó et al., 2007). This task is of particular interest to us as an example of a broader class of linguistic problems that involve productive constraints on composition. As has been stressed at least since Chomsky’s early work (Chomsky, 1957), no matter how large a corpus is, if a phenomenon is productive there will always be new well-formed instances that are not in the corpus. In the domain of selectional restrictions this is particularly obvious: we would not say that an algorithm learned the constraints on the possible objects/patients of eating simply by producing the list of all the at"
W09-0205,D08-1094,0,0.0713259,"Missing"
W09-0205,J06-3003,0,0.672034,"instantiating arbitrary semantic relations that keeps track of the contexts in which the words in the pair occur both together and independently. The resulting features are of sufficient generality to allow us, with the help of a standard supervised machine learning algorithm, to tackle a variety of unrelated semantic tasks with good results and almost no task-specific tailoring. 1 Introduction Co-occurrence statistics extracted from corpora lead to good performance on a wide range of tasks that involve the identification of the semantic relation between two words or concepts (Sahlgren, 2006; Turney, 2006). However, the difficulty of such tasks and the fact that they are apparently unrelated has led to the development of largely ad-hoc solutions, tuned to specific challenges. For many practical applications, this is a drawback: Given the large number of semantic relations that might be relevant to one or the other task, we need a multi-purpose approach that, given an appropriate representation and training examples instantiating an arbitrary target relation, can automatically mine new pairs characterized by the same relation. Building on a recent proposal in this direction by Turney (2008), we"
W09-0205,C08-1114,0,0.463287,"these domains, purely corpus-based methods play an increasingly important role in modeling constraints on composition of words, in particular verbal selectional preferences – finding out that, say, children are more likely to eat than apples, whereas the latter are more likely to be eaten (Erk, 2007; Padó et al., 2007). Tasks of this sort differ from relation extraction in that we need to capture productive patterns: we want to find out that shabu shabu (a Japanese meat dish) is eaten whereas ink is not, even if in our corpus neither noun is attested in proximity to forms of the verb to eat. Turney (2008) is the first, to the best of our knowledge, to raise the issue of a unified approach. In particular, he treats synonymy and association as special cases of relational similarity: in the same way in which we might be able to tell that hands and arms are in a part-of relation by comparing the contexts in which they co-occur to the contexts of known part-of pairs, we can guess that cars and automobiles are synonyms by comparing the contexts in which they co-occur to the contexts linking known synonym pairs. Here, we build on Turney’s work, adding two main methodological innovations that allow us"
W09-0205,P06-1015,0,\N,Missing
W09-3207,W09-0201,1,0.819944,"paired t-tests for the relevant degrees of freedom. Even though the SVD-based and pure-vector models are among the top achievers in general, we see that in different tasks different random walk models achieve comparable or even better performances. In particular, for phrasal associates and conceptual associates, the best results are obtained by random walks based on direct measures. the full graph are in Table 1, line 1. The SVD model clearly outperforms the pure-vector based approach and the graph-based approaches. Its performance is above that of previous models trained on the same corpus (Baroni and Lenci, 2009). The best model that we report is based on web search engine results (Chen et al., 2006). Among the graph-based random walk models, flexible walk with parameter 0.5 and fixed 1-step walk with indirect relatedness measures using dot product similarity achieve the highest performance. Concept categorization: Almuhareb (2006) proposed a set of 402 nouns to be categorized into 21 classes of both concrete (animals, fruit. . . ) and abstract (feelings, times. . . ) concepts. Our results on this clustering task are given in Table 1 (line 2). The difference between SVD and pure-vector models is negli"
W09-3207,P06-1127,0,0.0895379,"models are among the top achievers in general, we see that in different tasks different random walk models achieve comparable or even better performances. In particular, for phrasal associates and conceptual associates, the best results are obtained by random walks based on direct measures. the full graph are in Table 1, line 1. The SVD model clearly outperforms the pure-vector based approach and the graph-based approaches. Its performance is above that of previous models trained on the same corpus (Baroni and Lenci, 2009). The best model that we report is based on web search engine results (Chen et al., 2006). Among the graph-based random walk models, flexible walk with parameter 0.5 and fixed 1-step walk with indirect relatedness measures using dot product similarity achieve the highest performance. Concept categorization: Almuhareb (2006) proposed a set of 402 nouns to be categorized into 21 classes of both concrete (animals, fruit. . . ) and abstract (feelings, times. . . ) concepts. Our results on this clustering task are given in Table 1 (line 2). The difference between SVD and pure-vector models is negligible and they both obtain the best performance in terms of both cluster entropy (not sho"
W09-3207,D08-1095,0,0.080606,"e2 connected by an edge with weight a12 . Similarity measures. Let R(q) = p denote a specific random walk process which transforms an Introduction Vector space models, representing word meanings as points in high-dimensional space, have been used in a variety of semantic relatedness tasks (Sahlgren, 2006; Pad´o and Lapata, 2007). Graphs are another way of representing relations between linguistic entities, and they have been used to capture semantic relatedness by using both corpus-based evidence and the graph structure of WordNet and Wikipedia (Pedersen et al., 2004; Widdows and Dorow, 2002; Minkov and Cohen, 2008). We study the relationship between vector space models and graph random walk models by embedding vector space models in graphs. The flexibility offered by graph random walk models allows us to compare the vector space-based similarity measures to extended notions of relatedness and similarity. In particular, a random walk model can be viewed as smoothing direct similarity between two vectors using second-order and even higher-order vectors. This view leads to the second focal point of this paper: We investigate whether random walk models can simulate the smoothing effects obtained by methods"
W09-3207,J07-2002,0,0.438925,"Missing"
W09-3207,N04-3012,0,0.0255047,"aph, it corresponds to two nodes labeled e1 and e2 connected by an edge with weight a12 . Similarity measures. Let R(q) = p denote a specific random walk process which transforms an Introduction Vector space models, representing word meanings as points in high-dimensional space, have been used in a variety of semantic relatedness tasks (Sahlgren, 2006; Pad´o and Lapata, 2007). Graphs are another way of representing relations between linguistic entities, and they have been used to capture semantic relatedness by using both corpus-based evidence and the graph structure of WordNet and Wikipedia (Pedersen et al., 2004; Widdows and Dorow, 2002; Minkov and Cohen, 2008). We study the relationship between vector space models and graph random walk models by embedding vector space models in graphs. The flexibility offered by graph random walk models allows us to compare the vector space-based similarity measures to extended notions of relatedness and similarity. In particular, a random walk model can be viewed as smoothing direct similarity between two vectors using second-order and even higher-order vectors. This view leads to the second focal point of this paper: We investigate whether random walk models can s"
W09-3207,W09-0203,0,0.109769,"Missing"
W09-3207,C02-1114,0,0.0821388,"two nodes labeled e1 and e2 connected by an edge with weight a12 . Similarity measures. Let R(q) = p denote a specific random walk process which transforms an Introduction Vector space models, representing word meanings as points in high-dimensional space, have been used in a variety of semantic relatedness tasks (Sahlgren, 2006; Pad´o and Lapata, 2007). Graphs are another way of representing relations between linguistic entities, and they have been used to capture semantic relatedness by using both corpus-based evidence and the graph structure of WordNet and Wikipedia (Pedersen et al., 2004; Widdows and Dorow, 2002; Minkov and Cohen, 2008). We study the relationship between vector space models and graph random walk models by embedding vector space models in graphs. The flexibility offered by graph random walk models allows us to compare the vector space-based similarity measures to extended notions of relatedness and similarity. In particular, a random walk model can be viewed as smoothing direct similarity between two vectors using second-order and even higher-order vectors. This view leads to the second focal point of this paper: We investigate whether random walk models can simulate the smoothing eff"
W10-2007,E09-1035,0,0.0184,"nd rank 55 collocations, multi-word expressions or semantically related terms (Evert, 2008). From a technical point of view, the innovative aspect of our task is that we do not just look for co-occurrences between two items, but for co-occurrences in the context of a third element, i. e., we are interested in modifier– part pairs that are related when predicated of a certain concept. The method we apply to the extraction of modifier–part pairs when they co-occur with the target concept in a large window is similar to the idea of looking for partially untethered contextual patterns proposed by Garera and Yarowsky (2009), that extract name–pattern–property tuples where the pattern and the property must be adjacent, but the target name is only required to occur in the same sentence. part itself (elephant: trunk) and sometimes something special about the shape, colour, size, or other attributes of the part (elephant: big ears). The data set for modifier extraction and subsequent method evaluation comprises all the concept– modifier–part triples (e. g., onion: brown peel) produced by at least one subject, taken from the German and the Italian norms. The German (Italian) speakers described 41 (30) different conce"
W10-2007,J06-1005,0,0.072437,"Missing"
W10-2007,C92-2082,0,0.0583015,"et al., 2001; McRae et al., 2005; Vinson and Vigliocco, 2008). These feature norms (as collections of subject-elicited properties are called in the relevant literature) are used in simulations of cognitive tasks and experimental design. Moreover, vector spaces that have subject-generated properties as dimensions have been shown to be a good complement or alternative to traditional semantic models based on corpus collocates (Andrews et al., 2009; Baroni et al., 2010). Since the concept–property pairs in feature norms resemble the tuples that relation extraction algorithms extract from corpora (Hearst, 1992; Pantel and Pennacchiotti, 2006), recent research has attempted to extract feature-norm-like concept descriptions from corpora (Almuhareb, 2006; Baroni et al., 2010; Shaoul and Westbury, 2008). From 54 Proceedings of the 2010 Workshop on Cognitive Modeling and Computational Linguistics, ACL 2010, pages 54–62, c Uppsala, Sweden, 15 July 2010. 2010 Association for Computational Linguistics with an adjective–noun structure – together with numeral–noun cases, these constitute the near totality of composite parts in the norms described in section 3. Having thus delimited the scope of our explorati"
W10-2007,W08-1913,1,0.898451,"Missing"
W10-2007,P06-1015,0,0.0153502,"McRae et al., 2005; Vinson and Vigliocco, 2008). These feature norms (as collections of subject-elicited properties are called in the relevant literature) are used in simulations of cognitive tasks and experimental design. Moreover, vector spaces that have subject-generated properties as dimensions have been shown to be a good complement or alternative to traditional semantic models based on corpus collocates (Andrews et al., 2009; Baroni et al., 2010). Since the concept–property pairs in feature norms resemble the tuples that relation extraction algorithms extract from corpora (Hearst, 1992; Pantel and Pennacchiotti, 2006), recent research has attempted to extract feature-norm-like concept descriptions from corpora (Almuhareb, 2006; Baroni et al., 2010; Shaoul and Westbury, 2008). From 54 Proceedings of the 2010 Workshop on Cognitive Modeling and Computational Linguistics, ACL 2010, pages 54–62, c Uppsala, Sweden, 15 July 2010. 2010 Association for Computational Linguistics with an adjective–noun structure – together with numeral–noun cases, these constitute the near totality of composite parts in the norms described in section 3. Having thus delimited the scope of our exploration, we will adopt the following t"
W10-2007,J90-1003,0,0.359395,"elated work in feature norm collection and prediction, and mention some relevant literature on the extraction of significant cooccurrences from corpora. Feature-based concept description norms have been collected in psychology for decades. Among the more recent publicly available norms of this sort, there are those collected by Garrard et al. (2001), Vinson and Vigliocco (2008) and McRae et al. (2005). The latter was the main methodological inspiration for the bilingual norms we rely on (see section 3 below). The norms of McRae and Following a very long tradition in computational linguistics (Church and Hanks, 1990), we use cooccurrence statistics for words in certain contexts to hypothesise a meaningful connection between the words. In this respect, what we propose is not different from common methods to extract and rank 55 collocations, multi-word expressions or semantically related terms (Evert, 2008). From a technical point of view, the innovative aspect of our task is that we do not just look for co-occurrences between two items, but for co-occurrences in the context of a third element, i. e., we are interested in modifier– part pairs that are related when predicated of a certain concept. The method"
W11-1301,J10-4006,1,0.191309,"e that are attested and are formed by the combination of any of the 4K adjectives and 8K nouns in the vocabulary. 5 Semantic space construction For each of the items in our vocabulary, we first build 10K-dimensional vectors by recording their sentence-internal co-occurrence with the top 10K most frequent content words (nouns, adjectives or verbs) in the corpus. The raw co-occurrence counts are then transformed into Local Mutual Information scores (Local Mutual Information is an association measure that closely approximates the commonly used Log-Likelihood Ratio while being simpler to compute (Baroni and Lenci, 2010; Evert, 2005)). Next, we reduce the full co-occurrence matrix applying the Singular Value Decomposition (SVD) operation, like in LSA and related distributional semantic methods (Landauer and Dumais, 1997; Rapp, 2003; Sch¨utze, 1997). The original 45K-by10K-dimensional matrix is reduced in this way to a 45K-by-300 matrix, where vocabulary items are represented by their coordinates in the space spanned by the first 300 right singular vectors of the SVD solution. This step is motivated by the fact that we will estimate linear models to predict the values of each dimension of an AN from the dimen"
W11-1301,D10-1115,1,0.0623442,"age is infinite (Chomsky, 1957). This cricticism also applies to distributional semantic models that build semantic representations of words or phrases in terms of vectors recording their distributional co-occurrence patterns in a corpus (Turney and Pantel, 2010), but have no obvious way to generalize to word combinations that have not been observed in the corpus. To address this problem, there have been several recent attempts to incorporate into distributional semantic models a component that generates vectors for unseen linguistic structures by compositional operations in the vector space (Baroni and Zamparelli, 2010; Guevara, 2010; Mitchell and Lapata, 2010). The ability to work with unattested data leads to the question of why a linguistic expression might not be attested in even an extremely large and wellbalanced corpus. Its absence might be motivated by a number of factors: pure chance, the fact that the expression is ungrammatical, uses a rare structure, describes false facts, or, finally, is nonsensical. One criticism from generative linguists is precisely that statistical methods could not distinguish between these various possibilities. The difficulty of solving this problem can be illustrated by"
W11-1301,D08-1094,0,0.140491,"Missing"
W11-1301,W10-2805,0,0.551616,"7). This cricticism also applies to distributional semantic models that build semantic representations of words or phrases in terms of vectors recording their distributional co-occurrence patterns in a corpus (Turney and Pantel, 2010), but have no obvious way to generalize to word combinations that have not been observed in the corpus. To address this problem, there have been several recent attempts to incorporate into distributional semantic models a component that generates vectors for unseen linguistic structures by compositional operations in the vector space (Baroni and Zamparelli, 2010; Guevara, 2010; Mitchell and Lapata, 2010). The ability to work with unattested data leads to the question of why a linguistic expression might not be attested in even an extremely large and wellbalanced corpus. Its absence might be motivated by a number of factors: pure chance, the fact that the expression is ungrammatical, uses a rare structure, describes false facts, or, finally, is nonsensical. One criticism from generative linguists is precisely that statistical methods could not distinguish between these various possibilities. The difficulty of solving this problem can be illustrated by the difference"
W11-1301,P08-1028,0,0.216888,"s. The difficulty of solving this problem can be illustrated by the difference in semantics between the adjective-noun pairs in (1a) and (1b): In this paper, we present a first attempt to characterize the semantic deviance of composite expressions in distributional semantics. Specifically, we look for properties of adjective-noun combinations within a vectorbased semantic space that might cue their lack of meaning. We evaluate four different compositionality models shown to have various levels of success in representing the meaning of AN pairs: the simple additive and multiplicative models of Mitchell and Lapata (2008), and the linear-map-based models of Guevara (2010) and Baroni and Zamparelli (2010). For each model, we generate composite vectors for a set of AN combinations unattested in the source corpus and which have been deemed either acceptable or semantically deviant. We then compute measures that might cue semantic anomaly, and compare each model’s results for the two classes of ANs. Our study shows that simple, unsupervised cues can indeed significantly tell unattested but acceptable ANs apart from impossible, or deviant, ANs, and that the simple additive and multiplicative models are the most eff"
W11-1301,D09-1045,0,0.0196325,"al acid sound odd, in the absence of very particular contexts, while (2) sounds quite natural. Whatever the nature of coercion, we do not want it to run so smoothly that any combination of A and N (or V and its arguments) becomes meaningful and completely acceptable. 2.2 Distributional approaches to meaning composition Although the issue of how to compose meaning has attracted interest since the early days of distributional semantics (Landauer and Dumais, 1997), recently a very general framework for modeling compositionality has been proposed by Mitchell and Lapata (Mitchell and Lapata, 2008; Mitchell and Lapata, 2009; Mitchell and Lapata, 2010). Given two vectors u and v, they identify two general classes of composition models, (linear) additive models: p = Au + Bv (1) where A and B are weight matrices, and multiplicative models: p = Cuv where C is a weight tensor projecting the uv tensor product onto the space of p. Mitchell and Lapata derive two simplified models from these general forms: The simplified additive model given by p = αu + βv, and a simplified multiplicative approach that reduces to component-wise multiplication, where the i-th component of the composed vector is given by: pi = ui vi . Mitc"
W11-1301,2003.mtsummit-papers.42,0,0.0574192,"ecording their sentence-internal co-occurrence with the top 10K most frequent content words (nouns, adjectives or verbs) in the corpus. The raw co-occurrence counts are then transformed into Local Mutual Information scores (Local Mutual Information is an association measure that closely approximates the commonly used Log-Likelihood Ratio while being simpler to compute (Baroni and Lenci, 2010; Evert, 2005)). Next, we reduce the full co-occurrence matrix applying the Singular Value Decomposition (SVD) operation, like in LSA and related distributional semantic methods (Landauer and Dumais, 1997; Rapp, 2003; Sch¨utze, 1997). The original 45K-by10K-dimensional matrix is reduced in this way to a 45K-by-300 matrix, where vocabulary items are represented by their coordinates in the space spanned by the first 300 right singular vectors of the SVD solution. This step is motivated by the fact that we will estimate linear models to predict the values of each dimension of an AN from the dimensions of the components. We thus prefer to work in a smaller and denser space. As a sanity check, we verify that we obtain state-of-the-art-range results on various semantic tasks using this reduced semantic space (n"
W11-1301,J83-3004,0,\N,Missing
W11-2501,N09-1003,0,0.0619297,"Missing"
W11-2501,J10-4006,1,0.539292,"t will contain the highest random value. Thus, this RelationCardinality baseline will favor relations that tend to have large relata set across concepts, controlling for effects due to different cardinalities across semantic relations (again, see Table 1 above). DSMs. We choose a few ways to construct DSMs for illustrative purposes only. All the models contain vector representations for the same words, namely, approximately, the top 20K most frequent nouns, 5K most frequent adjectives and 5K most frequent verbs in the combined corpora. All the models use Local Mutual Information (Evert, 2005; Baroni and Lenci, 2010) to weight raw co-occurrence counts (this association measure is obtained by multiplying the raw count by Pointwise Mutual Information, and it is a close approximation to the Log-Likelihood Ratio). Three DSMs are based on counting co-occurrences with collocates within a window of fixed width, in the tradition of HAL (Lund and Burgess, 1996) and many later models. The ContentWindow2 model records sentence-internal co-occurrence with the nearest 2 content words to the left and right of each target concept (the same 30K target nouns, verbs and adjectives are also employed as context content words"
W11-2501,J06-1003,0,0.0168484,"SMs trained on very large corpora. Think of the presence of extremely rare nouns like casuarina in AP, of proper nouns in WordSim (it is not clear to us that DSMs are adequate semantic models for referring expressions – at the very least they should not be mixed up lightly with common nouns), or multi-word expressions in other data sets. 3 How we intend to BLESS distributional semantic evaluation DSMs measure the distributional similarity between words, under the assumption that proximity in distributional space models semantic relatedness, includ3 ing, as a special case, semantic similarity (Budanitsky and Hirst, 2006). However, semantically related words in turn differ for the type of relation holding between them: e.g., dog is strongly related to both animal and tail, but with different types of relations. Therefore, evaluating the intrinsic ability of DSMs to represent the semantic space of a word entails both (i) determining to what extent words close in semantic space are actually semantically related, and (ii) analyzing, among related words, which type of semantic relation they tend to instantiate. Two models can be equally very good in identifying semantically related words, while greatly differing f"
W11-2501,J06-3003,0,0.0188729,"a hypernym, and we have no reason, in general semantic terms, to penalize one or the other. To maximize coverage, we also make sure that, for each concept and relation, a reasonable number of relata are frequently attested in our reference corpora (see statistics below), we only include single-word relata and, where appropriate, we include multiple forms for the same relatum (both sock and socks as coordinates of scarf – as discussed in Section 4.1, we avoided similar ambiguous items as target concepts). Currently, distributional models for attributional similarity and relational similarity (Turney, 2006) are tested on different data sets, e.g., TOEFL and SAT respectively (briefly, attributional similarity 4 pertains to similarity between a pair of concepts in terms of shared properties, whereas relational similarity measures the similarity of the relations instantiated by couples of concept pairs). Conversely, BLESS is not biased towards any particular type of semantic similarity and thus allows both families of models to be evaluated on the same data set. Given a concept, we can analyze the types of relata that are selected by a model as more attributionally similar to the target. Alternativ"
W11-2503,N09-1003,0,0.0867135,"Missing"
W11-2503,J10-4006,1,0.0746075,"ual word. Local Mutual Information is an association measure that closely approximates the commonly used LogLikelihood Ratio while being simpler to compute (Evert, 2005). In this way, we obtain an image-based distribuhttp://clic.cimec.unitn.it/dm 25 Integrating distributional models Experimental setup The DM text-based model DM has been shown to be near or at the state of the art in a great variety of semantic tasks, ranging from modeling similarity judgments to concept categorization, predicting selectional preferences, relation classification and more. The DM model is described in detail by Baroni and Lenci (2010), where it is referred to as TypeDM. In brief, the model is trained on a large corpus of about 2.8 billion tokens that include Web documents, the Wikipedia and the BNC. DM is a structured model, where the collocates are labeled with the link that connect them to the target words. The links are determined by a mixture of dependency parse information and lexico-syntactic patterns, resulting in distributional features (the dimensions of the semantic space) such as subject kill, with gun or as sharp as. The score of a target word with a feature is not based on the absolute number of times they co-"
W11-2503,N10-1011,0,0.339127,"rch and retrieval have been built upon the connection between text and visual features. Such models learn the statistical models which characterize the joint statistical distribution of observed visual features and verbal image tags (Hofmann, 2001; Hare et al., 2008). This line of research is pursuing the reverse of what we are interested in: using text to improve the semantic description of images, whereas we want to exploit images to improve our approximation to word meaning. Feng and Lapata are the first trying to integrate authentic visual information in a text-based distributional model (Feng and Lapata, 2010). Using a collection of BBC news with pictures as corpus, they train a Topic model where text and visual words are represented in terms of the same shared latent dimensions (topics). In this framework, word meaning is modeled as a probability distribution over a set of latent multimodal topics and the similarity between two words can be estimated by measuring the topics they have in common. A better correlation with semantic intuitions is obtainable when visual modality is taken into account, in comparison to estimating the topic structure from text only. Although Feng and Lapata’s work is ver"
W11-2508,J10-4006,1,0.708546,"60s vector and a 90s vector, with the same dimensions (context elements), but different co-occurrence counts. The vectors can be compared by computing the cosine of their angle. Since the context vectors are computed in the same vector space, the procedure is completely equivalent to calculating similarity between two different words in the same corpora; the context vectors can be considered as belonging to one co-occurrence matrix and corresponding to two different row elements word 60s and word 90s. 2 LMI proved to be a good measure for different semantic tasks, see for example the work of Baroni and Lenci, 2010. group more frequent in 90s more frequent in 60s examples users sleep disease card dealers coach energy cent sim 0.29 0.23 0.87 0.17 0.16 0.25 0.79 0.99 freq -0.94 -0.32 -0.3 -0.1 0.04 0.12 0.14 1.13 all words frequent in 90s frequent in 60s sim-HR 0.386∗∗ 0.445∗∗ 0.163 freq-HR 0.301∗∗ 0.184 0.310 sim-freq 0.380∗∗ 0.278∗ 0.406∗ Table 2: Correlation between similarity (sim), frequency (freq) and human ranking (HR) values for all words, words more frequent in 60s and more frequent in 90s. Values statistically significant for p = 0.01(0.05) in onesample t-test are marked with ∗∗ (∗ ). Table 1: E"
W11-2508,cook-stevenson-2010-automatically,0,0.0282185,"l semantics model (Turney and Pantel, 2010). Semantic change, defined as a change of one or more meanings of the word in time (Lehmann, 1992), is of interest to historical linguistics and is related to the natural language processing task of unknown word sense detection (Erk, 2006). Developing automatic methods for identifying changes in word meaning can therefore be useful for both theoretical linguistics and a variety of NLP applications which depend on lexical information. Some first automatic approaches to the semantic change detection task were recently proposed by Sagi et al. (2009) and Cook and Stevenson (2010). These works focus on specific types of semantic change, i.e., Sagi et al. (2009) aim to identify widening and narrowing of meaning, while Cook and Stevenson (2010) concentrate on amelioration and pejoration cases. Their evaluation of the proposed methods is rather qualitative, concerning just a few examples. In present work we address the task of automatic detection of the semantic change of words in quantitative way, comparing our novel distributional similarity approach to a relative-frequency-based method. For the evaluation, we used the Google Books Ngram data from the 1960s and 1990s, t"
W11-2508,N06-1017,0,0.0665781,"ends in the last centuries. Mitchel et al. (2010) exploit the change in word frequency as the main measure for the quantitative investigation of cultural and linguistic phenomena; in this paper, we extend this approach by measuring the semantic similarity of the word occurrences in two different time points using distributional semantics model (Turney and Pantel, 2010). Semantic change, defined as a change of one or more meanings of the word in time (Lehmann, 1992), is of interest to historical linguistics and is related to the natural language processing task of unknown word sense detection (Erk, 2006). Developing automatic methods for identifying changes in word meaning can therefore be useful for both theoretical linguistics and a variety of NLP applications which depend on lexical information. Some first automatic approaches to the semantic change detection task were recently proposed by Sagi et al. (2009) and Cook and Stevenson (2010). These works focus on specific types of semantic change, i.e., Sagi et al. (2009) aim to identify widening and narrowing of meaning, while Cook and Stevenson (2010) concentrate on amelioration and pejoration cases. Their evaluation of the proposed methods"
W11-2508,D10-1114,0,0.0103254,"be made after accurate investigation of ‘false-positive’ examples — the ones that have low similarity but were ranked as ‘not changed’ by raters — like ‘sleep’ and ‘parent’. It is enough to have a look at their highest weighted co-occurrences to admit that the context of their usage has indeed changed (Table 3). These examples show the difference between the phenomenon of semantic change in linguistics and the case of context change. It is well known that the different contexts that distributional semantics catches do not always directly refer to what linguists would consider distinct senses (Reisinger and Mooney, 2010). Most people would agree that the word ‘parent’ has the same meaning now as it had 40 years before, still the social context in which it is used has evidently changed, reflected by the more frequent ‘single parent family(ies)’ collocate found in the 90s. The same is true for ‘sleep’, whose usage context did not change radically, but might have a 70 60s 90s ‘parent’ p. company 2643 p. education 1905 p. corporation 1617 p. material 1337 p. body 1082 p. compound 818 common p. 816 p. families 17710 single p. 10724 p. company 8367 p. education 5884 p. training 5847 p. involvement 5591 p. family 50"
W11-2508,W09-0214,0,0.062489,"nts using distributional semantics model (Turney and Pantel, 2010). Semantic change, defined as a change of one or more meanings of the word in time (Lehmann, 1992), is of interest to historical linguistics and is related to the natural language processing task of unknown word sense detection (Erk, 2006). Developing automatic methods for identifying changes in word meaning can therefore be useful for both theoretical linguistics and a variety of NLP applications which depend on lexical information. Some first automatic approaches to the semantic change detection task were recently proposed by Sagi et al. (2009) and Cook and Stevenson (2010). These works focus on specific types of semantic change, i.e., Sagi et al. (2009) aim to identify widening and narrowing of meaning, while Cook and Stevenson (2010) concentrate on amelioration and pejoration cases. Their evaluation of the proposed methods is rather qualitative, concerning just a few examples. In present work we address the task of automatic detection of the semantic change of words in quantitative way, comparing our novel distributional similarity approach to a relative-frequency-based method. For the evaluation, we used the Google Books Ngram da"
W13-0104,E12-1004,1,0.86183,"re white possible free hypothetical naive potential safe impossible severe presumed vile likely hard probable nasty mere intelligent putative meagre mock ripe theoretical stable Table 1: Evaluated adjectives. Intensional (I) and non-intensional (N) adjectives are paired by frequency. over challenging foils. Turney (2012) shows how the observed vectors outperform any compositionallyderived model in a paraphrasing task. Grefenstette et al. (2013) reach state-of-the-art performance on widely used sentence similarity test sets with composition functions optimized on the observed vectors (see also Baroni et al., 2012; Baroni and Zamparelli, 2010; Boleda et al., 2012). Since we use the same criterion to evaluate the quality of the models, we are careful to separate training phrases from those used for evaluation (we introduce the test set in the next section). The weighted additive, dilation and full-additive models require one single set of parameters for all adjectives, and we thus use the top 10K most frequent phrases in our semantic space (excluding test items) for training. For the lexical function model, we need to train a separate weight matrix for each adjective. We do this by using as training dat"
W13-0104,D10-1115,1,0.512885,"he noun are relevant factors in the distributional representation of adjective phrases. 1 Introduction Distributional semantics (see Turney and Pantel, 2010, for an overview) has been very successful in modeling lexical semantic phenomena, from psycholinguistic facts such as semantic priming (McDonald and Brew, 2004) to tasks such as picking the right synonym on a TOEFL exercise (Landauer and Dumais, 1997). More recently, interest has increased in using distributional models to account not only for word meaning but also for phrase meaning, i.e. semantic composition (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Socher et al., 2012; Garrette et al., 2012). Adjectival modification of nouns is a particularly useful and at the same time challenging testbed for different distributional models of composition, because syntactically it is very simple, while the semantic effect of the composition is very variable and potentially complex due to the frequent context dependence of the relation between the adjective and the noun (Asher, 2011, provides recent discussion). As a comparatively underexplored area of semantic theory, it is also an empirical domain where distributional models can give feedback to theo"
W13-0104,D12-1112,1,0.92692,"sues for the increasingly popular distributional approaches to semantics. First, as intensional adjectives cannot be modeled as first-order properties, it is hard to predict what their representations might look like or what their semantic effect would be in standard distributional models of composition based on vector addition or multiplication. This is so because addition and multiplication correspond to feature combination (see Section 2 for discussion), and it is not obvious what set of distinctive distributional features an intensional adjective would contribute on a consistent basis. In Boleda et al. (2012), we presented a first distributional semantic study of intensional adjectives. However, our study was limited in two ways. First, it compared intensional adjectives with a very narrow class of non-intensional adjectives, namely color terms; this raises doubts about the generality of our results. Second, the study had methodological weaknesses, as we did not separate training and test data, nor did we do any systematic parameter tuning prior to carrying out our experiments. This paper adresses these limitations by covering a wider variety of adjectives and using a better implementation of the"
W13-0104,W13-0112,0,0.0292883,"ative Matrix Factorization produces reduced dimensions that have no negative values, and are not fully dense. 4 I N I N alleged loose necessary modern former wide past black future white possible free hypothetical naive potential safe impossible severe presumed vile likely hard probable nasty mere intelligent putative meagre mock ripe theoretical stable Table 1: Evaluated adjectives. Intensional (I) and non-intensional (N) adjectives are paired by frequency. over challenging foils. Turney (2012) shows how the observed vectors outperform any compositionallyderived model in a paraphrasing task. Grefenstette et al. (2013) reach state-of-the-art performance on widely used sentence similarity test sets with composition functions optimized on the observed vectors (see also Baroni et al., 2012; Baroni and Zamparelli, 2010; Boleda et al., 2012). Since we use the same criterion to evaluate the quality of the models, we are careful to separate training phrases from those used for evaluation (we introduce the test set in the next section). The weighted additive, dilation and full-additive models require one single set of parameters for all adjectives, and we thus use the top 10K most frequent phrases in our semantic s"
W13-0104,W10-2805,0,0.495283,"experiments. This paper adresses these limitations by covering a wider variety of adjectives and using a better implementation of the composition functions, and performs several qualitative analyses on the results. Our results confirm that high quality adjective composition is possible in distributional models: Meaningful vectors can be composed, if we take phrase vectors directly extracted from the corpus as a benchmark. In addition, we find (perhaps unsurprisingly) that models that replicate higher-order predication within a distributional approach, such as Baroni and Zamparelli (2010) and Guevara (2010), fare better than models based on vector addition or multiplication (Mitchell and Lapata, 2010). However, unlike our previous study, we find no difference in the relative success of the different composition models on intensional vs. non-intensional modification, nor in relevant aspects of the distributional representations of corpus-harvested phrases. Rather, two relevant effects involve the polysemy of the noun and the extent to which the adjective denotes a typical attribute of the entity described by the noun. These results indicate that, in general, adjectival modification is more comple"
W13-0104,D11-1050,0,0.234325,"Missing"
W13-0104,J95-1001,0,0.181377,"l modification share properties that have gone unappreciated. If the type of modification does not explain the differences in the observed data, what does? An analysis reveals two relevant factors. The first one is the polysemy of the head noun. We find that, the more polysemous a noun is, the less similar its vector is to the corresponding phrase vector. It is plausible that modifying a noun has a larger impact when the noun is polysemous, as the adjective narrows down the meaning of the noun; indeed, adjectives have been independently shown to be powerful word sense disambiguators of nouns (Justeson and Katz, 1995). In distributional terms, the adjective notably “shifts” the vector of polysemous nouns, but for monosemous nouns there is just not much shifting room. This is reasonable but unsurprising; what is more worthy of attention is that this effect is invariant to adjective type. Both non-intensional and intensional adjectives have meaning modulating power, as shown in Table 3. For example, ripe selects for the sense of shock that has to do with a pile of sheaves of grain or corn. Similarly, past is incompatible with physical senses of range such as that referring to mountains or a cooking appliance"
W13-0104,P04-1003,0,0.0370005,"perform simple feature union or intersection, (3) contrary to what the theoretical literature might lead one to expect, do not yield a distinction between intensional and non-intensional modification, and (4) suggest that head noun polysemy and whether the adjective corresponds to a typical attribute of the noun are relevant factors in the distributional representation of adjective phrases. 1 Introduction Distributional semantics (see Turney and Pantel, 2010, for an overview) has been very successful in modeling lexical semantic phenomena, from psycholinguistic facts such as semantic priming (McDonald and Brew, 2004) to tasks such as picking the right synonym on a TOEFL exercise (Landauer and Dumais, 1997). More recently, interest has increased in using distributional models to account not only for word meaning but also for phrase meaning, i.e. semantic composition (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Socher et al., 2012; Garrette et al., 2012). Adjectival modification of nouns is a particularly useful and at the same time challenging testbed for different distributional models of composition, because syntactically it is very simple, while the semantic effect of the composition is very"
W13-0104,D12-1110,0,0.217634,"in the distributional representation of adjective phrases. 1 Introduction Distributional semantics (see Turney and Pantel, 2010, for an overview) has been very successful in modeling lexical semantic phenomena, from psycholinguistic facts such as semantic priming (McDonald and Brew, 2004) to tasks such as picking the right synonym on a TOEFL exercise (Landauer and Dumais, 1997). More recently, interest has increased in using distributional models to account not only for word meaning but also for phrase meaning, i.e. semantic composition (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Socher et al., 2012; Garrette et al., 2012). Adjectival modification of nouns is a particularly useful and at the same time challenging testbed for different distributional models of composition, because syntactically it is very simple, while the semantic effect of the composition is very variable and potentially complex due to the frequent context dependence of the relation between the adjective and the noun (Asher, 2011, provides recent discussion). As a comparatively underexplored area of semantic theory, it is also an empirical domain where distributional models can give feedback to theoreticians about how a"
W13-0603,E12-1004,1,0.838773,"ntence similarity considers word order seriously. We do not exclude that in real-world tasks systems which ignore word order may still attain satisfactory results (as the results of Blacoe and Lapata 2012 suggest), but this will not be evidence of having truly captured compositionality. Moreover, a hidden conclusion (or, better, assumption!) of the evaluations conducted so far on CDSMs seems to be that grammatical words, in particular determiners, play no role in sentence meaning and hence sentence similarity and paraphrase detection. A first study on this class of words has been presented in Baroni et al. (2012) where it is shown that DSMs can indeed capture determiner meaning and their role in the entailment between quantifier phrases. The data sets used in Mitchell and Lapata (2008) and Grefenstette and Sadrzadeh (2011b) focus on verb meaning and its sense disambiguation within context, and consider sentences where determiners are just place-holders to simply guarantee grammaticality, but do not play any role neither in the human judgments nor in the model evaluation – in which they are simply ignored. Similarly, Blacoe and Lapata (2012) evaluate the compositional models on full sentences but again"
W13-0603,D12-1050,0,0.463977,"rb were extracted from the BNC and sentences in simple past form (with articles if necessary) were generated. For example, starting from met, the two sentences “The system met the criterion” and “The child met the house” were generated. For each sentence, two new versions were created by replacing the verb with two synonyms representing two verb senses (e.g., “The system visited the criterion” and “The system satisfied the criterion”). The data set consists of 200 pairs of sentences annotated with human similarity judgments. Large-scale full sentence paraphrasing data Socher et al. (2011) and Blacoe and Lapata (2012) tackle the challenging task of evaluating CDSMs against large-scale full sentence data. They use the Microsoft Research Paraphrase Corpus (Dolan et al., 2004) as data set. The corpus consists of 5800 pairs of sentences extracted from news sources on the web, along with human annotations indicating whether each pair captures a paraphrase/semantic equivalence relationship. The evaluation experiments conducted against these data sets seem to support the following conclusions: • “the model should be sensitive to the order of the words in a phrase (for composition) or a word pair (for relations),"
W13-0603,P11-1020,0,0.0371892,"g whether a DSM is “cheating” in accomplishing this task, or, better, if it does detect paraphrases but does not properly capture compositionality. Paraphrase set The sentences are grouped into sets of paraphrases. Some groups are rather similar to each other (for instance they are about someone playing some instrument) though they clearly describe a different situation (the player is a man vs. a woman or the instrument is a guitar vs. a violin), as it happens for the sentences in Group 1 and Group 2 listed in Table 1. We took as starting point the Microsoft Research Video Description Corpus (Chen and Dolan, 2011) considering only those sentences that could be simplified to fit in the CFG described above. We have obtained 20 groups of sentences. Groups which were left with just a few sentences after grammar-based trimming have been extended adding sentences with the nouns modified by an attributive adjective (chosen so that it would not distort the meaning of the sentence, e.g. we have added tall as modifier of person in “A tall person makes a cake”, if there is no original sentence in the group that would describe the person differently), or adding sentences with a determiner similar to the one used i"
W13-0603,C04-1051,0,0.052391,"system met the criterion” and “The child met the house” were generated. For each sentence, two new versions were created by replacing the verb with two synonyms representing two verb senses (e.g., “The system visited the criterion” and “The system satisfied the criterion”). The data set consists of 200 pairs of sentences annotated with human similarity judgments. Large-scale full sentence paraphrasing data Socher et al. (2011) and Blacoe and Lapata (2012) tackle the challenging task of evaluating CDSMs against large-scale full sentence data. They use the Microsoft Research Paraphrase Corpus (Dolan et al., 2004) as data set. The corpus consists of 5800 pairs of sentences extracted from news sources on the web, along with human annotations indicating whether each pair captures a paraphrase/semantic equivalence relationship. The evaluation experiments conducted against these data sets seem to support the following conclusions: • “the model should be sensitive to the order of the words in a phrase (for composition) or a word pair (for relations), when the order affects the meaning.” (Turney, 2012) • “experimental results demonstrate that the multiplicative models are superior to the additive alternative"
W13-0603,W13-0112,0,0.748012,"e seriously, we built a data set of intransitive and transitive sentences in which word order and determiners have the chance to prove their worth in sentence similarity and paraphrase detection. 2 Compositional Distributional Semantic Models In this section we won’t present a proper overview of CDSMs, but focus only on those models we will be testing in our experiments, namely the multiplicative and additive models of Mitchell and Lapata (2008, 2009, 2010), and the lexical function model that represents the work carried out by Baroni and Zamparelli (2010), Grefenstette and Sadrzadeh (2011b), Grefenstette et al. (2013). We leave a re-implementation of Socher et al. (2012), another approach holding much promise for distributional composition with grammatical words, to future work. Multiplicative and additive models While Mitchell and Lapata (2008, 2009, 2010) propose a general framework that encompasses most of the CDSMs currently available, their empirical work focuses on two simple but effective models where the components of the vector resulting from the composition of two input vectors contain (functions of) geometric or additive averages of the corresponding input components. Given input vectors u and v"
W13-0603,D11-1129,0,0.299539,"Missing"
W13-0603,W11-2507,0,0.467995,"Missing"
W13-0603,W10-2805,0,0.0970267,"ive models, but it is not clear how to extend them to composition of more than two words. Non-terminals (Grammar) S → DP V P DP → DET N DP → N N → ADJ N V P → IV V P → T V DP Terminals (Lexicon) DET → a; some; the; one; two; three; no N → man; lady; violin; guitar; . . . ADJ → big; large; acoustic; . . . IV → performs; drinks; flies; . . . T V → cuts; eats; plays; . . . Figure 1: CFG of the fragment of English in our data set Adjective matrices are estimated from corpus-extracted examples of input noun vectors and the corresponding output adjective-noun phrase vectors, an idea also adopted by Guevara (2010). The approach of Baroni and Zamparelli, termed lexfunc (because specific lexical items act as functors), is actually a specific instantiation of the DisCoCat formalism (Clark, 2012; Coecke et al., 2010), that looks at the general case of n-ary composition functions, encoded in higher-order tensors, with function application modeled by tensor contraction, a generalization of matrix-by-vector multiplication to tensors of arbitrary order. The DisCoCat approach has also been applied to transitive verbs by Grefenstette and Sadrzadeh (2011a) and Grefenstette and Sadrzadeh (2011b). The regression me"
W13-0603,P08-1028,0,0.825558,"o similarity (Mitchell and Lapata, 2010), Turney (2012) obtains an extended version including word order variations of the original phrases, which are automatically judged to have a very low similarity (e.g., certain circumstance and case particular). Sentence similarity: Intransitive Sentences One of the first proposals to look at verb-argument composition traces back to Kintsch (2001) who was interested in capturing the different verb senses activated by different arguments, e.g., “The color ran” vs. “The horse ran”, but the model was tested only on a few sentences. Starting from this work, Mitchell and Lapata (2008) made an important step forward by developing a larger data set of subject+intransitive-verb sentences. They began with frequent noun-verb tuples (e.g., horse ran) extracted from the British National Corpus (BNC) and paired them with sentences with two synonyms of the verb, representing distinct verb senses, one compatible and the other incompatible with the argument (e.g., horse galloped and horse dissolved). The tuples were converted into simple sentences (in past tense form) and articles were added to nouns when appropriate. The final data set consists of 120 sentences with 15 original verb"
W13-0603,D09-1045,0,0.0388762,"Missing"
W13-0603,D12-1110,0,0.206824,"ive sentences in which word order and determiners have the chance to prove their worth in sentence similarity and paraphrase detection. 2 Compositional Distributional Semantic Models In this section we won’t present a proper overview of CDSMs, but focus only on those models we will be testing in our experiments, namely the multiplicative and additive models of Mitchell and Lapata (2008, 2009, 2010), and the lexical function model that represents the work carried out by Baroni and Zamparelli (2010), Grefenstette and Sadrzadeh (2011b), Grefenstette et al. (2013). We leave a re-implementation of Socher et al. (2012), another approach holding much promise for distributional composition with grammatical words, to future work. Multiplicative and additive models While Mitchell and Lapata (2008, 2009, 2010) propose a general framework that encompasses most of the CDSMs currently available, their empirical work focuses on two simple but effective models where the components of the vector resulting from the composition of two input vectors contain (functions of) geometric or additive averages of the corresponding input components. Given input vectors u and v, the multiplicative model (mult) returns a composed v"
W13-0603,D10-1115,1,\N,Missing
W13-3206,W10-2805,0,0.653071,"difficult to disentangle. Broadly, these concern (i) the input representations fed to composition; (ii) the composition operation proper; (iii) the method to estimate the parameters of the composition operation. For example, Mitchell and Lapata in their classic 2010 study propose a set of composition operations (multiplicative, additive, etc.), but they also experiment with two different kinds of input representations (vectors recording co-occurrence with words vs. distributions over latent topics) and use supervised training via a grid search over parameter settings to estimate their models. Guevara (2010), to give just one further example, is not only proposing a different composition method with respect to Mitchell and Lapata, but he is also adopting different input vectors (word cooccurrences compressed via SVD) and an unsupervised estimation method based on minimizing the distance of composed vectors to their equivalents directly extracted from the source corpus. Blacoe and Lapata (2012) have recently highlighted the importance of teasing apart the different aspects of a composition framework, presenting an evaluation in which different input vector representations are crossed with differen"
W13-3206,J10-4006,1,0.061212,"ces) and arguments (vectors). Given input terms u and v represented by (~u, Au ) and (~v , Av ), respectively, their composition vector is obtained by applying first a linear transformation and then the hyperbolic tangent function to the concatenation of the products Au~v and Av ~u (see Table 1 for the equation). Socher and colleagues also present a way to construct matrix representations for specific phrases, needed to scale this composition method to larger constituents. We ignore it here since we focus on the two-word case. idea of distributional semantics as a general similarity resource (Baroni and Lenci, 2010). Moreover, supervised methods are highly compositionmodel-dependent, and for models such as Fulladd and Lexfunc we are not aware of proposals about how to estimate them in a supervised manner. Socher et al. (2011) propose an autoencoding strategy. Given a decomposition function that reconstructs the constituent vectors from a phrase vector (e.g., it re-generates green and jacket vectors from the composed green jacket vector), the composition parameters minimize the distance between the original and reconstructed input vectors. This method does not require hand-labeled training data, but it is"
W13-3206,C12-2054,0,0.0389268,"Lapata (2010) and many others), vectors whose di1 We made the software we used to construct semantic models and estimate and test composition methods available online at http://clic.cimec.unitn.it/ composes/toolkit/ 2 Erk and Pad´o (2008) and Thater et al. (2010) use input vectors that have been adapted to their phrasal contexts, but then apply straightforward composition operations such as addition and multiplication to these contextualized vectors. Their approaches are thus not alternative cDSMs, but special ways to construct the input vectors. Grefenstette and Sadrzadeh (2011a; 2011b) and Kartsaklis et al. (2012) propose estimation techniques for the tensors in the functional model of Coecke et al. (2010). Turney (2012) does not compose representations but similarity scores. 51 Guevara (2010) and Zanzotto et al. (2010) explore a full form of the additive model (Fulladd), where the two vectors entering a composition process are pre-multiplied by weight matrices before being added, so that each output component is a weighted sum of all input components: p~ = W1 ~u + W2~v . Baroni and Zamparelli (2010) and Coecke et al. (2010), taking inspiration from formal semantics, characterize composition as functio"
W13-3206,D10-1115,1,0.589505,"entations are crossed with different composition methods. However, two out of three composition methods they evaluate are parameter-free, so that they can side-step the issue of fixing the parameter estimation method. In this work, we evaluate all composition methods we know of, excluding a few that lag beIn recent years, there has been widespread interest in compositional distributional semantic models (cDSMs), that derive meaning representations for phrases from their parts. We present an evaluation of alternative cDSMs under truly comparable conditions. In particular, we extend the idea of Baroni and Zamparelli (2010) and Guevara (2010) to use corpus-extracted examples of the target phrases for parameter estimation to the other models proposed in the literature, so that all models can be tested under the same training conditions. The linguistically motivated functional model of Baroni and Zamparelli (2010) and Coecke et al. (2010) emerges as the winner in all our tests. 1 Introduction The need to assess similarity in meaning is central to many language technology applications, and distributional methods are the most robust approach to the task. These methods measure word similarity based on patterns of occ"
W13-3206,P08-1028,0,0.74383,"tors recording their patterns of co-occurrence with corpus contexts (e.g., other words). There is an extensive literature on how to develop such models and on their evaluation (see, e.g., Clark (2012), Erk (2012), Turney and Pantel (2010)). We focus here on compositional DSMs (cDSMs). After discussing some options pertaining to the input vectors, we review all the composition operations we are aware of (excluding only the tensor-product-based models shown by Mitchell and Lapata (2010) to be much worse than simpler models),2 and then methods to estimate their parameters. Composition operations Mitchell and Lapata (2008; 2010) present a set of simple but effective models in which each component of the output vector is a function of the corresponding components of the inputs. Given input vectors ~u and ~v , the weighted additive model (Add) returns their weighted sum: p~ = w1 ~u + w2~v . In the dilation model (Dil), the output vector is obtained by decomposing one of the input vectors, say ~v , into a vector parallel to ~u and an orthogonal vector, and then dilating only the parallel vector by a factor λ before re-combining (formula in Table 1). Mitchell and Lapata also propose a simple multiplicative model i"
W13-3206,P13-2010,1,0.784693,"ank of the correct paraphrase in this list. The lower the rank, the better the model. We report median rank across the test items. [W1∗ , W2∗ ] = ((X T X)−1 X T Y )T   [Au1 v~1 , ..., Auk v~k ] T X= [Av1 u~1 , ..., Avk u~k ] Y = atanh(P T ) Fixing everything but Au for some element u, the objective function becomes: ||atanh(Pu )−W1 Au Vu −W2 [Av1 ~u, ..., Avk0 ~u]||F where v1 ...vk0 ∈ V are the elements occurring with u in the training data and Vu the matrix resulting from their concatenation. The update formula for the Au matrices becomes: Determiner phrases The last dataset, introduced in Bernardi et al. (2013), focuses on a class of grammatical terms (rather than content A∗u = W1−1 ((X T X)−1 X T Y )T X= Evaluation setup and implementation 6 Turney used a corpus of about 50 billion words, almost 20 times larger than ours, and we have very poor or no coverage of many original items, making the “multiple-choice” evaluation proposed by Turney meaningless in our case. VuT Y = (atanh(Pu ) − W2 [Av1 ~u, ..., Avk0 ~u])T 54 W1∗ , W2∗ , A∗u1 , ..., A∗v1 , ...   [Au1 v~1 , ..., Auk v~k ] = arg min ||atanh(P ) − [W1 , W2 ] ||F [Av1 u~1 , ..., Avk u~k ] Rm×m T = arg min ||atanh(P T ) − W1 [Au1 v~1 , ..., Auk"
W13-3206,D12-1050,0,0.474039,"fferent kinds of input representations (vectors recording co-occurrence with words vs. distributions over latent topics) and use supervised training via a grid search over parameter settings to estimate their models. Guevara (2010), to give just one further example, is not only proposing a different composition method with respect to Mitchell and Lapata, but he is also adopting different input vectors (word cooccurrences compressed via SVD) and an unsupervised estimation method based on minimizing the distance of composed vectors to their equivalents directly extracted from the source corpus. Blacoe and Lapata (2012) have recently highlighted the importance of teasing apart the different aspects of a composition framework, presenting an evaluation in which different input vector representations are crossed with different composition methods. However, two out of three composition methods they evaluate are parameter-free, so that they can side-step the issue of fixing the parameter estimation method. In this work, we evaluate all composition methods we know of, excluding a few that lag beIn recent years, there has been widespread interest in compositional distributional semantic models (cDSMs), that derive"
W13-3206,D12-1110,0,0.783417,"er a range of parameter settings for the input vector representations, to insure that our results are not too brittle or parameter-dependent.1 2 Composition function w1 ~ u + w2~v ~ uw1 ~v w2 ||~ u||22~v + (λ − 1)h~ u, ~v i~ u W1 ~ u + W2~v Au~v h i Au ~ v tanh([W1 , W2 ] A ) ~ vu Parameters w1 , w2 w1 , w2 λ W1 , W2 ∈ Rm×m Au ∈ Rm×m W1 , W2 , Au , Av ∈ Rm×m Table 1: Composition functions of inputs (u, v). mensions record the syntactic link between targets and collocates (Erk and Pad´o, 2008; Thater et al., 2010), and most recently vectors based on neural language models (Socher et al., 2011; Socher et al., 2012). Blacoe and Lapata (2012) compared the three representations on phrase similarity and paraphrase detection, concluding that “simple is best”, that is, the bag-of-words approach performs at least as good or better than either syntax-based or neural representations across the board. Here, we take their message home and we focus on bagof-words representations, exploring the impact of various parameters within this approach. Most frameworks assume that word vectors constitute rigid inputs fixed before composition, often using a separate word-similarity task independent of composition. The only ex"
W13-3206,P10-1097,0,0.115356,"Missing"
W13-3206,D08-1094,0,0.313831,"Missing"
W13-3206,C10-1142,0,0.551025,"kit/ 2 Erk and Pad´o (2008) and Thater et al. (2010) use input vectors that have been adapted to their phrasal contexts, but then apply straightforward composition operations such as addition and multiplication to these contextualized vectors. Their approaches are thus not alternative cDSMs, but special ways to construct the input vectors. Grefenstette and Sadrzadeh (2011a; 2011b) and Kartsaklis et al. (2012) propose estimation techniques for the tensors in the functional model of Coecke et al. (2010). Turney (2012) does not compose representations but similarity scores. 51 Guevara (2010) and Zanzotto et al. (2010) explore a full form of the additive model (Fulladd), where the two vectors entering a composition process are pre-multiplied by weight matrices before being added, so that each output component is a weighted sum of all input components: p~ = W1 ~u + W2~v . Baroni and Zamparelli (2010) and Coecke et al. (2010), taking inspiration from formal semantics, characterize composition as function application. For example, Baroni and Zamparelli model adjective-noun phrases by treating the adjective as a function from nouns onto (modified) nouns. Given that linear functions can be expressed by matrices"
W13-3206,D11-1129,0,0.0852131,"ity with SVD or other techniques (Mitchell and Lapata (2010) and many others), vectors whose di1 We made the software we used to construct semantic models and estimate and test composition methods available online at http://clic.cimec.unitn.it/ composes/toolkit/ 2 Erk and Pad´o (2008) and Thater et al. (2010) use input vectors that have been adapted to their phrasal contexts, but then apply straightforward composition operations such as addition and multiplication to these contextualized vectors. Their approaches are thus not alternative cDSMs, but special ways to construct the input vectors. Grefenstette and Sadrzadeh (2011a; 2011b) and Kartsaklis et al. (2012) propose estimation techniques for the tensors in the functional model of Coecke et al. (2010). Turney (2012) does not compose representations but similarity scores. 51 Guevara (2010) and Zanzotto et al. (2010) explore a full form of the additive model (Fulladd), where the two vectors entering a composition process are pre-multiplied by weight matrices before being added, so that each output component is a weighted sum of all input components: p~ = W1 ~u + W2~v . Baroni and Zamparelli (2010) and Coecke et al. (2010), taking inspiration from formal semantic"
W13-3206,W11-2507,0,0.110745,"ity with SVD or other techniques (Mitchell and Lapata (2010) and many others), vectors whose di1 We made the software we used to construct semantic models and estimate and test composition methods available online at http://clic.cimec.unitn.it/ composes/toolkit/ 2 Erk and Pad´o (2008) and Thater et al. (2010) use input vectors that have been adapted to their phrasal contexts, but then apply straightforward composition operations such as addition and multiplication to these contextualized vectors. Their approaches are thus not alternative cDSMs, but special ways to construct the input vectors. Grefenstette and Sadrzadeh (2011a; 2011b) and Kartsaklis et al. (2012) propose estimation techniques for the tensors in the functional model of Coecke et al. (2010). Turney (2012) does not compose representations but similarity scores. 51 Guevara (2010) and Zanzotto et al. (2010) explore a full form of the additive model (Fulladd), where the two vectors entering a composition process are pre-multiplied by weight matrices before being added, so that each output component is a weighted sum of all input components: p~ = W1 ~u + W2~v . Baroni and Zamparelli (2010) and Coecke et al. (2010), taking inspiration from formal semantic"
W15-2813,P14-2135,0,0.0313736,"igure 1 for the answer). Besides the inherent coolness of the task, it has many potential applications. Current qualitative analysis of distributed semantic models is limited to assessing the relation between words, e.g., by looking at, or plotting, nearest neighbour sets, but it lacks methods to inspect the properIn cognitive science, there is a lively debate on whether abstract words have embodied representations, (Barsalou and Wiemer-Hastings, 2005; Lakoff and Johnson, 1999), an issue that has recently attracted the attention of the distributed semantics community (Hill and Korhonen, 2014; Kiela et al., 2014; Lazaridou et al., 2015). An intriguing application of image synthesis would be to produce and assess imagery for abstract concepts. Recent work in neuroscience attempts to generate images of “what people think”, as encoded in vector-based representations of fMRI patterns (Naselaris et al., 2009; Nishimoto et al., 2011). With our method, we could then directly compare images produced from corpus-based representations to what humans visualize when thinking of the same words. 81 Proceedings of the 2015 Workshop on Vision and Language (VL’15), pages 81–86, c Lisbon, Portugal, 18 September 2015."
W15-2813,N13-1016,0,0.0305393,"capturing much faster than from textual neighbour lists. For example, a more “topical” model might produce pictures depicting the wider scenes in which objects occur (a ball being dribbled by soccer players), whereas a model capturing strictly conceptual aspects might produce narrow views of the denoted objects (a close-up of the ball). Image synthesis could also be used to explore the effect of different input corpora on representations: e.g., given a historical corpus, generate images for the car word representations induced from early 20th-century vs. 21st-century texts. As a last example, Aletras and Stevenson (2013) proposed to examine the topics of Topic Models by associating them with images retrieved from the Web. Given that topics are represented by vectors, we could directly generate images representing these topics. We introduce the task of visualizing distributed semantic representations by generating images from word vectors. Given the corpus-based vector encoding the word broccoli, we convert it to a visual representation by means of a cross-modal mapping function, and then use the mapped representation to generate an image of broccoli as “dreamed” by the distributed model. We propose a baseline"
W15-2813,J99-4009,0,0.0671009,"ord looks like, according to the model. Given, say, the word2vec vector of broccoli, we want to know how broccoli looks like to word2vec (see Figure 1 for the answer). Besides the inherent coolness of the task, it has many potential applications. Current qualitative analysis of distributed semantic models is limited to assessing the relation between words, e.g., by looking at, or plotting, nearest neighbour sets, but it lacks methods to inspect the properIn cognitive science, there is a lively debate on whether abstract words have embodied representations, (Barsalou and Wiemer-Hastings, 2005; Lakoff and Johnson, 1999), an issue that has recently attracted the attention of the distributed semantics community (Hill and Korhonen, 2014; Kiela et al., 2014; Lazaridou et al., 2015). An intriguing application of image synthesis would be to produce and assess imagery for abstract concepts. Recent work in neuroscience attempts to generate images of “what people think”, as encoded in vector-based representations of fMRI patterns (Naselaris et al., 2009; Nishimoto et al., 2011). With our method, we could then directly compare images produced from corpus-based representations to what humans visualize when thinking of"
W15-2813,P14-1023,1,0.672833,"ges extracted from ImageNet (Deng et al., 2009) representing 5.1K distinct seen words. The dreamed word set includes 510 concrete, base-level concepts from the semantic norms of McRae et al. (2005) (we excluded 31 McRae concepts because they were marked as ambiguous there, or for technical reasons). Linguistic and Visual Representations For all seen and dreamed concepts, we build 300dimensional word vectors with the word2vec toolkit,1 choosing the CBOW method.2 CBOW, which learns to predict a target word from the ones surrounding it, produces state-of-the-art results in many linguistic tasks (Baroni et al., 2014). Word vectors are induced from a corpus of 2.8 billion words.3 The 500K images are represented by 4096-dimensional visual vectors, extracted with the pre-trained convolutional neural network model of Krizhevsky et al. (2012) through the Caffe toolkit (Jia et al., 2014). The main aim of this paper is to present proofof-concept evidence that the task is feasible. To this end, we rely on state-of-the-art word representation and cross-modality mapping methods, but we adopt an image synthesis strategy that could be seen as an interesting baseline to compare other approaches against. Briefly, our p"
W15-2813,P14-1132,1,0.919895,"nging. However, various relevant strands of research have reached a level of maturity that makes it a realistic goal to pursue. First, tools such as word2vec (Mikolov et al., 2013a) and Glove (Pennington et al., 2014) produce high-quality word representations, making us confident that we are not trying to generate visual signals from semantic noise. Second, there is very promising recent work on learning to map between word representations and an (abstract) image space, for applications such as image retrieval and annotation (Frome et al., 2013; Karpathy and Fei-Fei, 2015; Kiros et al., 2014; Lazaridou et al., 2014; Socher et al., 2014). Finally, the computer vision community is starting to explore the task of image generation (Gregor et al., 2015), typically in an attempt to understand the inner workings of visual feature extraction algorithms (Zeiler and Fergus, 2014). 2 General setup We refer to the words we generate images for as dreamed words, and to the corresponding images as dreams. We refer to the set of words that are associated to real pictures as seen words. The real picture set contains approximately 500K images extracted from ImageNet (Deng et al., 2009) representing 5.1K distinct seen wor"
W15-2813,N15-1016,1,0.656575,"er). Besides the inherent coolness of the task, it has many potential applications. Current qualitative analysis of distributed semantic models is limited to assessing the relation between words, e.g., by looking at, or plotting, nearest neighbour sets, but it lacks methods to inspect the properIn cognitive science, there is a lively debate on whether abstract words have embodied representations, (Barsalou and Wiemer-Hastings, 2005; Lakoff and Johnson, 1999), an issue that has recently attracted the attention of the distributed semantics community (Hill and Korhonen, 2014; Kiela et al., 2014; Lazaridou et al., 2015). An intriguing application of image synthesis would be to produce and assess imagery for abstract concepts. Recent work in neuroscience attempts to generate images of “what people think”, as encoded in vector-based representations of fMRI patterns (Naselaris et al., 2009; Nishimoto et al., 2011). With our method, we could then directly compare images produced from corpus-based representations to what humans visualize when thinking of the same words. 81 Proceedings of the 2015 Workshop on Vision and Language (VL’15), pages 81–86, c Lisbon, Portugal, 18 September 2015. 2015 Association for Comp"
W15-2813,N13-1090,0,0.220487,"d like to move beyond words, towards generating images depicting the meaning of phrases (e.g., an angry cat vs. a cute cat vs. a white cat) and sentences. This would nicely complement current work on generating verbal descriptions of images (Karpathy and FeiFei, 2015; Kiros et al., 2014) with the inverse task of generating images from verbal descriptions. Generating images from vectorial word representations is of course extremely challenging. However, various relevant strands of research have reached a level of maturity that makes it a realistic goal to pursue. First, tools such as word2vec (Mikolov et al., 2013a) and Glove (Pennington et al., 2014) produce high-quality word representations, making us confident that we are not trying to generate visual signals from semantic noise. Second, there is very promising recent work on learning to map between word representations and an (abstract) image space, for applications such as image retrieval and annotation (Frome et al., 2013; Karpathy and Fei-Fei, 2015; Kiros et al., 2014; Lazaridou et al., 2014; Socher et al., 2014). Finally, the computer vision community is starting to explore the task of image generation (Gregor et al., 2015), typically in an att"
W15-2813,D14-1162,0,0.0750838,"ds generating images depicting the meaning of phrases (e.g., an angry cat vs. a cute cat vs. a white cat) and sentences. This would nicely complement current work on generating verbal descriptions of images (Karpathy and FeiFei, 2015; Kiros et al., 2014) with the inverse task of generating images from verbal descriptions. Generating images from vectorial word representations is of course extremely challenging. However, various relevant strands of research have reached a level of maturity that makes it a realistic goal to pursue. First, tools such as word2vec (Mikolov et al., 2013a) and Glove (Pennington et al., 2014) produce high-quality word representations, making us confident that we are not trying to generate visual signals from semantic noise. Second, there is very promising recent work on learning to map between word representations and an (abstract) image space, for applications such as image retrieval and annotation (Frome et al., 2013; Karpathy and Fei-Fei, 2015; Kiros et al., 2014; Lazaridou et al., 2014; Socher et al., 2014). Finally, the computer vision community is starting to explore the task of image generation (Gregor et al., 2015), typically in an attempt to understand the inner workings"
W15-2813,Q14-1017,0,0.0349871,"relevant strands of research have reached a level of maturity that makes it a realistic goal to pursue. First, tools such as word2vec (Mikolov et al., 2013a) and Glove (Pennington et al., 2014) produce high-quality word representations, making us confident that we are not trying to generate visual signals from semantic noise. Second, there is very promising recent work on learning to map between word representations and an (abstract) image space, for applications such as image retrieval and annotation (Frome et al., 2013; Karpathy and Fei-Fei, 2015; Kiros et al., 2014; Lazaridou et al., 2014; Socher et al., 2014). Finally, the computer vision community is starting to explore the task of image generation (Gregor et al., 2015), typically in an attempt to understand the inner workings of visual feature extraction algorithms (Zeiler and Fergus, 2014). 2 General setup We refer to the words we generate images for as dreamed words, and to the corresponding images as dreams. We refer to the set of words that are associated to real pictures as seen words. The real picture set contains approximately 500K images extracted from ImageNet (Deng et al., 2009) representing 5.1K distinct seen words. The dreamed word s"
W15-2813,D14-1032,0,\N,Missing
W17-6904,P14-1023,1,0.760312,"Missing"
W17-6904,J10-4006,1,0.56304,"ion. Dataset. We have constructed a dataset for the task containing 40k sequences for training, 5k for validation and 10k for testing.1 It is assembled on the basis of 2k object categories with 50 ImageNet2 images each, sampled from a larger dataset (Lazaridou et al., 2015). These are natural images, which makes the task challenging. The object categories given in the queries are those specified in ImageNet. We build a set of linguistic attributes for each object by first extracting the 500 most associated, and thus plausible, syntactic neighbors for the category according to the DM resource (Baroni and Lenci, 2010). This excludes nonsensical combinations such as repair:dog. We further retain only (relatively) abstract verbs taking the target item as direct object.3 This is because (a) concrete verbs are likely to have strong visual correlates that could conflict with the image (cf. walk dog); and (b) referential expressions routinely successfully mix concrete and abstract cues (e.g., the dog I own). We remove all verbs with a score over 2.5 (on a 1–5 scale) in the concreteness norms of Brysbaert et al. (2014). We then construct each sequence as follows. First, we sample two random categories, and three"
W17-6904,J16-4002,1,0.840649,"that are already in the library. Our model shows promise: it beats traditional neural network architectures on the task. However, it is still outperformed by Memory Networks, another model with external memory. 1 Introduction Language combines discrete and continuous facets, as exemplified by the phenomenon of reference (Frege, 1892; Abbott, 2010): When we refer to an object in the world with the noun phrase the mug I bought, we use content words such as mug, which are notoriously fuzzy or vague in their meaning (Van Deemter, 2012; Murphy, 2002) and are best modeled through continuous means (Boleda and Herbelot, 2016). Once the referent for the mug has been established, however, it becomes a linguistic entity that we can manipulate in a largely discrete fashion, retrieving it and updating it with new information as needed (Remember the mug I bought? My brother stole it! Kamp and Reyle, 1993). Put differently, managing reference requires two distinct abilities: 1. The ability to categorize, that is, to recognize that different entities are equivalent with regard to some concept of interest (e.g. two mugs, two instances of the “things to take on a camping trip” category; Barsalou, 1983). This implies being a"
W17-6904,W08-2222,0,0.0549761,"wo referents), or to treat it as a new referent. Our second contribution is a neural network architecture with a module for referent representations: DIstributed model of REference, DIRE. DIRE uses the concept of external memory from deep learning (Joulin and Mikolov, 2015; Graves et al., 2016) to build an entity library for an exposure sequence that conceptually corresponds to the set of DRT discourse referents, using similarity-based reasoning on distributed representations to decide between aggregating and initializing entity representations. In contrast to symbolic implementations of DRT (Bos, 2008), which manipulate discourse referents on the basis of manually specified algorithms, DIRE learns to make these decisions directly from observing reference acts using end-to-end training. We see our paper as a first, modest step in the direction of data-driven learning of DRT-like behavior, and are of course still far from learning anything resembling a fully fledged DRT system. 2 Cross-modal Entity Tracking: Task and Data Task. Imagine an office, with a desk where there are three mugs and other objects. Adam tells Barbara that he just bought two of the mugs and he particularly likes the one o"
W17-6904,P12-1015,1,0.803766,"lities of computational models, and a challenging dataset for the task. Our project is related to several areas of active research. Reference is a classic topic in philosophy of language and linguistics (Frege, 1892; Abbott, 2010; Kamp and Reyle, 1993; Kamp, 2015); emulating discrete aspects of language and reasoning through continuous means is a long-standing goal in artificial intelligence (Smolensky, 1990; Joulin and Mikolov, 2015), and recent work focuses on reference (Baroni et al., 2017; Herbelot, 2015; Herbelot and Vecchi, 2015); grounding language in perception (Chen and Mooney, 2011; Bruni et al., 2012; Silberer et al., 2013), as well as reference and co-reference (Krahmer and Van Deemter, 2012; Poesio et al., 2017) are important subjects in Computational Linguistics. Our programme puts these different strands together. Acknowledgments: We thank Angeliki Lazaridou for help producing the visual vectors used in the paper. This project has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No 715154; AMORE); EU Horizon 2020 programme under the Marie Skłodowska-Curie grant agreement No 655577 (LOVe"
W17-6904,W15-0120,0,0.0134861,"ented a new task, cross-modal entity tracking, that tests the categorization and individuation capabilities of computational models, and a challenging dataset for the task. Our project is related to several areas of active research. Reference is a classic topic in philosophy of language and linguistics (Frege, 1892; Abbott, 2010; Kamp and Reyle, 1993; Kamp, 2015); emulating discrete aspects of language and reasoning through continuous means is a long-standing goal in artificial intelligence (Smolensky, 1990; Joulin and Mikolov, 2015), and recent work focuses on reference (Baroni et al., 2017; Herbelot, 2015; Herbelot and Vecchi, 2015); grounding language in perception (Chen and Mooney, 2011; Bruni et al., 2012; Silberer et al., 2013), as well as reference and co-reference (Krahmer and Van Deemter, 2012; Poesio et al., 2017) are important subjects in Computational Linguistics. Our programme puts these different strands together. Acknowledgments: We thank Angeliki Lazaridou for help producing the visual vectors used in the paper. This project has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No 7"
W17-6904,D15-1003,0,0.0188569,", cross-modal entity tracking, that tests the categorization and individuation capabilities of computational models, and a challenging dataset for the task. Our project is related to several areas of active research. Reference is a classic topic in philosophy of language and linguistics (Frege, 1892; Abbott, 2010; Kamp and Reyle, 1993; Kamp, 2015); emulating discrete aspects of language and reasoning through continuous means is a long-standing goal in artificial intelligence (Smolensky, 1990; Joulin and Mikolov, 2015), and recent work focuses on reference (Baroni et al., 2017; Herbelot, 2015; Herbelot and Vecchi, 2015); grounding language in perception (Chen and Mooney, 2011; Bruni et al., 2012; Silberer et al., 2013), as well as reference and co-reference (Krahmer and Van Deemter, 2012; Poesio et al., 2017) are important subjects in Computational Linguistics. Our programme puts these different strands together. Acknowledgments: We thank Angeliki Lazaridou for help producing the visual vectors used in the paper. This project has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No 715154; AMORE); EU Horizon 20"
W17-6904,J12-1006,0,0.0559812,"Missing"
W17-6904,N15-1016,1,0.845774,"ecent survey), but focuses on identifying language-external objects from images rather than mentions of a referent in text; to Visual Question Answering (Antol et al., 2015), but it cannot be solved with visual information alone; and to Referring Expression Generation (Krahmer and Van Deemter, 2012), but involves identification rather than generation. Dataset. We have constructed a dataset for the task containing 40k sequences for training, 5k for validation and 10k for testing.1 It is assembled on the basis of 2k object categories with 50 ImageNet2 images each, sampled from a larger dataset (Lazaridou et al., 2015). These are natural images, which makes the task challenging. The object categories given in the queries are those specified in ImageNet. We build a set of linguistic attributes for each object by first extracting the 500 most associated, and thus plausible, syntactic neighbors for the category according to the DM resource (Baroni and Lenci, 2010). This excludes nonsensical combinations such as repair:dog. We further retain only (relatively) abstract verbs taking the target item as direct object.3 This is because (a) concrete verbs are likely to have strong visual correlates that could conflic"
W17-6904,P13-1056,0,0.0187395,"nal models, and a challenging dataset for the task. Our project is related to several areas of active research. Reference is a classic topic in philosophy of language and linguistics (Frege, 1892; Abbott, 2010; Kamp and Reyle, 1993; Kamp, 2015); emulating discrete aspects of language and reasoning through continuous means is a long-standing goal in artificial intelligence (Smolensky, 1990; Joulin and Mikolov, 2015), and recent work focuses on reference (Baroni et al., 2017; Herbelot, 2015; Herbelot and Vecchi, 2015); grounding language in perception (Chen and Mooney, 2011; Bruni et al., 2012; Silberer et al., 2013), as well as reference and co-reference (Krahmer and Van Deemter, 2012; Poesio et al., 2017) are important subjects in Computational Linguistics. Our programme puts these different strands together. Acknowledgments: We thank Angeliki Lazaridou for help producing the visual vectors used in the paper. This project has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No 715154; AMORE); EU Horizon 2020 programme under the Marie Skłodowska-Curie grant agreement No 655577 (LOVe); ERC 2011 Starting Ind"
W18-5413,D17-1030,1,0.767884,"RNNs fail the following basic compositionality test: Even after they acquired the meaning of “to run again” and “to dax”, they do not understand “to dax again” on first encounter. As Lake and Baroni show, the generalization problem is linked to the fact that RNNs fail to learn a representation (an embedding) for the new verb (“to dax”) that is similar to those of known verbs (“to run”, “to look”), and consequently it cannot rely on similarity information to correctly generalize verb usage. This is arguably more of an instance of the problem of quickly learning meaningful new-word embeddings (Herbelot and Baroni, 2017; Lampinen and McClelland, 2017), than strictly a failure of compositionality. In this paper, we repurpose SCAN to test another kind of compositionality, namely one that requires combining highly familiar words in new ways to create novel meaning. As illustrated above, this is what we do when we combine a functional term such as “slowly” with the verb “to run” to obtain the phrase “to run slowly”. Or, in terms of the SCAN commands that we test here, this is what is required to understand an expression such as “jump around right” when the meanings of “jump”, “right” and “around” are known. Our"
W18-5413,W14-4012,0,0.0143641,"Missing"
