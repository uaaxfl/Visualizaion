2020.lrec-1.606,E17-2092,0,0.0227357,"ould a supporter of X feel after reading this tweet?) The possible stance labels were: • positiv (positive) (1) • weder positiv noch negativ (neither positive nor negative) (0) • negativ (negative) (-1) • nicht lesbar / trifft nicht zu (cannot read / does not apply) (x) Annotators were given the tweet, its location, and the profile picture, name and description of the user who posted it. If a tweet was a response to another tweet, that tweet was shown also. Annotators were instructed to use this context to label more ambiguous tweets. Our prompt is inspired by the reader-perspective prompt in Buechel and Hahn (2017). The prompt is designed to capture subtle stance cues that the writer may not have included consciously. To compensate for the lower reliability of reader-perspective prompts (Buechel and Hahn, 2017), we instructed annotators to imagine the perspective of a proponent of the target, as in Card et al. (2015). After annotation we obtained final labels with MultiAnnotator Competence Estimation (MACE) (Hovy et al., 2013). MACE can be used to remove the least reliable annotators and to obtain a reliable majority vote even in quite unfavourable circumstances.7 Out of 28 original annotators, we remov"
2020.lrec-1.606,P15-2072,0,0.348457,"“to select some aspects of a perceived reality and make them more salient in a communication text, in such a way as to promote a particular problem definition, causal interpretation, moral evaluation, and/or treatment recommendation” towards that topic (Entman, 1993, p. 52). For example, a text may discuss the topic of immigration primarily in an economic frame that focuses on the need for workers, or in a cultural frame that focuses on issues of diversity and integration. Most work on framing in computational linguistics has focused on the framing of issues and events, rather than entities (Card et al., 2015; Fulgoni et al., 2016; Field et al., 2018). In previous work (van den Berg et al., 2019), we introduced entity framing as the presentation of an entity that (intentionally or not) promotes a particular viewpoint towards that entity. Our goal is to understand how bias and stance are expressed in computer-mediated discourse about political topics, in light of rising concern that discussions about politics on social media are less civil and objective than discussions on traditional platforms (Persily, 2017; Ott, 2017; Dang-Xuan et al., 2013). One area in which entities can be framed more or less"
2020.lrec-1.606,D16-1148,0,0.368455,"Missing"
2020.lrec-1.606,D19-1664,0,0.179967,"588/data/ AOSUY6. In compliance with Twitter usage guidelines, we provide tweet ids rather than full tweets with their texts. 2 Published along with the main GTTC. 4924 ing politicians with a doctoral degree • evidence for the status-indicating function of naming in a language other than English and for an academic degree instead of a professional title • evidence that the status-indicating function of naming and titling is weaker in left-leaning than in rightleaning discourse 2. Related Work The framing of entities is a fairly new topic covered in only a handful of papers (Card et al., 2016; Fan et al., 2019). The only currently existing dataset for entity framing is the BASIL dataset (Fan et al., 2019), which annotates framing segments and their polarity towards political entities in news articles. BASIL is not suitable for studying the impact of naming and titling on entity framing, as journalistic style guides prescribe certain naming conventions to ensure objectivity both in English (Siegal and Connolly, 1999) and in German (Raue, 2012). We therefore work with tweets. More common than datasets for entity framing are datasets annotated for explicit stance. These are typically tagged for stance"
2020.lrec-1.606,D18-1393,0,0.138455,"eality and make them more salient in a communication text, in such a way as to promote a particular problem definition, causal interpretation, moral evaluation, and/or treatment recommendation” towards that topic (Entman, 1993, p. 52). For example, a text may discuss the topic of immigration primarily in an economic frame that focuses on the need for workers, or in a cultural frame that focuses on issues of diversity and integration. Most work on framing in computational linguistics has focused on the framing of issues and events, rather than entities (Card et al., 2015; Fulgoni et al., 2016; Field et al., 2018). In previous work (van den Berg et al., 2019), we introduced entity framing as the presentation of an entity that (intentionally or not) promotes a particular viewpoint towards that entity. Our goal is to understand how bias and stance are expressed in computer-mediated discourse about political topics, in light of rising concern that discussions about politics on social media are less civil and objective than discussions on traditional platforms (Persily, 2017; Ott, 2017; Dang-Xuan et al., 2013). One area in which entities can be framed more or less positively is in the use of names and titl"
2020.lrec-1.606,L16-1591,0,0.289025,"pects of a perceived reality and make them more salient in a communication text, in such a way as to promote a particular problem definition, causal interpretation, moral evaluation, and/or treatment recommendation” towards that topic (Entman, 1993, p. 52). For example, a text may discuss the topic of immigration primarily in an economic frame that focuses on the need for workers, or in a cultural frame that focuses on issues of diversity and integration. Most work on framing in computational linguistics has focused on the framing of issues and events, rather than entities (Card et al., 2015; Fulgoni et al., 2016; Field et al., 2018). In previous work (van den Berg et al., 2019), we introduced entity framing as the presentation of an entity that (intentionally or not) promotes a particular viewpoint towards that entity. Our goal is to understand how bias and stance are expressed in computer-mediated discourse about political topics, in light of rising concern that discussions about politics on social media are less civil and objective than discussions on traditional platforms (Persily, 2017; Ott, 2017; Dang-Xuan et al., 2013). One area in which entities can be framed more or less positively is in the"
2020.lrec-1.606,N13-1132,0,0.027577,"er tweet, that tweet was shown also. Annotators were instructed to use this context to label more ambiguous tweets. Our prompt is inspired by the reader-perspective prompt in Buechel and Hahn (2017). The prompt is designed to capture subtle stance cues that the writer may not have included consciously. To compensate for the lower reliability of reader-perspective prompts (Buechel and Hahn, 2017), we instructed annotators to imagine the perspective of a proponent of the target, as in Card et al. (2015). After annotation we obtained final labels with MultiAnnotator Competence Estimation (MACE) (Hovy et al., 2013). MACE can be used to remove the least reliable annotators and to obtain a reliable majority vote even in quite unfavourable circumstances.7 Out of 28 original annotators, we removed 3 annotators for being unreliable as judged by MACE. To measure agreement, we used Krippendorff’s alpha (Krippendorff, 2018), which is suitable for multi-coders ordinal annotation (Antoine et al., 2014). The 25 competent annotators had an agreement of 0.62. This is a bit higher than the 0.58 alpha value for stance annotation in van den Berg et al. (2019) (also 3 classes), and than the alpha value of 0.57 obtained"
2020.lrec-1.606,P11-1016,0,0.0327561,"n et al., 2019), which annotates framing segments and their polarity towards political entities in news articles. BASIL is not suitable for studying the impact of naming and titling on entity framing, as journalistic style guides prescribe certain naming conventions to ensure objectivity both in English (Siegal and Connolly, 1999) and in German (Raue, 2012). We therefore work with tweets. More common than datasets for entity framing are datasets annotated for explicit stance. These are typically tagged for stance towards products and companies (Somasundaran and Wiebe, 2010; Meng et al., 2012; Jiang et al., 2011; Mohammad et al., 2016) where titling might play a lesser role. Datasets which do cover person entities typically include them as a subset among other target types such as companies, institutions and topics (Rosenthal et al., 2017; Amig´o et al., 2012; Amig´o et al., 2013; Amig´o et al., 2014), with the exception of Taddy (2013) which only has person entity targets. On these datasets, no studies were conducted on the use of names and titles. In previous work, we presented a dataset to examine the use of names and titles in English tweets mentioning presidents (van den Berg et al., 2019). We h"
2020.lrec-1.606,S16-1003,0,0.0303821,"ch annotates framing segments and their polarity towards political entities in news articles. BASIL is not suitable for studying the impact of naming and titling on entity framing, as journalistic style guides prescribe certain naming conventions to ensure objectivity both in English (Siegal and Connolly, 1999) and in German (Raue, 2012). We therefore work with tweets. More common than datasets for entity framing are datasets annotated for explicit stance. These are typically tagged for stance towards products and companies (Somasundaran and Wiebe, 2010; Meng et al., 2012; Jiang et al., 2011; Mohammad et al., 2016) where titling might play a lesser role. Datasets which do cover person entities typically include them as a subset among other target types such as companies, institutions and topics (Rosenthal et al., 2017; Amig´o et al., 2012; Amig´o et al., 2013; Amig´o et al., 2014), with the exception of Taddy (2013) which only has person entity targets. On these datasets, no studies were conducted on the use of names and titles. In previous work, we presented a dataset to examine the use of names and titles in English tweets mentioning presidents (van den Berg et al., 2019). We hypothesised that the rel"
2020.lrec-1.606,S17-2088,0,0.02895,"es prescribe certain naming conventions to ensure objectivity both in English (Siegal and Connolly, 1999) and in German (Raue, 2012). We therefore work with tweets. More common than datasets for entity framing are datasets annotated for explicit stance. These are typically tagged for stance towards products and companies (Somasundaran and Wiebe, 2010; Meng et al., 2012; Jiang et al., 2011; Mohammad et al., 2016) where titling might play a lesser role. Datasets which do cover person entities typically include them as a subset among other target types such as companies, institutions and topics (Rosenthal et al., 2017; Amig´o et al., 2012; Amig´o et al., 2013; Amig´o et al., 2014), with the exception of Taddy (2013) which only has person entity targets. On these datasets, no studies were conducted on the use of names and titles. In previous work, we presented a dataset to examine the use of names and titles in English tweets mentioning presidents (van den Berg et al., 2019). We hypothesised that the relation between naming and stance would depend on which naming function was more dominant: (i) marking of relative status (based on e.g. age or professional role) or (ii) marking of relative solidarity, also r"
2020.lrec-1.606,W10-0214,0,0.0113763,"taset for entity framing is the BASIL dataset (Fan et al., 2019), which annotates framing segments and their polarity towards political entities in news articles. BASIL is not suitable for studying the impact of naming and titling on entity framing, as journalistic style guides prescribe certain naming conventions to ensure objectivity both in English (Siegal and Connolly, 1999) and in German (Raue, 2012). We therefore work with tweets. More common than datasets for entity framing are datasets annotated for explicit stance. These are typically tagged for stance towards products and companies (Somasundaran and Wiebe, 2010; Meng et al., 2012; Jiang et al., 2011; Mohammad et al., 2016) where titling might play a lesser role. Datasets which do cover person entities typically include them as a subset among other target types such as companies, institutions and topics (Rosenthal et al., 2017; Amig´o et al., 2012; Amig´o et al., 2013; Amig´o et al., 2014), with the exception of Taddy (2013) which only has person entity targets. On these datasets, no studies were conducted on the use of names and titles. In previous work, we presented a dataset to examine the use of names and titles in English tweets mentioning presi"
2020.lrec-1.606,W19-2101,1,0.288319,"Missing"
2020.lrec-1.616,P98-1013,0,0.781603,"Missing"
2020.lrec-1.616,D14-1082,0,0.0310392,"old standard training data, several of our features also require a textual corpus to perform pattern recognition and to compare word frequencies. We use Amazon Product Review Data (Jindal and Liu, 2008), a corpus of 5.8 million product reviews, as it has previously been shown to be a good fit for polarity shifter classification (Schulder et al., 2017). To prepare the corpus for use in our features, we lemmatise it and merge particle verbs to be represented as a single token (e. g. tear down). To determine syntactic dependency relations within the corpus, we parse it using the Stanford Parser (Chen and Manning, 2014). 4. Methodology To automatically classify shifting directions, we train an SVM multi-class classifier, using the SVMmulticlass implementation by Tsochantaridis et al. (2005). We train the classifier using features that utilise existing linguistic resources as well as patterns in textual data. In addition we present several baselines. 4.1. Baselines We define two majority classifiers and a word embedding classifier as baselines. All baselines were also tested for their use as classifier features during exploratory experiments, but as each resulted in decreased performance for the best classifi"
2020.lrec-1.616,D14-1125,0,0.0407248,"Missing"
2020.lrec-1.616,W14-2618,0,0.0408975,"Missing"
2020.lrec-1.616,P11-1144,0,0.0629714,"Missing"
2020.lrec-1.616,N10-1138,0,0.0754665,"Missing"
2020.lrec-1.616,P13-2022,0,0.0663663,"Missing"
2020.lrec-1.616,N19-1423,0,0.0130406,"labeled as ‘affects both’, but adjectives receive the label ‘affects positives’. This is a stronger baseline than BASELINEmaj , as it takes into account the label distributions of individual parts of speech as observed in Table 2. Word Embedding: For BASELINEembed we train an SVM classifier on the dimensions of a word embedding.3 Word embeddings are vector spaces that represent semantic similarity based on distributional similarity. As embedding we use the 500-dimensional Word2Vec (Mikolov et al., 2013) embedding of the Amazon Product 3 Classifiers using contextualised embeddings, e. g. BERT (Devlin et al., 2019), present no advantage for our task, as lexical classification involves no context. In addition, the small number of 540 training items precludes the use of most other deep learning classifiers. 5012 Review Data corpus created by Schulder et al. (2017).4 Each dimension is treated as a weighted binary feature. Our expectation is that as shifting directionality is a semantic phenomenon, it may be encoded in specific embedding dimensions. Shifters that share shifting direction would be expected to be close to each other on those dimensions. 4.2. Task-specific Features The following features are d"
2020.lrec-1.616,P16-1191,0,0.0451842,"Missing"
2020.lrec-1.616,N09-1002,0,0.0770587,"Missing"
2020.lrec-1.616,W13-3514,0,0.0679004,"Missing"
2020.lrec-1.616,P14-1145,0,0.0605981,"Missing"
2020.lrec-1.616,W06-0301,0,0.0848014,"Missing"
2020.lrec-1.616,I17-1063,1,0.195101,"arity shifter’ or ‘negative polarity shifter’, where positive and negative refer to the polarity that the shifted expression receives. We found that in practice this terminology could cause confusion as to whether the prior or shifted polarity was being referred to. It is also unclear how to use it in cases where shifting results in a neutral polarity expression (Taboada et al., 2011). We therefore choose to instead mark shifters as ‘affects positive polarity’, ‘affects negative polarity’ or ‘affects both polarities’. While multiple resources exist that identify polarity shifters for English (Schulder et al., 2017; Schulder et al., 2018b; Schulder et al., forthcoming) and German (Schulder et al., 2018a), none of them specify their shifting direction. As 5010 a result, the polarities of sentences such as (8), (10), (12) and (14) would erroneously be assumed to have shifted. To prevent such mistakes, we introduce a supervised classifier for shifting directions that can enhance available shifter lexica. It labels each shifter as exactly one of three types: shifters that only affect positive polarities, only negative ones or shifters that can affect both. Our contributions are the following: 1. We design a"
2020.lrec-1.616,C18-1213,1,0.883501,"Missing"
2020.lrec-1.616,L18-1222,1,0.880408,"Missing"
2020.lrec-1.616,D13-1170,0,0.00637511,"olarity shifters among them. They also observed that some shifters would only affect specific polarities and took this into account. While individual negation words are more frequent than individual polarity shifters, Schulder et al. (2018b) showed that overall shifters are at least as frequent, even when only considering verbal shifters. However, so far research that concerns itself with compositional polarity has mostly focussed on negation words (see the survey by Wiegand et al. (2010)). This is at least in part due to the lack of resources that would help identify polarity shifters. While Socher et al. (2013) showed that negation words can be learned implicitly from labeled data, this fails for polarity shifters due to the relative low frequency of individual shifter words compared to negation words (Schulder et al., 2017). This is a general problem for implicit negation learning, even for current state of the art classifiers (Schulder et al., forthcoming). 1 Dataset Verbs Nouns Adj. Total Gold Standard 304 107 129 540 Bootstrapped 676 793 512 1,981 Total 980 900 641 2,521 Table 1: Number of polarity shifters in the English shifter lexicon (Schulder et al., forthcoming), grouped by part of speech."
2020.lrec-1.616,J11-2001,0,0.0164586,"e]+ ]+ . (13) Let us [amendshifter that [problem]− ]+ . (14) We can [amend the [solution]+ ]+ to improve its clarity. Wilson et al. (2005) specify shifting directions by marking the shifter as ‘general polarity shifter’, ‘positive polarity shifter’ or ‘negative polarity shifter’, where positive and negative refer to the polarity that the shifted expression receives. We found that in practice this terminology could cause confusion as to whether the prior or shifted polarity was being referred to. It is also unclear how to use it in cases where shifting results in a neutral polarity expression (Taboada et al., 2011). We therefore choose to instead mark shifters as ‘affects positive polarity’, ‘affects negative polarity’ or ‘affects both polarities’. While multiple resources exist that identify polarity shifters for English (Schulder et al., 2017; Schulder et al., 2018b; Schulder et al., forthcoming) and German (Schulder et al., 2018a), none of them specify their shifting direction. As 5010 a result, the polarities of sentences such as (8), (10), (12) and (14) would erroneously be assumed to have shifted. To prevent such mistakes, we introduce a supervised classifier for shifting directions that can enhan"
2020.lrec-1.616,W10-3111,1,0.734343,"olarity classifier that takes into account that the lexical polarity of words is affected by a number of contextual phenomena, polarity shifters among them. They also observed that some shifters would only affect specific polarities and took this into account. While individual negation words are more frequent than individual polarity shifters, Schulder et al. (2018b) showed that overall shifters are at least as frequent, even when only considering verbal shifters. However, so far research that concerns itself with compositional polarity has mostly focussed on negation words (see the survey by Wiegand et al. (2010)). This is at least in part due to the lack of resources that would help identify polarity shifters. While Socher et al. (2013) showed that negation words can be learned implicitly from labeled data, this fails for polarity shifters due to the relative low frequency of individual shifter words compared to negation words (Schulder et al., 2017). This is a general problem for implicit negation learning, even for current state of the art classifiers (Schulder et al., forthcoming). 1 Dataset Verbs Nouns Adj. Total Gold Standard 304 107 129 540 Bootstrapped 676 793 512 1,981 Total 980 900 641 2,521"
2020.lrec-1.616,H05-1044,0,0.684915,"[did notnegation [pass the exam]+ ]− . (2) Peter [failedshifter to [pass the exam]+ ]− . + − (3) Peter’s [failureshifter to [pass the exam] ] (4) Peter’s [failedshifter [attempt to pass the exam]+ ]− Many polarity shifters can affect both positive and negative expressions. In (5), the verbal shifter destroy shifts a positive polar expression to negative, while in (6) it shifts from negative to positive. (5) It [destroyedshifter their [hopes]+ ]− . (6) The medication will [destroyshifter the [cancer]− ]+ . Other shifters, however, are unidirectional and only shift polarities in one direction (Wilson et al., 2005). The verbal shifter to risk, for example, shifts only positive polar expressions like good health in (7), while the polarity of negative polar expressions like war in (8) remains unaffected. Similarly, the adjectival shifter antiquated shifts the positive noun ideal in (9), but not the negative noun stereotype in (8). (8) Their actions [risk a [war]− ]− . (9) The “American dream” is an [antiquatedshifter [ideal]+ ]− . (10) Women belonging in the kitchen is an [antiquated [stereotype]− ]− . Conversely there are shifters that only affect negative expressions but not positive ones, such as recou"
2021.eacl-main.27,W19-3501,0,0.0450428,"Missing"
2021.eacl-main.27,N19-1423,0,0.0617662,"Missing"
2021.eacl-main.27,P16-1191,0,0.0178745,"istical significance testing (paired t-test at p < 0.05): ∗ : better than fastTextCommon Crawl ; † : better than BERT Table 5: Comparisons of different classifiers. word. A word may be associated with more than one of the 8 emotion categories. We represent each comparison by the set of emotion categories for the words also occurring in the NRC lexicon. WordNet Supersenses (SUPER). We also consider WordNet supersenses (Miller et al., 1990) in our experiments. They represent a set of 45 coarsegrained semantic categories and have been found effective in related tasks, such as sentiment analysis (Flekova and Gurevych, 2016). A comparison is represented by the set of semantic categories associated with the words contained in the comparison. 5 5.1 Experiments Classification Performance As a supervised classifier, we chose BERT-Large (Devlin et al., 2019). We initially experimented with two versions: one in which we fine-tune the model by adding a layer on top of the pre-trained model and a SVM (Joachims, 1999) that is trained on the BERT embeddings of the final layer. Since we did not measure any statistically significant difference between these models, we decided in favor of SVM due to its simplicity. We carry o"
2021.eacl-main.27,W19-3510,0,0.0120138,"ill be invented in this work is not directed towards specific invididuals or identity groups. Therefore, we believe that this procedure is justifiable. In principle, creating morally disputable content as part of research is not unusual. Both in plagiarism detection (Potthast 2 2 Related Work Datasets in abusive language detection mostly focus on different targets (e.g. Islamophobia (Waseem and Hovy, 2016), antisemitism (Warner and Hirschberg, 2012), misogyny (Anzovino et al., ´ 2018)), different languages (e.g. Spanish (AlvarezCarmona et al., 2018), Arabic (Mubarak et al., 2017), Portuguese (Fortuna et al., 2019)) or different domains (e.g. Twitter (Waseem and Hovy, 2016), Facebook (Kumar et al., 2018), Wikipedia (Wulczyn et al., 2017)). Despite some theoretical work outlining distinct subtypes of abusive language (Waseem et al., 2017), there has been little work on datasets that focus on particular subtypes. Schmidt and Wiegand (2017) present a more detailed overview on abusive language detection. Comparisons, particularly figurative comparisons (similes), have been examined with regard to sentiment. Qadir et al. (2015) investigate automatic polarity classification of comparisons while Other foci are"
2021.eacl-main.27,D14-1215,0,0.0312131,"Missing"
2021.eacl-main.27,P11-1032,0,0.155115,"Missing"
2021.eacl-main.27,E17-2068,0,0.0218414,"e that the boundary between implicit and explicit insults is not clear-cut and that there are ambiguous abusive words contained in our comparisons that still have a strong semantic similarity to abusive words from a lexicon of abusive words. We took the lexicon from Wiegand et al. (2018), computed a centroid embedding vector of its entries and ranked our comparisons according to the semantic similarity to the centroid. A comparison was represented by the embedding vector of the word in that comparison whose similarity was highest to the centroid. As embeddings we chose the fastText emeddings (Joulin et al., 2017) induced on Common Crawl.8 Emotions (EMO). In order to take into account the recently reported correlation between abusive language and emotions (Rajamanickam et al., 2020), we use the NRC lexicon (Mohammad and Turney, 2013) which lists the emotion categories associated to a particular frequent English 8 363 https://commoncrawl.org Classifier Prec majority 25.0 random 50.9 fastTextplain 60.6 fastTextCommon Crawl 68.0 BERTonly pattern 53.7 BERTonly vehicle 67.1 BERT 70.2 linguistic featuresonly auto 65.9 linguistic features 68.9 BERT+linguistic featuresonly auto 72.2 BERT+linguistic features 72"
2021.eacl-main.27,W18-4401,0,0.0382961,"Missing"
2021.eacl-main.27,W17-3008,0,0.0206378,"the type of abusive language that will be invented in this work is not directed towards specific invididuals or identity groups. Therefore, we believe that this procedure is justifiable. In principle, creating morally disputable content as part of research is not unusual. Both in plagiarism detection (Potthast 2 2 Related Work Datasets in abusive language detection mostly focus on different targets (e.g. Islamophobia (Waseem and Hovy, 2016), antisemitism (Warner and Hirschberg, 2012), misogyny (Anzovino et al., ´ 2018)), different languages (e.g. Spanish (AlvarezCarmona et al., 2018), Arabic (Mubarak et al., 2017), Portuguese (Fortuna et al., 2019)) or different domains (e.g. Twitter (Waseem and Hovy, 2016), Facebook (Kumar et al., 2018), Wikipedia (Wulczyn et al., 2017)). Despite some theoretical work outlining distinct subtypes of abusive language (Waseem et al., 2017), there has been little work on datasets that focus on particular subtypes. Schmidt and Wiegand (2017) present a more detailed overview on abusive language detection. Comparisons, particularly figurative comparisons (similes), have been examined with regard to sentiment. Qadir et al. (2015) investigate automatic polarity classification"
2021.eacl-main.27,C10-2115,0,0.127529,"Missing"
2021.eacl-main.27,D15-1019,0,0.165835,"previously examined tasks and that the established datasets for general language abuse are less suitable for this task. (9) Your face is as pale as a sheet. (10) You look like you haven’t slept in days. (11) Talking to you is like walking against a strong wind. Unlike many other datasets for abusive language detection, we create our new dataset with abusive comparisons by inventing instances (i.e. comparisons) rather than by annotating automatically extracted instances. This design choice is necessary since existing datasets contain either insufficient or biased comparisons: The dataset from Qadir et al. (2015) includes about 180 implicitly abusive comparisons, however, we found, next to many nearduplicates, a heavy bias towards very few recurring images (e.g. You behave like a child, You look like a monkey). We observed the same phenomenon when extracting comparisons from Twitter directly. Established datasets for abusive language detection (Founta et al., 2018; Zampieri et al., 2019) contain just about 30-40 abusive comparisons. Our dataset will be created via crowdsourcing. Of course, having abusive comparisons be invented this way will inevitably result in some articificial data. However, we thi"
2021.eacl-main.27,2020.acl-main.394,0,0.0263883,"trong semantic similarity to abusive words from a lexicon of abusive words. We took the lexicon from Wiegand et al. (2018), computed a centroid embedding vector of its entries and ranked our comparisons according to the semantic similarity to the centroid. A comparison was represented by the embedding vector of the word in that comparison whose similarity was highest to the centroid. As embeddings we chose the fastText emeddings (Joulin et al., 2017) induced on Common Crawl.8 Emotions (EMO). In order to take into account the recently reported correlation between abusive language and emotions (Rajamanickam et al., 2020), we use the NRC lexicon (Mohammad and Turney, 2013) which lists the emotion categories associated to a particular frequent English 8 363 https://commoncrawl.org Classifier Prec majority 25.0 random 50.9 fastTextplain 60.6 fastTextCommon Crawl 68.0 BERTonly pattern 53.7 BERTonly vehicle 67.1 BERT 70.2 linguistic featuresonly auto 65.9 linguistic features 68.9 BERT+linguistic featuresonly auto 72.2 BERT+linguistic features 72.9 BERT+linguistic feat. on biased dataset 77.4 human baseline (upper bound) 77.6 Rec 50.0 51.0 53.9 67.5 53.2 66.9 70.0 65.9 68.9 72.1 72.8 77.3 77.5 F1 33.3 51.0 57.1 67."
2021.eacl-main.27,W17-1101,1,0.877623,"etection mostly focus on different targets (e.g. Islamophobia (Waseem and Hovy, 2016), antisemitism (Warner and Hirschberg, 2012), misogyny (Anzovino et al., ´ 2018)), different languages (e.g. Spanish (AlvarezCarmona et al., 2018), Arabic (Mubarak et al., 2017), Portuguese (Fortuna et al., 2019)) or different domains (e.g. Twitter (Waseem and Hovy, 2016), Facebook (Kumar et al., 2018), Wikipedia (Wulczyn et al., 2017)). Despite some theoretical work outlining distinct subtypes of abusive language (Waseem et al., 2017), there has been little work on datasets that focus on particular subtypes. Schmidt and Wiegand (2017) present a more detailed overview on abusive language detection. Comparisons, particularly figurative comparisons (similes), have been examined with regard to sentiment. Qadir et al. (2015) investigate automatic polarity classification of comparisons while Other foci are fairly unlikely to be abusive. 359 3 The supplementary material, which consists of the supplementary notes and the new dataset, is available at: https://github.com/miwieg/implicitly_ abusive_comparisons Component Topic (T) Eventuality (E) Comparator (C) Property (P) Pattern (T+E+C+P) Vehicle Example Sentence [You] are as smart"
2021.eacl-main.27,W12-2103,0,0.0409073,"to do research on this novel research topic. Having crowdworkers invent instances of abusive language may raise ethical concerns. However, the type of abusive language that will be invented in this work is not directed towards specific invididuals or identity groups. Therefore, we believe that this procedure is justifiable. In principle, creating morally disputable content as part of research is not unusual. Both in plagiarism detection (Potthast 2 2 Related Work Datasets in abusive language detection mostly focus on different targets (e.g. Islamophobia (Waseem and Hovy, 2016), antisemitism (Warner and Hirschberg, 2012), misogyny (Anzovino et al., ´ 2018)), different languages (e.g. Spanish (AlvarezCarmona et al., 2018), Arabic (Mubarak et al., 2017), Portuguese (Fortuna et al., 2019)) or different domains (e.g. Twitter (Waseem and Hovy, 2016), Facebook (Kumar et al., 2018), Wikipedia (Wulczyn et al., 2017)). Despite some theoretical work outlining distinct subtypes of abusive language (Waseem et al., 2017), there has been little work on datasets that focus on particular subtypes. Schmidt and Wiegand (2017) present a more detailed overview on abusive language detection. Comparisons, particularly figurative c"
2021.eacl-main.27,W17-3012,0,0.423768,"A New Dataset and Linguistic Analysis Maja Geulig Michael Wiegand Digital Age Research Center (D!ARC) Institute of Computational Linguistics Heidelberg University Alpen-Adria-Universit¨at Klagenfurt D-69120 Heidelberg, Germany AT-9020 Klagenfurt, Austria geulig@cl.uni-heidelberg.de michael.wiegand@aau.at Josef Ruppenhofer Leibniz Institute for German Language D-68161 Mannheim, Germany ruppenhofer@ids-mannheim.de Abstract Though there has been much work on abusive language detection in general, there is has been comparatively little work focusing on implicit forms of abusive language (4)-(5) (Waseem et al., 2017). We examine the task of detecting implicitly abusive comparisons (e.g. Your hair looks like you have been electrocuted). Implicitly abusive comparisons are abusive comparisons in which abusive words (e.g. dumbass or scum) are absent. We detail the process of creating a novel dataset for this task via crowdsourcing that includes several measures to obtain a sufficiently representative and unbiased set of comparisons. We also present classification experiments that include a range of linguistic features that help us better understand the mechanisms underlying abusive comparisons. 1 (4) I haven’"
2021.eacl-main.27,N16-2013,0,0.534371,"erstand the mechanisms underlying abusive comparisons. 1 (4) I haven’t had an intelligent conversation with a woman in my whole life. (5) Why aren’t there any Mexicans on Star Trek? Because they don’t work in the future either. Introduction Abusive or offensive language is commonly defined as hurtful, derogatory or obscene utterances made by one person to another person.1 Examples are (1)-(3). (1) stop editing this, you dumbass. (2) Just want to slap the stupid out of these bimbos!!! (3) Go lick a pig you arab muslim piece of scum. In the literature, closely related terms include hate speech (Waseem and Hovy, 2016) or cyber bullying (Zhong et al., 2016). While there may be nuanced differences in meaning, they are all compatible with the general definition above. The definition we follow in this work also restricts abusive language to those utterances that are made to deliberately insult the target. A second requirement of an utterance to be considered abusive is that the target itself has to perceive the utterance as abusive. Due to the rise of user-generated web content, the amount of abusive language is steadily growing. NLP methods are required to focus human review efforts towards the most relevant"
2021.eacl-main.27,N19-1060,1,0.879589,"lt the target. A second requirement of an utterance to be considered abusive is that the target itself has to perceive the utterance as abusive. Due to the rise of user-generated web content, the amount of abusive language is steadily growing. NLP methods are required to focus human review efforts towards the most relevant microposts. 1 By implicit we understand abusive language that is not conveyed by (unambiguously) abusive words (e.g. dumbass, bimbo, scum). Detailed analysis on the output of existing classifiers has also revealed that currently only explicit abuse can be reliably detected (Wiegand et al., 2019). Given that implicit abuse is a challenging problem, we believe that the only reasonable approach to solve this problem is to address specific subtypes individually rather than consider all types of implicit abuse at once. In this paper, we examine implicitly abusive comparisons. A comparison is the act of evaluating two or more things by determining the relevant characteristics of each thing and to determine which characteristics of each are similar/different to the other (Bredin, 1998). By an abusive comparison, we understand a comparison that is perceived as abusive. In this work, we only"
2021.eacl-main.27,N18-1095,1,0.799713,"comparison is the act of evaluating two or more things by determining the relevant characteristics of each thing and to determine which characteristics of each are similar/different to the other (Bredin, 1998). By an abusive comparison, we understand a comparison that is perceived as abusive. In this work, we only consider those comparisons in which no (explicitly) abusive words are contained (6)-(8). Those comparisons are referred to as implicitly abusive comparisons. We exclude comparisons with abusive words since they can be easily detected with recent lexical resources for language abuse (Wiegand et al., 2018). http://thelawdictionary.org/ (6) You have the face of someone only a mother could love. (7) Your hair looks like you have been electrocuted. (8) You run like a headless chicken. We address abusive comparisons since they make up a large proportion of comparisons on 358 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 358–368 April 19 - 23, 2021. ©2021 Association for Computational Linguistics the most related dataset by Qadir et al. (2015). That dataset was created to automatically detect the sentiment of a comparison: one has"
2021.eacl-main.27,N16-1092,1,0.827138,"uthor describes the emotional frame of mind of the target (23). Since all our comparisons are negative, typical emotional frames are pain, sorrow, exhaustion or shock (as in (23)). The author of such comparisons does not necessarily evaluate the target. For example, if one states that some other person is in pain, this is not meant as some criticism, but rather some concern. Such comparisons are rarely perceived as abusive. (22) You look like an overfed cat. (ABUSE) (23) You look like a shocked cat. (OTHER) This distinction bears a resemblance to the distinction of sentiment views proposed by Wiegand et al. (2016). That work proposes a binary distinction into speaker views, which resembles evaluative Feature Cohen’s κ ABSURD CONTRAD DEHUM FIGUR TABOO VIEW 0.82 1.00 0.88 0.63 0.74 0.73 Table 4: Agreement on manual features. comparisons, and actor views, which resembles descriptions of the emotional frame of mind. Wiegand et al. (2016) also provide a list of verbs, nouns and adjectives classified into either of the two categories. Due to the fact that this lexicon seems inaccurate when it comes to ambiguous words7 , we annotated the binary distinction of evaluation vs. emotional frame of mind manually in"
2021.eacl-main.27,N19-1144,0,0.133299,"enting instances (i.e. comparisons) rather than by annotating automatically extracted instances. This design choice is necessary since existing datasets contain either insufficient or biased comparisons: The dataset from Qadir et al. (2015) includes about 180 implicitly abusive comparisons, however, we found, next to many nearduplicates, a heavy bias towards very few recurring images (e.g. You behave like a child, You look like a monkey). We observed the same phenomenon when extracting comparisons from Twitter directly. Established datasets for abusive language detection (Founta et al., 2018; Zampieri et al., 2019) contain just about 30-40 abusive comparisons. Our dataset will be created via crowdsourcing. Of course, having abusive comparisons be invented this way will inevitably result in some articificial data. However, we think only thus can we produce a dataset of reasonable size that has also a very low degree of bias which are two important requirements to be able to do research on this novel research topic. Having crowdworkers invent instances of abusive language may raise ethical concerns. However, the type of abusive language that will be invented in this work is not directed towards specific i"
2021.eacl-main.28,D17-1169,0,0.0319472,"lexicon for the task. It has been induced with the help of a (seed) base lexicon which had been manually annotated. The bootstrapping step largely relies on resources that exist only for wellresourced languages, such as WordNet, sentiment intensity datasets or sentiment-view lexicons. Recently, there has been a general interest in exploiting extralinguistic information for natural language processing. Emoticons, such as :-), have been found useful for sentiment analysis, particularly emotion classification (Purver and Battersby, 2012). Emojis represent an even more fine-grained set of icons. Felbo et al. (2017) exploit them for pretraining neural models to produce a text representation of emotional content. Since this approach relies on a representative sample of tweets containing emojis, only the 64 most frequently occurring emojis are considered. This set, however, does not contain the very predictive emojis for abusive language detection (e.g. middle finger). Corazza et al. (2020) follow an approach similar to Felbo et al. (2017) in that they pretrain a language model with the help of emoji informarion. However, unlike Felbo et al. (2017), their emoji-based masked language model is evaluated for"
2021.eacl-main.28,L18-1686,0,0.0125943,"tweets containing the middle finger (Table 1). In order to obtain 10k Portuguese and German tweets more quickly, we included tweets with other predictive emojis. We extracted tweets containing one of the 4 most predictive emojis: face vomiting, pile of poo, angry face or middle finger. These 4 emojis are drawn from our English data (Table 1) in order to further demonstrate crosslingual validity. The distribution of emojis reflects their natural distribution on Twitter. For non-abusive text we sampled sentences from the Portuguese and German versions of the Web As Corpus (Baroni et al., 2009; Filho et al., 2018) from which we also induced word embeddings with word2vec (Mikolov et al., 2013). We decided against pre-trained Twitter embeddings since for many languages such resources are not available. We opted for a setting applicable to most languages. Evaluation. We evaluate our emoji-based lexicons on the Portuguese dataset from Fortuna et al. (2019) and the two German datasets from GermEval (Wiegand et al., 2018b; Struß et al., 2019). These are datasets for the classification of abusive microposts. As in our evaluation on English data (Table 6), we refrain from an in-domain evaluation since again we"
2021.eacl-main.28,W19-3510,0,0.0149415,") in order to further demonstrate crosslingual validity. The distribution of emojis reflects their natural distribution on Twitter. For non-abusive text we sampled sentences from the Portuguese and German versions of the Web As Corpus (Baroni et al., 2009; Filho et al., 2018) from which we also induced word embeddings with word2vec (Mikolov et al., 2013). We decided against pre-trained Twitter embeddings since for many languages such resources are not available. We opted for a setting applicable to most languages. Evaluation. We evaluate our emoji-based lexicons on the Portuguese dataset from Fortuna et al. (2019) and the two German datasets from GermEval (Wiegand et al., 2018b; Struß et al., 2019). These are datasets for the classification of abusive microposts. As in our evaluation on English data (Table 6), we refrain from an in-domain evaluation since again we want to avoid topic/author biases (§1). Instead, lexicon-based classifiers and a crosslingual approach are used as baselines. The former classifiers predict a micropost as abusive if one abusive word according to the lexicon has been found. In addition to the two variants of hurtlex (Bassignana et al., 2018), hl-conservative and hl-inclusive,"
2021.eacl-main.28,2020.acl-main.373,0,0.0110379,"nambiguously) abusive words; abusive words are identified with the help of the lexicon from Wiegand et al. (2018a) positive polar expressions rarely co-occur with abusive language, negative polar expressions, however, do; the polar expressions are obtained from the Subjectivity Lexicon (Wilson et al., 2005) 2nd person pronouns are typical of abusive usage: you are a bitch; 1st person pronouns are likely to indicate non-abusive usage: I am a bitch quotation signs indicate reported speech; a tweet may report an abusive remark, however, the reported remark itself may not be perceived as abusive (Chiril et al., 2020) a typical means of expressing high emotional intensity Table 8: Features for disambiguating a potentially abusive word (referred to as target word); context is defined as a window of 4 words neighbouring the target word. class abusive non abusive all bitch freq perc 248 24.8 752 75.2 1000 100.0 fuck freq perc 210 21.0 790 79.0 1000 100.0 bitch fuck approach classifier Acc F1 Acc F1 majority SVM 75.2 42.9 79.0 44.1 heuristic baseline SVM 74.3 58.3 77.5 57.6 text classif. (Kaggle) BERT 28.0 57.0 61.7 70.1 text classif. (Davidson) BERT 75.9 60.1 80.9 65.7 emoji SVM 77.3 66.3 82.9 71.7 word-speci"
2021.eacl-main.28,2020.findings-emnlp.84,0,0.0238975,"atural language processing. Emoticons, such as :-), have been found useful for sentiment analysis, particularly emotion classification (Purver and Battersby, 2012). Emojis represent an even more fine-grained set of icons. Felbo et al. (2017) exploit them for pretraining neural models to produce a text representation of emotional content. Since this approach relies on a representative sample of tweets containing emojis, only the 64 most frequently occurring emojis are considered. This set, however, does not contain the very predictive emojis for abusive language detection (e.g. middle finger). Corazza et al. (2020) follow an approach similar to Felbo et al. (2017) in that they pretrain a language model with the help of emoji informarion. However, unlike Felbo et al. (2017), their emoji-based masked language model is evaluated for zero-shot abusive language detection. The task is also considered in a multilingual setting: the target languages are English, German, Italian and Spanish. The improvements that Corazza et al. (2020) report over baseline language models that do not explicitly incorporate emoji information are only limited. Our work extends Felbo et al. (2017) and Corazza et al. (2020) in that w"
2021.eacl-main.28,N19-1423,0,0.0157172,"ibrary.com/icon/anger-icon-14.html about 7,000 English words. For our experiments on Portuguese and German data, we created similar word lists following Wiegand et al. (2018a). Tasks. In this work, there are two types of tasks: lexicon induction tasks in which we rank negative polar expressions where the high ranks should be abusive words, and classification of abusive microposts. The former is evaluated with precision at rank n (P@n), while the latter is evaluated with accuracy and macro-average F-score. Supervised Micropost Classification with BERT. In many experiments, we employ BERTLARGE (Devlin et al., 2019) as a baseline for stateof-the-art text classification for detecting abusive microposts. We always fine-tune the pretrained model by adding another layer on top of it. (The supplementary notes contain more details regarding all classifiers employed in this paper.) 4 4.1 Inducing a Lexicon of Abusive Words Methods for Lexicon Induction Pointwise Mutual Information (PMI). A standard method for inducing a lexicon from labeled documents is to rank the words according to the PMI with the target class (Turney, 2002). We use tweets in which either of the above emojis occur as abusive documents. In or"
2021.eacl-main.28,W17-5216,0,0.0557359,"Missing"
2021.eacl-main.28,D18-1471,0,0.0395923,"Missing"
2021.eacl-main.28,W18-5117,0,0.0238876,"or Computational Linguistics to detect abusive language primarily focuses on the detection of explicitly abusive language, i.e. abusive language that is conveyed by abusive words. Such an approach is currently the most effective clue known for cross-domain classification (Wiegand et al., 2018a). In general, other types of abusive language that are more implicit, such as sarcasm, jokes or stereotypes, require more contextual interpretation of words. Supervised classification is theoretically able to conduct such contextual interpretation. However, it has been reported to ˇ perform very poorly (Karan and Snajder, 2018; Arango et al., 2019) on this task because the biases these classifiers exploit are unlikely to be present across different datasets (Wiegand et al., 2019). Therefore, we focus on explicitly abusive language in this work, since there are no ways of reliably detecting implicitly abusive language. Despite the existence of lexicons for abusive words, induction methods are required, since new abusive words enter language constantly. Further, there are only few lexicons available in languages other than English. The aim of our work is not to detect completely new types of abusive language but to f"
2021.eacl-main.28,2020.alw-1.17,0,0.0181527,"the dataset from Holgate et al. (2018). That dataset was the only existing dataset with word-specific annotation that was available to us at the time we carried out our experiments so that we could use it as one baseline.8 For each of the two words, we extracted 1,000 tweets in which it occurs and had them annotated via crowdsourcing (ProlificAcademic9 ). Each tweet was annotated as abusive or profane based on the majority of 5 annotators (native speakers of English). (The supplementary notes contain the annotation guidelines.) 8 Meanwhile, two further datasets by Pamungkas et al. (2020) and Kurrek et al. (2020) have been made publicly available which might also be suitable for the kind of evaluation we present in our work. 9 www.prolific.co Baselines for Disambiguation Text Classification. We train a supervised text classifier (BERT) on each of the following two large datasets (containing several thousand microposts) manually annotated on the micropost level. The dataset from Davidson et al. (2017) distinguishes between the 3 classes: hate speech, offensive language and other. The first category matches our definition of abusive language whereas the second category resembles our category of profane"
2021.eacl-main.28,P09-1113,0,0.0612178,"by one person to another.1 In the literature, closely related terms include hate speech (Waseem and Hovy, 2016) or cyber bullying (Zhong et al., 2016). While there may be nuanced differences in meaning, they are all compatible with the general definition above. Due to the rise of user-generated web content, the amount of abusive language is also steadily growing. NLP methods are required to focus human review efforts towards the most relevant microposts. Building classifiers for abusive language detection requires expensive manually labeled data. In this paper we explore distant supervision (Mintz et al., 2009) for abusive language detection in which abusive emojis serve as a heuristic to identify abusive language (1)-(8). These texts are subsequently used as training data. The advantage 1 http://thelawdictionary.org Recently, there has been significant criticism of in-domain supervised classification in abusive language detection, whose evaluation has been shown to produce overly optimistic classification scores. They are the result of biases in the underlying datasets. Wiegand et al. (2019) show that on the most popular dataset for this task (Waseem and Hovy, 2016), classifiers learn co-incidental"
2021.eacl-main.28,2020.lrec-1.765,0,0.0241292,"ly ambiguous and frequent on the dataset from Holgate et al. (2018). That dataset was the only existing dataset with word-specific annotation that was available to us at the time we carried out our experiments so that we could use it as one baseline.8 For each of the two words, we extracted 1,000 tweets in which it occurs and had them annotated via crowdsourcing (ProlificAcademic9 ). Each tweet was annotated as abusive or profane based on the majority of 5 annotators (native speakers of English). (The supplementary notes contain the annotation guidelines.) 8 Meanwhile, two further datasets by Pamungkas et al. (2020) and Kurrek et al. (2020) have been made publicly available which might also be suitable for the kind of evaluation we present in our work. 9 www.prolific.co Baselines for Disambiguation Text Classification. We train a supervised text classifier (BERT) on each of the following two large datasets (containing several thousand microposts) manually annotated on the micropost level. The dataset from Davidson et al. (2017) distinguishes between the 3 classes: hate speech, offensive language and other. The first category matches our definition of abusive language whereas the second category resembles"
2021.eacl-main.28,D17-1117,0,0.0162929,"his paper. • We use emojis to disambiguate the context of potentially abusive words. We exemplify this on the two ambiguous and frequent words fuck and bitch. A by-product is a dataset of mentions of these words annotated in context. The supplementary material2 to this paper includes all resources newly created for our research and notes on implementation details. 2 https://github.com/miwieg/ emojis for abusive language detection 2 Related Work Abusive language detection is mostly framed as a supervised learning task (Schmidt and Wiegand, 2017). Feature-based (Nobata et al., 2016) and neural (Pavlopoulos et al., 2017) methods are applied. Lexicon induction for abusive language detection has received only little attention in previous work, the exceptions being Razavi et al. (2010) who present a lexicon generated using adaptive learning, Gitari et al. (2015) who bootstrap hate verbs and Wiegand et al. (2018a) who induce a lexicon of abusive words. This lexicon is currently the best performing lexicon for the task. It has been induced with the help of a (seed) base lexicon which had been manually annotated. The bootstrapping step largely relies on resources that exist only for wellresourced languages, such as"
2021.eacl-main.28,D14-1162,0,0.0841241,"Missing"
2021.eacl-main.28,P19-1493,0,0.0185399,"d base lexicon). Moreover, we consider Wiegand-translated, which is the English lexicon from Wiegand et al. (2018a) translated to the target language via GoogleTranslate7 . Unlike Wiegand-replic, this lexicon is cheap to construct as it only requires the original English lexicon. Our crosslingual baseline exploits the abundance of labeled training data for abusive language detection on English and neural methods to close the language gap between English and the target language. We use multilingual BERT in which English, Portuguese and German share the same representation space. As proposed by Pires et al. (2019), we train a text classifier on an English dataset for abusive language detection and test the resulting multilingual model on the Portuguese or German microposts. The model that is learnt on English should be usable on the other languages as well, since the three languages share the same representation space. Our crosslingual approach is trained on the dataset from Zampieri et al. (2019), which like our non-English datasets originates from Twitter. Table 7 shows the results. We also added an upper bound for our emoji-based approach (emoji+manual) in which we also include abusive words manuall"
2021.eacl-main.28,E12-1049,0,0.0272403,") who induce a lexicon of abusive words. This lexicon is currently the best performing lexicon for the task. It has been induced with the help of a (seed) base lexicon which had been manually annotated. The bootstrapping step largely relies on resources that exist only for wellresourced languages, such as WordNet, sentiment intensity datasets or sentiment-view lexicons. Recently, there has been a general interest in exploiting extralinguistic information for natural language processing. Emoticons, such as :-), have been found useful for sentiment analysis, particularly emotion classification (Purver and Battersby, 2012). Emojis represent an even more fine-grained set of icons. Felbo et al. (2017) exploit them for pretraining neural models to produce a text representation of emotional content. Since this approach relies on a representative sample of tweets containing emojis, only the 64 most frequently occurring emojis are considered. This set, however, does not contain the very predictive emojis for abusive language detection (e.g. middle finger). Corazza et al. (2020) follow an approach similar to Felbo et al. (2017) in that they pretrain a language model with the help of emoji informarion. However, unlike"
2021.eacl-main.28,W17-1101,1,0.814509,"that we make publicly available along with all other resources created in this paper. • We use emojis to disambiguate the context of potentially abusive words. We exemplify this on the two ambiguous and frequent words fuck and bitch. A by-product is a dataset of mentions of these words annotated in context. The supplementary material2 to this paper includes all resources newly created for our research and notes on implementation details. 2 https://github.com/miwieg/ emojis for abusive language detection 2 Related Work Abusive language detection is mostly framed as a supervised learning task (Schmidt and Wiegand, 2017). Feature-based (Nobata et al., 2016) and neural (Pavlopoulos et al., 2017) methods are applied. Lexicon induction for abusive language detection has received only little attention in previous work, the exceptions being Razavi et al. (2010) who present a lexicon generated using adaptive learning, Gitari et al. (2015) who bootstrap hate verbs and Wiegand et al. (2018a) who induce a lexicon of abusive words. This lexicon is currently the best performing lexicon for the task. It has been induced with the help of a (seed) base lexicon which had been manually annotated. The bootstrapping step large"
2021.eacl-main.28,D08-1061,0,0.037343,"n the basis of the projected embeddings we rank the negative polar expressions from our vocabulary (§3). Recall-based Expansion by Label Propagation (LP). While the very high ranks of an induction method typically coincide with the target class (in our case: abusive words), the lower a rank is, the more likely we are to encounter other words. Taking the high ranks as abusive seeds and then applying some form of label propagation on a wordsimilarity graph may increase the overall coverage of abusive words found. More specifically, we apply the Adsorption label propagation algorithm from junto (Talukdar et al., 2008) on a wordsimilarity graph where the words of our vocabulary are nodes and edges encode cosine-similarities of their embeddings. As negative (i.e. non-abusive) seeds, we take the most frequently occurring words from our vocabulary since they are unlikely to represent abusive words. In order to produce a meaningful comparison to PMI and projection-based induction, we need to convert the categorical output of label propagation to a ranking of our entire vocabulary. We achieve this by ranking the words pre5 We take the version with 200 dimensions which is a very frequently used configuration for"
2021.eacl-main.28,P02-1053,0,0.0456977,"Missing"
2021.eacl-main.28,W12-2103,0,0.1397,"Missing"
2021.eacl-main.28,N16-2013,0,0.161106,"sized ass kicking you little Twitt (3) @USER I challenge you to go on a diet you fat cunt (4) @USER You are so so stupid you monkey face (5) Send your location, I’ll send some killers (6) @USER @USER A vote for toddstone or any liberal. Id rather flush a toilet. (7) Fuck the 12 fuck the cops we aint forgot about you, kill em all kill em all (8) @USER She is such a disgusting despicable human being! Ugh! Introduction Abusive or offensive language is defined as hurtful, derogatory or obscene utterances made by one person to another.1 In the literature, closely related terms include hate speech (Waseem and Hovy, 2016) or cyber bullying (Zhong et al., 2016). While there may be nuanced differences in meaning, they are all compatible with the general definition above. Due to the rise of user-generated web content, the amount of abusive language is also steadily growing. NLP methods are required to focus human review efforts towards the most relevant microposts. Building classifiers for abusive language detection requires expensive manually labeled data. In this paper we explore distant supervision (Mintz et al., 2009) for abusive language detection in which abusive emojis serve as a heuristic to identify abus"
2021.eacl-main.28,N19-1060,1,0.926239,"ive language detection requires expensive manually labeled data. In this paper we explore distant supervision (Mintz et al., 2009) for abusive language detection in which abusive emojis serve as a heuristic to identify abusive language (1)-(8). These texts are subsequently used as training data. The advantage 1 http://thelawdictionary.org Recently, there has been significant criticism of in-domain supervised classification in abusive language detection, whose evaluation has been shown to produce overly optimistic classification scores. They are the result of biases in the underlying datasets. Wiegand et al. (2019) show that on the most popular dataset for this task (Waseem and Hovy, 2016), classifiers learn co-incidental correlations between specific words (e.g. football or sport) and the abusive class label. Such spurious correlations help classifiers to correctly classify difficult microposts on that particular dataset. Arango et al. (2019) show that since on the dataset from Waseem and Hovy (2016) the majority of abusive tweets originate from just 2 authors, classifiers learn the authors’ writing style rather than abusive language. In order to avoid an evaluation affected by such topic or author bia"
2021.eacl-main.28,N18-1095,1,0.162335,"language. In order to avoid an evaluation affected by such topic or author biases, we focus on learning a lexicon of abusive language. A lexicon-based approach 369 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 369–380 April 19 - 23, 2021. ©2021 Association for Computational Linguistics to detect abusive language primarily focuses on the detection of explicitly abusive language, i.e. abusive language that is conveyed by abusive words. Such an approach is currently the most effective clue known for cross-domain classification (Wiegand et al., 2018a). In general, other types of abusive language that are more implicit, such as sarcasm, jokes or stereotypes, require more contextual interpretation of words. Supervised classification is theoretically able to conduct such contextual interpretation. However, it has been reported to ˇ perform very poorly (Karan and Snajder, 2018; Arango et al., 2019) on this task because the biases these classifiers exploit are unlikely to be present across different datasets (Wiegand et al., 2019). Therefore, we focus on explicitly abusive language in this work, since there are no ways of reliably detecting i"
2021.eacl-main.28,H05-1044,0,0.136666,"context? which pronouns are in context? quotation signs in tweet? presence of exclamation sign? explanation may be helpful in order to learn phrases such as fuck off; larger context is avoided since we are likely to overfit to particular domains target word is likely to be abusive if it co-occurs with other (unambiguously) abusive words; abusive words are identified with the help of the lexicon from Wiegand et al. (2018a) positive polar expressions rarely co-occur with abusive language, negative polar expressions, however, do; the polar expressions are obtained from the Subjectivity Lexicon (Wilson et al., 2005) 2nd person pronouns are typical of abusive usage: you are a bitch; 1st person pronouns are likely to indicate non-abusive usage: I am a bitch quotation signs indicate reported speech; a tweet may report an abusive remark, however, the reported remark itself may not be perceived as abusive (Chiril et al., 2020) a typical means of expressing high emotional intensity Table 8: Features for disambiguating a potentially abusive word (referred to as target word); context is defined as a window of 4 words neighbouring the target word. class abusive non abusive all bitch freq perc 248 24.8 752 75.2 10"
2021.eacl-main.28,S19-2010,0,0.0476836,"ectionukwac on lower ranks. Cross-Domain Evaluation. Next, we test the best lexicon of our previous experiments (i.e. projectionukwac +LP(200 abusive seed words)) in cross-domain micropost classification. Posts are categorized into abusive and non-abusive posts. Through a cross-domain classification, in which we train on one dataset and test on another, we show that the chosen configuration is not overfit to a particular dataset. Table 5 provides some information on the datasets we consider. In addition to the datasets used in Wiegand et al. (2018a), we include the recent SemEval-dataset from Zampieri et al. (2019). Table 6 shows the results of cross-domain micropost classification. As baselines we use a majorityclass classifier, the feature-based approach from Nobata et al. (2016), BERT and the lexicon from Wiegand et al. (2018a). In order to demonstrate the intrinsic predictiveness of the words learned by our emoji-based approach, we do not train a classifier on the source domain (unlike Wiegand et al. (2018a) who use the rank of the lexicon entries as a feature) but simply classify a micropost as abusive if an abusive word from our emoji-based lexicon is found. As abusive words, we consider all 1,250"
2021.germeval-1.1,W17-4218,0,0.0176886,"Engaging Comments. The task of detecting engaging comments is motivated by the idea to highlight comments that encourage and foster reasoned and civil discussions (Ziegele et al., 2018). Napoles et al. (2017b) laid groundwork by creating an annotated dataset of engaging, respectful, and informative conversations. They identified characteristics of these conversations, such as being on-topic of the discussed news article and persuasive but not sarcastic or mean. The authors used these characteristics in their follow-up work to automatically identify these conversations (Napoles et al., 2017a). Kolhatkar and Taboada (2017) introduce another publicly available dataset and use editor picks of comments posted on the website of the New York Times as examples of constructive comments. Examples of non-constructive comSubtask 3: Fact-Claiming Comment Classification. Beyond the challenge to ensure non-hostile debates, platforms and moderators are under pressure to act due to the rapid spread of misinformation and disinformation. Platforms need to review and verify information that has been posted to meet their responsibility as information providers and 2 Proceedings of the GermEval 2021 Shared Task on the Identificati"
2021.germeval-1.1,S19-2007,0,0.0961199,"Missing"
2021.germeval-1.1,N19-1423,0,0.00814442,"in Table 6. As a baseline, we also included the performance of a majority-class classifier always predicting the majority class, which is the absence of fact-claiming comments. 6 General Conclusions Drawn from the Evaluation. Given that the overwhelming majority of participants followed generic classification approaches for the different subtasks, we discuss the results in this section jointly. All teams that participated in this year’s shared task tested some form of deep learning. All teams except one considered contextual embeddings, most predominantly some type of transformer (i.e. BERT (Devlin et al., 2019)). Since the participants made use of various publicly available pre-trained models and given that the models of the best performing systems are different, it is difficult to determine any publicly available model that is particularly effective. Other types of classifiers, be it traditional supervised classifiers (e.g. Support Vector Machines, Logistic Regression, Forests) or other deep learning algorithms (e.g. CNN, GRU, or LSTM) were only used by a handful of teams each. Only one participant also tested a rule-based classifier. An additional method that has already proved effective in previo"
2021.germeval-1.1,W17-0802,0,0.0290146,"ave been previously defined as comments that make readers join a discussion, e.g. by posting a reply or reacting with a thumbs up/thumbs down (Risch and Krestel, 2020). In this shared task, we expand the definition in favor of comments that meet communication standards of deliberative quality (Ziegele et al., 2018), namely rationality, reciprocity, and mutual respect (Gutmann and Thompson, 1998). Detection of Engaging Comments. The task of detecting engaging comments is motivated by the idea to highlight comments that encourage and foster reasoned and civil discussions (Ziegele et al., 2018). Napoles et al. (2017b) laid groundwork by creating an annotated dataset of engaging, respectful, and informative conversations. They identified characteristics of these conversations, such as being on-topic of the discussed news article and persuasive but not sarcastic or mean. The authors used these characteristics in their follow-up work to automatically identify these conversations (Napoles et al., 2017a). Kolhatkar and Taboada (2017) introduce another publicly available dataset and use editor picks of comments posted on the website of the New York Times as examples of constructive comments. Examples of non-co"
2021.germeval-1.1,2021.acl-short.114,0,0.0188719,"role in the overall performance of classifiers. As a basis for our analysis of the results, we asked all participants to complete a survey in which we asked about details of their submission. A summary of the survey responses is available on the shared task website. Only two teams considered exploiting the plethora of available English training datasets for this task by following some multilingual approach. This low number, too, is in line with recent findings. Even for subtask 1, i.e. toxicity detection, for which many English datasets exist (Vidgen and Derczynski, 2020; Risch et al., 2021), Nozza (2021) recently identified reasons why multilingual approaches are highly problematic. One team also explored harnessing synthetically generated training data. However, that approach did not produce the expected outcome. Despite the similarity of many approaches pursued by the different participants of this year’s edition of GermEval, the difference in performance for subtask 1 is still fairly large (Table 3). We assume that due to the complexity of those state-of-the-art learning methods and frame7 Conclusion In this paper, we described the GermEval 2021 shared task on the identification of toxic,"
2021.germeval-1.1,2021.semeval-1.6,0,0.0314314,"2012), or incivility (Stoll et al., 2020) is currently one of the most active fields in natural language processing. For a recent overview of different approaches, we refer the reader to Schmidt and Wiegand (2017) or Fortuna and Nunes (2018), and to Vidgen and Derczynski (2020); Risch et al. (2021) for a comprehensive overview of existing datasets. There has also been a high number of different shared tasks on this topic. For English, several of these shared tasks have been organized as part of the SemEval shared task series (Zampieri et al., 2019; Basile et al., 2019; Zampieri et al., 2020; Pavlopoulos et al., 2021). For German, there have also been two editions of GermEval focusing on this task (Wiegand et al., 2018; Struß et al., 2019). The major difference between those two editions and this year’s subtask on toxic comments is the data source. While the data by Wiegand et al. (2018) and Struß et al. (2019) exclusively comprise tweets, this shared task deals with Facebook posts. Subtask 2: Engaging Comment Classification. Normative approaches such as Online Deliberation Theory (Friess and Eilders, 2015) assume that rational, respectful, and reciprocal comments contribute to fostering constructive and n"
2021.germeval-1.1,W12-2103,0,0.0389928,"l., 2015; Prochazka et al., 2018; Ziegele et al., 2018). While the automatic detection of toxic content is considered to be a promising approach in tackling this problem, it remains challenging and new approaches are constantly being developed. With this subtask we continue the series of previous GermEval Shared Tasks on Offensive Language Identification (Wiegand et al., 2018; Struß et al., 2019). 3 Related Work Detection of Toxic Comments. The detection of toxicity, which may also be referred to as offensive language (Razavi et al., 2010), abusive language (Nobata et al., 2016), hate speech (Warner and Hirschberg, 2012), or incivility (Stoll et al., 2020) is currently one of the most active fields in natural language processing. For a recent overview of different approaches, we refer the reader to Schmidt and Wiegand (2017) or Fortuna and Nunes (2018), and to Vidgen and Derczynski (2020); Risch et al. (2021) for a comprehensive overview of existing datasets. There has also been a high number of different shared tasks on this topic. For English, several of these shared tasks have been organized as part of the SemEval shared task series (Zampieri et al., 2019; Basile et al., 2019; Zampieri et al., 2020; Pavlop"
2021.germeval-1.1,N19-1060,1,0.842043,"ods to identify engaging comments automatically, there has been no related work on transformer-based models for this task. and test data via random sampling to avoid similar word distributions in both data sets. Further, since different people post comments to different editions of the talk show, it is unlikely that our dataset is dominated by the same person posting comments of a particular category (e.g. toxic comments) to any topic: our training data contain user comments of 157 especially active users debating in 141 discussion threads. Therefore, we consider a topic bias and person bias (Wiegand et al., 2019) unlikely. The dataset is released in anonymized form, which means that all user information and comment IDs have been removed. For annotating our dataset, we made use of a theory-based annotation scheme, which is designed to identify fine-grained forms of toxic and engaging commentary behavior as well as factclaiming in online discussions (Wilms et al., 2021). An overview of the resulting fine-grained subcategories used in the annotation can be found in Table 1. For the shared task, these subcategories have been subsumed to the three main categories of the subtasks (i.e. toxic, engaging and f"
2021.germeval-1.1,S19-2010,0,0.0352701,"language (Nobata et al., 2016), hate speech (Warner and Hirschberg, 2012), or incivility (Stoll et al., 2020) is currently one of the most active fields in natural language processing. For a recent overview of different approaches, we refer the reader to Schmidt and Wiegand (2017) or Fortuna and Nunes (2018), and to Vidgen and Derczynski (2020); Risch et al. (2021) for a comprehensive overview of existing datasets. There has also been a high number of different shared tasks on this topic. For English, several of these shared tasks have been organized as part of the SemEval shared task series (Zampieri et al., 2019; Basile et al., 2019; Zampieri et al., 2020; Pavlopoulos et al., 2021). For German, there have also been two editions of GermEval focusing on this task (Wiegand et al., 2018; Struß et al., 2019). The major difference between those two editions and this year’s subtask on toxic comments is the data source. While the data by Wiegand et al. (2018) and Struß et al. (2019) exclusively comprise tweets, this shared task deals with Facebook posts. Subtask 2: Engaging Comment Classification. Normative approaches such as Online Deliberation Theory (Friess and Eilders, 2015) assume that rational, respect"
2021.germeval-1.1,2021.woah-1.17,1,0.889515,"val Shared Tasks on Offensive Language Identification (Wiegand et al., 2018; Struß et al., 2019). 3 Related Work Detection of Toxic Comments. The detection of toxicity, which may also be referred to as offensive language (Razavi et al., 2010), abusive language (Nobata et al., 2016), hate speech (Warner and Hirschberg, 2012), or incivility (Stoll et al., 2020) is currently one of the most active fields in natural language processing. For a recent overview of different approaches, we refer the reader to Schmidt and Wiegand (2017) or Fortuna and Nunes (2018), and to Vidgen and Derczynski (2020); Risch et al. (2021) for a comprehensive overview of existing datasets. There has also been a high number of different shared tasks on this topic. For English, several of these shared tasks have been organized as part of the SemEval shared task series (Zampieri et al., 2019; Basile et al., 2019; Zampieri et al., 2020; Pavlopoulos et al., 2021). For German, there have also been two editions of GermEval focusing on this task (Wiegand et al., 2018; Struß et al., 2019). The major difference between those two editions and this year’s subtask on toxic comments is the data source. While the data by Wiegand et al. (2018)"
2021.germeval-1.1,W17-1101,1,0.813706,"es are constantly being developed. With this subtask we continue the series of previous GermEval Shared Tasks on Offensive Language Identification (Wiegand et al., 2018; Struß et al., 2019). 3 Related Work Detection of Toxic Comments. The detection of toxicity, which may also be referred to as offensive language (Razavi et al., 2010), abusive language (Nobata et al., 2016), hate speech (Warner and Hirschberg, 2012), or incivility (Stoll et al., 2020) is currently one of the most active fields in natural language processing. For a recent overview of different approaches, we refer the reader to Schmidt and Wiegand (2017) or Fortuna and Nunes (2018), and to Vidgen and Derczynski (2020); Risch et al. (2021) for a comprehensive overview of existing datasets. There has also been a high number of different shared tasks on this topic. For English, several of these shared tasks have been organized as part of the SemEval shared task series (Zampieri et al., 2019; Basile et al., 2019; Zampieri et al., 2020; Pavlopoulos et al., 2021). For German, there have also been two editions of GermEval focusing on this task (Wiegand et al., 2018; Struß et al., 2019). The major difference between those two editions and this year’s"
2021.naacl-main.48,P15-2122,0,0.0218708,"defined as the activity of saying [...] the opposite of what you mean (Macmillan, 2007). The way in which is spoken is intended to make someone else feel stupid or show them that you are angry. This explains the strong connection towards abusive language as in (48): (48) It’s always fun watching sports with a woman in the room. Although the automatic detection of sarcasm has been investigated (Tsur et al., 2010; Riloff et al., 2013), the classification performance is still fairly limited. • Rhetorical questions. Rhetorical questions are asked not to elicit information but to make a statement (Bhattasali et al., 2015). They have been examined on social-media texts (Ranganath et al., 2016; Oraby et al., 2017). Future work needs to address what makes a rhetorical question abusive: (49) Did Stevie Wonder choose these ""models""? • Other implicit abuse. Our final category comprises all further forms of implicit abuse that require world knowledge and inferencing: (50) (51) (52) (53) She still thinks she matters. I live in Ethiopia. Happy new year 1219! These girls know skinny sausages are no fun. Welcome to the Hotel Islamfornia. You may check out any time but you can never leave. datasets subtype Kumar SBFrames"
2021.naacl-main.48,2020.figlang-1.20,0,0.033929,"r. (39) Liberals are not very smart. (40) I’m not excited about your existence. If we translate these euphemisms into their unequivocal counterparts (41)-(43), the abusive nature of these statements becomes more obvious. By dehumanization, we commonly understand the act of perceiving or treating people as less than human (Haslam and Loughnan, 2016). While Haslam (41) I want to kill you. and Loughnan (2016) propose a fairly comprehen(42) Liberals are retarded. sive set of different properties that characterize de(43) I hate you. humanization, we focus on the most commonly With the exception of Felt and Riloff (2020), accepted property of likening members of the target group to non-human entities (Haslam, 2006), euphemisms have not been addressed in natural language processing so far. such as machines, animals or diseases. We observed two different realizations of dehuAs a research question, one would need to anmanization. On the one hand, the target is explicitly swer how abusive euphemisms can be detected and equated with non-human entities (33). translated to their unequivocal counterpart. 579 4.6 Call for Action Calls for action represent another type of implicitly abusive language. By that we underst"
2021.naacl-main.48,D15-1294,0,0.0213675,"sonable starting point since not every negative sentence focusing on some identity group conveys some (abusive) stereotype (e.g. (8)-(10)). A first research question could be how to detect stereotypical statements among negative statements. (8) Gay people fight for the right to be accepted. (9) Muslims groan under the recession. (10) Jews mourn the loss of a member of their community. We believe that specific linguistic properties may be indicative for automatic classification. For example, stereotypes are more likely to co-occur with habitual aspect (11) rather than non-habitual aspect (12) (Friedrich and Pinkal, 2015). (11) Jews always support terror instability. (12) Jews currently fear displaying their faith in public. One should also examine whether generic phrases regarding identity groups (13) correlate with stereotypes (Reiter and Frank, 2010). Previous work already established that the definite article, which represents a subset of such generic phrases, is predictive for abusive language (Burnap and Williams, 2015; Palmer et al., 2017). (13) The jew does not care about the humankind. Further, the same stereotype can be expressed in different ways. For example, (14)-(17) convey the sexist stereotype"
2021.naacl-main.48,2020.acl-main.373,0,0.0794078,"Missing"
2021.naacl-main.48,2020.trac-1.1,0,0.0679416,"Missing"
2021.naacl-main.48,P19-1334,0,0.0607398,"Missing"
2021.naacl-main.48,P10-1005,0,0.0484349,"(8) Gay people fight for the right to be accepted. (9) Muslims groan under the recession. (10) Jews mourn the loss of a member of their community. We believe that specific linguistic properties may be indicative for automatic classification. For example, stereotypes are more likely to co-occur with habitual aspect (11) rather than non-habitual aspect (12) (Friedrich and Pinkal, 2015). (11) Jews always support terror instability. (12) Jews currently fear displaying their faith in public. One should also examine whether generic phrases regarding identity groups (13) correlate with stereotypes (Reiter and Frank, 2010). Previous work already established that the definite article, which represents a subset of such generic phrases, is predictive for abusive language (Burnap and Williams, 2015; Palmer et al., 2017). (13) The jew does not care about the humankind. Further, the same stereotype can be expressed in different ways. For example, (14)-(17) convey the sexist stereotype that women belong in the kitchen. (14) Men should drive and women should cook. (15) This is how America should be. 5 women slaving over a hot stove. (16) Get back in the kitchen. (17) Women should all stay at home in an apron, chained t"
2021.naacl-main.48,D13-1066,0,0.0955127,"Missing"
2021.naacl-main.48,2020.acl-main.486,0,0.0116408,", that, due to the sampling process, coincidentally only occur in abusive microposts. Although additional datasets containing larger amounts of implicit abuse have been released since Wiegand et al. (2019) published their findings, we found that these new datasets also suffer from bi5 What should(n’t) the datasets for ases. We outline these biases on the most recent implicit abuse look like? dataset that displays a high degree of implicit abuse Driven by the requirements of data-hungry deep- and that is also fairly large (Table 1): the dataset learning methods, the most common strategy for by Sap et al. (2020) (SBFrames). Of the recent abusive language detection is to create a single datasets, it is also the only dataset to cover a signifidataset and train a classifier on it. That dataset cant amount of abusive instances targeting common 581 identity groups (e.g. Jews, Muslims). In order to get a larger amount of microposts, existing datasets (e.g. Founta et al. (2018)) were merged into SBFrames. In addition, further raw data was added, such as posts from the whitesupremacist platform stormfront.org or subReddits on abusive jokes from reddit.com. While these additional data undoubtedly yield more a"
2021.naacl-main.48,W17-5537,0,0.01894,"in which is spoken is intended to make someone else feel stupid or show them that you are angry. This explains the strong connection towards abusive language as in (48): (48) It’s always fun watching sports with a woman in the room. Although the automatic detection of sarcasm has been investigated (Tsur et al., 2010; Riloff et al., 2013), the classification performance is still fairly limited. • Rhetorical questions. Rhetorical questions are asked not to elicit information but to make a statement (Bhattasali et al., 2015). They have been examined on social-media texts (Ranganath et al., 2016; Oraby et al., 2017). Future work needs to address what makes a rhetorical question abusive: (49) Did Stevie Wonder choose these ""models""? • Other implicit abuse. Our final category comprises all further forms of implicit abuse that require world knowledge and inferencing: (50) (51) (52) (53) She still thinks she matters. I live in Ethiopia. Happy new year 1219! These girls know skinny sausages are no fun. Welcome to the Hotel Islamfornia. You may check out any time but you can never leave. datasets subtype Kumar SBFrames Waseem Warner OffensEval average other implicit abuse 9.8 28.4 12.8 30.4 2.4 16.8 perpetrato"
2021.naacl-main.48,W17-1101,1,0.841904,"Computational Linguistics 2 The Story So Far By far the most prominent classification approaches applied to abusive language detection are supervised learning methods. Whereas initially, traditional learning algorithms, such as SVMs or logistic regression, were among the most popular methods for this task (Warner and Hirschberg, 2012; Burnap et al., 2015; Nobata et al., 2016), at present, best results are obtained by deep-learning methods, particularly transformers (Struß et al., 2019; Kumar et al., 2020; Zampieri et al., 2020). A more detailed summary of the methods explored can be found in Schmidt and Wiegand (2017) and Fortuna and Nunes (2018). Unfortunately, so far there has been little error analysis of system output for abusive language detection. As a consequence, the community is fairly unaware of what types of errors are made and why. The most notable exception is van Aken et al. (2018) who carry out experiments on the dataset of Google’s Toxic Comment Classification Challenge3 and the dataset by Davidson et al. (2017). As prominent errors that a supervised classifier makes, van Aken et al. (2018) list toxicity without swearwords, rhetorical questions and comparisons/metaphorical language. All the"
2021.naacl-main.48,W17-3014,0,0.0414765,"Missing"
2021.naacl-main.48,D15-1019,0,0.0234523,") is compared to some offensive entity, action or state (idiot in (25)). Abusive comparisons need not be explicitly abusive (25) but can also be implicitly abusive (26)-(27). (25) You talk like an idiot. (26) You look like someone only a mother could love. (27) You sing like a dying bird. A research question that would need to be answered is whether detecting abusive comparisons is not (almost) identical to the detection of comparisons conveying a negative sentiment. Such classification of comparisons into positive (28), neutral (29) and negative comparisons (30) has already been addressed by Qadir et al. (2015). (28) You look like a princess. (29) You look like your brother. (30) You look like a crackhead. Another research question would be to examine whether abusive comparisons are not identical to (negative) comparisons using figurative language (i.e. similes as (31)). Intuitively, comparisons employing literal language should be less abusive (32). (31) You look like the back end of a bus. (32) You look like you have slept badly. 4.4 Dehumanization (33) Black people are monkeys. On the other hand, a more difficult form of dehumanization involves metaphorical language in which the target is not exp"
2021.naacl-main.48,W18-5105,0,0.0423634,"Missing"
2021.naacl-main.48,W12-2103,0,0.0447923,"e. They are taken from actual web data and in no way reflect the opinion of the authors. 2 576 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 576–587 June 6–11, 2021. ©2021 Association for Computational Linguistics 2 The Story So Far By far the most prominent classification approaches applied to abusive language detection are supervised learning methods. Whereas initially, traditional learning algorithms, such as SVMs or logistic regression, were among the most popular methods for this task (Warner and Hirschberg, 2012; Burnap et al., 2015; Nobata et al., 2016), at present, best results are obtained by deep-learning methods, particularly transformers (Struß et al., 2019; Kumar et al., 2020; Zampieri et al., 2020). A more detailed summary of the methods explored can be found in Schmidt and Wiegand (2017) and Fortuna and Nunes (2018). Unfortunately, so far there has been little error analysis of system output for abusive language detection. As a consequence, the community is fairly unaware of what types of errors are made and why. The most notable exception is van Aken et al. (2018) who carry out experiments"
2021.naacl-main.48,W17-3012,0,0.0126222,"actually look like and why are we not getting there? Michael Wiegand Josef Ruppenhofer Digital Age Research Center (D!ARC) Leibniz Institute for German Language Alpen-Adria-Universität Klagenfurt D-68161 Mannheim, Germany AT-9020 Klagenfurt, Austria ruppenhofer@ids-mannheim.de michael.wiegand@aau.at Elisabeth Eder Institut für Germanistik Alpen-Adria-Universität Klagenfurt AT-9020 Klagenfurt, Austria elisabeth.eder@aau.at Abstract Though there has been much work on abusive language detection in general, comparatively little work has been focusing on implicit forms of abusive language (4)-(5) (Waseem et al., 2017). By implicit abuse we understand abusive language that is not conveyed by (unambiguously) abusive words (e.g. dumbass, bimbo, scum). Abusive language detection is an emerging field in natural language processing which has received a large amount of attention recently. Still the success of automatic detection is limited. Particularly, the detection of implicitly abusive language, i.e. abusive language that is not conveyed by abusive words (e.g. dumbass or scum), is not working well. In this position paper, we explain why existing datasets make learning implicit abuse difficult and what needs t"
2021.naacl-main.48,N16-2013,0,0.0295589,"tasets. Arguing for a divide-and-conquer strategy, we present a list of subtypes of implicitly abusive language and formulate research tasks and questions for future research. 1 (4) I haven’t had an intelligent conversation with a woman in my whole life. (5) Why aren’t there any Mexicans on Star Trek? Because they don’t work in the future either. Introduction Abusive or offensive language is commonly defined as hurtful, derogatory or obscene utterances made by one person to another person or group of persons.1 Examples are (1)-(3). In the literature, closely related terms include hate speech (Waseem and Hovy, 2016) or cyberbullying (Zhong et al., 2016). While there may be nuanced differences in meaning, they are all compatible with the general definition above.2 Detailed analyses of the output of existing classifiers have also revealed that currently only explicit abuse can be reliably detected (van Aken et al., 2018; Wiegand et al., 2019). In this position paper, we want to shed more light on the nature of implicitly abusive language. We identify subtypes of implicit abuse that can be found in existing datasets and the literature. We also outline shortcomings that prevent implicitly abusive language fr"
2021.naacl-main.48,2021.eacl-main.274,0,0.0290095,"uces fairly poor classifica- instances by merging datasets does not solve the tion performance on the cross-domain detection of problem. It may introduce further detrimental biabusive language, with lexicon-based approaches ases. Overall, our analysis supports the claim that performing much stronger (Wiegand et al., 2018). the currently available datasets are not really suitFurther, statistical debiasing methods for abusive able for effectively learning implicit abuse. We language detection have also been reported to yield strongly argue for new datasets that focus on parvery limited success (Zhou et al., 2021). The au- ticular subtypes of implicit abuse. This will also thors of that research argue that spending more facilitate thinking about appropriate negative data. efforts in ensuring a high quality of the datasets Larger datasets are not necessarily the best datasets during their creation is more worthwhile than ap- to train a classifier on, especially if they are domiplying sophisticated machine learning. nated by frequently observed words. Finally, it may We anticipate that there are also some subtasks in also make sense to learn on smaller units, such as the realm of implicit abuse that may"
2021.naacl-main.48,N19-1060,1,0.925322,"e future either. Introduction Abusive or offensive language is commonly defined as hurtful, derogatory or obscene utterances made by one person to another person or group of persons.1 Examples are (1)-(3). In the literature, closely related terms include hate speech (Waseem and Hovy, 2016) or cyberbullying (Zhong et al., 2016). While there may be nuanced differences in meaning, they are all compatible with the general definition above.2 Detailed analyses of the output of existing classifiers have also revealed that currently only explicit abuse can be reliably detected (van Aken et al., 2018; Wiegand et al., 2019). In this position paper, we want to shed more light on the nature of implicitly abusive language. We identify subtypes of implicit abuse that can be found in existing datasets and the literature. We also outline shortcomings that prevent implicitly abusive language from really being learned on its own terms. With this study, we hope to guide future research on implicitly abusive language. Our contributions in this paper are: • We present a list of subtypes of implicit abuse. This is accompanied by quantitative information from publicly available datasets. • We derive research tasks and questi"
2021.naacl-main.48,N18-1095,1,0.853625,"(explicitly) abusive types and perpetrators) are concerned, unsuitable language detection, machine learning has not pro- sampling causes biases that prevent classifiers from duced the anticipated results. For example, super- really learning these phenomena. Simply adding vised learning still produces fairly poor classifica- instances by merging datasets does not solve the tion performance on the cross-domain detection of problem. It may introduce further detrimental biabusive language, with lexicon-based approaches ases. Overall, our analysis supports the claim that performing much stronger (Wiegand et al., 2018). the currently available datasets are not really suitFurther, statistical debiasing methods for abusive able for effectively learning implicit abuse. We language detection have also been reported to yield strongly argue for new datasets that focus on parvery limited success (Zhou et al., 2021). The au- ticular subtypes of implicit abuse. This will also thors of that research argue that spending more facilitate thinking about appropriate negative data. efforts in ensuring a high quality of the datasets Larger datasets are not necessarily the best datasets during their creation is more worthwhi"
2021.naacl-main.48,W19-3502,0,0.0282718,"e content of a micropost is hidden in the nontextual components or results as an interplay of text and image/video. One could also regard many of these abusive posts as instances of implicit abuse since many of them do not contain mentions of abusive words. Therefore, a comprehensive classifier to detect implicitly abusive microposts should also consist of a multimodal component that analyses image or video content and fuses this information with text analysis. Indeed the community is aware of this form of abuse and there have been several attempts for multimodal analysis (Singh et al., 2017; Yang et al., 2019; Gomez et al., 2020). In our work, however, we do not address the aspect of multimodal abuse simply because many datasets only include the textual component of a micropost and the reconstruction of non-textual components of posts can only 580 • Jokes. Jokes as (47) can be severely abusive. (47) What’s better than winning gold in the paralympics? Walking. The computational modeling of humor remains a challenging task (Mihalcea and Strapparava, 2006). We are not aware of any research on the detection of abusive humor. • Sarcasm. Sarcasm is largely defined as the activity of saying [...] the opp"
2021.naacl-main.48,N19-1144,0,0.0443502,"Missing"
C14-1216,P98-1013,0,0.111955,"ght products. In many cases, the names of variants consist of the name of the original brand with some prefix or suffix indicating the particular type of variant (e.g. mini babybel or philadelphia light). We manually compiled 11 affixes and check for each food item how often it is accompanied by one of them. 4.5 Commerce Cues (COMMERCE) Presumably, brands are more likely to be mentioned in the context of commercial transaction events than types. Therefore, we created a list of words that indicate these types of events. The list was created ad hoc. We used external resources, such as FrameNet (Baker et al., 1998) or GermaNet (Hamp and Feldweg, 1997) (the German version of WordNet (Miller et al., 1990)), and made no attempt to tune that list to our domain-specific food corpus. The final list (85 cues in total) comprises: verbs (and deverbal nouns) that convey the event of a commercial transaction (e.g. buy, purchase or sell), persons involved in a commercial transaction (e.g. customer or shop assistant), means of purchase (e.g. money, credit card or bill), places of purchase (e.g. supermarket or shop) and judgment of price (e.g. cheap or expensive). 4.6 Food Modifier (PATmod ) Even though many mentions"
C14-1216,D12-1124,0,0.02156,"ferent types of classification have been explored including ontology mapping (van Hage et al., 2005), part-whole relations (van Hage et al., 2006), recipe attributes (Druck, 2013), dish detection and the categorization of food types according to the Food Guide Pyramid (Wiegand et al., 2014). Relation extraction tasks have also been examined. While a strong focus is on food-health relations (Yang et al., 2011; Miao et al., 2012; Kang et al., 2013; Wiegand and Klakow, 2013), relations relevant to customer advice have also been addressed (Wiegand et al., 2012; Wiegand et al., 2014). Beyond that, Chahuneau et al. (2012) relate sentiment information to food prices with the help of a large corpus consisting of restaurant menus and reviews. Druck and Pang (2012) extract actionable recipe refinements. To the best of our knowledge, we present the first work that explicitly addresses the detection of brands in the food domain. While brands as such present an additional dimension to previously examined types of categorization, we also show that the categorization according to the Food Guide Pyramid helps to decide whether a food item is a brand or not. 7 Conclusion We examined the task of separating types from bran"
C14-1216,chrupala-klakow-2010-named,1,0.792133,"at also employs features below the word level. As many of our food items will be unknown words, a character-level analysis may still be able to make useful predictions. 4.3 Contextual Named-Entity Recognition (NERcontext) We also count the number of other named entities that co-occur with the target food brand within the same sentence. We are only interested in organizations; an organization co-occurring with a brand is likely to be the company producing that brand (e.g. He loves Kellogg’scompany frostiesbrand .) For this feature, we rely on the output of a named-entity recognizer for German (Chrupała and Klakow, 2010). 4.4 Diversification (DIVERS) Once a product has established itself on the market for a substantial amount of time, many companies introduce variants of their brand to further consolidate their market position. The purpose of this diversification is to appeal to customers with special needs. A typical variant of food brands are light products. In many cases, the names of variants consist of the name of the original brand with some prefix or suffix indicating the particular type of variant (e.g. mini babybel or philadelphia light). We manually compiled 11 affixes and check for each food item h"
C14-1216,P12-1057,0,0.0244314,"6), recipe attributes (Druck, 2013), dish detection and the categorization of food types according to the Food Guide Pyramid (Wiegand et al., 2014). Relation extraction tasks have also been examined. While a strong focus is on food-health relations (Yang et al., 2011; Miao et al., 2012; Kang et al., 2013; Wiegand and Klakow, 2013), relations relevant to customer advice have also been addressed (Wiegand et al., 2012; Wiegand et al., 2014). Beyond that, Chahuneau et al. (2012) relate sentiment information to food prices with the help of a large corpus consisting of restaurant menus and reviews. Druck and Pang (2012) extract actionable recipe refinements. To the best of our knowledge, we present the first work that explicitly addresses the detection of brands in the food domain. While brands as such present an additional dimension to previously examined types of categorization, we also show that the categorization according to the Food Guide Pyramid helps to decide whether a food item is a brand or not. 7 Conclusion We examined the task of separating types from brands in the food domain. Framing the problem as a ranking task, we directly converted predictive features extracted from a domain-specific corpu"
C14-1216,W97-0802,0,0.0341492,"names of variants consist of the name of the original brand with some prefix or suffix indicating the particular type of variant (e.g. mini babybel or philadelphia light). We manually compiled 11 affixes and check for each food item how often it is accompanied by one of them. 4.5 Commerce Cues (COMMERCE) Presumably, brands are more likely to be mentioned in the context of commercial transaction events than types. Therefore, we created a list of words that indicate these types of events. The list was created ad hoc. We used external resources, such as FrameNet (Baker et al., 1998) or GermaNet (Hamp and Feldweg, 1997) (the German version of WordNet (Miller et al., 1990)), and made no attempt to tune that list to our domain-specific food corpus. The final list (85 cues in total) comprises: verbs (and deverbal nouns) that convey the event of a commercial transaction (e.g. buy, purchase or sell), persons involved in a commercial transaction (e.g. customer or shop assistant), means of purchase (e.g. money, credit card or bill), places of purchase (e.g. supermarket or shop) and judgment of price (e.g. cheap or expensive). 4.6 Food Modifier (PATmod ) Even though many mentions of brands are similar to those of ty"
C14-1216,P97-1023,0,0.121095,"address are (mostly) language universal. All examples are given as English translations. We use the term food item to refer to the union of food brands and food types. All food items will be written in lowercase reflecting the identical case spelling in German, i.e. types and brands are both written uppercase. In English, both types and brands can be written uppercase or lowercase2 , however, there is a tendency in user-generated content/social media to write mostly lowercase. 2 Motivation & Data Previous research on lexicon induction proposed a widely applicable method based on coordination (Hatzivassiloglou and McKeown, 1997; Riloff and Shepherd, 1997; Roark and Charniak, 1998): First, a set of seed expressions that are typical of the categories one wants to induce are defined. Then, additional instances of those categories are obtained by extracting conjuncts of the seed expressions (i.e. all expressions that match <seed> and/or <expression> are extracted as new instances). A detailed study of such lexicon induction has recently been published by Ziering et al. (2013), who also point out the great semantic coherence of conjuncts. This method can also be applied to the food domain. As a domain-specific dataset fo"
C14-1216,D13-1150,0,0.0311416,"focuses on temporal features to identify distinct product instances (these may also include brand names). The food domain has also recently received some attention. Different types of classification have been explored including ontology mapping (van Hage et al., 2005), part-whole relations (van Hage et al., 2006), recipe attributes (Druck, 2013), dish detection and the categorization of food types according to the Food Guide Pyramid (Wiegand et al., 2014). Relation extraction tasks have also been examined. While a strong focus is on food-health relations (Yang et al., 2011; Miao et al., 2012; Kang et al., 2013; Wiegand and Klakow, 2013), relations relevant to customer advice have also been addressed (Wiegand et al., 2012; Wiegand et al., 2014). Beyond that, Chahuneau et al. (2012) relate sentiment information to food prices with the help of a large corpus consisting of restaurant menus and reviews. Druck and Pang (2012) extract actionable recipe refinements. To the best of our knowledge, we present the first work that explicitly addresses the detection of brands in the food domain. While brands as such present an additional dimension to previously examined types of categorization, we also show that"
C14-1216,Y12-1010,0,0.029723,"Amazon. Their work focuses on temporal features to identify distinct product instances (these may also include brand names). The food domain has also recently received some attention. Different types of classification have been explored including ontology mapping (van Hage et al., 2005), part-whole relations (van Hage et al., 2006), recipe attributes (Druck, 2013), dish detection and the categorization of food types according to the Food Guide Pyramid (Wiegand et al., 2014). Relation extraction tasks have also been examined. While a strong focus is on food-health relations (Yang et al., 2011; Miao et al., 2012; Kang et al., 2013; Wiegand and Klakow, 2013), relations relevant to customer advice have also been addressed (Wiegand et al., 2012; Wiegand et al., 2014). Beyond that, Chahuneau et al. (2012) relate sentiment information to food prices with the help of a large corpus consisting of restaurant menus and reviews. Druck and Pang (2012) extract actionable recipe refinements. To the best of our knowledge, we present the first work that explicitly addresses the detection of brands in the food domain. While brands as such present an additional dimension to previously examined types of categorization"
C14-1216,Y12-1031,0,0.0294524,"ype, i.e. forum entries, that comprise full 2299 Feature WIKIoracle ranking+GRAPHpyramid ranking+GRAPHpyramid +VSM ranking+GRAPHpyramid +GRAPHbrand ranking+GRAPHpyramid +WIKI P@200 66.00 62.50 60.00 67.50 70.00 AP 0.429 0.626 0.619 0.638 0.688 -2nd reset P@200 AP -N/A- -N/A-N/A- -N/A63.00 0.661 65.50 0.662 73.00 0.718 Table 9: Impact of bootstrapping; -2nd reset: does not apply reset feature for a second time (Figure 1). sentences. Previous work also focuses on traditional (semi-)supervised algorithms. Hence, there are only few additional insights as to the specific properties of brand names. Min and Park (2012) examine the aspect of product instance distinction on the use case of product reviews on jeans from Amazon. Their work focuses on temporal features to identify distinct product instances (these may also include brand names). The food domain has also recently received some attention. Different types of classification have been explored including ontology mapping (van Hage et al., 2005), part-whole relations (van Hage et al., 2006), recipe attributes (Druck, 2013), dish detection and the categorization of food types according to the Food Guide Pyramid (Wiegand et al., 2014). Relation extraction"
C14-1216,D11-1144,0,0.165294,"ble 9. The reason for this is that we evaluate in isolation rather than in combination with other features (i.e. parts of the additional benefit included in GRAPHbrand may already be contained in ranking and reset features). Secondly, in a ranking task (Table 9), good performance is usually achieved by classifiers biased towards a high precision. Indeed, the best ranker in Table 9, i.e. WIKI, achieves the highest precision in Table 8. 6 Related Work Ling and Weld (2012) examine named-entity recognition on data that also include brands, however, the class of brands is not explicitly discussed. Putthividhya and Hu (2011) explore brands in the context of product attribute extraction. Entities are extracted from eBay’s clothing and shoe category. Nadeau et al. (2006) explicitly generate gazetteers of car brands obtained from corresponding websites. Those textual data are very restrictive in that they do not represent sentences but category listings or tables. In this paper, we consider as textual source a more general text type, i.e. forum entries, that comprise full 2299 Feature WIKIoracle ranking+GRAPHpyramid ranking+GRAPHpyramid +VSM ranking+GRAPHpyramid +GRAPHbrand ranking+GRAPHpyramid +WIKI P@200 66.00 62."
C14-1216,W97-0313,0,0.12838,"sal. All examples are given as English translations. We use the term food item to refer to the union of food brands and food types. All food items will be written in lowercase reflecting the identical case spelling in German, i.e. types and brands are both written uppercase. In English, both types and brands can be written uppercase or lowercase2 , however, there is a tendency in user-generated content/social media to write mostly lowercase. 2 Motivation & Data Previous research on lexicon induction proposed a widely applicable method based on coordination (Hatzivassiloglou and McKeown, 1997; Riloff and Shepherd, 1997; Roark and Charniak, 1998): First, a set of seed expressions that are typical of the categories one wants to induce are defined. Then, additional instances of those categories are obtained by extracting conjuncts of the seed expressions (i.e. all expressions that match <seed> and/or <expression> are extracted as new instances). A detailed study of such lexicon induction has recently been published by Ziering et al. (2013), who also point out the great semantic coherence of conjuncts. This method can also be applied to the food domain. As a domain-specific dataset for all our experiments, we u"
C14-1216,P98-2182,0,0.122356,"as English translations. We use the term food item to refer to the union of food brands and food types. All food items will be written in lowercase reflecting the identical case spelling in German, i.e. types and brands are both written uppercase. In English, both types and brands can be written uppercase or lowercase2 , however, there is a tendency in user-generated content/social media to write mostly lowercase. 2 Motivation & Data Previous research on lexicon induction proposed a widely applicable method based on coordination (Hatzivassiloglou and McKeown, 1997; Riloff and Shepherd, 1997; Roark and Charniak, 1998): First, a set of seed expressions that are typical of the categories one wants to induce are defined. Then, additional instances of those categories are obtained by extracting conjuncts of the seed expressions (i.e. all expressions that match <seed> and/or <expression> are extracted as new instances). A detailed study of such lexicon induction has recently been published by Ziering et al. (2013), who also point out the great semantic coherence of conjuncts. This method can also be applied to the food domain. As a domain-specific dataset for all our experiments, we use a crawl of chefkoch.de3"
C14-1216,I13-1003,1,0.83168,"features to identify distinct product instances (these may also include brand names). The food domain has also recently received some attention. Different types of classification have been explored including ontology mapping (van Hage et al., 2005), part-whole relations (van Hage et al., 2006), recipe attributes (Druck, 2013), dish detection and the categorization of food types according to the Food Guide Pyramid (Wiegand et al., 2014). Relation extraction tasks have also been examined. While a strong focus is on food-health relations (Yang et al., 2011; Miao et al., 2012; Kang et al., 2013; Wiegand and Klakow, 2013), relations relevant to customer advice have also been addressed (Wiegand et al., 2012; Wiegand et al., 2014). Beyond that, Chahuneau et al. (2012) relate sentiment information to food prices with the help of a large corpus consisting of restaurant menus and reviews. Druck and Pang (2012) extract actionable recipe refinements. To the best of our knowledge, we present the first work that explicitly addresses the detection of brands in the food domain. While brands as such present an additional dimension to previously examined types of categorization, we also show that the categorization accordi"
C14-1216,E14-1071,1,0.904724,"n the target item is likely to be some brand. This is due to the fact that many brands are often mentioned in combination with the food type that they represent, e.g. volvic mineral water, nutella chocolate spread. 4.7 Prepositional Phrase Embedding (PATpp) Instead of appearing as a modifier (§4.6), a brand may also be embedded in some prepositional phrase that has a similar meaning, e.g. We only buy the chocolate spread [by nutella]P P . 4.8 Graph-based Methods (GRAPH) We also employ some semi-supervised graph clustering method in order to assign semantic types to food items as introduced in Wiegand et al. (2014). The underlying data structure is a food graph that is generated automatically from our domain-specific corpus where nodes represent food items and edge weights 2295 Category MEAT BEVERAGE SWEET SPICE VEGE STARCH MILK FRUIT GRAIN FAT EGG Description meat and fish (products) beverages (incl. alcoholic drinks) sweets, pastries and snack mixes spices and sauces vegetables (incl. salads) starch-based side dishes milk products fruits grains, nuts and seeds fat eggs General 19.48 17.19 14.90 10.53 10.38 9.21 6.71 4.48 3.41 2.54 0.92 Brands 1.31 23.96 25.60 2.42 0.00 4.42 23.48 1.14 0.00 20.00 0.00"
C14-1216,I13-1188,0,0.0262032,"Missing"
C14-1216,C98-1013,0,\N,Missing
C14-1216,C98-2177,0,\N,Missing
C18-1213,E09-1005,0,0.0163956,"r. Heuristic using ‘jeglich’ (ANY): Negative polarity items (NPIs) are known to occur in the context of negation (Giannakidou, 2008). Schulder et al. (2017) showed that the English NPI any co-occurs with shifters, so its presence in a verb phrase can indicate the presence of a verbal shifter. We expect the same for the German NPI jeglich, as seen in (8). We collect all verbs with a polar direct object that is modified by the lemma jeglich. The resulting pattern matches are sorted by their frequency, normalized over their respective verb frequency and then reranked using Personalised PageRank (Agirre and Soroa, 2009). (8) Sie [verwehrtenshifter uns jegliche [Hilfedobj ]+ ]− . They [deniedshifter us any [helpdobj ]+ ]− . Anti-Shifter Feature (ANTI): This feature specifically targets anti-shifters, verbs that exhibit polar stability instead of causing polar shifting. These are commonly verbs indicating creation or continued existence, such as live, introduce, construct or prepare. Such verbs often co-occur with the adverbs ausschließlich, zuerst, neu and extra, as seen in (9)–(12). Accordingly, we can create a list of anti-shifters by selecting the verbs that most often co-occur with these adverbs. (9) Im W"
C18-1213,P17-1042,0,0.0528078,"Missing"
C18-1213,P98-1013,0,0.735741,"Missing"
C18-1213,burchardt-etal-2006-salsa,0,0.0445694,"ses, called paraphrases in GermaNet, GermaNet offers two variations: the paraphrases originally written for GermaNet, and a more extensive set of paraphrases harvested from Wiktionary (Henrich et al., 2014). To improve coverage we use this paraphrase extension in our experiments. Salsa FrameNet: Framenets provide semantic frames that group words with similar semantic behavior. Schulder et al. (2017) use the frame memberships of verbs as a feature, hypothesizing that verbal shifters will be found in the same frames. We reproduce this feature using frames from the German FrameNet project Salsa (Burchardt et al., 2006). 2520 EffektGermaNet: Wiebe and colleagues (Deng et al., 2013; Choi et al., 2014) introduced the idea that events can have harmful or beneficial effects on their objects. These effects are related but not identical to polarity shifting. Choi et al. (2014) provide lexical information on effects in their English resource EffectWordNet. We use its German counterpart, EffektGermaNet (Ruppenhofer and Brandes, 2015), to model the effect feature in our data. 4.2 New Features In §4.1 we described how we reproduce features already used for English shifter classification. Next we introduce new features"
C18-1213,W14-2618,0,0.365453,"iginally written for GermaNet, and a more extensive set of paraphrases harvested from Wiktionary (Henrich et al., 2014). To improve coverage we use this paraphrase extension in our experiments. Salsa FrameNet: Framenets provide semantic frames that group words with similar semantic behavior. Schulder et al. (2017) use the frame memberships of verbs as a feature, hypothesizing that verbal shifters will be found in the same frames. We reproduce this feature using frames from the German FrameNet project Salsa (Burchardt et al., 2006). 2520 EffektGermaNet: Wiebe and colleagues (Deng et al., 2013; Choi et al., 2014) introduced the idea that events can have harmful or beneficial effects on their objects. These effects are related but not identical to polarity shifting. Choi et al. (2014) provide lexical information on effects in their English resource EffectWordNet. We use its German counterpart, EffektGermaNet (Ruppenhofer and Brandes, 2015), to model the effect feature in our data. 4.2 New Features In §4.1 we described how we reproduce features already used for English shifter classification. Next we introduce new features that have not yet been used for the creation of a verbal shifter lexicon. 4.2.1 B"
C18-1213,N09-1016,0,0.028757,"fter lexicons recently introduced by Schulder et al. (2017) and Schulder et al. (2018). Schulder et al. (2017) automatically bootstrap a lexicon which covers 980 verbal shifters at the lemma level, while Schulder et al. (2018) manually annotate word senses of verbs, creating a lexicon of 2131 shifter senses across 1220 verbs. As we reproduce and extend the work of Schulder et al. (2017), all further use of and comparison to an English shifter lexicon refers to their bootstrapped lexicon as well. To create shifter lexicons at a large scale, automation and bootstrapping techniques are required. Danescu-Niculescu-Mizil et al. (2009) propose using negative polarity items (NPIs) to extract downwardentailing operators, which are closely related to polarity shifters. Schulder et al. (2017) also make use of NPIs in addition to a number of other features. Rather than using lexicons, another approach would be to learn polarity shifters from labelled corpora. In the case of negation, this has already been examined for the biomedical domain (Huang and Lowe, 2007; Morante and Daelemans, 2009; Zou et al., 2013), the review domain (Ikeda et al., 2008; Kessler and Schütze, 2012; Socher et al., 2013; Yu et al., 2016) and across domain"
C18-1213,P13-2022,0,0.213574,"the paraphrases originally written for GermaNet, and a more extensive set of paraphrases harvested from Wiktionary (Henrich et al., 2014). To improve coverage we use this paraphrase extension in our experiments. Salsa FrameNet: Framenets provide semantic frames that group words with similar semantic behavior. Schulder et al. (2017) use the frame memberships of verbs as a feature, hypothesizing that verbal shifters will be found in the same frames. We reproduce this feature using frames from the German FrameNet project Salsa (Burchardt et al., 2006). 2520 EffektGermaNet: Wiebe and colleagues (Deng et al., 2013; Choi et al., 2014) introduced the idea that events can have harmful or beneficial effects on their objects. These effects are related but not identical to polarity shifting. Choi et al. (2014) provide lexical information on effects in their English resource EffectWordNet. We use its German counterpart, EffektGermaNet (Ruppenhofer and Brandes, 2015), to model the effect feature in our data. 4.2 New Features In §4.1 we described how we reproduce features already used for English shifter classification. Next we introduce new features that have not yet been used for the creation of a verbal shif"
C18-1213,P16-1047,0,0.0270048,"opose using negative polarity items (NPIs) to extract downwardentailing operators, which are closely related to polarity shifters. Schulder et al. (2017) also make use of NPIs in addition to a number of other features. Rather than using lexicons, another approach would be to learn polarity shifters from labelled corpora. In the case of negation, this has already been examined for the biomedical domain (Huang and Lowe, 2007; Morante and Daelemans, 2009; Zou et al., 2013), the review domain (Ikeda et al., 2008; Kessler and Schütze, 2012; Socher et al., 2013; Yu et al., 2016) and across domains (Fancellu et al., 2016). Unfortunately, due to the considerably higher lexical diversity of polarity shifters, far larger corpora would be required for learning shifter than for learning negation. Available corpora that are suitable for negation learning, such as the Sentiment Treebank (Socher et al., 2013) or the BioScope corpus (Szarvas et al., 2008), are fairly small in size. Most verbs occur in them in very few instances or not at all. In the BioScope corpus, for example, there are only 6 verbal shifters (Morante, 2010). Polarity classifiers trained on such corpora, such as the state-of-the-art Recursive Neural"
C18-1213,D16-1057,0,0.0269314,"The text corpus was lemmatized using the TreeTagger (Schmid, 1994) and parsed for syntactic dependency structures with ParZu (Sennrich et al., 2009).2 For features requiring knowledge of polarities we use the PolArt Sentiment Lexicon (Klenner et al., 2009).3 2 Lacking an appropriate parser, a part-of-speech tagger may approximate required syntactic structures (Riloff et al., 2013). We chose to consider features that use a polarity lexicon to still be data-driven features as there exist robust methods to generate them automatically from unlabeled corpora (Turney, 2002; Velikovich et al., 2010; Hamilton et al., 2016). The lexicon we use was created using bootstrapping (Clematide and Klenner, 2010). 3 2519 Distributional Similarity (SIM): The distributional similarity feature assumes that words that are semantically similar to negation words are also likely to be polarity shifters. Semantic similarity is modeled as cosine similarity in a word embedding space. The word embeddings are created using Word2Vec (Mikolov et al., 2013) on the German web corpus DeWaC (Baroni et al., 2009), using the same hyperparameters as Schulder et al. (2017) and German translations of their negation seeds. Polarity Clash (CLASH"
C18-1213,W97-0802,0,0.346189,"ers (Morante, 2010). Polarity classifiers trained on such corpora, such as the state-of-the-art Recursive Neural Tensor Network tagger (Socher et al., 2013), fail to detect many instances of polarity shifting. Schulder et al. (2017) show that the explicit knowledge provided by a shifter lexicon can improve polarity classification in such cases. 3 Data We create a gold standard for German verbal shifters, following the approach Schulder et al. (2017) used for their English gold standard. An expert annotator, who is a native speaker of German, labeled 2000 verbs, randomly sampled from GermaNet (Hamp and Feldweg, 1997), a German wordnet resource. The remaining 7262 GermaNet verbs are used to bootstrap a larger lexicon in §5.3. Each verb is assigned a binary label of being a shifter or not. To qualify as a shifter, a verb must permit polar expressions as its dependents and cause the polarity of the expression that embeds both verb and polar expression to move towards the opposite of the polar expression. For example, in (6) verhindern shifts the negative polarity of its dependent ein Gemetzel, resulting in a positive expression. Annotation is performed at the lemma level, as word-sense disambiguation tends t"
C18-1213,I08-1039,0,0.0360223,"rge scale, automation and bootstrapping techniques are required. Danescu-Niculescu-Mizil et al. (2009) propose using negative polarity items (NPIs) to extract downwardentailing operators, which are closely related to polarity shifters. Schulder et al. (2017) also make use of NPIs in addition to a number of other features. Rather than using lexicons, another approach would be to learn polarity shifters from labelled corpora. In the case of negation, this has already been examined for the biomedical domain (Huang and Lowe, 2007; Morante and Daelemans, 2009; Zou et al., 2013), the review domain (Ikeda et al., 2008; Kessler and Schütze, 2012; Socher et al., 2013; Yu et al., 2016) and across domains (Fancellu et al., 2016). Unfortunately, due to the considerably higher lexical diversity of polarity shifters, far larger corpora would be required for learning shifter than for learning negation. Available corpora that are suitable for negation learning, such as the Sentiment Treebank (Socher et al., 2013) or the BioScope corpus (Szarvas et al., 2008), are fairly small in size. Most verbs occur in them in very few instances or not at all. In the BioScope corpus, for example, there are only 6 verbal shifters"
C18-1213,C12-2056,0,0.0175194,"n and bootstrapping techniques are required. Danescu-Niculescu-Mizil et al. (2009) propose using negative polarity items (NPIs) to extract downwardentailing operators, which are closely related to polarity shifters. Schulder et al. (2017) also make use of NPIs in addition to a number of other features. Rather than using lexicons, another approach would be to learn polarity shifters from labelled corpora. In the case of negation, this has already been examined for the biomedical domain (Huang and Lowe, 2007; Morante and Daelemans, 2009; Zou et al., 2013), the review domain (Ikeda et al., 2008; Kessler and Schütze, 2012; Socher et al., 2013; Yu et al., 2016) and across domains (Fancellu et al., 2016). Unfortunately, due to the considerably higher lexical diversity of polarity shifters, far larger corpora would be required for learning shifter than for learning negation. Available corpora that are suitable for negation learning, such as the Sentiment Treebank (Socher et al., 2013) or the BioScope corpus (Szarvas et al., 2008), are fairly small in size. Most verbs occur in them in very few instances or not at all. In the BioScope corpus, for example, there are only 6 verbal shifters (Morante, 2010). Polarity c"
C18-1213,W09-4635,0,0.0250517,"s (§4.1.1) and those that require complex semantic resources (§4.1.2). When working with languages with scarcer resources, it can be expected that the former will be more readily available than the latter. 4.1.1 Data-driven Features The main requirement of the following features is a reasonably sized text corpus to detect syntactic patterns and word frequencies. The text corpus was lemmatized using the TreeTagger (Schmid, 1994) and parsed for syntactic dependency structures with ParZu (Sennrich et al., 2009).2 For features requiring knowledge of polarities we use the PolArt Sentiment Lexicon (Klenner et al., 2009).3 2 Lacking an appropriate parser, a part-of-speech tagger may approximate required syntactic structures (Riloff et al., 2013). We chose to consider features that use a polarity lexicon to still be data-driven features as there exist robust methods to generate them automatically from unlabeled corpora (Turney, 2002; Velikovich et al., 2010; Hamilton et al., 2016). The lexicon we use was created using bootstrapping (Clematide and Klenner, 2010). 3 2519 Distributional Similarity (SIM): The distributional similarity feature assumes that words that are semantically similar to negation words are a"
C18-1213,W09-1105,0,0.0233824,"ir bootstrapped lexicon as well. To create shifter lexicons at a large scale, automation and bootstrapping techniques are required. Danescu-Niculescu-Mizil et al. (2009) propose using negative polarity items (NPIs) to extract downwardentailing operators, which are closely related to polarity shifters. Schulder et al. (2017) also make use of NPIs in addition to a number of other features. Rather than using lexicons, another approach would be to learn polarity shifters from labelled corpora. In the case of negation, this has already been examined for the biomedical domain (Huang and Lowe, 2007; Morante and Daelemans, 2009; Zou et al., 2013), the review domain (Ikeda et al., 2008; Kessler and Schütze, 2012; Socher et al., 2013; Yu et al., 2016) and across domains (Fancellu et al., 2016). Unfortunately, due to the considerably higher lexical diversity of polarity shifters, far larger corpora would be required for learning shifter than for learning negation. Available corpora that are suitable for negation learning, such as the Sentiment Treebank (Socher et al., 2013) or the BioScope corpus (Szarvas et al., 2008), are fairly small in size. Most verbs occur in them in very few instances or not at all. In the BioSc"
C18-1213,morante-2010-descriptive,0,0.0269621,"Kessler and Schütze, 2012; Socher et al., 2013; Yu et al., 2016) and across domains (Fancellu et al., 2016). Unfortunately, due to the considerably higher lexical diversity of polarity shifters, far larger corpora would be required for learning shifter than for learning negation. Available corpora that are suitable for negation learning, such as the Sentiment Treebank (Socher et al., 2013) or the BioScope corpus (Szarvas et al., 2008), are fairly small in size. Most verbs occur in them in very few instances or not at all. In the BioScope corpus, for example, there are only 6 verbal shifters (Morante, 2010). Polarity classifiers trained on such corpora, such as the state-of-the-art Recursive Neural Tensor Network tagger (Socher et al., 2013), fail to detect many instances of polarity shifting. Schulder et al. (2017) show that the explicit knowledge provided by a shifter lexicon can improve polarity classification in such cases. 3 Data We create a gold standard for German verbal shifters, following the approach Schulder et al. (2017) used for their English gold standard. An expert annotator, who is a native speaker of German, labeled 2000 verbs, randomly sampled from GermaNet (Hamp and Feldweg, 1"
C18-1213,P10-1114,0,0.053544,"Missing"
C18-1213,D13-1066,0,0.0859941,"Missing"
C18-1213,W15-2910,1,0.856892,"the frame memberships of verbs as a feature, hypothesizing that verbal shifters will be found in the same frames. We reproduce this feature using frames from the German FrameNet project Salsa (Burchardt et al., 2006). 2520 EffektGermaNet: Wiebe and colleagues (Deng et al., 2013; Choi et al., 2014) introduced the idea that events can have harmful or beneficial effects on their objects. These effects are related but not identical to polarity shifting. Choi et al. (2014) provide lexical information on effects in their English resource EffectWordNet. We use its German counterpart, EffektGermaNet (Ruppenhofer and Brandes, 2015), to model the effect feature in our data. 4.2 New Features In §4.1 we described how we reproduce features already used for English shifter classification. Next we introduce new features that have not yet been used for the creation of a verbal shifter lexicon. 4.2.1 Bilingual Dictionary The motivation behind the work of Schulder et al. (2017) was to introduce a large lexicon of verbal polarity shifters. Now that such a lexicon exists for English, it is an obvious resource to use when creating verbal shifter lexicons for other languages. We hypothesize that a verb with the same meaning as an En"
C18-1213,S16-1084,0,0.0141556,"To verify that expectation, we apply their approach to German, for which all resources required to reproduce their experiments are available. Keeping in mind that this is not the case for many other languages, we focus our evaluation on differentiating between features that rely on unstructured data and those requiring rare semantic resources. While polarity shifters are not restricted to a particular part of speech – shifter nouns (e.g. downfall), adjectives (devoid) and adverbs (barely) also exist – we limit ourselves to verbs. Verbs and nouns are the most important minimal semantic units (Schneider et al., 2016) and verbs are usually the main syntactic 2517 predicates of clauses, projecting far-reaching scopes. Focusing on verbs also allows us a closer comparison with Schulder et al. (2017) and to investigate cross-lingual similarities between verbal shifters. The contributions of this paper are: (i) we introduce a German lexicon of verbal polarity shifters; (ii) we reproduce and adapt the approach of Schulder et al. (2017) to German to extend our lexicon; (iii) we introduce additional methods that take advantage of the existence of the English verbal polarity shifter lexicon and improve upon the cur"
C18-1213,I17-1063,1,0.267253,"hifter ]− . She was [deniedshifter the [scholarship]+ ]− . (4) Die neue Behandlung hat ihre [[Schmerzen]− gelindertshifter ]+ . The new treatment has [alleviatedshifter her [pain]− ]+ . As can be seen for verhindern/prevent in (5) and (6), the same shifter can even affect both positive and negative expressions. (5) Seine Prinzipien [verhindertenshifter eine [Einigung]+ ]− . His principles [preventedshifter an [agreement]+ ]− . (6) Ihre Maßnahmen [verhindertenshifter ein [Gemetzel]− ]+ . Their measures [preventedshifter a [slaughter]− ]+ . We present a reproduction and extension to the work of Schulder et al. (2017), which introduced a lexicon of verbal polarity shifters, as well as methods to increase the size of this lexicon through bootstrapping. The lexicon lists verb lemmas and assigns a binary label (shifter or no shifter) to each. The original approach was developed on English. We apply it to German, validating the generality of the approach and creating a new resource, a German lexicon of 677 verbal polarity shifters. We also improve the bootstrapping process by adding features that leverage polarity shifter resources across languages. As is the case with negation, modeling polarity shifting is i"
C18-1213,L18-1222,1,0.733403,"ng German lexicon of 677 verbal polarity shifters is made publicly available.1 2 Related Work Existing work on negation modeling focuses almost exclusively on negation words (see the survey of Wiegand et al. (2010)). One reason for this is the lack of lexicons and corpora that cover other forms of polarity shifters. Even the most complex negation lexicon for English sentiment analysis (Wilson et al., 2005) includes a mere 12 verbal shifters. So far the only larger resources for polarity shifters are the English-language verbal shifter lexicons recently introduced by Schulder et al. (2017) and Schulder et al. (2018). Schulder et al. (2017) automatically bootstrap a lexicon which covers 980 verbal shifters at the lemma level, while Schulder et al. (2018) manually annotate word senses of verbs, creating a lexicon of 2131 shifter senses across 1220 verbs. As we reproduce and extend the work of Schulder et al. (2017), all further use of and comparison to an English shifter lexicon refers to their bootstrapped lexicon as well. To create shifter lexicons at a large scale, automation and bootstrapping techniques are required. Danescu-Niculescu-Mizil et al. (2009) propose using negative polarity items (NPIs) to"
C18-1213,D13-1170,0,0.0124462,"ues are required. Danescu-Niculescu-Mizil et al. (2009) propose using negative polarity items (NPIs) to extract downwardentailing operators, which are closely related to polarity shifters. Schulder et al. (2017) also make use of NPIs in addition to a number of other features. Rather than using lexicons, another approach would be to learn polarity shifters from labelled corpora. In the case of negation, this has already been examined for the biomedical domain (Huang and Lowe, 2007; Morante and Daelemans, 2009; Zou et al., 2013), the review domain (Ikeda et al., 2008; Kessler and Schütze, 2012; Socher et al., 2013; Yu et al., 2016) and across domains (Fancellu et al., 2016). Unfortunately, due to the considerably higher lexical diversity of polarity shifters, far larger corpora would be required for learning shifter than for learning negation. Available corpora that are suitable for negation learning, such as the Sentiment Treebank (Socher et al., 2013) or the BioScope corpus (Szarvas et al., 2008), are fairly small in size. Most verbs occur in them in very few instances or not at all. In the BioScope corpus, for example, there are only 6 verbal shifters (Morante, 2010). Polarity classifiers trained on"
C18-1213,W08-0606,0,0.0287512,"se of negation, this has already been examined for the biomedical domain (Huang and Lowe, 2007; Morante and Daelemans, 2009; Zou et al., 2013), the review domain (Ikeda et al., 2008; Kessler and Schütze, 2012; Socher et al., 2013; Yu et al., 2016) and across domains (Fancellu et al., 2016). Unfortunately, due to the considerably higher lexical diversity of polarity shifters, far larger corpora would be required for learning shifter than for learning negation. Available corpora that are suitable for negation learning, such as the Sentiment Treebank (Socher et al., 2013) or the BioScope corpus (Szarvas et al., 2008), are fairly small in size. Most verbs occur in them in very few instances or not at all. In the BioScope corpus, for example, there are only 6 verbal shifters (Morante, 2010). Polarity classifiers trained on such corpora, such as the state-of-the-art Recursive Neural Tensor Network tagger (Socher et al., 2013), fail to detect many instances of polarity shifting. Schulder et al. (2017) show that the explicit knowledge provided by a shifter lexicon can improve polarity classification in such cases. 3 Data We create a gold standard for German verbal shifters, following the approach Schulder et a"
C18-1213,P02-1053,0,0.027394,"ntactic patterns and word frequencies. The text corpus was lemmatized using the TreeTagger (Schmid, 1994) and parsed for syntactic dependency structures with ParZu (Sennrich et al., 2009).2 For features requiring knowledge of polarities we use the PolArt Sentiment Lexicon (Klenner et al., 2009).3 2 Lacking an appropriate parser, a part-of-speech tagger may approximate required syntactic structures (Riloff et al., 2013). We chose to consider features that use a polarity lexicon to still be data-driven features as there exist robust methods to generate them automatically from unlabeled corpora (Turney, 2002; Velikovich et al., 2010; Hamilton et al., 2016). The lexicon we use was created using bootstrapping (Clematide and Klenner, 2010). 3 2519 Distributional Similarity (SIM): The distributional similarity feature assumes that words that are semantically similar to negation words are also likely to be polarity shifters. Semantic similarity is modeled as cosine similarity in a word embedding space. The word embeddings are created using Word2Vec (Mikolov et al., 2013) on the German web corpus DeWaC (Baroni et al., 2009), using the same hyperparameters as Schulder et al. (2017) and German translatio"
C18-1213,P16-1157,0,0.0271813,"Missing"
C18-1213,N10-1119,0,0.0378265,"ns and word frequencies. The text corpus was lemmatized using the TreeTagger (Schmid, 1994) and parsed for syntactic dependency structures with ParZu (Sennrich et al., 2009).2 For features requiring knowledge of polarities we use the PolArt Sentiment Lexicon (Klenner et al., 2009).3 2 Lacking an appropriate parser, a part-of-speech tagger may approximate required syntactic structures (Riloff et al., 2013). We chose to consider features that use a polarity lexicon to still be data-driven features as there exist robust methods to generate them automatically from unlabeled corpora (Turney, 2002; Velikovich et al., 2010; Hamilton et al., 2016). The lexicon we use was created using bootstrapping (Clematide and Klenner, 2010). 3 2519 Distributional Similarity (SIM): The distributional similarity feature assumes that words that are semantically similar to negation words are also likely to be polarity shifters. Semantic similarity is modeled as cosine similarity in a word embedding space. The word embeddings are created using Word2Vec (Mikolov et al., 2013) on the German web corpus DeWaC (Baroni et al., 2009), using the same hyperparameters as Schulder et al. (2017) and German translations of their negation seed"
C18-1213,W10-3111,1,0.958229,"(shifter or no shifter) to each. The original approach was developed on English. We apply it to German, validating the generality of the approach and creating a new resource, a German lexicon of 677 verbal polarity shifters. We also improve the bootstrapping process by adding features that leverage polarity shifter resources across languages. As is the case with negation, modeling polarity shifting is important for various tasks in NLP, such as relation extraction (Sanchez-Graillet and Poesio, 2007), recognition of textual entailment (Harabagiu et al., 2006) and especially sentiment analysis (Wiegand et al., 2010). However, while there has been significant research on negation in sentiment analysis (Wiegand et al., 2010), current classifiers fail to handle polarity shifters adequately (Schulder et al., 2017). This is in part due to the lack of lexical resources for polarity shifters. Unlike negation words (no, not, never, etc.), of which there are only a few dozen in a language, polarity shifters are far more numerous. Among verbs alone there are many hundreds (Schulder et al., 2017). Comprehensive shifter lexicons are, therefore, considerably more expensive to create. Once available, they can be used"
C18-1213,H05-2018,0,0.0976251,"f the existence of the English verbal polarity shifter lexicon and improve upon the current state of the art. The focus of our work is the binary classification of verbal polarity shifters in German. The resulting German lexicon of 677 verbal polarity shifters is made publicly available.1 2 Related Work Existing work on negation modeling focuses almost exclusively on negation words (see the survey of Wiegand et al. (2010)). One reason for this is the lack of lexicons and corpora that cover other forms of polarity shifters. Even the most complex negation lexicon for English sentiment analysis (Wilson et al., 2005) includes a mere 12 verbal shifters. So far the only larger resources for polarity shifters are the English-language verbal shifter lexicons recently introduced by Schulder et al. (2017) and Schulder et al. (2018). Schulder et al. (2017) automatically bootstrap a lexicon which covers 980 verbal shifters at the lemma level, while Schulder et al. (2018) manually annotate word senses of verbs, creating a lexicon of 2131 shifter senses across 1220 verbs. As we reproduce and extend the work of Schulder et al. (2017), all further use of and comparison to an English shifter lexicon refers to their bo"
C18-1213,D13-1099,0,0.0196305,"ll. To create shifter lexicons at a large scale, automation and bootstrapping techniques are required. Danescu-Niculescu-Mizil et al. (2009) propose using negative polarity items (NPIs) to extract downwardentailing operators, which are closely related to polarity shifters. Schulder et al. (2017) also make use of NPIs in addition to a number of other features. Rather than using lexicons, another approach would be to learn polarity shifters from labelled corpora. In the case of negation, this has already been examined for the biomedical domain (Huang and Lowe, 2007; Morante and Daelemans, 2009; Zou et al., 2013), the review domain (Ikeda et al., 2008; Kessler and Schütze, 2012; Socher et al., 2013; Yu et al., 2016) and across domains (Fancellu et al., 2016). Unfortunately, due to the considerably higher lexical diversity of polarity shifters, far larger corpora would be required for learning shifter than for learning negation. Available corpora that are suitable for negation learning, such as the Sentiment Treebank (Socher et al., 2013) or the BioScope corpus (Szarvas et al., 2008), are fairly small in size. Most verbs occur in them in very few instances or not at all. In the BioScope corpus, for exa"
C18-1325,J90-1003,0,0.532214,"et al., 2016) in its default setting to train word embeddings on the SdeWaC-corpus (Faaß and Eckart, 2013), which contains about 880 million tokens and is a cleaned up version of the deWaC corpus (Baroni et al., 2009). The choice of fastText was motivated by the fact that fastText computes vectors for words by adding up the vectors for n-grams found in the words, which allows us to produce vectors for words not seen in the training data. Since many of our complex forms are (near-)hapaxes, this is a crucial benefit of fastText. Pointwise mutual information. We use Pointwise mutual information (Church and Hanks, 1990) to capture the level of association between the two components of the complex word. The expectation is that the components of regular compounds exhibit higher PMI-scores than the components of a complex word involving an affixoid. This is motivated by Tellenbach (1985)’s observation that for complex forms containing an affixoid use, paraphrases containing the morpheme in question as a free word are unlikely. By contrast, compositional compounds are often paraphrased using their components. As an example, the compositional compound Perserk¨onig is readily paraphrased as K¨onig der Perser ‘king"
C18-1325,W14-5805,0,0.0133934,"icular set of suffixoid candidates differences between the second component and the complex word’s supersenses may be particularly important, we experiment with an alternative set of supersense features (supersenses diffs): we use a series of indicator variables that code whether the second component and the complex word differ in their value for a given supersense.7 Polarity. Since affixoid uses are likely to have evaluative meanings, we explore whether this is reflected in the polarity of the two components and the complex form. We extract polarity information for all three from SentiMerge (Emerson and Declerck, 2014). With 96,918 entries, it is to date the largest available polarity lexicon for German. SentiMerge was created by harmonizing and combining three smaller lexicons (PolArt (Klenner et al., 2009); GermanPolarityClues (Waltinger, 2010); and SentiWS (Remus et al., 2010)) using a Bayesian probabilistic model. Psycholinguistic features. If available, we extract psycholinguistic ratings along four dimensions for the whole word and its components. This type of feature has been successfully used in various tasks, such as identifying metaphors (Turney et al., 2011; Klebanov et al., 2014); studying persu"
C18-1325,faass-etal-2010-design,0,0.066989,"Missing"
C18-1325,N09-1002,0,0.0355169,"Missing"
C18-1325,W97-0802,0,0.154792,"Missing"
C18-1325,W14-2302,0,0.0139202,"entiMerge (Emerson and Declerck, 2014). With 96,918 entries, it is to date the largest available polarity lexicon for German. SentiMerge was created by harmonizing and combining three smaller lexicons (PolArt (Klenner et al., 2009); GermanPolarityClues (Waltinger, 2010); and SentiWS (Remus et al., 2010)) using a Bayesian probabilistic model. Psycholinguistic features. If available, we extract psycholinguistic ratings along four dimensions for the whole word and its components. This type of feature has been successfully used in various tasks, such as identifying metaphors (Turney et al., 2011; Klebanov et al., 2014); studying persuasion (Tan et al., 2016); sarcasm detection (Bamman and Smith, 2015); and, most similar to us, polarity prediction for complex words (Ruppenhofer et al., 2017). The first dimension places words on a scale from abstract to concrete (abstconc). Abstract words refer to things that we cannot perceive directly with 7 Note that for other suffixoids not covered here such as Papst (lit. ‘pope’, suffixoid ‘expert’) and Nest (lit. ‘nest’, affixoid ‘den/hideout’) there is no difference at all between the supersenses of the second component and the complex word. 3858 our senses (integer, p"
C18-1325,W09-4635,0,0.0733123,"res (supersenses diffs): we use a series of indicator variables that code whether the second component and the complex word differ in their value for a given supersense.7 Polarity. Since affixoid uses are likely to have evaluative meanings, we explore whether this is reflected in the polarity of the two components and the complex form. We extract polarity information for all three from SentiMerge (Emerson and Declerck, 2014). With 96,918 entries, it is to date the largest available polarity lexicon for German. SentiMerge was created by harmonizing and combining three smaller lexicons (PolArt (Klenner et al., 2009); GermanPolarityClues (Waltinger, 2010); and SentiWS (Remus et al., 2010)) using a Bayesian probabilistic model. Psycholinguistic features. If available, we extract psycholinguistic ratings along four dimensions for the whole word and its components. This type of feature has been successfully used in various tasks, such as identifying metaphors (Turney et al., 2011; Klebanov et al., 2014); studying persuasion (Tan et al., 2016); sarcasm detection (Bamman and Smith, 2015); and, most similar to us, polarity prediction for complex words (Ruppenhofer et al., 2017). The first dimension places words"
C18-1325,L16-1413,0,0.0555122,"Missing"
C18-1325,P08-2028,0,0.0918526,"Missing"
C18-1325,remus-etal-2010-sentiws,0,0.0213281,"hether the second component and the complex word differ in their value for a given supersense.7 Polarity. Since affixoid uses are likely to have evaluative meanings, we explore whether this is reflected in the polarity of the two components and the complex form. We extract polarity information for all three from SentiMerge (Emerson and Declerck, 2014). With 96,918 entries, it is to date the largest available polarity lexicon for German. SentiMerge was created by harmonizing and combining three smaller lexicons (PolArt (Klenner et al., 2009); GermanPolarityClues (Waltinger, 2010); and SentiWS (Remus et al., 2010)) using a Bayesian probabilistic model. Psycholinguistic features. If available, we extract psycholinguistic ratings along four dimensions for the whole word and its components. This type of feature has been successfully used in various tasks, such as identifying metaphors (Turney et al., 2011; Klebanov et al., 2014); studying persuasion (Tan et al., 2016); sarcasm detection (Bamman and Smith, 2015); and, most similar to us, polarity prediction for complex words (Ruppenhofer et al., 2017). The first dimension places words on a scale from abstract to concrete (abstconc). Abstract words refer to"
C18-1325,ruppenhofer-etal-2017-evaluating,1,0.803388,"ining three smaller lexicons (PolArt (Klenner et al., 2009); GermanPolarityClues (Waltinger, 2010); and SentiWS (Remus et al., 2010)) using a Bayesian probabilistic model. Psycholinguistic features. If available, we extract psycholinguistic ratings along four dimensions for the whole word and its components. This type of feature has been successfully used in various tasks, such as identifying metaphors (Turney et al., 2011; Klebanov et al., 2014); studying persuasion (Tan et al., 2016); sarcasm detection (Bamman and Smith, 2015); and, most similar to us, polarity prediction for complex words (Ruppenhofer et al., 2017). The first dimension places words on a scale from abstract to concrete (abstconc). Abstract words refer to things that we cannot perceive directly with 7 Note that for other suffixoids not covered here such as Papst (lit. ‘pope’, suffixoid ‘expert’) and Nest (lit. ‘nest’, affixoid ‘den/hideout’) there is no difference at all between the supersenses of the second component and the complex word. 3858 our senses (integer, politics, . . . ) whereas concrete words refer to things we can perceive (sound, scent, . . . ). The second dimension concerns imageability (img). A large subset of concrete wo"
C18-1325,schmid-etal-2004-smor,0,0.0612312,"Missing"
C18-1325,S13-1038,0,0.0555683,"Missing"
C18-1325,N09-1001,1,0.79684,"Missing"
C18-1325,S14-2033,0,0.0245143,"unishment). The final dimension, arousal, represents the intensity of emotion caused by a stimulus (alert vs. calm).8 We obtain affective ratings from the resource of K¨oper and Schulte im Walde (2016). It provides information on 350k words and is far more comprehensive than the affective norm data of Kanske and Kotz (2010) or Lahl et al. (2009). It is also much larger than commonly used polarity lexicons for German such as PolArt (Klenner et al., 2009) or GermanPolarityClues (Waltinger, 2010). Emotion. Since emotion information is commonly used in sentiment-related classification tasks (e.g. Tang et al. (2014), Sulis et al. (2016)), we wanted to see to what extent emotion information could benefit our task. For this purpose, we use the NRC Word-Emotion Association Lexicon (EmoLex) for English which was created by Mohammad and Turner (2013) using a crowdsourcing approach. EmoLex contains binary associations of words with the eight basic emotions (joy, sadness, anger, fear, disgust, surprise, trust, anticipation) of Plutchik (1962) . Although the German version of the lexicon was produced using machine translation, we use it here because we do not have a similarly large natively produced resource ava"
C18-1325,D11-1063,0,0.0294224,"for all three from SentiMerge (Emerson and Declerck, 2014). With 96,918 entries, it is to date the largest available polarity lexicon for German. SentiMerge was created by harmonizing and combining three smaller lexicons (PolArt (Klenner et al., 2009); GermanPolarityClues (Waltinger, 2010); and SentiWS (Remus et al., 2010)) using a Bayesian probabilistic model. Psycholinguistic features. If available, we extract psycholinguistic ratings along four dimensions for the whole word and its components. This type of feature has been successfully used in various tasks, such as identifying metaphors (Turney et al., 2011; Klebanov et al., 2014); studying persuasion (Tan et al., 2016); sarcasm detection (Bamman and Smith, 2015); and, most similar to us, polarity prediction for complex words (Ruppenhofer et al., 2017). The first dimension places words on a scale from abstract to concrete (abstconc). Abstract words refer to things that we cannot perceive directly with 7 Note that for other suffixoids not covered here such as Papst (lit. ‘pope’, suffixoid ‘expert’) and Nest (lit. ‘nest’, affixoid ‘den/hideout’) there is no difference at all between the supersenses of the second component and the complex word. 385"
C18-1325,waltinger-2010-germanpolarityclues,0,0.213529,"indicator variables that code whether the second component and the complex word differ in their value for a given supersense.7 Polarity. Since affixoid uses are likely to have evaluative meanings, we explore whether this is reflected in the polarity of the two components and the complex form. We extract polarity information for all three from SentiMerge (Emerson and Declerck, 2014). With 96,918 entries, it is to date the largest available polarity lexicon for German. SentiMerge was created by harmonizing and combining three smaller lexicons (PolArt (Klenner et al., 2009); GermanPolarityClues (Waltinger, 2010); and SentiWS (Remus et al., 2010)) using a Bayesian probabilistic model. Psycholinguistic features. If available, we extract psycholinguistic ratings along four dimensions for the whole word and its components. This type of feature has been successfully used in various tasks, such as identifying metaphors (Turney et al., 2011; Klebanov et al., 2014); studying persuasion (Tan et al., 2016); sarcasm detection (Bamman and Smith, 2015); and, most similar to us, polarity prediction for complex words (Ruppenhofer et al., 2017). The first dimension places words on a scale from abstract to concrete ("
C18-1325,J04-3002,0,0.15836,"affixoids for the purposes of sentiment analysis. On the one hand, theoretical linguistic work that notes the expressive function of affixoids such as Meibauer This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 3853 Proceedings of the 27th International Conference on Computational Linguistics, pages 3853–3865 Santa Fe, New Mexico, USA, August 20-26, 2018. (2013) suggests this. On the other hand, it is known from prior research on sentiment analysis that hapax words in general are often subjective (Wiebe et al., 2004). As we show in §3, affixoids tend to generate many (near-)hapax forms, which meshes with observations on their productivity in the morphological literature and the general expectation about the Zipfian distribution of word frequencies. Since hapaxes, by definition, cannot be readily analyzed based on their distribution in corpora, it would be very useful if we could make use of their intrinsic properties to classify such forms as affixoid uses (and therefore likely subjective) or not. The main task that we set ourselves in this paper is morpheme sense disambiguation: we want to classify compl"
C18-1325,N16-1094,1,0.855997,"e our complex forms are unlikely to be listed in lexical resources, we will usually lack information such as glosses, supersenses or example sentences for them. 1 https://github.com/josefkr/affixoids https://de.wiktionary.org 3 http://wortwarte.de/ 4 This assumption can be made plausible by the observations that longer words have more specific meanings than shorter ones, and that they tend to have fewer meanings than their head words, and fewer meanings than their components do on average (Altmann, 2002). 2 3854 In another strand of research involving German morphology and sentiment analysis, Wiegand et al. (2016) developed an approach to classify the first element of German compounds as expressing either the source or target of evaluation, or neither, relative to the second element, if in the first step of analysis the second element was determined to be subjective. As do we, those authors focused on noun-noun compounds and they did not address polarity classification. However, their approach targets higher frequency words as it relies on the availability of sufficient corpus data to enable the use of distributional similarity. For our dataset, we cannot directly model the distributional properties of"
clematide-etal-2012-mlsa,brants-hansen-2002-developments,0,\N,Missing
clematide-etal-2012-mlsa,H05-1116,0,\N,Missing
clematide-etal-2012-mlsa,H05-1044,0,\N,Missing
clematide-etal-2012-mlsa,P10-1059,0,\N,Missing
clematide-etal-2012-mlsa,P07-1056,0,\N,Missing
clematide-etal-2012-mlsa,P05-1015,0,\N,Missing
clematide-etal-2012-mlsa,W02-1011,0,\N,Missing
clematide-etal-2012-mlsa,R11-1028,0,\N,Missing
E12-1033,P08-1034,0,0.0700927,"Missing"
E12-1033,J92-4003,0,0.0657639,"V. Madrid, Dresden, Bordeaux, Istanbul, Caracas, Manila, ... Toby, Betsy, Michele, Tim, Jean-Marie, Rory, Andrew, ... detest, resent, imply, liken, indicate, suggest, owe, expect, ... disappointment, unease, nervousness, dismay, optimism, ... remark, baby, book, saint, manhole, maxim, coin, batter, ... Table 2: Some automatically induced clusters. ETHICS 1.47 SPACE 2.70 FICTION 11.59 Table 3: Percentage of opinion holders as patients. al., 2010). Such a generalization is, in particular, attractive as it is cheaply produced. As a stateof-the-art clustering method, we consider Brown clustering (Brown et al., 1992) as implemented in the SRILM-toolkit (Stolcke, 2002). We induced 1000 clusters which is also the configuration used in (Turian et al., 2010).5 Table 2 illustrates a few of the clusters induced from our unlabeled dataset introduced in Section (§) 2. Some of these clusters represent location or person names (e.g. I. & II.). This exemplifies why clustering is effective for named-entity recognition. We also find clusters that intuitively seem to be meaningful for our task (e.g. III. & IV.) but, on the other hand, there are clusters that contain words that with the exception of their part of speech"
E12-1033,H05-1045,0,0.817911,"including the same generalization methods. 4.1 Conditional Random Fields (CRF) The supervised classifier most frequently used for information extraction tasks, in general, are conditional random fields (CRF) (Lafferty et al., 2001). Using CRF, the task of opinion holder extraction is framed as a tagging problem in which given a sequence of observations x = x1 x2 . . . xn (words in a sentence) a sequence of output tags y = y1 y2 . . . yn indicating the boundaries of opinion holders is computed by modeling the conditional probability P (x|y). The features we use (Table 5) are mostly inspired by Choi et al. (2005) and by the ones used for plain support vector machines (SVMs) in (Wiegand and Klakow, 2010). They are organized into groups. The basic group Plain does not contain any generalization method. Each other group is dedicated to one specific generalization method that we want to examine (Clus, Induc and Lex). Apart from considering generalization features indicating the presence of generalization types, we also consider those types in conjunction with semantic roles. As already indicated above, semantic roles are especially important for the detection of opinion holders. Unfortunately, the cor328"
E12-1033,W06-1651,0,0.0725387,"on holders that RB misses. CK has the advantage that it is not only bound to the relationship between candidate holder and predicate. It learns further heuristics, e.g. that sentence-initial mentions of persons are likely opinion holders. In (12), for example, this heuristics fires while RB overlooks this instance as to give someone a share of advice is not part of the lexicon. (12) She later gives Charlotte her share of advice on running a household. 7 Related Work The research on opinion holder extraction has been focusing on applying different data-driven approaches. Choi et al. (2005) and Choi et al. (2006) explore conditional random fields, Wiegand and Klakow (2010) examine different combinations of convolution kernels, while Johansson and Moschitti (2010) present a re-ranking approach modeling complex relations between multiple opinions in a sentence. A comparison of 332 Features SPACE (similar target domain) CRF CK Prec Rec F1 Prec Rec Plain +Clus +Induc +Lex +Clus+Induc +Lex+Induc +Clus+Lex All 47.32 49.00 42.92 49.65 46.61 48.75 49.72 49.87 48.62 48.62 49.15 49.07 48.78 50.87 50.87 51.03 47.96 48.81 45.82 49.36 47.67 49.78 50.29 50.44 45.89 49.23 46.66 49.60 48.65 49.92 53.70 51.68 57.07 57"
E12-1033,P05-1045,0,0.0610266,"or example, it would correspond to A1 shock. Therefore, we introduce for each generalization method an additional feature replacing the sparse lexical item by a generalization label, i.e. Clus: A1 CLUSTER-35265, Induc: A1 INDUC-PRED and Lex: A1 LEX-PRED.6 For this learning method, we use CRF++.7 We choose a configuration that provides good performance on our source domain (i.e. ETHICS).8 For semantic role labeling we use SWIRL9 , for chunk parsing CASS (Abney, 1991) and for constituency parsing Stanford Parser (Klein and Manning, 2003). Named-entity information is provided by Stanford Tagger (Finkel et al., 2005). convolution kernels, the structures to be compared within the kernel function are not vectors comprising manually designed features but the underlying discrete structures, such as syntactic parse trees or part-of-speech sequences. Since they are directly provided to the learning algorithm, a classifier can be built without taking the effort of implementing an explicit feature extraction. We take the best configuration from (Wiegand and Klakow, 2010) that comprises a combination of three different tree kernels being two tree kernels based on constituency parse trees (one with predicate and an"
E12-1033,C10-1059,0,0.042935,"output of clustering will exclusively be evaluated in the context of learning-based methods, since there is no straightforward way of incorporating this output into a rule-based classifier. 6 Experiments CK and RB have an instance space that is different from the one of CRF. While CRF produces a prediction for every word token in a sentence, CK and RB only produce a prediction for every noun phrase. For evaluation, we project the predictions from RB and CK to word token level in order to ensure comparability. We evaluate the sequential output with precision, recall and F-score as defined in (Johansson and Moschitti, 2010; Johansson and Moschitti, 2011). 6.1 Rule-based Classifier Table 6 shows the cross-domain performance of the different rule-based classifiers. RB-Lex performs better than RB-Induc. In comparison to the domains ETHICS and SPACE the difference is larger on FICTION. Presumably, this is due to the fact that the predicates in Induc are extracted from a news corpus (§2). Thus, Induc may slightly suffer from a domain mismatch. A combination of the two classifiers, i.e. RB-Lex+Induc, results in a notable improvement in the FICTION-domain. The approaches that also detect opinion holders as patients (A"
E12-1033,P11-2018,0,0.0146717,"usively be evaluated in the context of learning-based methods, since there is no straightforward way of incorporating this output into a rule-based classifier. 6 Experiments CK and RB have an instance space that is different from the one of CRF. While CRF produces a prediction for every word token in a sentence, CK and RB only produce a prediction for every noun phrase. For evaluation, we project the predictions from RB and CK to word token level in order to ensure comparability. We evaluate the sequential output with precision, recall and F-score as defined in (Johansson and Moschitti, 2010; Johansson and Moschitti, 2011). 6.1 Rule-based Classifier Table 6 shows the cross-domain performance of the different rule-based classifiers. RB-Lex performs better than RB-Induc. In comparison to the domains ETHICS and SPACE the difference is larger on FICTION. Presumably, this is due to the fact that the predicates in Induc are extracted from a news corpus (§2). Thus, Induc may slightly suffer from a domain mismatch. A combination of the two classifiers, i.e. RB-Lex+Induc, results in a notable improvement in the FICTION-domain. The approaches that also detect opinion holders as patients (AG+PT) including our novel approa"
E12-1033,W06-0301,0,0.0508218,"luated on the MPQA corpus. Some generalization methods are incorporated but unlike this paper they are neither systematically compared nor combined. The role of resources that provide the knowledge of argument positions of opinion holders is not covered in any of these works. This kind of knowledge should be directly learnt from the labeled training data. In this work, we found, however, that the distribution of argument positions of opinion holders varies throughout the different domains and, therefore, cannot be learnt from any arbitrary out-of-domain training set. Bethard et al. (2004) and Kim and Hovy (2006) explore the usefulness of semantic roles provided by FrameNet (Fillmore et al., 2003). Bethard et al. (2004) use this resource to acquire labeled training data while in (Kim and Hovy, 2006) FrameNet is used within a rule-based classifier mapping frame-elements of frames to opinion holders. Bethard et al. (2004) only evaluate on an artificial dataset (i.e. a subset of sentences from FrameNet and PropBank (Kingsbury and Palmer, 2002)). The only realistic test set on which Kim and Hovy (2006) evaluate their approach are news texts. Their method is compared against a simple rule-based baseline an"
E12-1033,kingsbury-palmer-2002-treebank,0,0.698244,"throughout the different domains. In (1) and (2), the opinion holders are agents of a predictive predicate, whereas the opinion holder her daughters in (3) is a patient2 of embarrasses. (3) Mrs. Bennet does what she can to get Jane and Bingley together and embarrasses her daughters by doing so. If only sentences, such as (1) and (2), occur in the training data, a classifier will not correctly extract the opinion holder in (3), unless it obtains additional knowledge as to which predicates take opinion holders as patients. 1 By agent we always mean constituents being labeled as A0 in PropBank (Kingsbury and Palmer, 2002). 2 By patient we always mean constituents being labeled as A1 in PropBank. 325 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 325–335, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics In this work, we will consider three different generalization methods being simple unsupervised word clustering, an induction method and the usage of lexical resources. We show that generalization causes significant improvements and that the impact of improvement depends on how much training and test data dif"
E12-1033,P03-1054,0,0.00603371,"the predicate is most likely a sparse feature. For the opinion holder me in (10), for example, it would correspond to A1 shock. Therefore, we introduce for each generalization method an additional feature replacing the sparse lexical item by a generalization label, i.e. Clus: A1 CLUSTER-35265, Induc: A1 INDUC-PRED and Lex: A1 LEX-PRED.6 For this learning method, we use CRF++.7 We choose a configuration that provides good performance on our source domain (i.e. ETHICS).8 For semantic role labeling we use SWIRL9 , for chunk parsing CASS (Abney, 1991) and for constituency parsing Stanford Parser (Klein and Manning, 2003). Named-entity information is provided by Stanford Tagger (Finkel et al., 2005). convolution kernels, the structures to be compared within the kernel function are not vectors comprising manually designed features but the underlying discrete structures, such as syntactic parse trees or part-of-speech sequences. Since they are directly provided to the learning algorithm, a classifier can be built without taking the effort of implementing an explicit feature extraction. We take the best configuration from (Wiegand and Klakow, 2010) that comprises a combination of three different tree kernels bein"
E12-1033,P09-1113,0,0.0611495,"inion holder mentions from those corpora. The table shows indeed that on the domains from the MPQA corpus, i.e. ETHICS and SPACE, those opinion holders play a minor role but there is a notably higher proportion on the FICTION-domain. 3.3 3.3.1 Distant Supervision with Prototypical Opinion Holders Lexical resources are potentially much more expressive than word clustering. This knowledge, however, is usually manually compiled, which makes this solution much more expensive. Wiegand and Klakow (2011a) present an intermediate solution for opinion holder extraction inspired by distant supervision (Mintz et al., 2009). The output of that method is also a lexicon of predicates but it is automatically extracted from a large unlabeled corpus. This is achieved by collecting predicates that frequently co-occur with prototypical opinion holders, i.e. common nouns such as opponents (7) or critics (8), if they are an agent of that predicate. The rationale behind this is that those nouns act very much like actual opinion holders and therefore can be seen as a proxy. (4) I always supported this idea. holder:agent. (5) This worries me. holder:patient (6) He disappointed me. holder:patient We follow Wiegand and Klakow"
E12-1033,R11-1028,0,0.0863495,"r from each other. We also address the less common case of opinion holders being realized in patient position and suggest approaches including a novel (linguisticallyinformed) extraction method how to detect those opinion holders without labeled training data as standard datasets contain too few instances of this type. 1 Introduction Opinion holder extraction is one of the most important subtasks in sentiment analysis. The extraction of sources of opinions is an essential component for complex real-life applications, such as opinion question answering systems or opinion summarization systems (Stoyanov and Cardie, 2011). Common approaches designed to extract opinion holders are based on data-driven methods, in particular supervised learning. In this paper, we examine the role of generalization for opinion holder extraction in both indomain and cross-domain classification. Generalization may not only help to compensate the availability of labeled training data but also conciliate domain mismatches. In order to illustrate this, compare for instance (1) and (2). (1) Malaysia did not agree to such treatment of Al-Qaeda soldiers as they were prisoners-of-war and should be accorded treatment as provided for under"
E12-1033,P10-1059,0,0.0346307,"ation would suit our purpose. It is henceforth called SPACE. Domain ETHICS SPACE FICTION The cluster is the union of documents with the following MPQA-topic labels: axisofevil, guantanamo, humanrights, mugabe and settlements. # Holders in sentence (average) 0.79 0.28 1.49 Table 1: Statistics of the different domain corpora. In addition to these two (sub)domains, we chose some text type that is not even news text in order to have a very distant domain. Therefore, we had to use some text not included in the MPQA corpus. Existing text collections containing product reviews (Kessler et al., 2010; Toprak et al., 2010), which are generally a popular resource for sentiment analysis, were not found suitable as they only contain few distinct opinion holders. We finally used a few summaries of fictional work (two Shakespeare plays and one novel by Jane Austen4 ) since their language is notably different from that of news texts and they contain a large number of different opinion holders (therefore opinion holder extraction is a meaningful task on this text type). These texts make up our third domain FICTION. We manually labeled it with opinion holder information by applying the annotation scheme of the MPQA cor"
E12-1033,P10-1040,0,0.0328556,", liken, indicate, suggest, owe, expect, ... disappointment, unease, nervousness, dismay, optimism, ... remark, baby, book, saint, manhole, maxim, coin, batter, ... Table 2: Some automatically induced clusters. ETHICS 1.47 SPACE 2.70 FICTION 11.59 Table 3: Percentage of opinion holders as patients. al., 2010). Such a generalization is, in particular, attractive as it is cheaply produced. As a stateof-the-art clustering method, we consider Brown clustering (Brown et al., 1992) as implemented in the SRILM-toolkit (Stolcke, 2002). We induced 1000 clusters which is also the configuration used in (Turian et al., 2010).5 Table 2 illustrates a few of the clusters induced from our unlabeled dataset introduced in Section (§) 2. Some of these clusters represent location or person names (e.g. I. & II.). This exemplifies why clustering is effective for named-entity recognition. We also find clusters that intuitively seem to be meaningful for our task (e.g. III. & IV.) but, on the other hand, there are clusters that contain words that with the exception of their part of speech do not have anything in common (e.g. V.). 3.2 Manually Compiled Lexicons (Lex) The major shortcoming of word clustering is that it lacks an"
E12-1033,J04-3002,0,0.0473889,"Missing"
E12-1033,N10-1121,1,0.859655,"datasets contain too few instances of them. In the context of generalization it is also important to consider different classification methods as the incorporation of generalization may have a varying impact depending on how robust the classifier is by itself, i.e. how well it generalizes even with a standard feature set. We compare two stateof-the-art learning methods, conditional random fields and convolution kernels, and a rule-based method. 2 Data As a labeled dataset we mainly use the MPQA 2.0 corpus (Wiebe et al., 2005). We adhere to the definition of opinion holders from previous work (Wiegand and Klakow, 2010; Wiegand and Klakow, 2011a; Wiegand and Klakow, 2011b), i.e. every source of a private state or a subjective speech event (Wiebe et al., 2005) is considered an opinion holder. This corpus contains almost exclusively news texts. In order to divide it into different domains, we use the topic labels from (Stoyanov et al., 2004). By inspecting those topics, we found that many of them can grouped to a cluster of news items discussing human rights issues mostly in the context of combating global terrorism. This means that there is little point in considering every single topic as a distinct (sub)do"
E12-1033,R11-1039,1,0.426157,"nstances of them. In the context of generalization it is also important to consider different classification methods as the incorporation of generalization may have a varying impact depending on how robust the classifier is by itself, i.e. how well it generalizes even with a standard feature set. We compare two stateof-the-art learning methods, conditional random fields and convolution kernels, and a rule-based method. 2 Data As a labeled dataset we mainly use the MPQA 2.0 corpus (Wiebe et al., 2005). We adhere to the definition of opinion holders from previous work (Wiegand and Klakow, 2010; Wiegand and Klakow, 2011a; Wiegand and Klakow, 2011b), i.e. every source of a private state or a subjective speech event (Wiebe et al., 2005) is considered an opinion holder. This corpus contains almost exclusively news texts. In order to divide it into different domains, we use the topic labels from (Stoyanov et al., 2004). By inspecting those topics, we found that many of them can grouped to a cluster of news items discussing human rights issues mostly in the context of combating global terrorism. This means that there is little point in considering every single topic as a distinct (sub)domain and, therefore, we co"
E12-1033,W11-4004,1,0.432624,"nstances of them. In the context of generalization it is also important to consider different classification methods as the incorporation of generalization may have a varying impact depending on how robust the classifier is by itself, i.e. how well it generalizes even with a standard feature set. We compare two stateof-the-art learning methods, conditional random fields and convolution kernels, and a rule-based method. 2 Data As a labeled dataset we mainly use the MPQA 2.0 corpus (Wiebe et al., 2005). We adhere to the definition of opinion holders from previous work (Wiegand and Klakow, 2010; Wiegand and Klakow, 2011a; Wiegand and Klakow, 2011b), i.e. every source of a private state or a subjective speech event (Wiebe et al., 2005) is considered an opinion holder. This corpus contains almost exclusively news texts. In order to divide it into different domains, we use the topic labels from (Stoyanov et al., 2004). By inspecting those topics, we found that many of them can grouped to a cluster of news items discussing human rights issues mostly in the context of combating global terrorism. This means that there is little point in considering every single topic as a distinct (sub)domain and, therefore, we co"
E12-1033,H05-1044,0,0.0160849,"-occur with prototypical opinion holders, i.e. common nouns such as opponents (7) or critics (8), if they are an agent of that predicate. The rationale behind this is that those nouns act very much like actual opinion holders and therefore can be seen as a proxy. (4) I always supported this idea. holder:agent. (5) This worries me. holder:patient (6) He disappointed me. holder:patient We follow Wiegand and Klakow (2011b) who found that those predicates can be best obtained by using a subset of Levin’s verb classes (Levin, 1993) and the strong subjective expressions of the Subjectivity Lexicon (Wilson et al., 2005). For those predicates it is also important to consider in which argument position they usually take an opinion holder. Bethard et al. (2004) found the 5 We also experimented with other sizes but they did not produce a better overall performance. Task-Specific Lexicon Induction (Induc) (7) Opponents say these arguments miss the point. (8) Critics argued that the proposed limits were unconstitutional. This method reduces the human effort to specifying a small set of such prototypes. Following the best configuration reported in (Wiegand and Klakow, 2011a), we extract 250 verbs, 100 nouns and 100"
E14-1071,N13-1121,0,0.0414127,"Missing"
E14-1071,W06-1642,0,0.0452757,"at individually manages to significantly outperform word is graph. The traditional features (i.e. pos, synt and brown) only produce some mild improvement when added jointly to word along some conjunctive features. When graph is added to this feature set (i.e. word+patt+pos+synt+brown+conj), we obtain another significant improvement. In conclusion, the information we induced from our domain-specific corpus cannot be obtained by other NLP-features, including other state-of-theart induction methods such as Brown clustering. The task of data-driven lexicon expansion has also been explored before (Kanayama and Nasukawa, 2006; Das and Smith, 2012), however, our paper presents the first attempt to carry out a comprehensive categorization for the food domain. For the first time, we also show that type information can effectively improve the extraction of very common relations. For the twitter domain, the usage of type information based on clustering has already been found effective for supervised learning (Bergsma et al., 2013). 6 Conclusion 5 Related Work We presented an induction method to assign semantic information to food items. We considered two types of categorizations being food-type information and informat"
E14-1071,P08-1119,0,0.039423,"Missing"
E14-1071,J92-4003,0,0.0622751,"currence is co-incidental. On a subset of 200 sentences, we measured a substantial interannotation agreement of Cohen’s κ = 0.67 (Landis and Koch, 1977). We train a supervised classifier and incorporate the knowledge induced from our domain-specific corpus as features. We chose Support Vector Machines with 5-fold cross-validation using SVMlight multi-class (Joachims, 1999). Table 13 displays all features that we examine for supervised classification. Most features are widely used throughout different NLP tasks. One special feature brown takes into consideration the output of Brown clustering (Brown et al., 1992) which like our graph-based optimization produces a corpus-driven categorization of words. Similar to UNSUP, this method is unsupervised but it considers the entire vocabulary of our text corpus rather than only food items. Therefore, this information can be considered as a generalization of all contextual words. Such type of information has been shown to be useful for named-entity recognition (Turian et al., 2010) and relation extraction (Plank and Moschitti, 2013). For syntactic parsing, Stanford Parser (Rafferty and Manning, 2008) was used. For Brown clustering, the SRILM-toolkit (Stolcke,"
E14-1071,P99-1016,0,0.273419,"Missing"
E14-1071,W03-0415,0,0.0841167,"Missing"
E14-1071,N12-1086,0,0.0218792,"gnificantly outperform word is graph. The traditional features (i.e. pos, synt and brown) only produce some mild improvement when added jointly to word along some conjunctive features. When graph is added to this feature set (i.e. word+patt+pos+synt+brown+conj), we obtain another significant improvement. In conclusion, the information we induced from our domain-specific corpus cannot be obtained by other NLP-features, including other state-of-theart induction methods such as Brown clustering. The task of data-driven lexicon expansion has also been explored before (Kanayama and Nasukawa, 2006; Das and Smith, 2012), however, our paper presents the first attempt to carry out a comprehensive categorization for the food domain. For the first time, we also show that type information can effectively improve the extraction of very common relations. For the twitter domain, the usage of type information based on clustering has already been found effective for supervised learning (Bergsma et al., 2013). 6 Conclusion 5 Related Work We presented an induction method to assign semantic information to food items. We considered two types of categorizations being food-type information and information about whether a fo"
E14-1071,W97-0802,0,0.283746,"For that we choose a pattern-based representation that outperforms a distributionalbased representation. For initialization, we examine some manually compiled seed words and 2 Data & Annotation 2.1 Domain-Specific Text Corpus In order to generate a dataset for our experiments, we used a crawl of chefkoch.de1 (Wiegand et al., 2012b) consisting of 418, 558 webpages of foodrelated forum entries. chefkoch.de is the largest German web portal for food-related issues. 2.2 Food Categorization As a food vocabulary, we employ a list of 1888 food items: 1104 items were directly extracted from GermaNet (Hamp and Feldweg, 1997), the German version of WordNet (Miller et al., 1990). The items were identified by extracting all hyponyms of the synset Nahrung (English: food). By 1 www.chefkoch.de 673 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 673–682, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics Class Description Size Perc. Class MEAT BEVERAGE VEGE SWEET SPICE STARCH MILK FRUIT GRAIN FAT EGG meat and fish (products) beverages (incl. alcoholic drinks) vegetables (incl. salads) sweets, pastries and snack mixes"
E14-1071,C92-2082,0,0.240777,"ks and all configurations. It is a setting that provided reasonable results without any notable bias for any particular configuration we examine. Figure 1: Illustration of the similarity graph. 3.1.2 P √1 F − q1 Fj + µ n i=1 kFi − Yi k i,j=1 Wij δi i δj Pn 3.1.3 Manually vs. Automatically Extracted Seeds We explore two types of seed initializations: (a) a manually compiled list of seed food items and (b) a small set of patterns (Table 4) by the help of which such seeds are automatically extracted. In order to extract seeds for Task I with the pattern-based approach, we apply the patterns from Hearst (1992). These patterns have been designed for the acquisition of hyponyms. Task I can also be regarded as some type of hyponym extraction. The food types (fruit, meat, sweets) represent the hypernyms for which we extract seed hyponyms (banana, beef, chocolate). In order to extract seeds for Task II, we apply two domain-specific sets of patterns (pattdish and pattatom ). We rank the food items according to the frequency of occurring with the respective pattern set. Since food items may occur in both rankings, we merge the two rankings in the following way: j and Wii = 0. The normalization weight λ is"
E14-1071,P98-2127,0,0.474451,"Missing"
E14-1071,P13-1147,0,0.0223673,"tures are widely used throughout different NLP tasks. One special feature brown takes into consideration the output of Brown clustering (Brown et al., 1992) which like our graph-based optimization produces a corpus-driven categorization of words. Similar to UNSUP, this method is unsupervised but it considers the entire vocabulary of our text corpus rather than only food items. Therefore, this information can be considered as a generalization of all contextual words. Such type of information has been shown to be useful for named-entity recognition (Turian et al., 2010) and relation extraction (Plank and Moschitti, 2013). For syntactic parsing, Stanford Parser (Rafferty and Manning, 2008) was used. For Brown clustering, the SRILM-toolkit (Stolcke, 2002) was used. Following Turian et al. (2010), we induced 1000 clusters (from our domain-specific corpus §2.1). Task Corpus +POSTP graph Acc F1 Acc F1 Food Type Wikipedia chefkoch.de X X 40.3 65.8 49.4 71.0 61.4 80.2 59.8 77.7 Dish Wikipedia chefkoch.de X X 50.4 66.5 53.1 71.3 75.4 83.0 71.1 80.1 Table 11: Comparison of Wikipedia and domainspecific corpus as a source for the similarity graph. dientOf follow the lexical pattern food item1 with food item2 (1). Howeve"
E14-1071,W08-1006,0,0.0591793,"Missing"
E14-1071,W97-0313,0,0.715377,"Missing"
E14-1071,P10-1040,0,0.0165206,"xamine for supervised classification. Most features are widely used throughout different NLP tasks. One special feature brown takes into consideration the output of Brown clustering (Brown et al., 1992) which like our graph-based optimization produces a corpus-driven categorization of words. Similar to UNSUP, this method is unsupervised but it considers the entire vocabulary of our text corpus rather than only food items. Therefore, this information can be considered as a generalization of all contextual words. Such type of information has been shown to be useful for named-entity recognition (Turian et al., 2010) and relation extraction (Plank and Moschitti, 2013). For syntactic parsing, Stanford Parser (Rafferty and Manning, 2008) was used. For Brown clustering, the SRILM-toolkit (Stolcke, 2002) was used. Following Turian et al. (2010), we induced 1000 clusters (from our domain-specific corpus §2.1). Task Corpus +POSTP graph Acc F1 Acc F1 Food Type Wikipedia chefkoch.de X X 40.3 65.8 49.4 71.0 61.4 80.2 59.8 77.7 Dish Wikipedia chefkoch.de X X 50.4 66.5 53.1 71.3 75.4 83.0 71.1 80.1 Table 11: Comparison of Wikipedia and domainspecific corpus as a source for the similarity graph. dientOf follow the le"
E14-1071,wiegand-etal-2012-gold,1,0.899461,"Missing"
E14-1071,D09-1097,0,0.0452431,"Missing"
E14-1071,C04-1146,0,0.0982564,"Missing"
E14-1071,C02-1114,0,0.117582,"Missing"
E14-1071,N03-1036,0,0.064969,"Missing"
E14-1071,P10-1029,0,\N,Missing
E14-1071,C98-2122,0,\N,Missing
E14-1071,S12-1012,0,\N,Missing
E14-4023,W10-0731,0,0.0309094,"Missing"
E14-4023,P98-1013,0,0.299999,"Missing"
E14-4023,Q13-1023,0,0.378287,"Missing"
E14-4023,P93-1023,0,0.706371,"fact that e.g. negation and adverbs such as very or slightly impact the perceived intensity of adjectives. We work with four scales of adjectives (cf. Table 1). Our polar adjectives include 29 adjectives referring to quality and 18 adjectives relating to intelligence. Our non-polar adjectives include 8 dimensional adjectives denoting size and 22 denoting duration. The adjectives are taken, in part, from FrameNet’s (Baker et al., 1998) frames for Table 1: Adjectives used grouped by human gold standard intensity classes 1 2 As there has been previous work on how to group adjectives into scales (Hatzivassiloglou and McKeown, 1993), we consider this grouping as given. The ratings we collected and our scripts are available at www.uni-hildesheim.de/ruppenhofer/ data/DISA_data.zip. 117 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 117–122, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics 2 Data and resources 4.1 Corpus-based methods Our first method, distinctive-collexeme analysis (Collex) (Gries and Stefanowitsch, 2004) assumes that adjectives with different types of intensities co-occur with different types of adver"
E14-4023,P97-1023,0,0.920184,"Missing"
E14-4023,kamps-etal-2004-using,0,0.1266,"Missing"
E14-4023,J11-2001,0,0.174941,"ves. One resource we consider are the affective ratings (elicited with AMT) for almost 14,000 English words collected by Warriner et al. (2013). They include scores of valence (unhappy to happy), arousal (calm to aroused) and dominance (in control to controlled) for each word in the list. This scoring system follows the dimensional theory of emotion by Osgood et al. (1957). We will interpret each of these dimensions as a separate intensity score, i.e. WarV al , WarAro and WarDom . Beyond Warriner’s ratings, we consider the two polarity lexicons SentiStrength (Thelwall et al., 2010) and SoCAL (Taboada et al., 2011) which also assign intensity scores to polar expressions. 5.2 Results The results of the pairwise correlations between the human-elicited gold standard and the rankings derived from various methods and resources are shown in Table 5. For polar adjectives, most rankings correlate fairly well with human judgments. Warriner’s arousal list, however, performs poorly on quality adjectives, whereas MeanStar and Warriner’s dominance and valence lists perform better on quality than on intelligence adjectives. For MeanStar, this does not come as a surprise as quality adjectives are much more frequent in"
E14-4023,N13-1059,1,0.887048,"Missing"
E14-4023,C98-1013,0,\N,Missing
I13-1003,R11-1019,0,0.0667058,"Missing"
I13-1003,D12-1124,0,0.103539,"n this paper, we take a first step towards this endeavour. We try to identify mentions that a food item is healthy (1) or unhealthy (2). 2 Related Work In the food domain, the most prominent research addresses ontology or thesaurus alignment (van Hage et al., 2010), a task in which concepts from different sources are related to each other. In this context, hyponymy relations (van Hage et al., 2005) and part-whole relations (van Hage et al., 2006) have been explored. More recently, Wiegand et al. (2012a) examined extraction methods for relations involved in customer advice in a supermarket. In Chahuneau et al. (2012), sentiment information has been related to food prices with the help of a large corpus consisting of restaurant menus and reviews. In the health/medical domain, the majority of research focus on domain-specific relations involving entities, such as genes, proteins and (1) There is not a healthy diet without a lot of fruits, vegetables and salads. (2) The day already began unhealthy: I had a piece of cake for breakfast. This task is a pre-requisite of more complex tasks, such as finding food items that are suitable for certain groups of people with a particular health condition (3) or identify"
I13-1003,R09-1034,0,0.0174061,"ion word lists and the scope modeling from Wilson et al. (2005). weird Sure, chocolate is veeeeery healthy. Regular expression detecting suspicious reduplications of characters in order to detect irony. comp We check for typical inflectional word forms (i.e. healthier and healthiest) and constructions, such as as healthy as. Does the context of healthy suggest another sense of the word? sense Contexts in which healthy has a different meaning (using online dictionaries, such as www.duden.de/rechtschreibung/gesund and de.wiktionary.org/wiki/gesund). Usage of the German PolArt sentiment lexicon (Klenner et al., 2009). Number of positive/negative polar expressions (excluding mentions of healthy) Number of near synonyms of (un)healthy polar* Number of diseases disease* syno* Examples for healthy: high in vitamin, tonic, etc.; examples for unhealthy: carcinogenic, harmful, etc. (manually compiled list of 99 synonyms by an annotator not involved in feature engineering). 411 entries, created with the help of the web (bildung.wikia.com/ wiki/Alphabetische Liste der Krankheiten). Task-specific Knowledge-based Features using a Healthiness Lexicon Feature Abbrev. Illustration/Further Information Is target food ite"
I13-1003,W11-4004,1,0.787551,"100g per day; in moderation; a teaspoon of; a of 75 quantifying expressions was collected from web (rezepte.nit.at/kuechenmasse.html de.wikibooks.org/wiki/Kochbuch/ Maßangaben). steamed vegetables; fried potatoes list the and breadtarget made of whole grains; caketarget with low-fat ingredients; Complementary feature to attrNoH (feature detects specifications in the form of contact clauses or prepositional phrases immediately attached to the target food item). Some people claim that chocolate is healthy. This feature relies on a set of predicates indicating the presence of an opinion holder (Wiegand and Klakow, 2011). question Is chocolate healthy? irrealis If honey were healthy; I wonder, whether honey is healthy. Translation of the cues used in hedge classification (Morante and Daelemans, 2009). modal Honey might be healthy. negTarget No cake is healthy. We adapted to German the negation word lists and the scope modeling from Wilson et al. (2005). negHealth Chocolate is not healthy. We adapted to German the negation word lists and the scope modeling from Wilson et al. (2005). weird Sure, chocolate is veeeeery healthy. Regular expression detecting suspicious reduplications of characters in order to detec"
I13-1003,P09-1113,0,0.00839799,"althy express this relation. This may already indicate that its extraction is difficult. 2 This is the only part of the dataset which was annotated by both annotators in parallel. www.chefkoch.de 20 Type Frequency Percentage Is-Healthy Is-Unhealthy Abbrev. HLTH UNHLTH 488 171 20.00 7.01 OTHER: No Relation Restricted Relation Unspecified Intersection Embedding Comparison Relation Unsupported Claim Other Sense Irony Question NOREL RESTR INTERS EMB COMP CLAIM SENSE IRO Q 788 312 198 157 121 87 77 25 16 32.30 12.79 8.11 6.43 4.96 3.57 3.16 1.02 0.66 considered an important cue (Zhou et al., 2005; Mintz et al., 2009). In particular, the specific type of syntactic relation needs to be considered. If in our task healthy is an attributive adjective of the target food item (16), this is not an indication of a genuine Is-Healthy relation that we are looking for. With this construction, one usually refers to all those entities that share the two properties (intersection) of being the target food item and being healthy. This case is different from both HLTH (17) and RESTR (18). Table 1: Statistics of the different (linguistic) phenomena. 4.2 (16) I usually buy the healthy fat. (17) Fat is healthy. (18) I usually"
I13-1003,wiegand-etal-2012-gold,1,0.890759,"Missing"
I13-1003,W09-1304,0,0.0229975,"teamed vegetables; fried potatoes list the and breadtarget made of whole grains; caketarget with low-fat ingredients; Complementary feature to attrNoH (feature detects specifications in the form of contact clauses or prepositional phrases immediately attached to the target food item). Some people claim that chocolate is healthy. This feature relies on a set of predicates indicating the presence of an opinion holder (Wiegand and Klakow, 2011). question Is chocolate healthy? irrealis If honey were healthy; I wonder, whether honey is healthy. Translation of the cues used in hedge classification (Morante and Daelemans, 2009). modal Honey might be healthy. negTarget No cake is healthy. We adapted to German the negation word lists and the scope modeling from Wilson et al. (2005). negHealth Chocolate is not healthy. We adapted to German the negation word lists and the scope modeling from Wilson et al. (2005). weird Sure, chocolate is veeeeery healthy. Regular expression detecting suspicious reduplications of characters in order to detect irony. comp We check for typical inflectional word forms (i.e. healthier and healthiest) and constructions, such as as healthy as. Does the context of healthy suggest another sense"
I13-1003,H05-1044,0,0.0297792,"ects specifications in the form of contact clauses or prepositional phrases immediately attached to the target food item). Some people claim that chocolate is healthy. This feature relies on a set of predicates indicating the presence of an opinion holder (Wiegand and Klakow, 2011). question Is chocolate healthy? irrealis If honey were healthy; I wonder, whether honey is healthy. Translation of the cues used in hedge classification (Morante and Daelemans, 2009). modal Honey might be healthy. negTarget No cake is healthy. We adapted to German the negation word lists and the scope modeling from Wilson et al. (2005). negHealth Chocolate is not healthy. We adapted to German the negation word lists and the scope modeling from Wilson et al. (2005). weird Sure, chocolate is veeeeery healthy. Regular expression detecting suspicious reduplications of characters in order to detect irony. comp We check for typical inflectional word forms (i.e. healthier and healthiest) and constructions, such as as healthy as. Does the context of healthy suggest another sense of the word? sense Contexts in which healthy has a different meaning (using online dictionaries, such as www.duden.de/rechtschreibung/gesund and de.wiktion"
I13-1003,P05-1053,0,0.0164625,"et food item and healthy express this relation. This may already indicate that its extraction is difficult. 2 This is the only part of the dataset which was annotated by both annotators in parallel. www.chefkoch.de 20 Type Frequency Percentage Is-Healthy Is-Unhealthy Abbrev. HLTH UNHLTH 488 171 20.00 7.01 OTHER: No Relation Restricted Relation Unspecified Intersection Embedding Comparison Relation Unsupported Claim Other Sense Irony Question NOREL RESTR INTERS EMB COMP CLAIM SENSE IRO Q 788 312 198 157 121 87 77 25 16 32.30 12.79 8.11 6.43 4.96 3.57 3.16 1.02 0.66 considered an important cue (Zhou et al., 2005; Mintz et al., 2009). In particular, the specific type of syntactic relation needs to be considered. If in our task healthy is an attributive adjective of the target food item (16), this is not an indication of a genuine Is-Healthy relation that we are looking for. With this construction, one usually refers to all those entities that share the two properties (intersection) of being the target food item and being healthy. This case is different from both HLTH (17) and RESTR (18). Table 1: Statistics of the different (linguistic) phenomena. 4.2 (16) I usually buy the healthy fat. (17) Fat is he"
I13-1003,W08-1006,0,0.030243,"e present. 7 Experiments In this section we present the results on automatic classification. 7.1 Classification of Individual Utterances In this subsection, we evaluate the performance of the different feature sets on sentence-level classification using supervised learning and rule-based classification. We investigate the detection of the two classes HLTH (§4.1) and UNHLTH (§4.2). Each instance to be classified is a sentence in which there is a co-occurrence of a target food item and a mention of healthy along its respective context sentences. The dataset was parsed using the Stanford Parser (Rafferty and Manning, 2008). We carry out a 5-fold cross-validation on our manually labeled dataset. As a supervised classifier, we use Support Vector Machines (SVMlight (Joachims, 1999) with a linear kernel). For each class, we train a binary classifier where positive instances represent the class to be extracted while negative instances are the remaining instances of the entire dataset (§4). (27) Chocolate is healthy as it’s high in magnesium and provides vitamin E. We use this knowledge as a baseline. If we cannot exceed the classification performance of prior (alone), then acquiring the knowledge of healthiness with"
I17-1063,D08-1083,0,0.115687,"and its dependent noun have the same polarity or not, the polarity is considered to have shifted or not shifted, as detailed in Table 8. These are the class labels onto which the output of the systems (and the annotation) will be mapped. The quantitative evaluation happens on these labels. There is currently no consensus as to how shifting is to be modeled in terms of resulting polarities. For example, the shifting of excellent in (16) could either be interpreted as the resulting phrase wasn’t excellent carrying negative or neutral polarity. The first interpretation simply flips the polarity (Choi and Cardie, 2008), while the second interpretation is driven by the fact that the negation of excellent is not synonymous with its antonym atrocious (Taboada et al., 2011; Kiritchenko and Mohammad, 2016). The polar inImpact on Sentiment Analysis We now investigate whether knowledge of verbal shifters can be useful for the identification of contextual phrase-level sentiment. Apart from being an intermediate step in compositional sentencelevel classification, phrase-level classification is also independently needed for applications such as knowledge base population (Mitchell, 2013), question answering (Dang, 200"
I17-1063,W14-2618,0,0.268664,"ures, we consider all verbs ranked by their frequency in our text corpus (FREQ), as well as all negative polar expressions3 ranked by frequency (NEGATIVE). 1 We assume that the scope of a verbal shifter is the set of its dependents (typically its subject or objects). 2 https://github.com/marcschulder/ ijcnlp2017 3 We consider negative polar expressions since the proportion of shifters is greatest among these expressions (Table 2). 625 EffectWordNet (EFFECT): This feature uses the idea that events may have beneficial or harmful effects on their objects. Wiebe and colleagues (Deng et al., 2013; Choi et al., 2014; Choi and Wiebe, 2014) introduced this idea in the context of annotation and lexical acquisition work for opinion inference.4 For example, in (5) the combined facts that fall has a negative effect (henceforth referred to as –effect) on its theme (i.e. Chavez) and that people are happy about Chavez’ fall suggest that people have a negative attitude towards Chavez. As (5) shows, verbs with a –effect, such as fall, often coincide with verbal shifters. However, –effect words do not necessarily shift polarity. For instance, while abuse has a –effect, it does not shift, as shown by the fact that th"
I17-1063,D14-1125,0,0.47024,"ll verbs ranked by their frequency in our text corpus (FREQ), as well as all negative polar expressions3 ranked by frequency (NEGATIVE). 1 We assume that the scope of a verbal shifter is the set of its dependents (typically its subject or objects). 2 https://github.com/marcschulder/ ijcnlp2017 3 We consider negative polar expressions since the proportion of shifters is greatest among these expressions (Table 2). 625 EffectWordNet (EFFECT): This feature uses the idea that events may have beneficial or harmful effects on their objects. Wiebe and colleagues (Deng et al., 2013; Choi et al., 2014; Choi and Wiebe, 2014) introduced this idea in the context of annotation and lexical acquisition work for opinion inference.4 For example, in (5) the combined facts that fall has a negative effect (henceforth referred to as –effect) on its theme (i.e. Chavez) and that people are happy about Chavez’ fall suggest that people have a negative attitude towards Chavez. As (5) shows, verbs with a –effect, such as fall, often coincide with verbal shifters. However, –effect words do not necessarily shift polarity. For instance, while abuse has a –effect, it does not shift, as shown by the fact that the verb phrase remains n"
I17-1063,N09-1016,0,0.244879,"formation on negation modeling. Approaches to learning negation from labeled corpora have been examined in the review domain (Ikeda et al., 2008; Kessler and Sch¨utze, 2012; Socher et al., 2013; Yu et al., 2016), the biomedical domain (Huang and Lowe, 2007; Morante and Daelemans, 2009; Zou et al., 2013) and across domains (Fancellu et al., 2016). However, as outlined in §1, due to their small size the labeled datasets include few different verbal shifters. Moreover, these works mostly focus on scope detection rather than the identification of shifters. The work most closely related to ours is Danescu-Niculescu-Mizil et al. (2009) who propose using NPIs for shifter extraction.10 However, our work substantially extends that previous work. We show how the usage of NPIs can be further refined to improve the recognition of shifters (i.e. require the direct object to be a polar noun and subsequently apply PageRank). Moreover, we successfully combine this information with other features. Unlike Danescu-Niculescu-Mizil et al. (2009), we also carry out a recall-oriented evaluation and examine the impact of explicit knowledge of verbal shifters on contextual sentiment analysis. 10 631 Shifters are referred to as downward entail"
I17-1063,N10-1138,0,0.0412321,"Missing"
I17-1063,P11-1144,0,0.190173,"Missing"
I17-1063,P13-2022,0,0.118488,"on As baseline features, we consider all verbs ranked by their frequency in our text corpus (FREQ), as well as all negative polar expressions3 ranked by frequency (NEGATIVE). 1 We assume that the scope of a verbal shifter is the set of its dependents (typically its subject or objects). 2 https://github.com/marcschulder/ ijcnlp2017 3 We consider negative polar expressions since the proportion of shifters is greatest among these expressions (Table 2). 625 EffectWordNet (EFFECT): This feature uses the idea that events may have beneficial or harmful effects on their objects. Wiebe and colleagues (Deng et al., 2013; Choi et al., 2014; Choi and Wiebe, 2014) introduced this idea in the context of annotation and lexical acquisition work for opinion inference.4 For example, in (5) the combined facts that fall has a negative effect (henceforth referred to as –effect) on its theme (i.e. Chavez) and that people are happy about Chavez’ fall suggest that people have a negative attitude towards Chavez. As (5) shows, verbs with a –effect, such as fall, often coincide with verbal shifters. However, –effect words do not necessarily shift polarity. For instance, while abuse has a –effect, it does not shift, as shown"
I17-1063,P16-1047,0,0.0497277,"− − (18) She [[brought down]V a curseN ]VP on the village. 6 Related Work Negation modeling is a central research issue in sentiment analysis, but only few works consider more than typical negation words. We refer the reader to the survey of Wiegand et al. (2010) for more information on negation modeling. Approaches to learning negation from labeled corpora have been examined in the review domain (Ikeda et al., 2008; Kessler and Sch¨utze, 2012; Socher et al., 2013; Yu et al., 2016), the biomedical domain (Huang and Lowe, 2007; Morante and Daelemans, 2009; Zou et al., 2013) and across domains (Fancellu et al., 2016). However, as outlined in §1, due to their small size the labeled datasets include few different verbal shifters. Moreover, these works mostly focus on scope detection rather than the identification of shifters. The work most closely related to ours is Danescu-Niculescu-Mizil et al. (2009) who propose using NPIs for shifter extraction.10 However, our work substantially extends that previous work. We show how the usage of NPIs can be further refined to improve the recognition of shifters (i.e. require the direct object to be a polar noun and subsequently apply PageRank). Moreover, we successful"
I17-1063,P16-1191,0,0.0822976,"Missing"
I17-1063,E09-1005,0,0.0331185,"Missing"
I17-1063,P98-1013,0,0.798538,"Missing"
I17-1063,N09-1002,0,0.487693,"Missing"
I17-1063,W13-3514,0,0.234365,"Missing"
I17-1063,morante-2010-descriptive,0,0.476051,"ources barely cover any verbal shifters at all. Even the most complex negation lexicon for sentiment analysis (Wilson et al., 2005) includes a mere 12 verbal shifters. In contrast, our initial random sample of 2000 verb lemmas contained 300 shifters. The corpora on which negation can be learned, such as the Sentiment Treebank (Socher et al., 2013) or the BioScope corpus (Szarvas et al., 2008), only comprise contiguous sentences of fairly small datasets, so only the most frequently occurring negation words are considered. For example, only 6 verbal shifters are observed on the BioScope corpus (Morante, 2010). Introduction We present an approach towards bootstrapping a lexicon of polarity shifters. Polarity shifters are content words that have semantic properties similar to negation. For example, the negated statement in (1) involving the negation word not can also be expressed by the verbal shifter fail in (2). (1) Peter did not pass the exam. (2) Peter failedshifter to pass the exam. Similarly, shifting is also caused by nouns (e.g. downfall) and adjectives (e.g. devoid). Polarity shifters are important for various tasks in NLP, such as relation extraction (SanchezGraillet and Poesio, 2007), rec"
I17-1063,W09-1105,0,0.468525,"Similarity (SIM): As our aim is to identify verbs whose semantics resembles that of negation words, a straightforward method is to extract verbs that are distributionally similar to negation words. Using Word2Vec (Mikolov et al., 2013), we compute word embeddings for our text corpus.5 All verbs are ranked by their cosine similarity to a given negation word. The highest ranking verbs are considered verbal shifters. As negation words we consider the intersection of two negation word lists: the negation category in the valence shifter lexicon by Wilson et al. (2005) and the negation signals from Morante and Daelemans (2009). The negation words are neither, never, no, none, nor, not and without. (7) This [tore downshifter our great [dream]+ ]− . (8) Please [lay asideshifter all your [worries]− ]+ . We only consider particles which typically indicate a complete transition to a negative end state: aside, away, back, down, off and out. To produce rankings, we sort the particle verbs by their absolute frequency in our text corpus. Heuristic using ‘any’ (ANY): Our final shifter feature rests on the linguistic insight that negative polarity items (NPIs) (Giannakidou, 2008), such as English any, typically appear in the"
I17-1063,I08-1039,0,0.401329,"evaluate the shifter lexicons generated by our best graph-based classifier (LEXLP ) and best supervised classifier (LEXSVM ) from §5.2. Our + (17) The revolution [[brought down]V the tyrant− N ]VP . − − (18) She [[brought down]V a curseN ]VP on the village. 6 Related Work Negation modeling is a central research issue in sentiment analysis, but only few works consider more than typical negation words. We refer the reader to the survey of Wiegand et al. (2010) for more information on negation modeling. Approaches to learning negation from labeled corpora have been examined in the review domain (Ikeda et al., 2008; Kessler and Sch¨utze, 2012; Socher et al., 2013; Yu et al., 2016), the biomedical domain (Huang and Lowe, 2007; Morante and Daelemans, 2009; Zou et al., 2013) and across domains (Fancellu et al., 2016). However, as outlined in §1, due to their small size the labeled datasets include few different verbal shifters. Moreover, these works mostly focus on scope detection rather than the identification of shifters. The work most closely related to ours is Danescu-Niculescu-Mizil et al. (2009) who propose using NPIs for shifter extraction.10 However, our work substantially extends that previous wor"
I17-1063,D13-1170,0,0.128113,"y on verbs. As the main predicates of phrases they tend to have larger scopes than nouns and adjectives, increasing the impact of their polarity shifting. Their vocabulary size is also smaller, allowing us to cover a reasonable share of it in our evaluation. Existing resources barely cover any verbal shifters at all. Even the most complex negation lexicon for sentiment analysis (Wilson et al., 2005) includes a mere 12 verbal shifters. In contrast, our initial random sample of 2000 verb lemmas contained 300 shifters. The corpora on which negation can be learned, such as the Sentiment Treebank (Socher et al., 2013) or the BioScope corpus (Szarvas et al., 2008), only comprise contiguous sentences of fairly small datasets, so only the most frequently occurring negation words are considered. For example, only 6 verbal shifters are observed on the BioScope corpus (Morante, 2010). Introduction We present an approach towards bootstrapping a lexicon of polarity shifters. Polarity shifters are content words that have semantic properties similar to negation. For example, the negated statement in (1) involving the negation word not can also be expressed by the verbal shifter fail in (2). (1) Peter did not pass th"
I17-1063,R11-1028,0,0.0264406,"erpretation is driven by the fact that the negation of excellent is not synonymous with its antonym atrocious (Taboada et al., 2011; Kiritchenko and Mohammad, 2016). The polar inImpact on Sentiment Analysis We now investigate whether knowledge of verbal shifters can be useful for the identification of contextual phrase-level sentiment. Apart from being an intermediate step in compositional sentencelevel classification, phrase-level classification is also independently needed for applications such as knowledge base population (Mitchell, 2013), question answering (Dang, 2009) and summarization (Stoyanov and Cardie, 2011). For that reason and because we specifically study compositionality between verbs and their object, we exclusively consider polarity classification for verb phrases. The experiment is treated as a binary classification task, where the polarity of a noun has either shifted in the context of a verb phrase (VP) or not. For example in (15), the VP lack her usual passion contains the positive polarity noun passion which is shifted by lack. − (15) The book seemed to [lackV [her usual passion+ N ]NP ]VP . We compiled sentences from our text corpus (Amazon Product Review Data, §2) that contain 9 630"
I17-1063,P14-1145,0,0.0825032,"the respective verb 4 Initially, events with positive/negative effects were referred to as good-for/bad-for events. We use the terminology Choi and Wiebe (2014) introduced for EffectWordNet. 5 Following the work of Wiegand and Ruppenhofer (2015) in verb category induction for sentiment roles, a task similar to ours, we use continuous bag-of-words with 500 dimensions. 626 (ANYnorm ). As a further constraint we demand that the direct object represents a polar expression (ANYnorm+polar ). This constraint is fulfilled in (10) since help is a positive polar expression. 2009; Choi and Wiebe, 2014; Kang et al., 2014). We assume that the explanatory texts of glosses are similar among shifters. We treat glosses as a bag-of-words feature. 3.2 We also use WordNet to assign semantic types. Our intuition is that verbal shifters share the same semantic types. We consider two types of information that have been previously found effective for sentiment analysis in general, namely the hypernyms of verbs (Breck et al., 2007) and their supersenses (Flekova and Gurevych, 2016). Anti-Shifter Feature (ANTI) We also introduce a feature for automatically retrieving verbs that – semantically speaking – are the exact opposi"
I17-1063,W08-0606,0,0.154026,"they tend to have larger scopes than nouns and adjectives, increasing the impact of their polarity shifting. Their vocabulary size is also smaller, allowing us to cover a reasonable share of it in our evaluation. Existing resources barely cover any verbal shifters at all. Even the most complex negation lexicon for sentiment analysis (Wilson et al., 2005) includes a mere 12 verbal shifters. In contrast, our initial random sample of 2000 verb lemmas contained 300 shifters. The corpora on which negation can be learned, such as the Sentiment Treebank (Socher et al., 2013) or the BioScope corpus (Szarvas et al., 2008), only comprise contiguous sentences of fairly small datasets, so only the most frequently occurring negation words are considered. For example, only 6 verbal shifters are observed on the BioScope corpus (Morante, 2010). Introduction We present an approach towards bootstrapping a lexicon of polarity shifters. Polarity shifters are content words that have semantic properties similar to negation. For example, the negated statement in (1) involving the negation word not can also be expressed by the verbal shifter fail in (2). (1) Peter did not pass the exam. (2) Peter failedshifter to pass the ex"
I17-1063,C12-2056,0,0.382928,"Missing"
I17-1063,J11-2001,0,0.218878,"s labels onto which the output of the systems (and the annotation) will be mapped. The quantitative evaluation happens on these labels. There is currently no consensus as to how shifting is to be modeled in terms of resulting polarities. For example, the shifting of excellent in (16) could either be interpreted as the resulting phrase wasn’t excellent carrying negative or neutral polarity. The first interpretation simply flips the polarity (Choi and Cardie, 2008), while the second interpretation is driven by the fact that the negation of excellent is not synonymous with its antonym atrocious (Taboada et al., 2011; Kiritchenko and Mohammad, 2016). The polar inImpact on Sentiment Analysis We now investigate whether knowledge of verbal shifters can be useful for the identification of contextual phrase-level sentiment. Apart from being an intermediate step in compositional sentencelevel classification, phrase-level classification is also independently needed for applications such as knowledge base population (Mitchell, 2013), question answering (Dang, 2009) and summarization (Stoyanov and Cardie, 2011). For that reason and because we specifically study compositionality between verbs and their object, we e"
I17-1063,D08-1061,0,0.0103467,"eds for anti-shifters8 (using ANTI from §3.2) to reflect the general bias towards non-shifter verbs (Table 1). In order to examine whether anti-shifters are actually necessary to get negative seeds of sufficient quality, we also run an alternative setting (noAntiShifter) in which the same number of negative seeds is simply extracted from the ranking of frequent verbs. The reasoning behind this is that the proportion of frequent verbs not being shifters is already fairly high, as shown by FREQ in Table 4. For LP, we considered the Adsorption label propagation algorithm as implemented in junto (Talukdar et al., 2008). For kNN, we set k = 10. Apart from the graph-based classifiers, we also consider a supervised classifier, namely Support Vector Machines (SVM) as implemented in SVMlight (Joachims, 1999). This classifier uses manually labeled training data, but, unlike LP and kNN, we may combine arbitrary feature sets. We perform 10-fold cross validation and report on accuracy and macro-average precision, recall and Fscore. For the task-specific features (§3) we use their most complex configurations from Table 3 (e.g. SIMcentroid rather than SIMnor or SIMwithout ). Table 5 shows that among the graph-based cl"
I17-1063,W06-0301,0,0.291745,"Missing"
I17-1063,W10-3111,1,0.949649,"r sheer number. On a sample of manually annotated verbs we examine a variety of linguistic features for this task. Then we build a supervised classifier to increase coverage. We show that this approach drastically reduces the annotation effort while ensuring a high-precision lexicon. We also show that our acquired knowledge of verbal polarity shifters improves phrase-level sentiment analysis. 1 (3) She was [deniedshifter the [scholarship]+ ]− . (4) The new treatment has [alleviatedshifter her [pain]− ]+ . Although there has been significant research on polarity shifting in sentiment analysis (Wiegand et al., 2010), this work has focused on the presence of negation words. Negation words (no, not, never, etc.) are typically function words, so only a few exist. Polarity shifters are content words, of which there are a lot more. For instance, WordNet (Miller et al., 1990) contains over 10k verbal, 20K adjectival and 110K nominal lemmas. An exhaustive manual annotation would be far too costly. To reduce cost, we introduce a bootstrapping approach for the acquisition of polarity shifters. In this work we focus exclusively on verbs. As the main predicates of phrases they tend to have larger scopes than nouns"
I17-1063,W16-0410,0,0.0492823,"e output of the systems (and the annotation) will be mapped. The quantitative evaluation happens on these labels. There is currently no consensus as to how shifting is to be modeled in terms of resulting polarities. For example, the shifting of excellent in (16) could either be interpreted as the resulting phrase wasn’t excellent carrying negative or neutral polarity. The first interpretation simply flips the polarity (Choi and Cardie, 2008), while the second interpretation is driven by the fact that the negation of excellent is not synonymous with its antonym atrocious (Taboada et al., 2011; Kiritchenko and Mohammad, 2016). The polar inImpact on Sentiment Analysis We now investigate whether knowledge of verbal shifters can be useful for the identification of contextual phrase-level sentiment. Apart from being an intermediate step in compositional sentencelevel classification, phrase-level classification is also independently needed for applications such as knowledge base population (Mitchell, 2013), question answering (Dang, 2009) and summarization (Stoyanov and Cardie, 2011). For that reason and because we specifically study compositionality between verbs and their object, we exclusively consider polarity clas"
I17-1063,K15-1022,1,0.845674,"ages (Krifka, 1991). (9) They did [not give us any [helpdobj ]+ ]− . (10) They [deniedshifter us any [helpdobj ]+ ]− . The feature we design collects all verbs that take a direct object that is modified by the NPI any, as in (10). We sort the verbs by their frequency of co-occurrence with this particular textual pattern (ANY). We normalize that pattern frequency by the frequency of the respective verb 4 Initially, events with positive/negative effects were referred to as good-for/bad-for events. We use the terminology Choi and Wiebe (2014) introduced for EffectWordNet. 5 Following the work of Wiegand and Ruppenhofer (2015) in verb category induction for sentiment roles, a task similar to ours, we use continuous bag-of-words with 500 dimensions. 626 (ANYnorm ). As a further constraint we demand that the direct object represents a polar expression (ANYnorm+polar ). This constraint is fulfilled in (10) since help is a positive polar expression. 2009; Choi and Wiebe, 2014; Kang et al., 2014). We assume that the explanatory texts of glosses are similar among shifters. We treat glosses as a bag-of-words feature. 3.2 We also use WordNet to assign semantic types. Our intuition is that verbal shifters share the same sem"
I17-1063,H05-1044,0,0.731135,"ectival and 110K nominal lemmas. An exhaustive manual annotation would be far too costly. To reduce cost, we introduce a bootstrapping approach for the acquisition of polarity shifters. In this work we focus exclusively on verbs. As the main predicates of phrases they tend to have larger scopes than nouns and adjectives, increasing the impact of their polarity shifting. Their vocabulary size is also smaller, allowing us to cover a reasonable share of it in our evaluation. Existing resources barely cover any verbal shifters at all. Even the most complex negation lexicon for sentiment analysis (Wilson et al., 2005) includes a mere 12 verbal shifters. In contrast, our initial random sample of 2000 verb lemmas contained 300 shifters. The corpora on which negation can be learned, such as the Sentiment Treebank (Socher et al., 2013) or the BioScope corpus (Szarvas et al., 2008), only comprise contiguous sentences of fairly small datasets, so only the most frequently occurring negation words are considered. For example, only 6 verbal shifters are observed on the BioScope corpus (Morante, 2010). Introduction We present an approach towards bootstrapping a lexicon of polarity shifters. Polarity shifters are con"
I17-1063,D13-1099,0,0.0763162,"[[brought down]V the tyrant− N ]VP . − − (18) She [[brought down]V a curseN ]VP on the village. 6 Related Work Negation modeling is a central research issue in sentiment analysis, but only few works consider more than typical negation words. We refer the reader to the survey of Wiegand et al. (2010) for more information on negation modeling. Approaches to learning negation from labeled corpora have been examined in the review domain (Ikeda et al., 2008; Kessler and Sch¨utze, 2012; Socher et al., 2013; Yu et al., 2016), the biomedical domain (Huang and Lowe, 2007; Morante and Daelemans, 2009; Zou et al., 2013) and across domains (Fancellu et al., 2016). However, as outlined in §1, due to their small size the labeled datasets include few different verbal shifters. Moreover, these works mostly focus on scope detection rather than the identification of shifters. The work most closely related to ours is Danescu-Niculescu-Mizil et al. (2009) who propose using NPIs for shifter extraction.10 However, our work substantially extends that previous work. We show how the usage of NPIs can be further refined to improve the recognition of shifters (i.e. require the direct object to be a polar noun and subsequent"
I17-1063,C98-1013,0,\N,Missing
I17-1063,D14-1082,0,\N,Missing
K15-1022,P97-1023,0,0.163194,"arity is obtained by leveraging coordination. Coordination is known to be a syntactic relation that also preserves great semantic coherence (Ziering et al., 2013), e.g. (15). It has been successfully applied (9) Opponents claim these arguments miss the point. (10) Critics argued that the proposed limits were unconstitutional. For PT-verbs, we make use of the adjective heuristic proposed by Wiegand and Klakow (2012). The 218 not only to noun categorization (Riloff and Shepherd, 1997; Roark and Charniak, 1998) but also to different tasks in sentiment analysis, including polarity classification (Hatzivassiloglou and McKeown, 1997), the induction of patient polarity verbs (Goyal et al., 2010) and connotation learning (Kang et al., 2014). We use the dependency relation from Stanford parser (Klein and Manning, 2003) to detect coordination (16). Rec F1 45.7 8.9 14.2 87.0 33.3 9.8 20.9 17.6 Coordination kNN graph 45.2 42.7 61.5 68.7 47.3 39.7 53.4 50.4 WordNet kNN graph 52.8 51.1 51.5 51.9 50.7 51.5 51.1 51.5 Embedding kNN graph 59.3 64.0 58.4 70.5 61.0 59.4 59.7 64.5 Dependency kNN graph 65.7 70.3 63.8 72.0 65.4 68.0 64.5 70.6 5 Experiments As a similarity function, we simply take the absolute frequency of observing two wo"
K15-1022,P98-1013,0,0.166676,"cs. We evaluate our approach against a new manuallycompiled opinion role lexicon and perform in-context classification. 1 Introduction While there has been much research in sentiment analysis on subjectivity detection and polarity classification, there has been less work on the extraction of opinion roles, i.e. entities that express an opinion (opinion holders), and entities or propositions at which sentiment is directed (opinion targets). Previous research relies on large amounts of labeled training data or leverages general semantic resources which are expensive to construct, e.g. FrameNet (Baker et al., 1998). In this paper, we present an approach to induce opinion roles of verbal predicates. The input is a set of opinion verbs that can be found in a common sentiment lexicon. Our model rests on the assumption that those verbs can be divided into three different types. Each type has a characteristic mapping between semantic roles and opinion holders and targets. Thus, the problem of opinion role induction is reduced to automatically categorizing opinion verbs. 215 Proceedings of the 19th Conference on Computational Language Learning, pages 215–225, c Beijing, China, July 30-31, 2015. 2015 Associati"
K15-1022,P14-1023,0,0.0128308,"sts for each of the verb types the 12 seeds most frequently occurring with the respective patterns. We observed that the SP-verb seeds are exclusively negative polar expressions. That is why we also extracted seeds from an additional pattern help to XVB producing prototypical positive SP-verbs, such as stabilize, allay or heal. 4.2 Similarity Metrics 4.2.1 Word Embeddings Recent research in machine learning has focused on inducing vector representations of words. As an example of a competitive word embedding method, we induce vectors for our opinion verbs with Word2Vec (Mikolov et al., 2013). Baroni et al. (2014) showed that this method outperforms count vector representations on a variety of tasks. For the similarity between two verbs, we compute the cosine-similarity between their vectors. 4.2.2 WordNet::Similarity We use WordNet::Similarity (Pedersen et al., 2004) as an alternative source for similarity metrics. The metrics are based on WordNet’s graph structure (Miller et al., 1990). Various relations within WordNet have been shown to be effective for polarity classification (Esuli and Sebastiani, 2006; Rao and Ravichandran, 2009). Pattern-based Seed Initialization For AG-verbs, we rely on the fin"
K15-1022,W09-1206,0,0.0329091,"Missing"
K15-1022,H05-1045,0,0.345073,"Missing"
K15-1022,P11-1144,0,0.0598548,"graph 59.3 64.0 58.4 70.5 61.0 59.4 59.7 64.5 Dependency kNN graph 65.7 70.3 63.8 72.0 65.4 68.0 64.5 70.6 5 Experiments As a similarity function, we simply take the absolute frequency of observing two words w1 and w2 in a conjunction, i.e. sim(w1 , w2 ) = f req(conj(w1 , w2 )). 5.1 Evaluation of the Induced Lexicon Dependency-based Similarity The metric proposed by Lin (1998) exploits the rich set of dependency-relation labels in the context of distributional similarity. Moreover, it has been effectively used for the related task of extending frames of unknown predicates in semantic parsing (Das and Smith, 2011). The metric is based on dependency triples (w, r, w′ ) where w and w′ are words and r is a dependency relation (e.g. (argue-V, nsubj, critics-N)). The metric is defined as: P 1 ,r,w)+I(w2 ,r,w)) (r,w)∈T (w1 )∩T (w2 ) (I(w P (r,w)∈T (w1 ) I(w1 ,r,w)+ (r,w)∈T (w2 ) I(w2 ,r,w) kw,r,w′ k×k∗,r,∗k I(w, r, w′ ) = log kw,r,∗k×k∗,r,w′ k sim(w1 , w2 ) = P where and T (w) is defined as the set of pairs (r, w′ ) such that kw,r,w′ k×k∗,r,∗k &gt; 0. log kw,r,∗k×k∗,r,w′k 4.3 Prec Majority Class Only Seeds Table 5: Eval. of similarity metrics and classifiers. (15) They criticize and hate him. (16) conj(criticiz"
K15-1022,D10-1101,0,0.139958,"Missing"
K15-1022,P10-1060,0,0.0608076,"Missing"
K15-1022,J13-3002,0,0.457996,"&lt; 0.05) ∗ : better than CK; ◦ : better than CK + inducWiegand 2012 ; † : better than CK + inducgraph ; ‡ : better than CK + coarse-grain lex Distribution of Verb Types Corpus MPQA CK CK + inducWiegand CK + inducgraph Table 12: Lexical resources and the impact of other (not lexicon-based) features (evaluation measure: macro-average F-score). out of domain Config Comparison to Previous Cross-Domain Opinion Holder Extraction We now compare our proposed induction approach with previous work on opinion holder ex7 The split-up of training and test set on the MPQA corpus follows the specification of Johansson and Moschitti (2013). 222 For classifiers, we consider convolution kernels CK from Wiegand and Klakow (2012) and the sequence labeler from Johansson and Moschitti (2013) MultiRel that incorporates relational features taking into account interactions between multiple opinion cues. It is currently the most sophisticated opinion holder extractor. CK can be combined with additional knowledge. We compare inducgraph with inducWiegand 2012 , which employs the word lists induced for AG- and PTverbs in the fashion of Wiegand and Klakow (2012), i.e. without graph clustering. As an upper bound for the induction methods, coa"
K15-1022,P14-1145,0,0.171151,"antic coherence (Ziering et al., 2013), e.g. (15). It has been successfully applied (9) Opponents claim these arguments miss the point. (10) Critics argued that the proposed limits were unconstitutional. For PT-verbs, we make use of the adjective heuristic proposed by Wiegand and Klakow (2012). The 218 not only to noun categorization (Riloff and Shepherd, 1997; Roark and Charniak, 1998) but also to different tasks in sentiment analysis, including polarity classification (Hatzivassiloglou and McKeown, 1997), the induction of patient polarity verbs (Goyal et al., 2010) and connotation learning (Kang et al., 2014). We use the dependency relation from Stanford parser (Klein and Manning, 2003) to detect coordination (16). Rec F1 45.7 8.9 14.2 87.0 33.3 9.8 20.9 17.6 Coordination kNN graph 45.2 42.7 61.5 68.7 47.3 39.7 53.4 50.4 WordNet kNN graph 52.8 51.1 51.5 51.9 50.7 51.5 51.1 51.5 Embedding kNN graph 59.3 64.0 58.4 70.5 61.0 59.4 59.7 64.5 Dependency kNN graph 65.7 70.3 63.8 72.0 65.4 68.0 64.5 70.6 5 Experiments As a similarity function, we simply take the absolute frequency of observing two words w1 and w2 in a conjunction, i.e. sim(w1 , w2 ) = f req(conj(w1 , w2 )). 5.1 Evaluation of the Induced L"
K15-1022,W06-0301,0,0.521525,"in this paper also notably outperforms the induction from Wiegand and Klakow (2012). Although the fine-grained lexicon is among the top performing systems, we only note large improvements on VERB. VERB has the highest proportion of PT- and SP-verbs (Table 13). Knowledge about role-assignment is most critical here. 6 Related Work Most approaches for opinion role extraction employ supervised learning. The feature design is mainly inspired by semantic role labeling (Bethard et al., 2004; Li et al., 2012). Some work also employs information from existing semantic role labelers based on FrameNet (Kim and Hovy, 2006) or PropBank (Johansson and Moschitti, 2013; Wiegand and Klakow, 2012). Although those resources give extra information for opinion role extraction in comparison to syntactic or other surface features, we showed in this work that further taskspecific knowledge, i.e. either opinion verb types or a manually-built opinion role lexicon, provide even more accurate information. There has been a substantial amount of research on opinion target extraction. It focuses, however, on the extraction of topic-specific opinion terms (Jijkoun et al., 2010; Qiu et al., 2011) rather than the variability of sema"
K15-1022,kingsbury-palmer-2002-treebank,0,0.485079,"ion that those verbs can be divided into three different types. Each type has a characteristic mapping between semantic roles and opinion holders and targets. Thus, the problem of opinion role induction is reduced to automatically categorizing opinion verbs. 215 Proceedings of the 19th Conference on Computational Language Learning, pages 215–225, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics Our lexicon covers the 1175 verb lemmas contained in the Subjectivity Lexicon (Wilson et al., 2005). We annotated the semantic roles similar to the format of PropBank (Kingsbury and Palmer, 2002). The basis of the annotation were online dictionaries (e.g. Macmillan Dictionary) which provide both a verb definition and example sentences. We do not annotate implicature-related information about effects (Deng and Wiebe, 2014) but inherent sentiment (the data release2 includes more details regarding the annotation process and our notion of holders and targets). On a sample of 400 verbs, we measured an interannotation agreement of Cohen’s κ = 60.8 for opinion holders, κ = 62.3 for opinion targets and κ = 59.9 for speaker views. This agreement is mostly substantial (Landis and Koch, 1977). s"
K15-1022,N10-1138,0,0.142252,"Missing"
K15-1022,P03-1054,0,0.005052,"ly applied (9) Opponents claim these arguments miss the point. (10) Critics argued that the proposed limits were unconstitutional. For PT-verbs, we make use of the adjective heuristic proposed by Wiegand and Klakow (2012). The 218 not only to noun categorization (Riloff and Shepherd, 1997; Roark and Charniak, 1998) but also to different tasks in sentiment analysis, including polarity classification (Hatzivassiloglou and McKeown, 1997), the induction of patient polarity verbs (Goyal et al., 2010) and connotation learning (Kang et al., 2014). We use the dependency relation from Stanford parser (Klein and Manning, 2003) to detect coordination (16). Rec F1 45.7 8.9 14.2 87.0 33.3 9.8 20.9 17.6 Coordination kNN graph 45.2 42.7 61.5 68.7 47.3 39.7 53.4 50.4 WordNet kNN graph 52.8 51.1 51.5 51.9 50.7 51.5 51.1 51.5 Embedding kNN graph 59.3 64.0 58.4 70.5 61.0 59.4 59.7 64.5 Dependency kNN graph 65.7 70.3 63.8 72.0 65.4 68.0 64.5 70.6 5 Experiments As a similarity function, we simply take the absolute frequency of observing two words w1 and w2 in a conjunction, i.e. sim(w1 , w2 ) = f req(conj(w1 , w2 )). 5.1 Evaluation of the Induced Lexicon Dependency-based Similarity The metric proposed by Lin (1998) exploits t"
K15-1022,S12-1029,0,0.0532905,"Missing"
K15-1022,W09-4635,0,0.170875,"hose verbs by frequency. Size and verb type distribution are preserved. We also examine what impact doubling the size of seeds (Gold|Patterndouble ) and halving them (Gold|Patternhalf ) has on classification. Dependency-based similarity and graph clustering is used for all configurations. Only if we double the amount of seeds are the gold seeds notably better than the automatically generated seeds. Since our induction approach just requires a sentiment lexicon and aims at low-resource languages, we replicated the experiments for German, as shown in Table 9. We use the PolArtsentiment lexicon (Klenner et al., 2009) (1416 entries). (As a gold standard, we manually annotated that lexicon according to our three verb types.) As an unlabeled corpus, we chose the Huge German Corpus6 . As a parser, we used ParZu (Sennrich et al., 2009). Instead of WordNet, we used GermaNet (Hamp and Feldweg, 1997). The automatically generated seeds were manually translated from English to German. Table 9 shows that as on English data, dependency-based similarity combined with graph clustering performs best. The fact that we can successfully replicate our approach in another language supports the general applicability of our pr"
K15-1022,E14-1040,0,0.0742685,"orizing opinion verbs. 215 Proceedings of the 19th Conference on Computational Language Learning, pages 215–225, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics Our lexicon covers the 1175 verb lemmas contained in the Subjectivity Lexicon (Wilson et al., 2005). We annotated the semantic roles similar to the format of PropBank (Kingsbury and Palmer, 2002). The basis of the annotation were online dictionaries (e.g. Macmillan Dictionary) which provide both a verb definition and example sentences. We do not annotate implicature-related information about effects (Deng and Wiebe, 2014) but inherent sentiment (the data release2 includes more details regarding the annotation process and our notion of holders and targets). On a sample of 400 verbs, we measured an interannotation agreement of Cohen’s κ = 60.8 for opinion holders, κ = 62.3 for opinion targets and κ = 59.9 for speaker views. This agreement is mostly substantial (Landis and Koch, 1977). semantic roles for holders and targets. 2 Lexicon-based Opinion Role Extraction Opinion holder and target extraction is a hard task (Ruppenhofer et al., 2008). Conventional syntactic or semantic levels of representation do not capt"
K15-1022,esuli-sebastiani-2006-sentiwordnet,0,0.272652,"e word embedding method, we induce vectors for our opinion verbs with Word2Vec (Mikolov et al., 2013). Baroni et al. (2014) showed that this method outperforms count vector representations on a variety of tasks. For the similarity between two verbs, we compute the cosine-similarity between their vectors. 4.2.2 WordNet::Similarity We use WordNet::Similarity (Pedersen et al., 2004) as an alternative source for similarity metrics. The metrics are based on WordNet’s graph structure (Miller et al., 1990). Various relations within WordNet have been shown to be effective for polarity classification (Esuli and Sebastiani, 2006; Rao and Ravichandran, 2009). Pattern-based Seed Initialization For AG-verbs, we rely on the findings of Wiegand and Klakow (2012) who suggest that verbs predictive for opinion holders can be induced with the help of prototypical opinion holders. These common nouns, e.g. opponents (9) or critics (10), act like opinion holders and, therefore, can be seen as a proxy. Verbs co-occurring with prototypical opinion holders do not represent the entire range of opinion verbs but coincide with AG-verbs. 4.2.3 Coordination Another method to measure similarity is obtained by leveraging coordination. Coo"
K15-1022,P05-1045,0,0.0246464,"Missing"
K15-1022,D10-1008,0,0.0168574,"ntactic relation that also preserves great semantic coherence (Ziering et al., 2013), e.g. (15). It has been successfully applied (9) Opponents claim these arguments miss the point. (10) Critics argued that the proposed limits were unconstitutional. For PT-verbs, we make use of the adjective heuristic proposed by Wiegand and Klakow (2012). The 218 not only to noun categorization (Riloff and Shepherd, 1997; Roark and Charniak, 1998) but also to different tasks in sentiment analysis, including polarity classification (Hatzivassiloglou and McKeown, 1997), the induction of patient polarity verbs (Goyal et al., 2010) and connotation learning (Kang et al., 2014). We use the dependency relation from Stanford parser (Klein and Manning, 2003) to detect coordination (16). Rec F1 45.7 8.9 14.2 87.0 33.3 9.8 20.9 17.6 Coordination kNN graph 45.2 42.7 61.5 68.7 47.3 39.7 53.4 50.4 WordNet kNN graph 52.8 51.1 51.5 51.9 50.7 51.5 51.1 51.5 Embedding kNN graph 59.3 64.0 58.4 70.5 61.0 59.4 59.7 64.5 Dependency kNN graph 65.7 70.3 63.8 72.0 65.4 68.0 64.5 70.6 5 Experiments As a similarity function, we simply take the absolute frequency of observing two words w1 and w2 in a conjunction, i.e. sim(w1 , w2 ) = f req(con"
K15-1022,W97-0802,0,0.282723,"gurations. Only if we double the amount of seeds are the gold seeds notably better than the automatically generated seeds. Since our induction approach just requires a sentiment lexicon and aims at low-resource languages, we replicated the experiments for German, as shown in Table 9. We use the PolArtsentiment lexicon (Klenner et al., 2009) (1416 entries). (As a gold standard, we manually annotated that lexicon according to our three verb types.) As an unlabeled corpus, we chose the Huge German Corpus6 . As a parser, we used ParZu (Sennrich et al., 2009). Instead of WordNet, we used GermaNet (Hamp and Feldweg, 1997). The automatically generated seeds were manually translated from English to German. Table 9 shows that as on English data, dependency-based similarity combined with graph clustering performs best. The fact that we can successfully replicate our approach in another language supports the general applicability of our proposed categorization of verbs into three types for opinion role extraction. SP-verbs graph Embedd. Table 9: Comparison of English and German data (evaluation measure: macro-average F-score). Table 6: The 12 most similar verbs to outrage (PT-verb) according to the different metric"
K15-1022,P98-2127,0,0.0844581,"in and Manning, 2003) to detect coordination (16). Rec F1 45.7 8.9 14.2 87.0 33.3 9.8 20.9 17.6 Coordination kNN graph 45.2 42.7 61.5 68.7 47.3 39.7 53.4 50.4 WordNet kNN graph 52.8 51.1 51.5 51.9 50.7 51.5 51.1 51.5 Embedding kNN graph 59.3 64.0 58.4 70.5 61.0 59.4 59.7 64.5 Dependency kNN graph 65.7 70.3 63.8 72.0 65.4 68.0 64.5 70.6 5 Experiments As a similarity function, we simply take the absolute frequency of observing two words w1 and w2 in a conjunction, i.e. sim(w1 , w2 ) = f req(conj(w1 , w2 )). 5.1 Evaluation of the Induced Lexicon Dependency-based Similarity The metric proposed by Lin (1998) exploits the rich set of dependency-relation labels in the context of distributional similarity. Moreover, it has been effectively used for the related task of extending frames of unknown predicates in semantic parsing (Das and Smith, 2011). The metric is based on dependency triples (w, r, w′ ) where w and w′ are words and r is a dependency relation (e.g. (argue-V, nsubj, critics-N)). The metric is defined as: P 1 ,r,w)+I(w2 ,r,w)) (r,w)∈T (w1 )∩T (w2 ) (I(w P (r,w)∈T (w1 ) I(w1 ,r,w)+ (r,w)∈T (w2 ) I(w2 ,r,w) kw,r,w′ k×k∗,r,∗k I(w, r, w′ ) = log kw,r,∗k×k∗,r,w′ k sim(w1 , w2 ) = P where and"
K15-1022,D13-1171,0,0.0264352,"Missing"
K15-1022,N04-3012,0,0.0630317,"ducing prototypical positive SP-verbs, such as stabilize, allay or heal. 4.2 Similarity Metrics 4.2.1 Word Embeddings Recent research in machine learning has focused on inducing vector representations of words. As an example of a competitive word embedding method, we induce vectors for our opinion verbs with Word2Vec (Mikolov et al., 2013). Baroni et al. (2014) showed that this method outperforms count vector representations on a variety of tasks. For the similarity between two verbs, we compute the cosine-similarity between their vectors. 4.2.2 WordNet::Similarity We use WordNet::Similarity (Pedersen et al., 2004) as an alternative source for similarity metrics. The metrics are based on WordNet’s graph structure (Miller et al., 1990). Various relations within WordNet have been shown to be effective for polarity classification (Esuli and Sebastiani, 2006; Rao and Ravichandran, 2009). Pattern-based Seed Initialization For AG-verbs, we rely on the findings of Wiegand and Klakow (2012) who suggest that verbs predictive for opinion holders can be induced with the help of prototypical opinion holders. These common nouns, e.g. opponents (9) or critics (10), act like opinion holders and, therefore, can be seen"
K15-1022,J11-1002,0,0.0429458,"role labelers based on FrameNet (Kim and Hovy, 2006) or PropBank (Johansson and Moschitti, 2013; Wiegand and Klakow, 2012). Although those resources give extra information for opinion role extraction in comparison to syntactic or other surface features, we showed in this work that further taskspecific knowledge, i.e. either opinion verb types or a manually-built opinion role lexicon, provide even more accurate information. There has been a substantial amount of research on opinion target extraction. It focuses, however, on the extraction of topic-specific opinion terms (Jijkoun et al., 2010; Qiu et al., 2011) rather than the variability of semantic roles for opinion holders and targets. Mitchell et al. (2013) present a low-resource approach for target extraction but their aim is to process Twitter messages without using general syntax tools. In this work, we use such tools. Our notion of low resources is different in that we mean the absence of semantic resources helpful for our task (e.g. FrameNet). 7 Conclusion We presented an approach for opinion role induction for verbal predicates. We assume that those predicates can be divided into three different verb types where each type is associated wit"
K15-1022,E09-1077,0,0.181561,"induce vectors for our opinion verbs with Word2Vec (Mikolov et al., 2013). Baroni et al. (2014) showed that this method outperforms count vector representations on a variety of tasks. For the similarity between two verbs, we compute the cosine-similarity between their vectors. 4.2.2 WordNet::Similarity We use WordNet::Similarity (Pedersen et al., 2004) as an alternative source for similarity metrics. The metrics are based on WordNet’s graph structure (Miller et al., 1990). Various relations within WordNet have been shown to be effective for polarity classification (Esuli and Sebastiani, 2006; Rao and Ravichandran, 2009). Pattern-based Seed Initialization For AG-verbs, we rely on the findings of Wiegand and Klakow (2012) who suggest that verbs predictive for opinion holders can be induced with the help of prototypical opinion holders. These common nouns, e.g. opponents (9) or critics (10), act like opinion holders and, therefore, can be seen as a proxy. Verbs co-occurring with prototypical opinion holders do not represent the entire range of opinion verbs but coincide with AG-verbs. 4.2.3 Coordination Another method to measure similarity is obtained by leveraging coordination. Coordination is known to be a sy"
K15-1022,W97-0313,0,0.144381,"n holders do not represent the entire range of opinion verbs but coincide with AG-verbs. 4.2.3 Coordination Another method to measure similarity is obtained by leveraging coordination. Coordination is known to be a syntactic relation that also preserves great semantic coherence (Ziering et al., 2013), e.g. (15). It has been successfully applied (9) Opponents claim these arguments miss the point. (10) Critics argued that the proposed limits were unconstitutional. For PT-verbs, we make use of the adjective heuristic proposed by Wiegand and Klakow (2012). The 218 not only to noun categorization (Riloff and Shepherd, 1997; Roark and Charniak, 1998) but also to different tasks in sentiment analysis, including polarity classification (Hatzivassiloglou and McKeown, 1997), the induction of patient polarity verbs (Goyal et al., 2010) and connotation learning (Kang et al., 2014). We use the dependency relation from Stanford parser (Klein and Manning, 2003) to detect coordination (16). Rec F1 45.7 8.9 14.2 87.0 33.3 9.8 20.9 17.6 Coordination kNN graph 45.2 42.7 61.5 68.7 47.3 39.7 53.4 50.4 WordNet kNN graph 52.8 51.1 51.5 51.9 50.7 51.5 51.1 51.5 Embedding kNN graph 59.3 64.0 58.4 70.5 61.0 59.4 59.7 64.5 Dependenc"
K15-1022,P98-2182,0,0.118416,"the entire range of opinion verbs but coincide with AG-verbs. 4.2.3 Coordination Another method to measure similarity is obtained by leveraging coordination. Coordination is known to be a syntactic relation that also preserves great semantic coherence (Ziering et al., 2013), e.g. (15). It has been successfully applied (9) Opponents claim these arguments miss the point. (10) Critics argued that the proposed limits were unconstitutional. For PT-verbs, we make use of the adjective heuristic proposed by Wiegand and Klakow (2012). The 218 not only to noun categorization (Riloff and Shepherd, 1997; Roark and Charniak, 1998) but also to different tasks in sentiment analysis, including polarity classification (Hatzivassiloglou and McKeown, 1997), the induction of patient polarity verbs (Goyal et al., 2010) and connotation learning (Kang et al., 2014). We use the dependency relation from Stanford parser (Klein and Manning, 2003) to detect coordination (16). Rec F1 45.7 8.9 14.2 87.0 33.3 9.8 20.9 17.6 Coordination kNN graph 45.2 42.7 61.5 68.7 47.3 39.7 53.4 50.4 WordNet kNN graph 52.8 51.1 51.5 51.9 50.7 51.5 51.1 51.5 Embedding kNN graph 59.3 64.0 58.4 70.5 61.0 59.4 59.7 64.5 Dependency kNN graph 65.7 70.3 63.8"
K15-1022,W12-3716,1,0.849551,"that we can compensate some lexical knowledge missing in induction by standard features. Since we could substantially outperform the features relying on FrameNet with our new lexical resources, we looked closer at the predicted frame structures. Beside obvious errors in automatic frame assignment, we also found that there are problems inherent in the frame design. Particularly, the notion of SP-verbs (§3.3) is not properly reflected. Many frames, such as S CRUTINY, typically devised for AG-verbs, such as investigate or analyse, also contain SP-verbs like pry. This observation is in line with Ruppenhofer and Rehbein (2012) who claim that extensions to FrameNet are necessary to properly represent opinions evoked by verbal predicates. 5.3 FICTION CK + coarse-grain lex 66.82∗ 64.13∗◦ 63.72∗◦† CK + fine-grain lex 66.16∗ 64.98∗◦ 70.85∗◦†‡ statistical significance testing (permutation test, significance level p &lt; 0.05) ∗ : better than CK; ◦ : better than CK + inducWiegand 2012 ; † : better than CK + inducgraph ; ‡ : better than CK + coarse-grain lex Distribution of Verb Types Corpus MPQA CK CK + inducWiegand CK + inducgraph Table 12: Lexical resources and the impact of other (not lexicon-based) features (evaluation m"
K15-1022,D08-1061,0,0.444093,"st neighbour classifier (kNN) (Cover and Hart, 1967) as a simple method for propagating labels from seeds to other instances. Alternatively, we consider verb categorization as a clustering task on a graph G = (V, E, W ) where V is the set of nodes (i.e. our opinion verbs), E is the set of edges connecting them with weights W : E → R+ . W can be directly derived from any of the similarity metrics (§4.2.1-§4.2.4). The aim is that all nodes v ∈ V are assigned a label l ∈ {AG, P T, SP }. Initially, only the verb seeds are labeled. We then use the Adsorption label propagation algorithm from junto (Talukdar et al., 2008) in order propagate the labels from the seeds to the remaining verbs. 219 Table 5 compares the performance of the different similarity metrics when incorporated in either kNN or graph clustering. The resulting categorizations are compared against the gold standard coarse-grained lexicon (§3.4). For kNN, we set k = 3 for which we obtained best performance in all our experiments. As seeds, we took the top 40 AG-verbs, 30 PTverbs and 50 SP-verbs produced by the respective initialization methods (§4.1). The seed proportions should vaguely correspond to the actual class distribution (Table 3). Larg"
K15-1022,P06-1134,0,0.0163662,"AG-view verbs with AG- and SP-view verbs with AG- and PT-view Relation to Fine-Grained Lexicon Table 2 provides statistics as to how clear-cut the three prototypical verb types are in the manuallycompiled fine-grained lexicon. These numbers suggest that many verbs evoke several opinion views (e.g. a verb with an AG-view may also evoke a PT-view). While the fine-grained lexicon is fairly exhaustive in listing semantic roles for opinion holders and targets, it may also occasionally overgenerate. One major reason for this is that we do not annotate on the sense-level (word-sense disambiguation (Wiebe and Mihalcea, 2006) is still in its infancy) but on the lemmalevel. Accordingly, we attribute all views to all senses, whereas actually certain views pertain only to specific senses. However, we found that usually one view is conveyed by most (if not all) senses of a word. For example, the lexicon lists both an AGview and a PT-view for appease. This is correct (5) (6) (7) (8) [Chamberlain]agent appeased [Hitler]patient . [The orange juice]agent appeased [him]patient for a while. [Mary]agent degrades [Henrietta]patient . [This technique]agent degrades [the local water supply]patient . 4 Induction of Verb Categori"
K15-1022,E12-1033,1,0.115441,", conspire authors make use of the observation that morphologically related adjectives exist for PT-verbs, unlike for AG- and SP-verbs. Therefore, in order to extract PT-verbs, one needs to check whether a verb in its past participle form, such as upset in (11), is identical to some predicate adjective (12). Table 4: The top 12 extracted verb seeds. ond step, a similarity metric (§4.2) is employed in order to propagate the verb type labels from the seeds to the remaining opinion verbs (§4.3). The North American News Text Corpus is used for seed extraction and computation of verb similarities. Wiegand and Klakow (2012) proposed methods for extracting AG- and PT-verbs. We will re-use these methods for generating seeds. A major contribution of this paper is the introduction of the third dimension, i.e. SP-verbs, in the context of induction. We show that in combination with this third dimension, one can categorize all opinion verbs contained in a sentiment lexicon. Furthermore, given this three-way classification, we also obtain better results on the detection of AG-verbs and PT-verbs than by just detecting those verbs in isolation without graph clustering (this will be shown in Table 7 and discussed in §5.1)."
K15-1022,H05-1044,0,0.0270769,"ion verbs that can be found in a common sentiment lexicon. Our model rests on the assumption that those verbs can be divided into three different types. Each type has a characteristic mapping between semantic roles and opinion holders and targets. Thus, the problem of opinion role induction is reduced to automatically categorizing opinion verbs. 215 Proceedings of the 19th Conference on Computational Language Learning, pages 215–225, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics Our lexicon covers the 1175 verb lemmas contained in the Subjectivity Lexicon (Wilson et al., 2005). We annotated the semantic roles similar to the format of PropBank (Kingsbury and Palmer, 2002). The basis of the annotation were online dictionaries (e.g. Macmillan Dictionary) which provide both a verb definition and example sentences. We do not annotate implicature-related information about effects (Deng and Wiebe, 2014) but inherent sentiment (the data release2 includes more details regarding the annotation process and our notion of holders and targets). On a sample of 400 verbs, we measured an interannotation agreement of Cohen’s κ = 60.8 for opinion holders, κ = 62.3 for opinion targets"
K15-1022,P94-1019,0,0.527485,"increases of the seed sets do not improve the quality (as shown below). 10 of the 50 SP-verbs are extracted from the positive SP-patterns, while the remaining verbs are extracted from the negative SP-patterns (§4.1). As baselines, we include a classifier only employing the seeds and a majority class classifier always predicting an SP-verb. For word embeddings (§4.2.1) and WordNet::Similarity (§4.2.2), we only report the performance of the best metric/configuration, i.e. for embeddings, the continuous bag-of-words model with 500 dimensions and for WordNet::Similarity, the Wu & Palmer measure (Wu and Palmer, 1994). Table 5 shows that the baselines can be outperformed by large margins. The performance of the different similarity metrics varies. The dependency-based metric performs notably better than the other metrics. Together with word embeddings, it is the only metric for which graph clustering produces a notable improvement over kNN. Table 6 illustrates the quality of the similarity metrics for the present task. The table shows that the dependency-based similarity metric provides the most suitable output. The poor quality of coordination may come as a surprise. That Coordin. appear, believe, refuse,"
K15-1022,P13-1161,0,0.0805788,"Missing"
K15-1022,I13-1188,0,0.0340422,"Missing"
K15-1022,C98-1013,0,\N,Missing
K15-1022,C98-2177,0,\N,Missing
K15-1022,C98-2122,0,\N,Missing
L18-1097,D09-1020,0,0.031681,"analysis (Flekova and Gurevych, 2016). 3.3. Polarity Information We assume that many ambiguous shifters convey a negation if they co-occur with polar expressions. This is illustrated in Table 4. Therefore, we count the number of polar expressions in a sentence to be classified. We identify such expressions with the help of the Subjectivity Lexicon (Wilson et al., 2005). 3.4. Focused Features Our task can be considered as a word-sense disambiguation (WSD) task. Therefore, we should also consider a feature set established in previous work on WSD. Table 5 lists those features mainly inspired by Akkaya et al. (2009). 2 Since we are not aware of any robust open-domain wordsense disambiguation software, we always consider the union of all synsets associated with a particular lemma. Feature bag of words embeddings bag of words + embed. unknown verbs Acc F1 63.8 59.6 64.4 60.8 64.3 60.7 known verbs Acc F1 70.7 67.9 70.8 68.1 70.9 68.2 Table 6: Bag of words vs. word embeddings. They have in common that they all only consider a very local context of the mention of the verbal shifter (i.e. typically its dependents). Some of these features incorporate syntactic information. We use the Stanford Parser (Chen and M"
L18-1097,J92-4003,0,0.572721,"se unknown words and words observed in the training data. Table 3 illustrates for the ambiguous verb spoil that the objects for the sense negation and no negation are of a specific semantic type. For the sense negation, the objects represent some form of activity, while for the sense no negation, they typically represent some human being (or at least some animate entity). These observed selectional preferences are an indication that some form of generalization of the context words might help for this task. Brown Clustering. A popular data-driven word generalization method is Brown Clustering (Brown et al., 1992). This is an unsupervised clustering method in which words with the similar distributional contexts are automatically assigned the same clusters. Clusters therefore represent a set Sense Contexts Type negation spoil a(n) {effort|idea|fun} activity no negation spoil one’s {partner|girlfriend|spouse} human Table 3: Illustration of word generalization. 1 https://github.com/miwieg/lrec2018 609 [Negation Sense [clear someone of murder− ]+ [spoil someone’s efforts+ ]− [peace+ crumples]− [cost someone their inner peace+ ]− No Negation Sense sky cleared spoil one’s spouse crumple a shirt cost some amo"
L18-1097,D14-1082,0,0.032304,"al. (2009). 2 Since we are not aware of any robust open-domain wordsense disambiguation software, we always consider the union of all synsets associated with a particular lemma. Feature bag of words embeddings bag of words + embed. unknown verbs Acc F1 63.8 59.6 64.4 60.8 64.3 60.7 known verbs Acc F1 70.7 67.9 70.8 68.1 70.9 68.2 Table 6: Bag of words vs. word embeddings. They have in common that they all only consider a very local context of the mention of the verbal shifter (i.e. typically its dependents). Some of these features incorporate syntactic information. We use the Stanford Parser (Chen and Manning, 2014) to obtain that information. 4. Experiments We evaluate our features in a 10-fold crossvalidation. The classifier, we consider is a Support Vector Machine. As a tool we use SVMlight (Joachims, 1999). We consider two different evaluation settings. On the one hand, we arrange the verbs in such a way that the test data contain contexts of verbs which have not been observed in the training data. On the other hand, we ensure that all test data contain contexts of verbs that have been observed in the training data. As a baseline, we also list the performance of a majority-class classifier. For the b"
L18-1097,P16-1191,0,0.0274857,"ypernyms and Supersenses. We use WordNet (Miller et al., 1990) as a resource-based method for word generalization. WordNet is the largest lexical ontology for the English language. It is organized in word senses called synsets.2 By considering hypernyms of a synset representing some set of lemmas as a feature, we enable similar words, i.e. synonyms or near-synonyms, to have a feature in common. While hypernyms are a fairly fine-grained form of generalization, we also consider supersenses, a set of coarse-grained classes, which have previously been found to be effective for sentiment analysis (Flekova and Gurevych, 2016). 3.3. Polarity Information We assume that many ambiguous shifters convey a negation if they co-occur with polar expressions. This is illustrated in Table 4. Therefore, we count the number of polar expressions in a sentence to be classified. We identify such expressions with the help of the Subjectivity Lexicon (Wilson et al., 2005). 3.4. Focused Features Our task can be considered as a word-sense disambiguation (WSD) task. Therefore, we should also consider a feature set established in previous work on WSD. Table 5 lists those features mainly inspired by Akkaya et al. (2009). 2 Since we are n"
L18-1097,N06-2015,0,0.180691,"Missing"
L18-1097,P06-1014,0,0.0833171,"Missing"
L18-1097,W04-2807,0,0.137902,"Missing"
L18-1097,I17-1063,1,0.839989,"her word classes, particularly the content words, such as verbs, nouns and adjectives, there also exist words expressing negation. For example, in (2) the verb failed has a similar function as the negation word not in (1). These negation content words, which are also called shifters, are often excluded from discussion since there does not (yet) exist a commonly accepted resource with these expressions. Even though the frequency of a single negation function word is much higher than that of a shifter, the overall number of shifters is significantly larger than those of negation function words. Schulder et al. (2017) identified 980 verbal shifters while the popular negation word lexicon proposed by Wilson et al. (2005) only includes 15 negation function words. Moreover, since content words are more ambiguous than function words, we also envisage shifters to be more ambiguous than negation function words. In this paper, we address the ambiguity of shifters. We select a set of 20 ambiguous verbal shifters and try to disambiguate them automatically. We focus on verbal shifters since it has been recently shown that a large amount of such verbs exist and they are important to polarity classification (Schulder"
L18-1097,D07-1107,0,0.0952475,"Missing"
L18-1097,P10-1040,0,0.0491317,"cost some amount of money Feature subcategorization frame of verbal shifter hypernym(s) of dependents of verbal shifter supersense(s) of dependents of verbal shifter is a polar expression among dependents of verbal shifter? is verbal shifter coordinated with another verbal shifter? words representing dependents of verbal shifter Table 4: Polar expressions. to indicate negation sense. Table 5: Focused features of words rather than individual ones. In our experiments, we induce 1000 clusters from the North American News Text Corpus. This is a standard configuration proven to yield good results (Turian et al., 2010). We induce the clusters using the SRILM-toolkit (Stolcke, 2002). Word Embeddings. A more recent alternative to Brown clustering is the usage of word embeddings. Word embeddings are (dense) vector representations of words that are automatically induced from corpora. They are devised as a more robust alternative to bag of words. Unlike a one-hot bag-of-words vector representation where different words (no matter how similar they are in meaning) are always orthogonal to each other, embeddings allow different words which are distributionally similar, such as partner and spouse, also to have simil"
L18-1097,W10-3111,1,0.790519,"egation takes place. We present a supervised learning approach to disambiguating verbal shifters. Our approach takes into consideration various features, particularly generalization features. Keywords: sentiment analysis, negation modeling, word-sense disambiguation 1. Introduction Negation is one of the most central linguistic phenomena. Therefore, negation modeling is essential to various common tasks in natural language processing, such as relation extraction (Sanchez-Graillet and Poesio, 2007), recognition of textual entailment (Harabagiu et al., 2006) and particularly sentiment analysis (Wiegand et al., 2010). In the latter task, a negation typically reverses the polarity of polar expressions. For example, in (1), the negated positive polar expression pass conveys negative polarity. (1) Peter did [not [pass]+ ]− the exam. (2) Peter [failed to [pass]+ ]− the exam. So far, most research in negation modeling for sentiment analysis has focused on negation (function) words, such as the particle not or the adverbs no or never. However, among other word classes, particularly the content words, such as verbs, nouns and adjectives, there also exist words expressing negation. For example, in (2) the verb fa"
L18-1097,H05-1044,0,0.163968,"rds expressing negation. For example, in (2) the verb failed has a similar function as the negation word not in (1). These negation content words, which are also called shifters, are often excluded from discussion since there does not (yet) exist a commonly accepted resource with these expressions. Even though the frequency of a single negation function word is much higher than that of a shifter, the overall number of shifters is significantly larger than those of negation function words. Schulder et al. (2017) identified 980 verbal shifters while the popular negation word lexicon proposed by Wilson et al. (2005) only includes 15 negation function words. Moreover, since content words are more ambiguous than function words, we also envisage shifters to be more ambiguous than negation function words. In this paper, we address the ambiguity of shifters. We select a set of 20 ambiguous verbal shifters and try to disambiguate them automatically. We focus on verbal shifters since it has been recently shown that a large amount of such verbs exist and they are important to polarity classification (Schulder et al., 2017). Examples for ambiguous verbal shifters are clear, spoil, cloud or slump which in (3), (5)"
L18-1222,W16-0410,0,0.106925,"erbs, together with nouns, are the most important minimal semantic units in text and thus are prime candidates for being tackled first. Verbs are usually the main syntactic predicates of clauses and sentences and thus verbal shifters can be expected to project far-reaching 2 Note that the example also illustrates how distinguishing between items that induce a switch between polarities and others that affect intensity without changing overall polarity is an idealization. Simple syntactic negation of a polar adjective may influence intensity as well as polarity (e.g. not terrible 6= excellent) (Kiritchenko and Mohammad, 2016). scopes. Most nominal shifters (e.g. failure, loss), on the other hand, have morphologically related verbs (e.g. fail, lose) and we expect that this connection can be exploited to spread shifter classification from verbs to nouns in the future. Related to this, the grammar of verbs, for instance with respect to the diversity of scope types, is more complex than that of nouns and so we expect it to be easier to project from verbs to nouns rather than in the opposite direction. 2.3. Related Work Existing lexicons and corpora that cover polarity shifting focus almost exclusively on negation word"
L18-1222,morante-2010-descriptive,0,0.349171,"Missing"
L18-1222,pak-paroubek-2010-twitter,0,0.0150347,"this work focusses on verbs, due to their importance as minimal semantic units, far-reaching scopes and potential basis for nominal shifter lexicons (see §2.2.). Knowledge of polarity shifting is important for a variety of tasks, especially sentiment analysis (Wiegand et al., 2010; Liu, 2012; Wilson et al., 2005), as well as relation extraction (Sanchez-Graillet and Poesio, 2007) and textual entailment recognition (Harabagiu et al., 2006). The majority of research into polarity shifting for sentiment analysis has focussed on negation words (Wiegand et al., 2010; Schouten and Frasincar, 2016; Pak and Paroubek, 2010). Negation words (e.g. not, no, never) are mostly function words, of which only a small number exists, so exhaustive coverage is comparatively simple. Content word classes, such as verbs, are considerably more difficult to cover comprehensively due to their sheer number. For example, WordNet (Miller et al., 1990) contains over 10k verbal lemmas. Most verbs are also far less frequent than 2. Background In this section we will provide a formal definition of polarity shifters (§2.1.), motivate our focus on verbal shifters (§2.2.) and discuss related work (§2.3.). 2.1. Polarity Shifters The notion"
L18-1222,W15-2910,1,0.700122,"rsed himdobj [for his + expenses]− pobj ] .” as pobj for. Clausal complement(comp): The verbal shifter affects a clausal complement, such as infinitive clauses like“He − [failed [to pass the exam]+ comp ] .” or gerunds like + “She [stopped [using drugs]− comp ] .” The given scopes assume that verb phrases are in their active form. In passive phrases, subject and object roles are inverted. To avoid this issue, sentence structure normalization should be performed before computing shifter scope. Synsets in WordNet only capture the semantic similarity of words, but almost no syntactic properties (Ruppenhofer and Brandes, 2015). The shifter scope of a verb depends on its syntactic arguments, which can differ between verbs of the same synset. For example, discard and dispose share the sense “throw or cast away”, but while discard shifts its direct object (10), dispose requires a prepositional object (11). For this reason we annotate lemma-synset pairs individually, instead of assigning scope labels to an entire synset. (10) He [discarded [the evidence]+ ]− . dobj (11) He [disposed [of the evidence]+ ]− . pobj We also consider cases where a verbal shifter has more than one potential scope for the same lemma-sense pair"
L18-1222,S16-1084,0,0.0756036,"excellent.”), something being somewhat good bounds its positiveness and opens up more negative meanings (“The performance was somewhat good, but overall rather disappointing”). Considering these properties of scales, one can see shifting at work even in the case of downtoning. 2.2. Verbal Shifters While the inclusion of shifting and scalar semantics in semantic representations is not limited to lexical items of particular parts-of-speech – we also find shifter adjectives (e.g. devoid) and adverbs (e.g. barely) – we limit our work to verbal shifters for several reasons. As shown by the work of Schneider et al. (2016), verbs, together with nouns, are the most important minimal semantic units in text and thus are prime candidates for being tackled first. Verbs are usually the main syntactic predicates of clauses and sentences and thus verbal shifters can be expected to project far-reaching 2 Note that the example also illustrates how distinguishing between items that induce a switch between polarities and others that affect intensity without changing overall polarity is an idealization. Simple syntactic negation of a polar adjective may influence intensity as well as polarity (e.g. not terrible 6= excellent"
L18-1222,I17-1063,1,0.779913,"k Existing lexicons and corpora that cover polarity shifting focus almost exclusively on negation words. The most complex negation lexicon for sentiment analysis (Wilson et al., 2005) includes a mere 12 verbal shifters. In contrast, our resource covers over 1200 verbal shifter lemmas. Corpora used as training data for negation processing, such as the Sentiment Treebank (Socher et al., 2013) or the BioScope corpus (Szarvas et al., 2008), are fairly small datasets, so only the most frequent negation words appear. The BioScope corpus, for example, contains only 6 verbal shifters (Morante, 2010). Schulder et al. (2017) show that state-of-the-art systems trained on such data do not reliably detect polarity shifting and should profit from explicit knowledge of verbal shifters. The only work to date that covers a larger number of verbal shifters is Schulder et al. (2017), who annotate a sample of the English verbs found in WordNet for whether they exhibit polarity shifting. They start by manually annotating an initial 2000 verbs. These verbs are used to train an SVM classifier using linguistic features and common language resources. The classifier is then run on the remaining WordNet verbs to bootstrap a list"
L18-1222,D13-1170,0,0.0168397,"Missing"
L18-1222,W08-0606,0,0.0586375,"Missing"
L18-1222,W10-3111,1,0.881473,"i) Annotations for shifter scope, indicating which parts of a sentence are affected by the shifting. The entire dataset is publicly available.1 (4) The new treatment has [alleviatedshifter her [pain]− ]+ . Polarity shifting is also caused by other content word classes, such as nouns (e.g. downfall) and adjectives (e.g. devoid). However, this work focusses on verbs, due to their importance as minimal semantic units, far-reaching scopes and potential basis for nominal shifter lexicons (see §2.2.). Knowledge of polarity shifting is important for a variety of tasks, especially sentiment analysis (Wiegand et al., 2010; Liu, 2012; Wilson et al., 2005), as well as relation extraction (Sanchez-Graillet and Poesio, 2007) and textual entailment recognition (Harabagiu et al., 2006). The majority of research into polarity shifting for sentiment analysis has focussed on negation words (Wiegand et al., 2010; Schouten and Frasincar, 2016; Pak and Paroubek, 2010). Negation words (e.g. not, no, never) are mostly function words, of which only a small number exists, so exhaustive coverage is comparatively simple. Content word classes, such as verbs, are considerably more difficult to cover comprehensively due to their s"
L18-1222,H05-1044,0,0.593442,"indicating which parts of a sentence are affected by the shifting. The entire dataset is publicly available.1 (4) The new treatment has [alleviatedshifter her [pain]− ]+ . Polarity shifting is also caused by other content word classes, such as nouns (e.g. downfall) and adjectives (e.g. devoid). However, this work focusses on verbs, due to their importance as minimal semantic units, far-reaching scopes and potential basis for nominal shifter lexicons (see §2.2.). Knowledge of polarity shifting is important for a variety of tasks, especially sentiment analysis (Wiegand et al., 2010; Liu, 2012; Wilson et al., 2005), as well as relation extraction (Sanchez-Graillet and Poesio, 2007) and textual entailment recognition (Harabagiu et al., 2006). The majority of research into polarity shifting for sentiment analysis has focussed on negation words (Wiegand et al., 2010; Schouten and Frasincar, 2016; Pak and Paroubek, 2010). Negation words (e.g. not, no, never) are mostly function words, of which only a small number exists, so exhaustive coverage is comparatively simple. Content word classes, such as verbs, are considerably more difficult to cover comprehensively due to their sheer number. For example, WordNet"
N10-1121,H05-1045,0,0.808081,"ion of boundaries of the structures for the convolution kernels is less straightforward in opinion holder extraction. The aim of this paper is to explore in how far convolution kernels can be beneficial for effective opinion holder detection. We are not only interested in how far different kernel types contribute to this extraction task but we also contrast the performance of these kernels with a manually designed feature set used as a standard vector kernel. Finally, we also examine the effectiveness of expanding word sequences or syntactic trees by additional prior knowledge. 2 Related Work Choi et al. (2005) examine opinion holder extraction using CRFs with various manually defined linguistic features and patterns automatically learnt by the AutoSlog system (Riloff, 1996). The linguistic features focus on named-entity information and syntactic relations to opinion words. In this paper, we use very similar settings. The features presented in Kim and Hovy (2005) and Bloom et al. (2007) resemble very much Choi et al. (2005). Bloom et al. (2007) also consider communication words to be predictive cues for opinion holders. Kim and Hovy (2006) and Bethard et al. (2005) explore the usefulness of semantic"
N10-1121,W06-1651,0,0.378973,"rmation and syntactic relations to opinion words. In this paper, we use very similar settings. The features presented in Kim and Hovy (2005) and Bloom et al. (2007) resemble very much Choi et al. (2005). Bloom et al. (2007) also consider communication words to be predictive cues for opinion holders. Kim and Hovy (2006) and Bethard et al. (2005) explore the usefulness of semantic roles provided by FrameNet (Fillmore et al., 2003) for both opinion holder and opinion target extraction. Due to data sparseness, Kim and Hovy (2006) expand FrameNet data by using an unsupervised clustering algorithm. Choi et al. (2006) is an extension of Choi et al. (2005) in that opinion holder extraction is learnt jointly with opinion detection. This requires that opinion expressions and their relations to opinion holders are annotated in the training data. Semantic roles are also taken as a potential source of information. In our work, we deliberately work with minimal annotation and, thus, do not consider any labeled opinion expressions and relations to opinion holders in the training data. We exclusively rely on entities marked as opinion holders. In many practical situations, the annotation beyond opinion holder label"
N10-1121,P05-1045,0,0.0302596,"ith the SVM-Light-TK toolkit7 . We evaluated on the basis of exact phrase matching. We set the trade-off parameter j = 5 for all feature sets. For the manual feature set we used a polynomial kernel of third degree. These two critical parameters were tuned on the development set. As far as the sequence and tree kernels are concerned, we used the parameter settings from Moschitti (2008), i.e. λ = 0.4 and µ = 0.4. Kernels were combined using plain summation. The documents were parsed using the Stanford Parser (Klein and Manning, 2003). Namedentity information was obtained by the Stanford tagger (Finkel et al., 2005). Semantic roles were obtained by using the parser by Zhang et al. (2008). Opinion expressions were identified using the Subjectivity Lexicon from the MPQA project (Wilson et al., 2005). Communication words were obtained by using the Appraisal Lexicon (Bloom et al., 2007). Nominalizations were recognized by looking relates to the candidate opinion holder. 7 available at disi.unitn.it/moschitti Figure 2: Illustration of the different scopes on a CON STAUG ; nodes belonging to the candidate opinion holder are marked with CAN D . up nouns in NOMLEX (Macleod et al., 1998). 5.1 Each Notation kernel"
N10-1121,W06-0301,0,0.176676,"syntactic trees by additional prior knowledge. 2 Related Work Choi et al. (2005) examine opinion holder extraction using CRFs with various manually defined linguistic features and patterns automatically learnt by the AutoSlog system (Riloff, 1996). The linguistic features focus on named-entity information and syntactic relations to opinion words. In this paper, we use very similar settings. The features presented in Kim and Hovy (2005) and Bloom et al. (2007) resemble very much Choi et al. (2005). Bloom et al. (2007) also consider communication words to be predictive cues for opinion holders. Kim and Hovy (2006) and Bethard et al. (2005) explore the usefulness of semantic roles provided by FrameNet (Fillmore et al., 2003) for both opinion holder and opinion target extraction. Due to data sparseness, Kim and Hovy (2006) expand FrameNet data by using an unsupervised clustering algorithm. Choi et al. (2006) is an extension of Choi et al. (2005) in that opinion holder extraction is learnt jointly with opinion detection. This requires that opinion expressions and their relations to opinion holders are annotated in the training data. Semantic roles are also taken as a potential source of information. In ou"
N10-1121,kingsbury-palmer-2002-treebank,0,0.104533,"G in Figure 2). All sources used for this type of generalization are known to be predictive for opinion holder classification (Choi et al., 2005; Kim and Hovy, 2005; Choi et al., 2006; Kim and Hovy, 2006; Bloom et al., 2007). Note that the grammatical relation paths, i.e. GRAMW RD and GRAMP OS , can only be applied in case there is another expression in the focus in addition to the candidate of the data instance itself, e.g. the nearest opinion expression to the candidate. Section 4.4 explains in detail how this is done. Predicate-argument structures (P AS) are represented by PropBank trees (Kingsbury and Palmer, 2002). 4.2 Support Vector Machines and Kernel Methods Support Vector Machines (SVMs) are one of the most robust supervised machine learning techniques in which training data instances ~x are separated by a hyperplane H(~x) = w ~ · ~x + b = 0 where w ∈ Rn and b ∈ R. One advantage of SVMs is that kernel methods can be applied which map the data to other feature spaces in which they can be separated more easily. Given a feature function φ : O → R, where O is the set of the objects, the kernel trick allows the decision hyperplane to be rewritten as: ! X yi αi ~xi · ~x + b = H(~x) = i=1...l X i=1...l yi"
N10-1121,P03-1054,0,0.00360156,"is of a paired t-test using 0.05 as the significance level. All experiments were done with the SVM-Light-TK toolkit7 . We evaluated on the basis of exact phrase matching. We set the trade-off parameter j = 5 for all feature sets. For the manual feature set we used a polynomial kernel of third degree. These two critical parameters were tuned on the development set. As far as the sequence and tree kernels are concerned, we used the parameter settings from Moschitti (2008), i.e. λ = 0.4 and µ = 0.4. Kernels were combined using plain summation. The documents were parsed using the Stanford Parser (Klein and Manning, 2003). Namedentity information was obtained by the Stanford tagger (Finkel et al., 2005). Semantic roles were obtained by using the parser by Zhang et al. (2008). Opinion expressions were identified using the Subjectivity Lexicon from the MPQA project (Wilson et al., 2005). Communication words were obtained by using the Appraisal Lexicon (Bloom et al., 2007). Nominalizations were recognized by looking relates to the candidate opinion holder. 7 available at disi.unitn.it/moschitti Figure 2: Illustration of the different scopes on a CON STAUG ; nodes belonging to the candidate opinion holder are mark"
N10-1121,J08-2003,0,0.0658684,"tion. In our work, we deliberately work with minimal annotation and, thus, do not consider any labeled opinion expressions and relations to opinion holders in the training data. We exclusively rely on entities marked as opinion holders. In many practical situations, the annotation beyond opinion holder labeling is too expensive. Complex convolution kernels have been successfully applied to various NLP tasks, such as relation extraction (Bunescu and Mooney, 2005; Zhang 796 et al., 2006; Nguyen et al., 2009), question answering (Zhang and Lee, 2003; Moschitti, 2008), and semantic role labeling (Moschitti et al., 2008). In all these tasks, they offer competitive performance to manually designed feature sets. Bunescu and Mooney (2005) combine different sequence kernels encoding different contexts of candidate entities in a sentence. They argue that several kernels encoding different contexts are more effective than just using one kernel with one specific context. We build on that idea and compare various scopes eligible for opinion holder extraction. Moschitti (2008) and Nguyen et al. (2009) suggest that different kinds of information, such as word sequences, part-of-speech tags, syntactic and semantic infor"
N10-1121,D09-1143,0,0.285997,"e about [Y]?”). Such systems need to be able to distinguish which entities in a candidate answer sentence are the sources of opinions (= opinion holder) and which are the targets. On other NLP tasks, in particular, on relation extraction, there has been much work on convolution kernels, i.e. kernel functions exploiting huge amounts of features without an explicit feature representation. Previous research on that task has shown that convolution kernels, such as sequence and tree kernels, are quite effective when compared to manual feature engineering (Moschitti, 2008; Bunescu and Mooney, 2005; Nguyen et al., 2009). In order to effectively use convolution kernels, it is often necessary to choose appropriate substructures of a sentence rather than represent the sentence as a whole structure (Bunescu and Mooney, 2005; Zhang et al., 2006; Moschitti, 2008). As for tree kernels, for example, one typically chooses the syntactic subtree immediately enclosing two entities potentially expressing a specific relation in a given sentence. The opinion holder detection task is different from this scenario. There can be several cues within a sentence to indicate the presence of a genuine opinion holder and these cues"
N10-1121,H05-1044,0,0.0969332,"omial kernel of third degree. These two critical parameters were tuned on the development set. As far as the sequence and tree kernels are concerned, we used the parameter settings from Moschitti (2008), i.e. λ = 0.4 and µ = 0.4. Kernels were combined using plain summation. The documents were parsed using the Stanford Parser (Klein and Manning, 2003). Namedentity information was obtained by the Stanford tagger (Finkel et al., 2005). Semantic roles were obtained by using the parser by Zhang et al. (2008). Opinion expressions were identified using the Subjectivity Lexicon from the MPQA project (Wilson et al., 2005). Communication words were obtained by using the Appraisal Lexicon (Bloom et al., 2007). Nominalizations were recognized by looking relates to the candidate opinion holder. 7 available at disi.unitn.it/moschitti Figure 2: Illustration of the different scopes on a CON STAUG ; nodes belonging to the candidate opinion holder are marked with CAN D . up nouns in NOMLEX (Macleod et al., 1998). 5.1 Each Notation kernel is represented as a triple hlevelOfRepresentation (Table 1), Scope (Table 3), typeOfKernel (Table 2)i, e.g. hCON ST, SEN T, ST Ki is a Subset Tree Kernel of a constituency parse having"
N10-1121,N06-1037,0,0.239088,"extraction, there has been much work on convolution kernels, i.e. kernel functions exploiting huge amounts of features without an explicit feature representation. Previous research on that task has shown that convolution kernels, such as sequence and tree kernels, are quite effective when compared to manual feature engineering (Moschitti, 2008; Bunescu and Mooney, 2005; Nguyen et al., 2009). In order to effectively use convolution kernels, it is often necessary to choose appropriate substructures of a sentence rather than represent the sentence as a whole structure (Bunescu and Mooney, 2005; Zhang et al., 2006; Moschitti, 2008). As for tree kernels, for example, one typically chooses the syntactic subtree immediately enclosing two entities potentially expressing a specific relation in a given sentence. The opinion holder detection task is different from this scenario. There can be several cues within a sentence to indicate the presence of a genuine opinion holder and these cues need not be member of a particular word group, e.g. they can be opinion words (see Sentences 1-3), communication words, such as maintained in Sentence 2, or other lexical cues, such as according in Sentence 3. 1. The U.S. co"
N10-1121,W08-2126,0,0.0340879,"matching. We set the trade-off parameter j = 5 for all feature sets. For the manual feature set we used a polynomial kernel of third degree. These two critical parameters were tuned on the development set. As far as the sequence and tree kernels are concerned, we used the parameter settings from Moschitti (2008), i.e. λ = 0.4 and µ = 0.4. Kernels were combined using plain summation. The documents were parsed using the Stanford Parser (Klein and Manning, 2003). Namedentity information was obtained by the Stanford tagger (Finkel et al., 2005). Semantic roles were obtained by using the parser by Zhang et al. (2008). Opinion expressions were identified using the Subjectivity Lexicon from the MPQA project (Wilson et al., 2005). Communication words were obtained by using the Appraisal Lexicon (Bloom et al., 2007). Nominalizations were recognized by looking relates to the candidate opinion holder. 7 available at disi.unitn.it/moschitti Figure 2: Illustration of the different scopes on a CON STAUG ; nodes belonging to the candidate opinion holder are marked with CAN D . up nouns in NOMLEX (Macleod et al., 1998). 5.1 Each Notation kernel is represented as a triple hlevelOfRepresentation (Table 1), Scope (Tabl"
N10-1121,P02-1034,0,\N,Missing
N13-1059,P05-1045,0,0.00752314,"ive) (10) [Evil witches are stereotypically dressed in black] and [good fairies in white]. We also experimented with other related weaklysupervised extraction methods, such as mutual information of two adjectives at the sentence level (or even smaller window sizes). However, using conjunctions largely outperformed these alternative approaches so we only pursue conjunctions here. 4 Experiments As a large unlabeled (training) corpus, we chose the North American News Text Corpus (LDC95T21) comprising approximately 350 million words of news text. For syntactic analysis we use the Stanford Parser (Finkel et al., 2005). In order to decide whether an extracted adjective is subjective or not, we employ two sentiment lexicons, namely the Subjectivity Lexicon (SUB) (Wilson et al., 2005) and SO-CAL (SOC) (Taboada et al., 2011). According to the recent in-depth evaluation presented in Taboada et al. (2011), these two sentiment lexicons are the most effective resources for English sentiment analysis. By taking into account two different lexicons, which have also been built independently of each other, we want to provide evidence that our proposed criterion to extract subjective adjectives is not sensitive towards"
N13-1059,P97-1023,0,0.533226,"ves from the nonsubjective adjectives. In this work, we are interested in an out-ofcontext assessment of adjectives and therefore evaluate them with the help of sentiment lexicons. We examine the property of being a predicative adjective as an extraction criterion. Predicative adjectives are adjectives that do not modify the head of a noun (3) (4) (5) (6) Her idea was brilliant. This is a financial problem. She came up with a brilliant idea. ?The problem is financial. 2 Related Work The extraction of subjective adjectives has already attracted some considerable attention in previous research. Hatzivassiloglou and McKeown (1997) extract polar adjectives by a weakly supervised method in which subjective adjectives are found by searching for adjectives that are conjuncts of a pre-defined set of polar seed adjectives. Wiebe (2000) induces subjective adjectives with the help of distributional similarity. Hatzivassiloglou and Wiebe (2000) examine the properties of dynamic, gradable and polar adjectives as a means to detect subjectivity. Vegnaduzzo (2004) presents another bootstrapping method of extracting subjective adjectives with the help of head nouns of the subjective candidates and distributional similarity. Baroni a"
N13-1059,C00-1044,0,0.53182,"ot modify the head of a noun (3) (4) (5) (6) Her idea was brilliant. This is a financial problem. She came up with a brilliant idea. ?The problem is financial. 2 Related Work The extraction of subjective adjectives has already attracted some considerable attention in previous research. Hatzivassiloglou and McKeown (1997) extract polar adjectives by a weakly supervised method in which subjective adjectives are found by searching for adjectives that are conjuncts of a pre-defined set of polar seed adjectives. Wiebe (2000) induces subjective adjectives with the help of distributional similarity. Hatzivassiloglou and Wiebe (2000) examine the properties of dynamic, gradable and polar adjectives as a means to detect subjectivity. Vegnaduzzo (2004) presents another bootstrapping method of extracting subjective adjectives with the help of head nouns of the subjective candidates and distributional similarity. Baroni and Vegnaduzzo 534 Proceedings of NAACL-HLT 2013, pages 534–539, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics (2004) employ Web-based Mutual information for this task and largely outperform the results produced by Vegnaduzzo (2004). 3 Method In the following, we present dif"
N13-1059,J11-2001,0,0.0323326,"ctives at the sentence level (or even smaller window sizes). However, using conjunctions largely outperformed these alternative approaches so we only pursue conjunctions here. 4 Experiments As a large unlabeled (training) corpus, we chose the North American News Text Corpus (LDC95T21) comprising approximately 350 million words of news text. For syntactic analysis we use the Stanford Parser (Finkel et al., 2005). In order to decide whether an extracted adjective is subjective or not, we employ two sentiment lexicons, namely the Subjectivity Lexicon (SUB) (Wilson et al., 2005) and SO-CAL (SOC) (Taboada et al., 2011). According to the recent in-depth evaluation presented in Taboada et al. (2011), these two sentiment lexicons are the most effective resources for English sentiment analysis. By taking into account two different lexicons, which have also been built independently of each other, we want to provide evidence that our proposed criterion to extract subjective adjectives is not sensitive towards a particular gold standard (which would challenge the general validity of the proposed method). ALL PRD other new last many first such next political federal own several few good∗ former same economic public"
N13-1059,J04-3002,0,0.0545981,"xtract subjective adjectives. We do not only compare this criterion with a weakly supervised extraction method but also with gradable adjectives, i.e. another highly subjective subset of adjectives that can be extracted in an unsupervised fashion. In order to prove the robustness of this extraction method, we will evaluate the extraction with the help of two different state-of-the-art sentiment lexicons (as a gold standard). 1 Introduction Since the early work on sentiment analysis, it has been established that the part of speech with the highest proportion of subjective words are adjectives (Wiebe et al., 2004) (see Sentence (1)). However, not all adjectives are subjective (2). (1) A grumpy guest made some impolite remarks to the insecure and inexperienced waitress. (2) The old man wearing a yellow pullover sat on a plastic chair. This justifies the exploration of criteria to automatically separate the subjective adjectives from the nonsubjective adjectives. In this work, we are interested in an out-ofcontext assessment of adjectives and therefore evaluate them with the help of sentiment lexicons. We examine the property of being a predicative adjective as an extraction criterion. Predicative adject"
N13-1059,H05-1044,0,0.0764966,"such as mutual information of two adjectives at the sentence level (or even smaller window sizes). However, using conjunctions largely outperformed these alternative approaches so we only pursue conjunctions here. 4 Experiments As a large unlabeled (training) corpus, we chose the North American News Text Corpus (LDC95T21) comprising approximately 350 million words of news text. For syntactic analysis we use the Stanford Parser (Finkel et al., 2005). In order to decide whether an extracted adjective is subjective or not, we employ two sentiment lexicons, namely the Subjectivity Lexicon (SUB) (Wilson et al., 2005) and SO-CAL (SOC) (Taboada et al., 2011). According to the recent in-depth evaluation presented in Taboada et al. (2011), these two sentiment lexicons are the most effective resources for English sentiment analysis. By taking into account two different lexicons, which have also been built independently of each other, we want to provide evidence that our proposed criterion to extract subjective adjectives is not sensitive towards a particular gold standard (which would challenge the general validity of the proposed method). ALL PRD other new last many first such next political federal own sever"
N16-1092,P98-1013,0,0.0507685,"005; Andreevskaia and Bergler, 2006; Gyamfi et al., 2009; Choi and Wiebe, 2014; Kang et al., 2014). We will consider them as a baseline, showing that our proposed high-level features are more suitable for our task. 4.1.2 Lexicographer Files (LEX) Lexicographer files organize the synset inventory of WordNet into a coarse-grained set of semantic categories. In total, there are 45 categories for the three parts of speech we consider.2 The advantage of such a coarse-grained inventory is that it should require only few labeled training data in supervised classification. 4.2 FrameNet (FN) FrameNet (Baker et al., 1998) is a semantic resource that has been found useful for subtasks of sentiment analysis related to ours, i.e. opinion holder/target extraction (Bethard et al., 2004; Kim and Hovy, 2006). It includes a large set of more than 1, 200 semantic frames that comprise words with similar semantic behaviour. As a feature we use the framemembership of the opinion words, assuming that different frames are associated with different sentiment views. We use FrameNet version 1.5. 4.3 Subcategorization Frames (SUB) Subcategorization frames could also be predictive. For example, actor views demand the presence of"
N16-1092,D14-1125,0,0.125435,"Missing"
N16-1092,C94-1042,0,0.0450304,"general for our task. Instead we use the lexicographer files of all nouns and verbs occurring in the glosses of those adjectives. Type Sentiment Neutral Affixes Used -able, dis-, mis-, over-, under-, -(i)sm adj → noun: -cy, -ity, -ness; adj/noun → verb: -ize; verb → adj: -ed, -ing; verb → noun: -ion, -ing Table 2: Affixes used as features. explicit entity that utters some opinion, i.e. the opinion holder. For a speaker view, this entity remains implicit. This should be reflected in the argument valence of the respective opinion words. We employ the subcategorization frames encoded in COMLEX (Grishman et al., 1994) for verbs and adjectives, and NOMLEX (Macleod et al., 1998) for nouns. 4.4 Morphological Information (MORPH) (11) The UN was blamed for misinterpretingverb climate data. (12) The UN was blamed for the misinterpretationnoun of climate data. 4.6 As morphological information, we consider derivational affixes. Table 2 lists our choice of prefixes and suffixes. We only included affixes that occurred at least 10 times in our dataset. We distinguish between sentiment and neutral affixes. The sentiment affixes are affixes which, due to their meaning, suggest a sentiment view. For example, mis- as in"
N16-1092,N09-1002,0,0.0138176,"words conveying the same sentiment view also contain similar glosses. Glosses are a special type of feature. It is basically a bag-of-words feature set, i.e. a low-level feature set, which is known to be sparse yet effective when sufficient training data are used. All the other features presented in this paper are high-level features, i.e. more frequently occurring features already being effective if only few labeled data are used. Glosses are one of the most frequently used features for lexicon induction tasks in sentiment analysis (Esuli and Sebastiani, 2005; Andreevskaia and Bergler, 2006; Gyamfi et al., 2009; Choi and Wiebe, 2014; Kang et al., 2014). We will consider them as a baseline, showing that our proposed high-level features are more suitable for our task. 4.1.2 Lexicographer Files (LEX) Lexicographer files organize the synset inventory of WordNet into a coarse-grained set of semantic categories. In total, there are 45 categories for the three parts of speech we consider.2 The advantage of such a coarse-grained inventory is that it should require only few labeled training data in supervised classification. 4.2 FrameNet (FN) FrameNet (Baker et al., 1998) is a semantic resource that has been"
N16-1092,N13-1111,0,0.0169224,"set of constants C. The probability distribution that is estimated is a log-linear model P (X = x) = 1 exp Z k X i=1 ! wi ni (x) (1) where ni (x) is the number of groundings in Fi in x and Z is a normalization constant. As an implementation, we use thebeast (Riedel, 2008). We employ MLNs since they allow us (in addition to including ordinary features, i.e. §4.1-§4.6) to formulate constraints holding between individual instances. Such global constraints have been effectively exploited with MLNs in related tasks, such as semantic-role labeling (Meza-Ruiz and Riedel, 2009), anaphora resolution (Hou et al., 2013), question answering (Khot et al., 2015) and discoursebased sentiment analysis (Zirn et al., 2011). We formulate three such constraints. Two of them are Abbrev. w2v lin morph Constraint as Logic Formula ∀x[∀y[∀z[∀u[[Opin.Word(x)∧Opin.Word (y)∧Word2Vec-Similar (x, y)∧ViewOf (z, x)∧ViewOf (u, y)] → (z == u)]]]] ∀x[∀y[∀z[∀u[[Opin.Word(x)∧Opin.Word (y)∧DekangLin-Similar (x, y)∧ViewOf (z, x)∧ViewOf (u, y)] → (z == u)]]]] ∀x[∀y[∀z[∀u[[Opin.Word(x)∧Opin.Word (y)∧MorphRelated (x, y)∧ViewOf (z, x)∧ViewOf (u, y)] → (z == u)]]]] Table 3: Global constraints enforcing sentiment view consistency as incorpor"
N16-1092,J13-3002,0,0.0805017,"morphologically related instances. Finally, we also examine the relationship between prior lexical information (i.e. our approach) and contextual annotation in the MPQA corpus. 2 Related Work The annotation scheme of the MPQA corpus (Wiebe et al., 2005) was the first work to address the distinction between different sentiment views. The two sentiment views are referred to as direct subjectivity (=actor view) and expressive subjectivity (=speaker view). In subsequent research, some approaches have been proposed to distinguish these two categories in the MPQA corpus. The most extensive work is Johansson and Moschitti (2013). Since MPQA provides annotation regarding sentiment in context, sentiment views are exclusively considered in contextual classification. The fact that it is the opinion words that convey those views, as we do in this paper, is not addressed. Unlike in this paper, the focus of Johansson and Moschitti (2013) is also on optimizing a machine-learning classifier, in particular to model the interaction between different subjective phrases within the same sentence. 779 Part of Speech adjective noun verb Actor View Freq Perc 223 8.9 487 29.1 618 52.6 Speaker View Freq Perc 2279 91.1 1189 70.9 557 47."
N16-1092,P14-1145,0,0.0931427,"o contain similar glosses. Glosses are a special type of feature. It is basically a bag-of-words feature set, i.e. a low-level feature set, which is known to be sparse yet effective when sufficient training data are used. All the other features presented in this paper are high-level features, i.e. more frequently occurring features already being effective if only few labeled data are used. Glosses are one of the most frequently used features for lexicon induction tasks in sentiment analysis (Esuli and Sebastiani, 2005; Andreevskaia and Bergler, 2006; Gyamfi et al., 2009; Choi and Wiebe, 2014; Kang et al., 2014). We will consider them as a baseline, showing that our proposed high-level features are more suitable for our task. 4.1.2 Lexicographer Files (LEX) Lexicographer files organize the synset inventory of WordNet into a coarse-grained set of semantic categories. In total, there are 45 categories for the three parts of speech we consider.2 The advantage of such a coarse-grained inventory is that it should require only few labeled training data in supervised classification. 4.2 FrameNet (FN) FrameNet (Baker et al., 1998) is a semantic resource that has been found useful for subtasks of sentiment an"
N16-1092,D15-1080,0,0.0144329,"stribution that is estimated is a log-linear model P (X = x) = 1 exp Z k X i=1 ! wi ni (x) (1) where ni (x) is the number of groundings in Fi in x and Z is a normalization constant. As an implementation, we use thebeast (Riedel, 2008). We employ MLNs since they allow us (in addition to including ordinary features, i.e. §4.1-§4.6) to formulate constraints holding between individual instances. Such global constraints have been effectively exploited with MLNs in related tasks, such as semantic-role labeling (Meza-Ruiz and Riedel, 2009), anaphora resolution (Hou et al., 2013), question answering (Khot et al., 2015) and discoursebased sentiment analysis (Zirn et al., 2011). We formulate three such constraints. Two of them are Abbrev. w2v lin morph Constraint as Logic Formula ∀x[∀y[∀z[∀u[[Opin.Word(x)∧Opin.Word (y)∧Word2Vec-Similar (x, y)∧ViewOf (z, x)∧ViewOf (u, y)] → (z == u)]]]] ∀x[∀y[∀z[∀u[[Opin.Word(x)∧Opin.Word (y)∧DekangLin-Similar (x, y)∧ViewOf (z, x)∧ViewOf (u, y)] → (z == u)]]]] ∀x[∀y[∀z[∀u[[Opin.Word(x)∧Opin.Word (y)∧MorphRelated (x, y)∧ViewOf (z, x)∧ViewOf (u, y)] → (z == u)]]]] Table 3: Global constraints enforcing sentiment view consistency as incorporated in MLNs. based on the two most effe"
N16-1092,W06-0301,0,0.181649,"are more suitable for our task. 4.1.2 Lexicographer Files (LEX) Lexicographer files organize the synset inventory of WordNet into a coarse-grained set of semantic categories. In total, there are 45 categories for the three parts of speech we consider.2 The advantage of such a coarse-grained inventory is that it should require only few labeled training data in supervised classification. 4.2 FrameNet (FN) FrameNet (Baker et al., 1998) is a semantic resource that has been found useful for subtasks of sentiment analysis related to ours, i.e. opinion holder/target extraction (Bethard et al., 2004; Kim and Hovy, 2006). It includes a large set of more than 1, 200 semantic frames that comprise words with similar semantic behaviour. As a feature we use the framemembership of the opinion words, assuming that different frames are associated with different sentiment views. We use FrameNet version 1.5. 4.3 Subcategorization Frames (SUB) Subcategorization frames could also be predictive. For example, actor views demand the presence of an 2 For adjectives there exist only two categories. These are too general for our task. Instead we use the lexicographer files of all nouns and verbs occurring in the glosses of tho"
N16-1092,P98-2127,0,0.0617041,"∀y[∀z[∀u[[Opin.Word(x)∧Opin.Word (y)∧DekangLin-Similar (x, y)∧ViewOf (z, x)∧ViewOf (u, y)] → (z == u)]]]] ∀x[∀y[∀z[∀u[[Opin.Word(x)∧Opin.Word (y)∧MorphRelated (x, y)∧ViewOf (z, x)∧ViewOf (u, y)] → (z == u)]]]] Table 3: Global constraints enforcing sentiment view consistency as incorporated in MLNs. based on the two most effective types of word similarities from Wiegand and Ruppenhofer (2015). The first word similarity measures the cosine of word vectors representing opinion words produced by Word2Vec-embeddings (Mikolov et al., 2013). The second word similarity is represented by the metric of Lin (1998), which exploits the rich set of dependency-relation labels in the context of distributional similarity.3 The third type of consistency considers morphological relatedness by which we understand two words deriving from two different parts of speech but belonging to the same lexical root and therefore carrying similar meaning (e.g. happiness.noun and happy.adj). We obtain that type of relatedness from WordNet (Miller et al., 1990). Table 3 lists our constraints. They state that if for two opinion words some similarity or morphological relatedness holds, then these words should convey the same s"
N16-1092,maks-vossen-2012-building,0,0.0180962,"eraction between different subjective phrases within the same sentence. 779 Part of Speech adjective noun verb Actor View Freq Perc 223 8.9 487 29.1 618 52.6 Speaker View Freq Perc 2279 91.1 1189 70.9 557 47.4 Table 1: Distribution of the different sentiment views. Some of the lexical resources we examine, i.e. WordNet (§4.1) and FrameNet (§4.2), have also been employed in Breck et al. (2007) who, like Johansson and Moschitti (2013), also deal with contextual (sentiment) classification. However, the authors do not examine in how far these individual resources separate speaker and actor views. Maks and Vossen (2012b) link sentiment views to opinion words as part of a lexicon model for sentiment analysis. Maks and Vossen (2012a) also examine a corpus-driven method to induce opinion words for the different sentiment views. The authors, however, conclude that their approach, which sees news articles as a source for actor views and news comments as a source for speaker views, is not sufficiently effective. The work most closely related to our research is Wiegand and Ruppenhofer (2015). Opinion words are categorized according to their sentiment view. Our work substantially goes beyond that previous research:"
N16-1092,N09-1018,0,0.0225131,"template for constructing a Markov network given a set of constants C. The probability distribution that is estimated is a log-linear model P (X = x) = 1 exp Z k X i=1 ! wi ni (x) (1) where ni (x) is the number of groundings in Fi in x and Z is a normalization constant. As an implementation, we use thebeast (Riedel, 2008). We employ MLNs since they allow us (in addition to including ordinary features, i.e. §4.1-§4.6) to formulate constraints holding between individual instances. Such global constraints have been effectively exploited with MLNs in related tasks, such as semantic-role labeling (Meza-Ruiz and Riedel, 2009), anaphora resolution (Hou et al., 2013), question answering (Khot et al., 2015) and discoursebased sentiment analysis (Zirn et al., 2011). We formulate three such constraints. Two of them are Abbrev. w2v lin morph Constraint as Logic Formula ∀x[∀y[∀z[∀u[[Opin.Word(x)∧Opin.Word (y)∧Word2Vec-Similar (x, y)∧ViewOf (z, x)∧ViewOf (u, y)] → (z == u)]]]] ∀x[∀y[∀z[∀u[[Opin.Word(x)∧Opin.Word (y)∧DekangLin-Similar (x, y)∧ViewOf (z, x)∧ViewOf (u, y)] → (z == u)]]]] ∀x[∀y[∀z[∀u[[Opin.Word(x)∧Opin.Word (y)∧MorphRelated (x, y)∧ViewOf (z, x)∧ViewOf (u, y)] → (z == u)]]]] Table 3: Global constraints enforcin"
N16-1092,D08-1061,0,0.0667403,"Missing"
N16-1092,R11-1039,1,0.844283,"adjectives. For speaker views (PATT speaker), Wiegand and Ruppenhofer introduced reproach patterns, e.g. blamed for X as in (11). These patterns can also be applied to nouns (12) but not to adjectives. For the latter, we did not find any pattern. The patterns were applied to the North American News Text Corpus (LDC95T21). Context Patterns (PATT) Wiegand and Ruppenhofer (2015) proposed patterns for actor-view and speaker-view verbs. For actor views (PATT actor), they rely on prototypical opinion holders (protoOHs), i.e. common nouns, such as opponents or critics, that act like opinion holders (Wiegand and Klakow, 2011). If a verb often co-occurs with an opinion holder – Wiegand and Ruppenhofer (2015) take protoOHs as a proxy – then this is a good indicator of being an actor view 781 Polarity Information (POLAR) We also investigate in how far polarity information correlates with sentiment views. This information is obtained from the Subjectivity Lexicon (Wilson et al., 2005). Each opinion word is assigned a polarity type, i.e. positive, negative or neutral. 5 Markov Logic Networks and Global Constraints Markov Logic Networks (MLNs) are a supervised classifier combining first-order logic with probabilities. M"
N16-1092,K15-1022,1,0.0992461,"adjectives. We consider many high-level features requiring only few labeled training data. A detailed feature analysis produces linguistic insights into the nature of sentiment views. We also examine how far global constraints between different opinion words help to increase classification performance. Finally, we show that our (prior) word-level annotation correlates with contextual sentiment views. The distinction between those categories is relevant for related tasks in sentiment analysis, most importantly, opinion holder and target extraction. This has already been demonstrated for verbs (Wiegand and Ruppenhofer, 2015). For example, even though the noun Peter has the same grammatical relation to the opinion verb in (5) & (6), in the former sentence it is a holder but in the latter it is a target. Similar cases can be observed for opinion nouns (7) & (8) and opinion adjectives (9) & (10). Only the knowledge of sentiment views helps us to assign opinion roles correctly. 1 Introduction While there has been much research in sentiment analysis on the tasks of subjectivity detection and polarity classification, there has been less work on other types of categorizations that can be imposed upon subjective expressi"
N16-1092,H05-1044,0,0.345613,"gand and Ruppenhofer (2015) distinguish between two types of actor views, agent views and patient views. The former take their opinion holder as an agent and their target as a patient (typical verbs are criticize, love, believe), while the latter align their roles inversely (typical verbs are disappoint, please, interest). Since this distinction between actor views does not exist among nouns or adjectives, we consider one merged (actor-view) category for all three parts of speech in this paper. 3 Data We manually annotated all verbs, nouns and adjectives contained in the Subjectivity Lexicon (Wilson et al., 2005) for view type. The dataset comprises 2502 adjectives, 1676 nouns and 1175 verbs. Since our new dataset1 is an extension of the dataset from Wiegand and Ruppenhofer (2015), we adhere to the annotation process proposed in that paper. That is, the basis of the annotation were online dictionaries (e.g. Macmillan Dictionary) which provide both a word definition and example sentences. Each word is either labeled as primarily conveying an actor or a speaker view. (Our categorization is binary.) On a subset of 250 words for each part of speech, we computed an interannotation agreement (Cohen’s κ) of"
N16-1092,I11-1038,0,0.0281247,"x) = 1 exp Z k X i=1 ! wi ni (x) (1) where ni (x) is the number of groundings in Fi in x and Z is a normalization constant. As an implementation, we use thebeast (Riedel, 2008). We employ MLNs since they allow us (in addition to including ordinary features, i.e. §4.1-§4.6) to formulate constraints holding between individual instances. Such global constraints have been effectively exploited with MLNs in related tasks, such as semantic-role labeling (Meza-Ruiz and Riedel, 2009), anaphora resolution (Hou et al., 2013), question answering (Khot et al., 2015) and discoursebased sentiment analysis (Zirn et al., 2011). We formulate three such constraints. Two of them are Abbrev. w2v lin morph Constraint as Logic Formula ∀x[∀y[∀z[∀u[[Opin.Word(x)∧Opin.Word (y)∧Word2Vec-Similar (x, y)∧ViewOf (z, x)∧ViewOf (u, y)] → (z == u)]]]] ∀x[∀y[∀z[∀u[[Opin.Word(x)∧Opin.Word (y)∧DekangLin-Similar (x, y)∧ViewOf (z, x)∧ViewOf (u, y)] → (z == u)]]]] ∀x[∀y[∀z[∀u[[Opin.Word(x)∧Opin.Word (y)∧MorphRelated (x, y)∧ViewOf (z, x)∧ViewOf (u, y)] → (z == u)]]]] Table 3: Global constraints enforcing sentiment view consistency as incorporated in MLNs. based on the two most effective types of word similarities from Wiegand and Ruppenho"
N16-1092,E06-1027,0,\N,Missing
N16-1092,C98-1013,0,\N,Missing
N16-1094,D09-1020,0,0.0444841,"Missing"
N16-1094,C08-1011,0,0.0363909,"Missing"
N16-1094,H05-1045,0,0.135799,"nd only contains about 200 unique compounds. We considered this amount insufficient for producing a gold standard. Also, none of the existing datasets on noun compounds (Lauer, 1995; Barker and Szpakowicz, 1998; Nastase and Szpakowicz, 2003; Girju et al., 2009; Kim and Baldwin, 2005; Tratz and Hovy, 2010; Dima et al., 2014) contain any information regarding opinion roles. 2 Related Work With regard to opinion role extraction, many features for supervised learning have been explored. They typically address the relationship between opinion word and opinion role on the basis of surface patterns (Choi et al., 2005), part-of-speech information (Wiegand and Klakow, 2010), syntactic information (Kessler and Nicolov, 2009; Jakob and Gurevych, 2010) or semantic role labeling (Johansson and Moschitti, 2013; Deng and Wiebe, 2015). The majority of those features cannot be applied to our task since for opinion compounds, there is no context between opinion role and opinion word. In the area of noun compound analysis, there are two predominant approaches. On the one hand, lexical resources, such as WordNet (Miller et al., 1990), are employed in order to assign semantic categories to head and modifier and infer fr"
N16-1094,E14-1040,0,0.0141671,"that encodes for each compound the category of its modifier. (4) (5) (6) (7) (8) (9) [user OH ] rating (i.e. user rates something) [consumer OH ] uncertainty (i.e. consumers are uncertain) [victim OT ] support (i.e. support for victims) [test OT ] anxiety (i.e. having anxiety towards test taking) spring upswing (i.e. economic upswing in spring) phone harassment (i.e. harassment inflicted via phone) Notice that we focus exclusively on opinion role extraction. We do not try to detect the polarity associated with the compound. Neither do we consider implicature-related information about effects (Deng and Wiebe, 2014), but only inherent sentiment. We study opinion role extraction on opinion compounds in German. German is known for its frequent 800 Proceedings of NAACL-HLT 2016, pages 800–810, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics compounds user rating; victim support; spring upswing immediate constituents user; victim; spring rating; support; upswing grammatical function modifier head Table 1: Internal structure of opinion compounds. use of noun compounds. In the STEPS-corpus, the benchmark dataset for German opinion role extraction (Ruppenhofer et al., 2"
N16-1094,D15-1018,0,0.028744,"Missing"
N16-1094,dima-etal-2014-tell,0,0.0304548,"uages. Apart from examining traditional features from noun compound analysis, in this paper, we also introduce novel features specially designed for the analysis of opinion compounds. We also created a new gold standard for this task (see also §3). The STEPS-corpus, as such, is fairly small and only contains about 200 unique compounds. We considered this amount insufficient for producing a gold standard. Also, none of the existing datasets on noun compounds (Lauer, 1995; Barker and Szpakowicz, 1998; Nastase and Szpakowicz, 2003; Girju et al., 2009; Kim and Baldwin, 2005; Tratz and Hovy, 2010; Dima et al., 2014) contain any information regarding opinion roles. 2 Related Work With regard to opinion role extraction, many features for supervised learning have been explored. They typically address the relationship between opinion word and opinion role on the basis of surface patterns (Choi et al., 2005), part-of-speech information (Wiegand and Klakow, 2010), syntactic information (Kessler and Nicolov, 2009; Jakob and Gurevych, 2010) or semantic role labeling (Johansson and Moschitti, 2013; Deng and Wiebe, 2015). The majority of those features cannot be applied to our task since for opinion compounds, the"
N16-1094,P07-1072,0,0.0294478,"Missing"
N16-1094,W97-0802,0,0.649858,"Missing"
N16-1094,N13-1111,0,0.0179909,"haring the same head.6 Given this selectional preference, we formulate a global head constraint (Table 7) that if two compounds have the same head, their modifiers should convey the same opinion role. In order to implement this constraint in a supervised classifier we employ Markov Logic Networks (MLNs), which combine first-order logic with probabilities. As a tool, we use thebeast (Riedel, 2008). MLNs have been effectively used in various related NLP tasks, such as discourse-based sentiment analysis (Zirn et al., 2011), semantic-role labeling (MezaRuiz and Riedel, 2009), anaphora resolution (Hou et al., 2013) or question answering (Khot et al., 2015). 6 On average, a head occurs in 5 different compounds on Dataset I, and in 4 different compounds on Dataset II. 805 88.86 Dataset II 91.36 Table 6: Role-purity of compounds with the same head. MLNs are a set of pairs (Fi , wi ) where Fi is a first-order logic formula and wi an associated realvalued weight. They build a template for constructing a Markov network given a set of constants C. The probability distribution that is estimated is a loglinear model P (X = x) = 1 exp Z k X i=1 ! wi ni (x) (1) where ni (x) is the number of groundings in Fi in x a"
N16-1094,D10-1101,0,0.0309836,"of the existing datasets on noun compounds (Lauer, 1995; Barker and Szpakowicz, 1998; Nastase and Szpakowicz, 2003; Girju et al., 2009; Kim and Baldwin, 2005; Tratz and Hovy, 2010; Dima et al., 2014) contain any information regarding opinion roles. 2 Related Work With regard to opinion role extraction, many features for supervised learning have been explored. They typically address the relationship between opinion word and opinion role on the basis of surface patterns (Choi et al., 2005), part-of-speech information (Wiegand and Klakow, 2010), syntactic information (Kessler and Nicolov, 2009; Jakob and Gurevych, 2010) or semantic role labeling (Johansson and Moschitti, 2013; Deng and Wiebe, 2015). The majority of those features cannot be applied to our task since for opinion compounds, there is no context between opinion role and opinion word. In the area of noun compound analysis, there are two predominant approaches. On the one hand, lexical resources, such as WordNet (Miller et al., 1990), are employed in order to assign semantic categories to head and modifier and infer from those labels the 801 Dataset I 2000 compounds 389 (unique) heads category of modifier role no role frequency 937 1063 proportion"
N16-1094,J13-3002,0,0.130096,"Missing"
N16-1094,I05-1082,0,0.0285967,"our approach can be replicated on other languages. Apart from examining traditional features from noun compound analysis, in this paper, we also introduce novel features specially designed for the analysis of opinion compounds. We also created a new gold standard for this task (see also §3). The STEPS-corpus, as such, is fairly small and only contains about 200 unique compounds. We considered this amount insufficient for producing a gold standard. Also, none of the existing datasets on noun compounds (Lauer, 1995; Barker and Szpakowicz, 1998; Nastase and Szpakowicz, 2003; Girju et al., 2009; Kim and Baldwin, 2005; Tratz and Hovy, 2010; Dima et al., 2014) contain any information regarding opinion roles. 2 Related Work With regard to opinion role extraction, many features for supervised learning have been explored. They typically address the relationship between opinion word and opinion role on the basis of surface patterns (Choi et al., 2005), part-of-speech information (Wiegand and Klakow, 2010), syntactic information (Kessler and Nicolov, 2009; Jakob and Gurevych, 2010) or semantic role labeling (Johansson and Moschitti, 2013; Deng and Wiebe, 2015). The majority of those features cannot be applied to"
N16-1094,P06-2064,0,0.0349529,"): a) Beurteilungnoun [von Sch¨ulern objpvon ] (assessmentnoun [of students objpof ]) b) Lehrer beurteilenverb Sch¨ulerobj . (Teachers assessverb studentsobj .) Even though the disambiguation of deverbal noun compounds with the help of verb relations has been examined before (Lapata, 2002), it has not been exploited for an actual application, such as opinion role extraction. Neither has it been compared against plain paraphrases, which use the head noun of the compound directly (§5.1). Our use of verb semantics for compound analysis is also different from its predominant use in previous work (Kim and Baldwin, 2006; Nakov and Hearst, 2013) where noun compounds are considered whose parts represent arguments of an abstract verbal relation (e.g. malaria mosquito are arguments of relation ‘mosquito causes malaria’). Thus, the aim has been to predict verbs for those compounds that match those abstract relations (e.g. to cause). We are looking for different verbs, namely those that are the morphological basis for the head noun. For this verb detour, we produce a mapping from nouns (i.e. the heads of our opinion compounds) to verbs by combining distributional and string similarity. We extracted the verbs most"
N16-1094,D11-1060,0,0.0289257,"Missing"
N16-1094,W09-4635,0,0.474796,"ion We created a new dataset1 by retrieving opinion compounds from the deWaC-corpus (Baroni et al., 2009) comprising 1.7 billion words. (Word embeddings (§5.2 & §5.6) and word similarity graphs (§5.7 & §6.4) were also created from this corpus.) In German, noun compounds are typically realized as single tokens. In order to obtain a set of opinion compounds, we extracted all noun compounds from deWaC whose second morpheme is an opinion noun. Morphological analysis was carried out using morphisto (Zielinski and Simon, 2009).2 As opinion nouns, we used the nouns from the PolArt sentiment lexicon (Klenner et al., 2009). Unfortunately, this lexicon is lacking in neutral opinion nouns, such as Meinung (opinion) or Erwartung (expectation) which frequently occur in compounds, e.g. Expertenmeinung (expert opinion) or Kundenerwartungen (customer expectations). Therefore, we translated the 235 neutral opinion nouns from the (English) Subjectivity Lexicon (Wilson et al., 2005) into German. From the opinion compounds extracted from deWaC, we created two manually annotated datasets (Table 2). We use more than one dataset as we consider our task as a multi-stage task as shown in Figure 1. We believe that this is neces"
N16-1094,J02-3004,0,0.0609779,"elation subj and patients with the relation obj.) (12) paraphrases for Leserkommentar (reader comments): a) Kommentarnoun [von Lesern objpvon ] (commentnoun [of readers objpof ]). b) Lesersubj kommentierenverb ein Ereignis. (Readerssubj commentverb on an event.) (13) paraphrases for Sch¨ulerbeurteilung (student assessment): a) Beurteilungnoun [von Sch¨ulern objpvon ] (assessmentnoun [of students objpof ]) b) Lehrer beurteilenverb Sch¨ulerobj . (Teachers assessverb studentsobj .) Even though the disambiguation of deverbal noun compounds with the help of verb relations has been examined before (Lapata, 2002), it has not been exploited for an actual application, such as opinion role extraction. Neither has it been compared against plain paraphrases, which use the head noun of the compound directly (§5.1). Our use of verb semantics for compound analysis is also different from its predominant use in previous work (Kim and Baldwin, 2006; Nakov and Hearst, 2013) where noun compounds are considered whose parts represent arguments of an abstract verbal relation (e.g. malaria mosquito are arguments of relation ‘mosquito causes malaria’). Thus, the aim has been to predict verbs for those compounds that ma"
N16-1094,P98-2127,0,0.0250485,"Missing"
N16-1094,N09-1018,0,0.0422085,"Missing"
N16-1094,P09-1113,0,0.00929895,"fier mentions part-of-speech tags before/after head mentions dependency paths between head and modifier mentions proportion of opinion words in the sentences each training/test instance represents the set of all sentences in which head and modifier of a specific compound co-occur Table 12: Features for distant supervision (baseline) classifier. 6.4 Comparison against Baselines Table 13 compares the best result from our previous experiments against 3 baselines. The first is a majority classifier predicting the majority class. The second baseline is a classifier inspired by distant supervision (Mintz et al., 2009). As in our paraphrase features, this classifier considers the context in which modifier and head of a compound occur as separate constituents. The difference is, however, that we consider every such co-occurrence (within the same sentence) as a context that conveys the same relation as the one that is (implicitly) conveyed by the compound. Even though such an assumption is naive, it has been shown to produce quite reasonable performance in relation extraction (Mintz et al., 2009). The advantage of such an approach is that a generic relation extraction/opinion role extraction classifier can be"
N16-1094,W01-0511,0,0.116221,"Missing"
N16-1094,D08-1061,0,0.0906089,"Missing"
N16-1094,P10-1070,0,0.0590467,"plicated on other languages. Apart from examining traditional features from noun compound analysis, in this paper, we also introduce novel features specially designed for the analysis of opinion compounds. We also created a new gold standard for this task (see also §3). The STEPS-corpus, as such, is fairly small and only contains about 200 unique compounds. We considered this amount insufficient for producing a gold standard. Also, none of the existing datasets on noun compounds (Lauer, 1995; Barker and Szpakowicz, 1998; Nastase and Szpakowicz, 2003; Girju et al., 2009; Kim and Baldwin, 2005; Tratz and Hovy, 2010; Dima et al., 2014) contain any information regarding opinion roles. 2 Related Work With regard to opinion role extraction, many features for supervised learning have been explored. They typically address the relationship between opinion word and opinion role on the basis of surface patterns (Choi et al., 2005), part-of-speech information (Wiegand and Klakow, 2010), syntactic information (Kessler and Nicolov, 2009; Jakob and Gurevych, 2010) or semantic role labeling (Johansson and Moschitti, 2013; Deng and Wiebe, 2015). The majority of those features cannot be applied to our task since for op"
N16-1094,N10-1121,1,0.739881,"considered this amount insufficient for producing a gold standard. Also, none of the existing datasets on noun compounds (Lauer, 1995; Barker and Szpakowicz, 1998; Nastase and Szpakowicz, 2003; Girju et al., 2009; Kim and Baldwin, 2005; Tratz and Hovy, 2010; Dima et al., 2014) contain any information regarding opinion roles. 2 Related Work With regard to opinion role extraction, many features for supervised learning have been explored. They typically address the relationship between opinion word and opinion role on the basis of surface patterns (Choi et al., 2005), part-of-speech information (Wiegand and Klakow, 2010), syntactic information (Kessler and Nicolov, 2009; Jakob and Gurevych, 2010) or semantic role labeling (Johansson and Moschitti, 2013; Deng and Wiebe, 2015). The majority of those features cannot be applied to our task since for opinion compounds, there is no context between opinion role and opinion word. In the area of noun compound analysis, there are two predominant approaches. On the one hand, lexical resources, such as WordNet (Miller et al., 1990), are employed in order to assign semantic categories to head and modifier and infer from those labels the 801 Dataset I 2000 compounds 389 (u"
N16-1094,R11-1039,1,0.902698,"Missing"
N16-1094,E12-1033,1,0.903414,"Missing"
N16-1094,K15-1022,1,0.920972,"s on a lexical level has only been examined for opinion verbs. Wiegand and Ruppenhofer (2015) propose a boot806 strapping approach in which seed verbs for the different sentiment views are automatically extracted.7 Then, a label propagation algorithm (Talukdar et al., 2008) is run on a word-similarity graph generated from the opinion verbs. Thus labels from the seeds can be expanded to the remaining opinion verbs. The nodes in the graph correspond to the opinion verbs. The best performing graph is based on the similarity metric introduced in Lin (1998). A critical step is the seed generation. Wiegand and Ruppenhofer (2015) extract seeds representing actor views by looking for opinion words frequently co-occurring with prototypical opinion holders (protoOHs). These are common nouns, such as opponents or critics, that typically act as opinion holders (Wiegand and Klakow, 2011). By definition, such explicit opinion holders indicate an actor view. Seeds for speaker-view verbs are obtained by extracting verbs co-occurring with reproach-patterns, such as obji(beschuldigt/blamed for, &lt;verb&gt;) (14) that matches in (15). (14) Pattern: obji(beschuldigt/blamed for, &lt;speaker-view verb&gt;) (15) Die UNO wurde beschuldigt, [die"
N16-1094,H05-1044,0,0.0155272,"we extracted all noun compounds from deWaC whose second morpheme is an opinion noun. Morphological analysis was carried out using morphisto (Zielinski and Simon, 2009).2 As opinion nouns, we used the nouns from the PolArt sentiment lexicon (Klenner et al., 2009). Unfortunately, this lexicon is lacking in neutral opinion nouns, such as Meinung (opinion) or Erwartung (expectation) which frequently occur in compounds, e.g. Expertenmeinung (expert opinion) or Kundenerwartungen (customer expectations). Therefore, we translated the 235 neutral opinion nouns from the (English) Subjectivity Lexicon (Wilson et al., 2005) into German. From the opinion compounds extracted from deWaC, we created two manually annotated datasets (Table 2). We use more than one dataset as we consider our task as a multi-stage task as shown in Figure 1. We believe that this is necessary as differ1 available at: www.coli.uni-saarland.de/ miwieg/naacl_2016_op_compounds_data.tgz ˜ 2 The data release provides more details regarding the gold standard, e.g. how compound instances were sampled. Each question (indicated by a rhombus) can be modeled with one binary supervised classifier. We build 3 classifiers, thus excluding the second ques"
N16-1094,I11-1038,0,0.0129991,"asured by the proportion of the most frequent role occurring within each group of compounds sharing the same head.6 Given this selectional preference, we formulate a global head constraint (Table 7) that if two compounds have the same head, their modifiers should convey the same opinion role. In order to implement this constraint in a supervised classifier we employ Markov Logic Networks (MLNs), which combine first-order logic with probabilities. As a tool, we use thebeast (Riedel, 2008). MLNs have been effectively used in various related NLP tasks, such as discourse-based sentiment analysis (Zirn et al., 2011), semantic-role labeling (MezaRuiz and Riedel, 2009), anaphora resolution (Hou et al., 2013) or question answering (Khot et al., 2015). 6 On average, a head occurs in 5 different compounds on Dataset I, and in 4 different compounds on Dataset II. 805 88.86 Dataset II 91.36 Table 6: Role-purity of compounds with the same head. MLNs are a set of pairs (Fi , wi ) where Fi is a first-order logic formula and wi an associated realvalued weight. They build a template for constructing a Markov network given a set of constants C. The probability distribution that is estimated is a loglinear model P (X"
N16-1094,C98-2122,0,\N,Missing
N16-1094,D15-1080,0,\N,Missing
N18-1095,P98-1013,0,0.678385,"Missing"
N18-1095,D14-1125,0,0.0367028,"Missing"
N18-1095,P16-1191,0,0.0651578,"Missing"
N18-1095,P97-1023,0,0.647125,"ings from the two largest corpora, i.e. AMZ and WAC (Table 2) using Word2Vec (Mikolov et al., 2013) in default configuration (i.e. 200 dimensions; cbow). The best performance was obtained by concatenating for each word the vectors induced from the two corpora.9 4.3 Baselines to Feature-based Approach In addition to a majority-class classifier we consider the following baselines: Weak Supervision (WSUP). With this baseline we want to build a lightweight classifier that does not require proper labeled training data. It is inspired by previous induction approaches for sentiment lexicons, such as Hatzivassiloglou and McKeown (1997) or Velikovich et al. (2010) which heuristically label some seed instances and then apply graph-based propagation to label the remaining words of a dataset. On the basis of word embeddings (§4.2), we build a word-similarity graph, where the nodes represent our negative polar expressions and each edge denotes the seman9 We also ran experiments with pretrained embeddings from GoogleNews but they did not improve classification. tic similarity between two arbitrary words. We compute it by the cosine of their word-embedding vectors. The output of PAT from Twitter (§4.1.4) is considered as positive"
N18-1095,E17-2068,0,0.0383637,"e13 , which has been used in Nobata et al. (2016) and Davidson et al. (2017), and the derogatory words from Wiktionary (Derogatory)14 .15 Finally, we also include our base lexicon (Table 1) in order to evaluate the expansion process of our two expanded lexicons (§5). For all lists, we train on a single feature indicating the frequency of abusive words in a micropost to be classified. Ottawa also contains weights assigned to abusive words. We weight the observed frequency with these weights. We further evaluate 3 classifiers representing the state of the art of in-domain evaluations: FastText (Joulin et al., 2017), Gated Recurrent Units Recurrent Neural Networks RNN, which have been reported to work best on English microposts (Pavlopoulos et al., 2017), and Yahoo, an SVM 13 www.hatebase.org https://en.wiktionary.org/wiki/ Category:English_derogatory_terms 15 There are also similar but smaller lists in Wiktionary, e.g. offensive terms. They produced no better results. 14 test Warner Waseem train Razavi Waseem Wulczyn Average Warner Razavi Wulczyn Average all 55.4 58.1 60.2 57.9 58.5 61.1 51.2 56.9 Yahoo explicit 65.2 55.9 72.8 64.6 61.2 63.1 68.2 64.2 feature-b. lex. all explicit 65.0 80.6 64.6 79.0 63."
N18-1095,P14-1145,0,0.0757256,"Missing"
N18-1095,N12-1071,0,0.0301779,"k of cross-domain classification of abusive documents (§6) where we use it as a highlevel feature. In this work, we consider microposts as documents. While for in-domain classification, supervised classifiers trained on generic features, such as bag of words or word embeddings, usually score very well, on cross-domain classification they perform poorly since they latch on to domain-specific information. In subjectivity, polarity and emotion classification, high-level features based on predictive domain-independent word lists have been proposed to bridge the domain mismatch (Dias et al., 2009; Mohammad, 2012; Wiegand et al., 2013). New abusive words constantly enter natural language. For example, according to Wiktionary4 the word gimboid, which refers to an incompetent person, was coined in the British television series Red Dwarf, possibly from the word gimp and the suffix -oid. According to Urban Dictionary5 , the word twunt, which is a portmanteau of the swearwords twat and cunt, has been invented 4 5 https://en.wiktionary.org www.urbandictionary.com 1046 Proceedings of NAACL-HLT 2018, pages 1046–1056 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics by"
N18-1095,D16-1057,0,0.0208676,"ctivity Lexicon with their respective polarities. As features, we use word embeddings (§4.2). In order to produce the feature-based lexicon of abusive words another SVM is trained on our base lexicon (Table 1) using the best feature set from Table 6. With 2989 abusive words, our expanded lexicon is 5 times as large as the base lexicon. In order to measure the impact of our proposed features on the quality of the resulting lexicon, we devised an alternative expansion which just employs word embeddings. For this, we used SentProp, the most effective induction method from the SocialSent package (Hamilton et al., 2016).11 6 6.1 : total number of microposts in the dataset Cross-domain Classification Motivation and Set Up We now apply our expanded lexicon (§5) to the classification of abusive microposts, i.e. we classify entire comments rather than words out of context. Table 8 shows the datasets of labeled microposts that we use. The difference between these datasets is the source from which they originate. Consequently, different topics are represented in the different datasets. Still, we find similar types 11 Since SentProp produces a ranking rather than a classification, we consider 2989 as a cut-off valu"
N18-1095,D17-1117,0,0.0422101,"5 Finally, we also include our base lexicon (Table 1) in order to evaluate the expansion process of our two expanded lexicons (§5). For all lists, we train on a single feature indicating the frequency of abusive words in a micropost to be classified. Ottawa also contains weights assigned to abusive words. We weight the observed frequency with these weights. We further evaluate 3 classifiers representing the state of the art of in-domain evaluations: FastText (Joulin et al., 2017), Gated Recurrent Units Recurrent Neural Networks RNN, which have been reported to work best on English microposts (Pavlopoulos et al., 2017), and Yahoo, an SVM 13 www.hatebase.org https://en.wiktionary.org/wiki/ Category:English_derogatory_terms 15 There are also similar but smaller lists in Wiktionary, e.g. offensive terms. They produced no better results. 14 test Warner Waseem train Razavi Waseem Wulczyn Average Warner Razavi Wulczyn Average all 55.4 58.1 60.2 57.9 58.5 61.1 51.2 56.9 Yahoo explicit 65.2 55.9 72.8 64.6 61.2 63.1 68.2 64.2 feature-b. lex. all explicit 65.0 80.6 64.6 79.0 63.4 80.7 64.3 80.1 63.3 62.0 58.7 78.8 62.9 78.5 61.6 73.1 Table 12: Cross-domain classification of microposts: all test data vs. explicit subs"
N18-1095,E14-4023,1,0.862613,"tor speaker 9.7 32.8 90.3 67.2 all numbers only refer to the subset of the base lexicon (Table 1) taken from the Subjectivity Lexicon (i.e. 1500 entries) Table 3: Percentage of abusive/not abusive instances among (binary) intensity and views. corpus. A review is awarded between 1 and 5 stars where 1 is the most negative score. We infer the polar intensity of a word by the distribution of starratings associated with the reviews in which it occurs. We assume negative polar expressions with a very high polar intensity to occur significantly more often in reviews assigned few stars (i.e. 1 or 2). Ruppenhofer et al. (2014) established that the most effective method to derive such polar intensity is by ranking words by their weighted mean of star ratings (Rill et al., 2012). All words of our base lexicon are ranked according to that score. As a feature we use the rank of a word. Intensity Directed towards Persons (INTperson ). Not all negative polar expressions with a high intensity are equally likely to be abusive. The high intensity expressions should also be words typically directed towards persons. Most polar statements in AMZ, however, are directed towards a movie, book or some electronic product. In order"
N18-1095,W17-3010,0,0.0611078,"ompiled lexicon from Razavi et al. (2010) and the lexicon of hate verbs from Gitari et al. (2015) have been compiled for this specific task. Since the latter lexicon is not publicly available we can only consider the former in our evaluation. In both publications, very little is said on the creation of these resources. Previous work focused on in-domain classification, a setting where generic features (e.g. bag of 6 https://github.com/miwieg/naacl2018 words) work well and word lists are less important. There have been investigations examining features on various datasets (Nobata et al., 2016; Samghabadi et al., 2017), however, these studies always trained and tested on the same domain. We show that a lexicon-based approach is effective in cross-domain classification. For a more detailed overview on previous work on the detection of abusive language in general, we refer the reader to Schmidt and Wiegand (2017). 3 Data Base Lexicon. Our base lexicon exclusively comprises negative polar expressions. It is a small set which we have annotated via crowdsourcing. We consider abusive words to be a proper subset of negative polar expressions. By just focusing on these types of words, we are more likely to obtain a"
N18-1095,W17-1101,1,0.937306,"tion of these resources. Previous work focused on in-domain classification, a setting where generic features (e.g. bag of 6 https://github.com/miwieg/naacl2018 words) work well and word lists are less important. There have been investigations examining features on various datasets (Nobata et al., 2016; Samghabadi et al., 2017), however, these studies always trained and tested on the same domain. We show that a lexicon-based approach is effective in cross-domain classification. For a more detailed overview on previous work on the detection of abusive language in general, we refer the reader to Schmidt and Wiegand (2017). 3 Data Base Lexicon. Our base lexicon exclusively comprises negative polar expressions. It is a small set which we have annotated via crowdsourcing. We consider abusive words to be a proper subset of negative polar expressions. By just focusing on these types of words, we are more likely to obtain a significant amount of abusive words than just considering a sample of arbitrary words. This lexicon will be used as a gold standard for calibrating features of a classifier. That classifier will be run on a large set of unlabeled negative polar expressions to produce our expanded lexicon (§5). We"
N18-1095,D08-1061,0,0.0389581,"t of PAT from Twitter (§4.1.4) is considered as positive class seed instances. We chose PAT since it is an effective feature that does not depend on a lexical resource. As negative class seeds, we use the most frequent words in the WAC corpus (Table 2). Our rationale is that highfrequency words are unlikely to be abusive. We chose WAC instead of Twitter since the evidence of PAT (Table 5) suggested less abusive language in that corpus. This word-similarity graph is illustrated in Figure 1. In order to propagate the labels to the unlabeled words from the seeds, we use the Adsorption algorithm (Talukdar et al., 2008). Using Labeled Microposts (MICR). With our last baseline we examine in how far we can detect abusive words by only using information from labeled microposts rather than labeled words. These experiments are driven by the fact that labeled microposts already exist. We consider two methods using the largest dataset comprising manually labeled microposts, Wulczyn (Table 8). The class labels of the microposts and our base lexicon (§3) are the same. Our aim is to produce a ranking of words where the high ranks represent words more likely to be abusive. Since we want to produce a strong baseline, we"
N18-1095,N10-1119,0,0.012075,"AMZ and WAC (Table 2) using Word2Vec (Mikolov et al., 2013) in default configuration (i.e. 200 dimensions; cbow). The best performance was obtained by concatenating for each word the vectors induced from the two corpora.9 4.3 Baselines to Feature-based Approach In addition to a majority-class classifier we consider the following baselines: Weak Supervision (WSUP). With this baseline we want to build a lightweight classifier that does not require proper labeled training data. It is inspired by previous induction approaches for sentiment lexicons, such as Hatzivassiloglou and McKeown (1997) or Velikovich et al. (2010) which heuristically label some seed instances and then apply graph-based propagation to label the remaining words of a dataset. On the basis of word embeddings (§4.2), we build a word-similarity graph, where the nodes represent our negative polar expressions and each edge denotes the seman9 We also ran experiments with pretrained embeddings from GoogleNews but they did not improve classification. tic similarity between two arbitrary words. We compute it by the cosine of their word-embedding vectors. The output of PAT from Twitter (§4.1.4) is considered as positive class seed instances. We cho"
N18-1095,W12-2103,0,0.698211,"Missing"
N18-1095,N16-2013,0,0.256071,"resources. These features are calibrated on a small manually annotated base lexicon which we use to produce a large lexicon. We show that the word-level information we learn cannot be equally derived from a large dataset of annotated microposts. We demonstrate the effectiveness of our (domain-independent) lexicon in the crossdomain detection of abusive microposts. 1 Introduction Abusive or offensive language is commonly defined as hurtful, derogatory or obscene utterances made by one person to another person.1 Examples are (1)-(3). In the literature, closely related terms include hate speech (Waseem and Hovy, 2016) or cyber bullying (Zhong et al., 2016). While there may be nuanced differences in meaning2 , they are all compatible with the general definition above for abusive language.3 (1) stop editing this, you dumbass. (2) Just want to slap the stupid out of these bimbos!!! (3) Go lick a pig you arab muslim piece of scum. Due to the rise of user-generated web content, in particular on social media networks, the amount of abusive language is also steadily growing. NLP methods are required to focus human review efforts towards the most relevant microposts. In this paper, we address the task of detecting"
N18-1095,N16-1094,1,0.876145,"ntensity directed towards persons, we replace the AMZ corpus with the RIA corpus (Table 2). RIA contains reviews on arbitrary entities rather than just commercial products as in the case of AMZ. Each review has a category label (e.g. computer, person, travel) that very easily allows us to extract from RIA just those reviews that concern persons. Table 4 compares a typical 1-star review from AMZ with one from RIA. We consider the RIAreview an abusive comment. It contains many words predictive of abusive language (e.g. selfabsorbed, loser, arrogant or loud-mouthed). 4.1.2 Sentiment Views (VIEW) Wiegand et al. (2016b) define sentiment views as the perspective of the opinion holder of polar expressions. They distinguish between expressions conveying the view of the implicit speaker of the utterance typically referred to as speaker views (e.g. cheating in (4); ugly and stinks in (5)), and expressions conveying the view of event participants typically referred to as actor views (e.g. disappointed and horrified in (6); protested in (7)). 1048 corpus RateItAll (RIA) Amazon (AMZ) Web as Corpus (WAC) size 4.7M 1.2B 2.3B properties review corpus focused on persons product review corpus large general web corpus p"
N18-1095,N16-1092,1,0.94168,"ntensity directed towards persons, we replace the AMZ corpus with the RIA corpus (Table 2). RIA contains reviews on arbitrary entities rather than just commercial products as in the case of AMZ. Each review has a category label (e.g. computer, person, travel) that very easily allows us to extract from RIA just those reviews that concern persons. Table 4 compares a typical 1-star review from AMZ with one from RIA. We consider the RIAreview an abusive comment. It contains many words predictive of abusive language (e.g. selfabsorbed, loser, arrogant or loud-mouthed). 4.1.2 Sentiment Views (VIEW) Wiegand et al. (2016b) define sentiment views as the perspective of the opinion holder of polar expressions. They distinguish between expressions conveying the view of the implicit speaker of the utterance typically referred to as speaker views (e.g. cheating in (4); ugly and stinks in (5)), and expressions conveying the view of event participants typically referred to as actor views (e.g. disappointed and horrified in (6); protested in (7)). 1048 corpus RateItAll (RIA) Amazon (AMZ) Web as Corpus (WAC) size 4.7M 1.2B 2.3B properties review corpus focused on persons product review corpus large general web corpus p"
N18-1095,H05-1044,0,0.205019,"Missing"
N18-1095,W17-3012,0,0.247349,"Missing"
N19-1060,D18-1302,0,0.157898,"64.7 Warner (Warner and Hirschberg, 2012) diverse 3,438 14.3 biased sampling 51.3 Waseem (Waseem and Hovy, 2016) Twitter 16,165 35.3 biased sampling 44.4 Kumar (Kumar et al., 2018) Facebook 15,000 58.1 biased sampling 32.7 F1 88.2 87.3 83.3 71.8 80.5 70.4 Table 1: Properties of the different datasets. (∗ : proportion of explicitly abusive microposts among abusive microposts. † : This is an extension of the dataset presented in Wulczyn et al. (2017). Details on the corpus creation about Kaggle can therefore be found in that publication.) rank 1 2 3 4 5 6 7 8 9 10 2018; Mishra et al., 2018a,b; Park et al., 2018; Qian et al., 2018; Sahlgren et al., 2018; Sharifirad et al., 2018; Unsv˚ag and Gamb¨ack, 2018; Wiegand et al., 2018a). This investigation is only possible since, fortunately, Waseem is one of the datasets whose creation process has been meticulously documented. 4 Founta bitch niggas motherfucker fucking nigga idiot asshole fuck fuckin pussy Waseem commentator comedian football announcer pedophile mankind sexist sport outlaw driver Table 2: Top 10 words having strongest correlation with abusive microposts according to PMI on Founta (dataset representing almost random sample) and Waseem (datas"
N19-1060,N18-2019,0,0.0243528,"r and Hirschberg, 2012) diverse 3,438 14.3 biased sampling 51.3 Waseem (Waseem and Hovy, 2016) Twitter 16,165 35.3 biased sampling 44.4 Kumar (Kumar et al., 2018) Facebook 15,000 58.1 biased sampling 32.7 F1 88.2 87.3 83.3 71.8 80.5 70.4 Table 1: Properties of the different datasets. (∗ : proportion of explicitly abusive microposts among abusive microposts. † : This is an extension of the dataset presented in Wulczyn et al. (2017). Details on the corpus creation about Kaggle can therefore be found in that publication.) rank 1 2 3 4 5 6 7 8 9 10 2018; Mishra et al., 2018a,b; Park et al., 2018; Qian et al., 2018; Sahlgren et al., 2018; Sharifirad et al., 2018; Unsv˚ag and Gamb¨ack, 2018; Wiegand et al., 2018a). This investigation is only possible since, fortunately, Waseem is one of the datasets whose creation process has been meticulously documented. 4 Founta bitch niggas motherfucker fucking nigga idiot asshole fuck fuckin pussy Waseem commentator comedian football announcer pedophile mankind sexist sport outlaw driver Table 2: Top 10 words having strongest correlation with abusive microposts according to PMI on Founta (dataset representing almost random sample) and Waseem (dataset produced by bias"
N19-1060,W18-5115,0,0.0270705,"012) diverse 3,438 14.3 biased sampling 51.3 Waseem (Waseem and Hovy, 2016) Twitter 16,165 35.3 biased sampling 44.4 Kumar (Kumar et al., 2018) Facebook 15,000 58.1 biased sampling 32.7 F1 88.2 87.3 83.3 71.8 80.5 70.4 Table 1: Properties of the different datasets. (∗ : proportion of explicitly abusive microposts among abusive microposts. † : This is an extension of the dataset presented in Wulczyn et al. (2017). Details on the corpus creation about Kaggle can therefore be found in that publication.) rank 1 2 3 4 5 6 7 8 9 10 2018; Mishra et al., 2018a,b; Park et al., 2018; Qian et al., 2018; Sahlgren et al., 2018; Sharifirad et al., 2018; Unsv˚ag and Gamb¨ack, 2018; Wiegand et al., 2018a). This investigation is only possible since, fortunately, Waseem is one of the datasets whose creation process has been meticulously documented. 4 Founta bitch niggas motherfucker fucking nigga idiot asshole fuck fuckin pussy Waseem commentator comedian football announcer pedophile mankind sexist sport outlaw driver Table 2: Top 10 words having strongest correlation with abusive microposts according to PMI on Founta (dataset representing almost random sample) and Waseem (dataset produced by biased sampling). Topic Bia"
N19-1060,E17-2068,0,0.10408,"Missing"
N19-1060,W18-5114,0,0.0344333,"biased sampling 51.3 Waseem (Waseem and Hovy, 2016) Twitter 16,165 35.3 biased sampling 44.4 Kumar (Kumar et al., 2018) Facebook 15,000 58.1 biased sampling 32.7 F1 88.2 87.3 83.3 71.8 80.5 70.4 Table 1: Properties of the different datasets. (∗ : proportion of explicitly abusive microposts among abusive microposts. † : This is an extension of the dataset presented in Wulczyn et al. (2017). Details on the corpus creation about Kaggle can therefore be found in that publication.) rank 1 2 3 4 5 6 7 8 9 10 2018; Mishra et al., 2018a,b; Park et al., 2018; Qian et al., 2018; Sahlgren et al., 2018; Sharifirad et al., 2018; Unsv˚ag and Gamb¨ack, 2018; Wiegand et al., 2018a). This investigation is only possible since, fortunately, Waseem is one of the datasets whose creation process has been meticulously documented. 4 Founta bitch niggas motherfucker fucking nigga idiot asshole fuck fuckin pussy Waseem commentator comedian football announcer pedophile mankind sexist sport outlaw driver Table 2: Top 10 words having strongest correlation with abusive microposts according to PMI on Founta (dataset representing almost random sample) and Waseem (dataset produced by biased sampling). Topic Bias The Waseem-dataset has"
N19-1060,W18-5117,0,0.140307,"Missing"
N19-1060,W18-5110,0,0.0943693,"Missing"
N19-1060,W18-5104,0,0.0424433,"Missing"
N19-1060,W12-2103,0,0.642114,"Missing"
N19-1060,W18-4401,0,0.101243,"Missing"
N19-1060,W17-3012,0,0.384825,"nces made by one person to another person.1 Examples are (1)-(3). In the literature, closely related terms include hate speech (Waseem and Hovy, 2016) or cyber bullying (Zhong et al., 2016). While there may be nuanced differences in meaning, they are all compatible with the general definition above. 2 (1) stop editing this, you dumbass. (2) Just want to slap the stupid out of these bimbos!!! (3) Go lick a pig you arab muslim piece of scum. Explicit and Implicit Abuse One major distinction that has been proposed in the literature is the division into explicitly and implicitly abusive language (Waseem et al., 2017). The former are microposts that employ some abusive words (1)-(3) (e.g. dumbass or scum), while the latter represents the more difficult case in which the abusive nature is conveyed by other means, such as sarcasm, jokes, and particularly the usage of negative stereotypes etc. (4)-(5). Due to the rise of user-generated web content, in particular on social media networks, the amount of abusive language is also steadily growing. NLP methods are required to focus human review efforts towards the most relevant microposts. In this paper, we examine the issue of data bias. For the creation of manua"
N19-1060,C18-1093,0,0.0325417,"boosted random sampling 64.7 Warner (Warner and Hirschberg, 2012) diverse 3,438 14.3 biased sampling 51.3 Waseem (Waseem and Hovy, 2016) Twitter 16,165 35.3 biased sampling 44.4 Kumar (Kumar et al., 2018) Facebook 15,000 58.1 biased sampling 32.7 F1 88.2 87.3 83.3 71.8 80.5 70.4 Table 1: Properties of the different datasets. (∗ : proportion of explicitly abusive microposts among abusive microposts. † : This is an extension of the dataset presented in Wulczyn et al. (2017). Details on the corpus creation about Kaggle can therefore be found in that publication.) rank 1 2 3 4 5 6 7 8 9 10 2018; Mishra et al., 2018a,b; Park et al., 2018; Qian et al., 2018; Sahlgren et al., 2018; Sharifirad et al., 2018; Unsv˚ag and Gamb¨ack, 2018; Wiegand et al., 2018a). This investigation is only possible since, fortunately, Waseem is one of the datasets whose creation process has been meticulously documented. 4 Founta bitch niggas motherfucker fucking nigga idiot asshole fuck fuckin pussy Waseem commentator comedian football announcer pedophile mankind sexist sport outlaw driver Table 2: Top 10 words having strongest correlation with abusive microposts according to PMI on Founta (dataset representing almost random sam"
N19-1060,N16-2013,0,0.677141,"hat classification scores on popular datasets reported in previous work are much lower under realistic settings in which this bias is reduced. Such biases are most notably observed on datasets that are created by focused sampling instead of random sampling. Datasets with a higher proportion of implicit abuse are more affected than datasets with a lower proportion. 1 Introduction Abusive or offensive language is commonly defined as hurtful, derogatory or obscene utterances made by one person to another person.1 Examples are (1)-(3). In the literature, closely related terms include hate speech (Waseem and Hovy, 2016) or cyber bullying (Zhong et al., 2016). While there may be nuanced differences in meaning, they are all compatible with the general definition above. 2 (1) stop editing this, you dumbass. (2) Just want to slap the stupid out of these bimbos!!! (3) Go lick a pig you arab muslim piece of scum. Explicit and Implicit Abuse One major distinction that has been proposed in the literature is the division into explicitly and implicitly abusive language (Waseem et al., 2017). The former are microposts that employ some abusive words (1)-(3) (e.g. dumbass or scum), while the latter represents the more di"
N19-1060,W18-5101,0,0.0345282,"boosted random sampling 64.7 Warner (Warner and Hirschberg, 2012) diverse 3,438 14.3 biased sampling 51.3 Waseem (Waseem and Hovy, 2016) Twitter 16,165 35.3 biased sampling 44.4 Kumar (Kumar et al., 2018) Facebook 15,000 58.1 biased sampling 32.7 F1 88.2 87.3 83.3 71.8 80.5 70.4 Table 1: Properties of the different datasets. (∗ : proportion of explicitly abusive microposts among abusive microposts. † : This is an extension of the dataset presented in Wulczyn et al. (2017). Details on the corpus creation about Kaggle can therefore be found in that publication.) rank 1 2 3 4 5 6 7 8 9 10 2018; Mishra et al., 2018a,b; Park et al., 2018; Qian et al., 2018; Sahlgren et al., 2018; Sharifirad et al., 2018; Unsv˚ag and Gamb¨ack, 2018; Wiegand et al., 2018a). This investigation is only possible since, fortunately, Waseem is one of the datasets whose creation process has been meticulously documented. 4 Founta bitch niggas motherfucker fucking nigga idiot asshole fuck fuckin pussy Waseem commentator comedian football announcer pedophile mankind sexist sport outlaw driver Table 2: Top 10 words having strongest correlation with abusive microposts according to PMI on Founta (dataset representing almost random sam"
N19-1060,N18-1095,1,0.890618,"marry children. Muslims do. All the time. To determine which of the datasets that we consider in this work contain which type of abusive language, we proceeded as follows. On the set of abusive microposts of each dataset, we computed the proportion of microposts that include at least 0 Present affiliation: Leibniz ScienceCampus, Heidelberg/Mannheim, Germany 1 http://thelawdictionary.org/ 602 Proceedings of NAACL-HLT 2019, pages 602–608 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics one abusive word according to the lexicon of abusive words from Wiegand et al. (2018a). Datasets with a high proportion of abusive words typically contain a high amount of explicitly abusive microposts, whereas datasets with a low proportion contain a higher amount of implicitly abusive language. The resulting figures, of course, are only a lower bound estimate for explicit language abuse. There will also be microposts containing abusive words that are missing from the lexicon. However, after manual inspection of a sample of microposts, we are fairly confident that this does not significantly change the relative order of datasets when ranked according to their degree of expli"
N19-1211,E09-1005,0,0.0160256,"of NEG. From that ranking we remove all those compounds which have not co-occurred at the high ranks of at least one of the other diagnostics (COMCON, OUT, PERSON).9 Compounds that are highly ranked by several diagnostics should more likely represent derogatory compounds. Re-Ranking by PageRank (PRANK). We observed that among the top ranks of COMB, the derogatory compounds are semantically similar (e.g. dwarf tosser, mischief maker, slimeball) while the non-derogatory compounds are semantically different from each other (e.g. biker club, spirit bear). Therefore, we run personalized PageRank (Agirre and Soroa, 2009) to further improve the ranking by enforcing the compounds on the high ranks to be distributionally similar. We build a word-similarity graph where our compounds are nodes and edges encode cosine-similarities of their embeddings. PageRank then produces a ranking of nodes where the highest ranked nodes are the ones most highly connected. In personalized PageRank prior information is added. A biased graph is constructed in which attention is drawn towards particular regions of interest. This is achieved by assigning re-entrance weights to the individual nodes. As prior information, we set the no"
N19-1211,D15-1188,0,0.031153,"(of which embeddings are the main contributor) on the constituents of the compounds. Moreover, we train an LSTM on the sequence of characters of the compound. Table 4 shows that information drawn from units other than the compound itself is less effective. The feature combination of head, modifier and compound is not effective either. Instead of applying embeddings on constituents and concatenating them, we also examine a sophisticated compositional model (Wmask) based on a masking process that takes into account the variation of a constituent depending on whether it is a head or a modifier (Dima, 2015). Table 5 shows the performance of the two best previous classifiers where compounds lacking an embedding are represented by an embedding approximated by Wmask (rather than a dummy vector). The table shows that the two classifiers can be improved by adding the approximated embeddings. 6 Conclusion We examined the new task of detecting derogatory compounds and proposed an unsupervised approach incorporating linguistic properties of compounds that mostly depend on a distributional representation. Our method outperforms linguistic features previously shown to be effective for the detection of der"
N19-1211,W09-4635,0,0.0196144,"d compounds, e.g. Milchbube (milk sop) or Schnapsdrossel (booze hound), we can employ standard tokenization7 for inducing embeddings for our compounds. 4.1 Individual High-Precision Diagnostics Negative Polarity (NEG). Derogatory words form a subset of negative polar expressions. Due to their sparsity, however, derogatory compounds are rarely part of any sentiment lexicon (containing polar expressions). We, therefore, rank all our compounds according to their cosine similarity to a centroid embedding-vector computed from all negative polar expressions from the German PolArt sentiment lexicon (Klenner et al., 2009). Compound Occurrence vs. Constituent Occurrence (COMCON). Derogatory compounds can be creative word constructions (e.g. booze hound, oxygen thief, keyboard warrior). Consequently, their constituents are often not semantically related. For instance, in booze hound, booze bears no common semantic relation to hound. Therefore, the corpus frequency of a derogatory compound should be much higher than its constituents co-occurring in a sentence (i.e. with other words occurring in between). Such cooccurrences should be coincidental. We capture this by the following formula (frequencies are computed"
N19-1211,D08-1061,0,0.0492548,"awn towards particular regions of interest. This is achieved by assigning re-entrance weights to the individual nodes. As prior information, we set the nodes representing the compounds returned by COMB with a uniform re-entrance weight (α)10 while all other nodes receive a weight of 0. Label Propagation (LP). While previous diagnostics were designed to isolate a few derogatory compounds with a high precision, LP aims for increasing recall. We define some high-precision seeds for the two categories of our task and then propagate the labels to the unlabeled compounds by using label propagation (Talukdar et al., 2008). The algorithm operates on the same wordsimilarity graph that we used for PRANK. We define highly ranked compounds from PRANK as 9 We took top 350 from all these rankings which resembles the number of derogatory compounds on our dataset. 10 Following Manning et al. (2008), we set α = 0.1. derogatory seeds and lowly ranked compounds as non-derogatory seeds. Unlike the previous diagnostics, the output of LP is a binary categorization rather than a ranking. In order to make this output comparable to the other diagnostics, we converted the output of LP to a ranking. This is achieved by ranking th"
N19-1211,N16-2013,0,0.0713261,"represented in lexical resources previously found effective for this task (e.g. Wiktionary). We propose an unsupervised classification approach that incorporates linguistic properties of compounds. It mostly depends on a simple distributional representation. We compare our approach against previously established methods proposed for extracting derogatory unigrams. 1 Introduction Abusive or offensive language is commonly defined as hurtful, derogatory or obscene utterances made by one person to another person.1 Examples are (1)-(3). In the literature, closely related terms include hate speech (Waseem and Hovy, 2016) or cyber bullying (Zhong et al., 2016). While there may be nuanced differences in meaning, they are all compatible with the general definition above. (1) stop editing this, you dumbass. (2) Just want to slap the stupid out of these bimbos!!! (3) Go lick a pig you arab muslim piece of scum. Due to the rise of user-generated web content, in particular on social media networks, the amount of abusive language is also steadily growing. NLP methods are required to focus human review efforts towards the most relevant microposts. A substantial amount of abusive utterances comprises derogatory words ("
N19-1211,N18-1095,1,0.930788,"l definition above. (1) stop editing this, you dumbass. (2) Just want to slap the stupid out of these bimbos!!! (3) Go lick a pig you arab muslim piece of scum. Due to the rise of user-generated web content, in particular on social media networks, the amount of abusive language is also steadily growing. NLP methods are required to focus human review efforts towards the most relevant microposts. A substantial amount of abusive utterances comprises derogatory words (e.g. bimbo or scum). Automatic extraction methods of such words are required since new derogatory words constantly enter language. Wiegand et al. (2018a) extracted a 0 Present affiliation: Leibniz ScienceCampus, Heidelberg/Mannheim, Germany 1 http://thelawdictionary.org/ large list of such expressions and demonstrated its importance for text classification. In this work, we focus on a subtype of derogatory terms, namely derogatory compounds (e.g. booze hound, curry muncher, fault finder). Distinguishing such multi-word expressions from nonderogatory ones (e.g. fox hound, mile muncher, branch finder) is more difficult than classifying unigrams since they are only sparsely represented in general-purpose lexical resources which have previously"
N19-1211,W17-1101,1,0.839554,"rhero.com/de/insults.htm www.schimpfwoerter.de www.seechat.de/warmduscher.htm 6 These lists contain many compounds commonly used in a non-offensive manner, e.g. Colatrinker (coke drinker). 4 0.01 0 0 200 400 600 800 1000 Rank Figure 1: Head distribution of derogatory compounds. Property Freq total compounds 3500 derogatory compounds 382 head groups (each group contains 20 compounds) 175 average no. of derogatory compounds in head group 2.2 Table 1: Some statistics of the gold standard. Lexical knowledge for the detection of abusive language has only received little attention in previous work (Schmidt and Wiegand, 2017), the notable exceptions are Razavi et al. (2010) who present a manually-compiled lexicon, Gitari et al. (2015) who bootstrap hate verbs and Wiegand et al. (2018a) who induce a lexicon of derogatory words. In all these researches, however, derogatory compounds are not explicitly addressed. 3 0.015 0.005 Related Work 3 0.02 stag hound). Since among those putative nonderogatory instances, there could well be further derogatory compounds, we manually annotated them as well. We limited the set of compounds sharing the same head, which we henceforth call head group, to 20 compounds. Thus, we hope t"
R11-1039,H05-1045,0,0.746629,"different ways of harnessing mentions of protoOHs for OH extraction. We compare their usage as labeled training data for supervised learning with a rule-based classifier that relies on a lexicon of predictive predicates that have been extracted from the contexts of protoOHs. Moreover, we investigate in how far the knowledge gained from these contexts can compensate the lack of large amounts of actually labeled training data in supervised classification by considering various amounts of labeled training sets. 2 Related Work There has been much research on supervised learning for OH extraction. Choi et al. (2005) explore OH extraction using CRFs with several manually defined linguistic features and automatically learnt surface patterns. The linguistic features focus on named-entity information and syntactic relations to opinion words. Kim and Hovy (2006) and Bethard et al. (2004) examine the usefulness of semantic roles provided by FrameNet1 for both OH and opinion target extraction. More recently, Wiegand and Klakow (2010) explored convolution kernels for OH extraction and found that tree kernels outperform all other kernel types. In (Johansson and Moschitti, 2010), a re-ranking approach modeling com"
R11-1039,C10-1059,0,0.173728,"earch on supervised learning for OH extraction. Choi et al. (2005) explore OH extraction using CRFs with several manually defined linguistic features and automatically learnt surface patterns. The linguistic features focus on named-entity information and syntactic relations to opinion words. Kim and Hovy (2006) and Bethard et al. (2004) examine the usefulness of semantic roles provided by FrameNet1 for both OH and opinion target extraction. More recently, Wiegand and Klakow (2010) explored convolution kernels for OH extraction and found that tree kernels outperform all other kernel types. In (Johansson and Moschitti, 2010), a re-ranking approach modeling complex relations between multiple opinions in a sentence is presented. Rule-based OH extraction heavily relies on lexical cues. Bloom et al. (2007), for example, use a list of manually compiled communication verbs. 1 Introduction Building an opinion holder (OH) extraction system on the basis of supervised classifiers requires large amounts of labeled training data which are expensive to obtain. Therefore, alternative methods requiring less human effort are required. Such methods would be particularly valuable for languages other than English as for most other"
R11-1039,W06-0301,0,0.210514,"from the contexts of protoOHs. Moreover, we investigate in how far the knowledge gained from these contexts can compensate the lack of large amounts of actually labeled training data in supervised classification by considering various amounts of labeled training sets. 2 Related Work There has been much research on supervised learning for OH extraction. Choi et al. (2005) explore OH extraction using CRFs with several manually defined linguistic features and automatically learnt surface patterns. The linguistic features focus on named-entity information and syntactic relations to opinion words. Kim and Hovy (2006) and Bethard et al. (2004) examine the usefulness of semantic roles provided by FrameNet1 for both OH and opinion target extraction. More recently, Wiegand and Klakow (2010) explored convolution kernels for OH extraction and found that tree kernels outperform all other kernel types. In (Johansson and Moschitti, 2010), a re-ranking approach modeling complex relations between multiple opinions in a sentence is presented. Rule-based OH extraction heavily relies on lexical cues. Bloom et al. (2007), for example, use a list of manually compiled communication verbs. 1 Introduction Building an opinio"
R11-1039,D09-1012,0,0.0255372,"s, and adjectives). That is, we rank every predicate according to its correlation, i.e. we use Pointwise Mutual Information, of having agentive protoOHs as an argument. The highly ranked predicates are used as predictive cues. The resulting rule-based classifier always classifies an NP as an OH if its head is an agent of a highly ranked discriminative predicate (as illustrated in the right half of Table 2). The supervised kernel-based classifier from §4.1 learns from a rich set of features. In a previous study on reverse engineering making implicit features within convolution kernels visible (Pighin and Moschitti, 2009), it has been shown that the learnt features are usually fairly small subtrees. There are plenty of structures which just contain one or two leaf nodes, i.e. sparse lexical information, coupled with some further structural nodes from the parse tree. These structures are fairly similar to low-level features, such as bag of words or bag of ngrams, in the sense that they are weak predictors and that there are plenty of them. For such types of features, it has been shown in both subjectivity detection (Lambov et al., 2009) and polarity classification (Andreevskaia and Bergler, 2008) that they gene"
R11-1039,P08-1034,0,0.0822727,"n kernels visible (Pighin and Moschitti, 2009), it has been shown that the learnt features are usually fairly small subtrees. There are plenty of structures which just contain one or two leaf nodes, i.e. sparse lexical information, coupled with some further structural nodes from the parse tree. These structures are fairly similar to low-level features, such as bag of words or bag of ngrams, in the sense that they are weak predictors and that there are plenty of them. For such types of features, it has been shown in both subjectivity detection (Lambov et al., 2009) and polarity classification (Andreevskaia and Bergler, 2008) that they generalize poorly across different domains. On the other hand, very few high-level features describing the presence of certain semantic classes or opinion words perform consistently well across different domains. These features can either be incorporated within a supervised learner (Lambov et al., 2009) or a lexicon-based rule-based classifier (Andreevskaia and Bergler, 2008). We assume that our rule-based classifier 4.2.1 Self-training A shortcoming of the rule-based classifier is that it incorporates no (or hardly any) domain knowledge. In other related sentiment classification ta"
R11-1039,P10-1040,0,0.145188,"ce, if doubt is such a predicate, we would replace the subtree [V BP doubt] by [V BP [P REDOH doubt]]. Moreover, we devise a simple vector kernel incorporating the prediction of the rule-based classifier. All kernels are combined by plain summation. domain-specific features the supervised classifier may learn could be useful prior weights towards some of these domain-specific NPs as to whether they might be an OH or not. 4.2.2 Generalization with Clustering and Knowledge Basis We also examine in how far the coverage of the discriminant predicates can be increased with the usage of clustering. Turian et al. (2010) have shown that in semi-supervised learning for namedentity recognition, i.e. a task which bears some resemblance to the present task, features referring to the clusters corresponding to groups of specific words with similar properties (induced in an unsupervised manner) help to improve performance. In the context of our rule-based classifier, we augment the set of discriminant predicates by all words which are also contained in the cluster associated with these discriminant predicates. Hopefully, due to the strong similarity among the words within the same cluster, the additional words will"
R11-1039,N10-1121,1,0.291454,"d training data in supervised classification by considering various amounts of labeled training sets. 2 Related Work There has been much research on supervised learning for OH extraction. Choi et al. (2005) explore OH extraction using CRFs with several manually defined linguistic features and automatically learnt surface patterns. The linguistic features focus on named-entity information and syntactic relations to opinion words. Kim and Hovy (2006) and Bethard et al. (2004) examine the usefulness of semantic roles provided by FrameNet1 for both OH and opinion target extraction. More recently, Wiegand and Klakow (2010) explored convolution kernels for OH extraction and found that tree kernels outperform all other kernel types. In (Johansson and Moschitti, 2010), a re-ranking approach modeling complex relations between multiple opinions in a sentence is presented. Rule-based OH extraction heavily relies on lexical cues. Bloom et al. (2007), for example, use a list of manually compiled communication verbs. 1 Introduction Building an opinion holder (OH) extraction system on the basis of supervised classifiers requires large amounts of labeled training data which are expensive to obtain. Therefore, alternative"
R11-1039,H05-1044,0,0.0692487,"o-)supervised classifier does not perform as well as our best rule-based classifier (induced by protoOHs). The fact that, in addition to that, our proposed method also largely outperforms the rule-based classifier relying on both AL and SL when no heuristics are used and is still slightly better when they are incorporated supports the effectiveness of our method. quality of the extracted words, we mark the words which can also be found in task-specific resources, i.e. communication verbs from the Appraisal Lexicon (AL) (Bloom et al., 2007) and opinion words from the Subjectivity Lexicon (SL) (Wilson et al., 2005). Both resources have been found predictive for OH extraction (Bloom et al., 2007; Wiegand and Klakow, 2010). Table 3 (lower part) shows the performance of the rule-based classifiers based on protoOHs using different parts of speech. As hard baselines, the table also shows other rule-based classifiers using the same dependency relations as our rulebased classifier (see Table 2) but employing different predicates. As lexical resources for these predicates, we again use AL and SL. The table also compares two different versions of the rule-based classifier being the classifier as presented in §4."
R11-1039,J92-4003,0,0.434474,"01 44.35 54.42 48.87 39.14 62.71 48.20 44.38 59.61 50.88 Table 5: Performance of extended rule-based classifiers. the entire set of protoOHs. The performance of the different subsets is very similar (i.e. 46.44, 46.28, and 46.17), so we may conclude that the configuration that we proposed, namely to consider all protoOHs, is more or less the optimal configuration for this method. 5.2.2 Self-training and Generalization Table 5 shows the performance of our method when extended by either self-training (SelfTr) or generalization. For generalization by clustering (Clus), we chose Brown clustering (Brown et al., 1992) which is the best performing algorithm in (Turian et al., 2010). The clusters are induced on our unlabeled corpus (see §3). We induced 1000 clusters (optimal size). For the knowledge-based generalization (WN), we used synonyms from WordNet 3. For both Clus and WN, we display the results extending only the most highly ranked V100+N50+A50 since it provided notably better results than extending all predicates, i.e. V250+N100+A100 (our baseline). The table shows that only self-training consistently improves the results. The impact of generalization is less advantageous since by increasing recall"
R11-1039,W08-2126,0,0.0237466,"discriminant predicates. Unlike our extraction phase for OH extraction in which only the correlation between predicates and protoOHs are considered (Table 2), we may find additional predicates as the clustering is induced from completely unrestricted text. The extension of discriminant predicates can also be done by taking into account manually built general-purpose lexical resources, such as WordNet.3 One simply adds the entire set of synonyms of each of the predicates. 4.3 5 Experiments The documents were parsed using the Stanford Parser.4 Semantic roles were obtained by using the parser by Zhang et al. (2008). 5.1 Supervised Learning All experiments using convolution kernels were done with the SVM-Light-TK toolkit.5 We test two versions of the supervised classifier. The first considers any mention of a protoOH as an OH, while the second is restricted to only those mentions of a protoOH which are an agent of some predicate. We also experimented with different amounts of (pseudo-)labeled training data from our unlabeled corpus varying from 12500 to 150000 instances. We found that from 25000 instances onwards the classifier does not notably improve when further training data are added. The results of"
R11-1039,E06-1015,0,\N,Missing
R15-1071,P10-1018,0,0.443956,"Missing"
R15-1071,Q13-1023,0,0.438187,"Missing"
R15-1071,C12-2036,0,0.0143082,". In subsequent work, Rill et al. (2012b) mention ways to infer the scores of unobserved adverb-adjective combinations based on observed combinations involving other, similar adjectives. However, the authors do not implement and evaluate these ideas. Finally, a great deal of research on intensity has focused on acquiring prior polarity scores for individual words, and specifically adjectives. Various methods have been explored, including phrasal patterns (Sheinman et al., 2013; de Melo and Bansal, 2013); the use of star ratings (Rill et al., 2012b); extracting knowledge from lexical resources Gatti and Guerini (2012); and collostructional analysis (Ruppenhofer et al., 2014). 7 Conclusion We examined various methods for ranking degree adverbs by their effect on the intensity of adjectives. We evaluated the methods against a new carefully-built gold standard that we collected experimentally as well as against a larger expertconstructed gold standard that we found to correlate well with ours for the overlapping members. While we found one method, Horn surface patterns, to currently not be workable at all due to the lack of suitable n-gram resources, we developed a MeanStar-based method that produces very goo"
R15-1071,E14-4023,1,0.799069,"lment. For instance, as illustrated by de Marneffe et al. (2010), when interpreting dialogue (A: Was it good? B: It was ok / great / excellent.), a yes/no question involving a gradable predicate may require understanding the entailment relations between that predicate and another contained in the answer. Another application is within sentiment analysis, where assessing the strength of subjective expressions (e.g. good &lt; great &lt; excellent) is one of the central tasks besides subjectivity detection and polarity classification (Rill et al., 2012b; Sheinman et al., 2013; de Melo and Bansal, 2013; Ruppenhofer et al., 2014, inter alia). It is also well known that subjective adjectives are frequently modified by adverbs that increase (very expensive) or decrease (fairly expensive) their intensity. As Benamara et al. (2007) have shown, it is useful to take such adverbial intensification into account when predicting document-level sentiment scores. However, Benamara et al. (2007) used human-assigned scores to model adverbs’ effect on adjectives. As far as we know, there is no well-established automatic method that can determine for degree adverbs what their effect will be on the intensity of various adjectives. In"
R15-1071,J11-2001,0,0.52357,"produces essentially the same result: the Spearman rank correlation with the absolute ranking in Table 5 is ρ=0.993. Due to space limitations, we only report results relative to the absolute gold standard in the remainder of the paper. In order to be able to experiment with more than the 14 prototypical and frequent adverbs that we could collect ratings for, we make use of the intensity ratings for 93 adverbs provided by Taboada et al.’s (2011) SoCaL resource. While various lexical resources provide polarity scores for nouns, verbs, and adjectives (Wilson et al., 2005; Thelwall et al., 2010; Taboada et al., 2011, inter alia), few resources cover and assign scores to degree adverbs. The adverb ranking obtained from the SoCaL resource for our 14 adverbs correlates strongly with our two gold standards, with coefficients of 0.969 against the absolute gold standard and 0.976 against the relative one. This gives us confidence that we can use the SoCaL ratings as an extended gold standard. Note that the set of 93 adverbs from SoCaL contains many adverbs that are less frequent and less grammaticized than the 14 adverbs from the smaller set. 4 Methods Our methods to determine the intensifying effect of adverb"
R15-1071,H05-1044,0,0.0227395,"combination involving adverb B. That method produces essentially the same result: the Spearman rank correlation with the absolute ranking in Table 5 is ρ=0.993. Due to space limitations, we only report results relative to the absolute gold standard in the remainder of the paper. In order to be able to experiment with more than the 14 prototypical and frequent adverbs that we could collect ratings for, we make use of the intensity ratings for 93 adverbs provided by Taboada et al.’s (2011) SoCaL resource. While various lexical resources provide polarity scores for nouns, verbs, and adjectives (Wilson et al., 2005; Thelwall et al., 2010; Taboada et al., 2011, inter alia), few resources cover and assign scores to degree adverbs. The adverb ranking obtained from the SoCaL resource for our 14 adverbs correlates strongly with our two gold standards, with coefficients of 0.969 against the absolute gold standard and 0.976 against the relative one. This gives us confidence that we can use the SoCaL ratings as an extended gold standard. Note that the set of 93 adverbs from SoCaL contains many adverbs that are less frequent and less grammaticized than the 14 adverbs from the smaller set. 4 Methods Our methods t"
ruppenhofer-etal-2017-evaluating,waltinger-2010-germanpolarityclues,0,\N,Missing
ruppenhofer-etal-2017-evaluating,J04-3002,0,\N,Missing
ruppenhofer-etal-2017-evaluating,P08-2028,0,\N,Missing
ruppenhofer-etal-2017-evaluating,W14-5805,0,\N,Missing
ruppenhofer-etal-2017-evaluating,E14-4023,1,\N,Missing
ruppenhofer-etal-2017-evaluating,D13-1170,0,\N,Missing
ruppenhofer-etal-2017-evaluating,W14-4721,1,\N,Missing
ruppenhofer-etal-2017-evaluating,N15-1071,0,\N,Missing
ruppenhofer-etal-2017-evaluating,N16-1094,1,\N,Missing
ruppenhofer-etal-2017-evaluating,W09-4635,0,\N,Missing
W09-4628,P04-1034,0,0.017029,"s assigned the polarity derived from the average of the po1 2 http://www.rateitall.com http://www.wjh.harvard.edu/∼inquirer larity scores of the words occurring within the document. The most recent semi-automatic lexicon is SentiWordNet (Esuli and Sebastiani, 2006) which assigns polarity to word senses in WordNet3 known as synsets. The polarity of manually annotated seed synsets is expanded onto the remaining synsets of the WordNet ontology by measuring the overlap between their respective glosses. The only works dealing with semi-supervised learning on this classification task we know of are Beineke et al. (2004) who combine Turney’s web mining approach with evidence from labeled training data, and Aue and Gamon (2005) who focus on domain adaptation. Neither different algorithms nor feature sets are compared in these works. In this paper, we look into adjectives & adverbs as features in detail. Pang et al. (2002) use feature sets exclusively comprising adjectives for supervised polarity classification but report performance to be worse than a standard bag-of-words representation. However, Ng et al. (2006) increase performance significantly by adding to a standard feature set higher order n-grams in wh"
W09-4628,P06-2079,0,0.247456,"rity lexicons are an alternative option, however, they are expensive to create and their individual effectiveness may vary across different domains. We show that a small list of frequently occurring adjectives & adverbs cheaply extracted from an unlabeled in-domain dataset usually has competitive performance. We consider polarity classification as a binary classification problem. That is, we assume that each document to be classified is subjective. We neglect the distinction between objective and subjective content since this classification is usually solved independently (Pang and Lee, 2004; Ng et al., 2006). Besides Ng et al. (2006) report that document-level subjectivity detection is a rather easy task compared to (binary) document-level poPredictive Features in Semi-Supervised Learning for Polarity Classification larity classification. In our experiments, we primarily use the standard dataset from Pang et al. (2002) comprising movie reviews. To substantiate that our insights carry over to other domains, we also use a multidomain dataset we created from Rate-It-All1 . To the best of our knowledge, this is the first time that several semi-supervised classifiers are evaluated on this learning tas"
W09-4628,P04-1035,0,0.0129909,"classification. Polarity lexicons are an alternative option, however, they are expensive to create and their individual effectiveness may vary across different domains. We show that a small list of frequently occurring adjectives & adverbs cheaply extracted from an unlabeled in-domain dataset usually has competitive performance. We consider polarity classification as a binary classification problem. That is, we assume that each document to be classified is subjective. We neglect the distinction between objective and subjective content since this classification is usually solved independently (Pang and Lee, 2004; Ng et al., 2006). Besides Ng et al. (2006) report that document-level subjectivity detection is a rather easy task compared to (binary) document-level poPredictive Features in Semi-Supervised Learning for Polarity Classification larity classification. In our experiments, we primarily use the standard dataset from Pang et al. (2002) comprising movie reviews. To substantiate that our insights carry over to other domains, we also use a multidomain dataset we created from Rate-It-All1 . To the best of our knowledge, this is the first time that several semi-supervised classifiers are evaluated on"
W09-4628,W02-1011,0,0.0358933,"onsider polarity classification as a binary classification problem. That is, we assume that each document to be classified is subjective. We neglect the distinction between objective and subjective content since this classification is usually solved independently (Pang and Lee, 2004; Ng et al., 2006). Besides Ng et al. (2006) report that document-level subjectivity detection is a rather easy task compared to (binary) document-level poPredictive Features in Semi-Supervised Learning for Polarity Classification larity classification. In our experiments, we primarily use the standard dataset from Pang et al. (2002) comprising movie reviews. To substantiate that our insights carry over to other domains, we also use a multidomain dataset we created from Rate-It-All1 . To the best of our knowledge, this is the first time that several semi-supervised classifiers are evaluated on this learning task in depth, in particular, in combination with various feature sets. 2 Related Work Fully supervised polarity classification has been extensively explored. Both discriminative methods, such as support vector machines (SVMs), and generative methods have been applied (Pang et al., 2002; Salvetti et al., 2006). Discrim"
W09-4628,P07-1056,0,0.0378195,"lassifiers, whether polarity lexicons improve performance, and whether adjectives and adverbs produce classifiers competitive to average polarity lexicons. We do not attempt to carry out detailed domain studies which would be beyond the scope of this section. We chose four domains from the list of Topic Categories of the website which we thought are very different from the movie domain and for which we could extract sufficient training data. We took Computer & Internet (computer), Products (products), Sports & Recreation (sports) and Travel, Food, & Culture (travel). We follow the method from Blitzer et al. (2007) to infer the polarity of the reviews. Ratings with less than 3 stars are considered negative reviews whereas ratings with more than 3 stars are positive reviews. We decided not to consider mixed reviews, i.e. reviews rated with 3 stars. In general, we found far fewer mixed reviews15 . On those domains which provided a reasonable amount of data, our initial supervised learning experiments showed that mixed polarity can only be poorly distinguished from definite polarity16 . Manual inspection of a random sample of reviews also showed that a great part of these documents are actually negative re"
W09-4628,P02-1053,0,0.00407775,"ually, the gain in performance hardly justifies the computational overhead of these methods (Gamon, 2004). There are several domain-independent polarity lexicons containing important polar expressions. The most prominent manual lexicons are General Inquirer2 , the subjectivity lexicon from the MPQA-project (Wilson et al., 2005), and Appraisal Groups (Whitelaw et al., 2005). They have been successfully applied to polarity classification (Kennedy and Inkpen, 2005; Wilson et al., 2005; Whitelaw et al., 2005). Moreover, several methods have been proposed to automatically induce polarity lexicons. Turney (2002) applies Pointwise Mutual Information in order to find similar words to a given list of polar seed words on web data. The polarity scores which are thus computed for each word can be used for a completely unsupervised classification algorithm of documents. A document is assigned the polarity derived from the average of the po1 2 http://www.rateitall.com http://www.wjh.harvard.edu/∼inquirer larity scores of the words occurring within the document. The most recent semi-automatic lexicon is SentiWordNet (Esuli and Sebastiani, 2006) which assigns polarity to word senses in WordNet3 known as synset"
W09-4628,esuli-sebastiani-2006-sentiwordnet,0,0.0216837,", several methods have been proposed to automatically induce polarity lexicons. Turney (2002) applies Pointwise Mutual Information in order to find similar words to a given list of polar seed words on web data. The polarity scores which are thus computed for each word can be used for a completely unsupervised classification algorithm of documents. A document is assigned the polarity derived from the average of the po1 2 http://www.rateitall.com http://www.wjh.harvard.edu/∼inquirer larity scores of the words occurring within the document. The most recent semi-automatic lexicon is SentiWordNet (Esuli and Sebastiani, 2006) which assigns polarity to word senses in WordNet3 known as synsets. The polarity of manually annotated seed synsets is expanded onto the remaining synsets of the WordNet ontology by measuring the overlap between their respective glosses. The only works dealing with semi-supervised learning on this classification task we know of are Beineke et al. (2004) who combine Turney’s web mining approach with evidence from labeled training data, and Aue and Gamon (2005) who focus on domain adaptation. Neither different algorithms nor feature sets are compared in these works. In this paper, we look into"
W09-4628,C04-1121,0,0.0308108,"s been extensively explored. Both discriminative methods, such as support vector machines (SVMs), and generative methods have been applied (Pang et al., 2002; Salvetti et al., 2006). Discriminative methods usually perform significantly better. If sufficient labeled data are available, supervised classifiers offer a reasonable performance even without dedicated feature selection. Various linguistic features, such as part-of-speech information, syntactic dependency information and semantic relations have been shown to increase performance of standard bag-of-words feature sets, (Ng et al., 2006; Gamon, 2004). However, Ng et al. (2006) report that the same improvement can be obtained by using higher order n-grams. We omit advanced linguistic features in this work, since, usually, the gain in performance hardly justifies the computational overhead of these methods (Gamon, 2004). There are several domain-independent polarity lexicons containing important polar expressions. The most prominent manual lexicons are General Inquirer2 , the subjectivity lexicon from the MPQA-project (Wilson et al., 2005), and Appraisal Groups (Whitelaw et al., 2005). They have been successfully applied to polarity classif"
W09-4628,H05-1044,0,0.235367,"tic relations have been shown to increase performance of standard bag-of-words feature sets, (Ng et al., 2006; Gamon, 2004). However, Ng et al. (2006) report that the same improvement can be obtained by using higher order n-grams. We omit advanced linguistic features in this work, since, usually, the gain in performance hardly justifies the computational overhead of these methods (Gamon, 2004). There are several domain-independent polarity lexicons containing important polar expressions. The most prominent manual lexicons are General Inquirer2 , the subjectivity lexicon from the MPQA-project (Wilson et al., 2005), and Appraisal Groups (Whitelaw et al., 2005). They have been successfully applied to polarity classification (Kennedy and Inkpen, 2005; Wilson et al., 2005; Whitelaw et al., 2005). Moreover, several methods have been proposed to automatically induce polarity lexicons. Turney (2002) applies Pointwise Mutual Information in order to find similar words to a given list of polar seed words on web data. The polarity scores which are thus computed for each word can be used for a completely unsupervised classification algorithm of documents. A document is assigned the polarity derived from the averag"
W10-3111,D09-1020,0,0.0238775,"inguistic phenomenon. In this section, we present the limits of negation modeling in sentiment analysis. Earlier in this paper, we stated that negation modeling depends on the knowledge of polar expressions. However, the recognition of genuine polar expressions is still fairly brittle. Many polar expressions, such as disease are ambiguous, i.e. they have a polar meaning in one context (Sentence 12) but do not have one in another (Sentence 13). 12. He is a disease to every team he has gone to. 13. Early symptoms of the disease are headaches, fevers, cold chills and body pain. In a pilot study (Akkaya et al., 2009), it has already been shown that applying subjectivity word sense disambiguation in addition to the featurebased negation modeling approach of Wilson et al. (2005) results in an improvement of performance in polarity classification. Another problem is that some polar opinions are not lexicalized. Sentence 14 is a negative pragmatic opinion (Somasundaran and Wiebe, 2009) which can only be detected with the help of external world knowledge. 67 14. The next time I hear this song on the radio, I’ll throw my radio out of the window. Moreover, the effectiveness of specific negation models can only b"
W10-3111,D08-1083,0,0.59404,"a more abstract level of representation being verb frames. The advantage of a more abstract level of representation is that it more accurately represents the meaning of the text it describes. Apart from that, Shaikh et al. (2007) design a model for sentence-level classification rather than for headlines or complex noun phrases. The approach by Moilanen and Pulman (2007) is not compared against another established classification method whereas the approach by Shaikh et al. (2007) is evaluated against a non-compositional rule-based system which it outperforms. 3.3.2 Shallow Semantic Composition Choi and Cardie (2008) present a more lightweight approach using compositional semantics towards classifying the polarity of expressions. Their working assumption is that the polarity of a phrase can be computed in two steps: An example rule, such as: P olarity([NP1]− [IN] [NP2]− ) = + (3) may be applied to expressions, such as − [lack]− NP1 [of]IN [crime]NP2 in rural areas. The advantage of these rules is that they restrict the scope of negation to specific constituents rather than using the scope of the entire target expression. Such inference rules are very reminiscent of polarity modification features (Wilson e"
W10-3111,R09-1034,0,0.0161424,"outperforms the (plain) rule-based method. 3.3.3 Scope Modeling In sentiment analysis, the most prominent work examining the impact of different scope models for negation is (Jia et al., 2009). The scope detection method that is proposed considers: • static delimiters • dynamic delimiters • heuristic rules focused on polar expressions Static delimiters are unambiguous words, such as because or unless marking the beginning of another clause. Dynamic delimiters are, however, 4 It is probably due to the latter, that these rules have been successfully re-used in subsequent works, most prominently Klenner et al. (2009). • the assessment of polarity of the constituents 64 ambiguous, e.g. like and for, and require disambiguation rules, using contextual information such as their pertaining part-of-speech tag. These delimiters suitably account for various complex sentence types so that only the clause containing the negation is considered. The heuristic rules focus on cases in which polar expressions in specific syntactic configurations are directly preceded by negation words which results in the polar expression becoming a delimiter itself. Unlike Choi and Cardie (2008), these rules require a proper parse and"
W10-3111,D09-1131,0,0.0065067,"lexicalized, such as flaw-less, and are consequently to be found a polarity lexicon, this phenomenon does not need to be accounted for in sentiment analysis. However, since this process is (at least theoretically) productive, fairly uncommon words, such as not-so-nice, anti-war or offensiveless which are not necessarily contained in lexical resources, may emerge as a result of this process. Therefore, a polarity classifier should also be able to decompose words and carry out negation modeling within words. There are only few works addressing this particular aspect (Moilanen and Pulman, 2008; Ku et al., 2009) so it is not clear how much impact this type of negation has on an overall polarity classification and what complexity of morphological analysis is really necessary. We argue, however, that in synthetic languages where negation may regularly be realized as an affix rather than an individual word, such an analysis is much more important. • the entire sentence 3.5 Negation in Various Languages • a simple negation scope using a fixed window size (similar to the negation feature in (Wilson et al., 2005)) The proposed method consistently outperforms the simpler methods proving that the incorporati"
W10-3111,D09-1017,0,0.0443385,"ruction. 10. Peter mag den Kuchen nicht. Peter likes the cake not. ‘Peter does not like the cake.’ 11. Der Kuchen ist nicht k¨ostlich. The cake is not delicious. ‘The cake is not delicious.’ These items show that, clearly, some more extensive cross-lingual examination is required in order to be able to make statements of the general applicability of specific negation models. 3.6 Bad and Not Good are Not the Same The standard approach of negation modeling suggests to consider a negated polar expression, such as not bad, as an unnegated polar expression with the opposite polarity, such as good. Liu and Seneff (2009) claim, however, that this is an oversimplification of language. Not bad and good may have the same polarity but they differ in their respective polar strength, i.e. not bad is less positive than good. That is why, Liu and Seneff (2009) suggest a compositional model in which for individual adjectives and adverbs (the latter include negations) a prior rating score encoding their intensity and polarity is estimated from pros and cons of on-line reviews. Moreover, compositional rules for polar phrases, such as adverb-adjective or negation-adverb-adjective are defined exclusively using the scores"
W10-3111,W09-1105,0,0.0426758,"hod consistently outperforms the simpler methods proving that the incorporation of linguistic insights into negation modeling is meaningful. Even on polarity document retrieval, i.e. a more coarse-grained classification task where contextual disambiguation usually results in a less significant improvement, the proposed method also outperforms the other scopes examined. There have only been few research efforts in sentiment analysis examining the impact of scope modeling for negation in contrast to other research areas, such as the biomedical domain (Huang and Lowe, 2007; Morante et al., 2008; Morante and Daelemans, 2009). This is presumably due to the fact that only for the biomedical domain, publicly available corpora containing annotation for the scope of negation exist (Szarvas et al., 2008). The 65 Current research in sentiment analysis mainly focuses on English texts. Since there are significant structural differences among the different languages, some particular methods may only capture the idiosyncratic properties of the English language. This may also affect negation modeling. The previous section already stated that the need for morphological analyses may differ across the different languages. Moreo"
W10-3111,D08-1075,0,0.0762849,"005)) The proposed method consistently outperforms the simpler methods proving that the incorporation of linguistic insights into negation modeling is meaningful. Even on polarity document retrieval, i.e. a more coarse-grained classification task where contextual disambiguation usually results in a less significant improvement, the proposed method also outperforms the other scopes examined. There have only been few research efforts in sentiment analysis examining the impact of scope modeling for negation in contrast to other research areas, such as the biomedical domain (Huang and Lowe, 2007; Morante et al., 2008; Morante and Daelemans, 2009). This is presumably due to the fact that only for the biomedical domain, publicly available corpora containing annotation for the scope of negation exist (Szarvas et al., 2008). The 65 Current research in sentiment analysis mainly focuses on English texts. Since there are significant structural differences among the different languages, some particular methods may only capture the idiosyncratic properties of the English language. This may also affect negation modeling. The previous section already stated that the need for morphological analyses may differ across"
W10-3111,P06-2079,0,0.0191036,"ion, e.g. it does not matter whether a classifier cannot model a negation if the text to be classified contains twenty polar opinions and only one or two contain a negation. Another advantage of these machine learning approaches on coarsegrained classification is their usage of higher order n-grams. Imagine a labeled training set of documents contains frequent bigrams, such as not appealing or less entertaining. Then a feature set using higher order n-grams implicitly contains negation modeling. This also partially explains the effectiveness of bigrams and trigrams for this task as stated in (Ng et al., 2006). The dataset used for the experiments in (Pang et al., 2002; Ng et al., 2006) has been established as a popular benchmark dataset for sentiment analysis and is publicly available1 . 3.2 Incorporating Negation in Models that Include Knowledge of Polar Expressions - Early Works The previous subsection suggested that appropriate negation modeling for sentiment analysis requires the awareness of polar expressions. One way of obtaining such expressions is by using a 1 http://www.cs.cornell.edu/people/ pabo/movie-review-data 62 polarity lexicon which contains a list of polar expressions and for eac"
W10-3111,P04-1035,0,0.019708,"sk dealing with the automatic detection and classification of opinions expressed in text written in natural language. Subjectivity is defined as the linguistic expression of somebody’s opinions, sentiments, emotions, evaluations, beliefs and speculations (Wiebe, 1994). Subjectivity is opposed to objectivity, which is the expression of facts. It is important to make the distinction between subjectivity detection and sentiment analysis, as they are two separate tasks in natural language processing. Sentiment analysis can be dependently or independently done from subjectivity detection, although Pang and Lee (2004) state that subjectivity detection performed prior to the sentiment analysis leads to better results in the latter. Although research in this area has started only recently, the substantial growth in subjective information on the world wide web in the past years has made sentiment analysis a task on which constantly growing efforts have been concentrated. The body of research published on sentiment analysis has shown that the task is difficult, not only due to the syntactic and semantic variability of language, but also because it involves the extraction of indirect or implicit assessments of"
W10-3111,W02-1011,0,0.0170526,"m these examples, modeling negation is a difficult yet important aspect of sentiment analysis. 3 The Survey In this survey, we focus on work that has presented novel aspects for negation modeling in sentiment analysis and we describe them chronologically. 3.1 Negation and Bag of Words in Supervised Machine Learning Several research efforts in polarity classification employ supervised machine-learning algorithms, like Support Vector Machines, Na¨ıve Bayes Classifiers or Maximum Entropy Classifiers. For these algorithms, already a low-level representation using bag of words is fairly effective (Pang et al., 2002). Using a bag-of-words representation, the supervised classifier has to figure out by itself which words in the dataset, or more precisely feature set, are polar and which are not. One either considers all words occurring in a dataset or, as in the case of Pang et al. (2002), one carries out a simple feature selection, such as removing infrequent words. Thus, the standard bag-of-words representation does not contain any explicit knowledge of polar expressions. As a consequence of this simple level of representation, the reversal of the polarity type of polar expressions as it is caused by a ne"
W10-3111,P09-1026,0,0.00780997,"r meaning in one context (Sentence 12) but do not have one in another (Sentence 13). 12. He is a disease to every team he has gone to. 13. Early symptoms of the disease are headaches, fevers, cold chills and body pain. In a pilot study (Akkaya et al., 2009), it has already been shown that applying subjectivity word sense disambiguation in addition to the featurebased negation modeling approach of Wilson et al. (2005) results in an improvement of performance in polarity classification. Another problem is that some polar opinions are not lexicalized. Sentence 14 is a negative pragmatic opinion (Somasundaran and Wiebe, 2009) which can only be detected with the help of external world knowledge. 67 14. The next time I hear this song on the radio, I’ll throw my radio out of the window. Moreover, the effectiveness of specific negation models can only be proven with the help of corpora containing those constructions or the type of language behaviour that is reflected in the models to be evaluated. This presumably explains why rare constructions, such as negations using connectives (Sentence 6 in §2), modals (Sentence 7 in §2) or other phenomena presented in the conceptual model of Polanyi and Zaenen (2004), have not y"
W10-3111,W08-0606,0,0.042933,"more coarse-grained classification task where contextual disambiguation usually results in a less significant improvement, the proposed method also outperforms the other scopes examined. There have only been few research efforts in sentiment analysis examining the impact of scope modeling for negation in contrast to other research areas, such as the biomedical domain (Huang and Lowe, 2007; Morante et al., 2008; Morante and Daelemans, 2009). This is presumably due to the fact that only for the biomedical domain, publicly available corpora containing annotation for the scope of negation exist (Szarvas et al., 2008). The 65 Current research in sentiment analysis mainly focuses on English texts. Since there are significant structural differences among the different languages, some particular methods may only capture the idiosyncratic properties of the English language. This may also affect negation modeling. The previous section already stated that the need for morphological analyses may differ across the different languages. Moreover, the complexity of scope modeling may also be language dependent. In English, for example, modeling the scope of a negation as a fixed window size of words following the occ"
W10-3111,J94-2004,0,0.0714206,"arious computational approaches modeling negation in sentiment analysis. We will, in particular, focus on aspects, such as level of representation used for sentiment analysis, negation word detection and scope of negation. We will also discuss limits and challenges of negation modeling on that task. 1 Introduction Sentiment analysis is the task dealing with the automatic detection and classification of opinions expressed in text written in natural language. Subjectivity is defined as the linguistic expression of somebody’s opinions, sentiments, emotions, evaluations, beliefs and speculations (Wiebe, 1994). Subjectivity is opposed to objectivity, which is the expression of facts. It is important to make the distinction between subjectivity detection and sentiment analysis, as they are two separate tasks in natural language processing. Sentiment analysis can be dependently or independently done from subjectivity detection, although Pang and Lee (2004) state that subjectivity detection performed prior to the sentiment analysis leads to better results in the latter. Although research in this area has started only recently, the substantial growth in subjective information on the world wide web in t"
W10-3111,H05-1044,0,0.619923,"on is thought to be negated if the negation word immediately precedes it. In an extension of this work (Kennedy and Inkpen, 2006) a parser is considered for scope computation. Unfortunately, no precise description of how the parse is used for scope modeling is given in that work. Neither is there a comparison of these two scope models measuring their respective impacts. Final results show that modeling negation is important and relevant, even in the case of such simple methods. The consideration of negation words is more important than that of diminishers. 3.2.2 Features for Negation Modeling Wilson et al. (2005) carry out more advanced negation modeling on expression-level polarity classification. The work uses supervised machine learning where negation modeling is mostly encoded as features using polar expressions. The features for negation modeling are organized in three groups: • negation features • shifter features • polarity modification features Negation features directly relate to negation expressions negating a polar expression. One feature checks whether a negation expression occurs in a fixed window of four words preceding the polar expression. The other feature accounts for a polar predica"
W10-3111,C08-1135,0,0.0112467,"and negation words. Its advantage is that polarity is treated compositionally and is interpreted as a continuum rather than a binary classification. This approach reflects its meaning in a more suitable manner. 3.7 Using Negations in Lexicon Induction Many classification approaches illustrated above depend on the knowledge of which natural lan66 guage expressions are polar. The process of acquiring such lexical resources is called lexicon induction. The observation that negations co-occur with polar expressions has been used for inducing polarity lexicons on Chinese in an unsupervised manner (Zagibalov and Carroll, 2008). One advantage of negation is that though the induction starts with just positive polar seeds, the method also accomplishes to extract negative polar expressions since negated mentions of the positive polar seeds co-occur with negative polar expressions. Moreover, and more importantly, the distribution of the co-occurrence between polar expressions and negations can be exploited for the selection of those seed lexical items. The model presented by Zagibalov and Carroll (2008) relies on the observation that a polar expression can be negated but it occurs more frequently without the negation. T"
W10-3111,J09-3003,0,\N,Missing
W10-3111,P08-2028,0,\N,Missing
W11-4004,H05-1045,0,0.793315,"n a corpus annotated with opinion holders. Our insights are, in particular, important for situations in which no labeled training data are available and only rule-based methods can be applied. 1 Introduction One of the most important tasks in sentiment analysis is opinion holder extraction in which the entities uttering an opinion, also known as opinion holders, need to be extracted from a natural language text. For example, the opinion holders in (1) and (2) are the vet and Russia, respectively. 2 Related Work There has been much research on supervised learning for opinion holder extraction. Choi et al. (2005) examine opinion holder extraction using CRFs with several manually defined linguistic features and automatically learnt surface patterns. Bethard et al. (2004) and Kim and Hovy (2006) explore the usefulness of semantic roles provided by FrameNet (Fillmore et al., 2003) for both opinion holder and opinion target extraction. The approaches of those two papers have mostly been evaluated on some artificial data sets. More recently, Wiegand and Klakow (2010) explored convolution kernels for opinion holder extraction. Rule-based opinion holder extraction heavily relies on lexical cues. Bloom et al."
W11-4004,P05-1045,0,0.03537,"e 10 most frequent opinion holder predicates. already provides a comparatively high recall. The clue PERSON checks whether the candidate opinion holder is a person. For some ambiguous predicates, such as critical, this would allow a correct disambiguation, i.e. Dr. Ren in (4) would be classified as an opinion holder while the crossstrait balance of military power in (5) would not. 4. Dr. Ren was critical of the government’s decision. 5. In his view, the cross-strait balance of military power is critical to the ROC’s national security. For this clue, we employ Stanford named-entity recognizer (Finkel et al., 2005) for detecting proper nouns and WordNet for recognizing common nouns denoting persons. The second clue SUBJ detects subjective evidence in a sentence. The heuristics applied should filter false positives, such as (6). 6. “We do not have special devices for inspecting large automobiles and cargoes”, Nazarov said. If an opinion holder has been found according to our standard procedure using opinion holder predicates, some additional property must hold so that the classifier predicts an opinion holder. Either the candidate opinion holder phrase contains a subjective expression (7), some subjectiv"
W11-4004,P03-1054,0,0.0198252,"eover, we also carry out a quantitative evaluation of those related phenomena. Unlike Ruppenhofer et al. (2008), we thus try to identify the most immediate problems of this task. By also considering resources in order to solve these problems we hope to be a helpful guide for practitioners building an opinion holder extraction system from scratch. 4.2 The Different Types of Grammatical Relations Table 2 shows the distribution of the most frequent grammatical relations between opinion holder and its related predicate listed separately for each unigram predicate type. We use the Stanford parser (Klein and Manning, 2003) for obtaining all syntactic information. The table displays the percentage of that grammatical relation within the particular predicate type when it is observed as a predicate of an opinion holder in our labeled data set (Perc.)3 , the property of being a fairly reliable relation for a semantic agent (Agent), and the precision of that grammatical relation in conjunction with that opinion holder predicate type for detecting opinion holders (Precision). As a goldstandard of opinion holder predicates we extracted all unigram predicates from our data set that cooccur at least twice with an actual"
W11-4004,P10-1059,0,0.0606293,"is marked as such for the other parts of speech. We found that subjects of predicate nouns can very often be found in constructions like (3). Clearly, this is not an agent of idea. 3 Data As a labeled (test) corpus, we use the MPQA 2.0 corpus1 which is a large text corpus containing fine-grained sentiment annotation. It (mainly) consists of news texts which can be considered as a primary domain for opinion holder extraction. Other popular domains for sentiment analysis, for example, product reviews contain much fewer opinion holders according to the pertaining data sets (Kessler et al., 2010; Toprak et al., 2010). Opinions uttered in those texts usually express the author’s point of view. Therefore, the extraction of sources of opinions is of minor importance. We use the definition of opinion holders as described in (Wiegand and Klakow, 2010), i.e. every source of a private state or a subjective speech event (Wiebe et al., 2003) is considered an opinion holder. This is a very strict definition and the scores produced in this work can only be put into relation to the numbers presented in (Wiegand and Klakow, 2010). The final corpus comprises approximately 11,000 sentences with more than 6,200 opinion h"
W11-4004,N10-1121,1,0.909956,"(1) and (2) are the vet and Russia, respectively. 2 Related Work There has been much research on supervised learning for opinion holder extraction. Choi et al. (2005) examine opinion holder extraction using CRFs with several manually defined linguistic features and automatically learnt surface patterns. Bethard et al. (2004) and Kim and Hovy (2006) explore the usefulness of semantic roles provided by FrameNet (Fillmore et al., 2003) for both opinion holder and opinion target extraction. The approaches of those two papers have mostly been evaluated on some artificial data sets. More recently, Wiegand and Klakow (2010) explored convolution kernels for opinion holder extraction. Rule-based opinion holder extraction heavily relies on lexical cues. Bloom et al. (2007) use a list of manually compiled communication verbs and 1. The owner put down the animal, although the vet had forbidden him to do so. 2. Russia favors creation of “international instruments” to regulate emissions. As this is an entity extraction problem it can be considered as a typical task in information extraction. Though there is much work on that subject, most work focuses on data-driven methods. Thus, to a great extent it fails to fully de"
W11-4004,H05-1044,0,0.391413,"b.motion). We consider the files noun.cognition, noun.communication, verb.cognition and verb.communication. Due to the coarse-grained nature of the WN-LF, the resulting set of words contains 10151 words (7684 nouns and 2467 verbs). Table 5 summarizes the properties of the different resources. Due to the high number of nouns in WN-LF, we will evaluate this lexicon both with and without nouns. For all resources only containing verbs, we also use Nomlex (Macleod et al., 1998) to find corresponding noun predicates, e.g. Subjectivity Lexicon (SL) The Subjectivity Lexicon (SL) from the MPQAproject (Wilson et al., 2005) is one of the most commonly used sentiment lexicons. The lexicon contains 8222 subjective expressions from different parts of speech. For our experiments we will only consider its verbs, nouns and adjectives. This lexicon has been used for various subtasks in sentiment analysis, primarily subjectivity detection and polarity classification (Wilson et al., 2005). It has also been used for opinion holder extraction (Choi et al., 2005; Wiegand and Klakow, 2010) though the lexicon does not contain any annotation specifically designed for this task which is why each entry is considered some clue fo"
W11-4635,J92-4003,0,0.0710569,"d Palmer, 2002). Semantic roles are obtained by using the parser by Zhang et al. (2008). A special property of PAS is that a data instance, i.e. the information regarding one target word and its particular context, is represented by a set of those structures rather than a single structure. Thus, the actual partial tree kernel function we use for this task, P T K, sums over all possible pairs P ASl and P ASm of two xi and xj : P T K(xi , xj ) = Xdata instances X P T Kbasic (P ASl , P ASm ). An P ASl ∈xi P ASm ∈xj illustration of these substructures is given in Figure 1(c). (a) CON (b) DEP ing (Brown et al., 1992) which is the best performing algorithm in (Turian et al., 2010). This algorithm induces clusters with the help of cooccurrence statistics of bigrams. We augment our structures with the clustering information. We add the node with a cluster label in such a way that it directly dominates the pertaining lexical node. As a software we use SRILM (Stolcke, 2002) with the default algorithm. The clusters are induced on the North American News Text Corpus (LDC95T21). We chose this corpus as it contains news texts similar to our evaluation corpus (i.e. MPQA). Following Turian et al. (2010), we induced"
W11-4635,P05-1045,0,0.00996647,"chose this corpus as it contains news texts similar to our evaluation corpus (i.e. MPQA). Following Turian et al. (2010), we induced 1000 clusters. As many names of persons and organizations can be very domain-specific, they may not appear in the corpus from which clusters are induced. Consequently, these expressions cannot be assigned to a cluster. We try to compensate this by incorporating the knowledge about named entities in tree kernels, i.e. instead of assigning some expression to a cluster we assign it to a named entity type. Named-entity information is obtained by the Stanford tagger (Finkel et al., 2005). 4.4 The Different Settings (c) PAS Figure 1: Illustration of the different tree structures employed for convolution kernels derived from Sentence 3 with reactions as the target word. 4.3.4 Augmentation with Clustering A common type of unsupervised generalization is clustering. Words which co-occur with each other are automatically grouped into clusters. Ideally, a cluster thus contains words with similar syntactic/semantic properties. The cluster membership of individual words is induced from a large unlabeled corpus. As the context windows of our target expressions contain fairly sparse lex"
W11-4635,D10-1101,0,0.0264447,"rser (Klein and Manning, 2003) for obtaining constituency parse trees. 4.3.2 Dependency Parse Structures (DEP) Apart from manually designed features, Johansson and Moschitti (2010) also test a tree kernel for subjectivity detection using a dependency parse. However, the entire parse comprising a sentence is considered. The resulting tree kernel does not show any significant improvement (again presumably because of the large amount of irrelevant information). The usefulness of particular (usually direct) relations, however, has been found effective on other related tasks in sentiment analysis (Jakob and Gurevych, 2010; Wiegand and Klakow, 2010). We therefore only consider the subtree exclusively containing the lexical units that are connected to the target word by a direct syntactic dependency relationship (i.e. direct parent and direct children). The precise encoding of the pertaining information (i.e. part-ofspeech, grammatical relation, and lexical information) in the resulting tree is taken from (Johansson and Moschitti, 2010). An illustration of this substructure is given in Figure 1(b). Again, we use the Stanford Parser (Klein and Manning, 2003) for obtaining dependency parse trees. 4.3.3 Predicate-A"
W11-4635,W10-2910,0,0.25335,"lder extraction (Wiegand and Klakow, 2010). There is no general agreement as to whether linguistic information is useful for text classification tasks in sentiment analysis (which next to subjectivity detection also comprises polarity classification1 ). For supervised document-level analysis, traditional word-level features (i.e. bag of words/n-grams) are usually sufficient (Ng et al., 2006). The usage of more expressive features has been found more effective on fine-grained sentiment analysis, in particular, the classification at word/phrase level (Wilson et al., 2005; Karlgren et al., 2010; Johansson and Moschitti, 2010). The features used in those works have been manually designed and comprise various levels of representation, such as grammatical relations or predicateargument structures. Johansson and Moschitti (2010) also use a tree kernel encoding dependency parse trees, however, there is no significant improvement achieved by that structure. In this work, we not only consider dependency parse trees for convolution kernels but also other linguistic levels of representation. Moreover, we also consider appropriate substructures rather than the structures derived from an entire sentence. The latter approach"
W11-4635,W06-0301,0,0.0202091,"arent and direct children). The precise encoding of the pertaining information (i.e. part-ofspeech, grammatical relation, and lexical information) in the resulting tree is taken from (Johansson and Moschitti, 2010). An illustration of this substructure is given in Figure 1(b). Again, we use the Stanford Parser (Klein and Manning, 2003) for obtaining dependency parse trees. 4.3.3 Predicate-Argument Structures (PAS) Predicate-argument structures (PAS), in particular, semantic role labeling has been shown to be effective for many information extraction tasks, including opinion holder extraction (Kim and Hovy, 2006; Wiegand and Klakow, 2010) and opinion target extraction (Kim and Hovy, 2006). Johansson and Moschitti (2010) also examine this level of representation for subjectivity detection. However, they employ manual features derived from these structures rather than using a corresponding tree kernel. We follow Wiegand and Klakow (2010) for the encoding of these structures as tree kernels, that is we restrict ourselves to structures in which the target word is either a predicate or some argument. We derive our predicate-argument structures from a semantic parse based on the PropBank annotation scheme"
W11-4635,kingsbury-palmer-2002-treebank,0,0.0169191,"Wiegand and Klakow, 2010) and opinion target extraction (Kim and Hovy, 2006). Johansson and Moschitti (2010) also examine this level of representation for subjectivity detection. However, they employ manual features derived from these structures rather than using a corresponding tree kernel. We follow Wiegand and Klakow (2010) for the encoding of these structures as tree kernels, that is we restrict ourselves to structures in which the target word is either a predicate or some argument. We derive our predicate-argument structures from a semantic parse based on the PropBank annotation scheme (Kingsbury and Palmer, 2002). Semantic roles are obtained by using the parser by Zhang et al. (2008). A special property of PAS is that a data instance, i.e. the information regarding one target word and its particular context, is represented by a set of those structures rather than a single structure. Thus, the actual partial tree kernel function we use for this task, P T K, sums over all possible pairs P ASl and P ASm of two xi and xj : P T K(xi , xj ) = Xdata instances X P T Kbasic (P ASl , P ASm ). An P ASl ∈xi P ASm ∈xj illustration of these substructures is given in Figure 1(c). (a) CON (b) DEP ing (Brown et al., 1"
W11-4635,P03-1054,0,0.00555918,"ived from scopes. The best performing subtree is the tree with the predicate scope, i.e. a subtree with the boundaries being the candidate or target expression and the nearest predicate. We also assume that this structure is meaningful for our task. As already discovered in previous work on the detection of subjective expressions (Riloff and Wiebe, 2003; Riloff and Wiebe, 2003; Wilson et al., 2005), discriminant patterns often encode a relation between the target expression and the nearest predicate. An illustration of this substructure is given in Figure 1(a). We use the Stanford 256 Parser (Klein and Manning, 2003) for obtaining constituency parse trees. 4.3.2 Dependency Parse Structures (DEP) Apart from manually designed features, Johansson and Moschitti (2010) also test a tree kernel for subjectivity detection using a dependency parse. However, the entire parse comprising a sentence is considered. The resulting tree kernel does not show any significant improvement (again presumably because of the large amount of irrelevant information). The usefulness of particular (usually direct) relations, however, has been found effective on other related tasks in sentiment analysis (Jakob and Gurevych, 2010; Wieg"
W11-4635,J08-2003,0,0.0162426,"rder to generalize from lexical information, we additionally augment these structures with clustering information and the task-specific knowledge of Bolette Sandford Pedersen, Gunta Neˇspore and Inguna Skadin¸a (Eds.) NODALIDA 2011 Conference Proceedings, pp. 254–261 subjective words. The convolution kernels will be compared with a standard vector kernel. 2 Related Work Convolution kernels have been shown to be effective in various tasks in natural language processing, ranging from relation extraction (Bunescu and Mooney, 2005; Zhang et al., 2006; Nguyen et al., 2009), semantic role labeling (Moschitti et al., 2008) to question answering (Zhang and Lee, 2003; Moschitti, 2008). In sentiment analysis, this method has been successfully applied on opinion holder extraction (Wiegand and Klakow, 2010). There is no general agreement as to whether linguistic information is useful for text classification tasks in sentiment analysis (which next to subjectivity detection also comprises polarity classification1 ). For supervised document-level analysis, traditional word-level features (i.e. bag of words/n-grams) are usually sufficient (Ng et al., 2006). The usage of more expressive features has been found more effec"
W11-4635,P06-2079,0,0.0291324,"al., 2006; Nguyen et al., 2009), semantic role labeling (Moschitti et al., 2008) to question answering (Zhang and Lee, 2003; Moschitti, 2008). In sentiment analysis, this method has been successfully applied on opinion holder extraction (Wiegand and Klakow, 2010). There is no general agreement as to whether linguistic information is useful for text classification tasks in sentiment analysis (which next to subjectivity detection also comprises polarity classification1 ). For supervised document-level analysis, traditional word-level features (i.e. bag of words/n-grams) are usually sufficient (Ng et al., 2006). The usage of more expressive features has been found more effective on fine-grained sentiment analysis, in particular, the classification at word/phrase level (Wilson et al., 2005; Karlgren et al., 2010; Johansson and Moschitti, 2010). The features used in those works have been manually designed and comprise various levels of representation, such as grammatical relations or predicateargument structures. Johansson and Moschitti (2010) also use a tree kernel encoding dependency parse trees, however, there is no significant improvement achieved by that structure. In this work, we not only consi"
W11-4635,D09-1143,0,0.128445,"tures, and predicate-argument structures. In order to generalize from lexical information, we additionally augment these structures with clustering information and the task-specific knowledge of Bolette Sandford Pedersen, Gunta Neˇspore and Inguna Skadin¸a (Eds.) NODALIDA 2011 Conference Proceedings, pp. 254–261 subjective words. The convolution kernels will be compared with a standard vector kernel. 2 Related Work Convolution kernels have been shown to be effective in various tasks in natural language processing, ranging from relation extraction (Bunescu and Mooney, 2005; Zhang et al., 2006; Nguyen et al., 2009), semantic role labeling (Moschitti et al., 2008) to question answering (Zhang and Lee, 2003; Moschitti, 2008). In sentiment analysis, this method has been successfully applied on opinion holder extraction (Wiegand and Klakow, 2010). There is no general agreement as to whether linguistic information is useful for text classification tasks in sentiment analysis (which next to subjectivity detection also comprises polarity classification1 ). For supervised document-level analysis, traditional word-level features (i.e. bag of words/n-grams) are usually sufficient (Ng et al., 2006). The usage of m"
W11-4635,W03-1014,0,0.0694711,"uces very low performing classifiers. Structures derived from an entire sentence contain too much irrelevant information for such a task at expression level. We assume that the same is true for the detection of subjectivity. Wiegand and Klakow (2010) use subtrees derived from scopes. The best performing subtree is the tree with the predicate scope, i.e. a subtree with the boundaries being the candidate or target expression and the nearest predicate. We also assume that this structure is meaningful for our task. As already discovered in previous work on the detection of subjective expressions (Riloff and Wiebe, 2003; Riloff and Wiebe, 2003; Wilson et al., 2005), discriminant patterns often encode a relation between the target expression and the nearest predicate. An illustration of this substructure is given in Figure 1(a). We use the Stanford 256 Parser (Klein and Manning, 2003) for obtaining constituency parse trees. 4.3.2 Dependency Parse Structures (DEP) Apart from manually designed features, Johansson and Moschitti (2010) also test a tree kernel for subjectivity detection using a dependency parse. However, the entire parse comprising a sentence is considered. The resulting tree kernel does not show"
W11-4635,P10-1040,0,0.151711,"r by Zhang et al. (2008). A special property of PAS is that a data instance, i.e. the information regarding one target word and its particular context, is represented by a set of those structures rather than a single structure. Thus, the actual partial tree kernel function we use for this task, P T K, sums over all possible pairs P ASl and P ASm of two xi and xj : P T K(xi , xj ) = Xdata instances X P T Kbasic (P ASl , P ASm ). An P ASl ∈xi P ASm ∈xj illustration of these substructures is given in Figure 1(c). (a) CON (b) DEP ing (Brown et al., 1992) which is the best performing algorithm in (Turian et al., 2010). This algorithm induces clusters with the help of cooccurrence statistics of bigrams. We augment our structures with the clustering information. We add the node with a cluster label in such a way that it directly dominates the pertaining lexical node. As a software we use SRILM (Stolcke, 2002) with the default algorithm. The clusters are induced on the North American News Text Corpus (LDC95T21). We chose this corpus as it contains news texts similar to our evaluation corpus (i.e. MPQA). Following Turian et al. (2010), we induced 1000 clusters. As many names of persons and organizations can be"
W11-4635,N10-1121,1,0.319936,"Neˇspore and Inguna Skadin¸a (Eds.) NODALIDA 2011 Conference Proceedings, pp. 254–261 subjective words. The convolution kernels will be compared with a standard vector kernel. 2 Related Work Convolution kernels have been shown to be effective in various tasks in natural language processing, ranging from relation extraction (Bunescu and Mooney, 2005; Zhang et al., 2006; Nguyen et al., 2009), semantic role labeling (Moschitti et al., 2008) to question answering (Zhang and Lee, 2003; Moschitti, 2008). In sentiment analysis, this method has been successfully applied on opinion holder extraction (Wiegand and Klakow, 2010). There is no general agreement as to whether linguistic information is useful for text classification tasks in sentiment analysis (which next to subjectivity detection also comprises polarity classification1 ). For supervised document-level analysis, traditional word-level features (i.e. bag of words/n-grams) are usually sufficient (Ng et al., 2006). The usage of more expressive features has been found more effective on fine-grained sentiment analysis, in particular, the classification at word/phrase level (Wilson et al., 2005; Karlgren et al., 2010; Johansson and Moschitti, 2010). The featur"
W11-4635,H05-1044,0,0.51944,"has been successfully applied on opinion holder extraction (Wiegand and Klakow, 2010). There is no general agreement as to whether linguistic information is useful for text classification tasks in sentiment analysis (which next to subjectivity detection also comprises polarity classification1 ). For supervised document-level analysis, traditional word-level features (i.e. bag of words/n-grams) are usually sufficient (Ng et al., 2006). The usage of more expressive features has been found more effective on fine-grained sentiment analysis, in particular, the classification at word/phrase level (Wilson et al., 2005; Karlgren et al., 2010; Johansson and Moschitti, 2010). The features used in those works have been manually designed and comprise various levels of representation, such as grammatical relations or predicateargument structures. Johansson and Moschitti (2010) also use a tree kernel encoding dependency parse trees, however, there is no significant improvement achieved by that structure. In this work, we not only consider dependency parse trees for convolution kernels but also other linguistic levels of representation. Moreover, we also consider appropriate substructures rather than the structure"
W11-4635,N06-1037,0,0.157945,"pendency parse structures, and predicate-argument structures. In order to generalize from lexical information, we additionally augment these structures with clustering information and the task-specific knowledge of Bolette Sandford Pedersen, Gunta Neˇspore and Inguna Skadin¸a (Eds.) NODALIDA 2011 Conference Proceedings, pp. 254–261 subjective words. The convolution kernels will be compared with a standard vector kernel. 2 Related Work Convolution kernels have been shown to be effective in various tasks in natural language processing, ranging from relation extraction (Bunescu and Mooney, 2005; Zhang et al., 2006; Nguyen et al., 2009), semantic role labeling (Moschitti et al., 2008) to question answering (Zhang and Lee, 2003; Moschitti, 2008). In sentiment analysis, this method has been successfully applied on opinion holder extraction (Wiegand and Klakow, 2010). There is no general agreement as to whether linguistic information is useful for text classification tasks in sentiment analysis (which next to subjectivity detection also comprises polarity classification1 ). For supervised document-level analysis, traditional word-level features (i.e. bag of words/n-grams) are usually sufficient (Ng et al.,"
W13-1108,D09-1020,0,0.0125799,"mbination (bow+task) 55 50 Table 10: Comparison of different feature sets (summary of features is displayed in Table 5); ∗ significantly better than bow at p &lt; 0.05 (based on paired t-test). F-score 45 40 35 30 fact the different feature types have different properties. On the one hand, there are unambiguous feature types, such as AUTH, which work fine with a wide scope. But we also have ambiguous feature types that require a fairly narrow context. A typical example are strong (positive) polar expressions (STROPO+ ). (Polar expressions are known to be very ambiguous (Wiebe and Mihalcea, 2006; Akkaya et al., 2009).) 5.2 Classification Table 10 compares the different feature sets with regard to extraction performance. We carry out a 5-fold cross-validation on our manually labeled dataset. As a classifier, we chose Support Vector Machines (Joachims, 1999). As a toolkit, we use SVMLight4 with a linear kernel. Table 10 clearly shows the strength of the highlevel features that we proposed. They do not only represent a strong feature set on their own but they can also usefully be combined with bag-of-words features. Apparently, neither part-of-speech nor parse information are predictive for this task. 5.3 Im"
W13-1108,W97-0802,0,0.132084,"rd features, such as bag of words. 3 Data & Annotation As a corpus for our experiments, we used a crawl of chefkoch.de1 (Wiegand et al., 2012a) consisting of 418, 558 webpages of food-related forum entries. chefkoch.de is the largest web portal for food-related issues in the German language. From this dataset, sentences in which some food item co-occurred with some health condition (e.g. pregnancy, diarrhoea or flu) were extracted. (In the following, we will also refer to these entities as target food item and target health condition.) The food items were identified with the help of GermaNet (Hamp and Feldweg, 1997), the German version of WordNet (Miller et al., 1990), and the health conditions were used 1 www.chefkoch.de 70 from Wiegand et al. (2012b). In total, 2604 sentences were thus obtained. For the manual annotation, each target sentence (i.e. a sentence with a co-occurrence of target food item and health condition) was presented in combination with the two sentences immediately preceding and following it. Each target sentence was manually assigned two labels, one specifying the type of suitability (§3.1) and another specifying whether the relation expressed is considered reliable or not (§3.2). 3"
W13-1108,P97-1023,0,0.238863,"orrible− or he sang terribly− ). We employ two methods to detect these ambiguous expressions. INTENSpolar requires a polar expression of a polarity lexicon to be modified by the intensifier, while INTENSadj requires an adjective to be modified. In order to identify polar expressions we use the polarity lexicon underlying the PolArt system (Klenner et al., 2009). We also consider adjectives since we must assume that our polarity lexicon does not cover all possible polar expressions. We chose adjectives as a complement criterion as this part of speech is known to contain most polar expressions (Hatzivassiloglou and McKeown, 1997; Hatzivassiloglou and Wiebe, 2000). 4.1.4 Strong Polar Expressions (STROPO) Instead of adding intensifiers in order to put more emphasis to a remark (§4.1.3), one may also use polar expressions that convey a high polar intensity (16). For instance, nice and excellent refer to the same scale and convey positive polarity but excellent has a much higher polar intensity than nice. Taboada et al. (2011) introduced an English polarity lexicon SO-CAL in which polar expressions were also assigned an intensity label. As our German polarity lexicon (§4.1.3) does not contain comparable intensity labels,"
W13-1108,C00-1044,0,0.520806,"employ two methods to detect these ambiguous expressions. INTENSpolar requires a polar expression of a polarity lexicon to be modified by the intensifier, while INTENSadj requires an adjective to be modified. In order to identify polar expressions we use the polarity lexicon underlying the PolArt system (Klenner et al., 2009). We also consider adjectives since we must assume that our polarity lexicon does not cover all possible polar expressions. We chose adjectives as a complement criterion as this part of speech is known to contain most polar expressions (Hatzivassiloglou and McKeown, 1997; Hatzivassiloglou and Wiebe, 2000). 4.1.4 Strong Polar Expressions (STROPO) Instead of adding intensifiers in order to put more emphasis to a remark (§4.1.3), one may also use polar expressions that convey a high polar intensity (16). For instance, nice and excellent refer to the same scale and convey positive polarity but excellent has a much higher polar intensity than nice. Taboada et al. (2011) introduced an English polarity lexicon SO-CAL in which polar expressions were also assigned an intensity label. As our German polarity lexicon (§4.1.3) does not contain comparable intensity labels, we used a German translation of SO"
W13-1108,R09-1034,0,0.0202961,"emely). The second group includes more ambiguous expressions, such as adjectives that only function as an intensifier if they modify a polar expression (e.g. horrible pain or terribly nice) otherwise they function as typical polar expressions (e.g. you are horrible− or he sang terribly− ). We employ two methods to detect these ambiguous expressions. INTENSpolar requires a polar expression of a polarity lexicon to be modified by the intensifier, while INTENSadj requires an adjective to be modified. In order to identify polar expressions we use the polarity lexicon underlying the PolArt system (Klenner et al., 2009). We also consider adjectives since we must assume that our polarity lexicon does not cover all possible polar expressions. We chose adjectives as a complement criterion as this part of speech is known to contain most polar expressions (Hatzivassiloglou and McKeown, 1997; Hatzivassiloglou and Wiebe, 2000). 4.1.4 Strong Polar Expressions (STROPO) Instead of adding intensifiers in order to put more emphasis to a remark (§4.1.3), one may also use polar expressions that convey a high polar intensity (16). For instance, nice and excellent refer to the same scale and convey positive polarity but exc"
W13-1108,J93-2004,0,0.0432563,"Missing"
W13-1108,Y12-1010,0,0.0310966,"tion of health relations from social media are concerned, the prediction of epidemics (Fisichella et al., 2011; Torii et al., 2011; Diaz-Aviles et al., 2012; Munro et al., 2012) has recently attracted the attention of the research community. Relation extraction involving food items has also been explored in the context of ontology alignment (van Hage et al., 2005; van Hage et al., 2006; van Hage et al., 2010) and also as a means of knowledge acquisition for virtual customer advice in a supermarket (Wiegand et al., 2012a). The works most closely related to this paper are Yang et al. (2011) and Miao et al. (2012). Both of these works address the extraction of food-health relationships. Unlike this work, they extract relations from scientific biomedical texts rather than social media. Yang et al. (2011) also cover the task of strength analysis which bears some resemblance to the task of finding reliable utterances to some extent. However, the features applied to that classification task are only standard features, such as bag of words. 3 Data & Annotation As a corpus for our experiments, we used a crawl of chefkoch.de1 (Wiegand et al., 2012a) consisting of 418, 558 webpages of food-related forum entrie"
W13-1108,W09-1304,0,0.0810857,"Missing"
W13-1108,W08-1006,0,0.0215284,"e 4 lists all the variants that we use. These variants are applied to all feature types except the types of suitability (§4.1.9) as this label has only been assigned to an entire target sentence. 4.3 Other Features Table 5 lists the entire set of features that we examine in this work. The simplest classifier that we can construct for our task is a trivial classifier that predicts all statements as reliable statements. The remaining features comprise bag of words, part-ofspeech and syntactic parse information. For the latter two features, we employ the output of the Stanford Parser for German (Rafferty and Manning, 2008). 74 Description trivial classifier that always predicts a reliable statement bag-of-words features: all words between the target food item and target health condition and the words immediately preceding and following each of them part-of-speech features: part-of-speech sequence between target food item and health condition and tags of the words immediately preceding and following each of the target expressions path from syntactic parse tree from target food item to target health condition all task-specific high-level feature types from §4.1 with their respective variants (§4.2) Table 5: Descr"
W13-1108,J11-2001,0,0.0191279,"hat our polarity lexicon does not cover all possible polar expressions. We chose adjectives as a complement criterion as this part of speech is known to contain most polar expressions (Hatzivassiloglou and McKeown, 1997; Hatzivassiloglou and Wiebe, 2000). 4.1.4 Strong Polar Expressions (STROPO) Instead of adding intensifiers in order to put more emphasis to a remark (§4.1.3), one may also use polar expressions that convey a high polar intensity (16). For instance, nice and excellent refer to the same scale and convey positive polarity but excellent has a much higher polar intensity than nice. Taboada et al. (2011) introduced an English polarity lexicon SO-CAL in which polar expressions were also assigned an intensity label. As our German polarity lexicon (§4.1.3) does not contain comparable intensity labels, we used a German translation of SO-CAL. We identified polar expressions with a high intensity score (i.e. ±4 or ±5) as strong polar expressions. It includes 221 highly positive and 344 highly negative polar expressions. We also distinguish the polarity type (i.e. STROPO+ refers to positive and STROPO− refers to negative polarity). (16) Baking soda is an excellent remedy against heartburn. 4.1.5 Sup"
W13-1108,P06-1134,0,0.0133646,"h-level features (task) combination (bow+task) 55 50 Table 10: Comparison of different feature sets (summary of features is displayed in Table 5); ∗ significantly better than bow at p &lt; 0.05 (based on paired t-test). F-score 45 40 35 30 fact the different feature types have different properties. On the one hand, there are unambiguous feature types, such as AUTH, which work fine with a wide scope. But we also have ambiguous feature types that require a fairly narrow context. A typical example are strong (positive) polar expressions (STROPO+ ). (Polar expressions are known to be very ambiguous (Wiebe and Mihalcea, 2006; Akkaya et al., 2009).) 5.2 Classification Table 10 compares the different feature sets with regard to extraction performance. We carry out a 5-fold cross-validation on our manually labeled dataset. As a classifier, we chose Support Vector Machines (Joachims, 1999). As a toolkit, we use SVMLight4 with a linear kernel. Table 10 clearly shows the strength of the highlevel features that we proposed. They do not only represent a strong feature set on their own but they can also usefully be combined with bag-of-words features. Apparently, neither part-of-speech nor parse information are predictive"
W13-1108,wiegand-etal-2012-gold,1,0.891497,"Missing"
W13-1108,H05-1044,0,0.014602,"ertain relation very frequently or even at all times, then there is a high likelihood that this relation actually holds (14). We use a set of adverbs (18 expressions) that express high frequency (e.g. often, frequently etc.) or constancy (e.g. always, at all times etc.). (14) What always helps me when I have the flu is a hot chicken broth. 4.1.3 Intensifiers (INTENS) Some utterances may also be perceived reliable if their speaker adds some emphasis to them. One way of doing so is by adding intensifiers to a remark (15). 72 The intensifiers we use are a translation of the lexicon introduced in Wilson et al. (2005). For the detection, we divide that list into two groups: The first group INTENSsimple are unambiguous adverbs that always function as intensifiers no matter in which context they appear (e.g. very or extremely). The second group includes more ambiguous expressions, such as adjectives that only function as an intensifier if they modify a polar expression (e.g. horrible pain or terribly nice) otherwise they function as typical polar expressions (e.g. you are horrible− or he sang terribly− ). We employ two methods to detect these ambiguous expressions. INTENSpolar requires a polar expression of"
W15-2921,baccianella-etal-2010-sentiwordnet,0,0.00662151,"show that, even with the PropBank-like semantic roles (i.e. agent, patient1 ) assigned to the entities, one may not be able to discriminate between the opinion roles. In §2, we demonstrated the need for acquiring more lexical knowledge about opinion verbs for open-domain opinion role extraction. This raises the question whether existing general-purpose resources could be exploited for this purpose. If one considers the plethora of different lexical resources developed for sentiment analysis, i.e. sentiment lexicons listing subjective expressions and their prior polarity (Wilson et al., 2005; Baccianella et al., 2010; Taboada et al., 2011), emotion lexicons (Mohammad and Turney, 2013) or connotation lexicons (Kang et al., 2014), one finds, however, that with respect to opinion role extraction there is a gap. What is missing is a lexicon that states for each opinion verb in which argument position an opinion role can be found. OT (1) [Peter]OH agent dislikes [Mary]patient . OH (2) [Peter]OT agent disappoints [Mary]patient . We assume that it is lexical information that decides in what argument position opinion roles are realized. That is, a verb, such as dislike, believe or applaud, belongs to a group with"
W15-2921,P98-1013,0,0.313186,"Missing"
W15-2921,D14-1125,0,0.0323956,"Missing"
W15-2921,N12-1086,0,0.023781,"Missing"
W15-2921,N10-1138,0,0.0264134,"on verb is presented in (4) and (5) where two viewpoints are evoked by the same verb in the same sentence. (4) denotes the sentiment view of Peter towards Mary while (5) represents the sentiment view of Mary towards Peter (i.e. Peter made Mary feel better). One resource that has previously been examined for this task is FrameNet (Baker et al., 1998). The idea is to identify in frames (which predominantly contain opinion expressions) those frame elements that typically contain either opinion holders or opinion targets. Once this mapping has been established, a FrameNet-parser, such as Semafor (Das et al., 2010), could be used to automatically recognize frame structures in natural language text. By consulting the mapping from frame elements to opinion roles, specific opinion roles could be extracted. Kim and Hovy (2006) followed this approach for a set of opinion verbs and adjectives. Thus, they were able to correctly resolve some problems which cannot be solved with the help of syntactic parsing or PropBank-like semantic roles, such as the role distinctions in (1) and (2). For instance, while the opinion holders in (6) and (7) map to the same frame element EXPERIENCER, the PropBank-like semantic rol"
W15-2921,N15-1146,0,0.0279268,"y focus on the relationship between opinion roles and their syntactic argument realization. Previous work hardly addressed this issue since either little variation between opinion roles and their syntactic arguments was perceived on the corpora on which this task was examined, or there were other domain-specific properties that could be used in order to extract opinion roles correctly without the knowledge about opinion role realization. Currently, there exists only one commonly accepted corpus for English containing manual annotation of both opinion holders and targets, i.e. the MPQA corpus (Deng and Wiebe, 2015). Apart from that, not a single lexical resource for that specific task is available. Moreover, there does not exist any publicly available tool that supports both opinion holder and target extraction. Typical applications, such as opinion summarization, however, require both components simultaneously (Stoyanov and Cardie, 2011). These facts indicate that there definitely needs to be more research on the task of opinion role extraction. In order to stimulate more research in this direction, we present a verb-based corpus for opinion role extraction. The difference to previous datasets is that"
W15-2921,erk-pado-2004-powerful,0,0.102974,"Missing"
W15-2921,esuli-sebastiani-2006-sentiwordnet,0,0.0165852,"ders and targets with the help of the WordNet ontology graph. One common way of doing so would be the application of some bootstrapping method in which one defines seed opinion verbs with distinct selectional preferences (for instance, one defines as one group opinion verbs that take agents as opinion holders, such as dislike, as another group verbs that take patients as opinion holders, such as disappoint, and so on) and propagate their labels to the remaining opinion verbs via the WordNet graph. Such bootstrapping on WordNet has been effectively used for the induction of sentiment lexicons (Esuli and Sebastiani, 2006; Rao and Ravichandran, 2009) or effect predicates (Choi and Wiebe, 2014). It relies on a good similarity metric in order to propagate the labels from labeled seed words to unlabeled words. We experimented with the metrics in WordNet::Similarity (Pedersen et al., 2004) and found that the opinion verbs most similar to a specified opinion verb do not necessarily share the same syntactic properties. For example, Table 2 lists the 12 opinion verbs most similar to outrage and please, which are typical opinion verbs that take an opinion holder in patient position and an opinion target in agent posit"
W15-2921,D10-1101,0,0.0326016,"at the problem of opinion role extraction can be appropriately evaluated on them. We start by looking at the review domain. 4.1 Why the review domain is not suitable for studying opinion role extraction for verbs There has been a lot of research on the review domain, which also means that there are several datasets from different domains allowing crossdomain sentiment analysis. However, for more indepth opinion role extraction evoked by verb predicates, these types of texts seem to be less suitable – despite the plethora of previous publications on opinion target extraction (Hu and Liu, 2004; Jakob and Gurevych, 2010; Liu et al., 2013b; Liu et al., 2013a; Liu et al., 2014). We identified the following reasons for that: Firstly, the subtask of opinion holder extraction is not really relevant on this text type. Product reviews typically reflect the author’s views on a particular product. Therefore, the overwhelming majority of explicitly mentioned opinion holders agent of verb patient of verb no (direct) relationship 21.8 44.5 33.8 Table 4: Proportion of relationships between opinion targets and opinion verbs in the Darmstadt Service Review Corpus (DSRC). 4 We chose this corpus as a typical representative c"
W15-2921,J13-3002,0,0.0495278,"Missing"
W15-2921,P14-1145,0,0.0149165,"able to discriminate between the opinion roles. In §2, we demonstrated the need for acquiring more lexical knowledge about opinion verbs for open-domain opinion role extraction. This raises the question whether existing general-purpose resources could be exploited for this purpose. If one considers the plethora of different lexical resources developed for sentiment analysis, i.e. sentiment lexicons listing subjective expressions and their prior polarity (Wilson et al., 2005; Baccianella et al., 2010; Taboada et al., 2011), emotion lexicons (Mohammad and Turney, 2013) or connotation lexicons (Kang et al., 2014), one finds, however, that with respect to opinion role extraction there is a gap. What is missing is a lexicon that states for each opinion verb in which argument position an opinion role can be found. OT (1) [Peter]OH agent dislikes [Mary]patient . OH (2) [Peter]OT agent disappoints [Mary]patient . We assume that it is lexical information that decides in what argument position opinion roles are realized. That is, a verb, such as dislike, believe or applaud, belongs to a group with different linguistic properties than verbs, such as disappoint, interest or frighten. However, the realizations"
W15-2921,W06-0301,0,0.0555054,"wards Peter (i.e. Peter made Mary feel better). One resource that has previously been examined for this task is FrameNet (Baker et al., 1998). The idea is to identify in frames (which predominantly contain opinion expressions) those frame elements that typically contain either opinion holders or opinion targets. Once this mapping has been established, a FrameNet-parser, such as Semafor (Das et al., 2010), could be used to automatically recognize frame structures in natural language text. By consulting the mapping from frame elements to opinion roles, specific opinion roles could be extracted. Kim and Hovy (2006) followed this approach for a set of opinion verbs and adjectives. Thus, they were able to correctly resolve some problems which cannot be solved with the help of syntactic parsing or PropBank-like semantic roles, such as the role distinctions in (1) and (2). For instance, while the opinion holders in (6) and (7) map to the same frame element EXPERIENCER, the PropBank-like semantic roles differ. Unfortunately, the resulting mapping lists from that work are not publicly available. OT (4) [Peter]OH agent consoles [Mary]patient . OH (5) [Peter]OT agent consoles [Mary]patient . These types of sele"
W15-2921,kingsbury-palmer-2002-treebank,0,0.117256,"(Wilson et al., 2005). Our main assumption is that the opinion verbs from that lexicon can be considered a representative choice of all kinds of opinion expressions that exists in the English language. OT (6) [Peter EXPERIENCER ]OH agent dislikes [Mary]patient . OH (7) [Peter]OT agent disappoints [Mary EXPERIENCER ]patient . Table 1 shows some statistics of our opinion verbs with regard to matched frames and frame elements. Considering that there are 615 different frame elements associated to the different frames2 1 By agent and patient, we mean constituents labeled as A0 and A1 in PropBank (Kingsbury and Palmer, 2002). 2 This count conflates frame elements of the same name that occur in different frames. 149 # opinion verbs (from the Subjectivity Lexicon) # opinion verbs with at least one frame # different frames associated with opinion verbs # different frame elements associated with opinion verbs 1175 691 306 615 con are all contained in WordNet). A straightforward solution for using that resource in the current task would be to group opinion verbs that share the same selectional preferences for opinion holders and targets with the help of the WordNet ontology graph. One common way of doing so would be t"
W15-2921,P13-1172,0,0.126589,"at adjectives are much more frequent than verbs. Thirdly, the review domain is typically focused on products, e.g. movies, books, electronic devices etc. This also means that only specific semantic types are eligible for opinion holders and targets, e.g. persons are less likely to be opinion targets. Therefore, much of the research in opinion target extraction relies on entity priors. By that we mean that (supervised) classifiers learn weights for specific entities (typically nouns or noun phrases) of how likely they represent a priori an opinion target (Zhuang et al., 2006; Qiu et al., 2011; Liu et al., 2013b; Liu et al., 2014). For example, in the movie domain Psycho is very likely to be an opinion target as will be iPhone in the electronics domain. However, as such features do not transfer to other domains, they distract research efforts from the universally applicable feature of selectional preferences. Table 4, for example, shows the proportion of different relationships between opinion targets and opinion verbs on DSRC. It shows that there is a considerable number of targets in both agent position (14) and patient position (13) & (15). So, it is not trivial to detect opinion targets here. Ho"
W15-2921,P14-1030,0,0.0657429,"uch more frequent than verbs. Thirdly, the review domain is typically focused on products, e.g. movies, books, electronic devices etc. This also means that only specific semantic types are eligible for opinion holders and targets, e.g. persons are less likely to be opinion targets. Therefore, much of the research in opinion target extraction relies on entity priors. By that we mean that (supervised) classifiers learn weights for specific entities (typically nouns or noun phrases) of how likely they represent a priori an opinion target (Zhuang et al., 2006; Qiu et al., 2011; Liu et al., 2013b; Liu et al., 2014). For example, in the movie domain Psycho is very likely to be an opinion target as will be iPhone in the electronics domain. However, as such features do not transfer to other domains, they distract research efforts from the universally applicable feature of selectional preferences. Table 4, for example, shows the proportion of different relationships between opinion targets and opinion verbs on DSRC. It shows that there is a considerable number of targets in both agent position (14) and patient position (13) & (15). So, it is not trivial to detect opinion targets here. However, if one looks"
W15-2921,N04-3012,0,0.0525926,"s that take agents as opinion holders, such as dislike, as another group verbs that take patients as opinion holders, such as disappoint, and so on) and propagate their labels to the remaining opinion verbs via the WordNet graph. Such bootstrapping on WordNet has been effectively used for the induction of sentiment lexicons (Esuli and Sebastiani, 2006; Rao and Ravichandran, 2009) or effect predicates (Choi and Wiebe, 2014). It relies on a good similarity metric in order to propagate the labels from labeled seed words to unlabeled words. We experimented with the metrics in WordNet::Similarity (Pedersen et al., 2004) and found that the opinion verbs most similar to a specified opinion verb do not necessarily share the same syntactic properties. For example, Table 2 lists the 12 opinion verbs most similar to outrage and please, which are typical opinion verbs that take an opinion holder in patient position and an opinion target in agent position.3 (They would be plausible candidates for verb seeds for that verb category.) Unfortunately, among the list of similar verbs, we find many opinion verbs which have opinion holder and target in a different argument position, such as hate on the list for outrage: Tab"
W15-2921,J11-1002,0,0.123132,"Missing"
W15-2921,E09-1077,0,0.0350976,"lp of the WordNet ontology graph. One common way of doing so would be the application of some bootstrapping method in which one defines seed opinion verbs with distinct selectional preferences (for instance, one defines as one group opinion verbs that take agents as opinion holders, such as dislike, as another group verbs that take patients as opinion holders, such as disappoint, and so on) and propagate their labels to the remaining opinion verbs via the WordNet graph. Such bootstrapping on WordNet has been effectively used for the induction of sentiment lexicons (Esuli and Sebastiani, 2006; Rao and Ravichandran, 2009) or effect predicates (Choi and Wiebe, 2014). It relies on a good similarity metric in order to propagate the labels from labeled seed words to unlabeled words. We experimented with the metrics in WordNet::Similarity (Pedersen et al., 2004) and found that the opinion verbs most similar to a specified opinion verb do not necessarily share the same syntactic properties. For example, Table 2 lists the 12 opinion verbs most similar to outrage and please, which are typical opinion verbs that take an opinion holder in patient position and an opinion target in agent position.3 (They would be plausibl"
W15-2921,P94-1019,0,0.631708,"Missing"
W15-2921,P13-1161,0,0.0207982,"s unattested. In order to demonstrate that our new corpus is a more suitable resource in order to study selectional preferences (Goal 2) and multiple viewpoint evocation (Goal 3), we prepared some statistics regarding mentions of opinion verbs and their properties in the MPQA corpus and our corpus (denoted by VERB). Due to the unavailability of MPQA 3.0, we had to use MPQA 2.0, whose annotation with regard to opinion targets is incomplete. We therefore compare opinion verbs only with regard to their opinion holders. However, given the strong interrelations between opinion holders and targets (Yang and Cardie, 2013), we think that if it is shown that our corpus better represents the versatility of opinion holders, this should (almost) equally also apply for opinion targets. Table 5 examines the types of argument positions in which an opinion holder is realized. We distinguish between three different roles (already informally introduced in §2): the holder is in agent position (example: dislike), the holder is in patient position (example: disappoint) or the holder is not an argument at all (example: gossip). The latter are cases in which the speaker (or some nested source) is the opinion holder. Table 5 a"
W15-2921,W12-3716,1,0.878384,"Missing"
W15-2921,R11-1028,0,0.0274209,"that could be used in order to extract opinion roles correctly without the knowledge about opinion role realization. Currently, there exists only one commonly accepted corpus for English containing manual annotation of both opinion holders and targets, i.e. the MPQA corpus (Deng and Wiebe, 2015). Apart from that, not a single lexical resource for that specific task is available. Moreover, there does not exist any publicly available tool that supports both opinion holder and target extraction. Typical applications, such as opinion summarization, however, require both components simultaneously (Stoyanov and Cardie, 2011). These facts indicate that there definitely needs to be more research on the task of opinion role extraction. In order to stimulate more research in this direction, we present a verb-based corpus for opinion role extraction. The difference to previous datasets is that it has been sampled in such a way that all opinion verbs of a common sentiment lexicon are widely represented. Previous corpora have a bias towards those opinion expressions that are frequent in a particular domain. We demonstrate on two opinion holder extraction systems that performance on the new corpus massively drops compare"
W15-2921,J11-2001,0,0.0603571,"PropBank-like semantic roles (i.e. agent, patient1 ) assigned to the entities, one may not be able to discriminate between the opinion roles. In §2, we demonstrated the need for acquiring more lexical knowledge about opinion verbs for open-domain opinion role extraction. This raises the question whether existing general-purpose resources could be exploited for this purpose. If one considers the plethora of different lexical resources developed for sentiment analysis, i.e. sentiment lexicons listing subjective expressions and their prior polarity (Wilson et al., 2005; Baccianella et al., 2010; Taboada et al., 2011), emotion lexicons (Mohammad and Turney, 2013) or connotation lexicons (Kang et al., 2014), one finds, however, that with respect to opinion role extraction there is a gap. What is missing is a lexicon that states for each opinion verb in which argument position an opinion role can be found. OT (1) [Peter]OH agent dislikes [Mary]patient . OH (2) [Peter]OT agent disappoints [Mary]patient . We assume that it is lexical information that decides in what argument position opinion roles are realized. That is, a verb, such as dislike, believe or applaud, belongs to a group with different linguistic p"
W15-2921,P10-1059,0,0.0456984,"Missing"
W15-2921,E12-1033,1,0.929205,"ance, while the opinion holders in (6) and (7) map to the same frame element EXPERIENCER, the PropBank-like semantic roles differ. Unfortunately, the resulting mapping lists from that work are not publicly available. OT (4) [Peter]OH agent consoles [Mary]patient . OH (5) [Peter]OT agent consoles [Mary]patient . These types of selectional preferences (1)-(5) have been observed before including the case of multiple viewpoint evocation (4)-(5), most prominently by Ruppenhofer et al. (2008). Yet little research on opinion role extraction has actually paid attention to this issue. One exception is Wiegand and Klakow (2012) who experiment with an induction approach to distinguish cases like (1) and (2). Nonetheless, datasets and lists of types of opinion verbs have not been publicly released. The above analysis suggests more research on lexical resources is required. In the following, we show that existing resources are not suitable to provide the type of information we are looking for. As a reference of opinion verbs, we use the set of 1175 verbs contained in the Subjectivity Lexicon (Wilson et al., 2005). Our main assumption is that the opinion verbs from that lexicon can be considered a representative choice"
W15-2921,H05-1044,0,0.0745814,"by (1) and (2) which show that, even with the PropBank-like semantic roles (i.e. agent, patient1 ) assigned to the entities, one may not be able to discriminate between the opinion roles. In §2, we demonstrated the need for acquiring more lexical knowledge about opinion verbs for open-domain opinion role extraction. This raises the question whether existing general-purpose resources could be exploited for this purpose. If one considers the plethora of different lexical resources developed for sentiment analysis, i.e. sentiment lexicons listing subjective expressions and their prior polarity (Wilson et al., 2005; Baccianella et al., 2010; Taboada et al., 2011), emotion lexicons (Mohammad and Turney, 2013) or connotation lexicons (Kang et al., 2014), one finds, however, that with respect to opinion role extraction there is a gap. What is missing is a lexicon that states for each opinion verb in which argument position an opinion role can be found. OT (1) [Peter]OH agent dislikes [Mary]patient . OH (2) [Peter]OT agent disappoints [Mary]patient . We assume that it is lexical information that decides in what argument position opinion roles are realized. That is, a verb, such as dislike, believe or applau"
W15-2921,C98-1013,0,\N,Missing
W17-1101,J92-4003,0,0.717978,"nce in hate speech detection, in order to work effectively these features require predictive words to appear in both training and test data. However, since hate speech detection is usually applied on small pieces of text (e.g. passages or even individual sentences), one may face a data sparsity problem. This is why several works address this issue by applying some form of word generalization. This can be achieved by carrying out word clustering and then using induced cluster IDs representing sets of words as additional (generalized) features. A standard algorithm for this is Brown clustering (Brown et al., 1992) which has been used as a feature in Warner and Hirschberg (2012). While Brown clustering produces hard clusters – that is, it assigns each individual word to one particular cluster – Latent Dirichlet Allocation (LDA) (Blei et al., 2003) produces for each word a topic distribution indicating to which degree a word belongs to each topic. Such information has similarly been used for hate speech detection (Xiang et al., 2012; Zhong et al., 2016). More recently, distributed word representations (based on neural networks), also referred to as word embeddings, have been proposed for a similar purpos"
W17-1101,W16-3638,0,0.369554,"focus on vulgar language and profanity-related offensive content. Xu et al. (2012)2 further look into jokingly formulated teasing in messages that represent (possibly less severe) bullying episodes. Finally, Burnap and Williams (2014) specifically look into othering language, characterized by an us-them dichotomy in racist communication. 3 based approaches since the unusual spelling variations will result in very rare or even unknown tokens in the training data. Character-level approaches, on the other hand, are more likely to capture the similarity to the canonical spelling of these tokens. Mehdad and Tetreault (2016) systematically compare character n-gram features with token n-grams for hate speech detection, and find that character n-grams prove to be more predictive than token n-grams. Apart from word- and character-based features, hate speech detection can also benefit from other surface features (Chen et al., 2012; Nobata et al., 2016), such as information on the frequency of URL mentions and punctuation, comment and token lengths, capitalization, words that cannot be found in English dictionaries, and the number of non-alpha numeric characters present in tokens. Features for Hate Speech Detection As"
W17-1101,R15-1086,0,0.296411,"Missing"
W17-1101,W12-2103,0,0.783,"so the most frequently used expression for this phenomenon, and is even a legal term in several countries. Below we list other terms that are used in the NLP community. This should also help readers with finding further literature on that task. In the earliest work on hate speech, Spertus (1997) refers to abusive messages, hostile messages or flames. More recently, many authors have shifted to employing the term cyberbullying (Xu et al., 2012; Hosseinmardi et al., 2015; Zhong et al., 2016; Van Hee et al., 2015; Dadvar et al., 2013; Dinakar et al., 2012). The actual term hate speech is used by Warner and Hirschberg (2012), Burnap and Williams (2015), Silva et al. (2016), Djuric et al. (2015), Gitari et al. (2015), Williams and Burnap (2015) and Kwok and Wang (2013). Further, (1) Go fucking kill yourself and die already useless ugly pile of shit scumbag. (2) The Jew Faggot Behind The Financial Collapse (3) Hope one of those bitches falls over and breaks her leg Due to the massive rise of user-generated web content, in particular on social media networks, the amount of hate speech is also steadily increasing. Over the past years, interest in online hate speech detection and particularly the automatization of thi"
W17-1101,D13-1066,0,0.055307,"Missing"
W17-1101,N16-2013,0,0.599956,"ave been proposed for a similar purposes. For each word a vector representation is induced (Mikolov et al., 2013) from a large (unlabelled) text corpus. Such vector representations have the advantage that different, semantiSimple Surface Features For any text classification task, the most obvious information to utilize are surface-level features, such as bag of words. Indeed, unigrams and larger n-grams are included in the feature sets by a majority of authors (Chen et al., 2012; Xu et al., 2012; Warner and Hirschberg, 2012; Sood et al., 2012b; Burnap and Williams, 2015; Van Hee et al., 2015; Waseem and Hovy, 2016; Burnap and Williams, 2016; Hosseinmardi et al., 2015; Nobata et al., 2016). These features are often reported to be highly predictive. Still, in many works n-gram features are combined with a large selection of other features. For example, in their recent work, Nobata et al. (2016) report that while token and character n-gram features are the most predictive single features in their experiments, combining them with all additional features further improves performance. Character-level n-gram features might provide a way to attenuate the spelling variation problem often faced when working with"
W17-1101,N12-1084,0,0.716668,"be considered a broad umbrella term for numerous kinds of insulting user-created content addressed in the individual works we summarize in this paper. Hate speech is also the most frequently used expression for this phenomenon, and is even a legal term in several countries. Below we list other terms that are used in the NLP community. This should also help readers with finding further literature on that task. In the earliest work on hate speech, Spertus (1997) refers to abusive messages, hostile messages or flames. More recently, many authors have shifted to employing the term cyberbullying (Xu et al., 2012; Hosseinmardi et al., 2015; Zhong et al., 2016; Van Hee et al., 2015; Dadvar et al., 2013; Dinakar et al., 2012). The actual term hate speech is used by Warner and Hirschberg (2012), Burnap and Williams (2015), Silva et al. (2016), Djuric et al. (2015), Gitari et al. (2015), Williams and Burnap (2015) and Kwok and Wang (2013). Further, (1) Go fucking kill yourself and die already useless ugly pile of shit scumbag. (2) The Jew Faggot Behind The Financial Collapse (3) Hope one of those bitches falls over and breaks her leg Due to the massive rise of user-generated web content, in particular on"
W19-2101,P15-2072,0,0.509626,"of six G20 countries, which we make publicly available.1 Framing is a field of research in communication theory and political science investigating how information is presented to audiences, especially in news media. According to a common definition, to frame is to “to select some aspects of a perceived reality and make them more salient in a communication text, in such a way as to promote a particular problem definition, causal interpretation, moral evaluation, and/or treatment recommendation” (Entman, 1993, p. 52). Most work on framing has focused on issues and events, rather than entities (Card et al., 2015; Fulgoni et al., 2016; Field et al., 2018). We therefore introduce entity framing, which we define as a presentation of an entity which intentionally or unintentionally promotes a particular viewpoint towards that entity. We focus on the framing of political figures on social media, in order to better understand computer-mediated civil political discourse. Online political discussion has been said to have an increasing influence on the democratic process, including on the tone and civility of political debates (Persily, 2017; Ott, 2017). Tweets on political themes are indeed retweeted more of"
W19-2101,D16-1148,0,0.135425,"in Section 5, we do not make this distinction. 1 Proceedings of the Third Workshop on Natural Language Processing and Computational Social Science, pages 1–6 c Minneapolis, Minnesota, June 6, 2019. 2019 Association for Computational Linguistics Subcorpus France Indonesia Russia South Africa Turkey United States interviews, whereas our work examines naming quantitatively and on social media data. The concept of framing has been applied to a variety of issues and events (Card et al., 2015; Tsur et al., 2015; Fulgoni et al., 2016; Field et al., 2018), and in one case to the framing of entities (Card et al., 2016), but not previously on social media data. Use of social media to express political opinions has instead been studied to forecast elections (Burnap et al., 2016), political mobilisation (Weeks et al., 2017), and assess political polarization (Bail et al., 2018). A prominent area of NLP that focuses on expressions of favour is stance detection, the detection of sentiment towards a specified target. Most systems focus on stance towards products, companies and abstract topics rather than persons (Somasundaran and Wiebe, 2010; Meng et al., 2012; Jiang et al., 2011; Mohammad et al., 2016). The data"
W19-2101,S16-1003,0,0.260767,"Missing"
W19-2101,S17-2088,0,0.0471267,"ata. Use of social media to express political opinions has instead been studied to forecast elections (Burnap et al., 2016), political mobilisation (Weeks et al., 2017), and assess political polarization (Bail et al., 2018). A prominent area of NLP that focuses on expressions of favour is stance detection, the detection of sentiment towards a specified target. Most systems focus on stance towards products, companies and abstract topics rather than persons (Somasundaran and Wiebe, 2010; Meng et al., 2012; Jiang et al., 2011; Mohammad et al., 2016). The datasets for SemEval 2017 (Task A and B) (Rosenthal et al., 2017) and RepLab (Amig´o et al., 2012, 2013, 2014) as well as the dataset created by (Taddy, 2013) do include a variety of person entities, but no stance detection work has investigated the influence of naming on stance. 3 Expert agr. 0.78 0.91 0.72 0.87 0.65 0.78 Table 1: Inter-annotator agreement for the on-target/offtarget task (Krippendorff alpha): agreement among FE workers and agreement between two experts adjudicating tweets where FE worker judgment was not unanimous. Subcorpus France Indonesia Russia South-Africa Turkey United States Adj. tweets 281 290 121 227 128 192 Diff. w/ expert 1 0.0"
W19-2101,W10-0214,0,0.0509992,"oni et al., 2016; Field et al., 2018), and in one case to the framing of entities (Card et al., 2016), but not previously on social media data. Use of social media to express political opinions has instead been studied to forecast elections (Burnap et al., 2016), political mobilisation (Weeks et al., 2017), and assess political polarization (Bail et al., 2018). A prominent area of NLP that focuses on expressions of favour is stance detection, the detection of sentiment towards a specified target. Most systems focus on stance towards products, companies and abstract topics rather than persons (Somasundaran and Wiebe, 2010; Meng et al., 2012; Jiang et al., 2011; Mohammad et al., 2016). The datasets for SemEval 2017 (Task A and B) (Rosenthal et al., 2017) and RepLab (Amig´o et al., 2012, 2013, 2014) as well as the dataset created by (Taddy, 2013) do include a variety of person entities, but no stance detection work has investigated the influence of naming on stance. 3 Expert agr. 0.78 0.91 0.72 0.87 0.65 0.78 Table 1: Inter-annotator agreement for the on-target/offtarget task (Krippendorff alpha): agreement among FE workers and agreement between two experts adjudicating tweets where FE worker judgment was not un"
W19-2101,D18-1393,0,0.228381,"cly available.1 Framing is a field of research in communication theory and political science investigating how information is presented to audiences, especially in news media. According to a common definition, to frame is to “to select some aspects of a perceived reality and make them more salient in a communication text, in such a way as to promote a particular problem definition, causal interpretation, moral evaluation, and/or treatment recommendation” (Entman, 1993, p. 52). Most work on framing has focused on issues and events, rather than entities (Card et al., 2015; Fulgoni et al., 2016; Field et al., 2018). We therefore introduce entity framing, which we define as a presentation of an entity which intentionally or unintentionally promotes a particular viewpoint towards that entity. We focus on the framing of political figures on social media, in order to better understand computer-mediated civil political discourse. Online political discussion has been said to have an increasing influence on the democratic process, including on the tone and civility of political debates (Persily, 2017; Ott, 2017). Tweets on political themes are indeed retweeted more often when their content is emotionally charg"
W19-2101,L16-1591,0,0.531323,"s, which we make publicly available.1 Framing is a field of research in communication theory and political science investigating how information is presented to audiences, especially in news media. According to a common definition, to frame is to “to select some aspects of a perceived reality and make them more salient in a communication text, in such a way as to promote a particular problem definition, causal interpretation, moral evaluation, and/or treatment recommendation” (Entman, 1993, p. 52). Most work on framing has focused on issues and events, rather than entities (Card et al., 2015; Fulgoni et al., 2016; Field et al., 2018). We therefore introduce entity framing, which we define as a presentation of an entity which intentionally or unintentionally promotes a particular viewpoint towards that entity. We focus on the framing of political figures on social media, in order to better understand computer-mediated civil political discourse. Online political discussion has been said to have an increasing influence on the democratic process, including on the tone and civility of political debates (Persily, 2017; Ott, 2017). Tweets on political themes are indeed retweeted more often when their content"
W19-2101,P15-1157,0,0.0269095,"fer to the conversation partner or as a form of reference to refer to a third party. For reasons described in Section 5, we do not make this distinction. 1 Proceedings of the Third Workshop on Natural Language Processing and Computational Social Science, pages 1–6 c Minneapolis, Minnesota, June 6, 2019. 2019 Association for Computational Linguistics Subcorpus France Indonesia Russia South Africa Turkey United States interviews, whereas our work examines naming quantitatively and on social media data. The concept of framing has been applied to a variety of issues and events (Card et al., 2015; Tsur et al., 2015; Fulgoni et al., 2016; Field et al., 2018), and in one case to the framing of entities (Card et al., 2016), but not previously on social media data. Use of social media to express political opinions has instead been studied to forecast elections (Burnap et al., 2016), political mobilisation (Weeks et al., 2017), and assess political polarization (Bail et al., 2018). A prominent area of NLP that focuses on expressions of favour is stance detection, the detection of sentiment towards a specified target. Most systems focus on stance towards products, companies and abstract topics rather than per"
W19-2101,N13-1132,0,0.0262985,"7). We expect it to better capture differences between tweets which are neutral in tone but reflect differently on the president, such as ‘Trump trailing in primaries’ vs ‘Jobs market improving under Trump’. Crucially, the prompt also allows annotators to give different ratings to ‘President Trump visits France’ and ‘Trump visits France’. As in Card et al. (2015), the perspective is anchored to that of a proponent of the target in order to combat the lower reliability of reader-perspective prompts (Buechel and Hahn, 2017). After annotation we used Multi-Annotator Competence Estimation (MACE) (Hovy et al., 2013) to identify and remove the least reliable annotators. We collected an additional two judgH0 Variation in naming and stance are not related. H1 Naming primarily downplays or emphasises the president’s status. Therefore, formality of naming is positively related to stance. H2 Naming primarily conveys the degree of solidarity with the president. Therefore, formality of naming is negatively related to stance. Table 4 gives examples of tweets which can be interpreted to support either H1 or H2, or to support the existence of alternative, context-specific functions of naming, such as sarcasm. We gr"
W19-2101,P11-1016,0,0.0645378,"case to the framing of entities (Card et al., 2016), but not previously on social media data. Use of social media to express political opinions has instead been studied to forecast elections (Burnap et al., 2016), political mobilisation (Weeks et al., 2017), and assess political polarization (Bail et al., 2018). A prominent area of NLP that focuses on expressions of favour is stance detection, the detection of sentiment towards a specified target. Most systems focus on stance towards products, companies and abstract topics rather than persons (Somasundaran and Wiebe, 2010; Meng et al., 2012; Jiang et al., 2011; Mohammad et al., 2016). The datasets for SemEval 2017 (Task A and B) (Rosenthal et al., 2017) and RepLab (Amig´o et al., 2012, 2013, 2014) as well as the dataset created by (Taddy, 2013) do include a variety of person entities, but no stance detection work has investigated the influence of naming on stance. 3 Expert agr. 0.78 0.91 0.72 0.87 0.65 0.78 Table 1: Inter-annotator agreement for the on-target/offtarget task (Krippendorff alpha): agreement among FE workers and agreement between two experts adjudicating tweets where FE worker judgment was not unanimous. Subcorpus France Indonesia Rus"
wiegand-etal-2012-gold,D10-1101,0,\N,Missing
wiegand-etal-2012-gold,chrupala-klakow-2010-named,1,\N,Missing
wiegand-etal-2012-gold,C92-2082,0,\N,Missing
wiegand-etal-2012-gold,P09-1113,0,\N,Missing
wiegand-etal-2012-gold,P06-1101,0,\N,Missing
wiegand-etal-2012-gold,N03-1011,0,\N,Missing
wiegand-klakow-2010-predictive,H05-1044,0,\N,Missing
wiegand-klakow-2010-predictive,D08-1013,0,\N,Missing
wiegand-klakow-2010-predictive,R09-1034,0,\N,Missing
wiegand-klakow-2010-predictive,P06-2079,0,\N,Missing
wiegand-klakow-2010-predictive,J04-3002,0,\N,Missing
wiegand-klakow-2010-predictive,P02-1053,0,\N,Missing
wiegand-klakow-2010-predictive,D09-1063,0,\N,Missing
wiegand-klakow-2010-predictive,W02-1011,0,\N,Missing
