2020.acl-main.776,D07-1101,0,0.178436,"features, even when employing a powerful pre-trained encoder, such as BERT. While the gains of higher-order features are small in the presence of a powerful encoder, they are consistent for long-range dependencies and long sentences. In particular, higher-order models are more accurate on full sentence parses and on the exact match of modifier lists, indicating that they deal better with larger, more complex structures. 1 Introduction Before the advent of neural networks in NLP, dependency parsers relied on higher-order features to better model sentence structure (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Martins et al., 2013, inter alia). Common choices for such features were siblings (a head word and two modifiers) and grandparents (a head word, its own head and a modifier). Kiperwasser and Goldberg (2016) showed that even without higher order features, a parser with an RNN encoder could achieve state-of-the-art results. This led folk wisdom to suggest that modeling higher-order features in a neural parser would not bring additional advantages, and nearly all recent research on dependency parsing was restricted to first-order models (Dozat and Manning, 2016; Smith et"
2020.acl-main.776,P19-1012,0,0.0645261,"dparents (a head word, its own head and a modifier). Kiperwasser and Goldberg (2016) showed that even without higher order features, a parser with an RNN encoder could achieve state-of-the-art results. This led folk wisdom to suggest that modeling higher-order features in a neural parser would not bring additional advantages, and nearly all recent research on dependency parsing was restricted to first-order models (Dozat and Manning, 2016; Smith et al., 2018a). Kulmizev et al. (2019) further reinforced this belief comparing transition and graph-based decoders (but none of which higher order); Falenska and Kuhn (2019) suggested that higher-order features become redundant because the parsing models encode them implicitly. However, there is some evidence that neural parsers still benefit from structure modeling. Zhang et al. (2019) showed that a parser trained with a global structure loss function has higher accuracy than when trained with a local objective (i.e., learning the head of each word independently). Falenska and Kuhn (2019) examined the impact of consecutive sibling features in a neural dependency parser. While they found mostly negative results in a transition-based setting, a graph-based parser"
2020.acl-main.776,Q16-1023,0,0.0455843,"and long sentences. In particular, higher-order models are more accurate on full sentence parses and on the exact match of modifier lists, indicating that they deal better with larger, more complex structures. 1 Introduction Before the advent of neural networks in NLP, dependency parsers relied on higher-order features to better model sentence structure (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Martins et al., 2013, inter alia). Common choices for such features were siblings (a head word and two modifiers) and grandparents (a head word, its own head and a modifier). Kiperwasser and Goldberg (2016) showed that even without higher order features, a parser with an RNN encoder could achieve state-of-the-art results. This led folk wisdom to suggest that modeling higher-order features in a neural parser would not bring additional advantages, and nearly all recent research on dependency parsing was restricted to first-order models (Dozat and Manning, 2016; Smith et al., 2018a). Kulmizev et al. (2019) further reinforced this belief comparing transition and graph-based decoders (but none of which higher order); Falenska and Kuhn (2019) suggested that higher-order features become redundant becau"
2020.acl-main.776,P10-1001,0,0.0623405,"when employing a powerful pre-trained encoder, such as BERT. While the gains of higher-order features are small in the presence of a powerful encoder, they are consistent for long-range dependencies and long sentences. In particular, higher-order models are more accurate on full sentence parses and on the exact match of modifier lists, indicating that they deal better with larger, more complex structures. 1 Introduction Before the advent of neural networks in NLP, dependency parsers relied on higher-order features to better model sentence structure (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Martins et al., 2013, inter alia). Common choices for such features were siblings (a head word and two modifiers) and grandparents (a head word, its own head and a modifier). Kiperwasser and Goldberg (2016) showed that even without higher order features, a parser with an RNN encoder could achieve state-of-the-art results. This led folk wisdom to suggest that modeling higher-order features in a neural parser would not bring additional advantages, and nearly all recent research on dependency parsing was restricted to first-order models (Dozat and Manning, 2016; Smith et al., 2018a). Kulmizev e"
2020.acl-main.776,D07-1015,0,0.059756,"andparent part is a tuple (h, m, g) such that g is the parent of h and h is the parent of m. There are no grandparent parts such that h is ROOT. Scoring The score for a higher order part (h, m, r) of type ρ (in our case, either grandparent or consecutive sibling) is computed as: sθ (h, m, r) = wρ&gt; · (λρ1 tanh(hρh + hρr ) + λρ2 tanh(hρm + hρr ) + λρ3 tanh(hρh + hρm + hρr )), ρ hρh = fhρ (hh ), hρm = fm (hm ), hρr = frρ (hr ). Lθ (x, y) = − log pθ (y |x) X = −sθ (y) + log exp(sθ (yi )). i We can compute the partition function over all possible trees yi efficiently using the Matrix-Tree Theorem (Koo et al., 2007), which also gives us arc marginal probabilities. The sentence score sθ (x, y) is computed as the sum of the score of its parts. Additionally, we try first-order models trained with a hinge loss, as Zhang et al. (2019) (also used with our second-order models; see §2.4), maximizing the margin between the correct parse tree y and ˆ: any other tree y ˆ ) − sθ (x, y) + ∆(y, y ˆ )], Lθ (x, y) = max[sθ (x, y ˆ y ˆ ) is the Hamming cost between y and where ∆(y, y ˆ , i.e., the number of arcs in which they differ. y 2 Second-Order Model We refer the reader to Qi et al. (2018) for further definition of"
2020.acl-main.776,D10-1125,0,0.0403514,"troduce a parameter vector h∅ to account for ∅. Decoding The drawback of higher-order feature templates is that exact decoding is intractable for the non-projective case. Classically, researchers have resorted to approximate decoding as well as using a first-order parser to eliminate unlikely arcs and their respective higher-order parts. We employ both of these techniques; specifically, we use the dual decomposition algorithm AD3 (Martins et al., 2011, 2013) for decoding, which often arrives at the exact solution. We use head automata factors 8796 to handle sibling and grandparent structures (Koo et al., 2010), and the traditional Chu-Liu-Edmonds algorithm to handle the tree constraint factor (McDonald et al., 2005). 2.5 Additional Training Details Multitask Learning Our models also predict UPOS, XPOS and morphology tags (UFeats), as training for these additional objectives increases parsing performance. They are implemented via softmax layers on top of the BiLSTM output, and have a cross-entropy loss. Parser and tagger share two BiLSTM layers, with an additional layer for each one (similar to Straka, 2018). We only consider UFeats singletons in the training data, i.e., we do not decompose them int"
2020.acl-main.776,D19-1277,0,0.112062,"Missing"
2020.acl-main.776,P13-2109,1,0.937262,"ul pre-trained encoder, such as BERT. While the gains of higher-order features are small in the presence of a powerful encoder, they are consistent for long-range dependencies and long sentences. In particular, higher-order models are more accurate on full sentence parses and on the exact match of modifier lists, indicating that they deal better with larger, more complex structures. 1 Introduction Before the advent of neural networks in NLP, dependency parsers relied on higher-order features to better model sentence structure (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Martins et al., 2013, inter alia). Common choices for such features were siblings (a head word and two modifiers) and grandparents (a head word, its own head and a modifier). Kiperwasser and Goldberg (2016) showed that even without higher order features, a parser with an RNN encoder could achieve state-of-the-art results. This led folk wisdom to suggest that modeling higher-order features in a neural parser would not bring additional advantages, and nearly all recent research on dependency parsing was restricted to first-order models (Dozat and Manning, 2016; Smith et al., 2018a). Kulmizev et al. (2019) further r"
2020.acl-main.776,D11-1022,1,0.875353,"binations of a second-order part with h, m, or both. There is no factor combining h and m only, since they are already present in the first-order scoring. We also introduce a parameter vector h∅ to account for ∅. Decoding The drawback of higher-order feature templates is that exact decoding is intractable for the non-projective case. Classically, researchers have resorted to approximate decoding as well as using a first-order parser to eliminate unlikely arcs and their respective higher-order parts. We employ both of these techniques; specifically, we use the dual decomposition algorithm AD3 (Martins et al., 2011, 2013) for decoding, which often arrives at the exact solution. We use head automata factors 8796 to handle sibling and grandparent structures (Koo et al., 2010), and the traditional Chu-Liu-Edmonds algorithm to handle the tree constraint factor (McDonald et al., 2005). 2.5 Additional Training Details Multitask Learning Our models also predict UPOS, XPOS and morphology tags (UFeats), as training for these additional objectives increases parsing performance. They are implemented via softmax layers on top of the BiLSTM output, and have a cross-entropy loss. Parser and tagger share two BiLSTM la"
2020.acl-main.776,P19-1562,0,0.518649,"m to suggest that modeling higher-order features in a neural parser would not bring additional advantages, and nearly all recent research on dependency parsing was restricted to first-order models (Dozat and Manning, 2016; Smith et al., 2018a). Kulmizev et al. (2019) further reinforced this belief comparing transition and graph-based decoders (but none of which higher order); Falenska and Kuhn (2019) suggested that higher-order features become redundant because the parsing models encode them implicitly. However, there is some evidence that neural parsers still benefit from structure modeling. Zhang et al. (2019) showed that a parser trained with a global structure loss function has higher accuracy than when trained with a local objective (i.e., learning the head of each word independently). Falenska and Kuhn (2019) examined the impact of consecutive sibling features in a neural dependency parser. While they found mostly negative results in a transition-based setting, a graph-based parser still showed significant gains on two out of 10 treebanks. In this paper, we test rigorously the hypothesis of the utility of second-order features. In particular, we experiment with consecutive sibling and grandpare"
2020.acl-main.776,E06-1011,0,0.23596,"ay benefit from higher-order features, even when employing a powerful pre-trained encoder, such as BERT. While the gains of higher-order features are small in the presence of a powerful encoder, they are consistent for long-range dependencies and long sentences. In particular, higher-order models are more accurate on full sentence parses and on the exact match of modifier lists, indicating that they deal better with larger, more complex structures. 1 Introduction Before the advent of neural networks in NLP, dependency parsers relied on higher-order features to better model sentence structure (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Martins et al., 2013, inter alia). Common choices for such features were siblings (a head word and two modifiers) and grandparents (a head word, its own head and a modifier). Kiperwasser and Goldberg (2016) showed that even without higher order features, a parser with an RNN encoder could achieve state-of-the-art results. This led folk wisdom to suggest that modeling higher-order features in a neural parser would not bring additional advantages, and nearly all recent research on dependency parsing was restricted to first-order models (Dozat and Manning,"
2020.acl-main.776,H05-1066,0,0.562973,"Missing"
2020.acl-main.776,K18-2016,0,0.180163,"BERT embeddings. Similar to Straka et al. (2019), when using BERT, we take the mean of its last four layers. When the BERT tokenizer splits a token into more than one, we take the first one and ignore the rest, and we use the special token [CLS] to represent ROOT. The word embeddings we use are the ones provided in the CoNLL 2018 shared task. 2.3 First-Order Model We start with a first-order model, which is used as a pruner before running the second-order parser as in Martins et al. (2013). It uses biaffine attention to compute arc and label scores (Dozat and Manning, 2016), and similarly to Qi et al. (2018), we also add distance and linearization terms.2 We want our pruner to be capable of estimating arc probabilities, and thus we train it with a marginal inference loss, maximizing the log probability of the correct parse tree y: Consecutive siblings A consecutive sibling part is a tuple (h, m, s) such that h is the parent of both m and s, which are both to the left or to the right of h, and no other child of h exists between them. Additionally, we consider tuples (h, m, ∅) to indicate that m is the first child (if to the left of h) or the last child (if to the right). Grandparents A grandparent"
2020.acl-main.776,K18-2011,0,0.0469571,"Missing"
2020.acl-main.776,D18-1291,0,0.121925,"Missing"
2020.blackboxnlp-1.10,D17-1042,0,0.0182027,"terms of CSR. Recent works questioned the interpretative ability of attention mechanisms (Jain and Wallace, 2019; Serrano and Smith, 2019). Wiegreffe and Pinter (2019) distinguished between faithful and plausible explanations and introduced several diagnostic tools. Mullenbach et al. (2018) use human evaluation to show that attention mechanisms produce plausible explanations, consistent with our findings in §6. None of these works, however, considered the sparse selective attention mechanisms proposed in §3. Hard stochastic attention has been considered by Xu et al. (2015); Lei et al. (2016); Alvarez-Melis and Jaakkola (2017); Bastings et al. (2019), but a comparison with sparse attention and explanation strategies was still missing. Besides attention-based methods, many other explainers have been proposed using gradients (Bach et al., 2015; Montavon et al., 2018; Ding et al., 2019), leave-one-out strategies (Feng et al., 2018; Serrano and Smith, 2019), or local perturbations (Ribeiro et al., 2016; Koh and Liang, 2017), but a link with filters and wrappers in the feature selection literature has never been made. We believe the connections revealed in §2 may be useful to develop new explainers in the future. Our tr"
2020.blackboxnlp-1.10,W16-1601,0,0.0817443,"Missing"
2020.blackboxnlp-1.10,W17-5221,0,0.120296,"Missing"
2020.blackboxnlp-1.10,P19-1284,0,0.121604,"Missing"
2020.blackboxnlp-1.10,2004.iwslt-evaluation.1,0,0.0262822,"Missing"
2020.blackboxnlp-1.10,P89-1010,0,0.166243,"ion, embed feature selection within the learning algorithm by using a sparse regularizer such as the `1 -norm (Tibshirani, 1996). Features that receive zero weight become irrelevant and can 1 In linear models this gradient equals the feature’s weight. Static selection (model interpretability) Dynamic selection (prediction explainability) Wrappers Forward selection, backward elimination (Kohavi and John, 1997) Input reduction (Feng et al., 2018), representation erasure (leave-one-out) (Li et al., 2016b; Serrano and Smith, 2019), LIME (Ribeiro et al., 2016) Filters Pointwise mutual information (Church and Hanks, 1989), recursive feature elimination (Guyon et al., 2002) Input gradient (Li et al., 2016a), layerwise relevance propagation (Bach et al., 2015), top-k softmax attention Embedded `1 -regularization (Tibshirani, 1996), elastic net (Zou and Hastie, 2005) Stochastic attention (Xu et al., 2015; Lei et al., 2016; Bastings et al., 2019), sparse attention (this paper, §3) Table 1: Overview of static and dynamic feature selection techniques. be removed from the model. In dynamic feature selection, this encompasses methods where the classifier produces rationales together with its decisions (Lei et al., 201"
2020.blackboxnlp-1.10,D19-1223,1,0.901166,"Missing"
2020.blackboxnlp-1.10,N19-1423,0,0.0231077,"ses these predictions and communicates an explanation (a message m) to the layperson L. Success of the communication is dictated by the ability of ? L and C to match their predictions: y˜ = yˆ. Both the explainer and layperson can be humans or machines. Introduction The widespread use of machine learning to assist humans in decision making brings the need for explaining models’ predictions (Doshi-Velez, 2017; Lipton, 2018; Rudin, 2019; Miller, 2019). This poses a challenge in NLP, where current state-ofthe-art neural systems are generally opaque (Goldberg and Hirst, 2017; Peters et al., 2018; Devlin et al., 2019). Despite the large body of recent work (reviewed in §7), a unified perspective modeling the human-machine interaction—a communication process in its essence—is still missing. Many methods have been proposed to generate explanations. Some neural network architectures are equipped with built-in components—attention mechanisms—which weigh the relevance of input features for triggering a decision (Bahdanau et al., 2015; Vaswani et al., 2017). Top-k attention weights provide plausible, but not always faithful, explanations (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 201"
2020.blackboxnlp-1.10,W19-5201,0,0.0240982,"et al. (2018) use human evaluation to show that attention mechanisms produce plausible explanations, consistent with our findings in §6. None of these works, however, considered the sparse selective attention mechanisms proposed in §3. Hard stochastic attention has been considered by Xu et al. (2015); Lei et al. (2016); Alvarez-Melis and Jaakkola (2017); Bastings et al. (2019), but a comparison with sparse attention and explanation strategies was still missing. Besides attention-based methods, many other explainers have been proposed using gradients (Bach et al., 2015; Montavon et al., 2018; Ding et al., 2019), leave-one-out strategies (Feng et al., 2018; Serrano and Smith, 2019), or local perturbations (Ribeiro et al., 2016; Koh and Liang, 2017), but a link with filters and wrappers in the feature selection literature has never been made. We believe the connections revealed in §2 may be useful to develop new explainers in the future. Our trained explainers from §4.2 draw inspiration from emergent communication (Lazaridou et al., 2016; Foerster et al., 2016; Havrylov and Titov, 2017). Some of our proposed ideas (e.g., using sparsemax for end-to-end differentiability) may also be relevant to that ta"
2020.blackboxnlp-1.10,P15-1144,0,0.0312033,"t a link with filters and wrappers in the feature selection literature has never been made. We believe the connections revealed in §2 may be useful to develop new explainers in the future. Our trained explainers from §4.2 draw inspiration from emergent communication (Lazaridou et al., 2016; Foerster et al., 2016; Havrylov and Titov, 2017). Some of our proposed ideas (e.g., using sparsemax for end-to-end differentiability) may also be relevant to that task. Our work is also related to sparse auto-encoders, which seek sparse overcomplete vector representations to improve model interpretability (Faruqui et al., 2015; Trifonov et al., 2018; Subramanian et al., 2018). In contrast to these works, we consider the non-zero attention probabilities as a form of explanation. Some recent work (Yu et al., 2019; DeYoung et al., 2020) advocates comprehensive rationales. While comprehensiveness could be useful in our framework to prevent trivial communication protocols between the explainer and layperson, we argue that it is not always a desirable property, since it leads to longer explanations and an increase of human cognitive load. In fact, our analysis of CSR as a function of message length (Figure 2) suggests th"
2020.blackboxnlp-1.10,D18-1407,0,0.0258104,"Missing"
2020.blackboxnlp-1.10,N18-2017,0,0.0625765,"Missing"
2020.blackboxnlp-1.10,2020.acl-main.386,0,0.179804,"Arras et al., 2017), querying the classifier with leave-one-out strategies (Li et al., 2016a; Feng et al., 2018), or training local sparse classifiers (Ribeiro et al., 2016). How should these different approaches be compared? Several diagnostic tests have been proposed: Jain and Wallace (2019) assessed the explanatory power of attention weights by measuring their correlation with input gradients; Wiegreffe and Pinter (2019) and DeYoung et al. (2020) developed more informative tests, including a combination of comprehensiveness and sufficiency metrics and the correlation with human rationales; Jacovi and Goldberg (2020) proposed a set of evaluation recommendations and a graded notion of faithfulness. Most proposed frameworks rely on correlations and counterfactual simulation, sidestepping the main practical goal of prediction explainability—the ability to communicate an explanation to a human user. 107 Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 107–118 c Online, November 20, 2020. 2020 Association for Computational Linguistics In this work, we fill the gap above by proposing a unified framework that regards explainability as a communication prob"
2020.blackboxnlp-1.10,N19-1357,0,0.284698,"paque (Goldberg and Hirst, 2017; Peters et al., 2018; Devlin et al., 2019). Despite the large body of recent work (reviewed in §7), a unified perspective modeling the human-machine interaction—a communication process in its essence—is still missing. Many methods have been proposed to generate explanations. Some neural network architectures are equipped with built-in components—attention mechanisms—which weigh the relevance of input features for triggering a decision (Bahdanau et al., 2015; Vaswani et al., 2017). Top-k attention weights provide plausible, but not always faithful, explanations (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019). Rationalizers with hard attention are arguably more faithful, but require stochastic networks, which are harder to train (Lei et al., 2016; Bastings et al., 2019). Other approaches include gradient methods (Li et al., 2016a; Arras et al., 2017), querying the classifier with leave-one-out strategies (Li et al., 2016a; Feng et al., 2018), or training local sparse classifiers (Ribeiro et al., 2016). How should these different approaches be compared? Several diagnostic tests have been proposed: Jain and Wallace (2019) assessed the explanatory"
2020.blackboxnlp-1.10,D19-3019,0,0.0663835,"Missing"
2020.blackboxnlp-1.10,D16-1011,0,0.126285,"Missing"
2020.blackboxnlp-1.10,N16-1082,0,0.637836,"generate explanations. Some neural network architectures are equipped with built-in components—attention mechanisms—which weigh the relevance of input features for triggering a decision (Bahdanau et al., 2015; Vaswani et al., 2017). Top-k attention weights provide plausible, but not always faithful, explanations (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019). Rationalizers with hard attention are arguably more faithful, but require stochastic networks, which are harder to train (Lei et al., 2016; Bastings et al., 2019). Other approaches include gradient methods (Li et al., 2016a; Arras et al., 2017), querying the classifier with leave-one-out strategies (Li et al., 2016a; Feng et al., 2018), or training local sparse classifiers (Ribeiro et al., 2016). How should these different approaches be compared? Several diagnostic tests have been proposed: Jain and Wallace (2019) assessed the explanatory power of attention weights by measuring their correlation with input gradients; Wiegreffe and Pinter (2019) and DeYoung et al. (2020) developed more informative tests, including a combination of comprehensiveness and sufficiency metrics and the correlation with human rationale"
2020.blackboxnlp-1.10,N18-1100,0,0.0141323,"C - Joint E and L Human highlights 2.7 - 96.75 - 98.50 - 89.25 - 91.50 - 2.8 2.8 58.00 83.25 93.50 83.50 70.00 83.25 78.50 83.50 Table 4: Results of the human evaluation. Reported are average message length k, human layperson CSRH /ACCH , and machine layperson CSRL /ACCL . Only explainers of the same classifier can be compared in terms of CSR. Recent works questioned the interpretative ability of attention mechanisms (Jain and Wallace, 2019; Serrano and Smith, 2019). Wiegreffe and Pinter (2019) distinguished between faithful and plausible explanations and introduced several diagnostic tools. Mullenbach et al. (2018) use human evaluation to show that attention mechanisms produce plausible explanations, consistent with our findings in §6. None of these works, however, considered the sparse selective attention mechanisms proposed in §3. Hard stochastic attention has been considered by Xu et al. (2015); Lei et al. (2016); Alvarez-Melis and Jaakkola (2017); Bastings et al. (2019), but a comparison with sparse attention and explanation strategies was still missing. Besides attention-based methods, many other explainers have been proposed using gradients (Bach et al., 2015; Montavon et al., 2018; Ding et al., 2"
2020.blackboxnlp-1.10,D16-1244,0,0.100132,"Missing"
2020.blackboxnlp-1.10,D14-1162,0,0.0823439,"Missing"
2020.blackboxnlp-1.10,P19-1146,1,0.940458,"s of the true output). We model this process as shown in Figure 1, by considering the interaction between a classifier (the model whose predictions we want to explain), an explainer (which provides the explanations), and a layperson (which must recover the classifier’s prediction). We show that different configurations of these components correspond to previously proposed explanation methods, and we experiment with explainers and laypeople being both humans and machines. Our framework also inspires two new methods: embedded explainers based on selective attention (Martins and Astudillo, 2016; Peters et al., 2019), and trainable explainers based on emergent communication (Foerster et al., 2016; Lazaridou et al., 2016). Overall, our contributions are: • We draw a link between recent techniques for explainability of neural networks and classic feature selection in linear models (§2). This leads to new embedded methods for explainability through selective, sparse attention (§3). • We propose a new framework to assess explanatory power as the communication success rate between an explainer and a layperson (§4). • We experiment with text classification, natural language inference, and machine translation, u"
2020.blackboxnlp-1.10,N18-1202,0,0.0261703,"ating post-hoc) accesses these predictions and communicates an explanation (a message m) to the layperson L. Success of the communication is dictated by the ability of ? L and C to match their predictions: y˜ = yˆ. Both the explainer and layperson can be humans or machines. Introduction The widespread use of machine learning to assist humans in decision making brings the need for explaining models’ predictions (Doshi-Velez, 2017; Lipton, 2018; Rudin, 2019; Miller, 2019). This poses a challenge in NLP, where current state-ofthe-art neural systems are generally opaque (Goldberg and Hirst, 2017; Peters et al., 2018; Devlin et al., 2019). Despite the large body of recent work (reviewed in §7), a unified perspective modeling the human-machine interaction—a communication process in its essence—is still missing. Many methods have been proposed to generate explanations. Some neural network architectures are equipped with built-in components—attention mechanisms—which weigh the relevance of input features for triggering a decision (Bahdanau et al., 2015; Vaswani et al., 2017). Top-k attention weights provide plausible, but not always faithful, explanations (Jain and Wallace, 2019; Serrano and Smith, 2019; Wie"
2020.blackboxnlp-1.10,P19-1282,0,0.254593,"t, 2017; Peters et al., 2018; Devlin et al., 2019). Despite the large body of recent work (reviewed in §7), a unified perspective modeling the human-machine interaction—a communication process in its essence—is still missing. Many methods have been proposed to generate explanations. Some neural network architectures are equipped with built-in components—attention mechanisms—which weigh the relevance of input features for triggering a decision (Bahdanau et al., 2015; Vaswani et al., 2017). Top-k attention weights provide plausible, but not always faithful, explanations (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019). Rationalizers with hard attention are arguably more faithful, but require stochastic networks, which are harder to train (Lei et al., 2016; Bastings et al., 2019). Other approaches include gradient methods (Li et al., 2016a; Arras et al., 2017), querying the classifier with leave-one-out strategies (Li et al., 2016a; Feng et al., 2018), or training local sparse classifiers (Ribeiro et al., 2016). How should these different approaches be compared? Several diagnostic tests have been proposed: Jain and Wallace (2019) assessed the explanatory power of attention weigh"
2020.blackboxnlp-1.10,W18-5422,0,0.135703,"= kh(E of C’s internal representation h. The objective function is a combination of these two terms, LΩ (φ, θ) := λΩ(θ) + L(φ, θ). We used λ = 1 in our experiments. This objective is minimized in a training set that contains pairs (x, yˆ). Therefore, in this model the message m is latent and works as a “bottleneck” for the layperson L, which does not have access to the full input x, to guess the classifier’s prediction yˆ—related models have been devised in the context of emergent communication (Lazaridou et al., 2016; Foerster et al., 2016; Havrylov and Titov, 2017) and sparse autoencoders (Trifonov et al., 2018; Subramanian et al., 2018). We minimize the objective above with gradient backpropagation. To ensure end-to-end differentiability, during this joint training we use sparsemax attention (§3) to select the relevant words in the message. One important concern in this model is to prevent E and L from learning a trivial protocol to maximize CSR. To ensure this, we forbid E from including stopwords in its messages and during training we use a linear schedule for the probability of the explainer accessing the predictions of the classifier (ˆ y ), which are hidden otherwise. At the end of training, t"
2020.blackboxnlp-1.10,D19-1002,0,0.280609,"018; Devlin et al., 2019). Despite the large body of recent work (reviewed in §7), a unified perspective modeling the human-machine interaction—a communication process in its essence—is still missing. Many methods have been proposed to generate explanations. Some neural network architectures are equipped with built-in components—attention mechanisms—which weigh the relevance of input features for triggering a decision (Bahdanau et al., 2015; Vaswani et al., 2017). Top-k attention weights provide plausible, but not always faithful, explanations (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019). Rationalizers with hard attention are arguably more faithful, but require stochastic networks, which are harder to train (Lei et al., 2016; Bastings et al., 2019). Other approaches include gradient methods (Li et al., 2016a; Arras et al., 2017), querying the classifier with leave-one-out strategies (Li et al., 2016a; Feng et al., 2018), or training local sparse classifiers (Ribeiro et al., 2016). How should these different approaches be compared? Several diagnostic tests have been proposed: Jain and Wallace (2019) assessed the explanatory power of attention weights by measuring their correla"
2020.blackboxnlp-1.10,D19-1420,0,0.060319,"produce simple and explanatory messages, in the sense that a simple model can learn with them. A more powerful layperson could potentially do well even with bad explanations. • A reconstruction term that controls the information about the classifier’s decision yˆ. We use a cross-entropy loss on the output of the layperson L, using yˆ (and not the true label y) as the ground truth: L(φ, θ) = − log pφ (ˆ y |m), where m is the output of the explainer Eθ . • A faithfulness term that encourages the explainer E to take into account the classifier’s 3 Other approaches, such as Lei et al. (2016) and Yu et al. (2019), develop rationalizers from cooperative or adversarial games between generators and encoders. However, those frameworks do not aim at explaining an external classifier. 110 decision process when producing its explanation m. This is done by adding a squared loss term ˜ θ ), hk2 where h ˜ is E’s prediction Ω(θ) = kh(E of C’s internal representation h. The objective function is a combination of these two terms, LΩ (φ, θ) := λΩ(θ) + L(φ, θ). We used λ = 1 in our experiments. This objective is minimized in a training set that contains pairs (x, yˆ). Therefore, in this model the message m is latent"
2020.blackboxnlp-1.10,D08-1004,0,0.0593976,"lity of neural networks and classic feature selection in linear models (§2). This leads to new embedded methods for explainability through selective, sparse attention (§3). • We propose a new framework to assess explanatory power as the communication success rate between an explainer and a layperson (§4). • We experiment with text classification, natural language inference, and machine translation, using different configurations of explainers and laypeople, both machines (§5) and humans (§6). 2 Revisiting Feature Selection A common way of generating explanations is by highlighting rationales (Zaidan and Eisner, 2008). The principle of parsimony (“Occam’s razor”) advocates simple explanations over complex ones. This principle inspired a large body of work in traditional feature selection for linear models. We draw here a link between that work and modern approaches to explainability. Table 1 highlights the connections. Traditional feature selection methods (Guyon and Elisseeff, 2003) are mostly concerned with model interpretability, i.e., understanding how models behave 108 globally. Feature selection happens statically during model training, after which irrelevant features are permanently deleted from the"
2020.blackboxnlp-1.10,J90-1003,0,\N,Missing
2020.blackboxnlp-1.10,N16-3020,0,\N,Missing
2020.eamt-1.22,W17-4772,0,0.042315,"Missing"
2020.eamt-1.22,W16-2377,0,0.0616652,"Missing"
2020.eamt-1.22,W18-6452,0,0.0182467,"Martins (2019) is a monotonic model following the sequence-to-sequence architecture and pre-trained on BERT (seq2seq BERT). B´erard et al. (2017) predict a sequence of actions in a left-to-right order. path. We compare different ground-truth action sequences based on minimum edit actions, all arriving at the same pe:  = 0.1 (Pereyra et al., 2017). We use batch size of 512 tokens and save checkpoints every 10,000 steps. • Left-to-right (l2r); 5 • Randomly shuffled (shuff ); We compare the effect of using different action orders on the development set and test sets of WMT 2018 APE shared task (Chatterjee et al., 2018). By using only training data overlapping with WMT’s training sets (as described in §2.1), we are able to use WMT’s development and test sets for evaluation. This allows to compare the performance of our model with that of previous submissions. Note however that our systems are in disadvantage, due to being trained on fewer data: out of the original 23,000 training samples we only found 16,068 in Specia et al. (2017). • Human-ordered (h-ord). Minimum edit actions are generated using the dynamic programming algorithm to compute Levenshtein distance. The algorithm is set to output left-to-right"
2020.eamt-1.22,P19-1292,1,0.791822,"rs and use them to train an automatic postediting system. We compare the resulting system with those trained with left-to-right and random post-editing orderings. We observe that humans tend to follow a nearly left-to-right order, but with interesting deviations, such as preferring to start by correcting punctuation or verbs. 1 Introduction Neural sequence generation models have been widely adopted for tasks such as machine translation (MT) (Bahdanau et al., 2015; Vaswani et al., 2017) and automatic post-editing of translations (Junczys-Dowmunt and Grundkiewicz, 2016; Chatterjee et al., 2016; Correia and Martins, 2019; Lopes et al., 2019). These models typically generate one word at a time, and rely on a factorizac 2020 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. * Work partly done during a research visit at New York University. 0: Die LMS ge¨offnet ist . 1: Die LMS ist ge¨offnet ist . 2: Die LMS ist ge¨offnet . [ I:2:ist ] [ D:4:ist ] Table 1: Example of a small post-edit from the training set. Each action is represented by three features: its type (I for insert and D for delete), its position in the sentence and the token to inser"
2020.eamt-1.22,N19-1423,0,0.0228577,"ans counts by left-to-right counts, discard words with a difference lower than 5, and group results by part-ofspeech tag. 3 Model Inspired by recent work in non-monotonic generation (Stern et al., 2019; Gu et al., 2019a; Emelianenko et al., 2019), we propose a model that receives a src, mt pair and outputs one action at a time. When a new action is predicted, there is no explicit memory of previous time-steps. The model can only observe the current state of the mt, which may have been changed by previous actions of the model. This model is based on a Transformer-Encoder pre-trained with BERT (Devlin et al., 2019). After producing one hidden state for each token, a linear transformation outputs two values per token: the logit of deleting that token and of inserting a word to its right. Out of all possible DEL or INS positions, the most likely operation is selected. A special operation is reserved to represent End-ofDecoding. If an INS operation is chosen, we still need to choose which token to insert. Another linear transformation is applied to the hidden state of the chosen position. We obtain a distribution over the vocabulary and select the most likely token. Figure 3 illustrates this procedure. Aft"
2020.eamt-1.22,W19-6605,1,0.893081,"Missing"
2020.eamt-1.22,W16-2378,0,0.144712,"this paper, we analyze the orderings produced by human post-editors and use them to train an automatic postediting system. We compare the resulting system with those trained with left-to-right and random post-editing orderings. We observe that humans tend to follow a nearly left-to-right order, but with interesting deviations, such as preferring to start by correcting punctuation or verbs. 1 Introduction Neural sequence generation models have been widely adopted for tasks such as machine translation (MT) (Bahdanau et al., 2015; Vaswani et al., 2017) and automatic post-editing of translations (Junczys-Dowmunt and Grundkiewicz, 2016; Chatterjee et al., 2016; Correia and Martins, 2019; Lopes et al., 2019). These models typically generate one word at a time, and rely on a factorizac 2020 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. * Work partly done during a research visit at New York University. 0: Die LMS ge¨offnet ist . 1: Die LMS ist ge¨offnet ist . 2: Die LMS ist ge¨offnet . [ I:2:ist ] [ D:4:ist ] Table 1: Example of a small post-edit from the training set. Each action is represented by three features: its type (I for insert and D for delete),"
2020.eamt-1.22,P17-4012,0,0.0177875,"f the chosen action is an INS, we obtain a distribution over the vocabulary by applying another linear transformation to H’s row of the chosen action position. sentence, we obtain the logit of INS (on the position to its right) and DEL (of the token itself) using a learnable matrix W ∈ Rh×2 . The distribution probability over all possible edit-operations is defined as: • When the model enters a loop; • When a limit of 50 actions is reached. Once decoding ends, the model outputs the final post-edited mt. Model details. We use BERT’s implementation from Wolf et al. (2019) together with OpenNMT (Klein et al., 2017), both based on PyTorch (Paszke et al., 2019). The pretrained BERT-Encoder contains 12 layers, embedding size and hidden size of 768. We begin with an input sequence x. This sequence is the concatenation of: src + [SEP] + &lt;S&gt; + mt + &lt;T&gt; where &lt;S&gt; and &lt;T&gt; are auxiliary tokens used to allow INS in position 0 and to represent End-ofDecoding. Tokens before and after [SEP] have a different segment embedding, to help differentiate between src and mt tokens. Let N be the length of x after applying a BERT pre-trained tokenizer (Wolf et al., 2019). This sequence is the input of the BERT-Encoder with hi"
2020.eamt-1.22,P07-2045,0,0.00552749,"e average count of actions computed from human keystrokes. Both average action counts exclude samples with zero actions. training set with keystroke logging information. Out of 23,000 training samples provided by the WMT 2016-17 shared tasks, 16,068 are also present in the dataset from Specia et al. (2017). This intersection is obtained by requiring the same triplet (src, mt, pe) to be present in both datasets. Since the WMT dataset comes already pre-processed, the following pre-processing is applied to the dataset containing keystrokes, to increase their intersection: using tools from Moses (Koehn et al., 2007), we apply En punctuationnormalization to the whole triplet, followed by tokenization of the corresponding language (either En or De). Additionally, we preprocess the raw keystrokes to obtain word-level DEL and INS actions (detailed in §2.2). Table 2 shows statistics from WMT’s original data and training set after intersecting with the keystrokes dataset from Specia et al. (2017). We denote by min-edit the average count of DEL and INS obtained from the Levenshtein distance. Average count of human actions (human-edit) is only available for the subset of the training data found in the keystrokes"
2020.eamt-1.22,2012.amta-wptp.3,0,0.0541014,"everaging on pre-trained models (Correia and Martins, 2019) and using conservativeness penalties (Lopes et al., 2019) to avoid over-editing. B´erard et al. (2017) post-edit by predicting a sequence of actions with an imposed left-to-right order. Another recent work directly models edits, without including order information but allowing to re-use edits in unseen contexts (Yin et al., 2019). Human post-editing. Previous work has explored keystrokes to understand the behavior of human editors. O’Brien (2006) investigates the relationship between pauses and cognitive effort, while later research (Lacruz et al., 2012; Lacruz and Shreve, 2014) examines keystroke logs for the same effect. Specia et al. (2017) introduce a dataset of human post-edits, containing information on keytrokes. Recently it was shown that detailed information from post-editing, such as sequences of edit-operations combined with mouseclicks and waiting times, contain structured information (G´ois and Martins, 2019). The same work provides evidence that this kind of information allows to identify and profile editors, and may be helpful in downstream tasks. 7 Conclusions In this work we explored different ways to order the edit operatio"
2020.eamt-1.22,D19-1001,0,0.0123917,"arching for orders that maximize the sequence likelihood, given the current model parameters. Emelianenko et al. (2019) train using sampled orders instead, to better escape local optima. They also drop the relative attention mechanism together with its better theoretical bound on time complexity – showing that, in practice, inference remains feasible. Welleck et al. (2019) propose a model that generates text as a binary tree. They learn order from a uniform distribution that slowly shifts to search among the model’s own preferences, or alternatively using a deterministic left-to-right oracle. Lawrence et al. (2019) use placeholders to represent yet-to-insert tokens, allowing for bidirectional attention without exposing future tokens. Decoding is either done left-to-right or by picking the most certain prediction. Alternatively all tokens can be decided in parallel, but with significant loss in performance. Non-autoregressive models. Another class of models focuses on parallel decoding of multiple tokens, moving away from the traditional autoregressive paradigm. This unlocks faster inference, but brings the difficult challenge of learning dependencies between tokens (Gu et al., 2018). Stern et al. (2019)"
2020.eamt-1.22,Q19-1042,1,0.915618,". 1: Die LMS ist ge¨offnet ist . 2: Die LMS ist ge¨offnet . [ I:2:ist ] [ D:4:ist ] Table 1: Example of a small post-edit from the training set. Each action is represented by three features: its type (I for insert and D for delete), its position in the sentence and the token to insert/delete. In this example, the token marked in red needs to be removed since it is incorrectly placed. The blue token is inserted to obtain the correct pe. tion that imposes a left-to-right generation ordering. Recent alternatives allow for different generation orderings (Welleck et al., 2019; Stern et al., 2019; Gu et al., 2019a), or even for parallel generation of multiple tokens (Gu et al., 2018; Stern et al., 2019; Gu et al., 2019b; Zhou et al., 2020), which allows exploiting dependencies among nonconsecutive tokens. One potential difficulty when training non-monotonic models is how to learn a good generation ordering. There are exponentially many valid orderings to generate a given sequence, and a model should prefer those that lead to accurate translations and can be efficiently learned. In previous work, to guide the search for a good ordering, oracle policies have been provided (Welleck et al., 2019), or anot"
2020.eamt-1.22,W19-5413,1,0.890645,"Missing"
2020.eamt-1.22,W07-0728,0,0.152433,"umans typing. It is known that edit operations performed by human translators are not arbitrary (G´ois and Martins, 2019). But it is not known how the orderings preferred by humans look like, or how they compare to orders learned by models. To investigate this question, we propose a model that learns generation orderings in a supervised manner from human keystrokes. Since a human is free to move back and forth arbitrarily while editing text, the chosen order of operations can be used as an additional learning signal. More specifically, we do this in the context of automatic postediting (APE) (Simard et al., 2007). APE consists in improving the output of a blackbox MT system by automatically fixing its mistakes. The act of post-editing text can be fully specified as a sequence of delete (DEL) and insert (INS) actions in given positions. Furthermore, if we do not include redundant actions in a sequence, that sequence can be arbitrarily reordered while still producing the same output. For instance, in Table 1, we can switch the order of the two actions, as long as we rectify to delete position 3 instead of position 4. We compare a model trained with human orderings to others trained with left-to-right an"
2020.eamt-1.22,W19-3620,1,0.814701,"Missing"
2020.eamt-1.24,N18-1118,1,0.938079,"aluation setup We perform both automatic and manual evaluation, in order to gain more insights into the differences between the models. Automatic evaluation: We first evaluate all methods with case-sensitive detokenized BLEU (Papineni et al., 2002).8 We then evaluate context-dependent discourse-level phenomena using the previously described contrastive test sets. For EN→DE this corresponds to the large-scale anaphoric pronoun test set of M¨uller et al. (2018) and for EN→FR our own analogous large-scale anaphoric pronoun test set (described in §5),9 as well as the manually crafted test sets of Bawden et al. (2018) for anaphora and coherence/cohesion. Manual evaluation: In the case of the chat translation task (using proprietary data), in addition to BLEU, we also manually assess the performance of the systems with professional human annotators, who mark the errors of the systems with different levels of severity (i.e. minor, major, critical). In the case of extra-sentential errors such as agreements we asked them to mark both the pronoun and its antecedent. We score the systems’ performance using Multidimensional Quality Metrics (MQM) (Lommel, 2013): M QM =100 − 8 minor + major ∗ 5 + critical ∗ 10 Word"
2020.eamt-1.24,W19-5321,0,0.0266307,"e queried when translating subsequent sentences. This is one of the first approaches that uses the global context. Other methods have been proposed to use both source and target history with different ranges of context. (Miculicich et al., 2018) attends to words from previous sentences with a 2-stage hierarchical approach, while (Maruf et al., 2019), similarly, attends to words in specific sentences using sparse hierarchical selective attention. (Voita et al., 2019a), which extends the concatenationbased approach to four sentences in a monolingual Automatic Post-Edition (APE) setting; whereas Junczys-Dowmunt (2019) proposes full document concatenation with a BERT model to improve the word embeddings through document context and full document APE. Ng et al. (2019) proposes a noisy channel approach with reranking, where the language model (LM) operates at document-level but the reranking does not. Yu et al. (2019) extends the previous work using conditionally dependent sentence reranking with the document-level LM. #Prev #Fut Src Concat2to1 (1) Concat2to2 (1) Multi-source context encoder (2) Cache-based (3) Star (4) - (see §4) 1 1 2 all all all (src) X X X X X Target APE (5) Sparse Hierarchical attn. (6)"
2020.eamt-1.24,P17-4012,0,0.0382818,"R, and EN→PT br respectively). We then separately finetune them to each domain. For the document MT task, we consider EN→DE and EN→FR and finetune on IWSLT17 (Cettolo et al., 2012) TED Talks, using the test sets 2011-2014 as dev sets, and 2015 as test sets. For the chat MT task, we finetune on (anonymized) proprietary data of 3 different domains and on an additional language pair (EN→PT br). Dataset sizes are shown in Table 3 (sentence-level pre-training data) and Tables 4–5 (document and chat task data respectively). EN-DE EN-FR EN-PT br Train Dev 18M 20M 5M 1K 1K 1K implemented in Open-NMT (Klein et al., 2017). 6.3 Table 3: Sentence-level corpus sizes (#sentences) EN-DE EN-FR Train Dev Test 206K 233K 5.4K 5.8k 1.1K 1.2K Table 4: TED talks document-level corpus sizes (#sentences) Domain1 Domain2 Domain3 EN-DE Train Dev Test 674k 37K 35K 62K 3.2K 3.6K 13K 0.6K 0.7K EN-FR Train Dev Test 395K 21K 22K 108K 6.3K 6.2K 110K 6.1K 6.3K EN-PT br Train Dev Test 235K 13K 13K 61K 3.4K 3.2K 13K 0.7K 0.7K Table 5: The corpora sizes of the chat translation task. We consider both speakers for this count. 6.2 Training Configuration For all experiments we use the Transformer base configuration (hidden size of 512, fee"
2020.eamt-1.24,2012.eamt-1.60,0,0.0197701,"and they for FR and PT br). We compare the three previous methods (§3) plus the Doc-Star-Transformer in two scenarios: (i) document MT, testing on TED talks (EN→FR and EN→PT br), and (ii) chat MT testing on proprietary conversation data for all three directions. 6.1 Data For both scenarios, we pre-train baseline models on large amounts of publicly available sentence-level parallel data (∼18M , ∼22M and ∼5M sentence pairs for EN→DE, EN→FR, and EN→PT br respectively). We then separately finetune them to each domain. For the document MT task, we consider EN→DE and EN→FR and finetune on IWSLT17 (Cettolo et al., 2012) TED Talks, using the test sets 2011-2014 as dev sets, and 2015 as test sets. For the chat MT task, we finetune on (anonymized) proprietary data of 3 different domains and on an additional language pair (EN→PT br). Dataset sizes are shown in Table 3 (sentence-level pre-training data) and Tables 4–5 (document and chat task data respectively). EN-DE EN-FR EN-PT br Train Dev 18M 20M 5M 1K 1K 1K implemented in Open-NMT (Klein et al., 2017). 6.3 Table 3: Sentence-level corpus sizes (#sentences) EN-DE EN-FR Train Dev Test 206K 233K 5.4K 5.8k 1.1K 1.2K Table 4: TED talks document-level corpus sizes ("
2020.eamt-1.24,P19-1285,0,0.0278369,"re 1: Doc-Star-Transformer encoder. The Doc-Star-Transformer decoder follows a similar structure to the encoder, except that the decoder does not have access to the sentence representation of the current sentence k, thus, remov(k) ing sentence si from (5). Source-side context is added through concatenation of the previous sentence embeddings from the final layer of the encoder with the decoder’s in (5). 4.2 Sentence-level Recurrence To overcome practical memory constraints (due to very long documents), we introduce a sentencelevel recurrence mechanism with state reuse, similar to that used by Dai et al. (2019). During training, a constant number of sentence embeddings are cached to provide context when translating the next segment. We cut off gradients to these cached sentence embeddings, but allow them to the decoder side. In the document-MT setting, (5) concatenates all sentences’ representations to include context from future source-side sentences during translation. Evaluating Context-Aware NMT The evaluation of context-aware MT is notoriously tricky (Hardmeier, 2012); standard automatic metrics such as B LEU (Papineni et al., 2002) are poorly suited to evaluating discourse phenomena (e.g. anap"
2020.eamt-1.24,N13-1073,0,0.0423232,"e found in any number of previous sentences. EN→FR: large-scale pronoun test set We automatically create a large-scale EN→FR test set from OpenSubtitles2018 (Lison et al., 2018) in the style of ContraPro, with some modifications to their protocol due to the limited quality of available tools. The test set is created as follows: 1. Instances of it and they and their antecedents are detected using NEURALCOREF.5 Unlike M¨uller et al. (2018), we only run English coreference due to a lack of an adequate French tool. 2. We align pronouns to their translations (il, elle, ils, elles) using FastAlign (Dyer et al., 2013). 3. Examples are filtered to only include subject pronouns (using Spacy6 ) with a nominal antecedent, aligned to a nominal French antecedent matching the pronoun’s gender. We also remove examples whose antecedent is more than five sentences away to avoid cases of imprecise coreference resolution. 4. Contrastive translations are created by inverting the pronouns’ gender (cf. Figure 2). We modify the gender of words that agree with the pronoun (e.g. adjectives and some past participles) using the Lefff lexicon (Sagot, 2010)). The test set consists of 3,500 examples for each target pronoun type"
2020.eamt-1.24,P07-2045,0,0.0152483,"Missing"
2020.eamt-1.24,D18-1512,0,0.16162,"Missing"
2020.eamt-1.24,L18-1275,0,0.0337944,"Missing"
2020.eamt-1.24,2013.tc-1.6,0,0.0518834,"9 as well as the manually crafted test sets of Bawden et al. (2018) for anaphora and coherence/cohesion. Manual evaluation: In the case of the chat translation task (using proprietary data), in addition to BLEU, we also manually assess the performance of the systems with professional human annotators, who mark the errors of the systems with different levels of severity (i.e. minor, major, critical). In the case of extra-sentential errors such as agreements we asked them to mark both the pronoun and its antecedent. We score the systems’ performance using Multidimensional Quality Metrics (MQM) (Lommel, 2013): M QM =100 − 8 minor + major ∗ 5 + critical ∗ 10 Word count Using Moses’ (Koehn et al., 2007) multi-bleu-detok. For both large-scale test sets, we make sure to exclude the documents they include from the training data. 9 By having access to the full conversation, the annotators can annotate both intra- and extrasentential errors (e.g. document-level error examples of agreement or lexical consistency). We prioritize documents with a large number of edits compared to the sentence-level baseline (normalized by document length) due to documentlevel systems tending to perform few edits with respec"
2020.eamt-1.24,N19-1313,1,0.950926,"slating sentences in context (i.e. at the document level) is essential for correctly handling discourse phenomena whose scope can go beyond the current sentence and which therefore require document context (Hardmeier, 2012; Bawden, 2018; Wang, 2019). Important examples include anaphora, lexical coherence and cohesion, deixis and ellipsis; crucial aspects in delivering high quality translations which often are poorly evaluated using standard automatic metrics. Numerous context-aware neural MT (NMT) approaches have been proposed in recent years (Tiedemann and Scherrer, 2017; Zhang et al., 2018; Maruf et al., 2019; Miculicich et al., 2018; Voita et al., 2019b; Tu et al., 2018), integrating source-side and sometimes target-side context. However, they have often been evaluated on different languages, datasets, and model sizes. Certain models have also previously been trained on few sentence pairs rather than in more realistic, highresource scenarios. A direct comparison and analysis of the methods, particularly concerning their individual strengths and weaknesses on different language pairs is therefore currently lacking. We fill these gaps by comparing a representative set of context-aware NMT solutions"
2020.eamt-1.24,W17-4814,0,0.0246347,"ing source- and target-side context. As such, current state-of-the-art NMT systems optimize the negative log-likelihood of the sentences: p(y (k) |x (k) )= n Y (k) (k) p(yt |y&lt;t , x(k) ), (1) t=1 where x(k) and y (k) are the k th source and target (k) training sentences, and yt is the tth token in y (k) . In this paper, the underlying architecture is a Transformer (Vaswani et al., 2017). Transformers are usually applied to sentence-level translation, using the sentence independence assumption above. This assumption precludes these systems from learning inter-sentential phenomena. For example, Smith (2017) analyzes certain discourse phenomena that sentence-level MT systems cannot capture, such as obtaining consistency and lexical coherence of named entities, among others. 2.2 Context-aware NMT Context-aware NMT relaxes the independence assumption of sentence-level NMT; each sentence is translated by conditioning on the current source sentence as well as other sentence pairs (source and target) in the same document. More formally, given a document D containing K sentence pairs {(x(1) , y (1) ), (x(2) , y (2) ), . . . , (x(K) , y (K) )}, the probability of translating x(k) into y (k) is: Chat tra"
2020.eamt-1.24,D18-1325,0,0.315614,"context (i.e. at the document level) is essential for correctly handling discourse phenomena whose scope can go beyond the current sentence and which therefore require document context (Hardmeier, 2012; Bawden, 2018; Wang, 2019). Important examples include anaphora, lexical coherence and cohesion, deixis and ellipsis; crucial aspects in delivering high quality translations which often are poorly evaluated using standard automatic metrics. Numerous context-aware neural MT (NMT) approaches have been proposed in recent years (Tiedemann and Scherrer, 2017; Zhang et al., 2018; Maruf et al., 2019; Miculicich et al., 2018; Voita et al., 2019b; Tu et al., 2018), integrating source-side and sometimes target-side context. However, they have often been evaluated on different languages, datasets, and model sizes. Certain models have also previously been trained on few sentence pairs rather than in more realistic, highresource scenarios. A direct comparison and analysis of the methods, particularly concerning their individual strengths and weaknesses on different language pairs is therefore currently lacking. We fill these gaps by comparing a representative set of context-aware NMT solutions under the same experimen"
2020.eamt-1.24,W17-4811,0,0.318285,"e, no derivative works, attribution, CCBY-ND. Translating sentences in context (i.e. at the document level) is essential for correctly handling discourse phenomena whose scope can go beyond the current sentence and which therefore require document context (Hardmeier, 2012; Bawden, 2018; Wang, 2019). Important examples include anaphora, lexical coherence and cohesion, deixis and ellipsis; crucial aspects in delivering high quality translations which often are poorly evaluated using standard automatic metrics. Numerous context-aware neural MT (NMT) approaches have been proposed in recent years (Tiedemann and Scherrer, 2017; Zhang et al., 2018; Maruf et al., 2019; Miculicich et al., 2018; Voita et al., 2019b; Tu et al., 2018), integrating source-side and sometimes target-side context. However, they have often been evaluated on different languages, datasets, and model sizes. Certain models have also previously been trained on few sentence pairs rather than in more realistic, highresource scenarios. A direct comparison and analysis of the methods, particularly concerning their individual strengths and weaknesses on different language pairs is therefore currently lacking. We fill these gaps by comparing a represent"
2020.eamt-1.24,W18-6312,0,0.0474662,"uation in terms of Multidimensional Quality Metrics (MQM) and analyse the correlation of the results obtained by the automatic metrics with human judgments. 1 Introduction There has been undeniable progress in Machine Translation (MT) in recent years, so much so that for certain languages and domains, when sentences are evaluated in isolation, it has been suggested that MT is on par with human translation (Hassan et al., 2018). However, it has been shown that human translation clearly outperforms MT at the document level, when the whole translation is taken into account (L¨aubli et al., 2018; Toral et al., 2018; Laubli et al., 2020). For example, the Conference on Machine Translation (WMT) now considers inter-sentential translations in their shared task (Barrault et al., 2019). This sets a demand for context-aware machine translation: systems that take the context into account when translating, as opposed to translating sentences independently. © 2020 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. Translating sentences in context (i.e. at the document level) is essential for correctly handling discourse phenomena whose scope can"
2020.eamt-1.24,W18-6307,0,0.0643728,"Missing"
2020.eamt-1.24,Q18-1029,0,0.52835,"tial for correctly handling discourse phenomena whose scope can go beyond the current sentence and which therefore require document context (Hardmeier, 2012; Bawden, 2018; Wang, 2019). Important examples include anaphora, lexical coherence and cohesion, deixis and ellipsis; crucial aspects in delivering high quality translations which often are poorly evaluated using standard automatic metrics. Numerous context-aware neural MT (NMT) approaches have been proposed in recent years (Tiedemann and Scherrer, 2017; Zhang et al., 2018; Maruf et al., 2019; Miculicich et al., 2018; Voita et al., 2019b; Tu et al., 2018), integrating source-side and sometimes target-side context. However, they have often been evaluated on different languages, datasets, and model sizes. Certain models have also previously been trained on few sentence pairs rather than in more realistic, highresource scenarios. A direct comparison and analysis of the methods, particularly concerning their individual strengths and weaknesses on different language pairs is therefore currently lacking. We fill these gaps by comparing a representative set of context-aware NMT solutions under the same experimental settings, providing: • A systematic"
2020.eamt-1.24,W19-5333,0,0.032358,"th source and target history with different ranges of context. (Miculicich et al., 2018) attends to words from previous sentences with a 2-stage hierarchical approach, while (Maruf et al., 2019), similarly, attends to words in specific sentences using sparse hierarchical selective attention. (Voita et al., 2019a), which extends the concatenationbased approach to four sentences in a monolingual Automatic Post-Edition (APE) setting; whereas Junczys-Dowmunt (2019) proposes full document concatenation with a BERT model to improve the word embeddings through document context and full document APE. Ng et al. (2019) proposes a noisy channel approach with reranking, where the language model (LM) operates at document-level but the reranking does not. Yu et al. (2019) extends the previous work using conditionally dependent sentence reranking with the document-level LM. #Prev #Fut Src Concat2to1 (1) Concat2to2 (1) Multi-source context encoder (2) Cache-based (3) Star (4) - (see §4) 1 1 2 all all all (src) X X X X X Target APE (5) Sparse Hierarchical attn. (6) 3 all 3 - X Trg 4 Doc-Star-Transformer We propose a scalable approach to document-level NMT inspired by the Star architecture (Guo et al., 2019) for se"
2020.eamt-1.24,P02-1040,0,0.106395,"Missing"
2020.eamt-1.24,D19-1081,0,0.234871,"Missing"
2020.eamt-1.24,E17-2025,0,0.0290662,"layers, 8 attention heads) with the learning rate schedule described in (Vaswani et al., 2017). We use label smoothing with an epsilon value of 0.1 (Pereyra et al., 2017) and early stopping of 5 consecutive non-improving validation points of both accuracy and perplexity. Selfattentive models are sensitive to batch size (Popel and Bojar, 2018), and so we use batches of 32k tokens for all methods.7 For all tasks, we use a subword unit vocabulary (Sennrich et al., 2016) with 32k operations. We share source and target embeddings, as well as target embeddings with the final vocab projection layer (Press and Wolf, 2017). For the document translation experiments, we run the same experimental setting with 3 different seeds and average the scores of each model. For the approaches that fine-tune just the document-level parameters (i.e. cache-based, multi-source encoder, and Doc-Star-Transformer), we reset all optimizer states and train with the same configuration as the baselines (with the base parameters frozen), as described in (Tu et al., 2018; Zhang et al., 2018). For Doc-Star-Transformer we use multi-heads of 2 and 8 heads. All methods are 7 The optimizer update is delayed to simulate the 32k tokens. Chat-s"
2020.eamt-1.24,W17-4702,0,0.0487383,"Missing"
2020.eamt-1.24,sagot-2010-lefff,0,0.0349041,"uns to their translations (il, elle, ils, elles) using FastAlign (Dyer et al., 2013). 3. Examples are filtered to only include subject pronouns (using Spacy6 ) with a nominal antecedent, aligned to a nominal French antecedent matching the pronoun’s gender. We also remove examples whose antecedent is more than five sentences away to avoid cases of imprecise coreference resolution. 4. Contrastive translations are created by inverting the pronouns’ gender (cf. Figure 2). We modify the gender of words that agree with the pronoun (e.g. adjectives and some past participles) using the Lefff lexicon (Sagot, 2010)). The test set consists of 3,500 examples for each target pronoun type (cf. Table 2 for the distribution of coreference distances). 6 Experimental Setup As mentioned in §1, we aim to provide a systematic comparison of the approaches over the same 4 https://github.com/ZurichNLP/ContraPro https://github.com/huggingface/neuralcoref 6 https://spacy.io 5 Context sentence Pies made from apples like these. Des tartesf faites avec des pommes comme celles-ci Current sentence Oh, they do look delicious.  Ellesf ont l’air d´elicieux. × Ilsm ont l’air d´elicieux. Figure 2: An example from the large-scal"
2020.eamt-1.24,P16-1162,0,0.139798,"this count. 6.2 Training Configuration For all experiments we use the Transformer base configuration (hidden size of 512, feedforward size of 2048, 6 layers, 8 attention heads) with the learning rate schedule described in (Vaswani et al., 2017). We use label smoothing with an epsilon value of 0.1 (Pereyra et al., 2017) and early stopping of 5 consecutive non-improving validation points of both accuracy and perplexity. Selfattentive models are sensitive to batch size (Popel and Bojar, 2018), and so we use batches of 32k tokens for all methods.7 For all tasks, we use a subword unit vocabulary (Sennrich et al., 2016) with 32k operations. We share source and target embeddings, as well as target embeddings with the final vocab projection layer (Press and Wolf, 2017). For the document translation experiments, we run the same experimental setting with 3 different seeds and average the scores of each model. For the approaches that fine-tune just the document-level parameters (i.e. cache-based, multi-source encoder, and Doc-Star-Transformer), we reset all optimizer states and train with the same configuration as the baselines (with the base parameters frozen), as described in (Tu et al., 2018; Zhang et al., 201"
2020.eamt-1.24,P19-1116,0,0.528569,"ument level) is essential for correctly handling discourse phenomena whose scope can go beyond the current sentence and which therefore require document context (Hardmeier, 2012; Bawden, 2018; Wang, 2019). Important examples include anaphora, lexical coherence and cohesion, deixis and ellipsis; crucial aspects in delivering high quality translations which often are poorly evaluated using standard automatic metrics. Numerous context-aware neural MT (NMT) approaches have been proposed in recent years (Tiedemann and Scherrer, 2017; Zhang et al., 2018; Maruf et al., 2019; Miculicich et al., 2018; Voita et al., 2019b; Tu et al., 2018), integrating source-side and sometimes target-side context. However, they have often been evaluated on different languages, datasets, and model sizes. Certain models have also previously been trained on few sentence pairs rather than in more realistic, highresource scenarios. A direct comparison and analysis of the methods, particularly concerning their individual strengths and weaknesses on different language pairs is therefore currently lacking. We fill these gaps by comparing a representative set of context-aware NMT solutions under the same experimental settings, provid"
2020.eamt-1.24,D18-1049,0,0.230591,"Missing"
2020.eamt-1.67,P19-1292,1,0.843387,"Missing"
2020.eamt-1.67,D19-1223,1,0.867072,"Missing"
2020.eamt-1.67,N19-1423,0,0.0606264,"Missing"
2020.eamt-1.67,W19-6605,1,0.882848,"Missing"
2020.eamt-1.67,P19-3020,1,0.883601,"Missing"
2020.eamt-1.67,P18-2059,1,0.900126,"Missing"
2020.eamt-1.67,D17-1036,1,0.88373,"Missing"
2020.eamt-1.67,N19-1313,1,0.870898,"Missing"
2020.eamt-1.67,P19-2049,1,0.857481,"Missing"
2020.eamt-1.67,P19-1146,1,0.888014,"Missing"
2020.eamt-1.68,W16-3609,0,0.0544569,"Missing"
2020.emnlp-main.171,N19-1115,0,0.0129199,"interpretation and insights would trigger future latent structure research. The code for the paper is available on https: //github.com/deep-spin/understanding-spigot. Discrete latent variable learning is often tackled in stochastic computation graphs, by estimating the gradient of an expected loss. An established method is the score function estimator (SFE) (Glynn, 1990; Williams, 1992; Kleijnen and Rubinstein, 1996). SFE is widely used in NLP, for tasks including minimum risk training in NMT (Shen et al., 2016; Wu et al., 2018) and latent linguistic structure learning (Yogatama et al., 2017; Havrylov et al., 2019). In this paper, we focus on the alternative strategy of surrogate gradients, which allows learning in deterministic graphs with discrete, argmax-like nodes, rather than in stochastic graphs. Examples are the straight-through estimator (STE) (Hinton, 2012; Bengio et al., 2013) and the structured projection of intermediate gradients optimization technique (SPIGOT; Peng et al. 2018). Recent work focuses on studying and explaining STE. Yin et al. (2019) obtained a convergence result in shallow networks for the unstructured case. Cheng et al. (2018) show that STE can be interpreted as the simulati"
2020.emnlp-main.171,D14-1162,0,0.101899,"e arc-factored dependency tree for the sentence, then uses the tree in predicting the downstream binary sentiment label. The model has the following components: • Encoder: Computes a score for every possible dependency arc i → j between words i and j. Each word is represented by its embedding hi ,3 then processed by an LSTM, yielding contextual ← → vectors hi . Then, arc scores are computed as 2 With relaxed methods, the V-measure is always calculated using the argmax, even though gθ sees a continuous relaxation. 2193  ← → ← → si→j = v &gt; tanh W &gt; [ hi ; hj ] + b . 3 Pretrained GloVe vectors (Pennington et al., 2014). (23) Model SST Valid. Acc. Test Acc. SNLI Valid. Acc. Test Acc. Baseline 83.79±0.17 83.99±0.32 85.54±0.14 85.09±0.21 Relaxed Marginals SparseMAP 84.43±0.27 83.94±0.41 83.45±0.56 83.61±0.33 85.60±0.11 85.54±0.10 85.01±0.11 85.35±0.06 Argmax *Perturb-and-MAP STE-S STE-I SPIGOT SPIGOT-CE SPIGOT-EG 84.06±0.59 83.25±0.83 83.44±0.70 84.51±0.80 82.22±0.61 82.94±1.06 82.92±0.61 83.32±0.88 83.17±0.11 84.80±1.10 83.01±0.55 82.88±0.90 84.62±0.14 82.07±0.50 81.39±0.63 84.03±0.28 80.22±1.02 85.36±0.16 83.80±0.06 81.10±0.65 81.00±0.32 83.52±0.24 79.20±0.68 84.84±0.16 Table 3: SST and SNLI average accuracy"
2020.emnlp-main.171,D07-1043,0,0.0239819,"compare stochastic methods, including score function estimators (with an optional moving average control variate), and the two Gumbel estimator variants (Jang et al., 2017; Maddison et al., 2017): GumbelSoftmax with relaxed softmax in the forward pass, and the other using argmax in the style of STE (hence dubbed ST-Gumbel). Results. We compare the discussed methods in Table 2. Knowledge of the data-generating process allows us to measure not only downstream accuracy, but also clustering quality, by comparing the model predictions with the known true z. We measure the latter via the V-measure (Rosenberg and Hirschberg, 2007), a clustering score independent of the cluster labels, i.e., invariant to permuting the 2192 100 80 80 valid. v-measure valid. accuracy 90 Linear Gold clusters Softmax STE-I 60 70 40 60 SPIGOT SPIGOT-CE SPIGOT-EG 20 50 0 2000 4000 6000 epoch 8000 0 10000 0 2000 4000 6000 epoch 8000 10000 Figure 2: Learning curves on synthetic data with 10 clusters. Softmax learns the downstream task fast, but mixes the clusters, yielding poor V-measure. SPIGOT fails on both metrics; STE-I and the novel SPIGOT-CE work well. Deterministic models may be preferable when likelihood assessment or sampling is not tr"
2020.emnlp-main.171,2020.acl-demos.38,0,0.0188784,"*. • Latent parser: We use the arc scores vector s to get a parse zˆ = ρ(s) for the sentence, where ρ(s) is the argmax, or combination of trees, such as Marg or SparseMAP. • Decoder: Following Peng et al. (2018), we ← → concatenate each hi with its predicted head ← → h head(i) . For relaxed methods, we average all possible heads, weighted by the corresponding P ← → ← → marginal: h head(i) := j µi→j hj . The concatenation is passed through an affine layer, a ReLU activation, an attention mechanism, and the result is fed into a linear output layer. For marginal inference, we use pytorch-struct (Rush, 2020). For the SparseMAP projection, we use the active set algorithm (Niculae et al., 2018a). The baseline we compare our models against is a BiLSTM, followed by feeding the sum of all hidden states to a two-layer ReLU-MLP. Results. The results from the experiments with the different methods are shown in Table 3. As in the unstructured case, the relaxed models lead to strong downstream classifiers. Unlike the unstructured case, SPIGOT is a top performer here. The effect of tuning the number of gradient update steps is not as big as in the unstructured case and did not lead to significant improvemen"
2020.emnlp-main.171,P16-1159,0,0.0204941,"methods do not outperform the relaxed alternatives using the same building blocks, we hope that our interpretation and insights would trigger future latent structure research. The code for the paper is available on https: //github.com/deep-spin/understanding-spigot. Discrete latent variable learning is often tackled in stochastic computation graphs, by estimating the gradient of an expected loss. An established method is the score function estimator (SFE) (Glynn, 1990; Williams, 1992; Kleijnen and Rubinstein, 1996). SFE is widely used in NLP, for tasks including minimum risk training in NMT (Shen et al., 2016; Wu et al., 2018) and latent linguistic structure learning (Yogatama et al., 2017; Havrylov et al., 2019). In this paper, we focus on the alternative strategy of surrogate gradients, which allows learning in deterministic graphs with discrete, argmax-like nodes, rather than in stochastic graphs. Examples are the straight-through estimator (STE) (Hinton, 2012; Bengio et al., 2013) and the structured projection of intermediate gradients optimization technique (SPIGOT; Peng et al. 2018). Recent work focuses on studying and explaining STE. Yin et al. (2019) obtained a convergence result in shallo"
2020.emnlp-main.171,P06-2101,0,0.0860923,"ace the argmax node by a stochastic node where z is modeled as a random variable Z parametrized by s (e.g., using a Gibbs distribution). Then, instead of optimizing a deterministic loss L(ˆ y (ˆ z ), y), optimize the expectation of the loss under the predicted distribution: We next provide a novel interpretation of SPIGOT as the minimization of a “pulled back” loss. SPIGOT uses the surrogate gradient: ˜ s L(ˆ ∇ y (ˆ z ), y) = zˆ − ΠM (ˆ z − ηγ) = zˆ − SparseMAP(ˆ z − ηγ), (7) The expectation ensures that the gradients are no longer null. This is sometimes referred to as minimum risk training (Smith and Eisner, 2006; Stoyanov et al., 2011), and typically optimized using the score function estimator (SFE; Glynn, 1990; Williams, 1992; Kleijnen and Rubinstein, 1996). Relaxing the argmax. Keep the network deterministic, but relax the argmax node into a continuous function, for example replacing it with softmax or sparsemax (Martins and Astudillo, 2016). In the structured case, this gives rise to structured attention networks (Kim et al., 2017) and their SparseMAP variant (Niculae et al., 2018a). This corresponds to moving the expectation  inside the loss, optimizing L yˆ(EZ∼p(z;s) [Z]), y . | {z } µ Inventi"
2020.emnlp-main.171,Q18-1019,0,0.0532274,"Missing"
2020.emnlp-main.171,D18-1397,0,0.0283125,"perform the relaxed alternatives using the same building blocks, we hope that our interpretation and insights would trigger future latent structure research. The code for the paper is available on https: //github.com/deep-spin/understanding-spigot. Discrete latent variable learning is often tackled in stochastic computation graphs, by estimating the gradient of an expected loss. An established method is the score function estimator (SFE) (Glynn, 1990; Williams, 1992; Kleijnen and Rubinstein, 1996). SFE is widely used in NLP, for tasks including minimum risk training in NMT (Shen et al., 2016; Wu et al., 2018) and latent linguistic structure learning (Yogatama et al., 2017; Havrylov et al., 2019). In this paper, we focus on the alternative strategy of surrogate gradients, which allows learning in deterministic graphs with discrete, argmax-like nodes, rather than in stochastic graphs. Examples are the straight-through estimator (STE) (Hinton, 2012; Bengio et al., 2013) and the structured projection of intermediate gradients optimization technique (SPIGOT; Peng et al. 2018). Recent work focuses on studying and explaining STE. Yin et al. (2019) obtained a convergence result in shallow networks for the"
2020.emnlp-main.348,D19-5602,0,0.0273996,"Missing"
2020.emnlp-main.348,P18-1082,0,0.488492,"cores on language modeling. However, the generated text is still often repetitive and incoherent (Table 1). A downside of current approaches is the mismatch between training and testing conditions: Table 1: Completion of a story from the WritingPrompts dataset, using the greedy decoding, top-k sampling, nucleus sampling, and entmax sampling (our proposal) methods. Repetitions and off-topic text are highlighted. models are usually trained to maximize the likelihood of observed text. However, when generating, state-of-the-art models sample from a truncated and renormalized softmax distribution (Fan et al., 2018; Holtzman et al., 2020). They do so as a compromise to avoid two extremes: a deterministic search for the most probable sentence (via greedy decoding or beam search) usually results in dull and repetitive “degenerate text” (Li et al., 2016a, 2017; Holtzman et al., 2020); stochastically sampling from the full softmax distribution, on the other hand, often generates many implausible 4252 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 4252–4273, c November 16–20, 2020. 2020 Association for Computational Linguistics words from the tail of the distrib"
2020.emnlp-main.348,W17-4912,0,0.024346,"et al. (2018, 2020) have shown that these methods lead to repetitions and dull text. To overcome this, several authors proposed beam search variants which promote word diversity (Li et al., 2016b; Vijayakumar et al., 2018; Kulikov et al., 2018). An alternative to deterministic text generation is to sample directly from the softmax distribution. However, since the probability mass tends to accumulate in a long tail, this procedure generates unlikely words too often, leading to degenerate text (Fan et al., 2018; Holtzman et al., 2020). This can be mitigated by lowering the softmax temperature (Ficler and Goldberg, 2017), by sampling from the top-k most probable words only (Fan et al., 2018; Radford et al., 2019), or through nucleus sampling (Holtzman et al., 2020). We compare against these methods in §5. Diversity-promoting models. In addition to new decoding methods, models that aim to increase word diversity and diminish repetition have also been introduced. Xu et al. (2018) proposed a diversity-promoting generative adversarial network, which rewards novel and fluent text. Holtzman et al. (2018) proposed augmenting the language model with several discriminators. More recently, Welleck et al. (2020) propose"
2020.emnlp-main.348,P02-1040,0,0.107367,"bel classification. This was generalized by Peters et al. 1 The code used for the experiments and for the proposed metrics is available at https://github.com/ deep-spin/sparse_text_generation. 4253 (2019) via their α-entmax transformation, which was applied to sequence-to-sequence models for morphological inflection and machine translation. In contrast to our work, they performed deterministic decoding with beam search, and they did not consider open-ended generation. Evaluation metrics. The most common metrics to evaluate text generation models are perplexity (Jelinek et al., 1977) and BLEU (Papineni et al., 2002). For open-ended generation, Zhu et al. (2018) observed that “no single metric is comprehensive enough”. Other evaluations include corpus n-gram overlap (Yu et al., 2017; Press et al., 2017), and the Fr´echet distance (C´ıfka et al., 2018). These approaches are aimed at the (harder) problem of evaluating the quality of generated text. By contrast, our paper proposes new metrics for evaluating language models in the task of predicting the next word conditioned on ground truth context (like perplexity does), but supporting sparse probability distributions (which perplexity does not). 2 3 Languag"
2020.emnlp-main.348,P19-1146,1,0.926663,"rd, it gets infinite perplexity for the entire sample. For this reason, previous works generate from a truncated softmax, but report the perplexity of the full softmax distribution (Welleck et al., 2020; Li et al., 2020). Others use the latter to compare perplexity on the generated text with that on human text (Holtzman et al., 2020, §4.2), or resort to distributional statistics (Zhu et al., 2018). In this paper, we propose a new approach— entmax sampling (§3)—that eliminates the mismatch between training and test conditions. Key to our approach is the recently proposed entmax transformation (Peters et al., 2019). Entmax transforms a vector of scores into a sparse probability distribution, preventing implausible words from receiving any probability mass. Moreover, it does so natively: it comes with a well-defined loss function that allows it to learn its sparsity automatically from the data, during training. This results in a new stochastic text generator where the number of possible word types varies with the context (like nucleus sampling), but that generates by sampling directly from its output distribution (like softmax), and where the sparsity of this distribution is present during training (unli"
2020.emnlp-main.348,K16-1013,0,0.0143518,"aluation of sparse language models (§4): -perplexity, sparsemax score, and Jensen-Shannon divergence. We show that these metrics are well supported theoretically and can be used to compare our method with various truncation and temperature techniques. Experiments in language modeling, story completion, and dialogue generation (§5) show that entmax sampling generates more diverse text and fewer repetitions than nucleus and top-k sampling.1 1.1 Related work Decoding methods. While greedy decoding and beam search are popular strategies for sequenceto-sequence tasks, such as machine translation, Knowles et al. (2016) and Stahlberg and Byrne (2019) showed that searching for the most probable sentence in a model trained with likelihood maximization has a bias for short sentences. In openended generation, Fan et al. (2018) and Holtzman et al. (2018, 2020) have shown that these methods lead to repetitions and dull text. To overcome this, several authors proposed beam search variants which promote word diversity (Li et al., 2016b; Vijayakumar et al., 2018; Kulikov et al., 2018). An alternative to deterministic text generation is to sample directly from the softmax distribution. However, since the probability m"
2020.emnlp-main.348,N16-1014,0,0.660543,"tingPrompts dataset, using the greedy decoding, top-k sampling, nucleus sampling, and entmax sampling (our proposal) methods. Repetitions and off-topic text are highlighted. models are usually trained to maximize the likelihood of observed text. However, when generating, state-of-the-art models sample from a truncated and renormalized softmax distribution (Fan et al., 2018; Holtzman et al., 2020). They do so as a compromise to avoid two extremes: a deterministic search for the most probable sentence (via greedy decoding or beam search) usually results in dull and repetitive “degenerate text” (Li et al., 2016a, 2017; Holtzman et al., 2020); stochastically sampling from the full softmax distribution, on the other hand, often generates many implausible 4252 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 4252–4273, c November 16–20, 2020. 2020 Association for Computational Linguistics words from the tail of the distribution (Fan et al., 2018). The recently proposed nucleus sampling approach (Holtzman et al., 2020) sets the truncation point based on the cumulative distribution function, i.e., it considers the top words with a cumulative probability P . In"
2020.emnlp-main.348,D16-1127,0,0.578606,"tingPrompts dataset, using the greedy decoding, top-k sampling, nucleus sampling, and entmax sampling (our proposal) methods. Repetitions and off-topic text are highlighted. models are usually trained to maximize the likelihood of observed text. However, when generating, state-of-the-art models sample from a truncated and renormalized softmax distribution (Fan et al., 2018; Holtzman et al., 2020). They do so as a compromise to avoid two extremes: a deterministic search for the most probable sentence (via greedy decoding or beam search) usually results in dull and repetitive “degenerate text” (Li et al., 2016a, 2017; Holtzman et al., 2020); stochastically sampling from the full softmax distribution, on the other hand, often generates many implausible 4252 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 4252–4273, c November 16–20, 2020. 2020 Association for Computational Linguistics words from the tail of the distribution (Fan et al., 2018). The recently proposed nucleus sampling approach (Holtzman et al., 2020) sets the truncation point based on the cumulative distribution function, i.e., it considers the top words with a cumulative probability P . In"
2020.emnlp-main.348,D17-1230,0,0.0655469,"Missing"
2020.emnlp-main.348,P16-1162,0,0.0145553,".50 .450 .364 21.90 .373 .370 15.51 .357 .366 610.06 .410 .373 13.23 .346 .162 .168 .210 .181 .167 .200 .160 sp .680 .677 .489 .676 .678 .475 .687 JS -ppl REP WREP .366 10.80 .376 .363 10.96 .391 .354 506.86 .461 .360 22.25 .399 .362 16.48 .392 .364 587.04 .418 .362 10.70 .374 .183 .191 .211 .203 .193 .203 .179 Table 2: Language model evaluation on WikiText-2, WikiText-103, and BookCorpus test sets. For all metrics except sp, lower is better. See App. F for the results on the validation set. to evaluate the methods’ tendency to generate repetitions. All metrics are computed at the BPE level (Sennrich et al., 2016). Fine-tuning GPT-2. We fine-tuned the GPT-2 medium model (Radford et al., 2019), which consists of a 24 layer transformer with 345 million parameters.7 We fine-tuned three models with the following losses: negative log-likelihood (used for softmax, greedy, top-k, and nucleus sampling), unlikelihood training (Welleck et al., 2020), and entmax loss. For the unlikelihood training objective we replicated the authors’ experiments. However, due to GPU memory constraints we had to reduce the context size from 512 to 256. The hyperparameters were chosen based on a grid search over α ∈ {1.1, 1.2, 1.3,"
2020.emnlp-main.348,D19-1331,0,0.0358539,"e models (§4): -perplexity, sparsemax score, and Jensen-Shannon divergence. We show that these metrics are well supported theoretically and can be used to compare our method with various truncation and temperature techniques. Experiments in language modeling, story completion, and dialogue generation (§5) show that entmax sampling generates more diverse text and fewer repetitions than nucleus and top-k sampling.1 1.1 Related work Decoding methods. While greedy decoding and beam search are popular strategies for sequenceto-sequence tasks, such as machine translation, Knowles et al. (2016) and Stahlberg and Byrne (2019) showed that searching for the most probable sentence in a model trained with likelihood maximization has a bias for short sentences. In openended generation, Fan et al. (2018) and Holtzman et al. (2018, 2020) have shown that these methods lead to repetitions and dull text. To overcome this, several authors proposed beam search variants which promote word diversity (Li et al., 2016b; Vijayakumar et al., 2018; Kulikov et al., 2018). An alternative to deterministic text generation is to sample directly from the softmax distribution. However, since the probability mass tends to accumulate in a lo"
2020.emnlp-main.348,D18-1428,0,0.0298785,"lity mass tends to accumulate in a long tail, this procedure generates unlikely words too often, leading to degenerate text (Fan et al., 2018; Holtzman et al., 2020). This can be mitigated by lowering the softmax temperature (Ficler and Goldberg, 2017), by sampling from the top-k most probable words only (Fan et al., 2018; Radford et al., 2019), or through nucleus sampling (Holtzman et al., 2020). We compare against these methods in §5. Diversity-promoting models. In addition to new decoding methods, models that aim to increase word diversity and diminish repetition have also been introduced. Xu et al. (2018) proposed a diversity-promoting generative adversarial network, which rewards novel and fluent text. Holtzman et al. (2018) proposed augmenting the language model with several discriminators. More recently, Welleck et al. (2020) proposed augmenting the loss with an unlikelihood term that penalizes the generation of tokens that are present in the context, a method against which we compare in §5. Sparse transformations and losses. At the core of our work are sparse alternatives to the softmax transformation. Martins and Astudillo (2016) proposed sparsemax and applied it to multi-label classifica"
2020.emnlp-main.348,P18-1205,0,0.0211384,"(Fleiss Kappa) is 0.45 for fluency, 0.41 for coherence, and 0.63 for engagement. coherence and engagement, having similar scores as nucleus sampling on fluency. 5.3 Dialogue Generation To evaluate the sampling methods in an interactive setting, we experiment with dialogue generation. Its goal is to generate an utterance, given a context consisting of the previous utterances in the dialogue and, in some cases, initial context sentences with related information that can be describing personas, knowledge, or scenarios. Datasets and metrics. We performed experiments with the PersonaChat dataset (Zhang et al., 2018). It is a crowd-sourced dialogue dataset in which speakers were asked to condition their utterances on predefined personas. It contains 164,356 utterances over 10,981 dialogues. As there is no public test set, we report results on the validation set. We evaluate the word F1 -score, perplexity, sparsemax score, and Jensen-Shannon divergence. As for the language modeling experiments, -perplexity, sparsemax score, and JensenShannon are computed at the BPE level. We also report distinct-n metric for n = {1, 2} and analyze how the models behave in dialogue simulations between two agents (Li et al"
2020.emnlp-main.348,P19-1285,0,\N,Missing
2020.sigmorphon-1.4,P17-1183,0,0.0224351,"of sparse functions for attention weights and output distributions, in place of the betterknown softmax (Bridle, 1990). Sparse functions have the following motivations: Lα (y, z) := (p? − ey )&gt; z + Hα (p? ), where p? := α-entmax(z). This is an instance of a Fenchel-Young loss (Blondel et al., 2020). • Sparse attention has previously shown success on morphological inflection (Peters and Martins, 2019). It allows the decoder to attend to a small number of source positions at each time step, unlike the dense softmax. While hard attention has previously performed well for character transduction (Aharoni and Goldberg, 2017; Makarov et al., 2017; Wu et al., 2018; Wu and Cotterell, 2019), it usually requires an elaborate and slow training procedure. On the other hand, sparse attention does not require any training techniques beyond those used for standard seq2seq models. 2.2 Entmax and its loss 2.3 Our tool for achieving sparsity is the entmax activation function (Peters et al., 2019), which is parameterized by a scalar α ≥ 1 and maps a vector z ∈ Rn onto the n–dimensional probability simplex 4n := {p ∈ Rn : p ≥ 0, 1&gt; p = 1}: (1) where Hα (p) := 1 α(α−1) − P P  j  pj − pαj , α 6= 1, j pj log pj , Handling Multi"
2020.sigmorphon-1.4,D19-3019,0,0.112572,"Missing"
2020.sigmorphon-1.4,D19-1091,0,0.0745872,"Missing"
2020.sigmorphon-1.4,P17-2031,0,0.04568,"Missing"
2020.sigmorphon-1.4,W18-6326,0,0.0449482,"Missing"
2020.sigmorphon-1.4,N18-1083,0,0.0265108,"Missing"
2020.sigmorphon-1.4,D15-1166,0,0.0507506,"encode the lemma character sequence and the set of inflectional tags. A unidirectional LSTM (Hochreiter and Schmidhuber, 1997) decoder then generates the target sequence. The decoder is similar to a conventional RNN decoder with input feeding, except that separate attention mechanisms compute context vectors independently for each encoder. A gate function then interpolates the two context vectors. Like Peters and Martins (2019), we use a sparse gate, which allows the model to completely ignore one encoder or the other at each time step. Each individual attention head uses bilinear attention (Luong et al., 2015). • Sparse output distributions allow probability mass to be concentrated in a small number of hypotheses. In practice, this happens frequently for morphological inflection (Peters et al., 2019), sometimes making beam search exact. 2.1 (3) (2) α=1 64 et al., 2017) and morphological inflection (Peters et al., 2019), and is similar to techniques for multilingual neural machine translation (Johnson et al., 2017). However, this technique has drawbacks: it forces the true characters and the language token to “compete” for attention, and it requires the learned language embedding to have the same si"
2020.sigmorphon-1.4,K17-2001,0,0.0926986,"Missing"
2020.sigmorphon-1.4,P16-1038,0,0.0275898,"features because orthographic similarity does not correlate with typological similarity. 6 Conclusion We showed that massively multilingual models are competitive with the individually-tuned state of the art for morphological inflection and g2p. We presented the first result applying entmax-based sparse attention and losses to g2p, showing that it performed with both RNN and transformer models. We release our code to facilitate further research. Acknowledgments Phonemes and multilinguality Multilingual methods have previously been used for low resource g2p in conjunction with both non-neural (Deri and Knight, 2016) and neural (Peters et al., 2017; Route et al., 2019) architectures. Our model is essentially identical to Peters et al. (2017)’s, but with a different mechanism for identifying the language, inspired by a technique for learning language embeddings from multilingual language ¨ modeling (Ostling and Tiedemann, 2017). A natural connection is to work that makes use of typological information in multilingual NLP (Tsvetkov et al., 2016). However, care needs to be taken when applying this to g2p: Bjerva and Augenstein (2018) This work was supported by the European Research Council (ERC StG DeepSPIN"
2020.sigmorphon-1.4,2020.sigmorphon-1.2,0,0.355068,"Missing"
2020.sigmorphon-1.4,E17-2102,0,0.0218711,"tion and losses to g2p, showing that it performed with both RNN and transformer models. We release our code to facilitate further research. Acknowledgments Phonemes and multilinguality Multilingual methods have previously been used for low resource g2p in conjunction with both non-neural (Deri and Knight, 2016) and neural (Peters et al., 2017; Route et al., 2019) architectures. Our model is essentially identical to Peters et al. (2017)’s, but with a different mechanism for identifying the language, inspired by a technique for learning language embeddings from multilingual language ¨ modeling (Ostling and Tiedemann, 2017). A natural connection is to work that makes use of typological information in multilingual NLP (Tsvetkov et al., 2016). However, care needs to be taken when applying this to g2p: Bjerva and Augenstein (2018) This work was supported by the European Research Council (ERC StG DeepSPIN 758969), and by the Fundac¸a˜ o para a Ciˆencia e Tecnologia through contracts UID/EEA/50008/2019 and CMUPERI/TIC/0046/2014 (GoLocal). We thank the anonymous reviewers for their helpful feedback. References ´ Judit Acs. 2018. BME-HAS system for CoNLL– SIGMORPHON 2018 shared task: Universal morphological reinflectio"
2020.sigmorphon-1.4,W17-5403,1,0.898206,"Missing"
2020.sigmorphon-1.4,P82-1020,0,0.747952,"Missing"
2020.sigmorphon-1.4,W12-2109,0,0.0266552,"Missing"
2020.sigmorphon-1.4,P19-1146,1,0.937061,"at hopefully ameliorates the data scarcity problem. We find both of these choices unsatisfying. First, older non-neural techniques have a higher floor but also a lower ceiling – previous SIGMORPHON shared tasks have shown that neural methods outpace them in the presence of even moderate quantities of data (Cotterell et al., 2017). Second, although data augmentation has proven helpful for morphological inflection (Anas• We reimplement gated sparse two-headed attention (Peters and Martins, 2019) and apply it to a massively multilingual setting. We submit versions of this model using 1.5-entmax (Peters et al., 2019) and sparsemax (Martins and Astudillo, 2016) as softmax alternatives. We tie for first place in Task 0 (Vylomova et al., 2020). Among the winners, ours are the only multilingual models. • We show that sparse seq2seq techniques, previously used for morphological inflection and machine translation (Peters et al., 2019), are also effective for multilingual g2p. We make four submissions to Task 1 (Gorman et al., 63 Proceedings of the Seventeenth SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 63–69 c Online, July 10, 2020. 2020 Association for Computati"
2020.sigmorphon-1.4,N18-1202,0,0.0146401,"9.96 88.78 87.30 98.37 96.49 97.32 100.00 83.45 98.15 84.76 89.89 59.43 94.76 89.21 83.75 Afro-Asiatic Algic Australian Austronesian Dravidian Germanic Indo-Aryan Iranian Niger-Congo Nilo-Saharan Oto-Manguean Romance Sino-Tibetan Siouan Tungusic Turkic Uralic Uto-Aztecan Table 3: Macro-averaged test results for Task 1. 100 90 Acc. (dev) #languages Family 80 Table 4: Task 0 dev accuracy by language family for I NFLECTION - SPARSEMAX. 70 60 4.2 50 0 25000 50000 Train size 75000 100000 Learning good word representations has been a prominent subject in NLP for several years (Mikolov et al., 2013; Peters et al., 2018). Although many models operate at the character level, relatively little attention has been paid to the character embeddings themselves. Characters lack semantic meaning, so character embeddings learned for “semantic” tasks are unlikely to learn any particular structure. However, Figure 2 shows that multilingual g2p may be useful for learning phonologically grounded character representations: graphemes from different scripts cluster together if they represent similar phonemes. We suspect that the multilingual training with phonological supervision is a necessary ingredient for this to work – c"
2020.sigmorphon-1.4,D19-6121,0,0.0160572,"ate with typological similarity. 6 Conclusion We showed that massively multilingual models are competitive with the individually-tuned state of the art for morphological inflection and g2p. We presented the first result applying entmax-based sparse attention and losses to g2p, showing that it performed with both RNN and transformer models. We release our code to facilitate further research. Acknowledgments Phonemes and multilinguality Multilingual methods have previously been used for low resource g2p in conjunction with both non-neural (Deri and Knight, 2016) and neural (Peters et al., 2017; Route et al., 2019) architectures. Our model is essentially identical to Peters et al. (2017)’s, but with a different mechanism for identifying the language, inspired by a technique for learning language embeddings from multilingual language ¨ modeling (Ostling and Tiedemann, 2017). A natural connection is to work that makes use of typological information in multilingual NLP (Tsvetkov et al., 2016). However, care needs to be taken when applying this to g2p: Bjerva and Augenstein (2018) This work was supported by the European Research Council (ERC StG DeepSPIN 758969), and by the Fundac¸a˜ o para a Ciˆencia e Tec"
2020.sigmorphon-1.4,W18-0314,0,0.0252825,"learned for “semantic” tasks are unlikely to learn any particular structure. However, Figure 2 shows that multilingual g2p may be useful for learning phonologically grounded character representations: graphemes from different scripts cluster together if they represent similar phonemes. We suspect that the multilingual training with phonological supervision is a necessary ingredient for this to work – characters from different scripts are never mixed within a single sample, so the grapheme contexts in which they occur are completely disjoint. This idea differs from work on phoneme embeddings (Silfverberg et al., 2018; Sofroniev and C¸o¨ ltekin, 2018) in that the focus is explicitly on the graphemes. Grapheme embeddings learned for phonological tasks may prove useful for transliteration, or for processing informally romanized text (Irvine et al., 2012) jointly with data from the official orthography. Figure 1: Single-language development set accuracies for I NFLECTION - SPARSEMAX. 4 Analysis Next we consider a few questions that multilingual models raise. 4.1 Crosslingual Character Embeddings How much data does inflection need? All other things being equal, we expect the performance of a model to improve a"
2020.sigmorphon-1.4,N16-1161,0,0.0677201,"Missing"
2020.sigmorphon-1.4,P19-1148,0,0.0347015,", in place of the betterknown softmax (Bridle, 1990). Sparse functions have the following motivations: Lα (y, z) := (p? − ey )&gt; z + Hα (p? ), where p? := α-entmax(z). This is an instance of a Fenchel-Young loss (Blondel et al., 2020). • Sparse attention has previously shown success on morphological inflection (Peters and Martins, 2019). It allows the decoder to attend to a small number of source positions at each time step, unlike the dense softmax. While hard attention has previously performed well for character transduction (Aharoni and Goldberg, 2017; Makarov et al., 2017; Wu et al., 2018; Wu and Cotterell, 2019), it usually requires an elaborate and slow training procedure. On the other hand, sparse attention does not require any training techniques beyond those used for standard seq2seq models. 2.2 Entmax and its loss 2.3 Our tool for achieving sparsity is the entmax activation function (Peters et al., 2019), which is parameterized by a scalar α ≥ 1 and maps a vector z ∈ Rn onto the n–dimensional probability simplex 4n := {p ∈ Rn : p ≥ 0, 1&gt; p = 1}: (1) where Hα (p) := 1 α(α−1) − P P  j  pj − pαj , α 6= 1, j pj log pj , Handling Multilinguality Multilingual NLP tasks are intrinsically more difficu"
2020.sigmorphon-1.4,D18-1473,0,0.0180403,"put distributions, in place of the betterknown softmax (Bridle, 1990). Sparse functions have the following motivations: Lα (y, z) := (p? − ey )&gt; z + Hα (p? ), where p? := α-entmax(z). This is an instance of a Fenchel-Young loss (Blondel et al., 2020). • Sparse attention has previously shown success on morphological inflection (Peters and Martins, 2019). It allows the decoder to attend to a small number of source positions at each time step, unlike the dense softmax. While hard attention has previously performed well for character transduction (Aharoni and Goldberg, 2017; Makarov et al., 2017; Wu et al., 2018; Wu and Cotterell, 2019), it usually requires an elaborate and slow training procedure. On the other hand, sparse attention does not require any training techniques beyond those used for standard seq2seq models. 2.2 Entmax and its loss 2.3 Our tool for achieving sparsity is the entmax activation function (Peters et al., 2019), which is parameterized by a scalar α ≥ 1 and maps a vector z ∈ Rn onto the n–dimensional probability simplex 4n := {p ∈ Rn : p ≥ 0, 1&gt; p = 1}: (1) where Hα (p) := 1 α(α−1) − P P  j  pj − pαj , α 6= 1, j pj log pj , Handling Multilinguality Multilingual NLP tasks are i"
2020.sigmorphon-1.4,N16-1004,0,0.0211411,"s Several previous works have considered ways to integrate information from 66 p 40 v w 20 y 40 kq r z Script æ b 25 a a a yi x x a 0 CYRILLIC GREEK LATIN e h g m l 50 ð d þ s n 20 t j f 0 c u o 25 50 Figure 2: t-SNE projection (Maaten and Hinton, 2008) of the grapheme embeddings learned by T RANSFORMER 1.5. For improved readability, we include only Cyrillic, Greek, and Latin graphemes. Graphemes that tend to represent similar phonemes are clustered together. multiple sources in a neural seq2seq model. Although initially proposed as a way to leverage multiparallel data in machine translation (Zoph and Knight, 2016), it has also been used for handling ´ (2018) applied it to mormultimodal data, and Acs phological inflection: our architecture is essentially a sparsified version of this model. Past works have also considered the effect of different strategies for merging the attention from the various encoders (Libovick`y and Helcl, 2017; Libovick`y et al., 2018). This is worth exploring for morphological inflection, as Peters and Martins (2019) showed that the behavior of the attention gating mechanism varies between language families. The optimal strategy is probably different for different languages. sho"
2020.wmt-1.119,W05-0909,0,0.15138,"in 7 different features for each sentence of each language-pair, the first 3 via (i) and the last 4 via (ii) (full details are in Fomicheva et al. (2020)): • TP - sentence average of word translation probability • Softmax-Ent - sentence average of softmax output distribution entropy • Sent-Std - sentence standard deviation of word probabilities • D-TP - average TP across N (N = 30) stochastic forward-passes • D-Var - variance of TP across N stochastic forward-passes • D-Combo - combination of D-TP and D-Var defined by 1 − D-TP/D-Var • D-Lex-Sim - lexical similarity - measured by METEOR score (Banerjee and Lavie, 2005) - of MT output generated in different stochastic passes. Figure 2: Architecture of the “Quality Estimator” module modified to include glass-box features. Table 1 shows the correlation between each one of these features and human DAs for every language pair in Task 1. As expected, features obtained using uncertainty quantification consistently display higher correlations across all languagepairs, D-TP being the most effective for high and medium resource languages, and D-Lex-Sim for low resource languages. 3.2.2 Glass-box + Black-box Model Different configurations were attempted in order to in"
2020.wmt-1.119,C04-1046,0,0.351976,"ent the joint contribution of IST and Unbabel to the WMT 2020 Shared Task on Quality Estimation. Our team participated on all tracks (Direct Assessment, Post-Editing Effort, Document-Level), encompassing a total of 14 submissions. Our submitted systems were developed by extending the OpenKiwi framework to a transformer-based predictorestimator architecture, and to cope with glassbox, uncertainty-based features coming from neural machine translation systems. 1 Introduction Quality estimation (QE) is the task of evaluating a translation system’s quality without access to reference translations (Blatz et al., 2004; Specia et al., 2018). This paper describes the joint contribution for Instituto Superior T´ecnico (IST) and Unbabel to the WMT20 Quality Estimation shared task, where systems were submitted to all three tasks: 1) sentence-level direct assessment; 2) word and sentence-level post-editing effort; and 3) documentlevel annotation and scoring. Unbabel’s participation in previous editions of the shared task (2016, 2017, 2019) used ensemble of strong individual systems, with varying architectures and hyper-parameters. While this strategy led to very strong results, large system ensembles are not a v"
2020.wmt-1.119,2020.acl-main.747,0,0.0403359,"Missing"
2020.wmt-1.119,W19-5406,1,0.820505,"Missing"
2020.wmt-1.119,P19-3020,1,0.820103,"Missing"
2020.wmt-1.119,W17-4763,0,0.314158,"Transformers2 Python package (Wolf et al., 2019), which means different models can be easily used. For this year’s shared task, we based all systems on this version of OpenKiwi and used pretrained XLM-Roberta models (Conneau et al., 2020), either base or large versions. We chose XLM-Roberta (called XLM-R from here on) instead of XLM, used in last year’s best individual model, due to its reported state-ofthe-art performance on downstream cross-lingual tasks and based on preliminary experiments. The architecture follows the overall pattern introduced originally in the Predictor-Estimator model (Kim et al., 2017), comprising a “Feature Extractor” module with a “Quality Estimator” module on top. Figure 1 depicts this general architecture. The Feature Extractor module consists of a pretrained XLM-R model and feature extraction methods on top, such that features for the target sentence, the target tokens, and the source tokens are returned separately. Source and target sentences 2 https://github.com/huggingface/ transformers TAGS Sentence Words Linear Layers FEATURE EXTRACTOR This year’s shared task edition comprised three tasks: 1) a newly introduced one for sentence-level direct assessment; 2) one for"
2020.wmt-1.119,2020.wmt-1.79,1,0.839175,"is general architecture. The Feature Extractor module consists of a pretrained XLM-R model and feature extraction methods on top, such that features for the target sentence, the target tokens, and the source tokens are returned separately. Source and target sentences 2 https://github.com/huggingface/ transformers TAGS Sentence Words Linear Layers FEATURE EXTRACTOR This year’s shared task edition comprised three tasks: 1) a newly introduced one for sentence-level direct assessment; 2) one for word and sentencelevel post-editing effort; and 3) one for documentlevel. Refer to the Findings paper (Specia et al., 2020) for full descriptions. Of noteworthy mention is that the NMT models for Tasks 1 and 2 where provided along with the data, which opened up the possibility of using glassbox approaches. SCORE QUALITY ESTIMATOR 2 Feature Extraction Layers XLM-R TARGET SOURCE Figure 1: General architecture of the implemented OpenKiwi-based systems. are passed as inputs in the format <s> target </s> <s> source </s>. Output features for tokens in the target sentence are averaged and then concatenated with the classifier token embedding (first <s> in the input), and returned as sentence features.3 For the Quality Es"
2020.wmt-1.3,P19-1285,0,0.0272739,"Missing"
2020.wmt-1.3,2020.emnlp-main.5,0,0.0391197,"were encouraged to submit both directions (i.e. modelling both speakers was desired), in this first round of the task, we emphasized on the agent side (English→German) and performed human evaluation in that direction exclusively. This decision is not entrenched and, thus, for future tasks we will aim at evaluating both translation directions. We decided to pursue this direction because the customer side (German→English) suffers from “translationese”: English was the original source, and it was recently shown that translationese has a significant impact in evaluation both in automatic metrics (Freitag et al., 2020) and human evaluation (L¨aubli et al., 2020). 3.1 Baseline 4 Participants Six participants submitted their systems to the Chat Translation shared task. Although the German→English direction (i.e. customer side) was optional, all participants submitted their systems for both directions. In total, 14 runs were submitted (although only primary submissions were considered for human evaluation). Table 3 summarizes the participants and their affiliations. Data The main data source for this shared task is BConTrasT. As mentioned in §2, the translated conversations are sampled from the original Taskma"
2020.wmt-1.3,2020.wmt-1.56,0,0.0343382,"tures for this task: (i) standard transformer pre-trained on WMT17 News and finetuned on the WMT20 Chat data, and (ii) modified transformer by including additional encoder to process one previous utterance in tandem with the current utterance, also pre-trained on WMT17 News and fine-tuned on a mix of WMT20 Chat data and a subset of WMT19 News data. The primary system is based on the first architecture while the second architecture is used for the two contrastive submissions. The contrastive submissions differ in the manner and timing in which training data was processed. For more details see (Bao et al., 2020). Universities of Edinburgh and Uppsala The joint submissions of University of Edinburgh and Uppsala University are based on the transformer-big architecture (Vaswani et al., 2017) and rely on fine-tuning pre-existing systems from the WMT 2019 News Translation Task (experiment with both UEdin’s submission based on Marian (Junczys-Dowmunt et al., 2018) and Facebook’s submission based on Fairseq (Ott et al., 2019)). They are fine-tuned on pseudo-in-domain web crawled data and in-domain task data. The authors also experiment with (i) domain and speaker-level adaptation by automatically tagging th"
2020.wmt-1.3,P82-1020,0,0.775149,"Missing"
2020.wmt-1.3,W19-5321,0,0.0202003,"Missing"
2020.wmt-1.3,P18-4020,1,0.756444,"Missing"
2020.wmt-1.3,2020.wmt-1.57,0,0.0725618,"Missing"
2020.wmt-1.3,D19-1459,0,0.238444,"man and sent to the customer. Translating conversational text, in particular customer support chats, is an important and challenging application task for machine translation technology. This type of content has so far not been extensively explored in prior MT research, largely due to the lack of publicly available data sets. Prior related work has mostly focused on movie subtitles and European Parliament speeches. To alleviate this problem, we created a corpus for this shared task, BConTrasT(§2), which is translated from English into German and is based on the monolingual Taskmaster-1 corpus (Byrne et al., 2019). We report the results of the first edition of the WMT shared task on Chat Translation. The task consisted of translating bilingual conversational text, in particular customer support chats for the English-German language pair (English agent, German customer). This task varies from the other translation shared tasks, i.e. news and biomedical, mainly due to the fact that the conversations are bilingual, less planned, more informal, and often ungrammatical. Furthermore, such conversations are usually characterized by shorter and simpler sentences and contain more pronouns. We received 14 submis"
2020.wmt-1.3,2005.mtsummit-papers.11,0,0.0593983,"er-1 corpus. Since pronouns are one of the main challenges in translating conversational data, we selected the conversations that contain at least one English anaphoric pronoun it. For this we used NEURALCOREF 2 and selected around 18k sentence pairs and then divided them into train, development, and test sets (see Table 2). Bilingual Conversational Data One of the main challenges of bilingual conversation translation is the lack of publicly available data sets targeted for the task. The most commonly used datasets are movie subtitles (Lison and Tiedemann, 2016), European Parliament speeches (Koehn, 2005), and conversations extracted from the public forums such as Ubuntu Dialogue corpus (Lowe et al., 2015). These corpora, however, usually involve more than two speakers, contain a significant amount of noise (e.g. speakers information missing in the case of movie subtitles), and usually cover very broad domains. For the Chat Translation task, we aim to develop a common ground for MT researchers to train and test their solutions by providing common training, validation, and test sets, as well as a common shared task definition. Unfortunately, due to the General Data Protection Regulation (GDPR),"
2020.wmt-1.3,E06-1032,0,0.0342614,"of the Chat Translation shared task we follow the standard procedure of WMT shared tasks and evaluate both on automatic metrics and human evaluation with context. Even though automatic metrics provide a cheap mechanism to evaluate Machine Translation (MT) systems outputs, they do not tell the whole story for highperforming systems (Ma et al., 2019). For example, recent “sentence-level human parity” claims do not seem to hold when the context of the document is considered (L¨aubli et al., 2018), and metrics such as BLEU (Papineni et al., 2002) fail to correlate properly with human assessment (Callison-Burch et al., 2006). In this edition of the shared task, we aim for both automatic and manual evaluations. NaverLabs-Sys1 NaverLabs-Sys2 58.8 60.4 26.8 25.1 59.4 61.6 24.6 23.1 UEdinUppsala-Sys1 UEdinUppsala-Sys2 60.2 59.8 25.3 25.4 61.8 61.5 22.8 23.8 Tencent-Sys1 Tencent-Sys2 53.6 58.6 30.6 26.6 54.0 61.9 28.8 23.2 UniMaryland-Sys1 UniMaryland-Sys2 55.6 56.4 28.3 28.1 49.4 49.4 32.0 32.0 Table 4: Automatic evaluation scores for the agent (En→De) and customer (De→En). Specifically, we build HITs (following the Mechanical Turk’s term human intelligence task) for the Segment Rating + Document Context (SR+DC) conf"
2020.wmt-1.3,2020.wmt-1.58,0,0.0235277,"Junczys-Dowmunt et al., 2018) and Facebook’s submission based on Fairseq (Ott et al., 2019)). They are fine-tuned on pseudo-in-domain web crawled data and in-domain task data. The authors also experiment with (i) domain and speaker-level adaptation by automatically tagging the source and target sentences with domain and speaker tags respectively, and (ii) contextual NMT by exploiting the previous context, varying the type and number of previous utterances used. The final submission is an ensemble of four models trained with domain tags and using noisy-channel re-ranking. For more details see (Moghe et al., 2020). 4.1.3 Tencent 4.1.6 Jordan University of Science and Technology Mohammed et al. (2020) train separate models for the agent and customer sides after combining the training and development datasets for each side. They use bidirectional RNN (LSTM) with pretrained BERT (Devlin et al., 2018) embeddings for each of the translation directions. In addition, the authors report using different parameters for training, resulting in different models which then are used for ensemble decoding. For more details see (Mohammed et al., 2020). Tao Wang (individual participant) Individual participant Tao Wang u"
2020.wmt-1.3,D18-1512,0,0.0910736,"Missing"
2020.wmt-1.3,2020.wmt-1.59,0,0.0334625,"2019)). They are fine-tuned on pseudo-in-domain web crawled data and in-domain task data. The authors also experiment with (i) domain and speaker-level adaptation by automatically tagging the source and target sentences with domain and speaker tags respectively, and (ii) contextual NMT by exploiting the previous context, varying the type and number of previous utterances used. The final submission is an ensemble of four models trained with domain tags and using noisy-channel re-ranking. For more details see (Moghe et al., 2020). 4.1.3 Tencent 4.1.6 Jordan University of Science and Technology Mohammed et al. (2020) train separate models for the agent and customer sides after combining the training and development datasets for each side. They use bidirectional RNN (LSTM) with pretrained BERT (Devlin et al., 2018) embeddings for each of the translation directions. In addition, the authors report using different parameters for training, resulting in different models which then are used for ensemble decoding. For more details see (Mohammed et al., 2020). Tao Wang (individual participant) Individual participant Tao Wang uses a sentencelevel system trained on all the WMT20 En-De parallel data. The author uses"
2020.wmt-1.3,W19-5333,0,0.0119796,"er and an agent. Customer lines words Training Dev Test 6,216 862 967 41,492 5,805 6,464 training data provided by the News shared task organizers. Moreover, they were allowed to use existing pre-trained models, such as BERT (Devlin et al., 2018), Transformer-XL (Dai et al., 2019), Reformer (Kitaev et al., 2020), among others. Agent lines words 7,629 1,040 1,133 70,193 9,569 10,187 3.2 Table 2: Statistics of the English side of the training, dev, and test sets. To define our non-human baseline, we use Facebook’s last year submissions to the document-level translation task for both directions (Ng et al., 2019) as the terms of comparison. Even though these models are not domain adapted for the Chat Translation task, we find them to have a reasonable quality for this domain. However, it is worth mentioning that we solely report the results of these models with the automatic metrics and we do not perform any type of direct assessment on these models. pared to the cases like news, biomedical, etc. In the first edition of this shared task we focused on this environment and asked the participants to translate the customer’s utterances from German into English and the agent’s from English into German. Alt"
2020.wmt-1.3,L16-1147,0,0.0152834,"is scarce, we translated only a small set of the Taskmaster-1 corpus. Since pronouns are one of the main challenges in translating conversational data, we selected the conversations that contain at least one English anaphoric pronoun it. For this we used NEURALCOREF 2 and selected around 18k sentence pairs and then divided them into train, development, and test sets (see Table 2). Bilingual Conversational Data One of the main challenges of bilingual conversation translation is the lack of publicly available data sets targeted for the task. The most commonly used datasets are movie subtitles (Lison and Tiedemann, 2016), European Parliament speeches (Koehn, 2005), and conversations extracted from the public forums such as Ubuntu Dialogue corpus (Lowe et al., 2015). These corpora, however, usually involve more than two speakers, contain a significant amount of noise (e.g. speakers information missing in the case of movie subtitles), and usually cover very broad domains. For the Chat Translation task, we aim to develop a common ground for MT researchers to train and test their solutions by providing common training, validation, and test sets, as well as a common shared task definition. Unfortunately, due to th"
2020.wmt-1.3,N19-4009,0,0.0122697,"second architecture is used for the two contrastive submissions. The contrastive submissions differ in the manner and timing in which training data was processed. For more details see (Bao et al., 2020). Universities of Edinburgh and Uppsala The joint submissions of University of Edinburgh and Uppsala University are based on the transformer-big architecture (Vaswani et al., 2017) and rely on fine-tuning pre-existing systems from the WMT 2019 News Translation Task (experiment with both UEdin’s submission based on Marian (Junczys-Dowmunt et al., 2018) and Facebook’s submission based on Fairseq (Ott et al., 2019)). They are fine-tuned on pseudo-in-domain web crawled data and in-domain task data. The authors also experiment with (i) domain and speaker-level adaptation by automatically tagging the source and target sentences with domain and speaker tags respectively, and (ii) contextual NMT by exploiting the previous context, varying the type and number of previous utterances used. The final submission is an ensemble of four models trained with domain tags and using noisy-channel re-ranking. For more details see (Moghe et al., 2020). 4.1.3 Tencent 4.1.6 Jordan University of Science and Technology Mohamm"
2020.wmt-1.3,2020.eamt-1.24,1,0.709928,"at Translation M. Amin Farajian1∗ Ant´onio V. Lopes1∗ Andr´e F. T. Martins1,3 Sameen Maruf2 Gholamreza Haffari2 1 Unbabel, Rua Castilho 52, 1250-069, Lisbon, Portugal 2 Monash University, VIC, Australia 3 Instituto de Telecomunicac¸o˜ es, Instituto Superior T´ecnico, Lisbon, Portugal {amin, antonio.lopes, andre.martins}@unbabel.com {sameen.maruf, gholamreza.haffari}@monash.edu Abstract and Scherrer, 2017; Zhang et al., 2018; Maruf et al., 2019; Miculicich et al., 2018; Voita et al., 2019b; Tu et al., 2018; Maruf et al., 2018; Jean et al., 2017; Voita et al., 2018, 2019a; JunczysDowmunt, 2019; Lopes et al., 2020), focusing on extending both Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Transformer (Vaswani et al., 2017) with additional encoders or decoders to incorporate previous sentences context. However, often, the approaches are developed for single speaker and document-like tasks. By contrast, in this shared task, we focus on the online multispeaker and multi-lingual setting, where each participant in the conversation speaks in their native language. This task has been first considered by Maruf et al. (2018). In the first round of the Chat Translation shared task, we propos"
2020.wmt-1.3,P02-1040,0,0.114414,"d by conversational data as a content type, which has a broad application in industry-level services. In this content type, the text is usually not carefully well formatted, frequently contains typos, abbreviations, and inconsistent casing, usually with shorter sentences, often informal and ungrammatical. Since chat sessions are interactive, the task of translating conversations can be seen as a two-in-one task, modelling both dialogue and document-level translation at the same time. In order to evaluate the translation quality of the participating systems we use both automatic metrics (BLEU (Papineni et al., 2002) and TER (Snover et al., 2006)), and human evaluation, consisting of Direct Assessment (DA). For DA, we define the evaluation process similarly to last year’s WMT News Translation task (Barrault et al., 2019) with document-level context and following the set of recommendations of L¨aubli et al. (2020). However, differently than the News task, here we rely on professional translators instead of a crowd. This is mainly based on the observations of L¨aubli et al. (2020), which provides evidence of the professional translators having better judgment and ability to detect fine-grained phenomena. Si"
2020.wmt-1.3,W15-4640,0,0.0167846,"selected the conversations that contain at least one English anaphoric pronoun it. For this we used NEURALCOREF 2 and selected around 18k sentence pairs and then divided them into train, development, and test sets (see Table 2). Bilingual Conversational Data One of the main challenges of bilingual conversation translation is the lack of publicly available data sets targeted for the task. The most commonly used datasets are movie subtitles (Lison and Tiedemann, 2016), European Parliament speeches (Koehn, 2005), and conversations extracted from the public forums such as Ubuntu Dialogue corpus (Lowe et al., 2015). These corpora, however, usually involve more than two speakers, contain a significant amount of noise (e.g. speakers information missing in the case of movie subtitles), and usually cover very broad domains. For the Chat Translation task, we aim to develop a common ground for MT researchers to train and test their solutions by providing common training, validation, and test sets, as well as a common shared task definition. Unfortunately, due to the General Data Protection Regulation (GDPR), 3 Task Description A critical challenge faced by international companies today is delivering customer"
2020.wmt-1.3,W19-5302,0,0.0121262,"their systems. 5 System 43.4 38.0 Primary NaverLabs UEdinUppsala IndTaoWang Tencent UniMaryland UJordan 60.1 60.2 59.7 58.6 56.7 46.4 25.7 25.4 26.0 26.7 28.2 38.2 49.7 32.0 61.0 62.4 61.3 62.3 49.4 42.5 23.3 22.8 23.5 23.0 32.0 40.2 Contrastive For the first round of the Chat Translation shared task we follow the standard procedure of WMT shared tasks and evaluate both on automatic metrics and human evaluation with context. Even though automatic metrics provide a cheap mechanism to evaluate Machine Translation (MT) systems outputs, they do not tell the whole story for highperforming systems (Ma et al., 2019). For example, recent “sentence-level human parity” claims do not seem to hold when the context of the document is considered (L¨aubli et al., 2018), and metrics such as BLEU (Papineni et al., 2002) fail to correlate properly with human assessment (Callison-Burch et al., 2006). In this edition of the shared task, we aim for both automatic and manual evaluations. NaverLabs-Sys1 NaverLabs-Sys2 58.8 60.4 26.8 25.1 59.4 61.6 24.6 23.1 UEdinUppsala-Sys1 UEdinUppsala-Sys2 60.2 59.8 25.3 25.4 61.8 61.5 22.8 23.8 Tencent-Sys1 Tencent-Sys2 53.6 58.6 30.6 26.6 54.0 61.9 28.8 23.2 UniMaryland-Sys1 UniMar"
2020.wmt-1.3,W18-6319,0,0.0210084,"delines, we use trusted professional translators from the Unbabel community to evaluate the adequacy of the translation on a scale of 0 to 100. The guidelines to the translators were as simple as possible to avoid any type of bias, asking them to rate each sentence taking the context into account and penalizing when there is a context error, as they would for a noncontextual error. For the first edition of this shared task, we perAutomatic Evaluation For the automatic evaluation, we use both BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) metrics. For the former, we use SacreBLEU3 (Post, 2018), while for TER we use v0.7.254 and report case-sensitive scores. The automatic metrics are used to measure the quality of the translations of both sides, i.e. customer and agent. 5.2 BLEU↑ TER↓ BLEU↑ TER↓ FAIR WMT’19 Evaluation Procedures 5.1 Customer Human Evaluation For the human evaluation we follow a similar procedure to last year’s WMT News shared task (Barrault et al., 2019) but take into account the set of recommendations defined by L¨aubli et al. (2020). 3 BLEU+case.mixed+lang.ende+numrefs.1+smooth.exp+tok.13a+version.1.4.13, BLEU+case.mixed+lang.deen+numrefs.1+smooth.exp+tok.13a+vers"
2020.wmt-1.3,2006.amta-papers.25,0,0.489412,"ntent type, which has a broad application in industry-level services. In this content type, the text is usually not carefully well formatted, frequently contains typos, abbreviations, and inconsistent casing, usually with shorter sentences, often informal and ungrammatical. Since chat sessions are interactive, the task of translating conversations can be seen as a two-in-one task, modelling both dialogue and document-level translation at the same time. In order to evaluate the translation quality of the participating systems we use both automatic metrics (BLEU (Papineni et al., 2002) and TER (Snover et al., 2006)), and human evaluation, consisting of Direct Assessment (DA). For DA, we define the evaluation process similarly to last year’s WMT News Translation task (Barrault et al., 2019) with document-level context and following the set of recommendations of L¨aubli et al. (2020). However, differently than the News task, here we rely on professional translators instead of a crowd. This is mainly based on the observations of L¨aubli et al. (2020), which provides evidence of the professional translators having better judgment and ability to detect fine-grained phenomena. Six teams participated in this f"
2020.wmt-1.3,N19-1313,1,0.869815,"Missing"
2020.wmt-1.3,W17-4811,0,0.0264946,"Missing"
2020.wmt-1.3,W18-6312,0,0.0139961,"s (BLEU and TER) for evaluating the translations of both agent and customer messages and human document-level direct assessments to evaluate the agent translations. 1 Introduction Despite the significant progress in Neural Machine Translation (NMT) in the last years (Vaswani et al., 2017; Hassan et al., 2018), most systems still operate at sentence-level, disregarding the context of previous sentences. It has been pointed out that ignoring the context may degrade the quality of translations, leading to incorrect choice of pronouns, lexical inconsistency, and incoherence (L¨aubli et al., 2018; Toral et al., 2018). This is particularly relevant in the context of bilingual chat translation, which normally consists of short messages, referencing each other, and where the correct lexical choice to translate a speaker might have been uttered in a previous turn by the other speaker. Numerous systems have been proposed recently to address document-level translation (Tiedemann ∗ These authors contributed equally. 65 Proceedings of the 5th Conference on Machine Translation (WMT), pages 65–75 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics The main motivation of this shared task i"
2020.wmt-1.3,W18-6311,1,0.544991,"ean et al., 2017; Voita et al., 2018, 2019a; JunczysDowmunt, 2019; Lopes et al., 2020), focusing on extending both Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Transformer (Vaswani et al., 2017) with additional encoders or decoders to incorporate previous sentences context. However, often, the approaches are developed for single speaker and document-like tasks. By contrast, in this shared task, we focus on the online multispeaker and multi-lingual setting, where each participant in the conversation speaks in their native language. This task has been first considered by Maruf et al. (2018). In the first round of the Chat Translation shared task, we propose translating dialogues with two speakers, where the first speaker is speaking in the German→English direction and the second is speaking in the English→German. Moreover, we tailor this task for a specific use case: translating conversational text of the customer support chats. In this setting the utterances of the German speaking customer are translated using a machine translation system into English. Then, the replies of the English speaking agent are translated into German and sent to the customer. Translating conversational"
2020.wmt-1.3,D18-1325,0,0.0292722,"Missing"
2020.wmt-1.3,Q18-1029,0,0.0312375,"Missing"
2020.wmt-1.3,D19-1081,0,0.0244263,"Missing"
2020.wmt-1.3,P19-1116,0,0.0720709,"Missing"
2020.wmt-1.3,P18-1117,0,0.0615757,"Missing"
2020.wmt-1.3,2020.wmt-1.60,0,0.0353678,".1.2 4.1.4 Tencent systems are based on self-attention networks including document-level multi-encoder and sentence-level Transformer. In order to get more in-domain data the authors use a multi-feature data selection method (e.g. FDA, n-gram LM, Transformer LM and BERT) to select data from news corpus. Furthermore, the systems have different fine-tuning strategies, ranging from sentence-level to document-level. Finally, these systems use large scale pre-trained language models including monolingual BERT (Devlin et al., 2018) and bilingual XLM (Lample and Conneau, 2019). For more details see (Wang et al., 2020). 4.1.5 University of Maryland The University of Maryland systems are both sentence and document-level systems, with two distinct architectures for this task: (i) standard transformer pre-trained on WMT17 News and finetuned on the WMT20 Chat data, and (ii) modified transformer by including additional encoder to process one previous utterance in tandem with the current utterance, also pre-trained on WMT17 News and fine-tuned on a mix of WMT20 Chat data and a subset of WMT19 News data. The primary system is based on the first architecture while the second architecture is used for the two contras"
2020.wmt-1.3,D18-1049,0,0.0402907,"Missing"
2020.wmt-1.79,N13-1073,0,0.0416194,"rip • Source side: Each word in the source side is labelled as OK (correctly translated) or BAD (caused a translation error). • Target side: Each word in the target side is labelled as OK (a correct translation) or BAD (should be replaced or deleted). Additionally, we consider gap ‘tokens’ at the beginning of the sentence, at the end and between each two words. They are labelled OK if no word should be inserted in that position (according to the post-edited version), and BAD otherwise. In order to obtain the labels, we first align source and MT using the IBM Model 2 alignments from FastAlign (Dyer et al., 2013), and compute edit distances between the generated and post-edited translations with TERCOM, using default settings and disabled shifts. 2.3 Task 3: Predicting document-level MQM This task consists in finding document-level translation errors and estimating a quality score according to the amount of minor, major, and critical errors present in the translation. The predictions are compared to a ground-truth obtained from annotations produced by crowd-sourced human translators from Unbabel community. Each document contains zero or more errors, annotated according to the MQM taxonomy6 , and 5 htt"
2020.wmt-1.79,D19-1632,1,0.894336,"Missing"
2020.wmt-1.79,2020.wmt-1.117,0,0.0617894,"Missing"
2020.wmt-1.79,2020.evalnlgeval-1.4,0,0.061105,"Missing"
2020.wmt-1.79,W19-5406,1,0.692374,"Missing"
2020.wmt-1.79,P19-3020,1,0.743217,"Missing"
2020.wmt-1.79,W17-4763,0,0.358683,"and 2019 to the training set, keeping the same development set from 2019, and released a new test set. The documents are short product title and descriptions in English, extracted from the Amazon Product Reviews dataset (McAuley et al., 2015; He and McAuley, 2016) (Sports and Outdoors category). The documents were machine translated into French using a state of the art online neural MT system. The dataset statistics are presented in Table 2. 3 Baseline systems Sentence-level baseline systems: For Tasks 1 and 2, both word and sentence-level, we used the LSTM-based Predictor-Estimator approach (Kim et al., 2017), implemented in OpenKiwi (Kepler et al., 2019b). The Predictor model was trained on the same parallel data as the NMT systems for each language pair (made available at the task website),7 while the the Estimator was trained on the 7, 000 QE labelled data for each task. Word-level baseline systems: For Task 2, we also used the Predictor-Estimator as above, but it was trained to predict jointly word-level tags and sentence-level scores. Document-level baseline system: For Task 3, similarly as last year, we used a baseline which treats sentences independently and casts the problem as word-level"
2020.wmt-1.79,2020.wmt-1.118,0,0.169813,"Missing"
2020.wmt-1.79,2020.wmt-1.119,1,0.89237,"Missing"
2020.wmt-1.79,2020.wmt-1.120,0,0.0548717,"Missing"
2020.wmt-1.79,W19-5333,0,0.0229976,"with a bilingual parallel corpus, and the entire model is then fine-tuned on the training quality labelled dataset of the shared task. At test time, the translation outputs, which are estimated with teacher forcing and special masking, are put together with the source sentences and put through a unified neural network model to predict the quality of the translations. Mak (T1): Mak represents the source and its translation sentence pairs as a set of 70 blackbox sentence-level features extracted with Quest++(Specia et al., 2015), using the resources used to train the English-Russian NMT system (Ng et al., 2019). Those features are then fitted into a support vector regressor with default settings. NICT Kyoto (T2): The English–German and English-Chinese sentence-level QE systems for Task 2 are ensembles of pre-trained crosslingual language models (XLM) (Conneau and Lample, 2019), fine-tuned in a multi-task fashion with two linear output layers for sentence and word-level quality estimation. A total of 8 XLM models with various masking hyper-parameters were domain-adapted using a subset of the additional resources provided by the QE shared task organisers, as well as a subset of the WikiMatrix corpus ["
2020.wmt-1.79,N19-4009,0,0.0363276,"refer to as direct assessment (DA). For that, a new dataset, was created containing seven languages pairs using sentences mostly from Wikipedia2 . These language pairs are divided into 3 categories: the high-resource English→German (En-De), English→Chinese (En-Zh) and Russian→English (Ru-En) pairs; the medium-resource Romanian→English (RoEn) and Estonian→English (Et-En) pairs; and the low-resource Sinhala→English (Si-En) and Nepali→English (Ne-En) pairs. Translations were produced with state-of-theart transformer-based NMT models trained using publicly available data and the fairseq toolkit (Ott et al., 2019); and were manually annotated for perceived quality. The quality label for this task ranges from 0 to 100, following the FLORES guidelines (Guzm´an et al., 2019). According to the guidelines given to annotators, the 0-10 range represents an incorrect translation; 11-29, a translation with few correct keywords, but the overall meaning is different from the source; 30-50, a translation with major mistakes; 51-69, a translation which is understandable and conveys the overall meaning of the source but contains typos or grammatical errors; 70-90, a translation that closely preserves the semantics o"
2020.wmt-1.79,2020.wmt-1.122,0,0.191132,"Missing"
2020.wmt-1.79,D19-1410,0,0.0226745,". For sentence-level, the different models are used as feature extractors, which are used as inputs of a dense layer to produce the predictions. For word-level, they use majority voting to ensemble the different models. Papago (T1, T3): Papago’s submission for Task 1 749 En-De is an ensemble of three models based on pre-trained contextualised representations: multilingual BERT (mBERT), XLM-MaskedLanguage-Modelling (XLM-MLM), and XLM-Causal-Language-Modelling (XLMCLM). Three scores were produced from these models: an extension of BERTScore using the multilingual BERT model, SentenceBERT score (Reimers and Gurevych, 2019), and target (German) language model score using a pre-trained GPT-2 model. Additionally, the scores were computed for synthetic data created using WMT News translation data by randomly performing different methods, including swapping word order, omiting words or repeating phrases. The three models are pre-trained from these data in a multi-task regression setting. Lastly, these pre-trained models are fine-tuned using the QE corpus. For Task 3, the submitted system uses an ensemble of four models leveraging either multilingual BERT or XLM. The training scheme is very task-oriented: erroneous s"
2020.wmt-1.79,2020.wmt-1.121,0,0.0571448,"Missing"
2020.wmt-1.79,2016.amta-researchers.2,0,0.082682,"for errors: minor (if it is not misleading nor changes meaning), major (if it changes meaning), and critical (if it changes meaning and carries any kind of implication, possibly offensive). Figure 1 shows an example of fine-grained error annotations for a sentence. Note that there is an annotation composed by two discontinuous spans: a whitespace and the token Grip — in this case, the annotation indicates wrong word order, and Grip should have been at the whitespace position. Document-level scores were then generated from the word-level errors and their severity using the method described in Sanchez-Torron and Koehn (2016, footnote 6). Namely, denoting by n the number of words in the document, and by nmin , nmaj , and ncri the number of annotated minor, major, and critical errors, the final quality scores were computed as: see MQM = 1 − 745 nminor + 5nmajor + 10ncrit (1) n Note that MQM values can be negative if the total severity exceeds the number of words. As this year’s dataset, we reused the training data from previous years, adding the test sets from 2018 and 2019 to the training set, keeping the same development set from 2019, and released a new test set. The documents are short product title and descri"
2020.wmt-1.79,P15-4020,1,0.771605,"irectional LSTM. The parameters of the Transformer bottleneck layer are first optimised with a bilingual parallel corpus, and the entire model is then fine-tuned on the training quality labelled dataset of the shared task. At test time, the translation outputs, which are estimated with teacher forcing and special masking, are put together with the source sentences and put through a unified neural network model to predict the quality of the translations. Mak (T1): Mak represents the source and its translation sentence pairs as a set of 70 blackbox sentence-level features extracted with Quest++(Specia et al., 2015), using the resources used to train the English-Russian NMT system (Ng et al., 2019). Those features are then fitted into a support vector regressor with default settings. NICT Kyoto (T2): The English–German and English-Chinese sentence-level QE systems for Task 2 are ensembles of pre-trained crosslingual language models (XLM) (Conneau and Lample, 2019), fine-tuned in a multi-task fashion with two linear output layers for sentence and word-level quality estimation. A total of 8 XLM models with various masking hyper-parameters were domain-adapted using a subset of the additional resources provi"
2020.wmt-1.79,2020.acl-main.558,1,0.893099,"Missing"
2020.wmt-1.79,2020.wmt-1.123,0,0.0620909,"Missing"
2020.wmt-1.79,2020.wmt-1.124,0,0.0510029,"Missing"
2020.wmt-1.79,2020.wmt-1.125,0,0.0843421,"Missing"
2021.acl-long.505,N18-1118,0,0.321312,"ms fail to outperform simpler baselines with respect to overall translation accuracy, pronoun translation, or lexical cohesion (Lopes et al., 2020). We hypothesize that one major reason for these lacklustre results is due to the fact that models with the architectural capacity to model cross-sentential context do not necessarily learn to do so when trained with existing training paradigms. However, even quantifying model usage of context is an ongoing challenge; while contrastive evaluation has been proposed to measure performance on inter-sentential discourse phenomena (M¨uller et al., 2018; Bawden et al., 2018), this approach is confined to a narrow set of phenomena, such as pronoun translation and lexical cohesion. A toolbox to measure the impact of context in broader settings is still missing. 6467 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6467–6478 August 1–6, 2021. ©2021 Association for Computational Linguistics Source: The Church is merciful. . . It always welcomes the misguided lamb. Target: Baseline Die Kirche ist barmherzig. . . Es heisst die fehlgeleiteten Sch¨afle"
2021.acl-long.505,2020.acl-main.149,0,0.250692,"ine Die Kirche ist barmherzig. . . Es heisst die fehlgeleiteten Sch¨aflein immer willkommen. Context-Aware Es heisst die fehlgeleiteten Sch¨aflein immer willkommen. Context-Aware Sie heisst die fehlgeleiteten Sch¨aflein immer w/ our method willkommen. Table 1: Example where context (italic) is needed to correctly translate the pronoun “it”. Both the sentencelevel baseline and context-aware model fail to correctly translate it while the context-aware model trained with C OW ORD dropout correctly captures the context. To address the limitations above, we take inspiration from the recent work of Bugliarello et al. (2020) and propose a new metric, conditional cross-mutual information (CXMI, §3), to measure quantitatively how much context-aware models actually use the provided context by comparing the model distributions over a dataset with and without context. Figure 1 illustrates how it measures context usage. This metric applies to any probabilistic context-aware machine translation model, not only the ones used in this paper. We release a software package to encourage the use of this metric in future context-aware machine translation research. We then perform a rigorous empirical analysis of the CXMI betwee"
2021.acl-long.505,2012.eamt-1.60,0,0.0344786,"s, training regimens, or random seeds. To address this we consider a single model, qM T , that is able to translate with and without context (more on how this achieved in §3.2). We can then set the context-agnostic model and the contextual model to be the same model qM TA = qM TC = qM T . This way we attribute the information gain to the introduction of context. Throughout the rest of this work, when we reference “context usage” we will precisely mean this information gain (or loss). 3.2 Experiments Data We experiment with a document-level translation task by training models on the IWSLT2017 (Cettolo et al., 2012) dataset for language pairs EN → DE and EN → FR (with approximately 200K sentences for both pairs). We use the test sets 2011-2014 as validation sets and the 2015 as test sets. To address the concerns pointed out by Lopes et al. (2020) that gains in performance are due to the use of small training corpora and weak baselines, we use Paracrawl (Espl`a et al., 2019) and perform some data cleaning based on language identification tools, creating a pretraining dataset of around 6469 82M and 104M sentence pairs for EN → DE and EN → FR respectively. All data is encoded/vectorized with byte-pair encod"
2021.acl-long.505,2020.autosimtrans-1.5,0,0.0177006,"proposed the simple approach of concatenating the previous sentences in both the source and target side to the input to the system; Jean et al. (2017), Bawden et al. (2018), and Zhang et al. (2018) used an additional contextspecific encoder to extract contextual features from the previous sentences; Maruf and Haffari (2018) and Tu et al. (2018b) used cache-based memories to encode context; Wang et al. (2017) used a hierarchical RNN to encode the global context from all previous sentences; Miculicich et al. (2018) and Maruf et al. (2019a) used hierarchical attention networks to encode context; Chen et al. (2020) added document-level discourse structure information to the input; Sun et al. (2020) trained a simple concatenation-based model with varying context size during training to have a model that is able to translate with any context size, similar to what is done in this work. Similarly to what we do with C OW ORD dropout, Jean and Cho (2019) attempted to maximise sensitivity to context by introducing a margin-based regularization term to explicitly encourage context usage. For a more detailed overview, Maruf et al. (2019b) extensively describe the different approaches and how they leverage contex"
2021.acl-long.505,W19-6721,0,0.0415648,"Missing"
2021.acl-long.505,D19-6503,0,0.0321931,"Missing"
2021.acl-long.505,D18-2012,0,0.0306719,"Missing"
2021.acl-long.505,D18-1512,0,0.0481132,"Missing"
2021.acl-long.505,2020.eamt-1.24,1,0.809112,"t models (Tiedemann and Scherrer, 2017), using different encoders for context (Zhang et al., 2018), having cache-based memories (Tu et al., 2018a), or using models with hierarchical attention mechanisms (Miculicich et al., 2018; Maruf et al., 2019a) — more details in §2. While such works report gains in translation quality compared to sentence-level baselines trained on small datasets, recent work has shown that, in more realistic high-resourced scenarios, these systems fail to outperform simpler baselines with respect to overall translation accuracy, pronoun translation, or lexical cohesion (Lopes et al., 2020). We hypothesize that one major reason for these lacklustre results is due to the fact that models with the architectural capacity to model cross-sentential context do not necessarily learn to do so when trained with existing training paradigms. However, even quantifying model usage of context is an ongoing challenge; while contrastive evaluation has been proposed to measure performance on inter-sentential discourse phenomena (M¨uller et al., 2018; Bawden et al., 2018), this approach is confined to a narrow set of phenomena, such as pronoun translation and lexical cohesion. A toolbox to measur"
2021.acl-long.505,P18-1118,0,0.0182091,"similar to the sentence-level baseline, while when dropout is applied, they are able to effectively start using context. Related Work Context-aware Machine Translation There have been many works in the literature that try to incorporate context into NMT systems. Tiedemann and Scherrer (2017) first proposed the simple approach of concatenating the previous sentences in both the source and target side to the input to the system; Jean et al. (2017), Bawden et al. (2018), and Zhang et al. (2018) used an additional contextspecific encoder to extract contextual features from the previous sentences; Maruf and Haffari (2018) and Tu et al. (2018b) used cache-based memories to encode context; Wang et al. (2017) used a hierarchical RNN to encode the global context from all previous sentences; Miculicich et al. (2018) and Maruf et al. (2019a) used hierarchical attention networks to encode context; Chen et al. (2020) added document-level discourse structure information to the input; Sun et al. (2020) trained a simple concatenation-based model with varying context size during training to have a model that is able to translate with any context size, similar to what is done in this work. Similarly to what we do with C OW"
2021.acl-long.505,N19-1313,1,0.873371,"el evaluation (Toral et al., 2018; L¨aubli et al., 2018). Recent work on contextaware NMT attempts to alleviate this discrepancy by incorporating the surrounding context sentences (in either or both the source and target sides) in the translation system. This can be done by, for example, feeding context sentences to standard NMT 1 https://github.com/neulab/contextual-mt models (Tiedemann and Scherrer, 2017), using different encoders for context (Zhang et al., 2018), having cache-based memories (Tu et al., 2018a), or using models with hierarchical attention mechanisms (Miculicich et al., 2018; Maruf et al., 2019a) — more details in §2. While such works report gains in translation quality compared to sentence-level baselines trained on small datasets, recent work has shown that, in more realistic high-resourced scenarios, these systems fail to outperform simpler baselines with respect to overall translation accuracy, pronoun translation, or lexical cohesion (Lopes et al., 2020). We hypothesize that one major reason for these lacklustre results is due to the fact that models with the architectural capacity to model cross-sentential context do not necessarily learn to do so when trained with existing tr"
2021.acl-long.505,D18-1325,0,0.087652,"er hold with document-level evaluation (Toral et al., 2018; L¨aubli et al., 2018). Recent work on contextaware NMT attempts to alleviate this discrepancy by incorporating the surrounding context sentences (in either or both the source and target sides) in the translation system. This can be done by, for example, feeding context sentences to standard NMT 1 https://github.com/neulab/contextual-mt models (Tiedemann and Scherrer, 2017), using different encoders for context (Zhang et al., 2018), having cache-based memories (Tu et al., 2018a), or using models with hierarchical attention mechanisms (Miculicich et al., 2018; Maruf et al., 2019a) — more details in §2. While such works report gains in translation quality compared to sentence-level baselines trained on small datasets, recent work has shown that, in more realistic high-resourced scenarios, these systems fail to outperform simpler baselines with respect to overall translation accuracy, pronoun translation, or lexical cohesion (Lopes et al., 2020). We hypothesize that one major reason for these lacklustre results is due to the fact that models with the architectural capacity to model cross-sentential context do not necessarily learn to do so when trai"
2021.acl-long.505,W18-6307,0,0.0348686,"Missing"
2021.acl-long.505,N19-4009,0,0.0603517,"Missing"
2021.acl-long.505,P02-1040,0,0.113275,"017) transformer small model (more details in Appendix §C). For all models with target context, when decoding, we use the previous decoded sentences as target context. Table 4 shows the performance across three different seeds of the baseline and contextual models for both the non-pretrained and pretrained setting, with increasing values of C OW ORD dropout p. We also run the baseline with C OW ORD dropout (which, as said previously, reduces to word dropout) to ensure that improvements were not only due to regularization effects on the current source/target. We report the standard BLEU score (Papineni et al., 2002) calculated using sacreBLEU (Post, 2018) and COMET, a more accurate evaluation method using multilingual embeddings (Rei et al., 2020). For the non-pretrained case, we can see that a C OW ORD dropout value p &gt; 0 consistently improves the performance of the contextual models when compared to models running with p = 0 and with the sentence-level baseline with the same values for word dropout. For the pretrained case, the improvements are not as noticeable, although models trained with C OW ORD dropout still always outperform models trained without it. This is perhaps a reflection of the general"
2021.acl-long.505,W18-6319,0,0.0146151,"dix §C). For all models with target context, when decoding, we use the previous decoded sentences as target context. Table 4 shows the performance across three different seeds of the baseline and contextual models for both the non-pretrained and pretrained setting, with increasing values of C OW ORD dropout p. We also run the baseline with C OW ORD dropout (which, as said previously, reduces to word dropout) to ensure that improvements were not only due to regularization effects on the current source/target. We report the standard BLEU score (Papineni et al., 2002) calculated using sacreBLEU (Post, 2018) and COMET, a more accurate evaluation method using multilingual embeddings (Rei et al., 2020). For the non-pretrained case, we can see that a C OW ORD dropout value p &gt; 0 consistently improves the performance of the contextual models when compared to models running with p = 0 and with the sentence-level baseline with the same values for word dropout. For the pretrained case, the improvements are not as noticeable, although models trained with C OW ORD dropout still always outperform models trained without it. This is perhaps a reflection of the general trend that better models are harder to i"
2021.acl-long.505,2020.emnlp-main.213,0,0.0373237,"sentences as target context. Table 4 shows the performance across three different seeds of the baseline and contextual models for both the non-pretrained and pretrained setting, with increasing values of C OW ORD dropout p. We also run the baseline with C OW ORD dropout (which, as said previously, reduces to word dropout) to ensure that improvements were not only due to regularization effects on the current source/target. We report the standard BLEU score (Papineni et al., 2002) calculated using sacreBLEU (Post, 2018) and COMET, a more accurate evaluation method using multilingual embeddings (Rei et al., 2020). For the non-pretrained case, we can see that a C OW ORD dropout value p &gt; 0 consistently improves the performance of the contextual models when compared to models running with p = 0 and with the sentence-level baseline with the same values for word dropout. For the pretrained case, the improvements are not as noticeable, although models trained with C OW ORD dropout still always outperform models trained without it. This is perhaps a reflection of the general trend that better models are harder to improve. 6472 Source Context Source More people watched games because it was faster. The ball c"
2021.acl-long.505,W16-2323,0,0.26832,"then perform a rigorous empirical analysis of the CXMI between the context and target for different context sizes, and between source and target context. We find that: (1) context-aware models use some information from the context, but the amount of information used does not increase uniformly with the context size, and can even lead to a reduction in context usage; (2) target context seems to be used more by models than source context. Given the findings, we next consider how to encourage models to use more context. Specifically, we introduce a simple but effective variation of word dropout (Sennrich et al., 2016a) for context-aware machine translation, dubbed C OW ORD dropout (§4). Put simply, we randomly drop words from the current source sentence by replacing them with a placeholder token. Intuitively, this encourages the model to use extra-sentential information to compensate for the missing information in the current source sentence. We show that models trained with C OW ORD dropout not only increase context usage compared to models trained without it but also improve the quality of translation, both according to standard evaluation metrics (BLEU and COMET) and according to contrastive evaluation"
2021.acl-long.505,P16-1162,0,0.392074,"then perform a rigorous empirical analysis of the CXMI between the context and target for different context sizes, and between source and target context. We find that: (1) context-aware models use some information from the context, but the amount of information used does not increase uniformly with the context size, and can even lead to a reduction in context usage; (2) target context seems to be used more by models than source context. Given the findings, we next consider how to encourage models to use more context. Specifically, we introduce a simple but effective variation of word dropout (Sennrich et al., 2016a) for context-aware machine translation, dubbed C OW ORD dropout (§4). Put simply, we randomly drop words from the current source sentence by replacing them with a placeholder token. Intuitively, this encourages the model to use extra-sentential information to compensate for the missing information in the current source sentence. We show that models trained with C OW ORD dropout not only increase context usage compared to models trained without it but also improve the quality of translation, both according to standard evaluation metrics (BLEU and COMET) and according to contrastive evaluation"
2021.acl-long.505,2020.coling-main.417,0,0.018091,"r measuring context usage and the proposed regularization method of C OW ORD dropout, can theoretically be applied to any of the above-mentioned methods. Evaluation In terms of evaluation, most previous work focuses on targeting a system’s performance on contrastive datasets for specific inter-sentential discourse phenomena. M¨uller et al. (2018) built a large-scale dataset for anaphoric pronoun resolution, Bawden et al. (2018) manually created a dataset for both pronoun resolution and lexical choice and Voita et al. (2019) created a dataset that targets deixis, ellipsis and lexical cohesion. Stojanovski et al. (2020) showed through adversarial attacks that models that do well on other contrastive datasets rely on surface heuristics and create a contrastive dataset to address this. In contrast, our CXMI metric is phenomenon-agnostic and can be measured with respect to all phenomena that require context in translation. Information-Theoretic Analysis Bugliarello et al. (2020) first proposed cross-mutual information (XMI) in the context of measuring the difficulty of translating between languages. Our work differs in that we propose a conditional version of XMI, where S is always observed, and we use it to as"
2021.acl-long.505,W17-4811,0,0.300471,"duction While neural machine translation (NMT) is reported to have achieved human parity in some domains and language pairs (Hassan et al., 2018), these claims seem overly optimistic and no longer hold with document-level evaluation (Toral et al., 2018; L¨aubli et al., 2018). Recent work on contextaware NMT attempts to alleviate this discrepancy by incorporating the surrounding context sentences (in either or both the source and target sides) in the translation system. This can be done by, for example, feeding context sentences to standard NMT 1 https://github.com/neulab/contextual-mt models (Tiedemann and Scherrer, 2017), using different encoders for context (Zhang et al., 2018), having cache-based memories (Tu et al., 2018a), or using models with hierarchical attention mechanisms (Miculicich et al., 2018; Maruf et al., 2019a) — more details in §2. While such works report gains in translation quality compared to sentence-level baselines trained on small datasets, recent work has shown that, in more realistic high-resourced scenarios, these systems fail to outperform simpler baselines with respect to overall translation accuracy, pronoun translation, or lexical cohesion (Lopes et al., 2020). We hypothesize tha"
2021.acl-long.505,W18-6312,0,0.0157198,"slation quality according to metrics such as BLEU and COMET, as well as performance on anaphoric pronoun resolution and lexical cohesion contrastive datasets.1 1 Figure 1: Illustration of how we can measure context usage by a model qM T as the amount of information gained when a model is given the context C and source X vs when the model is only given the X. Introduction While neural machine translation (NMT) is reported to have achieved human parity in some domains and language pairs (Hassan et al., 2018), these claims seem overly optimistic and no longer hold with document-level evaluation (Toral et al., 2018; L¨aubli et al., 2018). Recent work on contextaware NMT attempts to alleviate this discrepancy by incorporating the surrounding context sentences (in either or both the source and target sides) in the translation system. This can be done by, for example, feeding context sentences to standard NMT 1 https://github.com/neulab/contextual-mt models (Tiedemann and Scherrer, 2017), using different encoders for context (Zhang et al., 2018), having cache-based memories (Tu et al., 2018a), or using models with hierarchical attention mechanisms (Miculicich et al., 2018; Maruf et al., 2019a) — more detai"
2021.acl-long.505,Q18-1029,0,0.2931,"airs (Hassan et al., 2018), these claims seem overly optimistic and no longer hold with document-level evaluation (Toral et al., 2018; L¨aubli et al., 2018). Recent work on contextaware NMT attempts to alleviate this discrepancy by incorporating the surrounding context sentences (in either or both the source and target sides) in the translation system. This can be done by, for example, feeding context sentences to standard NMT 1 https://github.com/neulab/contextual-mt models (Tiedemann and Scherrer, 2017), using different encoders for context (Zhang et al., 2018), having cache-based memories (Tu et al., 2018a), or using models with hierarchical attention mechanisms (Miculicich et al., 2018; Maruf et al., 2019a) — more details in §2. While such works report gains in translation quality compared to sentence-level baselines trained on small datasets, recent work has shown that, in more realistic high-resourced scenarios, these systems fail to outperform simpler baselines with respect to overall translation accuracy, pronoun translation, or lexical cohesion (Lopes et al., 2020). We hypothesize that one major reason for these lacklustre results is due to the fact that models with the architectural cap"
2021.acl-long.505,P19-1116,0,0.0310266,"Missing"
2021.acl-long.505,D17-1301,0,0.0440336,"ctively start using context. Related Work Context-aware Machine Translation There have been many works in the literature that try to incorporate context into NMT systems. Tiedemann and Scherrer (2017) first proposed the simple approach of concatenating the previous sentences in both the source and target side to the input to the system; Jean et al. (2017), Bawden et al. (2018), and Zhang et al. (2018) used an additional contextspecific encoder to extract contextual features from the previous sentences; Maruf and Haffari (2018) and Tu et al. (2018b) used cache-based memories to encode context; Wang et al. (2017) used a hierarchical RNN to encode the global context from all previous sentences; Miculicich et al. (2018) and Maruf et al. (2019a) used hierarchical attention networks to encode context; Chen et al. (2020) added document-level discourse structure information to the input; Sun et al. (2020) trained a simple concatenation-based model with varying context size during training to have a model that is able to translate with any context size, similar to what is done in this work. Similarly to what we do with C OW ORD dropout, Jean and Cho (2019) attempted to maximise sensitivity to context by intr"
2021.acl-long.505,D18-1049,0,0.0701837,"Missing"
2021.acl-long.65,D18-1216,0,0.0285654,"e above 795 disparities by collecting human supporting context to regularize model attention heads during training. 6.2 Attention Mechanisms Though attention is usually learned in an unsupervised manner, recent work supervises attention with word alignments (Mi et al., 2016; Liu et al., 2016), event arguments and trigger words (Liu et al., 2017; Zhao et al., 2018), syntactic dependencies (Strubell et al., 2018) or word lexicons (Zou et al., 2018). Our work is closely related to a large body of work that supervises attention using human rationales for text classification (Barrett et al., 2018; Bao et al., 2018; Zhong et al., 2019; Choi et al., 2020; Pruthi et al., 2020). Our work, however, is the first to collect human evidence for document translation and use it to regularize the attention of NMT models. 7 evaluated to perform translation, not disambiguation. A future step would be to explore alternative methods for extracting supporting context, such as eye-tracking during translation (O’Brien, 2009). Implications and Future Work In this work, we collected a corpus of supporting context for translating ambiguous words. We examined how baseline context-aware translation models use context, and dem"
2021.acl-long.65,K18-1030,0,0.0269805,"Our work addresses the above 795 disparities by collecting human supporting context to regularize model attention heads during training. 6.2 Attention Mechanisms Though attention is usually learned in an unsupervised manner, recent work supervises attention with word alignments (Mi et al., 2016; Liu et al., 2016), event arguments and trigger words (Liu et al., 2017; Zhao et al., 2018), syntactic dependencies (Strubell et al., 2018) or word lexicons (Zou et al., 2018). Our work is closely related to a large body of work that supervises attention using human rationales for text classification (Barrett et al., 2018; Bao et al., 2018; Zhong et al., 2019; Choi et al., 2020; Pruthi et al., 2020). Our work, however, is the first to collect human evidence for document translation and use it to regularize the attention of NMT models. 7 evaluated to perform translation, not disambiguation. A future step would be to explore alternative methods for extracting supporting context, such as eye-tracking during translation (O’Brien, 2009). Implications and Future Work In this work, we collected a corpus of supporting context for translating ambiguous words. We examined how baseline context-aware translation models us"
2021.acl-long.65,N18-1118,0,0.268261,"(2018), inter alia), it is unclear whether models rely on the “right” context that is actually sufficient to disambiguate difficult translations. Even when additional context 1 Our SCAT data and code for experiments are available at https://github.com/neulab/contextual-mt. Look after her a lot. Okay. Any questions? Have we got her report? Yes, it’s in the infirmary already Dorlotez-la. D’accord. Vous avez des questions ? On dispose de son rapport. Oui, il est a` l’infirmerie. is provided, models often perform poorly on evaluation of relatively simple discourse phenomena (M¨uller et al., 2018; Bawden et al., 2018; Voita et al., 2019b,a; Lopes et al., 2020) and rely on spurious word co-occurences during translation of polysemous words (Emelin et al., 2020). Some evidence suggests that models attend to uninformative tokens (Voita et al., 2018) and do not use contextual information adequately (Kim et al., 2019). To understand plausibly why current NMT models are unable to fully leverage the disambiguating context they are provided, and how we can develop models that use context more effectively, we pose the following research questions: (i) In context aware translation, what context is intrinsically usef"
2021.acl-long.65,E06-1032,0,0.283281,"Missing"
2021.acl-long.65,2020.autosimtrans-1.5,0,0.0145477,"ent context-aware NMT approaches enhance NMT by including source- and/or targetside surrounding sentences as context to the model. Tiedemann and Scherrer (2017) concatenate the previous sentences to the input; Jean et al. (2017); Bawden et al. (2018); Zhang et al. (2018) use an additional encoder to extract contextual features; Wang et al. (2017) use a hierarchical RNN to encode the global context from all previous sentences; Maruf and Haffari (2018); Tu et al. (2018) use cache-based memories to encode context; Miculicich et al. (2018); Maruf et al. (2019) use hierarchical attention networks; Chen et al. (2020) add document-level discourse structure information to the input. While Maruf et al. (2019); Voita et al. (2018) also find higher attention mass attributed to relevant tokens in selected examples, our work is the first to guide model attention in context-aware NMT using human supervision and analyze its attention distribution in a quantitative manner. However, recent studies suggest that current context-aware NMT models often do not use context meaningfully. Kim et al. (2019) claim that improvements by context-aware models are mostly from regularization by reserving parameters for context inpu"
2021.acl-long.65,2020.emnlp-main.543,0,0.0252693,"human supporting context to regularize model attention heads during training. 6.2 Attention Mechanisms Though attention is usually learned in an unsupervised manner, recent work supervises attention with word alignments (Mi et al., 2016; Liu et al., 2016), event arguments and trigger words (Liu et al., 2017; Zhao et al., 2018), syntactic dependencies (Strubell et al., 2018) or word lexicons (Zou et al., 2018). Our work is closely related to a large body of work that supervises attention using human rationales for text classification (Barrett et al., 2018; Bao et al., 2018; Zhong et al., 2019; Choi et al., 2020; Pruthi et al., 2020). Our work, however, is the first to collect human evidence for document translation and use it to regularize the attention of NMT models. 7 evaluated to perform translation, not disambiguation. A future step would be to explore alternative methods for extracting supporting context, such as eye-tracking during translation (O’Brien, 2009). Implications and Future Work In this work, we collected a corpus of supporting context for translating ambiguous words. We examined how baseline context-aware translation models use context, and demonstrated how context annotations can i"
2021.acl-long.65,2020.eamt-1.24,1,0.738327,"odels rely on the “right” context that is actually sufficient to disambiguate difficult translations. Even when additional context 1 Our SCAT data and code for experiments are available at https://github.com/neulab/contextual-mt. Look after her a lot. Okay. Any questions? Have we got her report? Yes, it’s in the infirmary already Dorlotez-la. D’accord. Vous avez des questions ? On dispose de son rapport. Oui, il est a` l’infirmerie. is provided, models often perform poorly on evaluation of relatively simple discourse phenomena (M¨uller et al., 2018; Bawden et al., 2018; Voita et al., 2019b,a; Lopes et al., 2020) and rely on spurious word co-occurences during translation of polysemous words (Emelin et al., 2020). Some evidence suggests that models attend to uninformative tokens (Voita et al., 2018) and do not use contextual information adequately (Kim et al., 2019). To understand plausibly why current NMT models are unable to fully leverage the disambiguating context they are provided, and how we can develop models that use context more effectively, we pose the following research questions: (i) In context aware translation, what context is intrinsically useful to disambiguate hard translation phenomen"
2021.acl-long.65,P18-1118,0,0.112577,", otherwise “elle”. The model with regularized attention translates the pronoun correctly, with the largest attention on the referent “report”. Top 3 words with the highest attention are highlighted. Introduction There is a growing consensus in machine translation research that it is necessary to move beyond sentence-level translation and incorporate document-level context (Guillou et al., 2018; L¨aubli et al., 2018; Toral et al., 2018). While various methods to incorporate context in neural machine translation (NMT) have been proposed (Tiedemann and Scherrer (2017); Miculicich et al. (2018); Maruf and Haffari (2018), inter alia), it is unclear whether models rely on the “right” context that is actually sufficient to disambiguate difficult translations. Even when additional context 1 Our SCAT data and code for experiments are available at https://github.com/neulab/contextual-mt. Look after her a lot. Okay. Any questions? Have we got her report? Yes, it’s in the infirmary already Dorlotez-la. D’accord. Vous avez des questions ? On dispose de son rapport. Oui, il est a` l’infirmerie. is provided, models often perform poorly on evaluation of relatively simple discourse phenomena (M¨uller et al., 2018; Bawden"
2021.acl-long.65,N19-1313,1,0.826755,"fRelated Work Context-Aware Machine Translation Most current context-aware NMT approaches enhance NMT by including source- and/or targetside surrounding sentences as context to the model. Tiedemann and Scherrer (2017) concatenate the previous sentences to the input; Jean et al. (2017); Bawden et al. (2018); Zhang et al. (2018) use an additional encoder to extract contextual features; Wang et al. (2017) use a hierarchical RNN to encode the global context from all previous sentences; Maruf and Haffari (2018); Tu et al. (2018) use cache-based memories to encode context; Miculicich et al. (2018); Maruf et al. (2019) use hierarchical attention networks; Chen et al. (2020) add document-level discourse structure information to the input. While Maruf et al. (2019); Voita et al. (2018) also find higher attention mass attributed to relevant tokens in selected examples, our work is the first to guide model attention in context-aware NMT using human supervision and analyze its attention distribution in a quantitative manner. However, recent studies suggest that current context-aware NMT models often do not use context meaningfully. Kim et al. (2019) claim that improvements by context-aware models are mostly from"
2021.acl-long.65,D16-1249,0,0.0275114,"use context meaningfully. Kim et al. (2019) claim that improvements by context-aware models are mostly from regularization by reserving parameters for context inputs, and Li et al. (2020b) show that replacing the context in multi-encoder models with random signals leads to similar accuracy as using the actual context. Our work addresses the above 795 disparities by collecting human supporting context to regularize model attention heads during training. 6.2 Attention Mechanisms Though attention is usually learned in an unsupervised manner, recent work supervises attention with word alignments (Mi et al., 2016; Liu et al., 2016), event arguments and trigger words (Liu et al., 2017; Zhao et al., 2018), syntactic dependencies (Strubell et al., 2018) or word lexicons (Zou et al., 2018). Our work is closely related to a large body of work that supervises attention using human rationales for text classification (Barrett et al., 2018; Bao et al., 2018; Zhong et al., 2019; Choi et al., 2020; Pruthi et al., 2020). Our work, however, is the first to collect human evidence for document translation and use it to regularize the attention of NMT models. 7 evaluated to perform translation, not disambiguation. A"
2021.acl-long.65,D18-1325,0,0.089293,"report) then “il” is used, otherwise “elle”. The model with regularized attention translates the pronoun correctly, with the largest attention on the referent “report”. Top 3 words with the highest attention are highlighted. Introduction There is a growing consensus in machine translation research that it is necessary to move beyond sentence-level translation and incorporate document-level context (Guillou et al., 2018; L¨aubli et al., 2018; Toral et al., 2018). While various methods to incorporate context in neural machine translation (NMT) have been proposed (Tiedemann and Scherrer (2017); Miculicich et al. (2018); Maruf and Haffari (2018), inter alia), it is unclear whether models rely on the “right” context that is actually sufficient to disambiguate difficult translations. Even when additional context 1 Our SCAT data and code for experiments are available at https://github.com/neulab/contextual-mt. Look after her a lot. Okay. Any questions? Have we got her report? Yes, it’s in the infirmary already Dorlotez-la. D’accord. Vous avez des questions ? On dispose de son rapport. Oui, il est a` l’infirmerie. is provided, models often perform poorly on evaluation of relatively simple discourse phenomena (M¨"
2021.acl-long.65,W18-6307,0,0.0324485,"Missing"
2021.acl-long.65,2020.lrec-1.457,0,0.0159497,"is dataset, SCAT: Supporting Context for Ambiguous Translations, are provided in Appendix A. Tasks and Data Quality We perform this study for two tasks: pronoun anaphora resolution (PAR), where the translators are tasked with choosing the correct French gendered pronoun associated to a neutral English pronoun, and word sense disambiguation (WSD), where the translators pick the correct translation of a polysemous word. PAR, and WSD to a lesser extent, have been commonly studied to evaluate context-aware NMT models (Voita et al., 2018; Lopes et al., 2020; M¨uller et al., 2018; Huo et al., 2020; Nagata and Morishita, 2020). 2 https://www.upwork.com 790 Word Sense Disambiguation. There are no existing contrastive datasets for WSD with a context window larger than 1 sentence, therefore, we automatically generate contrastive examples with context window of 5 sentences from OpenSubtitles2018 by identifying polysemous English words and possible French translations. We describe our methodology in Appendix B. Quality. For quality control, we asked 8 internal speakers of English and French, with native or bilingual proficiency in both languages, to carefully annotate the same 100 examples given to all professional tran"
2021.acl-long.65,P02-1040,0,0.112704,"ility of a target document Q Y given the source document X: Pθ (Y |X) = Jj=1 Pθ (y j |xj , C j ), where y j and xj are the j-th target and source sentences, and C j is the collection of contextual sentences for the j-th sentence pair. There are many methods for incorporating context (§6), but even simple concatenation (Tiedemann and Scherrer, 2017), which prepends the previous source or target sentences to the current sentence separated by a hBRKi tag, achieves comparable performance to more sophisticated approaches, especially in highresource scenarios (Lopes et al., 2020). Evaluation. BLEU (Papineni et al., 2002) is most widely used to evaluate MT, but it can be poorly correlated with human evaluation (CallisonBurch et al., 2006; Reiter, 2018). Recently, a number of neural evaluation methods, such as COMET (Rei et al., 2020), have shown better correlation with human judgement. Nevertheless, common automatic metrics have limited ability to evaluate discourse in MT (Hardmeier, 2012). As a remedy to this, researchers often use contrastive test sets for a targeted discourse phenomenon (M¨uller et al., 2018), such as pronoun anaphora resolution and word sense disambiguation, to verify if the model ranks th"
2021.acl-long.65,W18-6319,0,0.032967,"Missing"
2021.acl-long.65,J18-3002,0,0.0126862,"rce sentences, and C j is the collection of contextual sentences for the j-th sentence pair. There are many methods for incorporating context (§6), but even simple concatenation (Tiedemann and Scherrer, 2017), which prepends the previous source or target sentences to the current sentence separated by a hBRKi tag, achieves comparable performance to more sophisticated approaches, especially in highresource scenarios (Lopes et al., 2020). Evaluation. BLEU (Papineni et al., 2002) is most widely used to evaluate MT, but it can be poorly correlated with human evaluation (CallisonBurch et al., 2006; Reiter, 2018). Recently, a number of neural evaluation methods, such as COMET (Rei et al., 2020), have shown better correlation with human judgement. Nevertheless, common automatic metrics have limited ability to evaluate discourse in MT (Hardmeier, 2012). As a remedy to this, researchers often use contrastive test sets for a targeted discourse phenomenon (M¨uller et al., 2018), such as pronoun anaphora resolution and word sense disambiguation, to verify if the model ranks the correct translation of an ambiguous sentence higher than the incorrect translation. Document-Level Translation 3 Neural Machine Tra"
2021.acl-long.65,P16-1162,0,0.0343624,"Missing"
2021.acl-long.65,D18-1548,0,0.0429611,"Missing"
2021.acl-long.65,W17-4811,0,0.401074,"ent of “it” is masculine (e.g., report) then “il” is used, otherwise “elle”. The model with regularized attention translates the pronoun correctly, with the largest attention on the referent “report”. Top 3 words with the highest attention are highlighted. Introduction There is a growing consensus in machine translation research that it is necessary to move beyond sentence-level translation and incorporate document-level context (Guillou et al., 2018; L¨aubli et al., 2018; Toral et al., 2018). While various methods to incorporate context in neural machine translation (NMT) have been proposed (Tiedemann and Scherrer (2017); Miculicich et al. (2018); Maruf and Haffari (2018), inter alia), it is unclear whether models rely on the “right” context that is actually sufficient to disambiguate difficult translations. Even when additional context 1 Our SCAT data and code for experiments are available at https://github.com/neulab/contextual-mt. Look after her a lot. Okay. Any questions? Have we got her report? Yes, it’s in the infirmary already Dorlotez-la. D’accord. Vous avez des questions ? On dispose de son rapport. Oui, il est a` l’infirmerie. is provided, models often perform poorly on evaluation of relatively simp"
2021.acl-long.65,W18-6312,0,0.0149766,"son rapport ? Oui, il est d´ej`a a` l’hˆopital Table 1: Translation of the ambiguous pronoun “it”. In French, if the referent of “it” is masculine (e.g., report) then “il” is used, otherwise “elle”. The model with regularized attention translates the pronoun correctly, with the largest attention on the referent “report”. Top 3 words with the highest attention are highlighted. Introduction There is a growing consensus in machine translation research that it is necessary to move beyond sentence-level translation and incorporate document-level context (Guillou et al., 2018; L¨aubli et al., 2018; Toral et al., 2018). While various methods to incorporate context in neural machine translation (NMT) have been proposed (Tiedemann and Scherrer (2017); Miculicich et al. (2018); Maruf and Haffari (2018), inter alia), it is unclear whether models rely on the “right” context that is actually sufficient to disambiguate difficult translations. Even when additional context 1 Our SCAT data and code for experiments are available at https://github.com/neulab/contextual-mt. Look after her a lot. Okay. Any questions? Have we got her report? Yes, it’s in the infirmary already Dorlotez-la. D’accord. Vous avez des questions"
2021.acl-long.65,Q18-1029,0,0.0147868,"ng all context, which may indicate that having irrelevant context can have an adverse efRelated Work Context-Aware Machine Translation Most current context-aware NMT approaches enhance NMT by including source- and/or targetside surrounding sentences as context to the model. Tiedemann and Scherrer (2017) concatenate the previous sentences to the input; Jean et al. (2017); Bawden et al. (2018); Zhang et al. (2018) use an additional encoder to extract contextual features; Wang et al. (2017) use a hierarchical RNN to encode the global context from all previous sentences; Maruf and Haffari (2018); Tu et al. (2018) use cache-based memories to encode context; Miculicich et al. (2018); Maruf et al. (2019) use hierarchical attention networks; Chen et al. (2020) add document-level discourse structure information to the input. While Maruf et al. (2019); Voita et al. (2018) also find higher attention mass attributed to relevant tokens in selected examples, our work is the first to guide model attention in context-aware NMT using human supervision and analyze its attention distribution in a quantitative manner. However, recent studies suggest that current context-aware NMT models often do not use context meani"
2021.acl-long.65,D19-1081,0,0.0248863,"Missing"
2021.acl-long.65,P19-1116,0,0.0280655,"Missing"
2021.acl-long.65,P18-1117,0,0.222083,"https://github.com/neulab/contextual-mt. Look after her a lot. Okay. Any questions? Have we got her report? Yes, it’s in the infirmary already Dorlotez-la. D’accord. Vous avez des questions ? On dispose de son rapport. Oui, il est a` l’infirmerie. is provided, models often perform poorly on evaluation of relatively simple discourse phenomena (M¨uller et al., 2018; Bawden et al., 2018; Voita et al., 2019b,a; Lopes et al., 2020) and rely on spurious word co-occurences during translation of polysemous words (Emelin et al., 2020). Some evidence suggests that models attend to uninformative tokens (Voita et al., 2018) and do not use contextual information adequately (Kim et al., 2019). To understand plausibly why current NMT models are unable to fully leverage the disambiguating context they are provided, and how we can develop models that use context more effectively, we pose the following research questions: (i) In context aware translation, what context is intrinsically useful to disambiguate hard translation phenomena such as ambiguous pronouns or word senses?; (ii) Are context-aware MT models paying attention to the relevant context or not?; and (iii) If not, can we 788 Proceedings of the 59th Annual"
2021.acl-long.65,D17-1301,0,0.0244694,"rting context. Furthermore, for attnreg-pre, the score after masking supporting context is significantly lower than when masking all context, which may indicate that having irrelevant context can have an adverse efRelated Work Context-Aware Machine Translation Most current context-aware NMT approaches enhance NMT by including source- and/or targetside surrounding sentences as context to the model. Tiedemann and Scherrer (2017) concatenate the previous sentences to the input; Jean et al. (2017); Bawden et al. (2018); Zhang et al. (2018) use an additional encoder to extract contextual features; Wang et al. (2017) use a hierarchical RNN to encode the global context from all previous sentences; Maruf and Haffari (2018); Tu et al. (2018) use cache-based memories to encode context; Miculicich et al. (2018); Maruf et al. (2019) use hierarchical attention networks; Chen et al. (2020) add document-level discourse structure information to the input. While Maruf et al. (2019); Voita et al. (2018) also find higher attention mass attributed to relevant tokens in selected examples, our work is the first to guide model attention in context-aware NMT using human supervision and analyze its attention distribution in"
2021.acl-long.65,D18-1049,0,0.029498,"Missing"
2021.acl-long.65,P18-2066,0,0.0224531,"ls are mostly from regularization by reserving parameters for context inputs, and Li et al. (2020b) show that replacing the context in multi-encoder models with random signals leads to similar accuracy as using the actual context. Our work addresses the above 795 disparities by collecting human supporting context to regularize model attention heads during training. 6.2 Attention Mechanisms Though attention is usually learned in an unsupervised manner, recent work supervises attention with word alignments (Mi et al., 2016; Liu et al., 2016), event arguments and trigger words (Liu et al., 2017; Zhao et al., 2018), syntactic dependencies (Strubell et al., 2018) or word lexicons (Zou et al., 2018). Our work is closely related to a large body of work that supervises attention using human rationales for text classification (Barrett et al., 2018; Bao et al., 2018; Zhong et al., 2019; Choi et al., 2020; Pruthi et al., 2020). Our work, however, is the first to collect human evidence for document translation and use it to regularize the attention of NMT models. 7 evaluated to perform translation, not disambiguation. A future step would be to explore alternative methods for extracting supporting context, such"
2021.acl-long.65,C18-1074,0,0.0276775,"et al. (2020b) show that replacing the context in multi-encoder models with random signals leads to similar accuracy as using the actual context. Our work addresses the above 795 disparities by collecting human supporting context to regularize model attention heads during training. 6.2 Attention Mechanisms Though attention is usually learned in an unsupervised manner, recent work supervises attention with word alignments (Mi et al., 2016; Liu et al., 2016), event arguments and trigger words (Liu et al., 2017; Zhao et al., 2018), syntactic dependencies (Strubell et al., 2018) or word lexicons (Zou et al., 2018). Our work is closely related to a large body of work that supervises attention using human rationales for text classification (Barrett et al., 2018; Bao et al., 2018; Zhong et al., 2019; Choi et al., 2020; Pruthi et al., 2020). Our work, however, is the first to collect human evidence for document translation and use it to regularize the attention of NMT models. 7 evaluated to perform translation, not disambiguation. A future step would be to explore alternative methods for extracting supporting context, such as eye-tracking during translation (O’Brien, 2009). Implications and Future Work In"
2021.emnlp-main.525,P13-1020,1,0.806078,"e extracted by structured prediction on factor graphs. Sentence Compression and Summarization. Work on sentence compression and summarization bears some resemblance to selective rationalization for text highlights extraction. Titov and McDonald (2008) propose a statistical model which is able to discover corresponding topics in text and extract informative snippets of text by predicting a stochastic mask via Gibbs sampling. McDonald (2006) proposes a budgeted dynamic program in the same vein as that of the H:SeqBudget strategy for text highlights extraction. Berg-Kirkpatrick et al. (2011) and Almeida and Martins (2013) propose models that jointly extract and compress sentences. Our work differs in that our setting is completely unsupervised and we need to differentiate through the extractive layers. 7 Conclusions We have proposed SPECTRA, an easy-to-train fully differentiable rationalizer that allows for flexible constrained rationale extraction. We have provided a comparative study with stochastic and deterministic approaches for rationalization, showing that SPECTRA generally outperforms previous rationalizers in text classification and natural language inference tasks. Moreover, it does so while exhibiti"
2021.emnlp-main.525,D18-1216,0,0.0443674,"Missing"
2021.emnlp-main.525,P19-1284,0,0.316297,"R for regression or y ∈ {1, . . . , C} for classification. A generator model, gen, encodes the input text x into token-level scores. Then, a rationale z, e.g. a binary mask over the tokens, is extracted based on these scores. Subsequently, the predictor model makes predictions conditioned only on the rationale yˆ = pred(z x), where denotes the Hadamard (elementwise) product. End-to-end Training and Testing Procedure. While most rationalization methods deterministically select the rationale at test time, there are differences on how these models are trained. For instance, Lei et al. (2016) and Bastings et al. (2019) use stochastic binary variables (Bernoulli and HardKuma, respectively), and sample the rationale z ∼ gen(x) ∈ {0, 1}L , whereas Treviso and Martins (2020) make a continuous relaxation of these binary variables and define the rationale as a sparse probability distribution over the tokens, z = sparsemax(gen(x)) or z = α-entmax(gen(x)). In the latter approach, instead of a binary vector, we have z ∈ 4L−1 , where 4L−1 is the L − 1 probability simplex 4L−1 := {p ∈ RL : 1> p = 1, p ≥ 0}. Words receiving non-zero probability are considered part of the rationale. Rationalizers that use hard attention"
2021.emnlp-main.525,P11-1049,0,0.0316755,"and explaining with latent structure extracted by structured prediction on factor graphs. Sentence Compression and Summarization. Work on sentence compression and summarization bears some resemblance to selective rationalization for text highlights extraction. Titov and McDonald (2008) propose a statistical model which is able to discover corresponding topics in text and extract informative snippets of text by predicting a stochastic mask via Gibbs sampling. McDonald (2006) proposes a budgeted dynamic program in the same vein as that of the H:SeqBudget strategy for text highlights extraction. Berg-Kirkpatrick et al. (2011) and Almeida and Martins (2013) propose models that jointly extract and compress sentences. Our work differs in that our setting is completely unsupervised and we need to differentiate through the extractive layers. 7 Conclusions We have proposed SPECTRA, an easy-to-train fully differentiable rationalizer that allows for flexible constrained rationale extraction. We have provided a comparative study with stochastic and deterministic approaches for rationalization, showing that SPECTRA generally outperforms previous rationalizers in text classification and natural language inference tasks. More"
2021.emnlp-main.525,D15-1075,0,0.0687462,"Missing"
2021.emnlp-main.525,P17-1152,0,0.17743,"he premise. In the second factor graph – M:AtMostOne2 – we alleviate the XOR restriction on the premise words to an AtMostOne restriction. The expected output is a sparser matching for there is no requirement of an active alignment for each word of the premise. The third factor graph – M:Budget – allows us to have more refined control on the sparsity of the resulting matching, by adding an extra global BUDGET factor (with budget B) to the factor graph of M:AtMostOne2 so that the resulting matching will have at most B active alignments. Model Architecture. Our architecture is inspired by ESIM (Chen et al., 2017). First, a generator model encodes two documents xP , xH sepa- Stochastic Matchings Extraction. Prior work for selective rationalization of text matching uses ˜P , . . . , h ˜ P ) and rately to obtain the encodings (h 1 LP constrained variants of optimal transport to obtain ˜H, . . . , h ˜ H ), respectively. Then, we compute (h 1 LH the rationale (Swanson et al., 2020). Their model alignment dot-product pairwise scores between the is end-to-end differentiable using the Sinkhorn alencoded representations to produce a score matrix gorithm (Cuturi, 2013a). Thus, in order to provide L ×L P H ˜ ,h"
2021.emnlp-main.525,P19-1551,0,0.0230268,"score function can be used to define a Gibbs disindicate if a premise word is aligned to a word in tribution p(z; s) ∝ exp(score(z; s)). The MAP the hypothesis. We let Z ⊆ {0, 1}L be the set of in (4) is the mode of this distribution. Sometimes rationales that satisfy the given constraints, and let (e.g. in stochastic rationalizers) we want to sams = gen(x) ∈ RL be a vector of scores. ple from this distribution, zˆ ∼ p(z; s). Exact, 6536 unbiased samples are often intractable to obtain, and approximate sampling strategies have to be used, such as perturb-and-MAP (Papandreou and Yuille, 2011; Corro and Titov, 2019a,b). These strategies necessitate gradient estimators for endto-end training, which are often obtained via REINFORCE (Williams, 1992) or reparametrized gradients (Kingma and Welling, 2014; Jang et al., 2017). LP-MAP inference. In many cases, the MAP problem (4) is intractable due to the overlapping interaction of the factors f ∈ F. A commonly used relaxation is to replace the integer constraints z ∈ {0, 1}L by continuous constraints, leading to: zˆ = arg max score(z; s). z∈[0,1]L (5) LP-SparseMAP inference. The optimization problem respective to LP-SparseMAP is the `2 regularized LP-MAP (Nicu"
2021.emnlp-main.525,D10-1125,0,0.0563643,"Deterministic Structured Rationalizers The idea behind our approach for selective rationalization is very simple: leverage the inherent flexibility and modularity of LP-SparseMAP for constrained, deterministic and fully differentiable rationale extraction. 3.1 The problem above is known as LP-MAP inference (Wainwright and Jordan, 2008). In some cases (for example, when the factor graph F does not have cycles), LP-MAP inference is exact, i.e., it gives the same results as MAP inference. In general, this does not happen, but for many problems in NLP, LP-MAP relaxations are often nearly optimal (Koo et al., 2010; Martins et al., 2015). Importantly, computation in the hidden layer of these problems may render the network unsuitable for gradientbased training, as with MAP inference.  zˆ = arg max score(z; s)−1/2kzk2 . Factor Name Highlights Extraction Model Architecture. We use the model setting described in §2. First, a generator model produces token-level scores si , i ∈ {1, . . . , L}. We propose replacing the current rationale extraction mechanisms (e.g. sampling from a Bernoulli distribution, or using sparse attention mechanisms) with an LP-SparseMAP extraction layer that computes token-level val"
2021.emnlp-main.525,D16-1011,0,0.362938,"ses the question: how can we build an easy-to-train fully differentiable rationalizer that allows for flexible constrained rationale extraction? To answer this question, we introduce sparse structured text rationalization (SPECTRA), which employs LP-SparseMAP (Niculae and Martins, 2020), a constrained structured prediction algorithm, to provide a deterministic, flexible and modular rationale extraction process. We exploit our method’s inherent flexibility to extract highlights and interpretable text matchings with a diverse set of constraints. Our contributions are: Selective rationalization (Lei et al., 2016; Bastings • We present a unified framework for determinet al., 2019; Swanson et al., 2020) is a powerful existic extraction of structured rationales (§3) plainability method, in which we construct models such as constrained highlights and matchings; (rationalizers) that produce an explanation or ra• We show how to add constraints on the ratiotionale (e.g: text highlights or alignments; Zaidan nale extraction, and experiment with several et al. 2007) along with the decision. structured and hard constraint factors, exhibitOne, if not the main, drawback of rationalizers ing the modularity of our"
2021.emnlp-main.525,2020.acl-main.496,0,0.462402,"that allows for flexible constrained rationale extraction? To answer this question, we introduce sparse structured text rationalization (SPECTRA), which employs LP-SparseMAP (Niculae and Martins, 2020), a constrained structured prediction algorithm, to provide a deterministic, flexible and modular rationale extraction process. We exploit our method’s inherent flexibility to extract highlights and interpretable text matchings with a diverse set of constraints. Our contributions are: Selective rationalization (Lei et al., 2016; Bastings • We present a unified framework for determinet al., 2019; Swanson et al., 2020) is a powerful existic extraction of structured rationales (§3) plainability method, in which we construct models such as constrained highlights and matchings; (rationalizers) that produce an explanation or ra• We show how to add constraints on the ratiotionale (e.g: text highlights or alignments; Zaidan nale extraction, and experiment with several et al. 2007) along with the decision. structured and hard constraint factors, exhibitOne, if not the main, drawback of rationalizers ing the modularity of our strategy; is that it is difficult to train the generator and the predictor jointly under i"
2021.emnlp-main.525,P08-1036,0,0.102672,"ith an easy-to-train fully differentiable rationalizer that allows for flexible constrained rationale extraction. Our strategy for rationalization based on sparse structured prediction on factor graphs constitutes a unified framework for deterministic extraction of different structured rationales. for rationalization focuses on learning and explaining with latent structure extracted by structured prediction on factor graphs. Sentence Compression and Summarization. Work on sentence compression and summarization bears some resemblance to selective rationalization for text highlights extraction. Titov and McDonald (2008) propose a statistical model which is able to discover corresponding topics in text and extract informative snippets of text by predicting a stochastic mask via Gibbs sampling. McDonald (2006) proposes a budgeted dynamic program in the same vein as that of the H:SeqBudget strategy for text highlights extraction. Berg-Kirkpatrick et al. (2011) and Almeida and Martins (2013) propose models that jointly extract and compress sentences. Our work differs in that our setting is completely unsupervised and we need to differentiate through the extractive layers. 7 Conclusions We have proposed SPECTRA,"
2021.emnlp-main.525,D19-1420,0,0.0142275,"oticed that these were similar between the two models. Thus, the effect of the augmented data resides on how the information from the matchings is used after the extraction layer. We show examples of matchings in §H. 6 Related Work Selective Rationalization. There is a long string of work on interpreting predictions made by neural networks (Lipton, 2017; Doshi-Velez and Kim, 2017; Gilpin et al., 2019; Wiegreffe and Marasovi´c, 2021; Zhang et al., 2021a). Our paper focus on selective rationalizers, which have been used for extraction of text highlights (Lei et al., 2016; Bastings et al., 2019; Yu et al., 2019; DeYoung et al., 2019; Treviso and Martins, 2020; Zhang et al., 2021b) and text matchings (Swanson et al., 2020). Most works rely on stochastic rationale generation or deterministic attention mechanisms, but the two approaches have never been extensively compared. Our work adds that comparison and contributes with an easy-to-train fully differentiable rationalizer that allows for flexible constrained rationale extraction. Our strategy for rationalization based on sparse structured prediction on factor graphs constitutes a unified framework for deterministic extraction of different structured"
2021.emnlp-main.525,N07-1033,0,0.186031,"Missing"
2021.eval4nlp-1.14,C04-1046,0,0.546359,"terpretable patterns that clarify how decisions emerge from attention heads and across hidden states at each layer (De Cao et al., 2020; Abnar and Zuidema, 2020; Voita et al., 2021). In this shared task, we experiment with several of these methods to extract the relevance of input tokens from sentence-level QE models built on top of multilingual pre-trained transformers1 . For the constrained track, where models are unaware of word-level supervision, our best results were deQuality estimation (QE) aims at assessing the quality of a translation system without relying on reference translations (Blatz et al., 2004; Specia et al., 2018). This paper describes the joint contribution of Instituto Superior Técnico (IST) and Unbabel to the Explainable Quality Estimation shared task (Fomicheva et al., 2021a). The goal of the shared task is to identify translation errors without direct word-level supervision (constrained track) or with access to word-level labels (unconstrained track). Recent advances in QE have led to consistent improvements at predicting quality assessments such as Direct Assessments (DAs, Graham et al. 2013). Traditional QE systems had to predict Human Translation Error Rate (HTER, Snover e"
2021.eval4nlp-1.14,N19-1357,0,0.0565709,"d by a learnable linear transformation W O : This way, heads have the capability of learning specialized phenomena. Transformers with only encoder-blocks, such as BERT (Devlin et al., 2019) and XLM-RoBERTa (Conneau et al., 2020), have only the encoder self-attention, and thus m = n. Explainability in NLP. There is a large body of work on the analysis and interpretation of models in NLP. Some of these models are built on top of attention mechanisms, which automatically learn a weighted representation of input features. Attention weights provide plausible, but not always faithful, explanations (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019). In contrast, rationalizers with hard attention are arguably more faithful but require stochastic networks (Lei et al., 2016; Bastings et al., 2019), with recent works avoiding stochasticity via sparse deterministic selections (Guerreiro and Martins, 2021). Other approaches seek local explanations by considering gradient measures (Arras et al., 2016; Bastings and Filippova, 2020), or by perturbing the input and querying the classifier in a post-hoc manner (Ribeiro et al., 2016; Kim et al., 2020). Since transformers are composed of several layers and attention head"
2021.eval4nlp-1.14,W19-5406,1,0.922204,"from models trained with different pre-trained transformers, achieving strong results for in-domain and zero-shot language pairs. 1 Introduction quacy errors (Martindale and Carpuat, 2018). For that reason, DAs started getting used as the groundtruth score for assessing the quality of translations (Specia et al., 2020). However, with DAs we lose the ability to generate word-level supervision, impacting the interpretability of sentence-level predictions in terms of lower granularity elements such as word-level translation errors. At the same time, state-of-the-art QE systems such as OpenKiwi (Kepler et al., 2019b) and TransQuest (Ranasinghe et al., 2020b) build on top of multilingual pre-trained models such as BERT (Devlin et al., 2019) and XLM-RoBERTa (Conneau et al., 2020), which are largely responsible for the performance boost we have observed in the last two editions of the WMT QE shared task (Fonseca et al., 2019; Specia et al., 2020). Due to the usage of such overparametrized black-box models, this performance boost also comes at the cost of efficiency and interpretability. Research in explainable NLP uncovered several strategies to interpret models’ decisions, either in a post-hoc manner by q"
2021.eval4nlp-1.14,P19-3020,1,0.883915,"from models trained with different pre-trained transformers, achieving strong results for in-domain and zero-shot language pairs. 1 Introduction quacy errors (Martindale and Carpuat, 2018). For that reason, DAs started getting used as the groundtruth score for assessing the quality of translations (Specia et al., 2020). However, with DAs we lose the ability to generate word-level supervision, impacting the interpretability of sentence-level predictions in terms of lower granularity elements such as word-level translation errors. At the same time, state-of-the-art QE systems such as OpenKiwi (Kepler et al., 2019b) and TransQuest (Ranasinghe et al., 2020b) build on top of multilingual pre-trained models such as BERT (Devlin et al., 2019) and XLM-RoBERTa (Conneau et al., 2020), which are largely responsible for the performance boost we have observed in the last two editions of the WMT QE shared task (Fonseca et al., 2019; Specia et al., 2020). Due to the usage of such overparametrized black-box models, this performance boost also comes at the cost of efficiency and interpretability. Research in explainable NLP uncovered several strategies to interpret models’ decisions, either in a post-hoc manner by q"
2021.eval4nlp-1.14,2020.emnlp-main.255,0,0.305439,"eights provide plausible, but not always faithful, explanations (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019). In contrast, rationalizers with hard attention are arguably more faithful but require stochastic networks (Lei et al., 2016; Bastings et al., 2019), with recent works avoiding stochasticity via sparse deterministic selections (Guerreiro and Martins, 2021). Other approaches seek local explanations by considering gradient measures (Arras et al., 2016; Bastings and Filippova, 2020), or by perturbing the input and querying the classifier in a post-hoc manner (Ribeiro et al., 2016; Kim et al., 2020). Since transformers are composed of several layers and attention heads, many works analyze and improve the multi-head attention mechanism directly to produce better explanations (Kobayashi et al., 2020; Hao et al., 2021). More elaborated methods consider the entire flow of information coming from attention weights, hidden states, or gradients to interpret the model’s decision (De Cao et al., 2020; Abnar and Zuidema, 2020; Voita et al., 2021). 3 Constrained Track The goal of the constrained track is to identify machine translation errors without explicit word-level annotation. More precisely,"
2021.eval4nlp-1.14,2020.emnlp-main.574,0,0.243796,"equire stochastic networks (Lei et al., 2016; Bastings et al., 2019), with recent works avoiding stochasticity via sparse deterministic selections (Guerreiro and Martins, 2021). Other approaches seek local explanations by considering gradient measures (Arras et al., 2016; Bastings and Filippova, 2020), or by perturbing the input and querying the classifier in a post-hoc manner (Ribeiro et al., 2016; Kim et al., 2020). Since transformers are composed of several layers and attention heads, many works analyze and improve the multi-head attention mechanism directly to produce better explanations (Kobayashi et al., 2020; Hao et al., 2021). More elaborated methods consider the entire flow of information coming from attention weights, hidden states, or gradients to interpret the model’s decision (De Cao et al., 2020; Abnar and Zuidema, 2020; Voita et al., 2021). 3 Constrained Track The goal of the constrained track is to identify machine translation errors without explicit word-level annotation. More precisely, it aims at performing word-level quality estimation by casting the task as a prediction explainability problem. In the context of QE, explanations can be seen as highlights, representing the relevance o"
2021.eval4nlp-1.14,D16-1011,0,0.202365,"Missing"
2021.eval4nlp-1.14,W18-1803,0,0.178314,"t on top of multilingual pre-trained transformers. Among the different tested methods, composing explanations in the form of attention weights scaled by the norm of value vectors yielded the best results. When word-level labels are used during training, our best results were obtained by using word-level predicted probabilities. We further improve the performance of our methods on the two tracks by ensembling explanation scores extracted from models trained with different pre-trained transformers, achieving strong results for in-domain and zero-shot language pairs. 1 Introduction quacy errors (Martindale and Carpuat, 2018). For that reason, DAs started getting used as the groundtruth score for assessing the quality of translations (Specia et al., 2020). However, with DAs we lose the ability to generate word-level supervision, impacting the interpretability of sentence-level predictions in terms of lower granularity elements such as word-level translation errors. At the same time, state-of-the-art QE systems such as OpenKiwi (Kepler et al., 2019b) and TransQuest (Ranasinghe et al., 2020b) build on top of multilingual pre-trained models such as BERT (Devlin et al., 2019) and XLM-RoBERTa (Conneau et al., 2020), wh"
2021.eval4nlp-1.14,N18-1202,0,0.0659555,"Missing"
2021.eval4nlp-1.14,2020.emnlp-demos.7,0,0.353345,"Missing"
2021.eval4nlp-1.14,2020.wmt-1.122,0,0.444298,"e-trained transformers, achieving strong results for in-domain and zero-shot language pairs. 1 Introduction quacy errors (Martindale and Carpuat, 2018). For that reason, DAs started getting used as the groundtruth score for assessing the quality of translations (Specia et al., 2020). However, with DAs we lose the ability to generate word-level supervision, impacting the interpretability of sentence-level predictions in terms of lower granularity elements such as word-level translation errors. At the same time, state-of-the-art QE systems such as OpenKiwi (Kepler et al., 2019b) and TransQuest (Ranasinghe et al., 2020b) build on top of multilingual pre-trained models such as BERT (Devlin et al., 2019) and XLM-RoBERTa (Conneau et al., 2020), which are largely responsible for the performance boost we have observed in the last two editions of the WMT QE shared task (Fonseca et al., 2019; Specia et al., 2020). Due to the usage of such overparametrized black-box models, this performance boost also comes at the cost of efficiency and interpretability. Research in explainable NLP uncovered several strategies to interpret models’ decisions, either in a post-hoc manner by querying a trained model for extracting per"
2021.eval4nlp-1.14,2020.coling-main.445,0,0.634372,"e-trained transformers, achieving strong results for in-domain and zero-shot language pairs. 1 Introduction quacy errors (Martindale and Carpuat, 2018). For that reason, DAs started getting used as the groundtruth score for assessing the quality of translations (Specia et al., 2020). However, with DAs we lose the ability to generate word-level supervision, impacting the interpretability of sentence-level predictions in terms of lower granularity elements such as word-level translation errors. At the same time, state-of-the-art QE systems such as OpenKiwi (Kepler et al., 2019b) and TransQuest (Ranasinghe et al., 2020b) build on top of multilingual pre-trained models such as BERT (Devlin et al., 2019) and XLM-RoBERTa (Conneau et al., 2020), which are largely responsible for the performance boost we have observed in the last two editions of the WMT QE shared task (Fonseca et al., 2019; Specia et al., 2020). Due to the usage of such overparametrized black-box models, this performance boost also comes at the cost of efficiency and interpretability. Research in explainable NLP uncovered several strategies to interpret models’ decisions, either in a post-hoc manner by querying a trained model for extracting per"
2021.eval4nlp-1.14,2020.emnlp-main.213,1,0.866001,"ity, likely due to the low number of translation errors for those sentences. A simple way to circumvent this problem is to force the explainer to “focus” on words associated with lower scores (or to the BAD class in a classification setting). Thus, strategies such as framing the prediction of DA scores as a classification problem or inducing class-wise rationalizers (Chang et al., 2019) can be helpful. This shared task focused only on the intersection between explainability and Quality Estimation, yet for future work we plan to apply explainability methods to recent MT metrics such as C OMET (Rei et al., 2020a,b; Glushkova et al., 2021) and B LEURT (Sellam et al., 2020a,b). Acknowledgements References Samira Abnar and Willem Zuidema. 2020. Quantifying attention flow in transformers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4190–4197, Online. Association for Computational Linguistics. Leila Arras, Franziska Horn, Grégoire Montavon, Klaus-Robert Müller, and Wojciech Samek. 2016. Explaining predictions of non-linear classifiers in NLP. In Proceedings of the 1st Workshop on Representation Learning for NLP, pages 1–7, Berlin, Germany. Association"
2021.eval4nlp-1.14,2020.wmt-1.101,1,0.79624,"ity, likely due to the low number of translation errors for those sentences. A simple way to circumvent this problem is to force the explainer to “focus” on words associated with lower scores (or to the BAD class in a classification setting). Thus, strategies such as framing the prediction of DA scores as a classification problem or inducing class-wise rationalizers (Chang et al., 2019) can be helpful. This shared task focused only on the intersection between explainability and Quality Estimation, yet for future work we plan to apply explainability methods to recent MT metrics such as C OMET (Rei et al., 2020a,b; Glushkova et al., 2021) and B LEURT (Sellam et al., 2020a,b). Acknowledgements References Samira Abnar and Willem Zuidema. 2020. Quantifying attention flow in transformers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4190–4197, Online. Association for Computational Linguistics. Leila Arras, Franziska Horn, Grégoire Montavon, Klaus-Robert Müller, and Wojciech Samek. 2016. Explaining predictions of non-linear classifiers in NLP. In Proceedings of the 1st Workshop on Representation Learning for NLP, pages 1–7, Berlin, Germany. Association"
2021.eval4nlp-1.14,N16-3020,0,0.916655,"pre-trained models such as BERT (Devlin et al., 2019) and XLM-RoBERTa (Conneau et al., 2020), which are largely responsible for the performance boost we have observed in the last two editions of the WMT QE shared task (Fonseca et al., 2019; Specia et al., 2020). Due to the usage of such overparametrized black-box models, this performance boost also comes at the cost of efficiency and interpretability. Research in explainable NLP uncovered several strategies to interpret models’ decisions, either in a post-hoc manner by querying a trained model for extracting perturbation or gradient measures (Ribeiro et al., 2016; Arras et al., 2016), or by building models that are inherently interpretable (Lei et al., 2016; Chang et al., 2020). Recent works have also put transformers under the lens of explainability, aiming at unraveling interpretable patterns that clarify how decisions emerge from attention heads and across hidden states at each layer (De Cao et al., 2020; Abnar and Zuidema, 2020; Voita et al., 2021). In this shared task, we experiment with several of these methods to extract the relevance of input tokens from sentence-level QE models built on top of multilingual pre-trained transformers1 . For the"
2021.eval4nlp-1.14,2020.acl-main.704,0,0.0472983,"r those sentences. A simple way to circumvent this problem is to force the explainer to “focus” on words associated with lower scores (or to the BAD class in a classification setting). Thus, strategies such as framing the prediction of DA scores as a classification problem or inducing class-wise rationalizers (Chang et al., 2019) can be helpful. This shared task focused only on the intersection between explainability and Quality Estimation, yet for future work we plan to apply explainability methods to recent MT metrics such as C OMET (Rei et al., 2020a,b; Glushkova et al., 2021) and B LEURT (Sellam et al., 2020a,b). Acknowledgements References Samira Abnar and Willem Zuidema. 2020. Quantifying attention flow in transformers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4190–4197, Online. Association for Computational Linguistics. Leila Arras, Franziska Horn, Grégoire Montavon, Klaus-Robert Müller, and Wojciech Samek. 2016. Explaining predictions of non-linear classifiers in NLP. In Proceedings of the 1st Workshop on Representation Learning for NLP, pages 1–7, Berlin, Germany. Association for Computational Linguistics. Jasmijn Bastings, Wilker Aziz"
2021.eval4nlp-1.14,2020.wmt-1.102,0,0.225443,"r those sentences. A simple way to circumvent this problem is to force the explainer to “focus” on words associated with lower scores (or to the BAD class in a classification setting). Thus, strategies such as framing the prediction of DA scores as a classification problem or inducing class-wise rationalizers (Chang et al., 2019) can be helpful. This shared task focused only on the intersection between explainability and Quality Estimation, yet for future work we plan to apply explainability methods to recent MT metrics such as C OMET (Rei et al., 2020a,b; Glushkova et al., 2021) and B LEURT (Sellam et al., 2020a,b). Acknowledgements References Samira Abnar and Willem Zuidema. 2020. Quantifying attention flow in transformers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4190–4197, Online. Association for Computational Linguistics. Leila Arras, Franziska Horn, Grégoire Montavon, Klaus-Robert Müller, and Wojciech Samek. 2016. Explaining predictions of non-linear classifiers in NLP. In Proceedings of the 1st Workshop on Representation Learning for NLP, pages 1–7, Berlin, Germany. Association for Computational Linguistics. Jasmijn Bastings, Wilker Aziz"
2021.eval4nlp-1.14,2006.amta-papers.25,0,0.109059,"Missing"
2021.eval4nlp-1.14,2021.acl-long.91,0,0.245191,"ility. Research in explainable NLP uncovered several strategies to interpret models’ decisions, either in a post-hoc manner by querying a trained model for extracting perturbation or gradient measures (Ribeiro et al., 2016; Arras et al., 2016), or by building models that are inherently interpretable (Lei et al., 2016; Chang et al., 2020). Recent works have also put transformers under the lens of explainability, aiming at unraveling interpretable patterns that clarify how decisions emerge from attention heads and across hidden states at each layer (De Cao et al., 2020; Abnar and Zuidema, 2020; Voita et al., 2021). In this shared task, we experiment with several of these methods to extract the relevance of input tokens from sentence-level QE models built on top of multilingual pre-trained transformers1 . For the constrained track, where models are unaware of word-level supervision, our best results were deQuality estimation (QE) aims at assessing the quality of a translation system without relying on reference translations (Blatz et al., 2004; Specia et al., 2018). This paper describes the joint contribution of Instituto Superior Técnico (IST) and Unbabel to the Explainable Quality Estimation shared ta"
2021.eval4nlp-1.14,D19-1002,0,0.0549926,"transformation W O : This way, heads have the capability of learning specialized phenomena. Transformers with only encoder-blocks, such as BERT (Devlin et al., 2019) and XLM-RoBERTa (Conneau et al., 2020), have only the encoder self-attention, and thus m = n. Explainability in NLP. There is a large body of work on the analysis and interpretation of models in NLP. Some of these models are built on top of attention mechanisms, which automatically learn a weighted representation of input features. Attention weights provide plausible, but not always faithful, explanations (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019). In contrast, rationalizers with hard attention are arguably more faithful but require stochastic networks (Lei et al., 2016; Bastings et al., 2019), with recent works avoiding stochasticity via sparse deterministic selections (Guerreiro and Martins, 2021). Other approaches seek local explanations by considering gradient measures (Arras et al., 2016; Bastings and Filippova, 2020), or by perturbing the input and querying the classifier in a post-hoc manner (Ribeiro et al., 2016; Kim et al., 2020). Since transformers are composed of several layers and attention heads, many works analyze and imp"
2021.eval4nlp-1.14,2021.eval4nlp-1.14,1,0.0530913,"Missing"
2021.findings-emnlp.330,K16-1021,0,0.0219359,"017). The results show that our uncertainty-aware systems exhibit better calibration with respect to human direct assessments (DA; Graham et al. 2013), multi-dimensional quality metric scores (MQM; Lommel et al. 2014), and human translation error rates (HTER; Snover et al. 2006) than a simple baseline, while their average quality scores achieve similar or better correlation than the vanilla C OMET system. Finally, we illustrate a potential quality estimation use case enabled by our approach: automatically detecting low-quality translations with a risk-based criterion. with Gaussian processes (Beck et al., 2016), which are not competitive or easy to integrate with current neural architectures. Confidence estimation in MT A related line of work is confidence estimation of sentence-level MT outputs (Blatz et al., 2004; Quirk, 2004; Wang et al., 2019). The work that relates the most with ours is the one by Fomicheva et al. (2020), who propose an unsupervised glass-box approach to QE, extracting uncertainty-related features from the MT system via MC dropout. They show that the more confident the decoder (as measured by the lower variance of its output), the higher the quality of the MT output. Our work b"
2021.findings-emnlp.330,C04-1046,0,0.218504,"al. 2014), and human translation error rates (HTER; Snover et al. 2006) than a simple baseline, while their average quality scores achieve similar or better correlation than the vanilla C OMET system. Finally, we illustrate a potential quality estimation use case enabled by our approach: automatically detecting low-quality translations with a risk-based criterion. with Gaussian processes (Beck et al., 2016), which are not competitive or easy to integrate with current neural architectures. Confidence estimation in MT A related line of work is confidence estimation of sentence-level MT outputs (Blatz et al., 2004; Quirk, 2004; Wang et al., 2019). The work that relates the most with ours is the one by Fomicheva et al. (2020), who propose an unsupervised glass-box approach to QE, extracting uncertainty-related features from the MT system via MC dropout. They show that the more confident the decoder (as measured by the lower variance of its output), the higher the quality of the MT output. Our work builds upon this perspective to propose uncertainty estimation of the QE systems themselves, rather than uncertainty of MT. Performance prediction in NLP A related problem is that of predicting the performance"
2021.findings-emnlp.330,W14-3348,0,0.0148117,"imates are with a suite of performance indicators. Uncertainty estimation Overall the concepts and methods of uncertainty quantification (Huellermeier and Waegeman, 2021) have been widely ex2 Related Work plored and compared for many different tasks, including MT (Ott et al., 2018). Uncertainty estimaAutomatic MT evaluation Reference-based aption in neural networks has traditionally been approaches for MT evaluation include traditional metproached with Bayesian methods, replacing point rics such as B LEU (Papineni et al., 2002) and M E estimates of weights with probability distributions TEOR (Denkowski and Lavie, 2014), as well as (Mackay, 1992; Graves, 2011; Welling and Teh, recently proposed B LEURT (Sellam et al., 2020), 2011; Tran et al., 2019). However, Bayesian neural BERTS CORE (Zhang et al., 2020), P RISM (Thompnetworks are costly, and in order to avoid high trainson and Post, 2020a) and C OMET (Rei et al., ing costs, various approximations come in handy. 2020a). Approaches that do not make use of human Model ensembling (Dietterich, 2000; Garmash and references are generally referred to as QE systems Monz, 2016; McClure and Kriegeskorte, 2017; Lak(Specia et al., 2018; Kepler et al., 2019; Ranasinghe"
2021.findings-emnlp.330,2020.emnlp-main.21,0,0.0222224,"r (as measured by the lower variance of its output), the higher the quality of the MT output. Our work builds upon this perspective to propose uncertainty estimation of the QE systems themselves, rather than uncertainty of MT. Performance prediction in NLP A related problem is that of predicting the performance of an NLP system without having to train it (Xia et al., 2020). Recent approaches perform such predictions by adding confidence intervals (Ye et al., 2021) and measuring calibration error. We take inspiration from these works to improve the calibration of our methods (Guo et al., 2017; Desai and Durrett, 2020) and to evaluate how good our uncertainty estimates are with a suite of performance indicators. Uncertainty estimation Overall the concepts and methods of uncertainty quantification (Huellermeier and Waegeman, 2021) have been widely ex2 Related Work plored and compared for many different tasks, including MT (Ott et al., 2018). Uncertainty estimaAutomatic MT evaluation Reference-based aption in neural networks has traditionally been approaches for MT evaluation include traditional metproached with Bayesian methods, replacing point rics such as B LEU (Papineni et al., 2002) and M E estimates of"
2021.findings-emnlp.330,W18-6460,0,0.0160789,"d by h to rank translations, (2) using the mean µ ˆ of the estimated distribution pˆQ (q) instead of the single point estimate qˆ, and (3) using the uncertainty-aware parametric models to compute and rank by the probability of qerr . Since this scenario is more relevant to realtime/on demand translation evaluation, we test it under the assumption that there is no access to a human reference. To handle this referenceless case (R = ∅, also known as quality estimation), we can use translations produced by an MT system outside the WMT20 participants as pseudo-references (Scarton and Specia, 2014; Duma and Menzel, 2018). We use P RISM6 , which was originally trained as a multilingual NMT model, (Thompson and Post, 2020b,a). We evaluate all scoring approaches using Recall@N and Precision@N as shown in Figure 2. We can see that while for very small values all approaches perform similarly, the uncertainty-aware approach (UA-C OMET) outperforms the other two for Recall as N increases, while it also demonstrates higher Precision especially for small N values, which are of greatest interest since we want to correct as many critical errors as possible with minimal human intervention. 6 Conclusions We introduced unc"
2021.findings-emnlp.330,2020.emnlp-main.5,0,0.0937422,"s To obtain pˆQ (q), our approach builds upon a vanilla MT evaluation system h (such as C OMET) that produces point estimates qˆ = h(hs, t, Ri), and 2. Noisy or insufficient references. The refer- augments it to produce uncertainty estimates. Our approach is completely agnostic about the system ences R do not always have good quality, and h, as long as it can be ensembled or perturbed. their sparsity (small |R|) is often insufficient to The first step is to use h to produce a set represent the space of possible correct translaQ = {ˆ q1 , . . . , qˆN } of quality scores for a given tions well (Freitag et al., 2020).2 An extreme input hs, t, Ri, which will be interpreted as a sam2 From the perspective of the MT system, the existence of ple from pˆQ (q). For this, we experiment with multiple valid translations for a single source sentence can be seen as inherent uncertainty of the task (Ott et al., 2018). two methods: MC dropout (Gal and Ghahramani, 3922 2016), which obtains Q by running N stochastic forward-passes on h with units dropped out with a given probability; and deep ensembles (Lakshminarayanan et al., 2017), in which N separate models are trained with different random initializations and then r"
2021.findings-emnlp.330,C16-1133,0,0.0625012,"Missing"
2021.findings-emnlp.330,W13-2305,0,0.0312935,"ing the same system with a varying number of references. We show that confidence intervals tend to shrink as more references are added, which matches the intuition that MT evaluation systems should become more confident as they have access to more information. We evaluate our approach using data from the WMT20 metrics task (Mathur et al., 2020), including its recent extension with Google MQM annotations (Freitag et al., 2021), and the QT21 dataset (Specia et al., 2017). The results show that our uncertainty-aware systems exhibit better calibration with respect to human direct assessments (DA; Graham et al. 2013), multi-dimensional quality metric scores (MQM; Lommel et al. 2014), and human translation error rates (HTER; Snover et al. 2006) than a simple baseline, while their average quality scores achieve similar or better correlation than the vanilla C OMET system. Finally, we illustrate a potential quality estimation use case enabled by our approach: automatically detecting low-quality translations with a risk-based criterion. with Gaussian processes (Beck et al., 2016), which are not competitive or easy to integrate with current neural architectures. Confidence estimation in MT A related line of wo"
2021.findings-emnlp.330,2020.emnlp-main.8,0,0.0214976,"the single point estimate qˆ, and (3) using the uncertainty-aware parametric models to compute and rank by the probability of qerr . Since this scenario is more relevant to realtime/on demand translation evaluation, we test it under the assumption that there is no access to a human reference. To handle this referenceless case (R = ∅, also known as quality estimation), we can use translations produced by an MT system outside the WMT20 participants as pseudo-references (Scarton and Specia, 2014; Duma and Menzel, 2018). We use P RISM6 , which was originally trained as a multilingual NMT model, (Thompson and Post, 2020b,a). We evaluate all scoring approaches using Recall@N and Precision@N as shown in Figure 2. We can see that while for very small values all approaches perform similarly, the uncertainty-aware approach (UA-C OMET) outperforms the other two for Recall as N increases, while it also demonstrates higher Precision especially for small N values, which are of greatest interest since we want to correct as many critical errors as possible with minimal human intervention. 6 Conclusions We introduced uncertainty-aware MT evaluation and showed how MT-related applications can benefit from this approach. W"
2021.findings-emnlp.330,2014.eamt-1.21,0,0.0476981,"ing the scores qˆ predicted by h to rank translations, (2) using the mean µ ˆ of the estimated distribution pˆQ (q) instead of the single point estimate qˆ, and (3) using the uncertainty-aware parametric models to compute and rank by the probability of qerr . Since this scenario is more relevant to realtime/on demand translation evaluation, we test it under the assumption that there is no access to a human reference. To handle this referenceless case (R = ∅, also known as quality estimation), we can use translations produced by an MT system outside the WMT20 participants as pseudo-references (Scarton and Specia, 2014; Duma and Menzel, 2018). We use P RISM6 , which was originally trained as a multilingual NMT model, (Thompson and Post, 2020b,a). We evaluate all scoring approaches using Recall@N and Precision@N as shown in Figure 2. We can see that while for very small values all approaches perform similarly, the uncertainty-aware approach (UA-C OMET) outperforms the other two for Recall as N increases, while it also demonstrates higher Precision especially for small N values, which are of greatest interest since we want to correct as many critical errors as possible with minimal human intervention. 6 Concl"
2021.findings-emnlp.330,2020.wmt-1.67,0,0.0373525,"the single point estimate qˆ, and (3) using the uncertainty-aware parametric models to compute and rank by the probability of qerr . Since this scenario is more relevant to realtime/on demand translation evaluation, we test it under the assumption that there is no access to a human reference. To handle this referenceless case (R = ∅, also known as quality estimation), we can use translations produced by an MT system outside the WMT20 participants as pseudo-references (Scarton and Specia, 2014; Duma and Menzel, 2018). We use P RISM6 , which was originally trained as a multilingual NMT model, (Thompson and Post, 2020b,a). We evaluate all scoring approaches using Recall@N and Precision@N as shown in Figure 2. We can see that while for very small values all approaches perform similarly, the uncertainty-aware approach (UA-C OMET) outperforms the other two for Recall as N increases, while it also demonstrates higher Precision especially for small N values, which are of greatest interest since we want to correct as many critical errors as possible with minimal human intervention. 6 Conclusions We introduced uncertainty-aware MT evaluation and showed how MT-related applications can benefit from this approach. W"
2021.findings-emnlp.330,2020.acl-main.704,0,0.138134,"ces receive quality estimates that are far from reference translations are available, the increasing their true quality (as illustrated in Table 1). This quality of neural MT systems has made traditional lexical-based metrics such as B LEU (Papineni et al., may lead to translations with critical mistakes being 2002) or CHR F (Popovi´c, 2015) insufficient to dis- undetected, and hinders worst-case performance analysis of MT systems. tinguish the best systems. This fostered a line of work on neural-based metrics, with recent proposIn this paper, we propose a simple and effecals such as B LEURT (Sellam et al., 2020), C OMET tive method to obtain uncertainty-aware qual(Rei et al., 2020a) and P RISM (Thompson and Post, ity/metric estimation systems, by representing qual2020a). Metrics for quality estimation (QE; when ity as a distribution, rather than a single value. references are not available) have also been devel- To this end, we make use of and compare two 3920 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3920–3938 November 7–11, 2021. ©2021 Association for Computational Linguistics well-studied techniques for uncertainty estimation: Monte Carlo (MC) dropout (Gal and Gh"
2021.findings-emnlp.330,2020.eamt-1.20,0,0.0268043,"0.093 - 1.237 1.225 1.259 0.011 0.015 0.035 0.591 0.556 0.725 E N -C S MCD DE Basel. 0.691 0.729 0.695 0.207 0.163 - 1.163 1.100 1.172 0.013 0.013 0.036 0.548 0.455 0.608 E N -RU MCD DE Basel. 0.536 0.578 0.532 0.142 0.139 - 1.378 1.320 1.383 0.021 0.023 0.041 0.767 0.670 0.925 E N -P L MCD DE Basel. 0.611 0.650 0.608 0.199 0.176 - 1.275 1.224 1.301 0.015 0.012 0.042 0.650 0.581 0.783 E N -I U scale are necessary to fully validate and compare uncertainty-aware methods, as the numbers in Table 2 are influenced by the inconsistencies in DA annotations, which are known to be particularly noisy (Toral, 2020; Freitag et al., 2021). To mitigate this, we further compare performance on the recently released Google MQM annotations for E N DE and Z H -E N, shown in Table 3. As expected from the higher quality of these annotations, and even though the underlying C OMET system was still trained on DAs and evaluated on the MQM assessments, we get higher uncertainty correlations, with the MC dropout approach benefiting the most. We also notice a significant improvement across all indicators for the Z H -E N dataset, which was poorly correlated with the predictions on the DA dataset. We use the MQM annotat"
2021.findings-emnlp.330,2020.acl-main.764,0,0.0207849,"at relates the most with ours is the one by Fomicheva et al. (2020), who propose an unsupervised glass-box approach to QE, extracting uncertainty-related features from the MT system via MC dropout. They show that the more confident the decoder (as measured by the lower variance of its output), the higher the quality of the MT output. Our work builds upon this perspective to propose uncertainty estimation of the QE systems themselves, rather than uncertainty of MT. Performance prediction in NLP A related problem is that of predicting the performance of an NLP system without having to train it (Xia et al., 2020). Recent approaches perform such predictions by adding confidence intervals (Ye et al., 2021) and measuring calibration error. We take inspiration from these works to improve the calibration of our methods (Guo et al., 2017; Desai and Durrett, 2020) and to evaluate how good our uncertainty estimates are with a suite of performance indicators. Uncertainty estimation Overall the concepts and methods of uncertainty quantification (Huellermeier and Waegeman, 2021) have been widely ex2 Related Work plored and compared for many different tasks, including MT (Ott et al., 2018). Uncertainty estimaAuto"
2021.findings-emnlp.330,2021.eacl-main.324,0,0.14851,"ed glass-box approach to QE, extracting uncertainty-related features from the MT system via MC dropout. They show that the more confident the decoder (as measured by the lower variance of its output), the higher the quality of the MT output. Our work builds upon this perspective to propose uncertainty estimation of the QE systems themselves, rather than uncertainty of MT. Performance prediction in NLP A related problem is that of predicting the performance of an NLP system without having to train it (Xia et al., 2020). Recent approaches perform such predictions by adding confidence intervals (Ye et al., 2021) and measuring calibration error. We take inspiration from these works to improve the calibration of our methods (Guo et al., 2017; Desai and Durrett, 2020) and to evaluate how good our uncertainty estimates are with a suite of performance indicators. Uncertainty estimation Overall the concepts and methods of uncertainty quantification (Huellermeier and Waegeman, 2021) have been widely ex2 Related Work plored and compared for many different tasks, including MT (Ott et al., 2018). Uncertainty estimaAutomatic MT evaluation Reference-based aption in neural networks has traditionally been approach"
2021.naacl-main.210,D19-1091,0,0.0238441,") used entmax only for transformer attention. Table 2: Macro-averaged MI results on the SIGMORPHON 2019 Task 1 test set. When  &gt; 0, it is tuned separately for each language pair. Training. We reimplemented G ATEDATTN (Peters and Martins, 2019), an RNN model with separate encoders for lemma and morphological tags. We copied their hyperparameters, except that we used two layers for all encoders. We concatenated the high and low resource training data. In order to make sure the model paid attention to the low resource training data, we either oversampled it 100 times or used data hallucination (Anastasopoulos and Neubig, 2019) to generate synthetic examples. Hallucination worked well for some languages but not others, so we treated it as a hyperparameter. Results. We compare to CMU-036 (Anastasopoulos and Neubig, 2019), a two-encoder model with a sophisticated multi-stage training schedule. Despite our models’ simpler training technique, 6 Machine Translation We specifically use the official task numbers from McCarthy et al. (2019), which are more complete than those reported in Anastasopoulos and Neubig (2019). Data. We made use of these language pairs: • IWSLT 2017 German ↔ English (DE↔EN, Cettolo et al., 2017):"
2021.naacl-main.210,D19-1223,1,0.921514,"n the search space: that is, it can learn to give inadequate hypotheses zero probability, instead of Sequence-to-sequence models (seq2seq: Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., counting on beam search to prune them. This has already been demonstrated for MI, where the set of 2017) have become a powerful and flexible tool for a variety of NLP tasks, including machine transla- possible hypotheses is often small enough to make beam search exact (Peters et al., 2019; Peters and tion (MT), morphological inflection (MI; Faruqui et al., 2016), and grapheme-to-phoneme conver- Martins, 2019). We extend this analysis to MT: although exact beam search is not possible for this sion (G2P; Yao and Zweig, 2015). These models often perform well, but they have a bias that fa- large vocabulary task, we show that entmax modvors short hypotheses. This bias is problematic: els prune many inadequate hypotheses, effectively solving the cat got your tongue problem. it has been pointed out as the cause (Koehn and Knowles, 2017; Yang et al., 2018; Murray and ChiDespite this useful result, one drawback of ent2642 Proceedings of the 2021 Conference of the North American Chapter of the Association f"
2021.naacl-main.210,2020.coling-main.398,0,0.0125199,"baseline model is more confident of. Other works have smoothed over sequences instead of tokens (Norouzi et al., 2016; Elbayad et al., 2018; Lukasik et al., 2020), but this requires approximate techniques for deciding which sequences to smooth. MAP decoding and the empty string. We showed that sparse distributions suffer less from the cat got your tongue problem than their dense counterparts. This makes sense in light of the finding that exact MAP decoding works for MI, where probabilities are very peaked even with softmax (Forster and Meister, 2020). For tasks like MT, this is not the case: Eikema and Aziz (2020) pointed out that the argmax receives so little mass that it is almost arbitrary, so seeking it with MAP decoding (which beam search approximates) itself causes many deficiencies of decoding. On the other hand, Meister et al. (2020a) showed that beam search has a helpful bias and introduced regularization penalties for MAP decoding that encode it explicitly. Entmax neither directly addresses the faults of MAP decoding nor compensates for the locality P |Bm | 10 biases of beam search, instead shrinking the gap ECE = M | acc(B )−conf(B )| partitions m m m=1 N the model’s N force-decoded predicti"
2021.naacl-main.210,P18-1195,0,0.0171298,"h reverses the direction of the KL divergence in the smoothing expression. Meister et al. (2020b) then introduced a parameterized family of generalized smoothing techniques, different from Fenchel-Young Label Smoothing, which recovers vanilla label smoothing and the confidence penalty as special cases. In a different direction, Wang et al. (2020) improved inference calibration with a graduated label smoothing technique that uses larger smoothing weights for predictions that a baseline model is more confident of. Other works have smoothed over sequences instead of tokens (Norouzi et al., 2016; Elbayad et al., 2018; Lukasik et al., 2020), but this requires approximate techniques for deciding which sequences to smooth. MAP decoding and the empty string. We showed that sparse distributions suffer less from the cat got your tongue problem than their dense counterparts. This makes sense in light of the finding that exact MAP decoding works for MI, where probabilities are very peaked even with softmax (Forster and Meister, 2020). For tasks like MT, this is not the case: Eikema and Aziz (2020) pointed out that the argmax receives so little mass that it is almost arbitrary, so seeking it with MAP decoding (whi"
2021.naacl-main.210,N16-1077,0,0.0284682,"ary. This allows a sparse model to shrink 1 Introduction the search space: that is, it can learn to give inadequate hypotheses zero probability, instead of Sequence-to-sequence models (seq2seq: Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., counting on beam search to prune them. This has already been demonstrated for MI, where the set of 2017) have become a powerful and flexible tool for a variety of NLP tasks, including machine transla- possible hypotheses is often small enough to make beam search exact (Peters et al., 2019; Peters and tion (MT), morphological inflection (MI; Faruqui et al., 2016), and grapheme-to-phoneme conver- Martins, 2019). We extend this analysis to MT: although exact beam search is not possible for this sion (G2P; Yao and Zweig, 2015). These models often perform well, but they have a bias that fa- large vocabulary task, we show that entmax modvors short hypotheses. This bias is problematic: els prune many inadequate hypotheses, effectively solving the cat got your tongue problem. it has been pointed out as the cause (Koehn and Knowles, 2017; Yang et al., 2018; Murray and ChiDespite this useful result, one drawback of ent2642 Proceedings of the 2021 Conference of"
2021.naacl-main.210,2020.sigmorphon-1.10,0,0.0205246,"g technique that uses larger smoothing weights for predictions that a baseline model is more confident of. Other works have smoothed over sequences instead of tokens (Norouzi et al., 2016; Elbayad et al., 2018; Lukasik et al., 2020), but this requires approximate techniques for deciding which sequences to smooth. MAP decoding and the empty string. We showed that sparse distributions suffer less from the cat got your tongue problem than their dense counterparts. This makes sense in light of the finding that exact MAP decoding works for MI, where probabilities are very peaked even with softmax (Forster and Meister, 2020). For tasks like MT, this is not the case: Eikema and Aziz (2020) pointed out that the argmax receives so little mass that it is almost arbitrary, so seeking it with MAP decoding (which beam search approximates) itself causes many deficiencies of decoding. On the other hand, Meister et al. (2020a) showed that beam search has a helpful bias and introduced regularization penalties for MAP decoding that encode it explicitly. Entmax neither directly addresses the faults of MAP decoding nor compensates for the locality P |Bm | 10 biases of beam search, instead shrinking the gap ECE = M | acc(B )−co"
2021.naacl-main.210,2020.sigmorphon-1.2,0,0.0601619,"Missing"
2021.naacl-main.210,W17-3204,0,0.0269396,"ses is often small enough to make beam search exact (Peters et al., 2019; Peters and tion (MT), morphological inflection (MI; Faruqui et al., 2016), and grapheme-to-phoneme conver- Martins, 2019). We extend this analysis to MT: although exact beam search is not possible for this sion (G2P; Yao and Zweig, 2015). These models often perform well, but they have a bias that fa- large vocabulary task, we show that entmax modvors short hypotheses. This bias is problematic: els prune many inadequate hypotheses, effectively solving the cat got your tongue problem. it has been pointed out as the cause (Koehn and Knowles, 2017; Yang et al., 2018; Murray and ChiDespite this useful result, one drawback of ent2642 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2642–2654 June 6–11, 2021. ©2021 Association for Computational Linguistics max is that it is not compatible with label smoothing (Szegedy et al., 2016), a useful regularization technique that is widely used for transformers (Vaswani et al., 2017). We solve this problem by generalizing label smoothing from the crossentropy loss to the wider class of Fenchel-Youn"
2021.naacl-main.210,D19-3019,0,0.0186935,"niform smoothing distribution, discouraging sparsity. We use  ∈ {0, 0.01, 0.02, . . . , 0.15} for G2P,  ∈ {0, 0.01, 0.05, 0.1} for MI, and  ∈ {0, 0.01, 0.1} for MT. We trained all models with early stopping for a maximum of 30 epochs for MI, 15 epochs for WMT 2014 English → German MT, and 100 epochs otherwise, keeping the best checkpoint according to a task-specific validation metric: Phoneme Error Rate for G2P, average Levenshtein distance for MI, and detokenized BLEU score for MT. At test time, we decoded with a beam width of 5. Our PyTorch code (Paszke et al., 2017) is based on JoeyNMT (Kreutzer et al., 2019) and the entmax implementation from the entmax package.4 4.1 Multilingual G2P Data. We use the data from SIGMORPHON 2020 Task 1 (Gorman et al., 2020), which includes 3600 training examples in each of 15 languages. We train a single multilingual model (following Peters and Martins, 2020) which must learn to apply spelling rules from several writing systems. Training. Our models are similar to Peters and Martins (2020)’s RNNs, but with entmax 1.5 attention, and language embeddings only in the source. 4 https://github.com/deep-spin/entmax Results. Multilingual G2P results are shown in Table 1, al"
2021.naacl-main.210,2020.emnlp-main.405,0,0.0325524,"on of the KL divergence in the smoothing expression. Meister et al. (2020b) then introduced a parameterized family of generalized smoothing techniques, different from Fenchel-Young Label Smoothing, which recovers vanilla label smoothing and the confidence penalty as special cases. In a different direction, Wang et al. (2020) improved inference calibration with a graduated label smoothing technique that uses larger smoothing weights for predictions that a baseline model is more confident of. Other works have smoothed over sequences instead of tokens (Norouzi et al., 2016; Elbayad et al., 2018; Lukasik et al., 2020), but this requires approximate techniques for deciding which sequences to smooth. MAP decoding and the empty string. We showed that sparse distributions suffer less from the cat got your tongue problem than their dense counterparts. This makes sense in light of the finding that exact MAP decoding works for MI, where probabilities are very peaked even with softmax (Forster and Meister, 2020). For tasks like MT, this is not the case: Eikema and Aziz (2020) pointed out that the argmax receives so little mass that it is almost arbitrary, so seeking it with MAP decoding (which beam search approxim"
2021.naacl-main.210,W19-4226,0,0.359966,"nstrating that entmax loss is an elegant way to remove a major class of NMT model errors. • We generalize label smoothing from the crossentropy loss to the wider class of FenchelYoung losses, exhibiting a formulation for label smoothing which, to our knowledge, is novel. • We show that Fenchel-Young label smoothing with entmax loss is highly effective on both character- and word-level tasks. Our technique allows us to set a new state of the art on the SIGMORPHON 2020 shared task for multilingual G2P (Gorman et al., 2020). It also delivers improvements for crosslingual MI from SIGMORPHON 2019 (McCarthy et al., 2019) and for MT on IWSLT 2017 German ↔ English (Cettolo et al., 2017), KFTT Japanese ↔ English (Neubig, 2011), WMT 2016 Romanian ↔ English (Bojar et al., 2016), and WMT 2014 English → German (Bojar et al., 2014) compared to smoothed and unsmoothed cross-entropy loss. 2 Background A seq2seq model learns a probability distribution pθ (y |x) over sequences y from a target vocabulary V , conditioned on a source sequence x. This 1 Our code is available at https://github.com/ deep-spin/S7. distribution is then used at decoding time to find the most likely sequence yˆ: yˆ = argmax pθ (y |x), (1) y∈V ∗ wh"
2021.naacl-main.210,2020.emnlp-main.170,0,0.0302584,"Missing"
2021.naacl-main.210,2020.acl-main.615,0,0.364709,"e models present a els to assign higher scores to good translations than possible solution, since they can shrink the to bad ones, rather than to depend on search errors search space by assigning zero probability to to make up for model errors. bad hypotheses, but their ability to handle word-level tasks with transformers has never The most common way to alleviate this shortbeen tested. In this work, we show that coming is by altering the decoding objective (Wu entmax-based models effectively solve the cat et al., 2016; He et al., 2016; Yang et al., 2018; got your tongue problem, removing a maMeister et al., 2020a), but this does not address jor source of model error for neural machine the underlying problem: the model overestimates translation. In addition, we generalize label the probability of implausible hypotheses. Other smoothing, a critical regularization technique, to the broader family of Fenchel-Young losses, solutions use alternate training strategies (Murray which includes both cross-entropy and the entand Chiang, 2018; Shen et al., 2016), but it would max losses. Our resulting label-smoothed entbe preferable not to change the training algorithm. max loss models set a new state of the art"
2021.naacl-main.210,W18-6322,0,0.0345304,"Missing"
2021.naacl-main.210,P02-1040,0,0.11134,"8 ± 0.16 RO  EN 29.41 ± 0.20 30.03 ± 0.05 30.27 ± 0.16 30.37 ± 0.04 EN  RO 22.84 ± 0.12 23.15 ± 0.27 23.74 ± 0.08 23.47 ± 0.04 WMT 14 25.10 ± 0.18 25.21 ± 0.04 25.46 ± 0.19 25.45 ± 0.11 Table 3: MT results, averaged over three runs. For label smoothing, we select the best  on the development set. Note that WMT 14 refers to WMT 2014 English → German. steps for WMT 14 and 10,000 steps for the other pairs. The batch size was 8192 tokens. loss does better with  = 0.01 for every pair except RO  EN in terms of BLEU. Results. Table 3 reports our models’ performance in terms of untokenized BLEU (Papineni et al., 2002), which we computed with SacreBLEU (Post, 2018). The results show a clear advantage for label smoothing and entmax loss, both separately and together: label-smoothed entmax loss is the best-performing configuration on 3 out of 7 language pairs, unsmoothed entmax loss performs best on another 3 out of 7, and they tie on the remaining one. Although label-smoothed cross-entropy is seen as an essential ingredient for transformer training, entmax loss models beat it even without label smoothing for every pair except ENDE. Other inadequate strings. Even if a model rules out the empty string, it mig"
2021.naacl-main.210,W19-4207,1,0.925096,"α = 2, we recover sparsemax (Martins and Astudillo, 2016). For α ∈ {1.5, 2}, fast algorithms to compute (4) are available which are almost as fast as evaluating softmax. For other values of α, slower bisection algorithms exist. Entmax transformations are sparse for any α &gt; 1, with higher values tending to produce sparser outputs. This sparsity allows a model to assign exactly zero probability to implausible hypotheses. For tasks where there is only one correct target sequence, this often allows the model to concentrate all probability mass into a small set of hypotheses, making search exact (Peters and Martins, 2019). This is not possible for open-ended tasks like machine translation, but the model is still locally sparse, assigning zero probability to many hypotheses. These hypotheses will never be selected at any beam width. Fenchel-Young Losses. Inspired by the softmax generalization above, Blondel et al. (2020) provided a tool for constructing a convex loss function. Let Ω : 4|V |→ R be a strictly convex regularizer which is symmetric, i.e., Ω(Πp) = Ω(p) for any permutation Π and any p ∈ 4|V |.3 Equipped with Ω, we can define a regularized prediction function π ˆΩ : R|V |→ 4|V |, with this form: π ˆΩ"
2021.naacl-main.210,2020.sigmorphon-1.4,1,0.784582,"MT, and 100 epochs otherwise, keeping the best checkpoint according to a task-specific validation metric: Phoneme Error Rate for G2P, average Levenshtein distance for MI, and detokenized BLEU score for MT. At test time, we decoded with a beam width of 5. Our PyTorch code (Paszke et al., 2017) is based on JoeyNMT (Kreutzer et al., 2019) and the entmax implementation from the entmax package.4 4.1 Multilingual G2P Data. We use the data from SIGMORPHON 2020 Task 1 (Gorman et al., 2020), which includes 3600 training examples in each of 15 languages. We train a single multilingual model (following Peters and Martins, 2020) which must learn to apply spelling rules from several writing systems. Training. Our models are similar to Peters and Martins (2020)’s RNNs, but with entmax 1.5 attention, and language embeddings only in the source. 4 https://github.com/deep-spin/entmax Results. Multilingual G2P results are shown in Table 1, along with the best previous result (Yu et al., 2020). We report two error metrics, each of which is computed per-language and averaged: • Word Error Rate (WER) is the percentage of hypotheses which do not exactly match the reference. This harsh metric gives no credit for partial matches."
2021.naacl-main.210,P19-1146,1,0.851923,"erestimates translation. In addition, we generalize label the probability of implausible hypotheses. Other smoothing, a critical regularization technique, to the broader family of Fenchel-Young losses, solutions use alternate training strategies (Murray which includes both cross-entropy and the entand Chiang, 2018; Shen et al., 2016), but it would max losses. Our resulting label-smoothed entbe preferable not to change the training algorithm. max loss models set a new state of the art In this paper, we propose a solution based on on multilingual grapheme-to-phoneme conversparse seq2seq models (Peters et al., 2019), which sion and deliver improvements and better calireplace the output softmax (Bridle, 1990) with the bration properties on cross-lingual morphologentmax transformation. Entmax, unlike softmax, ical inflection and machine translation for 7 language pairs. can learn locally sparse distributions over the target vocabulary. This allows a sparse model to shrink 1 Introduction the search space: that is, it can learn to give inadequate hypotheses zero probability, instead of Sequence-to-sequence models (seq2seq: Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., counting on beam search"
2021.naacl-main.210,W18-6319,0,0.0178075,".37 ± 0.04 EN  RO 22.84 ± 0.12 23.15 ± 0.27 23.74 ± 0.08 23.47 ± 0.04 WMT 14 25.10 ± 0.18 25.21 ± 0.04 25.46 ± 0.19 25.45 ± 0.11 Table 3: MT results, averaged over three runs. For label smoothing, we select the best  on the development set. Note that WMT 14 refers to WMT 2014 English → German. steps for WMT 14 and 10,000 steps for the other pairs. The batch size was 8192 tokens. loss does better with  = 0.01 for every pair except RO  EN in terms of BLEU. Results. Table 3 reports our models’ performance in terms of untokenized BLEU (Papineni et al., 2002), which we computed with SacreBLEU (Post, 2018). The results show a clear advantage for label smoothing and entmax loss, both separately and together: label-smoothed entmax loss is the best-performing configuration on 3 out of 7 language pairs, unsmoothed entmax loss performs best on another 3 out of 7, and they tie on the remaining one. Although label-smoothed cross-entropy is seen as an essential ingredient for transformer training, entmax loss models beat it even without label smoothing for every pair except ENDE. Other inadequate strings. Even if a model rules out the empty string, it might assign nonzero probability to other short, i"
2021.naacl-main.210,P16-1162,0,0.0304263,"’ simpler training technique, 6 Machine Translation We specifically use the official task numbers from McCarthy et al. (2019), which are more complete than those reported in Anastasopoulos and Neubig (2019). Data. We made use of these language pairs: • IWSLT 2017 German ↔ English (DE↔EN, Cettolo et al., 2017): 200k training examples. • KFTT Japanese ↔ English (JA↔EN, Neubig, 2011): 330k training examples. • WMT 2016 Romanian ↔ English (RO↔EN, Bojar et al., 2016): 610k training examples. • WMT 2014 English → German (WMT 14, Bojar et al., 2014): 4.4 million training examples. We used joint BPE (Sennrich et al., 2016) for all language pairs,7 with 25,000 merges for WMT 14 and 32,000 merges for all other pairs. Training. We trained transformers with the base dimension and layer settings (Vaswani et al., 2017). We optimized with Adam (Kingma and Ba, 2015) and used Noam scheduling with 20,000 warmup 7 Although English and Japanese have different writing systems, we still found it beneficial to use joint BPE for JA↔ EN because many subwords occur in both the English and Japanese training corpora. These include many named entities, which are often written with the native form alongside the transliteration. 2647"
2021.naacl-main.210,P16-1159,0,0.0263884,"ing objective (Wu entmax-based models effectively solve the cat et al., 2016; He et al., 2016; Yang et al., 2018; got your tongue problem, removing a maMeister et al., 2020a), but this does not address jor source of model error for neural machine the underlying problem: the model overestimates translation. In addition, we generalize label the probability of implausible hypotheses. Other smoothing, a critical regularization technique, to the broader family of Fenchel-Young losses, solutions use alternate training strategies (Murray which includes both cross-entropy and the entand Chiang, 2018; Shen et al., 2016), but it would max losses. Our resulting label-smoothed entbe preferable not to change the training algorithm. max loss models set a new state of the art In this paper, we propose a solution based on on multilingual grapheme-to-phoneme conversparse seq2seq models (Peters et al., 2019), which sion and deliver improvements and better calireplace the output softmax (Bridle, 1990) with the bration properties on cross-lingual morphologentmax transformation. Entmax, unlike softmax, ical inflection and machine translation for 7 language pairs. can learn locally sparse distributions over the target vo"
2021.naacl-main.210,D19-1331,0,0.0694181,"pace Ben Peters† and Andr´e F. T. Martins†‡∗ Instituto de Telecomunicac¸o˜ es, Instituto Superior T´ecnico, Lisbon, Portugal ‡ LUMLIS (Lisbon ELLIS Unit), Lisbon, Portugal ∗ Unbabel, Lisbon, Portugal benzurdopeters@gmail.com, andre.t.martins@tecnico.ulisboa.pt † Abstract ang, 2018) of the beam search curse, in which increasing the width of beam search actually deCurrent sequence-to-sequence models are creases performance on neural machine translation trained to minimize cross-entropy and use soft(NMT). Further illustrating the severity of the probmax to compute the locally normalized problem, Stahlberg and Byrne (2019) showed that the abilities over target sequences. While this highest-scoring target sequence in NMT is often setup has led to strong results in a variety of tasks, one unsatisfying aspect is its length bias: the empty string, a phenomenon they dubbed the models give high scores to short, inadequate cat got your tongue problem. These results are hypotheses and often make the empty string undesirable because they show that NMT models’ the argmax—the so-called cat got your tongue performance depends on the search errors induced problem. Recently proposed entmax-based by a narrow beam. It would be"
2021.naacl-main.210,2020.acl-main.278,0,0.0195792,"ake the tail heavier. Setting α = 1.5 with a moderate  value seems to be a sensible balance. 6 Related Work Label smoothing. Our work fits into a larger family of techniques that penalize model overconfidence. Pereyra et al. (2017) proposed the confidence penalty, which reverses the direction of the KL divergence in the smoothing expression. Meister et al. (2020b) then introduced a parameterized family of generalized smoothing techniques, different from Fenchel-Young Label Smoothing, which recovers vanilla label smoothing and the confidence penalty as special cases. In a different direction, Wang et al. (2020) improved inference calibration with a graduated label smoothing technique that uses larger smoothing weights for predictions that a baseline model is more confident of. Other works have smoothed over sequences instead of tokens (Norouzi et al., 2016; Elbayad et al., 2018; Lukasik et al., 2020), but this requires approximate techniques for deciding which sequences to smooth. MAP decoding and the empty string. We showed that sparse distributions suffer less from the cat got your tongue problem than their dense counterparts. This makes sense in light of the finding that exact MAP decoding works"
2021.naacl-main.210,D18-1342,0,0.0306268,"Missing"
2021.naacl-main.210,2020.sigmorphon-1.5,0,0.0665407,"the entmax package.4 4.1 Multilingual G2P Data. We use the data from SIGMORPHON 2020 Task 1 (Gorman et al., 2020), which includes 3600 training examples in each of 15 languages. We train a single multilingual model (following Peters and Martins, 2020) which must learn to apply spelling rules from several writing systems. Training. Our models are similar to Peters and Martins (2020)’s RNNs, but with entmax 1.5 attention, and language embeddings only in the source. 4 https://github.com/deep-spin/entmax Results. Multilingual G2P results are shown in Table 1, along with the best previous result (Yu et al., 2020). We report two error metrics, each of which is computed per-language and averaged: • Word Error Rate (WER) is the percentage of hypotheses which do not exactly match the reference. This harsh metric gives no credit for partial matches. • Phoneme Error Rate (PER) is the sum of Levenshtein distances between each hypothesis and the corresponding reference, divided by the total length of the references. These results show that the benefits of sparse losses and label smoothing can be combined. Individually, both label smoothing and sparse loss functions (α &gt; 1) consistently improve over unsmoothed"
almeida-etal-2014-priberam,D10-1001,0,\N,Missing
almeida-etal-2014-priberam,D12-1022,0,\N,Missing
almeida-etal-2014-priberam,W09-1801,1,\N,Missing
almeida-etal-2014-priberam,W00-0403,0,\N,Missing
almeida-etal-2014-priberam,W04-1013,0,\N,Missing
almeida-etal-2014-priberam,W03-1101,0,\N,Missing
almeida-etal-2014-priberam,P13-1020,1,\N,Missing
almeida-etal-2014-priberam,barreto-etal-2006-open,0,\N,Missing
almeida-etal-2014-priberam,P11-1049,0,\N,Missing
almeida-etal-2014-priberam,P13-2109,1,\N,Missing
amaral-etal-2004-design,P99-1023,0,\N,Missing
D08-1017,P99-1070,0,0.0394765,"Missing"
D08-1017,W06-2920,0,0.089328,"l the arcs using a log-linear classifier with access to the full unlabeled parse (McDonald et al., 2005a; 162 McDonald et al., 2005b; McDonald and Pereira, 2006). In stacking experiments, the arc labels from the level 0 parser are also used as a feature.4 In the following subsections, we refer to our modification of the MSTParser as MST 1O (the arcfactored version) and MST 2O (the second-order arc-pair-factored version). All our experiments use the non-projective version of this parser. We refer to the MaltParser as Malt. We report experiments on twelve languages from the CoNLL-X shared task (Buchholz and Marsi, 2006).5 All experiments are evaluated using the labeled attachment score (LAS), using the default settings.6 Statistical significance is measured using Dan Bikel’s randomized parsing evaluation comparator with 10,000 iterations.7 The additional features used in the level 1 parser are enumerated in Table 1 and their various subsets are depicted in Table 2. The PredEdge features are exactly the six features used by Nivre and McDonald (2008) in their MSTMalt parser; therefore, feature set A is a replication of this parser except for modifications noted in footnote 4. In all our experiments, the number"
D08-1017,P04-1054,0,0.0321457,"Missing"
D08-1017,P05-1067,0,0.0603954,"Missing"
D08-1017,P99-1059,0,0.362595,"art dependency parsers. 1 • Feature-rich parsers must resort to search or greediness, (Ratnaparkhi et al., 1994; Sagae and Lavie, 2005; Hall et al., 2006), so that parsing solutions are inexact and learned models may be subject to certain kinds of bias (Lafferty et al., 2001). Introduction In this paper we address a representation-efficiency tradeoff in statistical natural language processing through the use of stacked learning (Wolpert, 1992). This tradeoff is exemplified in dependency parsing, illustrated in Fig. 1, on which we focus in this paper: • Exact algorithms for dependency parsing (Eisner and Satta, 1999; McDonald et al., 2005b) are tractable only when the model makes very strong, linguistically unsupportable independence A solution that leverages the complementary strengths of these two approaches—described in detail by McDonald and Nivre (2007)—was recently and successfully explored by Nivre and McDonald (2008). Our contribution begins by reinterpreting and generalizing their parser combination scheme as a stacking of parsers. We give a new theoretical motivation for stacking parsers, in terms of extending a parsing model’s feature space. Specifically, we view stacked learning as a way of a"
D08-1017,C96-1058,0,0.903966,"of these two approaches—described in detail by McDonald and Nivre (2007)—was recently and successfully explored by Nivre and McDonald (2008). Our contribution begins by reinterpreting and generalizing their parser combination scheme as a stacking of parsers. We give a new theoretical motivation for stacking parsers, in terms of extending a parsing model’s feature space. Specifically, we view stacked learning as a way of approximating non-local features in a linear model, rather than making empirically dubious independence (McDonald et al., 2005b) or structural assumptions (e.g., projectivity, Eisner, 1996), using search approximations (Sagae and Lavie, 2005; Hall et al., 2006; McDonald and Pereira, 2006), solving a (generally NP-hard) integer linear program (Riedel and Clarke, 2006), or adding latent variables (Titov and Henderson, 2007). Notably, we introduce the use of very rich non-local approximate features in one parser, through the output of another parser. Related approaches are the belief propagation algorithm of Smith and Eisner (2008), and the “trading of structure for features” explored by Liang et al. 157 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Pr"
D08-1017,P06-2041,0,0.494122,"tures for another. We show that this is an example of stacked learning, in which a second predictor is trained to improve the performance of the first. Further, we argue that this technique is a novel way of approximating rich non-local features in the second parser, without sacrificing efficient, model-optimal prediction. Experiments on twelve languages show that stacking transition-based and graphbased parsers improves performance over existing state-of-the-art dependency parsers. 1 • Feature-rich parsers must resort to search or greediness, (Ratnaparkhi et al., 1994; Sagae and Lavie, 2005; Hall et al., 2006), so that parsing solutions are inexact and learned models may be subject to certain kinds of bias (Lafferty et al., 2001). Introduction In this paper we address a representation-efficiency tradeoff in statistical natural language processing through the use of stacked learning (Wolpert, 1992). This tradeoff is exemplified in dependency parsing, illustrated in Fig. 1, on which we focus in this paper: • Exact algorithms for dependency parsing (Eisner and Satta, 1999; McDonald et al., 2005b) are tractable only when the model makes very strong, linguistically unsupportable independence A solution"
D08-1017,D07-1013,0,0.0628025,"of bias (Lafferty et al., 2001). Introduction In this paper we address a representation-efficiency tradeoff in statistical natural language processing through the use of stacked learning (Wolpert, 1992). This tradeoff is exemplified in dependency parsing, illustrated in Fig. 1, on which we focus in this paper: • Exact algorithms for dependency parsing (Eisner and Satta, 1999; McDonald et al., 2005b) are tractable only when the model makes very strong, linguistically unsupportable independence A solution that leverages the complementary strengths of these two approaches—described in detail by McDonald and Nivre (2007)—was recently and successfully explored by Nivre and McDonald (2008). Our contribution begins by reinterpreting and generalizing their parser combination scheme as a stacking of parsers. We give a new theoretical motivation for stacking parsers, in terms of extending a parsing model’s feature space. Specifically, we view stacked learning as a way of approximating non-local features in a linear model, rather than making empirically dubious independence (McDonald et al., 2005b) or structural assumptions (e.g., projectivity, Eisner, 1996), using search approximations (Sagae and Lavie, 2005; Hall"
D08-1017,E06-1011,0,0.769242,"and successfully explored by Nivre and McDonald (2008). Our contribution begins by reinterpreting and generalizing their parser combination scheme as a stacking of parsers. We give a new theoretical motivation for stacking parsers, in terms of extending a parsing model’s feature space. Specifically, we view stacked learning as a way of approximating non-local features in a linear model, rather than making empirically dubious independence (McDonald et al., 2005b) or structural assumptions (e.g., projectivity, Eisner, 1996), using search approximations (Sagae and Lavie, 2005; Hall et al., 2006; McDonald and Pereira, 2006), solving a (generally NP-hard) integer linear program (Riedel and Clarke, 2006), or adding latent variables (Titov and Henderson, 2007). Notably, we introduce the use of very rich non-local approximate features in one parser, through the output of another parser. Related approaches are the belief propagation algorithm of Smith and Eisner (2008), and the “trading of structure for features” explored by Liang et al. 157 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 157–166, c Honolulu, October 2008. 2008 Association for Computational Linguistics le"
D08-1017,W07-2216,0,0.226249,"nts to solving arg maxy∈Y(x) w> f (x, y), where w is a weight vector. With a projectivity constraint and arc factorization, the parsing problem can be solved in cubic time by dynamic programming (Eisner, 1996), and with a weaker “tree” constraint (permitting nonprojective parses) and arc factorization, a quadratic-time algorithm exists (Chu and Liu, 1965; Edmonds, 1967), as shown by McDonald et al. (2005b). In the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta, 1999), but not in the nonprojective case (McDonald and Satta, 2007), where finding the highest-scoring tree becomes NP-hard. McDonald and Pereira (2006) adopted an approximation based on O(n3 ) projective parsing followed by rearrangement to permit crossing arcs, achieving higher performance. In §3 we adopt a framework that maintains O(n2 ) runtime (still exploiting the Chu-Liu-Edmonds algorithm) while approximating non arc-factored features. 2.2 Stacked Learning Stacked generalization was first proposed by Wolpert (1992) and Breiman (1996) for regression. The idea is to include two “levels” of predictors. The first level, “level 0,” includes one or more pred"
D08-1017,P05-1012,0,0.565159,"c P. Xing∗ ∗ School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA † Instituto de Telecomunicac¸o˜ es, Instituto Superior T´ecnico, Lisboa, Portugal {afm,dipanjan,nasmith,epxing}@cs.cmu.edu Abstract assumptions, such as “arc factorization” for nonprojective dependency parsing (McDonald and Satta, 2007). We explore a stacked framework for learning to predict dependency structures for natural language sentences. A typical approach in graph-based dependency parsing has been to assume a factorized model, where local features are used but a global function is optimized (McDonald et al., 2005b). Recently Nivre and McDonald (2008) used the output of one dependency parser to provide features for another. We show that this is an example of stacked learning, in which a second predictor is trained to improve the performance of the first. Further, we argue that this technique is a novel way of approximating rich non-local features in the second parser, without sacrificing efficient, model-optimal prediction. Experiments on twelve languages show that stacking transition-based and graphbased parsers improves performance over existing state-of-the-art dependency parsers. 1 • Feature-rich p"
D08-1017,H05-1066,0,0.389842,"c P. Xing∗ ∗ School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA † Instituto de Telecomunicac¸o˜ es, Instituto Superior T´ecnico, Lisboa, Portugal {afm,dipanjan,nasmith,epxing}@cs.cmu.edu Abstract assumptions, such as “arc factorization” for nonprojective dependency parsing (McDonald and Satta, 2007). We explore a stacked framework for learning to predict dependency structures for natural language sentences. A typical approach in graph-based dependency parsing has been to assume a factorized model, where local features are used but a global function is optimized (McDonald et al., 2005b). Recently Nivre and McDonald (2008) used the output of one dependency parser to provide features for another. We show that this is an example of stacked learning, in which a second predictor is trained to improve the performance of the first. Further, we argue that this technique is a novel way of approximating rich non-local features in the second parser, without sacrificing efficient, model-optimal prediction. Experiments on twelve languages show that stacking transition-based and graphbased parsers improves performance over existing state-of-the-art dependency parsers. 1 • Feature-rich p"
D08-1017,W06-2932,0,0.0406989,"odifications noted in footnote 4. In all our experiments, the number of ˜ is L = 2. partitions used to create D 5.2 Experiment: MST 2O + MST 2O Our first experiment stacks the highly accurate MST 2O parser with itself. At level 0, the parser uses only the standard features (§5.1), and at level 1, these are augmented by various subsets of features of x along with the output of the level 0 parser, g(x) (Table 2). The results are shown in Table 3. While we see improvements over the single-parser baseline 4 We made other modifications to MSTParser, implementing many of the successes described by (McDonald et al., 2006). Our version of the code is publicly available at http: //www.ark.cs.cmu.edu/MSTParserStacked. The modifications included an approximation to lemmas for datasets without lemmas (three-character prefixes), and replacing morphology/word and morphology/lemma features with morphology/POS features. 5 The CoNLL-X shared task actually involves thirteen languages; our experiments do not include Czech (the largest dataset), due to time constraints. Therefore, the average results plotted in the last rows of Tables 3, 4, and 5 are not directly comparable with previously published averages over thirteen"
D08-1017,P08-1108,0,0.189783,"ience, Carnegie Mellon University, Pittsburgh, PA 15213, USA † Instituto de Telecomunicac¸o˜ es, Instituto Superior T´ecnico, Lisboa, Portugal {afm,dipanjan,nasmith,epxing}@cs.cmu.edu Abstract assumptions, such as “arc factorization” for nonprojective dependency parsing (McDonald and Satta, 2007). We explore a stacked framework for learning to predict dependency structures for natural language sentences. A typical approach in graph-based dependency parsing has been to assume a factorized model, where local features are used but a global function is optimized (McDonald et al., 2005b). Recently Nivre and McDonald (2008) used the output of one dependency parser to provide features for another. We show that this is an example of stacked learning, in which a second predictor is trained to improve the performance of the first. Further, we argue that this technique is a novel way of approximating rich non-local features in the second parser, without sacrificing efficient, model-optimal prediction. Experiments on twelve languages show that stacking transition-based and graphbased parsers improves performance over existing state-of-the-art dependency parsers. 1 • Feature-rich parsers must resort to search or greedi"
D08-1017,W04-2407,0,0.0834669,"Missing"
D08-1017,W06-2933,0,0.0492279,"Missing"
D08-1017,W06-1616,0,0.178317,"reinterpreting and generalizing their parser combination scheme as a stacking of parsers. We give a new theoretical motivation for stacking parsers, in terms of extending a parsing model’s feature space. Specifically, we view stacked learning as a way of approximating non-local features in a linear model, rather than making empirically dubious independence (McDonald et al., 2005b) or structural assumptions (e.g., projectivity, Eisner, 1996), using search approximations (Sagae and Lavie, 2005; Hall et al., 2006; McDonald and Pereira, 2006), solving a (generally NP-hard) integer linear program (Riedel and Clarke, 2006), or adding latent variables (Titov and Henderson, 2007). Notably, we introduce the use of very rich non-local approximate features in one parser, through the output of another parser. Related approaches are the belief propagation algorithm of Smith and Eisner (2008), and the “trading of structure for features” explored by Liang et al. 157 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 157–166, c Honolulu, October 2008. 2008 Association for Computational Linguistics learning via the EM algorithm – none of which have 2.1 Dependency Parsing previous"
D08-1017,W05-1513,0,0.192799,"y parser to provide features for another. We show that this is an example of stacked learning, in which a second predictor is trained to improve the performance of the first. Further, we argue that this technique is a novel way of approximating rich non-local features in the second parser, without sacrificing efficient, model-optimal prediction. Experiments on twelve languages show that stacking transition-based and graphbased parsers improves performance over existing state-of-the-art dependency parsers. 1 • Feature-rich parsers must resort to search or greediness, (Ratnaparkhi et al., 1994; Sagae and Lavie, 2005; Hall et al., 2006), so that parsing solutions are inexact and learned models may be subject to certain kinds of bias (Lafferty et al., 2001). Introduction In this paper we address a representation-efficiency tradeoff in statistical natural language processing through the use of stacked learning (Wolpert, 1992). This tradeoff is exemplified in dependency parsing, illustrated in Fig. 1, on which we focus in this paper: • Exact algorithms for dependency parsing (Eisner and Satta, 1999; McDonald et al., 2005b) are tractable only when the model makes very strong, linguistically unsupportable inde"
D08-1017,D08-1016,0,0.103348,"non-local features in a linear model, rather than making empirically dubious independence (McDonald et al., 2005b) or structural assumptions (e.g., projectivity, Eisner, 1996), using search approximations (Sagae and Lavie, 2005; Hall et al., 2006; McDonald and Pereira, 2006), solving a (generally NP-hard) integer linear program (Riedel and Clarke, 2006), or adding latent variables (Titov and Henderson, 2007). Notably, we introduce the use of very rich non-local approximate features in one parser, through the output of another parser. Related approaches are the belief propagation algorithm of Smith and Eisner (2008), and the “trading of structure for features” explored by Liang et al. 157 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 157–166, c Honolulu, October 2008. 2008 Association for Computational Linguistics learning via the EM algorithm – none of which have 2.1 Dependency Parsing previously been to have exact learning via the known EM algorithm – nonenon-projective of which have implementations. previously been known to have exact non-projective Dependency syntax is a lightweight syntactic repWe then switch to models that account for where implementa"
D08-1017,W07-2218,0,0.0295174,"on scheme as a stacking of parsers. We give a new theoretical motivation for stacking parsers, in terms of extending a parsing model’s feature space. Specifically, we view stacked learning as a way of approximating non-local features in a linear model, rather than making empirically dubious independence (McDonald et al., 2005b) or structural assumptions (e.g., projectivity, Eisner, 1996), using search approximations (Sagae and Lavie, 2005; Hall et al., 2006; McDonald and Pereira, 2006), solving a (generally NP-hard) integer linear program (Riedel and Clarke, 2006), or adding latent variables (Titov and Henderson, 2007). Notably, we introduce the use of very rich non-local approximate features in one parser, through the output of another parser. Related approaches are the belief propagation algorithm of Smith and Eisner (2008), and the “trading of structure for features” explored by Liang et al. 157 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 157–166, c Honolulu, October 2008. 2008 Association for Computational Linguistics learning via the EM algorithm – none of which have 2.1 Dependency Parsing previously been to have exact learning via the known EM algorith"
D08-1017,D07-1003,1,0.392434,"Missing"
D08-1017,W03-3023,0,0.499777,"Missing"
D10-1004,W06-2920,0,0.0814866,", using only features in F. Since the other features have not been used before, they have a zero weight, hence can be ignored. When β = ∞, the variational problem in Eq. 24 consists of a MAP computation and the soluˆ t ∈ Y(xt ). Only the tion corresponds to one output y ˆ t but not in yt , or vice-versa, parts that are active in y will have features that might receive a nonzero update. Those parts are reexamined for new features and the active set F is updated accordingly. 6 Experiments We trained non-projective dependency parsers for 14 languages, using datasets from the CoNLL-X shared task (Buchholz and Marsi, 2006) and two datasets for English: one from the CoNLL-2008 shared task (Surdeanu et al., 2008), which contains non-projective arcs, and another derived from the Penn Treebank applying the standard head rules of Yamada and Matsumoto (2003), in which all parse trees are projective.12 We implemented Alg. 1, 12 We used the provided train/test splits for all datasets. For English, we used the standard test partitions (section 23 of the Wall Street Journal). We did not exploit the fact that some datasets only contain projective trees and have unique roots. 42 which handles any loss function Lβ,γ .13 Whe"
D10-1004,P08-1109,0,0.0540744,"light on the optimization problems tackled in each method. We also propose a new aggressive online algorithm to learn the model parameters, which makes use of the underlying variational representation. The algorithm does not require a learning rate parameter and provides a single framework for a wide family of convex loss functions, including CRFs and structured SVMs. Experiments show state-of-the-art performance for 14 languages. 1 Introduction Feature-rich discriminative models that break locality/independence assumptions can boost a parser’s performance (McDonald et al., 2006; Huang, 2008; Finkel et al., 2008; Smith and Eisner, 2008; Martins et al., 2009; Koo and Collins, 2010). Often, inference with such models becomes computationally intractable, causing a demand for understanding and improving approximate parsing algorithms. In this paper, we show a formal connection between two recently-proposed approximate inference techniques for non-projective dependency parsing: loopy belief propagation (Smith and Eisner, 2008) and linear programming relaxation (Martins et al., 2009). While those two parsers are differently motivated, we show that both correspond to inference in a factor graph, and both op"
D10-1004,N10-1112,1,0.181926,"constant, and L(θ; x, y) is a nonnegative convex loss. Examples include the logistic loss used in CRFs (− log Prθ (y|x)) and the hinge loss of structured SVMs (maxy0 ∈Y(x) θ &gt; (φ(x, y0 )− • O(n2 ) XOR - WITH - OUTPUT factors to impose the φ(x, y)) + `(y0 , y) for some cost function `). These constraint that words don’t consume other words’ are both special cases of the family defined in Fig. 4, commodities; i.e., if h 6= k and k 6= 0, then there which also includes the structured perceptron’s loss is a path from h to k iff exactly one outgoing arc (β → ∞, γ = 0) and the softmax-margin loss of Gimpel and Smith (2010; β = γ = 1). in {hh, mi}m=1,...,n carries flow to k: Pn Alg. 1 is closely related to stochastic or online k , h, k ∈ {0, . . . , n}, k ∈ / {0, h}. phk = m=1 fhh,mi (19) gradient descent methods, but with the key advantage of not needing a learning rate hyperparameter. L(G0x ) is thus defined by the constraints in Eq. 12 We sketch the derivation of Alg. 1; full details can and 15–19. The approximate MAP problem, that be found in Martins et al. (2010a). On the tth round, replaces M(G0x ) by L(G0x ) in Eq. 10, thus becomes: one example hxt , yt i is considered. We seek to solve maxz,f ,p θ &gt; F(x"
D10-1004,P08-1067,0,0.058961,"aph, we shed light on the optimization problems tackled in each method. We also propose a new aggressive online algorithm to learn the model parameters, which makes use of the underlying variational representation. The algorithm does not require a learning rate parameter and provides a single framework for a wide family of convex loss functions, including CRFs and structured SVMs. Experiments show state-of-the-art performance for 14 languages. 1 Introduction Feature-rich discriminative models that break locality/independence assumptions can boost a parser’s performance (McDonald et al., 2006; Huang, 2008; Finkel et al., 2008; Smith and Eisner, 2008; Martins et al., 2009; Koo and Collins, 2010). Often, inference with such models becomes computationally intractable, causing a demand for understanding and improving approximate parsing algorithms. In this paper, we show a formal connection between two recently-proposed approximate inference techniques for non-projective dependency parsing: loopy belief propagation (Smith and Eisner, 2008) and linear programming relaxation (Martins et al., 2009). While those two parsers are differently motivated, we show that both correspond to inference in a fact"
D10-1004,P98-1106,0,0.0963536,"qs. 12 and 15–19 are satisfied. This is exactly the LP relaxation considered by Martins et al. (2009) in their multi-commodity flow model, for the configuration with siblings and grandparent features.7 They also considered a configuration with non-projectivity features—which fire if an arc is non-projective.8 That configuration can also be obtained here if variables {nhh,mi } are 7 To be precise, the constraints of Martins et al. (2009) are recovered after eliminating the path variables, via Eqs. 18–19. 8 An arc hh, mi is non-projective if there is some word in its span not descending from h (Kahane et al., 1998). 40 2 minθ,ξ λm 2 kθ − θ t k + ξ s.t. L(θ; xt , yt ) ≤ ξ, ξ ≥ 0, 9 (23) Given what was just exposed, it seems appealing to try max-product loopy BP on the factor graph of Fig. 1, or sumproduct loopy BP on the one in Fig. 3. Both attempts present serious challenges: the former requires computing messages sent by the tree factor, which requires O(n2 ) calls to the Chu-LiuEdmonds algorithm and hence O(n5 ) time. No obvious strategy seems to exist for simultaneous computation of all messages, unlike in the sum-product case. The latter is even more challenging, as standard sum-product loopy BP has"
D10-1004,P10-1001,0,0.382192,"propose a new aggressive online algorithm to learn the model parameters, which makes use of the underlying variational representation. The algorithm does not require a learning rate parameter and provides a single framework for a wide family of convex loss functions, including CRFs and structured SVMs. Experiments show state-of-the-art performance for 14 languages. 1 Introduction Feature-rich discriminative models that break locality/independence assumptions can boost a parser’s performance (McDonald et al., 2006; Huang, 2008; Finkel et al., 2008; Smith and Eisner, 2008; Martins et al., 2009; Koo and Collins, 2010). Often, inference with such models becomes computationally intractable, causing a demand for understanding and improving approximate parsing algorithms. In this paper, we show a formal connection between two recently-proposed approximate inference techniques for non-projective dependency parsing: loopy belief propagation (Smith and Eisner, 2008) and linear programming relaxation (Martins et al., 2009). While those two parsers are differently motivated, we show that both correspond to inference in a factor graph, and both optimize objective functions over local approximations of the marginal p"
D10-1004,D07-1015,0,0.0201707,"anning tree. There is a unary soft factor per arc, whose log-potential reflects the score of that arc. There are also O(n3 ) pairwise factors; their log-potentials reflect the scores of sibling and grandparent arcs. These factors create loops, thus calling for approximate inference. Without them, the model is arc-factored, and exact inference in it is well studied: finding the most probable parse tree takes O(n3 ) time with the ChuLiu-Edmonds algorithm (McDonald et al., 2005),2 and computing posterior marginals for all arcs takes O(n3 ) time via the matrix-tree theorem (Smith and Smith, 2007; Koo et al., 2007). Message-passing algorithms. In general factor graphs, both inference problems— obtaining the most probable output (the MAP) argmaxy∈Y(x) Prθ (y|x), and computing the marginals Prθ (Yi = yi |x)—can be addressed with the belief propagation (BP) algorithm (Pearl, 1988), which iteratively passes messages between variables and factors reflecting their local “beliefs.” There is a faster but more involved O(n2 ) algorithm due to Tarjan (1977). 2 v1 , . . . , vn ∈ SC where SC ⊆ {0, 1}n . otherwise, P Q vi • Message-induced distribution: ω , hmj→C ij=1,...,n • Partition function: ZC (ω) , hv1 ,...,vn"
D10-1004,D10-1125,0,0.168528,"ences (Smith and Eisner, 2008), as well as high order factors with countbased potentials (Tarlow et al., 2010), among others. Some of our combinatorial factors (OR, OR - WITH OUTPUT ) and the analogous entropy computations were never considered, to the best of our knowledge. Prop. 1 appears in Wainwright and Jordan (2008) for canonical overcomplete models; we adapt it here for models with shared features. We rely on the variational interpretation of loopy BP, due to Yedidia et al. (2001), to derive the objective being optimized by Smith and Eisner’s loopy BP parser. Independently of our work, Koo et al. (2010) recently proposed an efficient dual decomposition method to solve an LP problem similar (but not equal) to the one in Eq. 20,15 with excellent parsing performance. Their parser is also an instance of a turbo parser since it relies on a local approximation of a marginal polytope. While one can also use dual decomposition to address our MAP problem, the fact that our model does not decompose as nicely as the one in Koo et al. (2010) would likely result in slower convergence. 8 Conclusion We presented a unified view of two recent approximate dependency parsers, by stating their underlying factor"
D10-1004,D08-1017,1,0.731315,"Missing"
D10-1004,P09-1039,1,0.118924,"rtins∗† Noah A. Smith∗ Eric P. Xing∗ ∗ School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213, USA {afm,nasmith,epxing}@cs.cmu.edu M´ario A. T. Figueiredo† † Instituto de Telecomunicac¸o˜ es Instituto Superior T´ecnico Lisboa, Portugal mtf@lx.it.pt Pedro M. Q. Aguiar‡ ‡ Instituto de Sistemas e Rob´otica Instituto Superior T´ecnico Lisboa, Portugal aguiar@isr.ist.utl.pt Abstract We present a unified view of two state-of-theart non-projective dependency parsers, both approximate: the loopy belief propagation parser of Smith and Eisner (2008) and the relaxed linear program of Martins et al. (2009). By representing the model assumptions with a factor graph, we shed light on the optimization problems tackled in each method. We also propose a new aggressive online algorithm to learn the model parameters, which makes use of the underlying variational representation. The algorithm does not require a learning rate parameter and provides a single framework for a wide family of convex loss functions, including CRFs and structured SVMs. Experiments show state-of-the-art performance for 14 languages. 1 Introduction Feature-rich discriminative models that break locality/independence assumptions c"
D10-1004,D10-1004,1,0.135472,"1: Factor graph corresponding to the dependency parsing model of Smith and Eisner (2008) with sibling and grandparent features. Circles denote variable nodes, and squares denote factor nodes. Note the loops created by the inclusion of pairwise factors (GRAND and SIB). In Table 1 we present closed-form expressions for the factor-to-variable message ratios mC→i , MC→i (1)/MC→i (0) in terms of their variable-tofactor counterparts mi→C , Mi→C (1)/Mi→C (0); these ratios are all that is necessary when the variables are binary. Detailed derivations are presented in an extended version of this paper (Martins et al., 2010b). 3 Variational Representations Let Px , {Prθ (.|x) |θ ∈ Rd } be the family of all distributions of the form in Eq. 2. We next present an alternative parametrization for the distributions in Px in terms of factor marginals. We will see that each distribution can be seen as a point in the socalled marginal polytope (Wainwright and Jordan, 2008); this will pave the way for the variational representations to be derived next. Parts and Output Indicators. A part is a pair hC, yC i, where C is a soft factor and yC a partial output assignment. We let R = {hC, yC i |C ∈ Q Csoft , yC ∈ i∈C Yi } be th"
D10-1004,H05-1066,0,0.804287,"gs to the dependency tree. There is a hard factor TREE connected to all variables, that constrains the overall arc configurations to form a spanning tree. There is a unary soft factor per arc, whose log-potential reflects the score of that arc. There are also O(n3 ) pairwise factors; their log-potentials reflect the scores of sibling and grandparent arcs. These factors create loops, thus calling for approximate inference. Without them, the model is arc-factored, and exact inference in it is well studied: finding the most probable parse tree takes O(n3 ) time with the ChuLiu-Edmonds algorithm (McDonald et al., 2005),2 and computing posterior marginals for all arcs takes O(n3 ) time via the matrix-tree theorem (Smith and Smith, 2007; Koo et al., 2007). Message-passing algorithms. In general factor graphs, both inference problems— obtaining the most probable output (the MAP) argmaxy∈Y(x) Prθ (y|x), and computing the marginals Prθ (Yi = yi |x)—can be addressed with the belief propagation (BP) algorithm (Pearl, 1988), which iteratively passes messages between variables and factors reflecting their local “beliefs.” There is a faster but more involved O(n2 ) algorithm due to Tarjan (1977). 2 v1 , . . . , vn ∈"
D10-1004,W06-2932,0,0.140018,"ptions with a factor graph, we shed light on the optimization problems tackled in each method. We also propose a new aggressive online algorithm to learn the model parameters, which makes use of the underlying variational representation. The algorithm does not require a learning rate parameter and provides a single framework for a wide family of convex loss functions, including CRFs and structured SVMs. Experiments show state-of-the-art performance for 14 languages. 1 Introduction Feature-rich discriminative models that break locality/independence assumptions can boost a parser’s performance (McDonald et al., 2006; Huang, 2008; Finkel et al., 2008; Smith and Eisner, 2008; Martins et al., 2009; Koo and Collins, 2010). Often, inference with such models becomes computationally intractable, causing a demand for understanding and improving approximate parsing algorithms. In this paper, we show a formal connection between two recently-proposed approximate inference techniques for non-projective dependency parsing: loopy belief propagation (Smith and Eisner, 2008) and linear programming relaxation (Martins et al., 2009). While those two parsers are differently motivated, we show that both correspond to infere"
D10-1004,N03-1028,0,0.0351156,"challenges. Often only “supported” features—those observed in the training data—are included, and even those are commonly eliminated when their frequencies fall below a threshold. Important information may be lost as a result of these expedient choices. S Formally, the supported feature set is Fsupp , m i=1 supp φ(xi , yi ), where supp u , {j |uj 6= 0} denotes the support of vector u. Fsupp is a subset of the complete feature set, comprised of those features output, S occur in some candidate Sm that 0 Fcomp , i=1 y0 ∈Y(xi ) supp φ(xi , yi ). Features i in Fcomp Fsupp are called unsupported. Sha and Pereira (2003) have shown that training a CRF-based shallow parser with the complete feature set may improve performance (over the supported one), at the cost of 4.6 times more features. Dependency parsing has a much higher ratio (around 20 for bilexical word-word features, as estimated in the Penn Treebank), due to the quadratic or faster growth of the number of parts, of which only a few are active in a legal output. We propose a simple strategy for handling Fcomp efficiently, which can be applied for those losses in Fig. 4 where β = ∞. (e.g., the structured SVM and perceptron). Our procedure is the follo"
D10-1004,D08-1016,0,0.063561,"rsing by Approximate Variational Inference Andr´e F. T. Martins∗† Noah A. Smith∗ Eric P. Xing∗ ∗ School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213, USA {afm,nasmith,epxing}@cs.cmu.edu M´ario A. T. Figueiredo† † Instituto de Telecomunicac¸o˜ es Instituto Superior T´ecnico Lisboa, Portugal mtf@lx.it.pt Pedro M. Q. Aguiar‡ ‡ Instituto de Sistemas e Rob´otica Instituto Superior T´ecnico Lisboa, Portugal aguiar@isr.ist.utl.pt Abstract We present a unified view of two state-of-theart non-projective dependency parsers, both approximate: the loopy belief propagation parser of Smith and Eisner (2008) and the relaxed linear program of Martins et al. (2009). By representing the model assumptions with a factor graph, we shed light on the optimization problems tackled in each method. We also propose a new aggressive online algorithm to learn the model parameters, which makes use of the underlying variational representation. The algorithm does not require a learning rate parameter and provides a single framework for a wide family of convex loss functions, including CRFs and structured SVMs. Experiments show state-of-the-art performance for 14 languages. 1 Introduction Feature-rich discriminati"
D10-1004,D07-1014,1,0.820051,"Missing"
D10-1004,W08-2121,0,0.0715542,"Missing"
D10-1004,W03-3023,0,0.164051,"y the tion corresponds to one output y ˆ t but not in yt , or vice-versa, parts that are active in y will have features that might receive a nonzero update. Those parts are reexamined for new features and the active set F is updated accordingly. 6 Experiments We trained non-projective dependency parsers for 14 languages, using datasets from the CoNLL-X shared task (Buchholz and Marsi, 2006) and two datasets for English: one from the CoNLL-2008 shared task (Surdeanu et al., 2008), which contains non-projective arcs, and another derived from the Penn Treebank applying the standard head rules of Yamada and Matsumoto (2003), in which all parse trees are projective.12 We implemented Alg. 1, 12 We used the provided train/test splits for all datasets. For English, we used the standard test partitions (section 23 of the Wall Street Journal). We did not exploit the fact that some datasets only contain projective trees and have unique roots. 42 which handles any loss function Lβ,γ .13 When β &lt; ∞, Turbo Parser #1 and the loopy BP algorithm of Smith and Eisner (2008) is used; otherwise, Turbo Parser #2 is used and the LP relaxation is solved with CPLEX. In both cases, we employed the same pruning strategy as Martins et"
D10-1004,C98-1102,0,\N,Missing
D11-1022,P11-1048,0,0.0382728,"demands some form of approximate decoding, such as sampling, heuristic search, or variational inference. In this paper, we focus on parsers built from linear programming relaxations, the so-called “turbo parsers” (Martins et al., 2009a; Martins et al., 2010). Rush et al. (2010) applied dual decomposition as a way of combining models which alone permit efficient decoding, but whose combination is intractable. This results in a relaxation of the original problem that is elegantly solved with the subgradient algorithm. While this technique has proven quite effective in parsing (Koo et al., 2010; Auli and Lopez, 2011) as well as machine translation (Rush and Collins, 2011), we show here that its 238 success is strongly tied to the ability of finding a “good” decomposition, i.e., one involving few overlapping components (or slaves). With many components, the subgradient algorithm exhibits extremely slow convergence (cf. Fig. 2). Unfortunately, a lightweight decomposition is not always at hand, either because the problem does not factor in a natural way, or because one would like to incorporate features that cannot be easily absorbed in few tractable components. Examples include features generated by stateme"
D11-1022,P11-1049,0,0.0365357,"Missing"
D11-1022,W06-2920,0,0.0135036,"e are now endowed with a procedure for many other cases, such that AND - WITH - OUTPUT and formulas with universal quantifiers (e.g., R(x) := ∀y : Q(x, y)). Up to a log-factor, the runtimes will be linear in the number of predicates. and Bertsekas (1992) show that the ADMM algorithm may still converge when the z-updates are inexact. Hence the method may still work if the slaves are solved numerically up to some accuracy. We defer this to future investigation. 5 Experiments: Dependency Parsing We used 14 datasets with non-projective dependencies from the CoNLL-2006 and CoNLL-2008 shared tasks (Buchholz and Marsi, 2006; Surdeanu et al., 2008). We also used a projective English dataset derived from the Penn Treebank by applying the standard head rules of Yamada and Matsumoto (2003).11 We did not force the parser to output projective trees or unique roots for any of the datasets; everything is learned from the data. We trained by running 10 iterations of the cost-augmented MIRA algorithm (Crammer et al., 2006) with LP-relaxed decoding, as in Martins et al. (2009b). Following common practice (Charniak and Johnson, 2005; Carreras et al., 2008), we employed a coarse-to-fine procedure to prune away unlikely candi"
D11-1022,W08-2102,0,0.019225,"tive dependencies from the CoNLL-2006 and CoNLL-2008 shared tasks (Buchholz and Marsi, 2006; Surdeanu et al., 2008). We also used a projective English dataset derived from the Penn Treebank by applying the standard head rules of Yamada and Matsumoto (2003).11 We did not force the parser to output projective trees or unique roots for any of the datasets; everything is learned from the data. We trained by running 10 iterations of the cost-augmented MIRA algorithm (Crammer et al., 2006) with LP-relaxed decoding, as in Martins et al. (2009b). Following common practice (Charniak and Johnson, 2005; Carreras et al., 2008), we employed a coarse-to-fine procedure to prune away unlikely candidate arcs, as described by Koo and Collins (2010). To ensure valid parse trees at test time, we rounded fractional Larger Slaves. The only disadvantage of DDADMM in comparison with the subgradient algorithm is that there is not an obvious way of solving the subproblem in Eq. 14 exactly for large combinatorial factors, such as the TREE constraint in dependency parsing, or a sequence model. Hence, our method seems to be more suitable for decompositions which involve “simple slaves,” even if their 11 As usual, we train on sectio"
D11-1022,D07-1101,0,0.0925167,"see Fig. 1); in phrase-based parsing, it can be the set of possible spans; in sequence labeling, it can be the set of possible labels at each position. Our only assumption is that we can “read out” y from the basic parts it contains. For convenience, we represent y as a binary vector, y = hy(r)ir∈R , where y(r) = 1 if part r belongs to y, and 0 otherwise. Figure 1: Parts used by our parser. Arcs are the basic parts: any dependency tree can be “read out” from the arcs it contains. Consecutive siblings and grandparent parts introduce horizontal and vertical Markovization (McDonald et al., 2006; Carreras, 2007). We break the horizontal Markov assumption via all siblings parts and the vertical one through parts which indicate a directed path between two words. Inspired by transition-based parsers, we also adopt head bigram parts, which look at the heads attached to consecutive words. Finally, we follow Martins et al. (2009a) and have parts which indicate if an arc is non-projective (i.e., if it spans words that do not descend from its head). Each Ys is associated with its own set of parts Rs , in the same sense as above; we represent the elements of Ys as binary vectors zs = hzs (r)ir∈Rs . Examples a"
D11-1022,P05-1022,0,0.0602341,"14 datasets with non-projective dependencies from the CoNLL-2006 and CoNLL-2008 shared tasks (Buchholz and Marsi, 2006; Surdeanu et al., 2008). We also used a projective English dataset derived from the Penn Treebank by applying the standard head rules of Yamada and Matsumoto (2003).11 We did not force the parser to output projective trees or unique roots for any of the datasets; everything is learned from the data. We trained by running 10 iterations of the cost-augmented MIRA algorithm (Crammer et al., 2006) with LP-relaxed decoding, as in Martins et al. (2009b). Following common practice (Charniak and Johnson, 2005; Carreras et al., 2008), we employed a coarse-to-fine procedure to prune away unlikely candidate arcs, as described by Koo and Collins (2010). To ensure valid parse trees at test time, we rounded fractional Larger Slaves. The only disadvantage of DDADMM in comparison with the subgradient algorithm is that there is not an obvious way of solving the subproblem in Eq. 14 exactly for large combinatorial factors, such as the TREE constraint in dependency parsing, or a sequence model. Hence, our method seems to be more suitable for decompositions which involve “simple slaves,” even if their 11 As u"
D11-1022,P08-1109,0,0.0101225,"ce of rich features or logical constraints), the original subgradient algorithm is inefficient. We sidestep that difficulty by adopting an augmented Lagrangian method that accelerates model consensus by regularizing towards the averaged votes. We show how first-order logical constraints can be handled efficiently, even though the corresponding subproblems are no longer combinatorial, and report experiments in dependency parsing, with state-of-the-art results. 1 Introduction The last years have witnessed increasingly accurate models for syntax, semantics, and machine translation (Chiang, 2007; Finkel et al., 2008; Petrov and Klein, 2008; Smith and Eisner, 2008; Martins et al., 2009a; Johansson and Nugues, 2008; Koo et al., 2010). The predictive power of such models stems from their ability to break locality assumptions. The resulting combinatorial explosion typically demands some form of approximate decoding, such as sampling, heuristic search, or variational inference. In this paper, we focus on parsers built from linear programming relaxations, the so-called “turbo parsers” (Martins et al., 2009a; Martins et al., 2010). Rush et al. (2010) applied dual decomposition as a way of combining models which"
D11-1022,gimenez-marquez-2004-svmtool,0,0.0165788,"Missing"
D11-1022,P10-1110,0,0.0100494,"1 we recover known methods: • Resorting to the tree and consecutive sibling formulae gives one of the models in Koo et al. (2010), with the same linear relaxation (a proof of this fact is included in App. F); • Resorting to tree, all siblings, grandparent, and non-projective arcs, recovers a multi-commodity flow configuration proposed by Martins et al. (2009a); the relaxation is also the same.12 The experimental results are shown in Tab. 2. For comparison, we include the best published results for each dataset (at the best of our knowledge), among transition-based parsers (Nivre et al., 2006; Huang and Sagae, 2010), graph-based parsers (McDonald et al., 2006; Koo and Collins, 2010), hybrid methods (Nivre and McDonald, 2008; Martins et al., 2008), and turbo parsers (Martins et al., 2010; Koo et al., 2010). Our full model achieved the best reported scores for 7 datasets. The last two columns show a consistent improvement (with the exceptions of Chinese and Arabic) when using the full set of features over a second order model with grandparent and consecutive siblings, which is our reproduction of the model of Koo et al. (2010).13 12 Although Martins et al. (2009a) also incorporated consecutive siblings in"
D11-1022,D08-1008,0,0.0139122,"nt. We sidestep that difficulty by adopting an augmented Lagrangian method that accelerates model consensus by regularizing towards the averaged votes. We show how first-order logical constraints can be handled efficiently, even though the corresponding subproblems are no longer combinatorial, and report experiments in dependency parsing, with state-of-the-art results. 1 Introduction The last years have witnessed increasingly accurate models for syntax, semantics, and machine translation (Chiang, 2007; Finkel et al., 2008; Petrov and Klein, 2008; Smith and Eisner, 2008; Martins et al., 2009a; Johansson and Nugues, 2008; Koo et al., 2010). The predictive power of such models stems from their ability to break locality assumptions. The resulting combinatorial explosion typically demands some form of approximate decoding, such as sampling, heuristic search, or variational inference. In this paper, we focus on parsers built from linear programming relaxations, the so-called “turbo parsers” (Martins et al., 2009a; Martins et al., 2010). Rush et al. (2010) applied dual decomposition as a way of combining models which alone permit efficient decoding, but whose combination is intractable. This results in a relaxatio"
D11-1022,P10-1001,0,0.486395,"e also used a projective English dataset derived from the Penn Treebank by applying the standard head rules of Yamada and Matsumoto (2003).11 We did not force the parser to output projective trees or unique roots for any of the datasets; everything is learned from the data. We trained by running 10 iterations of the cost-augmented MIRA algorithm (Crammer et al., 2006) with LP-relaxed decoding, as in Martins et al. (2009b). Following common practice (Charniak and Johnson, 2005; Carreras et al., 2008), we employed a coarse-to-fine procedure to prune away unlikely candidate arcs, as described by Koo and Collins (2010). To ensure valid parse trees at test time, we rounded fractional Larger Slaves. The only disadvantage of DDADMM in comparison with the subgradient algorithm is that there is not an obvious way of solving the subproblem in Eq. 14 exactly for large combinatorial factors, such as the TREE constraint in dependency parsing, or a sequence model. Hence, our method seems to be more suitable for decompositions which involve “simple slaves,” even if their 11 As usual, we train on sections §02–21, use §22 as validation number is large. However, this does not rule out the data, and test on §23. We ran SV"
D11-1022,D10-1125,0,0.143941,"lty by adopting an augmented Lagrangian method that accelerates model consensus by regularizing towards the averaged votes. We show how first-order logical constraints can be handled efficiently, even though the corresponding subproblems are no longer combinatorial, and report experiments in dependency parsing, with state-of-the-art results. 1 Introduction The last years have witnessed increasingly accurate models for syntax, semantics, and machine translation (Chiang, 2007; Finkel et al., 2008; Petrov and Klein, 2008; Smith and Eisner, 2008; Martins et al., 2009a; Johansson and Nugues, 2008; Koo et al., 2010). The predictive power of such models stems from their ability to break locality assumptions. The resulting combinatorial explosion typically demands some form of approximate decoding, such as sampling, heuristic search, or variational inference. In this paper, we focus on parsers built from linear programming relaxations, the so-called “turbo parsers” (Martins et al., 2009a; Martins et al., 2010). Rush et al. (2010) applied dual decomposition as a way of combining models which alone permit efficient decoding, but whose combination is intractable. This results in a relaxation of the original p"
D11-1022,P11-1060,0,0.00812487,"ence properties of DD-ADMM, complementing the theoretical treatment in Martins et al. (2011). DD-ADMM compares favourably against the subgradient method in several aspects: it is faster to reach a consensus, it has better stopping conditions, and it works better in non-lightweight decompositions. While its slave subproblems are more involved, we derived closedform solutions for many cases of interest, such as first-order logic formulas and combinatorial factors. DD-ADMM may be useful in other frameworks involving logical constraints, such as the models for compositional semantics presented by Liang et al. (2011). Non-logical constraints may also yield efficient subproblems, e.g., the length constraints in summarization and compression (Clarke and Lapata, 2008; Martins and Smith, 2009; BergKirkpatrick et al., 2011). Finally, DD-ADMM can be adapted to tighten its relaxations towards exact decoding, as in Sontag et al. (2008) and Rush and Collins (2011). We defer this for future work. Acknowledgments We thank all reviewers for their comments, Eric Xing for helpful discussions, and Terry Koo and Sasha Rush for answering questions about their parser and for providing code. A. M. was supported by a FCT/ICT"
D11-1022,W09-1801,1,0.81558,": it is faster to reach a consensus, it has better stopping conditions, and it works better in non-lightweight decompositions. While its slave subproblems are more involved, we derived closedform solutions for many cases of interest, such as first-order logic formulas and combinatorial factors. DD-ADMM may be useful in other frameworks involving logical constraints, such as the models for compositional semantics presented by Liang et al. (2011). Non-logical constraints may also yield efficient subproblems, e.g., the length constraints in summarization and compression (Clarke and Lapata, 2008; Martins and Smith, 2009; BergKirkpatrick et al., 2011). Finally, DD-ADMM can be adapted to tighten its relaxations towards exact decoding, as in Sontag et al. (2008) and Rush and Collins (2011). We defer this for future work. Acknowledgments We thank all reviewers for their comments, Eric Xing for helpful discussions, and Terry Koo and Sasha Rush for answering questions about their parser and for providing code. A. M. was supported by a FCT/ICTI grant through the CMU-Portugal Program, and by Priberam. This work was partially supported by the FET programme (EU FP7), under the SIMBAD project (contract 213250). N. S. w"
D11-1022,D08-1017,1,0.847301,"h the same linear relaxation (a proof of this fact is included in App. F); • Resorting to tree, all siblings, grandparent, and non-projective arcs, recovers a multi-commodity flow configuration proposed by Martins et al. (2009a); the relaxation is also the same.12 The experimental results are shown in Tab. 2. For comparison, we include the best published results for each dataset (at the best of our knowledge), among transition-based parsers (Nivre et al., 2006; Huang and Sagae, 2010), graph-based parsers (McDonald et al., 2006; Koo and Collins, 2010), hybrid methods (Nivre and McDonald, 2008; Martins et al., 2008), and turbo parsers (Martins et al., 2010; Koo et al., 2010). Our full model achieved the best reported scores for 7 datasets. The last two columns show a consistent improvement (with the exceptions of Chinese and Arabic) when using the full set of features over a second order model with grandparent and consecutive siblings, which is our reproduction of the model of Koo et al. (2010).13 12 Although Martins et al. (2009a) also incorporated consecutive siblings in one of their configurations, our constraints are tighter than theirs. See App. F. 13 Note however that the actual results of Koo et a"
D11-1022,P09-1039,1,0.362218,"algorithm is inefficient. We sidestep that difficulty by adopting an augmented Lagrangian method that accelerates model consensus by regularizing towards the averaged votes. We show how first-order logical constraints can be handled efficiently, even though the corresponding subproblems are no longer combinatorial, and report experiments in dependency parsing, with state-of-the-art results. 1 Introduction The last years have witnessed increasingly accurate models for syntax, semantics, and machine translation (Chiang, 2007; Finkel et al., 2008; Petrov and Klein, 2008; Smith and Eisner, 2008; Martins et al., 2009a; Johansson and Nugues, 2008; Koo et al., 2010). The predictive power of such models stems from their ability to break locality assumptions. The resulting combinatorial explosion typically demands some form of approximate decoding, such as sampling, heuristic search, or variational inference. In this paper, we focus on parsers built from linear programming relaxations, the so-called “turbo parsers” (Martins et al., 2009a; Martins et al., 2010). Rush et al. (2010) applied dual decomposition as a way of combining models which alone permit efficient decoding, but whose combination is intractable"
D11-1022,D10-1004,1,0.107347,"ingly accurate models for syntax, semantics, and machine translation (Chiang, 2007; Finkel et al., 2008; Petrov and Klein, 2008; Smith and Eisner, 2008; Martins et al., 2009a; Johansson and Nugues, 2008; Koo et al., 2010). The predictive power of such models stems from their ability to break locality assumptions. The resulting combinatorial explosion typically demands some form of approximate decoding, such as sampling, heuristic search, or variational inference. In this paper, we focus on parsers built from linear programming relaxations, the so-called “turbo parsers” (Martins et al., 2009a; Martins et al., 2010). Rush et al. (2010) applied dual decomposition as a way of combining models which alone permit efficient decoding, but whose combination is intractable. This results in a relaxation of the original problem that is elegantly solved with the subgradient algorithm. While this technique has proven quite effective in parsing (Koo et al., 2010; Auli and Lopez, 2011) as well as machine translation (Rush and Collins, 2011), we show here that its 238 success is strongly tied to the ability of finding a “good” decomposition, i.e., one involving few overlapping components (or slaves). With many componen"
D11-1022,W06-2932,0,0.389702,"sible dependency arcs (see Fig. 1); in phrase-based parsing, it can be the set of possible spans; in sequence labeling, it can be the set of possible labels at each position. Our only assumption is that we can “read out” y from the basic parts it contains. For convenience, we represent y as a binary vector, y = hy(r)ir∈R , where y(r) = 1 if part r belongs to y, and 0 otherwise. Figure 1: Parts used by our parser. Arcs are the basic parts: any dependency tree can be “read out” from the arcs it contains. Consecutive siblings and grandparent parts introduce horizontal and vertical Markovization (McDonald et al., 2006; Carreras, 2007). We break the horizontal Markov assumption via all siblings parts and the vertical one through parts which indicate a directed path between two words. Inspired by transition-based parsers, we also adopt head bigram parts, which look at the heads attached to consecutive words. Finally, we follow Martins et al. (2009a) and have parts which indicate if an arc is non-projective (i.e., if it spans words that do not descend from its head). Each Ys is associated with its own set of parts Rs , in the same sense as above; we represent the elements of Ys as binary vectors zs = hzs (r)i"
D11-1022,P08-1108,0,0.0219951,"in Koo et al. (2010), with the same linear relaxation (a proof of this fact is included in App. F); • Resorting to tree, all siblings, grandparent, and non-projective arcs, recovers a multi-commodity flow configuration proposed by Martins et al. (2009a); the relaxation is also the same.12 The experimental results are shown in Tab. 2. For comparison, we include the best published results for each dataset (at the best of our knowledge), among transition-based parsers (Nivre et al., 2006; Huang and Sagae, 2010), graph-based parsers (McDonald et al., 2006; Koo and Collins, 2010), hybrid methods (Nivre and McDonald, 2008; Martins et al., 2008), and turbo parsers (Martins et al., 2010; Koo et al., 2010). Our full model achieved the best reported scores for 7 datasets. The last two columns show a consistent improvement (with the exceptions of Chinese and Arabic) when using the full set of features over a second order model with grandparent and consecutive siblings, which is our reproduction of the model of Koo et al. (2010).13 12 Although Martins et al. (2009a) also incorporated consecutive siblings in one of their configurations, our constraints are tighter than theirs. See App. F. 13 Note however that the act"
D11-1022,W06-2933,0,0.105724,"Missing"
D11-1022,D08-1091,0,0.0183398,"r logical constraints), the original subgradient algorithm is inefficient. We sidestep that difficulty by adopting an augmented Lagrangian method that accelerates model consensus by regularizing towards the averaged votes. We show how first-order logical constraints can be handled efficiently, even though the corresponding subproblems are no longer combinatorial, and report experiments in dependency parsing, with state-of-the-art results. 1 Introduction The last years have witnessed increasingly accurate models for syntax, semantics, and machine translation (Chiang, 2007; Finkel et al., 2008; Petrov and Klein, 2008; Smith and Eisner, 2008; Martins et al., 2009a; Johansson and Nugues, 2008; Koo et al., 2010). The predictive power of such models stems from their ability to break locality assumptions. The resulting combinatorial explosion typically demands some form of approximate decoding, such as sampling, heuristic search, or variational inference. In this paper, we focus on parsers built from linear programming relaxations, the so-called “turbo parsers” (Martins et al., 2009a; Martins et al., 2010). Rush et al. (2010) applied dual decomposition as a way of combining models which alone permit efficient"
D11-1022,W06-1616,0,0.369967,"Missing"
D11-1022,P11-1008,0,0.282099,"pling, heuristic search, or variational inference. In this paper, we focus on parsers built from linear programming relaxations, the so-called “turbo parsers” (Martins et al., 2009a; Martins et al., 2010). Rush et al. (2010) applied dual decomposition as a way of combining models which alone permit efficient decoding, but whose combination is intractable. This results in a relaxation of the original problem that is elegantly solved with the subgradient algorithm. While this technique has proven quite effective in parsing (Koo et al., 2010; Auli and Lopez, 2011) as well as machine translation (Rush and Collins, 2011), we show here that its 238 success is strongly tied to the ability of finding a “good” decomposition, i.e., one involving few overlapping components (or slaves). With many components, the subgradient algorithm exhibits extremely slow convergence (cf. Fig. 2). Unfortunately, a lightweight decomposition is not always at hand, either because the problem does not factor in a natural way, or because one would like to incorporate features that cannot be easily absorbed in few tractable components. Examples include features generated by statements in first-order logic, features that violate Markov a"
D11-1022,D10-1001,0,0.521096,"or syntax, semantics, and machine translation (Chiang, 2007; Finkel et al., 2008; Petrov and Klein, 2008; Smith and Eisner, 2008; Martins et al., 2009a; Johansson and Nugues, 2008; Koo et al., 2010). The predictive power of such models stems from their ability to break locality assumptions. The resulting combinatorial explosion typically demands some form of approximate decoding, such as sampling, heuristic search, or variational inference. In this paper, we focus on parsers built from linear programming relaxations, the so-called “turbo parsers” (Martins et al., 2009a; Martins et al., 2010). Rush et al. (2010) applied dual decomposition as a way of combining models which alone permit efficient decoding, but whose combination is intractable. This results in a relaxation of the original problem that is elegantly solved with the subgradient algorithm. While this technique has proven quite effective in parsing (Koo et al., 2010; Auli and Lopez, 2011) as well as machine translation (Rush and Collins, 2011), we show here that its 238 success is strongly tied to the ability of finding a “good” decomposition, i.e., one involving few overlapping components (or slaves). With many components, the subgradient"
D11-1022,D08-1016,0,0.539453,"the original subgradient algorithm is inefficient. We sidestep that difficulty by adopting an augmented Lagrangian method that accelerates model consensus by regularizing towards the averaged votes. We show how first-order logical constraints can be handled efficiently, even though the corresponding subproblems are no longer combinatorial, and report experiments in dependency parsing, with state-of-the-art results. 1 Introduction The last years have witnessed increasingly accurate models for syntax, semantics, and machine translation (Chiang, 2007; Finkel et al., 2008; Petrov and Klein, 2008; Smith and Eisner, 2008; Martins et al., 2009a; Johansson and Nugues, 2008; Koo et al., 2010). The predictive power of such models stems from their ability to break locality assumptions. The resulting combinatorial explosion typically demands some form of approximate decoding, such as sampling, heuristic search, or variational inference. In this paper, we focus on parsers built from linear programming relaxations, the so-called “turbo parsers” (Martins et al., 2009a; Martins et al., 2010). Rush et al. (2010) applied dual decomposition as a way of combining models which alone permit efficient decoding, but whose comb"
D11-1022,W08-2121,0,0.129424,"Missing"
D11-1022,N06-1054,0,0.0733185,"Computational Linguistics (1) Designing the model must obey certain practical considerations. If efficiency is the major concern, a simple model is usually chosen so that Eq. 1 can be solved efficiently, at the cost of limited expressive power. If we care more about accuracy, a model with richer features and more involved score functions may be designed. Decoding, however, will be more expensive, and approximations are often necessary. A typical source of intractability comes from the combinatorial explosion inherent in the composition of two or more tractable models (Bar-Hillel et al., 1964; Tromble and Eisner, 2006). Recently, Rush et al. (2010) have proposed a dual decomposition framework to address NLP problems in which the global score decomposes as f (y) = f1 (z1 )+f2 (z2 ), where z1 and z2 are two overlapping “views” of the output, so that Eq. 1 becomes: maximize w.r.t. s.t. f1 (z1 ) + f2 (z2 ) z1 ∈ Y1 , z2 ∈ Y2 z1 ∼ z2 . (2) Above, the notation z1 ∼ z2 means that z1 and z2 “agree on their overlaps,” and an isomorphism Y &apos; {hz1 , z2 i ∈ Y1 × Y2 |z1 ∼ z2 } is assumed. We next formalize these notions and proceed to compositions of an arbitrary number of models. Of special interest is the unexplored se"
D11-1022,W03-3023,0,0.0260678,"a log-factor, the runtimes will be linear in the number of predicates. and Bertsekas (1992) show that the ADMM algorithm may still converge when the z-updates are inexact. Hence the method may still work if the slaves are solved numerically up to some accuracy. We defer this to future investigation. 5 Experiments: Dependency Parsing We used 14 datasets with non-projective dependencies from the CoNLL-2006 and CoNLL-2008 shared tasks (Buchholz and Marsi, 2006; Surdeanu et al., 2008). We also used a projective English dataset derived from the Penn Treebank by applying the standard head rules of Yamada and Matsumoto (2003).11 We did not force the parser to output projective trees or unique roots for any of the datasets; everything is learned from the data. We trained by running 10 iterations of the cost-augmented MIRA algorithm (Crammer et al., 2006) with LP-relaxed decoding, as in Martins et al. (2009b). Following common practice (Charniak and Johnson, 2005; Carreras et al., 2008), we employed a coarse-to-fine procedure to prune away unlikely candidate arcs, as described by Koo and Collins (2010). To ensure valid parse trees at test time, we rounded fractional Larger Slaves. The only disadvantage of DDADMM in"
D11-1022,J07-2003,0,\N,Missing
D11-1139,W06-2920,0,0.015104,"Missing"
D11-1139,W02-1001,0,0.649601,"king, entity recognition, and dependency parsing) show gains in performance, compactness, and model interpretability. 1 Introduction Models for structured outputs are in demand across natural language processing, with applications in information extraction, parsing, and machine translation. State-of-the-art models usually involve linear combinations of features and are trained discriminatively; examples are conditional random fields (Lafferty et al., 2001), structured support vector machines (Altun et al., 2003; Taskar et al., 2003; Tsochantaridis et al., 2004), and the structured perceptron (Collins, 2002a). In all these cases, the underlying optimization problems differ only in the choice of loss function; choosing among them has usually a small impact on predictive performance. In this paper, we are concerned with model selection: which features should be used to define the prediction score? The fact that models with few features (“sparse” models) are desirable for several 1500 reasons (compactness, interpretability, good generalization) has stimulated much research work which has produced a wide variety of methods (Della Pietra et al., 1997; Guyon and Elisseeff, 2003; McCallum, 2003). Our f"
D11-1139,P11-1137,1,0.784,"eatures individually), or to make the inclusion of some features depend on the inclusion of other features. We achieve the goal stated above by employing regularizers which promote structured sparsity. Such regularizers are able to encode prior knowledge and guide the selection of features by modeling the structure of the feature space. Lately, this type of regularizers has received a lot of attention in computer vision, signal processing, and computational biology (Zhao et al., 2009; Kim and Xing, 2010; Jenatton et al., 2009; Obozinski et al., 2010; Jenatton et al., 2010; Bach et al., 2011). Eisenstein et al. (2011) employed structured sparsity in computational sociolinguistics. However, none of these works have addressed structured prediction. Here, we combine these two levels of structure: structure in the output space, and structure in the feature space. The result is a framework that allows building structured predictors with high predictive power, while reducing manual feature engineering. We obtain models that are interpretable, accurate, and often much more compact than L2 -regularized ones. Compared with L1 -regularized models, ours are often more accurate and yield faster runtime. Proceedings of"
D11-1139,P07-1104,0,0.0101358,"sparsity. While this has been a topic of intense research in signal processing and 1501 λ 2 2 kθk2 = λ 2 PD 2 d=1 θd ; (6) • L1 -regularization (Kazama and Tsujii, 2003; Goodman, 2004): 1 ΩL τ (θ) , τ kθk1 = τ PD d=1 |θd |. (7) The latter is known as “Lasso,” as popularized by Tibshirani (1996) in the context of sparse regression. In the two cases above, λ and τ are nonnegative coefficients controlling the intensity of the reg2 ularization. ΩL λ usually leads to easier optimization 1 and robust performance; ΩL τ encourages sparser models, where only a few features receive nonzero weights; see Gao et al. (2007) for an empirical comparison. More recently, Petrov and Klein (2008b) applied L1 regularization for structure learning in phrase-based parsing; a comparison with L2 appears in Petrov and Klein (2008a). Elastic nets interpolate between L1 and L2 , having been proposed by Zou and Hastie (2005) and used by Lavergne et al. (2010) to regularize CRFs. Neither of the regularizers just described “looks” at the structure of the feature space, since they all treat each dimension independently—we call them unstructured regularizers, as opposed to the structured ones that we next describe. 3 Structured Sp"
D11-1139,N04-1039,0,0.490061,"Missing"
D11-1139,W03-1018,0,0.365199,"Missing"
D11-1139,P10-1052,0,0.0483335,"Missing"
D11-1139,D10-1004,1,0.232465,"e output is y; our goal is to learn θ with small expected cost on unseen data. To achieve this goal, linear models are usually trained by solving a problem of the form b = arg minθ Ω(θ) + 1 PN L(θ, xi , yi ), (2) θ i=1 N LCRF (θ, x, y) = − log Pθ (y|x), computational biology (Jenatton et al., 2009; Liu and Ye, 2010; Bach et al., 2011), it has not yet received much attention in the NLP community, where the choice of regularization for supervised learning has essentially been limited to the following: (5) Empirical comparison among these loss functions can be found in the literature (see, e.g., Martins et al., 2010, who also consider interpolations of the losses above). In practice, it has been observed that the choice of loss has far less impact than the model design and choice of features. Hence, in this paper, we focus our attention on the regularization term in Eq. 2. We specifically address ways in which this term can be used to help design the model by promoting structured sparsity. While this has been a topic of intense research in signal processing and 1501 λ 2 2 kθk2 = λ 2 PD 2 d=1 θd ; (6) • L1 -regularization (Kazama and Tsujii, 2003; Goodman, 2004): 1 ΩL τ (θ) , τ kθk1 = τ PD d=1 |θd |. (7)"
D11-1139,H05-1066,0,0.0347046,"on-overlapping group-Lasso, coarse-to-fine group-Lasso (C2F), and a filter-based method based on information gain for selecting feature templates in multilingual dependency parsing. The x-axis is the total number of features at different regularization levels, and the y-axis is the unlabeled attachment score. The plots illustrate how accurate the parsers are as a function of the model sparsity achieved, for each method. The standard Lasso (which does not select templates, but individual features) is also shown for comparison. We use arc-factored models, for which exact inference is tractable (McDonald et al., 2005). We defined M = 684 feature templates for each candidate arc by conjoining the words, shapes, lemmas, and POS of the head and the modifier, as well as the contextual POS, and the distance and direction of attachment. We followed the same two-stage approach as before, and compared with a baseline which selects feature templates by ranking them according to the information gain criterion. This baseline assigns a score to each template Tm which reflects an empirical estimate of the mutual information between Tm and the binary variable A that indicates the presence/absence of a dependency link: I"
D11-1139,D08-1091,0,0.0249143,"ignal processing and 1501 λ 2 2 kθk2 = λ 2 PD 2 d=1 θd ; (6) • L1 -regularization (Kazama and Tsujii, 2003; Goodman, 2004): 1 ΩL τ (θ) , τ kθk1 = τ PD d=1 |θd |. (7) The latter is known as “Lasso,” as popularized by Tibshirani (1996) in the context of sparse regression. In the two cases above, λ and τ are nonnegative coefficients controlling the intensity of the reg2 ularization. ΩL λ usually leads to easier optimization 1 and robust performance; ΩL τ encourages sparser models, where only a few features receive nonzero weights; see Gao et al. (2007) for an empirical comparison. More recently, Petrov and Klein (2008b) applied L1 regularization for structure learning in phrase-based parsing; a comparison with L2 appears in Petrov and Klein (2008a). Elastic nets interpolate between L1 and L2 , having been proposed by Zou and Hastie (2005) and used by Lavergne et al. (2010) to regularize CRFs. Neither of the regularizers just described “looks” at the structure of the feature space, since they all treat each dimension independently—we call them unstructured regularizers, as opposed to the structured ones that we next describe. 3 Structured Sparsity 1 We are interested in regularizers that share with ΩL τ the"
D11-1139,W00-0726,0,0.12932,"regularization and using the loss L which one wants to optimize. 7 To see why this is the case, note that both gradient and proximal updates come scaled by η0 ; and that the gradient of the loss is ∇LSP (θ, xt , yt ) = φ(xt , ybt ) − φ(xt , yt ), where ybt is the prediction under the current model, which is insensitive to the scaling of θ. This independence on η0 does not hold when the loss is LSVM or LCRF . 1506 5 Experiments We present experiments in three structured prediction tasks for several group choices. Text Chunking. We use the English dataset provided in the CoNLL 2000 shared task (Sang and Buchholz, 2000), which consists of 8,936 training and 2,012 testing sentences (sections 15–18 and 20 of the WSJ.) The input observations are the token words and their POS tags; we want to predict the sequences of IOB tags representing phrase chunks. We built 96 contextual feature templates as follows: • Up to 5-grams of POS tags, in windows of 5 tokens on the left and 5 tokens on the right; • Up to 3-grams of words, in windows of 3 tokens on the left and 3 tokens on the right; • Up to 2-grams of word shapes, in windows of 2 tokens on the left and 2 tokens on the right. Each shape replaces characters by their"
D11-1139,W03-0419,0,0.111532,"Missing"
D11-1139,W02-2024,0,0.0385244,"Missing"
D11-1139,P09-1054,0,0.0344113,"= kθ m k2 −dm d kθ m k2 θ m otherwise. (14) which can be seen as a generalization of Eq. 13: if the L2 -norm of the m-th group is less than dm , the entire group is discarded; otherwise it is scaled so that its L2 -norm decreases by an amount of dm . When groups overlap, the proximity operator lacks a closed form. When G is tree-structured, it can still be efficiently computed by a recursive procedure (Jenatton et al., 2010). When G is not treestructured, no specialized procedure is known, and a convex optimizer is necessary to solve Eq. 12. 4 Online Prox-Grad Algorithm Langford et al., 2009; Tsuruoka et al., 2009). The algorithm that we use in this paper (Alg. 1) extends the stochastic gradient methods for group-Lasso regularization; a similar algorithm was used by Martins et al. (2011) for multiple kernel learning. Alg. 1 addresses the learning problem in Eq. 2 by alternating between online (sub-)gradient steps with respect to the loss term, and proximal steps with respect to the regularizer. Proximal-gradient methods are very popular in sparse modeling, both in batch (Liu and Ye, 2010; Bach et al., 2011) and online (Duchi and Singer, 2009; Xiao, 2009) settings. The reason we have chosen the algorithm"
D11-1139,P02-1062,0,\N,Missing
D16-1028,D13-1059,0,0.0334481,"Missing"
D16-1028,N10-1083,0,0.0606133,"Missing"
D16-1028,J92-4003,0,0.620219,"Missing"
D16-1028,P14-1099,1,0.91772,"rvised learning algorithms for the structured problems found in NLP are computationally expensive, requiring multiple decoding passes through the unlabeled data, or expensive similarity graphs. More scalable learning algorithms are in demand. In this paper, we propose a moment-matching method for semi-supervised learning of sequence models. Spectral learning and moment-matching approaches have recently proved a viable alternative to expectation-maximization (EM) for unsupervised learning (Hsu et al., 2012; Balle and Mohri, 2012; Bailly et al., 2013), supervised learning with latent variables (Cohen and Collins, 2014; Quattoni et al., 2014; Stratos et al., 2013) and topic modeling (Arora et al., 2013; Nguyen et al., 2015). These methods have learnability guarantees, do not suffer from local optima, and are computationally less demanding. Unlike spectral methods, ours does not require an orthogonal decomposition of any matrix or tensor. Instead, it considers a more restricted form of supervision: words that have unambiguous annotations, so-called anchor words (Arora et al., 2013). Rather than identifying anchor words from unlabeled data (Stratos et al., 2016), we extract them from a small labeled dataset o"
D16-1028,N13-1015,1,0.928849,"w), we can estimate the emission probabilities O by direct application of Bayes rule: Therefore, given the set of anchor words A(h), the bw,h = p(H = h |X = w) × p(X = w) (15) O hth column of R can be estimated in a single pass p(H = h) over the unlabeled data, as follows: Eq. 7 z}|{ P γ bw,c × pbw ψ (z)1(x ∈ A(h)) c =P . (16) bc,h = x,z∈D P U R (12) bw0 ,c × pbw0 w0 γ 1(x ∈ A(h)) x,z∈DU 290 These parameters are guaranteed to lie in the probability simplex, avoiding the need of heuristics for dealing with “negative” and “unnormalized” probabilities required by prior work in spectral learning (Cohen et al., 2013). 3.5 Transition Distributions It remains to estimate the transition matrix T. For the problems tackled in this paper, the number of labels K is small, compared to the vocabulary size V . The transition matrix has only O(K 2 ) degrees of freedom, and we found it effective to estimate it using the labeled sequences in DL alone, without any refinement. This was done by smoothed maximum likelihood estimation on the labeled data, which boils down to counting occurrences of consecutive labels, applying add-one smoothing to avoid zero probabilities for unobserved transitions, and normalizing. For pr"
D16-1028,P13-1057,0,0.224936,"rameters are estimated by solving a small quadratic program for each feature. Experiments on part-of-speech (POS) tagging for Twitter and for a low-resource language (Malagasy) show that our method can learn from very few annotated sentences. 1 Introduction Statistical learning of NLP models is often limited by the scarcity of annotated data. Weakly supervised methods have been proposed as an alternative to laborious manual annotation, combining large amounts of unlabeled data with limited resources, such as tag dictionaries or small annotated datasets (Merialdo, 1994; Smith and Eisner, 2005; Garrette et al., 2013). Unfortunately, most semisupervised learning algorithms for the structured problems found in NLP are computationally expensive, requiring multiple decoding passes through the unlabeled data, or expensive similarity graphs. More scalable learning algorithms are in demand. In this paper, we propose a moment-matching method for semi-supervised learning of sequence models. Spectral learning and moment-matching approaches have recently proved a viable alternative to expectation-maximization (EM) for unsupervised learning (Hsu et al., 2012; Balle and Mohri, 2012; Bailly et al., 2013), supervised le"
D16-1028,P11-2008,1,0.79776,"Missing"
D16-1028,Q15-1016,0,0.0308812,"s the matrix Q ∈ RC×V , defined as: Qc,w := E[ψc (Z) |X = w]. (3) Expectations here are taken with respect to the probabilistic model in Eq. 1 that generates the data. The following quantities will also be necessary: Figure 1: HMM, context (green) conditionally independent of present (red) w` given state h` . and will be formally defined in §3.1. Such cooccurrence matrices are often collected in NLP, for various problems, ranging from dimensionality reduction of documents using latent semantic indexing (Deerwester et al., 1990; Landauer et al., 1998), distributional semantics (Sch¨utze, 1998; Levy et al., 2015) and word embedding generation (Dhillon et al., 2015; Osborne et al., 2016). We can build such a moment matrix entirely from the unlabeled data DU . The same unlabeled data is used to build an estimate of a context-label moment matrix R ∈ RC×K , as explained in §3.3. This is done by first identifying words that are unambiguously associated with each label h, called anchor words, with the aid of a few labeled data; this is outlined in §3.2. Finally, given empirical estimates of Q and R, we estimate the emission matrix O by solving a small optimization problem independently per word (§3.4). The"
D16-1028,P12-3005,0,0.0268494,"ter experiments, we also evaluated a stacked architecture in which we derived features from our model’s predictions to improve a state-ofthe-art POS tagger (MEMM).4 6.1 Twitter POS Tagging For the Twitter experiment, we used the Oct27 dataset of Gimpel et al. (2011), with the provided partitions (1,000 tweets for training and 328 for validation), and tested on the Daily547 dataset (547 tweets). Anchor words were selected from the training partition as described in §5. We used 2.7M unlabeled tweets (O’Connor et al., 2010) to train the semi-supervised methods, filtering the English tweets as in Lui and Baldwin (2012), tokenizing them as in Owoputi et al. (2013), and normalizing at-mentions, URLs, and emoticons. We used as word features φ(X) the word iself, as well as binary features for capitalization, titles, and digits (Berg-Kirkpatrick et al., 2010), the word shape, and the Unicode class of each character. Similarly to Owoputi et al. (2013), we also used suffixes and prefixes (up to length 3), and Twitter4 http://www.ark.cs.cmu.edu/TweetNLP/ 293 0.95 Tagging accuracy (0/1 loss) select the anchors on the validation set, using steps of 0.1 in the unit interval, and making sure that all tags have at least"
D16-1028,J94-2001,0,0.254846,"collect moment statistics. The model parameters are estimated by solving a small quadratic program for each feature. Experiments on part-of-speech (POS) tagging for Twitter and for a low-resource language (Malagasy) show that our method can learn from very few annotated sentences. 1 Introduction Statistical learning of NLP models is often limited by the scarcity of annotated data. Weakly supervised methods have been proposed as an alternative to laborious manual annotation, combining large amounts of unlabeled data with limited resources, such as tag dictionaries or small annotated datasets (Merialdo, 1994; Smith and Eisner, 2005; Garrette et al., 2013). Unfortunately, most semisupervised learning algorithms for the structured problems found in NLP are computationally expensive, requiring multiple decoding passes through the unlabeled data, or expensive similarity graphs. More scalable learning algorithms are in demand. In this paper, we propose a moment-matching method for semi-supervised learning of sequence models. Spectral learning and moment-matching approaches have recently proved a viable alternative to expectation-maximization (EM) for unsupervised learning (Hsu et al., 2012; Balle and"
D16-1028,N15-1076,0,0.0174388,"ultiple decoding passes through the unlabeled data, or expensive similarity graphs. More scalable learning algorithms are in demand. In this paper, we propose a moment-matching method for semi-supervised learning of sequence models. Spectral learning and moment-matching approaches have recently proved a viable alternative to expectation-maximization (EM) for unsupervised learning (Hsu et al., 2012; Balle and Mohri, 2012; Bailly et al., 2013), supervised learning with latent variables (Cohen and Collins, 2014; Quattoni et al., 2014; Stratos et al., 2013) and topic modeling (Arora et al., 2013; Nguyen et al., 2015). These methods have learnability guarantees, do not suffer from local optima, and are computationally less demanding. Unlike spectral methods, ours does not require an orthogonal decomposition of any matrix or tensor. Instead, it considers a more restricted form of supervision: words that have unambiguous annotations, so-called anchor words (Arora et al., 2013). Rather than identifying anchor words from unlabeled data (Stratos et al., 2016), we extract them from a small labeled dataset or from a dictionary. Given the anchor words, the estimation of the model parameters can be made efficient b"
D16-1028,Q16-1030,1,0.819216,"ations here are taken with respect to the probabilistic model in Eq. 1 that generates the data. The following quantities will also be necessary: Figure 1: HMM, context (green) conditionally independent of present (red) w` given state h` . and will be formally defined in §3.1. Such cooccurrence matrices are often collected in NLP, for various problems, ranging from dimensionality reduction of documents using latent semantic indexing (Deerwester et al., 1990; Landauer et al., 1998), distributional semantics (Sch¨utze, 1998; Levy et al., 2015) and word embedding generation (Dhillon et al., 2015; Osborne et al., 2016). We can build such a moment matrix entirely from the unlabeled data DU . The same unlabeled data is used to build an estimate of a context-label moment matrix R ∈ RC×K , as explained in §3.3. This is done by first identifying words that are unambiguously associated with each label h, called anchor words, with the aid of a few labeled data; this is outlined in §3.2. Finally, given empirical estimates of Q and R, we estimate the emission matrix O by solving a small optimization problem independently per word (§3.4). The transition matrix T is obtained directly from the labeled dataset DL by max"
D16-1028,N13-1039,1,0.813851,"Missing"
D16-1028,petrov-etal-2012-universal,0,0.0129586,"mpute a mapping from mean parameters µh 3 As shown by Xiaojin Zhu (1999) and Yasemin Altun to canonical parameters θ h , we use the well-known (2006), this regularization is equivalent, in the dual, to a “soft” Fenchel-Legendre duality between the entropy and constraint kEθh [φ(X) |H = h] − µh k2 ≤ , as opposed to a the log-partition function (Wainwright and Jordan, strict equality. 292 6 Experiments We evaluated our method on two tasks: POS tagging of Twitter text (in English), and POS tagging for a low-resource language (Malagasy). For all the experiments, we used the universal POS tagset (Petrov et al., 2012), which consists of K = 12 tags. We compared our method against supervised baselines (HMM and FHMM), which use the labeled data only, and two semi-supervised baselines that exploit the unlabeled data: self-training and EM. For the Twitter experiments, we also evaluated a stacked architecture in which we derived features from our model’s predictions to improve a state-ofthe-art POS tagger (MEMM).4 6.1 Twitter POS Tagging For the Twitter experiment, we used the Oct27 dataset of Gimpel et al. (2011), with the provided partitions (1,000 tweets for training and 328 for validation), and tested on th"
D16-1028,J98-1004,0,0.524216,"Missing"
D16-1028,P05-1044,1,0.844823,"statistics. The model parameters are estimated by solving a small quadratic program for each feature. Experiments on part-of-speech (POS) tagging for Twitter and for a low-resource language (Malagasy) show that our method can learn from very few annotated sentences. 1 Introduction Statistical learning of NLP models is often limited by the scarcity of annotated data. Weakly supervised methods have been proposed as an alternative to laborious manual annotation, combining large amounts of unlabeled data with limited resources, such as tag dictionaries or small annotated datasets (Merialdo, 1994; Smith and Eisner, 2005; Garrette et al., 2013). Unfortunately, most semisupervised learning algorithms for the structured problems found in NLP are computationally expensive, requiring multiple decoding passes through the unlabeled data, or expensive similarity graphs. More scalable learning algorithms are in demand. In this paper, we propose a moment-matching method for semi-supervised learning of sequence models. Spectral learning and moment-matching approaches have recently proved a viable alternative to expectation-maximization (EM) for unsupervised learning (Hsu et al., 2012; Balle and Mohri, 2012; Bailly et a"
D16-1028,W13-3507,1,0.860689,"oblems found in NLP are computationally expensive, requiring multiple decoding passes through the unlabeled data, or expensive similarity graphs. More scalable learning algorithms are in demand. In this paper, we propose a moment-matching method for semi-supervised learning of sequence models. Spectral learning and moment-matching approaches have recently proved a viable alternative to expectation-maximization (EM) for unsupervised learning (Hsu et al., 2012; Balle and Mohri, 2012; Bailly et al., 2013), supervised learning with latent variables (Cohen and Collins, 2014; Quattoni et al., 2014; Stratos et al., 2013) and topic modeling (Arora et al., 2013; Nguyen et al., 2015). These methods have learnability guarantees, do not suffer from local optima, and are computationally less demanding. Unlike spectral methods, ours does not require an orthogonal decomposition of any matrix or tensor. Instead, it considers a more restricted form of supervision: words that have unambiguous annotations, so-called anchor words (Arora et al., 2013). Rather than identifying anchor words from unlabeled data (Stratos et al., 2016), we extract them from a small labeled dataset or from a dictionary. Given the anchor words, t"
D16-1028,Q16-1018,0,0.0111305,"), supervised learning with latent variables (Cohen and Collins, 2014; Quattoni et al., 2014; Stratos et al., 2013) and topic modeling (Arora et al., 2013; Nguyen et al., 2015). These methods have learnability guarantees, do not suffer from local optima, and are computationally less demanding. Unlike spectral methods, ours does not require an orthogonal decomposition of any matrix or tensor. Instead, it considers a more restricted form of supervision: words that have unambiguous annotations, so-called anchor words (Arora et al., 2013). Rather than identifying anchor words from unlabeled data (Stratos et al., 2016), we extract them from a small labeled dataset or from a dictionary. Given the anchor words, the estimation of the model parameters can be made efficient by collecting moment statistics from unlabeled data, then solving a small quadratic program for each word. Our contributions are as follows: • We adapt anchor methods to semi-supervised learning of generative sequence models. • We show how our method can also handle loglinear feature-based emissions. • We apply this model to POS tagging. Our experiments on the Twitter dataset introduced by Gimpel et al. (2011) and on the dataset introduced by"
D17-1036,P16-1231,0,0.0642119,"Missing"
D17-1036,P79-1000,0,0.153204,"Missing"
D17-1036,P16-1059,0,0.0302234,"in a discrete space to pick the easiest actions (the non-differentiable argmax in line 6 of Alg. 1). Generalizing this idea to “continuous” operations is at the very core of our paper, allowing gradients to be fully backpropagated. In a different context, building differentiable computation structures has also been addressed by Graves et al. (2014); Grefenstette et al. (2015). An important contribution of our paper is the constrained softmax transformation. Others have proposed alternatives to softmax attention, including the sparsemax (Martins and Astudillo, 2016) and multi-focal attention (Globerson et al., 2016). The latter computes a KL projection onto a budget polytope to focus on multiple words. Our constrained softmax also corresponds to a KL projecConclusions We introduced novel fully-differentiable easy-first taggers that learn to make predictions over sequences in an order that is adapted to the task at hand. The decoder iteratively updates a sketch of the predictions by interacting with an attention mechanism. To spread attention evenly through all words, we introduced a new constrained softmax transformation, along with an algorithm to backpropagate its gradients. Our neural-easy first decod"
D17-1036,N10-1115,0,0.565915,"e Neural Easy-First Taggers Julia Kreutzer∗ Andr´e F. T. Martins Computational Linguistics Unbabel Heidelberg University, Germany & Instituto de Telecomunicac¸o˜ es kreutzer@cl.uni-heidelberg.de Lisbon, Portugal andre.martins@unbabel.com Abstract greediness leads to error propagation and suboptimal classification performance. This can partly be mitigated by globally normalized models (Andor et al., 2016) and imitation learning (Daum´e et al., 2009; Ross et al., 2011; Bengio et al., 2015), however these techniques still have a left-to-right bias. Easy-first decoders (Tsuruoka and Tsujii, 2005; Goldberg and Elhadad, 2010, §2) are an interesting alternative: instead of a fixed decoding order, these methods schedule their own actions by prefering “easier” decisions over more difficult ones. A disadvantage is that these models are harder to learn, due to the factorial number of orderings leading to correct predictions. Usually, gradients are not backpropagated over this combinatorial latent space (Kiperwasser and Goldberg, 2016a), or a separate model is used to determine the easiest next move (Clark and Manning, 2016). In this paper, we develop novel, fully differentiable, neural easy-first sequence taggers (§3)"
D17-1036,C04-1046,0,0.0461751,"r an English sentence, for fullstate models trained with N ∈ {5, L}. The model with L sketch steps learned that it is easiest to focus on the beginning of a named entity, and then to move to the right to identify the full span. The model with only 5 sketch steps learns to go straight to the point, placing most attention on the entity words and ignoring most of the O -tokens. 5.3 Word-Level Quality Estimation Finally, we evaluate our model’s performance on word-level translation quality estimation. The goal is to evaluate a translation system’s quality without access to reference translations (Blatz et al., 2004; Specia et al., 2013). Given a sentence pair (a source sentence and its machine translated sentence in a target language), a word-level system classifies each target word as OK or BAD. We used the official English-German dataset from the WMT16 shared task (Bojar et al., 2016). This task differs from the previous ones in which its input is a sentence pair and not a single eters, mixing character and word-based models, sharing a model across languages, or combining CRFs with convolutional and recurrent layers. We used simpler models in our experiments since our goal is to assess how much the ne"
D17-1036,P16-1061,0,0.0628142,"e techniques still have a left-to-right bias. Easy-first decoders (Tsuruoka and Tsujii, 2005; Goldberg and Elhadad, 2010, §2) are an interesting alternative: instead of a fixed decoding order, these methods schedule their own actions by prefering “easier” decisions over more difficult ones. A disadvantage is that these models are harder to learn, due to the factorial number of orderings leading to correct predictions. Usually, gradients are not backpropagated over this combinatorial latent space (Kiperwasser and Goldberg, 2016a), or a separate model is used to determine the easiest next move (Clark and Manning, 2016). In this paper, we develop novel, fully differentiable, neural easy-first sequence taggers (§3). Instead of taking discrete actions, our decoders use an attention mechanism to decide (in a soft manner) which word to focus on for the next tagging decision. Our models are able to learn their own sense of “easiness”: the words receiving focus may not be the ones the model is most confident about, but the best to avoid error propagation in the long run. To make sure that all words receive the same cumulative attention, we further contribute with a new constrained softmax transformation (§4). This"
D17-1036,Q16-1032,0,0.257701,"and imitation learning (Daum´e et al., 2009; Ross et al., 2011; Bengio et al., 2015), however these techniques still have a left-to-right bias. Easy-first decoders (Tsuruoka and Tsujii, 2005; Goldberg and Elhadad, 2010, §2) are an interesting alternative: instead of a fixed decoding order, these methods schedule their own actions by prefering “easier” decisions over more difficult ones. A disadvantage is that these models are harder to learn, due to the factorial number of orderings leading to correct predictions. Usually, gradients are not backpropagated over this combinatorial latent space (Kiperwasser and Goldberg, 2016a), or a separate model is used to determine the easiest next move (Clark and Manning, 2016). In this paper, we develop novel, fully differentiable, neural easy-first sequence taggers (§3). Instead of taking discrete actions, our decoders use an attention mechanism to decide (in a soft manner) which word to focus on for the next tagging decision. Our models are able to learn their own sense of “easiness”: the words receiving focus may not be the ones the model is most confident about, but the best to avoid error propagation in the long run. To make sure that all words receive the same cumulati"
D17-1036,Q16-1023,0,0.295884,"and imitation learning (Daum´e et al., 2009; Ross et al., 2011; Bengio et al., 2015), however these techniques still have a left-to-right bias. Easy-first decoders (Tsuruoka and Tsujii, 2005; Goldberg and Elhadad, 2010, §2) are an interesting alternative: instead of a fixed decoding order, these methods schedule their own actions by prefering “easier” decisions over more difficult ones. A disadvantage is that these models are harder to learn, due to the factorial number of orderings leading to correct predictions. Usually, gradients are not backpropagated over this combinatorial latent space (Kiperwasser and Goldberg, 2016a), or a separate model is used to determine the easiest next move (Clark and Manning, 2016). In this paper, we develop novel, fully differentiable, neural easy-first sequence taggers (§3). Instead of taking discrete actions, our decoders use an attention mechanism to decide (in a soft manner) which word to focus on for the next tagging decision. Our models are able to learn their own sense of “easiness”: the words receiving focus may not be the ones the model is most confident about, but the best to avoid error propagation in the long run. To make sure that all words receive the same cumulati"
D17-1036,P15-1033,0,0.017979,"m that controls which parts of the input are strategically the best to process next. We present a new constrained softmax transformation that ensures the same cumulative attention to every word, and show how to efficiently evaluate and backpropagate over it. Our models compare favourably to BILSTM taggers on three sequence tagging tasks. 1 Introduction In the last years, neural models have led to major advances in several structured NLP problems, including sequence tagging (Plank et al., 2016; Lample et al., 2016), sequence-to-sequence prediction (Sutskever et al., 2014), and sequence-totree (Dyer et al., 2015). Part of the success comes from clever architectures such as (bidirectional) long-short term memories (BILSTMs; Hochreiter and Schmidhuber (1997); Graves et al. (2005)) and attention mechanisms (Bahdanau et al., 2015), which are able to select the pieces of context relevant for prediction. A noticeable aspect about many of the systems above is that they typically decode from left to right, greedily or with a narrow beam. While this is computationally convenient and reminiscent of the way humans process spoken language, the combination of unidirectional decoding and ∗ This research was partial"
D17-1036,W15-3037,1,0.83952,"ur proposed model, we carried out an ablation study for NER on the English dataset. The following alternate configurations were tried and compared against the NEF-CRF-F model with csoftmax attention and 5 sketch steps: sentence. We replaced the affix embeddings by the concatenation of the 64-dimensional embeddings of the target words with those of the aligned source words (we used the alignments provided in the shared task), yielding 128-dimensional representations. We used the same hyperparameters as in the POS tagging task, except the dropout probability, set to 0.1. We followed prior work (Kreutzer et al., 2015) and upweighted the BAD words in the loss function to make the model more pessimistic; we used a weight of 5 (tuned in the validation set). Table 3 shows the results. We see that all our NEF-S and NEF-F models outperform the BILSTM, and that the NEF-F model with 5 sketch steps achieved the best results.7 Figure 4 illustrates the attention over the target words for 5 sketches. We observe that the attention focuses early in the areas predicted BAD and moves left and right within these areas, not wasting attention on the OK part of the sentence. This block-wise fo• A NEF-CRF-F model for which the"
D17-1036,N16-1030,0,0.588865,"atively updates a “sketch” of the predictions over the sequence. At its core is an attention mechanism that controls which parts of the input are strategically the best to process next. We present a new constrained softmax transformation that ensures the same cumulative attention to every word, and show how to efficiently evaluate and backpropagate over it. Our models compare favourably to BILSTM taggers on three sequence tagging tasks. 1 Introduction In the last years, neural models have led to major advances in several structured NLP problems, including sequence tagging (Plank et al., 2016; Lample et al., 2016), sequence-to-sequence prediction (Sutskever et al., 2014), and sequence-totree (Dyer et al., 2015). Part of the success comes from clever architectures such as (bidirectional) long-short term memories (BILSTMs; Hochreiter and Schmidhuber (1997); Graves et al. (2005)) and attention mechanisms (Bahdanau et al., 2015), which are able to select the pieces of context relevant for prediction. A noticeable aspect about many of the systems above is that they typically decode from left to right, greedily or with a narrow beam. While this is computationally convenient and reminiscent of the way humans"
D17-1036,D16-1011,0,0.0330746,"“coverage criterion” (see their Eq. 11), however their heuristic is non-differentiable. Our sketch generation step is similar in spirit to the “deep recurrent attentive writer” (DRAW, Gregor et al. (2015)) which generates images by iteratively refining sketches with a recurrent neural network (RNN). However, our goal is very different: instead of generating images, we generate vectors that lead to a final sequence tagging prediction. Finally, the visualization provided in Figures 2– 4 brings up the question how to understand and rationalize predictions by neural network systems, addressed by Lei et al. (2016). Their model, however, uses a form of stochastic attention and it does not perform any iterative refinement like ours. Table 4: Ablation experiments. Reported are F1 scores for NER in the English test set. systems is relatively small. Removing the concatenation in Eq. 6 is harmful, which suggests that there is information about the input not retained in the sketches. The uniform attention performs surprisingly well, and so do the left-to-right and right-to-left models, but they are still about half a point behind. The vanilla easy-first system has the worst performance with N = 5. This is due"
D17-1036,P13-2020,0,0.128662,"nd while Figure 1: A neural easy-first system applied to a POS tagging problem. Given the current input/sketch representation, an attention mechanism decides where to focus (see bar plot) and is used to generate the next sketch. Right: A sequence of sketches (Sn )N n=1 generated along the way. tions produced by the decoder, to help understand what tagging decisions the model finds the easiest. 2 Easy-First Decoders The idea behind easy-first decoding is to perform “easier” and less risky decisions before committing to more difficult ones (Tsuruoka and Tsujii, 2005; Goldberg and Elhadad, 2010; Ma et al., 2013). Alg. 1 shows the overall procedure for a sequence tagging problem (the idea carries out to other structured problems). Let x1:L be an input sequence (e.g. words in a sentence) and y1:L be the corresponding tag sequence (e.g. their POS tags). The algorithm assigns tags one position i at the time, maintaining a set B of covered positions. It also maintains a set S of pairs (i, ybi ), storing the tags that have already been predicted at those positions. We can regard this set as a sketch of the output sequence, built incrementally while the algorithm is executed. At each time step, the model co"
D17-1036,P16-1101,0,0.0265328,"English-German dataset from the WMT16 shared task (Bojar et al., 2016). This task differs from the previous ones in which its input is a sentence pair and not a single eters, mixing character and word-based models, sharing a model across languages, or combining CRFs with convolutional and recurrent layers. We used simpler models in our experiments since our goal is to assess how much the neural easy-first systems can bring in addition to a BILSTM system, rather than building a state-of-the-art system. 6 The current state of the art on these datasets (Gillick et al., 2016; Lample et al., 2016; Ma and Hovy, 2016) is achieved by more sophisticated systems with more param355 BILSTM 39.71 NEF-S N =5 N =L NEF-F N =5 N =L 40.91 41.18 40.99 40.84 Table 3: F1 -MULT scores (product of F1 for OK and BAD words) for word-level quality estimation, computed by the official shared task script. Figure 4: Example for word-level quality estimation. The source sentence is “To open the Actions panel, from the main menu, choose Window > Actions.” BAD words are red (bold font), OK words are green. Figure 3: Attention visualization for English NER, for 5 (top) and L (bottom) sketch steps. Words tagged as B -* are marked in"
D17-1036,P13-2109,1,0.891652,"Missing"
D17-1036,W16-2387,1,0.847254,"p was replaced by a uniform distribution over the input words. • A vanilla easy-first system (Alg. 1). Since this system can only focus on one word at the time (unlike the models with soft attention), we tried both N = 5 and N = L sketch steps. • A left-to-right and right-to-left model, which replaces the attention mechanism by one of these two prescribed orders. 7 Our best system would rank third in the shared task, out of 13 submissions. The winner system, which achieved 49.52 F1 -MULT, was considerably more complex than ours, using an ensemble of three neural networks with a linear system (Martins et al., 2016). Table 4 shows the results. As expected, the neural easy-first system was the best performing one, although the difference with respect to the ablated 356 NEF-CRF-F, N = 5 88.01 NEF-CRF-F w/out concat, N = 5 Uniform Attention, N = 5 Vanilla EF + CRF, N = 5 Vanilla EF + CRF, N = L Left-to-right + CRF, N = L Right-to-left + CRF, N = L 87.47 87.46 87.17 87.46 87.57 87.53 tion, but (i) it involves box constraints instead of a budget, (ii) it is normalized to 1, and (iii) we also backpropagate the gradient over the constraint variables. It also achieves sparsity (see the “raindrop” plots in Figure"
D17-1036,D14-1162,0,0.087184,"). We made two experiments: one using the exact same BILSTM and NEF models with a standard softmax output layer, as in §5.1 (which does not guarantee valid segmentations), and another one replacing the output softmax layer by a sequential CRF layer, which requires learning O(K 2 ) additional parameters for pairs of consecutive tags (Huang et al., 2015; Lample et al., 2016). We used the same hyperparameters as in the POS tagging experiments, except the dropout probability, which was set to 0.3 (tuned on the validation set). For English, we used pre-trained 300dimensional GloVe-840B embeddings (Pennington et al., 2014); for Spanish and German, we used the 64-dimensional word embeddings from Lample et al. (2016); for Dutch we used the aforementioned Polyglot vectors. All embeddings are finetuned during training. Since many words are not entities, and those receive a default “outside” tag, we expect that fewer sketch steps are necessary to achieve top performance. Table 2 shows the results, which confirm this hypothesis. We compare the same BILSTM baseline to our NEF-S and NEF-F models with csoftmax attention (with and without the CRF output layer), varying the maximum number of sketch steps. We also compare"
D17-1036,P16-2067,0,0.0894626,"le. The decoder iteratively updates a “sketch” of the predictions over the sequence. At its core is an attention mechanism that controls which parts of the input are strategically the best to process next. We present a new constrained softmax transformation that ensures the same cumulative attention to every word, and show how to efficiently evaluate and backpropagate over it. Our models compare favourably to BILSTM taggers on three sequence tagging tasks. 1 Introduction In the last years, neural models have led to major advances in several structured NLP problems, including sequence tagging (Plank et al., 2016; Lample et al., 2016), sequence-to-sequence prediction (Sutskever et al., 2014), and sequence-totree (Dyer et al., 2015). Part of the success comes from clever architectures such as (bidirectional) long-short term memories (BILSTMs; Hochreiter and Schmidhuber (1997); Graves et al. (2005)) and attention mechanisms (Bahdanau et al., 2015), which are able to select the pieces of context relevant for prediction. A noticeable aspect about many of the systems above is that they typically decode from left to right, greedily or with a narrow beam. While this is computationally convenient and reminisc"
D17-1036,W02-2024,0,0.111778,", which causes the attention over a word to increase gradually until the cumulative attention is exhausted. This constrasts with the softmax attention (less diverse and non-sparse) and the sparsemax (sparse, but not even). We show for comparison the (hard) decisions made by the vanilla easy-first decoder. 5.2 Gillick et al. (2016) Lample et al. (2016) BILSTM NEF-S, N NEF-F, N NEF-F, N NEF-F, N =5 =5 = 10 =L BILSTM-CRF NEF-CRF-S, N NEF-CRF-F, N NEF-CRF-F, N NEF-CRF-F, N Named Entity Recognition Next, we applied our model to NER. We used the official datasets from the CoNLL 2002-3 shared tasks (Sang, 2002; Sang and De Meulder, 2003), which tag names, locations, and organizations using a BIO scheme, and cover four languages (Dutch, English, German, and Spanish). We made two experiments: one using the exact same BILSTM and NEF models with a standard softmax output layer, as in §5.1 (which does not guarantee valid segmentations), and another one replacing the output softmax layer by a sequential CRF layer, which requires learning O(K 2 ) additional parameters for pairs of consecutive tags (Huang et al., 2015; Lample et al., 2016). We used the same hyperparameters as in the POS tagging experiments"
D17-1036,W03-0419,0,0.329555,"Missing"
D17-1036,P07-1096,0,0.0357602,"N = 5. This is due to the fact that the vanilla model is uncapable of processing words “in parallel” in the same sketch step, a disadvantage with respect to the neural easy-first models, which have this capability due to their soft attention mechanisms (see the top image in Fig. 3). 6 Related Work 7 Vanilla easy-first decoders have been used in POS tagging (Tsuruoka and Tsujii, 2005; Ma et al., 2013), dependency parsing (Goldberg and Elhadad, 2010), and coreference resolution (Stoyanov and Eisner, 2012), being related to cyclic dependency networks and guided learning (Toutanova et al., 2003; Shen et al., 2007). More recent works compute scores with a neural network (Socher et al., 2011; Clark and Manning, 2016; Kiperwasser and Goldberg, 2016a), but they still operate in a discrete space to pick the easiest actions (the non-differentiable argmax in line 6 of Alg. 1). Generalizing this idea to “continuous” operations is at the very core of our paper, allowing gradients to be fully backpropagated. In a different context, building differentiable computation structures has also been addressed by Graves et al. (2014); Grefenstette et al. (2015). An important contribution of our paper is the constrained s"
D17-1036,P13-4014,0,0.0232531,"Missing"
D17-1036,C12-1154,0,0.0239329,"ft models, but they are still about half a point behind. The vanilla easy-first system has the worst performance with N = 5. This is due to the fact that the vanilla model is uncapable of processing words “in parallel” in the same sketch step, a disadvantage with respect to the neural easy-first models, which have this capability due to their soft attention mechanisms (see the top image in Fig. 3). 6 Related Work 7 Vanilla easy-first decoders have been used in POS tagging (Tsuruoka and Tsujii, 2005; Ma et al., 2013), dependency parsing (Goldberg and Elhadad, 2010), and coreference resolution (Stoyanov and Eisner, 2012), being related to cyclic dependency networks and guided learning (Toutanova et al., 2003; Shen et al., 2007). More recent works compute scores with a neural network (Socher et al., 2011; Clark and Manning, 2016; Kiperwasser and Goldberg, 2016a), but they still operate in a discrete space to pick the easiest actions (the non-differentiable argmax in line 6 of Alg. 1). Generalizing this idea to “continuous” operations is at the very core of our paper, allowing gradients to be fully backpropagated. In a different context, building differentiable computation structures has also been addressed by"
D17-1036,N03-1033,0,0.0219395,"e worst performance with N = 5. This is due to the fact that the vanilla model is uncapable of processing words “in parallel” in the same sketch step, a disadvantage with respect to the neural easy-first models, which have this capability due to their soft attention mechanisms (see the top image in Fig. 3). 6 Related Work 7 Vanilla easy-first decoders have been used in POS tagging (Tsuruoka and Tsujii, 2005; Ma et al., 2013), dependency parsing (Goldberg and Elhadad, 2010), and coreference resolution (Stoyanov and Eisner, 2012), being related to cyclic dependency networks and guided learning (Toutanova et al., 2003; Shen et al., 2007). More recent works compute scores with a neural network (Socher et al., 2011; Clark and Manning, 2016; Kiperwasser and Goldberg, 2016a), but they still operate in a discrete space to pick the easiest actions (the non-differentiable argmax in line 6 of Alg. 1). Generalizing this idea to “continuous” operations is at the very core of our paper, allowing gradients to be fully backpropagated. In a different context, building differentiable computation structures has also been addressed by Graves et al. (2014); Grefenstette et al. (2015). An important contribution of our paper"
D17-1036,H05-1059,0,0.686018,"s Easy: Fully Differentiable Neural Easy-First Taggers Julia Kreutzer∗ Andr´e F. T. Martins Computational Linguistics Unbabel Heidelberg University, Germany & Instituto de Telecomunicac¸o˜ es kreutzer@cl.uni-heidelberg.de Lisbon, Portugal andre.martins@unbabel.com Abstract greediness leads to error propagation and suboptimal classification performance. This can partly be mitigated by globally normalized models (Andor et al., 2016) and imitation learning (Daum´e et al., 2009; Ross et al., 2011; Bengio et al., 2015), however these techniques still have a left-to-right bias. Easy-first decoders (Tsuruoka and Tsujii, 2005; Goldberg and Elhadad, 2010, §2) are an interesting alternative: instead of a fixed decoding order, these methods schedule their own actions by prefering “easier” decisions over more difficult ones. A disadvantage is that these models are harder to learn, due to the factorial number of orderings leading to correct predictions. Usually, gradients are not backpropagated over this combinatorial latent space (Kiperwasser and Goldberg, 2016a), or a separate model is used to determine the easiest next move (Clark and Manning, 2016). In this paper, we develop novel, fully differentiable, neural easy"
D17-1036,W16-2301,0,\N,Missing
D17-1036,N16-1155,0,\N,Missing
D18-1108,D15-1075,0,0.0792148,"Missing"
D18-1108,P16-1139,0,0.0665255,"Missing"
D18-1108,P17-1152,0,0.0280828,"han the flat baseline. 5 Conclusions and future work We presented a novel approach for training latent structure neural models, based on the key idea of sparsifying the set of possible structures, and demonstrated our method with competitive latent dependency TreeLSTM models. Our method’s generality opens up several avenues for future work: since it supports any structure for which MAP inference is available (e.g., matchings, alignments), and we have no restrictions on the downstream pξ (y |h, x), we may design latent versions of more complicated state-of-the-art models, such as ESIM for NLI (Chen et al., 2017). In concurrent work, Peng et al. (2018) proposed an approximate MAP backward pass, relying on a relaxation and a gradient projection. Unlike our method, theirs does not support multiple latent structures; we intend to further study the relationship between the methods. 909 Acknowledgments This work was supported by the European Research Council (ERC StG DeepSPIN 758969) and by the Fundação para a Ciência e Tecnologia through contract UID/EEA/50008/2013. We thank Annabelle Carrell, Chris Dyer, Jack Hessel, Tim Vieira, Justine Zhang, Sydney Zink, and the anonymous reviewers, for helpful and wel"
D18-1108,P14-5010,0,0.00410869,"nsisting of any autodifferentiable computation w.r.t. x, conditioned on subj. Dependency TreeLSTM. We combine the word vectors vi in a sentence into a single vector using a tree-structured Child-Sum LSTM, which allows an arbitrary number of children at any node (Tai et al., 2015). Our baselines consist in extreme cases of dependency trees: where the parent of word i is word i+1 (resulting in a left-to-right sequential LSTM), and where all words are direct children of the root node (resulting in a flat additive model). We also consider off-line dependency trees precomputed by Stanford CoreNLP (Manning et al., 2014). Neural arc-factored dependency parsing. We compute arc scores sθ (a; x) with one-hidden-layer perceptrons (Kiperwasser and Goldberg, 2016). Experimental setup. All networks are trained via stochastic gradient with 16 samples per batch. We tune the learning rate on a log-grid, using a decay factor of 0.9 after every epoch at which the validation performance is not the best seen, and stop after five epochs without improvement. At test time, we scale the arc scores sθ by a temperature t 907 rank seen acc10 acc100 unseen acc10 acc100 rank rank concepts acc10 acc100 left-to-right flat latent 17 1"
D18-1108,D16-1046,0,0.0550654,"Missing"
D18-1108,Q16-1002,0,0.0266522,"Missing"
D18-1108,Q16-1023,0,0.0173434,"in a sentence into a single vector using a tree-structured Child-Sum LSTM, which allows an arbitrary number of children at any node (Tai et al., 2015). Our baselines consist in extreme cases of dependency trees: where the parent of word i is word i+1 (resulting in a left-to-right sequential LSTM), and where all words are direct children of the root node (resulting in a flat additive model). We also consider off-line dependency trees precomputed by Stanford CoreNLP (Manning et al., 2014). Neural arc-factored dependency parsing. We compute arc scores sθ (a; x) with one-hidden-layer perceptrons (Kiperwasser and Goldberg, 2016). Experimental setup. All networks are trained via stochastic gradient with 16 samples per batch. We tune the learning rate on a log-grid, using a decay factor of 0.9 after every epoch at which the validation performance is not the best seen, and stop after five epochs without improvement. At test time, we scale the arc scores sθ by a temperature t 907 rank seen acc10 acc100 unseen acc10 acc100 rank rank concepts acc10 acc100 left-to-right flat latent 17 18 12 42.6 45.1 47.5 73.8 71.1 74.6 43 31 40 33.2 38.2 35.6 61.8 65.6 60.1 28 29 20 35.9 34.3 38.4 66.7 68.2 70.7 Maillard et al. (2017) Hill"
D18-1108,Q18-1005,0,0.37657,"ized modules (Hu et al., 2017; Johnson et al., 2017), and composing sentence representations using latent syntactic parse trees (Yogatama et al., 2017). But how to learn a model that is able to condition on such combinatorial variables? The question then becomes: how to marginalize over all possible latent structures? For tractability, existing approaches have to make a choice. Some of them eschew global latent structure, resorting to computation graphs built from smaller local decisions: e.g., structured attention networks use local posterior marginals as attention weights (Kim et al., 2017; Liu and Lapata, 2018), and Maillard et al. (2017) construct sentence representations from parser chart entries. Others allow more flexibility at the cost of losing end-to-end differentiability, ending up with reinforcement learning C) can marginalize over full global structures. This contrasts with off-line and with reinforcement learning-based approaches, which satisfy B and C but not A; and with local marginal-based methods such as structured attention networks, which satisfy A and B, but not C. Key to our approach is the recently proposed SparseMAP inference (Niculae et al., 2018), which induces, for each data"
D18-1108,P18-1173,0,0.18684,"Missing"
D18-1108,D14-1162,0,0.0850412,"t structure h in arbitrary, nondifferentiable ways. We then compute X r¯(x) := pθ (h |x)rξ (h, x) = Eh∼pθ rξ (h, x). h∈H(x) This strategy is demonstrated in our reversedictionary experiments in §3.4. In addition, our approach is not limited to trees: any structured model with tractable MAP inference may be used. 3 Experiments We evaluate our approach on three natural language processing tasks: sentence classification, natural language inference, and reverse dictionary lookup. 3.1 Common aspects Word vectors. Unless otherwise mentioned, we initialize with 300-dimensional GloVe word embeddings (Pennington et al., 2014) We transform every sentence via a bidirectional LSTM encoder, to produce a context-aware vector vi encoding word i. Backward pass. We next show how to compute end-to-end gradients efficiently. Recall from Eqn. 1 P p(y |x) = h∈H pθ (h |x) pξ (y |h, x), where h is a discrete index of P a tree. To train the classifier, ∂p(y|x) /∂ξ = h∈H pθ (h |x)∂pξ (y|h,x)/∂ξ, we have therefore only the terms with nonzero probabil¯ contribute to the gradient. ity (i.e., h ∈ H) ∂pξ (y|h,x)/∂ξ is readily available by implementing pξ in an automatic differentiation library.1 To train ∂p(y|x)/θ is the the latent P"
D18-1108,D13-1170,0,0.0138999,"Missing"
D18-1108,P15-1150,0,0.0617874,"trix. The proof, given in Appendix B, is a novel extension of the SparseMAP backward pass (Niculae et al., 2018). Generality. Our description focuses on probabilistic classifiers, but our method can be readily applied to networks that output any representation, not necessarily a probability. For this, we define a function rξ (h, x), consisting of any autodifferentiable computation w.r.t. x, conditioned on subj. Dependency TreeLSTM. We combine the word vectors vi in a sentence into a single vector using a tree-structured Child-Sum LSTM, which allows an arbitrary number of children at any node (Tai et al., 2015). Our baselines consist in extreme cases of dependency trees: where the parent of word i is word i+1 (resulting in a left-to-right sequential LSTM), and where all words are direct children of the root node (resulting in a flat additive model). We also consider off-line dependency trees precomputed by Stanford CoreNLP (Manning et al., 2014). Neural arc-factored dependency parsing. We compute arc scores sθ (a; x) with one-hidden-layer perceptrons (Kiperwasser and Goldberg, 2016). Experimental setup. All networks are trained via stochastic gradient with 16 samples per batch. We tune the learning"
D18-1108,Q18-1019,0,0.450293,"Missing"
D19-1223,W14-3302,0,0.0702933,"Missing"
D19-1223,N19-1313,1,0.892821,"Missing"
D19-1223,W18-6301,0,0.0353021,"he adaptive span Transformer (Sukhbaatar et al., 2019) only attend to words within a contiguous span of the past tokens, our model is not only able to obtain different and not necessarily contiguous sparsity patterns for each attention head, but is also able to tune its support over which tokens to attend adaptively. Introduction The Transformer architecture (Vaswani et al., 2017) for deep neural networks has quickly risen to prominence in NLP through its efﬁciency and performance, leading to improvements in the state of the art of Neural Machine Translation (NMT; JunczysDowmunt et al., 2018; Ott et al., 2018), as well as inspiring other powerful general-purpose models like BERT (Devlin et al., 2019) and GPT-2 (Radford et al., 2019). At the heart of the Transformer lie multi-head attention mechanisms: each word is represented by multiple different weighted averages of its relevant context. As suggested by recent works on interpreting attention head roles, separate attention heads may learn to look for various relationships between tokens (Tang et al., 2018; Raganato and Tiedemann, 2018; Mareˇcek and Rosa, 2018; Tenney et al., 2019; Voita et al., 2019). The attention distribution of each head is pre"
D19-1223,N19-1423,0,0.268693,"iguous span of the past tokens, our model is not only able to obtain different and not necessarily contiguous sparsity patterns for each attention head, but is also able to tune its support over which tokens to attend adaptively. Introduction The Transformer architecture (Vaswani et al., 2017) for deep neural networks has quickly risen to prominence in NLP through its efﬁciency and performance, leading to improvements in the state of the art of Neural Machine Translation (NMT; JunczysDowmunt et al., 2018; Ott et al., 2018), as well as inspiring other powerful general-purpose models like BERT (Devlin et al., 2019) and GPT-2 (Radford et al., 2019). At the heart of the Transformer lie multi-head attention mechanisms: each word is represented by multiple different weighted averages of its relevant context. As suggested by recent works on interpreting attention head roles, separate attention heads may learn to look for various relationships between tokens (Tang et al., 2018; Raganato and Tiedemann, 2018; Mareˇcek and Rosa, 2018; Tenney et al., 2019; Voita et al., 2019). The attention distribution of each head is predicted typically using the softmax normalizing transform. As a result, all context words hav"
D19-1223,P02-1040,0,0.104499,"Missing"
D19-1223,N19-1357,0,0.119999,"oder. • Decoder self-attention: attends over the partial output sentence fragment produced so far. Together, these mechanisms enable the contextualized ﬂow of information between the input sentence and the sequential decoder. 2.2 Sparse Attention The softmax mapping (Equation 2) is elementwise proportional to exp, therefore it can never assign a weight of exactly zero. Thus, unnecessary items are still taken into consideration to some extent. Since its output sums to one, this invariably means less weight is assigned to the relevant items, potentially harming performance and interpretability (Jain and Wallace, 2019). This has motivated a line of research on learning networks with sparse mappings (Martins and Astudillo, 2016; Niculae and Blondel, 2017; Louizos et al., 2018; Shao et al., 2175 2019). We focus on a recently-introduced ﬂexible family of transformations, α-entmax (Blondel et al., 2019; Peters et al., 2019), deﬁned as: α-entmax(z) := argmax p⊤ z + HαT (p), (4) p∈△d P where △d := {p ∈ Rd : i pi = 1} is the probability simplex, and, for α ≥ 1, HαT is the Tsallis continuous family of entropies (Tsallis, 1988):  ( P  1 α , α 6= 1, p − p j j j (5) HαT (p) := α(α−1) P − j pj log pj , α = 1. This fa"
D19-1223,W18-2716,0,0.0511758,"Missing"
D19-1223,D18-1317,0,0.0194277,"tion heads have. Mareˇcek and Rosa (2018) study the syntactic abilities of the Transformer self-attention, while Raganato and Tiedemann (2018) extract dependency relations from the attention weights. Tenney et al. (2019) ﬁnd that the self-attentions in BERT (Devlin et al., 2019) follow a sequence of processes that resembles a classical NLP pipeline. Regarding redundancy of heads, Voita et al. (2019) develop a method that is able to prune heads of the multi-head attention module and make an empirical study of the role that each head has in self-attention (positional, syntactic and rare words). Li et al. (2018) also aim to reduce head redundancy by adding a regularization term to the loss that maximizes head disagreement and obtain improved results. While not considering Transformer attentions, Jain and Wallace (2019) show that traditional attention mechanisms do not necessarily improve interpretability since softmax attention is vulnerable to an adversarial attack leading to wildly different model predictions for the same attention weights. Sparse attention may mitigate these issues; however, our work focuses mostly on a more mechanical aspect of interpretation by analyzing head behavior, rather th"
D19-1223,P19-1146,1,0.888612,"onships between tokens (Tang et al., 2018; Raganato and Tiedemann, 2018; Mareˇcek and Rosa, 2018; Tenney et al., 2019; Voita et al., 2019). The attention distribution of each head is predicted typically using the softmax normalizing transform. As a result, all context words have non-zero attention weight. Recent work on single attention architectures suggest that using sparse normalizing transforms in attention mechanisms such as sparsemax – which can yield exactly zero probabilities for irrelevant words – may improve performance and interpretability (Malaviya et al., 2018; Deng et al., 2018; Peters et al., 2019). Qualitative analysis of attention heads (Vaswani et al., 2017, Figure 5) suggests that, depending on what phenomena they capture, heads tend to favor ﬂatter or more peaked distributions. Recent works have proposed sparse Transform2174 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2174–2184, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics ers (Child et al., 2019) and adaptive span Transformers (Sukhbaatar et al., 2019). However, the “"
D19-1223,W18-5431,0,0.145231,"performance, leading to improvements in the state of the art of Neural Machine Translation (NMT; JunczysDowmunt et al., 2018; Ott et al., 2018), as well as inspiring other powerful general-purpose models like BERT (Devlin et al., 2019) and GPT-2 (Radford et al., 2019). At the heart of the Transformer lie multi-head attention mechanisms: each word is represented by multiple different weighted averages of its relevant context. As suggested by recent works on interpreting attention head roles, separate attention heads may learn to look for various relationships between tokens (Tang et al., 2018; Raganato and Tiedemann, 2018; Mareˇcek and Rosa, 2018; Tenney et al., 2019; Voita et al., 2019). The attention distribution of each head is predicted typically using the softmax normalizing transform. As a result, all context words have non-zero attention weight. Recent work on single attention architectures suggest that using sparse normalizing transforms in attention mechanisms such as sparsemax – which can yield exactly zero probabilities for irrelevant words – may improve performance and interpretability (Malaviya et al., 2018; Deng et al., 2018; Peters et al., 2019). Qualitative analysis of attention heads (Vaswani"
D19-1223,P16-1162,0,0.14971,"Missing"
D19-1223,P19-1032,0,0.290182,"ps to uncover different head specializations. 1 Andr´e F.T. Martinsä ã andre.martins@unbabel.com Th e qu ic k br ow fo n x ju m p ov s er Th e qu ic k br ow fo n x ju m p ov s er Th e qu ic k br ow fo n x ju m p ov s er ä Vlad Niculaeä vlad@vene.ro head 1 head 2 head 3 head 4 Sparse Transformer Adaptive Span Transformer Adaptively Sparse Transformer (Ours) Figure 1: Attention distributions of different selfattention heads for the time step of the token “over”, shown to compare our model to other related work. While the sparse Transformer (Child et al., 2019) and the adaptive span Transformer (Sukhbaatar et al., 2019) only attend to words within a contiguous span of the past tokens, our model is not only able to obtain different and not necessarily contiguous sparsity patterns for each attention head, but is also able to tune its support over which tokens to attend adaptively. Introduction The Transformer architecture (Vaswani et al., 2017) for deep neural networks has quickly risen to prominence in NLP through its efﬁciency and performance, leading to improvements in the state of the art of Neural Machine Translation (NMT; JunczysDowmunt et al., 2018; Ott et al., 2018), as well as inspiring other powerful"
D19-1223,D18-1458,0,0.0721654,"Missing"
D19-1223,D15-1166,0,0.0908133,": • Encoder self-attention: builds rich, layered representations of each input word, by attending on the entire input sentence. Background 2.1 The Transformer In NMT, the Transformer (Vaswani et al., 2017) is a sequence-to-sequence (seq2seq) model which maps an input sequence to an output sequence through hierarchical multi-head attention mechanisms, yielding a dynamic, context-dependent strategy for propagating information within and across sentences. It contrasts with previous seq2seq models, which usually rely either on costly gated recurrent operations (often LSTMs: Bahdanau et al., 2015; Luong et al., 2015) or static convolutions (Gehring et al., 2017). Given n query contexts and m sequence items under consideration, attention mechanisms compute, for each query, a weighted representation of the items. The particular attention mechanism used in Vaswani et al. (2017) is called scaled dot-product attention, and it is computed in the following way:   QK ⊤ √ V, (1) Att(Q, K, V ) = π d 1 Code and pip package available at https://github. com/deep-spin/entmax. • Context attention: selects a representative weighted average of the encodings of the input words, at each time step of the decoder. • Decoder"
D19-1223,P19-1452,0,0.10564,"e art of Neural Machine Translation (NMT; JunczysDowmunt et al., 2018; Ott et al., 2018), as well as inspiring other powerful general-purpose models like BERT (Devlin et al., 2019) and GPT-2 (Radford et al., 2019). At the heart of the Transformer lie multi-head attention mechanisms: each word is represented by multiple different weighted averages of its relevant context. As suggested by recent works on interpreting attention head roles, separate attention heads may learn to look for various relationships between tokens (Tang et al., 2018; Raganato and Tiedemann, 2018; Mareˇcek and Rosa, 2018; Tenney et al., 2019; Voita et al., 2019). The attention distribution of each head is predicted typically using the softmax normalizing transform. As a result, all context words have non-zero attention weight. Recent work on single attention architectures suggest that using sparse normalizing transforms in attention mechanisms such as sparsemax – which can yield exactly zero probabilities for irrelevant words – may improve performance and interpretability (Malaviya et al., 2018; Deng et al., 2018; Peters et al., 2019). Qualitative analysis of attention heads (Vaswani et al., 2017, Figure 5) suggests that, dependi"
D19-1223,P18-2059,1,0.924894,"heads may learn to look for various relationships between tokens (Tang et al., 2018; Raganato and Tiedemann, 2018; Mareˇcek and Rosa, 2018; Tenney et al., 2019; Voita et al., 2019). The attention distribution of each head is predicted typically using the softmax normalizing transform. As a result, all context words have non-zero attention weight. Recent work on single attention architectures suggest that using sparse normalizing transforms in attention mechanisms such as sparsemax – which can yield exactly zero probabilities for irrelevant words – may improve performance and interpretability (Malaviya et al., 2018; Deng et al., 2018; Peters et al., 2019). Qualitative analysis of attention heads (Vaswani et al., 2017, Figure 5) suggests that, depending on what phenomena they capture, heads tend to favor ﬂatter or more peaked distributions. Recent works have proposed sparse Transform2174 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2174–2184, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics ers (Child et al., 2019) and adaptive span Transformers"
D19-1223,W18-5444,0,0.0429147,"Missing"
D19-1223,P18-1117,0,0.0297708,"e higher this metric is, the more the heads are taking different roles in the model. Figure 6 shows that both sparse Transformer variants show more diversity than the traditional softmax one. Interestingly, diversity seems to peak in the middle layers of the encoder self-attention and context attention, while this is not the case for the decoder self-attention. The statistics shown in this section can be found for the other language pairs in Appendix A. 5.2 Identifying Head Specializations Previous work pointed out some speciﬁc roles played by different heads in the softmax Transformer model (Voita et al., 2018; Tang et al., 2018; Voita et al., 2019). Identifying the specialization of a head can be done by observing the type of tokens Decoder Self-Attention 10 0 -entmax 1.5-entmax 50k 30k 10k 0 50k 30k 10k 0 Figure 4: Distribution of attention densities (average number of tokens receiving non-zero attention weight) for all attention heads and all validation sentences. When compared to 1.5-entmax, α-entmax distributes the sparsity in a more uniform manner, with a clear mode at fully dense attentions, corresponding to the heads with low α. In the softmax case, this distribution would lead to a single"
D19-1223,P19-1580,0,0.441204,"ne Translation (NMT; JunczysDowmunt et al., 2018; Ott et al., 2018), as well as inspiring other powerful general-purpose models like BERT (Devlin et al., 2019) and GPT-2 (Radford et al., 2019). At the heart of the Transformer lie multi-head attention mechanisms: each word is represented by multiple different weighted averages of its relevant context. As suggested by recent works on interpreting attention head roles, separate attention heads may learn to look for various relationships between tokens (Tang et al., 2018; Raganato and Tiedemann, 2018; Mareˇcek and Rosa, 2018; Tenney et al., 2019; Voita et al., 2019). The attention distribution of each head is predicted typically using the softmax normalizing transform. As a result, all context words have non-zero attention weight. Recent work on single attention architectures suggest that using sparse normalizing transforms in attention mechanisms such as sparsemax – which can yield exactly zero probabilities for irrelevant words – may improve performance and interpretability (Malaviya et al., 2018; Deng et al., 2018; Peters et al., 2019). Qualitative analysis of attention heads (Vaswani et al., 2017, Figure 5) suggests that, depending on what phenomena"
E14-1005,P13-1020,1,0.860669,"m i to `: ψi→∗ k,` = πi→∗ k ∧ πi→∗ ` , ∀i, k, ` 6.1 i6=0 The objective to optimize is linear in the arc and pair variables (hence the problem can be represented as an integer linear program by turning the logical constraints into linear inequalities). Dual Decomposition To decode, we employ the alternating directions dual decomposition algorithm (AD3 ), which solves a relaxation of the ILP above. AD3 has been used successfully in various NLP tasks, such as dependency parsing (Martins et al., 2011; Martins et al., 2013), semantic role labeling (Das et al., 2012), and compressive summarization (Almeida and Martins, 2013). At test time, if the solution is not integer, we apply a simple rounding procedure to obtain an actual tree: for each node j, obtain the antecedent (or root) i with the highest ai→j , solving ties arbitrarily. 5.3 Dataset We used the 597 documents of the Wall Street Journal (WSJ) corpus that were disclosed for the CoNLL-2011 coreference shared task (Pradhan et al., 2011) as a dataset for coreference resolution. This dataset includes train, development and test partitions, annotated with coreference information, as well as gold and automatically generated syntactic and semantic information. T"
E14-1005,D08-1031,0,0.0224716,"e (a lexical anchor of the attribution relation, such as a reported speech verb), and the source span (the author of the quote). The same reference introduced the PARC dataset, which we use in our experiments (§6) and which is based on the annotation of a database of attribution relations from the Penn Discourse Treebank (Prasad et al., 2008). Several machine learning algorithms have been applied to this task, either 40 Rahman and Ng, 2011) and seek the set of entities that optimize the sum of scores; this can also be promoted in a decentralized manner (Durrett et al., 2013). Pairwise models (Bengtson and Roth, 2008; Finkel et al., 2008; Versley et al., 2008), on the other hand, define scores for each pair of mentions to be coreferent, and define the clusters as the transitive closure of these pairwise relations. A disadvantage of these two methods is that they lead to intractable decoding problems, so approximate methods must be used. For comprehensive overviews, see Stoyanov et al. (2009), Ng (2010), Pradhan et al. (2011) and Pradhan et al. (2012). Our joint approach (to be fully described in §4) draws inspiration from recent work that shifts from entity clusters to coreference trees (Fernandes et al.,"
E14-1005,P08-1109,0,0.0127649,"e attribution relation, such as a reported speech verb), and the source span (the author of the quote). The same reference introduced the PARC dataset, which we use in our experiments (§6) and which is based on the annotation of a database of attribution relations from the Penn Discourse Treebank (Prasad et al., 2008). Several machine learning algorithms have been applied to this task, either 40 Rahman and Ng, 2011) and seek the set of entities that optimize the sum of scores; this can also be promoted in a decentralized manner (Durrett et al., 2013). Pairwise models (Bengtson and Roth, 2008; Finkel et al., 2008; Versley et al., 2008), on the other hand, define scores for each pair of mentions to be coreferent, and define the clusters as the transitive closure of these pairwise relations. A disadvantage of these two methods is that they lead to intractable decoding problems, so approximate methods must be used. For comprehensive overviews, see Stoyanov et al. (2009), Ng (2010), Pradhan et al. (2011) and Pradhan et al. (2012). Our joint approach (to be fully described in §4) draws inspiration from recent work that shifts from entity clusters to coreference trees (Fernandes et al., 2012; Durrett and Kl"
E14-1005,P06-1005,0,0.0551021,"function, to be described next. 4 4.1.2 4.1 Basic Model Our basic model is a feature-based linear model which assigns a score to each candidate arc linking two mentions (mention-mention arcs), or linking a mention to a quote (mention-quotation arcs). Our basic system is called Q UOTE B EFORE C OREF for reasons we will detail in section 4.2. 4.1.1 Coreference features For the mention-mention arcs, we use the same coreference features as the SURFACE model of the Berkeley Coreference Resolution System (Durrett and Klein, 2013), plus features for gender and number obtained through the dataset of Bergsma and Lin (2006). This is a very simple lexicaldriven model which achieves state-of-the-art results. The features are shown in Table 1. Joint Quotations and Coreferences Quotation features For the quote attribution features, we use features inspired by O’Keefe et al. (2012), shown in Table 2. The same set of features works for speakers that are individual mentions (in the model just described), and for speakers that are clusters of mentions (used in §6 for the baseline Q UOTE A FTER C OREF). These features include various distances between the mention and the quote, the indication of the speaker being inside"
E14-1005,P07-1107,0,0.0281246,"go beyond simple arc-factored models and consider paths in the tree. We formulate the resulting problem as a logic program, which we tackle using a dual decomposition strategy (§5). We provide an empirical comparison between our method and baselines for each of the tasks and a pipeline system, defining suitable metrics for entity-level quotation attribution (§6). 2 3 Coreference Resolution In coreference resolution, we are given a set of mentions M := {m1 , . . . , mK }, and the goal is to cluster them into discourse entities, E := {e1 , . . . , eJ }, where each ej ⊆ M and ej 6= ∅. We follow Haghighi and Klein (2007) and distinguish between proper, nominal, and pronominal mentions. Each requires different types of information to be resolved. Thus, the task involves determining anaphoricity, resolving pronouns, and identifying semantic compatibility among mentions. To resolve these references, one typically exploits contextual and grammatical clues, as well as semantic information and world knowledge, to understand whether mentions refer to people, places, organizations, and so on. The importance of coreference resolution has led to it being the subject of recent CoNLL shared tasks (Pradhan et al., 2011; P"
E14-1005,N10-1061,0,0.016378,"n. The importance of coreference resolution has led to it being the subject of recent CoNLL shared tasks (Pradhan et al., 2011; Pradhan et al., 2012). There has been a variety of approaches for this problem. Early work used local discriminative classifiers, making independent decisions for each mention or pair of mentions (Soon et al., 2001; Ng and Cardie, 2002). Lee et al. (2011) proposed a competitive non-learned sieve-based method, which constructs clusters by aglomerating mentions in a greedy manner. Entity-centric models define scores for the entire entity clusters (Culotta et al., 2007; Haghighi and Klein, 2010; Quotation Attribution The task of quotation attribution can be formally defined as follows. Given a document containing a sequence of quotations, hq1 , . . . , qL i, and a set of candidate speakers, {s1 , . . . , sM }, the goal is to a assign a speaker to every quote. Previous work has handled direct and mixed quotations (Sarmento et al., 2009; O’Keefe et al., 2012), easily extractable with regular expressions for detecting quotation marks, as well as indirect quotations (Pareti et al., 2013), which are more involved and require syntactic or semantic patterns. In this work, we resort to dire"
E14-1005,N07-1011,0,0.0186631,"rganizations, and so on. The importance of coreference resolution has led to it being the subject of recent CoNLL shared tasks (Pradhan et al., 2011; Pradhan et al., 2012). There has been a variety of approaches for this problem. Early work used local discriminative classifiers, making independent decisions for each mention or pair of mentions (Soon et al., 2001; Ng and Cardie, 2002). Lee et al. (2011) proposed a competitive non-learned sieve-based method, which constructs clusters by aglomerating mentions in a greedy manner. Entity-centric models define scores for the entire entity clusters (Culotta et al., 2007; Haghighi and Klein, 2010; Quotation Attribution The task of quotation attribution can be formally defined as follows. Given a document containing a sequence of quotations, hq1 , . . . , qL i, and a set of candidate speakers, {s1 , . . . , sM }, the goal is to a assign a speaker to every quote. Previous work has handled direct and mixed quotations (Sarmento et al., 2009; O’Keefe et al., 2012), easily extractable with regular expressions for detecting quotation marks, as well as indirect quotations (Pareti et al., 2013), which are more involved and require syntactic or semantic patterns. In th"
E14-1005,W04-3250,0,0.00961483,"below a threshold. We also freeze • Representative Speaker Match (RSM): for each annotated quote, we obtain the full gold coreference set of the gold annotated speaker, and 44 in Table 4, the perfromance of our baseline is comparable with the one of the SURFACE system of Durrett and Klein (2013), which is denoted as S URFACE -DK-2013.2 Table 4 also show the CoNLL metrics obtained for the proposed system of joint coreference resolution and quotation attribution. Our joint system outperformed the baseline with statistical significance (with p < 0.05 and according to a bootstrap resampling test (Koehn, 2004)) for all metrics expect for the CEAFE F1 measure, whose value was only slighty improved. These results confirm that the coreference resolution task benefits for being tackled jointly with quotation attribution. choose a representative speaker from that cluster. We define this representative speaker as the proper mention which is the closest to the quote (if available); if the cluster does not contain proper mentions, we use the closest nominal mention; if only pronominal mentions are available, we use the original annotated speaker. The final measure is the percentage of predicted speakers th"
E14-1005,S12-1029,1,0.857693,"of k and ` iff there is a path from i to k and from i to `: ψi→∗ k,` = πi→∗ k ∧ πi→∗ ` , ∀i, k, ` 6.1 i6=0 The objective to optimize is linear in the arc and pair variables (hence the problem can be represented as an integer linear program by turning the logical constraints into linear inequalities). Dual Decomposition To decode, we employ the alternating directions dual decomposition algorithm (AD3 ), which solves a relaxation of the ILP above. AD3 has been used successfully in various NLP tasks, such as dependency parsing (Martins et al., 2011; Martins et al., 2013), semantic role labeling (Das et al., 2012), and compressive summarization (Almeida and Martins, 2013). At test time, if the solution is not integer, we apply a simple rounding procedure to obtain an actual tree: for each node j, obtain the antecedent (or root) i with the highest ai→j , solving ties arbitrarily. 5.3 Dataset We used the 597 documents of the Wall Street Journal (WSJ) corpus that were disclosed for the CoNLL-2011 coreference shared task (Pradhan et al., 2011) as a dataset for coreference resolution. This dataset includes train, development and test partitions, annotated with coreference information, as well as gold and au"
E14-1005,W11-1902,0,0.0323797,"among mentions. To resolve these references, one typically exploits contextual and grammatical clues, as well as semantic information and world knowledge, to understand whether mentions refer to people, places, organizations, and so on. The importance of coreference resolution has led to it being the subject of recent CoNLL shared tasks (Pradhan et al., 2011; Pradhan et al., 2012). There has been a variety of approaches for this problem. Early work used local discriminative classifiers, making independent decisions for each mention or pair of mentions (Soon et al., 2001; Ng and Cardie, 2002). Lee et al. (2011) proposed a competitive non-learned sieve-based method, which constructs clusters by aglomerating mentions in a greedy manner. Entity-centric models define scores for the entire entity clusters (Culotta et al., 2007; Haghighi and Klein, 2010; Quotation Attribution The task of quotation attribution can be formally defined as follows. Given a document containing a sequence of quotations, hq1 , . . . , qL i, and a set of candidate speakers, {s1 , . . . , sM }, the goal is to a assign a speaker to every quote. Previous work has handled direct and mixed quotations (Sarmento et al., 2009; O’Keefe et"
E14-1005,D08-1069,0,0.157803,"ribute (incorrectly) this quote to Pilson. In example (b), there are two quotes with the same speaker entity (as indicated by the cue she added). This gives evidence that M1 and M6 should be coreferent. A pipeline approach would not be able to exploit these correlations. We argue that this type of mistakes, among others, can be prevented by a system that performs quote attribution and coreference resolution jointly (§4). Our joint model is inspired by recent work in coreference resolution that independently ranks the possible mention’s antecedents, forming a latent coreference tree structure (Denis and Baldridge, 2008; Fernandes et al., 2012; Durrett et al., 2013; Durrett and Klein, 2013). We consider a generalization of these structures which we call a quotation-coreference tree. To effectively couple the two tasks, we need to go beyond simple arc-factored models and consider paths in the tree. We formulate the resulting problem as a logic program, which we tackle using a dual decomposition strategy (§5). We provide an empirical comparison between our method and baselines for each of the tasks and a pipeline system, defining suitable metrics for entity-level quotation attribution (§6). 2 3 Coreference Res"
E14-1005,P09-1039,1,0.813482,"IN NUMBER OF WORDS] [DISTANCE IN NUMBER OF SENTENCES] this model breaks the independence assumption between the arcs. However, given the relatively small amount of node pairs that have scores (only mentions inside quotations and consecutive quotations), we expect this “perturbation” to be small enough not to affect the quality of an approximate decoder. The situation resembles other problems in NLP, such as non-projective dependency parsing, which becomes intractable if higher order interactions between the arcs are considered, but can still be well approximated. Inspired by work in parsing (Martins et al., 2009) using linear relaxations with multi-commodity flow models, we propose a similar strategy by defining auxiliary variables and coupling them in a logic program. Table 3: Features used in the JOINT system for mention-quote pairs (only for mentions inside quotes) and for quote pairs (only for consecutive quotes). These features are associated to pairs in the same branch of the quotation-coreference tree. span is within the quotation span (mention-insidequotation pairs), and pairs of quotations that appear consecutively in the document (consecutivequotation pairs). The idea is that, if consecutive"
E14-1005,D13-1203,0,0.102444,"quotes with the same speaker entity (as indicated by the cue she added). This gives evidence that M1 and M6 should be coreferent. A pipeline approach would not be able to exploit these correlations. We argue that this type of mistakes, among others, can be prevented by a system that performs quote attribution and coreference resolution jointly (§4). Our joint model is inspired by recent work in coreference resolution that independently ranks the possible mention’s antecedents, forming a latent coreference tree structure (Denis and Baldridge, 2008; Fernandes et al., 2012; Durrett et al., 2013; Durrett and Klein, 2013). We consider a generalization of these structures which we call a quotation-coreference tree. To effectively couple the two tasks, we need to go beyond simple arc-factored models and consider paths in the tree. We formulate the resulting problem as a logic program, which we tackle using a dual decomposition strategy (§5). We provide an empirical comparison between our method and baselines for each of the tasks and a pipeline system, defining suitable metrics for entity-level quotation attribution (§6). 2 3 Coreference Resolution In coreference resolution, we are given a set of mentions M := {"
E14-1005,D11-1022,1,0.867323,"ables in the second stage. (3) i<j≤k 6 • Node i is a common ancestor of k and ` iff there is a path from i to k and from i to `: ψi→∗ k,` = πi→∗ k ∧ πi→∗ ` , ∀i, k, ` 6.1 i6=0 The objective to optimize is linear in the arc and pair variables (hence the problem can be represented as an integer linear program by turning the logical constraints into linear inequalities). Dual Decomposition To decode, we employ the alternating directions dual decomposition algorithm (AD3 ), which solves a relaxation of the ILP above. AD3 has been used successfully in various NLP tasks, such as dependency parsing (Martins et al., 2011; Martins et al., 2013), semantic role labeling (Das et al., 2012), and compressive summarization (Almeida and Martins, 2013). At test time, if the solution is not integer, we apply a simple rounding procedure to obtain an actual tree: for each node j, obtain the antecedent (or root) i with the highest ai→j , solving ties arbitrarily. 5.3 Dataset We used the 597 documents of the Wall Street Journal (WSJ) corpus that were disclosed for the CoNLL-2011 coreference shared task (Pradhan et al., 2011) as a dataset for coreference resolution. This dataset includes train, development and test partitio"
E14-1005,P13-1012,0,0.074474,"le (b), there are two quotes with the same speaker entity (as indicated by the cue she added). This gives evidence that M1 and M6 should be coreferent. A pipeline approach would not be able to exploit these correlations. We argue that this type of mistakes, among others, can be prevented by a system that performs quote attribution and coreference resolution jointly (§4). Our joint model is inspired by recent work in coreference resolution that independently ranks the possible mention’s antecedents, forming a latent coreference tree structure (Denis and Baldridge, 2008; Fernandes et al., 2012; Durrett et al., 2013; Durrett and Klein, 2013). We consider a generalization of these structures which we call a quotation-coreference tree. To effectively couple the two tasks, we need to go beyond simple arc-factored models and consider paths in the tree. We formulate the resulting problem as a logic program, which we tackle using a dual decomposition strategy (§5). We provide an empirical comparison between our method and baselines for each of the tasks and a pipeline system, defining suitable metrics for entity-level quotation attribution (§6). 2 3 Coreference Resolution In coreference resolution, we are give"
E14-1005,P13-2109,1,0.802282,"age. (3) i<j≤k 6 • Node i is a common ancestor of k and ` iff there is a path from i to k and from i to `: ψi→∗ k,` = πi→∗ k ∧ πi→∗ ` , ∀i, k, ` 6.1 i6=0 The objective to optimize is linear in the arc and pair variables (hence the problem can be represented as an integer linear program by turning the logical constraints into linear inequalities). Dual Decomposition To decode, we employ the alternating directions dual decomposition algorithm (AD3 ), which solves a relaxation of the ILP above. AD3 has been used successfully in various NLP tasks, such as dependency parsing (Martins et al., 2011; Martins et al., 2013), semantic role labeling (Das et al., 2012), and compressive summarization (Almeida and Martins, 2013). At test time, if the solution is not integer, we apply a simple rounding procedure to obtain an actual tree: for each node j, obtain the antecedent (or root) i with the highest ai→j , solving ties arbitrarily. 5.3 Dataset We used the 597 documents of the Wall Street Journal (WSJ) corpus that were disclosed for the CoNLL-2011 coreference shared task (Pradhan et al., 2011) as a dataset for coreference resolution. This dataset includes train, development and test partitions, annotated with core"
E14-1005,P02-1014,0,0.0981328,"emantic compatibility among mentions. To resolve these references, one typically exploits contextual and grammatical clues, as well as semantic information and world knowledge, to understand whether mentions refer to people, places, organizations, and so on. The importance of coreference resolution has led to it being the subject of recent CoNLL shared tasks (Pradhan et al., 2011; Pradhan et al., 2012). There has been a variety of approaches for this problem. Early work used local discriminative classifiers, making independent decisions for each mention or pair of mentions (Soon et al., 2001; Ng and Cardie, 2002). Lee et al. (2011) proposed a competitive non-learned sieve-based method, which constructs clusters by aglomerating mentions in a greedy manner. Entity-centric models define scores for the entire entity clusters (Culotta et al., 2007; Haghighi and Klein, 2010; Quotation Attribution The task of quotation attribution can be formally defined as follows. Given a document containing a sequence of quotations, hq1 , . . . , qL i, and a set of candidate speakers, {s1 , . . . , sM }, the goal is to a assign a speaker to every quote. Previous work has handled direct and mixed quotations (Sarmento et al"
E14-1005,J01-4004,0,0.128708,", and identifying semantic compatibility among mentions. To resolve these references, one typically exploits contextual and grammatical clues, as well as semantic information and world knowledge, to understand whether mentions refer to people, places, organizations, and so on. The importance of coreference resolution has led to it being the subject of recent CoNLL shared tasks (Pradhan et al., 2011; Pradhan et al., 2012). There has been a variety of approaches for this problem. Early work used local discriminative classifiers, making independent decisions for each mention or pair of mentions (Soon et al., 2001; Ng and Cardie, 2002). Lee et al. (2011) proposed a competitive non-learned sieve-based method, which constructs clusters by aglomerating mentions in a greedy manner. Entity-centric models define scores for the entire entity clusters (Culotta et al., 2007; Haghighi and Klein, 2010; Quotation Attribution The task of quotation attribution can be formally defined as follows. Given a document containing a sequence of quotations, hq1 , . . . , qL i, and a set of candidate speakers, {s1 , . . . , sM }, the goal is to a assign a speaker to every quote. Previous work has handled direct and mixed quot"
E14-1005,P10-1142,0,0.0124627,"task, either 40 Rahman and Ng, 2011) and seek the set of entities that optimize the sum of scores; this can also be promoted in a decentralized manner (Durrett et al., 2013). Pairwise models (Bengtson and Roth, 2008; Finkel et al., 2008; Versley et al., 2008), on the other hand, define scores for each pair of mentions to be coreferent, and define the clusters as the transitive closure of these pairwise relations. A disadvantage of these two methods is that they lead to intractable decoding problems, so approximate methods must be used. For comprehensive overviews, see Stoyanov et al. (2009), Ng (2010), Pradhan et al. (2011) and Pradhan et al. (2012). Our joint approach (to be fully described in §4) draws inspiration from recent work that shifts from entity clusters to coreference trees (Fernandes et al., 2012; Durrett and Klein, 2013). These models define scores for each mention to link to its antecedent or to an artifical root symbol $ (in which case it is not anaphoric). The computation of the best tree can be done exactly with spanning tree algorithms, or by independently choosing the best antecedent (or the root) for each mention, if only left-to-right arcs are allowed. The same idea u"
E14-1005,P09-1074,0,0.0125089,"ave been applied to this task, either 40 Rahman and Ng, 2011) and seek the set of entities that optimize the sum of scores; this can also be promoted in a decentralized manner (Durrett et al., 2013). Pairwise models (Bengtson and Roth, 2008; Finkel et al., 2008; Versley et al., 2008), on the other hand, define scores for each pair of mentions to be coreferent, and define the clusters as the transitive closure of these pairwise relations. A disadvantage of these two methods is that they lead to intractable decoding problems, so approximate methods must be used. For comprehensive overviews, see Stoyanov et al. (2009), Ng (2010), Pradhan et al. (2011) and Pradhan et al. (2012). Our joint approach (to be fully described in §4) draws inspiration from recent work that shifts from entity clusters to coreference trees (Fernandes et al., 2012; Durrett and Klein, 2013). These models define scores for each mention to link to its antecedent or to an artifical root symbol $ (in which case it is not anaphoric). The computation of the best tree can be done exactly with spanning tree algorithms, or by independently choosing the best antecedent (or the root) for each mention, if only left-to-right arcs are allowed. The"
E14-1005,D12-1072,0,0.184886,"Missing"
E14-1005,D13-1101,0,0.134677,"Missing"
E14-1005,P08-4003,0,0.0199852,"n, such as a reported speech verb), and the source span (the author of the quote). The same reference introduced the PARC dataset, which we use in our experiments (§6) and which is based on the annotation of a database of attribution relations from the Penn Discourse Treebank (Prasad et al., 2008). Several machine learning algorithms have been applied to this task, either 40 Rahman and Ng, 2011) and seek the set of entities that optimize the sum of scores; this can also be promoted in a decentralized manner (Durrett et al., 2013). Pairwise models (Bengtson and Roth, 2008; Finkel et al., 2008; Versley et al., 2008), on the other hand, define scores for each pair of mentions to be coreferent, and define the clusters as the transitive closure of these pairwise relations. A disadvantage of these two methods is that they lead to intractable decoding problems, so approximate methods must be used. For comprehensive overviews, see Stoyanov et al. (2009), Ng (2010), Pradhan et al. (2011) and Pradhan et al. (2012). Our joint approach (to be fully described in §4) draws inspiration from recent work that shifts from entity clusters to coreference trees (Fernandes et al., 2012; Durrett and Klein, 2013). These model"
E14-1005,pareti-2012-database,0,0.470005,"mining systems are not yet fully satisfactory, even when only direct quotes are considered. Part of the problem, as we next describe, has to do with inaccuracies in coreference resolution (§3). The “easiest” instances of quotation attribution problems arise when the speaker and the quote are semantically connected, e.g., through a reported speech verb like said. However, in newswire text, the subject of this verb is commonly a pronoun or another uninformative anaphoric mention. While the speaker thus determined may well be correct— being in most cases consistent with human annotation choices (Pareti, 2012)—from a practical perspective, it will be of little use without a coreference system that correctly resolves the anaphora. Since the current state of the art in coreference resolution is far from perfect, errors at this stage tend to propagate to the quote attribution system. Consider the following examples for illustration (taken from the WSJ-1057 and WSJ-0089 documents in the Penn Treebank), where we have annotated with subscripts some of the mentions: We address the problem of automatically attributing quotations to speakers, which has great relevance in text mining and media monitoring app"
E14-1005,W11-1901,0,0.268187,"ghighi and Klein (2007) and distinguish between proper, nominal, and pronominal mentions. Each requires different types of information to be resolved. Thus, the task involves determining anaphoricity, resolving pronouns, and identifying semantic compatibility among mentions. To resolve these references, one typically exploits contextual and grammatical clues, as well as semantic information and world knowledge, to understand whether mentions refer to people, places, organizations, and so on. The importance of coreference resolution has led to it being the subject of recent CoNLL shared tasks (Pradhan et al., 2011; Pradhan et al., 2012). There has been a variety of approaches for this problem. Early work used local discriminative classifiers, making independent decisions for each mention or pair of mentions (Soon et al., 2001; Ng and Cardie, 2002). Lee et al. (2011) proposed a competitive non-learned sieve-based method, which constructs clusters by aglomerating mentions in a greedy manner. Entity-centric models define scores for the entire entity clusters (Culotta et al., 2007; Haghighi and Klein, 2010; Quotation Attribution The task of quotation attribution can be formally defined as follows. Given a"
E14-1005,W12-4501,0,0.0878636,") and distinguish between proper, nominal, and pronominal mentions. Each requires different types of information to be resolved. Thus, the task involves determining anaphoricity, resolving pronouns, and identifying semantic compatibility among mentions. To resolve these references, one typically exploits contextual and grammatical clues, as well as semantic information and world knowledge, to understand whether mentions refer to people, places, organizations, and so on. The importance of coreference resolution has led to it being the subject of recent CoNLL shared tasks (Pradhan et al., 2011; Pradhan et al., 2012). There has been a variety of approaches for this problem. Early work used local discriminative classifiers, making independent decisions for each mention or pair of mentions (Soon et al., 2001; Ng and Cardie, 2002). Lee et al. (2011) proposed a competitive non-learned sieve-based method, which constructs clusters by aglomerating mentions in a greedy manner. Entity-centric models define scores for the entire entity clusters (Culotta et al., 2007; Haghighi and Klein, 2010; Quotation Attribution The task of quotation attribution can be formally defined as follows. Given a document containing a s"
E14-1005,prasad-etal-2008-penn,0,0.0128414,"rect quotations (Pareti et al., 2013), which are more involved and require syntactic or semantic patterns. In this work, we resort to direct and mixed quotations. Pareti (2012) defines quotation attributions in terms of their content span (the quotation text itself), their cue (a lexical anchor of the attribution relation, such as a reported speech verb), and the source span (the author of the quote). The same reference introduced the PARC dataset, which we use in our experiments (§6) and which is based on the annotation of a database of attribution relations from the Penn Discourse Treebank (Prasad et al., 2008). Several machine learning algorithms have been applied to this task, either 40 Rahman and Ng, 2011) and seek the set of entities that optimize the sum of scores; this can also be promoted in a decentralized manner (Durrett et al., 2013). Pairwise models (Bengtson and Roth, 2008; Finkel et al., 2008; Versley et al., 2008), on the other hand, define scores for each pair of mentions to be coreferent, and define the clusters as the transitive closure of these pairwise relations. A disadvantage of these two methods is that they lead to intractable decoding problems, so approximate methods must be"
E14-1005,miltsakaki-etal-2004-penn,0,\N,Missing
E14-1005,W12-4502,0,\N,Missing
J14-1002,S07-1018,0,0.666381,"Missing"
J14-1002,boas-2002-bilingual,0,0.0274075,"experiments on recently released FrameNet 1.5 data. In other work based on FrameNet, Matsubayashi, Okazaki, and Tsujii (2009) investigated various uses of FrameNet’s taxonomic relations for learning generalizations over roles; they trained a log-linear model on the SemEval 2007 data to evaluate features for the subtask of argument identification. Another line of work has sought to extend the coverage of FrameNet by exploiting VerbNet and WordNet (Shi and Mihalcea 2005; Giuglea and Moschitti 2006; Pennacchiotti et al. 2008) and by projecting entries and annotations within and across languages (Boas 2002; Fung and Chen 2004; Pado ¨ and Lapata 2005; Furstenau and Lapata 2009b). Others have explored the application of frame-semantic structures to tasks such as information extraction (Moschitti, Morarescu, and Harabagiu 2003; Surdeanu et al. 2003), textual entailment (Burchardt and Frank 2006; Burchardt et al. 2009), question answering (Narayanan and Harabagiu 2004; Shen and Lapata 2007), and paraphrase recognition (Pado´ and Erk 2005). 2.3 Semi-Supervised Methods Although there has been a significant amount of work in supervised shallow semantic parsing using both PropBank- and FrameNet-style r"
J14-1002,W04-2412,0,0.0162126,"Missing"
J14-1002,W05-0620,0,0.105832,"Missing"
J14-1002,D11-1003,0,0.0540671,"kled with off-the-shelf solvers, Rush et al. (2010) proposed subgradient-based dual decomposition (also called Lagrangian relaxation) as a way of exploiting the structure of the problem and existing combinatorial algorithms. The method allows the combination of models that are individually tractable, but not jointly tractable, by solving a relaxation of the original problem. Since then, dual decomposition has been used to build more accurate models for dependency parsing (Koo et al. 2010), combinatory categorical grammar supertagging and parsing (Auli and Lopez 2011), and machine translation (Chang and Collins 2011; DeNero and Macherey 2011; Rush and Collins 2011). Recently, Martins et al. (2011b) showed that the success of subgradient-based dual decomposition strongly relies on breaking down the original problem into a “good” 4 In prior work, there are exceptions where identification and classification of arguments have been treated in one step; for more details, please refer to the systems participating in the CoNLL-2004 shared task on semantic role labeling (Carreras and M`arquez 2004). 16 Das et al. Frame-Semantic Parsing decomposition, that is, one with few overlapping components. This leaves out m"
J14-1002,S10-1059,1,0.89659,"y filled; in the SemEval 2007 development data, the average number of roles an evoked frame defines is 6.7, but the average number of overt arguments is only 1.7.29 In 29 In the annotated data, each core role is filled with one of three types of null instantiations indicating how the role is conveyed implicitly. For instance, the imperative construction implicitly designates a role as filled by the addressee, and the corresponding filler is thus CNI (constructional null instantiation). In this work we do not distinguish different types of null instantiation. The interested reader may refer to Chen et al. (2010), who handle the different types of null instantions during argument identification. 31 Computational Linguistics Volume 40, Number 1 training, if a labeled argument is not a subtree of the dependency parse, we add its span to S.30 Let Ai denote the mapping of roles in Rfi to spans in S. Our model makes a prediction for each Ai (rk ) (for all roles rk ∈ Rfi ) using: Ai (rk ) ← argmax pψ (s |rk , fi , ti , x) s∈S (7) We use a conditional log-linear model over spans for each role of each evoked frame: exp ψ h(s, rk , fi , ti , x) pψ (Ai (rk ) = s |fi , ti , x) =  exp ψ h(s , rk , fi , ti , x"
J14-1002,S12-1029,1,0.386125,"Missing"
J14-1002,P11-1061,1,0.647271,"Missing"
J14-1002,N10-1138,1,0.945675,"sition algorithm (Section 7) that collectively predicts all the arguments of a frame together, thereby incorporating linguistic constraints in a principled fashion. Our open-source parser, named SEMAFOR (Semantic Analyzer of Frame Representations)1 achieves the best published results to date on the SemEval 2007 frame-semantic structure extraction task (Baker, Ellsworth, and Erk 2007). Herein, we also present results on newly released data with FrameNet 1.5, the latest edition of the lexicon. Some of the material presented in this article has appeared in previously published conference papers: Das et al. (2010) presented the basic model, Das and Smith (2011) described semi-supervised lexicon expansion, Das and Smith (2012) demonstrated a sparse variant of lexicon expansion, and Das, Martins, and Smith (2012) presented the dual decomposition algorithm for constrained joint argument identification. We present here a synthesis of those results and several additional details: 1. The set of features used in the two statistical models for frame identification and argument identification. 2. Details of a greedy beam search algorithm for argument identification that avoids illegal argument overlap. 3. Error"
J14-1002,P11-1144,1,0.926493,"y predicts all the arguments of a frame together, thereby incorporating linguistic constraints in a principled fashion. Our open-source parser, named SEMAFOR (Semantic Analyzer of Frame Representations)1 achieves the best published results to date on the SemEval 2007 frame-semantic structure extraction task (Baker, Ellsworth, and Erk 2007). Herein, we also present results on newly released data with FrameNet 1.5, the latest edition of the lexicon. Some of the material presented in this article has appeared in previously published conference papers: Das et al. (2010) presented the basic model, Das and Smith (2011) described semi-supervised lexicon expansion, Das and Smith (2012) demonstrated a sparse variant of lexicon expansion, and Das, Martins, and Smith (2012) presented the dual decomposition algorithm for constrained joint argument identification. We present here a synthesis of those results and several additional details: 1. The set of features used in the two statistical models for frame identification and argument identification. 2. Details of a greedy beam search algorithm for argument identification that avoids illegal argument overlap. 3. Error analysis pertaining to the dual decomposition a"
J14-1002,N12-1086,1,0.870084,"ating linguistic constraints in a principled fashion. Our open-source parser, named SEMAFOR (Semantic Analyzer of Frame Representations)1 achieves the best published results to date on the SemEval 2007 frame-semantic structure extraction task (Baker, Ellsworth, and Erk 2007). Herein, we also present results on newly released data with FrameNet 1.5, the latest edition of the lexicon. Some of the material presented in this article has appeared in previously published conference papers: Das et al. (2010) presented the basic model, Das and Smith (2011) described semi-supervised lexicon expansion, Das and Smith (2012) demonstrated a sparse variant of lexicon expansion, and Das, Martins, and Smith (2012) presented the dual decomposition algorithm for constrained joint argument identification. We present here a synthesis of those results and several additional details: 1. The set of features used in the two statistical models for frame identification and argument identification. 2. Details of a greedy beam search algorithm for argument identification that avoids illegal argument overlap. 3. Error analysis pertaining to the dual decomposition argument identification algorithm, in contrast with the beam search"
J14-1002,P11-1043,0,0.0102251,"solvers, Rush et al. (2010) proposed subgradient-based dual decomposition (also called Lagrangian relaxation) as a way of exploiting the structure of the problem and existing combinatorial algorithms. The method allows the combination of models that are individually tractable, but not jointly tractable, by solving a relaxation of the original problem. Since then, dual decomposition has been used to build more accurate models for dependency parsing (Koo et al. 2010), combinatory categorical grammar supertagging and parsing (Auli and Lopez 2011), and machine translation (Chang and Collins 2011; DeNero and Macherey 2011; Rush and Collins 2011). Recently, Martins et al. (2011b) showed that the success of subgradient-based dual decomposition strongly relies on breaking down the original problem into a “good” 4 In prior work, there are exceptions where identification and classification of arguments have been treated in one step; for more details, please refer to the systems participating in the CoNLL-2004 shared task on semantic role labeling (Carreras and M`arquez 2004). 16 Das et al. Frame-Semantic Parsing decomposition, that is, one with few overlapping components. This leaves out many declarative constraine"
J14-1002,D09-1003,0,0.0126412,"een verbs using a graph alignment method; this method represents sentences and their syntactic analysis as graphs and graph alignment is used to project annotations from seed examples to unlabeled sentences. This alignment problem is again modeled as a linear program. ¨ Furstenau and Lapata (2012) present an detailed expansion of the aforementioned papers. Although this line of work presents a novel direction in the area of SRL, the published approach does not yet deal with non-verbal predicates and does not evaluate the presented methods on the full text annotations of the FrameNet releases. Deschacht and Moens (2009) present a technique of incorporating additional information from unlabeled data by using a latent words language model. Latent variables are used to model the underlying representation of words, and parameters of this model 15 Computational Linguistics Volume 40, Number 1 are estimated using standard unsupervised methods. Next, the latent information is used as features for an SRL model. Improvements over supervised SRL techniques are observed with the augmentation of these extra features. The authors also compare ¨ their method with the aforementioned two methods of Furstenau and Lapata (200"
J14-1002,W03-1007,0,0.112387,"Missing"
J14-1002,C04-1134,0,0.0437403,"on recently released FrameNet 1.5 data. In other work based on FrameNet, Matsubayashi, Okazaki, and Tsujii (2009) investigated various uses of FrameNet’s taxonomic relations for learning generalizations over roles; they trained a log-linear model on the SemEval 2007 data to evaluate features for the subtask of argument identification. Another line of work has sought to extend the coverage of FrameNet by exploiting VerbNet and WordNet (Shi and Mihalcea 2005; Giuglea and Moschitti 2006; Pennacchiotti et al. 2008) and by projecting entries and annotations within and across languages (Boas 2002; Fung and Chen 2004; Pado ¨ and Lapata 2005; Furstenau and Lapata 2009b). Others have explored the application of frame-semantic structures to tasks such as information extraction (Moschitti, Morarescu, and Harabagiu 2003; Surdeanu et al. 2003), textual entailment (Burchardt and Frank 2006; Burchardt et al. 2009), question answering (Narayanan and Harabagiu 2004; Shen and Lapata 2007), and paraphrase recognition (Pado´ and Erk 2005). 2.3 Semi-Supervised Methods Although there has been a significant amount of work in supervised shallow semantic parsing using both PropBank- and FrameNet-style representations, a fe"
J14-1002,P10-1160,0,0.00634321,"ables z are binary. Here, apart from the ILP formulation, we will consider the following relaxation of Equation (11), which replaces the binary constraint z ∈ {0, 1}d by a unit interval constraint z ∈ [0, 1]d , yielding a linear program: maximize  c(r, s) × zr,s r∈Rf s∈S with respect to such that z ∈ [0, 1]d Az ≤ b. (17) 42 We noticed that, in the annotated data, in some cases, the “requires” constraint is violated by the FrameNet annotators. This happens mostly when one of the required roles is absent in the sentence containing the target, but is rather instantiated in an earlier sentence (Gerber and Chai 2010). We apply the hard constraint in Equation (16), though extending our algorithm to seek arguments outside the sentence is straightforward. For preliminary work extending SEMAFOR this way, see Chen et al. (2010). 41 Computational Linguistics Volume 40, Number 1 There are several LP and ILP solvers available, and a great deal of effort has been spent by the optimization community to devise efficient generic solvers. An example is CPLEX, a state-of-the-art solver for mixed integer programming that we use as a baseline to solve the ILP in Equation (11) as well as its LP relaxation in Equation (17)"
J14-1002,J02-3001,0,0.989881,"012; accepted for publication: 22 December 2012. doi:10.1162/COLI a 00163 © 2014 Association for Computational Linguistics Computational Linguistics Volume 40, Number 1 1. Introduction FrameNet (Fillmore, Johnson, and Petruck 2003) is a linguistic resource storing considerable information about lexical and predicate-argument semantics in English. Grounded in the theory of frame semantics (Fillmore 1982), it suggests—but does not formally define—a semantic representation that blends representations familiar from word-sense disambiguation (Ide and V´eronis 1998) and semantic role labeling (SRL; Gildea and Jurafsky 2002). Given the limited size of available resources, accurately producing richly structured frame-semantic structures with high coverage will require data-driven techniques beyond simple supervised classification, such as latent variable modeling, semi-supervised learning, and joint inference. In this article, we present a computational and statistical model for frame-semantic parsing, the problem of extracting from text semantic predicate-argument structures such as those shown in Figure 1. We aim to predict a frame-semantic representation with two statistical models rather than a collection of l"
J14-1002,S07-1003,0,0.00995843,"Missing"
J14-1002,W09-1201,0,0.0598567,"Missing"
J14-1002,J98-1001,0,0.0264943,"Missing"
J14-1002,S07-1048,0,0.102556,"s handling many more labels, and resulting in richer frame-semantic parses. Recent work in frame-semantic parsing—in which sentences may contain multiple frames which need to be recognized along with their arguments—was undertaken as the SemEval 2007 task 19 of frame-semantic structure extraction (Baker, Ellsworth, and Erk 2007). This task leveraged FrameNet 1.3, and also released a small corpus 3 Available at http://framenet.icsi.berkeley.edu as of 19 January 2013. 14 Das et al. Frame-Semantic Parsing containing a little more than 2,000 sentences with full text annotations. The LTH system of Johansson and Nugues (2007), which we use as our baseline (Section 3.4), had the best performance in the SemEval 2007 task in terms of full frame-semantic parsing. Johansson and Nugues broke down the task as identifying targets that could evoke frames in a sentence, identifying the correct semantic frame for a target, and finally determining the arguments that fill the semantic roles of a frame. They used a series of SVMs to classify the frames for a given target, associating unseen lexical items to frames and identifying and classifying token spans as various semantic roles. Both the full text annotation corpus as well"
J14-1002,D08-1008,0,0.0243643,"Missing"
J14-1002,kingsbury-palmer-2002-treebank,0,0.713501,"ly discuss work done on PropBank-style semantic role labeling, following which we will concentrate on the more relevant problem of frame-semantic structure extraction. Next, we review previous work that has used semi-supervised learning for shallow semantic parsing. Finally, we discuss prior work on joint structure prediction relevant to frame-semantic parsing. 2.1 Semantic Role Labeling Since Gildea and Jurafsky (2002) pioneered statistical semantic role labeling, there has been a great deal of computational work using predicate-argument structures for semantics. The development of PropBank (Kingsbury and Palmer 2002), followed by CoNLL shared tasks on semantic role labeling (Carreras and M`arquez 2004, 2005) boosted research in this area. Figure 2(a) shows an annotation from PropBank. PropBank annotations are closely tied to syntax, because the data set consists of the 1 See http://www.ark.cs.cmu.edu/SEMAFOR. 11 Computational Linguistics Volume 40, Number 1 (a) (b) Figure 2 (a) A phrase-structure tree taken from the Penn Treebank and annotated with PropBank predicate-argument structures. The verbs created and pushed serve as predicates in this sentence. Dotted arrows connect each predicate to its semantic"
J14-1002,D10-1125,0,0.0072494,", as well as PropBank SRL (Punyakanok et al. 2004). Whereas early work in this area focused on declarative formulations tackled with off-the-shelf solvers, Rush et al. (2010) proposed subgradient-based dual decomposition (also called Lagrangian relaxation) as a way of exploiting the structure of the problem and existing combinatorial algorithms. The method allows the combination of models that are individually tractable, but not jointly tractable, by solving a relaxation of the original problem. Since then, dual decomposition has been used to build more accurate models for dependency parsing (Koo et al. 2010), combinatory categorical grammar supertagging and parsing (Auli and Lopez 2011), and machine translation (Chang and Collins 2011; DeNero and Macherey 2011; Rush and Collins 2011). Recently, Martins et al. (2011b) showed that the success of subgradient-based dual decomposition strongly relies on breaking down the original problem into a “good” 4 In prior work, there are exceptions where identification and classification of arguments have been treated in one step; for more details, please refer to the systems participating in the CoNLL-2004 shared task on semantic role labeling (Carreras and M`"
J14-1002,N10-1137,0,0.00679874,"d Lapata (2009a, 2009b) and show relative improvements. Experiments are performed on the CoNLL 2008 shared task data set (Surdeanu et al. 2008), which follows the PropBank conventions and only labels verbal and nominal predicates—in contrast to our work, which includes most lexicosyntactic categories. A similar approach is presented by Weston, Ratle, and Collobert (2008), who use neural embeddings of words, which are eventually used for SRL; improvements over state-of-the-art PropBank-style SRL systems are observed. Recently, there has been related work in unsupervised semantic role labeling (Lang and Lapata 2010, 2011; Titov and Klementiev 2012) that attempts to induce semantic roles automatically from unannotated data. This line of work may be useful in discovering new semantic frames and roles, but here we stick to the concrete representation provided in FrameNet, without seeking to expand its inventory of semantic types. We present a new semi-supervised technique to expand the set of lexical items with the potential semantic frames that they could evoke; we use a graph-based semi-supervised learning framework to achieve this goal (Section 5.5). 2.4 Joint Inference and Shallow Semantic Parsing Most"
J14-1002,D11-1122,0,0.0108005,"Missing"
J14-1002,P93-1016,0,0.0449607,"e constraints on frame identification. 5.5.1 Graph Construction. We construct a graph with lexical units as vertices. Thus, each vertex corresponds to a lemmatized word or phrase appended with a coarse POS tag. We use two resources for graph construction. First, we take all the words and phrases present in a dependency-based thesaurus constructed using syntactic cooccurrence statistics (Lin 1998), and aggregate words and phrases that share the same lemma and coarse POS tag. To construct this resource, Lin used a corpus containing 64 million words that was parsed with a fast dependency parser (Lin 1993, 1994), and syntactic contexts were used to find similar lexical items for a given word or phrase. Lin separately treated nouns, verbs, and adjectives/adverbs, so these form the three parts of the thesaurus. This resource gave us a list of possible LUs, much larger in size than the LUs present in FrameNet data. The second component of graph construction comes from FrameNet itself. We scanned the exemplar sentences in FrameNet 1.5 and the training section of the full text annotations and gathered a distribution over frames for each LU appearing in FrameNet data. For a pair of LUs, we measured"
J14-1002,C94-1079,0,0.14144,"Missing"
J14-1002,P98-2127,0,0.0298059,"ed learning of POS taggers by using bilingual graph-based projections (Das and Petrov 2011). We describe our approach to graph construction, propagation for lexicon expansion, and the use of the result to impose constraints on frame identification. 5.5.1 Graph Construction. We construct a graph with lexical units as vertices. Thus, each vertex corresponds to a lemmatized word or phrase appended with a coarse POS tag. We use two resources for graph construction. First, we take all the words and phrases present in a dependency-based thesaurus constructed using syntactic cooccurrence statistics (Lin 1998), and aggregate words and phrases that share the same lemma and coarse POS tag. To construct this resource, Lin used a corpus containing 64 million words that was parsed with a fast dependency parser (Lin 1993, 1994), and syntactic contexts were used to find similar lexical items for a given word or phrase. Lin separately treated nouns, verbs, and adjectives/adverbs, so these form the three parts of the thesaurus. This resource gave us a list of possible LUs, much larger in size than the LUs present in FrameNet data. The second component of graph construction comes from FrameNet itself. We sca"
J14-1002,S07-1005,0,0.0401753,"Missing"
J14-1002,J93-2004,0,0.0478041,"Missing"
J14-1002,J08-2001,0,0.0552021,"Missing"
J14-1002,D11-1022,1,0.788077,"Missing"
J14-1002,P09-1039,1,0.427643,"Missing"
J14-1002,D10-1004,1,0.864365,"r knowledge, the separate line of work investigating frame-semantic parsing has not previously dealt with joint inference. A common trait in prior work, both in PropBank and FrameNet conventions, has been the use of a two-stage model that identifies arguments first, then labels them, often using dynamic programming or integer linear programs (ILPs); we treat both problems together here.4 Recent work in natural language processing (NLP) problems has focused on ILP formulations for complex structure prediction tasks like dependency parsing (Riedel and Clarke 2006; Martins, Smith, and Xing 2009; Martins et al. 2010), sequence tagging (Roth and Yih 2004), as well as PropBank SRL (Punyakanok et al. 2004). Whereas early work in this area focused on declarative formulations tackled with off-the-shelf solvers, Rush et al. (2010) proposed subgradient-based dual decomposition (also called Lagrangian relaxation) as a way of exploiting the structure of the problem and existing combinatorial algorithms. The method allows the combination of models that are individually tractable, but not jointly tractable, by solving a relaxation of the original problem. Since then, dual decomposition has been used to build more ac"
J14-1002,P09-1003,0,0.0644821,"Missing"
J14-1002,P05-1012,0,0.0484643,"Missing"
J14-1002,W04-2705,0,0.72468,".v, ... Inheritance relation Causative_of relation Excludes relation Figure 3 Partial illustration of frames, roles, and lexical units related to the C AUSE TO MAKE NOISE frame, from the FrameNet lexicon. Core roles are filled bars. Non-core roles (such as Place and Time) are unfilled bars. No particular significance is ascribed to the ordering of a frame’s roles in its lexicon entry (the selection and ordering of roles above is for illustrative convenience). C AUSE TO MAKE NOISE defines a total of 14 roles, many of them not shown here. Whereas PropBank contains verbal predicates and NomBank (Meyers et al. 2004) contains nominal predicates, FrameNet counts these as well as allowing adjectives, adverbs, and prepositions among its lexical units. Finally, FrameNet frames organize predicates according to semantic principles, both by allowing related terms to evoke a common frame (e.g., push.V, raise.V, and growth.N for C AUSE CHANGE POSITION ON A SCALE) and by defining frames and their roles within a hierarchy (see Figure 3). PropBank does not explicitly encode relationships among predicates. Most early work on frame-semantic parsing has made use of the exemplar sentences in the FrameNet corpus (see Sect"
J14-1002,C04-1100,0,0.0140332,"n. Another line of work has sought to extend the coverage of FrameNet by exploiting VerbNet and WordNet (Shi and Mihalcea 2005; Giuglea and Moschitti 2006; Pennacchiotti et al. 2008) and by projecting entries and annotations within and across languages (Boas 2002; Fung and Chen 2004; Pado ¨ and Lapata 2005; Furstenau and Lapata 2009b). Others have explored the application of frame-semantic structures to tasks such as information extraction (Moschitti, Morarescu, and Harabagiu 2003; Surdeanu et al. 2003), textual entailment (Burchardt and Frank 2006; Burchardt et al. 2009), question answering (Narayanan and Harabagiu 2004; Shen and Lapata 2007), and paraphrase recognition (Pado´ and Erk 2005). 2.3 Semi-Supervised Methods Although there has been a significant amount of work in supervised shallow semantic parsing using both PropBank- and FrameNet-style representations, a few improve¨ ments over vanilla supervised methods using unlabeled data are notable. Furstenau and Lapata (2009b) present a method of projecting predicate-argument structures from some seed examples to unlabeled sentences, and use a linear program formulation to find the best alignment explaining the projection. Next, the projected information a"
J14-1002,H05-1108,0,0.0829981,"Missing"
J14-1002,D08-1048,0,0.100472,"Missing"
J14-1002,N04-1030,0,0.00857398,"-DIR, and ARGM-TMP are shown in the figure. PropBank defines core roles ARG0 through ARG5, which receive different interpretations for different predicates. Additional modifier roles ARGM-* include ARGM-TMP (temporal) and ARGM-DIR (directional), as shown in Figure 2(a). The PropBank representation therefore has a small number of roles, and the training data set comprises some 40,000 sentences, thus making the semantic role labeling task an attractive one from the perspective of machine learning. There are many instances of influential work on semantic role labeling using PropBank conventions. Pradhan et al. (2004) present a system that uses support vector machines (SVMs) to identify the arguments in a syntax tree that can serve as semantic roles, followed by classification of the identified arguments to role names via a collection of binary SVMs. Punyakanok et al. (2004) describe a semantic role labeler that uses integer linear programming for inference and uses several global constraints to find the best 12 Das et al. Frame-Semantic Parsing suited predicate-argument structures. Joint modeling for semantic role labeling with discriminative log-linear models is presented by Toutanova, Haghighi, and Mann"
J14-1002,J08-2005,0,0.157863,"Missing"
J14-1002,C04-1197,0,0.401501,"igure 2(a). The PropBank representation therefore has a small number of roles, and the training data set comprises some 40,000 sentences, thus making the semantic role labeling task an attractive one from the perspective of machine learning. There are many instances of influential work on semantic role labeling using PropBank conventions. Pradhan et al. (2004) present a system that uses support vector machines (SVMs) to identify the arguments in a syntax tree that can serve as semantic roles, followed by classification of the identified arguments to role names via a collection of binary SVMs. Punyakanok et al. (2004) describe a semantic role labeler that uses integer linear programming for inference and uses several global constraints to find the best 12 Das et al. Frame-Semantic Parsing suited predicate-argument structures. Joint modeling for semantic role labeling with discriminative log-linear models is presented by Toutanova, Haghighi, and Manning (2005), where global features looking at all arguments of a particular verb together are incorporated into a dynamic programming and reranking framework. The Computational Linguistics special issue on semantic role labeling (M`arquez et al. 2008) includes ot"
J14-1002,W96-0213,0,0.464731,"Missing"
J14-1002,W06-1616,0,0.0176524,"or semantic role labeling (M`arquez et al. 2008). To our knowledge, the separate line of work investigating frame-semantic parsing has not previously dealt with joint inference. A common trait in prior work, both in PropBank and FrameNet conventions, has been the use of a two-stage model that identifies arguments first, then labels them, often using dynamic programming or integer linear programs (ILPs); we treat both problems together here.4 Recent work in natural language processing (NLP) problems has focused on ILP formulations for complex structure prediction tasks like dependency parsing (Riedel and Clarke 2006; Martins, Smith, and Xing 2009; Martins et al. 2010), sequence tagging (Roth and Yih 2004), as well as PropBank SRL (Punyakanok et al. 2004). Whereas early work in this area focused on declarative formulations tackled with off-the-shelf solvers, Rush et al. (2010) proposed subgradient-based dual decomposition (also called Lagrangian relaxation) as a way of exploiting the structure of the problem and existing combinatorial algorithms. The method allows the combination of models that are individually tractable, but not jointly tractable, by solving a relaxation of the original problem. Since th"
J14-1002,W04-2401,0,0.0218668,"nvestigating frame-semantic parsing has not previously dealt with joint inference. A common trait in prior work, both in PropBank and FrameNet conventions, has been the use of a two-stage model that identifies arguments first, then labels them, often using dynamic programming or integer linear programs (ILPs); we treat both problems together here.4 Recent work in natural language processing (NLP) problems has focused on ILP formulations for complex structure prediction tasks like dependency parsing (Riedel and Clarke 2006; Martins, Smith, and Xing 2009; Martins et al. 2010), sequence tagging (Roth and Yih 2004), as well as PropBank SRL (Punyakanok et al. 2004). Whereas early work in this area focused on declarative formulations tackled with off-the-shelf solvers, Rush et al. (2010) proposed subgradient-based dual decomposition (also called Lagrangian relaxation) as a way of exploiting the structure of the problem and existing combinatorial algorithms. The method allows the combination of models that are individually tractable, but not jointly tractable, by solving a relaxation of the original problem. Since then, dual decomposition has been used to build more accurate models for dependency parsing ("
J14-1002,P11-1008,0,0.0341732,") proposed subgradient-based dual decomposition (also called Lagrangian relaxation) as a way of exploiting the structure of the problem and existing combinatorial algorithms. The method allows the combination of models that are individually tractable, but not jointly tractable, by solving a relaxation of the original problem. Since then, dual decomposition has been used to build more accurate models for dependency parsing (Koo et al. 2010), combinatory categorical grammar supertagging and parsing (Auli and Lopez 2011), and machine translation (Chang and Collins 2011; DeNero and Macherey 2011; Rush and Collins 2011). Recently, Martins et al. (2011b) showed that the success of subgradient-based dual decomposition strongly relies on breaking down the original problem into a “good” 4 In prior work, there are exceptions where identification and classification of arguments have been treated in one step; for more details, please refer to the systems participating in the CoNLL-2004 shared task on semantic role labeling (Carreras and M`arquez 2004). 16 Das et al. Frame-Semantic Parsing decomposition, that is, one with few overlapping components. This leaves out many declarative constrained problems, for which su"
J14-1002,D10-1001,0,0.0101386,"Missing"
J14-1002,N03-1028,0,0.0145026,"labels (Subramanya and Bilmes 2008, 2009; Talukdar and Crammer 2009). For us, graph vertices correspond to natural language types (not tokens) and undirected edges between them are weighted using a similarity metric. Recently, this set-up has been used to learn soft labels on natural language types (say, word n-grams or in our case, syntactically disambiguated 27 The use of unsupported features (i.e., those that can fire for an analysis in the partition function but not observed to fire in the training data) has been observed to give performance improvements in NLP problems; see, for example, Sha and Pereira (2003) and Martins et al. (2010). 27 Computational Linguistics Volume 40, Number 1 predicates) from seed data, resulting in large but noisy lexicons, which are used to constrain structured prediction models. Applications have ranged from domain adaptation of sequence models (Subramanya, Petrov, and Pereira 2010) to unsupervised learning of POS taggers by using bilingual graph-based projections (Das and Petrov 2011). We describe our approach to graph construction, propagation for lexicon expansion, and the use of the result to impose constraints on frame identification. 5.5.1 Graph Construction. We c"
J14-1002,D07-1002,0,0.273124,"ught to extend the coverage of FrameNet by exploiting VerbNet and WordNet (Shi and Mihalcea 2005; Giuglea and Moschitti 2006; Pennacchiotti et al. 2008) and by projecting entries and annotations within and across languages (Boas 2002; Fung and Chen 2004; Pado ¨ and Lapata 2005; Furstenau and Lapata 2009b). Others have explored the application of frame-semantic structures to tasks such as information extraction (Moschitti, Morarescu, and Harabagiu 2003; Surdeanu et al. 2003), textual entailment (Burchardt and Frank 2006; Burchardt et al. 2009), question answering (Narayanan and Harabagiu 2004; Shen and Lapata 2007), and paraphrase recognition (Pado´ and Erk 2005). 2.3 Semi-Supervised Methods Although there has been a significant amount of work in supervised shallow semantic parsing using both PropBank- and FrameNet-style representations, a few improve¨ ments over vanilla supervised methods using unlabeled data are notable. Furstenau and Lapata (2009b) present a method of projecting predicate-argument structures from some seed examples to unlabeled sentences, and use a linear program formulation to find the best alignment explaining the projection. Next, the projected information as well as the seeds are"
J14-1002,W04-2008,0,0.0362235,"r roles within a hierarchy (see Figure 3). PropBank does not explicitly encode relationships among predicates. Most early work on frame-semantic parsing has made use of the exemplar sentences in the FrameNet corpus (see Section 3.1), each of which is annotated for a single frame and its arguments. Gildea and Jurafsky (2002) presented a discriminative model for arguments given the frame; Thompson, Levy, and Manning (2003) used a generative model for both the frame and its arguments. Fleischman, Kwon, and Hovy (2003) first used maximum entropy models to find and label arguments given the frame. Shi and Mihalcea (2004) developed a rule-based system to predict frames and their arguments in text, and Erk and Pado´ (2006) introduced the Shalmaneser tool, which uses naive Bayes classifiers to do the same. Other FrameNet SRL systems (Giuglea and Moschitti 2006, for instance) have used SVMs. Most of this work was done on an older, smaller version of FrameNet, containing around 300 frames and fewer than 500 unique semantic roles. Unlike this body of work, we experimented with the larger SemEval 2007 shared task data set, and also the newer FrameNet 1.5,3 which lists 877 frames and 1,068 role types—thus handling ma"
J14-1002,D08-1016,0,0.00944644,"Missing"
J14-1002,D08-1114,0,0.00506907,"en shown to perform better than several other semi-supervised algorithms ¨ on benchmark data sets (Chapelle, Scholkopf, and Zien 2006, chapter 21). The method constructs a graph where a small portion of vertices correspond to labeled instances, and the rest are unlabeled. Pairs of vertices are connected by weighted edges denoting the similarity between the pair. Traditionally, Markov random walks (Szummer and Jaakkola 2001; Baluja et al. 2008) or optimization of a loss function based on smoothness properties of the graph (e.g., Corduneanu and Jaakkola 2003; Zhu, Ghahramani, and Lafferty 2003; Subramanya and Bilmes 2008) are performed to propagate labels from the labeled vertices to the unlabeled ones. In our work, we are interested in multi-class generalizations of graph-propagation algorithms suitable for NLP applications, where each graph vertex can assume one or more out of many possible labels (Subramanya and Bilmes 2008, 2009; Talukdar and Crammer 2009). For us, graph vertices correspond to natural language types (not tokens) and undirected edges between them are weighted using a similarity metric. Recently, this set-up has been used to learn soft labels on natural language types (say, word n-grams or i"
J14-1002,D10-1017,0,0.0238482,"Missing"
J14-1002,P03-1002,0,0.0695193,"rained a log-linear model on the SemEval 2007 data to evaluate features for the subtask of argument identification. Another line of work has sought to extend the coverage of FrameNet by exploiting VerbNet and WordNet (Shi and Mihalcea 2005; Giuglea and Moschitti 2006; Pennacchiotti et al. 2008) and by projecting entries and annotations within and across languages (Boas 2002; Fung and Chen 2004; Pado ¨ and Lapata 2005; Furstenau and Lapata 2009b). Others have explored the application of frame-semantic structures to tasks such as information extraction (Moschitti, Morarescu, and Harabagiu 2003; Surdeanu et al. 2003), textual entailment (Burchardt and Frank 2006; Burchardt et al. 2009), question answering (Narayanan and Harabagiu 2004; Shen and Lapata 2007), and paraphrase recognition (Pado´ and Erk 2005). 2.3 Semi-Supervised Methods Although there has been a significant amount of work in supervised shallow semantic parsing using both PropBank- and FrameNet-style representations, a few improve¨ ments over vanilla supervised methods using unlabeled data are notable. Furstenau and Lapata (2009b) present a method of projecting predicate-argument structures from some seed examples to unlabeled sentences, and"
J14-1002,E12-1003,0,0.0172185,"show relative improvements. Experiments are performed on the CoNLL 2008 shared task data set (Surdeanu et al. 2008), which follows the PropBank conventions and only labels verbal and nominal predicates—in contrast to our work, which includes most lexicosyntactic categories. A similar approach is presented by Weston, Ratle, and Collobert (2008), who use neural embeddings of words, which are eventually used for SRL; improvements over state-of-the-art PropBank-style SRL systems are observed. Recently, there has been related work in unsupervised semantic role labeling (Lang and Lapata 2010, 2011; Titov and Klementiev 2012) that attempts to induce semantic roles automatically from unannotated data. This line of work may be useful in discovering new semantic frames and roles, but here we stick to the concrete representation provided in FrameNet, without seeking to expand its inventory of semantic types. We present a new semi-supervised technique to expand the set of lexical items with the potential semantic frames that they could evoke; we use a graph-based semi-supervised learning framework to achieve this goal (Section 5.5). 2.4 Joint Inference and Shallow Semantic Parsing Most high-performance SRL systems that"
J14-1002,P05-1073,0,0.0166888,"Missing"
J14-1002,P10-1040,0,0.0142959,"Missing"
J14-1002,W04-3212,0,0.126024,"Missing"
J14-1002,N07-1069,0,0.0626288,"Missing"
J14-1002,erk-pado-2006-shalmaneser,0,\N,Missing
J14-1002,W08-2121,0,\N,Missing
J14-1002,E09-1026,0,\N,Missing
J14-1002,D09-1002,0,\N,Missing
J14-1002,P11-1048,0,\N,Missing
J14-1002,J12-1005,0,\N,Missing
J14-1002,C98-2122,0,\N,Missing
J14-1002,P10-2069,0,\N,Missing
N12-4002,N12-4002,1,0.0513054,"Missing"
N19-1313,N18-1118,0,0.0945833,"(Miculicich Werlen and Popescu-Belis, 2017) to name a few. Most of these approaches are, however, restrictive as they mostly involve using handcrafted features similar to the conventional MT approaches. Document-level Neural MT The works here can again be divided into two categories: online— use previous context only, and offline—use both past and future contexts. Most works fall into the former category, with those that use only a single 3099 previous sentence in the source (Jean et al., 2017; Tiedemann and Scherrer, 2017; Voita et al., 2018); one previous sentence both in source and target (Bawden et al., 2018); more than one previous source sentence (Wang et al., 2017; Zhang et al., 2018); or a few previous source and target sentences (Miculicich et al., 2018). Apart from fixing the context length, there are few works which use cache-based memories to store contextual information (Tu et al., 2018; Kuang et al., 2018) and use that to improve the MT system performance. A recent work (Maruf et al., 2018) reports promising results when using the complete history for translating online conversations. For the offline setting, however, there is only one work that effectively uses the full documentcontext"
N19-1313,2012.eamt-1.60,0,0.215051,"both encoder and decoder as it would have redundant information from the source (the context incorporated in the decoder is bilingual), in addition to increasing the complexity of the model. 3095 Domain TED News Europarl #Sentences 0.21M/9K/2.3K 0.24M/2K/3K 1.67M/3.6K/5.1K Document length 120.89/96.42/98.74 38.93/26.78/19.35 14.14/14.95/14.06 Table 1: Training/development/test corpora statistics: number of sentences (K stands for thousands and M for millions), and average document length (in sentences). in genre, style and level of formality: • TED This corpus is from the IWSLT 2017 MT track (Cettolo et al., 2012) and contains transcripts of TED talks aligned at sentence level. Each talk is considered to be a document. We combine tst2016-2017 into the test set and the rest are used for development. • News-Commentary We obtain the sentencealigned document-delimited News Commentary v11 corpus for training.6 The WMT’16 newstest2015 and news-test2016 are used for development and testing, respectively. Figure 3: Decoder-side context integration. representations in that sentence. The queries Qw , Qs are linear transformations of the output of the Lth encoder layer which are then matched with the correspondin"
N19-1313,W14-4012,0,0.130924,"Missing"
N19-1313,P11-2031,0,0.0690388,"memory.10 7 https://github.com/duyvuleo/Transformer-DyNet The code is available at https://github.com/ sameenmaruf/selective-attn 8 9 We found this configuration to be much more stable than using 6 layers with almost no difference in performance as reported by Xia et al. (2018). 10 The experiments can also be run on GPUs with 1012GBs of memory by reducing the batch size at the expense Evaluation Metrics For evaluation, we use BLEU (Papineni et al., 2002) and Meteor (Lavie and Agarwal, 2007) scores on tokenised text, and measure statistical significance with respect to the baselines, p < 0.05 (Clark et al., 2011). 4.2 Main Results We divide our experiments into two parts: offline and online document MT. Offline Document MT From the scores of the two context-agnostic baselines in Table 2, we can see that the Transformer beats the RNNSearch model in all cases by atleast +2.5 BLEU and +2.1 Meteor scores showing that our hyperparameter choice for the Transformer is indeed effective. For the Encoder Context integration, our Hierarchical Attention models perform the (near) best for News and Europarl datasets with +1.98 and +1 BLEU and +1.99 and +0.82 Meteor improvements with respect to the Transformer. For"
N19-1313,N16-1102,1,0.857967,"fline document MT, we have two context-agnostic baselines: (i) a modified version of RNNSearch (Bahdanau et al., 2015), which incorporates dropout on the output layer and improves the attention model by feeding the previously generated word, and (ii) the stateof-the-art Transformer architecture. For the online case, we again have the Transformer as a contextagnostic baseline and two context-aware baselines (Zhang et al., 2018; Miculicich et al., 2018). All models are implemented in C++ using DyNet (Neubig et al., 2017). For RNNSearch, we modify the sentence-based NMT implementation in mantis (Cohn et al., 2016). The encoder is a single layer bidirectional GRU (Cho et al., 2014) and 3096 6 www.casmacat.eu/corpus/news-commentary.html Integration into Encoder TED News Europarl Model BLEU Meteor BLEU Meteor BLEU Meteor RNNSearch 19.24 40.81 16.51 36.79 26.26 44.14 Transformer 23.28 44.17 22.78 42.19 28.72 46.22 +Attention, sentence 24.47 45.25 24.78 43.90 29.60 46.98 word 24.55 44.89 24.55 43.75 29.63 46.94 24.23 44.81 24.76 44.10 29.72 47.03 +H-Attention, sparse-soft sparse-sparse 24.27 45.07 24.66 44.18 29.64 47.04 Integration into Decoder TED News Europarl BLEU Meteor BLEU Meteor BLEU Meteor 19.24 40"
N19-1313,W07-0734,0,0.0894651,"use Iterative Decoding only when using the bilingual context. All experiments are run on a single Nvidia P100 GPU with 16GBs of memory.10 7 https://github.com/duyvuleo/Transformer-DyNet The code is available at https://github.com/ sameenmaruf/selective-attn 8 9 We found this configuration to be much more stable than using 6 layers with almost no difference in performance as reported by Xia et al. (2018). 10 The experiments can also be run on GPUs with 1012GBs of memory by reducing the batch size at the expense Evaluation Metrics For evaluation, we use BLEU (Papineni et al., 2002) and Meteor (Lavie and Agarwal, 2007) scores on tokenised text, and measure statistical significance with respect to the baselines, p < 0.05 (Clark et al., 2011). 4.2 Main Results We divide our experiments into two parts: offline and online document MT. Offline Document MT From the scores of the two context-agnostic baselines in Table 2, we can see that the Transformer beats the RNNSearch model in all cases by atleast +2.5 BLEU and +2.1 Meteor scores showing that our hyperparameter choice for the Transformer is indeed effective. For the Encoder Context integration, our Hierarchical Attention models perform the (near) best for New"
N19-1313,D11-1084,0,0.11888,"eater commitment and sincerity in eliminating the obstacles to the return of Croatia ’s Serbian population . sj−4 : by signing a border arbitration agreement with its neighbour Slovenia , the new Croatian Government has not only eliminated an obstacle to the negotiating process , but has also paved the way for the resolution of other issues . Table 7: Example of pronoun disambiguation. Context sentences are ordered in decreasing probability mass. Conventional Document-level MT These can further be classified into two main categories. The first, which use cache-based memories (Tiedemann, 2010; Gong et al., 2011) and the second, which focus on specific discourse phenomema like anaphora (Hardmeier and Federico, 2010), lexical cohesion (Xiong et al., 2013; Gong et al., 2015; Mascarell, 2017) and coreference (Miculicich Werlen and Popescu-Belis, 2017) to name a few. Most of these approaches are, however, restrictive as they mostly involve using handcrafted features similar to the conventional MT approaches. Document-level Neural MT The works here can again be divided into two categories: online— use previous context only, and offline—use both past and future contexts. Most works fall into the former cate"
N19-1313,P18-2059,1,0.861235,"Missing"
N19-1313,W15-2504,0,0.0171576,"its neighbour Slovenia , the new Croatian Government has not only eliminated an obstacle to the negotiating process , but has also paved the way for the resolution of other issues . Table 7: Example of pronoun disambiguation. Context sentences are ordered in decreasing probability mass. Conventional Document-level MT These can further be classified into two main categories. The first, which use cache-based memories (Tiedemann, 2010; Gong et al., 2011) and the second, which focus on specific discourse phenomema like anaphora (Hardmeier and Federico, 2010), lexical cohesion (Xiong et al., 2013; Gong et al., 2015; Mascarell, 2017) and coreference (Miculicich Werlen and Popescu-Belis, 2017) to name a few. Most of these approaches are, however, restrictive as they mostly involve using handcrafted features similar to the conventional MT approaches. Document-level Neural MT The works here can again be divided into two categories: online— use previous context only, and offline—use both past and future contexts. Most works fall into the former category, with those that use only a single 3099 previous sentence in the source (Jean et al., 2017; Tiedemann and Scherrer, 2017; Voita et al., 2018); one previous s"
N19-1313,2010.iwslt-papers.10,0,0.158973,"n population . sj−4 : by signing a border arbitration agreement with its neighbour Slovenia , the new Croatian Government has not only eliminated an obstacle to the negotiating process , but has also paved the way for the resolution of other issues . Table 7: Example of pronoun disambiguation. Context sentences are ordered in decreasing probability mass. Conventional Document-level MT These can further be classified into two main categories. The first, which use cache-based memories (Tiedemann, 2010; Gong et al., 2011) and the second, which focus on specific discourse phenomema like anaphora (Hardmeier and Federico, 2010), lexical cohesion (Xiong et al., 2013; Gong et al., 2015; Mascarell, 2017) and coreference (Miculicich Werlen and Popescu-Belis, 2017) to name a few. Most of these approaches are, however, restrictive as they mostly involve using handcrafted features similar to the conventional MT approaches. Document-level Neural MT The works here can again be divided into two categories: online— use previous context only, and offline—use both past and future contexts. Most works fall into the former category, with those that use only a single 3099 previous sentence in the source (Jean et al., 2017; Tiedeman"
N19-1313,P18-1118,1,0.861108,"ng an internship at Unbabel. basis, thus ignoring the long-range dependencies which may be useful when it comes to translating discourse phenomena. More recently, context-aware NMT has been gaining significant traction from the MT community with majority of works coming out in the past two years. Most of these focus on using a few previous sentences as context (Jean et al., 2017; Wang et al., 2017; Tu et al., 2018; Voita et al., 2018; Zhang et al., 2018; Miculicich et al., 2018) and neglect the rest of the document. Only one existing work has endeavoured to consider the full document context (Maruf and Haffari, 2018), thus proposing a more generalised approach to document-level NMT. However, the model is restrictive as the document-level attention computed is sentence-based and static (computed only once for the sentence being translated). A more recent work (Miculicich et al., 2018) proposes to use a hierarchical attention network (HAN) (Yang et al., 2016) to model the contextual information in a structured manner using word-level and sentencelevel abstractions; yet, it uses a limited number of past source and target sentences as context and is not scalable to entire document. In this work, we propose a"
N19-1313,2005.mtsummit-papers.11,0,0.13058,"the values are composed of the hidden representations of the target words, both from the last decoder layer. Again the keys Kw and Ks are either for individual target words or target sentences, and same goes for Vw and Vs . The queries Qw , Qs for the Context Layer come from the Source Attention sub-layer in the Lth layer of the decoder (Figure 3). 4 4.1 Experiments Setup Datasets We conduct experiments for English→German on three different domains: TED talks, News-Commentary and Europarl. These datasets are chosen based on their variance • Europarl This dataset is extracted from Europarl v7 (Koehn, 2005). The source and target sentences are aligned using the links provided by Tiedemann (2012). Following Maruf and Haffari (2018), we use the SPEAKER tag as the document delimiter. Documents longer than 5 sentences are kept and the resulting corpus is randomly split into training, dev and test sets. The corpora statistics are provided in Table 1. All datasets are tokenised and truecased using the Moses toolkit (Koehn et al., 2007), and split into subword units using a joint BPE model with 30K merge operations (Sennrich et al., 2016). Models and Baselines For offline document MT, we have two conte"
N19-1313,P07-2045,0,0.015997,"n on three different domains: TED talks, News-Commentary and Europarl. These datasets are chosen based on their variance • Europarl This dataset is extracted from Europarl v7 (Koehn, 2005). The source and target sentences are aligned using the links provided by Tiedemann (2012). Following Maruf and Haffari (2018), we use the SPEAKER tag as the document delimiter. Documents longer than 5 sentences are kept and the resulting corpus is randomly split into training, dev and test sets. The corpora statistics are provided in Table 1. All datasets are tokenised and truecased using the Moses toolkit (Koehn et al., 2007), and split into subword units using a joint BPE model with 30K merge operations (Sennrich et al., 2016). Models and Baselines For offline document MT, we have two context-agnostic baselines: (i) a modified version of RNNSearch (Bahdanau et al., 2015), which incorporates dropout on the output layer and improves the attention model by feeding the previously generated word, and (ii) the stateof-the-art Transformer architecture. For the online case, we again have the Transformer as a contextagnostic baseline and two context-aware baselines (Zhang et al., 2018; Miculicich et al., 2018). All models"
N19-1313,C18-1050,0,0.109805,"only, and offline—use both past and future contexts. Most works fall into the former category, with those that use only a single 3099 previous sentence in the source (Jean et al., 2017; Tiedemann and Scherrer, 2017; Voita et al., 2018); one previous sentence both in source and target (Bawden et al., 2018); more than one previous source sentence (Wang et al., 2017; Zhang et al., 2018); or a few previous source and target sentences (Miculicich et al., 2018). Apart from fixing the context length, there are few works which use cache-based memories to store contextual information (Tu et al., 2018; Kuang et al., 2018) and use that to improve the MT system performance. A recent work (Maruf et al., 2018) reports promising results when using the complete history for translating online conversations. For the offline setting, however, there is only one work that effectively uses the full documentcontext on both source and target-side using memory networks (Maruf and Haffari, 2018). The debate in document-level NMT today is mostly about how much of the previous context to use and there has been no comparison between the online and offline setting except using only one previous and following sentence (Voita et al"
N19-1313,D18-1512,0,0.131459,"Missing"
N19-1313,W18-6311,1,0.852023,"ategory, with those that use only a single 3099 previous sentence in the source (Jean et al., 2017; Tiedemann and Scherrer, 2017; Voita et al., 2018); one previous sentence both in source and target (Bawden et al., 2018); more than one previous source sentence (Wang et al., 2017; Zhang et al., 2018); or a few previous source and target sentences (Miculicich et al., 2018). Apart from fixing the context length, there are few works which use cache-based memories to store contextual information (Tu et al., 2018; Kuang et al., 2018) and use that to improve the MT system performance. A recent work (Maruf et al., 2018) reports promising results when using the complete history for translating online conversations. For the offline setting, however, there is only one work that effectively uses the full documentcontext on both source and target-side using memory networks (Maruf and Haffari, 2018). The debate in document-level NMT today is mostly about how much of the previous context to use and there has been no comparison between the online and offline setting except using only one previous and following sentence (Voita et al., 2018). Sparse Attention Sparse attention and its constrained variants have been use"
N19-1313,W17-4813,0,0.0183569,"nia , the new Croatian Government has not only eliminated an obstacle to the negotiating process , but has also paved the way for the resolution of other issues . Table 7: Example of pronoun disambiguation. Context sentences are ordered in decreasing probability mass. Conventional Document-level MT These can further be classified into two main categories. The first, which use cache-based memories (Tiedemann, 2010; Gong et al., 2011) and the second, which focus on specific discourse phenomema like anaphora (Hardmeier and Federico, 2010), lexical cohesion (Xiong et al., 2013; Gong et al., 2015; Mascarell, 2017) and coreference (Miculicich Werlen and Popescu-Belis, 2017) to name a few. Most of these approaches are, however, restrictive as they mostly involve using handcrafted features similar to the conventional MT approaches. Document-level Neural MT The works here can again be divided into two categories: online— use previous context only, and offline—use both past and future contexts. Most works fall into the former category, with those that use only a single 3099 previous sentence in the source (Jean et al., 2017; Tiedemann and Scherrer, 2017; Voita et al., 2018); one previous sentence both in so"
N19-1313,D18-1325,0,0.151064,"ever, all of these models share the same inherent problem: the translation is still performed on a sentence-by-sentence ∗ Work initiated during an internship at Unbabel. basis, thus ignoring the long-range dependencies which may be useful when it comes to translating discourse phenomena. More recently, context-aware NMT has been gaining significant traction from the MT community with majority of works coming out in the past two years. Most of these focus on using a few previous sentences as context (Jean et al., 2017; Wang et al., 2017; Tu et al., 2018; Voita et al., 2018; Zhang et al., 2018; Miculicich et al., 2018) and neglect the rest of the document. Only one existing work has endeavoured to consider the full document context (Maruf and Haffari, 2018), thus proposing a more generalised approach to document-level NMT. However, the model is restrictive as the document-level attention computed is sentence-based and static (computed only once for the sentence being translated). A more recent work (Miculicich et al., 2018) proposes to use a hierarchical attention network (HAN) (Yang et al., 2016) to model the contextual information in a structured manner using word-level and sentencelevel abstractions; yet"
N19-1313,W17-1505,0,0.0481647,"Missing"
N19-1313,W18-6307,0,0.137183,"Missing"
N19-1313,P02-1040,0,0.105119,"escribed in §2.2. For inference, we use Iterative Decoding only when using the bilingual context. All experiments are run on a single Nvidia P100 GPU with 16GBs of memory.10 7 https://github.com/duyvuleo/Transformer-DyNet The code is available at https://github.com/ sameenmaruf/selective-attn 8 9 We found this configuration to be much more stable than using 6 layers with almost no difference in performance as reported by Xia et al. (2018). 10 The experiments can also be run on GPUs with 1012GBs of memory by reducing the batch size at the expense Evaluation Metrics For evaluation, we use BLEU (Papineni et al., 2002) and Meteor (Lavie and Agarwal, 2007) scores on tokenised text, and measure statistical significance with respect to the baselines, p < 0.05 (Clark et al., 2011). 4.2 Main Results We divide our experiments into two parts: offline and online document MT. Offline Document MT From the scores of the two context-agnostic baselines in Table 2, we can see that the Transformer beats the RNNSearch model in all cases by atleast +2.5 BLEU and +2.1 Meteor scores showing that our hyperparameter choice for the Transformer is indeed effective. For the Encoder Context integration, our Hierarchical Attention m"
N19-1313,P16-1162,0,0.400895,"d on their variance • Europarl This dataset is extracted from Europarl v7 (Koehn, 2005). The source and target sentences are aligned using the links provided by Tiedemann (2012). Following Maruf and Haffari (2018), we use the SPEAKER tag as the document delimiter. Documents longer than 5 sentences are kept and the resulting corpus is randomly split into training, dev and test sets. The corpora statistics are provided in Table 1. All datasets are tokenised and truecased using the Moses toolkit (Koehn et al., 2007), and split into subword units using a joint BPE model with 30K merge operations (Sennrich et al., 2016). Models and Baselines For offline document MT, we have two context-agnostic baselines: (i) a modified version of RNNSearch (Bahdanau et al., 2015), which incorporates dropout on the output layer and improves the attention model by feeding the previously generated word, and (ii) the stateof-the-art Transformer architecture. For the online case, we again have the Transformer as a contextagnostic baseline and two context-aware baselines (Zhang et al., 2018; Miculicich et al., 2018). All models are implemented in C++ using DyNet (Neubig et al., 2017). For RNNSearch, we modify the sentence-based N"
N19-1313,W10-2602,0,0.0487561,"d finances and greater commitment and sincerity in eliminating the obstacles to the return of Croatia ’s Serbian population . sj−4 : by signing a border arbitration agreement with its neighbour Slovenia , the new Croatian Government has not only eliminated an obstacle to the negotiating process , but has also paved the way for the resolution of other issues . Table 7: Example of pronoun disambiguation. Context sentences are ordered in decreasing probability mass. Conventional Document-level MT These can further be classified into two main categories. The first, which use cache-based memories (Tiedemann, 2010; Gong et al., 2011) and the second, which focus on specific discourse phenomema like anaphora (Hardmeier and Federico, 2010), lexical cohesion (Xiong et al., 2013; Gong et al., 2015; Mascarell, 2017) and coreference (Miculicich Werlen and Popescu-Belis, 2017) to name a few. Most of these approaches are, however, restrictive as they mostly involve using handcrafted features similar to the conventional MT approaches. Document-level Neural MT The works here can again be divided into two categories: online— use previous context only, and offline—use both past and future contexts. Most works fall"
N19-1313,tiedemann-2012-parallel,0,0.0352592,"he last decoder layer. Again the keys Kw and Ks are either for individual target words or target sentences, and same goes for Vw and Vs . The queries Qw , Qs for the Context Layer come from the Source Attention sub-layer in the Lth layer of the decoder (Figure 3). 4 4.1 Experiments Setup Datasets We conduct experiments for English→German on three different domains: TED talks, News-Commentary and Europarl. These datasets are chosen based on their variance • Europarl This dataset is extracted from Europarl v7 (Koehn, 2005). The source and target sentences are aligned using the links provided by Tiedemann (2012). Following Maruf and Haffari (2018), we use the SPEAKER tag as the document delimiter. Documents longer than 5 sentences are kept and the resulting corpus is randomly split into training, dev and test sets. The corpora statistics are provided in Table 1. All datasets are tokenised and truecased using the Moses toolkit (Koehn et al., 2007), and split into subword units using a joint BPE model with 30K merge operations (Sennrich et al., 2016). Models and Baselines For offline document MT, we have two context-agnostic baselines: (i) a modified version of RNNSearch (Bahdanau et al., 2015), which"
N19-1313,Q18-1029,0,0.218452,"oving the translation quality (Vaswani et al., 2017). However, all of these models share the same inherent problem: the translation is still performed on a sentence-by-sentence ∗ Work initiated during an internship at Unbabel. basis, thus ignoring the long-range dependencies which may be useful when it comes to translating discourse phenomena. More recently, context-aware NMT has been gaining significant traction from the MT community with majority of works coming out in the past two years. Most of these focus on using a few previous sentences as context (Jean et al., 2017; Wang et al., 2017; Tu et al., 2018; Voita et al., 2018; Zhang et al., 2018; Miculicich et al., 2018) and neglect the rest of the document. Only one existing work has endeavoured to consider the full document context (Maruf and Haffari, 2018), thus proposing a more generalised approach to document-level NMT. However, the model is restrictive as the document-level attention computed is sentence-based and static (computed only once for the sentence being translated). A more recent work (Miculicich et al., 2018) proposes to use a hierarchical attention network (HAN) (Yang et al., 2016) to model the contextual information in a stru"
N19-1313,P18-1117,0,0.2424,"tion quality (Vaswani et al., 2017). However, all of these models share the same inherent problem: the translation is still performed on a sentence-by-sentence ∗ Work initiated during an internship at Unbabel. basis, thus ignoring the long-range dependencies which may be useful when it comes to translating discourse phenomena. More recently, context-aware NMT has been gaining significant traction from the MT community with majority of works coming out in the past two years. Most of these focus on using a few previous sentences as context (Jean et al., 2017; Wang et al., 2017; Tu et al., 2018; Voita et al., 2018; Zhang et al., 2018; Miculicich et al., 2018) and neglect the rest of the document. Only one existing work has endeavoured to consider the full document context (Maruf and Haffari, 2018), thus proposing a more generalised approach to document-level NMT. However, the model is restrictive as the document-level attention computed is sentence-based and static (computed only once for the sentence being translated). A more recent work (Miculicich et al., 2018) proposes to use a hierarchical attention network (HAN) (Yang et al., 2016) to model the contextual information in a structured manner using"
N19-1313,D17-1301,0,0.160643,"e effective in improving the translation quality (Vaswani et al., 2017). However, all of these models share the same inherent problem: the translation is still performed on a sentence-by-sentence ∗ Work initiated during an internship at Unbabel. basis, thus ignoring the long-range dependencies which may be useful when it comes to translating discourse phenomena. More recently, context-aware NMT has been gaining significant traction from the MT community with majority of works coming out in the past two years. Most of these focus on using a few previous sentences as context (Jean et al., 2017; Wang et al., 2017; Tu et al., 2018; Voita et al., 2018; Zhang et al., 2018; Miculicich et al., 2018) and neglect the rest of the document. Only one existing work has endeavoured to consider the full document context (Maruf and Haffari, 2018), thus proposing a more generalised approach to document-level NMT. However, the model is restrictive as the document-level attention computed is sentence-based and static (computed only once for the sentence being translated). A more recent work (Miculicich et al., 2018) proposes to use a hierarchical attention network (HAN) (Yang et al., 2016) to model the contextual info"
N19-1313,D13-1163,0,0.0373133,"tion agreement with its neighbour Slovenia , the new Croatian Government has not only eliminated an obstacle to the negotiating process , but has also paved the way for the resolution of other issues . Table 7: Example of pronoun disambiguation. Context sentences are ordered in decreasing probability mass. Conventional Document-level MT These can further be classified into two main categories. The first, which use cache-based memories (Tiedemann, 2010; Gong et al., 2011) and the second, which focus on specific discourse phenomema like anaphora (Hardmeier and Federico, 2010), lexical cohesion (Xiong et al., 2013; Gong et al., 2015; Mascarell, 2017) and coreference (Miculicich Werlen and Popescu-Belis, 2017) to name a few. Most of these approaches are, however, restrictive as they mostly involve using handcrafted features similar to the conventional MT approaches. Document-level Neural MT The works here can again be divided into two categories: online— use previous context only, and offline—use both past and future contexts. Most works fall into the former category, with those that use only a single 3099 previous sentence in the source (Jean et al., 2017; Tiedemann and Scherrer, 2017; Voita et al., 20"
N19-1313,N16-1174,0,0.0618912,"s context (Jean et al., 2017; Wang et al., 2017; Tu et al., 2018; Voita et al., 2018; Zhang et al., 2018; Miculicich et al., 2018) and neglect the rest of the document. Only one existing work has endeavoured to consider the full document context (Maruf and Haffari, 2018), thus proposing a more generalised approach to document-level NMT. However, the model is restrictive as the document-level attention computed is sentence-based and static (computed only once for the sentence being translated). A more recent work (Miculicich et al., 2018) proposes to use a hierarchical attention network (HAN) (Yang et al., 2016) to model the contextual information in a structured manner using word-level and sentencelevel abstractions; yet, it uses a limited number of past source and target sentences as context and is not scalable to entire document. In this work, we propose a selective attention approach to first selectively focus on relevant sentences in the global document-context and then attend to key words in those sentences, while ignoring the rest.1 Towards this goal, we use sparse attention, enabling an efficient and scalable use of the context. The intuition behind this is the way humans translate a sentence"
N19-1313,D18-1049,0,0.142625,"i et al., 2017). However, all of these models share the same inherent problem: the translation is still performed on a sentence-by-sentence ∗ Work initiated during an internship at Unbabel. basis, thus ignoring the long-range dependencies which may be useful when it comes to translating discourse phenomena. More recently, context-aware NMT has been gaining significant traction from the MT community with majority of works coming out in the past two years. Most of these focus on using a few previous sentences as context (Jean et al., 2017; Wang et al., 2017; Tu et al., 2018; Voita et al., 2018; Zhang et al., 2018; Miculicich et al., 2018) and neglect the rest of the document. Only one existing work has endeavoured to consider the full document context (Maruf and Haffari, 2018), thus proposing a more generalised approach to document-level NMT. However, the model is restrictive as the document-level attention computed is sentence-based and static (computed only once for the sentence being translated). A more recent work (Miculicich et al., 2018) proposes to use a hierarchical attention network (HAN) (Yang et al., 2016) to model the contextual information in a structured manner using word-level and sente"
N19-1313,W17-4811,0,0.15642,"Missing"
N19-1397,P13-1020,1,0.874328,"r methods rely on an abstractive approach with strongly conditioned generation on the source document (See et al., 2017). In fact, the best results for abstractive summarization have been achieved with models that are more extractive in nature than abstractive, since most of the words in the summary are copied from the document (Gehrmann et al., 2018). Due to the lack of training corpora, there is almost no work on neural architectures for compressive summarization. Most compressive summarization work has been applied to smaller datasets (Martins and Smith, 2009; BergKirkpatrick et al., 2011; Almeida and Martins, 2013). Other non-neural summarization systems apply this idea to select and compress the summary. Dorr et al. (2003) introduced a method to first extract the first sentence of a news article and then use linguistically-motivated heuristics to iteratively trim parts of it. Durrett et al. (2016) also learns a system that selects textual units to include in the summary and compresses them by deleting word spans guided by anaphoric constraints to improve coherence. Recently, Zhang et al. (2018) trained an abstractive sentence compression model using attention-based sequence-to-sequence architecture (Ru"
N19-1397,P11-1049,0,0.175783,"Missing"
N19-1397,P18-1063,0,0.127952,"Missing"
N19-1397,P16-1046,0,0.661102,"nt summarization—the task of generating a short summary of a document preserving its informative content (Sp¨arck Jones, 2007)—has been a highly studied research topic in recent years (Nallapati et al., 2016b; See et al., 2017; Fan et al., 2018; Pasunuru and Bansal, 2018). Modern approaches to single document summarization using neural network architectures 1 Our dataset and code is available at https:// github.com/Priberam/exconsumm. ∗ Now at Google London. have primarily focused on two strategies: extractive and abstractive. The former select a subset of the sentences to assemble a summary (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018a,c). The latter generates sentences that do not appear in the original document (See et al., 2017; Narayan et al., 2018b; Paulus et al., 2018). Both methods suffer from significant drawbacks: extractive systems are wasteful since they cannot trim the original sentences to fit into the summary, and they lack a mechanism to ensure overall coherence. In contrast, abstractive systems require natural language generation and semantic representation, problems that are inherently harder to solve than just extracting sentences from the original document. I"
N19-1397,R13-1027,0,0.0258907,"Missing"
N19-1397,W03-0501,0,0.109136,"Missing"
N19-1397,P16-1188,0,0.084033,"ummary are copied from the document (Gehrmann et al., 2018). Due to the lack of training corpora, there is almost no work on neural architectures for compressive summarization. Most compressive summarization work has been applied to smaller datasets (Martins and Smith, 2009; BergKirkpatrick et al., 2011; Almeida and Martins, 2013). Other non-neural summarization systems apply this idea to select and compress the summary. Dorr et al. (2003) introduced a method to first extract the first sentence of a news article and then use linguistically-motivated heuristics to iteratively trim parts of it. Durrett et al. (2016) also learns a system that selects textual units to include in the summary and compresses them by deleting word spans guided by anaphoric constraints to improve coherence. Recently, Zhang et al. (2018) trained an abstractive sentence compression model using attention-based sequence-to-sequence architecture (Rush et al., 2015) to map a sentence in the document selected by the extractive model to a sentence in the summary. However, as the sentences in the document and in the summary are not aligned for compression, their compression model is significantly inferior to the extractive model. In thi"
N19-1397,W18-2706,0,0.021044,"ummaries derived automatically from the CNN/DailyMail 1 reference summaries. 1 Figure 1: Summaries produced by our model. For illustration, the compressive summary shows the removed spans strike-through. Introduction Text summarization is an important NLP problem with a wide range of applications in data-driven industries (e.g., news, health, and defense). Single document summarization—the task of generating a short summary of a document preserving its informative content (Sp¨arck Jones, 2007)—has been a highly studied research topic in recent years (Nallapati et al., 2016b; See et al., 2017; Fan et al., 2018; Pasunuru and Bansal, 2018). Modern approaches to single document summarization using neural network architectures 1 Our dataset and code is available at https:// github.com/Priberam/exconsumm. ∗ Now at Google London. have primarily focused on two strategies: extractive and abstractive. The former select a subset of the sentences to assemble a summary (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018a,c). The latter generates sentences that do not appear in the original document (See et al., 2017; Narayan et al., 2018b; Paulus et al., 2018). Both methods suffer from signif"
N19-1397,D15-1042,0,0.0902161,"Missing"
N19-1397,D13-1155,0,0.0222192,"rayan et al., 2018c). To build a compressive oracle, we trained a supervised sentence labeling classifier, adapted from 3959 Oracle Extractive Oracle Compressive Oracle R1 54.67 57.12 R2 30.37 32.59 RL 50.81 53.27 Table 1: Oracle scores obtained for the CNN and DailyMail testsets. We report ROUGE-1 (R1), ROUGE-2 (R2) and ROUGE-L (RL) F1 scores. the Transition-Based Chunking Model (Lample et al., 2016), to annotate spans in every sentence that can be dropped in the final summary. We used the publicly released set of 10,000 sentencecompression pairs from the Google sentence compression dataset (Filippova and Altun, 2013; Filippova et al., 2015) for training. After tagging all sentences in the CNN and DailyMail corpora using this compression model, we generated oracle compressive summaries based on the best average of ROUGE-1 (R1) and ROUGE-2 (R2) F1 scores from the combination of all possible sentences and all removals of the marked compression chunks. To verify the adequacy of our proposed oracles, we show in Table 1 a comparison of their scores. Our compressive oracle achieves much better scores than the extractive oracle, because of its capability to make summaries concise. Moreover, the linguistic qualit"
N19-1397,D18-1443,0,0.100831,". They select sentences based on an LSTM classifier that predicts a binary label for each sentence (Cheng and Lapata, 2016), based on ranking using reinforcement learning (Narayan et al., 2018c), or even by training an extractive latent model (Zhang et al., 2018). Other methods rely on an abstractive approach with strongly conditioned generation on the source document (See et al., 2017). In fact, the best results for abstractive summarization have been achieved with models that are more extractive in nature than abstractive, since most of the words in the summary are copied from the document (Gehrmann et al., 2018). Due to the lack of training corpora, there is almost no work on neural architectures for compressive summarization. Most compressive summarization work has been applied to smaller datasets (Martins and Smith, 2009; BergKirkpatrick et al., 2011; Almeida and Martins, 2013). Other non-neural summarization systems apply this idea to select and compress the summary. Dorr et al. (2003) introduced a method to first extract the first sentence of a news article and then use linguistically-motivated heuristics to iteratively trim parts of it. Durrett et al. (2016) also learns a system that selects tex"
N19-1397,N18-1065,0,0.319626,"Missing"
N19-1397,P18-1013,0,0.0974388,"Missing"
N19-1397,N16-1030,0,0.0472101,"mmaries prior to training using two types of oracles. We used an extractive oracle to identify the set of sentences which collectively gives the highest ROUGE (Lin and Hovy, 2003) with respect to the gold summary (Narayan et al., 2018c). To build a compressive oracle, we trained a supervised sentence labeling classifier, adapted from 3959 Oracle Extractive Oracle Compressive Oracle R1 54.67 57.12 R2 30.37 32.59 RL 50.81 53.27 Table 1: Oracle scores obtained for the CNN and DailyMail testsets. We report ROUGE-1 (R1), ROUGE-2 (R2) and ROUGE-L (RL) F1 scores. the Transition-Based Chunking Model (Lample et al., 2016), to annotate spans in every sentence that can be dropped in the final summary. We used the publicly released set of 10,000 sentencecompression pairs from the Google sentence compression dataset (Filippova and Altun, 2013; Filippova et al., 2015) for training. After tagging all sentences in the CNN and DailyMail corpora using this compression model, we generated oracle compressive summaries based on the best average of ROUGE-1 (R1) and ROUGE-2 (R2) F1 scores from the combination of all possible sentences and all removals of the marked compression chunks. To verify the adequacy of our proposed"
N19-1397,N18-2009,0,0.063515,"Missing"
N19-1397,N03-1020,0,0.659093,"Missing"
N19-1397,W09-1801,1,0.936062,"2017; Narayan et al., 2018b; Paulus et al., 2018). Both methods suffer from significant drawbacks: extractive systems are wasteful since they cannot trim the original sentences to fit into the summary, and they lack a mechanism to ensure overall coherence. In contrast, abstractive systems require natural language generation and semantic representation, problems that are inherently harder to solve than just extracting sentences from the original document. In this paper, we present a novel architecture that attempts to mitigate the problems above via a middle ground, compressive summarization (Martins and Smith, 2009). Our model selects a set of sentences from the input document, and 3955 Proceedings of NAACL-HLT 2019, pages 3955–3966 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics compresses them by removing unnecessary words, while keeping the summaries informative, concise and grammatical. We achieve this by dynamically modeling the generated summary using a Long Short Term Memory (LSTM; Hochreiter and Schmidhuber, 1997) to produce summary state representations. This state provides crucial information to iteratively increment summaries based on previously"
N19-1397,E06-1038,0,0.169088,"Missing"
N19-1397,K16-1028,0,0.0784861,"Missing"
N19-1397,P18-1188,1,0.119729,"summary of a document preserving its informative content (Sp¨arck Jones, 2007)—has been a highly studied research topic in recent years (Nallapati et al., 2016b; See et al., 2017; Fan et al., 2018; Pasunuru and Bansal, 2018). Modern approaches to single document summarization using neural network architectures 1 Our dataset and code is available at https:// github.com/Priberam/exconsumm. ∗ Now at Google London. have primarily focused on two strategies: extractive and abstractive. The former select a subset of the sentences to assemble a summary (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018a,c). The latter generates sentences that do not appear in the original document (See et al., 2017; Narayan et al., 2018b; Paulus et al., 2018). Both methods suffer from significant drawbacks: extractive systems are wasteful since they cannot trim the original sentences to fit into the summary, and they lack a mechanism to ensure overall coherence. In contrast, abstractive systems require natural language generation and semantic representation, problems that are inherently harder to solve than just extracting sentences from the original document. In this paper, we present a novel architecture"
N19-1397,D18-1206,1,0.135235,"summary of a document preserving its informative content (Sp¨arck Jones, 2007)—has been a highly studied research topic in recent years (Nallapati et al., 2016b; See et al., 2017; Fan et al., 2018; Pasunuru and Bansal, 2018). Modern approaches to single document summarization using neural network architectures 1 Our dataset and code is available at https:// github.com/Priberam/exconsumm. ∗ Now at Google London. have primarily focused on two strategies: extractive and abstractive. The former select a subset of the sentences to assemble a summary (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018a,c). The latter generates sentences that do not appear in the original document (See et al., 2017; Narayan et al., 2018b; Paulus et al., 2018). Both methods suffer from significant drawbacks: extractive systems are wasteful since they cannot trim the original sentences to fit into the summary, and they lack a mechanism to ensure overall coherence. In contrast, abstractive systems require natural language generation and semantic representation, problems that are inherently harder to solve than just extracting sentences from the original document. In this paper, we present a novel architecture"
N19-1397,N18-1158,1,0.111916,"summary of a document preserving its informative content (Sp¨arck Jones, 2007)—has been a highly studied research topic in recent years (Nallapati et al., 2016b; See et al., 2017; Fan et al., 2018; Pasunuru and Bansal, 2018). Modern approaches to single document summarization using neural network architectures 1 Our dataset and code is available at https:// github.com/Priberam/exconsumm. ∗ Now at Google London. have primarily focused on two strategies: extractive and abstractive. The former select a subset of the sentences to assemble a summary (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018a,c). The latter generates sentences that do not appear in the original document (See et al., 2017; Narayan et al., 2018b; Paulus et al., 2018). Both methods suffer from significant drawbacks: extractive systems are wasteful since they cannot trim the original sentences to fit into the summary, and they lack a mechanism to ensure overall coherence. In contrast, abstractive systems require natural language generation and semantic representation, problems that are inherently harder to solve than just extracting sentences from the original document. In this paper, we present a novel architecture"
N19-1397,N18-2102,0,0.127757,"utomatically from the CNN/DailyMail 1 reference summaries. 1 Figure 1: Summaries produced by our model. For illustration, the compressive summary shows the removed spans strike-through. Introduction Text summarization is an important NLP problem with a wide range of applications in data-driven industries (e.g., news, health, and defense). Single document summarization—the task of generating a short summary of a document preserving its informative content (Sp¨arck Jones, 2007)—has been a highly studied research topic in recent years (Nallapati et al., 2016b; See et al., 2017; Fan et al., 2018; Pasunuru and Bansal, 2018). Modern approaches to single document summarization using neural network architectures 1 Our dataset and code is available at https:// github.com/Priberam/exconsumm. ∗ Now at Google London. have primarily focused on two strategies: extractive and abstractive. The former select a subset of the sentences to assemble a summary (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018a,c). The latter generates sentences that do not appear in the original document (See et al., 2017; Narayan et al., 2018b; Paulus et al., 2018). Both methods suffer from significant drawbacks: extractive"
N19-1397,D15-1044,0,0.825797,"for each sentence, and a set of compressed oracle summaries (see §4). Experimental results show that when evaluated automatically, both the extractive and compressive variants of our model provide state-of-the-art results. Human evaluation further shows that our model is better than previous state-of-the-art systems at generating informative and concise summaries. 2 Related Work Recent work on neural summarization has mainly focused on sequence-to-sequence (seq2seq) architectures (Sutskever et al., 2014), a formulation particularly suited and initially employed for abstractive summarization (Rush et al., 2015). However, state-of-the-art results have been achieved by RNN-based methods which are extractive. They select sentences based on an LSTM classifier that predicts a binary label for each sentence (Cheng and Lapata, 2016), based on ranking using reinforcement learning (Narayan et al., 2018c), or even by training an extractive latent model (Zhang et al., 2018). Other methods rely on an abstractive approach with strongly conditioned generation on the source document (See et al., 2017). In fact, the best results for abstractive summarization have been achieved with models that are more extractive i"
N19-1397,E17-2007,0,0.0363789,"after every 5,000 batches. We trained with Adam (Kingma and Ba, 2015) with an initial learning rate of 0.001. Our system was implemented using DyNet (Neubig et al., 2017). 4.3 Model Evaluation We evaluated summarization quality using F1 ROUGE (Lin and Hovy, 2003). We report results 4 We show examples of both oracles in Appendix §A.1. in terms of unigram and bigram overlap (R1) and (R2) as a means of assessing informativeness, and the longest common subsequence (RL) as a means 5 of assessing fluency. In addition to ROUGE, which can be misleading when used as the only means to assess summaries (Schluter, 2017), we also conducted a question-answering based human evaluation to assess the informativeness of our summaries in their ability to preserve key informa6 tion from the document (Narayan et al., 2018c). First, questions are written using the gold summary, we then examined how many questions participants were able to answer by reading system 7 summaries alone, without access to the article. Figure 5 shows a set of candidate summaries along with questions used for this evaluation. 4.4 Model and Baselines We evaluated our model E X C ON S UMM in two settings: Extractive (selects sentences to assemb"
N19-1397,P17-1099,0,0.476379,"acle compressive summaries derived automatically from the CNN/DailyMail 1 reference summaries. 1 Figure 1: Summaries produced by our model. For illustration, the compressive summary shows the removed spans strike-through. Introduction Text summarization is an important NLP problem with a wide range of applications in data-driven industries (e.g., news, health, and defense). Single document summarization—the task of generating a short summary of a document preserving its informative content (Sp¨arck Jones, 2007)—has been a highly studied research topic in recent years (Nallapati et al., 2016b; See et al., 2017; Fan et al., 2018; Pasunuru and Bansal, 2018). Modern approaches to single document summarization using neural network architectures 1 Our dataset and code is available at https:// github.com/Priberam/exconsumm. ∗ Now at Google London. have primarily focused on two strategies: extractive and abstractive. The former select a subset of the sentences to assemble a summary (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018a,c). The latter generates sentences that do not appear in the original document (See et al., 2017; Narayan et al., 2018b; Paulus et al., 2018). Both methods"
N19-1397,P17-1108,0,0.102084,"Missing"
N19-1397,D18-1088,0,0.509737,"presses them by removing unnecessary words, while keeping the summaries informative, concise and grammatical. We achieve this by dynamically modeling the generated summary using a Long Short Term Memory (LSTM; Hochreiter and Schmidhuber, 1997) to produce summary state representations. This state provides crucial information to iteratively increment summaries based on previously extracted information. It also facilitates the generation of variable length summaries as opposed to fixed lengths, in previous extractive systems (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018c; Zhang et al., 2018). Our model can be trained in both extractive (labeling sentences for extraction) or compressive (labeling words for extraction) settings. Figure 1 shows a summary example generated by our model. Our contributions in this paper are three-fold: • we present the first end-to-end neural architecture for EXtractive and COmpressive Neural SUMMarization (dubbed E X C ON S UMM, see §3), • we validate this architecture on the CNN/DailyMail and the Newsroom datasets (Hermann et al., 2015; Grusky et al., 2018), showing that our model generates variablelength summaries which correlate well with gold summ"
P09-1039,W07-2216,0,0.220657,"that th shown by McDonald et al. (2005), is an instance of the maximal arborescence problem. Combinatorial algorithms (Chu and Liu, 1965; Edmonds, 1967) can solve this problem in cubic time.4 If the dependency parse trees are restricted to be projective, cubic-time algorithms are available via dynamic programming (Eisner, 1996). While in the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta, 1999), the same does not happen in the nonprojective case, where finding the highest-scoring tree becomes NP-hard (McDonald and Satta, 2007). Approximate algorithms have been employed to handle models that are not arc-factored (although features are still fairly local): McDonald and Pereira (2006) adopted an approximation based on O(n3 ) projective parsing followed by a hillclimbing algorithm to rearrange arcs, and Smith and Eisner (2008) proposed an algorithm based on loopy belief propagation. 3 fact, fast solvers are available today that make this a practical solution for many problems. Their performance depends on the dimensions and degree of sparsity of the constraint matrix A. Riedel and Clarke (2006) proposed an ILP formulat"
P09-1039,H05-1066,0,0.685035,"Missing"
P09-1039,P08-1108,0,0.628299,". featureHowrepning, 2002; McDonald and Pereira, resentations over the inputparsing (McDonald et al., 2.2ever, Arc Factorization and Locality in the data-driven setting this2005a). can be The goal of this is to further our current partially bywork incorporating feature repThere has adverted been much recent workrich on dependency understanding of the computational nature of nonresentations over the input (McDonald et al., 2005a). parsing using graph-based, transition-based, and projective forfurther both learning and The goalparsing of thisalgorithms work is to our current hybrid methods; see Nivre and McDonald (2008) inference within setting. We start by understanding ofthe thedata-driven computational nature of nonforprojective an overview. Typical graph-based methods investigating and extending the edge-factored model parsing algorithms for both learning and consider linear classifiers of the form of McDonald et the al. data-driven (2005b). Insetting. particular, we apinference within We start by peal to the Matrix Tree Theorem for multi-digraphs investigating and extending the edge-factored model hw (x) = argmaxy∈Y w&gt; f (x,fory),calculat-(1) to McDonald design polynomial-time algorithms of et al. (2005"
P09-1039,C04-1197,0,0.0420515,"), model word valency, and can learn to favor nearly-projective parses. Introduction We evaluate the performance of the new parsers on standard parsing tasks in seven languages. The techniques that we present are also compatible with scenarios where expert knowledge is available, for example in the form of hard or soft firstorder logic constraints (Richardson and Domingos, 2006; Chang et al., 2008). Much attention has recently been devoted to integer linear programming (ILP) formulations of NLP problems, with interesting results in applications like semantic role labeling (Roth and Yih, 2005; Punyakanok et al., 2004), dependency parsing (Riedel and Clarke, 2006), word alignment for machine translation (Lacoste-Julien et al., 2006), summarization (Clarke and Lapata, 2008), and coreference resolution (Denis and Baldridge, 2007), among others. In general, the rationale for the development of ILP formulations is to incorporate non-local features or global constraints, which are often difficult to handle with traditional algorithms. ILP formulations focus more on the modeling of problems, rather than algorithm design. While solving an ILP is NP-hard in general, fast solvers are available today that make it a p"
P09-1039,W06-1616,0,0.30569,"nearly-projective parses. Introduction We evaluate the performance of the new parsers on standard parsing tasks in seven languages. The techniques that we present are also compatible with scenarios where expert knowledge is available, for example in the form of hard or soft firstorder logic constraints (Richardson and Domingos, 2006; Chang et al., 2008). Much attention has recently been devoted to integer linear programming (ILP) formulations of NLP problems, with interesting results in applications like semantic role labeling (Roth and Yih, 2005; Punyakanok et al., 2004), dependency parsing (Riedel and Clarke, 2006), word alignment for machine translation (Lacoste-Julien et al., 2006), summarization (Clarke and Lapata, 2008), and coreference resolution (Denis and Baldridge, 2007), among others. In general, the rationale for the development of ILP formulations is to incorporate non-local features or global constraints, which are often difficult to handle with traditional algorithms. ILP formulations focus more on the modeling of problems, rather than algorithm design. While solving an ILP is NP-hard in general, fast solvers are available today that make it a practical solution for many NLP problems. This"
P09-1039,D08-1016,0,0.497506,"programming (Eisner, 1996). While in the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta, 1999), the same does not happen in the nonprojective case, where finding the highest-scoring tree becomes NP-hard (McDonald and Satta, 2007). Approximate algorithms have been employed to handle models that are not arc-factored (although features are still fairly local): McDonald and Pereira (2006) adopted an approximation based on O(n3 ) projective parsing followed by a hillclimbing algorithm to rearrange arcs, and Smith and Eisner (2008) proposed an algorithm based on loopy belief propagation. 3 fact, fast solvers are available today that make this a practical solution for many problems. Their performance depends on the dimensions and degree of sparsity of the constraint matrix A. Riedel and Clarke (2006) proposed an ILP formulation for dependency parsing which refines the arc-factored model by imposing linguistically motivated “hard” constraints that forbid some arc configurations. Their formulation includes an exponential number of constraints—one for each possible cycle. Since it is intractable to throw in all constraints"
P09-1039,W08-2121,0,0.0794721,"Missing"
P09-1039,D07-1003,1,0.197007,"NP-hard in general, fast solvers are available today that make it a practical solution for many NLP problems. This paper presents new, concise ILP formulations for projective and non-projective depen2 2.1 Dependency Parsing Preliminaries A dependency tree is a lightweight syntactic representation that attempts to capture functional relationships between words. Lately, this formalism has been used as an alternative to phrase-based parsing for a variety of tasks, ranging from machine translation (Ding and Palmer, 2005) to relation extraction (Culotta and Sorensen, 2004) and question answering (Wang et al., 2007). Let us first describe formally the set of legal dependency parse trees. Consider a sentence x = 342 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 342–350, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP hw0 , . . . , wn i, where wi denotes the word at the ith position, and w0 = $ is a wall symbol. We form the (complete1 ) directed graph D = hV, Ai, with vertices in V = {0, . . . , n} (the i-th vertex corresponding to the i-th word) and arcs in A = V 2 . Using terminology from graph theory, we say that B ⊆ A is an r-arborescence2 of the dire"
P09-1039,D08-1059,0,0.19634,"Missing"
P09-1039,W06-2920,0,0.447326,"Consider 3 (or 30 ) from §3.1. φka = −1, k ∈ V  {0} (18) a∈δ + (0) φka − (23) From the definition of projective arcs in §2.1, we np have that za = 1 if and only if the arc is active (za = 1) and there is some vertex k in the span of a = hi, ji such that ψik = 0. We are led to the following O(|A |· |V |) constraints for hi, ji ∈ A: • Any node consumes its own commodity and no other: X k ∈ V  {0}. zanp , I(a ∈ y and a is nonprojective). i∈V • The root sends one unit of commodity to each node: φka − j, k ∈ V  {0} np For most languages, dependency parse trees tend to be nearly projective (cf. Buchholz and Marsi, 2006). We wish to make our model capable of learning to prefer “nearly” projective parses whenever that behavior is observed in the data. The multicommodity directed flow model of Magnanti and Wolsey (1994) is a refinement of the model described in §3.1 which offers a compact and elegant way to indicate nonprojective arcs, requiring O(n3 ) variables and constraints. In this model, every node k 6= 0 defines a commodity: one unit of commodity k originates at the root node and must be delivered to node k; the variable φkij denotes the flow of commodity k in arc hi, ji. We first replace (4–9) by (18–22"
P09-1039,W08-2102,0,0.0499262,"ed simple higher order features that look at the word, part-of-speech tag, and (if available) morphological information of the words being correlated through the indicator variables. For scalability (and noting that some of the models require O(|V |· |A|) constraints and variables, which, when A = V 2 , grows cubically with the number of words), we first prune the base graph by running a simple algorithm that ranks the k-best candidate parents for each word in the sentence (we set k = 10); this reduces the number of candidate arcs to |A |= kn.11 This strategy is similar to the one employed by Carreras et al. (2008) to prune the search space of the actual parser. The ranker is a local model trained using a max-margin criterion; it is arc-factored and not subject to any structural constraints, so it is very fast. The actual parser was trained via the online structured passive-aggressive algorithm of Crammer et al. (2006); it differs from the 1-best MIRA algorithm of McDonald et al. (2005) by solving a sequence of loss-augmented inference problems.12 The number of iterations was set to 10. The results are summarized in Table 1; for the sake of comparison, we reproduced three strong 8 We used the provided t"
P09-1039,D07-1101,0,0.261932,"the first child of a given word. The ability to handle such “ordered” features is intimately associated with Eisner’s dynamic programming parsing algorithm and with the Markovian assumptions made explicitly by his generative model. We next show how similar features zijk V but this would yield a constraint matrix with O(n4 ) non-zero elements. Instead, we define auxiliary variables βjk and γij : sibl zijk ≥ zij + zik − 1 (14) for all triples hi, j, ki ∈ Rsibl , and zijk if hi, ji and hi, ki are consecutive siblings,   0 otherwise, ( first child zij As shown by McDonald and Pereira (2006) and Carreras (2007), the inclusion of features that correlate sibling and grandparent arcs may be highly beneficial, even if doing so requires resorting to approximate algorithms.7 Define Rsibl , {hi, j, ki |hi, ji ∈ A, hi, ki ∈ A} and Rgrand , {hi, j, ki |hi, ji ∈ A, hj, ki ∈ A}. To include such features in our formulation, we need to add extra variables zsibl , hzr ir∈Rsibl and zgrand , hzr ir∈Rgrand that indicate the presence of sibling and grandparent arcs. Observe that these indicator variables are conjunctions of arc indicator varisibl = z ∧ z and z grand = z ∧ z . ables, i.e., zijk ij ij ik jk ijk Hence,"
P09-1039,P04-1054,0,0.018088,"ather than algorithm design. While solving an ILP is NP-hard in general, fast solvers are available today that make it a practical solution for many NLP problems. This paper presents new, concise ILP formulations for projective and non-projective depen2 2.1 Dependency Parsing Preliminaries A dependency tree is a lightweight syntactic representation that attempts to capture functional relationships between words. Lately, this formalism has been used as an alternative to phrase-based parsing for a variety of tasks, ranging from machine translation (Ding and Palmer, 2005) to relation extraction (Culotta and Sorensen, 2004) and question answering (Wang et al., 2007). Let us first describe formally the set of legal dependency parse trees. Consider a sentence x = 342 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 342–350, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP hw0 , . . . , wn i, where wi denotes the word at the ith position, and w0 = $ is a wall symbol. We form the (complete1 ) directed graph D = hV, Ai, with vertices in V = {0, . . . , n} (the i-th vertex corresponding to the i-th word) and arcs in A = V 2 . Using terminology from graph theory, we say t"
P09-1039,N07-1030,0,0.0284769,"e also compatible with scenarios where expert knowledge is available, for example in the form of hard or soft firstorder logic constraints (Richardson and Domingos, 2006; Chang et al., 2008). Much attention has recently been devoted to integer linear programming (ILP) formulations of NLP problems, with interesting results in applications like semantic role labeling (Roth and Yih, 2005; Punyakanok et al., 2004), dependency parsing (Riedel and Clarke, 2006), word alignment for machine translation (Lacoste-Julien et al., 2006), summarization (Clarke and Lapata, 2008), and coreference resolution (Denis and Baldridge, 2007), among others. In general, the rationale for the development of ILP formulations is to incorporate non-local features or global constraints, which are often difficult to handle with traditional algorithms. ILP formulations focus more on the modeling of problems, rather than algorithm design. While solving an ILP is NP-hard in general, fast solvers are available today that make it a practical solution for many NLP problems. This paper presents new, concise ILP formulations for projective and non-projective depen2 2.1 Dependency Parsing Preliminaries A dependency tree is a lightweight syntactic"
P09-1039,P05-1067,0,0.026854,"tions focus more on the modeling of problems, rather than algorithm design. While solving an ILP is NP-hard in general, fast solvers are available today that make it a practical solution for many NLP problems. This paper presents new, concise ILP formulations for projective and non-projective depen2 2.1 Dependency Parsing Preliminaries A dependency tree is a lightweight syntactic representation that attempts to capture functional relationships between words. Lately, this formalism has been used as an alternative to phrase-based parsing for a variety of tasks, ranging from machine translation (Ding and Palmer, 2005) to relation extraction (Culotta and Sorensen, 2004) and question answering (Wang et al., 2007). Let us first describe formally the set of legal dependency parse trees. Consider a sentence x = 342 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 342–350, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP hw0 , . . . , wn i, where wi denotes the word at the ith position, and w0 = $ is a wall symbol. We form the (complete1 ) directed graph D = hV, Ai, with vertices in V = {0, . . . , n} (the i-th vertex corresponding to the i-th word) and arcs in A ="
P09-1039,P99-1059,0,0.0534909,"grammar b of Wang and Har work on empiric note include the w ing systems, no showing that the of Wang and Ha note include the showing that th shown by McDonald et al. (2005), is an instance of the maximal arborescence problem. Combinatorial algorithms (Chu and Liu, 1965; Edmonds, 1967) can solve this problem in cubic time.4 If the dependency parse trees are restricted to be projective, cubic-time algorithms are available via dynamic programming (Eisner, 1996). While in the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta, 1999), the same does not happen in the nonprojective case, where finding the highest-scoring tree becomes NP-hard (McDonald and Satta, 2007). Approximate algorithms have been employed to handle models that are not arc-factored (although features are still fairly local): McDonald and Pereira (2006) adopted an approximation based on O(n3 ) projective parsing followed by a hillclimbing algorithm to rearrange arcs, and Smith and Eisner (2008) proposed an algorithm based on loopy belief propagation. 3 fact, fast solvers are available today that make this a practical solution for many problems. Their per"
P09-1039,C96-1058,0,0.889999,"ss; the typical loss functhat they can be used in many important learning tions over all possible dependency graphs for a given problems including min-risk decodtionandis inference the Hamming loss, `(y 0 ; y) , |{hi, ji ∈ sentence. To motivate these algorithms, we show ing, training globally normalized log-linear mody 0 that : hi,they ji ∈ /cany}|. Tractability is usually ensured be used in many important learning els, syntactic language modeling, and unsupervised byand strong factorization like the one inference problemsassumptions, including min-risk decodunderlying the arc-factored model (Eisner, 1996; ing, training globally normalized log-linear modMcDonald et al., 2005),modeling, which forbids any feature els, syntactic language and unsupervised that depends on two or more arcs. This induces a decomposition of the feature vector f (x, y) as: 1 The general case where A ⊆ V 2 is also of interest; it arises whenever a constraint or a lexicon forbids some arcs from appearing in dependency tree. It may also arise as a consequence of a first-stage pruning step where some candidate arcs are eliminated; this will be further discussed in §4. 2 Or “directed spanning tree with designated root r.” 3"
P09-1039,P98-1106,0,0.0175876,"0, . . . , n} (the i-th vertex corresponding to the i-th word) and arcs in A = V 2 . Using terminology from graph theory, we say that B ⊆ A is an r-arborescence2 of the directed graph D if hV, Bi is a (directed) tree rooted at r. We define the set of legal dependency parse trees of x (denoted Y(x)) as the set of 0-arborescences of D, i.e., we admit each arborescence as a potential dependency tree. Let y ∈ Y(x) be a legal dependency tree for x; if the arc a = hi, ji ∈ y, we refer to i as the parent of j (denoted i = π(j)) and j as a child of i. We also say that a is projective (in the sense of Kahane et al., 1998) if any vertex k in the span of a is reachable from i (in other words, if for any k satisfying min(i, j) &lt; k &lt; max(i, j), there is a directed path in y from i to k). A dependency tree is called projective if it only contains projective arcs. Fig. 1 illustrates this concept.3 The formulation to be introduced in §3 makes use of the notion of the incidence vector associated with a dependency tree y ∈ Y(x). This is the binary vector z , hza ia∈A with each component defined as za = I(a ∈ y) (here, I(.) denotes the indicator function). Considering simultaneously all incidence vectors of legal depend"
P09-1039,N06-1015,0,0.00916773,"of the new parsers on standard parsing tasks in seven languages. The techniques that we present are also compatible with scenarios where expert knowledge is available, for example in the form of hard or soft firstorder logic constraints (Richardson and Domingos, 2006; Chang et al., 2008). Much attention has recently been devoted to integer linear programming (ILP) formulations of NLP problems, with interesting results in applications like semantic role labeling (Roth and Yih, 2005; Punyakanok et al., 2004), dependency parsing (Riedel and Clarke, 2006), word alignment for machine translation (Lacoste-Julien et al., 2006), summarization (Clarke and Lapata, 2008), and coreference resolution (Denis and Baldridge, 2007), among others. In general, the rationale for the development of ILP formulations is to incorporate non-local features or global constraints, which are often difficult to handle with traditional algorithms. ILP formulations focus more on the modeling of problems, rather than algorithm design. While solving an ILP is NP-hard in general, fast solvers are available today that make it a practical solution for many NLP problems. This paper presents new, concise ILP formulations for projective and non-pr"
P09-1039,D08-1017,1,0.875982,": • Disabled arcs do not carry any flow: φka ≤ za , a ∈ A, k ∈ V (20)  0 0   i ≤ π(k) ≤ j , if i0 &lt; k &lt; j 0 , π(k) &lt; i0 ∨ π(k) &gt; j 0 , if k &lt; i0 or k &gt; j 0   or k = i. • There are exactly n enabled arcs: P a∈A za =n (21) 347 Then, Y(x) will be redefined as the set of projective dependency parse trees. baselines, all of them state-of-the-art parsers based on non-arc-factored models: the second order model of McDonald and Pereira (2006), the hybrid model of Nivre and McDonald (2008), which combines a (labeled) transition-based and a graphbased parser, and a refinement of the latter, due to Martins et al. (2008), which attempts to approximate non-local features.13 We did not reproduce the model of Riedel and Clarke (2006) since the latter is tailored for labeled dependency parsing; however, experiments reported in that paper for Dutch (and extended to other languages in the CoNLL-X task) suggest that their model performs worse than our three baselines. By looking at the middle four columns, we can see that adding non-arc-factored features makes the models more accurate, for all languages. With the exception of Portuguese, the best results are achieved with the full set of features. We can also observ"
P09-1039,E06-1011,0,0.807214,"n solve this problem in cubic time.4 If the dependency parse trees are restricted to be projective, cubic-time algorithms are available via dynamic programming (Eisner, 1996). While in the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta, 1999), the same does not happen in the nonprojective case, where finding the highest-scoring tree becomes NP-hard (McDonald and Satta, 2007). Approximate algorithms have been employed to handle models that are not arc-factored (although features are still fairly local): McDonald and Pereira (2006) adopted an approximation based on O(n3 ) projective parsing followed by a hillclimbing algorithm to rearrange arcs, and Smith and Eisner (2008) proposed an algorithm based on loopy belief propagation. 3 fact, fast solvers are available today that make this a practical solution for many problems. Their performance depends on the dimensions and degree of sparsity of the constraint matrix A. Riedel and Clarke (2006) proposed an ILP formulation for dependency parsing which refines the arc-factored model by imposing linguistically motivated “hard” constraints that forbid some arc configurations. T"
P09-1039,C98-1102,0,\N,Missing
P13-1020,P13-1020,1,0.106198,"Missing"
P13-1020,P11-1049,0,0.730488,"Missing"
P13-1020,D11-1003,0,0.0291757,"Missing"
P13-1020,D07-1001,0,0.0202351,"Missing"
P13-1020,C08-1018,0,0.0372508,"Missing"
P13-1020,W03-1101,0,0.116763,"Missing"
P13-1020,S12-1029,1,0.790816,"Missing"
P13-1020,W04-1013,0,0.135089,"Missing"
P13-1020,P07-1033,0,0.202493,"Missing"
P13-1020,W09-1801,1,0.86418,"Missing"
P13-1020,C04-1057,0,0.368749,"Missing"
P13-1020,D11-1022,1,0.801606,"Missing"
P13-1020,P10-1074,0,0.016643,"Missing"
P13-1020,D12-1065,0,0.151794,"Missing"
P13-1020,E06-1038,0,0.159294,"Missing"
P13-1020,N04-1019,0,0.197325,"Missing"
P13-1020,N10-1134,0,0.40211,"Missing"
P13-1020,W00-0403,0,0.492696,"Missing"
P13-1020,D10-1001,0,0.134452,"Missing"
P13-1020,P08-2052,0,0.0161402,"Missing"
P13-1020,E12-1023,0,0.192034,"Missing"
P13-1020,P10-1058,0,0.269214,"Missing"
P13-1020,D11-1038,0,0.0396277,"Missing"
P13-1020,D12-1022,0,0.542652,"Missing"
P13-2109,C96-1058,0,0.508512,"1 Introduction • We apply the third-order feature models of Koo and Collins (2010) to non-projective parsing. Dependency parsing has become a prominent approach to syntax in the last few years, with increasingly fast and accurate models being devised (K¨ubler et al., 2009; Huang and Sagae, 2010; Zhang and Nivre, 2011; Rush and Petrov, 2012). In projective parsing, the arcs in the dependency tree are constrained to be nested, and the problem of finding the best tree can be addressed with dynamic programming. This results in cubic-time decoders for arc-factored and sibling second-order models (Eisner, 1996; McDonald and Pereira, 2006), and quartic-time for grandparent models (Carreras, 2007) and third-order models (Koo and Collins, 2010). Recently, Rush and Petrov (2012) trained third-order parsers with vine pruning cascades, achieving runtimes only a small factor slower than first-order systems. Third-order features have also been included in transition systems (Zhang and Nivre, 2011) and graph-based parsers with cube-pruning (Zhang and McDonald, 2012). Unfortunately, non-projective dependency parsers (appropriate for languages with a more flexible word order, such as Czech, Dutch, and German)"
P13-2109,W06-2933,0,0.0918345,"Missing"
P13-2109,P10-1110,0,0.0539693,"proach uses AD3 , an accelerated dual decomposition algorithm which we extend to handle specialized head automata and sequential head bigram models. Experiments in fourteen languages yield parsing speeds competitive to projective parsers, with state-ofthe-art accuracies for the largest datasets (English, Czech, and German). 1 Introduction • We apply the third-order feature models of Koo and Collins (2010) to non-projective parsing. Dependency parsing has become a prominent approach to syntax in the last few years, with increasingly fast and accurate models being devised (K¨ubler et al., 2009; Huang and Sagae, 2010; Zhang and Nivre, 2011; Rush and Petrov, 2012). In projective parsing, the arcs in the dependency tree are constrained to be nested, and the problem of finding the best tree can be addressed with dynamic programming. This results in cubic-time decoders for arc-factored and sibling second-order models (Eisner, 1996; McDonald and Pereira, 2006), and quartic-time for grandparent models (Carreras, 2007) and third-order models (Koo and Collins, 2010). Recently, Rush and Petrov (2012) trained third-order parsers with vine pruning cascades, achieving runtimes only a small factor slower than first-or"
P13-2109,N12-1054,0,0.683007,"ition algorithm which we extend to handle specialized head automata and sequential head bigram models. Experiments in fourteen languages yield parsing speeds competitive to projective parsers, with state-ofthe-art accuracies for the largest datasets (English, Czech, and German). 1 Introduction • We apply the third-order feature models of Koo and Collins (2010) to non-projective parsing. Dependency parsing has become a prominent approach to syntax in the last few years, with increasingly fast and accurate models being devised (K¨ubler et al., 2009; Huang and Sagae, 2010; Zhang and Nivre, 2011; Rush and Petrov, 2012). In projective parsing, the arcs in the dependency tree are constrained to be nested, and the problem of finding the best tree can be addressed with dynamic programming. This results in cubic-time decoders for arc-factored and sibling second-order models (Eisner, 1996; McDonald and Pereira, 2006), and quartic-time for grandparent models (Carreras, 2007) and third-order models (Koo and Collins, 2010). Recently, Rush and Petrov (2012) trained third-order parsers with vine pruning cascades, achieving runtimes only a small factor slower than first-order systems. Third-order features have also bee"
P13-2109,P10-1001,0,0.831259,"ll behind those of projective parsers. This paper bridges the gap above by presenting the following contributions: We present fast, accurate, direct nonprojective dependency parsers with thirdorder features. Our approach uses AD3 , an accelerated dual decomposition algorithm which we extend to handle specialized head automata and sequential head bigram models. Experiments in fourteen languages yield parsing speeds competitive to projective parsers, with state-ofthe-art accuracies for the largest datasets (English, Czech, and German). 1 Introduction • We apply the third-order feature models of Koo and Collins (2010) to non-projective parsing. Dependency parsing has become a prominent approach to syntax in the last few years, with increasingly fast and accurate models being devised (K¨ubler et al., 2009; Huang and Sagae, 2010; Zhang and Nivre, 2011; Rush and Petrov, 2012). In projective parsing, the arcs in the dependency tree are constrained to be nested, and the problem of finding the best tree can be addressed with dynamic programming. This results in cubic-time decoders for arc-factored and sibling second-order models (Eisner, 1996; McDonald and Pereira, 2006), and quartic-time for grandparent models"
P13-2109,D10-1001,0,0.0581119,"s et al. (2011), the problem of obtaining the best-scored tree can be written as follows: PS maximize s=1 fs (z s ) Figure 1: Parts considered in this paper. Firstorder models factor over arcs (Eisner, 1996; McDonald et al., 2005), and second-order models include also consecutive siblings and grandparents (Carreras, 2007). Our parsers add also arbitrary siblings (not necessarily consecutive) and head bigrams, as in Martins et al. (2011), in addition to third-order features for grand- and tri-siblings (Koo and Collins, 2010). problems in a modular and extensible manner (Komodakis et al., 2007; Rush et al., 2010). In this paper, we employ alternating directions dual decomposition (AD3 ; Martins et al., 2011). Like the subgradient algorithm of Rush et al. (2010), AD3 splits the original problem into local subproblems, and seeks an agreement on the overlapping variables. The difference is that the AD3 subproblems have an additional quadratic term to accelerate consensus. Recent analysis (Martins et al., 2012) has shown that: (i) AD3 converges at a faster rate,2 and (ii) the quadratic subproblems can be solved using the same combinatorial machinery that is used in the subgradient algorithm. This opens th"
P13-2109,D10-1125,0,0.826073,"ng is NP-hard beyond arc-factored models (McDonald • This extension is non-trivial since exact dynamic programming is not applicable. Instead, we adapt AD3 , the dual decomposition algorithm proposed by Martins et al. (2011), to handle third-order features, by introducing specialized head automata. • We make our parser substantially faster than the many-components approach of Martins et al. (2011). While AD3 requires solving quadratic subproblems as an intermediate step, recent results (Martins et al., 2012) show that they can be addressed with the same oracles used in the subgradient method (Koo et al., 2010). This enables AD3 to exploit combinatorial subproblems like the the head automata above. Along with this paper, we provide a free distribution of our parsers, including training code.1 2 Dependency Parsing with AD3 Dual decomposition is a class of optimization techniques that tackle the dual of combinatorial 1 Released as TurboParser 2.1, and publicly available at http://www.ark.cs.cmu.edu/TurboParser. 617 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 617–622, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics fo"
P13-2109,D08-1016,0,0.138862,"/TurboParser. 617 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 617–622, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics formed trees. Let {As }Ss=1 be a cover of A, where each As ⊆ A. We assume that the score PS of a parse tree u ∈ Y decomposes as f (u) := s=1 fs (z s ), where each z s := hzs,a ia∈As is a “partial view” of u, and each local score function fs comes from a feature-based linear model. Past work in dependency parsing considered either (i) a few “large” components, such as trees and head automata (Smith and Eisner, 2008; Koo et al., 2010), or (ii) many “small” components, coming from a multi-commodity flow formulation (Martins et al., 2009, 2011). Let Ys ⊆ R|As |denote the set of feasible realizations of z s , i.e., those that are partial views of an actual Q parse tree. A tuple of views hz 1 , . . . , z S i ∈ Ss=1 Ys is said to be globally consistent if zs,a = zs0 ,a holds for every a, s and s0 such that a ∈ As ∩As0 . We assume each parse u ∈ Y corresponds uniquely to a globally consistent tuple of views, and vice-versa. Following Martins et al. (2011), the problem of obtaining the best-scored tree can be w"
P13-2109,P09-1039,1,0.954383,"Fast Third-Order Non-Projective Turbo Parsers Andr´e F. T. Martins∗† Miguel B. Almeida∗† Noah A. Smith# Priberam Labs, Alameda D. Afonso Henriques, 41, 2o , 1000-123 Lisboa, Portugal Instituto de Telecomunicac¸o˜ es, Instituto Superior T´ecnico, 1049-001 Lisboa, Portugal # School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA {atm,mba}@priberam.pt, nasmith@cs.cmu.edu ∗ † Abstract and Satta, 2007). Approximate parsers have therefore been introduced, based on belief propagation (Smith and Eisner, 2008), dual decomposition (Koo et al., 2010), or multi-commodity flows (Martins et al., 2009, 2011). These are all instances of turbo parsers, as shown by Martins et al. (2010): the underlying approximations come from the fact that they run global inference in factor graphs ignoring loop effects. While this line of research has led to accuracy gains, none of these parsers use third-order contexts, and their speeds are well behind those of projective parsers. This paper bridges the gap above by presenting the following contributions: We present fast, accurate, direct nonprojective dependency parsers with thirdorder features. Our approach uses AD3 , an accelerated dual decomposition al"
P13-2109,W08-2121,0,0.235101,"Missing"
P13-2109,D10-1004,1,0.919751,"ida∗† Noah A. Smith# Priberam Labs, Alameda D. Afonso Henriques, 41, 2o , 1000-123 Lisboa, Portugal Instituto de Telecomunicac¸o˜ es, Instituto Superior T´ecnico, 1049-001 Lisboa, Portugal # School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA {atm,mba}@priberam.pt, nasmith@cs.cmu.edu ∗ † Abstract and Satta, 2007). Approximate parsers have therefore been introduced, based on belief propagation (Smith and Eisner, 2008), dual decomposition (Koo et al., 2010), or multi-commodity flows (Martins et al., 2009, 2011). These are all instances of turbo parsers, as shown by Martins et al. (2010): the underlying approximations come from the fact that they run global inference in factor graphs ignoring loop effects. While this line of research has led to accuracy gains, none of these parsers use third-order contexts, and their speeds are well behind those of projective parsers. This paper bridges the gap above by presenting the following contributions: We present fast, accurate, direct nonprojective dependency parsers with thirdorder features. Our approach uses AD3 , an accelerated dual decomposition algorithm which we extend to handle specialized head automata and sequential head bigr"
P13-2109,W03-3023,0,0.510551,"Missing"
P13-2109,D11-1022,1,0.264644,"der systems. Third-order features have also been included in transition systems (Zhang and Nivre, 2011) and graph-based parsers with cube-pruning (Zhang and McDonald, 2012). Unfortunately, non-projective dependency parsers (appropriate for languages with a more flexible word order, such as Czech, Dutch, and German) lag behind these recent advances. The main obstacle is that non-projective parsing is NP-hard beyond arc-factored models (McDonald • This extension is non-trivial since exact dynamic programming is not applicable. Instead, we adapt AD3 , the dual decomposition algorithm proposed by Martins et al. (2011), to handle third-order features, by introducing specialized head automata. • We make our parser substantially faster than the many-components approach of Martins et al. (2011). While AD3 requires solving quadratic subproblems as an intermediate step, recent results (Martins et al., 2012) show that they can be addressed with the same oracles used in the subgradient method (Koo et al., 2010). This enables AD3 to exploit combinatorial subproblems like the the head automata above. Along with this paper, we provide a free distribution of our parsers, including training code.1 2 Dependency Parsing"
P13-2109,D12-1030,0,0.831572,"em of finding the best tree can be addressed with dynamic programming. This results in cubic-time decoders for arc-factored and sibling second-order models (Eisner, 1996; McDonald and Pereira, 2006), and quartic-time for grandparent models (Carreras, 2007) and third-order models (Koo and Collins, 2010). Recently, Rush and Petrov (2012) trained third-order parsers with vine pruning cascades, achieving runtimes only a small factor slower than first-order systems. Third-order features have also been included in transition systems (Zhang and Nivre, 2011) and graph-based parsers with cube-pruning (Zhang and McDonald, 2012). Unfortunately, non-projective dependency parsers (appropriate for languages with a more flexible word order, such as Czech, Dutch, and German) lag behind these recent advances. The main obstacle is that non-projective parsing is NP-hard beyond arc-factored models (McDonald • This extension is non-trivial since exact dynamic programming is not applicable. Instead, we adapt AD3 , the dual decomposition algorithm proposed by Martins et al. (2011), to handle third-order features, by introducing specialized head automata. • We make our parser substantially faster than the many-components approach"
P13-2109,P11-2033,0,0.121241,"celerated dual decomposition algorithm which we extend to handle specialized head automata and sequential head bigram models. Experiments in fourteen languages yield parsing speeds competitive to projective parsers, with state-ofthe-art accuracies for the largest datasets (English, Czech, and German). 1 Introduction • We apply the third-order feature models of Koo and Collins (2010) to non-projective parsing. Dependency parsing has become a prominent approach to syntax in the last few years, with increasingly fast and accurate models being devised (K¨ubler et al., 2009; Huang and Sagae, 2010; Zhang and Nivre, 2011; Rush and Petrov, 2012). In projective parsing, the arcs in the dependency tree are constrained to be nested, and the problem of finding the best tree can be addressed with dynamic programming. This results in cubic-time decoders for arc-factored and sibling second-order models (Eisner, 1996; McDonald and Pereira, 2006), and quartic-time for grandparent models (Carreras, 2007) and third-order models (Koo and Collins, 2010). Recently, Rush and Petrov (2012) trained third-order parsers with vine pruning cascades, achieving runtimes only a small factor slower than first-order systems. Third-orde"
P13-2109,E06-1011,0,0.644051,"n • We apply the third-order feature models of Koo and Collins (2010) to non-projective parsing. Dependency parsing has become a prominent approach to syntax in the last few years, with increasingly fast and accurate models being devised (K¨ubler et al., 2009; Huang and Sagae, 2010; Zhang and Nivre, 2011; Rush and Petrov, 2012). In projective parsing, the arcs in the dependency tree are constrained to be nested, and the problem of finding the best tree can be addressed with dynamic programming. This results in cubic-time decoders for arc-factored and sibling second-order models (Eisner, 1996; McDonald and Pereira, 2006), and quartic-time for grandparent models (Carreras, 2007) and third-order models (Koo and Collins, 2010). Recently, Rush and Petrov (2012) trained third-order parsers with vine pruning cascades, achieving runtimes only a small factor slower than first-order systems. Third-order features have also been included in transition systems (Zhang and Nivre, 2011) and graph-based parsers with cube-pruning (Zhang and McDonald, 2012). Unfortunately, non-projective dependency parsers (appropriate for languages with a more flexible word order, such as Czech, Dutch, and German) lag behind these recent adva"
P13-2109,W07-2216,0,0.17719,"Missing"
P13-2109,H05-1066,0,0.829343,"⊆ R|As |denote the set of feasible realizations of z s , i.e., those that are partial views of an actual Q parse tree. A tuple of views hz 1 , . . . , z S i ∈ Ss=1 Ys is said to be globally consistent if zs,a = zs0 ,a holds for every a, s and s0 such that a ∈ As ∩As0 . We assume each parse u ∈ Y corresponds uniquely to a globally consistent tuple of views, and vice-versa. Following Martins et al. (2011), the problem of obtaining the best-scored tree can be written as follows: PS maximize s=1 fs (z s ) Figure 1: Parts considered in this paper. Firstorder models factor over arcs (Eisner, 1996; McDonald et al., 2005), and second-order models include also consecutive siblings and grandparents (Carreras, 2007). Our parsers add also arbitrary siblings (not necessarily consecutive) and head bigrams, as in Martins et al. (2011), in addition to third-order features for grand- and tri-siblings (Koo and Collins, 2010). problems in a modular and extensible manner (Komodakis et al., 2007; Rush et al., 2010). In this paper, we employ alternating directions dual decomposition (AD3 ; Martins et al., 2011). Like the subgradient algorithm of Rush et al. (2010), AD3 splits the original problem into local subproblems, and"
P13-2109,W06-2932,0,0.0291159,"2 785 93.52 2,996 92.69 740 86.01 366 85.59 318 91.14 684 76.90 793 Best published UAS UAS Tok/sec 81.12 - Ma11 93.50 - Ma11 91.89 - Ma10 89.46 - Ma11 91.86 - Ma11 85.81 121 Ko10 91.89 - Ma11 92.68 - Ma11 93.72 - Ma11 93.03 79 Ko10 86.95 - Ma11 87.48 - ZM12 91.44 - ZM12 77.55 258 Ko10 RP12 UAS Tok/sec 91.9 3,980 90.9 7,800 90.8 2,880 92.3 8,600 91.5 2,900 90.1 5,320 - ZM12 UAS 93.08 91.35 93.24 91.69 87.48 91.44 - Table 3: Results for the CoNLL-2006 datasets and the non-projective English dataset of CoNLL-2008. “Best Published UAS” includes the most accurate parsers among Nivre et al. (2006), McDonald et al. (2006), Martins et al. (2010, 2011), Koo et al. (2010), Rush and Petrov (2012), Zhang and McDonald (2012). The last two are shown separately in the rightmost columns. work was partially supported by the EU/FEDER programme, QREN/POR Lisboa (Portugal), under the Intelligo project (contract 2012/24803), by a FCT grant PTDC/EEI-SII/2312/2012, and by NSF grant IIS-1054319. In our second experiment (Tab. 3), we used 14 datasets, most of which are non-projective, from the CoNLL 2006 and 2008 shared tasks (Buchholz and Marsi, 2006; Surdeanu et al., 2008). Our third-order model achieved the best reported sco"
P13-2109,W06-2920,0,\N,Missing
P13-2109,D07-1101,0,\N,Missing
P15-1040,H05-1073,0,0.0424634,"abeled data), and to a delexicalized model trained using universal tags and bilingual word embeddings. 1 Introduction The goal of opinion mining is to extract opinions and sentiments from text (Pang and Lee, 2008; Wilson, 2008; Liu, 2012). With the advent of social media and the increasing amount of data available on the Web, this has become a very active area of research, with applications in summarization of customer reviews (Hu and Liu, 2004; Wu et al., 2011), tracking of newswire and blogs (Ku et al., 2006), question answering (Yu and Hatzivassiloglou, 2003), and text-to-speech synthesis (Alm et al., 2005). While early work has focused on determining sentiment at document and sentence level (Pang et al., 2002; Turney, 2002; Balog et al., 2006), research has gradually progressed towards finegrained opinion mining, where rather than determining global sentiment, the goal is to parse text 1 http://mpqa.cs.pitt.edu/corpora/mpqa_ corpus. 2 Besides English, monolingual systems have also been developed for Chinese and Japanese (Seki et al., 2007), German (Clematide et al., 2012) and Bengali (Das and Bandyopadhyay, 2010). 408 Proceedings of the 53rd Annual Meeting of the Association for Computational L"
P15-1040,D14-1080,0,0.0592607,"Missing"
P15-1040,almeida-etal-2014-priberam,1,0.8381,"s below 0.95. After annotating the English side of FAPESP b e in Algorithm 1, with the pre-trained system (D with a total of 166,719 sentences and 81,492 opinions), the high confidence alignments (De↔f ) are used to project the annotations to the Portuguese side of the corpus. The automatic annotations produced by our dependency-based system are easily 6.3 Portuguese Opinion Mining Corpus For validation purposes, we also created a Portuguese corpus with manually annotated finegrained opinions. The corpus consists of a subset of the documents of the Priberam Compressive Summarization Corpus11 (Almeida et al., 2014), which contains 80 news topics with 10 documents each, collected from several Portuguese newspapers, TV and radio websites in the biennia 2010– 2011 and 2012–2013. In the scope of the current work, we selected and annotated one document of each of the 80 topics. The first biennium was selected as the test set and the second biennium was split into development and training sets (see Ta11 http://labs.priberam.com/Resources/ PCSC 414 ble 2 for statistics). Train Dev Test #doc. 20 20 40 #sent. 441 225 560 Op. Op-Ag. Op-Tg. Op-Pol. #opin. 240 197 391 7 HM 77.0 69.1 61.9 49.4 PM 76.7 72.3 65.4 49.1"
P15-1040,C10-1059,0,0.34685,"s. While all these works are relevant, none addresses fine-grained opinion mining in its full generality, where the goal is to predict full opinion frames. Related Work A considerable amount of work on fine-grained opinion mining is based on the MPQA corpus. Kim and Hovy (2006) proposed a method for finding opinion holders and topics, with the aid of a semantic role labeler. Choi et al. (2005) and Breck et al. (2007) used CRFs for finding opinion holders and recognizing opinion expressions, respectively. The two things are predicted jointly by Choi et al. (2006), with integer programming, and Johansson and Moschitti (2010), via reranking. The same method was applied later for joint prediction of opinion expressions and their polarities (Johansson and Moschitti, 2011). The advantage of a joint model was also shown by Choi and Cardie (2010) and Yang and Cardie (2014). Yang and Cardie (2012) classified expressions with a semiMarkov decoder, outperforming a B-I-O tagger; in later work, the same authors proposed an ILP decoder to jointly retrieve opinion expressions, holders, and targets (Yang and Cardie, 2013). A more recent work (˙Irsoy and Cardie, 2014) proposes a recurrent neural network to identify opinion span"
P15-1040,P11-2018,0,0.127366,"frames. Related Work A considerable amount of work on fine-grained opinion mining is based on the MPQA corpus. Kim and Hovy (2006) proposed a method for finding opinion holders and topics, with the aid of a semantic role labeler. Choi et al. (2005) and Breck et al. (2007) used CRFs for finding opinion holders and recognizing opinion expressions, respectively. The two things are predicted jointly by Choi et al. (2006), with integer programming, and Johansson and Moschitti (2010), via reranking. The same method was applied later for joint prediction of opinion expressions and their polarities (Johansson and Moschitti, 2011). The advantage of a joint model was also shown by Choi and Cardie (2010) and Yang and Cardie (2014). Yang and Cardie (2012) classified expressions with a semiMarkov decoder, outperforming a B-I-O tagger; in later work, the same authors proposed an ILP decoder to jointly retrieve opinion expressions, holders, and targets (Yang and Cardie, 2013). A more recent work (˙Irsoy and Cardie, 2014) proposes a recurrent neural network to identify opinion spans. All the approaches above rely on a span-based representation of the opinion elements. This makes joint decoding procedures more complicated, sin"
P15-1040,E06-2031,0,0.0212436,"Missing"
P15-1040,J13-3002,0,0.209782,"ed as OPINION if and only if it comes from the root node. 3. Arcs labeled as AGENT or TARGET must come from an opinion node (i.e., a node with an incoming OPINION arc). 4. Every opinion node has exactly one AGENT and one TARGET outgoing arcs (possibly implicit).5 Dependency Graph Figure 1 depicts a sentence-level dependency representation for fine-grained opinion mining. The overall structure is a graph whose nodes are head words (plus two special nodes, root and null), connected by labeled arcs, as outlined below. Similarly to prior work (Choi and Cardie, 2010; Johansson and Moschitti, 2011; Johansson and Moschitti, 2013), we map the MPQA’s polarityinto three levels: positive, negative and neutral, where the latter includes spans without polarity annotation or annotated as “both”. As in Johansson and Moschitti (2013), we also ignore the “uncertain” aspect of the annotated polarities. Determining head nodes. The three opinion elements that we want to detect (opinions, agents and targets) are each represented by a head node, which corresponds to a single word (underlined in Figure 1). When converting the MPQA corpus to dependencies, we determine this “representative” word automatically, by using the following si"
P15-1040,D08-1014,0,0.144994,"also used by Wu et al. (2011) for fine-grained opinion mining. Our work differs in which we mine opinions in news articles instead of product reviews, a considerably different task. In addition, the approach of Wu et al. (2011) relies on “span nodes” (instead of head words), requiring solving an ILP followed by an approximate heuristic. Query-based multilingual opinion mining was addressed in several NTCIR shared tasks (Seki et al., 2007; Seki et al., 2010).4 However, to our best knowledge, a cross-lingual approach has never been attempted. Some steps were taken by Mihalcea et al. (2007) and Banea et al. (2008), who translated an English lexicon and the MPQA corpus to Romanian and Spanish, but for the much simpler task of sentence-level subjectivity analysis. Cross-lingual sentiment classification was addressed by Wan (2009), Prettenhofer and Stein (2010) and Wei and Pal (2010) at document level, and by Lu et al. (2011) at sentence level. Recently, Gui et al. (2013) applied projection learning for opinion mining in Chinese. However, this work only addresses agent detection and requires translating the MPQA corpus. While all these works are relevant, none addresses fine-grained opinion mining in its"
P15-1040,W06-0301,0,0.419931,"iment classification was addressed by Wan (2009), Prettenhofer and Stein (2010) and Wei and Pal (2010) at document level, and by Lu et al. (2011) at sentence level. Recently, Gui et al. (2013) applied projection learning for opinion mining in Chinese. However, this work only addresses agent detection and requires translating the MPQA corpus. While all these works are relevant, none addresses fine-grained opinion mining in its full generality, where the goal is to predict full opinion frames. Related Work A considerable amount of work on fine-grained opinion mining is based on the MPQA corpus. Kim and Hovy (2006) proposed a method for finding opinion holders and topics, with the aid of a semantic role labeler. Choi et al. (2005) and Breck et al. (2007) used CRFs for finding opinion holders and recognizing opinion expressions, respectively. The two things are predicted jointly by Choi et al. (2006), with integer programming, and Johansson and Moschitti (2010), via reranking. The same method was applied later for joint prediction of opinion expressions and their polarities (Johansson and Moschitti, 2011). The advantage of a joint model was also shown by Choi and Cardie (2010) and Yang and Cardie (2014)."
P15-1040,W04-3250,0,0.0730223,"on. 413 Op. Op-Ag. Op-Tg. Op-Pol. JM13, BASIC HM PM OM 56.3 56.2 60.6 40.3 47.1 44.9 46.1 45.9 49.3 JM13, R ERANKING HM PM OM 58.6 59.2 63.7 42.4 51.4 48.1 48.5 48.9* 52.5 HM 61.6* 45.7* 31.3* 47.9 O UR S YSTEM PM OM 59.8 65.1 51.4 50.3* 48.3* 48.3* 47.0 50.7 Table 1: Method comparison: F1 scores obtained in the MPQA corpus, for our dependency based method and the approaches in Johansson and Moschitti (2013), with and without reranking. The symbol * indicates that the best system beats the other systems with statistical significance, with p &lt; 0.05 and according to a bootstrap resampling test (Koehn, 2004). Figure 2: Excerpt of a bitext document from FAPESP, with automatic opinion dependencies. The annotations are directly projected to Portuguese via automatic word alignments. Algorithm 1 Cross-Lingual Opinion Mining Input: Labeled data Le , parallel data De and Df . Output: Target opinion mining system S f . 1: S e ← L EARN O PINION M INER(Le ) b e ← RUN O PINION M INER(S e , De ) 2: D 3: D e↔f ← RUN W ORDA LIGNER(D e , D f ) b f ← P ROJECTA ND F ILTER(De↔f , D be) 4: D f f b 5: S ← L EARN O PINION M INER(D ) transferred at a word level (for words with high confidence alignments), as illustrat"
P15-1040,W06-2920,0,0.0571036,"rior work (Choi et al., 2005; Kim and Hovy, 2006; Johansson and Moschitti, 2010), one source of difficulty when learning opinion miners on MPQA is with the boundaries of the entity spans. The fact that no criterion for choosing these boundaries is explicitly defined in the annotation guidelines (Wiebe et al., 2005) leads to a low inter-annotator agreement. To circumvent this problem and make the learning task easier, we depart from the classical span-based approaches toward dependency-based opinion mining. This decision is inspired by the success of dependency models for syntax and semantics (Buchholz and Marsi, 2006; Surdeanu et al., 2008). These dependency relations can be further converted to opinion spans (as described in §3.3), or directly used as features in downstream applications. As we will see, a compact representation based on dependencies can achieve state-of-the-art results and has the advantage of being easily transferred to other languages through a parallel corpus. 3.2 Dependency opinion graph. We have the following requirements for a well-formed dependency opinion graph: 1. No self-arcs or arcs linking root to null. 2. An arc is labeled as OPINION if and only if it comes from the root nod"
P15-1040,P10-2050,0,0.139811,"g is based on the MPQA corpus. Kim and Hovy (2006) proposed a method for finding opinion holders and topics, with the aid of a semantic role labeler. Choi et al. (2005) and Breck et al. (2007) used CRFs for finding opinion holders and recognizing opinion expressions, respectively. The two things are predicted jointly by Choi et al. (2006), with integer programming, and Johansson and Moschitti (2010), via reranking. The same method was applied later for joint prediction of opinion expressions and their polarities (Johansson and Moschitti, 2011). The advantage of a joint model was also shown by Choi and Cardie (2010) and Yang and Cardie (2014). Yang and Cardie (2012) classified expressions with a semiMarkov decoder, outperforming a B-I-O tagger; in later work, the same authors proposed an ILP decoder to jointly retrieve opinion expressions, holders, and targets (Yang and Cardie, 2013). A more recent work (˙Irsoy and Cardie, 2014) proposes a recurrent neural network to identify opinion spans. All the approaches above rely on a span-based representation of the opinion elements. This makes joint decoding procedures more complicated, since they must forbid overlap of opinion elements or add further constraint"
P15-1040,N06-1014,0,0.154243,"Missing"
P15-1040,H05-1045,0,0.530116,"l, and by Lu et al. (2011) at sentence level. Recently, Gui et al. (2013) applied projection learning for opinion mining in Chinese. However, this work only addresses agent detection and requires translating the MPQA corpus. While all these works are relevant, none addresses fine-grained opinion mining in its full generality, where the goal is to predict full opinion frames. Related Work A considerable amount of work on fine-grained opinion mining is based on the MPQA corpus. Kim and Hovy (2006) proposed a method for finding opinion holders and topics, with the aid of a semantic role labeler. Choi et al. (2005) and Breck et al. (2007) used CRFs for finding opinion holders and recognizing opinion expressions, respectively. The two things are predicted jointly by Choi et al. (2006), with integer programming, and Johansson and Moschitti (2010), via reranking. The same method was applied later for joint prediction of opinion expressions and their polarities (Johansson and Moschitti, 2011). The advantage of a joint model was also shown by Choi and Cardie (2010) and Yang and Cardie (2014). Yang and Cardie (2012) classified expressions with a semiMarkov decoder, outperforming a B-I-O tagger; in later work,"
P15-1040,W06-1651,0,0.435877,"detection and requires translating the MPQA corpus. While all these works are relevant, none addresses fine-grained opinion mining in its full generality, where the goal is to predict full opinion frames. Related Work A considerable amount of work on fine-grained opinion mining is based on the MPQA corpus. Kim and Hovy (2006) proposed a method for finding opinion holders and topics, with the aid of a semantic role labeler. Choi et al. (2005) and Breck et al. (2007) used CRFs for finding opinion holders and recognizing opinion expressions, respectively. The two things are predicted jointly by Choi et al. (2006), with integer programming, and Johansson and Moschitti (2010), via reranking. The same method was applied later for joint prediction of opinion expressions and their polarities (Johansson and Moschitti, 2011). The advantage of a joint model was also shown by Choi and Cardie (2010) and Yang and Cardie (2014). Yang and Cardie (2012) classified expressions with a semiMarkov decoder, outperforming a B-I-O tagger; in later work, the same authors proposed an ILP decoder to jointly retrieve opinion expressions, holders, and targets (Yang and Cardie, 2013). A more recent work (˙Irsoy and Cardie, 2014"
P15-1040,P11-1033,0,0.038337,"pproximate heuristic. Query-based multilingual opinion mining was addressed in several NTCIR shared tasks (Seki et al., 2007; Seki et al., 2010).4 However, to our best knowledge, a cross-lingual approach has never been attempted. Some steps were taken by Mihalcea et al. (2007) and Banea et al. (2008), who translated an English lexicon and the MPQA corpus to Romanian and Spanish, but for the much simpler task of sentence-level subjectivity analysis. Cross-lingual sentiment classification was addressed by Wan (2009), Prettenhofer and Stein (2010) and Wei and Pal (2010) at document level, and by Lu et al. (2011) at sentence level. Recently, Gui et al. (2013) applied projection learning for opinion mining in Chinese. However, this work only addresses agent detection and requires translating the MPQA corpus. While all these works are relevant, none addresses fine-grained opinion mining in its full generality, where the goal is to predict full opinion frames. Related Work A considerable amount of work on fine-grained opinion mining is based on the MPQA corpus. Kim and Hovy (2006) proposed a method for finding opinion holders and topics, with the aid of a semantic role labeler. Choi et al. (2005) and Bre"
P15-1040,clematide-etal-2012-mlsa,0,0.0242553,"racking of newswire and blogs (Ku et al., 2006), question answering (Yu and Hatzivassiloglou, 2003), and text-to-speech synthesis (Alm et al., 2005). While early work has focused on determining sentiment at document and sentence level (Pang et al., 2002; Turney, 2002; Balog et al., 2006), research has gradually progressed towards finegrained opinion mining, where rather than determining global sentiment, the goal is to parse text 1 http://mpqa.cs.pitt.edu/corpora/mpqa_ corpus. 2 Besides English, monolingual systems have also been developed for Chinese and Japanese (Seki et al., 2007), German (Clematide et al., 2012) and Bengali (Das and Bandyopadhyay, 2010). 408 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 408–418, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics its simplicity, this model is on par with state-ofthe-art opinion mining systems for English (§5). Then, through bitext projection, we transfer these dependency-based opinion frames to Portuguese (our target language), and train a system on the resulting corpus (§6). As part of this work, a"
P15-1040,S14-2082,1,0.838576,"ing opinion mining system. We conjecture that this is due to the ability to extract opinions, agents, and targets jointly using exact decoding. Note that our proposed dependency scheme would also be able to include additional global features relating pairs of opinions (by adding scores to pairs of opinion arcs) or two opinions having the same agent (by adding scores to pairs of agent arcs sharing its argument), similar to the reranking features used by Johansson and Moschitti (2013). Similar second-order scores have been used in syntactic and semantic dependency parsing (Martins et al., 2013; Martins and Almeida, 2014), but with an increase in the complexity of the model and of the decoder. Results: Dependency-Based Model 6 We assess the quality of our monolingual dependency-based model by comparing it to the recent state-of-the-art approach of Johansson and Moschitti (2013), whose code is available online.8 That paper reports the performance of a basic span-based pipeline system (which extracts opinions with a CRF, followed by two separate classifiers to detect polarities and agents), and of a more sophisticated system that applies a reranking procedure to account for more complex features that consider in"
P15-1040,P13-2109,1,0.896169,"Missing"
P15-1040,P15-1138,1,0.796037,"ned by the heavy effort of annotation necessary for current learning-based approaches to succeed, which delays the deployment of opinion miners for new languages. We bridge the existing gap by proposing a cross-lingual approach to fine-grained opinion mining via bitext projection. This technique has been quite effective in several NLP tasks, such as part-of-speech (POS) tagging (T¨ackstr¨om et al., 2013), named entity recognition (Wang and Manning, 2014), syntactic parsing (Yarowsky and Ngai, 2001; Hwa et al., 2005), semantic role labeling (Pad´o and Lapata, 2009), and coreference resolution (Martins, 2015). Given a corpus of parallel sentences (bitext), the idea is to run a pre-trained system on the source side and then to use word alignments to transfer the produced annotations to the target side, creating an automatic training corpus for the impoverished language. To alleviate the complexity of the task, we start by introducing a lightweight representation— called dependency-based opinion mining—and convert the MPQA corpus to this formalism (§3). We propose a simple arc-factored model that permits easy decoding (§4) and we show that, despite We propose a cross-lingual framework for fine-grain"
P15-1040,W10-3207,0,0.0154675,"al., 2006), question answering (Yu and Hatzivassiloglou, 2003), and text-to-speech synthesis (Alm et al., 2005). While early work has focused on determining sentiment at document and sentence level (Pang et al., 2002; Turney, 2002; Balog et al., 2006), research has gradually progressed towards finegrained opinion mining, where rather than determining global sentiment, the goal is to parse text 1 http://mpqa.cs.pitt.edu/corpora/mpqa_ corpus. 2 Besides English, monolingual systems have also been developed for Chinese and Japanese (Seki et al., 2007), German (Clematide et al., 2012) and Bengali (Das and Bandyopadhyay, 2010). 408 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 408–418, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics its simplicity, this model is on par with state-ofthe-art opinion mining systems for English (§5). Then, through bitext projection, we transfer these dependency-based opinion frames to Portuguese (our target language), and train a system on the resulting corpus (§6). As part of this work, a validation corpus in Portuguese with subj"
P15-1040,D11-1006,0,0.0407884,"guists and then revised by the third linguist, who (in case of any doubts) discussed with the initial annotators to reach for the final consensus. Scores for inter-annotator agreement are shown in Table 3. Baseline #2: Delexicalized System with Bilingual Embeddings. This baseline consists of a direct model transfer: a D ELEXICALIZED system is trained in the source language, without language specific features, so that it can be directly applied to the target language. Despite its simplicity, this strategy managed to provide a fairly strong baseline in several NLP tasks (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011). To achieve a unified feature representation, we mapped all language-specific POS tags to universal tags (Petrov et al., 2012), and removed all features depending on the dependency relations, but maintained those depending on the syntactic path (but not on the dependency relations themselves). In addition, we replaced the lexical features by 128-dimensional cross-lingual word embeddings.13 To obtain these bilingual neural embeddings, we ran the method of Hermann and Blunsom (2014) on the parallel data (§6.1). We scaled the embeddings by a factor of 2.0 (selected on the dev-set"
P15-1040,P07-1123,0,0.0669242,"de. A dependency scheme was also used by Wu et al. (2011) for fine-grained opinion mining. Our work differs in which we mine opinions in news articles instead of product reviews, a considerably different task. In addition, the approach of Wu et al. (2011) relies on “span nodes” (instead of head words), requiring solving an ILP followed by an approximate heuristic. Query-based multilingual opinion mining was addressed in several NTCIR shared tasks (Seki et al., 2007; Seki et al., 2010).4 However, to our best knowledge, a cross-lingual approach has never been attempted. Some steps were taken by Mihalcea et al. (2007) and Banea et al. (2008), who translated an English lexicon and the MPQA corpus to Romanian and Spanish, but for the much simpler task of sentence-level subjectivity analysis. Cross-lingual sentiment classification was addressed by Wan (2009), Prettenhofer and Stein (2010) and Wei and Pal (2010) at document level, and by Lu et al. (2011) at sentence level. Recently, Gui et al. (2013) applied projection learning for opinion mining in Chinese. However, this work only addresses agent detection and requires translating the MPQA corpus. While all these works are relevant, none addresses fine-graine"
P15-1040,D13-1203,0,0.0203692,"r of the opinion; • the opinion target, i.e., what is being argued about; • the opinion polarity, i.e., the sentiment (positive, negative or neutral) towards the target. As an example, consider the sentence in Figure 1, which has two opinions, expressed by the 4 NTCIR-8 had a cross-lingual track but in a very different sense: there, queries and documents are in different languages; in contrast, we transfer a model accross languages. 3 The Portuguese corpus and the lexicon are available at http://labs.priberam.com/Resources. 409 identifying the heads of mention spans in coreference resolution (Durrett and Klein, 2013). spans “is believed” (O1 ) and “are against” (O2 ). The first opinion has an implicit agent and a neutral polarity toward the target “the rich elites” (T1 ). This target is also the agent (A2 ) of the second opinion, which has a negative polarity toward “Hugo Ch´avez” (T2 ). 3.1 Defining labeled arcs. The opinion relations are represented as labeled arcs that link these head nodes. Two artificial nodes are added: a root node, which links to all nodes that represent opinion words, with the label OPINION; and a null node, which is used for representing implicit relations. To represent opinion-a"
P15-1040,W02-1011,0,0.0163413,"Introduction The goal of opinion mining is to extract opinions and sentiments from text (Pang and Lee, 2008; Wilson, 2008; Liu, 2012). With the advent of social media and the increasing amount of data available on the Web, this has become a very active area of research, with applications in summarization of customer reviews (Hu and Liu, 2004; Wu et al., 2011), tracking of newswire and blogs (Ku et al., 2006), question answering (Yu and Hatzivassiloglou, 2003), and text-to-speech synthesis (Alm et al., 2005). While early work has focused on determining sentiment at document and sentence level (Pang et al., 2002; Turney, 2002; Balog et al., 2006), research has gradually progressed towards finegrained opinion mining, where rather than determining global sentiment, the goal is to parse text 1 http://mpqa.cs.pitt.edu/corpora/mpqa_ corpus. 2 Besides English, monolingual systems have also been developed for Chinese and Japanese (Seki et al., 2007), German (Clematide et al., 2012) and Bengali (Das and Bandyopadhyay, 2010). 408 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 408–418, c Be"
P15-1040,D11-1123,0,0.128488,"integer programming or reranking. In cross-lingual mode (English to Portuguese), our approach compares favorably to a supervised system (with scarce labeled data), and to a delexicalized model trained using universal tags and bilingual word embeddings. 1 Introduction The goal of opinion mining is to extract opinions and sentiments from text (Pang and Lee, 2008; Wilson, 2008; Liu, 2012). With the advent of social media and the increasing amount of data available on the Web, this has become a very active area of research, with applications in summarization of customer reviews (Hu and Liu, 2004; Wu et al., 2011), tracking of newswire and blogs (Ku et al., 2006), question answering (Yu and Hatzivassiloglou, 2003), and text-to-speech synthesis (Alm et al., 2005). While early work has focused on determining sentiment at document and sentence level (Pang et al., 2002; Turney, 2002; Balog et al., 2006), research has gradually progressed towards finegrained opinion mining, where rather than determining global sentiment, the goal is to parse text 1 http://mpqa.cs.pitt.edu/corpora/mpqa_ corpus. 2 Besides English, monolingual systems have also been developed for Chinese and Japanese (Seki et al., 2007), Germa"
P15-1040,petrov-etal-2012-universal,0,0.0442307,"s. Scores for inter-annotator agreement are shown in Table 3. Baseline #2: Delexicalized System with Bilingual Embeddings. This baseline consists of a direct model transfer: a D ELEXICALIZED system is trained in the source language, without language specific features, so that it can be directly applied to the target language. Despite its simplicity, this strategy managed to provide a fairly strong baseline in several NLP tasks (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011). To achieve a unified feature representation, we mapped all language-specific POS tags to universal tags (Petrov et al., 2012), and removed all features depending on the dependency relations, but maintained those depending on the syntactic path (but not on the dependency relations themselves). In addition, we replaced the lexical features by 128-dimensional cross-lingual word embeddings.13 To obtain these bilingual neural embeddings, we ran the method of Hermann and Blunsom (2014) on the parallel data (§6.1). We scaled the embeddings by a factor of 2.0 (selected on the dev-set), following the procedure described in Turian et al. (2010). We trained the English delexicalized system on the MPQA corpus, using the same te"
P15-1040,P10-1114,0,0.0207084,"“span nodes” (instead of head words), requiring solving an ILP followed by an approximate heuristic. Query-based multilingual opinion mining was addressed in several NTCIR shared tasks (Seki et al., 2007; Seki et al., 2010).4 However, to our best knowledge, a cross-lingual approach has never been attempted. Some steps were taken by Mihalcea et al. (2007) and Banea et al. (2008), who translated an English lexicon and the MPQA corpus to Romanian and Spanish, but for the much simpler task of sentence-level subjectivity analysis. Cross-lingual sentiment classification was addressed by Wan (2009), Prettenhofer and Stein (2010) and Wei and Pal (2010) at document level, and by Lu et al. (2011) at sentence level. Recently, Gui et al. (2013) applied projection learning for opinion mining in Chinese. However, this work only addresses agent detection and requires translating the MPQA corpus. While all these works are relevant, none addresses fine-grained opinion mining in its full generality, where the goal is to predict full opinion frames. Related Work A considerable amount of work on fine-grained opinion mining is based on the MPQA corpus. Kim and Hovy (2006) proposed a method for finding opinion holders and topics, w"
P15-1040,D12-1122,0,0.199865,"proposed a method for finding opinion holders and topics, with the aid of a semantic role labeler. Choi et al. (2005) and Breck et al. (2007) used CRFs for finding opinion holders and recognizing opinion expressions, respectively. The two things are predicted jointly by Choi et al. (2006), with integer programming, and Johansson and Moschitti (2010), via reranking. The same method was applied later for joint prediction of opinion expressions and their polarities (Johansson and Moschitti, 2011). The advantage of a joint model was also shown by Choi and Cardie (2010) and Yang and Cardie (2014). Yang and Cardie (2012) classified expressions with a semiMarkov decoder, outperforming a B-I-O tagger; in later work, the same authors proposed an ILP decoder to jointly retrieve opinion expressions, holders, and targets (Yang and Cardie, 2013). A more recent work (˙Irsoy and Cardie, 2014) proposes a recurrent neural network to identify opinion spans. All the approaches above rely on a span-based representation of the opinion elements. This makes joint decoding procedures more complicated, since they must forbid overlap of opinion elements or add further constraints, leading to integer programming or reranking stra"
P15-1040,W03-1014,0,0.0665389,"5.7 19.0 16.6 16.4 17.6 B ITEXT P ROJECTION HM PM OM 58.0* 55.7* 58.0* 30.8* 31.2* 36.2* 29.4* 29.4* 35.6* 35.7* 34.1* 35.7* Table 5: Comparison of cross-lingual approaches. F1 scores obtained in our Portuguese validation corpus using: a S UPERVISED system trained on the small available data, a D ELEXICALIZED system trained with universal POS tags and multilingual embeddings and our B ITEXT P ROJECTION OF D EPENDENCIES. The symbol * indicates that the best system beats the other systems with statistical significance, with p &lt; 0.05 and according to a bootstrap resampling test (Koehn, 2004). as Riloff and Wiebe (2003) and whose list is available with the corpus, but selecting only documents annotated with targets. We randomly split the remaining documents into train and development sets, respectively with a total of 6,471 and 782 sentences.14 Table 4 shows the performance of the delexicalized baseline in English, compared with a lexicalized system. We will see how this model behaves in a cross-lingual setting in §7.2. the D ELEXICALIZED system is rather disappointing. This result is justified by a decrease of performance in English due to the delexicalization (cf. Table 4), followed by an extra loss of qua"
P15-1040,P13-1161,0,0.168179,"ctively. The two things are predicted jointly by Choi et al. (2006), with integer programming, and Johansson and Moschitti (2010), via reranking. The same method was applied later for joint prediction of opinion expressions and their polarities (Johansson and Moschitti, 2011). The advantage of a joint model was also shown by Choi and Cardie (2010) and Yang and Cardie (2014). Yang and Cardie (2012) classified expressions with a semiMarkov decoder, outperforming a B-I-O tagger; in later work, the same authors proposed an ILP decoder to jointly retrieve opinion expressions, holders, and targets (Yang and Cardie, 2013). A more recent work (˙Irsoy and Cardie, 2014) proposes a recurrent neural network to identify opinion spans. All the approaches above rely on a span-based representation of the opinion elements. This makes joint decoding procedures more complicated, since they must forbid overlap of opinion elements or add further constraints, leading to integer programming or reranking strategies. Besides, there is little consensus about what should be the correct span boundaries, the inter-annotator agreement being quite low (Wiebe et al., 2005). In 3 Dependency-Based Opinion Mining This work addresses vari"
P15-1040,Q14-1039,0,0.0120637,"us. Kim and Hovy (2006) proposed a method for finding opinion holders and topics, with the aid of a semantic role labeler. Choi et al. (2005) and Breck et al. (2007) used CRFs for finding opinion holders and recognizing opinion expressions, respectively. The two things are predicted jointly by Choi et al. (2006), with integer programming, and Johansson and Moschitti (2010), via reranking. The same method was applied later for joint prediction of opinion expressions and their polarities (Johansson and Moschitti, 2011). The advantage of a joint model was also shown by Choi and Cardie (2010) and Yang and Cardie (2014). Yang and Cardie (2012) classified expressions with a semiMarkov decoder, outperforming a B-I-O tagger; in later work, the same authors proposed an ILP decoder to jointly retrieve opinion expressions, holders, and targets (Yang and Cardie, 2013). A more recent work (˙Irsoy and Cardie, 2014) proposes a recurrent neural network to identify opinion spans. All the approaches above rely on a span-based representation of the opinion elements. This makes joint decoding procedures more complicated, since they must forbid overlap of opinion elements or add further constraints, leading to integer progr"
P15-1040,N01-1026,0,0.1923,"volume of prior work, opinion mining has by and large been limited to monolingual approaches in English.2 This is explained by the heavy effort of annotation necessary for current learning-based approaches to succeed, which delays the deployment of opinion miners for new languages. We bridge the existing gap by proposing a cross-lingual approach to fine-grained opinion mining via bitext projection. This technique has been quite effective in several NLP tasks, such as part-of-speech (POS) tagging (T¨ackstr¨om et al., 2013), named entity recognition (Wang and Manning, 2014), syntactic parsing (Yarowsky and Ngai, 2001; Hwa et al., 2005), semantic role labeling (Pad´o and Lapata, 2009), and coreference resolution (Martins, 2015). Given a corpus of parallel sentences (bitext), the idea is to run a pre-trained system on the source side and then to use word alignments to transfer the produced annotations to the target side, creating an automatic training corpus for the impoverished language. To alleviate the complexity of the task, we start by introducing a lightweight representation— called dependency-based opinion mining—and convert the MPQA corpus to this formalism (§3). We propose a simple arc-factored mod"
P15-1040,P13-1045,0,0.0650928,"re the latter includes spans without polarity annotation or annotated as “both”. As in Johansson and Moschitti (2013), we also ignore the “uncertain” aspect of the annotated polarities. Determining head nodes. The three opinion elements that we want to detect (opinions, agents and targets) are each represented by a head node, which corresponds to a single word (underlined in Figure 1). When converting the MPQA corpus to dependencies, we determine this “representative” word automatically, by using the following simple heuristic: we first parse the sentence using the Stanford dependency parser (Socher et al., 2013a); then, we pick the last word in the span whose syntactic parent is outside the span (if the span is a syntactic phrase, there is only one word whose parent is outside the span, which is the lexical head). The same heuristic has been used for 3.3 Dependency-to-Span Conversion To evaluate the opinion miner against manual annotations and compare with other systems, we need a procedure to convert back from predicted dependencies to spans. In this work, we used a very simple procedure that we next describe, 5 Even though this assumption is not always met in practice, it is typical in MPQA (only"
P15-1040,W03-1017,0,0.127249,"roach compares favorably to a supervised system (with scarce labeled data), and to a delexicalized model trained using universal tags and bilingual word embeddings. 1 Introduction The goal of opinion mining is to extract opinions and sentiments from text (Pang and Lee, 2008; Wilson, 2008; Liu, 2012). With the advent of social media and the increasing amount of data available on the Web, this has become a very active area of research, with applications in summarization of customer reviews (Hu and Liu, 2004; Wu et al., 2011), tracking of newswire and blogs (Ku et al., 2006), question answering (Yu and Hatzivassiloglou, 2003), and text-to-speech synthesis (Alm et al., 2005). While early work has focused on determining sentiment at document and sentence level (Pang et al., 2002; Turney, 2002; Balog et al., 2006), research has gradually progressed towards finegrained opinion mining, where rather than determining global sentiment, the goal is to parse text 1 http://mpqa.cs.pitt.edu/corpora/mpqa_ corpus. 2 Besides English, monolingual systems have also been developed for Chinese and Japanese (Seki et al., 2007), German (Clematide et al., 2012) and Bengali (Das and Bandyopadhyay, 2010). 408 Proceedings of the 53rd Annu"
P15-1040,D13-1170,0,0.0351119,"re the latter includes spans without polarity annotation or annotated as “both”. As in Johansson and Moschitti (2013), we also ignore the “uncertain” aspect of the annotated polarities. Determining head nodes. The three opinion elements that we want to detect (opinions, agents and targets) are each represented by a head node, which corresponds to a single word (underlined in Figure 1). When converting the MPQA corpus to dependencies, we determine this “representative” word automatically, by using the following simple heuristic: we first parse the sentence using the Stanford dependency parser (Socher et al., 2013a); then, we pick the last word in the span whose syntactic parent is outside the span (if the span is a syntactic phrase, there is only one word whose parent is outside the span, which is the lexical head). The same heuristic has been used for 3.3 Dependency-to-Span Conversion To evaluate the opinion miner against manual annotations and compare with other systems, we need a procedure to convert back from predicted dependencies to spans. In this work, we used a very simple procedure that we next describe, 5 Even though this assumption is not always met in practice, it is typical in MPQA (only"
P15-1040,I08-3008,0,0.0805412,"by two of the three linguists and then revised by the third linguist, who (in case of any doubts) discussed with the initial annotators to reach for the final consensus. Scores for inter-annotator agreement are shown in Table 3. Baseline #2: Delexicalized System with Bilingual Embeddings. This baseline consists of a direct model transfer: a D ELEXICALIZED system is trained in the source language, without language specific features, so that it can be directly applied to the target language. Despite its simplicity, this strategy managed to provide a fairly strong baseline in several NLP tasks (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011). To achieve a unified feature representation, we mapped all language-specific POS tags to universal tags (Petrov et al., 2012), and removed all features depending on the dependency relations, but maintained those depending on the syntactic path (but not on the dependency relations themselves). In addition, we replaced the lexical features by 128-dimensional cross-lingual word embeddings.13 To obtain these bilingual neural embeddings, we ran the method of Hermann and Blunsom (2014) on the parallel data (§6.1). We scaled the embeddings by a factor of 2.0 ("
P15-1040,P11-2120,0,0.0374839,"by the third linguist, who (in case of any doubts) discussed with the initial annotators to reach for the final consensus. Scores for inter-annotator agreement are shown in Table 3. Baseline #2: Delexicalized System with Bilingual Embeddings. This baseline consists of a direct model transfer: a D ELEXICALIZED system is trained in the source language, without language specific features, so that it can be directly applied to the target language. Despite its simplicity, this strategy managed to provide a fairly strong baseline in several NLP tasks (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011). To achieve a unified feature representation, we mapped all language-specific POS tags to universal tags (Petrov et al., 2012), and removed all features depending on the dependency relations, but maintained those depending on the syntactic path (but not on the dependency relations themselves). In addition, we replaced the lexical features by 128-dimensional cross-lingual word embeddings.13 To obtain these bilingual neural embeddings, we ran the method of Hermann and Blunsom (2014) on the parallel data (§6.1). We scaled the embeddings by a factor of 2.0 (selected on the dev-set), following the"
P15-1040,Q13-1001,0,0.14641,"Missing"
P15-1040,P10-1040,0,0.0323253,"eature representation, we mapped all language-specific POS tags to universal tags (Petrov et al., 2012), and removed all features depending on the dependency relations, but maintained those depending on the syntactic path (but not on the dependency relations themselves). In addition, we replaced the lexical features by 128-dimensional cross-lingual word embeddings.13 To obtain these bilingual neural embeddings, we ran the method of Hermann and Blunsom (2014) on the parallel data (§6.1). We scaled the embeddings by a factor of 2.0 (selected on the dev-set), following the procedure described in Turian et al. (2010). We trained the English delexicalized system on the MPQA corpus, using the same test documents The corpus was annotated with automatic POS tags and dependency parse trees using TurboParser (Martins et al., 2013).12 We used an in-house lemmatizer to obtain lemmas for each inflected word in the corpus. A Portuguese lexicon of subjectivity was created by translating the words in the Subjectivity Lexicon of Wilson et al. (2005). The annotated corpus and the translated subjectivity lexicon are available at http://labs.priberam.com/ Resources/Fine-Grained-Opinion-Corpus, and http://labs.priberam.co"
P15-1040,P02-1053,0,0.00513214,"al of opinion mining is to extract opinions and sentiments from text (Pang and Lee, 2008; Wilson, 2008; Liu, 2012). With the advent of social media and the increasing amount of data available on the Web, this has become a very active area of research, with applications in summarization of customer reviews (Hu and Liu, 2004; Wu et al., 2011), tracking of newswire and blogs (Ku et al., 2006), question answering (Yu and Hatzivassiloglou, 2003), and text-to-speech synthesis (Alm et al., 2005). While early work has focused on determining sentiment at document and sentence level (Pang et al., 2002; Turney, 2002; Balog et al., 2006), research has gradually progressed towards finegrained opinion mining, where rather than determining global sentiment, the goal is to parse text 1 http://mpqa.cs.pitt.edu/corpora/mpqa_ corpus. 2 Besides English, monolingual systems have also been developed for Chinese and Japanese (Seki et al., 2007), German (Clematide et al., 2012) and Bengali (Das and Bandyopadhyay, 2010). 408 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 408–418, c Beijing, China,"
P15-1040,P09-1027,0,0.0602003,") relies on “span nodes” (instead of head words), requiring solving an ILP followed by an approximate heuristic. Query-based multilingual opinion mining was addressed in several NTCIR shared tasks (Seki et al., 2007; Seki et al., 2010).4 However, to our best knowledge, a cross-lingual approach has never been attempted. Some steps were taken by Mihalcea et al. (2007) and Banea et al. (2008), who translated an English lexicon and the MPQA corpus to Romanian and Spanish, but for the much simpler task of sentence-level subjectivity analysis. Cross-lingual sentiment classification was addressed by Wan (2009), Prettenhofer and Stein (2010) and Wei and Pal (2010) at document level, and by Lu et al. (2011) at sentence level. Recently, Gui et al. (2013) applied projection learning for opinion mining in Chinese. However, this work only addresses agent detection and requires translating the MPQA corpus. While all these works are relevant, none addresses fine-grained opinion mining in its full generality, where the goal is to predict full opinion frames. Related Work A considerable amount of work on fine-grained opinion mining is based on the MPQA corpus. Kim and Hovy (2006) proposed a method for findin"
P15-1040,Q14-1005,0,0.0574454,"produced (reviewed in §2). Despite the large volume of prior work, opinion mining has by and large been limited to monolingual approaches in English.2 This is explained by the heavy effort of annotation necessary for current learning-based approaches to succeed, which delays the deployment of opinion miners for new languages. We bridge the existing gap by proposing a cross-lingual approach to fine-grained opinion mining via bitext projection. This technique has been quite effective in several NLP tasks, such as part-of-speech (POS) tagging (T¨ackstr¨om et al., 2013), named entity recognition (Wang and Manning, 2014), syntactic parsing (Yarowsky and Ngai, 2001; Hwa et al., 2005), semantic role labeling (Pad´o and Lapata, 2009), and coreference resolution (Martins, 2015). Given a corpus of parallel sentences (bitext), the idea is to run a pre-trained system on the source side and then to use word alignments to transfer the produced annotations to the target side, creating an automatic training corpus for the impoverished language. To alleviate the complexity of the task, we start by introducing a lightweight representation— called dependency-based opinion mining—and convert the MPQA corpus to this formalis"
P15-1040,P10-2048,0,0.019883,"s), requiring solving an ILP followed by an approximate heuristic. Query-based multilingual opinion mining was addressed in several NTCIR shared tasks (Seki et al., 2007; Seki et al., 2010).4 However, to our best knowledge, a cross-lingual approach has never been attempted. Some steps were taken by Mihalcea et al. (2007) and Banea et al. (2008), who translated an English lexicon and the MPQA corpus to Romanian and Spanish, but for the much simpler task of sentence-level subjectivity analysis. Cross-lingual sentiment classification was addressed by Wan (2009), Prettenhofer and Stein (2010) and Wei and Pal (2010) at document level, and by Lu et al. (2011) at sentence level. Recently, Gui et al. (2013) applied projection learning for opinion mining in Chinese. However, this work only addresses agent detection and requires translating the MPQA corpus. While all these works are relevant, none addresses fine-grained opinion mining in its full generality, where the goal is to predict full opinion frames. Related Work A considerable amount of work on fine-grained opinion mining is based on the MPQA corpus. Kim and Hovy (2006) proposed a method for finding opinion holders and topics, with the aid of a semant"
P15-1040,H05-1044,0,0.594326,"nd the 7th International Joint Conference on Natural Language Processing, pages 408–418, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics its simplicity, this model is on par with state-ofthe-art opinion mining systems for English (§5). Then, through bitext projection, we transfer these dependency-based opinion frames to Portuguese (our target language), and train a system on the resulting corpus (§6). As part of this work, a validation corpus in Portuguese with subjectivity annotations was created, along with a translation of the MPQA Subjectivity lexicon of Wilson et al. (2005).3 Experimental evaluation (§7) shows that our cross-lingual approach surpasses a supervised system trained on a small corpus in the target language, as well as a delexicalized baseline trained using universal POS tags, bilingual word embeddings and a projected lexicon. 2 constrast, we use dependencies to model opinion elements and relations, leading to a compact representation that does not depend on spans and which is tractable to decode. A dependency scheme was also used by Wu et al. (2011) for fine-grained opinion mining. Our work differs in which we mine opinions in news articles instead"
P15-1040,W08-2121,0,\N,Missing
P15-1040,P14-1006,0,\N,Missing
P15-1138,P15-1040,1,0.796037,"erence resolution as a way of transferring information from a rich-resource language to build coreference resolvers for languages with scarcer resources; as a testbed, we transfer from English to Spanish and to Brazilian Portuguese. We build upon the recent successes of cross-lingual learning in NLP, which proved quite effective in several structured prediction tasks, such as POS tagging (T¨ackstr¨om et al., 2013), named entity recognition (Wang and Manning, 2014), dependency parsing (McDonald et al., 2011), semantic role labeling (Titov and Klementiev, 2012), and fine-grained opinion mining (Almeida et al., 2015). The potential of these techniques, however, has never been fully exploited in coreference resolution (despite some existing work, reviewed in §6, but none resulting in an endto-end coreference resolver). We bridge this gap by proposing a simple learning-based method with weak supervision, based on posterior regularization (Ganchev et al., 2010). We adapt this framework to handle softmax-margin objective functions (Gimpel and Smith, 2010), leading to softmax-margin posterior regularization (§4). This step, while fairly simple, opens the door for incorporating taskspecific cost functions, whic"
P15-1138,S10-1022,0,0.0124849,"for the level of performance we expect to achieve with the weakly-supervised systems. An important step in coreference resolution systems is mention prediction. For English, mention spans were predicted from the noun phrases given by the Berkeley parser (Petrov and Klein, 2007), the same procedure as Durrett and Klein (2013). For Spanish and Portuguese, this prediction relied on the output of the dependency parser, using a simple heuristic: besides pronouns, each maximal span formed by contiguous descendants of a noun becomes a candidate mention. This heuristic is quite effective, as shown by Attardi et al. (2010). 5.1 Supervised Systems Table 2 shows the performance of supervised systems for English, Spanish and Portuguese. All optimize Eq. 4 appended with an extra regularization term γ2 kwk2 , by running 20 epochs of stochastic gradient descent (SGD; we set γ = 1.0 and selected the best epoch using the dev-set). All lexicalized systems use the same features as the SUR FACE model of Durrett and Klein (2013), plus features for gender and number.7 We collected a list of pronouns for all languages along with their gender, number, and person information. For English, we trained on the WSJ portion of the O"
P15-1138,D08-1031,0,0.0126827,"ependency parses by using TurboParser (Martins et al., 2013). 3 3.1 Coreference Resolution Problem Definition and Prior Work In coreference resolution, we are given a set of mentions M := {m1 , . . . , mM }, and the goal is to cluster them into discourse entities, E := {e1 , . . . , eE }, where each ej ⊆ M and ej 6= ∅. The set SEE must form a partition of M, i.e., we must have j=1 ej = M, and ei ∩ ej = ∅ for i 6= j. A variety of approaches have been proposed to this problem, including entity-centric models (Haghighi and Klein, 2010; Rahman and Ng, 2011; Durrett et al., 2013), pairwise models (Bengtson and Roth, 2008; Versley et al., 2008), greedy rule-based methods (Raghunathan et al., 2010), and mention-ranking decoders (Denis and Baldridge, 2008; Durrett and Klein, 2013). We chose to base our coreference resolvers on this last class of methods, which permit efficient decoding by shifting from entity clusters to latent coreference trees. In particular, the inclusion of lexicalized features by Durrett and Klein (2013) yields nearly state-of-the-art performance with surface information only. Given that our goal is to prototype resolvers for resource-poor languages, this model is a good fit—we next describ"
P15-1138,P06-1005,0,0.0508723,"Missing"
P15-1138,D08-1069,0,0.04662,"ference resolution, we are given a set of mentions M := {m1 , . . . , mM }, and the goal is to cluster them into discourse entities, E := {e1 , . . . , eE }, where each ej ⊆ M and ej 6= ∅. The set SEE must form a partition of M, i.e., we must have j=1 ej = M, and ei ∩ ej = ∅ for i 6= j. A variety of approaches have been proposed to this problem, including entity-centric models (Haghighi and Klein, 2010; Rahman and Ng, 2011; Durrett et al., 2013), pairwise models (Bengtson and Roth, 2008; Versley et al., 2008), greedy rule-based methods (Raghunathan et al., 2010), and mention-ranking decoders (Denis and Baldridge, 2008; Durrett and Klein, 2013). We chose to base our coreference resolvers on this last class of methods, which permit efficient decoding by shifting from entity clusters to latent coreference trees. In particular, the inclusion of lexicalized features by Durrett and Klein (2013) yields nearly state-of-the-art performance with surface information only. Given that our goal is to prototype resolvers for resource-poor languages, this model is a good fit—we next describe it in detail. 3.2 Latent Coreference Tree Models Let x be a document containing M mentions, sorted from left to right. We associate"
P15-1138,D13-1203,0,0.0644808,"r regularization (Ganchev et al., 2010). We adapt this framework to handle softmax-margin objective functions (Gimpel and Smith, 2010), leading to softmax-margin posterior regularization (§4). This step, while fairly simple, opens the door for incorporating taskspecific cost functions, which are important to manage the precision/recall trade-offs in coreference resolution systems. We show that the resulting problem involves optimizing the difference of two cost-augmented log-partition functions, making a bridge with supervised systems based on latent coreference trees (Fernandes et al., 2012; Durrett and Klein, 2013), reviewed in §3. Inspired by this idea, we consider a simple penalized variant of posterior regularization that tunes the Lagrange multipliers directly, bypassing the saddle-point problem of existing EM and alternating stochastic gradient algorithms (Ganchev et al., 2010; Liang et al., 2009). Experiments (§5) show that the proposed method outperforms commonly used cross-lingual approaches, such as delexicalized transfer with bilingual embeddings, direct projection, and “vanilla” posterior regularization. 2 Architecture and Experimental Setup Our methodology, outlined as Algorithm 1, is inspir"
P15-1138,P13-1012,0,0.01189,"se, we obtained automatic POS tags and dependency parses by using TurboParser (Martins et al., 2013). 3 3.1 Coreference Resolution Problem Definition and Prior Work In coreference resolution, we are given a set of mentions M := {m1 , . . . , mM }, and the goal is to cluster them into discourse entities, E := {e1 , . . . , eE }, where each ej ⊆ M and ej 6= ∅. The set SEE must form a partition of M, i.e., we must have j=1 ej = M, and ei ∩ ej = ∅ for i 6= j. A variety of approaches have been proposed to this problem, including entity-centric models (Haghighi and Klein, 2010; Rahman and Ng, 2011; Durrett et al., 2013), pairwise models (Bengtson and Roth, 2008; Versley et al., 2008), greedy rule-based methods (Raghunathan et al., 2010), and mention-ranking decoders (Denis and Baldridge, 2008; Durrett and Klein, 2013). We chose to base our coreference resolvers on this last class of methods, which permit efficient decoding by shifting from entity clusters to latent coreference trees. In particular, the inclusion of lexicalized features by Durrett and Klein (2013) yields nearly state-of-the-art performance with surface information only. Given that our goal is to prototype resolvers for resource-poor languages"
P15-1138,N10-1061,0,0.0165795,"test partitions. For both Spanish and Portuguese, we obtained automatic POS tags and dependency parses by using TurboParser (Martins et al., 2013). 3 3.1 Coreference Resolution Problem Definition and Prior Work In coreference resolution, we are given a set of mentions M := {m1 , . . . , mM }, and the goal is to cluster them into discourse entities, E := {e1 , . . . , eE }, where each ej ⊆ M and ej 6= ∅. The set SEE must form a partition of M, i.e., we must have j=1 ej = M, and ei ∩ ej = ∅ for i 6= j. A variety of approaches have been proposed to this problem, including entity-centric models (Haghighi and Klein, 2010; Rahman and Ng, 2011; Durrett et al., 2013), pairwise models (Bengtson and Roth, 2008; Versley et al., 2008), greedy rule-based methods (Raghunathan et al., 2010), and mention-ranking decoders (Denis and Baldridge, 2008; Durrett and Klein, 2013). We chose to base our coreference resolvers on this last class of methods, which permit efficient decoding by shifting from entity clusters to latent coreference trees. In particular, the inclusion of lexicalized features by Durrett and Klein (2013) yields nearly state-of-the-art performance with surface information only. Given that our goal is to pro"
P15-1138,A00-1020,0,0.0924489,"possessives, respectively. In Portuguese, where many possessives are not annotated in the gold data, we observe a similar but much less pronounced trend. 6 Related Work While multilingual coreference resolution has been the subject of recent SemEval and CoNLL shared tasks, no submitted system attempted cross-lingual training. As shown by Recasens and Hovy (2010), language-specific issues pose a challenge, due to phenomena as pronoun dropping and grammatical gender that are absent in English but exist in other languages. We have discussed some of these issues in the scope of the present work. Harabagiu and Maiorano (2000) and Postolache et al. (2006) projected English corpora to Romanian to bootstrap human annotation, either manually or via automatic alignments. Rahman and Ng (2012) applied translation-based projection at test time (but require an external translation service). Hardmeier et al. (2013) addressed the related task of cross-lingual pronoun prediction. While all these approaches help alleviate the corpus annotation bottleneck, none resulted in a full coreference resolver, which our work accomplished. The work most related with ours is Souza and Or˘asan (2011), who also used parallel data to transfe"
P15-1138,D13-1037,0,0.035807,"tem attempted cross-lingual training. As shown by Recasens and Hovy (2010), language-specific issues pose a challenge, due to phenomena as pronoun dropping and grammatical gender that are absent in English but exist in other languages. We have discussed some of these issues in the scope of the present work. Harabagiu and Maiorano (2000) and Postolache et al. (2006) projected English corpora to Romanian to bootstrap human annotation, either manually or via automatic alignments. Rahman and Ng (2012) applied translation-based projection at test time (but require an external translation service). Hardmeier et al. (2013) addressed the related task of cross-lingual pronoun prediction. While all these approaches help alleviate the corpus annotation bottleneck, none resulted in a full coreference resolver, which our work accomplished. The work most related with ours is Souza and Or˘asan (2011), who also used parallel data to transfer an English coreference resolver to Portuguese, but could not beat a simple baseline that clusters together mentions with the same head. Their approach is similar to our bitext direct projection baseline, except that they used Reconcile (Stoyanov et al., 2010) instead of the Berkeley"
P15-1138,N06-1014,0,0.0802014,"ext detail all the datasets and tools involved in our experimental setup. Table 1 provides a summary, along with some statistics. Parallel Data. As parallel data, we use a sentence-aligned trilingual (English-PortugueseSpanish) parallel corpus based on the scientific news Brazilian magazine Revista Pesquisa FAPESP, collected by Aziz and Specia (2011).1 We preprocessed this dataset as follows. We labeled the English side with the Berkeley Coreference Resolution system v1.0, using the provided English model (Durrett and Klein, 2013). Then, we computed word alignments using the Berkeley aligner (Liang et al., 2006), intersected them and filtered out all the alignments whose confi1 We found that other commonly used parallel data (such as Europarl or the UN corpus) have a predominance of direct speech that is not suitable for our newswire test domain, so we decided not to use these data. # Doc. 2,374 303 322 2,704 875 140 168 2,823 30 7 13 # Sent. 48,762 6,894 8,262 142,633 8,999 1,417 1,704 166,719 469 111 257 # Tok. 1,007,359 136,257 152,728 3,840,936 295,276 46,167 53,042 4,538,147 11,771 2,983 6,491 Table 1: Corpus statistics. EN, ES, and PT denote English, Spanish, and Portuguese, respectively. dence"
P15-1138,H05-1004,0,0.466507,"Missing"
P15-1138,D10-1004,1,0.899527,"Missing"
P15-1138,P13-2109,1,0.893378,"Missing"
P15-1138,D13-1205,0,0.408562,"d by this idea, we consider a simple penalized variant of posterior regularization that tunes the Lagrange multipliers directly, bypassing the saddle-point problem of existing EM and alternating stochastic gradient algorithms (Ganchev et al., 2010; Liang et al., 2009). Experiments (§5) show that the proposed method outperforms commonly used cross-lingual approaches, such as delexicalized transfer with bilingual embeddings, direct projection, and “vanilla” posterior regularization. 2 Architecture and Experimental Setup Our methodology, outlined as Algorithm 1, is inspired by the recent work of Ganchev and Das (2013) on cross-lingual learning of sequence models. For simplicity, we call the source and tar1427 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1427–1437, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Figure 1: Excerpt of a bitext document with automatic coreference annotations (from FAPESP). The English side had its coreferences resolved by a state-of-the-art system (Durrett and Klein, 2013). The predicted coreference chains {The pulmonary"
P15-1138,D11-1006,0,0.154515,"ut they often require complex generative models or expert linguistic knowledge. We propose cross-lingual coreference resolution as a way of transferring information from a rich-resource language to build coreference resolvers for languages with scarcer resources; as a testbed, we transfer from English to Spanish and to Brazilian Portuguese. We build upon the recent successes of cross-lingual learning in NLP, which proved quite effective in several structured prediction tasks, such as POS tagging (T¨ackstr¨om et al., 2013), named entity recognition (Wang and Manning, 2014), dependency parsing (McDonald et al., 2011), semantic role labeling (Titov and Klementiev, 2012), and fine-grained opinion mining (Almeida et al., 2015). The potential of these techniques, however, has never been fully exploited in coreference resolution (despite some existing work, reviewed in §6, but none resulting in an endto-end coreference resolver). We bridge this gap by proposing a simple learning-based method with weak supervision, based on posterior regularization (Ganchev et al., 2010). We adapt this framework to handle softmax-margin objective functions (Gimpel and Smith, 2010), leading to softmax-margin posterior regulariza"
P15-1138,P02-1014,0,0.202086,"information from the source to the target. To handle task-specific costs, we propose a softmax-margin variant of posterior regularization, and we use it to achieve robustness to projection errors. We show empirically that this strategy outperforms competitive cross-lingual methods, such as delexicalized transfer with bilingual word embeddings, bitext direct projection, and vanilla posterior regularization. 1 Introduction The goal of coreference resolution is to find the mentions in text that refer to the same discourse entity. While early work focused primarily on English (Soon et al., 2001; Ng and Cardie, 2002), efforts have been made toward multilingual systems, this being addressed in recent shared tasks (Recasens et al., 2010; Pradhan et al., 2012). However, the lack of annotated data hinders rapid system deployment for new languages. Unsupervised methods (Haghighi and Klein, 2007; Ng, 2008) and rule-based approaches (Raghunathan et al., 2010) avoid this data annotation bottleneck, but they often require complex generative models or expert linguistic knowledge. We propose cross-lingual coreference resolution as a way of transferring information from a rich-resource language to build coreference r"
P15-1138,N10-1112,0,0.176199,"on (Wang and Manning, 2014), dependency parsing (McDonald et al., 2011), semantic role labeling (Titov and Klementiev, 2012), and fine-grained opinion mining (Almeida et al., 2015). The potential of these techniques, however, has never been fully exploited in coreference resolution (despite some existing work, reviewed in §6, but none resulting in an endto-end coreference resolver). We bridge this gap by proposing a simple learning-based method with weak supervision, based on posterior regularization (Ganchev et al., 2010). We adapt this framework to handle softmax-margin objective functions (Gimpel and Smith, 2010), leading to softmax-margin posterior regularization (§4). This step, while fairly simple, opens the door for incorporating taskspecific cost functions, which are important to manage the precision/recall trade-offs in coreference resolution systems. We show that the resulting problem involves optimizing the difference of two cost-augmented log-partition functions, making a bridge with supervised systems based on latent coreference trees (Fernandes et al., 2012; Durrett and Klein, 2013), reviewed in §3. Inspired by this idea, we consider a simple penalized variant of posterior regularization th"
P15-1138,D08-1067,0,0.0250907,"lized transfer with bilingual word embeddings, bitext direct projection, and vanilla posterior regularization. 1 Introduction The goal of coreference resolution is to find the mentions in text that refer to the same discourse entity. While early work focused primarily on English (Soon et al., 2001; Ng and Cardie, 2002), efforts have been made toward multilingual systems, this being addressed in recent shared tasks (Recasens et al., 2010; Pradhan et al., 2012). However, the lack of annotated data hinders rapid system deployment for new languages. Unsupervised methods (Haghighi and Klein, 2007; Ng, 2008) and rule-based approaches (Raghunathan et al., 2010) avoid this data annotation bottleneck, but they often require complex generative models or expert linguistic knowledge. We propose cross-lingual coreference resolution as a way of transferring information from a rich-resource language to build coreference resolvers for languages with scarcer resources; as a testbed, we transfer from English to Spanish and to Brazilian Portuguese. We build upon the recent successes of cross-lingual learning in NLP, which proved quite effective in several structured prediction tasks, such as POS tagging (T¨ac"
P15-1138,P07-1107,0,0.0183105,"methods, such as delexicalized transfer with bilingual word embeddings, bitext direct projection, and vanilla posterior regularization. 1 Introduction The goal of coreference resolution is to find the mentions in text that refer to the same discourse entity. While early work focused primarily on English (Soon et al., 2001; Ng and Cardie, 2002), efforts have been made toward multilingual systems, this being addressed in recent shared tasks (Recasens et al., 2010; Pradhan et al., 2012). However, the lack of annotated data hinders rapid system deployment for new languages. Unsupervised methods (Haghighi and Klein, 2007; Ng, 2008) and rule-based approaches (Raghunathan et al., 2010) avoid this data annotation bottleneck, but they often require complex generative models or expert linguistic knowledge. We propose cross-lingual coreference resolution as a way of transferring information from a rich-resource language to build coreference resolvers for languages with scarcer resources; as a testbed, we transfer from English to Spanish and to Brazilian Portuguese. We build upon the recent successes of cross-lingual learning in NLP, which proved quite effective in several structured prediction tasks, such as POS ta"
P15-1138,N07-1051,0,0.0120623,"ow present experiments using the setup in §2. We compare our coreference resolvers trained with softmax-margin PR (§5.5) with three other weakly-supervised baselines: delexicalized transfer with cross-lingual embeddings (§5.2), bitext projection (§5.3), and vanilla PR (§5.4). We also run fully supervised systems (§5.1), to obtain upper bounds for the level of performance we expect to achieve with the weakly-supervised systems. An important step in coreference resolution systems is mention prediction. For English, mention spans were predicted from the noun phrases given by the Berkeley parser (Petrov and Klein, 2007), the same procedure as Durrett and Klein (2013). For Spanish and Portuguese, this prediction relied on the output of the dependency parser, using a simple heuristic: besides pronouns, each maximal span formed by contiguous descendants of a noun becomes a candidate mention. This heuristic is quite effective, as shown by Attardi et al. (2010). 5.1 Supervised Systems Table 2 shows the performance of supervised systems for English, Spanish and Portuguese. All optimize Eq. 4 appended with an extra regularization term γ2 kwk2 , by running 20 epochs of stochastic gradient descent (SGD; we set γ = 1."
P15-1138,petrov-etal-2012-universal,0,0.144011,"Missing"
P15-1138,postolache-etal-2006-transferring,0,0.265457,"tuguese, where many possessives are not annotated in the gold data, we observe a similar but much less pronounced trend. 6 Related Work While multilingual coreference resolution has been the subject of recent SemEval and CoNLL shared tasks, no submitted system attempted cross-lingual training. As shown by Recasens and Hovy (2010), language-specific issues pose a challenge, due to phenomena as pronoun dropping and grammatical gender that are absent in English but exist in other languages. We have discussed some of these issues in the scope of the present work. Harabagiu and Maiorano (2000) and Postolache et al. (2006) projected English corpora to Romanian to bootstrap human annotation, either manually or via automatic alignments. Rahman and Ng (2012) applied translation-based projection at test time (but require an external translation service). Hardmeier et al. (2013) addressed the related task of cross-lingual pronoun prediction. While all these approaches help alleviate the corpus annotation bottleneck, none resulted in a full coreference resolver, which our work accomplished. The work most related with ours is Souza and Or˘asan (2011), who also used parallel data to transfer an English coreference reso"
P15-1138,W12-4501,0,0.0695919,"d we use it to achieve robustness to projection errors. We show empirically that this strategy outperforms competitive cross-lingual methods, such as delexicalized transfer with bilingual word embeddings, bitext direct projection, and vanilla posterior regularization. 1 Introduction The goal of coreference resolution is to find the mentions in text that refer to the same discourse entity. While early work focused primarily on English (Soon et al., 2001; Ng and Cardie, 2002), efforts have been made toward multilingual systems, this being addressed in recent shared tasks (Recasens et al., 2010; Pradhan et al., 2012). However, the lack of annotated data hinders rapid system deployment for new languages. Unsupervised methods (Haghighi and Klein, 2007; Ng, 2008) and rule-based approaches (Raghunathan et al., 2010) avoid this data annotation bottleneck, but they often require complex generative models or expert linguistic knowledge. We propose cross-lingual coreference resolution as a way of transferring information from a rich-resource language to build coreference resolvers for languages with scarcer resources; as a testbed, we transfer from English to Spanish and to Brazilian Portuguese. We build upon the"
P15-1138,P14-2006,0,0.0314599,"Missing"
P15-1138,D10-1048,0,0.235854,"beddings, bitext direct projection, and vanilla posterior regularization. 1 Introduction The goal of coreference resolution is to find the mentions in text that refer to the same discourse entity. While early work focused primarily on English (Soon et al., 2001; Ng and Cardie, 2002), efforts have been made toward multilingual systems, this being addressed in recent shared tasks (Recasens et al., 2010; Pradhan et al., 2012). However, the lack of annotated data hinders rapid system deployment for new languages. Unsupervised methods (Haghighi and Klein, 2007; Ng, 2008) and rule-based approaches (Raghunathan et al., 2010) avoid this data annotation bottleneck, but they often require complex generative models or expert linguistic knowledge. We propose cross-lingual coreference resolution as a way of transferring information from a rich-resource language to build coreference resolvers for languages with scarcer resources; as a testbed, we transfer from English to Spanish and to Brazilian Portuguese. We build upon the recent successes of cross-lingual learning in NLP, which proved quite effective in several structured prediction tasks, such as POS tagging (T¨ackstr¨om et al., 2013), named entity recognition (Wang"
P15-1138,N12-1052,0,0.0610218,"Missing"
P15-1138,Q13-1001,0,0.127588,"Missing"
P15-1138,P12-1068,0,0.0280417,"or expert linguistic knowledge. We propose cross-lingual coreference resolution as a way of transferring information from a rich-resource language to build coreference resolvers for languages with scarcer resources; as a testbed, we transfer from English to Spanish and to Brazilian Portuguese. We build upon the recent successes of cross-lingual learning in NLP, which proved quite effective in several structured prediction tasks, such as POS tagging (T¨ackstr¨om et al., 2013), named entity recognition (Wang and Manning, 2014), dependency parsing (McDonald et al., 2011), semantic role labeling (Titov and Klementiev, 2012), and fine-grained opinion mining (Almeida et al., 2015). The potential of these techniques, however, has never been fully exploited in coreference resolution (despite some existing work, reviewed in §6, but none resulting in an endto-end coreference resolver). We bridge this gap by proposing a simple learning-based method with weak supervision, based on posterior regularization (Ganchev et al., 2010). We adapt this framework to handle softmax-margin objective functions (Gimpel and Smith, 2010), leading to softmax-margin posterior regularization (§4). This step, while fairly simple, opens the"
P15-1138,P10-1040,0,0.222308,"Missing"
P15-1138,P08-4003,0,0.0307468,"TurboParser (Martins et al., 2013). 3 3.1 Coreference Resolution Problem Definition and Prior Work In coreference resolution, we are given a set of mentions M := {m1 , . . . , mM }, and the goal is to cluster them into discourse entities, E := {e1 , . . . , eE }, where each ej ⊆ M and ej 6= ∅. The set SEE must form a partition of M, i.e., we must have j=1 ej = M, and ei ∩ ej = ∅ for i 6= j. A variety of approaches have been proposed to this problem, including entity-centric models (Haghighi and Klein, 2010; Rahman and Ng, 2011; Durrett et al., 2013), pairwise models (Bengtson and Roth, 2008; Versley et al., 2008), greedy rule-based methods (Raghunathan et al., 2010), and mention-ranking decoders (Denis and Baldridge, 2008; Durrett and Klein, 2013). We chose to base our coreference resolvers on this last class of methods, which permit efficient decoding by shifting from entity clusters to latent coreference trees. In particular, the inclusion of lexicalized features by Durrett and Klein (2013) yields nearly state-of-the-art performance with surface information only. Given that our goal is to prototype resolvers for resource-poor languages, this model is a good fit—we next describe it in detail. 3.2 Lat"
P15-1138,M95-1005,0,0.235322,"Missing"
P15-1138,N12-1090,0,0.533842,"le multilingual coreference resolution has been the subject of recent SemEval and CoNLL shared tasks, no submitted system attempted cross-lingual training. As shown by Recasens and Hovy (2010), language-specific issues pose a challenge, due to phenomena as pronoun dropping and grammatical gender that are absent in English but exist in other languages. We have discussed some of these issues in the scope of the present work. Harabagiu and Maiorano (2000) and Postolache et al. (2006) projected English corpora to Romanian to bootstrap human annotation, either manually or via automatic alignments. Rahman and Ng (2012) applied translation-based projection at test time (but require an external translation service). Hardmeier et al. (2013) addressed the related task of cross-lingual pronoun prediction. While all these approaches help alleviate the corpus annotation bottleneck, none resulted in a full coreference resolver, which our work accomplished. The work most related with ours is Souza and Or˘asan (2011), who also used parallel data to transfer an English coreference resolver to Portuguese, but could not beat a simple baseline that clusters together mentions with the same head. Their approach is similar"
P15-1138,Q14-1005,0,0.0865973,"010) avoid this data annotation bottleneck, but they often require complex generative models or expert linguistic knowledge. We propose cross-lingual coreference resolution as a way of transferring information from a rich-resource language to build coreference resolvers for languages with scarcer resources; as a testbed, we transfer from English to Spanish and to Brazilian Portuguese. We build upon the recent successes of cross-lingual learning in NLP, which proved quite effective in several structured prediction tasks, such as POS tagging (T¨ackstr¨om et al., 2013), named entity recognition (Wang and Manning, 2014), dependency parsing (McDonald et al., 2011), semantic role labeling (Titov and Klementiev, 2012), and fine-grained opinion mining (Almeida et al., 2015). The potential of these techniques, however, has never been fully exploited in coreference resolution (despite some existing work, reviewed in §6, but none resulting in an endto-end coreference resolver). We bridge this gap by proposing a simple learning-based method with weak supervision, based on posterior regularization (Ganchev et al., 2010). We adapt this framework to handle softmax-margin objective functions (Gimpel and Smith, 2010), le"
P15-1138,P10-1144,0,0.0406315,"Missing"
P15-1138,H01-1035,0,0.480199,"ainly due to the fact that this method does not take into account the intricacies of each language—e.g., possessive forms have different agreement rules in English and in Romance languages;9 those, on the other hand, have clitic pronouns that are absent in English. Feature weights that promote certain English agreement relations may then harm performance more than they help. 5.3 Baseline #2: Bitext Direct Projection Another popular strategy for cross-lingual learning is bitext direct projection, which consists in projecting annotations through parallel data in the source and target languages (Yarowsky et al., 2001; Hwa et al., 2005). This is essentially the same as Algorithm 1, except that line 4 is replaced by simple supervised learning, via a minimization 9 For example, in Figure 1, their agrees in number with the possessor (the alveoli), but the corresponding sua agrees in number and gender with the thing possessed (func¸a˜ o). 1433 of the loss function in Eq. 4 with `2 -regularization. This procedure has the disadvantage of being very sensitive to annotation errors, as we shall see. For Portuguese, this baseline is a near-reproduction of Souza and Or˘asan (2011)’s work, discussed in §6. The third a"
P15-1138,S10-1001,0,0.050991,"Missing"
P15-1138,I08-3008,0,0.208059,"Missing"
P15-1138,P05-1044,0,0.0339639,"over gradient iterations, with strong oscillations in initial epochs and somewhat slow convergence. Right: impact in the averaged F1 scores (on the dev-set). Contrast with the more “stable” scores achieved by the penalized method. b vanish and we get Zu0 (w, x) = Z(w, x), recovering the supervised case (see Eq. 6). Intuitively, this formulation pushes probability mass toward structures that respect the constraints in Eq. 7, while moving away from those that have a large task-specific cost. A similar idea, but applied to the generative case, underlies the framework of constrastive estimation (Smith and Eisner, 2005). 4.3 Cost Function Denote by Em the entire coreference chain of the S mth mention (so E = m∈M {Em }), and by Msing := {m ∈ M |Em = {m}} the set of mentions that are projected as singleton in the data (we call this gold-singleton mentions). We design a task-specific cost `(b y , Y(E)) as in Durrett and Klein (2013) to balance three kinds of mistakes: (i) false anaphora (b ym 6= 0 while m ∈ Msing ); (ii) false new (b ym = 0 while m ∈ / Msing ); and (iii) wrong link (b ym 6= 0 but Em 6= Eybm ). Letting IFA (b ym , E), IFN (b ym , E), and IWL (b ym , E) be indicators for these events, we define a"
P15-1138,J01-4004,0,0.287698,"d bitext to project information from the source to the target. To handle task-specific costs, we propose a softmax-margin variant of posterior regularization, and we use it to achieve robustness to projection errors. We show empirically that this strategy outperforms competitive cross-lingual methods, such as delexicalized transfer with bilingual word embeddings, bitext direct projection, and vanilla posterior regularization. 1 Introduction The goal of coreference resolution is to find the mentions in text that refer to the same discourse entity. While early work focused primarily on English (Soon et al., 2001; Ng and Cardie, 2002), efforts have been made toward multilingual systems, this being addressed in recent shared tasks (Recasens et al., 2010; Pradhan et al., 2012). However, the lack of annotated data hinders rapid system deployment for new languages. Unsupervised methods (Haghighi and Klein, 2007; Ng, 2008) and rule-based approaches (Raghunathan et al., 2010) avoid this data annotation bottleneck, but they often require complex generative models or expert linguistic knowledge. We propose cross-lingual coreference resolution as a way of transferring information from a rich-resource language"
P15-1138,P10-2029,0,0.0148318,"translation service). Hardmeier et al. (2013) addressed the related task of cross-lingual pronoun prediction. While all these approaches help alleviate the corpus annotation bottleneck, none resulted in a full coreference resolver, which our work accomplished. The work most related with ours is Souza and Or˘asan (2011), who also used parallel data to transfer an English coreference resolver to Portuguese, but could not beat a simple baseline that clusters together mentions with the same head. Their approach is similar to our bitext direct projection baseline, except that they used Reconcile (Stoyanov et al., 2010) instead of the Berkeley Coreference System, and a smaller version of the FAPESP corpus. We have shown that our softmaxmargin PR procedure is superior to this approach. Discriminative PR has been proposed by Ganchev et al. (2010). The same idea underlies the generalized expectation criterion (Mann and McCallum, 2010; Wang and Manning, 2014). An SGD algorithm for solving the resulting saddle point problem has been proposed by Liang et al. (2009), and used by Ganchev and Das (2013) for cross-lingual learning of sequence models. We extended this framework in two aspects: by incorporating a task-s"
P15-1138,W12-4502,0,\N,Missing
P15-1138,P14-1006,0,\N,Missing
P15-1138,W11-4533,0,\N,Missing
P15-1147,P11-1049,0,0.0230101,"Priberam Labs, Alameda D. Afonso Henriques, 41, 2o , 1000-123 Lisboa, Portugal danifg@uvigo.es, atm@priberam.pt Abstract Constituent parsing is a central problem in NLP—one at which statistical models trained on treebanks have excelled (Charniak, 1996; Klein and Manning, 2003; Petrov and Klein, 2007). However, most existing parsers are slow, since they need to deal with a heavy grammar constant. Dependency parsers are generally faster, but less informative, since they do not produce constituents, which are often required by downstream applications (Johansson and Nugues, 2008; Wu et al., 2009; Berg-Kirkpatrick et al., 2011; Elming et al., 2013). How to get the best of both worlds? Coarse-to-fine decoding (Charniak and Johnson, 2005) and shift-reduce parsing (Sagae and Lavie, 2005; Zhu et al., 2013) were a step forward to accelerate constituent parsing, but typical runtimes still lag those of dependency parsers. This is only made worse if discontinuous constituents are allowed—such discontinuities are convenient to represent wh-movement, scrambling, extraposition, and other linguistic phenomena common in free word order languages. While non-projective dependency parsers, which are able to model such phenomena, h"
P15-1147,W14-6110,0,0.14837,"Missing"
P15-1147,P92-1024,0,0.0317213,"TP-Full TP-Full + Lab., H&N enc. TP-Full + Lab, direct enc. TP-Full + Lab., delta enc. UAS 90.93 92.17 92.93 92.13 93.55 93.70 93.80 93.80 93.80 LAS 88.95 89.86 91.28 90.23 91.58 91.70 87.86 91.99 92.00 F1 # toks/s. 86.87 5,392 87.93 363 89.50 1,022 87.63 2,585 90.41 1,658 90.53 959 89.39 871 90.89 912 90.94 912 Table 1: Results on English PTB §22 achieved by various dparsers and encoding strategies. For dependencies, we report unlabeled/labeled attachment scores (UAS/LAS), excluding punctuation. For constituents, we show F1 -scores (without punctuation and root nodes), as provided by EVALB (Black et al., 1992). We report total parsing speeds in tokens per second (including time spent on pruning, decoding, and feature evaluation), measured on a Intel Xeon processor @2.30GHz. direct enc. Basque French German Hebrew Hungarian Korean Polish Swedish # labels 26 61 66 62 24 44 47 29 delta enc. F1 # labels 85.04 17 79.93 56 83.44 59 83.26 43 86.54 15 79.79 16 92.39 34 77.02 25 F1 85.17 80.05 83.39 83.29 86.67 79.97 92.64 77.19 Table 2: Impact of direct and delta encodings on the dev sets of the SPMRL14 shared task. Reported are the number of labels and the F1 -scores yielded by each encoding technique. a"
P15-1147,W07-1506,0,0.111496,"rniak and Johnson, 2005; Huang, 2008; Bj¨orkelund et al., 2014). Discontinuous c-parsing is considered a much harder problem, involving mildly context-sensitive formalisms such as LCFRS or range concatenation grammars, with treebankderived c-parsers exhibiting near-exponential runtime (Kallmeyer and Maier, 2013, Figure 27). To speed up decoding, prior work has considered restrictons, such as bounding the fan-out (Maier et al., 2012) and requiring well-nestedness (Kuhlmann and Nivre, 2006; G´omez-Rodr´ıguez et al., 2010). Other approaches eliminate the discontinuities via tree transformations (Boyd, 2007; K¨ubler et al., 2008), sometimes as a pruning step in a coarse-to-fine parsing approach (van Cranenburgh and Bod, 2013). However, reported runtimes are still superior to 10 seconds per sentence, which is not practical. Recently, Versley (2014a) proposed an easy-first approach that leads to considerable speed-ups, but is less accurate. In this paper, we design fast discontinuous c-parsers that outperform all the ones above by a wide margin, with similar runtimes as Versley (2014a). 2.2 Dependency Trees In this paper, we use d-parsers as a black box to parse constituents. Given a sentence w1 ."
P15-1147,W08-2102,0,0.808088,"s (#1, #2, . . .) to denote the order of events, as we do in Figure 1. A d-tree endowed with a strict order for each head is called a strictly ordered d-tree. We establish below a correspondence between strictly ordered d-trees and binary c-trees. Before doing so, we need a few more definitions about c-trees. For each word position h ∈ {1, . . . , L}, we define ψ(h) as the node higher in the c-tree whose lexical head is h. We call the path from ψ(h) down to the pre-terminal ph the spine of h. We may regard a c-tree as a set of L spines, one per word, which attach to each other to form a tree (Carreras et al., 2008). We then have the following Proposition 1. Binary c-trees and strictly-ordered d-trees are isomorphic, i.e., there is a one-to-one correspondence between the two sets, where the number of symbols is preserved. Proof. We use the construction in Figure 3. A formal proof is given as supplementary material. 3.2 Weakly Ordered Dependency Trees Next, we relax the strict order assumption, restricting the modifier sets Mh = {m1 , . . . , mK } to be only weakly ordered. This means that we can partition the into J equivalence classes, S K modifiers ¯ j , and define a strict order ≺h on Mh = Jj=1 M h ¯"
P15-1147,P05-1022,0,0.398261,"stract Constituent parsing is a central problem in NLP—one at which statistical models trained on treebanks have excelled (Charniak, 1996; Klein and Manning, 2003; Petrov and Klein, 2007). However, most existing parsers are slow, since they need to deal with a heavy grammar constant. Dependency parsers are generally faster, but less informative, since they do not produce constituents, which are often required by downstream applications (Johansson and Nugues, 2008; Wu et al., 2009; Berg-Kirkpatrick et al., 2011; Elming et al., 2013). How to get the best of both worlds? Coarse-to-fine decoding (Charniak and Johnson, 2005) and shift-reduce parsing (Sagae and Lavie, 2005; Zhu et al., 2013) were a step forward to accelerate constituent parsing, but typical runtimes still lag those of dependency parsers. This is only made worse if discontinuous constituents are allowed—such discontinuities are convenient to represent wh-movement, scrambling, extraposition, and other linguistic phenomena common in free word order languages. While non-projective dependency parsers, which are able to model such phenomena, have been widely developed in the last decade (Nivre et al., 2007; McDonald et al., 2006; Martins et al., 2013),"
P15-1147,A00-2018,0,0.839678,"Missing"
P15-1147,P99-1065,0,0.584849,"Missing"
P15-1147,de-marneffe-etal-2006-generating,0,0.121016,"Missing"
P15-1147,P99-1059,0,0.171517,"ive d-trees can be obtained from continuous c-trees by reading off the lexical heads and dropping the internal nodes (Gaifman, 1965). However, this relation is many-to-one: as shown in Figure 2, several c-trees may project onto the same d-tree, differing on their flatness and on left or right-branching decisions. In the next section, we introduce the concept of head-ordered d-trees and express one-to-one mappings between these two representations. Prior work. There has been a considerable amount of work developing rich-feature d-parsers. While projective d-parsers can use dynamic programming (Eisner and Satta, 1999; Koo and consider an extra root symbol, as often done in the literature. Collins, 2010), non-projective d-parsers typically rely on approximate decoders, since the underlying problem is NP-hard beyond arc-factored models (McDonald and Satta, 2007). An alternative are transition-based d-parsers (Nivre et al., 2006; Zhang and Nivre, 2011), which achieve observed linear time. Since d-parsing algorithms do not have a grammar constant, typical implementations are significantly faster than c-parsers (Rush and Petrov, 2012; Martins et al., 2013). The key contribution of this paper is to reduce c-par"
P15-1147,C96-1058,0,0.0151289,"er node. A node is called unary if it has exactly one child. A c-tree without unary proper nodes is called unaryless. If all proper nodes have exactly two children then it is called a binary c-tree. Continuous binary trees may be regarded as having been generated by a CFG in Chomsky normal form. Prior work. There has been a long string of work in statistical c-parsing, shifting from simple models (Charniak, 1996) to more sophisticated ones using structural annotation (Johnson, 1998; Klein and Manning, 2003), latent grammars (Matsuzaki et al., 2005; Petrov and Klein, 2007), and lexicalization (Eisner, 1996; Collins, 1999). An orthogonal line of work uses ensemble or reranking strategies to further improve accuracy (Charniak and Johnson, 2005; Huang, 2008; Bj¨orkelund et al., 2014). Discontinuous c-parsing is considered a much harder problem, involving mildly context-sensitive formalisms such as LCFRS or range concatenation grammars, with treebankderived c-parsers exhibiting near-exponential runtime (Kallmeyer and Maier, 2013, Figure 27). To speed up decoding, prior work has considered restrictons, such as bounding the fan-out (Maier et al., 2012) and requiring well-nestedness (Kuhlmann and Nivr"
P15-1147,N13-1070,0,0.0326529,"o Henriques, 41, 2o , 1000-123 Lisboa, Portugal danifg@uvigo.es, atm@priberam.pt Abstract Constituent parsing is a central problem in NLP—one at which statistical models trained on treebanks have excelled (Charniak, 1996; Klein and Manning, 2003; Petrov and Klein, 2007). However, most existing parsers are slow, since they need to deal with a heavy grammar constant. Dependency parsers are generally faster, but less informative, since they do not produce constituents, which are often required by downstream applications (Johansson and Nugues, 2008; Wu et al., 2009; Berg-Kirkpatrick et al., 2011; Elming et al., 2013). How to get the best of both worlds? Coarse-to-fine decoding (Charniak and Johnson, 2005) and shift-reduce parsing (Sagae and Lavie, 2005; Zhu et al., 2013) were a step forward to accelerate constituent parsing, but typical runtimes still lag those of dependency parsers. This is only made worse if discontinuous constituents are allowed—such discontinuities are convenient to represent wh-movement, scrambling, extraposition, and other linguistic phenomena common in free word order languages. While non-projective dependency parsers, which are able to model such phenomena, have been widely develo"
P15-1147,N10-1035,0,0.0968775,"Missing"
P15-1147,W08-1007,0,0.546036,"ke other constituent parsers, ours does not require estimating a grammar, nor binarizing the treebank. Moreover, when the dependency parser is non-projective, our method can perform discontinuous constituent parsing in a very natural way. Key to our approach is the notion of headordered dependency trees (shown in Figure 1): by endowing dependency trees with this additional layer of structure, we show that they become isomorphic to constituent trees. We encode this structure as part of the dependency labels, enabling a dependency-to-constituent conversion. A related conversion was attempted by Hall and Nivre (2008) to parse German, but their complex encoding scheme blows up the number of arc labels, affecting the final parser’s quality. By contrast, our light encoding achieves a 10-fold decrease in the label alphabet, leading to more accurate parsing. While simple, our reduction-based parsers are on par with the Berkeley parser for English (Petrov ∗ This research was carried out during an internship at Priberam Labs. 1 The title of this paper is inspired by the seminal paper of Pereira and Warren (1983) “Parsing as Deduction.” We reduce phrase-based parsing to dependency parsing. Our reduction is ground"
P15-1147,P14-1022,0,0.48429,"rategy, we use this configuration in the sequel. To further explore the impact of delta encoding, we report in Table 2 the scores obtained by direct and delta encodings on eight other treebanks (see §5.2 for details on these datasets). With the exception of German, in all cases the delta encoding yielded better EVALB F1 -scores with fewer labels. 5 Experiments To evaluate the performance of our reductionbased parsers, we conduct experiments in a variety Parser Charniak (2000) Klein and Manning (2003) Petrov and Klein (2007) Carreras et al. (2008) Zhu et al. (2013) Stanford Shift-Reduce (2014) Hall et al. (2014) This work Charniak and Johnson (2005)∗ Socher et al. (2013)∗ Zhu et al. (2013)∗ LR 89.5 85.3 90.0 90.7 90.3 89.1 88.4 89.9 91.2 89.1 91.1 LP 89.9 86.5 90.3 91.4 90.6 89.1 88.8 90.4 91.8 89.7 91.5 F1 #Toks/s. 89.5 – 85.9 143 90.1 169 91.1 – 90.4 1,290 89.1 655 88.6 12 90.2 957 91.5 84 89.4 70 91.3 – Table 3: Results on the English PTB §23. All systems reporting runtimes were run on the same machine. Marked as ∗ are reranking and semi-supervised c-parsers. of treebanks, both continuous and discontinuous. 5.1 Results on the English PTB Table 3 shows the accuracies and speeds achieved by our syst"
P15-1147,P08-1067,0,0.0348695,"wo children then it is called a binary c-tree. Continuous binary trees may be regarded as having been generated by a CFG in Chomsky normal form. Prior work. There has been a long string of work in statistical c-parsing, shifting from simple models (Charniak, 1996) to more sophisticated ones using structural annotation (Johnson, 1998; Klein and Manning, 2003), latent grammars (Matsuzaki et al., 2005; Petrov and Klein, 2007), and lexicalization (Eisner, 1996; Collins, 1999). An orthogonal line of work uses ensemble or reranking strategies to further improve accuracy (Charniak and Johnson, 2005; Huang, 2008; Bj¨orkelund et al., 2014). Discontinuous c-parsing is considered a much harder problem, involving mildly context-sensitive formalisms such as LCFRS or range concatenation grammars, with treebankderived c-parsers exhibiting near-exponential runtime (Kallmeyer and Maier, 2013, Figure 27). To speed up decoding, prior work has considered restrictons, such as bounding the fan-out (Maier et al., 2012) and requiring well-nestedness (Kuhlmann and Nivre, 2006; G´omez-Rodr´ıguez et al., 2010). Other approaches eliminate the discontinuities via tree transformations (Boyd, 2007; K¨ubler et al., 2008), s"
P15-1147,D08-1008,0,0.0225631,"perior T´ecnico, 1049-001 Lisboa, Portugal # Priberam Labs, Alameda D. Afonso Henriques, 41, 2o , 1000-123 Lisboa, Portugal danifg@uvigo.es, atm@priberam.pt Abstract Constituent parsing is a central problem in NLP—one at which statistical models trained on treebanks have excelled (Charniak, 1996; Klein and Manning, 2003; Petrov and Klein, 2007). However, most existing parsers are slow, since they need to deal with a heavy grammar constant. Dependency parsers are generally faster, but less informative, since they do not produce constituents, which are often required by downstream applications (Johansson and Nugues, 2008; Wu et al., 2009; Berg-Kirkpatrick et al., 2011; Elming et al., 2013). How to get the best of both worlds? Coarse-to-fine decoding (Charniak and Johnson, 2005) and shift-reduce parsing (Sagae and Lavie, 2005; Zhu et al., 2013) were a step forward to accelerate constituent parsing, but typical runtimes still lag those of dependency parsers. This is only made worse if discontinuous constituents are allowed—such discontinuities are convenient to represent wh-movement, scrambling, extraposition, and other linguistic phenomena common in free word order languages. While non-projective dependency pa"
P15-1147,J98-4004,0,0.250091,"branches, if the leaves are drawn in left-to-right surface order. An internal node which is not a preterminal is called a proper node. A node is called unary if it has exactly one child. A c-tree without unary proper nodes is called unaryless. If all proper nodes have exactly two children then it is called a binary c-tree. Continuous binary trees may be regarded as having been generated by a CFG in Chomsky normal form. Prior work. There has been a long string of work in statistical c-parsing, shifting from simple models (Charniak, 1996) to more sophisticated ones using structural annotation (Johnson, 1998; Klein and Manning, 2003), latent grammars (Matsuzaki et al., 2005; Petrov and Klein, 2007), and lexicalization (Eisner, 1996; Collins, 1999). An orthogonal line of work uses ensemble or reranking strategies to further improve accuracy (Charniak and Johnson, 2005; Huang, 2008; Bj¨orkelund et al., 2014). Discontinuous c-parsing is considered a much harder problem, involving mildly context-sensitive formalisms such as LCFRS or range concatenation grammars, with treebankderived c-parsers exhibiting near-exponential runtime (Kallmeyer and Maier, 2013, Figure 27). To speed up decoding, prior work"
P15-1147,P98-1106,0,0.153775,"d. Note that the English unary nodes ADVP and ADJP are dropped in the conversion. VP really RB VP VP needs VBZ caution NN VBZ RB really needs caution VP VP VP RB VBZ NN VP NN really needs caution RB NN VBZ really needs caution Figure 2: Three different c-structures for the VP “really needs caution.” All are consistent with the d-structure at the top left. hh, m, `i, expressing a typed dependency relation ` between the head word wh and the modifier wm . A d-tree is projective if for every arc hh, m, `i there is a directed path from h to all words that lie between h and m in the surface string (Kahane et al., 1998). Projective d-trees can be obtained from continuous c-trees by reading off the lexical heads and dropping the internal nodes (Gaifman, 1965). However, this relation is many-to-one: as shown in Figure 2, several c-trees may project onto the same d-tree, differing on their flatness and on left or right-branching decisions. In the next section, we introduce the concept of head-ordered d-trees and express one-to-one mappings between these two representations. Prior work. There has been a considerable amount of work developing rich-feature d-parsers. While projective d-parsers can use dynamic prog"
P15-1147,J13-1006,0,0.8526,"ccelerate constituent parsing, but typical runtimes still lag those of dependency parsers. This is only made worse if discontinuous constituents are allowed—such discontinuities are convenient to represent wh-movement, scrambling, extraposition, and other linguistic phenomena common in free word order languages. While non-projective dependency parsers, which are able to model such phenomena, have been widely developed in the last decade (Nivre et al., 2007; McDonald et al., 2006; Martins et al., 2013), discontinuous constituent parsing is still taking its first steps (Maier and Søgaard, 2008; Kallmeyer and Maier, 2013). In this paper, we show that an off-the-shelf, trainable, dependency parser is enough to build a highly-competitive constituent parser. This (surprising) result is based on a reduction1 of constituent to dependency parsing, followed by a simple post-processing procedure to recover unaries. Unlike other constituent parsers, ours does not require estimating a grammar, nor binarizing the treebank. Moreover, when the dependency parser is non-projective, our method can perform discontinuous constituent parsing in a very natural way. Key to our approach is the notion of headordered dependency trees"
P15-1147,P03-1054,0,0.169318,"he leaves are drawn in left-to-right surface order. An internal node which is not a preterminal is called a proper node. A node is called unary if it has exactly one child. A c-tree without unary proper nodes is called unaryless. If all proper nodes have exactly two children then it is called a binary c-tree. Continuous binary trees may be regarded as having been generated by a CFG in Chomsky normal form. Prior work. There has been a long string of work in statistical c-parsing, shifting from simple models (Charniak, 1996) to more sophisticated ones using structural annotation (Johnson, 1998; Klein and Manning, 2003), latent grammars (Matsuzaki et al., 2005; Petrov and Klein, 2007), and lexicalization (Eisner, 1996; Collins, 1999). An orthogonal line of work uses ensemble or reranking strategies to further improve accuracy (Charniak and Johnson, 2005; Huang, 2008; Bj¨orkelund et al., 2014). Discontinuous c-parsing is considered a much harder problem, involving mildly context-sensitive formalisms such as LCFRS or range concatenation grammars, with treebankderived c-parsers exhibiting near-exponential runtime (Kallmeyer and Maier, 2013, Figure 27). To speed up decoding, prior work has considered restrictons"
P15-1147,N15-1080,0,0.501955,"Missing"
P15-1147,P10-1001,0,0.0675049,"Missing"
P15-1147,kubler-etal-2008-compare,0,0.0625132,"Missing"
P15-1147,P06-2066,0,0.157983,"ion (Eisner, 1996; Collins, 1999). An orthogonal line of work uses ensemble or reranking strategies to further improve accuracy (Charniak and Johnson, 2005; Huang, 2008; Bj¨orkelund et al., 2014). Discontinuous c-parsing is considered a much harder problem, involving mildly context-sensitive formalisms such as LCFRS or range concatenation grammars, with treebankderived c-parsers exhibiting near-exponential runtime (Kallmeyer and Maier, 2013, Figure 27). To speed up decoding, prior work has considered restrictons, such as bounding the fan-out (Maier et al., 2012) and requiring well-nestedness (Kuhlmann and Nivre, 2006; G´omez-Rodr´ıguez et al., 2010). Other approaches eliminate the discontinuities via tree transformations (Boyd, 2007; K¨ubler et al., 2008), sometimes as a pruning step in a coarse-to-fine parsing approach (van Cranenburgh and Bod, 2013). However, reported runtimes are still superior to 10 seconds per sentence, which is not practical. Recently, Versley (2014a) proposed an easy-first approach that leads to considerable speed-ups, but is less accurate. In this paper, we design fast discontinuous c-parsers that outperform all the ones above by a wide margin, with similar runtimes as Versley (20"
P15-1147,W12-4615,0,0.356888,"al., 2005; Petrov and Klein, 2007), and lexicalization (Eisner, 1996; Collins, 1999). An orthogonal line of work uses ensemble or reranking strategies to further improve accuracy (Charniak and Johnson, 2005; Huang, 2008; Bj¨orkelund et al., 2014). Discontinuous c-parsing is considered a much harder problem, involving mildly context-sensitive formalisms such as LCFRS or range concatenation grammars, with treebankderived c-parsers exhibiting near-exponential runtime (Kallmeyer and Maier, 2013, Figure 27). To speed up decoding, prior work has considered restrictons, such as bounding the fan-out (Maier et al., 2012) and requiring well-nestedness (Kuhlmann and Nivre, 2006; G´omez-Rodr´ıguez et al., 2010). Other approaches eliminate the discontinuities via tree transformations (Boyd, 2007; K¨ubler et al., 2008), sometimes as a pruning step in a coarse-to-fine parsing approach (van Cranenburgh and Bod, 2013). However, reported runtimes are still superior to 10 seconds per sentence, which is not practical. Recently, Versley (2014a) proposed an easy-first approach that leads to considerable speed-ups, but is less accurate. In this paper, we design fast discontinuous c-parsers that outperform all the ones abov"
P15-1147,J93-2004,0,0.05111,"the equivalence results established in §3 to build c-parsers when only a trainable dparser is available. Given a c-treebank provided as input, our procedure is outlined as follows: 1. Convert the c-treebank to dependencies (Algorithm 1). 2. Train a labeled d-parser on this treebank. 3. For each test sentence, run the labeled d-parser and convert the predicted d-tree into a c-tree without unary nodes (Algorithm 2). 4. Do post-processing to recover unaries. The next subsections describe each of these steps in detail. Along the way, we illustrate with experiments using the English Penn Treebank (Marcus et al., 1993), which we lexicalized by applying the head rules of Collins (1999).4 4.1 Dependency Encoding The first step is to convert the c-treebank to headordered dependencies, which we do using Algorithm 1. If the original treebank has discontinuous c-trees, we end up with non-projective d-trees or with violations of the nested property, as established in Proposition 3. We handle this gracefully by training a non-projective d-parser in the subsequent stage (see §4.2). Note also that this conversion drops the unary nodes (a consequence of Proposition 2). These nodes will be recovered in the last stage,"
P15-1147,P13-2109,1,0.905236,"Missing"
P15-1147,P05-1010,0,0.0954834,"e order. An internal node which is not a preterminal is called a proper node. A node is called unary if it has exactly one child. A c-tree without unary proper nodes is called unaryless. If all proper nodes have exactly two children then it is called a binary c-tree. Continuous binary trees may be regarded as having been generated by a CFG in Chomsky normal form. Prior work. There has been a long string of work in statistical c-parsing, shifting from simple models (Charniak, 1996) to more sophisticated ones using structural annotation (Johnson, 1998; Klein and Manning, 2003), latent grammars (Matsuzaki et al., 2005; Petrov and Klein, 2007), and lexicalization (Eisner, 1996; Collins, 1999). An orthogonal line of work uses ensemble or reranking strategies to further improve accuracy (Charniak and Johnson, 2005; Huang, 2008; Bj¨orkelund et al., 2014). Discontinuous c-parsing is considered a much harder problem, involving mildly context-sensitive formalisms such as LCFRS or range concatenation grammars, with treebankderived c-parsers exhibiting near-exponential runtime (Kallmeyer and Maier, 2013, Figure 27). To speed up decoding, prior work has considered restrictons, such as bounding the fan-out (Maier et"
P15-1147,W07-2216,0,0.0221821,"differing on their flatness and on left or right-branching decisions. In the next section, we introduce the concept of head-ordered d-trees and express one-to-one mappings between these two representations. Prior work. There has been a considerable amount of work developing rich-feature d-parsers. While projective d-parsers can use dynamic programming (Eisner and Satta, 1999; Koo and consider an extra root symbol, as often done in the literature. Collins, 2010), non-projective d-parsers typically rely on approximate decoders, since the underlying problem is NP-hard beyond arc-factored models (McDonald and Satta, 2007). An alternative are transition-based d-parsers (Nivre et al., 2006; Zhang and Nivre, 2011), which achieve observed linear time. Since d-parsing algorithms do not have a grammar constant, typical implementations are significantly faster than c-parsers (Rush and Petrov, 2012; Martins et al., 2013). The key contribution of this paper is to reduce c-parsing to dparsing, allowing to bring these runtimes closer. 3 Head-Ordered Dependency Trees We next endow d-trees with another layer of structure, namely order information. In this framework, not all modifiers of a head are “born equal.” Instead, th"
P15-1147,H05-1066,0,0.324737,"curate than H&N-encoding. 4.2 Training the Labeled Dependency Parser The next step is to train a labeled d-parser on the converted treebank. If we are doing continuous cparsing, we train a projective d-parser; otherwise we train a non-projective one. In our experiments, we found it advantageous to perform labeled d-parsing in two stages, as done by McDonald et al. (2006): first, train an unlabeled d-parser; then, train a dependency labeler.6 Table 1 compares this approach against a oneshot strategy, experimenting with various off-theshelf d-parsers: MaltParser (Nivre et al., 2007), MSTParser (McDonald et al., 2005), ZPar (Zhang and Nivre, 2011), and TurboParser (Martins et al., 2013), all with the default settings. For TurboParser, we used basic, standard and full models. Our separate d-labeler receives as input a backbone d-structure and predicts a label for each arc. For each head h, we predict the modifiers’ labels using a simple sequence model, with features of the form φ(h, m, `) and φ(h, m, m0 , `, `0 ), where m and m0 are two consecutive modifiers (possibly on opposite sides of the head) and ` and `0 are their labels. We use the same arc label features φ(h, m, `) as TurboParser. For φ(h, m, m0 ,"
P15-1147,W06-2932,0,0.24775,"-to-fine decoding (Charniak and Johnson, 2005) and shift-reduce parsing (Sagae and Lavie, 2005; Zhu et al., 2013) were a step forward to accelerate constituent parsing, but typical runtimes still lag those of dependency parsers. This is only made worse if discontinuous constituents are allowed—such discontinuities are convenient to represent wh-movement, scrambling, extraposition, and other linguistic phenomena common in free word order languages. While non-projective dependency parsers, which are able to model such phenomena, have been widely developed in the last decade (Nivre et al., 2007; McDonald et al., 2006; Martins et al., 2013), discontinuous constituent parsing is still taking its first steps (Maier and Søgaard, 2008; Kallmeyer and Maier, 2013). In this paper, we show that an off-the-shelf, trainable, dependency parser is enough to build a highly-competitive constituent parser. This (surprising) result is based on a reduction1 of constituent to dependency parsing, followed by a simple post-processing procedure to recover unaries. Unlike other constituent parsers, ours does not require estimating a grammar, nor binarizing the treebank. Moreover, when the dependency parser is non-projective, ou"
P15-1147,W06-2933,0,0.05129,"Missing"
P15-1147,P83-1021,0,0.856503,"dependency labels, enabling a dependency-to-constituent conversion. A related conversion was attempted by Hall and Nivre (2008) to parse German, but their complex encoding scheme blows up the number of arc labels, affecting the final parser’s quality. By contrast, our light encoding achieves a 10-fold decrease in the label alphabet, leading to more accurate parsing. While simple, our reduction-based parsers are on par with the Berkeley parser for English (Petrov ∗ This research was carried out during an internship at Priberam Labs. 1 The title of this paper is inspired by the seminal paper of Pereira and Warren (1983) “Parsing as Deduction.” We reduce phrase-based parsing to dependency parsing. Our reduction is grounded on a new intermediate representation, “head-ordered dependency trees,” shown to be isomorphic to constituent trees. By encoding order information in the dependency labels, we show that any off-theshelf, trainable dependency parser can be used to produce constituents. When this parser is non-projective, we can perform discontinuous parsing in a very natural manner. Despite the simplicity of our approach, experiments show that the resulting parsers are on par with strong baselines, such as th"
P15-1147,N07-1051,0,0.740605,"e which is not a preterminal is called a proper node. A node is called unary if it has exactly one child. A c-tree without unary proper nodes is called unaryless. If all proper nodes have exactly two children then it is called a binary c-tree. Continuous binary trees may be regarded as having been generated by a CFG in Chomsky normal form. Prior work. There has been a long string of work in statistical c-parsing, shifting from simple models (Charniak, 1996) to more sophisticated ones using structural annotation (Johnson, 1998; Klein and Manning, 2003), latent grammars (Matsuzaki et al., 2005; Petrov and Klein, 2007), and lexicalization (Eisner, 1996; Collins, 1999). An orthogonal line of work uses ensemble or reranking strategies to further improve accuracy (Charniak and Johnson, 2005; Huang, 2008; Bj¨orkelund et al., 2014). Discontinuous c-parsing is considered a much harder problem, involving mildly context-sensitive formalisms such as LCFRS or range concatenation grammars, with treebankderived c-parsers exhibiting near-exponential runtime (Kallmeyer and Maier, 2013, Figure 27). To speed up decoding, prior work has considered restrictons, such as bounding the fan-out (Maier et al., 2012) and requiring"
P15-1147,N12-1054,0,0.0257982,"h-feature d-parsers. While projective d-parsers can use dynamic programming (Eisner and Satta, 1999; Koo and consider an extra root symbol, as often done in the literature. Collins, 2010), non-projective d-parsers typically rely on approximate decoders, since the underlying problem is NP-hard beyond arc-factored models (McDonald and Satta, 2007). An alternative are transition-based d-parsers (Nivre et al., 2006; Zhang and Nivre, 2011), which achieve observed linear time. Since d-parsing algorithms do not have a grammar constant, typical implementations are significantly faster than c-parsers (Rush and Petrov, 2012; Martins et al., 2013). The key contribution of this paper is to reduce c-parsing to dparsing, allowing to bring these runtimes closer. 3 Head-Ordered Dependency Trees We next endow d-trees with another layer of structure, namely order information. In this framework, not all modifiers of a head are “born equal.” Instead, their attachment to the head occurs as a sequence of “events,” which reflect the head’s preference for attaching some modifiers before others. As we will see, this additional structure will undo the ambiguity expressed in Figure 2. 3.1 Strictly Ordered Dependency Trees Let us"
P15-1147,D10-1001,0,0.0604632,"to ours in §4.1: they encode the entire spines into the dependency labels, which become rather complex and numerous. A similar strategy has been used by Versley (2014a) for discontinuous c-parsing. Both are largely outperformed by our system, as shown in §5.3. The crucial difference is that we encode only the top node’s label and its position in the spine— besides being a much lighter representation, ours has an interpretation as a weak ordering, leading to the isomorphisms expressed in Propositions 1–3. Joint constituent and dependency parsing have been tackled by Carreras et al. (2008) and Rush et al. (2010), but the resulting parsers, while accurate, are more expensive than a single c-parser. Very recently, Kong et al. (2015) proposed a much cheaper pipeline in which d-parsing is performed first, followed by a c-parser constrained to be con1530 Parser Basque French German Hebrew Hungar. Korean Polish Swedish Avg. Berkeley 70.50 80.38 78.30 86.96 81.62 71.42 79.23 79.19 78.45 Berkeley Tagged 74.74 79.76 78.28 85.42 85.22 78.56 86.75 80.64 81.17 Hall et al. (2014) 83.39 79.70 78.43 87.18 88.25 80.18 90.66 82.00 83.72 Crabb´e and Seddah (2014) 85.35 79.68 77.15 86.19 87.51 79.35 91.60 82.72 83.69 T"
P15-1147,W05-1513,0,0.637978,"—one at which statistical models trained on treebanks have excelled (Charniak, 1996; Klein and Manning, 2003; Petrov and Klein, 2007). However, most existing parsers are slow, since they need to deal with a heavy grammar constant. Dependency parsers are generally faster, but less informative, since they do not produce constituents, which are often required by downstream applications (Johansson and Nugues, 2008; Wu et al., 2009; Berg-Kirkpatrick et al., 2011; Elming et al., 2013). How to get the best of both worlds? Coarse-to-fine decoding (Charniak and Johnson, 2005) and shift-reduce parsing (Sagae and Lavie, 2005; Zhu et al., 2013) were a step forward to accelerate constituent parsing, but typical runtimes still lag those of dependency parsers. This is only made worse if discontinuous constituents are allowed—such discontinuities are convenient to represent wh-movement, scrambling, extraposition, and other linguistic phenomena common in free word order languages. While non-projective dependency parsers, which are able to model such phenomena, have been widely developed in the last decade (Nivre et al., 2007; McDonald et al., 2006; Martins et al., 2013), discontinuous constituent parsing is still takin"
P15-1147,W14-6111,0,0.147129,"Missing"
P15-1147,A97-1014,0,0.727573,"tperforms the Berkeley parser (Petrov and Klein, 2007), with or without prescribed POS tags. Our average F1 -scores are superior to the best non-reranking system participating in the shared task (Crabb´e and Seddah, 2014) and to the c-parser of Hall et al. (2014), achieving the best results for 4 out of 8 languages. 5.3 V14b, gold Ours, gold V14b, pred Ours, pred TIGER - H & N HN08, gold V14a, gold Ours, gold HN08, pred CB13, pred Ours, pred Results on the Discontinuous Treebanks Finally, we experimented on two widely-used discontinuous German treebanks: TIGER (Brants et al., 2002) and NEGRA (Skut et al., 1997). For the former, we used two different splits: TIGER SPMRL , provided in the SPMRL14 shared task; and TIGER - H & N, used by Hall and Nivre (2008). For NEGRA, we used the standard splits. In these experiments, we skipped the unary recovery stage, since very few unary nodes exist in the data.7 We ran TurboTagger to predict POS tags for TIGER H & N and NEGRA , while in TIGER - SPMRL we used the predicted POS tags provided in the shared task. All treebanks were lexicalized using the head-rule sets of Rehbein (2009). For comparison to related work, sentence length cut-offs of 30, 40 and 70 were a"
P15-1147,P13-1045,0,0.123073,"Missing"
P15-1147,W13-5701,0,0.332692,"Missing"
P15-1147,E12-1047,0,0.357143,"Missing"
P15-1147,W14-6104,0,0.271033,"-parsers exhibiting near-exponential runtime (Kallmeyer and Maier, 2013, Figure 27). To speed up decoding, prior work has considered restrictons, such as bounding the fan-out (Maier et al., 2012) and requiring well-nestedness (Kuhlmann and Nivre, 2006; G´omez-Rodr´ıguez et al., 2010). Other approaches eliminate the discontinuities via tree transformations (Boyd, 2007; K¨ubler et al., 2008), sometimes as a pruning step in a coarse-to-fine parsing approach (van Cranenburgh and Bod, 2013). However, reported runtimes are still superior to 10 seconds per sentence, which is not practical. Recently, Versley (2014a) proposed an easy-first approach that leads to considerable speed-ups, but is less accurate. In this paper, we design fast discontinuous c-parsers that outperform all the ones above by a wide margin, with similar runtimes as Versley (2014a). 2.2 Dependency Trees In this paper, we use d-parsers as a black box to parse constituents. Given a sentence w1 . . . wL , a d-tree is a directed tree spanning all the words in the sentence.3 Each arc in this tree is a tuple 3 We assume throughout that dependency trees have a single root among {w1 , . . . , wL }. Therefore, there is no need to 1524 S NN D"
P15-1147,P87-1015,0,0.891031,"onstituent and dependency representations, and setting up the notation. Following Kong and Smith (2014), we use c-/d- prefixes for convenience (e.g., we write c-parser for constituent parser and d-tree for dependency tree). 2.1 Constituent Trees Constituent-based representations are commonly seen as derivations according to a context-free grammar (CFG). Here, we focus on properties of the c-trees, rather than of the grammars used to generate them. We consider a broad scenario that permits c-trees with discontinuities, such as the ones derived with linear context-free rewriting systems (LCFRS; Vijay-Shanker et al. (1987)). We also assume that the c-trees are lexicalized. Formally, let w1 w2 . . . wL be a sentence, where wi denotes the word in the ith position. A ctree is a rooted tree whose leaves are the words {wi }L i=1 , and whose internal nodes (constituents) are represented as a tuple hZ, h, Ii, where Z is a non-terminal symbol, h ∈ {1, . . . , L} indicates the lexical head, and I ⊆ {1, . . . , L} is the node’s yield. Each word’s parent is a pre-terminal unary node of the form hpi , i, {i}i, where pi denotes the word’s part-of-speech (POS) tag. The yields and lexical heads are defined so that for every c"
P15-1147,D09-1159,0,0.0237631,"sboa, Portugal # Priberam Labs, Alameda D. Afonso Henriques, 41, 2o , 1000-123 Lisboa, Portugal danifg@uvigo.es, atm@priberam.pt Abstract Constituent parsing is a central problem in NLP—one at which statistical models trained on treebanks have excelled (Charniak, 1996; Klein and Manning, 2003; Petrov and Klein, 2007). However, most existing parsers are slow, since they need to deal with a heavy grammar constant. Dependency parsers are generally faster, but less informative, since they do not produce constituents, which are often required by downstream applications (Johansson and Nugues, 2008; Wu et al., 2009; Berg-Kirkpatrick et al., 2011; Elming et al., 2013). How to get the best of both worlds? Coarse-to-fine decoding (Charniak and Johnson, 2005) and shift-reduce parsing (Sagae and Lavie, 2005; Zhu et al., 2013) were a step forward to accelerate constituent parsing, but typical runtimes still lag those of dependency parsers. This is only made worse if discontinuous constituents are allowed—such discontinuities are convenient to represent wh-movement, scrambling, extraposition, and other linguistic phenomena common in free word order languages. While non-projective dependency parsers, which are"
P15-1147,H01-1014,0,0.260927,"Missing"
P15-1147,W03-3023,0,0.184502,"presented as a tuple hZ, h, Ii, where Z is a non-terminal symbol, h ∈ {1, . . . , L} indicates the lexical head, and I ⊆ {1, . . . , L} is the node’s yield. Each word’s parent is a pre-terminal unary node of the form hpi , i, {i}i, where pi denotes the word’s part-of-speech (POS) tag. The yields and lexical heads are defined so that for every constituent hZ, h, Ii withSchildren K {hXk , mk , Jk i}K k=1 Jk ; k=1 , (i) we have I = and (ii) there is a unique k such that h = mk . This kth node (called the head-child node) is commonly chosen applying a handwritten set of head rules (Collins, 1999; Yamada and Matsumoto, 2003). A c-tree is continuous if all nodes hZ, h, Ii have a contiguous yield I, and discontinuous otherwise. Trees derived by a CFG are always continuous; those derived by a LCFRS may have discontinuities, the yield of a node being a union of spans, possibly with gaps in the middle. Figure 1 2 http://www.ark.cs.cmu.edu/TurboParser shows an example of a continuous and a discontinuous c-tree. Discontinuous c-trees have crossing branches, if the leaves are drawn in left-to-right surface order. An internal node which is not a preterminal is called a proper node. A node is called unary if it has exactly"
P15-1147,P11-2033,0,0.347822,"introduce the concept of head-ordered d-trees and express one-to-one mappings between these two representations. Prior work. There has been a considerable amount of work developing rich-feature d-parsers. While projective d-parsers can use dynamic programming (Eisner and Satta, 1999; Koo and consider an extra root symbol, as often done in the literature. Collins, 2010), non-projective d-parsers typically rely on approximate decoders, since the underlying problem is NP-hard beyond arc-factored models (McDonald and Satta, 2007). An alternative are transition-based d-parsers (Nivre et al., 2006; Zhang and Nivre, 2011), which achieve observed linear time. Since d-parsing algorithms do not have a grammar constant, typical implementations are significantly faster than c-parsers (Rush and Petrov, 2012; Martins et al., 2013). The key contribution of this paper is to reduce c-parsing to dparsing, allowing to bring these runtimes closer. 3 Head-Ordered Dependency Trees We next endow d-trees with another layer of structure, namely order information. In this framework, not all modifiers of a head are “born equal.” Instead, their attachment to the head occurs as a sequence of “events,” which reflect the head’s prefe"
P15-1147,P13-1043,0,0.592641,"al models trained on treebanks have excelled (Charniak, 1996; Klein and Manning, 2003; Petrov and Klein, 2007). However, most existing parsers are slow, since they need to deal with a heavy grammar constant. Dependency parsers are generally faster, but less informative, since they do not produce constituents, which are often required by downstream applications (Johansson and Nugues, 2008; Wu et al., 2009; Berg-Kirkpatrick et al., 2011; Elming et al., 2013). How to get the best of both worlds? Coarse-to-fine decoding (Charniak and Johnson, 2005) and shift-reduce parsing (Sagae and Lavie, 2005; Zhu et al., 2013) were a step forward to accelerate constituent parsing, but typical runtimes still lag those of dependency parsers. This is only made worse if discontinuous constituents are allowed—such discontinuities are convenient to represent wh-movement, scrambling, extraposition, and other linguistic phenomena common in free word order languages. While non-projective dependency parsers, which are able to model such phenomena, have been widely developed in the last decade (Nivre et al., 2007; McDonald et al., 2006; Martins et al., 2013), discontinuous constituent parsing is still taking its first steps ("
P15-1147,J03-4003,0,\N,Missing
P15-1147,C98-1102,0,\N,Missing
P16-1190,N15-1184,0,0.0137302,"exploit those resources. In this scenario, the goal is to use P¯ and the dataset Du to obtain “good” embeddings for the target languages (possibly tweaking the source embeddings too, P ≈ P¯ ). Our joint formulation in Eq. 8 can also be used to address this problem. It suffices to set K = L and V = IL (as in the multi-view learning case discussed above) and to define an auxiliary task that pushes P and P¯ to be similar. The simplest way is to use a reconstruction loss: 1 L`2 (P , P¯ ) := kP − P¯ k2F . 2 (10) The resulting optimization problem has resemblances with the retrofitting approach of Faruqui et al. (2015), except that the goal here is to extend the embeddings to other languages, instead of pushing monolingual embeddings to agree with a semantic lexicon. We will present some experiments in §5.2 using this framework. 4 Limitations of the Euclidean Co-Regularizer One may wonder how much the embedding dimension K influences the learned classifier. The next proposition shows the (surprising) result that, with the formulation in Eq. 8 with R = R`2 , it makes absolutely no difference to increase K past the number of labels L. Below, T ∈ RVT ×N denotes the matrix with columns t(1) , . . . , t(N ) . Pr"
P16-1190,W13-3520,0,0.0321344,"retrained embeddings were trained on a large amount of English monolingual data. Yet, the amount of target language data is the same. L = 300 dimensions (the dimension of the pretrained embeddings). A small sample of the multilingual embeddings produced by the winner system is shown in Table 4. Finally, we did a last experiment in which we use our multilingual embeddings obtained with Joint w/ Aux to train monolingual systems for each language. This time, we compare with a bag-ofwords naïve Bayes system (reported by Hermann and Blunsom (2014)), a system trained on the Polyglot embeddings from Al-Rfou et al. (2013) (which are multilingual, but not in a shared representation space), and the two systems developed by Hermann and Blunsom (2014). The results are shown in Table 3. We observe that, with the exception of Turkish, our systems consistently outperform all the competitors. Comparing the bottom two rows of Tables 2 and 3 we also observe that, for the `2 -regularized system, there is not much degradation caused by cross-lingual training versus training on the target language directly (in fact, for Spanish, Polish, and Brazilian Portuguese, the former scores are even higher). This suggests that the mu"
P16-1190,2012.eamt-1.60,0,0.0366178,"Missing"
P16-1190,D14-1082,0,0.02993,"highest reported scores on 10 out of 11 languages. 1 Introduction Distributed representations of text (embeddings) have been the target of much research in natural language processing (Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014; Levy et al., 2015). Word embeddings partially capture semantic and syntactic properties of text in the form of dense real vectors, making them apt for a wide variety of tasks, such as language modeling (Bengio et al., 2003), sentence tagging (Turian et al., 2010; Collobert et al., 2011), sentiment analysis (Socher et al., 2011), parsing (Chen and Manning, 2014), and machine translation (Zou et al., 2013). At the same time, there has been a consistent progress in devising “universal” multilingual models via cross-lingual transfer techniques of various kinds (Hwa et al., 2005; Zeman and Resnik, 2008; McDonald et al., 2011; Ganchev and Das, 2013; Martins, 2015). This line of research seeks ways of using data from resourcerich languages to solve tasks in resource-poor languages. Given the difficulty of handcrafting language-independent features, it is highly appealing to obtain rich, delexicalized, multilingual representations embedded in a shared space"
P16-1190,D15-1131,0,0.0364238,"ing of work started with Klementiev et al. (2012) on learning bilingual embeddings for text classification. Hermann and Blunsom (2014) proposed a noise-contrastive objective to push the embeddings of parallel sentences to be close in space. A bilingual auto-encoder was proposed by Chandar et al. (2014), while Faruqui and Dyer (2014) applied canonical correlation analysis to parallel data to improve monolingual embeddings. Other works optimize a sum of monolingual and cross-lingual terms (Gouws et al., 2015; Soyer et al., 2015), or introduce bilingual variants of skip-gram (Luong et al., 2015; Coulmance et al., 2015). Recently, Pham et al. (2015) extended the non-compositional paragraph vectors of Le and Mikolov (2014) to a bilingual setting, achieving a new state of the art at the cost of more expensive (and non-deterministic) prediction. In this paper, we propose an alternative joint formulation that learns embeddings suited to a particular task, together with the corresponding classifier for that task. We do this by minimizing a combination of a supervised loss function and a multilingual regularization term. Our approach leads to a convex optimization problem and makes a bridge between classical co-re"
P16-1190,E14-1049,0,0.0608587,"research seeks ways of using data from resourcerich languages to solve tasks in resource-poor languages. Given the difficulty of handcrafting language-independent features, it is highly appealing to obtain rich, delexicalized, multilingual representations embedded in a shared space. A string of work started with Klementiev et al. (2012) on learning bilingual embeddings for text classification. Hermann and Blunsom (2014) proposed a noise-contrastive objective to push the embeddings of parallel sentences to be close in space. A bilingual auto-encoder was proposed by Chandar et al. (2014), while Faruqui and Dyer (2014) applied canonical correlation analysis to parallel data to improve monolingual embeddings. Other works optimize a sum of monolingual and cross-lingual terms (Gouws et al., 2015; Soyer et al., 2015), or introduce bilingual variants of skip-gram (Luong et al., 2015; Coulmance et al., 2015). Recently, Pham et al. (2015) extended the non-compositional paragraph vectors of Le and Mikolov (2014) to a bilingual setting, achieving a new state of the art at the cost of more expensive (and non-deterministic) prediction. In this paper, we propose an alternative joint formulation that learns embeddings s"
P16-1190,D13-1205,0,0.013181,"ddings partially capture semantic and syntactic properties of text in the form of dense real vectors, making them apt for a wide variety of tasks, such as language modeling (Bengio et al., 2003), sentence tagging (Turian et al., 2010; Collobert et al., 2011), sentiment analysis (Socher et al., 2011), parsing (Chen and Manning, 2014), and machine translation (Zou et al., 2013). At the same time, there has been a consistent progress in devising “universal” multilingual models via cross-lingual transfer techniques of various kinds (Hwa et al., 2005; Zeman and Resnik, 2008; McDonald et al., 2011; Ganchev and Das, 2013; Martins, 2015). This line of research seeks ways of using data from resourcerich languages to solve tasks in resource-poor languages. Given the difficulty of handcrafting language-independent features, it is highly appealing to obtain rich, delexicalized, multilingual representations embedded in a shared space. A string of work started with Klementiev et al. (2012) on learning bilingual embeddings for text classification. Hermann and Blunsom (2014) proposed a noise-contrastive objective to push the embeddings of parallel sentences to be close in space. A bilingual auto-encoder was proposed b"
P16-1190,P14-1006,0,0.222869,"niversal” multilingual models via cross-lingual transfer techniques of various kinds (Hwa et al., 2005; Zeman and Resnik, 2008; McDonald et al., 2011; Ganchev and Das, 2013; Martins, 2015). This line of research seeks ways of using data from resourcerich languages to solve tasks in resource-poor languages. Given the difficulty of handcrafting language-independent features, it is highly appealing to obtain rich, delexicalized, multilingual representations embedded in a shared space. A string of work started with Klementiev et al. (2012) on learning bilingual embeddings for text classification. Hermann and Blunsom (2014) proposed a noise-contrastive objective to push the embeddings of parallel sentences to be close in space. A bilingual auto-encoder was proposed by Chandar et al. (2014), while Faruqui and Dyer (2014) applied canonical correlation analysis to parallel data to improve monolingual embeddings. Other works optimize a sum of monolingual and cross-lingual terms (Gouws et al., 2015; Soyer et al., 2015), or introduce bilingual variants of skip-gram (Luong et al., 2015; Coulmance et al., 2015). Recently, Pham et al. (2015) extended the non-compositional paragraph vectors of Le and Mikolov (2014) to a b"
P16-1190,C12-1089,0,0.513128,"t al., 2013). At the same time, there has been a consistent progress in devising “universal” multilingual models via cross-lingual transfer techniques of various kinds (Hwa et al., 2005; Zeman and Resnik, 2008; McDonald et al., 2011; Ganchev and Das, 2013; Martins, 2015). This line of research seeks ways of using data from resourcerich languages to solve tasks in resource-poor languages. Given the difficulty of handcrafting language-independent features, it is highly appealing to obtain rich, delexicalized, multilingual representations embedded in a shared space. A string of work started with Klementiev et al. (2012) on learning bilingual embeddings for text classification. Hermann and Blunsom (2014) proposed a noise-contrastive objective to push the embeddings of parallel sentences to be close in space. A bilingual auto-encoder was proposed by Chandar et al. (2014), while Faruqui and Dyer (2014) applied canonical correlation analysis to parallel data to improve monolingual embeddings. Other works optimize a sum of monolingual and cross-lingual terms (Gouws et al., 2015; Soyer et al., 2015), or introduce bilingual variants of skip-gram (Luong et al., 2015; Coulmance et al., 2015). Recently, Pham et al. (2"
P16-1190,2005.mtsummit-papers.11,0,0.0342439,"e next section, we experiment with R ∈ {R`2 , R`1 , Rns }, where Rns is the (non-convex) noise-contrastive regularizer of Eq. 3. 5 Experiments We report results on two experiments: one on cross-lingual classification on the Reuters RCV1/RCV2 dataset, and another on multi-label classification with multilingual embeddings on the TED Corpus.5 5.1 Reuters RCV1/RCV2 We evaluate our framework on the cross-lingual document classification task introduced by Klementiev et al. (2012). Following prior work, our dataset Du consists of 500,000 parallel sentences from the Europarl v7 English-German corpus (Koehn, 2005); and our labeled dataset Dl consists of English and German documents from the RCV1/RCV2 corpora (Lewis et al., 2004), each categorized with one out of L = 4 labels. We used the same split as Klementiev et al. (2012): 1,000 documents for training, of which 200 are held out as validation data, and 5,000 for testing. 4 For regression tasks (such as the one presented in the last paragraph of 3), instead of the “number of labels,” L should be regarded as the number of output variables to regress. 5 Our code is available at https: //github.com/dcferreira/ multilingual-joint-embeddings. Note that, i"
P16-1190,Q15-1016,0,0.04105,"arizers to increase the embedding dimension, a limitation which does not exist for other co-regularizers (such as the `1 distance). Despite its simplicity, our method achieves state-of-the-art accuracies on the RCV1/RCV2 dataset when transferring from English to German, with training times below 1 minute. On the TED Corpus, we obtain the highest reported scores on 10 out of 11 languages. 1 Introduction Distributed representations of text (embeddings) have been the target of much research in natural language processing (Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014; Levy et al., 2015). Word embeddings partially capture semantic and syntactic properties of text in the form of dense real vectors, making them apt for a wide variety of tasks, such as language modeling (Bengio et al., 2003), sentence tagging (Turian et al., 2010; Collobert et al., 2011), sentiment analysis (Socher et al., 2011), parsing (Chen and Manning, 2014), and machine translation (Zou et al., 2013). At the same time, there has been a consistent progress in devising “universal” multilingual models via cross-lingual transfer techniques of various kinds (Hwa et al., 2005; Zeman and Resnik, 2008; McDonald et"
P16-1190,P15-1138,1,0.839183,"e semantic and syntactic properties of text in the form of dense real vectors, making them apt for a wide variety of tasks, such as language modeling (Bengio et al., 2003), sentence tagging (Turian et al., 2010; Collobert et al., 2011), sentiment analysis (Socher et al., 2011), parsing (Chen and Manning, 2014), and machine translation (Zou et al., 2013). At the same time, there has been a consistent progress in devising “universal” multilingual models via cross-lingual transfer techniques of various kinds (Hwa et al., 2005; Zeman and Resnik, 2008; McDonald et al., 2011; Ganchev and Das, 2013; Martins, 2015). This line of research seeks ways of using data from resourcerich languages to solve tasks in resource-poor languages. Given the difficulty of handcrafting language-independent features, it is highly appealing to obtain rich, delexicalized, multilingual representations embedded in a shared space. A string of work started with Klementiev et al. (2012) on learning bilingual embeddings for text classification. Hermann and Blunsom (2014) proposed a noise-contrastive objective to push the embeddings of parallel sentences to be close in space. A bilingual auto-encoder was proposed by Chandar et al."
P16-1190,D11-1006,0,0.0518241,"t al., 2015). Word embeddings partially capture semantic and syntactic properties of text in the form of dense real vectors, making them apt for a wide variety of tasks, such as language modeling (Bengio et al., 2003), sentence tagging (Turian et al., 2010; Collobert et al., 2011), sentiment analysis (Socher et al., 2011), parsing (Chen and Manning, 2014), and machine translation (Zou et al., 2013). At the same time, there has been a consistent progress in devising “universal” multilingual models via cross-lingual transfer techniques of various kinds (Hwa et al., 2005; Zeman and Resnik, 2008; McDonald et al., 2011; Ganchev and Das, 2013; Martins, 2015). This line of research seeks ways of using data from resourcerich languages to solve tasks in resource-poor languages. Given the difficulty of handcrafting language-independent features, it is highly appealing to obtain rich, delexicalized, multilingual representations embedded in a shared space. A string of work started with Klementiev et al. (2012) on learning bilingual embeddings for text classification. Hermann and Blunsom (2014) proposed a noise-contrastive objective to push the embeddings of parallel sentences to be close in space. A bilingual auto"
P16-1190,D11-1014,0,0.0600604,"the TED Corpus, we obtain the highest reported scores on 10 out of 11 languages. 1 Introduction Distributed representations of text (embeddings) have been the target of much research in natural language processing (Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014; Levy et al., 2015). Word embeddings partially capture semantic and syntactic properties of text in the form of dense real vectors, making them apt for a wide variety of tasks, such as language modeling (Bengio et al., 2003), sentence tagging (Turian et al., 2010; Collobert et al., 2011), sentiment analysis (Socher et al., 2011), parsing (Chen and Manning, 2014), and machine translation (Zou et al., 2013). At the same time, there has been a consistent progress in devising “universal” multilingual models via cross-lingual transfer techniques of various kinds (Hwa et al., 2005; Zeman and Resnik, 2008; McDonald et al., 2011; Ganchev and Das, 2013; Martins, 2015). This line of research seeks ways of using data from resourcerich languages to solve tasks in resource-poor languages. Given the difficulty of handcrafting language-independent features, it is highly appealing to obtain rich, delexicalized, multilingual represen"
P16-1190,P10-1040,0,0.0531049,"ing from English to German, with training times below 1 minute. On the TED Corpus, we obtain the highest reported scores on 10 out of 11 languages. 1 Introduction Distributed representations of text (embeddings) have been the target of much research in natural language processing (Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014; Levy et al., 2015). Word embeddings partially capture semantic and syntactic properties of text in the form of dense real vectors, making them apt for a wide variety of tasks, such as language modeling (Bengio et al., 2003), sentence tagging (Turian et al., 2010; Collobert et al., 2011), sentiment analysis (Socher et al., 2011), parsing (Chen and Manning, 2014), and machine translation (Zou et al., 2013). At the same time, there has been a consistent progress in devising “universal” multilingual models via cross-lingual transfer techniques of various kinds (Hwa et al., 2005; Zeman and Resnik, 2008; McDonald et al., 2011; Ganchev and Das, 2013; Martins, 2015). This line of research seeks ways of using data from resourcerich languages to solve tasks in resource-poor languages. Given the difficulty of handcrafting language-independent features, it is hi"
P16-1190,I08-3008,0,0.0337746,"ton et al., 2014; Levy et al., 2015). Word embeddings partially capture semantic and syntactic properties of text in the form of dense real vectors, making them apt for a wide variety of tasks, such as language modeling (Bengio et al., 2003), sentence tagging (Turian et al., 2010; Collobert et al., 2011), sentiment analysis (Socher et al., 2011), parsing (Chen and Manning, 2014), and machine translation (Zou et al., 2013). At the same time, there has been a consistent progress in devising “universal” multilingual models via cross-lingual transfer techniques of various kinds (Hwa et al., 2005; Zeman and Resnik, 2008; McDonald et al., 2011; Ganchev and Das, 2013; Martins, 2015). This line of research seeks ways of using data from resourcerich languages to solve tasks in resource-poor languages. Given the difficulty of handcrafting language-independent features, it is highly appealing to obtain rich, delexicalized, multilingual representations embedded in a shared space. A string of work started with Klementiev et al. (2012) on learning bilingual embeddings for text classification. Hermann and Blunsom (2014) proposed a noise-contrastive objective to push the embeddings of parallel sentences to be close in"
P16-1190,D13-1141,0,0.0561754,"1 Introduction Distributed representations of text (embeddings) have been the target of much research in natural language processing (Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014; Levy et al., 2015). Word embeddings partially capture semantic and syntactic properties of text in the form of dense real vectors, making them apt for a wide variety of tasks, such as language modeling (Bengio et al., 2003), sentence tagging (Turian et al., 2010; Collobert et al., 2011), sentiment analysis (Socher et al., 2011), parsing (Chen and Manning, 2014), and machine translation (Zou et al., 2013). At the same time, there has been a consistent progress in devising “universal” multilingual models via cross-lingual transfer techniques of various kinds (Hwa et al., 2005; Zeman and Resnik, 2008; McDonald et al., 2011; Ganchev and Das, 2013; Martins, 2015). This line of research seeks ways of using data from resourcerich languages to solve tasks in resource-poor languages. Given the difficulty of handcrafting language-independent features, it is highly appealing to obtain rich, delexicalized, multilingual representations embedded in a shared space. A string of work started with Klementiev e"
P16-1190,D14-1162,0,0.0813121,"ion of Euclidean co-regularizers to increase the embedding dimension, a limitation which does not exist for other co-regularizers (such as the `1 distance). Despite its simplicity, our method achieves state-of-the-art accuracies on the RCV1/RCV2 dataset when transferring from English to German, with training times below 1 minute. On the TED Corpus, we obtain the highest reported scores on 10 out of 11 languages. 1 Introduction Distributed representations of text (embeddings) have been the target of much research in natural language processing (Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014; Levy et al., 2015). Word embeddings partially capture semantic and syntactic properties of text in the form of dense real vectors, making them apt for a wide variety of tasks, such as language modeling (Bengio et al., 2003), sentence tagging (Turian et al., 2010; Collobert et al., 2011), sentiment analysis (Socher et al., 2011), parsing (Chen and Manning, 2014), and machine translation (Zou et al., 2013). At the same time, there has been a consistent progress in devising “universal” multilingual models via cross-lingual transfer techniques of various kinds (Hwa et al., 2005; Zeman and Resnik"
P16-1190,P15-2093,0,\N,Missing
P16-1190,W15-1521,0,\N,Missing
P16-1190,W15-1512,0,\N,Missing
P18-2059,P13-1020,1,0.861816,"Missing"
P18-2059,D15-1166,0,0.0556709,"ention probabilities are bounded by u. Martins and Kreutzer (2017) have shown that this transformation can be evaluated in O(J log J) time and its gradients backpropagated in O(J) time. To use this transformation in the attention mechanism, we make use of the idea of fertility (Brown et al., 1993). Namely, let βt−1 := Pt−1 τ =1 ατ denote the cumulative attention that each source word has received up to time step t, and let f := (fj )Jj=1 be a vector containing fertility upper bounds for each source word. The attention at step t is computed as (2) where zt ∈ RJ is a vector of scores. We follow Luong et al. (2015) and define zt,j := s> t−1 W hj as a bilinear transformation of encoder and decoder states, where W is a model parameter.2 3 (4) Sparse and Constrained Attention In this work, we consider alternatives to Eq. 2. Since the softmax is strictly positive, it forces all words in the source to receive some probability mass in the resulting attention distribution, which can be wasteful. Moreover, it may happen that the decoder attends repeatedly to the same source words across time steps, causing repetitions in the generated translation, as Tu et al. (2016) observed. With this in mind, we replace Eq."
P18-2059,D17-1036,1,0.835833,"ins @tecnico.ulisboa.pt @unbabel.com Abstract implicitly in their proposed model. Their fertility conditioned decoder uses a coverage vector and an extract gate which are incorporated in the decoding recurrent unit, increasing the number of parameters. In this paper, we propose a different solution that does not change the overall architecture, but only the attention transformation. Namely, we replace the traditional softmax by other recently proposed transformations that either promote attention sparsity (Martins and Astudillo, 2016) or upper bound the amount of attention a word can receive (Martins and Kreutzer, 2017). The bounds are determined by the fertility values of the source words. While these transformations have given encouraging results in various NLP problems, they have never been applied to NMT, to the best of our knowledge. Furthermore, we combine these two ideas and propose a novel attention transformation, constrained sparsemax, which produces both sparse and bounded attention weights, yielding a compact and interpretable set of alignments. While being in-between soft and hard alignments (Figure 2), the constrained sparsemax transformation is end-to-end differentiable, hence amenable for tra"
P18-2059,J93-2003,0,0.108506,"artins and Kreutzer (2017) in the context of easy-first sequence tagging, being defined as follows: csoftmax(z; u) := arg min KL(αk softmax(z)) α∈∆J s.t. α ≤ u, where u is a vector of upper bounds, and KL(.k.) is the Kullback-Leibler divergence. In other words, it returns the distribution closest to softmax(z) whose attention probabilities are bounded by u. Martins and Kreutzer (2017) have shown that this transformation can be evaluated in O(J log J) time and its gradients backpropagated in O(J) time. To use this transformation in the attention mechanism, we make use of the idea of fertility (Brown et al., 1993). Namely, let βt−1 := Pt−1 τ =1 ατ denote the cumulative attention that each source word has received up to time step t, and let f := (fj )Jj=1 be a vector containing fertility upper bounds for each source word. The attention at step t is computed as (2) where zt ∈ RJ is a vector of scores. We follow Luong et al. (2015) and define zt,j := s> t−1 W hj as a bilinear transformation of encoder and decoder states, where W is a model parameter.2 3 (4) Sparse and Constrained Attention In this work, we consider alternatives to Eq. 2. Since the softmax is strictly positive, it forces all words in the s"
P18-2059,D16-1096,0,0.0220612,"ile NMT is generally more fluent than previous statistical systems, adequacy is still a major concern (Koehn and Knowles, 2017): common mistakes include dropping source words and repeating words in the generated translation. Previous work has attempted to mitigate this problem in various ways. Wu et al. (2016) incorporate coverage and length penalties during beam search—a simple yet limited solution, since it only affects the scores of translation hypotheses that are already in the beam. Other approaches involve architectural changes: providing coverage vectors to track the attention history (Mi et al., 2016; Tu et al., 2016), using gating architectures and adaptive attention to control the amount of source context provided (Tu et al., 2017a; Li and Zhu, 2017), or adding a reconstruction loss (Tu et al., 2017b). Feng et al. (2016) also use the notion of fertility ∗ • We formulate constrained sparsemax and derive efficient linear and sublinear-time algorithms for running forward and backward propagation. This transformation has two levels of sparsity: over time steps, and over the attended words at each step. • We provide a detailed empirical comparison of various attention transformations, includ"
P18-2059,W14-3348,0,0.0373829,"/sparsemax. At training time this may be problematic, since the target length is fixed and the problems in Eqs. 4–6 can become infeasible. By adding the sink token we P guarantee j fj = ∞, eliminating the problem. • C OV V ECTOR (Tu et al., 2016). At training and test time, coverage vectors β and additional parameters v are used to condition the next attention step. We adapted this to our bilinear attention by defining zt,j = s> t−1 (W hj + vβt−1,j ). We also experimented combining the strategies above with the sparsemax transformation. As evaluation metrics, we report tokenized BLEU, METEOR (Denkowski and Lavie (2014), as well as two new metrics that we describe next to account for over and under-translation.6 Exhaustion strategies. To avoid missing source words, we implemented a simple strategy to encourage more attention to words with larger credit: we redefine the pre-attention word scores as zt0 = zt + cut , where c is a constant (c = 0.2 in our experiments). This increases the score of words which have not yet exhausted their fertility (we may regard it as a “soft” lower bound in Eqs. 4–6). 4 We used a 2-layer LSTM, embedding and hidden size of 500, dropout 0.3, and the SGD optimizer for 13 epochs. 5"
P18-2059,C16-1290,0,0.0195783,"Previous work has attempted to mitigate this problem in various ways. Wu et al. (2016) incorporate coverage and length penalties during beam search—a simple yet limited solution, since it only affects the scores of translation hypotheses that are already in the beam. Other approaches involve architectural changes: providing coverage vectors to track the attention history (Mi et al., 2016; Tu et al., 2016), using gating architectures and adaptive attention to control the amount of source context provided (Tu et al., 2017a; Li and Zhu, 2017), or adding a reconstruction loss (Tu et al., 2017b). Feng et al. (2016) also use the notion of fertility ∗ • We formulate constrained sparsemax and derive efficient linear and sublinear-time algorithms for running forward and backward propagation. This transformation has two levels of sparsity: over time steps, and over the attended words at each step. • We provide a detailed empirical comparison of various attention transformations, including softmax (Bahdanau et al., 2014), sparse1 Our software code is available at the OpenNMT fork www.github.com/Unbabel/OpenNMT-py/tree/dev and the running scripts at www.github.com/Unbabel/ sparse constrained attention. Work do"
P18-2059,P16-1162,0,0.101534,"tion transformations on three language pairs. We focused on small datasets, as they are the most affected by coverage mistakes. We use the IWSLT 2014 corpus for D E -E N, the KFTT corpus for JA -E N (Neubig, 2011), and the WMT 2016 dataset for RO E N. The training sets have 153,326, 329,882, and 560,767 parallel sentences, respectively. Our reason to prefer smaller datasets is that this regime is what brings more adequacy issues and demands more structural biases, hence it is a good test bed for our methods. We tokenized the data using the Moses scripts and preprocessed it with subword units (Sennrich et al., 2016) with a joint vocabulary and 32k merge operations. Our implementation was done on a fork of the OpenNMT-py toolkit (Klein et al., 2017) with the default parameters 4 . We used a validation set to tune hyperparameters introduced by our model. Even though our attention implementations are CPU-based using NumPy (unlike the rest of the computation which is done on the GPU), we did not observe any noticeable slowdown using multiple devices. As baselines, we use softmax attention, as well as two recently proposed coverage models: • C OV P ENALTY (Wu et al., 2016, §7). At test time, the hypotheses in"
P18-2059,P17-1012,0,0.0217496,"translation. We explore novel strategies to address the coverage problem that change only the attention transformation. Our approach allocates fertilities to source words, used to bound the attention each word can receive. We experiment with various sparse and constrained attention transformations and propose a new one, constrained sparsemax, shown to be differentiable and sparse. Empirical evaluation is provided in three languages pairs. 1 Introduction Neural machine translation (NMT) emerged in the last few years as a very successful paradigm (Sutskever et al., 2014; Bahdanau et al., 2014; Gehring et al., 2017; Vaswani et al., 2017). While NMT is generally more fluent than previous statistical systems, adequacy is still a major concern (Koehn and Knowles, 2017): common mistakes include dropping source words and repeating words in the generated translation. Previous work has attempted to mitigate this problem in various ways. Wu et al. (2016) incorporate coverage and length penalties during beam search—a simple yet limited solution, since it only affects the scores of translation hypotheses that are already in the beam. Other approaches involve architectural changes: providing coverage vectors to tr"
P18-2059,Q17-1007,0,0.0251115,"mistakes include dropping source words and repeating words in the generated translation. Previous work has attempted to mitigate this problem in various ways. Wu et al. (2016) incorporate coverage and length penalties during beam search—a simple yet limited solution, since it only affects the scores of translation hypotheses that are already in the beam. Other approaches involve architectural changes: providing coverage vectors to track the attention history (Mi et al., 2016; Tu et al., 2016), using gating architectures and adaptive attention to control the amount of source context provided (Tu et al., 2017a; Li and Zhu, 2017), or adding a reconstruction loss (Tu et al., 2017b). Feng et al. (2016) also use the notion of fertility ∗ • We formulate constrained sparsemax and derive efficient linear and sublinear-time algorithms for running forward and backward propagation. This transformation has two levels of sparsity: over time steps, and over the attended words at each step. • We provide a detailed empirical comparison of various attention transformations, including softmax (Bahdanau et al., 2014), sparse1 Our software code is available at the OpenNMT fork www.github.com/Unbabel/OpenNMT-py/tree/"
P18-2059,P17-4012,0,0.032886,"IWSLT 2014 corpus for D E -E N, the KFTT corpus for JA -E N (Neubig, 2011), and the WMT 2016 dataset for RO E N. The training sets have 153,326, 329,882, and 560,767 parallel sentences, respectively. Our reason to prefer smaller datasets is that this regime is what brings more adequacy issues and demands more structural biases, hence it is a good test bed for our methods. We tokenized the data using the Moses scripts and preprocessed it with subword units (Sennrich et al., 2016) with a joint vocabulary and 32k merge operations. Our implementation was done on a fork of the OpenNMT-py toolkit (Klein et al., 2017) with the default parameters 4 . We used a validation set to tune hyperparameters introduced by our model. Even though our attention implementations are CPU-based using NumPy (unlike the rest of the computation which is done on the GPU), we did not observe any noticeable slowdown using multiple devices. As baselines, we use softmax attention, as well as two recently proposed coverage models: • C OV P ENALTY (Wu et al., 2016, §7). At test time, the hypotheses in the beam are rescored with a global score that includes a length and a coverage penalty.5 We tuned α and β with grid search on {0.2k}5"
P18-2059,P16-1008,0,0.290033,"lly more fluent than previous statistical systems, adequacy is still a major concern (Koehn and Knowles, 2017): common mistakes include dropping source words and repeating words in the generated translation. Previous work has attempted to mitigate this problem in various ways. Wu et al. (2016) incorporate coverage and length penalties during beam search—a simple yet limited solution, since it only affects the scores of translation hypotheses that are already in the beam. Other approaches involve architectural changes: providing coverage vectors to track the attention history (Mi et al., 2016; Tu et al., 2016), using gating architectures and adaptive attention to control the amount of source context provided (Tu et al., 2017a; Li and Zhu, 2017), or adding a reconstruction loss (Tu et al., 2017b). Feng et al. (2016) also use the notion of fertility ∗ • We formulate constrained sparsemax and derive efficient linear and sublinear-time algorithms for running forward and backward propagation. This transformation has two levels of sparsity: over time steps, and over the attended words at each step. • We provide a detailed empirical comparison of various attention transformations, including softmax (Bahda"
P18-2059,W17-3204,0,0.0276455,"ities to source words, used to bound the attention each word can receive. We experiment with various sparse and constrained attention transformations and propose a new one, constrained sparsemax, shown to be differentiable and sparse. Empirical evaluation is provided in three languages pairs. 1 Introduction Neural machine translation (NMT) emerged in the last few years as a very successful paradigm (Sutskever et al., 2014; Bahdanau et al., 2014; Gehring et al., 2017; Vaswani et al., 2017). While NMT is generally more fluent than previous statistical systems, adequacy is still a major concern (Koehn and Knowles, 2017): common mistakes include dropping source words and repeating words in the generated translation. Previous work has attempted to mitigate this problem in various ways. Wu et al. (2016) incorporate coverage and length penalties during beam search—a simple yet limited solution, since it only affects the scores of translation hypotheses that are already in the beam. Other approaches involve architectural changes: providing coverage vectors to track the attention history (Mi et al., 2016; Tu et al., 2016), using gating architectures and adaptive attention to control the amount of source context pr"
P18-2059,1983.tc-1.13,0,0.567585,"Missing"
P18-4020,P07-2045,1,0.0306875,"Missing"
P18-4020,E17-2025,0,0.0357645,"83.9 73.0 67.9 54.8 46.6 35.3 40 23.5 20 12.4 60 0 100 Deep RNN 80 60 40 20 7.8 42.5 33.437.1 28.2 22.8 17.2 13.1 0 100 Transformer 80 54.9 49.0 42.9 37.6 30.4 23.4 16.4 60 40 20 0 9.1 1 2 3 4 5 6 7 8 Number of GPUs Figure 1: Training speed in thousands of source tokens per second for shallow RNN, deep RNN and Transformer model. Dashed line projects linear scale-up based on single-GPU performance. ory to maximize speed and memory usage. This guarantees that a chosen memory budget will not be exceeded during training. All models use tied embeddings between source, target and output embeddings (Press and Wolf, 2017). Contrary to Sennrich et al. (2017a) or Vaswani et al. (2017), we do not average checkpoints, but maintain a continuously updated exponentially averaged model over the entire training run. Following Vaswani et al. (2017), the learning rate is set to 0.0003 and decayed as the inverse square root of the number of updates after 16,000 updates. When training the transformer model, a linearly growing learning rate is used during the first 16,000 iterations, starting with 0 until the base learning rate is reached. 118 W a¨ Si hl e e n ei n Ta en s be tatu - rfe h ss la im tz M e fe nu¨ s . tleg e E"
P18-4020,W17-4774,1,0.824066,"s. Scaling is not linear (dashed lines), but close. The tokens-per-second rate (w/s) for Nematus on the same data on a single GPU is about 2800 w/s for the shallow model. Nematus does not have multiGPU training. Marian achieves about 4 times faster training on a single GPU and about 30 times faster training on 8 GPUs for identical models. Translation speed. The back-translation of 10M sentences with a shallow model takes about four State-of-the-art in Neural Automatic Post-Editing In our submission to the Automatic Post-Editing shared task at WMT-2017 (Bojar et al., 2017b) and follow-up work (Junczys-Dowmunt and Grundkiewicz, 2017a,b), we explore multiple neural architectures adapted for the task of automatic postediting of machine translation output as implementations in Marian. We focus on neural end-to-end models that combine both inputs mt (raw MT output) and src (source language input) in a single neural architecture, modeling {mt, src} → pe directly, where pe is post-edited corrected output. These models are based on multi-source neural translation models introduced by Zoph and Knight (2016). Furthermore, we investigate the effect of hard-attention models or neural transductors (Aharoni and Goldberg, 2016) which"
P18-4020,E17-3017,1,0.852816,"Missing"
P18-4020,I17-1013,1,0.851844,"s. Scaling is not linear (dashed lines), but close. The tokens-per-second rate (w/s) for Nematus on the same data on a single GPU is about 2800 w/s for the shallow model. Nematus does not have multiGPU training. Marian achieves about 4 times faster training on a single GPU and about 30 times faster training on 8 GPUs for identical models. Translation speed. The back-translation of 10M sentences with a shallow model takes about four State-of-the-art in Neural Automatic Post-Editing In our submission to the Automatic Post-Editing shared task at WMT-2017 (Bojar et al., 2017b) and follow-up work (Junczys-Dowmunt and Grundkiewicz, 2017a,b), we explore multiple neural architectures adapted for the task of automatic postediting of machine translation output as implementations in Marian. We focus on neural end-to-end models that combine both inputs mt (raw MT output) and src (source language input) in a single neural architecture, modeling {mt, src} → pe directly, where pe is post-edited corrected output. These models are based on multi-source neural translation models introduced by Zoph and Knight (2016). Furthermore, we investigate the effect of hard-attention models or neural transductors (Aharoni and Goldberg, 2016) which"
P18-4020,P16-1162,1,0.624087,"work, we implemented many efficient meta- ple scripts at https://github.com/marian-nmt/ algorithms. These include multi-device (GPU or marian-examples. 117 test2017 UEdin WMT17 (single) +Ensemble of 4 +R2L Reranking 33.9 35.1 36.2 27.5 28.3 28.3 Deep RNN (single) +Ensemble of 4 +R2L Reranking 34.3 35.3 35.9 27.7 28.2 28.7 Transformer (single) +Ensemble of 4 +R2L Reranking 35.6 36.4 36.8 28.8 29.4 29.5 Source tokens per second ×103 test2016 Source tokens per second ×103 System • preprocessing of training data, tokenization, true-casing4 , vocabulary reduction to 36,000 joint BPE subword units (Sennrich et al., 2016) with a separate tool.5 • training of a shallow model for backtranslation on parallel WMT17 data; • translation of 10M German monolingual news sentences to English; concatenation of artificial training corpus with original data (times two) to produce new training data; • training of four left-to-right (L2R) deep models (either RNN-based or Transformer-based); • training of four additional deep models with right-to-left (R2L) orientation; 6 • ensemble-decoding with four L2R models resulting in an n-best list of 12 hypotheses per input sentence; • rescoring of n-best list with four R2L models, a"
P18-4020,N18-1055,1,0.879647,"Missing"
P18-4020,P17-4012,0,0.0756084,"lconquers-patent-translation-in-majorwipo-roll-out/ Marian has minimal dependencies (only Boost and CUDA or a BLAS library) and enables barrierfree optimization at all levels: meta-algorithms such as MPI-based multi-node training, efficient batched beam search, compact implementations of new models, custom operators, and custom GPU kernels. Intel has contributed and is optimizing a CPU backend. Marian grew out of a C++ re-implementation of Nematus (Sennrich et al., 2017b), and still maintains binary-compatibility for common models. Hence, we will compare speed mostly against Nematus. OpenNMT (Klein et al., 2017), perhaps one of the most popular toolkits, has been reported to have training speed competitive to Nematus. Marian is distributed under the MIT license and available from https://marian-nmt. github.io or the GitHub repository https: //github.com/marian-nmt/marian. 2 Design Outline We will very briefly discuss the design of Marian. Technical details of the implementations will be provided in later work. 2.1 Custom Auto-Differentiation Engine The deep-learning back-end included in Marian is based on reverse-mode auto-differentiation with dynamic computation graphs and among the established mach"
P19-1146,P82-1020,0,0.844168,"Missing"
P19-1146,N19-1357,0,0.0359477,"cularly harmful for long sequences, as shown by Luong et al. (2015), who propose “local attention” to mitigate this problem. Combining sparse attention with fertility constraints has been recently proposed by Malaviya et al. (2018). Hard attention (Xu et al., 2015; Aharoni and Goldberg, 2017; Wu et al., 2018) selects exactly one source token. Its discrete, non-differentiable nature requires imitation learning or Monte Carlo policy gradient approximations, which drastically complicate training. In contrast, entmax is a differentiable, easy to use, drop-in softmax replacement. A recent study by Jain and Wallace (2019) tackles the limitations of attention probabilities to provide interpretability. They only study dense attention in classification tasks, where attention is less crucial for the final predictions. In their conclusions, the authors defer Losses for seq2seq models. Mostly motivated by the challenges of large vocabulary sizes in seq2seq, an important research direction tackles replacing the cross-entropy loss with other losses or approximations (Bengio and Sen´ecal, 2008; Morin and Bengio, 2005; Kumar and Tsvetkov, 2019). While differently motivated, some of the above strategies (e.g., hierarchic"
P19-1146,W16-2323,0,0.012363,"al seq2seq model that can offer optimality guarantees. 4.2 Machine Translation We now turn to a highly different seq2seq regime in which the vocabulary size is much larger, there is a great deal of ambiguity, and sequences can generally be translated in several ways. We train models for three language pairs in both directions: • IWSLT 2017 German ↔ English (DE↔EN, Cettolo et al., 2017): training size 206,112. • KFTT Japanese ↔ English (JA↔EN, Neubig, 2011): training size of 329,882. • WMT 2016 Romanian ↔ English (RO↔EN, Bojar et al., 2016): training size 612,422, diacritics removed (following Sennrich et al., 2016b). 1509 method softmax 1.5-entmax sparsemax DE  EN EN  DE JA  EN EN  JA RO  EN EN  RO 25.70 ± 0.15 26.17 ± 0.13 24.69 ± 0.22 21.86 ± 0.09 22.42 ± 0.08 20.82 ± 0.19 20.22 ± 0.08 20.55 ± 0.30 18.54 ± 0.11 25.21 ± 0.29 26.00 ± 0.31 23.84 ± 0.37 29.12 ± 0.18 30.15 ± 0.06 29.20 ± 0.16 28.12 ± 0.18 28.84 ± 0.10 28.03 ± 0.16 Table 2: Machine translation comparison of softmax, sparsemax, and the proposed 1.5-entmax as both attention mapping and loss function. Reported is tokenized test BLEU averaged across three runs (higher is better). Training. We use byte pair encoding (BPE; Sennrich et al.,"
P19-1146,D15-1166,0,0.109309,"exact algorithm for the case of 1.5-entmax, achieving processing speed close to softmax on the GPU, even with large vocabulary sizes. For arbitrary α, we investigate a GPU-friendly approximate algorithm.1 We experiment on two tasks: one character-level with little ambiguity (morphological inflection generation) and another word-level, with more ambiguity (neural machine translation). The results show clear benefits of our approach, both in terms of accuracy and interpretability. 2 Background The underlying architecture we focus on is an RNNbased seq2seq with global attention and inputfeeding (Luong et al., 2015). We provide a brief description of this architecture, with an emphasis on the attention mapping and the loss function. Notation. Scalars, vectors, and matrices are denoted respectively as a, a, and A. We denote the d– probability simplex (the set of vectors representing probability distributions over d choices) by △d := {p ∈ Rd : p ≥ 0, kpk1 = 1}. We denote the positive part as [a]+ := max{a, 0}, and by [a]+ its elementwise application to vectors. We denote the indicator vector ey := [0, . . . , 0, |{z} 1 , 0, . . . , 0]. y Encoder. Given an input sequence of tokens x := [x1 , . . . , xJ ], t"
P19-1146,K18-3001,0,0.0627941,"Missing"
P19-1146,P18-2059,1,0.845412,"Missing"
P19-1146,D18-1473,0,0.0451831,"rsity in the attention and in the output have different, but related, motivations. Sparse attention can be justified as a form of inductive bias, since for tasks such as machine translation one expects only a few source words to be relevant for each translated word. Dense attention probabilities are particularly harmful for long sequences, as shown by Luong et al. (2015), who propose “local attention” to mitigate this problem. Combining sparse attention with fertility constraints has been recently proposed by Malaviya et al. (2018). Hard attention (Xu et al., 2015; Aharoni and Goldberg, 2017; Wu et al., 2018) selects exactly one source token. Its discrete, non-differentiable nature requires imitation learning or Monte Carlo policy gradient approximations, which drastically complicate training. In contrast, entmax is a differentiable, easy to use, drop-in softmax replacement. A recent study by Jain and Wallace (2019) tackles the limitations of attention probabilities to provide interpretability. They only study dense attention in classification tasks, where attention is less crucial for the final predictions. In their conclusions, the authors defer Losses for seq2seq models. Mostly motivated by the"
P19-1146,P02-1040,0,0.115096,"ss three runs (higher is better). Training. We use byte pair encoding (BPE; Sennrich et al., 2016a) to ensure an open vocabulary. We use separate segmentations with 25k merge operations per language for RO↔EN and a joint segmentation with 32k merges for the other language pairs. DE↔EN is validated once every 5k steps because of its smaller size, while the other sets are validated once every 10k steps. We set the maximum number of training steps at 120k for RO↔ EN and 100k for other language pairs. We use 500 dimensions for word vectors and hidden states. Evaluation. Table 2 shows BLEU scores (Papineni et al., 2002) for the three models with α ∈ {1, 1.5, 2}, using the same value of α for the attention mechanism and loss function. We observe that the 1.5-entmax configuration consistently performs best across all six choices of language pair and direction. These results support the notion that the optimal function is somewhere between softmax and sparsemax, which motivates a more fine-grained search for α; we explore this next. Fine-grained impact of α. Algorithm 1 allows us to further investigate the marginal effect of varying the attention α and the loss α, while keeping the other fixed. We report DEEN"
P19-1146,W17-5403,1,0.90343,"Missing"
P19-1146,P16-1162,0,0.0692588,"al seq2seq model that can offer optimality guarantees. 4.2 Machine Translation We now turn to a highly different seq2seq regime in which the vocabulary size is much larger, there is a great deal of ambiguity, and sequences can generally be translated in several ways. We train models for three language pairs in both directions: • IWSLT 2017 German ↔ English (DE↔EN, Cettolo et al., 2017): training size 206,112. • KFTT Japanese ↔ English (JA↔EN, Neubig, 2011): training size of 329,882. • WMT 2016 Romanian ↔ English (RO↔EN, Bojar et al., 2016): training size 612,422, diacritics removed (following Sennrich et al., 2016b). 1509 method softmax 1.5-entmax sparsemax DE  EN EN  DE JA  EN EN  JA RO  EN EN  RO 25.70 ± 0.15 26.17 ± 0.13 24.69 ± 0.22 21.86 ± 0.09 22.42 ± 0.08 20.82 ± 0.19 20.22 ± 0.08 20.55 ± 0.30 18.54 ± 0.11 25.21 ± 0.29 26.00 ± 0.31 23.84 ± 0.37 29.12 ± 0.18 30.15 ± 0.06 29.20 ± 0.16 28.12 ± 0.18 28.84 ± 0.10 28.03 ± 0.16 Table 2: Machine translation comparison of softmax, sparsemax, and the proposed 1.5-entmax as both attention mapping and loss function. Reported is tokenized test BLEU averaged across three runs (higher is better). Training. We use byte pair encoding (BPE; Sennrich et al.,"
P19-1292,E14-2007,0,0.133032,"Missing"
P19-1292,P17-4012,0,0.146998,"Missing"
P19-1292,W17-4772,0,0.15275,"Missing"
P19-1292,W19-5413,1,0.78077,"Missing"
P19-1292,L18-1004,0,0.204873,") system. APE is particularly appealing for rapidly customizing MT, avoiding to train new systems from scratch. Interfaces where human translators can post-edit and improve the quality of MT sentences (Alabau et al., 2014; Federico et al., 2014; Denkowski, 2015; Hokamp, 2018) are a common data source for APE models, since they provide triplets of source sentences (src), machine translation outputs (mt), and human post-edits (pe). Unfortunately, human post-edits are typically scarce. Existing APE systems circumvent this by generating artificial triplets (Junczys-Dowmunt and Grundkiewicz, 2016; Negri et al., 2018). However, this requires access to a high quality MT system, similar to (or better than) the one used in the black-box MT itself. This spoils the motivation of APE as an alternative to large-scale MT training in the first place: the time to train MT systems in order to extract these artificial triplets, combined with the time to train an APE system on the resulting large dataset, may well exceed the time to train a MT system from scratch. Meanwhile, there have been many successes of transfer learning for NLP: models such as CoVe (McCann et al., 2017), ELMo (Peters et al., 2018), OpenAI GPT (Ra"
P19-1292,N19-1423,0,0.568027,"ilar to (or better than) the one used in the black-box MT itself. This spoils the motivation of APE as an alternative to large-scale MT training in the first place: the time to train MT systems in order to extract these artificial triplets, combined with the time to train an APE system on the resulting large dataset, may well exceed the time to train a MT system from scratch. Meanwhile, there have been many successes of transfer learning for NLP: models such as CoVe (McCann et al., 2017), ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018), ULMFiT (Howard and Ruder, 2018), and BERT (Devlin et al., 2019) obtain powerful representations by training large-scale language models and use them to improve performance in many sentence-level and word-level tasks. However, a language generation task such as APE presents additional challenges. In this paper, we build upon the successes above and show that transfer learning is an effective and time-efficient strategy for APE, using a pretrained BERT model. This is an appealing strategy in practice: while large language models like BERT are expensive to train, this step is only done once and covers many languages, reducing engineering efforts substantiall"
P19-1292,C14-2028,0,0.045916,"tences for 3 hours on a single GPU we obtain results that are competitive with systems that were trained on 5M artificial sentences. When we add this artificial data, our method obtains state-of-the-art results. 1 Introduction The goal of automatic post-editing (APE; Simard et al., 2007) is to automatically correct the mistakes produced by a black-box machine translation (MT) system. APE is particularly appealing for rapidly customizing MT, avoiding to train new systems from scratch. Interfaces where human translators can post-edit and improve the quality of MT sentences (Alabau et al., 2014; Federico et al., 2014; Denkowski, 2015; Hokamp, 2018) are a common data source for APE models, since they provide triplets of source sentences (src), machine translation outputs (mt), and human post-edits (pe). Unfortunately, human post-edits are typically scarce. Existing APE systems circumvent this by generating artificial triplets (Junczys-Dowmunt and Grundkiewicz, 2016; Negri et al., 2018). However, this requires access to a high quality MT system, similar to (or better than) the one used in the black-box MT itself. This spoils the motivation of APE as an alternative to large-scale MT training in the first pla"
P19-1292,P18-1031,0,0.131896,"ss to a high quality MT system, similar to (or better than) the one used in the black-box MT itself. This spoils the motivation of APE as an alternative to large-scale MT training in the first place: the time to train MT systems in order to extract these artificial triplets, combined with the time to train an APE system on the resulting large dataset, may well exceed the time to train a MT system from scratch. Meanwhile, there have been many successes of transfer learning for NLP: models such as CoVe (McCann et al., 2017), ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018), ULMFiT (Howard and Ruder, 2018), and BERT (Devlin et al., 2019) obtain powerful representations by training large-scale language models and use them to improve performance in many sentence-level and word-level tasks. However, a language generation task such as APE presents additional challenges. In this paper, we build upon the successes above and show that transfer learning is an effective and time-efficient strategy for APE, using a pretrained BERT model. This is an appealing strategy in practice: while large language models like BERT are expensive to train, this step is only done once and covers many languages, reducing"
P19-1292,W16-2378,0,0.455447,"d by a black-box machine translation (MT) system. APE is particularly appealing for rapidly customizing MT, avoiding to train new systems from scratch. Interfaces where human translators can post-edit and improve the quality of MT sentences (Alabau et al., 2014; Federico et al., 2014; Denkowski, 2015; Hokamp, 2018) are a common data source for APE models, since they provide triplets of source sentences (src), machine translation outputs (mt), and human post-edits (pe). Unfortunately, human post-edits are typically scarce. Existing APE systems circumvent this by generating artificial triplets (Junczys-Dowmunt and Grundkiewicz, 2016; Negri et al., 2018). However, this requires access to a high quality MT system, similar to (or better than) the one used in the black-box MT itself. This spoils the motivation of APE as an alternative to large-scale MT training in the first place: the time to train MT systems in order to extract these artificial triplets, combined with the time to train an APE system on the resulting large dataset, may well exceed the time to train a MT system from scratch. Meanwhile, there have been many successes of transfer learning for NLP: models such as CoVe (McCann et al., 2017), ELMo (Peters et al.,"
P19-1292,W18-6467,0,0.613239,"ittle data available (e.g WMT 2018 APE shared task has 23K triplets), most research has focused on creating artificial triplets to achieve the scale that is needed for powerful sequence-to-sequence models to outperform the MT baseline, either from “round-trip” translations (Junczys-Dowmunt and Grundkiewicz, 2016) or starting from parallel data, as in the eSCAPE corpus of Negri et al. (2018), which contains 8M synthetic triplets. Dual-Source Transformer. The current state of the art in APE uses a Transformer (Vaswani et al., 2017) with two encoders, for the src and mt, and one decoder, for pe (Junczys-Dowmunt and Grundkiewicz, 2018; Tebbifakhr et al., 2018). When concatenating human post-edited data and artificial triplets, these systems greatly improve the MT baseline. However, little successes are known using the shared task training data only. By contrast, with transfer learning, our work outperforms this baseline considerably, even without any auxiliary synthetic dataset; and, as shown in §3, it achieves state-of-the-art results by combining it with the aforementioned artificial datasets. Tokens Segments Positions Context Attention Feed Forward (Multi-Head Att.) Add & Norm Add & Norm Self-Attention Self-Attention In"
P19-1292,P02-1040,0,0.109052,"Missing"
P19-1292,N18-1202,0,0.146177,"ndkiewicz, 2016; Negri et al., 2018). However, this requires access to a high quality MT system, similar to (or better than) the one used in the black-box MT itself. This spoils the motivation of APE as an alternative to large-scale MT training in the first place: the time to train MT systems in order to extract these artificial triplets, combined with the time to train an APE system on the resulting large dataset, may well exceed the time to train a MT system from scratch. Meanwhile, there have been many successes of transfer learning for NLP: models such as CoVe (McCann et al., 2017), ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018), ULMFiT (Howard and Ruder, 2018), and BERT (Devlin et al., 2019) obtain powerful representations by training large-scale language models and use them to improve performance in many sentence-level and word-level tasks. However, a language generation task such as APE presents additional challenges. In this paper, we build upon the successes above and show that transfer learning is an effective and time-efficient strategy for APE, using a pretrained BERT model. This is an appealing strategy in practice: while large language models like BERT are expensive to tra"
P19-1292,W18-6327,0,0.0238481,"stems trained on 8M triplets. When adding the eSCAPE corpus (8M triplets), performance surpasses the state of the art in all test sets. By ensembling, we improve even further, achieving a final 17.15 TER score in test 2018 (−0.85 TER than the previous state of the art). 4 Related Work In their Dual-Source Transformer model, JunczysDowmunt and Grundkiewicz (2018) also found gains by tying together encoder parameters, and the embeddings of both encoders and decoder. Our work confirms this but shows further gains by using segment embeddings and more careful sharing and initialization strategies. Sachan and Neubig (2018) explore parameter sharing between Transformer layers. However, they focus on sharing decoder parameters in a one-to-many multilingual MT system. In our work, we share parameters between the encoder and the decoder. As stated in §3, B´erard et al. (2017) also showed improved results over the MT baseline, using exclusively the shared task data. Their system outputs edit operations that decide whether to insert, keep or delete tokens from the machine translated sentence. Instead of relying on edit operations, our approach mitigates the small amount of data with transfer learning through BERT. Ou"
P19-1292,W07-0728,0,0.578903,"ed through backtranslations, a time-consuming process often no easier than training a MT system from scratch. In this paper, we propose an alternative where we fine-tune pre-trained BERT models on both the encoder and decoder of an APE system, exploring several parameter sharing strategies. By only training on a dataset of 23K sentences for 3 hours on a single GPU we obtain results that are competitive with systems that were trained on 5M artificial sentences. When we add this artificial data, our method obtains state-of-the-art results. 1 Introduction The goal of automatic post-editing (APE; Simard et al., 2007) is to automatically correct the mistakes produced by a black-box machine translation (MT) system. APE is particularly appealing for rapidly customizing MT, avoiding to train new systems from scratch. Interfaces where human translators can post-edit and improve the quality of MT sentences (Alabau et al., 2014; Federico et al., 2014; Denkowski, 2015; Hokamp, 2018) are a common data source for APE models, since they provide triplets of source sentences (src), machine translation outputs (mt), and human post-edits (pe). Unfortunately, human post-edits are typically scarce. Existing APE systems ci"
P19-1292,2006.amta-papers.25,0,0.470519,"Missing"
P19-1292,W18-6471,0,0.377046,"hared task has 23K triplets), most research has focused on creating artificial triplets to achieve the scale that is needed for powerful sequence-to-sequence models to outperform the MT baseline, either from “round-trip” translations (Junczys-Dowmunt and Grundkiewicz, 2016) or starting from parallel data, as in the eSCAPE corpus of Negri et al. (2018), which contains 8M synthetic triplets. Dual-Source Transformer. The current state of the art in APE uses a Transformer (Vaswani et al., 2017) with two encoders, for the src and mt, and one decoder, for pe (Junczys-Dowmunt and Grundkiewicz, 2018; Tebbifakhr et al., 2018). When concatenating human post-edited data and artificial triplets, these systems greatly improve the MT baseline. However, little successes are known using the shared task training data only. By contrast, with transfer learning, our work outperforms this baseline considerably, even without any auxiliary synthetic dataset; and, as shown in §3, it achieves state-of-the-art results by combining it with the aforementioned artificial datasets. Tokens Segments Positions Context Attention Feed Forward (Multi-Head Att.) Add & Norm Add & Norm Self-Attention Self-Attention Input Embedding Output Embed"
P19-1292,W18-5446,0,0.0324227,"d improved results over the MT baseline, using exclusively the shared task data. Their system outputs edit operations that decide whether to insert, keep or delete tokens from the machine translated sentence. Instead of relying on edit operations, our approach mitigates the small amount of data with transfer learning through BERT. Our work makes use of the recent advances in transfer learning for NLP (Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019). Pre-training these large language models has largely improved the state of the art of the GLUE benchmark (Wang et al., 2018). Particularly, our work uses the BERT pre-trained model and makes use of the representations obtained not only in the encoder but also on the decoder in a language generation task. More closely related to our work, Lample and Conneau (2019) pre-trained a BERT-like language model using parallel data, which they used to initialize the encoder and decoder for supervised and unsupervised MT systems. They also used segment embeddings (along with word and position embeddings) to differentiate between a pair of sentences in different languages. However, this is only used in one of the pre-training p"
P19-1292,1983.tc-1.13,0,0.654247,"Missing"
P19-2026,C18-1139,0,0.407343,"essing sentence “Obama met Donald Trump”. The predicted types and detected mentions are contained in the Output and the entities the mentions refer to in the Entity. tity typing, EL, and coreference using a structured CRF, also with hand-engineered features. In contrast, in our model we perform multi-task learning (Caruana, 1997; Evgeniou and Pontil, 2004), using learned features. et al., 2016; Chiu and Nichols, 2016). Recently, NER systems have been achieving state-of-the-art results by using word contextual embeddings, obtained with language models (Peters et al., 2018; Devlin et al., 2018; Akbik et al., 2018). Most EL systems discard mention detection, performing only entity disambiguation of previously detected mentions. Thus, in these cases the dependency between the two tasks is ignored. EL state-of-the-art methods often correspond to local methods which use as main features a candidate entity representation, a mention representation, and a representation of the mention’s context (Sun et al., 2015; Yamada et al., 2016, 2017; Ganea and Hofmann, 2017). Recently, there has also been an increasing interest in attempting to improve EL performance by leveraging knowledge base information (Radhakrishn"
P19-2026,C18-1057,0,0.0150343,"etween the two tasks is ignored. EL state-of-the-art methods often correspond to local methods which use as main features a candidate entity representation, a mention representation, and a representation of the mention’s context (Sun et al., 2015; Yamada et al., 2016, 2017; Ganea and Hofmann, 2017). Recently, there has also been an increasing interest in attempting to improve EL performance by leveraging knowledge base information (Radhakrishnan et al., 2018) or by allying local and global features, using information about the neighbouring mentions and respective entities (Le and Titov, 2018; Cao et al., 2018; Yang et al., 2018). However, these approaches involve knowing the surrounding mentions which can be impractical in a real case because we might not have information about the following sentences. It also adds extraneous complexity that might implicate a longer time to process. 3 Model Description In this section firstly, we briefly explain the StackLSTM (Dyer et al., 2015; Lample et al., 2016), model that inspired our system. Then we will give a detailed explanation of our modifications and of how we extended it to also perform EL, as showed in the diagram of Figure 1. An example of how the"
P19-2026,Q16-1026,0,0.0301015,"ama)-PER, met] [(Obama)-PER, met] [(Obama)-PER, met, (Donald Trump)-PER] Entity Barack Obama Barack Obama Barack Obama Barack Obama Barack Obama, Donald Trump Table 2: Actions and stack states when processing sentence “Obama met Donald Trump”. The predicted types and detected mentions are contained in the Output and the entities the mentions refer to in the Entity. tity typing, EL, and coreference using a structured CRF, also with hand-engineered features. In contrast, in our model we perform multi-task learning (Caruana, 1997; Evgeniou and Pontil, 2004), using learned features. et al., 2016; Chiu and Nichols, 2016). Recently, NER systems have been achieving state-of-the-art results by using word contextual embeddings, obtained with language models (Peters et al., 2018; Devlin et al., 2018; Akbik et al., 2018). Most EL systems discard mention detection, performing only entity disambiguation of previously detected mentions. Thus, in these cases the dependency between the two tasks is ignored. EL state-of-the-art methods often correspond to local methods which use as main features a candidate entity representation, a mention representation, and a representation of the mention’s context (Sun et al., 2015; Y"
P19-2026,D18-1217,0,0.0556417,"Missing"
P19-2026,N15-1026,0,0.0955364,"Missing"
P19-2026,P05-1045,0,0.0347425,"ity a particular mention refers to, by assigning a knowledge base entity id. In this example, the knowledge base id of the entity “Leeds United A.F.C.” should be selected. • A system that jointly performs NER and EL, with competitive results in both tasks. • A empirical qualitative analysis of the advantage of doing joint learning vs using separate models and of the influence of the different components to the result obtained. 2 Related work The majority of NER systems treat the task has sequence labelling and model it using conditional random fields (CRFs) on top of hand-engineered features (Finkel et al., 2005) or bi-directional Long Short Term Memory Networks (LSTMs) (Lample 190 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 190–196 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Action Shift Reduce-PER Out Shift Shift Reduce-PER Buffer [Obama, met, Donald, Trump] [met, Donald, Trump] [met, Donald, Trump] [Donald, Trump] [Trump] [] [] Stack [] [Obama] [] [] [Donald] [Donald, Trump] [] Output [] [] [(Obama)-PER] [(Obama)-PER, met] [(Obama)-PER, met] [(Obama)-PER, met] [(Obama)-PER,"
P19-2026,N18-1202,0,0.380674,"able 2: Actions and stack states when processing sentence “Obama met Donald Trump”. The predicted types and detected mentions are contained in the Output and the entities the mentions refer to in the Entity. tity typing, EL, and coreference using a structured CRF, also with hand-engineered features. In contrast, in our model we perform multi-task learning (Caruana, 1997; Evgeniou and Pontil, 2004), using learned features. et al., 2016; Chiu and Nichols, 2016). Recently, NER systems have been achieving state-of-the-art results by using word contextual embeddings, obtained with language models (Peters et al., 2018; Devlin et al., 2018; Akbik et al., 2018). Most EL systems discard mention detection, performing only entity disambiguation of previously detected mentions. Thus, in these cases the dependency between the two tasks is ignored. EL state-of-the-art methods often correspond to local methods which use as main features a candidate entity representation, a mention representation, and a representation of the mention’s context (Sun et al., 2015; Yamada et al., 2016, 2017; Ganea and Hofmann, 2017). Recently, there has also been an increasing interest in attempting to improve EL performance by leveragi"
P19-2026,D17-1277,0,0.165438,"Missing"
P19-2026,N18-1167,0,0.0207089,"al., 2018). Most EL systems discard mention detection, performing only entity disambiguation of previously detected mentions. Thus, in these cases the dependency between the two tasks is ignored. EL state-of-the-art methods often correspond to local methods which use as main features a candidate entity representation, a mention representation, and a representation of the mention’s context (Sun et al., 2015; Yamada et al., 2016, 2017; Ganea and Hofmann, 2017). Recently, there has also been an increasing interest in attempting to improve EL performance by leveraging knowledge base information (Radhakrishnan et al., 2018) or by allying local and global features, using information about the neighbouring mentions and respective entities (Le and Titov, 2018; Cao et al., 2018; Yang et al., 2018). However, these approaches involve knowing the surrounding mentions which can be impractical in a real case because we might not have information about the following sentences. It also adds extraneous complexity that might implicate a longer time to process. 3 Model Description In this section firstly, we briefly explain the StackLSTM (Dyer et al., 2015; Lample et al., 2016), model that inspired our system. Then we will gi"
P19-2026,D11-1072,0,0.744259,"Missing"
P19-2026,K18-1050,0,0.540132,"odel, since we perform EL when a mention is detected. This model is composed by four stacks: the Stack, that contains the words that are being processed, the Output, that is filled with the completed chunks, the Action stack, which contains the previous actions performed during the processing of the current document, and the Buffer, that contains the words to be processed. For NER, in the Stack-LSTM there are three possible types of actions: Some works, as in this paper, perform endto-end EL trying to leverage the relatedness of mention detection or NER and EL, and obtained promising results. Kolitsas et al. (2018) proposed a model that performs mention detection instead of NER, not identifying the type of the detected mentions, as in our approach. Sil and Yates (2013), Luo et al. (2015), and Nguyen et al. (2016) introduced models that do joint learning of NER and EL using hand-engineered features. (Durrett and Klein, 2014) performed joint learning of en• Shift, that pops a word off the Buffer and pushes it into the Stack. It means that the last word of the Buffer is part of a named entity. 191 Mention rt Entity (ŷt EL) Type Action (ŷt NER) c0 pt at afﬁneNER Buffer LSTM ot Output LSTM st Stack LSTM at ."
P19-2026,N16-1030,0,0.0902644,"by leveraging knowledge base information (Radhakrishnan et al., 2018) or by allying local and global features, using information about the neighbouring mentions and respective entities (Le and Titov, 2018; Cao et al., 2018; Yang et al., 2018). However, these approaches involve knowing the surrounding mentions which can be impractical in a real case because we might not have information about the following sentences. It also adds extraneous complexity that might implicate a longer time to process. 3 Model Description In this section firstly, we briefly explain the StackLSTM (Dyer et al., 2015; Lample et al., 2016), model that inspired our system. Then we will give a detailed explanation of our modifications and of how we extended it to also perform EL, as showed in the diagram of Figure 1. An example of how the model processes a sentence can be viewed in Table 2. 3.1 Stack-LSTM The Stack-LSTM corresponds to an action-based system which is composed by LSTMs augmented with a stack pointer. In contrast to the most common approaches which detect the entity mentions for a whole sequence, with Stack-LSTMs the entity mentions are detected and classified on the fly. This is a fundamental property to our model,"
P19-2026,K16-1025,0,0.0636633,"). Recently, NER systems have been achieving state-of-the-art results by using word contextual embeddings, obtained with language models (Peters et al., 2018; Devlin et al., 2018; Akbik et al., 2018). Most EL systems discard mention detection, performing only entity disambiguation of previously detected mentions. Thus, in these cases the dependency between the two tasks is ignored. EL state-of-the-art methods often correspond to local methods which use as main features a candidate entity representation, a mention representation, and a representation of the mention’s context (Sun et al., 2015; Yamada et al., 2016, 2017; Ganea and Hofmann, 2017). Recently, there has also been an increasing interest in attempting to improve EL performance by leveraging knowledge base information (Radhakrishnan et al., 2018) or by allying local and global features, using information about the neighbouring mentions and respective entities (Le and Titov, 2018; Cao et al., 2018; Yang et al., 2018). However, these approaches involve knowing the surrounding mentions which can be impractical in a real case because we might not have information about the following sentences. It also adds extraneous complexity that might implica"
P19-2026,P18-1148,0,0.143094,"dependency between the two tasks is ignored. EL state-of-the-art methods often correspond to local methods which use as main features a candidate entity representation, a mention representation, and a representation of the mention’s context (Sun et al., 2015; Yamada et al., 2016, 2017; Ganea and Hofmann, 2017). Recently, there has also been an increasing interest in attempting to improve EL performance by leveraging knowledge base information (Radhakrishnan et al., 2018) or by allying local and global features, using information about the neighbouring mentions and respective entities (Le and Titov, 2018; Cao et al., 2018; Yang et al., 2018). However, these approaches involve knowing the surrounding mentions which can be impractical in a real case because we might not have information about the following sentences. It also adds extraneous complexity that might implicate a longer time to process. 3 Model Description In this section firstly, we briefly explain the StackLSTM (Dyer et al., 2015; Lample et al., 2016), model that inspired our system. Then we will give a detailed explanation of our modifications and of how we extended it to also perform EL, as showed in the diagram of Figure 1. An e"
P19-2026,Q17-1028,0,0.0529857,"on F1 Macro Micro 86.6 89.4 77.0 79.0 82.8 85.2 Test F1 Macro Micro 82.6 82.4 80.0 80.0 78.7 81.2 81.9 Table 4: End-to-end EL results on validation and test sets in AIDA/CoNLL. In our work, we used 100 dimensional word embeddings pre-trained with structured skip-gram on the Gigaword corpus (Ling et al., 2015). These were concatenated with 50 dimensional character embeddings obtained using a bi-LSTM over the sentences. In addition, we use contextual embeddings obtained using a character bi-LSTM language model by Akbik et al. (2018). The entity embeddings are 300 dimensional and were trained by Yamada et al. (2017) on Wikipedia. To get the set of candidate entities to be ranked for each mention, we use a pre-built dictionary (Pershina et al., 2015). The LSTM used to extract the sentence and mention representations, v t and m is composed by 2 hidden layers with a size of 100 and the ones used in the Stack-LSTM have 1 hidden layer of size 100. The feedforward layer used to determine the entity id has a size of 5000. The affine layer used to predict whether the mention is NIL has a size of 100. A dropout ratio of 0.3 was used throughout the model. The model was trained using the ADAM optimiser (Kingma and"
P19-2026,N15-1142,0,0.0232304,"s were obtained using strong matching settings, which requires exactly predicting the gold mention boundaries and their corresponding entity. 4.2 Table 3: NER results in CoNLL 2003 test set. System Kolitsas et al. (2018) Cao et al. (2018) Nguyen et al. (2016) Our model Training details and settings Validation F1 Macro Micro 86.6 89.4 77.0 79.0 82.8 85.2 Test F1 Macro Micro 82.6 82.4 80.0 80.0 78.7 81.2 81.9 Table 4: End-to-end EL results on validation and test sets in AIDA/CoNLL. In our work, we used 100 dimensional word embeddings pre-trained with structured skip-gram on the Gigaword corpus (Ling et al., 2015). These were concatenated with 50 dimensional character embeddings obtained using a bi-LSTM over the sentences. In addition, we use contextual embeddings obtained using a character bi-LSTM language model by Akbik et al. (2018). The entity embeddings are 300 dimensional and were trained by Yamada et al. (2017) on Wikipedia. To get the set of candidate entities to be ranked for each mention, we use a pre-built dictionary (Pershina et al., 2015). The LSTM used to extract the sentence and mention representations, v t and m is composed by 2 hidden layers with a size of 100 and the ones used in the"
P19-2026,N18-1071,0,0.0162972,"ks is ignored. EL state-of-the-art methods often correspond to local methods which use as main features a candidate entity representation, a mention representation, and a representation of the mention’s context (Sun et al., 2015; Yamada et al., 2016, 2017; Ganea and Hofmann, 2017). Recently, there has also been an increasing interest in attempting to improve EL performance by leveraging knowledge base information (Radhakrishnan et al., 2018) or by allying local and global features, using information about the neighbouring mentions and respective entities (Le and Titov, 2018; Cao et al., 2018; Yang et al., 2018). However, these approaches involve knowing the surrounding mentions which can be impractical in a real case because we might not have information about the following sentences. It also adds extraneous complexity that might implicate a longer time to process. 3 Model Description In this section firstly, we briefly explain the StackLSTM (Dyer et al., 2015; Lample et al., 2016), model that inspired our system. Then we will give a detailed explanation of our modifications and of how we extended it to also perform EL, as showed in the diagram of Figure 1. An example of how the model processes a se"
P19-2026,D15-1104,0,0.228254,"with the completed chunks, the Action stack, which contains the previous actions performed during the processing of the current document, and the Buffer, that contains the words to be processed. For NER, in the Stack-LSTM there are three possible types of actions: Some works, as in this paper, perform endto-end EL trying to leverage the relatedness of mention detection or NER and EL, and obtained promising results. Kolitsas et al. (2018) proposed a model that performs mention detection instead of NER, not identifying the type of the detected mentions, as in our approach. Sil and Yates (2013), Luo et al. (2015), and Nguyen et al. (2016) introduced models that do joint learning of NER and EL using hand-engineered features. (Durrett and Klein, 2014) performed joint learning of en• Shift, that pops a word off the Buffer and pushes it into the Stack. It means that the last word of the Buffer is part of a named entity. 191 Mention rt Entity (ŷt EL) Type Action (ŷt NER) c0 pt at afﬁneNER Buffer LSTM ot Output LSTM st Stack LSTM at ... c1 et m et Attention (αt) bt afﬁneEL ce0 cp0 at et m ce1 cp1 bi-LSTM2 Ht Action LSTM qt bi-LSTM1 Figure 1: Simplified diagram of our model. The dashed arrows only occur when"
P19-2026,Q16-1016,0,0.327044,"ks, the Action stack, which contains the previous actions performed during the processing of the current document, and the Buffer, that contains the words to be processed. For NER, in the Stack-LSTM there are three possible types of actions: Some works, as in this paper, perform endto-end EL trying to leverage the relatedness of mention detection or NER and EL, and obtained promising results. Kolitsas et al. (2018) proposed a model that performs mention detection instead of NER, not identifying the type of the detected mentions, as in our approach. Sil and Yates (2013), Luo et al. (2015), and Nguyen et al. (2016) introduced models that do joint learning of NER and EL using hand-engineered features. (Durrett and Klein, 2014) performed joint learning of en• Shift, that pops a word off the Buffer and pushes it into the Stack. It means that the last word of the Buffer is part of a named entity. 191 Mention rt Entity (ŷt EL) Type Action (ŷt NER) c0 pt at afﬁneNER Buffer LSTM ot Output LSTM st Stack LSTM at ... c1 et m et Attention (αt) bt afﬁneEL ce0 cp0 at et m ce1 cp1 bi-LSTM2 Ht Action LSTM qt bi-LSTM1 Figure 1: Simplified diagram of our model. The dashed arrows only occur when the action is Reduce. The"
P19-2049,P16-1162,0,0.0915024,"words, weighted by a softmax of the scores si−1 . • An alternative of using argmax is sampling an embedding from the softmax distribution. Also based on the work of Goyal et al. (2017), we use the Gumbel Softmax (Maddison et al., 2016; Jang et al., 2016) approximation to sample the embedding: e¯i−1 = X y exp(α(si−1 (y)) + Gy ) 0 y 0 exp(α(si−1 (y ) + Gy 0 )) e(y) P Table 1: Hyperparameters shared across models = • KFTT Japanese−English (JA−EN, Neubig (2011)). • Finally, we experiment with passing a sparsemax mix of the embeddings (Martins and Astudillo, 2016). We use byte pair encoding (BPE; (Sennrich et al., 2016)) with a joint segmentation with 32,000 merges for both language pairs. Hyperparameters used across experiments are shown in Table 1. All models were implemented in a fork of OpenNMT-py (Klein et al., 2017). We compare our model to a teacher forcing baseline, i.e. a standard transformer model, without scheduled sampling, with the hyperparameters given in Table 1. We did hyperparameter tuning by trying several different values for dropout and warmup steps, and choosing the best BLEU score on the validation set for the baseline model. With the scheduled sampling method, the teacher forcing proba"
P19-2049,D16-1137,0,0.135666,"Missing"
P19-2049,2004.iwslt-evaluation.1,0,0.149999,"Missing"
P19-2049,P17-2058,0,0.222418,"Missing"
P19-2049,P17-4012,0,0.0363805,"ax (Maddison et al., 2016; Jang et al., 2016) approximation to sample the embedding: e¯i−1 = X y exp(α(si−1 (y)) + Gy ) 0 y 0 exp(α(si−1 (y ) + Gy 0 )) e(y) P Table 1: Hyperparameters shared across models = • KFTT Japanese−English (JA−EN, Neubig (2011)). • Finally, we experiment with passing a sparsemax mix of the embeddings (Martins and Astudillo, 2016). We use byte pair encoding (BPE; (Sennrich et al., 2016)) with a joint segmentation with 32,000 merges for both language pairs. Hyperparameters used across experiments are shown in Table 1. All models were implemented in a fork of OpenNMT-py (Klein et al., 2017). We compare our model to a teacher forcing baseline, i.e. a standard transformer model, without scheduled sampling, with the hyperparameters given in Table 1. We did hyperparameter tuning by trying several different values for dropout and warmup steps, and choosing the best BLEU score on the validation set for the baseline model. With the scheduled sampling method, the teacher forcing probability continuously decreases over the course of training according to a predefined function of the training steps. Among the decay strategies proposed for scheduled sampling, we found that linear decay is"
P19-3020,2015.iwslt-papers.11,0,0.0338203,"nslated content; deciding if a translation is ready for publishing or if it requires human post-editing; and highlighting the words that need to be post-edited. While there has been tremendous progress in QE in the last years (Martins et al., 2016, 2017; Kim et al., 2017; Wang et al., 2018), the ability of researchers to reproduce state-of-the-art systems has been hampered by the fact that these are either based on complex ensemble systems, complicated architectures, or require not well-documented pretraining and fine-tuning of some components. Existing open-source frameworks such as WCE-LIG (Servan et al., 2015), QuEST++ (Specia et al., 2015), Marmot (Logacheva et al., 2016), or DeepQuest (Ive et al., 2018), while helpful, are currently behind the recent best systems in WMT QE shared tasks. To address the shortcoming ∗ • Implementation in Python using PyTorch as the deep learning framework; • Ability to train new QE models on new data; • Ability to run pre-trained QE models on data from the WMT 2018 campaign; • Easy to track and reproduce experiments via YAML configuration files and (optionally) MLflow; • Open-source license (Affero GPL). This project is hosted at https://github. com/Unbabel/OpenKiwi"
P19-3020,2006.amta-papers.25,0,0.0681082,"available at https:// github.com/Unbabel/OpenKiwi/blob/master/demo/KiwiViz.ipynb. (SRC), its machine translation (MT) and a human post-edition (PE) of the machine translation: a larger dataset of 26,273 training and 1,000 development triplets, where the MT is generated by a phrase-based statistical machine translation (SMT); and a smaller dataset of 13,442 training and 1,000 development triplets, where the MT is generated by a neural machine translation system (NMT). The data also contains word-level quality labels and sentencelevel scores that are obtained from the posteditions using TERCOM (Snover et al., 2006). If used inside another Python project, OpenKiwi can be easily used like the following: import kiwi config = 'config.yml' run_info = kiwi.train(config) After training, predicting on new data can be performed by simply calling model = kiwi.load_model( run_info.model_path ) source = [ 'the Sharpen tool sharpens ' 'areas in an image .' ] target = [ 'der Sch¨ arfen-Werkezug ' 'Bereiche in einem Bild ' 'sch¨ arfer erscheint .' ] examples = [{ 'source': source, 'target': target }] out = model.predict(examples) • A corpus of 526,368 artificially generated sentence triplets, obtained by first cross-e"
P19-3020,N13-1073,0,0.046403,"ly the architecture proposed by Kim et al. (2017), which consists of two modules: QUETCH. The “QUality Estimation from scraTCH” system (Kreutzer et al., 2015) is designed as a multilayer perceptron with one hidden layer, non-linear tanh activation functions and a lookup-table layer mapping words to continuous dense vectors. For each position in the MT, a window of fixed size surrounding that position, • a predictor, which is trained to predict each token of the target sentence given the source and 3 The alignments are provided by the shared task organizers, which are computed with fast align (Dyer et al., 2013). 118 • Each input data, like source text and MT text, is defined as a Field, which holds information about how data should be tokenized, how the inner vocabulary is built, how the mapping to IDs is done, and how a list of samples is padded into a tensor; the left and right context of the target sentence; • an estimator, which takes features produced by the predictor and uses them to classify each word as OK or BAD. Our predictor uses a bidirectional LSTM to encode the source, and two unidirectional LSTMs processing the target in left-to-right (LSTM-L2R) and right-to-left (LSTM-R2L) order. For"
P19-3020,W18-6451,1,0.872756,"Missing"
P19-3020,C18-1266,0,0.250436,"ing; and highlighting the words that need to be post-edited. While there has been tremendous progress in QE in the last years (Martins et al., 2016, 2017; Kim et al., 2017; Wang et al., 2018), the ability of researchers to reproduce state-of-the-art systems has been hampered by the fact that these are either based on complex ensemble systems, complicated architectures, or require not well-documented pretraining and fine-tuning of some components. Existing open-source frameworks such as WCE-LIG (Servan et al., 2015), QuEST++ (Specia et al., 2015), Marmot (Logacheva et al., 2016), or DeepQuest (Ive et al., 2018), while helpful, are currently behind the recent best systems in WMT QE shared tasks. To address the shortcoming ∗ • Implementation in Python using PyTorch as the deep learning framework; • Ability to train new QE models on new data; • Ability to run pre-trained QE models on data from the WMT 2018 campaign; • Easy to track and reproduce experiments via YAML configuration files and (optionally) MLflow; • Open-source license (Affero GPL). This project is hosted at https://github. com/Unbabel/OpenKiwi. We welcome and encourage contributions from the research community.2 2 Quality Estimation The g"
P19-3020,P15-4020,0,0.11406,"Missing"
P19-3020,W17-4763,0,0.4195,"roduce OpenKiwi, a PyTorch-based open source framework for translation quality estimation. OpenKiwi supports training and testing of word-level and sentence-level quality estimation systems, implementing the winning systems of the WMT 2015–18 quality estimation campaigns. We benchmark OpenKiwi on two datasets from WMT 2018 (English-German SMT and NMT), yielding state-of-the-art performance on the word-level tasks and near state-of-the-art in the sentencelevel tasks. 1 • Implementation of four QE systems: QUETCH (Kreutzer et al., 2015), N U QE (Martins et al., 2016, 2017), Predictor-Estimator (Kim et al., 2017; Wang et al., 2018), and a stacked ensemble with a linear system (Martins et al., 2016, 2017); Introduction • Easy to use API: can be imported as a package in other projects or run from the command line; Quality estimation (QE) provides the missing link between machine and human translation: its goal is to evaluate a translation system’s quality without access to reference translations (Specia et al., 2018b). Among its potential usages are: informing an end user about the reliability of automatically translated content; deciding if a translation is ready for publishing or if it requires human"
P19-3020,W18-6465,0,0.492887,"PyTorch-based open source framework for translation quality estimation. OpenKiwi supports training and testing of word-level and sentence-level quality estimation systems, implementing the winning systems of the WMT 2015–18 quality estimation campaigns. We benchmark OpenKiwi on two datasets from WMT 2018 (English-German SMT and NMT), yielding state-of-the-art performance on the word-level tasks and near state-of-the-art in the sentencelevel tasks. 1 • Implementation of four QE systems: QUETCH (Kreutzer et al., 2015), N U QE (Martins et al., 2016, 2017), Predictor-Estimator (Kim et al., 2017; Wang et al., 2018), and a stacked ensemble with a linear system (Martins et al., 2016, 2017); Introduction • Easy to use API: can be imported as a package in other projects or run from the command line; Quality estimation (QE) provides the missing link between machine and human translation: its goal is to evaluate a translation system’s quality without access to reference translations (Specia et al., 2018b). Among its potential usages are: informing an end user about the reliability of automatically translated content; deciding if a translation is ready for publishing or if it requires human post-editing; and h"
P19-3020,P17-4012,0,0.0561161,"tor model. Systems. In addition to the models that are part of OpenKiwi, in the experiments below, we also use Automatic Post-Editing (APE) adapted for QE (APE-QE). APE-QE has been used by Martins et al. (2017) as an intermediate step for quality estimation, where an APE system is trained on the human post-edits and its outputs are used as pseudo-post-editions to generate word-level quality labels and sentence-level scores in the same way that the original labels were created. Since OpenKiwi’s focus is not on implementing a sequence-to-sequence model, we used an external software, OpenNMT-py (Klein et al., 2017), to train two separate translation models: Figure 2 shows an example of QE predictions using the framework. 5 Benchmark Experiments Datasets. To benchmark OpenKiwi, we use the following datasets from the WMT 2018 quality estimation shared task, all English-German (En-De): • SRC → PE: trained first on the in-domain corpus provided, then fine-tuned on the shared task data. • Two quality estimation datasets of sentence triplets, each consisting of a source sentence 120 Model En-De SMT source r MT gaps QUETCH N U QE P RED -E ST APE-QE 39.90 50.04 57.29 55.12 17.10 35.53 43.68 47.04 36.10 42.08 33"
P19-3020,W15-3037,0,0.573302,"nting under the same framework. The main features of OpenKiwi are: Abstract We introduce OpenKiwi, a PyTorch-based open source framework for translation quality estimation. OpenKiwi supports training and testing of word-level and sentence-level quality estimation systems, implementing the winning systems of the WMT 2015–18 quality estimation campaigns. We benchmark OpenKiwi on two datasets from WMT 2018 (English-German SMT and NMT), yielding state-of-the-art performance on the word-level tasks and near state-of-the-art in the sentencelevel tasks. 1 • Implementation of four QE systems: QUETCH (Kreutzer et al., 2015), N U QE (Martins et al., 2016, 2017), Predictor-Estimator (Kim et al., 2017; Wang et al., 2018), and a stacked ensemble with a linear system (Martins et al., 2016, 2017); Introduction • Easy to use API: can be imported as a package in other projects or run from the command line; Quality estimation (QE) provides the missing link between machine and human translation: its goal is to evaluate a translation system’s quality without access to reference translations (Specia et al., 2018b). Among its potential usages are: informing an end user about the reliability of automatically translated conten"
P19-3020,L16-1582,0,0.0235335,"shing or if it requires human post-editing; and highlighting the words that need to be post-edited. While there has been tremendous progress in QE in the last years (Martins et al., 2016, 2017; Kim et al., 2017; Wang et al., 2018), the ability of researchers to reproduce state-of-the-art systems has been hampered by the fact that these are either based on complex ensemble systems, complicated architectures, or require not well-documented pretraining and fine-tuning of some components. Existing open-source frameworks such as WCE-LIG (Servan et al., 2015), QuEST++ (Specia et al., 2015), Marmot (Logacheva et al., 2016), or DeepQuest (Ive et al., 2018), while helpful, are currently behind the recent best systems in WMT QE shared tasks. To address the shortcoming ∗ • Implementation in Python using PyTorch as the deep learning framework; • Ability to train new QE models on new data; • Ability to run pre-trained QE models on data from the WMT 2018 campaign; • Easy to track and reproduce experiments via YAML configuration files and (optionally) MLflow; • Open-source license (Affero GPL). This project is hosted at https://github. com/Unbabel/OpenKiwi. We welcome and encourage contributions from the research commu"
P19-3020,W16-2387,1,\N,Missing
P19-3020,Q17-1015,1,\N,Missing
P19-4001,Q18-1005,0,0.0349012,"iz and Schulz, 2018); 1 • Deep latent-variable models of natural language (Kim et al., 2018).2 Our tutorial offers a complementary perspective in which the latent variables are structured and discrete, corresponding to linguistic structure. We will briefly discuss the modeling alternatives above in the final discussion. End-to-end differentiable approaches. Here, we directly replace the argmax by a continuous relaxation for which the exact gradient can be computed and backpropagated normally. Examples are structured attention networks and related work (Kim et al., 2017; Maillard et al., 2017; Liu and Lapata, 2018; Mensch and Blondel, 2018), which use marginal inference, or SparseMAP (Niculae et al., 2018a,b), a new inference strategy which yields a sparse set of structures. While the former is usually limited in which the downstream model can only depend on local substructures (not the entire latent structure), the latter allows combining the best of both worlds. Another line of work imbues structure into neural attention via sparsity-inducing priors (Martins and Astudillo, 2016; Niculae and Blondel, 2017; Malaviya et al., 2018). This tutorial will highlight connections among all these methods, enumer"
P19-4001,W18-2903,0,0.0194786,"ethod is best suited for their problem. Surrogate gradients. Such techniques usually involve approximating the gradient of a discrete, argmax-like mapping by the gradient of a continuous relaxation. Examples are the straight-through estimator (Bengio et al., 2013) and the structured projection of intermediate gradients optimization technique (SPIGOT; Peng et al. 2018). In stochastic graphs, surrogate gradients yield biased but lower-variance gradient estimators compared to the score function estimator. Related is the Gumbel softmax (Jang et al., 2017; Maddison et al., 2017; Choi et al., 2018; Maillard and Clark, 2018), which uses the reparametrization trick and a temperature parameter to build a continuous surrogate of the argmax operation, which one can then differentiate over. Structured versions were recently explored by Corro and Titov (2019a,b). One limitation of straight-through estimators is that backpropagating with respect to the sample-independent means may cause discrepancies between the forward and backward pass, which biases learning. 2 Type of Tutorial & Relationship to Recent Tutorials The proposed tutorial mixes the introductory and cutting-edge types. It will offer a gentle introduction to"
P19-4001,P18-2059,1,0.819103,"tention networks and related work (Kim et al., 2017; Maillard et al., 2017; Liu and Lapata, 2018; Mensch and Blondel, 2018), which use marginal inference, or SparseMAP (Niculae et al., 2018a,b), a new inference strategy which yields a sparse set of structures. While the former is usually limited in which the downstream model can only depend on local substructures (not the entire latent structure), the latter allows combining the best of both worlds. Another line of work imbues structure into neural attention via sparsity-inducing priors (Martins and Astudillo, 2016; Niculae and Blondel, 2017; Malaviya et al., 2018). This tutorial will highlight connections among all these methods, enumerating their strengths and weaknesses. The models we present and analyze have been applied to a wide variety of NLP tasks, including sentiment analysis, natural language inference, language modeling, machine translation, and semantic parsing. In addition, evaluations specific to latent structure recovery have been pro3 Outline Below we sketch an outline of the tutorial, which will take three hours, separated by a 30-minutes coffee break. 1. Introduction (30 min) • Why latent variables? • Motivation and examples of latent"
P19-4001,P18-5003,0,0.0253748,"s that backpropagating with respect to the sample-independent means may cause discrepancies between the forward and backward pass, which biases learning. 2 Type of Tutorial & Relationship to Recent Tutorials The proposed tutorial mixes the introductory and cutting-edge types. It will offer a gentle introduction to recent advances in structured modeling with discrete latent variables, which were not previously covered in any ACL/EMNLP/IJCNLP/NAACL related tutorial. The closest related topics covered in recent tutorials at NLP conferences are: • Variational inference and deep generative models (Aziz and Schulz, 2018); 1 • Deep latent-variable models of natural language (Kim et al., 2018).2 Our tutorial offers a complementary perspective in which the latent variables are structured and discrete, corresponding to linguistic structure. We will briefly discuss the modeling alternatives above in the final discussion. End-to-end differentiable approaches. Here, we directly replace the argmax by a continuous relaxation for which the exact gradient can be computed and backpropagated normally. Examples are structured attention networks and related work (Kim et al., 2017; Maillard et al., 2017; Liu and Lapata, 2018"
P19-4001,N18-4013,1,0.76483,"ama et al., 2017); similar to maximizing an expected reward in reinforcement learning with discrete actions. Estimated stochastic gradients are typically obtained with a combination 1 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, pages 1–5 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics of Monte Carlo sampling and the score function estimator (a.k.a. REINFORCE, Williams, 1992). Such estimators often suffer from instability and high variance, requiring care (Havrylov et al., 2019). posed (Nangia and Bowman, 2018; Williams et al., 2018). Examples and evaluation will be covered throughout the tutorial. After attending the tutorial, a practitioner will be better informed about which method is best suited for their problem. Surrogate gradients. Such techniques usually involve approximating the gradient of a discrete, argmax-like mapping by the gradient of a continuous relaxation. Examples are the straight-through estimator (Bengio et al., 2013) and the structured projection of intermediate gradients optimization technique (SPIGOT; Peng et al. 2018). In stochastic graphs, surrogate gradients yield biased"
P19-4001,P19-1551,0,0.0143726,"imator (Bengio et al., 2013) and the structured projection of intermediate gradients optimization technique (SPIGOT; Peng et al. 2018). In stochastic graphs, surrogate gradients yield biased but lower-variance gradient estimators compared to the score function estimator. Related is the Gumbel softmax (Jang et al., 2017; Maddison et al., 2017; Choi et al., 2018; Maillard and Clark, 2018), which uses the reparametrization trick and a temperature parameter to build a continuous surrogate of the argmax operation, which one can then differentiate over. Structured versions were recently explored by Corro and Titov (2019a,b). One limitation of straight-through estimators is that backpropagating with respect to the sample-independent means may cause discrepancies between the forward and backward pass, which biases learning. 2 Type of Tutorial & Relationship to Recent Tutorials The proposed tutorial mixes the introductory and cutting-edge types. It will offer a gentle introduction to recent advances in structured modeling with discrete latent variables, which were not previously covered in any ACL/EMNLP/IJCNLP/NAACL related tutorial. The closest related topics covered in recent tutorials at NLP conferences are:"
P19-4001,N19-1423,0,0.0110866,"nbabel.com, tsvetomila.mihaylova@gmail.com, nikitanangia@nyu.edu, vlad@vene.ro Link to materials: https://deep-spin.github.io/tutorial/ 1 in an unsupervised or semi-supervised fashion, from the signal of higher-level downstream tasks like sentiment analysis or machine translation. This avoids the need for preprocessing data with offthe-shelf tools (e.g., parsers, word aligners) and engineering features based on their outputs; and it is an alternative to techniques based on parameter sharing, transfer learning, multi-task learning, or scaffolding (Swayamdipta et al., 2018; Peters et al., 2018; Devlin et al., 2019; Strubell et al., 2018), as well as techniques that incorporate structural bias directly in model design (Dyer et al., 2016; Shen et al., 2019). The proposed tutorial is about such discrete latent structure models. We discuss their motivation, potential, and limitations, then explore in detail three strategies for designing such models: Description Latent structure models are a powerful tool for modeling compositional data, discovering linguistic structure, and building NLP pipelines (Smith, 2011). Words, sentences, paragraphs, and documents represent the fundamental units in NLP, and their d"
P19-4001,D18-1108,1,0.766159,"2 Our tutorial offers a complementary perspective in which the latent variables are structured and discrete, corresponding to linguistic structure. We will briefly discuss the modeling alternatives above in the final discussion. End-to-end differentiable approaches. Here, we directly replace the argmax by a continuous relaxation for which the exact gradient can be computed and backpropagated normally. Examples are structured attention networks and related work (Kim et al., 2017; Maillard et al., 2017; Liu and Lapata, 2018; Mensch and Blondel, 2018), which use marginal inference, or SparseMAP (Niculae et al., 2018a,b), a new inference strategy which yields a sparse set of structures. While the former is usually limited in which the downstream model can only depend on local substructures (not the entire latent structure), the latter allows combining the best of both worlds. Another line of work imbues structure into neural attention via sparsity-inducing priors (Martins and Astudillo, 2016; Niculae and Blondel, 2017; Malaviya et al., 2018). This tutorial will highlight connections among all these methods, enumerating their strengths and weaknesses. The models we present and analyze have been applied to"
P19-4001,N16-1024,0,0.032824,"tutorial/ 1 in an unsupervised or semi-supervised fashion, from the signal of higher-level downstream tasks like sentiment analysis or machine translation. This avoids the need for preprocessing data with offthe-shelf tools (e.g., parsers, word aligners) and engineering features based on their outputs; and it is an alternative to techniques based on parameter sharing, transfer learning, multi-task learning, or scaffolding (Swayamdipta et al., 2018; Peters et al., 2018; Devlin et al., 2019; Strubell et al., 2018), as well as techniques that incorporate structural bias directly in model design (Dyer et al., 2016; Shen et al., 2019). The proposed tutorial is about such discrete latent structure models. We discuss their motivation, potential, and limitations, then explore in detail three strategies for designing such models: Description Latent structure models are a powerful tool for modeling compositional data, discovering linguistic structure, and building NLP pipelines (Smith, 2011). Words, sentences, paragraphs, and documents represent the fundamental units in NLP, and their discrete, compositional nature is well suited to combinatorial representations such as trees, sequences, segments, or alignme"
P19-4001,P18-1173,0,0.0274553,"ce, requiring care (Havrylov et al., 2019). posed (Nangia and Bowman, 2018; Williams et al., 2018). Examples and evaluation will be covered throughout the tutorial. After attending the tutorial, a practitioner will be better informed about which method is best suited for their problem. Surrogate gradients. Such techniques usually involve approximating the gradient of a discrete, argmax-like mapping by the gradient of a continuous relaxation. Examples are the straight-through estimator (Bengio et al., 2013) and the structured projection of intermediate gradients optimization technique (SPIGOT; Peng et al. 2018). In stochastic graphs, surrogate gradients yield biased but lower-variance gradient estimators compared to the score function estimator. Related is the Gumbel softmax (Jang et al., 2017; Maddison et al., 2017; Choi et al., 2018; Maillard and Clark, 2018), which uses the reparametrization trick and a temperature parameter to build a continuous surrogate of the argmax operation, which one can then differentiate over. Structured versions were recently explored by Corro and Titov (2019a,b). One limitation of straight-through estimators is that backpropagating with respect to the sample-independen"
P19-4001,N19-1115,0,0.0129472,"oss on a downstream task (Yogatama et al., 2017); similar to maximizing an expected reward in reinforcement learning with discrete actions. Estimated stochastic gradients are typically obtained with a combination 1 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, pages 1–5 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics of Monte Carlo sampling and the score function estimator (a.k.a. REINFORCE, Williams, 1992). Such estimators often suffer from instability and high variance, requiring care (Havrylov et al., 2019). posed (Nangia and Bowman, 2018; Williams et al., 2018). Examples and evaluation will be covered throughout the tutorial. After attending the tutorial, a practitioner will be better informed about which method is best suited for their problem. Surrogate gradients. Such techniques usually involve approximating the gradient of a discrete, argmax-like mapping by the gradient of a continuous relaxation. Examples are the straight-through estimator (Bengio et al., 2013) and the structured projection of intermediate gradients optimization technique (SPIGOT; Peng et al. 2018). In stochastic graphs, s"
P19-4001,N18-1202,0,0.0352364,"tugal andre.martins@unbabel.com, tsvetomila.mihaylova@gmail.com, nikitanangia@nyu.edu, vlad@vene.ro Link to materials: https://deep-spin.github.io/tutorial/ 1 in an unsupervised or semi-supervised fashion, from the signal of higher-level downstream tasks like sentiment analysis or machine translation. This avoids the need for preprocessing data with offthe-shelf tools (e.g., parsers, word aligners) and engineering features based on their outputs; and it is an alternative to techniques based on parameter sharing, transfer learning, multi-task learning, or scaffolding (Swayamdipta et al., 2018; Peters et al., 2018; Devlin et al., 2019; Strubell et al., 2018), as well as techniques that incorporate structural bias directly in model design (Dyer et al., 2016; Shen et al., 2019). The proposed tutorial is about such discrete latent structure models. We discuss their motivation, potential, and limitations, then explore in detail three strategies for designing such models: Description Latent structure models are a powerful tool for modeling compositional data, discovering linguistic structure, and building NLP pipelines (Smith, 2011). Words, sentences, paragraphs, and documents represent the fundamental unit"
P19-4001,D18-1548,0,0.0137878,"a.mihaylova@gmail.com, nikitanangia@nyu.edu, vlad@vene.ro Link to materials: https://deep-spin.github.io/tutorial/ 1 in an unsupervised or semi-supervised fashion, from the signal of higher-level downstream tasks like sentiment analysis or machine translation. This avoids the need for preprocessing data with offthe-shelf tools (e.g., parsers, word aligners) and engineering features based on their outputs; and it is an alternative to techniques based on parameter sharing, transfer learning, multi-task learning, or scaffolding (Swayamdipta et al., 2018; Peters et al., 2018; Devlin et al., 2019; Strubell et al., 2018), as well as techniques that incorporate structural bias directly in model design (Dyer et al., 2016; Shen et al., 2019). The proposed tutorial is about such discrete latent structure models. We discuss their motivation, potential, and limitations, then explore in detail three strategies for designing such models: Description Latent structure models are a powerful tool for modeling compositional data, discovering linguistic structure, and building NLP pipelines (Smith, 2011). Words, sentences, paragraphs, and documents represent the fundamental units in NLP, and their discrete, compositional n"
P19-4001,D18-1412,0,0.0282224,"US ã Unbabel, Lisbon, Portugal andre.martins@unbabel.com, tsvetomila.mihaylova@gmail.com, nikitanangia@nyu.edu, vlad@vene.ro Link to materials: https://deep-spin.github.io/tutorial/ 1 in an unsupervised or semi-supervised fashion, from the signal of higher-level downstream tasks like sentiment analysis or machine translation. This avoids the need for preprocessing data with offthe-shelf tools (e.g., parsers, word aligners) and engineering features based on their outputs; and it is an alternative to techniques based on parameter sharing, transfer learning, multi-task learning, or scaffolding (Swayamdipta et al., 2018; Peters et al., 2018; Devlin et al., 2019; Strubell et al., 2018), as well as techniques that incorporate structural bias directly in model design (Dyer et al., 2016; Shen et al., 2019). The proposed tutorial is about such discrete latent structure models. We discuss their motivation, potential, and limitations, then explore in detail three strategies for designing such models: Description Latent structure models are a powerful tool for modeling compositional data, discovering linguistic structure, and building NLP pipelines (Smith, 2011). Words, sentences, paragraphs, and documents represent"
P19-4001,Q18-1019,0,0.0198631,"r to maximizing an expected reward in reinforcement learning with discrete actions. Estimated stochastic gradients are typically obtained with a combination 1 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, pages 1–5 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics of Monte Carlo sampling and the score function estimator (a.k.a. REINFORCE, Williams, 1992). Such estimators often suffer from instability and high variance, requiring care (Havrylov et al., 2019). posed (Nangia and Bowman, 2018; Williams et al., 2018). Examples and evaluation will be covered throughout the tutorial. After attending the tutorial, a practitioner will be better informed about which method is best suited for their problem. Surrogate gradients. Such techniques usually involve approximating the gradient of a discrete, argmax-like mapping by the gradient of a continuous relaxation. Examples are the straight-through estimator (Bengio et al., 2013) and the structured projection of intermediate gradients optimization technique (SPIGOT; Peng et al. 2018). In stochastic graphs, surrogate gradients yield biased but lower-variance gradi"
Q17-1015,W13-3520,0,0.0303314,"Missing"
Q17-1015,W08-0330,0,0.0181167,"tem and the TER-tuned APE ensemble are much weaker in terms of F1MULT . This is less surprising in the case of the full ensemble, as it has been tuned towards TER for the APE task specifically. However, we can obtain even better APEbased QE systems for both shared task settings by tuning the full APE ensembles towards F1MULT , the official WMT16 QE metric, and towards F1BAD for WMT15.12 With this approach, we produce our new best stand-alone QE-systems for both shared tasks, which we denote as A PE QE. 11 Note that this system resembles other QE approaches which use pseudo-reference features (Albrecht and Hwa, 2008; Soricut and Narsale, 2012; Shah et al., 2013), since the s → p is essentially an “alternative” MT system. 12 Using again MERT and executing 7 iterations on the official development set with an n-best list size of 12. F1BAD dev F1BAD test Best system in WMT15 43.1 43.12 L INEAR QE N EURAL QE S TACKED QE A PE QE F ULL S TACKED QE 43.68 43.51 44.68 46.44 47.61 42.50 43.35 43.70 46.05 47.08 F1MULT dev F1MULT test Best system in WMT16 49.25 49.52 L INEAR QE N EURAL QE S TACKED QE A PE QE F ULL S TACKED QE 46.11 46.80 49.16 54.95 56.80 46.16 47.29 50.27 55.68 57.47 Table 10: Performance of the sev"
Q17-1015,W12-3108,0,0.0237159,"potential to dramatically reduce post-editing times and to save labor costs (Specia, 2011). The increasing interest in this problem from an industrial angle comes as no surprise (Turchi et al., 2014; de Souza et al., 2015; Martins et al., 2016; Kozlova et al., 2016). In this paper, we tackle word-level QE, whose goal is to assign a label of OK or BAD to each word in the translation (Figure 1). Past approaches to this problem include linear classifiers with handcrafted features (Ueffing and Ney, 2007; Bic¸ici, 2013; Shah et al., 2013; Luong et al., 2014), often combined with feature selection (Avramidis, 2012; Beck et al., 2013), recurrent neural networks (de Souza et al., 2014; Kim and Lee, 2016), and systems that combine linear and neural models (Kreutzer et al., 2015; Martins et al., 2016). We start by proposing a “pure” QE system (§3) consisting of a new, carefully engineered neural model (N EURAL QE), stacked into a linear feature-rich classifier (L INEAR QE). Along the way, we provide a rigorous empirical analysis to better understand the contribution of the several groups of features and to justify the architecture of the neural system. A second contribution of this paper is bringing in the"
Q17-1015,W13-2241,0,0.0141301,"atically reduce post-editing times and to save labor costs (Specia, 2011). The increasing interest in this problem from an industrial angle comes as no surprise (Turchi et al., 2014; de Souza et al., 2015; Martins et al., 2016; Kozlova et al., 2016). In this paper, we tackle word-level QE, whose goal is to assign a label of OK or BAD to each word in the translation (Figure 1). Past approaches to this problem include linear classifiers with handcrafted features (Ueffing and Ney, 2007; Bic¸ici, 2013; Shah et al., 2013; Luong et al., 2014), often combined with feature selection (Avramidis, 2012; Beck et al., 2013), recurrent neural networks (de Souza et al., 2014; Kim and Lee, 2016), and systems that combine linear and neural models (Kreutzer et al., 2015; Martins et al., 2016). We start by proposing a “pure” QE system (§3) consisting of a new, carefully engineered neural model (N EURAL QE), stacked into a linear feature-rich classifier (L INEAR QE). Along the way, we provide a rigorous empirical analysis to better understand the contribution of the several groups of features and to justify the architecture of the neural system. A second contribution of this paper is bringing in the related task of aut"
Q17-1015,W15-3035,0,0.0424635,"Missing"
Q17-1015,W13-2242,0,0.0744382,"Missing"
Q17-1015,C04-1046,0,0.0567254,"Missing"
Q17-1015,W14-3340,0,0.0500303,"Missing"
Q17-1015,2015.mtsummit-users.5,0,0.0860722,"Missing"
Q17-1015,W15-3036,0,0.100031,"Missing"
Q17-1015,P15-1174,0,0.0310394,"word-level QE task); see Figure 1. Besides these datasets, for training the APE system we make use of artificial roundtrip translations; this will be detailed in §4. Evaluation. For all experiments, we report the official evaluation metrics of each dataset’s year. For WMT15, the official metric for the word-level QE task is the F1 score of the BAD labels (F1BAD ). For WMT16, it is the product of the F1 scores for the OK and BAD labels (denoted F1MULT ). For sentencelevel QE, we report the Pearson’s r correlation for HTER prediction and the Spearman’s ρ correlation score for sentence ranking (Graham, 2015). From post-edited sentences to quality labels. In the datasets above, the word quality labels are obtained automatically by aligning the translated and the post-edited sentence with the TERCOM software tool (Snover et al., 2006)2 , with the default settings (tokenized, case insensitive, exact matching only, shifts disabled). This tool computes the HTER (the normalized edit distance) between the translated and post-edited sentence. As a by-product, it aligns the words in the two sentences, identifying substitution errors, word deletions (i.e. words omitted by the translation system), and inser"
Q17-1015,W15-3014,0,0.0133895,"s predictions to avoid overfitting the training set. This is done by splitting the training set in K folds (we set K = 10) and training K different instances of the first system, where each instance is trained on K − 1 folds and makes predictions for the left-out fold. The concatenation of all the predictions yields an unbiased training set for the second classifier. Neural intra-ensembles. We also evaluate the performance of intra-ensembled neural systems. We train independent instances of N EURAL QE with different random initializations and different data shuffles, following the approach of Jean et al. (2015) in neural MT. In Tables 5–6, we report the performance on the WMT15 and WMT16 datasets of systems ensembling 5 and 15 of these instances, called respectively N EURAL QE-5 and N EURAL QE-15. The in210 F1BAD dev F1BAD test Best system in WMT15 QUETCH+ (2nd best) 43.1– 43.12 43.05 L INEAR QE N EURAL QE N EURAL QE-5 N EURAL QE-15 S TACKED QE 43.68 43.51 44.21 44.11 44.68 42.50 43.35 43.54 43.93 43.70 Table 5: Performance of the pure QE systems on the WMT15 datasets. The best performing system in the WMT15 competition was by Espl`a-Gomis et al. (2015), followed by Kreutzer et al. (2015)’s QUETCH+,"
Q17-1015,W16-2378,1,0.791496,"improvement in the development set, but a degradation in the test set. In WMT16, however, stacking is clearly beneficial, with a boost of about 2 points over the best intraensembled neural system and 3–4 points above the linear system, both in the development and test partitions. For the remainder of this paper, we will take S TACKED QE as our pure QE system. 4 APE-Based Quality Estimation Now that we have described a pure QE system, we move on to an APE-based QE system (A PE QE). Our starting point is the system submitted by the Adam Mickiewicz University (AMU) team to the APE task of WMT16 (Junczys-Dowmunt and Grundkiewicz, 2016). They explored the application of neural translation models to the APE problem and achieved good results by treating different models as components in a log-linear model, allowing for multiple inputs (the source s and the translated sentence t) that were decoded to the same target language (post-edited translation p). Two systems were considered, one using s as the input (s → p) and another using t as the input (t → p). A simple string-matching penalty integrated within the loglinear model was used to control for higher faithfulness with regard to the raw MT output. The penalty fires if the A"
Q17-1015,W16-2384,0,0.130055,"11). The increasing interest in this problem from an industrial angle comes as no surprise (Turchi et al., 2014; de Souza et al., 2015; Martins et al., 2016; Kozlova et al., 2016). In this paper, we tackle word-level QE, whose goal is to assign a label of OK or BAD to each word in the translation (Figure 1). Past approaches to this problem include linear classifiers with handcrafted features (Ueffing and Ney, 2007; Bic¸ici, 2013; Shah et al., 2013; Luong et al., 2014), often combined with feature selection (Avramidis, 2012; Beck et al., 2013), recurrent neural networks (de Souza et al., 2014; Kim and Lee, 2016), and systems that combine linear and neural models (Kreutzer et al., 2015; Martins et al., 2016). We start by proposing a “pure” QE system (§3) consisting of a new, carefully engineered neural model (N EURAL QE), stacked into a linear feature-rich classifier (L INEAR QE). Along the way, we provide a rigorous empirical analysis to better understand the contribution of the several groups of features and to justify the architecture of the neural system. A second contribution of this paper is bringing in the related task of automatic post-editing (APE; Simard et al. (2007)), which aims to au205 T"
Q17-1015,W16-2385,0,0.177565,"al., 2013). This has many potential usages: informing an end user about the reliability of translated content; deciding if a translation is ready for publishing or if it requires human post-editing; highlighting the words that need to be changed. QE systems are particularly appealing for crowd-sourced and professional translation services, due to their potential to dramatically reduce post-editing times and to save labor costs (Specia, 2011). The increasing interest in this problem from an industrial angle comes as no surprise (Turchi et al., 2014; de Souza et al., 2015; Martins et al., 2016; Kozlova et al., 2016). In this paper, we tackle word-level QE, whose goal is to assign a label of OK or BAD to each word in the translation (Figure 1). Past approaches to this problem include linear classifiers with handcrafted features (Ueffing and Ney, 2007; Bic¸ici, 2013; Shah et al., 2013; Luong et al., 2014), often combined with feature selection (Avramidis, 2012; Beck et al., 2013), recurrent neural networks (de Souza et al., 2014; Kim and Lee, 2016), and systems that combine linear and neural models (Kreutzer et al., 2015; Martins et al., 2016). We start by proposing a “pure” QE system (§3) consisting of a"
Q17-1015,W15-3037,0,0.640642,"omes as no surprise (Turchi et al., 2014; de Souza et al., 2015; Martins et al., 2016; Kozlova et al., 2016). In this paper, we tackle word-level QE, whose goal is to assign a label of OK or BAD to each word in the translation (Figure 1). Past approaches to this problem include linear classifiers with handcrafted features (Ueffing and Ney, 2007; Bic¸ici, 2013; Shah et al., 2013; Luong et al., 2014), often combined with feature selection (Avramidis, 2012; Beck et al., 2013), recurrent neural networks (de Souza et al., 2014; Kim and Lee, 2016), and systems that combine linear and neural models (Kreutzer et al., 2015; Martins et al., 2016). We start by proposing a “pure” QE system (§3) consisting of a new, carefully engineered neural model (N EURAL QE), stacked into a linear feature-rich classifier (L INEAR QE). Along the way, we provide a rigorous empirical analysis to better understand the contribution of the several groups of features and to justify the architecture of the neural system. A second contribution of this paper is bringing in the related task of automatic post-editing (APE; Simard et al. (2007)), which aims to au205 Transactions of the Association for Computational Linguistics, vol. 5, pp."
Q17-1015,W15-3038,0,0.0367556,"Missing"
Q17-1015,W14-3342,0,0.0457533,"sourced and professional translation services, due to their potential to dramatically reduce post-editing times and to save labor costs (Specia, 2011). The increasing interest in this problem from an industrial angle comes as no surprise (Turchi et al., 2014; de Souza et al., 2015; Martins et al., 2016; Kozlova et al., 2016). In this paper, we tackle word-level QE, whose goal is to assign a label of OK or BAD to each word in the translation (Figure 1). Past approaches to this problem include linear classifiers with handcrafted features (Ueffing and Ney, 2007; Bic¸ici, 2013; Shah et al., 2013; Luong et al., 2014), often combined with feature selection (Avramidis, 2012; Beck et al., 2013), recurrent neural networks (de Souza et al., 2014; Kim and Lee, 2016), and systems that combine linear and neural models (Kreutzer et al., 2015; Martins et al., 2016). We start by proposing a “pure” QE system (§3) consisting of a new, carefully engineered neural model (N EURAL QE), stacked into a linear feature-rich classifier (L INEAR QE). Along the way, we provide a rigorous empirical analysis to better understand the contribution of the several groups of features and to justify the architecture of the neural system"
Q17-1015,D08-1017,1,0.671671,"1.20) Table 4: Effect of architectural changes in N EURAL QE on the WMT16 development set. without POS tags as input performs almost 2.5 points worse. Finally, varying the size of the hidden layers and the depth of the network hurts the final model’s performance, albeit more slightly. 3.3 Stacking Neural and Linear Models We now stack the N EURAL QE system (§3.2) into the L INEAR QE system (§3.1) as an ensemble strategy; we call the resulting system S TACKED QE. Stacking architectures (Wolpert, 1992; Breiman, 1996) have proved effective in structured NLP problems (Cohen and de Carvalho, 2005; Martins et al., 2008). The underlying idea is to combine two systems by letting the prediction of the first system be used as an input feature for the second system. During training, it is necessary to jackknife the first system’s predictions to avoid overfitting the training set. This is done by splitting the training set in K folds (we set K = 10) and training K different instances of the first system, where each instance is trained on K − 1 folds and makes predictions for the left-out fold. The concatenation of all the predictions yields an unbiased training set for the second classifier. Neural intra-ensembles"
Q17-1015,P13-2109,1,0.813822,"Missing"
Q17-1015,W16-2387,1,0.607823,"al., 2004; Specia et al., 2013). This has many potential usages: informing an end user about the reliability of translated content; deciding if a translation is ready for publishing or if it requires human post-editing; highlighting the words that need to be changed. QE systems are particularly appealing for crowd-sourced and professional translation services, due to their potential to dramatically reduce post-editing times and to save labor costs (Specia, 2011). The increasing interest in this problem from an industrial angle comes as no surprise (Turchi et al., 2014; de Souza et al., 2015; Martins et al., 2016; Kozlova et al., 2016). In this paper, we tackle word-level QE, whose goal is to assign a label of OK or BAD to each word in the translation (Figure 1). Past approaches to this problem include linear classifiers with handcrafted features (Ueffing and Ney, 2007; Bic¸ici, 2013; Shah et al., 2013; Luong et al., 2014), often combined with feature selection (Avramidis, 2012; Beck et al., 2013), recurrent neural networks (de Souza et al., 2014; Kim and Lee, 2016), and systems that combine linear and neural models (Kreutzer et al., 2015; Martins et al., 2016). We start by proposing a “pure” QE syste"
Q17-1015,P03-1021,0,0.0756007,"heckpoints of each training run are averaged element-wise (JunczysDowmunt et al., 2016) resulting in new single models with generally improved performance. To verify the quality of the APE system, we ensemble the 8 resulting models (4 times s → p and 4 times t → p) and add the APE penalty described in Junczys-Dowmunt and Grundkiewicz (2016). This large ensemble across folds is only used during test time. For creating the jackknifed training data, only the models from the corresponding fold are used. Since we combine models of different types, we tune weights on the development set with MERT9 (Och, 2003) towards TER, yielding the model denoted as “APE TER-tuned”. Results are listed in Table 7 for the APE shared task (WMT 16). For the purely s → p and t → p ensembles, models are weighted equally. We achieve slightly better results in terms of TER, the main task metric, than the original system, using less data. For completeness, we also apply this procedure to WMT15 data, generating a similar resource of 500K artificial English-Spanish-Spanish postediting triplets via roundtrip translation.10 The training, jackknifing and ensembling methods are the same as for the WMT16 setting. For the WMT15"
Q17-1015,W12-3117,0,0.0273681,"an on false negatives (cFP ∈ {0.5, 0.55, . . . , 0.95}, cFN = 1 − cFP ), to account for the existence of fewer BAD labels than OK labels in the data. These values are tuned on the development set. Results and feature contribution. Table 3 shows the performance of the L INEAR QE system. To help understand the contribution of each group of features, we evaluated different variants of the L INEAR QE system on the development sets of WMT15/16. As expected, the use of bigrams improves the simple unigram model, and the syntac4 While syntactic features have been used previously in sentence-level QE (Rubino et al., 2012), they have never been applied to the finer-grained word-level variant tackled here. 5 http://www.cs.cmu.edu/˜ark/TurboParser. 208 Features WMT15 (F1BAD ) WMT16 (F1MULT ) 41.77 42.20 42.80 43.68 40.05 40.63 43.65 46.11 unigrams only +simple bigram +rich bigrams +syntactic (full) Table 3: Performance on the WMT15 (En-Es) and WMT16 (En-De) development sets of several configurations of L INEAR QE. We report the official metric for these shared tasks, F1BAD for WMT15 and F1MULT for WMT16. tic features help even further. The impact of these features is more prominent in WMT16: the rich bigram featu"
Q17-1015,2013.mtsummit-papers.21,0,0.432432,"ppealing for crowd-sourced and professional translation services, due to their potential to dramatically reduce post-editing times and to save labor costs (Specia, 2011). The increasing interest in this problem from an industrial angle comes as no surprise (Turchi et al., 2014; de Souza et al., 2015; Martins et al., 2016; Kozlova et al., 2016). In this paper, we tackle word-level QE, whose goal is to assign a label of OK or BAD to each word in the translation (Figure 1). Past approaches to this problem include linear classifiers with handcrafted features (Ueffing and Ney, 2007; Bic¸ici, 2013; Shah et al., 2013; Luong et al., 2014), often combined with feature selection (Avramidis, 2012; Beck et al., 2013), recurrent neural networks (de Souza et al., 2014; Kim and Lee, 2016), and systems that combine linear and neural models (Kreutzer et al., 2015; Martins et al., 2016). We start by proposing a “pure” QE system (§3) consisting of a new, carefully engineered neural model (N EURAL QE), stacked into a linear feature-rich classifier (L INEAR QE). Along the way, we provide a rigorous empirical analysis to better understand the contribution of the several groups of features and to justify the architecture"
Q17-1015,W07-0728,0,0.17708,"Missing"
Q17-1015,2006.amta-papers.25,0,0.0932633,"evaluation metrics of each dataset’s year. For WMT15, the official metric for the word-level QE task is the F1 score of the BAD labels (F1BAD ). For WMT16, it is the product of the F1 scores for the OK and BAD labels (denoted F1MULT ). For sentencelevel QE, we report the Pearson’s r correlation for HTER prediction and the Spearman’s ρ correlation score for sentence ranking (Graham, 2015). From post-edited sentences to quality labels. In the datasets above, the word quality labels are obtained automatically by aligning the translated and the post-edited sentence with the TERCOM software tool (Snover et al., 2006)2 , with the default settings (tokenized, case insensitive, exact matching only, shifts disabled). This tool computes the HTER (the normalized edit distance) between the translated and post-edited sentence. As a by-product, it aligns the words in the two sentences, identifying substitution errors, word deletions (i.e. words omitted by the translation system), and insertions (redundant words in the translation). Words in the MT output that need to be edited are marked by the BAD quality labels. The fact that the quality labels are automatically obtained from the post-edited sentences is not jus"
Q17-1015,W12-3121,0,0.0213076,"E ensemble are much weaker in terms of F1MULT . This is less surprising in the case of the full ensemble, as it has been tuned towards TER for the APE task specifically. However, we can obtain even better APEbased QE systems for both shared task settings by tuning the full APE ensembles towards F1MULT , the official WMT16 QE metric, and towards F1BAD for WMT15.12 With this approach, we produce our new best stand-alone QE-systems for both shared tasks, which we denote as A PE QE. 11 Note that this system resembles other QE approaches which use pseudo-reference features (Albrecht and Hwa, 2008; Soricut and Narsale, 2012; Shah et al., 2013), since the s → p is essentially an “alternative” MT system. 12 Using again MERT and executing 7 iterations on the official development set with an n-best list size of 12. F1BAD dev F1BAD test Best system in WMT15 43.1 43.12 L INEAR QE N EURAL QE S TACKED QE A PE QE F ULL S TACKED QE 43.68 43.51 44.68 46.44 47.61 42.50 43.35 43.70 46.05 47.08 F1MULT dev F1MULT test Best system in WMT16 49.25 49.52 L INEAR QE N EURAL QE S TACKED QE A PE QE F ULL S TACKED QE 46.11 46.80 49.16 54.95 56.80 46.16 47.29 50.27 55.68 57.47 Table 10: Performance of the several word-level QE systems"
Q17-1015,P13-4014,0,0.0326539,"Missing"
Q17-1015,2011.eamt-1.12,0,0.059343,"Introduction The goal of quality estimation (QE) is to evaluate a translation system’s quality without access to reference translations (Blatz et al., 2004; Specia et al., 2013). This has many potential usages: informing an end user about the reliability of translated content; deciding if a translation is ready for publishing or if it requires human post-editing; highlighting the words that need to be changed. QE systems are particularly appealing for crowd-sourced and professional translation services, due to their potential to dramatically reduce post-editing times and to save labor costs (Specia, 2011). The increasing interest in this problem from an industrial angle comes as no surprise (Turchi et al., 2014; de Souza et al., 2015; Martins et al., 2016; Kozlova et al., 2016). In this paper, we tackle word-level QE, whose goal is to assign a label of OK or BAD to each word in the translation (Figure 1). Past approaches to this problem include linear classifiers with handcrafted features (Ueffing and Ney, 2007; Bic¸ici, 2013; Shah et al., 2013; Luong et al., 2014), often combined with feature selection (Avramidis, 2012; Beck et al., 2013), recurrent neural networks (de Souza et al., 2014; Kim"
Q17-1015,P14-1067,0,0.140631,"Missing"
Q17-1015,J07-1003,0,0.168213,"changed. QE systems are particularly appealing for crowd-sourced and professional translation services, due to their potential to dramatically reduce post-editing times and to save labor costs (Specia, 2011). The increasing interest in this problem from an industrial angle comes as no surprise (Turchi et al., 2014; de Souza et al., 2015; Martins et al., 2016; Kozlova et al., 2016). In this paper, we tackle word-level QE, whose goal is to assign a label of OK or BAD to each word in the translation (Figure 1). Past approaches to this problem include linear classifiers with handcrafted features (Ueffing and Ney, 2007; Bic¸ici, 2013; Shah et al., 2013; Luong et al., 2014), often combined with feature selection (Avramidis, 2012; Beck et al., 2013), recurrent neural networks (de Souza et al., 2014; Kim and Lee, 2016), and systems that combine linear and neural models (Kreutzer et al., 2015; Martins et al., 2016). We start by proposing a “pure” QE system (§3) consisting of a new, carefully engineered neural model (N EURAL QE), stacked into a linear feature-rich classifier (L INEAR QE). Along the way, we provide a rigorous empirical analysis to better understand the contribution of the several groups of featur"
Q17-1015,W16-2301,0,\N,Missing
S12-1029,P11-1048,0,0.0143799,"emantic frame abstractions, and employ explicit role labels. A common trait in prior work has been the use of a two-stage model that identifies arguments first, then labels them. They are treated jointly here, unlike what has typically been done in PropBank-style SRL (M`arquez et al., 2008). Dual decomposition: Rush et al. (2010) proposed subgradient-based dual decomposition as a way of combining models which are tractable individually, but not jointly, by solving a relaxation of the original problem. This was followed by work adopting this method for syntax and translation (Koo et al., 2010; Auli and Lopez, 2011; DeNero and Macherey, 2011; Rush and Collins, 2011; Chang and Collins, 2011). Recently, Martins et al. (2011b) showed that the success of subgradient-based dual decomposition strongly relies on breaking down the original problem into a “good” decomposition, i.e., one with few overlapping components. This leaves out many declarative constrained problems, for which such a good decomposition is not readily available. For those, Martins et al. (2011b) proposed the AD3 al216 gorithm, which retains the modularity of previous methods, but can handle thousands of small overlapping components. Exact d"
S12-1029,S07-1018,0,0.0523873,"xact solution. Two observations are noteworthy. First, the optimal value of the relaxed problem (Eq. 11) provides an upper bound to the original problem (Eq. 2). This is because Eq. 2 has the additional integer constraint on the variables. In particular, any feasible dual point provides an upper bound to the original 214 4.1 Experiments and Results Dataset, Preprocessing, and Learning In our experiments, we use FrameNet 1.5, which contains a lexicon of 877 frames and 1,068 role labels, and 78 documents with multiple predicateargument annotations (a superset of the SemEval shared task dataset; Baker et al., 2007). We used the same split as Das and Smith (2011), with 55 documents for training (containing 19,582 frame annotations) and 23 for testing (with 4,458 annotations). We randomly selected 4,462 predicates in the training set as development data. The raw sentences in all the training and test documents were preprocessed using MXPOST (Ratnaparkhi, 1996) and the MST dependency parser (McDonald et al., 2005). The state-of-the-art system for this task is SEMAFOR, an open source tool (Das et al., 2010a)5 that provides a baseline benchmark for our new algorithm. We use the components of SEMAFOR as-is to"
S12-1029,D11-1003,0,0.0195552,"it in prior work has been the use of a two-stage model that identifies arguments first, then labels them. They are treated jointly here, unlike what has typically been done in PropBank-style SRL (M`arquez et al., 2008). Dual decomposition: Rush et al. (2010) proposed subgradient-based dual decomposition as a way of combining models which are tractable individually, but not jointly, by solving a relaxation of the original problem. This was followed by work adopting this method for syntax and translation (Koo et al., 2010; Auli and Lopez, 2011; DeNero and Macherey, 2011; Rush and Collins, 2011; Chang and Collins, 2011). Recently, Martins et al. (2011b) showed that the success of subgradient-based dual decomposition strongly relies on breaking down the original problem into a “good” decomposition, i.e., one with few overlapping components. This leaves out many declarative constrained problems, for which such a good decomposition is not readily available. For those, Martins et al. (2011b) proposed the AD3 al216 gorithm, which retains the modularity of previous methods, but can handle thousands of small overlapping components. Exact decoding: This paper contributes an exact branch-and-bound technique wrapped a"
S12-1029,S10-1059,1,0.488928,"Missing"
S12-1029,P11-1144,1,0.396255,"First, the optimal value of the relaxed problem (Eq. 11) provides an upper bound to the original problem (Eq. 2). This is because Eq. 2 has the additional integer constraint on the variables. In particular, any feasible dual point provides an upper bound to the original 214 4.1 Experiments and Results Dataset, Preprocessing, and Learning In our experiments, we use FrameNet 1.5, which contains a lexicon of 877 frames and 1,068 role labels, and 78 documents with multiple predicateargument annotations (a superset of the SemEval shared task dataset; Baker et al., 2007). We used the same split as Das and Smith (2011), with 55 documents for training (containing 19,582 frame annotations) and 23 for testing (with 4,458 annotations). We randomly selected 4,462 predicates in the training set as development data. The raw sentences in all the training and test documents were preprocessed using MXPOST (Ratnaparkhi, 1996) and the MST dependency parser (McDonald et al., 2005). The state-of-the-art system for this task is SEMAFOR, an open source tool (Das et al., 2010a)5 that provides a baseline benchmark for our new algorithm. We use the components of SEMAFOR as-is to define the features h and train the weights ψ u"
S12-1029,N10-1138,1,0.116028,"cuments with multiple predicateargument annotations (a superset of the SemEval shared task dataset; Baker et al., 2007). We used the same split as Das and Smith (2011), with 55 documents for training (containing 19,582 frame annotations) and 23 for testing (with 4,458 annotations). We randomly selected 4,462 predicates in the training set as development data. The raw sentences in all the training and test documents were preprocessed using MXPOST (Ratnaparkhi, 1996) and the MST dependency parser (McDonald et al., 2005). The state-of-the-art system for this task is SEMAFOR, an open source tool (Das et al., 2010a)5 that provides a baseline benchmark for our new algorithm. We use the components of SEMAFOR as-is to define the features h and train the weights ψ used in the scoring function c. We also use its 5 http://www.ark.cs.cmu.edu/SEMAFOR heuristic mechanism to find potential spans St for a given predicate t. SEMAFOR learns weights using `2 -penalized log-likelihood; we augmented its dev set-tuning procedure to tune both the regularization strength and the AD3 penalty strength ρ. We initialize ρ = 0.1 and follow Martins et al. (2011b) in dynamically adjusting it. Note that we do not use SEMAFOR’s a"
S12-1029,P11-1043,0,0.0256002,"ions, and employ explicit role labels. A common trait in prior work has been the use of a two-stage model that identifies arguments first, then labels them. They are treated jointly here, unlike what has typically been done in PropBank-style SRL (M`arquez et al., 2008). Dual decomposition: Rush et al. (2010) proposed subgradient-based dual decomposition as a way of combining models which are tractable individually, but not jointly, by solving a relaxation of the original problem. This was followed by work adopting this method for syntax and translation (Koo et al., 2010; Auli and Lopez, 2011; DeNero and Macherey, 2011; Rush and Collins, 2011; Chang and Collins, 2011). Recently, Martins et al. (2011b) showed that the success of subgradient-based dual decomposition strongly relies on breaking down the original problem into a “good” decomposition, i.e., one with few overlapping components. This leaves out many declarative constrained problems, for which such a good decomposition is not readily available. For those, Martins et al. (2011b) proposed the AD3 al216 gorithm, which retains the modularity of previous methods, but can handle thousands of small overlapping components. Exact decoding: This paper contrib"
S12-1029,P10-1160,0,0.0220157,"of effort has been spent by the optimization community to devise efficient generic solvers. An example is CPLEX, a state-of-the-art solver for mixed integer programming that we employ as a baseline to solve the ILP in Eq. 2 as well as its LP relaxation in Eq. 11. Like many of the best implementations, CPLEX is proprietary. 4 We noticed in the annotated data, in some cases, the “requires” constraint is violated by the FrameNet annotators. This happens mostly when one of the required roles is absent in the sentence containing the predicate, but is rather instantiated in an earlier sentence; see Gerber and Chai (2010). We apply the hard constraint in Eq. 10, though extending our algorithm to seek arguments outside the sentence is straightforward (Chen et al., 2010). 2.2 Linguistic Constraints from FrameNet a pair of roles that share the “requires” relationship. Although enforcing the four different sets of constraints above is intuitive from a general linguistic perspective, we ground their use in definitive linguistic information present in the FrameNet lexicon (Fillmore et al., 2003). FrameNet, along with lists of semantic frames, associated semantic roles, and predicates that could evoke the frames, giv"
S12-1029,J02-3001,0,0.124019,"“requires” and “excludes” constraints of §2. Finally decoding time (without feature computation steps) on the whole test set is shown in the last column averaged over 5 runs. 5 Related Work Semantic role labeling: Most SRL systems use conventions from PropBank (Kingsbury and Palmer, 2002) and NomBank (Meyers et al., 2004), which store information about verbal and nominal predicates and corresponding symbolic and meaningspecific semantic roles. A separate line of work, including this paper, investigates SRL systems that use FrameNet conventions; while less popular, these systems, pioneered by Gildea and Jurafsky (2002), consider predicates of a wider variety of syntactic categories, use semantic frame abstractions, and employ explicit role labels. A common trait in prior work has been the use of a two-stage model that identifies arguments first, then labels them. They are treated jointly here, unlike what has typically been done in PropBank-style SRL (M`arquez et al., 2008). Dual decomposition: Rush et al. (2010) proposed subgradient-based dual decomposition as a way of combining models which are tractable individually, but not jointly, by solving a relaxation of the original problem. This was followed by w"
S12-1029,S07-1048,0,0.0706484,"Missing"
S12-1029,kingsbury-palmer-2002-treebank,0,0.514983,"using FrameNet, these interactions have been largely ignored, though they 1 have the potential to improve the quality and consistency of semantic analysis. In this paper, we present an algorithm that finds the full collection of arguments of a predicate given its semantic frame. Although we work within the conventions of FrameNet, our approach is generalizable to other semantic role labeling (SRL) frameworks. We model this argument identification task as constrained optimization, where the constraints come from expert knowledge encoded in a lexicon. Following prior work on PropBank-style SRL (Kingsbury and Palmer, 2002) that dealt with similar constrained problems (Punyakanok et al., 2004; Punyakanok et al., 2008, inter alia), we incorporate this declarative knowledge in an integer linear program (ILP). Because general-purpose ILP solvers are proprietary and do not fully exploit the structure of the problem, we turn to a class of optimization techniques called dual decomposition (Komodakis et al., 2007; Rush et al., 2010; Martins et al., 2011a). We derive a modular, extensible, parallelizable approach in which semantic constraints map not just to declarative components of the algorithm, but also to procedura"
S12-1029,D10-1125,0,0.0857449,"categories, use semantic frame abstractions, and employ explicit role labels. A common trait in prior work has been the use of a two-stage model that identifies arguments first, then labels them. They are treated jointly here, unlike what has typically been done in PropBank-style SRL (M`arquez et al., 2008). Dual decomposition: Rush et al. (2010) proposed subgradient-based dual decomposition as a way of combining models which are tractable individually, but not jointly, by solving a relaxation of the original problem. This was followed by work adopting this method for syntax and translation (Koo et al., 2010; Auli and Lopez, 2011; DeNero and Macherey, 2011; Rush and Collins, 2011; Chang and Collins, 2011). Recently, Martins et al. (2011b) showed that the success of subgradient-based dual decomposition strongly relies on breaking down the original problem into a “good” decomposition, i.e., one with few overlapping components. This leaves out many declarative constrained problems, for which such a good decomposition is not readily available. For those, Martins et al. (2011b) proposed the AD3 al216 gorithm, which retains the modularity of previous methods, but can handle thousands of small overlappi"
S12-1029,J08-2001,0,0.0417426,"Missing"
S12-1029,D10-1004,1,0.572151,"rtner 1 and Partner 2 that share the “requires” relationship. In fact, out of 877 frames in FrameNet 1.5, the lexicon’s latest edition, 204 frames have at least a pair of roles that share the “excludes” relationship, and 54 list at least 2.3 Constraints as Factors in a Graphical Model 212 The LP in Eq. 11 can be represented as a maximum a posteriori (MAP) inference problem in an undirected graphical model. In the factor graph, each component of z corresponds to a binary variable, and each instantiation of a constraint in Eqs. 3, 5, 8 and 10 corresponds to a factor. Smith and Eisner (2008) and Martins et al. (2010) used such a representation to impose constraints in a dependency parsing problem; the latter discussed the equivalence of linear programs and factor graphs for representing discrete optimization problems. Each of our constraints take standard factor forms we can describe using the terminology of Smith and Eisner (2008) and Martins et al. (2010). The uniqueness constraint in Eq. 3 corresponds to an X OR factor, while the overlap constraint in Eq. 5 corresponds to an AT M OST O NE factor. The constraints in Eq. 8 enforcing the “excludes” relationship can be represented with an O R factor. Final"
S12-1029,D11-1022,1,0.770273,"entification task as constrained optimization, where the constraints come from expert knowledge encoded in a lexicon. Following prior work on PropBank-style SRL (Kingsbury and Palmer, 2002) that dealt with similar constrained problems (Punyakanok et al., 2004; Punyakanok et al., 2008, inter alia), we incorporate this declarative knowledge in an integer linear program (ILP). Because general-purpose ILP solvers are proprietary and do not fully exploit the structure of the problem, we turn to a class of optimization techniques called dual decomposition (Komodakis et al., 2007; Rush et al., 2010; Martins et al., 2011a). We derive a modular, extensible, parallelizable approach in which semantic constraints map not just to declarative components of the algorithm, but also to procedural ones, in the form of “workers.” While dual decomposition algorithms only solve a relaxation of the original problem, we make a novel contribution by wrapping the algorithm in a branch-andbound search procedure, resulting in exact solutions. We experimentally find that our algorithm achieves accuracy comparable to a state-of-the-art system, while respecting all imposed linguistic constraints. In comparison to inexact beam sear"
S12-1029,P05-1012,0,0.0760088,"ments, we use FrameNet 1.5, which contains a lexicon of 877 frames and 1,068 role labels, and 78 documents with multiple predicateargument annotations (a superset of the SemEval shared task dataset; Baker et al., 2007). We used the same split as Das and Smith (2011), with 55 documents for training (containing 19,582 frame annotations) and 23 for testing (with 4,458 annotations). We randomly selected 4,462 predicates in the training set as development data. The raw sentences in all the training and test documents were preprocessed using MXPOST (Ratnaparkhi, 1996) and the MST dependency parser (McDonald et al., 2005). The state-of-the-art system for this task is SEMAFOR, an open source tool (Das et al., 2010a)5 that provides a baseline benchmark for our new algorithm. We use the components of SEMAFOR as-is to define the features h and train the weights ψ used in the scoring function c. We also use its 5 http://www.ark.cs.cmu.edu/SEMAFOR heuristic mechanism to find potential spans St for a given predicate t. SEMAFOR learns weights using `2 -penalized log-likelihood; we augmented its dev set-tuning procedure to tune both the regularization strength and the AD3 penalty strength ρ. We initialize ρ = 0.1 and f"
S12-1029,W04-2705,0,0.0292647,".17 ± 0.01 4.78 ± 0.04 Table 1: Comparison of decoding strategies in §4.2. We evaluate in terms of precision, recall and F1 score on a test set containing 4,458 predicates. We also compute the number of structural violations each model makes: number of overlapping arguments and violations of the “requires” and “excludes” constraints of §2. Finally decoding time (without feature computation steps) on the whole test set is shown in the last column averaged over 5 runs. 5 Related Work Semantic role labeling: Most SRL systems use conventions from PropBank (Kingsbury and Palmer, 2002) and NomBank (Meyers et al., 2004), which store information about verbal and nominal predicates and corresponding symbolic and meaningspecific semantic roles. A separate line of work, including this paper, investigates SRL systems that use FrameNet conventions; while less popular, these systems, pioneered by Gildea and Jurafsky (2002), consider predicates of a wider variety of syntactic categories, use semantic frame abstractions, and employ explicit role labels. A common trait in prior work has been the use of a two-stage model that identifies arguments first, then labels them. They are treated jointly here, unlike what has t"
S12-1029,C04-1197,0,0.149314,"1 have the potential to improve the quality and consistency of semantic analysis. In this paper, we present an algorithm that finds the full collection of arguments of a predicate given its semantic frame. Although we work within the conventions of FrameNet, our approach is generalizable to other semantic role labeling (SRL) frameworks. We model this argument identification task as constrained optimization, where the constraints come from expert knowledge encoded in a lexicon. Following prior work on PropBank-style SRL (Kingsbury and Palmer, 2002) that dealt with similar constrained problems (Punyakanok et al., 2004; Punyakanok et al., 2008, inter alia), we incorporate this declarative knowledge in an integer linear program (ILP). Because general-purpose ILP solvers are proprietary and do not fully exploit the structure of the problem, we turn to a class of optimization techniques called dual decomposition (Komodakis et al., 2007; Rush et al., 2010; Martins et al., 2011a). We derive a modular, extensible, parallelizable approach in which semantic constraints map not just to declarative components of the algorithm, but also to procedural ones, in the form of “workers.” While dual decomposition algorithms"
S12-1029,J08-2005,0,0.890274,"mprove the quality and consistency of semantic analysis. In this paper, we present an algorithm that finds the full collection of arguments of a predicate given its semantic frame. Although we work within the conventions of FrameNet, our approach is generalizable to other semantic role labeling (SRL) frameworks. We model this argument identification task as constrained optimization, where the constraints come from expert knowledge encoded in a lexicon. Following prior work on PropBank-style SRL (Kingsbury and Palmer, 2002) that dealt with similar constrained problems (Punyakanok et al., 2004; Punyakanok et al., 2008, inter alia), we incorporate this declarative knowledge in an integer linear program (ILP). Because general-purpose ILP solvers are proprietary and do not fully exploit the structure of the problem, we turn to a class of optimization techniques called dual decomposition (Komodakis et al., 2007; Rush et al., 2010; Martins et al., 2011a). We derive a modular, extensible, parallelizable approach in which semantic constraints map not just to declarative components of the algorithm, but also to procedural ones, in the form of “workers.” While dual decomposition algorithms only solve a relaxation o"
S12-1029,W96-0213,0,0.0498838,"Dataset, Preprocessing, and Learning In our experiments, we use FrameNet 1.5, which contains a lexicon of 877 frames and 1,068 role labels, and 78 documents with multiple predicateargument annotations (a superset of the SemEval shared task dataset; Baker et al., 2007). We used the same split as Das and Smith (2011), with 55 documents for training (containing 19,582 frame annotations) and 23 for testing (with 4,458 annotations). We randomly selected 4,462 predicates in the training set as development data. The raw sentences in all the training and test documents were preprocessed using MXPOST (Ratnaparkhi, 1996) and the MST dependency parser (McDonald et al., 2005). The state-of-the-art system for this task is SEMAFOR, an open source tool (Das et al., 2010a)5 that provides a baseline benchmark for our new algorithm. We use the components of SEMAFOR as-is to define the features h and train the weights ψ used in the scoring function c. We also use its 5 http://www.ark.cs.cmu.edu/SEMAFOR heuristic mechanism to find potential spans St for a given predicate t. SEMAFOR learns weights using `2 -penalized log-likelihood; we augmented its dev set-tuning procedure to tune both the regularization strength and t"
S12-1029,P11-1008,0,0.0134412,"ole labels. A common trait in prior work has been the use of a two-stage model that identifies arguments first, then labels them. They are treated jointly here, unlike what has typically been done in PropBank-style SRL (M`arquez et al., 2008). Dual decomposition: Rush et al. (2010) proposed subgradient-based dual decomposition as a way of combining models which are tractable individually, but not jointly, by solving a relaxation of the original problem. This was followed by work adopting this method for syntax and translation (Koo et al., 2010; Auli and Lopez, 2011; DeNero and Macherey, 2011; Rush and Collins, 2011; Chang and Collins, 2011). Recently, Martins et al. (2011b) showed that the success of subgradient-based dual decomposition strongly relies on breaking down the original problem into a “good” decomposition, i.e., one with few overlapping components. This leaves out many declarative constrained problems, for which such a good decomposition is not readily available. For those, Martins et al. (2011b) proposed the AD3 al216 gorithm, which retains the modularity of previous methods, but can handle thousands of small overlapping components. Exact decoding: This paper contributes an exact branch-and"
S12-1029,D10-1001,0,0.260942,"el this argument identification task as constrained optimization, where the constraints come from expert knowledge encoded in a lexicon. Following prior work on PropBank-style SRL (Kingsbury and Palmer, 2002) that dealt with similar constrained problems (Punyakanok et al., 2004; Punyakanok et al., 2008, inter alia), we incorporate this declarative knowledge in an integer linear program (ILP). Because general-purpose ILP solvers are proprietary and do not fully exploit the structure of the problem, we turn to a class of optimization techniques called dual decomposition (Komodakis et al., 2007; Rush et al., 2010; Martins et al., 2011a). We derive a modular, extensible, parallelizable approach in which semantic constraints map not just to declarative components of the algorithm, but also to procedural ones, in the form of “workers.” While dual decomposition algorithms only solve a relaxation of the original problem, we make a novel contribution by wrapping the algorithm in a branch-andbound search procedure, resulting in exact solutions. We experimentally find that our algorithm achieves accuracy comparable to a state-of-the-art system, while respecting all imposed linguistic constraints. In compariso"
S12-1029,D08-1016,0,0.0185648,"tners, also has two roles Partner 1 and Partner 2 that share the “requires” relationship. In fact, out of 877 frames in FrameNet 1.5, the lexicon’s latest edition, 204 frames have at least a pair of roles that share the “excludes” relationship, and 54 list at least 2.3 Constraints as Factors in a Graphical Model 212 The LP in Eq. 11 can be represented as a maximum a posteriori (MAP) inference problem in an undirected graphical model. In the factor graph, each component of z corresponds to a binary variable, and each instantiation of a constraint in Eqs. 3, 5, 8 and 10 corresponds to a factor. Smith and Eisner (2008) and Martins et al. (2010) used such a representation to impose constraints in a dependency parsing problem; the latter discussed the equivalence of linear programs and factor graphs for representing discrete optimization problems. Each of our constraints take standard factor forms we can describe using the terminology of Smith and Eisner (2008) and Martins et al. (2010). The uniqueness constraint in Eq. 3 corresponds to an X OR factor, while the overlap constraint in Eq. 5 corresponds to an AT M OST O NE factor. The constraints in Eq. 8 enforcing the “excludes” relationship can be represented"
S12-1029,P05-1073,0,0.0878624,"t line imposes constraints on the mapping between roles and spans; these are motivated on linguistic grounds and are described next.3 Uniqueness: Each role r is filled by at most one span in St . This constraint can be expressed by: P ∀r ∈ Rf , s∈St zr,s = 1. (3) There are O(|Rf |) such constraints. Note that since St contains the null span ∅, non-overt roles are also captured using the above constraints. Such a constraint is used extensively in prior literature (Punyakanok et al., 2008, §3.4.1). Overlap: SRL systems commonly constrain roles to be filled by non-overlapping spans. For example, Toutanova et al. (2005) used dynamic programming over a phrase structure tree to prevent overlaps between arguments, and Punyakanok et al. (2008) used 3 Note that equality constraints a · z = b can be transformed into double-side inequalities a · z ≤ b and −a · z ≤ −b. constraints in an ILP to respect this requirement. Inspired by the latter, we require that each input sentence position of x be covered by at most one argument. For each role r ∈ Rf , we define: Gr (i) = {s |s ∈ St , s covers position i in x}. (4) We can define our overlap constraints in terms of Gr as follows, for every sentence position i: P P ∀i ∈"
S14-2082,S12-1029,1,0.288255,"Missing"
S14-2082,P14-1134,0,0.0485564,"Missing"
S14-2082,P10-1001,0,0.0875183,"Missing"
S14-2082,D10-1125,0,0.0943483,"Missing"
S14-2082,P09-1039,1,0.797483,"Missing"
S14-2082,D10-1004,1,0.864326,"Missing"
S14-2082,D11-1022,1,0.934895,"Missing"
S14-2082,P13-2109,1,0.447287,"Missing"
S14-2082,P05-1012,0,0.166994,"Missing"
S14-2082,W06-2932,0,0.108304,"Missing"
S14-2082,D10-1001,0,0.026563,"Missing"
S14-2082,D08-1016,0,0.126813,"Missing"
S14-2082,P05-1073,0,0.154993,"Missing"
S14-2082,D12-1133,0,\N,Missing
S14-2082,W08-2123,0,\N,Missing
S14-2082,D07-1101,0,\N,Missing
S15-2162,W06-1615,0,0.283669,"Missing"
S15-2162,D12-1133,0,0.0237766,"is a serious issue, suggesting domain adaptation as an interesting avenue for future research. 1 Introduction The last years have witnessed a continuous progress in statistical multilingual models for syntax, thanks to shared tasks such as CoNLL 2006-7 (Buchholz and Marsi, 2006; Nivre et al., 2007) and, more recently, SPMRL 2013-14 (Seddah et al., 2013; Seddah et al., 2014). As a global trend, we observe that models that incorporate rich global features are typically more accurate, even if pruning is necessary or decoding needs to be approximate (McDonald et al., 2006; Koo and Collins, 2010; Bohnet and Nivre, 2012; Martins et al., 2009, 2013). The same rationale applies to semantic dependency parsing, also a structured prediction problem, but where the output variable is a semantic graph, rather than a syntactic tree. Indeed, the best performing systems in last year shared task on broad-coverage semantic dependency parsing follow this principle (Oepen et al., 2014). This year, a new challenge was put forth: how to handle multiple languages and out-ofdomain data? Our proposed parser (§2) is essentially the same that we submitted in the previous year to the same SemEval task (Martins and Almeida, 2014),"
S15-2162,W06-2920,0,0.19028,"Missing"
S15-2162,S12-1029,1,0.839071,"e. Our second-order model looks at some pairs of arcs: arcs bearing a grandparent relationship, arguments of the same predicate, predicates sharing the same argument, and consecutive versions of these two. Martins and Almeida (2014) for further details. The parser was built as an extension of a recent dependency parser, TurboParser (Martins et al., 2010, 2013), with the goal of performing semantic parsing using any of the three formalisms considered in the shared task (DM, PAS, and PSD). We have followed prior work in semantic role labeling (Toutanova et al., 2005; Johansson and Nugues, 2008; Das et al., 2012; Flanigan et al., 2014), by adding constraints and modeling interactions among arguments within the same frame; however, we went beyond such sibling interactions to consider more complex grandparent and co-parent structures, effectively correlating different predicates. The overall set of parts used by our parser is illustrated in Figure 1; note that by using only a subset of the parts (predicate, arc, labeled arc, and sibling parts), the semantic parser decodes each predicate frame independently from other predicates; it is the co-parent and grandparent parts that have the effect of creating"
S15-2162,P07-1033,0,0.34716,"Missing"
S15-2162,P14-1134,0,0.0392408,"r model looks at some pairs of arcs: arcs bearing a grandparent relationship, arguments of the same predicate, predicates sharing the same argument, and consecutive versions of these two. Martins and Almeida (2014) for further details. The parser was built as an extension of a recent dependency parser, TurboParser (Martins et al., 2010, 2013), with the goal of performing semantic parsing using any of the three formalisms considered in the shared task (DM, PAS, and PSD). We have followed prior work in semantic role labeling (Toutanova et al., 2005; Johansson and Nugues, 2008; Das et al., 2012; Flanigan et al., 2014), by adding constraints and modeling interactions among arguments within the same frame; however, we went beyond such sibling interactions to consider more complex grandparent and co-parent structures, effectively correlating different predicates. The overall set of parts used by our parser is illustrated in Figure 1; note that by using only a subset of the parts (predicate, arc, labeled arc, and sibling parts), the semantic parser decodes each predicate frame independently from other predicates; it is the co-parent and grandparent parts that have the effect of creating inter-dependence among"
S15-2162,P10-1001,0,0.026046,"formance), domain shift is a serious issue, suggesting domain adaptation as an interesting avenue for future research. 1 Introduction The last years have witnessed a continuous progress in statistical multilingual models for syntax, thanks to shared tasks such as CoNLL 2006-7 (Buchholz and Marsi, 2006; Nivre et al., 2007) and, more recently, SPMRL 2013-14 (Seddah et al., 2013; Seddah et al., 2014). As a global trend, we observe that models that incorporate rich global features are typically more accurate, even if pruning is necessary or decoding needs to be approximate (McDonald et al., 2006; Koo and Collins, 2010; Bohnet and Nivre, 2012; Martins et al., 2009, 2013). The same rationale applies to semantic dependency parsing, also a structured prediction problem, but where the output variable is a semantic graph, rather than a syntactic tree. Indeed, the best performing systems in last year shared task on broad-coverage semantic dependency parsing follow this principle (Oepen et al., 2014). This year, a new challenge was put forth: how to handle multiple languages and out-ofdomain data? Our proposed parser (§2) is essentially the same that we submitted in the previous year to the same SemEval task (Mart"
S15-2162,S14-2082,1,0.809443,"o English). For the English language, we participated in the closed and open tracks, using as additional resources the syntactic dependency annotations provided by the organizers. For Czech and Chinese, we only addressed the closed track, since no companion data were provided for these languages. We did not participate in the gold track that uses gold-standard syntactic annotations; and we did not address the prediction of predicate senses. 2 Semantic Parser For this year’s shared task, we re-run the semantic parser that we developed last year, which is fully desc1ribed in Martins and Almeida (2014), on the new datasets. Since this parser was designed to be multi-lingual, it was straightforward to apply it to the languages introduced this year (Chinese and Czech), as well as on the out-of-domain data. We briefly describe our semantic parser (which we dub TurboSemanticParser and release as opensource software1 ), and refer the interested reader to 1 http://labs.priberam.com/Resources/ TurboSemanticParser 970 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 970–973, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics"
S15-2162,P09-1039,1,0.853029,"gesting domain adaptation as an interesting avenue for future research. 1 Introduction The last years have witnessed a continuous progress in statistical multilingual models for syntax, thanks to shared tasks such as CoNLL 2006-7 (Buchholz and Marsi, 2006; Nivre et al., 2007) and, more recently, SPMRL 2013-14 (Seddah et al., 2013; Seddah et al., 2014). As a global trend, we observe that models that incorporate rich global features are typically more accurate, even if pruning is necessary or decoding needs to be approximate (McDonald et al., 2006; Koo and Collins, 2010; Bohnet and Nivre, 2012; Martins et al., 2009, 2013). The same rationale applies to semantic dependency parsing, also a structured prediction problem, but where the output variable is a semantic graph, rather than a syntactic tree. Indeed, the best performing systems in last year shared task on broad-coverage semantic dependency parsing follow this principle (Oepen et al., 2014). This year, a new challenge was put forth: how to handle multiple languages and out-ofdomain data? Our proposed parser (§2) is essentially the same that we submitted in the previous year to the same SemEval task (Martins and Almeida, 2014), where we scored top in"
S15-2162,D10-1004,1,0.823616,"tational Linguistics Figure 1: Parts considered by our semantic parser. The top row illustrate the basic parts, representing the event that a word is a predicate, or the existence of an arc between a predicate and an argument, eventually labeled with a semantic role. Our second-order model looks at some pairs of arcs: arcs bearing a grandparent relationship, arguments of the same predicate, predicates sharing the same argument, and consecutive versions of these two. Martins and Almeida (2014) for further details. The parser was built as an extension of a recent dependency parser, TurboParser (Martins et al., 2010, 2013), with the goal of performing semantic parsing using any of the three formalisms considered in the shared task (DM, PAS, and PSD). We have followed prior work in semantic role labeling (Toutanova et al., 2005; Johansson and Nugues, 2008; Das et al., 2012; Flanigan et al., 2014), by adding constraints and modeling interactions among arguments within the same frame; however, we went beyond such sibling interactions to consider more complex grandparent and co-parent structures, effectively correlating different predicates. The overall set of parts used by our parser is illustrated in Figur"
S15-2162,D11-1022,1,0.808981,"es; we will analyze the effect of these dependencies in the experimental section (§3). For each part in our model (shown in Figure 1), we computed binary features based on various combination of lexical forms, lemmas, POS tags and syntactic dependency relations of words related to the corresponding predicates and arguments. Most of these features were taken from TurboParser (Martins et al., 2013), and others were inspired by the 971 semantic parser of Johansson and Nugues (2008). To tackle all the parts, we formulate parsing as a global optimization problem and solve a relaxation through AD3 (Martins et al., 2011), a fast dual decomposition algorithm in which several simple local subproblems are solved iteratively. Through a rich set of features, we arrive at top accuracies at parsing speeds around 1,000 tokens per second. See Martins and Almeida (2014) for details on the model, features and decoding process that were used. 3 Experimental Results All models were trained by running 10 epochs of max-loss MIRA with C = 0.01 (Crammer et al., 2006). The cost function takes into account mismatches between predicted and gold dependencies, with a cost cP on labeled arcs incorrectly predicted (false positives)"
S15-2162,P13-2109,1,0.918123,"Missing"
S15-2162,W06-2932,0,0.0396707,"oo far from English performance), domain shift is a serious issue, suggesting domain adaptation as an interesting avenue for future research. 1 Introduction The last years have witnessed a continuous progress in statistical multilingual models for syntax, thanks to shared tasks such as CoNLL 2006-7 (Buchholz and Marsi, 2006; Nivre et al., 2007) and, more recently, SPMRL 2013-14 (Seddah et al., 2013; Seddah et al., 2014). As a global trend, we observe that models that incorporate rich global features are typically more accurate, even if pruning is necessary or decoding needs to be approximate (McDonald et al., 2006; Koo and Collins, 2010; Bohnet and Nivre, 2012; Martins et al., 2009, 2013). The same rationale applies to semantic dependency parsing, also a structured prediction problem, but where the output variable is a semantic graph, rather than a syntactic tree. Indeed, the best performing systems in last year shared task on broad-coverage semantic dependency parsing follow this principle (Oepen et al., 2014). This year, a new challenge was put forth: how to handle multiple languages and out-ofdomain data? Our proposed parser (§2) is essentially the same that we submitted in the previous year to the"
S15-2162,S14-2008,0,0.410703,"Missing"
S15-2162,W14-6111,0,0.0448354,"Missing"
S15-2162,P05-1073,0,0.0180084,"Missing"
S15-2162,W08-2123,0,\N,Missing
S15-2162,W13-4917,0,\N,Missing
S15-2162,D07-1096,0,\N,Missing
W09-1801,C08-1018,0,0.134658,"Missing"
W09-1801,P02-1057,0,0.377314,"Missing"
W09-1801,W03-0501,0,0.620052,"; Carbonell and Goldstein, 1998; Radev et al., 2000). Most of this work aims only to extract relevant sentences from the original documents and present them as the summary; this simplification of the problem yields scalable solutions. Some attention has been devoted by the NLP community to the related problem of sentence compression (Knight and Marcu, 2000): given a long sentence, how to maximally compress it into a grammatical sentence that still preserves all the relevant information? While sentence compression is a promising framework with applications, for example, in headline generation (Dorr et al., 2003; Jin, 2003), little work has been done to include it as a module in document summarization systems. Most existing approaches (with some exceptions, like the vine-growth model of Daum´e, 2006) use a two-stage architecture, either by first extracting a certain number of salient sentences and then feeding them into a sentence compressor, or by first compressing all sentences and extracting later. However, regardless of which operation is performed first—compression or extraction—two-step “pipeline” approaches may fail to find overall-optimal solutions; often the summaries are not better that the"
W09-1801,W02-0406,0,0.0137543,"Missing"
W09-1801,W03-1101,0,0.236222,"systems. Most existing approaches (with some exceptions, like the vine-growth model of Daum´e, 2006) use a two-stage architecture, either by first extracting a certain number of salient sentences and then feeding them into a sentence compressor, or by first compressing all sentences and extracting later. However, regardless of which operation is performed first—compression or extraction—two-step “pipeline” approaches may fail to find overall-optimal solutions; often the summaries are not better that the ones produced by extractive summarization. On the other hand, a pilot study carried out by Lin (2003) suggests that summarization systems that perform sentence compression have the potential to beat pure extractive systems if they model cross-sentence effects. In this work, we address this issue by merging the tasks of sentence extraction and sentence compression into a global optimization problem. A careful design of the objective function encourages “sparse solutions,” i.e., solutions that involve only a small number of sentences whose compressions are to be included in the summary. Our contributions are: • We cast joint sentence extraction and compression as an integer linear program (ILP)"
W09-1801,H05-1066,0,0.023209,"Missing"
W09-1801,E06-1038,0,0.568619,"a short, informative and grammatical sentence. Such a sentence compressor is given a sentence t , hw1 , . . . , wN i as input and outputs a subsequence of length L, c , hwj1 , . . . , wjL i, with 1 ≤ j1 &lt; . . . &lt; jL ≤ N . We may represent this output as a binary vector s of length N , where sj = 1 iff word wj is included in the compression. Note that there are O(2N ) possible subsequences. 3.1 2000; Daum´e and Marcu, 2002), heuristic methods that parse the sentence and then trim constituents according to linguistic criteria (Dorr et al., 2003; Zajic et al., 2006), a pure discriminative model (McDonald, 2006), and an ILP formulation (Clarke and Lapata, 2008). We next give an overview of the two latter approaches. McDonald (2006) uses the outputs of two parsers (a phrase-based and a dependency parser) as features in a discriminative model that decomposes over pairs of consecutive words. Formally, given a sentence t = hw1 , . . . , wN i, the score of a compression c = hwj1 , . . . , wjL i decomposes as: αi θ &gt; f (t, 0, i) + βi θ &gt; f (t, i, n + 1) + i=1 N −1 X N X γij θ &gt; f (t, i, j), (10) i=1 j=i+1 where αi , βi , and γij are additional binary variables with the following meanings: • αi = 1 iff word"
W09-1801,W02-0401,0,0.060482,"oach for document summarization that jointly performs sentence extraction and compression by solving an integer linear program. We report favorable experimental results on newswire data. 1 Introduction Automatic text summarization dates back to the 1950s and 1960s (Luhn, 1958; Baxendale, 1958; Edmundson, 1969). Today, the proliferation of digital information makes research on summarization technologies more important than ever before. In the last two decades, machine learning techniques have been employed in extractive summarization of single documents (Kupiec et al., 1995; Aone et al., 1999; Osborne, 2002) and multiple documents (Radev and McKeown, 1998; Carbonell and Goldstein, 1998; Radev et al., 2000). Most of this work aims only to extract relevant sentences from the original documents and present them as the summary; this simplification of the problem yields scalable solutions. Some attention has been devoted by the NLP community to the related problem of sentence compression (Knight and Marcu, 2000): given a long sentence, how to maximally compress it into a grammatical sentence that still preserves all the relevant information? While sentence compression is a promising framework with app"
W09-1801,J98-3005,0,0.0563716,"intly performs sentence extraction and compression by solving an integer linear program. We report favorable experimental results on newswire data. 1 Introduction Automatic text summarization dates back to the 1950s and 1960s (Luhn, 1958; Baxendale, 1958; Edmundson, 1969). Today, the proliferation of digital information makes research on summarization technologies more important than ever before. In the last two decades, machine learning techniques have been employed in extractive summarization of single documents (Kupiec et al., 1995; Aone et al., 1999; Osborne, 2002) and multiple documents (Radev and McKeown, 1998; Carbonell and Goldstein, 1998; Radev et al., 2000). Most of this work aims only to extract relevant sentences from the original documents and present them as the summary; this simplification of the problem yields scalable solutions. Some attention has been devoted by the NLP community to the related problem of sentence compression (Knight and Marcu, 2000): given a long sentence, how to maximally compress it into a grammatical sentence that still preserves all the relevant information? While sentence compression is a promising framework with applications, for example, in headline generation ("
W09-1801,W00-0403,0,0.0196396,"lving an integer linear program. We report favorable experimental results on newswire data. 1 Introduction Automatic text summarization dates back to the 1950s and 1960s (Luhn, 1958; Baxendale, 1958; Edmundson, 1969). Today, the proliferation of digital information makes research on summarization technologies more important than ever before. In the last two decades, machine learning techniques have been employed in extractive summarization of single documents (Kupiec et al., 1995; Aone et al., 1999; Osborne, 2002) and multiple documents (Radev and McKeown, 1998; Carbonell and Goldstein, 1998; Radev et al., 2000). Most of this work aims only to extract relevant sentences from the original documents and present them as the summary; this simplification of the problem yields scalable solutions. Some attention has been devoted by the NLP community to the related problem of sentence compression (Knight and Marcu, 2000): given a long sentence, how to maximally compress it into a grammatical sentence that still preserves all the relevant information? While sentence compression is a promising framework with applications, for example, in headline generation (Dorr et al., 2003; Jin, 2003), little work has been"
W09-1801,J03-4003,0,\N,Missing
W16-2387,W13-3520,0,0.035111,"Missing"
W16-2387,W14-3342,0,0.0806625,"Missing"
W16-2387,P13-2109,1,0.378405,"Missing"
W16-2387,W14-6111,0,0.0653045,"Missing"
W16-2387,P13-4014,0,0.0571435,"Missing"
W16-2387,W15-3014,0,0.0743205,"Missing"
W16-2387,W15-3037,0,0.262003,"Missing"
W17-4764,C04-1046,0,0.194048,"Missing"
W17-4764,Q17-1015,1,0.837244,"Estimation Shared Task Andr´e F. T. Martins Fabio N. Kepler Unbabel & Instituto de Telecomunicac¸o˜ es Unbabel Lisbon, Portugal University of Pampa, Alegrete, Brazil andre.martins@unbabel.com kepler@unbabel.com Jos´e L. Monteiro Unbabel Lisbon, Portugal jose@unbabel.com Abstract the translation. The sentence-level task attempts to predict the HTER of each sentence, along with a ranking of the sentences. Two language pairs and domains are considered: English-German (IT domain) and German-English (medical domain). Our submission is largely based on the approach that we have recently proposed in Martins et al. (2017), which ensembles a “pure” quality estimation system with predictions derived from an automatic post-editing system. The focus was on developing a word-level system, and to use the word label predictions to predict the sentencelevel HTER. Our system architecture is described in full detail in the following sections. We first describe our “pure” QE system (§2), which consists of a neural model (N EURAL QE) stacked into a linear feature-rich classifier (L INEAR QE). Then, we train an APE system (using a large amount of artificial “roundtrip translations”) and adapt it to predict word-level quali"
W17-4764,W14-6111,0,0.0735321,"Missing"
W17-4764,2006.amta-papers.25,0,0.114062,"task. This subset was obtained through crossentropy filtering. For this, we built an in-domain trigram language model from the English postedited training data. We then calculated crossentropy scores for the UFAL corpus according to the language model. We sorted the corpus by increasing cross-entropy and kept the first 500K sentences to be used as additional training data. To convert the resulting APE systems into word-level quality estimators, we need to turn the automatic post-edited sentences into word quality labels. This is done in a straightforward way by using the TERCOM software tool (Snover et al., 2006)4 with the default settings (tokenized, case insensitive, exact matching only, shifts disabled). This tool computes the HTER (the normalized edit distance) between the translated and post-edited sentence. As a by-product, it aligns the words in the two sentences, identifying substitution errors, 4 4 Full Stacked System Finally, we consider a larger stacked system where we stack both N EURAL QE and A PE QE into L IN EAR QE. This mixes pure QE with APE-based QE systems; we call the result F ULL S TACKED QE. The procedure is analogous to that described in §2.3, with extra binary features for the"
W17-4764,W16-2378,0,0.0493957,"e QE-tuning step when training the log-linear APE model; instead, we kept the output of the s → p and the t → p systems, converted each to word-level quality labels, and then include the two predictions as additional features in the F ULL S TACKED QE system, described below. We denote the individual systems as A PE QE s → p and A PE QE t → p, and the combined system as F ULL S TACKED QE s, t → p. APE-Based Quality Estimation To develop our APE-based QE system (A PE QE), we followed a similar approach as the one described in Martins et al. (2017), with a few minor differences, explained below. Junczys-Dowmunt and Grundkiewicz (2016) applied neural translation models to the APE problem, treating different models as components in a log-linear model, allowing for multiple inputs (the source s and the translated sentence t) that were decoded to the same target language (post-edited translation p). Two systems were considered, one using s as the input (s → p) and another using t as the input (t → p). For English-German, we used the 500K artificial roundtrip translations provided by the shared task organizers, along with the original data from the shared task (oversampled 20 times, as in Junczys-Dowmunt and Grundkiewicz (2016)"
W17-4764,P13-4014,0,0.250428,"Missing"
W17-4764,W15-3037,0,0.196558,"depend on the target word and its aligned source word, as well as the context surrounding them.2 We include also syntactic fea2 200 2 x 400 3 x 64 3 x 64 3 x 50 3 x 50 Neural System Next, we describe the neural component of our pure QE system, which we call N EURAL QE. The architecture of N EURAL QE is depicted in Figure 1. We used Keras (Chollet, 2015) to implement our model. The system receives as input the source and target sentences s and t, their word-level alignments A, and their corresponding POS tags obtained from TurboTagger. The input layer follows a similar architecture as QUETCH (Kreutzer et al., 2015), with the addition of POS features. A vector representing each target word is obtained by concatenating the embedding of that word with those of the aligned word in the source.3 The immediate left and right contexts for source and target words are also concatenated. We use w&gt; φu (s, t, A, yi ) w&gt; φb (s, t, A, yi , yi−1 ). 2 x 200 ... ... tures to detect grammatically incorrect constructions. We use features that involve the dependency relation, the head word, and second-order sibling and grandparent structures. Features involving part-of-speech (POS) tags and syntactic information are obtaine"
W17-4764,P13-2109,1,0.880685,"Missing"
W18-5450,J93-2003,0,0.0944868,"he hypothesis on the x-axis. Sequential alignment encourages monotonic alignments, matching induces a single symmetrical alignment. Constrained attention. Some forms of parsimony must be strictly enforced using constraints, rather than simply encouraged via regularization. One such constraint is to add an upper bound to the cumulative attention an input variable may receive. This can be done using constrained softmax (Martins and Kreutzer, 2017) or its sparse analogue, constrained sparsemax (Malaviya et al., 2018). Constraining attention weights can be interpreted as specifying the fertility (Brown et al., 1993) of the alignments between the source and target, in machine translation. Allowing hidden layers to output structured representations can be valuable for modelling perspective but also for interpretability: discrete structures provide organized representations, in contrast to unstructured vectors of neuron activations. SparseMAP (Niculae et al., 2018) allows handling discrete structures within end-to-end differentiable neural networks, able to automatically select only a few global structures. On natural language inference, for a word-to-word alignment joint attention mechanism, SparseMAP can"
W18-5450,D15-1166,0,0.127598,"Missing"
W18-5450,P18-2059,1,0.896787,"Missing"
W18-5450,D17-1036,1,0.825238,"bution of irrelevant words is now shrunk to exactly 0 (Fig. 1, right). (b) matching Figure 2: Structured alignment on SNLI (Niculae et al., 2018). The premise is on the y-axis, the hypothesis on the x-axis. Sequential alignment encourages monotonic alignments, matching induces a single symmetrical alignment. Constrained attention. Some forms of parsimony must be strictly enforced using constraints, rather than simply encouraged via regularization. One such constraint is to add an upper bound to the cumulative attention an input variable may receive. This can be done using constrained softmax (Martins and Kreutzer, 2017) or its sparse analogue, constrained sparsemax (Malaviya et al., 2018). Constraining attention weights can be interpreted as specifying the fertility (Brown et al., 1993) of the alignments between the source and target, in machine translation. Allowing hidden layers to output structured representations can be valuable for modelling perspective but also for interpretability: discrete structures provide organized representations, in contrast to unstructured vectors of neuron activations. SparseMAP (Niculae et al., 2018) allows handling discrete structures within end-to-end differentiable neural"
W18-6311,D17-1151,0,0.0188792,"nimum overall perplexity on the bilingual dev set. For the source context representations, we use the sentence representations generated by two sentence-level bidirectional RNNLMs (one each for English and Foreign) trained offline. For the target sentence representations, we use the last hidden states of the decoder generated from the pre-trained base model10 . At decoding time, however, we use the last hidden state of the decoder computed by our model (not the base) as the target sentence representations. Further training details are provided in Appendix B. 8 We follow Cohn et al. (2016) and Britz et al. (2017) in choosing hyperparameters for our model. 9 For each language-pair, we use BPE (Sennrich et al., 2016) to obtain a joint vocabulary of size ≈30k. 10 Even though the paramaters of the base model are updated, the target sentence representations are fixed throughout training. We experimented with a scheduled updating scheme in preliminary experiments but it did not yield significant improvement. Base Model Europarl Subtitles En-Fr En-Et En-De En-Ru Overall En→Fr Fr→En Overall En→Et Et→En Overall En→De De→En Overall En→Ru Ru→En 37.36 38.13 36.03 20.68 18.64 26.65 24.74 21.80 27.74 19.05 14.90 23"
W18-6311,W14-4012,0,0.150826,"Missing"
W18-6311,P11-2031,0,0.0694498,"Src-Tgt-Mix 38.76† 39.24† 39.57† 39.51† 39.52† 37.06† 37.35† 37.50† 37.43† 21.84† 21.77† 21.74† 21.68† 19.58† 19.68† 19.60† 19.63† 28.43† 27.86† 27.98† 27.71† 26.49† 26.21† 26.39† 26.37† 23.49† 23.16† 23.28† 23.26† 29.49† 29.26† 29.50† 29.48† 19.09 19.23 18.89 19.26 14.59 14.77 14.52 14.86 22.98 23.23 23.06 23.01 Table 2: BLEU scores for the bilingual test sets. Here all contexts are incorporated as InitDec for Europarl and InitDec+AddDec for Subtitles unless otherwise specified. bold: Best performance, †: Statistically significantly better than the base model, based on bootstrap resampling (Clark et al., 2011) with p &lt; 0.05. 5.1 Results just the turn-level one. Finally, our results with source, target and dual contexts are reported. Interestingly, just using the source context is sufficient for English-Estonian and English-German. For English-French, on the other hand, we see significant improvements for the models using the target-side conversation history over using only the source-side. We attribute this to the base model being more efficient and able to generate better translations for En-Fr as it had been trained on a larger corpus as opposed to the other two language-pairs. Unlike Europarl, f"
W18-6311,2005.mtsummit-papers.11,0,0.0848971,"significant improvements over all baselines when translating subtitles and comparable performance for news and TED talks. Along similar lines, Kuang et al. (2018) propose dynamic and topic caches to capture contextual information either from recently translated sentences or the entire document to model coherence for NMT. Voita et al. (2018) introduce a context3.2 Data Standard machine translation datasets are inappropriate for Bi-MSMT task since they are not composed of conversations or the speaker annotations are missing. In this section, we describe how we extract data from raw Europarl v7 (Koehn, 2005) and OpenSubtitles20161 (Lison and Tiedemann, 2016) for this task2 . Europarl The raw Europarl v7 corpus (Koehn, 2005) contains SPEAKER and LANGUAGE tags where the latter indicates the language the speaker was actually using. The individual files are first split into conversations. The data is tokenised (using scripts by Koehn (2005)), and cleaned (headings and single token sentences removed). Conversations are divided into smaller ones if the number of speakers is greater than 5.3 The corpus is then randomly split into train/dev/test sets with respect to conversations in ratio 100:2:3. The En"
W18-6311,C18-1050,0,0.0491435,"roposed by Jean et al. (2017); Wang et al. (2017). Apart from being difficult to scale, they report deteriorated BLEU scores when using the target-side context. Tu et al. (2017) augment the vanilla NMT model with a continuous cache-like memory, along the same lines as the cache-based system for traditional document MT (Gong et al., 2011), which stores hidden representations of recently generated words as translation history. The proposed approach shows significant improvements over all baselines when translating subtitles and comparable performance for news and TED talks. Along similar lines, Kuang et al. (2018) propose dynamic and topic caches to capture contextual information either from recently translated sentences or the entire document to model coherence for NMT. Voita et al. (2018) introduce a context3.2 Data Standard machine translation datasets are inappropriate for Bi-MSMT task since they are not composed of conversations or the speaker annotations are missing. In this section, we describe how we extract data from raw Europarl v7 (Koehn, 2005) and OpenSubtitles20161 (Lison and Tiedemann, 2016) for this task2 . Europarl The raw Europarl v7 corpus (Koehn, 2005) contains SPEAKER and LANGUAGE t"
W18-6311,N16-1102,1,0.867266,"n training set D. For example, for a conversation having alternating turns of English and Foreign language, the log-likelihood is: |T | −1 2 X |t2k+1 X| k=0 i=1 |t2k+2 | log Pθ (yi |xi , oi ) + X j=1 log Pθ (xj |yj , oj )  where i, j denote sentences belonging to 2k + 1th or 2k + 2th turn; o(.) is a representation of the conversation history, and |T |is the total number of turns (assumed to be even here). 106 Experiments Implementation and Hyperparameters We implement our conversational Bi-MSMT model in C++ using the DyNet library (Neubig et al., 2017). The base model is built using mantis (Cohn et al., 2016) which is an implementation of the generic sentence-level NMT model using DyNet. The base model has single layer bidirectional GRUs in the encoder and 2-layer GRU in the decoder8 . The hidden dimensions and word embedding sizes are set to 256, and the alignment dimension (for the attention mechanism in the decoder) is set to 128. Models and Training We do a stage-wise training for the base model, i.e., we first train the English→Foreign architecture and the Foreign→English architecture, using the sentence-level parallel corpus. Both architectures have the same vocabulary9 but separate paramete"
W18-6311,L16-1147,0,0.024601,"elines when translating subtitles and comparable performance for news and TED talks. Along similar lines, Kuang et al. (2018) propose dynamic and topic caches to capture contextual information either from recently translated sentences or the entire document to model coherence for NMT. Voita et al. (2018) introduce a context3.2 Data Standard machine translation datasets are inappropriate for Bi-MSMT task since they are not composed of conversations or the speaker annotations are missing. In this section, we describe how we extract data from raw Europarl v7 (Koehn, 2005) and OpenSubtitles20161 (Lison and Tiedemann, 2016) for this task2 . Europarl The raw Europarl v7 corpus (Koehn, 2005) contains SPEAKER and LANGUAGE tags where the latter indicates the language the speaker was actually using. The individual files are first split into conversations. The data is tokenised (using scripts by Koehn (2005)), and cleaned (headings and single token sentences removed). Conversations are divided into smaller ones if the number of speakers is greater than 5.3 The corpus is then randomly split into train/dev/test sets with respect to conversations in ratio 100:2:3. The English side of the corpus is set as reference, and 1"
W18-6311,P18-1118,1,0.908436,"on, e.g. metadata, and Tran et al. (2016) use inter-document context to boost the performance 101 Proceedings of the Third Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 101–112 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64011 aware NMT model in which they control and analyse the flow of information from the extended context to the translation model. They show that using the previous sentence as context their model is able to implicitly capture anaphora. For the offline setting, Maruf and Haffari (2018) incorporate the global source and target document contexts into the base NMT model via memory networks. They report significant improvements using BLEU and METEOR for the contextual model over the baseline. To the best of our knowledge, there has been no work on Multi-Speaker MT or its variation to date. of RNN language models. For conversational language modelling, Ji and Bilmes (2004) propose a statistical multi-speaker language model (MSLM) that considers words from other speakers when predicting words from the current one. By taking the inter-speaker dependency into account using a normal"
W18-6311,D11-1084,0,0.121514,"generate a summary of three previous source sentences via a hierarchical RNN, which is then added as an auxiliary input to the decoder. Bawden et al. (2017) explore various ways to exploit context from the previous sentence on the source and target-side by extending the models proposed by Jean et al. (2017); Wang et al. (2017). Apart from being difficult to scale, they report deteriorated BLEU scores when using the target-side context. Tu et al. (2017) augment the vanilla NMT model with a continuous cache-like memory, along the same lines as the cache-based system for traditional document MT (Gong et al., 2011), which stores hidden representations of recently generated words as translation history. The proposed approach shows significant improvements over all baselines when translating subtitles and comparable performance for news and TED talks. Along similar lines, Kuang et al. (2018) propose dynamic and topic caches to capture contextual information either from recently translated sentences or the entire document to model coherence for NMT. Voita et al. (2018) introduce a context3.2 Data Standard machine translation datasets are inappropriate for Bi-MSMT task since they are not composed of convers"
W18-6311,2010.iwslt-papers.10,0,0.0683895,"odels. For conversational language modelling, Ji and Bilmes (2004) propose a statistical multi-speaker language model (MSLM) that considers words from other speakers when predicting words from the current one. By taking the inter-speaker dependency into account using a normal trigram context, they report significant reduction in perplexity. Statistical Machine Translation The few SMTbased attempts to document MT are either restrictive or do not lead to significant improvements upon automatic evaluation. Few of these deal with specific discourse phenomena, such as resolving anaphoric pronouns (Hardmeier and Federico, 2010) or lexical consistency of translations (Garcia et al., 2017). Others are based on a twopass approach i.e., to improve the translations already obtained by a sentence-level model (Hardmeier et al., 2012; Garcia et al., 2014). 3 Preliminaries 3.1 Problem Formulation We are given a dataset that comprises parallel conversations, and each conversation consists of turns. Each turn is constituted by sentences spoken by a single speaker, denoted by x or y, if the sentence is in English or Foreign language, respectively. The goal is to learn a model that is able to leverage the mixed-language conversa"
W18-6311,P16-1162,0,0.478312,"the source language is English, otherwise Foreign. The sentences in the source-side of the corpus are kept or swapped with those in the target-side based on this tag. We perform the aforementioned steps for English-French, English-Estonian and EnglishGerman, and obtain the bilingual multi-speaker corpora for the three language pairs. Before splitting into train/dev/test sets, we remove conversations with sentences having more than 100 tokens for English-French, English-German and more than 80 tokens for English-Estonian4 respectively, to limit the sentence-length for using subwords with BPE (Sennrich et al., 2016). The data statistics are given in Table 1 and Appendix A5 . Encoder It maps each source word xm to a distributed representation hm which is the concatenation of the corresponding hidden states of two RNNs running in opposite directions over the source sentence. The forward and backward RNNs are taken to be GRUs (gated-recurrent unit; Cho et al. (2014)) in this work. Decoder The generation of each target word yn is conditioned on all the previously generated words y&lt;n via the state sn of the decoder, and the source sentence via a dynamic context vector cn : yn un sn Subtitles There has been re"
W18-6311,D12-1108,0,0.0178585,"By taking the inter-speaker dependency into account using a normal trigram context, they report significant reduction in perplexity. Statistical Machine Translation The few SMTbased attempts to document MT are either restrictive or do not lead to significant improvements upon automatic evaluation. Few of these deal with specific discourse phenomena, such as resolving anaphoric pronouns (Hardmeier and Federico, 2010) or lexical consistency of translations (Garcia et al., 2017). Others are based on a twopass approach i.e., to improve the translations already obtained by a sentence-level model (Hardmeier et al., 2012; Garcia et al., 2014). 3 Preliminaries 3.1 Problem Formulation We are given a dataset that comprises parallel conversations, and each conversation consists of turns. Each turn is constituted by sentences spoken by a single speaker, denoted by x or y, if the sentence is in English or Foreign language, respectively. The goal is to learn a model that is able to leverage the mixed-language conversation history in order to produce high quality translations. Neural Machine Translation Using contextbased neural models for improving online and offline NMT is a popular trend recently. Jean et al. (201"
W18-6311,N16-1090,1,0.798072,"ng the discourse or document context to improve NMT, in an online setting, by using the past context (Jean et al., 2017; Wang et al., 2017; Bawden et al., 2017; Voita 2 Related Work Our research builds upon prior work in the field of context-based language modelling and contextbased machine translation. Language Modelling There have been few works on leveraging context information for language modelling. Ji et al. (2015) introduced Document Context Language Model (DCLM) which incorporates inter and intra-sentential contexts. Hoang et al. (2016) make use of side information, e.g. metadata, and Tran et al. (2016) use inter-document context to boost the performance 101 Proceedings of the Third Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 101–112 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64011 aware NMT model in which they control and analyse the flow of information from the extended context to the translation model. They show that using the previous sentence as context their model is able to implicitly capture anaphora. For the offline setting, Maruf and Haffari (2018) incorporate the"
W18-6311,N16-1149,1,0.858868,"ored in the literature. Recently, there has been work focusing on using the discourse or document context to improve NMT, in an online setting, by using the past context (Jean et al., 2017; Wang et al., 2017; Bawden et al., 2017; Voita 2 Related Work Our research builds upon prior work in the field of context-based language modelling and contextbased machine translation. Language Modelling There have been few works on leveraging context information for language modelling. Ji et al. (2015) introduced Document Context Language Model (DCLM) which incorporates inter and intra-sentential contexts. Hoang et al. (2016) make use of side information, e.g. metadata, and Tran et al. (2016) use inter-document context to boost the performance 101 Proceedings of the Third Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 101–112 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64011 aware NMT model in which they control and analyse the flow of information from the extended context to the translation model. They show that using the previous sentence as context their model is able to implicitly capture anaphora"
W18-6311,P18-1117,0,0.0617836,"gment the vanilla NMT model with a continuous cache-like memory, along the same lines as the cache-based system for traditional document MT (Gong et al., 2011), which stores hidden representations of recently generated words as translation history. The proposed approach shows significant improvements over all baselines when translating subtitles and comparable performance for news and TED talks. Along similar lines, Kuang et al. (2018) propose dynamic and topic caches to capture contextual information either from recently translated sentences or the entire document to model coherence for NMT. Voita et al. (2018) introduce a context3.2 Data Standard machine translation datasets are inappropriate for Bi-MSMT task since they are not composed of conversations or the speaker annotations are missing. In this section, we describe how we extract data from raw Europarl v7 (Koehn, 2005) and OpenSubtitles20161 (Lison and Tiedemann, 2016) for this task2 . Europarl The raw Europarl v7 corpus (Koehn, 2005) contains SPEAKER and LANGUAGE tags where the latter indicates the language the speaker was actually using. The individual files are first split into conversations. The data is tokenised (using scripts by Koehn ("
W18-6311,N04-4034,0,0.0768255,"he flow of information from the extended context to the translation model. They show that using the previous sentence as context their model is able to implicitly capture anaphora. For the offline setting, Maruf and Haffari (2018) incorporate the global source and target document contexts into the base NMT model via memory networks. They report significant improvements using BLEU and METEOR for the contextual model over the baseline. To the best of our knowledge, there has been no work on Multi-Speaker MT or its variation to date. of RNN language models. For conversational language modelling, Ji and Bilmes (2004) propose a statistical multi-speaker language model (MSLM) that considers words from other speakers when predicting words from the current one. By taking the inter-speaker dependency into account using a normal trigram context, they report significant reduction in perplexity. Statistical Machine Translation The few SMTbased attempts to document MT are either restrictive or do not lead to significant improvements upon automatic evaluation. Few of these deal with specific discourse phenomena, such as resolving anaphoric pronouns (Hardmeier and Federico, 2010) or lexical consistency of translatio"
W18-6311,D17-1301,0,0.191981,"urce and target languages. We investigate neural architectures that exploit the bilingual conversation history for this scenario, which is a challenging problem as the history consists of utterances in both languages. The ultimate aim of all machine translation systems for dialogue is to enable a multi-lingual conversation between multiple speakers. However, translation of such conversations is not wellexplored in the literature. Recently, there has been work focusing on using the discourse or document context to improve NMT, in an online setting, by using the past context (Jean et al., 2017; Wang et al., 2017; Bawden et al., 2017; Voita 2 Related Work Our research builds upon prior work in the field of context-based language modelling and contextbased machine translation. Language Modelling There have been few works on leveraging context information for language modelling. Ji et al. (2015) introduced Document Context Language Model (DCLM) which incorporates inter and intra-sentential contexts. Hoang et al. (2016) make use of side information, e.g. metadata, and Tran et al. (2016) use inter-document context to boost the performance 101 Proceedings of the Third Conference on Machine Translation (WMT"
W18-6311,L16-1436,0,0.297821,"enation of the corresponding hidden states of two RNNs running in opposite directions over the source sentence. The forward and backward RNNs are taken to be GRUs (gated-recurrent unit; Cho et al. (2014)) in this work. Decoder The generation of each target word yn is conditioned on all the previously generated words y&lt;n via the state sn of the decoder, and the source sentence via a dynamic context vector cn : yn un sn Subtitles There has been recent work to obtain speaker labels via automatic turn segmentation for the OpenSubtitles2016 corpus (Lison and Meena, 2016; van der Wees et al., 2016; Wang et al., 2016). We obtain the English side of OpenSubtitles2016 corpus annotated with speaker information by Lison and Meena (2016).6 To obtain the parallel corpus, we use the OpenSubtitles alignment links to align foreign subtitles to the annotated English ones. For each subtitle, we extract individual conversations with more than 5 sentences and at least two turns. Conversations with more than 30 turns are discarded. Finally, since subtitles are in a single language, we assign language tag such that the same language occurs in alternating turns. We thus obtain the Bi-MSMT corpus for English-Russian, which"
W18-6311,C16-1242,0,0.0526471,"Missing"
W18-6451,W18-6458,0,0.0410522,"Missing"
W18-6451,W13-2242,0,0.025847,"suring the closeness of the test sentences to the training data, the difficulty of translating them, and to identify translation acts between any two data sets for building prediction models. Task-specific quality prediction RTM models are built using the WMT News translation task corpora, taking MT models as a black-box and predicting translation scores independently on the MT model. Multiple machine learning techniques are used and averaged based on their training set performance for label prediction. For sequence classification tasks (T2 and T3), Global Linear Models with dynamic learning (Bicici, 2013) are used. via simple arithmetic means on rescaled values, i.e., no machine learning is used. Since it is unsupervised, the method can only be meaningfully evaluated on the ranking task. sMQE uses the same two features as uMQE, but with supervision. A Support Vector Regressor based on these two features is trained on the available data and used to predict QE scores. QEbrain (T1, T2): QE brain uses a conditional target language model as a robust feature extractor with a novel bidirectional transformer which is pretrained on a large parallel corpus filtered to contain “in-domain like” sentences."
W18-6451,aziz-etal-2012-pet,1,0.863758,"Missing"
W18-6451,W18-6457,0,0.051013,"Missing"
W18-6451,W17-4759,0,0.063973,"Missing"
W18-6451,W15-3037,0,0.0394885,"per source word in the phrase as given by the IBM model 1 extracted using the SMT parallel corpus (with different translation probability thresholds: 0.01, 0.05, 0.1, 0.2, 0.5). – average number of translations per source word in the phrase as given by the IBM model 1 extracted using the SMT parallel corpus (with different translation probability thresholds: 0.01, 0.05, 0.1, 0.2, 0.5) weighted by the frequency of each word in the source side of the parallel SMT corpus. In addition to that, six new features were included which contain combinations of other features, and which proved useful in (Kreutzer et al., 2015; Martins et al., 2016): Target word + left context. Target word + right context. Target word + aligned source word. POS of target word + POS of aligned source word. • Target word + left context + source word. • Target word + right context + source word. • • • • The baseline system models the task as a sequence prediction problem using the LinearChain Conditional Random Fields (CRF) algorithm within the CRFSuite tool.3 The model was trained using passive-aggressive optimisation algorithm. We note that this baseline system was only used to predict OK/BAD classes for existing words in the MT out"
W18-6451,W12-3102,1,0.84705,"Missing"
W18-6451,W18-6460,0,0.0606221,"Missing"
W18-6451,N13-1073,0,0.0490344,"ing the TERCOM tool. Default settings were used and shifts were disabled. Target tokens originating from insertion or substitution errors were labeled as BAD. All other tokens were labeled as OK. Gap and source word labels To annotate deletion errors, gap ‘tokens’ between each word and at the beginning of each target sentence were introduced. These gaps tokens were labeled as BAD in the presence of one or more deletion errors and OK otherwise. To annotate the source words related to insertion or substitution errors in the machine translated sentence, the IBM Model 2 alignments from fastalign (Dyer et al., 2013) were used. Each token in the source sentence was aligned to the post-edited sentence. For each token in the 12 https://github.com/Unbabel/ word-level-qe-corpus-builder 697 Pearson r SMT DATASET 0.74 • QEBrain DoubleBi w/ BPE+word-tok (ensemble) QEBrain DoubleBi w/ BPE-tok 0.73 UNQE 0.70 TSKQE2 0.49 SHEF-PT 0.49 TSKQE1 0.48 0.43 UTartu/QuEst+Attention UTartu/QuEst+Att+CrEmb3 0.42 sMQE 0.40 RTM MIX7 0.39 RTM MIX6 0.39 SHEF-bRNN 0.37 BASELINE 0.37 uMQE – UAlacant** 0.39 NMT DATASET • UNQE 0.51 • QEBrain DoubleBi w/ BPE+word-tok (ensemble) 0.50 • QEBrain DoubleBi w/ word-tok 0.50 0.42 TSKQE1 TSKQ"
W18-6451,L16-1582,1,0.867516,"rch with 5-fold cross validation on the training set, resulting in γ=0.01,  = 0.0825, C = 20. This baseline system has been consistently used as the baseline system for all editions of the sentence-level task (Callison-Burch et al., 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017), and has proved strong enough for predicting various forms of post-editing effort across a range of language pairs and text domains for statistical MT systems. This year it is also benchmarked on neural MT outputs. Word-level baseline system: For Task 2, the baseline features were extracted with the M AR MOT tool (Logacheva et al., 2016). These are 28 features that have been deemed the most informative in previous research on word-level QE, mostly inspired by (Luong et al., 2014). This is the same baseline system used in WMT17: Baseline systems Sentence-level baseline system: For Task 1, Q U E ST ++1 (Specia et al., 2015) was used to extract 17 MT system-independent features from the source and translation (target) files and parallel corpora: • Word count in the source and target sentences, and source and target token count ratio. Although these features are sentencelevel (i.e. their values will be the same for all words in a"
W18-6451,W18-6461,0,0.0415047,"Missing"
W18-6451,W14-0301,0,0.0184819,"baseline system for all editions of the sentence-level task (Callison-Burch et al., 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017), and has proved strong enough for predicting various forms of post-editing effort across a range of language pairs and text domains for statistical MT systems. This year it is also benchmarked on neural MT outputs. Word-level baseline system: For Task 2, the baseline features were extracted with the M AR MOT tool (Logacheva et al., 2016). These are 28 features that have been deemed the most informative in previous research on word-level QE, mostly inspired by (Luong et al., 2014). This is the same baseline system used in WMT17: Baseline systems Sentence-level baseline system: For Task 1, Q U E ST ++1 (Specia et al., 2015) was used to extract 17 MT system-independent features from the source and translation (target) files and parallel corpora: • Word count in the source and target sentences, and source and target token count ratio. Although these features are sentencelevel (i.e. their values will be the same for all words in a sentence), the length of a sentence might influence the probability of a word being wrong. • Target token, its left and right contexts of one wo"
W18-6451,W18-6462,0,0.0661511,"Missing"
W18-6451,W16-2387,1,0.855333,"phrase as given by the IBM model 1 extracted using the SMT parallel corpus (with different translation probability thresholds: 0.01, 0.05, 0.1, 0.2, 0.5). – average number of translations per source word in the phrase as given by the IBM model 1 extracted using the SMT parallel corpus (with different translation probability thresholds: 0.01, 0.05, 0.1, 0.2, 0.5) weighted by the frequency of each word in the source side of the parallel SMT corpus. In addition to that, six new features were included which contain combinations of other features, and which proved useful in (Kreutzer et al., 2015; Martins et al., 2016): Target word + left context. Target word + right context. Target word + aligned source word. POS of target word + POS of aligned source word. • Target word + left context + source word. • Target word + right context + source word. • • • • The baseline system models the task as a sequence prediction problem using the LinearChain Conditional Random Fields (CRF) algorithm within the CRFSuite tool.3 The model was trained using passive-aggressive optimisation algorithm. We note that this baseline system was only used to predict OK/BAD classes for existing words in the MT output. No baseline system"
W18-6451,C18-1266,1,0.718016,"Missing"
W18-6451,W18-6463,1,0.879266,"Missing"
W18-6451,W18-6464,0,0.0250119,"Missing"
W18-6451,W17-4763,0,0.0612296,"ter the feature extraction model is trained, the features are extracted and combined with human-crafted features from the QE baseline system and fed into a Bi-LSTM predictive model for QE. A greedy ensemble selection method is used to decrease the individual model errors and increase model diversity. The bi-LSTM QE model is trained on the official QE data plus artificially generated data and fine-tuned with only the official WMT18 QE data. SHEF (T1, T2, T3, T4): SHEF submitted two systems per task variant: SHEF-PT and SHEF-bRNN. SHEFPT is based on a re-implementation of the POSTECH system of (Kim et al., 2017), SHEF-bRNN uses a bidirectional recurrent neural network (bRNN) (Ive et al., 2018a). PT systems are pre-trained using in-domain corpora provided by the organisers. bRNN systems uses two encoders to learn representations of &lt;source, MT> sentence pairs. These representations are used directly to make word-level predictions. A weighted sum over word representations as defined by an attention mechanism is used to make sentence-level predictions. For phrase-level, RTM (T1, T2, T3, T4): 693 a standard attention-based neural MT architecture is used. Different parts of the source sentence are attende"
W18-6451,P15-4020,1,0.85056,"proved strong enough for predicting various forms of post-editing effort across a range of language pairs and text domains for statistical MT systems. This year it is also benchmarked on neural MT outputs. Word-level baseline system: For Task 2, the baseline features were extracted with the M AR MOT tool (Logacheva et al., 2016). These are 28 features that have been deemed the most informative in previous research on word-level QE, mostly inspired by (Luong et al., 2014). This is the same baseline system used in WMT17: Baseline systems Sentence-level baseline system: For Task 1, Q U E ST ++1 (Specia et al., 2015) was used to extract 17 MT system-independent features from the source and translation (target) files and parallel corpora: • Word count in the source and target sentences, and source and target token count ratio. Although these features are sentencelevel (i.e. their values will be the same for all words in a sentence), the length of a sentence might influence the probability of a word being wrong. • Target token, its left and right contexts of one word. • Source word aligned to the target token, its left and right contexts of one word. The alignments were given by the SMT system that produced"
W18-6451,W18-6465,0,0.0844531,"Missing"
W18-6451,W18-6466,0,0.0668061,"Missing"
W18-6451,C00-2137,0,0.065581,"ed or substituted in the machine translated text, the corresponding aligned source tokens were labeled as BAD. In this way, deletion errors also result in BAD tokens in the source, related to the missing words. All other words were labeled as OK. Evaluation Analogously to last year’s task, the primary evaluation metric is the multiplication of F1 -scores for the OK and BAD classes, denoted as F1 -Mult. The same metric was applied to gap and source token labels. We also report F1 -scores for individual classes for completeness. We test the significance of the results using randomisation tests (Yeh, 2000) with Bonferroni correction (Abdi, 2007). Task 2: Predicting word-level quality This task evaluates the extent to which we can detect word-level errors in MT output. Often the overall quality of a translated segment is significantly harmed by specific errors in a small number of words. As in previous years, each token of the target sentence is labeled as OK/BAD based on an available post-edited sentence. In addition to this, this year we also took into consideration word omission errors and the detection of words in the source related to target side errors. These types of errors become particu"
W19-4207,P17-1183,0,0.0229486,"erts an arbitrary real-valued vector into a probability distribution. The critical difference is that sparsemax can assign exactly zero probability, whereas softmax is strictly positive. Sparsemax is differentiable almost everywhere and can be computed quickly, allowing its use as a drop-in replacement for softmax. It has previously been used in seq2seq for computing both attention weights (Malaviya et al., 2018) and output probabilities (Peters et al., 2019). Sparse attention is attractive in morphological inflection because it resembles hard attention, which has been successful on the task (Aharoni and Goldberg, 2017; Wu et al., 2018). 2.2 1. Alignment Compute a vector a ∈ RJ of alignment scores between st and H. We use the general attention scorer (Luong et al., 2015), which computes aj := s&gt; t Wa hj . 2. Context Compute the context vector PJ ct as a weighted sum of H: ct := j=1 πj hj , where π = sparsemax(a) is a sparse vector of alignment scores in the simplex. In Luong et al.’s single-headed attention, the attentional hidden state s˜t = tanh(Ws [ct ; st ]) is computed by a concatenation layer from the context vector and pre-attention hidden state. However, our two-headed attention mechanism produces t"
W19-4207,P19-1146,1,0.930749,"lectional tags are not relevant. The sparse gate allows the model to learn to shift focus between the two attentions while ignoring the other at a given time step. Introduction Morphological inflection is the task of producing an inflected form, given a lemma and a set of inflectional tags. A widespread approach to the task is the attention-based sequence-to-sequence model (seq2seq; Bahdanau et al., 2015; Kann and Sch¨utze, 2016); such models perform well but are difficult to interpret. To mitigate this shortcoming, we employ an alternative architecture which combines sparse seq2seq modeling (Peters et al., 2019) with two-headed attention that attends sep´ arately to the lemma and inflectional tags (Acs, 2018). The attention and output distributions are computed with the sparsemax function and models are trained to minimize sparsemax loss (Martins and Astudillo, 2016). Sparsemax, unlike softmax, can assign exactly zero attention weight to irrelevant source tokens and exactly zero probability to implausible hypotheses. We apply our models to Task 1 at the SIGMORPHON 2019 Shared Task (McCarthy et al., 2019), which extends morphological inflection to a cross-lingual setting. We present two sparse seq2seq"
W19-4207,K18-3001,0,0.0547614,"ken representations into dense embeddings. To account for the bilingual nature of Task 1, each of our embedding layers contains two look-up tables: one for the sequence of input tokens, and the other for the language of the sequence. At each time step, the current token’s embedding is concatenated to a language embedding.1 Each encoder and decoder uses a separate embedding layer; no weights are tied. Characters and inflectional tags use embeddings of size Dc , while language embeddings are of size D` . Thus the total embedding size is D = Dc + D` . D OUBLE ATTN uses the same strategy for com´ (2018): the bining multiple context vectors as Acs lemma and inflection context vectors ut and vt and the target hidden state st are inputs to a concatenation layer: s˜t = tanh(Wd [ut ; vt ; st ]) (2) where Wd ∈ RD×3D . Encoders The lemma and inflection encoders are both bidirectional LSTMs (Graves and Schmidhuber, 2005). An encoder’s forward and backward hidden states are concatenated, forming a sequence of source hidden states. We set the size of these hidden states as D in all experiments. G ATEDATTN, on the other hand, computes separate candidate attentional hidden states for the two context vec"
W19-4207,D18-1473,0,0.0217592,"d vector into a probability distribution. The critical difference is that sparsemax can assign exactly zero probability, whereas softmax is strictly positive. Sparsemax is differentiable almost everywhere and can be computed quickly, allowing its use as a drop-in replacement for softmax. It has previously been used in seq2seq for computing both attention weights (Malaviya et al., 2018) and output probabilities (Peters et al., 2019). Sparse attention is attractive in morphological inflection because it resembles hard attention, which has been successful on the task (Aharoni and Goldberg, 2017; Wu et al., 2018). 2.2 1. Alignment Compute a vector a ∈ RJ of alignment scores between st and H. We use the general attention scorer (Luong et al., 2015), which computes aj := s&gt; t Wa hj . 2. Context Compute the context vector PJ ct as a weighted sum of H: ct := j=1 πj hj , where π = sparsemax(a) is a sparse vector of alignment scores in the simplex. In Luong et al.’s single-headed attention, the attentional hidden state s˜t = tanh(Ws [ct ; st ]) is computed by a concatenation layer from the context vector and pre-attention hidden state. However, our two-headed attention mechanism produces two context vectors"
W19-4207,P19-1148,0,0.174631,"Missing"
W19-4207,P82-1020,0,0.819656,"Missing"
W19-4207,P16-2090,0,0.066887,"Missing"
W19-4207,P17-4012,0,0.0342669,"tch size of 64. We used the Adam optimizer (Kingma and Ba, 2015) with a learning rate of 10−3 , which was halved when validation accuracy failed to improve for three consecutive epochs. We tuned the dropout and the number of inflection encoder layers on a separate grid for each language pair. Our hyperparameter ranges are shown in Table 2. At test time, we decoded with a beam size of 5. We oversampled the low resource training data 100 times and did not use synthetic data or filter the corpora. We implemented our models in PyTorch (Paszke et al., 2017) with a codebase derived from OpenNMT-py (Klein et al., 2017).2 We then stack the two candidate states s˜ut and s˜vt ˜t ∈ R2×D and use the gate weights into a matrix S to compute s˜t as a weighted sum of them: ˜ s˜t = pt S 180 180 20 200 2 {1, 2} {0.3, 0.4, 0.5} Table 2: Hyperparameters for all models. Bracketed values were tuned individually for each language pair. use a sparse gate to compute weights pt ∈ 42 for the two candidate states: pt = sparsemax(Wg [ut ; vt ; st ] + bg ) Value 4 Analysis Next we interpret our models’ behavior on a selection of language pairs from Task 1. 4.1 Sparse Attention Table 3 shows the sparsity of our attention mechanism"
W19-4207,D15-1166,0,0.328246,"on the shared task metrics, showing that the models “know what they know”. 2 Models Our architecture is mostly the same as a standard RNN-based seq2seq model with attention. In this section, we outline the changes needed to extend this model to use sparsemax and two-headed attention in a multilingual setting. • D OUBLE ATTN (it-ist-01-1) is a reimplementation of the two-headed attention 50 Proceedings of the 16th Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 50–56 c Florence, Italy. August 2, 2019 2019 Association for Computational Linguistics 2.1 feeding (Luong et al., 2015). At time step t, it computes a hidden state st ∈ RD . Conditioned on st and the hidden state sequences from the lemma and inflection encoders, a two-headed attention mechanism then computes an attentional hidden state s˜t ∈ RD . The decoder LSTM is initialized only with the lemma encoder’s state. Sparsemax Our models’ sparsity comes from the sparsemax function (Martins and Astudillo, 2016), which computes the Euclidean projection of a vector z ∈ Rn onto the n–dimensional probability simplex 4n := {p ∈ Rn : p ≥ 0, 1&gt; p = 1}: sparsemax(z) := argmin kp − zk2 Attention head At time t, an attentio"
W19-4207,P18-2059,1,0.843142,"d computes a context vector ct ∈ RD conditioned on the decoder state st and an encoder state sequence H = [h1 , . . . , hJ ]. A head consists of two modular components: (1) p∈4n Like softmax, sparsemax converts an arbitrary real-valued vector into a probability distribution. The critical difference is that sparsemax can assign exactly zero probability, whereas softmax is strictly positive. Sparsemax is differentiable almost everywhere and can be computed quickly, allowing its use as a drop-in replacement for softmax. It has previously been used in seq2seq for computing both attention weights (Malaviya et al., 2018) and output probabilities (Peters et al., 2019). Sparse attention is attractive in morphological inflection because it resembles hard attention, which has been successful on the task (Aharoni and Goldberg, 2017; Wu et al., 2018). 2.2 1. Alignment Compute a vector a ∈ RJ of alignment scores between st and H. We use the general attention scorer (Luong et al., 2015), which computes aj := s&gt; t Wa hj . 2. Context Compute the context vector PJ ct as a weighted sum of H: ct := j=1 πj hj , where π = sparsemax(a) is a sparse vector of alignment scores in the simplex. In Luong et al.’s single-headed att"
W19-5401,D18-1274,0,0.107749,"ntence-level task. Their setup is similar to ETRI’s, but they pretrain a BiLSTM encoder to predict words in the target conditioned on the source. Then, a regressor is fed the concatenation of each encoded word vector in the target with the embeddings of its neighbours and a mismatch feature indicating the difference between the prediction score of the target word and the highest one in the vocabulary. 5.4 Unbabel Unbabel participated in Tasks 1 and 2 for all language pairs. Their submissions were built upon the OpenKiwi framework: they combined linear, neural, and predictor-estimator systems (Chollampatt and Ng, 2018) with new transfer learning approaches using BERT (Devlin et al., 2019) and XLM (Lample and Conneau, 2019) pre-trained models. They proposed new ensemble techniques for word and sentence-level predictions. For Task 2, they combined a predictor-estimator for wordlevel predictions with a simple technique for converting word labels into document-level predictions. 5.5 UTartu UTartu participated in the sentence-level track of task 1 and in task 3. They combined BERT (Devlin et al., 2019) and LASER (Artetxe and Schwenk, 2018) embeddings to train a regression neural network model. The output objecti"
W19-5401,N19-1423,0,0.330155,"y describe their strategies and which sub-tasks they participated in. 5.1 MIPT MIPT only participated in the word-level ENDE task. They used a BiLSTM, BERT and a baseline hand designed-feature extractor to generate word representations, followed by Conditional Random Fields (CRF) to output token labels. Their BiLISTM did not have any pretraining, unlike BERT, and combined the source and target vectors using a global attention mechanism. Their submitted runs combining the baseline features with the BiLSTM and with BERT. 5.2 ETRI ETRI participated in Task 1 only. They pretrained bilingual BERT (Devlin et al., 2019) models (one for EN-RU and another for EN-DE), and then finetuned them to predict all the outputs for each language pair, using different output weight matrices for each subtask (predicting source tags, target word tags, target gap tags, and the HTER score). Training the same model for both subtasks effectively enhanced the amount of training data. 5.3 CMU CMU participated only in the sentence-level task. Their setup is similar to ETRI’s, but they pretrain a BiLSTM encoder to predict words in the target conditioned on the source. Then, a regressor is fed the concatenation of each encoded word"
W19-5401,N13-1073,0,0.0598994,"ing and another in the end of the sentence. Words correctly aligned with the source are tagged as OK, and BAD otherwise. If one or more words are missing in the translation, the gap where they should have been is tagged as BAD, and OK otherwise. As in previous years, in order to obtain word level labels, first both the machine translated sentence and the source sentence are aligned with the post-edited version. Machine translation and post-edited pairs are aligned using the TERCOM tool (https://github. com/jhclark/tercom);2 source and postedited use the IBM Model 2 alignments from fast align (Dyer et al., 2013). Target word and gap labels Target tokens originating from insertion or substitution errors were labeled as BAD (i.e., tokens absent in the postedit sentence), and all other tokens were labeled as OK. Similarly to last year, we interleave these target word labels with gap labels: gaps were labeled as BAD in the presence of one or more deletion errors (i.e., a word from the source missing in the translation) and OK otherwise. Source word labels For each token in the postedited sentence deleted or substituted in the machine translated text, the corresponding aligned 1 This is true for tasks 1 a"
W19-5401,C18-1266,0,0.0919623,"ion task data, and to train the estimator, the WMT 2019 sentence level QE task data. 5.9 DCU DCU submitted two unsupervised metrics to task 3, both based on the IBM1 word alignment model. The main idea is to align the source and hypothesis using a model trained on a parallel corpus, and then use the average alignment strength (average word pair probabilities) as the metric. The varieties and other details are described in (Popovi´c et al., 2011). 5.10 USFD The two Sheffield submissions to the task 3 are based on the BiRNN sentence-level QE model from the deepQuest toolkit for neural-based QE (Ive et al., 2018). The BiRNN model uses two bi-directional recurrent neural networks (RNNs) as encoders to learn the representation of a ¡source,translation¿ sentence pair. The two encoders are trained independently from each other, before being combined as the weighted sum of the two sentence representations, using an attention mechanism. The first variant of our submission, ’USFD’, is a BiRNN model trained on Direct Assessment data from WMT’18. In this setting, the DA score is used as a sentence-level quality label. The second variant, ’USFD-TL’, is a BiRNN model previously trained on submissions to the WMT"
W19-5401,P19-3020,1,0.848097,"Missing"
W19-5401,W19-5358,0,0.0834842,"t variant of our submission, ’USFD’, is a BiRNN model trained on Direct Assessment data from WMT’18. In this setting, the DA score is used as a sentence-level quality label. The second variant, ’USFD-TL’, is a BiRNN model previously trained on submissions to the WMT News task from 2011 to 2017, with sent-BLEU as a quality label. We only considered the best performing submission, as well as one of the worst performing one. The model is then adapted to the downstream task of predicting DA score, using a transfer learning and fine-tuning approach. 5.11 NRC-CNRC The submissions from NRC-CNRC (kiu Lo, 2019) included two metrics submitted to task 3. They constitute a unified automatic semantic machine translation quality evaluation and estimation metric for languages with different levels of available resources. They use BERT (Devlin et al., 2019) and semantic role-labelling as additional sources of information. 6 Results The results for Task 1 are shown in Tables 4, 5, 6 and 7. Systems are ranked according to their F1 on the target side. The evaluation scripts are available at https://github. com/deep-spin/qe-evaluation. We computed the statistical significance of the results, and considered as"
W19-5401,P15-4020,0,0.123226,"ps inside an error annotation are given BAD tags, and all others are given OK. Then, we train the same wordlevel estimator as in the baseline for Task 1. At test time, for the fine-grained subtask, we group consecutive BAD tags produced by the word-level baseline in a single error annotation and always give it severity major (the most common in the training data). As such, the baseline only produces error annotations with a single error span. For the MQM score, we consider the ratio of bad tags to the document size: nbad (3) n This simple baseline contrasts with last year, which used QuEst++ (Specia et al., 2015), a QE tool based on training an SVR on features extracted from the data. We found that the new baseline performed better than QuEst++ on the development data, and thus adopted it as the official baseline. MQM = 1 − have an HTER of 0, this is not always the case. When preprocessing the shared task data, word-level tags were determined in a case-sensitive fashion, while sentence-level scores were not. The same issue also happened last year, but unfortunately we only noticed it after releasing the training data for this edition. 4.4 QE as a Metric The QE as a metric task included two baselines,"
W19-5401,W19-5410,1,0.843237,"Missing"
W19-5401,C00-2137,0,0.130863,"or languages with different levels of available resources. They use BERT (Devlin et al., 2019) and semantic role-labelling as additional sources of information. 6 Results The results for Task 1 are shown in Tables 4, 5, 6 and 7. Systems are ranked according to their F1 on the target side. The evaluation scripts are available at https://github. com/deep-spin/qe-evaluation. We computed the statistical significance of the results, and considered as winning systems the ones which had significantly better scores than all the rest with p &lt; 0.05. For the word-level task, we used randomization tests (Yeh, 2000) with Bonferroni correction6 (Abdi, 2007); for Pearson correlation scores used in the sentence-level and MQM scoring tasks, we used William’s test7 . In the word-level task, there is a big gap between Unbabel’s winning submission and ETRI’s, which in turn also had significantly better results than MIPT and BOUN. Unfortunately, we cannot do a direct comparison with last year’s results, since i) we now evaluate a single score for target words and gaps, which were evaluated separately before, and ii) only two systems submitted results for source words last year. The newly proposed metric, MCC, is"
W19-5406,P17-4012,0,0.312747,"which consists of two modules: • The En-De QE corpus provided by the shared task, consisting of 13,442 train triplets. • The En-Ru QE corpus provided by the shared task, consisting of 15,089 train triplets. • a predictor, which is trained to predict each token of the target sentence given the source and the left and right context of the target sentence; • The En-De parallel dataset of 3,396,364 sentences from the IT domain provided by the shared task organizers. which we extend in the style of the eSCAPE corpus to contain artificial triplets. To do this, we use OpenNMT with 5fold jackknifing (Klein et al., 2017) to obtain unbiased translations of the source sentences. • an estimator, which takes features produced by the predictor and uses them to classify each word as OK or BAD. Our predictor uses a biLSTM to encode the source, and two unidirectional LSTMs processing the target in left-to-right (LSTM-L2R) and right-to-left (LSTM-R2L) order. For each target token ti , the representations of its left and right context are concatenated and used as query to an attention module before a final softmax layer. It is trained on the large parallel corpora mentioned above. The estimator takes as input a sequenc"
W19-5406,W19-5413,1,0.881862,"ose a simple technique for converting word labels into document-level predictions. Overall, our submitted systems achieve the best results on all tracks and language pairs by a considerable margin. 1 • We extend OpenKiwi with a Transformer predictor-estimator model (Wang et al., 2018). • We apply transfer learning techniques, finetuning BERT (Devlin et al., 2018) and XLM (Lample and Conneau, 2019) models in a predictor-estimator architecture. • We incorporate predictions coming from the APE-BERT system described in Correia and Martins (2019), also used in this year’s Unbabel’s APE submission (Lopes et al., 2019). • We propose new ensembling techniques for combining word-level and sentence-level predictions, which outperform previously used stacking approaches (Martins et al., 2016). • We build upon our BERT-based predictorestimator model to obtain document-level annotation and MQM predictions via a simple wordto-annotation conversion scheme. Introduction Quality estimation (QE) is the task of evaluating a translation system’s quality without access to reference translations (Blatz et al., 2004; Specia et al., 2018). This paper describes the contribution of the Unbabel team to the Shared Task on Word,"
W19-5406,W13-3520,0,0.0322654,"r and than pass the result through a feed-forward layer that projects the result to a single unit. For jointly learning source tags, we take the source text embeddings, project them with a feed-forward layer, and then sum the MT tags output vectors that are aligned. The result is then passed through a feed-forward layer, a bi-GRU, two other feed-forward layers, and finally an output layer. The layer dimensions are the same as in the normal model. It is worth noting that N U QE is trained from scratch using only the shared task data, with no pre-trained components, besides Polyglot embeddings (Al-Rfou et al., 2013). (to account for context that needs to be inserted), and source words (to denote words in the original sentence that have been mistranslated or omitted in the target). The goal of the Sentence-level QE task, on the other hand, is to predict the quality of the whole translated sentence, based on how many edit operations are required to fix it, in terms of HTER (Human Translation Error Rate) (Specia et al., 2018). We next describe the datasets, resources, and models that we used for these tasks. 2.1 Datasets and Resources The data resources we use to train our systems are of three types: the QE"
W19-5406,P13-2109,1,0.862267,"Missing"
W19-5406,C04-1046,0,0.560838,"APE-BERT system described in Correia and Martins (2019), also used in this year’s Unbabel’s APE submission (Lopes et al., 2019). • We propose new ensembling techniques for combining word-level and sentence-level predictions, which outperform previously used stacking approaches (Martins et al., 2016). • We build upon our BERT-based predictorestimator model to obtain document-level annotation and MQM predictions via a simple wordto-annotation conversion scheme. Introduction Quality estimation (QE) is the task of evaluating a translation system’s quality without access to reference translations (Blatz et al., 2004; Specia et al., 2018). This paper describes the contribution of the Unbabel team to the Shared Task on Word, Sentence, and Document-Level (QE Tasks 1 and 2) at WMT 2019. Our system adapts OpenKiwi,1 a recently released open-source framework for QE that implements the best QE systems from WMT 201518 shared tasks (Martins et al., 2016, 2017; Kim et al., 2017; Wang et al., 2018), which we extend to leverage recently proposed pre-trained models 1 Our submitted systems achieve the best results on all tracks and all language pairs by a considerable margin: on English-Russian (En-Ru), our sentence-l"
W19-5406,P19-1292,1,0.918697,"e new ensemble techniques for word and sentence-level predictions. We also propose a simple technique for converting word labels into document-level predictions. Overall, our submitted systems achieve the best results on all tracks and language pairs by a considerable margin. 1 • We extend OpenKiwi with a Transformer predictor-estimator model (Wang et al., 2018). • We apply transfer learning techniques, finetuning BERT (Devlin et al., 2018) and XLM (Lample and Conneau, 2019) models in a predictor-estimator architecture. • We incorporate predictions coming from the APE-BERT system described in Correia and Martins (2019), also used in this year’s Unbabel’s APE submission (Lopes et al., 2019). • We propose new ensembling techniques for combining word-level and sentence-level predictions, which outperform previously used stacking approaches (Martins et al., 2016). • We build upon our BERT-based predictorestimator model to obtain document-level annotation and MQM predictions via a simple wordto-annotation conversion scheme. Introduction Quality estimation (QE) is the task of evaluating a translation system’s quality without access to reference translations (Blatz et al., 2004; Specia et al., 2018). This paper de"
W19-5406,L18-1004,0,0.29818,"een mistranslated or omitted in the target). The goal of the Sentence-level QE task, on the other hand, is to predict the quality of the whole translated sentence, based on how many edit operations are required to fix it, in terms of HTER (Human Translation Error Rate) (Specia et al., 2018). We next describe the datasets, resources, and models that we used for these tasks. 2.1 Datasets and Resources The data resources we use to train our systems are of three types: the QE shared task corpora, additional parallel corpora, and artificial triplets (src, pe, mt) in the style of the eSCAPE corpus (Negri et al., 2018). 2.4 Our implementation of the RNN-based prediction estimator (P RED E ST-RNN) is described in Kepler et al. (2019). It follows closely the architecture proposed by Kim et al. (2017), which consists of two modules: • The En-De QE corpus provided by the shared task, consisting of 13,442 train triplets. • The En-Ru QE corpus provided by the shared task, consisting of 15,089 train triplets. • a predictor, which is trained to predict each token of the target sentence given the source and the left and right context of the target sentence; • The En-De parallel dataset of 3,396,364 sentences from th"
W19-5406,P03-1021,0,0.0160025,"ctions, with the weights learned on the development set. We use Powell’s conjugate direction method (Powell, 1964)2 as implemented in SciPy (Jones et al., 2001) to directly optimize for the task metric (F1 -MULT). Using the development set for learning carries a risk of overfitting; by using k-fold cross-validation we avoided this, and indeed the performance is equal or superior to the linear stacking ensemble (Table 1), while being computationally cheaper as only the development set is needed to learn an ensemble, avoiding jackknifing. 2 This is the method underlying the popular MERT method (Och, 2003), widely used in the MT literature. 81 Source: resistance bands are great for home use, gym use, ofﬁces, and are ideal for travel. PAIR Target: les bandes de résistance sont parfaits pour l’usage domestique, l’utilisation de la salle de gym, bureaux et sont idéales pour les voyages. En-De Figure 1: Example of a multi-span annotation containing two spans: parfaits does not agree with bandes due to gender—it should be parfaites. This mistake corresponds to a single annotation with severity “minor”. En-Ru 3.2 Implemented System To predict annotations within a document the problem is first treated"
W19-5406,W18-6463,0,0.0223612,"els are converted to BAD tags. As a baseline, all spans are assigned the most frequent severity, “major.” • Span borders are defined on character-level, whose positions may not match exactly the beginning or ending of a token. This will cause all characters of a partially correct token to be annotated with an error. 4 4.1 • Contiguous BAD tokens will always be mapped to a single annotation, even if they belong to different ones. Experimental Results Word and Sentence-Level Task The results achieved by each of the systems described in §2 for En-De and En-Ru on the valida4 Using the approach of Ive et al. (2018) proved less robust to this year’s data due to differences in the annotations. Particularly some outliers containing zero annotations would strongly harm the final Pearson score when mis-predicted. • Non-contiguous BAD tokens will always be mapped to separate annotations, even if they belong to the same one. 82 TARGET F1 TARGET M CC S OURCE F1 S OURCE M CC P EARSON Baseline E NSEMBLE 1 E NSEMBLE 2 0.2412 0.4629 0.4780 0.2145 0.4412 0.4577 0.2647 0.4174 0.4541 0.1887 0.3729 0.4212 0.2601 0.5889 0.5923 Baseline L INEAR E NSEMBLE P OWELL’ S E NSEMBLE 0.2974 0.4621 0.4752 0.2541 0.4387 0.4585 0.29"
W19-5406,P19-3020,1,0.86338,"Missing"
W19-5406,W18-6465,0,0.189341,"d English-French. Our submissions build upon the recent OpenKiwi framework: we combine linear, neural, and predictor-estimator systems with new transfer learning approaches using BERT and XLM pre-trained models. We compare systems individually and propose new ensemble techniques for word and sentence-level predictions. We also propose a simple technique for converting word labels into document-level predictions. Overall, our submitted systems achieve the best results on all tracks and language pairs by a considerable margin. 1 • We extend OpenKiwi with a Transformer predictor-estimator model (Wang et al., 2018). • We apply transfer learning techniques, finetuning BERT (Devlin et al., 2018) and XLM (Lample and Conneau, 2019) models in a predictor-estimator architecture. • We incorporate predictions coming from the APE-BERT system described in Correia and Martins (2019), also used in this year’s Unbabel’s APE submission (Lopes et al., 2019). • We propose new ensembling techniques for combining word-level and sentence-level predictions, which outperform previously used stacking approaches (Martins et al., 2016). • We build upon our BERT-based predictorestimator model to obtain document-level annotation"
W19-5406,W16-2387,1,\N,Missing
W19-5406,Q17-1015,1,\N,Missing
W19-5406,W17-4763,0,\N,Missing
W19-5413,W19-5406,1,0.882285,"Missing"
W19-5413,W15-3001,0,0.0320914,"Missing"
W19-5413,P17-4012,0,0.0683711,"rs the probability of all non conservative tokens, either increasing the confidence of an already picked conservative token, or favouring these tokens that are close to the best candidate – thus being closer to scores rather than probabilities. In contrast, negative penalties might require carefully selected values and truncating at the upper boundary – we did not experiment with negative values in this work, however the Quality Estimation shared task winning system used an APEQE system with negative conservativeness (Kepler et al., 2019). 1. Split the corpus into 5 folds fi . 2. Use OpenNMT (Klein et al., 2017) to train 5 LSTM based translation models, one model Mi for every subset created by removing fold fi from the training data. 3. Translate each fold fi using the translation Model Mi . 4. Join the translations to get an unbiased machine translated version of the full corpus. In contrast with Junczys-Dowmunt and Grundkiewicz, our model takes into account both src and mt, allowing to copy either of them directly. This is beneficial to handle proper nouns as they should be preserved in the post edition without any modification. Moreover, instead of setting the penalty as a fixed value of −1, we de"
W19-5413,P19-1441,0,0.0291655,"quality of NMT systems (Sennrich et al., 2016) and then it was applied to the APE task (Junczys-Dowmunt and Grundkiewicz, 2016). This approach, however, showed limited success on automatically post editing the high quality translations of APE systems. Transfer learning is another solution to deal with data sparsity in such tasks. It is based on the assumption that the knowledge extracted from other well-resourced tasks can be transferred to the new tasks/domains. Recently, large models pre-trained on multiple tasks with vast amounts of data, for instance BERT and MT-DNN (Devlin et al., 2018a; Liu et al., 2019), have obtained stateof-the-art results when fine-tuned over a small set of training samples. Following Correia and Martins (2019), in this paper we use BERT (Devlin et al., 2018a) within the encoder-decoder framework (§2.1) and formulate the task of Automatic This paper describes Unbabel’s submission to the WMT2019 APE Shared Task for the English-German language pair. Following the recent rise of large, powerful, pretrained models, we adapt the BERT pretrained model to perform Automatic Post-Editing in an encoder-decoder framework. Analogously to dual-encoder architectures we develop a BERT-b"
W19-5413,L18-1004,0,0.187629,"tps://github.com/google-research/ bert 119 3 spectively. For the sake of argument we define Vc for decoding a single APE triplet, which can be generalized to batch decoding with Vc defined for each batch element. Given the |V |sized vector of candidates ht at each decoding step t, we modify the score/probability of each candidate v as: ( ht (v) − c ht (v) = ht (v) if v ∈ V  Vc otherwise Experiments 3.1 Data This year for the English-German language pair the participants were provided an in-domain training set and the eSCAPE corpus, an artificially synthesized generic training corpus for APE (Negri et al., 2018). In addition to these corpora, they were allowed to use any additional data to train their systems. Considering this, and the fact that the in-domain training set belongs to the IT domain, we decided to use our own synthetic training corpus. Thus, we trained our models on a combination of the in-domain data released by the APE task organizers and this synthetic dataset. In-domain training set: we use the 13k triplets of <src,mt,pe> in the IT domain without any preprocessing as they are already preprocessed by the shared task organizers. Despite the previous year where the mt side was generate"
W19-5413,P19-1292,1,0.822151,"2016). This approach, however, showed limited success on automatically post editing the high quality translations of APE systems. Transfer learning is another solution to deal with data sparsity in such tasks. It is based on the assumption that the knowledge extracted from other well-resourced tasks can be transferred to the new tasks/domains. Recently, large models pre-trained on multiple tasks with vast amounts of data, for instance BERT and MT-DNN (Devlin et al., 2018a; Liu et al., 2019), have obtained stateof-the-art results when fine-tuned over a small set of training samples. Following Correia and Martins (2019), in this paper we use BERT (Devlin et al., 2018a) within the encoder-decoder framework (§2.1) and formulate the task of Automatic This paper describes Unbabel’s submission to the WMT2019 APE Shared Task for the English-German language pair. Following the recent rise of large, powerful, pretrained models, we adapt the BERT pretrained model to perform Automatic Post-Editing in an encoder-decoder framework. Analogously to dual-encoder architectures we develop a BERT-based encoder-decoder (BED) model in which a single pretrained BERT encoder receives both the source src and machine translation mt"
W19-5413,P02-1040,0,0.10727,"conservativeness penalty, a simple yet effective mechanism that controls the freedom of our APE in modifying the given MT output. As we show in §2.2, in the cases where the automatic translations are of high quality, this factor forces the APE system to do less modifications, hence avoids the well-known problem of over-correction. Finally, we augmented our original in-domain training data with a synthetic corpus which contains around 3M <src,mt,pe> triplets (§3.1). As discussed in §4, our system is able to improve significantly the MT outputs by −0.78 TER (Snover et al., 2016) and +1.23 BLEU (Papineni et al., 2002), achieving an ex-aequo firstplace in the English-German track. 2 Softmax Linear Add & Norm Feed Forward N× Add & Norm Add & Norm Tokens Segments Positions Context Attention Feed Forward (Multi-Head Att.) Add & Norm Add & Norm Self-Attention Self-Attention Input Embedding Output Embedding (Multi-Head Att.) (Multi-Head Att.) src1, …, srcN mt1, …, mtK pe1, …, peM A, …, A 0, …, N-1 B, …, B 0, …, K-1 B, …, B 0, …, M-1 Figure 1: BERT encoder decoder, taken from Correia and Martins (2019). Approach to (Junczys-Dowmunt and Grundkiewicz, 2018) where a dual-source encoder with shared parameters is used"
W19-5413,E17-2025,0,0.0428953,"ectively. “logprobs” and “logits” refer, respectively, to the state where we apply the conservativeness factor (see §2.2) 3.2 BED training the evaluation script available on the website to access the performance of our model. We follow Correia and Martins for training our BERT-based Encoder-Decoder APE models. In particular, we set the learning rate to 5e−5 and use bertadam optimizer to perform 200k steps from which 20k are warmup steps. We set the effective batch size to 2048 tokens. Furthermore, we also use a shared matrix for the input and output token embedddings and the projection layer (Press and Wolf, 2017). Finally, we share the self-attention weights between the encoder and the decoder and initialize the multi-head attention of the decoder with the self-attention weights of the encoder. Similarly to Junczys-Dowmunt (2018), we apply a data weighting strategy during training. However, we use a different weighting approach, where each sample si is assigned a weight, wsi , defined as 1 − T ER(si ). This results in assigning higher weights to the samples with less MT errors and vice versa, which might sound counter intuitive since in the APE task the goal is to learn more from the samples with larg"
W19-5413,P16-1009,0,0.0595357,"ly to the NMT paradigm this year and ignore the SMT technology. They also provide the previous year in-domain training set (i.e. 13k of <src,mt,pe> triplets) further increasing the difficulty of the task. Training state-of-the-art APE systems capable of improving high quality NMT outputs requires large amounts of training data, which is not always available, in particular for this WMT shared task. Augmenting the training set with artificially synthesized data is one of the popular and effective approaches for coping with this challenge. It was first used to improve the quality of NMT systems (Sennrich et al., 2016) and then it was applied to the APE task (Junczys-Dowmunt and Grundkiewicz, 2016). This approach, however, showed limited success on automatically post editing the high quality translations of APE systems. Transfer learning is another solution to deal with data sparsity in such tasks. It is based on the assumption that the knowledge extracted from other well-resourced tasks can be transferred to the new tasks/domains. Recently, large models pre-trained on multiple tasks with vast amounts of data, for instance BERT and MT-DNN (Devlin et al., 2018a; Liu et al., 2019), have obtained stateof-the-a"
W19-5413,W18-6415,0,0.0172347,"odel. We follow Correia and Martins for training our BERT-based Encoder-Decoder APE models. In particular, we set the learning rate to 5e−5 and use bertadam optimizer to perform 200k steps from which 20k are warmup steps. We set the effective batch size to 2048 tokens. Furthermore, we also use a shared matrix for the input and output token embedddings and the projection layer (Press and Wolf, 2017). Finally, we share the self-attention weights between the encoder and the decoder and initialize the multi-head attention of the decoder with the self-attention weights of the encoder. Similarly to Junczys-Dowmunt (2018), we apply a data weighting strategy during training. However, we use a different weighting approach, where each sample si is assigned a weight, wsi , defined as 1 − T ER(si ). This results in assigning higher weights to the samples with less MT errors and vice versa, which might sound counter intuitive since in the APE task the goal is to learn more from the samples with larger number of errors. However, in this task, where the translations are provided by strong NMT systems with very small number of errors, our APE system needs to be conservative and learn to perform limited number of modifi"
W19-5413,W16-2378,0,0.376994,"They also provide the previous year in-domain training set (i.e. 13k of <src,mt,pe> triplets) further increasing the difficulty of the task. Training state-of-the-art APE systems capable of improving high quality NMT outputs requires large amounts of training data, which is not always available, in particular for this WMT shared task. Augmenting the training set with artificially synthesized data is one of the popular and effective approaches for coping with this challenge. It was first used to improve the quality of NMT systems (Sennrich et al., 2016) and then it was applied to the APE task (Junczys-Dowmunt and Grundkiewicz, 2016). This approach, however, showed limited success on automatically post editing the high quality translations of APE systems. Transfer learning is another solution to deal with data sparsity in such tasks. It is based on the assumption that the knowledge extracted from other well-resourced tasks can be transferred to the new tasks/domains. Recently, large models pre-trained on multiple tasks with vast amounts of data, for instance BERT and MT-DNN (Devlin et al., 2018a; Liu et al., 2019), have obtained stateof-the-art results when fine-tuned over a small set of training samples. Following Correi"
W19-5413,W18-6471,0,0.338411,"ystem Ours (Unbabel) POSTECH USSAR DFKI FBK UdS MTL IC USFD Baseline ADAP DCU servativeness penalty. We, however, leave it to be explored in future work. Regarding the performance of our model on the official test set, as the results of Table 2 show, we outperform last year’s winning systems by almost −0.4 TER and +0.5 BLEU, which for strong performing NMT systems is significant. In addition, our submission ranks first in the official results 3 , ex aequo with 3 other systems. Table 3 depicts the official results of the shared task, considering only the best submission of each team. Baseline (Tebbifakhr et al., 2018) Primary Contrastive ↓TER 16.84 16.46 16.08 16.21 ↑BLEU 75.96 76.22 75.75 75.71 75.03 74.88 74.73 74.30 Table 3: APE Results as provided by the shared task organizers. We only present the best score of each team. ? indicates not statistically significantly different, ex aequo. ↑BLEU 74.73 75.53 75.96 75.70 Finally, using APE to improve strong indomain Neural Machine Translation systems is increasingly more challenging, and ideally the editing system will tend to perform less and less modifications of the raw mt. In line with JunczysDowmunt and Grundkiewicz’s suggestion, studying how to apply A"
W19-5413,W18-6467,0,0.28555,"ificantly the MT outputs by −0.78 TER (Snover et al., 2016) and +1.23 BLEU (Papineni et al., 2002), achieving an ex-aequo firstplace in the English-German track. 2 Softmax Linear Add & Norm Feed Forward N× Add & Norm Add & Norm Tokens Segments Positions Context Attention Feed Forward (Multi-Head Att.) Add & Norm Add & Norm Self-Attention Self-Attention Input Embedding Output Embedding (Multi-Head Att.) (Multi-Head Att.) src1, …, srcN mt1, …, mtK pe1, …, peM A, …, A 0, …, N-1 B, …, B 0, …, K-1 B, …, B 0, …, M-1 Figure 1: BERT encoder decoder, taken from Correia and Martins (2019). Approach to (Junczys-Dowmunt and Grundkiewicz, 2018) where a dual-source encoder with shared parameters is used to encode both input strings. On the target side, following (Correia and Martins, 2019) we use a single decoder where the context attention block is initialized with the self attention weights, and all the weights of the self-attention are shared with the respective selfattention weights in the encoder. In this section we describe the main features of our APE system: the BERT-based encoder-decoder (BED) and the conservativeness penalty. 2.1 N× BERT-based encoder-decoder Following (Correia and Martins, 2019) we adapt the BERT model to"
W19-5413,W16-2301,0,\N,Missing
W19-6605,E14-2007,0,\N,Missing
W19-6605,2012.amta-wptp.2,0,\N,Missing
W19-6605,C14-2028,0,\N,Missing
W19-6605,2011.eamt-1.12,0,\N,Missing
W19-6605,D14-1162,0,\N,Missing
W19-6605,E14-1042,0,\N,Missing
W19-6605,D14-1130,0,\N,Missing
W19-6605,W14-0307,0,\N,Missing
W19-6605,P13-1004,0,\N,Missing
W19-6605,aziz-etal-2012-pet,0,\N,Missing
W19-6605,2012.eamt-1.31,0,\N,Missing
W19-6605,2016.amta-researchers.2,0,\N,Missing
W19-6605,P18-4020,1,\N,Missing
W19-6605,D18-1028,0,\N,Missing
W19-6605,E17-2068,0,\N,Missing
W19-6605,2012.tc-1.5,0,\N,Missing
W19-6605,W14-0311,0,\N,Missing
W19-6605,P17-1012,0,\N,Missing
W19-6605,D17-1213,0,\N,Missing
W19-6722,Q17-1024,0,0.0494836,"Missing"
