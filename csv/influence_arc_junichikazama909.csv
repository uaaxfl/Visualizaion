C08-1024,P99-1008,0,0.0241378,"d, whose experimental results are discussed in Section 5. 2 Related Work Our goal of automatically acquiring object-trouble pairs from Web documents is perhaps best viewed as a problem of semantic relation extraction. Recently the Automatic Content Extraction (ACE) program (Doddington et al., 2004) is a wellknown benchmark task concerned with the automatic recognition of semantic relations from unstructured text. Typical target relations include “Reaction” and “Production” (Pantel and Pennacchiootti, 2006), “person-affiliation” and “organization-location” (Zelenko et al., 2002), “part-whole” (Berland and Charniak, 1999; Girju et al., 2006) and temporal precedence relations between events (Chklovski and Pantel, 2004; Torisawa, 2006). Our current task of acquiring “objecttrouble” relations is new and object-trouble relations are inherently more abstract and indirect than relations like “person-affiliation” — they crucially depend on additional knowledge about whether and how a given object’s use might be hampered by a specific trouble. Another line of research closely related to our work is the recognition of semantic orientation and sentiment analysis (Turney, 2002; Takamura et al., 2006; Kaji and Kitsuregaw"
C08-1024,W04-3205,0,0.029568,"acquiring object-trouble pairs from Web documents is perhaps best viewed as a problem of semantic relation extraction. Recently the Automatic Content Extraction (ACE) program (Doddington et al., 2004) is a wellknown benchmark task concerned with the automatic recognition of semantic relations from unstructured text. Typical target relations include “Reaction” and “Production” (Pantel and Pennacchiootti, 2006), “person-affiliation” and “organization-location” (Zelenko et al., 2002), “part-whole” (Berland and Charniak, 1999; Girju et al., 2006) and temporal precedence relations between events (Chklovski and Pantel, 2004; Torisawa, 2006). Our current task of acquiring “objecttrouble” relations is new and object-trouble relations are inherently more abstract and indirect than relations like “person-affiliation” — they crucially depend on additional knowledge about whether and how a given object’s use might be hampered by a specific trouble. Another line of research closely related to our work is the recognition of semantic orientation and sentiment analysis (Turney, 2002; Takamura et al., 2006; Kaji and Kitsuregawa, 2006). Clearly troubles should be associated with a negative orientation of an expression, but"
C08-1024,D07-1114,0,0.0149061,"nowledge about whether and how a given object’s use might be hampered by a specific trouble. Another line of research closely related to our work is the recognition of semantic orientation and sentiment analysis (Turney, 2002; Takamura et al., 2006; Kaji and Kitsuregawa, 2006). Clearly troubles should be associated with a negative orientation of an expression, but studies on the acquisition of semantic orientation traditionally do not bother with the context of evaluation. While recent work on sentiment analysis has started to associate sentiment-related attribute-evaluation pairs to objects (Kobayashi et al., 2007), these attributes usually concern intrinsic properties of the objects, such as a digital camera’s colors — they do not extend to sentiment-related factors external to the object like “traffic jams” for theme parks. The acquisition method proposed in this work addresses both these matters. Finally, our task of acquiring trouble expressions can be regarded as hyponymy acquisition, where target expressions are hyponyms of the word “trouble”. Although we used the classical lexico-syntactic patterns for hyponymy acquisition (Hearst, 1992; Imasumi, 2001; Ando et al., 2003) to reflect this intuition"
C08-1024,P06-1015,0,0.0820586,"out related work. Section 3 examines the notion of trouble expressions and their evidences. Section 4 describes our method, whose experimental results are discussed in Section 5. 2 Related Work Our goal of automatically acquiring object-trouble pairs from Web documents is perhaps best viewed as a problem of semantic relation extraction. Recently the Automatic Content Extraction (ACE) program (Doddington et al., 2004) is a wellknown benchmark task concerned with the automatic recognition of semantic relations from unstructured text. Typical target relations include “Reaction” and “Production” (Pantel and Pennacchiootti, 2006), “person-affiliation” and “organization-location” (Zelenko et al., 2002), “part-whole” (Berland and Charniak, 1999; Girju et al., 2006) and temporal precedence relations between events (Chklovski and Pantel, 2004; Torisawa, 2006). Our current task of acquiring “objecttrouble” relations is new and object-trouble relations are inherently more abstract and indirect than relations like “person-affiliation” — they crucially depend on additional knowledge about whether and how a given object’s use might be hampered by a specific trouble. Another line of research closely related to our work is the r"
C08-1024,N04-1010,1,0.817504,"parks. The acquisition method proposed in this work addresses both these matters. Finally, our task of acquiring trouble expressions can be regarded as hyponymy acquisition, where target expressions are hyponyms of the word “trouble”. Although we used the classical lexico-syntactic patterns for hyponymy acquisition (Hearst, 1992; Imasumi, 2001; Ando et al., 2003) to reflect this intuition, our experiments show we were unable to attain satisfactory performance using lexico-syntactic patterns alone. Thus, we also use verb-noun dependencies as evidence in learning (Pantel and Ravichandran, 2004; Shinzato and Torisawa, 2004). We treat the evidences uniformly as elements in a feature vector given to a supervised learning method, which allowed us to extract a considerably larger number of trouble expressions than could be acquired by sparse lexicosyntactic patterns alone, while still keeping decent precision. What kind of hyponymy relations can be acquired by noun-verb dependencies is still an open question in NLP. In this work we show that at least trouble expressions can successfully be acquired based on noun-verb dependency information alone. 3 Trouble Expressions and Features for Their Acquisition In section 1"
C08-1024,E06-1026,0,0.0264045,", “part-whole” (Berland and Charniak, 1999; Girju et al., 2006) and temporal precedence relations between events (Chklovski and Pantel, 2004; Torisawa, 2006). Our current task of acquiring “objecttrouble” relations is new and object-trouble relations are inherently more abstract and indirect than relations like “person-affiliation” — they crucially depend on additional knowledge about whether and how a given object’s use might be hampered by a specific trouble. Another line of research closely related to our work is the recognition of semantic orientation and sentiment analysis (Turney, 2002; Takamura et al., 2006; Kaji and Kitsuregawa, 2006). Clearly troubles should be associated with a negative orientation of an expression, but studies on the acquisition of semantic orientation traditionally do not bother with the context of evaluation. While recent work on sentiment analysis has started to associate sentiment-related attribute-evaluation pairs to objects (Kobayashi et al., 2007), these attributes usually concern intrinsic properties of the objects, such as a digital camera’s colors — they do not extend to sentiment-related factors external to the object like “traffic jams” for theme parks. The acqui"
C08-1024,N06-1008,1,0.852253,"irs from Web documents is perhaps best viewed as a problem of semantic relation extraction. Recently the Automatic Content Extraction (ACE) program (Doddington et al., 2004) is a wellknown benchmark task concerned with the automatic recognition of semantic relations from unstructured text. Typical target relations include “Reaction” and “Production” (Pantel and Pennacchiootti, 2006), “person-affiliation” and “organization-location” (Zelenko et al., 2002), “part-whole” (Berland and Charniak, 1999; Girju et al., 2006) and temporal precedence relations between events (Chklovski and Pantel, 2004; Torisawa, 2006). Our current task of acquiring “objecttrouble” relations is new and object-trouble relations are inherently more abstract and indirect than relations like “person-affiliation” — they crucially depend on additional knowledge about whether and how a given object’s use might be hampered by a specific trouble. Another line of research closely related to our work is the recognition of semantic orientation and sentiment analysis (Turney, 2002; Takamura et al., 2006; Kaji and Kitsuregawa, 2006). Clearly troubles should be associated with a negative orientation of an expression, but studies on the ac"
C08-1024,P02-1053,0,0.00441826,"et al., 2002), “part-whole” (Berland and Charniak, 1999; Girju et al., 2006) and temporal precedence relations between events (Chklovski and Pantel, 2004; Torisawa, 2006). Our current task of acquiring “objecttrouble” relations is new and object-trouble relations are inherently more abstract and indirect than relations like “person-affiliation” — they crucially depend on additional knowledge about whether and how a given object’s use might be hampered by a specific trouble. Another line of research closely related to our work is the recognition of semantic orientation and sentiment analysis (Turney, 2002; Takamura et al., 2006; Kaji and Kitsuregawa, 2006). Clearly troubles should be associated with a negative orientation of an expression, but studies on the acquisition of semantic orientation traditionally do not bother with the context of evaluation. While recent work on sentiment analysis has started to associate sentiment-related attribute-evaluation pairs to objects (Kobayashi et al., 2007), these attributes usually concern intrinsic properties of the objects, such as a digital camera’s colors — they do not extend to sentiment-related factors external to the object like “traffic jams” for"
C08-1024,W02-1010,0,0.025201,"dences. Section 4 describes our method, whose experimental results are discussed in Section 5. 2 Related Work Our goal of automatically acquiring object-trouble pairs from Web documents is perhaps best viewed as a problem of semantic relation extraction. Recently the Automatic Content Extraction (ACE) program (Doddington et al., 2004) is a wellknown benchmark task concerned with the automatic recognition of semantic relations from unstructured text. Typical target relations include “Reaction” and “Production” (Pantel and Pennacchiootti, 2006), “person-affiliation” and “organization-location” (Zelenko et al., 2002), “part-whole” (Berland and Charniak, 1999; Girju et al., 2006) and temporal precedence relations between events (Chklovski and Pantel, 2004; Torisawa, 2006). Our current task of acquiring “objecttrouble” relations is new and object-trouble relations are inherently more abstract and indirect than relations like “person-affiliation” — they crucially depend on additional knowledge about whether and how a given object’s use might be hampered by a specific trouble. Another line of research closely related to our work is the recognition of semantic orientation and sentiment analysis (Turney, 2002;"
C08-1024,doddington-etal-2004-automatic,0,0.0128655,"s underlying the unsupervised method for training sample selection in the first step, and the final filtering mechanism in the third step. The rest of this paper is organized as follows. Section 2 points out related work. Section 3 examines the notion of trouble expressions and their evidences. Section 4 describes our method, whose experimental results are discussed in Section 5. 2 Related Work Our goal of automatically acquiring object-trouble pairs from Web documents is perhaps best viewed as a problem of semantic relation extraction. Recently the Automatic Content Extraction (ACE) program (Doddington et al., 2004) is a wellknown benchmark task concerned with the automatic recognition of semantic relations from unstructured text. Typical target relations include “Reaction” and “Production” (Pantel and Pennacchiootti, 2006), “person-affiliation” and “organization-location” (Zelenko et al., 2002), “part-whole” (Berland and Charniak, 1999; Girju et al., 2006) and temporal precedence relations between events (Chklovski and Pantel, 2004; Torisawa, 2006). Our current task of acquiring “objecttrouble” relations is new and object-trouble relations are inherently more abstract and indirect than relations like “p"
C08-1024,J06-1005,0,0.0259437,"s are discussed in Section 5. 2 Related Work Our goal of automatically acquiring object-trouble pairs from Web documents is perhaps best viewed as a problem of semantic relation extraction. Recently the Automatic Content Extraction (ACE) program (Doddington et al., 2004) is a wellknown benchmark task concerned with the automatic recognition of semantic relations from unstructured text. Typical target relations include “Reaction” and “Production” (Pantel and Pennacchiootti, 2006), “person-affiliation” and “organization-location” (Zelenko et al., 2002), “part-whole” (Berland and Charniak, 1999; Girju et al., 2006) and temporal precedence relations between events (Chklovski and Pantel, 2004; Torisawa, 2006). Our current task of acquiring “objecttrouble” relations is new and object-trouble relations are inherently more abstract and indirect than relations like “person-affiliation” — they crucially depend on additional knowledge about whether and how a given object’s use might be hampered by a specific trouble. Another line of research closely related to our work is the recognition of semantic orientation and sentiment analysis (Turney, 2002; Takamura et al., 2006; Kaji and Kitsuregawa, 2006). Clearly tro"
C08-1024,C92-2082,0,0.310374,"nt-related attribute-evaluation pairs to objects (Kobayashi et al., 2007), these attributes usually concern intrinsic properties of the objects, such as a digital camera’s colors — they do not extend to sentiment-related factors external to the object like “traffic jams” for theme parks. The acquisition method proposed in this work addresses both these matters. Finally, our task of acquiring trouble expressions can be regarded as hyponymy acquisition, where target expressions are hyponyms of the word “trouble”. Although we used the classical lexico-syntactic patterns for hyponymy acquisition (Hearst, 1992; Imasumi, 2001; Ando et al., 2003) to reflect this intuition, our experiments show we were unable to attain satisfactory performance using lexico-syntactic patterns alone. Thus, we also use verb-noun dependencies as evidence in learning (Pantel and Ravichandran, 2004; Shinzato and Torisawa, 2004). We treat the evidences uniformly as elements in a feature vector given to a supervised learning method, which allowed us to extract a considerably larger number of trouble expressions than could be acquired by sparse lexicosyntactic patterns alone, while still keeping decent precision. What kind of"
C08-1024,P06-2059,0,0.0129167,"and Charniak, 1999; Girju et al., 2006) and temporal precedence relations between events (Chklovski and Pantel, 2004; Torisawa, 2006). Our current task of acquiring “objecttrouble” relations is new and object-trouble relations are inherently more abstract and indirect than relations like “person-affiliation” — they crucially depend on additional knowledge about whether and how a given object’s use might be hampered by a specific trouble. Another line of research closely related to our work is the recognition of semantic orientation and sentiment analysis (Turney, 2002; Takamura et al., 2006; Kaji and Kitsuregawa, 2006). Clearly troubles should be associated with a negative orientation of an expression, but studies on the acquisition of semantic orientation traditionally do not bother with the context of evaluation. While recent work on sentiment analysis has started to associate sentiment-related attribute-evaluation pairs to objects (Kobayashi et al., 2007), these attributes usually concern intrinsic properties of the objects, such as a digital camera’s colors — they do not extend to sentiment-related factors external to the object like “traffic jams” for theme parks. The acquisition method proposed in thi"
C08-1024,N04-1041,0,\N,Missing
C10-2015,W06-2920,0,0.0858243,"accuracy on the standard PTB test set for English and the standard Penn Chinese Treebank (CTB) test set for Chinese. 1 Introduction Dependency parsing is an approach to syntactic analysis inspired by dependency grammar. In recent years, interest in this approach has surged due to its usefulness in such applications as machine translation (Nakazawa et al., 2006), information extraction (Culotta and Sorensen, 2004). Graph-based parsing models (McDonald and Pereira, 2006; Carreras, 2007) have achieved state-of-the-art accuracy for a wide range of languages as shown in recent CoNLL shared tasks (Buchholz et al., 2006; Nivre et al., 2007). However, to make parsing tractable, these models are forced to restrict features over a very limited history of parsing decisions (McDonald and Pereira, 2006; McDonald and Nivre, 2007). Previous work showed that rich features over a wide range of decision history can lead to significant improvements in accuracy for transition-based models (Yamada and Matsumoto, 2003a; Nivre et al., 2004). In this paper, we propose an approach to improve graph-based dependency parsing by using decision history. Here, we make an assumption: the dependency relations between words with a sho"
C10-2015,D07-1101,0,0.167428,"d parsing model and introducing a set of new features. The experimental results show that our system achieves state-ofthe-art accuracy on the standard PTB test set for English and the standard Penn Chinese Treebank (CTB) test set for Chinese. 1 Introduction Dependency parsing is an approach to syntactic analysis inspired by dependency grammar. In recent years, interest in this approach has surged due to its usefulness in such applications as machine translation (Nakazawa et al., 2006), information extraction (Culotta and Sorensen, 2004). Graph-based parsing models (McDonald and Pereira, 2006; Carreras, 2007) have achieved state-of-the-art accuracy for a wide range of languages as shown in recent CoNLL shared tasks (Buchholz et al., 2006; Nivre et al., 2007). However, to make parsing tractable, these models are forced to restrict features over a very limited history of parsing decisions (McDonald and Pereira, 2006; McDonald and Nivre, 2007). Previous work showed that rich features over a wide range of decision history can lead to significant improvements in accuracy for transition-based models (Yamada and Matsumoto, 2003a; Nivre et al., 2004). In this paper, we propose an approach to improve graph"
C10-2015,I08-1012,1,0.900686,"Missing"
C10-2015,D09-1060,1,0.694572,"by 1.02 points for Chi132 Table 5: Results for English Table 4: Results for Chinese Baseline OURS OURS+STACK Zhao2009 Yu2008 STACK Chen2009 UAS 88.41 89.43(+1.02) 89.53 87.0 87.26 88.95 89.91 Complete 48.85 50.86 49.42 – – 49.42 48.56 nese and 0.29 points for English. The improvements of (OURS) were significant in McNemar’s Test with p &lt; 10−4 for Chinese and p &lt; 10−3 for English. 5.3 Comparative results Table 4 shows the comparative results for Chinese, where Zhao2009 refers to the result of (Zhao et al., 2009), Yu2008 refers to the result of Yu et al. (2008), Chen2009 refers to the result of Chen et al. (2009) that is the best reported result on this data, and STACK refers to our implementation of the combination parser of Nivre and McDonald (2008) using our baseline system and the MALTParser7 . The results indicated that OURS performed better than Zhao2009, Yu2008, and STACK, but worse than Chen2009 that used largescale unlabeled data (Chen et al., 2009). We also implemented the combination system of OURS and the MALTParser, referred as OURS+STACK in Table 4. The new system achieved further improvement. In future work, we can combine our approach with the parser of Chen et al. (2009). Table 5 show"
C10-2015,N06-1021,0,0.0247458,"Missing"
C10-2015,P04-1054,0,0.037329,"e long dependencies. The mechanism can easily be implemented by modifying a graphbased parsing model and introducing a set of new features. The experimental results show that our system achieves state-ofthe-art accuracy on the standard PTB test set for English and the standard Penn Chinese Treebank (CTB) test set for Chinese. 1 Introduction Dependency parsing is an approach to syntactic analysis inspired by dependency grammar. In recent years, interest in this approach has surged due to its usefulness in such applications as machine translation (Nakazawa et al., 2006), information extraction (Culotta and Sorensen, 2004). Graph-based parsing models (McDonald and Pereira, 2006; Carreras, 2007) have achieved state-of-the-art accuracy for a wide range of languages as shown in recent CoNLL shared tasks (Buchholz et al., 2006; Nivre et al., 2007). However, to make parsing tractable, these models are forced to restrict features over a very limited history of parsing decisions (McDonald and Pereira, 2006; McDonald and Nivre, 2007). Previous work showed that rich features over a wide range of decision history can lead to significant improvements in accuracy for transition-based models (Yamada and Matsumoto, 2003a; Ni"
C10-2015,C96-1058,0,0.451123,"s in w10 A w3 . This simple example shows how to use the decision history to help parse the long distance dependencies. 3 Background: graph-based parsing models Before we describe our method, we briefly introduce the graph-based parsing models. We denote input sentence w by w = (w0 , w1 , ..., wn ), where w0 = ROOT is an artificial root token inserted at the beginning of the sentence and does not depend on any other token in w and wi refers to a word. We employ the second-order projective graphbased parsing model of Carreras (2007), which is an extension of the projective parsing algorithm of Eisner (1996). The parsing algorithms used in Carreras (2007) independently find the left and right dependents of a word and then combine them later in a bottomup style based on Eisner (1996). A subtree that spans the words in [s, t] (and roots at s or t) is represented by chart item [s, t, right/lef t, C/I], where right (left) indicates that the root of the subtree is s (t) and C means that the item is complete while I means that the item is incomplete (McDonald, 2006). Here, complete item in the right (left) direction means that the words other than s (t) cannot have dependents outside [s, t] and incompl"
C10-2015,P07-1050,0,0.0219216,"93.16 93.79 Complete 44.28 45.24 38.4 37.6 45.4 47.06 – 47.15 – based and transition-based models. OURS performed better than Z&C 2008, but worse than STACK. The last three systems that used largescale unlabeled data performed better than OURS. 6 Related work There are several studies that tried to overcome the limited feature scope of graph-based dependency parsing models . Nakagawa (2007) proposed a method to deal with the intractable inference problem in a graphbased model by introducing the Gibbs sampling algorithm. Compared with their approach, our approach is much simpler yet effective. Hall (2007) used a re-ranking scheme to provide global features while we simply augment the features of an existing parser. Nivre and McDonald (2008) and Zhang and Clark (2008) proposed stacking methods to combine graph-based parsers with transition-based parsers. One parser uses dependency predictions made by another parser. Our results show that our approach can be used in the stacking frameworks to achieve higher accuracy. 7 Conclusions This paper proposes an approach for improving graph-based dependency parsing by using the decision history. For the graph-based model, we design a set of features over"
C10-2015,P08-1068,0,0.158647,"ub-sentences. • Distance: use the history-based features for the relation of two words within a predefined distance. We set the thresholds to 3, 5, and 10. 5 Experimental results and our new systems OURS. In order to evaluate the effectiveness of the history-based features, we conducted experiments on Chinese and English data. For English, we used the Penn Treebank (Marcus et al., 1993) in our experiments and the tool “Penn2Malt”3 to convert the data into dependency structures using a standard set of head rules (Yamada and Matsumoto, 2003a). To match previous work (McDonald and Pereira, 2006; Koo et al., 2008), we split the data into a training set (sections 2-21), a development set (Section 22), and a test set (section 23). Following the work of Koo et al. (2008), we used the MXPOST (Ratnaparkhi, 1996) tagger trained on training data to provide part-of-speech tags for the development and the test set, and we used 10-way jackknifing to generate tags for the training set. For Chinese, we used the Chinese Treebank (CTB) version 4.04 in the experiments. We also used the “Penn2Malt” tool to convert the data and created a data split: files 1-270 and files 400-931 for training, files 271-300 for testing,"
C10-2015,J93-2004,0,0.0336939,"history-based features for all the word pairs without any restriction. • Sub-sentences: use the history-based features only for the relation of two words from sub-sentences. Here, we use punctuation marks to split sentences into sub-sentences. • Distance: use the history-based features for the relation of two words within a predefined distance. We set the thresholds to 3, 5, and 10. 5 Experimental results and our new systems OURS. In order to evaluate the effectiveness of the history-based features, we conducted experiments on Chinese and English data. For English, we used the Penn Treebank (Marcus et al., 1993) in our experiments and the tool “Penn2Malt”3 to convert the data into dependency structures using a standard set of head rules (Yamada and Matsumoto, 2003a). To match previous work (McDonald and Pereira, 2006; Koo et al., 2008), we split the data into a training set (sections 2-21), a development set (Section 22), and a test set (section 23). Following the work of Koo et al. (2008), we used the MXPOST (Ratnaparkhi, 1996) tagger trained on training data to provide part-of-speech tags for the development and the test set, and we used 10-way jackknifing to generate tags for the training set. For"
C10-2015,D07-1013,0,0.0985631,"pendency grammar. In recent years, interest in this approach has surged due to its usefulness in such applications as machine translation (Nakazawa et al., 2006), information extraction (Culotta and Sorensen, 2004). Graph-based parsing models (McDonald and Pereira, 2006; Carreras, 2007) have achieved state-of-the-art accuracy for a wide range of languages as shown in recent CoNLL shared tasks (Buchholz et al., 2006; Nivre et al., 2007). However, to make parsing tractable, these models are forced to restrict features over a very limited history of parsing decisions (McDonald and Pereira, 2006; McDonald and Nivre, 2007). Previous work showed that rich features over a wide range of decision history can lead to significant improvements in accuracy for transition-based models (Yamada and Matsumoto, 2003a; Nivre et al., 2004). In this paper, we propose an approach to improve graph-based dependency parsing by using decision history. Here, we make an assumption: the dependency relations between words with a short distance are more reliable than ones between words with a long distance. This is supported by the fact that the accuracy of short dependencies is in general greater than that of long dependencies as repor"
C10-2015,E06-1011,0,0.179838,"ted by modifying a graphbased parsing model and introducing a set of new features. The experimental results show that our system achieves state-ofthe-art accuracy on the standard PTB test set for English and the standard Penn Chinese Treebank (CTB) test set for Chinese. 1 Introduction Dependency parsing is an approach to syntactic analysis inspired by dependency grammar. In recent years, interest in this approach has surged due to its usefulness in such applications as machine translation (Nakazawa et al., 2006), information extraction (Culotta and Sorensen, 2004). Graph-based parsing models (McDonald and Pereira, 2006; Carreras, 2007) have achieved state-of-the-art accuracy for a wide range of languages as shown in recent CoNLL shared tasks (Buchholz et al., 2006; Nivre et al., 2007). However, to make parsing tractable, these models are forced to restrict features over a very limited history of parsing decisions (McDonald and Pereira, 2006; McDonald and Nivre, 2007). Previous work showed that rich features over a wide range of decision history can lead to significant improvements in accuracy for transition-based models (Yamada and Matsumoto, 2003a; Nivre et al., 2004). In this paper, we propose an approach"
C10-2015,D07-1100,0,0.0140656,"systems that were based on single models. Z&C 2008 and STACK were the combination systems of graph7 Baseline OURS Y&M2003 CO2006 Z&C2008 STACK KOO2008 Chen2009 Suzuki2009 UAS 91.92 92.21 (+0.29) 90.3 90.8 92.1 92.53 93.16 93.16 93.79 Complete 44.28 45.24 38.4 37.6 45.4 47.06 – 47.15 – based and transition-based models. OURS performed better than Z&C 2008, but worse than STACK. The last three systems that used largescale unlabeled data performed better than OURS. 6 Related work There are several studies that tried to overcome the limited feature scope of graph-based dependency parsing models . Nakagawa (2007) proposed a method to deal with the intractable inference problem in a graphbased model by introducing the Gibbs sampling algorithm. Compared with their approach, our approach is much simpler yet effective. Hall (2007) used a re-ranking scheme to provide global features while we simply augment the features of an existing parser. Nivre and McDonald (2008) and Zhang and Clark (2008) proposed stacking methods to combine graph-based parsers with transition-based parsers. One parser uses dependency predictions made by another parser. Our results show that our approach can be used in the stacking fr"
C10-2015,2006.iwslt-evaluation.9,0,0.0170896,"models and may be used as features to help parse long dependencies. The mechanism can easily be implemented by modifying a graphbased parsing model and introducing a set of new features. The experimental results show that our system achieves state-ofthe-art accuracy on the standard PTB test set for English and the standard Penn Chinese Treebank (CTB) test set for Chinese. 1 Introduction Dependency parsing is an approach to syntactic analysis inspired by dependency grammar. In recent years, interest in this approach has surged due to its usefulness in such applications as machine translation (Nakazawa et al., 2006), information extraction (Culotta and Sorensen, 2004). Graph-based parsing models (McDonald and Pereira, 2006; Carreras, 2007) have achieved state-of-the-art accuracy for a wide range of languages as shown in recent CoNLL shared tasks (Buchholz et al., 2006; Nivre et al., 2007). However, to make parsing tractable, these models are forced to restrict features over a very limited history of parsing decisions (McDonald and Pereira, 2006; McDonald and Nivre, 2007). Previous work showed that rich features over a wide range of decision history can lead to significant improvements in accuracy for tra"
C10-2015,P08-1108,0,0.0909482,"n2009 UAS 88.41 89.43(+1.02) 89.53 87.0 87.26 88.95 89.91 Complete 48.85 50.86 49.42 – – 49.42 48.56 nese and 0.29 points for English. The improvements of (OURS) were significant in McNemar’s Test with p &lt; 10−4 for Chinese and p &lt; 10−3 for English. 5.3 Comparative results Table 4 shows the comparative results for Chinese, where Zhao2009 refers to the result of (Zhao et al., 2009), Yu2008 refers to the result of Yu et al. (2008), Chen2009 refers to the result of Chen et al. (2009) that is the best reported result on this data, and STACK refers to our implementation of the combination parser of Nivre and McDonald (2008) using our baseline system and the MALTParser7 . The results indicated that OURS performed better than Zhao2009, Yu2008, and STACK, but worse than Chen2009 that used largescale unlabeled data (Chen et al., 2009). We also implemented the combination system of OURS and the MALTParser, referred as OURS+STACK in Table 4. The new system achieved further improvement. In future work, we can combine our approach with the parser of Chen et al. (2009). Table 5 shows the comparative results for English, where Y&M2003 refers to the parser of Yamada and Matsumoto (2003b), CO2006 refers to the parser of Cor"
C10-2015,W04-2407,0,0.0205074,"4). Graph-based parsing models (McDonald and Pereira, 2006; Carreras, 2007) have achieved state-of-the-art accuracy for a wide range of languages as shown in recent CoNLL shared tasks (Buchholz et al., 2006; Nivre et al., 2007). However, to make parsing tractable, these models are forced to restrict features over a very limited history of parsing decisions (McDonald and Pereira, 2006; McDonald and Nivre, 2007). Previous work showed that rich features over a wide range of decision history can lead to significant improvements in accuracy for transition-based models (Yamada and Matsumoto, 2003a; Nivre et al., 2004). In this paper, we propose an approach to improve graph-based dependency parsing by using decision history. Here, we make an assumption: the dependency relations between words with a short distance are more reliable than ones between words with a long distance. This is supported by the fact that the accuracy of short dependencies is in general greater than that of long dependencies as reported in McDonald and Nivre (2007) for graph-based models. Our idea is to use decision history, which is made in previous scans in a bottom-up procedure, to help parse other words in later scans. In the botto"
C10-2015,W96-0213,0,0.215296,"ms OURS. In order to evaluate the effectiveness of the history-based features, we conducted experiments on Chinese and English data. For English, we used the Penn Treebank (Marcus et al., 1993) in our experiments and the tool “Penn2Malt”3 to convert the data into dependency structures using a standard set of head rules (Yamada and Matsumoto, 2003a). To match previous work (McDonald and Pereira, 2006; Koo et al., 2008), we split the data into a training set (sections 2-21), a development set (Section 22), and a test set (section 23). Following the work of Koo et al. (2008), we used the MXPOST (Ratnaparkhi, 1996) tagger trained on training data to provide part-of-speech tags for the development and the test set, and we used 10-way jackknifing to generate tags for the training set. For Chinese, we used the Chinese Treebank (CTB) version 4.04 in the experiments. We also used the “Penn2Malt” tool to convert the data and created a data split: files 1-270 and files 400-931 for training, files 271-300 for testing, and files 301-325 for development. We used gold standard segmentation and part-of-speech tags in the CTB. The data partition and part-of-speech settings were chosen to match previous work (Chen et"
C10-2015,D09-1058,0,0.0509405,"tem achieved further improvement. In future work, we can combine our approach with the parser of Chen et al. (2009). Table 5 shows the comparative results for English, where Y&M2003 refers to the parser of Yamada and Matsumoto (2003b), CO2006 refers to the parser of Corston-Oliver et al. (2006), Z&C 2008 refers to the combination system of Zhang and Clark (2008), STACK refers to our implementation of the combination parser of Nivre and McDonald (2008), KOO2008 refers to the parser of Koo et al. (2008), Chen2009 refers to the parser of Chen et al. (2009), and Suzuki2009 refers to the parser of Suzuki et al. (2009) that is the best reported result for this data. The results shows that OURS outperformed the first two systems that were based on single models. Z&C 2008 and STACK were the combination systems of graph7 Baseline OURS Y&M2003 CO2006 Z&C2008 STACK KOO2008 Chen2009 Suzuki2009 UAS 91.92 92.21 (+0.29) 90.3 90.8 92.1 92.53 93.16 93.16 93.79 Complete 44.28 45.24 38.4 37.6 45.4 47.06 – 47.15 – based and transition-based models. OURS performed better than Z&C 2008, but worse than STACK. The last three systems that used largescale unlabeled data performed better than OURS. 6 Related work There are seve"
C10-2015,W03-3023,0,0.326783,"on (Culotta and Sorensen, 2004). Graph-based parsing models (McDonald and Pereira, 2006; Carreras, 2007) have achieved state-of-the-art accuracy for a wide range of languages as shown in recent CoNLL shared tasks (Buchholz et al., 2006; Nivre et al., 2007). However, to make parsing tractable, these models are forced to restrict features over a very limited history of parsing decisions (McDonald and Pereira, 2006; McDonald and Nivre, 2007). Previous work showed that rich features over a wide range of decision history can lead to significant improvements in accuracy for transition-based models (Yamada and Matsumoto, 2003a; Nivre et al., 2004). In this paper, we propose an approach to improve graph-based dependency parsing by using decision history. Here, we make an assumption: the dependency relations between words with a short distance are more reliable than ones between words with a long distance. This is supported by the fact that the accuracy of short dependencies is in general greater than that of long dependencies as reported in McDonald and Nivre (2007) for graph-based models. Our idea is to use decision history, which is made in previous scans in a bottom-up procedure, to help parse other words in lat"
C10-2015,C08-1132,0,0.0733273,"ned on training data to provide part-of-speech tags for the development and the test set, and we used 10-way jackknifing to generate tags for the training set. For Chinese, we used the Chinese Treebank (CTB) version 4.04 in the experiments. We also used the “Penn2Malt” tool to convert the data and created a data split: files 1-270 and files 400-931 for training, files 271-300 for testing, and files 301-325 for development. We used gold standard segmentation and part-of-speech tags in the CTB. The data partition and part-of-speech settings were chosen to match previous work (Chen et al., 2008; Yu et al., 2008). We measured the parser quality by the unlabeled attachment score (UAS), i.e., the percentage of tokens with the correct HEAD 5 . And we also evaluated on complete dependency analysis. In our experiments, we implemented our systems on the MSTParser6 and extended with the parent-child-grandchild structures (McDonald and Pereira, 2006; Carreras, 2007). For the baseline systems, we used the first- and second-order (parent-sibling) features that were used in McDonald and Pereira (2006) and other second-order features (parent-child-grandchild) that were used in Carreras (2007). In the following se"
C10-2015,D08-1059,0,0.136222,"hat OURS performed better than Zhao2009, Yu2008, and STACK, but worse than Chen2009 that used largescale unlabeled data (Chen et al., 2009). We also implemented the combination system of OURS and the MALTParser, referred as OURS+STACK in Table 4. The new system achieved further improvement. In future work, we can combine our approach with the parser of Chen et al. (2009). Table 5 shows the comparative results for English, where Y&M2003 refers to the parser of Yamada and Matsumoto (2003b), CO2006 refers to the parser of Corston-Oliver et al. (2006), Z&C 2008 refers to the combination system of Zhang and Clark (2008), STACK refers to our implementation of the combination parser of Nivre and McDonald (2008), KOO2008 refers to the parser of Koo et al. (2008), Chen2009 refers to the parser of Chen et al. (2009), and Suzuki2009 refers to the parser of Suzuki et al. (2009) that is the best reported result for this data. The results shows that OURS outperformed the first two systems that were based on single models. Z&C 2008 and STACK were the combination systems of graph7 Baseline OURS Y&M2003 CO2006 Z&C2008 STACK KOO2008 Chen2009 Suzuki2009 UAS 91.92 92.21 (+0.29) 90.3 90.8 92.1 92.53 93.16 93.16 93.79 Comple"
C10-2015,P09-1007,0,0.0129002,"s are shown in parentheses. The results show that OURS provided better performance over the Baselines by 1.02 points for Chi132 Table 5: Results for English Table 4: Results for Chinese Baseline OURS OURS+STACK Zhao2009 Yu2008 STACK Chen2009 UAS 88.41 89.43(+1.02) 89.53 87.0 87.26 88.95 89.91 Complete 48.85 50.86 49.42 – – 49.42 48.56 nese and 0.29 points for English. The improvements of (OURS) were significant in McNemar’s Test with p &lt; 10−4 for Chinese and p &lt; 10−3 for English. 5.3 Comparative results Table 4 shows the comparative results for Chinese, where Zhao2009 refers to the result of (Zhao et al., 2009), Yu2008 refers to the result of Yu et al. (2008), Chen2009 refers to the result of Chen et al. (2009) that is the best reported result on this data, and STACK refers to our implementation of the combination parser of Nivre and McDonald (2008) using our baseline system and the MALTParser7 . The results indicated that OURS performed better than Zhao2009, Yu2008, and STACK, but worse than Chen2009 that used largescale unlabeled data (Chen et al., 2009). We also implemented the combination system of OURS and the MALTParser, referred as OURS+STACK in Table 4. The new system achieved further improv"
C10-2015,D07-1096,0,\N,Missing
C12-1169,J92-4003,0,0.0554287,"Missing"
C12-1169,W10-2910,0,0.35934,"to deal with the polarity shifting, 179 Chinese polarity shifting words were collected and used in the CEIA. As for the features, we used the same features as those in Nakagawa et al. (2010). 3 New Features In this section, we describe our approach that effectively employs the dependency information, semantic class and distance information into the above evaluative information extraction (specifically evaluative expression extraction and evaluation target extraction). 3.1 Dependency Features The use of syntactic or deep linguistic features has been tried in opinion analysis in the literature. Johansson and Moschitti (2010) demonstrated that the features derived from grammatical and 2780 w w w i w i w w i+1 (1) w i+1 (2) w w i (3) w i w i+1 h h i+1 w i+1 (4) w h w i (5) w h w w h1 h2 w i i+1 (6) Figure 3: Different dependencies between w i and w i+1 that can be linked by one or two arcs semantic role structure can be used to improve the detection of opinionated expressions in subjectivity analysis. However, based on their evaluation, the precision decreases while the F-measure is increased. In addition, they claimed that a sequence tagging model cannot be used when using syntactic features, and they used reranki"
C12-1169,P06-2059,0,0.122589,"first introduce the specifications of the evaluative information on which this study is focused, and then we explain how an evaluative information corpus is constructed. Finally, we explain each process of CEIA in detail. 2.1 Evaluative Information There is a wide variety of evaluative information on the web, such as reviews of products and criticisms of policies. The information reflects various perspectives of individuals or organizations. Research on evaluative information analysis are conducted from different points of views and at different levels of granularity (Kobayashi et al., 2004; Kaji and Kitsuregawa, 2006; Liu, 2010; Pang and Lee, 2008; Akamine et al., 2010). In this section, we describe the specifications of evaluative information on which this study is focused. We analyze the evaluative information at a fine-grained level. We use a 5-tuple that consists of (1) an evaluative expression, (2) an evaluation holder, (3) an evaluation target, (4) an evaluation type, and (5) sentiment polarity as the basic unit of evaluative information and call it an evaluative information set. Each item is defined as follows. Evaluative expression is a span of text that describes the evaluation. It can be a singl"
C12-1169,P08-1047,1,0.848584,"’s head in a sentence. Since the grammatical information is very important for evaluation target extraction, new features of { depri−2 , depri−1 , depri , depri+1 , depri+2 , depri−1 &depri , depri &d epri+1 } are added for evaluation target extraction. With these features, the grammatical function information can be encoded in both the evaluative expression and evaluation target extraction tasks. 3.2 Semantic Class Features The idea of combining semantic classes of words with discriminative learning has been previously reported in the context of named entity recognition (Miller et al., 2004; Kazama and Torisawa, 2008), dependency parsing (Koo et al., 2008) and Chinese word segmentation and POS tagging (Wang et al., 2011). We adopt and extend these techniques to evaluative information analysis and demonstrate their effectiveness in this task. We produced the semantic classes of various levels of granularity, by using the Brown cluster hierarchy (Brown et al., 1992) at various lengths. Note that a semantic class is represented by a bit string that reflects the branching of the semantic class hierarchy. We designed two kinds of semantic class features: (i) full string feature: full string of the semantic clas"
C12-1169,P08-1068,0,0.0197209,"ormation is very important for evaluation target extraction, new features of { depri−2 , depri−1 , depri , depri+1 , depri+2 , depri−1 &depri , depri &d epri+1 } are added for evaluation target extraction. With these features, the grammatical function information can be encoded in both the evaluative expression and evaluation target extraction tasks. 3.2 Semantic Class Features The idea of combining semantic classes of words with discriminative learning has been previously reported in the context of named entity recognition (Miller et al., 2004; Kazama and Torisawa, 2008), dependency parsing (Koo et al., 2008) and Chinese word segmentation and POS tagging (Wang et al., 2011). We adopt and extend these techniques to evaluative information analysis and demonstrate their effectiveness in this task. We produced the semantic classes of various levels of granularity, by using the Brown cluster hierarchy (Brown et al., 1992) at various lengths. Note that a semantic class is represented by a bit string that reflects the branching of the semantic class hierarchy. We designed two kinds of semantic class features: (i) full string feature: full string of the semantic class for w i ; (ii) 6-bit prefix feature:"
C12-1169,N04-1043,0,0.0182696,"between w i and w i ’s head in a sentence. Since the grammatical information is very important for evaluation target extraction, new features of { depri−2 , depri−1 , depri , depri+1 , depri+2 , depri−1 &depri , depri &d epri+1 } are added for evaluation target extraction. With these features, the grammatical function information can be encoded in both the evaluative expression and evaluation target extraction tasks. 3.2 Semantic Class Features The idea of combining semantic classes of words with discriminative learning has been previously reported in the context of named entity recognition (Miller et al., 2004; Kazama and Torisawa, 2008), dependency parsing (Koo et al., 2008) and Chinese word segmentation and POS tagging (Wang et al., 2011). We adopt and extend these techniques to evaluative information analysis and demonstrate their effectiveness in this task. We produced the semantic classes of various levels of granularity, by using the Brown cluster hierarchy (Brown et al., 1992) at various lengths. Note that a semantic class is represented by a bit string that reflects the branching of the semantic class hierarchy. We designed two kinds of semantic class features: (i) full string feature: full"
C12-1169,N10-1120,0,0.298116,"s to the evaluation expressions and the words next to the evaluative expressions. For CRF model, we use the same features as in Section 2.4.2. If no holder was found by the CRF model, [undefined] was set as the evaluation holder of the current evaluation expression. 2.4.5 Determination of Sentiment Polarity A typical approach for sentiment classification is to use supervised machine learning algorithms with bag-of-words as features (Pang et al., 2002). However, this method cannot consider syntactic structures that seem essential to infer the polarity of a whole sentence. We follow the work of Nakagawa et al. (2010) and use a dependency tree-based method, which was demonstrated to perform better than other methods based on bag-of-words in both English and Japanese sentiment classification tasks. The sentiment polarity is classified using conditional random fields (CRFs) with hidden variables. In the method, the sentiment polarity of each dependency subtree, which is not observable in training data, is represented by a hidden variable. The polarity of the whole sentence is calculated by considering the interactions between the hidden variables. For example in Figure 2, each phrase (indicated by a circle)"
C12-1169,W02-1011,0,0.0115801,"..ws−1 &ws , t s−1 &t s } and bigram n {ws+1+1 &ws+n+2 , t s+n+1 &t s+n+2 ...w l−1 &w l , t l−1 &t l } features to add the bigram information for the words previous to the evaluation expressions and the words next to the evaluative expressions. For CRF model, we use the same features as in Section 2.4.2. If no holder was found by the CRF model, [undefined] was set as the evaluation holder of the current evaluation expression. 2.4.5 Determination of Sentiment Polarity A typical approach for sentiment classification is to use supervised machine learning algorithms with bag-of-words as features (Pang et al., 2002). However, this method cannot consider syntactic structures that seem essential to infer the polarity of a whole sentence. We follow the work of Nakagawa et al. (2010) and use a dependency tree-based method, which was demonstrated to perform better than other methods based on bag-of-words in both English and Japanese sentiment classification tasks. The sentiment polarity is classified using conditional random fields (CRFs) with hidden variables. In the method, the sentiment polarity of each dependency subtree, which is not observable in training data, is represented by a hidden variable. The p"
C12-1169,I11-1035,1,0.926005,"features of { depri−2 , depri−1 , depri , depri+1 , depri+2 , depri−1 &depri , depri &d epri+1 } are added for evaluation target extraction. With these features, the grammatical function information can be encoded in both the evaluative expression and evaluation target extraction tasks. 3.2 Semantic Class Features The idea of combining semantic classes of words with discriminative learning has been previously reported in the context of named entity recognition (Miller et al., 2004; Kazama and Torisawa, 2008), dependency parsing (Koo et al., 2008) and Chinese word segmentation and POS tagging (Wang et al., 2011). We adopt and extend these techniques to evaluative information analysis and demonstrate their effectiveness in this task. We produced the semantic classes of various levels of granularity, by using the Brown cluster hierarchy (Brown et al., 1992) at various lengths. Note that a semantic class is represented by a bit string that reflects the branching of the semantic class hierarchy. We designed two kinds of semantic class features: (i) full string feature: full string of the semantic class for w i ; (ii) 6-bit prefix feature: 6-bit prefix of the semantic class for w i . 3.3 Distance Feature"
D07-1033,P04-1056,0,0.0159494,"to “local” features, which only depend on a very small number of labels (usually two: the previous and the current). Although this limitation makes training and inference tractable, it also excludes the use of possibly useful “non-local” features that are accessible after all labels are determined. For example, non-local features such as “same phrases in a document do not We propose a new perceptron algorithm in this paper that can use non-local features along with local features. Although several methods have already been proposed to incorporate non-local features (Sutton and McCallum, 2004; Bunescu and Mooney, 2004; Finkel et al., 2005; Roth and Yih, 2005; Krishnan and Manning, 2006; Nakagawa and Matsumoto, 2006), these present a problem that the types of non-local features are somewhat constrained. For example, Finkel et al. (2005) enabled the use of non-local features by using Gibbs sampling. However, it is unclear how to apply their method of determining the parameters of a non-local model to other types of non-local features, which they did not used. Roth and Yih (2005) enabled the use of hard constraints on labels by using integer linear programming. However, this is equivalent to only allowing non"
D07-1033,P02-1034,0,0.253726,"ctable for the same reason. To deal with this problem, we ﬁrst relaxed our objective. The modiﬁed objective was to ﬁnd a good model from those with the form: {y n } = n-besty Φl (x, y) · α y ′ = argmaxy ∈{y n } Φa (x, y) · α, (1) That is, we ﬁrst generate n-best candidates {y n } under the local model, Φl (x, y) · α. This can be done efﬁciently using the A* algorithm. We then ﬁnd the best scoring candidate under the total model, Φa (x, y)·α, only from these n-best candidates. If n is moderately small, this can also be done in a practical amount of time. This resembles the re-ranking approach (Collins and Duffy, 2002; Collins, 2002b). However, unlike the re-ranking approach, the local model, Φl (x, y) · α, and the total model, Φa (x, y) · α, correlate since they share a part of the vector and are trained at the same time in our algorithm. The re-ranking approach has the disadvantage that it is necessary to use different training corpora for the ﬁrst model and for the second, or to use cross validation type training, to make the training for the second meaningful. This reduces the effective size of training data or increases training time substantially. On the other hand, our algorithm has no such disadvan"
D07-1033,P04-1015,0,0.080534,"local features. Therefore, our objective in this study was to establish a framework, where all 315 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 315–324, Prague, June 2007. 2007 Association for Computational Linguistics types of non-local features are allowed. With non-local features, we cannot use efﬁcient procedures such as forward-backward procedures and the Viterbi algorithm that are required in training CRFs (Lafferty et al., 2001) and perceptrons (Collins, 2002a). Recently, several methods (Collins and Roark, 2004; Daum´e III and Marcu, 2005; McDonald and Pereira, 2006) have been proposed with similar motivation to ours. These methods alleviate this problem by using some approximation in perceptron-type learning. In this paper, we follow this line of research and try to solve the problem by extending Collins’ perceptron algorithm (Collins, 2002a). We exploited the not-so-familiar fact that we can design a perceptron algorithm with guaranteed convergence if we can ﬁnd at least one wrong labeling candidate even if we cannot perform exact inference. We ﬁrst ran the A* search only using local features to g"
D07-1033,W02-1001,0,0.0818314,"are determined from the sequence and the labels. The weights of local and non-local features are learned together in the training process with guaranteed convergence. We present experimental results from the CoNLL 2003 named entity recognition (NER) task to demonstrate the performance of the proposed algorithm. 1 Introduction Many NLP tasks such as POS tagging and named entity recognition have recently been solved as sequence labeling. Discriminative methods such as Conditional Random Fields (CRFs) (Lafferty et al., 2001), Semi-Markov Random Fields (Sarawagi and Cohen, 2004), and perceptrons (Collins, 2002a) have been popular approaches for sequence labeling because of their excellent performance, which is mainly due to their ability to incorporate many kinds of overlapping and non-independent features. However, the common limitation of these methods is that the features are limited to “local” features, which only depend on a very small number of labels (usually two: the previous and the current). Although this limitation makes training and inference tractable, it also excludes the use of possibly useful “non-local” features that are accessible after all labels are determined. For example, non-"
D07-1033,P06-1096,0,0.052969,"Missing"
D07-1033,E06-1011,0,0.352989,"y was to establish a framework, where all 315 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 315–324, Prague, June 2007. 2007 Association for Computational Linguistics types of non-local features are allowed. With non-local features, we cannot use efﬁcient procedures such as forward-backward procedures and the Viterbi algorithm that are required in training CRFs (Lafferty et al., 2001) and perceptrons (Collins, 2002a). Recently, several methods (Collins and Roark, 2004; Daum´e III and Marcu, 2005; McDonald and Pereira, 2006) have been proposed with similar motivation to ours. These methods alleviate this problem by using some approximation in perceptron-type learning. In this paper, we follow this line of research and try to solve the problem by extending Collins’ perceptron algorithm (Collins, 2002a). We exploited the not-so-familiar fact that we can design a perceptron algorithm with guaranteed convergence if we can ﬁnd at least one wrong labeling candidate even if we cannot perform exact inference. We ﬁrst ran the A* search only using local features to generate n-best candidates (this can be efﬁciently perform"
D07-1033,P05-1012,0,0.0453541,"een, the margin approaches at least half of true 317 margin δ (at the cost of inﬁnite training time), as C → ∞. Note that if the features are all local, the secondbest candidate (generally n-best candidates) can also be found efﬁciently by using an A* search that uses the best scores calculated during a Viterbi search as the heuristic estimation (Soong and Huang, 1991). There are other methods for improving robustness by making margin larger for the structural output problem. Such methods include ALMA (Gentile, 2001) used in (Daum´e III and Marcu, 2005)2 , MIRA (Crammer et al., 2006) used in (McDonald et al., 2005), and Max-Margin Markov Networks (Taskar et al., 2003). However, to the best of our knowledge, there has been no prior work that has applied a perceptron with a margin (Krauth and M´ezard, 1987) to structured output.3 Our method described in this section is one of the easiest to implement, while guaranteeing a large margin. We found in the experiments that our method outperformed the Collins’ averaged perceptron by a large margin. 4 Algorithm 4.1 Deﬁnition and Basic Idea Having described the basic perceptron algorithms, we will know explain our algorithm that learns the weights of local and no"
D07-1033,P06-1089,0,0.0217222,"ous and the current). Although this limitation makes training and inference tractable, it also excludes the use of possibly useful “non-local” features that are accessible after all labels are determined. For example, non-local features such as “same phrases in a document do not We propose a new perceptron algorithm in this paper that can use non-local features along with local features. Although several methods have already been proposed to incorporate non-local features (Sutton and McCallum, 2004; Bunescu and Mooney, 2004; Finkel et al., 2005; Roth and Yih, 2005; Krishnan and Manning, 2006; Nakagawa and Matsumoto, 2006), these present a problem that the types of non-local features are somewhat constrained. For example, Finkel et al. (2005) enabled the use of non-local features by using Gibbs sampling. However, it is unclear how to apply their method of determining the parameters of a non-local model to other types of non-local features, which they did not used. Roth and Yih (2005) enabled the use of hard constraints on labels by using integer linear programming. However, this is equivalent to only allowing non-local features whose weights are ﬁxed to negative inﬁnity. Krishnan and Manning (2006) divided the"
D07-1033,W95-0107,0,0.0226383,"ﬂing the training examples.4 However, it is very time consuming to run the complete training process several times. We thus ran the training in only one pass over the shufﬂed examples several times, and used the averaged output weight vectors as a new initial weight vector, because we thought that the early part of training would be more seriously affected by the order of examples. We call this “BPM initialization”. 5 5 Named Entity Recognition and Non-Local Features We evaluated the performance of the proposed algorithm using the named entity recognition task. We adopted IOB (IOB2) labeling (Ramshaw and Marcus, 1995), where the ﬁrst word of an entity of class “C” is labeled “B-C”, the words in the entity are labeled “I-C”, and other words are labeled “O”. We used non-local features based on Finkel et al. (2005). These features are based on observations such as “same phrases in a document tend to have the same entity class” (phrase consistency) and “a sub-phrase of a phrase tends to have the same entity class as the phrase” (sub-phrase consistency). We also implemented the “majority” version of these features as used in Krishnan and Manning (2006). In addition, we used non-local features, which are based o"
D07-1033,P05-1045,0,0.221453,"only depend on a very small number of labels (usually two: the previous and the current). Although this limitation makes training and inference tractable, it also excludes the use of possibly useful “non-local” features that are accessible after all labels are determined. For example, non-local features such as “same phrases in a document do not We propose a new perceptron algorithm in this paper that can use non-local features along with local features. Although several methods have already been proposed to incorporate non-local features (Sutton and McCallum, 2004; Bunescu and Mooney, 2004; Finkel et al., 2005; Roth and Yih, 2005; Krishnan and Manning, 2006; Nakagawa and Matsumoto, 2006), these present a problem that the types of non-local features are somewhat constrained. For example, Finkel et al. (2005) enabled the use of non-local features by using Gibbs sampling. However, it is unclear how to apply their method of determining the parameters of a non-local model to other types of non-local features, which they did not used. Roth and Yih (2005) enabled the use of hard constraints on labels by using integer linear programming. However, this is equivalent to only allowing non-local features whose"
D07-1033,P06-1141,0,0.350627,"bels (usually two: the previous and the current). Although this limitation makes training and inference tractable, it also excludes the use of possibly useful “non-local” features that are accessible after all labels are determined. For example, non-local features such as “same phrases in a document do not We propose a new perceptron algorithm in this paper that can use non-local features along with local features. Although several methods have already been proposed to incorporate non-local features (Sutton and McCallum, 2004; Bunescu and Mooney, 2004; Finkel et al., 2005; Roth and Yih, 2005; Krishnan and Manning, 2006; Nakagawa and Matsumoto, 2006), these present a problem that the types of non-local features are somewhat constrained. For example, Finkel et al. (2005) enabled the use of non-local features by using Gibbs sampling. However, it is unclear how to apply their method of determining the parameters of a non-local model to other types of non-local features, which they did not used. Roth and Yih (2005) enabled the use of hard constraints on labels by using integer linear programming. However, this is equivalent to only allowing non-local features whose weights are ﬁxed to negative inﬁnity. Krishnan"
D07-1033,W03-0419,0,0.0443427,"Missing"
D07-1033,P02-1062,0,\N,Missing
D07-1073,E06-1002,0,0.397121,"Missing"
D07-1073,W02-1028,0,0.0347985,". These category labels are used as features in a CRF-based NE tagger. We demonstrate using the CoNLL 2003 dataset that the Wikipedia category labels extracted by such a simple method actually improve the accuracy of NER. 1 Introduction It has been known that Gazetteers, or entity dictionaries, are important for improving the performance of named entity recognition. However, building and maintaining high-quality gazetteers is very time consuming. Many methods have been proposed for solving this problem by automatically extracting gazetteers from large amounts of texts (Riloff and Jones, 1999; Thelen and Riloff, 2002; Etzioni et al., 2005; Shinzato et al., 2006; Talukdar et al., 2006; Nadeau et al., 2006). However, these methods require complicated induction of patterns or statistical methods to extract high-quality gazetteers. We have recently seen a rapid and successful growth of Wikipedia (http://www.wikipedia.org), which is an open, collaborative encyclopedia on the Web. Wikipedia has now more than 1,700,000 articles on the English version (March 2007) and the number is still increasing. Since Wikipedia aims to be an encyclopedia, most articles are about named entities and they are more structured tha"
D07-1073,W03-0419,0,0.0793043,"Missing"
D07-1073,W06-2809,0,0.267886,"Missing"
D07-1073,D07-1033,1,0.327366,"ost $17,000”, as: RareO JimiB-guitarist HendrixI-guitarist songO draftO forO almostO $17,000O .O Note that we adopted the leftmost longest match if there were several possible matchings. These IOB2 tags were used in the same way as other features 7 http://www.cs.utah.edu/˜hal/TagChunk/ We use bare “B”, “I”, and “O” tags if we want to represent only the matching information. 8 701 4 Experiments In this section, we demonstrate the usefulness of the extracted category labels for NER. 4.1 Data and setting 9 We used sentence concatenation because we found it improves the accuracy in another study (Kazama and Torisawa, 2007). 10 http://www.cs.utah.edu/˜hal/TagChunk/ 11 This is not because TagChunk overﬁts the CoNLL 2003 dataset (TagChunk is trained on the Penn Treebank (Wall Street Journal), while the CoNLL 2003 data are taken from the Reuters corpus). Table 1: Baseline features. The value of a node feature is determined from the current label, y0 , and a surface feature determined only from x. The value of an edge feature is determined by the previous label, y−1 , the current label, y0 , and a surface feature. Used surface features are the word (w), the downcased word (wl), the POS tag (pos), the chunk tag (chk)"
D07-1073,N06-1025,0,0.0722441,"rce, extracting knowledge such as gazetteers from Wikipedia will be much easier than from raw texts or from usual Web texts because of its structure. It is also important that Wikipedia is updated every day and therefore new named entities are added constantly. We think that extracting knowledge from Wikipedia for natural language processing is one of the promising ways towards enabling large-scale, real-life applications. In fact, many studies that try to exploit Wikipedia as a knowledge source have recently emerged (Bunescu and Pas¸ca, 2006; Toral and Mu˜noz, 2006; Ruiz-Casado et al., 2006; Ponzetto and Strube, 2006; Strube and Ponzetto, 2006; Zesch et al., 2007). We explore the use of Wikipedia as external knowledge to improve named entity recognition (NER). Our method retrieves the corresponding Wikipedia entry for each candidate word sequence and extracts a category label from the ﬁrst sentence of the entry, which can be thought of as a deﬁnition part. These category labels are used as features in a CRF-based NE tagger. We demonstrate using the CoNLL 2003 dataset that the Wikipedia category labels extracted by such a simple method actually improve the accuracy of NER. 1 Introduction It has been known"
D07-1073,sekine-etal-2002-extended,0,0.172569,"Missing"
D07-1073,W06-2919,0,0.0298674,"We demonstrate using the CoNLL 2003 dataset that the Wikipedia category labels extracted by such a simple method actually improve the accuracy of NER. 1 Introduction It has been known that Gazetteers, or entity dictionaries, are important for improving the performance of named entity recognition. However, building and maintaining high-quality gazetteers is very time consuming. Many methods have been proposed for solving this problem by automatically extracting gazetteers from large amounts of texts (Riloff and Jones, 1999; Thelen and Riloff, 2002; Etzioni et al., 2005; Shinzato et al., 2006; Talukdar et al., 2006; Nadeau et al., 2006). However, these methods require complicated induction of patterns or statistical methods to extract high-quality gazetteers. We have recently seen a rapid and successful growth of Wikipedia (http://www.wikipedia.org), which is an open, collaborative encyclopedia on the Web. Wikipedia has now more than 1,700,000 articles on the English version (March 2007) and the number is still increasing. Since Wikipedia aims to be an encyclopedia, most articles are about named entities and they are more structured than raw As a ﬁrst step towards such approach, we demonstrate in this p"
D09-1060,W08-2102,0,0.197319,"orating the subtree-based features. Table 2 shows the performance of the systems that were compared, where Y&M2003 refers to the parser of Yamada and Matsumoto (2003), CO2006 refers to the parser of Corston-Oliver et al. (2006), Hall2006 refers to the parser of Hall et al. (2006), Wang2007 refers to the parser of Wang et al. (2007), Z&C 2008 refers to the combination graph-based and transition-based system of Zhang and Clark (2008), KOO08-dep1c/KOO08dep2c refers to a graph-based system with first/second-order cluster-based features by Koo et al. (2008), and Carreras2008 refers to the paper of Carreras et al. (2008). The results showed that Ord2s performed better than the first five systems. The second-order system of Koo et al. (2008) performed better than our systems. The reason may be that the MSTParser only uses sibling interactions for second-order, while Koo et al. (2008) uses both sibling and grandparent interactions, and uses cluster-based features. Carreras et al. (2008) reported a very high accuracy using information of constituent structure of the TAG grammar formalism. In our systems, we did not use such knowledge. Our subtree-based features could be combined 4.1.1 Main results of English dat"
D09-1060,N06-1021,0,0.0749069,"Missing"
D09-1060,A00-1031,0,0.0251006,"or testing, and files 301-325 for development. We used gold standard segmentation and part-of-speech tags in the CTB. The data partition and part-of-speech settings were chosen to match previous work (Chen et al., 2008; Yu et al., 2008). For the unannotated data, we used the PFR corpus10 , which has approximately 15 million words whose segmentation and POS tags are given. We used its original segmentation though there are differences in segmentation policy between CTB and this corpus. As for POS tags, we discarded the original POS tags and assigned CTB style POS tags using a TNT-based tagger (Brants, 2000) trained on the training data. We used the Basic Parser to process all the sentences of the PFR corpus. We measured the parser quality by the unlabeled attachment score (UAS), i.e., the percentage of tokens (excluding all punctuation tokens) with the correct HEAD. And we also evaluated on complete dependency analysis. The results are shown in Table 1, where Ord1/Ord2 refers to a first-/second-order MSTParser with basic features, Ord1s/Ord2s refers to a first-/second-order MSTParser with basic+subtree-based features, and the improvements by the subtree-based features over the basic features are"
D09-1060,W06-2920,0,0.0596271,"features for the parsing models. 3.1 Subtrees extraction To ease explanation, we transform the dependency structure into a more tree-like structure as shown in Figure 2, the sentence is the same as the one in Figure 1. ROOT ROOT I ate the fish with a fork ate . Figure 1: Example for dependency structure I fish with . the fork 2.1 Parsing approach For dependency parsing, there are two main types of parsing models (Nivre and McDonald, 2008): graph-based model and transition-based model, which achieved state-of-the-art accuracy for a wide range of languages as shown in recent CoNLL shared tasks (Buchholz et al., 2006; Nivre et al., 2007). Our subtree-based features can be applied in both of the two parsing models. In this paper, as the base parsing system, we employ the graph-based MST parsing model proposed by McDonald et al. (2005) and McDonald and Pereira (2006), which uses the idea of Maximum Spanning Trees of a graph and large margin structured learning algorithms. The details a I ate the fish with a fork . Figure 2: Example for dependency structure in tree-format Our task is to extract subtrees from dependency trees. If a subtree contains two nodes, we call it a bigram-subtree. If a subtree contains"
D09-1060,D07-1101,0,0.315431,"forms of heads. Specifically, for any feature related to word form, we remove this feature if the word is not one of the Top-N most frequent words in the training data. We used N=1000 for the experiments in this paper. This method can reduce the size of the feature sets. In this paper, we only used bigram-subtrees and the limited form of trigram-subtrees, though in theory we can use k-gram-subtrees, which are limited in the same way as our trigram subtrees, in (k-1)th-order MST parsing models mentioned in McDonald and Pereira (2006) or use grandparenttype trigram-subtrees in parsing models of Carreras (2007). Although the higher-order MST parsing models will be slow with exact inference, requiring O(nk ) time (McDonald and Pereira, 2006), it might be possible to use higher-order kgram subtrees with approximated parsing model in the future. Of course, our method can also be easily extended to the labeled dependency case. d+1 … (a) …h … d1 … d2 … (b) Figure 4: Word pairs and triple for feature representation in Figure 5, where h is “ate” and d is “with”. We can generate the features for the pairs linked by dashed-lines, such as h − d, h − d+1 and so on. Then we have the temporary bigram-subtrees “a"
D09-1060,I08-1012,1,0.908673,"he BLLIP corpus. For Chinese, we used the Chinese Treebank 7 http://w3.msi.vxu.se/˜nivre/research/Penn2Malt.html We ensured that the text used for extracting subtrees did not include the sentences of the Penn Treebank. 8 (CTB) version 4.09 in the experiments. We also used the “Penn2Malt” tool to convert the data and created a data split: files 1-270 and files 400-931 for training, files 271-300 for testing, and files 301-325 for development. We used gold standard segmentation and part-of-speech tags in the CTB. The data partition and part-of-speech settings were chosen to match previous work (Chen et al., 2008; Yu et al., 2008). For the unannotated data, we used the PFR corpus10 , which has approximately 15 million words whose segmentation and POS tags are given. We used its original segmentation though there are differences in segmentation policy between CTB and this corpus. As for POS tags, we discarded the original POS tags and assigned CTB style POS tags using a TNT-based tagger (Brants, 2000) trained on the training data. We used the Basic Parser to process all the sentences of the PFR corpus. We measured the parser quality by the unlabeled attachment score (UAS), i.e., the percentage of token"
D09-1060,P06-2041,0,0.0127447,"r MST parsing models. For baseline systems, we used the first- and second-order basic features, which were the same as the features used by McDonald and Pereira (2006), and we used the default settings of MSTParser throughout the paper: iters=10; training-k=1; decode-type=proj. We implemented our systems based on the MSTParser by incorporating the subtree-based features. Table 2 shows the performance of the systems that were compared, where Y&M2003 refers to the parser of Yamada and Matsumoto (2003), CO2006 refers to the parser of Corston-Oliver et al. (2006), Hall2006 refers to the parser of Hall et al. (2006), Wang2007 refers to the parser of Wang et al. (2007), Z&C 2008 refers to the combination graph-based and transition-based system of Zhang and Clark (2008), KOO08-dep1c/KOO08dep2c refers to a graph-based system with first/second-order cluster-based features by Koo et al. (2008), and Carreras2008 refers to the paper of Carreras et al. (2008). The results showed that Ord2s performed better than the first five systems. The second-order system of Koo et al. (2008) performed better than our systems. The reason may be that the MSTParser only uses sibling interactions for second-order, while Koo et a"
D09-1060,C08-1054,0,0.0118978,"rds because their numbers were very small. The Better and Worse curves showed that our approach always provided better results. The results indicated that the improvements apparently became larger when the sentences had more unknown words for the Chinese data. And for the English data, the graph also showed the similar trend, although the improvements for the sentences have three and four unknown words were slightly less than the others. 4.2.2 Coordinating conjunctions We analyzed our new parsers’ behavior for coordinating conjunction structures, which is a very difficult problem for parsing (Kawahara and Kurohashi, 2008). Here, we compared the Ord2 system with the Ord2s system. Figures 9 and 10 show how the subtree-based features affect accuracy as a function of the number of conjunctions, where the x axis refers to the number of conjunctions in one sentence and the y axis shows the percentages of the three classes. The figures indicated that the subtree-based features improved the coordinating conjunction problem. In the trigram-subtree list, many subtrees are related to coordinating conjunctions, such as “utilities:1:3 and:2:3 businesses:3:0” and “pull:1:0 and:2:1 protect:3:1”. These subtrees can provide ad"
D09-1060,P08-1068,0,0.550631,"ive idea to improve dependency parsing performance. In this paper, we present an approach that extracts subtrees from dependency trees in autoparsed data to improve dependency parsing. The 570 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 570–579, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP glish and Chinese data. We show that this simple approach greatly improves the accuracy and that the use of richer structures (i.e, word triples) indeed gives additional improvement. We also demonstrate that our approach and other improvement techniques (Koo et al., 2008; Nivre and McDonald, 2008) are complementary and that we can achieve very high accuracies when we combine our method with other improvement techniques. Specifically, we achieve the best accuracy for the Chinese data. The rest of this paper is as follows: Section 2 introduces the background of dependency parsing. Section 3 proposes an approach for extracting subtrees and represents the subtree-based features for dependency parsers. Section 4 explains the experimental results and Section 5 discusses related work. Finally, in section 6 we draw conclusions. 2 Dependency parsing Dependency parsing"
D09-1060,J93-2004,0,0.0327919,"m-subtree is formed by the word forms of h, d1, and d2. Then we retrieve the subtree in Lst to get its set ID. In addition, we consider the triples of “h-NULL”6 , d1, and d2, which means that we only check the words of sibling nodes without checking the head word. Then, we generate second-order subtree-based features, consisting of indicator functions for set IDs of the retrieved trigram-subtrees. 6 h-NULL is a dummy token 573 In order to evaluate the effectiveness of the subtree-based features, we conducted experiments on English data and Chinese Data. For English, we used the Penn Treebank (Marcus et al., 1993) in our experiments and the tool “Penn2Malt”7 to convert the data into dependency structures using a standard set of head rules (Yamada and Matsumoto, 2003). To match previous work (McDonald et al., 2005; McDonald and Pereira, 2006; Koo et al., 2008), we split the data into a training set (sections 2-21), a development set (Section 22), and a test set (section 23). Following the work of Koo et al. (2008), we used the MXPOST (Ratnaparkhi, 1996) tagger trained on training data to provide part-of-speech tags for the development and the test set, and we used 10way jackknifing to generate tags for"
D09-1060,P06-1043,0,0.179445,"Missing"
D09-1060,D08-1059,0,0.606476,"d Pereira (2006), and we used the default settings of MSTParser throughout the paper: iters=10; training-k=1; decode-type=proj. We implemented our systems based on the MSTParser by incorporating the subtree-based features. Table 2 shows the performance of the systems that were compared, where Y&M2003 refers to the parser of Yamada and Matsumoto (2003), CO2006 refers to the parser of Corston-Oliver et al. (2006), Hall2006 refers to the parser of Hall et al. (2006), Wang2007 refers to the parser of Wang et al. (2007), Z&C 2008 refers to the combination graph-based and transition-based system of Zhang and Clark (2008), KOO08-dep1c/KOO08dep2c refers to a graph-based system with first/second-order cluster-based features by Koo et al. (2008), and Carreras2008 refers to the paper of Carreras et al. (2008). The results showed that Ord2s performed better than the first five systems. The second-order system of Koo et al. (2008) performed better than our systems. The reason may be that the MSTParser only uses sibling interactions for second-order, while Koo et al. (2008) uses both sibling and grandparent interactions, and uses cluster-based features. Carreras et al. (2008) reported a very high accuracy using infor"
D09-1060,E06-1011,0,0.206252,"explains the experimental results and Section 5 discusses related work. Finally, in section 6 we draw conclusions. 2 Dependency parsing Dependency parsing assigns head-dependent relations between the words in a sentence. A simple example is shown in Figure 1, where an arc between two words indicates a dependency relation between them. For example, the arc between “ate” and “fish” indicates that “ate” is the head of “fish” and “fish” is the dependent. The arc between “ROOT” and “ate” indicates that “ate” is the ROOT of the sentence. of parsing model were presented in McDonald et al. (2005) and McDonald and Pereira (2006). 2.2 Baseline Parser In the MST parsing model, there are two well-used modes: the first-order and the second-order. The first-order model uses first-order features that are defined over single graph edges and the secondorder model adds second-order features that are defined on adjacent edges. For the parsing of unannotated data, we use the first-order MST parsing model, because we need to parse a large number of sentences and the parser must be fast. We call this parser the Baseline Parser. 3 Our approach In this section, we describe our approach of extracting subtrees from unannotated data."
D09-1060,W08-2127,0,0.0175142,"cy parsing results for English, for our parsers and previous work To demonstrate that our approach and other work are complementary, we thus implemented a system using all the techniques we had at hand that used subtree- and cluster-based features and applied the integrating method of Nivre and McDonald (2008). We used the word clustering tool12 , which was used by Koo et al. (2008), to produce word clusters on the BLLIP corpus. The cluster-based features were the same as the features used by Koo et al. (2008). For the integrating method, we used the transition MaxEnt-based parser of Zhao and Kit (2008) because it was faster than the MaltParser. The results are shown in the bottom part of Table 2, where Ord1c/Ord2c refers to a first-/second-order MSTParser with cluster-based features, Ord1i/Ordli refers to a first/second-order MSTParser with integrating-based features, Ord1sc/Ord2sc refers to a first-/secondorder MSTParser with subtree-based+clusterbased features, and Ord1sci/Ord2sci refers to a first-/second-order MSTParser with subtreebased+cluster-based+integrating-based features. Ord1c/Ord2c was worse than KOO08-dep1c/dep2c, but Ord1sci outperformed KOO08-dep1c Chinese UAS 86.38 87.68(+1"
D09-1060,P05-1012,0,0.843284,"endency parsers. Section 4 explains the experimental results and Section 5 discusses related work. Finally, in section 6 we draw conclusions. 2 Dependency parsing Dependency parsing assigns head-dependent relations between the words in a sentence. A simple example is shown in Figure 1, where an arc between two words indicates a dependency relation between them. For example, the arc between “ate” and “fish” indicates that “ate” is the head of “fish” and “fish” is the dependent. The arc between “ROOT” and “ate” indicates that “ate” is the ROOT of the sentence. of parsing model were presented in McDonald et al. (2005) and McDonald and Pereira (2006). 2.2 Baseline Parser In the MST parsing model, there are two well-used modes: the first-order and the second-order. The first-order model uses first-order features that are defined over single graph edges and the secondorder model adds second-order features that are defined on adjacent edges. For the parsing of unannotated data, we use the first-order MST parsing model, because we need to parse a large number of sentences and the parser must be fast. We call this parser the Baseline Parser. 3 Our approach In this section, we describe our approach of extracting"
D09-1060,2006.iwslt-evaluation.9,0,0.0600461,"onstrate the effectiveness of our proposed approach, we present the experimental results on the English Penn Treebank and the Chinese Penn Treebank. These results show that our approach significantly outperforms baseline systems. And, it achieves the best accuracy for the Chinese data and an accuracy which is competitive with the best known systems for the English data. 1 Introduction Dependency parsing, which attempts to build dependency links between words in a sentence, has experienced a surge of interest in recent times, owing to its usefulness in such applications as machine translation (Nakazawa et al., 2006) and question answering (Cui et al., 2005). To obtain dependency parsers with high accuracy, supervised techniques require a large amount of handannotated data. While hand-annotated data are very expensive, large-scale unannotated data can be obtained easily. Therefore, the use of largescale unannotated data in training is an attractive idea to improve dependency parsing performance. In this paper, we present an approach that extracts subtrees from dependency trees in autoparsed data to improve dependency parsing. The 570 Proceedings of the 2009 Conference on Empirical Methods in Natural Langu"
D09-1060,P08-1108,0,0.351178,"e dependency parsing performance. In this paper, we present an approach that extracts subtrees from dependency trees in autoparsed data to improve dependency parsing. The 570 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 570–579, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP glish and Chinese data. We show that this simple approach greatly improves the accuracy and that the use of richer structures (i.e, word triples) indeed gives additional improvement. We also demonstrate that our approach and other improvement techniques (Koo et al., 2008; Nivre and McDonald, 2008) are complementary and that we can achieve very high accuracies when we combine our method with other improvement techniques. Specifically, we achieve the best accuracy for the Chinese data. The rest of this paper is as follows: Section 2 introduces the background of dependency parsing. Section 3 proposes an approach for extracting subtrees and represents the subtree-based features for dependency parsers. Section 4 explains the experimental results and Section 5 discusses related work. Finally, in section 6 we draw conclusions. 2 Dependency parsing Dependency parsing assigns head-dependent rel"
D09-1060,H94-1048,0,0.0447416,"the x axis refers to the number of conjunctions in one sentence and the y axis shows the percentages of the three classes. The figures indicated that the subtree-based features improved the coordinating conjunction problem. In the trigram-subtree list, many subtrees are related to coordinating conjunctions, such as “utilities:1:3 and:2:3 businesses:3:0” and “pull:1:0 and:2:1 protect:3:1”. These subtrees can provide additional information for parsing models. 4.2.3 PP attachment We analyzed our new parsers’ behavior for preposition-phrase attachment, which is also a difficult task for parsing (Ratnaparkhi et al., 1994). We compared the Ord2 system with the Ord2s system. Figures 11 and 12 show how the subtreebased features affect accuracy as a function of the number of prepositions, where the x axis refers to the number of prepositions in one sentence and the 577 y axis shows the percentages of the three classes. The figures indicated that the subtree-based features improved preposition-phrase attachment. 5 Related work Our approach is to incorporate unannotated data into parsing models for dependency parsing. Several previous studies relevant to our approach have been conducted. Chen et al. (2008) previousl"
D09-1060,W96-0213,0,0.611923,"te the effectiveness of the subtree-based features, we conducted experiments on English data and Chinese Data. For English, we used the Penn Treebank (Marcus et al., 1993) in our experiments and the tool “Penn2Malt”7 to convert the data into dependency structures using a standard set of head rules (Yamada and Matsumoto, 2003). To match previous work (McDonald et al., 2005; McDonald and Pereira, 2006; Koo et al., 2008), we split the data into a training set (sections 2-21), a development set (Section 22), and a test set (section 23). Following the work of Koo et al. (2008), we used the MXPOST (Ratnaparkhi, 1996) tagger trained on training data to provide part-of-speech tags for the development and the test set, and we used 10way jackknifing to generate tags for the training set. For the unannotated data, we used the BLLIP corpus (Charniak et al., 2000) that contains about 43 million words of WSJ text.8 We used the MXPOST tagger trained on training data to assign part-of-speech tags and used the Basic Parser to process the sentences of the BLLIP corpus. For Chinese, we used the Chinese Treebank 7 http://w3.msi.vxu.se/˜nivre/research/Penn2Malt.html We ensured that the text used for extracting subtrees"
D09-1060,D07-1111,0,0.0905789,"Missing"
D09-1060,E03-1008,0,0.0906958,"Missing"
D09-1060,W07-2201,0,0.202814,"Missing"
D09-1060,W03-3023,0,0.921881,"f “h-NULL”6 , d1, and d2, which means that we only check the words of sibling nodes without checking the head word. Then, we generate second-order subtree-based features, consisting of indicator functions for set IDs of the retrieved trigram-subtrees. 6 h-NULL is a dummy token 573 In order to evaluate the effectiveness of the subtree-based features, we conducted experiments on English data and Chinese Data. For English, we used the Penn Treebank (Marcus et al., 1993) in our experiments and the tool “Penn2Malt”7 to convert the data into dependency structures using a standard set of head rules (Yamada and Matsumoto, 2003). To match previous work (McDonald et al., 2005; McDonald and Pereira, 2006; Koo et al., 2008), we split the data into a training set (sections 2-21), a development set (Section 22), and a test set (section 23). Following the work of Koo et al. (2008), we used the MXPOST (Ratnaparkhi, 1996) tagger trained on training data to provide part-of-speech tags for the development and the test set, and we used 10way jackknifing to generate tags for the training set. For the unannotated data, we used the BLLIP corpus (Charniak et al., 2000) that contains about 43 million words of WSJ text.8 We used the"
D09-1060,C08-1132,0,0.5151,"r Chinese, we used the Chinese Treebank 7 http://w3.msi.vxu.se/˜nivre/research/Penn2Malt.html We ensured that the text used for extracting subtrees did not include the sentences of the Penn Treebank. 8 (CTB) version 4.09 in the experiments. We also used the “Penn2Malt” tool to convert the data and created a data split: files 1-270 and files 400-931 for training, files 271-300 for testing, and files 301-325 for development. We used gold standard segmentation and part-of-speech tags in the CTB. The data partition and part-of-speech settings were chosen to match previous work (Chen et al., 2008; Yu et al., 2008). For the unannotated data, we used the PFR corpus10 , which has approximately 15 million words whose segmentation and POS tags are given. We used its original segmentation though there are differences in segmentation policy between CTB and this corpus. As for POS tags, we discarded the original POS tags and assigned CTB style POS tags using a TNT-based tagger (Brants, 2000) trained on the training data. We used the Basic Parser to process all the sentences of the PFR corpus. We measured the parser quality by the unlabeled attachment score (UAS), i.e., the percentage of tokens (excluding all p"
D09-1060,D07-1096,0,\N,Missing
D09-1097,bond-etal-2008-boot,1,0.581211,"Missing"
D09-1097,P99-1016,0,0.0133585,"ctic pattern-based methods by using dependency path features for machine learning. Then, they extended the framework such that this method was capable of making use of heterogenous evidence (Snow et al. 2006). These pattern-based methods require the co-occurrences of a target word and the hypernym in a document. It should be noted that the requirement of such cooccurrences actually poses a problem when we extract a large set of hyponymy relations since they are not frequently observed (Shinzato et al. 2004, Pantel et al. 2004b). Clustering-based methods have been proposed as another approach. Caraballo (1999), Pantel et al. (2004b), and Shinzato et al. (2004) proposed a method to find a common hypernym for word classes, which are automatically constructed using some measures of word similarities or hierarchical structures in HTML documents. Etzioni et Documents hypernym ..… word The same hypernym is selected for all words in a class. Pattern-based method (Hearst 1992, Pantel et al. 2004a, Ando et al. 2003, Snow et al. 2005, Snow et al. 2006, and Etzioni et al. 2005) word word word word word Word Class Clustering-based method (Caraballo 1999, Pantel et al. 2004b, Shinzato et al. 2004, and Etzioni e"
D09-1097,C92-2082,0,0.762187,"pproach. duce some related works in Section 2. Section 3 describes the Wikipedia relation database. Section 4 describes the distributional similarity calculated by the two methods. In Section 5, we describe a method to discover an appropriate hypernym for each target word. The experimental results are presented in Section 6 before concluding the paper in Section 7. 2 Corpus/documents hypernym such as word Co-occurrences in a pattern are needed Related Works Most previous researchers have relied on lexico-syntactic patterns for hyponymy acquisition. Lexico-syntactic patterns were first used by Hearst (1992). The patterns used by her included “NP0 such as NP1,” in which NP0 is a hypernym of NP1. Using these patterns as seeds, Hearst discovered new patterns by which to semiautomatically extract hyponymy relations. Pantel et al. (2004a) proposed a method to automatically discover the patterns using a minimal edit distance. Ando et al. (2003) applied predefined lexico-syntactic patterns to Japanese news articles. Snow et al. (2005) generalized these lexicosyntactic pattern-based methods by using dependency path features for machine learning. Then, they extended the framework such that this method wa"
D09-1097,P08-1047,1,0.864076,"Missing"
D09-1097,P99-1004,0,0.0542805,"etween v and n. In Japanese, a relation rel is represented by postpositions attached to n and the phrase composed of n and rel modifies v. Each triple is divided into two parts. The first is &lt;v, rel&gt; and the second is n. Then, we consider the conditional probability of occurrence of the pair &lt;v, rel&gt;: P(&lt;v, rel&gt;|n). P(&lt;v, rel&gt;|n) can be regarded as the distribution of the grammatical contexts of the noun phrase n. The distributional similarity can be defined as the distance between these distributions. There are several kinds of functions for evaluating the distance between two distributions (Lee 1999). Our method uses the Jensen-Shannon divergence. The Jensen-Shannon divergence between two probability distributions, P (⋅ |n1 ) and P (⋅ |n2 ) , can be calculated as follows: DJS ( P (⋅ |n1 ) ||P (⋅ |n2 )) P (⋅ |n1 ) + P (⋅ |n2 ) 1 ) ( DKL ( P (⋅ |n1 ) || 2 2 P (⋅ |n1 ) + P (⋅ |n2 ) + DKL ( P (⋅ |n2 ) || )), 2 = &lt;v ,rel &gt;∈D if f (&lt; v, rel , n &gt;) &gt; 0, where f(&lt;v, rel, n&gt;) is the frequency of a triple &lt;v, rel, n&gt; and D is the set defined as { &lt;v, rel &gt; | f(&lt;v, rel, n&gt;) &gt; 0 }. In the case of f(&lt;v, rel, n&gt;) = 0, P(&lt;v, rel&gt;|n) is set to 0. Instead of using the observed frequency directly as in the"
D09-1097,P06-1101,0,0.0472608,"P1,” in which NP0 is a hypernym of NP1. Using these patterns as seeds, Hearst discovered new patterns by which to semiautomatically extract hyponymy relations. Pantel et al. (2004a) proposed a method to automatically discover the patterns using a minimal edit distance. Ando et al. (2003) applied predefined lexico-syntactic patterns to Japanese news articles. Snow et al. (2005) generalized these lexicosyntactic pattern-based methods by using dependency path features for machine learning. Then, they extended the framework such that this method was capable of making use of heterogenous evidence (Snow et al. 2006). These pattern-based methods require the co-occurrences of a target word and the hypernym in a document. It should be noted that the requirement of such cooccurrences actually poses a problem when we extract a large set of hyponymy relations since they are not frequently observed (Shinzato et al. 2004, Pantel et al. 2004b). Clustering-based methods have been proposed as another approach. Caraballo (1999), Pantel et al. (2004b), and Shinzato et al. (2004) proposed a method to find a common hypernym for word classes, which are automatically constructed using some measures of word similarities o"
D09-1097,sumida-etal-2008-boosting,1,0.937771,"aper, we use the term “word” for both “a single-word word” and “a multi-word word.” 929 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 929–937, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP A hypernym is selected for each word independently. Wikipedia relation database Wikipedia Hyponymy relations are extracted using the layout information of Wikipedia. hypernym : Selected from hypernyms in the Wikipedia relation database. No direct co-occurrences of hypernym and hyponym in corpora are needed. Wikipedia-based approach (Ponzetto et al. 2007 and Sumida et al. 2008) Target word: Selected from the Web : word k similar words Figure 1: Overview of the proposed approach. duce some related works in Section 2. Section 3 describes the Wikipedia relation database. Section 4 describes the distributional similarity calculated by the two methods. In Section 5, we describe a method to discover an appropriate hypernym for each target word. The experimental results are presented in Section 6 before concluding the paper in Section 7. 2 Corpus/documents hypernym such as word Co-occurrences in a pattern are needed Related Works Most previous researchers have relied on le"
D09-1097,N04-1041,0,0.0180963,"Missing"
D09-1097,P99-1014,0,0.0117575,"rel&gt;|n) is set to 0. Instead of using the observed frequency directly as in the usual maximum likelihood estimation, we modified it as above. Although this might seems strange, this kind of modification is common in information retrieval as a term weighing method (Manning et al. 1999) and it is also applied in some studies to yield better word similarities (Terada et al. 2006, Kazama et al. 2009). We also adopted this idea in this study. 4.2 P(⋅ |n1 ) . P(⋅ |n2 ) Finally, the distributional similarity between two words, n1 and n2, is defined as follows: Distributional Similarity Based on CVD Rooth et al. (1999) and Torisawa (2001) showed that EM-based clustering using verb-noun dependencies can produce semantically clean noun clusters. We exploit these EM-based clustering results as the smoothed contexts for noun n. In Torisawa’s model (2001), the probability of occurrence of the triple &lt;v, rel, n&gt; is defined as follows: P(&lt; v, rel, n &gt;) where DKL indicates the Kullback-Leibler divergence and is defined as follows: DKL ( P(⋅ |n1 ) ||P(⋅ |n2 ))= ∑ P(⋅ |n1 ) log log( f (&lt; v, rel , n &gt;)) + 1 ∑ log( f (&lt; v, rel , n &gt;) + 1 =def ∑a∈A P(&lt; v, rel &gt; |a) P(n |a) P(a), where a denotes a hidden class of &lt;v,rel&gt;"
D09-1097,N04-1010,1,0.869582,"Missing"
D09-1097,shinzato-etal-2008-large,0,\N,Missing
D09-1122,D07-1017,0,\N,Missing
D09-1122,W03-1608,0,\N,Missing
D09-1122,C08-1107,0,\N,Missing
D09-1122,N06-1007,0,\N,Missing
D09-1122,N06-1008,1,\N,Missing
D09-1122,N06-1023,0,\N,Missing
D09-1122,W03-1011,0,\N,Missing
D09-1122,C08-1029,0,\N,Missing
D09-1122,N03-1003,0,\N,Missing
D09-1122,P05-1014,0,\N,Missing
D09-1122,P05-1074,0,\N,Missing
D09-1122,P98-2127,0,\N,Missing
D09-1122,C98-2122,0,\N,Missing
D09-1122,P06-1107,0,\N,Missing
D09-1122,kawahara-kurohashi-2006-case,0,\N,Missing
D09-1122,W04-3206,0,\N,Missing
D11-1007,D08-1092,0,0.318764,"design a set of effective bilingual features for parsing models based on the verified results. The experimental results show that our new parsers significantly outperform state-of-theart baselines. Moreover, our approach is still able to provide improvement when we use a larger monolingual treebank that results in a much stronger baseline. Especially notable is that our approach can be used in a purely monolingual setting with the help of SMT. 1 Introduction Recently there have been several studies aiming to improve the performance of parsing bilingual texts (bitexts) (Smith and Smith, 2004; Burkett and Klein, 2008; Huang et al., 2009; Zhao et al., 2009; Chen et al., 2010). In bitext parsing, we can use the information based on “bilingual constraints” (Burkett and Klein, 2008), which do not exist in monolingual sentences. More accurate bitext parsing results can be effectively used in the training of syntax-based machine translation systems (Liu and Huang, 2010). Most previous studies rely on bilingual treebanks to provide bilingual constraints for bitext parsing. Burkett and Klein (2008) proposed joint models on bitexts to improve the performance on either or both sides. Their method uses bilingual tre"
D11-1007,D07-1101,0,0.144195,"sponding to “技巧(jiqiao)/skill” is a grandchild of the word “play” corresponding to “发挥(fahui)/demonstrate”. This is a positive evidence for supporting “发 挥(fahui)/demonstrate” as being the head of “技 巧(jiqiao)/skill”. From this example, although the sentences and parse trees on the target side are not perfect, we still can explore useful information to improve bitext parsing. In this paper, we focus on how to design a method to verify such unreliable bilingual constraints. 3 Parsing model In this paper, we implement our approach based on graph-based parsing models (McDonald and Pereira, 2006; Carreras, 2007). Note that our approach can also be applied to transition-based parsing models (Nivre, 2003; Yamada and Matsumoto, 2003). The graph-based parsing model is to search for the maximum spanning tree (MST) in a graph (McDonald and Pereira, 2006). The formulation defines the score of a dependency tree to be the sum of edge scores, s(x, y) = X g∈y score(w, x, g) = X g∈y 4.2 Bilingual constraint functions w ·f (x, g) (1) where x is an input sentence, y is a dependency tree for x, and g is a spanning subgraph of y. f (x, g) can be based on arbitrary features of the subgraph and the input sequence x an"
D11-1007,D09-1060,1,0.927841,"list of the target monolingual subtrees 1 For the second order features, Dir is the combination of the directions of two dependencies. or bilingual subtrees, this constraint will probably be reliable. We first parse the large-scale unannotated monolingual and bilingual data. Subsequently, we extract the monolingual and bilingual subtrees from the parsed data. We then verify the bilingual constraints using the extracted subtrees. Finally, we generate the bilingual features based on the verified results for the parsing models. 5.1 Verified constraint functions 5.1.1 Monolingual target subtrees Chen et al. (2009) proposed a simple method to extract subtrees from large-scale monolingual data and used them as features to improve monolingual parsing. Following their method, we parse large unannotated data with the Parsert and obtain the subtree list (STt ) on the target side. We extract two types of subtrees: bigram (two words) subtree and trigram (three words) subtree. 5.1.2 Verified target constraint function: Fvt (rtk ) We use the extracted target subtrees to verify the rtk of the bilingual constraints. In fact, rtk is a candidate subtree. If the rtk is included in STt , function Fvt (rtk ) = T ype(rt"
D11-1007,P10-1003,1,0.243719,"based on the verified results. The experimental results show that our new parsers significantly outperform state-of-theart baselines. Moreover, our approach is still able to provide improvement when we use a larger monolingual treebank that results in a much stronger baseline. Especially notable is that our approach can be used in a purely monolingual setting with the help of SMT. 1 Introduction Recently there have been several studies aiming to improve the performance of parsing bilingual texts (bitexts) (Smith and Smith, 2004; Burkett and Klein, 2008; Huang et al., 2009; Zhao et al., 2009; Chen et al., 2010). In bitext parsing, we can use the information based on “bilingual constraints” (Burkett and Klein, 2008), which do not exist in monolingual sentences. More accurate bitext parsing results can be effectively used in the training of syntax-based machine translation systems (Liu and Huang, 2010). Most previous studies rely on bilingual treebanks to provide bilingual constraints for bitext parsing. Burkett and Klein (2008) proposed joint models on bitexts to improve the performance on either or both sides. Their method uses bilingual treebanks that have human-annotated tree structures on both si"
D11-1007,P07-1003,0,0.126522,"ale unannotated data. 4.1 Auto-generated bilingual treebank Assuming that we have monolingual treebanks on the source side, an SMT system that can translate the source sentences into the target language, and a Parsert trained on the target monolingual treebank. We first translate the sentences of the source monolingual treebank into the target language using the SMT system. Usually, SMT systems can output the word alignment links directly. If they can not, we perform word alignment using some publicly available tools, such as Giza++ (Och and Ney, 2003) or Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007). The translated sentences are parsed by the Parsert . Then, we have a newly auto-generated bilingual treebank. 76 In this paper, we focus on the first- and secondorder graph models (McDonald and Pereira, 2006; Carreras, 2007). Thus we produce the constraints for bigram (a single edge) and trigram (adjacent edges) dependencies in the graph model. For the trigram dependencies, we consider the parent-sibling and parent-child-grandchild structures described in McDonald and Pereira (2006) and Carreras (2007). We leave the third-order models (Koo and Collins, 2010) for a future study. Suppose that"
D11-1007,D09-1127,0,0.0657088,"Missing"
D11-1007,N03-1017,0,0.0160094,"Missing"
D11-1007,P10-1001,0,0.0121108,"ley Aligner (Liang et al., 2006; DeNero and Klein, 2007). The translated sentences are parsed by the Parsert . Then, we have a newly auto-generated bilingual treebank. 76 In this paper, we focus on the first- and secondorder graph models (McDonald and Pereira, 2006; Carreras, 2007). Thus we produce the constraints for bigram (a single edge) and trigram (adjacent edges) dependencies in the graph model. For the trigram dependencies, we consider the parent-sibling and parent-child-grandchild structures described in McDonald and Pereira (2006) and Carreras (2007). We leave the third-order models (Koo and Collins, 2010) for a future study. Suppose that we have a (candidate) dependency relation rs that can be a bigram or trigram dependency. We examine whether the corresponding words of the source words of rs have a dependency relation rt in the target trees. We also consider the direction of the dependency relation. The corresponding word of the head should also be the head in rt . We define a binary function for this bilingual constraint: Fbn (rsn : rtk ), where n and k refers to the types of the dependencies (2 for bigram and 3 for trigram). For example, in rs2 : rt3 , rs2 is a bigram dependency on the sour"
D11-1007,P09-1058,1,0.820133,"//www.itl.nist.gov/iad/mig//tests/mt/2008/ 4 data. To extract English subtrees, we used the BLLIP corpus (Charniak et al., 2000) that contains about 43 million words of WSJ texts. We used the MXPOST tagger (Ratnaparkhi, 1996) trained on training data to assign POS tags and used the first-order Parsert to process the sentences of the BLLIP corpus. To extract bilingual subtrees, we used the FBIS corpus and an additional bilingual corpus containing 800,000 sentence pairs from the training data of NIST MT08 evaluation campaign. On the Chinese side, we used the morphological analyzer described in (Kruengkrai et al., 2009) trained on the training data of CTBtp to perform word segmentation and POS tagging and used the first-order Parsers to parse all the sentences in the data. On the English side, we used the same procedure as we did for the BLLIP corpus. Word alignment was performed using the Berkeley Aligner. We reported the parser quality by the UAS, i.e., the percentage of tokens (excluding all punctuation tokens) with correct HEADs. 6.1 Experimental settings For baseline systems, we used the monolingual features mentioned in Section 3. We called these features basic features. To compare the results of (Burk"
D11-1007,N06-1014,0,0.079112,"ed by using large-scale unannotated data. 4.1 Auto-generated bilingual treebank Assuming that we have monolingual treebanks on the source side, an SMT system that can translate the source sentences into the target language, and a Parsert trained on the target monolingual treebank. We first translate the sentences of the source monolingual treebank into the target language using the SMT system. Usually, SMT systems can output the word alignment links directly. If they can not, we perform word alignment using some publicly available tools, such as Giza++ (Och and Ney, 2003) or Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007). The translated sentences are parsed by the Parsert . Then, we have a newly auto-generated bilingual treebank. 76 In this paper, we focus on the first- and secondorder graph models (McDonald and Pereira, 2006; Carreras, 2007). Thus we produce the constraints for bigram (a single edge) and trigram (adjacent edges) dependencies in the graph model. For the trigram dependencies, we consider the parent-sibling and parent-child-grandchild structures described in McDonald and Pereira (2006) and Carreras (2007). We leave the third-order models (Koo and Collins, 2010) for a fu"
D11-1007,P10-5002,0,0.0237031,"able is that our approach can be used in a purely monolingual setting with the help of SMT. 1 Introduction Recently there have been several studies aiming to improve the performance of parsing bilingual texts (bitexts) (Smith and Smith, 2004; Burkett and Klein, 2008; Huang et al., 2009; Zhao et al., 2009; Chen et al., 2010). In bitext parsing, we can use the information based on “bilingual constraints” (Burkett and Klein, 2008), which do not exist in monolingual sentences. More accurate bitext parsing results can be effectively used in the training of syntax-based machine translation systems (Liu and Huang, 2010). Most previous studies rely on bilingual treebanks to provide bilingual constraints for bitext parsing. Burkett and Klein (2008) proposed joint models on bitexts to improve the performance on either or both sides. Their method uses bilingual treebanks that have human-annotated tree structures on both sides. Huang et al. (2009) presented a method to train a source-language parser by using the reordering information on words between the sentences on two sides. It uses another type of bilingual treebanks that have tree structures on the source sentences and their human-translated sentences. Chen"
D11-1007,J93-2004,0,0.0430468,"ingual treebanks that have tree structures on the source sentences and their human-translated sentences. Chen et al. (2010) also used bilingual treebanks and made use of tree structures on the target side. However, the bilingual treebanks are hard to obtain, partly because of the high cost of human translation. Thus, in their experiments, they applied their methods to a small data set, the manually translated portion of the Chinese Treebank (CTB) which contains only about 3,000 sentences. On the other hand, many large-scale monolingual treebanks exist, such as the Penn English Treebank (PTB) (Marcus et al., 1993) (about 40,000 sentences in Version 3) and the latest version of CTB (over 50,000 sentences in Version 7). In this paper, we propose a bitext parsing approach in which we produce the bilingual constraints on existing monolingual treebanks with the help of SMT systems. In other words, we aim to improve source-language parsing with the help of automatic translations. In our approach, we first use an SMT system to translate the sentences of a source monolingual treebank into the target language. Then, the target sentences are parsed by a parser trained on a target monolingual treebank. We then ob"
D11-1007,E06-1011,0,0.17547,"ure, the word “skills” corresponding to “技巧(jiqiao)/skill” is a grandchild of the word “play” corresponding to “发挥(fahui)/demonstrate”. This is a positive evidence for supporting “发 挥(fahui)/demonstrate” as being the head of “技 巧(jiqiao)/skill”. From this example, although the sentences and parse trees on the target side are not perfect, we still can explore useful information to improve bitext parsing. In this paper, we focus on how to design a method to verify such unreliable bilingual constraints. 3 Parsing model In this paper, we implement our approach based on graph-based parsing models (McDonald and Pereira, 2006; Carreras, 2007). Note that our approach can also be applied to transition-based parsing models (Nivre, 2003; Yamada and Matsumoto, 2003). The graph-based parsing model is to search for the maximum spanning tree (MST) in a graph (McDonald and Pereira, 2006). The formulation defines the score of a dependency tree to be the sum of edge scores, s(x, y) = X g∈y score(w, x, g) = X g∈y 4.2 Bilingual constraint functions w ·f (x, g) (1) where x is an input sentence, y is a dependency tree for x, and g is a spanning subgraph of y. f (x, g) can be based on arbitrary features of the subgraph and the in"
D11-1007,W03-3017,0,0.0373708,"monstrate”. This is a positive evidence for supporting “发 挥(fahui)/demonstrate” as being the head of “技 巧(jiqiao)/skill”. From this example, although the sentences and parse trees on the target side are not perfect, we still can explore useful information to improve bitext parsing. In this paper, we focus on how to design a method to verify such unreliable bilingual constraints. 3 Parsing model In this paper, we implement our approach based on graph-based parsing models (McDonald and Pereira, 2006; Carreras, 2007). Note that our approach can also be applied to transition-based parsing models (Nivre, 2003; Yamada and Matsumoto, 2003). The graph-based parsing model is to search for the maximum spanning tree (MST) in a graph (McDonald and Pereira, 2006). The formulation defines the score of a dependency tree to be the sum of edge scores, s(x, y) = X g∈y score(w, x, g) = X g∈y 4.2 Bilingual constraint functions w ·f (x, g) (1) where x is an input sentence, y is a dependency tree for x, and g is a spanning subgraph of y. f (x, g) can be based on arbitrary features of the subgraph and the input sequence x and the feature weight vector w are the parameters to be learned by using MIRA (Crammer and Si"
D11-1007,J03-1002,0,0.00306085,"ased on the bilingual constraints verified by using large-scale unannotated data. 4.1 Auto-generated bilingual treebank Assuming that we have monolingual treebanks on the source side, an SMT system that can translate the source sentences into the target language, and a Parsert trained on the target monolingual treebank. We first translate the sentences of the source monolingual treebank into the target language using the SMT system. Usually, SMT systems can output the word alignment links directly. If they can not, we perform word alignment using some publicly available tools, such as Giza++ (Och and Ney, 2003) or Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007). The translated sentences are parsed by the Parsert . Then, we have a newly auto-generated bilingual treebank. 76 In this paper, we focus on the first- and secondorder graph models (McDonald and Pereira, 2006; Carreras, 2007). Thus we produce the constraints for bigram (a single edge) and trigram (adjacent edges) dependencies in the graph model. For the trigram dependencies, we consider the parent-sibling and parent-child-grandchild structures described in McDonald and Pereira (2006) and Carreras (2007). We leave the third-order"
D11-1007,W96-0213,0,0.0608525,"e trained first-order and second-order Parsert on the training data. The unlabeled attachment score (UAS) of the second-order Parsert was 91.92, indicating state-of-the-art accuracy on the test data. We used the second-order Parsert to parse the autotranslated/human-made target sentences in the CTB 3 http://www.statmt.org/moses/ http://www.speech.sri.com/projects/srilm/download.html 5 http://www.itl.nist.gov/iad/mig//tests/mt/2008/ 4 data. To extract English subtrees, we used the BLLIP corpus (Charniak et al., 2000) that contains about 43 million words of WSJ texts. We used the MXPOST tagger (Ratnaparkhi, 1996) trained on training data to assign POS tags and used the first-order Parsert to process the sentences of the BLLIP corpus. To extract bilingual subtrees, we used the FBIS corpus and an additional bilingual corpus containing 800,000 sentence pairs from the training data of NIST MT08 evaluation campaign. On the Chinese side, we used the morphological analyzer described in (Kruengkrai et al., 2009) trained on the training data of CTBtp to perform word segmentation and POS tagging and used the first-order Parsers to parse all the sentences in the data. On the English side, we used the same proced"
D11-1007,W04-3207,0,0.0134497,"ify the constraints and design a set of effective bilingual features for parsing models based on the verified results. The experimental results show that our new parsers significantly outperform state-of-theart baselines. Moreover, our approach is still able to provide improvement when we use a larger monolingual treebank that results in a much stronger baseline. Especially notable is that our approach can be used in a purely monolingual setting with the help of SMT. 1 Introduction Recently there have been several studies aiming to improve the performance of parsing bilingual texts (bitexts) (Smith and Smith, 2004; Burkett and Klein, 2008; Huang et al., 2009; Zhao et al., 2009; Chen et al., 2010). In bitext parsing, we can use the information based on “bilingual constraints” (Burkett and Klein, 2008), which do not exist in monolingual sentences. More accurate bitext parsing results can be effectively used in the training of syntax-based machine translation systems (Liu and Huang, 2010). Most previous studies rely on bilingual treebanks to provide bilingual constraints for bitext parsing. Burkett and Klein (2008) proposed joint models on bitexts to improve the performance on either or both sides. Their"
D11-1007,W03-3023,0,0.0472776,"his is a positive evidence for supporting “发 挥(fahui)/demonstrate” as being the head of “技 巧(jiqiao)/skill”. From this example, although the sentences and parse trees on the target side are not perfect, we still can explore useful information to improve bitext parsing. In this paper, we focus on how to design a method to verify such unreliable bilingual constraints. 3 Parsing model In this paper, we implement our approach based on graph-based parsing models (McDonald and Pereira, 2006; Carreras, 2007). Note that our approach can also be applied to transition-based parsing models (Nivre, 2003; Yamada and Matsumoto, 2003). The graph-based parsing model is to search for the maximum spanning tree (MST) in a graph (McDonald and Pereira, 2006). The formulation defines the score of a dependency tree to be the sum of edge scores, s(x, y) = X g∈y score(w, x, g) = X g∈y 4.2 Bilingual constraint functions w ·f (x, g) (1) where x is an input sentence, y is a dependency tree for x, and g is a spanning subgraph of y. f (x, g) can be based on arbitrary features of the subgraph and the input sequence x and the feature weight vector w are the parameters to be learned by using MIRA (Crammer and Singer, 2003) during training."
D11-1007,P09-1007,0,0.0415887,"for parsing models based on the verified results. The experimental results show that our new parsers significantly outperform state-of-theart baselines. Moreover, our approach is still able to provide improvement when we use a larger monolingual treebank that results in a much stronger baseline. Especially notable is that our approach can be used in a purely monolingual setting with the help of SMT. 1 Introduction Recently there have been several studies aiming to improve the performance of parsing bilingual texts (bitexts) (Smith and Smith, 2004; Burkett and Klein, 2008; Huang et al., 2009; Zhao et al., 2009; Chen et al., 2010). In bitext parsing, we can use the information based on “bilingual constraints” (Burkett and Klein, 2008), which do not exist in monolingual sentences. More accurate bitext parsing results can be effectively used in the training of syntax-based machine translation systems (Liu and Huang, 2010). Most previous studies rely on bilingual treebanks to provide bilingual constraints for bitext parsing. Burkett and Klein (2008) proposed joint models on bitexts to improve the performance on either or both sides. Their method uses bilingual treebanks that have human-annotated tree s"
D11-1076,P08-1004,0,0.251978,"5 Related Work Using lexico-syntactic patterns to extract semantic relations was first explored by Hearst (Hearst, 1992), and has inspired a large body of work on semisupervised relation acquisition methods (Berland and Charniak, 1999; Agichtein and Gravano, 2000; Etzioni et al., 2004; Pantel and Pennacchiotti, 2006b; Pas¸ca et al., 2006; De Saeger et al., 2009), two of which were used in this work. Some researchers have addressed the sparseness problems inherent in pattern based methods. Downey et al. (2007) starts from the output of the unsupervised information extraction system TextRunner (Banko and Etzioni, 2008), and uses language modeling techniques to estimate the reliability of sparse extractions. Pas¸ca et al. (2006) alleviates pattern sparseness by using infix patterns that are generalized using classes of distributionally similar words. In addition, their method employs clustering based semantic similarities to filter newly extracted instances in each iteration of the bootstrapping process. A comparison with our method would have been instructive, but we were unable to implement their method because the original paper contains insufficient detail to allow replication. There is a large body of r"
D11-1076,P99-1008,0,0.0342991,"t in combination they do give a marked improvement over the base features, at least for some relations like causation and material. In other words, the main contribution of semantic word classes and partial patterns to our method’s performance lies not in the final classification step but seems to occur at earlier stages of the process, in the candidate and training data generation steps. 5 Related Work Using lexico-syntactic patterns to extract semantic relations was first explored by Hearst (Hearst, 1992), and has inspired a large body of work on semisupervised relation acquisition methods (Berland and Charniak, 1999; Agichtein and Gravano, 2000; Etzioni et al., 2004; Pantel and Pennacchiotti, 2006b; Pas¸ca et al., 2006; De Saeger et al., 2009), two of which were used in this work. Some researchers have addressed the sparseness problems inherent in pattern based methods. Downey et al. (2007) starts from the output of the unsupervised information extraction system TextRunner (Banko and Etzioni, 2008), and uses language modeling techniques to estimate the reliability of sparse extractions. Pas¸ca et al. (2006) alleviates pattern sparseness by using infix patterns that are generalized using classes of distri"
D11-1076,H05-1091,0,0.0559931,"ng infix patterns that are generalized using classes of distributionally similar words. In addition, their method employs clustering based semantic similarities to filter newly extracted instances in each iteration of the bootstrapping process. A comparison with our method would have been instructive, but we were unable to implement their method because the original paper contains insufficient detail to allow replication. There is a large body of research in the supervised tradition that does not use explicit pattern representations — kernel based methods (Zelenko et al., 2003; Culotta, 2004; Bunescu and Mooney, 2005) and CRF based methods (Culotta et al., 2006). These approaches are all fully supervised, whereas in our work the automatic generation of candidates and training data is an integral part of the method. An interesting alternative is distant supervision (Mintz et al., 2009), which trains a classifier using an existing database (Freebase) containing thousands of semantic relations, with millions of instances. We believe our method is more general, as depending on external resources like a database of semantic relations limits both the range of semantic relations (i.e., Freebase contains only rela"
D11-1076,N06-1038,0,0.0282956,"ses of distributionally similar words. In addition, their method employs clustering based semantic similarities to filter newly extracted instances in each iteration of the bootstrapping process. A comparison with our method would have been instructive, but we were unable to implement their method because the original paper contains insufficient detail to allow replication. There is a large body of research in the supervised tradition that does not use explicit pattern representations — kernel based methods (Zelenko et al., 2003; Culotta, 2004; Bunescu and Mooney, 2005) and CRF based methods (Culotta et al., 2006). These approaches are all fully supervised, whereas in our work the automatic generation of candidates and training data is an integral part of the method. An interesting alternative is distant supervision (Mintz et al., 2009), which trains a classifier using an existing database (Freebase) containing thousands of semantic relations, with millions of instances. We believe our method is more general, as depending on external resources like a database of semantic relations limits both the range of semantic relations (i.e., Freebase contains only relations between named entities, and none of the"
D11-1076,P04-1054,0,0.00902849,"arseness by using infix patterns that are generalized using classes of distributionally similar words. In addition, their method employs clustering based semantic similarities to filter newly extracted instances in each iteration of the bootstrapping process. A comparison with our method would have been instructive, but we were unable to implement their method because the original paper contains insufficient detail to allow replication. There is a large body of research in the supervised tradition that does not use explicit pattern representations — kernel based methods (Zelenko et al., 2003; Culotta, 2004; Bunescu and Mooney, 2005) and CRF based methods (Culotta et al., 2006). These approaches are all fully supervised, whereas in our work the automatic generation of candidates and training data is an integral part of the method. An interesting alternative is distant supervision (Mintz et al., 2009), which trains a classifier using an existing database (Freebase) containing thousands of semantic relations, with millions of instances. We believe our method is more general, as depending on external resources like a database of semantic relations limits both the range of semantic relations (i.e.,"
D11-1076,P07-1088,0,0.0220274,"but seems to occur at earlier stages of the process, in the candidate and training data generation steps. 5 Related Work Using lexico-syntactic patterns to extract semantic relations was first explored by Hearst (Hearst, 1992), and has inspired a large body of work on semisupervised relation acquisition methods (Berland and Charniak, 1999; Agichtein and Gravano, 2000; Etzioni et al., 2004; Pantel and Pennacchiotti, 2006b; Pas¸ca et al., 2006; De Saeger et al., 2009), two of which were used in this work. Some researchers have addressed the sparseness problems inherent in pattern based methods. Downey et al. (2007) starts from the output of the unsupervised information extraction system TextRunner (Banko and Etzioni, 2008), and uses language modeling techniques to estimate the reliability of sparse extractions. Pas¸ca et al. (2006) alleviates pattern sparseness by using infix patterns that are generalized using classes of distributionally similar words. In addition, their method employs clustering based semantic similarities to filter newly extracted instances in each iteration of the bootstrapping process. A comparison with our method would have been instructive, but we were unable to implement their m"
D11-1076,P05-1053,0,0.0215326,"he noun, which is captured by the unigram features. In addition to these base features, we include the semantic classes to which the candidate noun pair belongs, the partial patterns they match in this sentence, and the infix words inbetween the target noun pair. Note that this feature set is not intended to be optimal beyond the actual claims of this paper, and we have deliberately avoided exhaustive feature engineering so as not to obscure the contribution of semantic classes and partial pattern to our approach. Clearly an optimal classifier will incorporate many more advanced features (see GuoDong et al. (2005) for a comprehensive overview), but even without sophisticated feature engineering our classifier achieved sufficient performance levels to support our claims. An overview of the feature set is given in Table 1. The relative contribution of each type of features is discussed in section 4. In preliminary experiments we found a polynomial kernel of degree 3 gave the best results, which suggests the effectiveness of combining different types of indirect evidence. The SVM classifier outputs (noun pair, sentence) triples, ranked by SVM score. To obtain the final output of our method we assign each"
D11-1076,C92-2082,0,0.300524,"that our method performs on par with state-of-the-art pattern based methods, and maintains a reasonable level of accuracy even for instances acquired from infrequent patterns. This ability to acquire long tail instances is crucial for risk management and innovation, where an exhaustive database of high-level semantic relations like causation is of vital importance. “Curing hypertension alleviates the deterioration speed of the renal function, thereby lowering the risk of causing intracranial bleeding” 1 Introduction Pattern based relation acquisition methods rely on lexico-syntactic patterns (Hearst, 1992) for extracting relation instances. These are templates of natural language expressions such as “X causes Y ” that signal an instance of some semantic relation (i.e., causality). Pattern based methods (Agichtein and Gravano, 2000; Pantel and Pennacchiotti, 2006b; Pas¸ca et al., 2006; De Saeger et al., 2009) learn many ∗ This work was done when all authors were at the National Institute of Information and Communications Technology. Humans can infer that this sentence expresses a causal relation between the underlined noun phrases. But the actual pattern connecting them, i.e., “Curing X alleviat"
D11-1076,P08-1047,1,0.772197,", together with the highest scoring class dependent pattern each noun pair co-occurs with. This list becomes the input to Stage 2 of our method, as shown in Figure 1. We adopted CDP as Stage 1 extractor because, besides having generally good performance, the class dependent patterns provide the two fundamental ingredients for Stage 2 of our method — the target semantic word classes for a given relation (in the form of the semantic class restrictions attached to patterns), and partial patterns. To obtain fine-grained semantic word classes we used the large scale word clustering algorithm from (Kazama and Torisawa, 2008), which uses the EM algorithm to compute the probability that a word w belongs to class c, i.e., P (c|w). Probabilistic clustering defines no discrete boundary between members and non-members of a semantic class, so we simply assume w belongs to c whenever P (c|w) ≥ 0.2. For this work we clustered 106 nouns into 500 classes. Finally, we adopt the structural representation of patterns introduced in (Lin and Pantel, 2001). All sentences in our corpus are dependency parsed, and patterns consist of words on the path of dependency relations connecting two nouns. 827 3 Stage 2 Extractor We use CDP a"
D11-1076,D08-1106,0,0.030624,"Missing"
D11-1076,P09-1113,0,0.0421639,"d have been instructive, but we were unable to implement their method because the original paper contains insufficient detail to allow replication. There is a large body of research in the supervised tradition that does not use explicit pattern representations — kernel based methods (Zelenko et al., 2003; Culotta, 2004; Bunescu and Mooney, 2005) and CRF based methods (Culotta et al., 2006). These approaches are all fully supervised, whereas in our work the automatic generation of candidates and training data is an integral part of the method. An interesting alternative is distant supervision (Mintz et al., 2009), which trains a classifier using an existing database (Freebase) containing thousands of semantic relations, with millions of instances. We believe our method is more general, as depending on external resources like a database of semantic relations limits both the range of semantic relations (i.e., Freebase contains only relations between named entities, and none of the relations in this work) and languages (i.e., no resource comparable to Freebase exists for Japanese) to which the technology can be applied. Furthermore, it is unclear whether distant supervision can deal with noisy input such"
D11-1076,P06-1102,0,0.161549,"Missing"
D11-1076,P06-1015,0,0.697909,"gement and innovation, where an exhaustive database of high-level semantic relations like causation is of vital importance. “Curing hypertension alleviates the deterioration speed of the renal function, thereby lowering the risk of causing intracranial bleeding” 1 Introduction Pattern based relation acquisition methods rely on lexico-syntactic patterns (Hearst, 1992) for extracting relation instances. These are templates of natural language expressions such as “X causes Y ” that signal an instance of some semantic relation (i.e., causality). Pattern based methods (Agichtein and Gravano, 2000; Pantel and Pennacchiotti, 2006b; Pas¸ca et al., 2006; De Saeger et al., 2009) learn many ∗ This work was done when all authors were at the National Institute of Information and Communications Technology. Humans can infer that this sentence expresses a causal relation between the underlined noun phrases. But the actual pattern connecting them, i.e., “Curing X alleviates the deterioration speed of the renal function, thereby lowering the risk of causing Y ”, is rarely observed more than once even in a 108 page Web corpus. In the sense that the term pattern implies a recurring event, this expression contains no pattern for de"
D11-1076,D10-1106,0,0.0805018,"tions, with millions of instances. We believe our method is more general, as depending on external resources like a database of semantic relations limits both the range of semantic relations (i.e., Freebase contains only relations between named entities, and none of the relations in this work) and languages (i.e., no resource comparable to Freebase exists for Japanese) to which the technology can be applied. Furthermore, it is unclear whether distant supervision can deal with noisy input such as automatically acquired relation instances. Finally, inference based methods (Carlson et al., 2010; Schoenmackers et al., 2010; Tsuchida et al., 2010) are another attempt at relation acquisition that goes beyond pattern matching. Carlson et al. (2010) proposed a method based on inductive logic programming (Quinlan, 1990). Schoenmackers et al. (2010) takes relation instances produced by TextRunner (Banko and Etzioni, 2008) as input and induces first-order Horn clauses, and new instances are infered using a Markov Logic Network (Richardson and Domingo, 2006; Huynh and Mooney, 2008). Tsuchida et al. (2010) generated new relation hypotheses by substituting words in seed instances with distributionally similar words. The"
D11-1076,I08-1025,1,0.836691,"4.1 Experimental Setting We evaluate our method on three semantic relation acquisition tasks: causality, prevention and material. Two concepts stand in a causal relation when the source concept (the “cause”) is directly or indirectly responsible for the subsequent occurrence of the target concept (its “effect”). In a prevention relation the source concept directly or indirectly acts to avoid the occurrence of the target concept, and in a material relation the source concept is a material or ingredient of the target concept. For our experiments we used the latest version of the TSUBAKI corpus (Shinzato et al., 2008), a collection of 600 million Japanese Web pages dependency parsed by the Japanese dependency parser KNP2 . In our implementation of CDP, lexicosyntactic patterns consist of words on the path connecting two nouns in a dependency parse tree. We discard patterns from dependency paths longer than 8 constituent nodes. Furthermore, we estimated pattern frequencies in a subset of the corpus (50 million pages, or 1/12th of the entire corpus) and discarded patterns that co-occur with less than 10 unique noun pairs in this smaller corpus. These restrictions do not apply to the proposed method, which ca"
D12-1034,D12-1057,1,0.152513,"Missing"
D12-1034,I08-1055,0,0.769914,"queries for obtaining answer candidates. Each document in the result of document retrieval is split into a set of answer candidates consisting of five subsequent sentences4 . Subsequent answer candidates can share up to two sentences to avoid errors due to wrong document segmentation. 2 http://lucene.apache.org/solr To the best of our knowledge, few Japanese non-factoid QA systems in the literature have used such a large-scale corpus. 4 The length of acceptable answer candidates for whyQA in the literature ranges from one sentence to two paragraphs (Fukumoto et al., 2007; Murata et al., 2007; Higashinaka and Isozaki, 2008; Verberne et al., 2007; Verberne et al., 2010). 3 Answer candidate ac for question q is ranked according to scoring function S(q, ac) given in Eq. (1) (Murata et al., 2007). Murata et al. (2007)’s method uses text search to look for answer candidates containing terms from the question with additional clue terms referring to “reason” or “cause.” Following the original method we used riyuu (reason), genin (cause) and youin (cause) as clue terms. The top-20 answer candidates for each question are passed on to the next step, which is answer reranking. S(q, ac) assigns a score to answer candidates"
D12-1034,P08-1047,1,0.530577,"e-ranker candidates that themselves contain a term from the question (e.g., “types of cancer” in example A1-2). MSA3 is the n-gram feature that contains one of the clue terms used for answer retrieval (riyuu (reason), genin (cause) or youin (cause)). Here too, n-grams obtained from the questions and answer candidates are distinguished. Finally, MSA4 is the percentage of the question terms found in an answer candidate. 3.2 Semantic Word Class Semantic word classes are sets of semantically similar words. We construct these semantic word classes by using the noun clustering algorithm proposed in Kazama and Torisawa (2008). The algorithm follows the distributional hypothesis, which states that semantically similar words tend to appear in similar contexts (Harris, 1954). By treating syntactic dependency relations between words as “contexts,” the clustering method defines a probabilistic model of noun-verb dependencies with hidden classes as: p(n, v, r) = X p(n|c)p(hv, ri|c)p(c) (2) c 371 Here, n is a noun, v is a verb or noun on which n depends via a grammatical relation r (post-positions in Japanese), and c is a hidden class. Dependency relation frequencies were obtained from our 600-million page web corpus, an"
D12-1034,P09-1083,0,0.0131861,"features and Verberne et al. (2010) exploited WordNet features as a kind of semantic features for training their re-ranker, where we used these features, respectively, for B-Ranker+CR and B-Ranker+WN in our experiment. Our work differs from the above approaches in that we propose semantic word classes and sentiment analysis as a new type of semantic features, and show their usefulness in why-QA. Sentiment analysis has been used before on the slightly unusual task of opinion question answering, where the system is asked to answer subjective opinion questions (Stoyanov et al., 2005; Dang, 2008; Li et al., 2009). To the best of our knowledge though, no previous work has systematically explored the use of sentiment analysis in a general QA setting beyond opinion questions. 7 Conclusion In this paper, we have explored the utility of sentiment analysis and semantic word classes for ranking answer candidates to why-questions. We proposed a set of semantic features that exploit sentiment analysis and semantic word classes obtained from largescale noun clustering, and used them to train an answer candidate re-ranker. Through a series of experiments on 850 why-questions, we showed that the proposed semantic"
D12-1034,N10-1120,0,0.0119335,"their respective semantic word classes. For example, Wdisease in word class 2-gram “cause Wdisease ” from A2 is the semantic word class of rickets, one of the question 372 terms. These features capture the correspondence between semantic word classes in the question and answer candidates. 3.3 Sentiment Analysis Sentiment analysis (SA) features are classified into word-polarity and phrase-polarity features. We use opinion extraction tool8 and sentiment orientation lexicon in the tool for these features. 3.3.1 Opinion Extraction Tool Opinion extraction tool is a software, the implementation of Nakagawa et al. (2010). It extracts linguistic expressions representing opinions (henceforth, we call them sentiment phrases) from a Japanese sentence and then identifies the polarity of these sentiment phrases using machine learning techniques. For example, rickets occur in Q2 and Deficiency of vitamin D can cause rickets in A2 can be identified as sentiment phrases with a negative polarity. The tool identifies sentiment phrases and their polarity by using polarities of words and dependency subtrees as evidence, where these polarities are given in a word polarity dictionary. In this paper, we use a trained model a"
D12-1034,W02-1011,0,0.0168213,"Missing"
D12-1034,N04-3012,0,0.0542856,"these CR features are introduced only for comparing our semantic features with ones in Higashinaka and Isozaki (2008) and they are not a part of our method. B-Ranker+WN: its re-ranker is trained with our MSA features and the WordNet features in Verberne et al. (2010). The WordNet features include the percentage of the question terms and their synonyms in WordNet synsets found in an answer candidate and the semantic relatedness score between a question and its answer candidate, the average of the concept similarity between each question term and all of the answer terms by WordNet::Similarity (Pedersen et al., 2004). We used the Japanese WordNet 1.1 (Bond et al., 2009) for these WordNet features. Note that the Japanese WordNet 1.1 has 93,834 Japanese words linked to 57,238 WordNet synsets, while the English WordNet 3.0 covers 155,287 words linked to 117,659 synsets. Due to this lower coverage, the WordNet features in Japanese may have a less power for finding a correct answer than those in English used in Verberne et al. (2010). Proposed: our proposed method. All of the MSA, SWC and SA features are used for training our 376 re-ranker. UpperBound: a system that ranks all n correct answers as the top n res"
D12-1034,H05-1116,0,0.0201051,"sozaki (2008) used causal relation features and Verberne et al. (2010) exploited WordNet features as a kind of semantic features for training their re-ranker, where we used these features, respectively, for B-Ranker+CR and B-Ranker+WN in our experiment. Our work differs from the above approaches in that we propose semantic word classes and sentiment analysis as a new type of semantic features, and show their usefulness in why-QA. Sentiment analysis has been used before on the slightly unusual task of opinion question answering, where the system is asked to answer subjective opinion questions (Stoyanov et al., 2005; Dang, 2008; Li et al., 2009). To the best of our knowledge though, no previous work has systematically explored the use of sentiment analysis in a general QA setting beyond opinion questions. 7 Conclusion In this paper, we have explored the utility of sentiment analysis and semantic word classes for ranking answer candidates to why-questions. We proposed a set of semantic features that exploit sentiment analysis and semantic word classes obtained from largescale noun clustering, and used them to train an answer candidate re-ranker. Through a series of experiments on 850 why-questions, we sho"
D12-1034,J11-2003,0,0.0266434,"their score given by SVMs. We trained and tested the re-ranker using 10-fold cross validation on a corpus composed of 850 why-questions and their top20 answer candidates provided by the answer retrieval procedure in Section 2.1. The answer candidates were manually annotated by three human annotators (not by the authors). Our corpus construction method is described in more detail in Section 4. 370 Morphological and Syntactic Analysis MSA including n-grams of morphemes, words, and syntactic dependencies has been widely used for reranking answers in non-factoid QA (Higashinaka and Isozaki, 2008; Surdeanu et al., 2011; Verberne et al., 2007; Verberne et al., 2010). We use MSA as a baseline feature set in this work. We represent all sentences in a question and its answer candidate in three ways: morphemes, word phrases (bunsetsu5 ) and syntactic dependency chains. These are obtained using a morphological analyzer6 and a dependency parser7 . From each question and answer candidate we extract n-grams of morphemes, word phrases, and syntactic dependencies, where n ranges from 1 to 3. Syntactic dependency n-grams are defined as a syntactic dependency chain containing n word phrases. Syntactic dependency 1-grams"
D12-1034,P02-1053,0,0.0123562,"Missing"
D12-1034,J10-2003,0,0.23433,"in the result of document retrieval is split into a set of answer candidates consisting of five subsequent sentences4 . Subsequent answer candidates can share up to two sentences to avoid errors due to wrong document segmentation. 2 http://lucene.apache.org/solr To the best of our knowledge, few Japanese non-factoid QA systems in the literature have used such a large-scale corpus. 4 The length of acceptable answer candidates for whyQA in the literature ranges from one sentence to two paragraphs (Fukumoto et al., 2007; Murata et al., 2007; Higashinaka and Isozaki, 2008; Verberne et al., 2007; Verberne et al., 2010). 3 Answer candidate ac for question q is ranked according to scoring function S(q, ac) given in Eq. (1) (Murata et al., 2007). Murata et al. (2007)’s method uses text search to look for answer candidates containing terms from the question with additional clue terms referring to “reason” or “cause.” Following the original method we used riyuu (reason), genin (cause) and youin (cause) as clue terms. The top-20 answer candidates for each question are passed on to the next step, which is answer reranking. S(q, ac) assigns a score to answer candidates like tf -idf , where 1/dist(t1 , t2 ) function"
D12-1034,W09-3401,0,\N,Missing
D12-1057,C08-1001,0,0.124343,"rientation. Most previous methods of contradiction extraction require either thesauri like Roget’s or WordNet (Harabagiu et al., 2006; Mohammad et al., 2008; de Marneffe et al., 2008) or large training data for supervision (Turney, 2008). In contrast, our method requires only a few seed templates. Lin et al. (2003) used a few “incompatibility” patterns to acquire antonyms, but they did not report their method’s performance on the incompatibility identification task. Many methods for extracting causality or scriptlike knowledge between events exist (Girju, 2003; Torisawa, 2005; Torisawa, 2006; Abe et al., 2008; Chambers and Jurafsky, 2009; Do et al., 2011; Shibata and Kurohashi, 2011), but none uses a notion similar to Excitation. As we have shown, we expect that Excitation will improve their performance. Regarding the acquisition of semantic knowledge that is not explicitly written in corpora, Tsuchida et al. (2011) proposed a novel method to generate semantic relation instances as hypotheses using automatically discovered inference rules. We think that automatically generating plausible semantic knowledge that is not written (explicitly) in corpora as hypotheses and augmenting semantic knowledge"
D12-1057,andreevskaia-bergler-2006-semantic,0,0.0104394,"e latter is negative. General Inquirer (Stone et al., 1966) deals with semantic factors some of which were proposed by Osgood et al. (1957). Their ‘activity’ factor involves binary opposition between ‘active’ and ‘passive.’ Notice that activity and Excitation are independent. In General Inquirer, both accelerate X and abolish X are active, but only the former is excitatory. Both accept X and abate X are passive, but only the latter is inhibitory. Pustejovsky (1995) proposed telic 620 and agentive roles, which inspired our excitatory notion, but they have no corresponding notion of inhibitory. Andreevskaia and Bergler (2006) acquired the increase/decrease semantic orientation, which is a subclass of Excitation. Excitation is inverted if a template’s predicate is negated. For example, preserve X is excitatory, while don’t preserve X is inhibitory. We acknowledge that this may seem somewhat counter-intuitive and will address this issue in future work. 3 Excitation Template Acquisition This section presents our acquisition method of Excitation templates. We introduce constraints in the co-occurrence of templates in text that seem both robust and language independent in Section 3.1. Our method exploits these constrai"
D12-1057,P09-1068,0,0.00625682,"revious methods of contradiction extraction require either thesauri like Roget’s or WordNet (Harabagiu et al., 2006; Mohammad et al., 2008; de Marneffe et al., 2008) or large training data for supervision (Turney, 2008). In contrast, our method requires only a few seed templates. Lin et al. (2003) used a few “incompatibility” patterns to acquire antonyms, but they did not report their method’s performance on the incompatibility identification task. Many methods for extracting causality or scriptlike knowledge between events exist (Girju, 2003; Torisawa, 2005; Torisawa, 2006; Abe et al., 2008; Chambers and Jurafsky, 2009; Do et al., 2011; Shibata and Kurohashi, 2011), but none uses a notion similar to Excitation. As we have shown, we expect that Excitation will improve their performance. Regarding the acquisition of semantic knowledge that is not explicitly written in corpora, Tsuchida et al. (2011) proposed a novel method to generate semantic relation instances as hypotheses using automatically discovered inference rules. We think that automatically generating plausible semantic knowledge that is not written (explicitly) in corpora as hypotheses and augmenting semantic knowledge base is important for the dis"
D12-1057,D11-1027,0,0.0828657,"ion extraction require either thesauri like Roget’s or WordNet (Harabagiu et al., 2006; Mohammad et al., 2008; de Marneffe et al., 2008) or large training data for supervision (Turney, 2008). In contrast, our method requires only a few seed templates. Lin et al. (2003) used a few “incompatibility” patterns to acquire antonyms, but they did not report their method’s performance on the incompatibility identification task. Many methods for extracting causality or scriptlike knowledge between events exist (Girju, 2003; Torisawa, 2005; Torisawa, 2006; Abe et al., 2008; Chambers and Jurafsky, 2009; Do et al., 2011; Shibata and Kurohashi, 2011), but none uses a notion similar to Excitation. As we have shown, we expect that Excitation will improve their performance. Regarding the acquisition of semantic knowledge that is not explicitly written in corpora, Tsuchida et al. (2011) proposed a novel method to generate semantic relation instances as hypotheses using automatically discovered inference rules. We think that automatically generating plausible semantic knowledge that is not written (explicitly) in corpora as hypotheses and augmenting semantic knowledge base is important for the discovery of so-call"
D12-1057,W03-1210,0,0.388258,"itation represents a genuinely new semantic orientation. Most previous methods of contradiction extraction require either thesauri like Roget’s or WordNet (Harabagiu et al., 2006; Mohammad et al., 2008; de Marneffe et al., 2008) or large training data for supervision (Turney, 2008). In contrast, our method requires only a few seed templates. Lin et al. (2003) used a few “incompatibility” patterns to acquire antonyms, but they did not report their method’s performance on the incompatibility identification task. Many methods for extracting causality or scriptlike knowledge between events exist (Girju, 2003; Torisawa, 2005; Torisawa, 2006; Abe et al., 2008; Chambers and Jurafsky, 2009; Do et al., 2011; Shibata and Kurohashi, 2011), but none uses a notion similar to Excitation. As we have shown, we expect that Excitation will improve their performance. Regarding the acquisition of semantic knowledge that is not explicitly written in corpora, Tsuchida et al. (2011) proposed a novel method to generate semantic relation instances as hypotheses using automatically discovered inference rules. We think that automatically generating plausible semantic knowledge that is not written (explicitly) in corpor"
D12-1057,P97-1023,0,0.700856,"ate X, close to X) For example, when fire fills the X slot of cause X, it suggests that the effect of fire is activated. If prevent X’s slot is filled with flu, the effect of flu is suppressed. In this study, we aim to acquire excitatory and inhibitory templates that are useful for extracting contradiction and causality, though neutral templates are the most frequent in our data (See Section 5.1). Collectively we call excitatory and inhibitory templates Excitation templates, and excitatory and inhibitory two opposite polarities. Excitation is independent of the good/bad semantic orientation. (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Rao and Ravichandran, 2009). For example, sophisticate X and complicate X are both excitatory, but only the former has a positive connotation. Similarly, remedy X and degrade X are both inhibitory but only the latter is negative. General Inquirer (Stone et al., 1966) deals with semantic factors some of which were proposed by Osgood et al. (1957). Their ‘activity’ factor involves binary opposition between ‘active’ and ‘passive.’ Notice that activity and Excitation are independent. In General Inquirer, both accelerate X and abolish X are active, but only the former is excitatory."
D12-1057,N06-1023,0,0.017355,"Missing"
D12-1057,P98-2127,0,0.15841,"contrast in Excitation value. Concretely, we 622 extract two phrases as a contradiction pair if (a) their templates have opposite Excitation polarities, (b) they share the same argument noun, and (c) the part-of-speech of their predicates is the same. Then the contradiction pairs are ranked by Ct: Ct(p1 , p2 ) = |s1 |× |s2 |× sim(t1 , t2 ) Here p1 and p2 are two phrases that satisfy conditions (a), (b) and (c) above, t1 and t2 are their respective templates, and |s1 |and |s2 |are the absolute values of t1 and t2 ’s excitation values. sim(t1 , t2 ) is the distributional similarity proposed by Lin (1998). Note that “contradiction” here includes what we call “quasi-contradiction.” This consists of two phrases such that, if the tendencies of the events they describe get stronger, they eventually become contradictions. For example, the pair emit smells ⊥ reduce smells is not logically contradictory since the two events can happen at the same time. However, they become almost contradictory when their tendencies get stronger (i.e., emit smells more strongly ⊥ thoroughly reduce smells). We believe quasicontradictions are useful for NLP tasks. 4.2 Causality Extraction Our second knowledge acquisitio"
D12-1057,D08-1103,0,0.0497279,"Missing"
D12-1057,D12-1034,1,0.153229,"Missing"
D12-1057,E09-1077,0,0.0403667,"e X slot of cause X, it suggests that the effect of fire is activated. If prevent X’s slot is filled with flu, the effect of flu is suppressed. In this study, we aim to acquire excitatory and inhibitory templates that are useful for extracting contradiction and causality, though neutral templates are the most frequent in our data (See Section 5.1). Collectively we call excitatory and inhibitory templates Excitation templates, and excitatory and inhibitory two opposite polarities. Excitation is independent of the good/bad semantic orientation. (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Rao and Ravichandran, 2009). For example, sophisticate X and complicate X are both excitatory, but only the former has a positive connotation. Similarly, remedy X and degrade X are both inhibitory but only the latter is negative. General Inquirer (Stone et al., 1966) deals with semantic factors some of which were proposed by Osgood et al. (1957). Their ‘activity’ factor involves binary opposition between ‘active’ and ‘passive.’ Notice that activity and Excitation are independent. In General Inquirer, both accelerate X and abolish X are active, but only the former is excitatory. Both accept X and abate X are passive, but"
D12-1057,I11-1115,0,0.0147558,"quire either thesauri like Roget’s or WordNet (Harabagiu et al., 2006; Mohammad et al., 2008; de Marneffe et al., 2008) or large training data for supervision (Turney, 2008). In contrast, our method requires only a few seed templates. Lin et al. (2003) used a few “incompatibility” patterns to acquire antonyms, but they did not report their method’s performance on the incompatibility identification task. Many methods for extracting causality or scriptlike knowledge between events exist (Girju, 2003; Torisawa, 2005; Torisawa, 2006; Abe et al., 2008; Chambers and Jurafsky, 2009; Do et al., 2011; Shibata and Kurohashi, 2011), but none uses a notion similar to Excitation. As we have shown, we expect that Excitation will improve their performance. Regarding the acquisition of semantic knowledge that is not explicitly written in corpora, Tsuchida et al. (2011) proposed a novel method to generate semantic relation instances as hypotheses using automatically discovered inference rules. We think that automatically generating plausible semantic knowledge that is not written (explicitly) in corpora as hypotheses and augmenting semantic knowledge base is important for the discovery of so-called “unknown unknowns” (Torisaw"
D12-1057,P05-1017,0,0.236301,"Missing"
D12-1057,N06-1008,1,0.773225,"y new semantic orientation. Most previous methods of contradiction extraction require either thesauri like Roget’s or WordNet (Harabagiu et al., 2006; Mohammad et al., 2008; de Marneffe et al., 2008) or large training data for supervision (Turney, 2008). In contrast, our method requires only a few seed templates. Lin et al. (2003) used a few “incompatibility” patterns to acquire antonyms, but they did not report their method’s performance on the incompatibility identification task. Many methods for extracting causality or scriptlike knowledge between events exist (Girju, 2003; Torisawa, 2005; Torisawa, 2006; Abe et al., 2008; Chambers and Jurafsky, 2009; Do et al., 2011; Shibata and Kurohashi, 2011), but none uses a notion similar to Excitation. As we have shown, we expect that Excitation will improve their performance. Regarding the acquisition of semantic knowledge that is not explicitly written in corpora, Tsuchida et al. (2011) proposed a novel method to generate semantic relation instances as hypotheses using automatically discovered inference rules. We think that automatically generating plausible semantic knowledge that is not written (explicitly) in corpora as hypotheses and augmenting s"
D12-1057,I11-1101,1,0.787648,"Missing"
D12-1057,P02-1053,0,0.0137921,"fire fills the X slot of cause X, it suggests that the effect of fire is activated. If prevent X’s slot is filled with flu, the effect of flu is suppressed. In this study, we aim to acquire excitatory and inhibitory templates that are useful for extracting contradiction and causality, though neutral templates are the most frequent in our data (See Section 5.1). Collectively we call excitatory and inhibitory templates Excitation templates, and excitatory and inhibitory two opposite polarities. Excitation is independent of the good/bad semantic orientation. (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Rao and Ravichandran, 2009). For example, sophisticate X and complicate X are both excitatory, but only the former has a positive connotation. Similarly, remedy X and degrade X are both inhibitory but only the latter is negative. General Inquirer (Stone et al., 1966) deals with semantic factors some of which were proposed by Osgood et al. (1957). Their ‘activity’ factor involves binary opposition between ‘active’ and ‘passive.’ Notice that activity and Excitation are independent. In General Inquirer, both accelerate X and abolish X are active, but only the former is excitatory. Both accept X"
D12-1057,C08-1114,0,0.0198351,"results, we conclude that our assumption on causality hypothesis generation is valid. 6 Related Work While the semantic orientation involving good/bad (or desirable/undesirable) has been extensively stud628 ied (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Rao and Ravichandran, 2009; Velikovich et al., 2010), we believe Excitation represents a genuinely new semantic orientation. Most previous methods of contradiction extraction require either thesauri like Roget’s or WordNet (Harabagiu et al., 2006; Mohammad et al., 2008; de Marneffe et al., 2008) or large training data for supervision (Turney, 2008). In contrast, our method requires only a few seed templates. Lin et al. (2003) used a few “incompatibility” patterns to acquire antonyms, but they did not report their method’s performance on the incompatibility identification task. Many methods for extracting causality or scriptlike knowledge between events exist (Girju, 2003; Torisawa, 2005; Torisawa, 2006; Abe et al., 2008; Chambers and Jurafsky, 2009; Do et al., 2011; Shibata and Kurohashi, 2011), but none uses a notion similar to Excitation. As we have shown, we expect that Excitation will improve their performance. Regarding the acquisi"
D12-1057,N10-1119,0,0.0194522,"the case was erroneous since its original causality was erroneous. The second 6 case was due to the fact that one of the contradiction phrase pairs used to generate the hypothesis was in fact not contradictory (景 気をコントロールする 6⊥ 景気が良くなる ‘control economic conditions 6⊥ economic conditions improve’). From these results, we conclude that our assumption on causality hypothesis generation is valid. 6 Related Work While the semantic orientation involving good/bad (or desirable/undesirable) has been extensively stud628 ied (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Rao and Ravichandran, 2009; Velikovich et al., 2010), we believe Excitation represents a genuinely new semantic orientation. Most previous methods of contradiction extraction require either thesauri like Roget’s or WordNet (Harabagiu et al., 2006; Mohammad et al., 2008; de Marneffe et al., 2008) or large training data for supervision (Turney, 2008). In contrast, our method requires only a few seed templates. Lin et al. (2003) used a few “incompatibility” patterns to acquire antonyms, but they did not report their method’s performance on the incompatibility identification task. Many methods for extracting causality or scriptlike knowledge betwee"
D12-1057,P08-1008,0,0.088906,"Missing"
D12-1057,P08-1118,0,\N,Missing
D12-1057,C98-2122,0,\N,Missing
H05-1018,P04-1054,0,0.116891,"788 84.79 λ = 0.25, C = 8.647 86.20 λ = 0.20, C = 3.344 83.58 λ = 0.15, C = 20.57 91.11 λ = 0.15, C = 13.95 89.59 F1 (test) 87.90 85.56 84.70 83.51 86.64 83.72 92.92 91.32 marking where we determine the mark (tag) of a node. Kashima and Koyanagi (2002) dealt with this task by inserting the node representing the mark above the node to be tagged and classifying the transformed tree using SVMs with tree kernels such as Klo . For the SRL task, Moschitti (2004) applied the tree kernel (Kc ) to tree fragments that are heuristically extracted to reflect the role of interest. For relation extraction, Culotta and Sorensen (2004) proposed a tree kernel that operates on only the smallest tree fragment including two entities for which a relation is assigned. Our kernels on marked labeled ordered trees differ in what subtrees are permitted. Although comparisons are needed, we think our kernels are intuitive and general. There are many possible structures for which tree kernels can be defined. Shen et al. (2003) proposed a tree kernel for LTAG derivation trees to focus only on linguistically meaningful structures. Culotta and Sorensen (2004) proposed a tree kernel for dependency trees. An important future task is to find"
H05-1018,J02-3001,0,0.176394,", 1995). Previous studies (Collins and Duffy, 2001; Kashima and Koyanagi, 2002) showed that although it is difficult to explicitly calculate the inner product in Eq. (1) because we need to consider an exponential number of possible subtrees, the tree kernels can be computed in O(|T1 ||T2 |) time by using dynamic programming (DP) procedures. However, these DP procedures are time-consuming in practice. In this paper, we present a method for speeding up the training with tree kernels. Our target application is node relation labeling, which includes NLP tasks such as semantic role labeling (SRL) (Gildea and Jurafsky, 2002; Moschitti, 2004; Hacioglu et al., 2004). For this purpose, we designed kernels on marked labeled ordered trees and derived O(|T1 ||T2 |) procedures. However, the lengthy training due to the cost of kernel calculation prevented us from assessing the performance of these kernels and motivated us to make the training practically fast. Our speed-up method is based on the observation that very few pairs in the training set have a great many common subtrees (we call such pairs malicious pairs) and most pairs have a very small number of common subtrees. This leads to a drastic variance in kernel va"
H05-1018,W04-2416,0,0.042368,", 2001; Kashima and Koyanagi, 2002) showed that although it is difficult to explicitly calculate the inner product in Eq. (1) because we need to consider an exponential number of possible subtrees, the tree kernels can be computed in O(|T1 ||T2 |) time by using dynamic programming (DP) procedures. However, these DP procedures are time-consuming in practice. In this paper, we present a method for speeding up the training with tree kernels. Our target application is node relation labeling, which includes NLP tasks such as semantic role labeling (SRL) (Gildea and Jurafsky, 2002; Moschitti, 2004; Hacioglu et al., 2004). For this purpose, we designed kernels on marked labeled ordered trees and derived O(|T1 ||T2 |) procedures. However, the lengthy training due to the cost of kernel calculation prevented us from assessing the performance of these kernels and motivated us to make the training practically fast. Our speed-up method is based on the observation that very few pairs in the training set have a great many common subtrees (we call such pairs malicious pairs) and most pairs have a very small number of common subtrees. This leads to a drastic variance in kernel values, e.g., when W (Si ) = 1. We thus cal"
H05-1018,P03-1004,0,0.18425,"ate the kernel values with all the training examples for a given example Ti : KS(Ti ) = {K(Ti , T1 ), . . . , K(Ti , TL )}, where L is the number of training examples. Using occurrence pattern OP (Fi ) = {(k, #Fi (Tk ))|#Fi (Tk ) 6= 0} preAlgorithm 4.2: FREQTM(D, R) Algorithm 4.1: C ALCULATE KS(Ti ) for each F such that #F (Ti ) 6= 0 do for each (j, #F (Tj )) ∈ OP (F ) do KS(j) ← KS(j) + W (F ) · #F (Ti ) · #F (Tj ) (A) for j = 1 to L do if (i, j) is malicious then KS(j) ← K(Ti , Tj ) (DP) pared beforehand, we can calculate KS(Ti ) efficiently (Algorithm 4.1). A similar technique was used in (Kudo and Matsumoto, 2003a) to speed up the calculation of inner products. We can show that the per-pair cost of Algorithm 4.1 is O(c1 Q + rm c2 |Ti ||Tj |), where Q is the average number of common feature subtrees in a tree pair, rm is the rate of malicious pairs, c1 and c2 are the constant factors for vector operations and DP operations. This cost is independent of the number of training examples. We expect from our observations that both Q and rm are very small and that c1 ¿ c2 . 4.2 Feature Subtree Enumeration with Malicious Pair Detection To detect malicious pairs and enumerate feature subtrees F (and to convert"
H05-1018,W04-3239,0,0.147842,"are very small and that c1 ¿ c2 . 4.2 Feature Subtree Enumeration with Malicious Pair Detection To detect malicious pairs and enumerate feature subtrees F (and to convert each tree to a feature vector), we developed an algorithm based on the FREQT algorithm (Asai et al., 2002). The FREQT algorithm can efficiently enumerate subtrees that are included (Definition 2.1) in more than a pre-specified number of trees in the training examples by generating candidate subtrees using right most expansions (RMEs). FREQT-based algorithms have recently been used in methods that treat subtrees as features (Kudo and Matsumoto, 2004; Kudo and Matsumoto, 2003b). To develop the algorithm, we made the definition of maliciousness more search-oriented since it is costly to check for maliciousness based on the exact number of common subtrees or the kernel values (i.e., by using the DP procedure for all L2 pairs). Whatever definition we use, the correctness is preserved as long as we do not fail to enumerate the subtrees that appear in the pairs we consider nonmalicious. First, we consider pairs (i, i) to always be malicious. Then, we use a FREQT search that enumerates the subtrees that are included in at least two trees as a b"
H05-1018,P04-1043,0,0.16701,"Collins and Duffy, 2001; Kashima and Koyanagi, 2002) showed that although it is difficult to explicitly calculate the inner product in Eq. (1) because we need to consider an exponential number of possible subtrees, the tree kernels can be computed in O(|T1 ||T2 |) time by using dynamic programming (DP) procedures. However, these DP procedures are time-consuming in practice. In this paper, we present a method for speeding up the training with tree kernels. Our target application is node relation labeling, which includes NLP tasks such as semantic role labeling (SRL) (Gildea and Jurafsky, 2002; Moschitti, 2004; Hacioglu et al., 2004). For this purpose, we designed kernels on marked labeled ordered trees and derived O(|T1 ||T2 |) procedures. However, the lengthy training due to the cost of kernel calculation prevented us from assessing the performance of these kernels and motivated us to make the training practically fast. Our speed-up method is based on the observation that very few pairs in the training set have a great many common subtrees (we call such pairs malicious pairs) and most pairs have a very small number of common subtrees. This leads to a drastic variance in kernel values, e.g., when"
H05-1018,W03-1012,0,0.131291,"ith tree kernels such as Klo . For the SRL task, Moschitti (2004) applied the tree kernel (Kc ) to tree fragments that are heuristically extracted to reflect the role of interest. For relation extraction, Culotta and Sorensen (2004) proposed a tree kernel that operates on only the smallest tree fragment including two entities for which a relation is assigned. Our kernels on marked labeled ordered trees differ in what subtrees are permitted. Although comparisons are needed, we think our kernels are intuitive and general. There are many possible structures for which tree kernels can be defined. Shen et al. (2003) proposed a tree kernel for LTAG derivation trees to focus only on linguistically meaningful structures. Culotta and Sorensen (2004) proposed a tree kernel for dependency trees. An important future task is to find suitable structures for each task (the SRL task in our case). Our speed-up method will be beneficial as long as there is unbalanced similarity. 7 Conclusion We have presented a method for speeding up the training with tree kernels. Using the SRL task, we demonstrated that our speed-up method made the training substantially faster. training set size = 8,000 best setting F1 (dev) λ = 0"
H05-1018,P04-1016,0,0.0199587,"be made faster by exploiting the mark information for pruning. Although our method is not a complete solution in a classification setting, it might be in a clustering setting (in a sense it is training only). However, it is an open question whether unbalanced similarity, which is the key to our speed-up, is ubiquitous in NLP tasks and under what conditions our method scales better than the SVMs or other kernel-based methods. Several studies claim that learning using tree kernels and other convolution kernels tends to overfit and propose selecting or restricting features (Cumby and Roth, 2003; Suzuki et al., 2004; Kudo and Matsumoto, 2004). Sometimes, the classification becomes faster as a result (Suzuki et al., 2004; Kudo and Matsumoto, 2004). We do not disagree with these studies. The fact that small λ values resulted in the highest accuracy in our experiment implies that too large subtrees are not so useful. However, since this tendency depends on the task, we need to assess the performance of full tree kernels for comparison. In this sense, our method is of great importance. Node relation labeling is a generalization of node 6 We used 106 as the maximum number of iterations. r and K r . Table 4: C"
H05-1018,W05-0620,0,0.235376,"Missing"
I05-1010,P03-1001,0,0.0346989,"y understood or described, is one of such type of knowledge. For example, the attributes of car objects will be weight, engine, steering wheel, driving feel, and manufacturer. In other words, attributes are items whose values we want to know when we want to know about the object. More analytically, we tend to regard A as an attribute for objects of class C when A works as if function v = A(o), o ∈ C where v is necessary to us to identify o (especially to distinguish o from o (= o) ∈ C). Therefore, obvious applications of attributes are ones such as summarization [1,2] and question-answering [3]. Moreover, they can be useful as features in word clustering [4] or machine learning. Although the knowledge base for attributes can be prepared manually (e.g., WordNet [5]), problems are cost and coverage. To overcome these, we propose a method that automatically acquires attribute knowledge from the Web. To acquire the attributes for a given class, C (e.g., car ), the proposed method ﬁrst downloads documents that contain class label C (e.g., “car”) from the Web.1 We extract the candidates of attribute words from these documents and score them according to the statistics of words, lexico-syn"
I05-1010,W04-3221,0,0.0219875,"example, the attributes of car objects will be weight, engine, steering wheel, driving feel, and manufacturer. In other words, attributes are items whose values we want to know when we want to know about the object. More analytically, we tend to regard A as an attribute for objects of class C when A works as if function v = A(o), o ∈ C where v is necessary to us to identify o (especially to distinguish o from o (= o) ∈ C). Therefore, obvious applications of attributes are ones such as summarization [1,2] and question-answering [3]. Moreover, they can be useful as features in word clustering [4] or machine learning. Although the knowledge base for attributes can be prepared manually (e.g., WordNet [5]), problems are cost and coverage. To overcome these, we propose a method that automatically acquires attribute knowledge from the Web. To acquire the attributes for a given class, C (e.g., car ), the proposed method ﬁrst downloads documents that contain class label C (e.g., “car”) from the Web.1 We extract the candidates of attribute words from these documents and score them according to the statistics of words, lexico-syntactic patterns, and HTML tags. Highly scored words are output as"
I05-1010,C92-2082,0,0.0212173,"erage. To overcome these, we propose a method that automatically acquires attribute knowledge from the Web. To acquire the attributes for a given class, C (e.g., car ), the proposed method ﬁrst downloads documents that contain class label C (e.g., “car”) from the Web.1 We extract the candidates of attribute words from these documents and score them according to the statistics of words, lexico-syntactic patterns, and HTML tags. Highly scored words are output as attributes for the class. Lexico-syntactic patterns and other statistics have been used in other lexical knowledge acquisition systems [3,4,6,7,8]. We speciﬁcally used lexico-syntactic patterns involving the Japanese postposition “no” as used in [8] such as “C no A” where A is an attribute word, which is almost equivalent to pattern “A of C” used in [7] to 1 We use C to denote both the class and its class label (the word representing the class). We also use A to denote both the attribute and the word representing it. R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 106–118, 2005. c Springer-Verlag Berlin Heidelberg 2005  Automatic Discovery of Attribute Words from Web Documents 107 ﬁnd part-whole relations. Novel features of our meth"
I05-1010,P99-1008,0,0.116221,"erage. To overcome these, we propose a method that automatically acquires attribute knowledge from the Web. To acquire the attributes for a given class, C (e.g., car ), the proposed method ﬁrst downloads documents that contain class label C (e.g., “car”) from the Web.1 We extract the candidates of attribute words from these documents and score them according to the statistics of words, lexico-syntactic patterns, and HTML tags. Highly scored words are output as attributes for the class. Lexico-syntactic patterns and other statistics have been used in other lexical knowledge acquisition systems [3,4,6,7,8]. We speciﬁcally used lexico-syntactic patterns involving the Japanese postposition “no” as used in [8] such as “C no A” where A is an attribute word, which is almost equivalent to pattern “A of C” used in [7] to 1 We use C to denote both the class and its class label (the word representing the class). We also use A to denote both the attribute and the word representing it. R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 106–118, 2005. c Springer-Verlag Berlin Heidelberg 2005  Automatic Discovery of Attribute Words from Web Documents 107 ﬁnd part-whole relations. Novel features of our meth"
I05-1010,C00-1060,1,0.757873,"Missing"
I05-1010,N04-1010,1,0.701575,"Missing"
I11-1035,D08-1017,0,0.0345737,"t. 3.2 New Features for POS Tagging We generate n-gram and lexicon features for POS tagging as well. In addition, the features that incorporate word clusters derived from a large autoanalyzed corpus (referred to as cluster features) are introduced. • For the development and test sets, we collect a lexicon using the entire training corpus and use it for feature generation. Because the lexicon is extracted from other sets, the weights for this feature will not be overestimated by the learning algorithm. This kind of cross-validation-like techniques are used in studies such as Collins (2002) and Martins et al. (2008) to avoid over-fitting to the training data. Our method can be considered as its application to lexicon extraction. Using the extracted lexicon, we generate lexicon features as follows. If a character sequence starting with character c0 matches some words in the lexicon, we greedily choose the longest such matching word w. Letting LEN (w) be the length (the number of characters) of w, we add the following feature for each character ck in c0 , c1 , ..., cLEN (w) : (b) P (ck )/LEN (w)-P OSs(w) Here, P (ck ) is the position number (i.e., k) of the character ck in w and P OSs(w) represents the POS"
I11-1035,P09-1058,1,0.736673,"Missing"
I11-1035,W03-1719,0,0.0122318,"81 0.9112 CTB7 0.8996 0.9017 0.9020 0.9019 0.9046 Table 7: Results of word segmentation POS tag method Baseline +(c) n-gram +(d) cluster +(e) lexicon +(c)+(d)+(e) CTB5 0.9318 0.9333 0.9350 0.9346 0.9359 CTB6 0.8999 0.9014 0.9026 0.9015 0.9048 CTB7 0.8937 0.8958 0.8959 0.8959 0.8985 POS tag method Baseline +(c) n-gram +(d) cluster +(e) lexicon +(c)+(d)+(e) Table 8: F1 results of segmentation and POS tagging (baseline model for word segmentation) Table 9: F1 results of segmentation and POS tagging (our best model for word segmentation) the words in the test set that are not in the training set (Sproat and Emerson, 2003). The development sets were used to obtain the optimal values of tunable parameters and feature configurations. The unlabeled data for our experiments were taken from the XIN_CMN portion of Chinese Gigaword Version 2.0 (LDC2009T14), which has approximately 311 million words. Some of CTB data and Chinese Gigaword data are from the same source: Xinhua newswire between 1994 and 1998. In order to avoid overlap between the CTB data and the unlabeled data, we used only the articles published in 1991- 1993 and 1999-2004 as unlabeled data, with 204 million words.8 Note that we only used one million wo"
I11-1035,Y06-1012,0,0.122089,"(and labeled) data into the above baseline models through features. We preprocess unlabeled data with our baseline models and obtain wordsegmented sentences with POS tags, and generate new features from the auto-analyzed data. Although the focus of the paper is semi-supervised learning, we also extract a lexicon from the training corpus and use it to generate features. Figure 1 shows an overview of our approach. The rest of this section describes our features in detail. Segmentation and POS tagging Models We implement our approach using sequential tagging models. Following the previous work (Zhao et al., 2006; Zhao et al., 2010), we employ the linear chain CRFs (Lafferty et al., 2001) as our learning model. Specifically, we use its CRF++ (version 0.54) implementation by Taku Kudo. 1 2.1 Baseline Segmentation Model 3.1 New features for Word Segmentation We employ character-based sequence labeling for word segmentation. In addition to its simplicity, the advantage of a character-based model is its robustness to the unknown word problem (Xue, 2003). In a character-based Chinese word segmentation task, a character in a given sequence is labeled by a tag that stands for its position in the word that th"
I11-1035,P08-1068,0,0.0436031,"Missing"
I11-1035,P07-2055,0,0.142272,"ry is correctly identified. For Seg &Tag, a word is considered correct only when both the word boundary and its POS tag are correctly identified. Table 13 summarizes the results on test sets. These tests suggest that although the difference from K 09b for CTB5 data is not statistically significant, all other differences are clearly statistically significant (p < 10−5 ). 4.4 Comparative Results In this section, we compare our approach with the best previous approaches reported in the literature. The performance scores of previous studies are directly taken from their papers, except for N&U 07 (Nakagawa and Uchimoto, 2007), which is taken from Kruengkrai et al. (2009b). Z&C 10 refers to Zhang and Clark (2010). Two methods in Kruengkrai et al. (2009a; 2009b) are referred to as K 09a and K 09b. Jiang 08a and Jiang 08b refer to Jiang et al. (2008a; 2008b). Table 10 compares F1 results on CTB5.0. The best score in each column is in boldface. The results of our approach are superior to those of previous studies for both 4.6 Comparison with Self-Training An alternative method of incorporating unlabeled data is self-training, so we also compared our results to the self-training method. Because no existing research was"
I11-1035,P08-1102,0,0.21839,"rence from K 09b for CTB5 data is not statistically significant, all other differences are clearly statistically significant (p < 10−5 ). 4.4 Comparative Results In this section, we compare our approach with the best previous approaches reported in the literature. The performance scores of previous studies are directly taken from their papers, except for N&U 07 (Nakagawa and Uchimoto, 2007), which is taken from Kruengkrai et al. (2009b). Z&C 10 refers to Zhang and Clark (2010). Two methods in Kruengkrai et al. (2009a; 2009b) are referred to as K 09a and K 09b. Jiang 08a and Jiang 08b refer to Jiang et al. (2008a; 2008b). Table 10 compares F1 results on CTB5.0. The best score in each column is in boldface. The results of our approach are superior to those of previous studies for both 4.6 Comparison with Self-Training An alternative method of incorporating unlabeled data is self-training, so we also compared our results to the self-training method. Because no existing research was found concerning the selftraining method on word segmentation and POS 9 We used the version with Yates’ correction, using correction factor 0.5 315 Sentences added 0(Baseline) 5k 10k 30k 150k 300k 600k Segmentation F1 0.9498"
I11-1035,C08-1049,0,0.103149,"rence from K 09b for CTB5 data is not statistically significant, all other differences are clearly statistically significant (p < 10−5 ). 4.4 Comparative Results In this section, we compare our approach with the best previous approaches reported in the literature. The performance scores of previous studies are directly taken from their papers, except for N&U 07 (Nakagawa and Uchimoto, 2007), which is taken from Kruengkrai et al. (2009b). Z&C 10 refers to Zhang and Clark (2010). Two methods in Kruengkrai et al. (2009a; 2009b) are referred to as K 09a and K 09b. Jiang 08a and Jiang 08b refer to Jiang et al. (2008a; 2008b). Table 10 compares F1 results on CTB5.0. The best score in each column is in boldface. The results of our approach are superior to those of previous studies for both 4.6 Comparison with Self-Training An alternative method of incorporating unlabeled data is self-training, so we also compared our results to the self-training method. Because no existing research was found concerning the selftraining method on word segmentation and POS 9 We used the version with Yates’ correction, using correction factor 0.5 315 Sentences added 0(Baseline) 5k 10k 30k 150k 300k 600k Segmentation F1 0.9498"
I11-1035,I08-1012,1,0.81753,"gging by incorporating large unlabeled data. We first preprocess unlabeled data with our baseline models. We then extract various items of dictionary information from the auto-analyzed data. Finally, we generate new features that incorporate the extracted information for both word segmentation and POS tagging. We also perform word clustering on the auto-segmented data and use word clusters as features in POS tagging. In addition, we introduce lexicon features by using a crossvalidation technique. The use of sub-structures from the autoannotated data has been presented previously (Noord, 2007; Chen et al., 2008; Chen et al., 2009). Chen et al. (2009) extracted different types of subtrees from the auto-parsed data and used them as new features in standard learning methods. They showed this simple method greatly improves the accuracy of dependency parsing. The idea of combining word clusters with discriminative learning has been previously reported in the context of named entity recognition (Miller et al., 2004; Kazama and Torisawa, 2008) and dependency parsing (Koo et al., 2008). We adapt and extend these techniques to Chinese word segmentation and POS tagging, and demonstrate their effectiveness in"
I11-1035,D09-1060,1,0.381889,"Missing"
I11-1035,I08-4029,0,0.134356,"be a character n-gram (e.g., uni-gram ci , bi-gram ci ci+1 , trigram ci−1 ci ci+1 and so on)2 , and seg be a segmentation profile for n-gram g observed at each position. The segmentation profile can be tag ti or the combination of tags. Take a bi-gram for example, seg may be ti or ti ti+1 . Then, 2.2 Baseline POS Tagging Model Since we employ a pipelined method, the POS tagging can be performed as a word labeling task, where the input is a sequence of segmented words. We use a CRF here as well. The feature set of our baseline POS tagger, is listed in Table 3. These are adopted from Wu et al. (2008). 1 2 Note that there are several alternative ways for extracting n-grams at position i, for example ci−1 ci for a bi-gram. In this paper, we used the way as explained here. Available from http://crfpp.sourceforge.net/ 310 Feature Type Word Unigram Nearing Word Bigram Jump Word Bigram First Character Last Character Length Context Position w−2 ,w−1 ,w0 ,w1 ,w2 (w−2 w−1 ),(w−1 w0 ),(w1 w0 ),(w1 w2 ) (w−1 ,w1 ) F c(w0 ) Lc(w0 ) Len(w0 ) Description Word unigram Word bigram Previous word and next word First character of current word Last character of current word Length of current word Table 3: Fe"
I11-1035,D09-1058,0,0.0118399,"r-level NLP tasks such as parsing and information extraction. Although the performance of Chinese word segmentation and POS tagging has been greatly improved over the past years, the task is still challenging. To improve the accuracy of NLP systems, one of the current trends is semi-supervised learning, which utilizes large unlabeled data in supervised learning. Several studies have demonstrated that the use of unlabeled data can improve the performance of NLP tasks, such as text chunking (Ando and Zhang, 2005), POS tagging and named entity recognition (Suzuki and Isozaki, 2008), and parsing (Suzuki et al., 2009; Chen et al., 2009; Koo et al., 2008). Therefore, it is attractive to consider adopting semi-supervised learning in Chinese word segmentation and POS tagging tasks. 309 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 309–317, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP Word Length Tags 1 S 2 BE 3 BB2 E 4 BB2 B3 E 5 BB2 B3 M E 6 BB2 B3 M M E 7 or more BB2 B3 M...M E Table 1: Word representation with a 6-tag tagset : S, B, B2 , B3 , M, E Type Character Unigram Nearing Character Bigram Jump Character Bigram Punctuation Character Type Feat"
I11-1035,D10-1082,0,0.4394,"boundary and its POS tag are correctly identified. Table 13 summarizes the results on test sets. These tests suggest that although the difference from K 09b for CTB5 data is not statistically significant, all other differences are clearly statistically significant (p < 10−5 ). 4.4 Comparative Results In this section, we compare our approach with the best previous approaches reported in the literature. The performance scores of previous studies are directly taken from their papers, except for N&U 07 (Nakagawa and Uchimoto, 2007), which is taken from Kruengkrai et al. (2009b). Z&C 10 refers to Zhang and Clark (2010). Two methods in Kruengkrai et al. (2009a; 2009b) are referred to as K 09a and K 09b. Jiang 08a and Jiang 08b refer to Jiang et al. (2008a; 2008b). Table 10 compares F1 results on CTB5.0. The best score in each column is in boldface. The results of our approach are superior to those of previous studies for both 4.6 Comparison with Self-Training An alternative method of incorporating unlabeled data is self-training, so we also compared our results to the self-training method. Because no existing research was found concerning the selftraining method on word segmentation and POS 9 We used the ver"
I11-1035,N04-1043,0,\N,Missing
I11-1035,W07-2201,0,\N,Missing
I11-1035,P08-1047,1,\N,Missing
I11-1035,P08-1076,0,\N,Missing
I11-1035,O03-4002,0,\N,Missing
I11-1035,I05-3025,0,\N,Missing
I11-1035,W03-1726,0,\N,Missing
I11-1035,P02-1062,0,\N,Missing
I11-1035,N06-1020,0,\N,Missing
I11-1060,J92-4003,0,0.100893,"Missing"
I11-1060,N03-2003,0,0.0396781,"and the resulting speech recognition performance was worse than that obtained by a corpus randomly sampled from our Web archive. To achieve the performance compatible to our method, thorough research must be conducted on generating only natural questions. 6 Related Work The Web has been used as a relatively inexpensive source of large-scale data. “Just-in-time” language modeling (Berger et al., 1998) submits content words from previous user sentences as queries to a web search engine, Zhu et al. (2001) use a search engine to update the probabilities of already existing n-grams. More recently Bulyko et al. (2003) use frequent n-grams of the seed corpus as queries to retrieve similar text from the Web. Sarikaya et al. (2005) retrieves relevant text based on the BLEU score. Word perplexity is another frequently used similarity measure (Misu and Kawahara, 2006; Creutz et al., 2009). Some 7 Conclusions We have proposed a similarity based language model construction method for Ikkyu, a voice driven open-domain QA system. We used the combination of a distributional similarity based noun replacement method and a statistical domain adaptation method. Our best language model outperformed the baseline model con"
I11-1060,E09-1019,0,0.0309227,"Missing"
I11-1060,P01-1068,0,\N,Missing
I11-1060,P10-1026,1,\N,Missing
I11-1060,I08-1025,0,\N,Missing
I11-1098,I08-1025,1,0.92656,"ven corpus. In the E-step, probability P (a |< v, p &gt;) is calculated. In the M-step, probabilities P (< v, p &gt; |a), P (n|a), and P (a) are updated to arrive at the maximum likelihood using the results of the E-step. From the results of estimation by this EM-based clustering method, probabilities P (< v, p &gt; |a), P (n|a), and P (a) for < v, p &gt;, n, and a are obtained. P (a|n) is then calculated by the following equation: P (a|n) = P P (n|a)P (a) a∈A P (n|a)P (a) (5) With the aim of enabling large-scale clustering and using the resulting clusters in named entity recognition, Kazama and Torisawa (2008) proposed parallelization of this EM-based clustering method. Kazama et al. (2009) then reported the calculation of distributional similarity by using the clustering results. We applied their method to the TSUBAKI corpus (Shinzato et al., 2008), a collection of 100-million Japanese Web pages containing 6 × 109 sentences. We prepared 3.2.2 Measuring Distributional Similarity The distributional similarity between two terms (n1 and n2 ) is defined as sim(n1 , n2 ) = 1 − DJS (P (a|n1 )kP (a|n2 )) (3) 877 3.3 about 1,000,000 terms for calculating the distributional similarity. These one million ter"
I11-1098,P06-1101,0,0.225254,"rmance of the classifier when one of the features was ignored) were conducted. Table 3 lists the results of these tests. This table shows that Feature set All w/o Hyper (f1) w/o Name of SIM (f2) w/o Value of P S(I, syn) (f3) w/o Synset ID (f4) w/o Suffix of the trg (f5) w/o Suffix of the hyper (f6) freq. 336 336 132 68 44 44 28 25 25 23 22 22 19 17 17 4.6 Analysis 4.6.1 Advantage of Proposed Method The advantage of the proposed method compared to previous methods that use nothing but either distributional similarity of a trg or co-occurrence with their hypernyms via lexico-syntactic patterns (Snow et al., 2006; Yamada et al., 2009) (or both) was demonstrated as follows. Specifically, it was shown that many terms do not have reliable distributional similarity (owing to their infrequency in a corpus) and do not co-occur with their hypernyms via any lexico-syntactic pattern in a sentence. Even so, our method can correctly identify the synset of such terms thanks to their hyPrecision 82.6 (661/800) 82.1 (657/800) 82.5 (660/800) 82.0 (656/800) 81.1 (649/800) 82.1 (657/800) 82.4 (659/800) Table 3: Results of ablation test. 880 pers and sibs acquired from the internal structure of Wikipedia articles. Firs"
I11-1098,C92-2082,0,0.271005,"Missing"
I11-1098,P08-1047,1,0.858574,"lities by using a given corpus. In the E-step, probability P (a |< v, p &gt;) is calculated. In the M-step, probabilities P (< v, p &gt; |a), P (n|a), and P (a) are updated to arrive at the maximum likelihood using the results of the E-step. From the results of estimation by this EM-based clustering method, probabilities P (< v, p &gt; |a), P (n|a), and P (a) for < v, p &gt;, n, and a are obtained. P (a|n) is then calculated by the following equation: P (a|n) = P P (n|a)P (a) a∈A P (n|a)P (a) (5) With the aim of enabling large-scale clustering and using the resulting clusters in named entity recognition, Kazama and Torisawa (2008) proposed parallelization of this EM-based clustering method. Kazama et al. (2009) then reported the calculation of distributional similarity by using the clustering results. We applied their method to the TSUBAKI corpus (Shinzato et al., 2008), a collection of 100-million Japanese Web pages containing 6 × 109 sentences. We prepared 3.2.2 Measuring Distributional Similarity The distributional similarity between two terms (n1 and n2 ) is defined as sim(n1 , n2 ) = 1 − DJS (P (a|n1 )kP (a|n2 )) (3) 877 3.3 about 1,000,000 terms for calculating the distributional similarity. These one million ter"
I11-1098,I08-2126,1,0.899772,"Missing"
I11-1098,sumida-etal-2008-boosting,1,0.84964,"ned by the judges’ majority vote. The interrater agreement between the three judges (Siegel’s Kappa) was 0.785, indicating substantial agreement. We performed parameter optimization by using the development data. The parameters used in our method showing the best performance for development data were used in our experiments, namely, the number of similar terms k = 60, the parameter for score propagation λ = 0.6, and weights for S(n, syn) in P S(I, syn) (α = 0.6 and β = 0.4 for SIM2 and α = 0.5, β = 0.4, and γ = 0.1 for SIM3 ). 4.2 can be considered simple extensions of an existing research of Sumida et al. (2008) for estimating Wordnet synsets. Table 2 shows the precision rate of each system. We could not evaluate all 1,800 samples for B1, SB1, EB1 and EB2. B1 and SB1 were able to generate outputs for 614 Is, where trg in Is was included in the target terms for calculating the distributional similarity. EB1 and EB2 can select the synset when hyper or the suffix of hyper is registered in WordNet. For this reason, it was not possible to select a synset for 174 trgs out of the 1,800 samples in EB1 and EB2. As a result, we used 1,636 samples in evaluating EB1 and EB2. SB1–SB3, CB2, EB2, and the proposed m"
I11-1098,P09-1049,1,0.90431,"Missing"
I11-1098,toral-etal-2008-named,0,0.183001,"Missing"
I11-1098,R09-1080,0,0.0502895,"Missing"
I11-1098,C10-1095,1,0.813257,"Missing"
I11-1098,D09-1097,1,0.867635,"Missing"
I11-1098,N04-1010,1,0.875766,"Missing"
I11-1098,W09-3401,0,\N,Missing
I11-1101,P08-1004,0,0.294231,"tances, which are not written in any single sentence and may not be even written in a large corpus. We develop a method to infer new semantic relation instances by applying auto-discovered inference rules, and show that our method inferred a considerable number of valid instances that were not written in single sentences even in 600 million Web pages. 1 Most existing relation acquisition methods acquire relation instances using lexico-syntactic patterns (Pantel and Pennacchiotti, 2006; Pas¸ca et al., 2006; De Saeger et al., 2009) such as “X causes Y” or probabilistic sequence labeling models (Banko and Etzioni, 2008). These methods basically rely on the structure of single sentences and they are for acquiring SS instances. Thus we consider that NS instances are practically beyond their reach. A few attempts to overcome this limitation include inference-based methods. These methods take SS instances provided by other methods as input and infer relation instances including NS ones using auto-discovered inference rules (Schoenmackers et al., 2010; Carlson et al., 2010) or distributional similarities (Tsuchida et al., 2010). We consider such inference approaches to be promising for acquiring NS instances. Int"
I11-1101,D10-1106,0,0.134826,"exico-syntactic patterns (Pantel and Pennacchiotti, 2006; Pas¸ca et al., 2006; De Saeger et al., 2009) such as “X causes Y” or probabilistic sequence labeling models (Banko and Etzioni, 2008). These methods basically rely on the structure of single sentences and they are for acquiring SS instances. Thus we consider that NS instances are practically beyond their reach. A few attempts to overcome this limitation include inference-based methods. These methods take SS instances provided by other methods as input and infer relation instances including NS ones using auto-discovered inference rules (Schoenmackers et al., 2010; Carlson et al., 2010) or distributional similarities (Tsuchida et al., 2010). We consider such inference approaches to be promising for acquiring NS instances. Introduction Recent advances in automatic relation acquisition methods (Agichtein and Luis, 2001; Etzioni et al., 2004; Pantel and Pennacchiotti, 2006; Pas¸ca et al., 2006; Banko and Etzioni, 2008; De Saeger et al., 2009) have opened the way to build large knowledge bases containing a huge number of semantic relation instances such as “CAUSE(allergen, allergy)” and “PREVENTION(coffee, drowsiness)”. Such a massive knowledge base is val"
I11-1101,D11-1076,1,0.877572,"Missing"
I11-1101,P08-1047,1,0.850247,"cquire SS instances from highly complex and infrequent expressions, using word classes and lexico-syntactic pattern fragments, which they call partial patterns. Such approach may prove useful for acquiring NS instances too, as the method can acquire relation instances without considering any pattern connecting the two words of the instance. Yet their work focused only on SS instances. tions of the same pattern (e.g., “Y:products from X:company”, as in “iPhone from Apple”) may not yield a valid paraphrase of “X causes Y”. To obtain word classes they use a large-scale word clustering algorithm (Kazama and Torisawa, 2008), and rank each instance in the corpus according to a score based on the semantic similarity between the seed patterns and each class dependent pattern the instance co-occurs with. Although much work in this category successfully extracts the instances implicitly written in a single sentence based on a wide range of nontrivial evidence including paraphrases (e.g., “Y by X” for causality), the applicability of these methods is restricted to SS instances. In the second category, Schoenmackers et al. (2010), which is the most relevant to our work, takes relation instances provided by TextRunner ("
I11-1101,P06-1102,0,0.0646067,"Missing"
I11-1101,P06-1015,0,0.267203,"sed on semantic relations explicitly expressed in single sentences. Our goal in this work is to obtain valid non-single sentence relation instances, which are not written in any single sentence and may not be even written in a large corpus. We develop a method to infer new semantic relation instances by applying auto-discovered inference rules, and show that our method inferred a considerable number of valid instances that were not written in single sentences even in 600 million Web pages. 1 Most existing relation acquisition methods acquire relation instances using lexico-syntactic patterns (Pantel and Pennacchiotti, 2006; Pas¸ca et al., 2006; De Saeger et al., 2009) such as “X causes Y” or probabilistic sequence labeling models (Banko and Etzioni, 2008). These methods basically rely on the structure of single sentences and they are for acquiring SS instances. Thus we consider that NS instances are practically beyond their reach. A few attempts to overcome this limitation include inference-based methods. These methods take SS instances provided by other methods as input and infer relation instances including NS ones using auto-discovered inference rules (Schoenmackers et al., 2010; Carlson et al., 2010) or dis"
N13-1007,N03-1003,0,0.255464,"原文意思的前提下，完全改变原 文 的 句 子 结 构 。 (Paraphrasing refers to the transformation of sentence structure by the translator without changing the meaning of original text.) b. 意译是指只保持原文内容，不保持原文形式的翻译方法。 (Paraphrasing is a translation method of keeping the content of original text but not keeping the expression.) Figure 1: Multilingual definition pairs on “paraphrasing.” Introduction Automatic paraphrasing has been recognized as an important component for NLP systems, and many methods have been proposed to acquire paraphrase knowledge (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004; Callison-Burch, 2008; Hashimoto et al., 2011; Fujita et al., 2012). We propose a minimally supervised method for multilingual paraphrase extraction. Hashimoto et al. (2011) developed a method to extract paraphrases from definition sentences on the Web, based on their observation that definition sentences defining the same concept tend to contain many paraphrases. Their method consists of two steps; they extract definition sentences from the Web, and extract phrasal paraphrases from the definition sentences. Both steps require supervised classifiers trained by manually ann"
N13-1007,P01-1008,0,0.129702,"ase fragments in global contexts. Our paraphrase extraction method is mostly language-independent and, through experiments for the three languages, we show that it outperforms unsupervised methods (Pas¸ca and Dienes, 2005; Koehn et al., 2007) and is comparable to Hashimoto et al.’s supervised method for Japanese. Previous methods for paraphrase (and entailment) 64 extraction can be classified into a distributional similarity based approach (Lin and Pantel, 2001; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor and Dagan, 2008; Hashimoto et al., 2009) and a parallel corpus based approach (Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004; Callison-Burch, 2008). The former can exploit large scale monolingual corpora, but is known to be unable to distinguish paraphrase pairs from antonymous pairs (Lin et al., 2003). The latter rarely mistakes antonymous pairs for paraphrases, but preparing parallel corpora is expensive. As with Hashimoto et al. (2011), our method is a kind of parallel corpus approach in that it uses definition pairs as a parallel corpus. However, our method does not suffer from a high labor cost of preparing parallel corpora, since it can automa"
N13-1007,D07-1017,0,0.0245304,"otation and instead introduce a method that uses a novel similarity measure considering the occurrence of phrase fragments in global contexts. Our paraphrase extraction method is mostly language-independent and, through experiments for the three languages, we show that it outperforms unsupervised methods (Pas¸ca and Dienes, 2005; Koehn et al., 2007) and is comparable to Hashimoto et al.’s supervised method for Japanese. Previous methods for paraphrase (and entailment) 64 extraction can be classified into a distributional similarity based approach (Lin and Pantel, 2001; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor and Dagan, 2008; Hashimoto et al., 2009) and a parallel corpus based approach (Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004; Callison-Burch, 2008). The former can exploit large scale monolingual corpora, but is known to be unable to distinguish paraphrase pairs from antonymous pairs (Lin et al., 2003). The latter rarely mistakes antonymous pairs for paraphrases, but preparing parallel corpora is expensive. As with Hashimoto et al. (2011), our method is a kind of parallel corpus approach in that it uses definition pairs as a parallel c"
N13-1007,D08-1021,0,0.21472,"efers to the transformation of sentence structure by the translator without changing the meaning of original text.) b. 意译是指只保持原文内容，不保持原文形式的翻译方法。 (Paraphrasing is a translation method of keeping the content of original text but not keeping the expression.) Figure 1: Multilingual definition pairs on “paraphrasing.” Introduction Automatic paraphrasing has been recognized as an important component for NLP systems, and many methods have been proposed to acquire paraphrase knowledge (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004; Callison-Burch, 2008; Hashimoto et al., 2011; Fujita et al., 2012). We propose a minimally supervised method for multilingual paraphrase extraction. Hashimoto et al. (2011) developed a method to extract paraphrases from definition sentences on the Web, based on their observation that definition sentences defining the same concept tend to contain many paraphrases. Their method consists of two steps; they extract definition sentences from the Web, and extract phrasal paraphrases from the definition sentences. Both steps require supervised classifiers trained by manually annotated data, and heavily depend on their t"
N13-1007,D09-1060,1,0.872534,"Missing"
N13-1007,C04-1051,0,0.595873,"构 。 (Paraphrasing refers to the transformation of sentence structure by the translator without changing the meaning of original text.) b. 意译是指只保持原文内容，不保持原文形式的翻译方法。 (Paraphrasing is a translation method of keeping the content of original text but not keeping the expression.) Figure 1: Multilingual definition pairs on “paraphrasing.” Introduction Automatic paraphrasing has been recognized as an important component for NLP systems, and many methods have been proposed to acquire paraphrase knowledge (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004; Callison-Burch, 2008; Hashimoto et al., 2011; Fujita et al., 2012). We propose a minimally supervised method for multilingual paraphrase extraction. Hashimoto et al. (2011) developed a method to extract paraphrases from definition sentences on the Web, based on their observation that definition sentences defining the same concept tend to contain many paraphrases. Their method consists of two steps; they extract definition sentences from the Web, and extract phrasal paraphrases from the definition sentences. Both steps require supervised classifiers trained by manually annotated data, and hea"
N13-1007,P05-1045,0,0.0139239,"Missing"
N13-1007,D12-1058,0,0.338671,"Missing"
N13-1007,P05-1014,0,0.029409,"iminate the need for annotation and instead introduce a method that uses a novel similarity measure considering the occurrence of phrase fragments in global contexts. Our paraphrase extraction method is mostly language-independent and, through experiments for the three languages, we show that it outperforms unsupervised methods (Pas¸ca and Dienes, 2005; Koehn et al., 2007) and is comparable to Hashimoto et al.’s supervised method for Japanese. Previous methods for paraphrase (and entailment) 64 extraction can be classified into a distributional similarity based approach (Lin and Pantel, 2001; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor and Dagan, 2008; Hashimoto et al., 2009) and a parallel corpus based approach (Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004; Callison-Burch, 2008). The former can exploit large scale monolingual corpora, but is known to be unable to distinguish paraphrase pairs from antonymous pairs (Lin et al., 2003). The latter rarely mistakes antonymous pairs for paraphrases, but preparing parallel corpora is expensive. As with Hashimoto et al. (2011), our method is a kind of parallel corpus approach in that it uses definition"
N13-1007,D09-1122,1,0.90277,"Missing"
N13-1007,P11-1109,1,0.835368,"Missing"
N13-1007,D07-1073,1,0.788208,"istinguishing definition from non-definition. The classifier is learnt from the first sentences in 63 Proceedings of NAACL-HLT 2013, pages 63–73, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics Web Wikipedia Ranking by Score Automa&cally constructed training data Web Classiﬁer Deﬁni&on sentences Deﬁni&on Extrac&on (Sec&on 2.1) Deﬁni&on pairs Paraphrase candidates Ranked paraphrase candidates Paraphrase Extrac&on (Sec&on 2.2) Figure 2: Overall picture of our method. Wikipedia articles, which can be regarded as the definition of the title of Wikipedia article (Kazama and Torisawa, 2007) and hence can be used as positive examples. Our method relies on a POS tagger, a dependency parser, a NER tool, noun phrase chunking rules, and frequency thresholds for each language, in addition to Wikipedia articles, which can be seen as a manually annotated knowledge base. However, our method needs no additional manual annotation particularly for this task and thus we categorize our method as a minimally supervised method. On the other hand, Hashimoto et al.’s method heavily depends on the properties of Japanese like the assumption that characteristic expressions of definition sentences te"
N13-1007,P07-2045,0,0.0134423,"to develop a minimally supervised method for multilingual paraphrase extraction from definition sentences. Again, Hashimoto et al.’s method utilizes a supervised classifier trained with annotated data particularly prepared for this task. We eliminate the need for annotation and instead introduce a method that uses a novel similarity measure considering the occurrence of phrase fragments in global contexts. Our paraphrase extraction method is mostly language-independent and, through experiments for the three languages, we show that it outperforms unsupervised methods (Pas¸ca and Dienes, 2005; Koehn et al., 2007) and is comparable to Hashimoto et al.’s supervised method for Japanese. Previous methods for paraphrase (and entailment) 64 extraction can be classified into a distributional similarity based approach (Lin and Pantel, 2001; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor and Dagan, 2008; Hashimoto et al., 2009) and a parallel corpus based approach (Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004; Callison-Burch, 2008). The former can exploit large scale monolingual corpora, but is known to be unable to distinguish paraphrase pairs from anton"
N13-1007,P09-1058,1,0.885519,"Missing"
N13-1007,W06-2932,0,0.10292,"Missing"
N13-1007,P10-1134,0,0.0562029,"ikipedia articles, which can be seen as a manually annotated knowledge base. However, our method needs no additional manual annotation particularly for this task and thus we categorize our method as a minimally supervised method. On the other hand, Hashimoto et al.’s method heavily depends on the properties of Japanese like the assumption that characteristic expressions of definition sentences tend to appear at the end of sentence in Japanese. We show that our method is applicable to English, Japanese, and Chinese, and that its performance is comparable to state-of-the-art supervised methods (Navigli and Velardi, 2010). Since the three languages are very different we believe that our definition extraction method is applicable to any language as long as Wikipedia articles of the language exist. The second contribution of our work is to develop a minimally supervised method for multilingual paraphrase extraction from definition sentences. Again, Hashimoto et al.’s method utilizes a supervised classifier trained with annotated data particularly prepared for this task. We eliminate the need for annotation and instead introduce a method that uses a novel similarity measure considering the occurrence of phrase fr"
N13-1007,navigli-etal-2010-annotated,0,0.102681,"Missing"
N13-1007,I05-1011,0,0.219049,"Missing"
N13-1007,I08-4017,0,0.0123272,"samples were shuffled so that human annotators could not know which sample was from which method. Annotators were the same as those who conducted the evaluation in Section 3.1.3. Cohen’s kappa (Cohen, 1960) was 0.83 for English, 0.88 for Japanese, 5 We filtered out phrase pairs in which one phrase contained a named entity but the other did not contain the named entity from the output of ProposedScore , Proposedlocal , SMT, and P&D, since most of them were not paraphrases. We used Stanford NER (Finkel et al., 2005) for English named entity recognition (NER), KNP for Japanese NER, and BaseNER (Zhao and Kit, 2008) for Chinese NER. Hashisup and Hashiuns did the named entity filtering of the same kind (footnote 3 of Hashimoto et al. (2011)), and thus we did not apply the filter to them any further. 70 Exp2 We compared ProposedScore and P&D. Since P&D restricted its output to phrase pairs in which each phrase consists of two to four words, we restricted the output of ProposedScore to 2-to-4words phrase pairs, too. We randomly sampled 200 from the top 3,000 phrase pairs from each method for evaluation, and the annotators checked entailment relation of both directions between two phrases using Web sentence"
N13-1007,C08-1107,0,\N,Missing
P08-1047,N03-1002,0,0.438359,"than UNK). Various statistics for this extraction are shown in Table 1. The number of distinct hypernyms in the gazetteer was 12,786. Although this Wikipedia gazetteer is much smaller than the English version used by Kazama and Torisawa (2007) that has over 2,000,000 entries, it is the largest gazetteer that can be freely used for Japanese NER. Our experimental results show that this Wikipedia gazetteer can be used to improve the accuracy of Japanese NER. 3 Using Gazetteers as Features of NER Since Japanese has no spaces between words, there are several choices for the token unit used in NER. Asahara and Motsumoto (2003) proposed using characters instead of morphemes as the unit to alleviate the effect of segmentation errors in morphological analysis and we also used their character-based method. The NER task is then treated as a tagging task, which assigns IOB tags to each character in a sentence.10 We use Conditional Random Fields (CRFs) (Lafferty et al., 2001) to perform this tagging. The information of a gazetteer is incorporated 8 They handled “redirections” as well by following redirection links and extracting a hypernym from the article reached. 9 http://mecab.sourceforge.net 10 Precisely, we use IOB2"
P08-1047,J92-4003,0,0.085671,"ions extracted from Wikipedia for the English NER, and reported improved accuracies with such a gazetteer. We focused on the automatically induced clusters of multi-word nouns (MNs) as the source of gazetteers. We call the constructed gazetteers cluster gazetteers. In the context of tagging, there are several studies that utilized word clusters to prevent the data sparseness problem (Kazama et al., 2001; Miller et al., 2004). However, these methods cannot produce the MN clusters required for constructing gazetteers. In addition, the clustering methods used, such as HMMs and Brown’s algorithm (Brown et al., 1992), seem unable to adequately capture the semantics of MNs since they are based only on the information of adjacent words. We utilized richer 1 We used the term, “multi-word”, to emphasize that a gazetteer includes not only one-word expressions but also multi-word expressions. 2 Although several categories can be associated in general, we assume that only one category is associated. 407 Proceedings of ACL-08: HLT, pages 407–415, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics syntactic/semantic structures, i.e., verb-MN dependencies to make clean MN clusters. Roo"
P08-1047,C92-2082,0,0.0144215,"al., 2006). Most studies using gazetteers for NER are based on the assumption that a gazetteer is a mapping from a multi-word noun (MN)1 to named entity categories such as “Tokyo Stock Exchange → {ORGANIZATION}”.2 However, since the correspondence between the labels and the NE categories can be learned by tagging models, a gazetteer will be useful as long as it returns consistent labels even if those returned are not the NE categories. By changing the perspective in such a way, we can explore more broad classes of gazetteers. For example, we can use automatically extracted hyponymy relations (Hearst, 1992; Shinzato and Torisawa, 2004), or automatically induced MN clusters (Rooth et al., 1999; Torisawa, 2001). For instance, Kazama and Torisawa (2007) used the hyponymy relations extracted from Wikipedia for the English NER, and reported improved accuracies with such a gazetteer. We focused on the automatically induced clusters of multi-word nouns (MNs) as the source of gazetteers. We call the constructed gazetteers cluster gazetteers. In the context of tagging, there are several studies that utilized word clusters to prevent the data sparseness problem (Kazama et al., 2001; Miller et al., 2004)."
P08-1047,D07-1073,1,0.617175,"N)1 to named entity categories such as “Tokyo Stock Exchange → {ORGANIZATION}”.2 However, since the correspondence between the labels and the NE categories can be learned by tagging models, a gazetteer will be useful as long as it returns consistent labels even if those returned are not the NE categories. By changing the perspective in such a way, we can explore more broad classes of gazetteers. For example, we can use automatically extracted hyponymy relations (Hearst, 1992; Shinzato and Torisawa, 2004), or automatically induced MN clusters (Rooth et al., 1999; Torisawa, 2001). For instance, Kazama and Torisawa (2007) used the hyponymy relations extracted from Wikipedia for the English NER, and reported improved accuracies with such a gazetteer. We focused on the automatically induced clusters of multi-word nouns (MNs) as the source of gazetteers. We call the constructed gazetteers cluster gazetteers. In the context of tagging, there are several studies that utilized word clusters to prevent the data sparseness problem (Kazama et al., 2001; Miller et al., 2004). However, these methods cannot produce the MN clusters required for constructing gazetteers. In addition, the clustering methods used, such as HMMs"
P08-1047,W02-2016,0,0.0222009,"ano and Hirai, 2004). head-of-bunsetsu features. See (Nakano and Hirai, 2004). Table 2: Atomic features used in baseline model. velopment set (1,446 sentences), and the testing set (1,446 sentences). 4.2 Baseline Model We extracted the atomic features listed in Table 2 at each character for our baseline model. Though there may be slight differences, these features are based on the standard ones proposed and used in previous studies on Japanese NER such as those by Asahara and Motsumoto (2003), Nakano and Hirai (2004), and Yamada (2007). We used MeCab as a morphological analyzer and CaboCha14 (Kudo and Matsumoto, 2002) as the dependency parser to ﬁnd the boundaries of the bunsetsu. We generated the node and the edge features of a CRF model as described in Table 3 using these atomic features. 4.3 Training To train CRF models, we used Taku Kudo’s CRF++ (ver. 0.44) 15 with some modiﬁcations.16 We http://chasen.org/∼taku/software/ CaboCha 15 http://chasen.org/˜taku/software/CRF++ 16 We implemented scaling, which is similar to that for HMMs (Rabiner, 1989), in the forward-backward phase and replaced the optimization module in the original package with the 14 Node features: {””, x−2 , x−1 , x0 , x+1 , x+2 } × y0"
P08-1047,N04-1043,0,0.421618,"lations (Hearst, 1992; Shinzato and Torisawa, 2004), or automatically induced MN clusters (Rooth et al., 1999; Torisawa, 2001). For instance, Kazama and Torisawa (2007) used the hyponymy relations extracted from Wikipedia for the English NER, and reported improved accuracies with such a gazetteer. We focused on the automatically induced clusters of multi-word nouns (MNs) as the source of gazetteers. We call the constructed gazetteers cluster gazetteers. In the context of tagging, there are several studies that utilized word clusters to prevent the data sparseness problem (Kazama et al., 2001; Miller et al., 2004). However, these methods cannot produce the MN clusters required for constructing gazetteers. In addition, the clustering methods used, such as HMMs and Brown’s algorithm (Brown et al., 1992), seem unable to adequately capture the semantics of MNs since they are based only on the information of adjacent words. We utilized richer 1 We used the term, “multi-word”, to emphasize that a gazetteer includes not only one-word expressions but also multi-word expressions. 2 Although several categories can be associated in general, we assume that only one category is associated. 407 Proceedings of ACL-08"
P08-1047,P99-1014,0,0.77808,"a gazetteer is a mapping from a multi-word noun (MN)1 to named entity categories such as “Tokyo Stock Exchange → {ORGANIZATION}”.2 However, since the correspondence between the labels and the NE categories can be learned by tagging models, a gazetteer will be useful as long as it returns consistent labels even if those returned are not the NE categories. By changing the perspective in such a way, we can explore more broad classes of gazetteers. For example, we can use automatically extracted hyponymy relations (Hearst, 1992; Shinzato and Torisawa, 2004), or automatically induced MN clusters (Rooth et al., 1999; Torisawa, 2001). For instance, Kazama and Torisawa (2007) used the hyponymy relations extracted from Wikipedia for the English NER, and reported improved accuracies with such a gazetteer. We focused on the automatically induced clusters of multi-word nouns (MNs) as the source of gazetteers. We call the constructed gazetteers cluster gazetteers. In the context of tagging, there are several studies that utilized word clusters to prevent the data sparseness problem (Kazama et al., 2001; Miller et al., 2004). However, these methods cannot produce the MN clusters required for constructing gazette"
P08-1047,I08-2080,0,0.018718,"1.77-point improvement from the baseline for the testing set. 5 Comparison with Previous Studies Since many previous studies on Japanese NER used 5-fold cross validation for the IREX dataset, we also performed it for some our models that had the best σ 2 found in the previous experiments. The results are listed in Table 7 with references to the results of recent studies. These results not only reconﬁrmed the effects of the gazetteer features shown in the previous experiments, but they also showed that our best model is comparable to the state-of-theart models. The system recently proposed by Sasano and Kurohashi (2008) is currently the best system for the IREX dataset. It uses many structural features that are not used in our model. Incorporating such features might improve our model further. 6 Related Work and Discussion There are several studies that used automatically extracted gazetteers for NER (Shinzato et al., 2006; Talukdar et al., 2006; Nadeau et al., 2006; Kazama and Torisawa, 2007). Most of the methods (Shinzato et al., 2006; Talukdar et al., 2006; Nadeau et al., 2006) are oriented at the NE category. They extracted a gazetteer for each NE category and utilized it in a NE tagger. On the other han"
P08-1047,sekine-isahara-2000-irex,0,0.0616897,"the accuracy of NER is difﬁcult. We enabled such large-scale clustering by parallelizing the clustering algorithm, and we demonstrate the usefulness of the gazetteer constructed. We parallelized the algorithm of (Torisawa, 2001) using the Message Passing Interface (MPI), with the prime goal being to distribute parameters and thus enable clustering with a large vocabulary. Applying the parallelized clustering to a large set of dependencies collected from Web documents enabled us to construct gazetteers with up to 500,000 entries and 3,000 classes. In our experiments, we used the IREX dataset (Sekine and Isahara, 2000) to demonstrate the usefulness of cluster gazetteers. We also compared the cluster gazetteers with the Wikipedia gazetteer constructed by following the method of (Kazama and Torisawa, 2007). The improvement was larger for the cluster gazetteer than for the Wikipedia gazetteer. We also investigated whether these gazetteers improve the accuracies further when they are used in combination. The experimental results indicated that the accuracy improved further in several cases and showed that these gazetteers complement each other. The paper is organized as follows. In Section 2, we explain the con"
P08-1047,N04-1010,1,0.533019,"st studies using gazetteers for NER are based on the assumption that a gazetteer is a mapping from a multi-word noun (MN)1 to named entity categories such as “Tokyo Stock Exchange → {ORGANIZATION}”.2 However, since the correspondence between the labels and the NE categories can be learned by tagging models, a gazetteer will be useful as long as it returns consistent labels even if those returned are not the NE categories. By changing the perspective in such a way, we can explore more broad classes of gazetteers. For example, we can use automatically extracted hyponymy relations (Hearst, 1992; Shinzato and Torisawa, 2004), or automatically induced MN clusters (Rooth et al., 1999; Torisawa, 2001). For instance, Kazama and Torisawa (2007) used the hyponymy relations extracted from Wikipedia for the English NER, and reported improved accuracies with such a gazetteer. We focused on the automatically induced clusters of multi-word nouns (MNs) as the source of gazetteers. We call the constructed gazetteers cluster gazetteers. In the context of tagging, there are several studies that utilized word clusters to prevent the data sparseness problem (Kazama et al., 2001; Miller et al., 2004). However, these methods cannot"
P08-1047,W06-2919,0,0.404639,"ng the accuracy of NER. Moreover, we demonstrate that the combination of the cluster gazetteer and a gazetteer extracted from Wikipedia, which is also useful for NER, can further improve the accuracy in several cases. 1 Introduction Gazetteers, or entity dictionaries, are important for performing named entity recognition (NER) accurately. Since building and maintaining high-quality gazetteers by hand is very expensive, many methods have been proposed for automatic extraction of gazetteers from texts (Riloff and Jones, 1999; Thelen and Riloff, 2002; Etzioni et al., 2005; Shinzato et al., 2006; Talukdar et al., 2006; Nadeau et al., 2006). Most studies using gazetteers for NER are based on the assumption that a gazetteer is a mapping from a multi-word noun (MN)1 to named entity categories such as “Tokyo Stock Exchange → {ORGANIZATION}”.2 However, since the correspondence between the labels and the NE categories can be learned by tagging models, a gazetteer will be useful as long as it returns consistent labels even if those returned are not the NE categories. By changing the perspective in such a way, we can explore more broad classes of gazetteers. For example, we can use automatically extracted hyponymy"
P08-1047,W02-1028,0,0.0102524,"sters as a gazetteer (cluster gazetteer) is a effective way of improving the accuracy of NER. Moreover, we demonstrate that the combination of the cluster gazetteer and a gazetteer extracted from Wikipedia, which is also useful for NER, can further improve the accuracy in several cases. 1 Introduction Gazetteers, or entity dictionaries, are important for performing named entity recognition (NER) accurately. Since building and maintaining high-quality gazetteers by hand is very expensive, many methods have been proposed for automatic extraction of gazetteers from texts (Riloff and Jones, 1999; Thelen and Riloff, 2002; Etzioni et al., 2005; Shinzato et al., 2006; Talukdar et al., 2006; Nadeau et al., 2006). Most studies using gazetteers for NER are based on the assumption that a gazetteer is a mapping from a multi-word noun (MN)1 to named entity categories such as “Tokyo Stock Exchange → {ORGANIZATION}”.2 However, since the correspondence between the labels and the NE categories can be learned by tagging models, a gazetteer will be useful as long as it returns consistent labels even if those returned are not the NE categories. By changing the perspective in such a way, we can explore more broad classes of"
P09-1058,J96-2001,0,0.0305502,"of unknown words (with characterlevel nodes) as well as those of known words (with word-level nodes). We can directly estimate the statistics of known words from an annotated corpus where a sentence is already segmented into words and assigned POS tags. If we select the correct path yt that corresponds to the annotated sentence, it will only consist of word-level nodes that do not allow learning for unknown words. We therefore need to choose character-level nodes as correct nodes instead of word-level nodes for some words. We expect that those words could reflect unknown words in the future. Baayen and Sproat (1996) proposed that the characteristics of infrequent words in a training corpus resemble those of unknown words. Their idea has proven effective for estimating the statistics of unknown words in previous studies (Ratnaparkhi, 1996; Nagata, 1999; Nakagawa, 2004). We adopt Baayen and Sproat’s approach as the baseline policy in our word-character hybrid model. In the baseline policy, we first count the frequencies of words3 in the training corpus. We then collect infrequent words that appear less than or equal to r times.4 If these infrequent words are in the correct path, we use character-level node"
P09-1058,J95-4004,0,0.503119,"Missing"
P09-1058,P99-1036,0,0.028539,"s. If we select the correct path yt that corresponds to the annotated sentence, it will only consist of word-level nodes that do not allow learning for unknown words. We therefore need to choose character-level nodes as correct nodes instead of word-level nodes for some words. We expect that those words could reflect unknown words in the future. Baayen and Sproat (1996) proposed that the characteristics of infrequent words in a training corpus resemble those of unknown words. Their idea has proven effective for estimating the statistics of unknown words in previous studies (Ratnaparkhi, 1996; Nagata, 1999; Nakagawa, 2004). We adopt Baayen and Sproat’s approach as the baseline policy in our word-character hybrid model. In the baseline policy, we first count the frequencies of words3 in the training corpus. We then collect infrequent words that appear less than or equal to r times.4 If these infrequent words are in the correct path, we use character-level nodes to represent them, and hence the characteristics of unknown words can be learned. For example, in Figure 1 we select the character-level nodes of the word “ ” (Chongming) as the correct nodes. As a result, the correct path yt can contain"
P09-1058,W02-1001,0,0.0750953,"lgorithm has two main search steps: forward and backward. For the forward search, we use Viterbi-style decoding to find the best partial path and its score up to each node in the lattice. For the backward search, we use A∗ style decoding to generate the top k-best paths. A complete path is found when the backward search reaches the beginning node of the lattice, and the algorithm terminates when the number of generated paths equals k. In summary, we use k-best MIRA to iteratively update w(i) . The final weight vector w is the average of the weight vectors after each iteration. As reported in (Collins, 2002; McDonald et al., 2005), parameter averaging can effectively avoid overfitting. For inference, we can use Viterbi-style decoding to search for the most likely path y∗ for a given sentence x where: Input: Training set S = {(xt , yt )}Tt=1 Output: Model weight vector w 1: w(0) = 0; v = 0; i = 0 2: for iter = 1 to N do 3: for t = 1 to T do 4: w(i+1) = update w(i) according to (xt , yt ) 5: v = v + w(i+1) 6: i=i+1 7: end for 8: end for 9: w = v/(N × T ) within a few iterations (McDonald, 2006). Algorithm 1 outlines the generic online learning algorithm (McDonald, 2006) used in our framework. 4.2"
P09-1058,P07-2055,1,0.292461,"taka Uchimoto‡ and Jun’ichi Kazama‡ Yiou Wang‡ and Kentaro Torisawa‡ and Hitoshi Isahara†‡ † Graduate School of Engineering, Kobe University 1-1 Rokkodai-cho, Nada-ku, Kobe 657-8501 Japan ‡ National Institute of Information and Communications Technology 3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0289 Japan {canasai,uchimoto,kazama,wangyiou,torisawa,isahara}@nict.go.jp tem’s word dictionary1 . The word boundaries and the POS tags of unknown words, which are very difficult to identify, cause numerous errors. The word-character hybrid model proposed by Nakagawa and Uchimoto (Nakagawa, 2004; Nakagawa and Uchimoto, 2007) shows promising properties for solving this problem. However, it suffers from structural complexity. Nakagawa (2004) described a training method based on a word-based Markov model and a character-based maximum entropy model that can be completed in a reasonable time. However, this training method is limited by the generatively-trained Markov model in which informative features are hard to exploit. In this paper, we overcome such limitations concerning both efficiency and effectiveness. We propose a new framework for training the wordcharacter hybrid model based on the Margin Infused Relaxed A"
P09-1058,C04-1067,0,0.417122,"gkrai†‡ and Kiyotaka Uchimoto‡ and Jun’ichi Kazama‡ Yiou Wang‡ and Kentaro Torisawa‡ and Hitoshi Isahara†‡ † Graduate School of Engineering, Kobe University 1-1 Rokkodai-cho, Nada-ku, Kobe 657-8501 Japan ‡ National Institute of Information and Communications Technology 3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0289 Japan {canasai,uchimoto,kazama,wangyiou,torisawa,isahara}@nict.go.jp tem’s word dictionary1 . The word boundaries and the POS tags of unknown words, which are very difficult to identify, cause numerous errors. The word-character hybrid model proposed by Nakagawa and Uchimoto (Nakagawa, 2004; Nakagawa and Uchimoto, 2007) shows promising properties for solving this problem. However, it suffers from structural complexity. Nakagawa (2004) described a training method based on a word-based Markov model and a character-based maximum entropy model that can be completed in a reasonable time. However, this training method is limited by the generatively-trained Markov model in which informative features are hard to exploit. In this paper, we overcome such limitations concerning both efficiency and effectiveness. We propose a new framework for training the wordcharacter hybrid model based o"
P09-1058,W04-3236,0,0.900606,"our approach on the Penn Chinese Treebank, and show that it achieves superior performance compared to the state-ofthe-art approaches reported in the literature. 1 Introduction In Chinese, word segmentation and part-of-speech (POS) tagging are indispensable steps for higherlevel NLP tasks. Word segmentation and POS tagging results are required as inputs to other NLP tasks, such as phrase chunking, dependency parsing, and machine translation. Word segmentation and POS tagging in a joint process have received much attention in recent research and have shown improvements over a pipelined fashion (Ng and Low, 2004; Nakagawa and Uchimoto, 2007; Zhang and Clark, 2008; Jiang et al., 2008a; Jiang et al., 2008b). In joint word segmentation and the POS tagging process, one serious problem is caused by unknown words, which are defined as words that are not found in a training corpus or in a sys1 A system’s word dictionary usually consists of a word list, and each word in the list has its own POS category. In this paper, we constructed the system’s word dictionary from a training corpus. 513 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 513–521, c Suntec, Singapore, 2"
P09-1058,W96-0213,0,0.439508,"nd assigned POS tags. If we select the correct path yt that corresponds to the annotated sentence, it will only consist of word-level nodes that do not allow learning for unknown words. We therefore need to choose character-level nodes as correct nodes instead of word-level nodes for some words. We expect that those words could reflect unknown words in the future. Baayen and Sproat (1996) proposed that the characteristics of infrequent words in a training corpus resemble those of unknown words. Their idea has proven effective for estimating the statistics of unknown words in previous studies (Ratnaparkhi, 1996; Nagata, 1999; Nakagawa, 2004). We adopt Baayen and Sproat’s approach as the baseline policy in our word-character hybrid model. In the baseline policy, we first count the frequencies of words3 in the training corpus. We then collect infrequent words that appear less than or equal to r times.4 If these infrequent words are in the correct path, we use character-level nodes to represent them, and hence the characteristics of unknown words can be learned. For example, in Figure 1 we select the character-level nodes of the word “ ” (Chongming) as the correct nodes. As a result, the correct path y"
P09-1058,W03-1719,0,0.0612588,"nese Treebank (CTB) (Xia et al., 2000) in experiments. However, versions of CTB and experimental settings vary across different studies. In this paper, we used CTB 5.0 (LDC2005T01) as our main corpus, defined the training, development and test sets according to (Jiang et al., 2008a; Jiang et al., 2008b), and designed our experiments to explore the impact of the training corpus size on our approach. Table 5 provides the statistics of our experimental settings on the small and large training data. The out-of-vocabulary (OOV) is defined as tokens in the test set that are not in the training set (Sproat and Emerson, 2003). Note that the development set was only used for evaluating the trained model to obtain the optimal values of tunable parameters. 5.4 Impact of policies for correct path selection Table 6 shows the results of our word-character hybrid model using the error-driven and baseline policies. The third and fourth columns indicate the numbers of known and artificial unknown words in the training phase. The total number of words is the same, but the different policies yield different balances between the known and artificial unknown words for learning the hybrid model. Optimal balances were selected u"
P09-1058,P08-1102,0,0.671136,"Missing"
P09-1058,W01-0512,1,0.806822,"cond is a discriminative online learning algorithm based on MIRA that enables us to incorporate arbitrary features to our hybrid model. Based on extensive comparisons, we showed that our approach is superior to the existing approaches reported in the literature. In future work, we plan to apply our framework to other Asian languages, including Thai and Japanese. 7 Related work In this section, we discuss related approaches based on several aspects of learning algorithms and search space representation methods. Maximum entropy models are widely used for word segmentation and POS tagging tasks (Uchimoto et al., 2001; Ng and Low, 2004; Nakagawa, 2004; Nakagawa and Uchimoto, 2007) since they only need moderate training times while they provide reasonable performance. Conditional random fields (CRFs) (Lafferty et al., 2001) further improve the performance (Kudo et al., 2004; Shi and Wang, 2007) by performing whole-sequence normalization to avoid label-bias and length-bias problems. However, CRF-based algorithms typically require longer training times, and we observed an infeasible convergence time for our hybrid model. Online learning has recently gained popularity for many NLP tasks since it performs compa"
P09-1058,C08-1049,0,0.775572,"uperior performance compared to the state-ofthe-art approaches reported in the literature. 1 Introduction In Chinese, word segmentation and part-of-speech (POS) tagging are indispensable steps for higherlevel NLP tasks. Word segmentation and POS tagging results are required as inputs to other NLP tasks, such as phrase chunking, dependency parsing, and machine translation. Word segmentation and POS tagging in a joint process have received much attention in recent research and have shown improvements over a pipelined fashion (Ng and Low, 2004; Nakagawa and Uchimoto, 2007; Zhang and Clark, 2008; Jiang et al., 2008a; Jiang et al., 2008b). In joint word segmentation and the POS tagging process, one serious problem is caused by unknown words, which are defined as words that are not found in a training corpus or in a sys1 A system’s word dictionary usually consists of a word list, and each word in the list has its own POS category. In this paper, we constructed the system’s word dictionary from a training corpus. 513 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 513–521, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP Figure 1: Lattice used in word-charac"
P09-1058,W04-3230,0,0.04645,"n future work, we plan to apply our framework to other Asian languages, including Thai and Japanese. 7 Related work In this section, we discuss related approaches based on several aspects of learning algorithms and search space representation methods. Maximum entropy models are widely used for word segmentation and POS tagging tasks (Uchimoto et al., 2001; Ng and Low, 2004; Nakagawa, 2004; Nakagawa and Uchimoto, 2007) since they only need moderate training times while they provide reasonable performance. Conditional random fields (CRFs) (Lafferty et al., 2001) further improve the performance (Kudo et al., 2004; Shi and Wang, 2007) by performing whole-sequence normalization to avoid label-bias and length-bias problems. However, CRF-based algorithms typically require longer training times, and we observed an infeasible convergence time for our hybrid model. Online learning has recently gained popularity for many NLP tasks since it performs comparably or better than batch learning using shorter training times (McDonald, 2006). For example, a perceptron algorithm is used for joint Chinese word segmentation and POS tagging (Zhang and Clark, 2008; Jiang et al., 2008a; Jiang et al., 2008b). Another potent"
P09-1058,xia-etal-2000-developing,0,0.388731,"2005; McDonald, 2006). We describe k-best decoding for our hybrid model and design its loss function and the features appropriate for our task. In our word-character hybrid model, allowing the model to learn the characteristics of both known and unknown words is crucial to achieve optimal performance. Here, we describe our strategies that yield good balance for learning these two characteristics. We propose an errordriven policy that delivers this balance by acquiring examples of unknown words from particular errors in a training corpus. We conducted our experiments on Penn Chinese Treebank (Xia et al., 2000) and compared our approach with the best previous approaches reported in the literature. Experimental results indicate that our approach can achieve state-of-the-art performance. Abstract In this paper, we present a discriminative word-character hybrid model for joint Chinese word segmentation and POS tagging. Our word-character hybrid model offers high performance since it can handle both known and unknown words. We describe our strategies that yield good balance for learning the characteristics of known and unknown words and propose an errordriven policy that delivers such balance by acquiri"
P09-1058,P08-1101,0,0.620356,"show that it achieves superior performance compared to the state-ofthe-art approaches reported in the literature. 1 Introduction In Chinese, word segmentation and part-of-speech (POS) tagging are indispensable steps for higherlevel NLP tasks. Word segmentation and POS tagging results are required as inputs to other NLP tasks, such as phrase chunking, dependency parsing, and machine translation. Word segmentation and POS tagging in a joint process have received much attention in recent research and have shown improvements over a pipelined fashion (Ng and Low, 2004; Nakagawa and Uchimoto, 2007; Zhang and Clark, 2008; Jiang et al., 2008a; Jiang et al., 2008b). In joint word segmentation and the POS tagging process, one serious problem is caused by unknown words, which are defined as words that are not found in a training corpus or in a sys1 A system’s word dictionary usually consists of a word list, and each word in the list has its own POS category. In this paper, we constructed the system’s word dictionary from a training corpus. 513 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 513–521, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP Figure 1: Lattice"
P09-1058,H05-1066,0,0.00424798,"o main search steps: forward and backward. For the forward search, we use Viterbi-style decoding to find the best partial path and its score up to each node in the lattice. For the backward search, we use A∗ style decoding to generate the top k-best paths. A complete path is found when the backward search reaches the beginning node of the lattice, and the algorithm terminates when the number of generated paths equals k. In summary, we use k-best MIRA to iteratively update w(i) . The final weight vector w is the average of the weight vectors after each iteration. As reported in (Collins, 2002; McDonald et al., 2005), parameter averaging can effectively avoid overfitting. For inference, we can use Viterbi-style decoding to search for the most likely path y∗ for a given sentence x where: Input: Training set S = {(xt , yt )}Tt=1 Output: Model weight vector w 1: w(0) = 0; v = 0; i = 0 2: for iter = 1 to N do 3: for t = 1 to T do 4: w(i+1) = update w(i) according to (xt , yt ) 5: v = v + w(i+1) 6: i=i+1 7: end for 8: end for 9: w = v/(N × T ) within a few iterations (McDonald, 2006). Algorithm 1 outlines the generic online learning algorithm (McDonald, 2006) used in our framework. 4.2 k-best MIRA We focus on"
P09-1058,C94-1032,0,0.27228,"hybrid model since it quickly converges 3 We consider a word and its POS tag a single entry. In our experiments, the optimal threshold value r is selected by evaluating the performance of joint word segmentation and POS tagging on the development set. 4 515 Algorithm 1 Generic Online Learning Algorithm above quadratic programming (QP) problem can be solved using Hildreth’s algorithm (Yair Censor, 1997). Replacing Eq. (2) into line 4 of Algorithm 1, we obtain k-best MIRA. The next question is how to efficiently generate bestk (xt ; w(i) ). In this paper, we apply a dynamic programming search (Nagata, 1994) to kbest MIRA. The algorithm has two main search steps: forward and backward. For the forward search, we use Viterbi-style decoding to find the best partial path and its score up to each node in the lattice. For the backward search, we use A∗ style decoding to generate the top k-best paths. A complete path is found when the backward search reaches the beginning node of the lattice, and the algorithm terminates when the number of generated paths equals k. In summary, we use k-best MIRA to iteratively update w(i) . The final weight vector w is the average of the weight vectors after each iterat"
P09-1058,W03-1726,0,\N,Missing
P10-1003,N03-1017,0,0.00598786,"al Linguistics the target side that might be useful for ambiguity resolution. Our method achieves much greater improvement because it uses the richer subtree constraints. Our approach takes the same input as Huang et al. (2009) and exploits the subtree structure on the target side to provide the bilingual constraints. The subtrees are extracted from large-scale autoparsed monolingual data on the target side. The main problem to be addressed is mapping words on the source side to the target subtree because there are many to many mappings and reordering problems that often occur in translation (Koehn et al., 2003). We use an automatic way for generating mapping rules to solve the problems. Based on the mapping rules, we design a set of features for parsing models. The basic idea is as follows: if the words form a subtree on one side, their corresponding words on the another side will also probably form a subtree. Experiments on the translated portion of the Chinese Treebank (Xue et al., 2002; Bies et al., 2007) show that our system outperforms state-ofthe-art monolingual parsers by 2.93 points for Chinese and 1.64 points for English. The results also show that our system provides higher accuracies than"
P10-1003,P09-1058,1,0.81326,"n a bilingual corpus having approximately 0.8M sentence pairs. We removed notoriously bad links in {a, an, the}×{的(DE), 了(LE)} following the work of Huang et al. (2009). For Chinese unannotated data, we used the XIN CMN portion of Chinese Gigaword Version 2.0 (LDC2009T14) (Huang, 2009), which has approximately 311 million words whose segmentation and POS tags are given. To avoid unfair comparison, we excluded the sentences of the CTB data from the Gigaword data. We discarded the annotations because there are differences in annotation policy between CTB and this corpus. We used the MMA system (Kruengkrai et al., 2009) trained on the training data to perform word segmentation and POS tagging and used the Baseline Parser to parse all the sentences in the data. For English unannotated data, we used the BLLIP corpus that contains about 43 million words of WSJ text. The POS tags were assigned by the MXPOST tagger trained on training data. Then we used the Baseline Parser to parse all the sentences in the data. We reported the parser quality by the unlabeled attachment score (UAS), i.e., the percentage of tokens (excluding all punctuation tokens) with correct HEADs. 4.3.2 Features for 2to3 In the 2to3 case, a ne"
P10-1003,N06-1014,0,0.266158,"n the above cases. For example, in Figure 6, trigram-subtree “在(NULL):3-上(at):1-说(say):0” is mapped onto bigram-subtree “said:0-at:1”. Since asking linguists to define the mapping rules is very expensive, we propose a simple method to easily obtain the mapping rules. 4.2.2 Bilingual subtree mapping 4.2.3 Generalized mapping rules To solve the mapping problems, we use a bilingual corpus, which includes sentence pairs, to automatically generate the mapping rules. First, the sentence pairs are parsed by monolingual parsers on both sides. Then we perform word alignment using a word-level aligner (Liang et al., 2006; DeNero and Klein, 2007). Figure 8 shows an example of a processed sentence pair that has tree structures on both sides and word alignment links. ROOT ROOT ԆԜ ༴Ҿ ⽮Պ 䗩㕈 To increase the mapping coverage, we generalize the mapping rules from the extracted subtree pairs by using the following procedure. The rules are divided by “=&gt;” into two parts: source (left) and target (right). The source part is from the source subtree and the target part is from the target subtree. For the source part, we replace nouns and verbs using their POS tags (coarse grained tags). For the target part, we use the wor"
P10-1003,D08-1092,0,0.602536,"nguage sentence is mapped to a subtree in the corresponding target-language sentence by using word alignment and mapping rules that are automatically learned. The target subtree is verified by checking the subtree list that is collected from unlabeled sentences in the target language parsed by a usual monolingual parser. The result is used as additional features for the source side dependency parser. In this paper, our task is to improve the source side parser with the help of the translations on the target side. Many researchers have investigated the use of bilingual constraints for parsing (Burkett and Klein, 2008; Zhao et al., 2009; Huang et al., 2009). For example, Burkett and Klein (2008) show that parsing with joint models on bitexts improves performance on either or both sides. However, their methods require that the training data have tree structures on both sides, which are hard to obtain. Our method only requires dependency annotation on the source side and is much simpler and faster. Huang et al. (2009) proposes a method, bilingual-constrained monolingual parsing, in which a source-language parser is extended to use the re-ordering of words between two sides’ sentences as additional informatio"
P10-1003,E06-1011,0,0.712091,"odes, we call it a trigram-subtree. From the dependency tree of Figure 3, we obtain the subtrees, as shown in Figure 4 and Figure 5. Figure 4 shows the extracted bigram-subtrees and Figure 5 shows the extracted trigram-subtrees. After extraction, we obtain a set of subtrees. We remove the subtrees occurring only once in the data. Following Chen et al. (2009), we also group the subtrees into different sets based on their frequencies. . Figure 3: Example of dependency tree In our systems, the monolingual features include the first- and second- order features presented in (McDonald et al., 2005; McDonald and Pereira, 2006) and the parent-child-grandchild features used in (Carreras, 2007). We call the parser with the monolingual features monolingual parser. 3.2 Parsing with bilingual features In this paper, we parse source sentences with the help of their translations. A set of bilingual features are designed for the parsing model. 3.2.1 Bilingual subtree features We design bilingual subtree features, as described in Section 4, based on the constraints between the source subtrees and the target subtrees that are verified by the subtree list on the target side. The source subtrees are from the possible dependency"
P10-1003,P05-1012,0,0.689073,"d undirected links are word alignment links, and the directed links between words indicate that they have a (candidate) dependency relation. In the English side, it is difficult for a parser to determine the head of word “with” because there is a PP-attachment problem. However, in Chinese it is unambiguous. Therefore, we can use the information on the Chinese side to help disambigua3 Dependency parsing For dependency parsing, there are two main types of parsing models (Nivre and McDonald, 2008; Nivre and Kubler, 2006): transition-based (Nivre, 2003; Yamada and Matsumoto, 2003) and graphbased (McDonald et al., 2005; Carreras, 2007). Our approach can be applied to both parsing models. In this paper, we employ the graph-based MST parsing model proposed by McDonald and Pereira 22 4 Bilingual subtree constraints (2006), which is an extension of the projective parsing algorithm of Eisner (1996). To use richer second-order information, we also implement parent-child-grandchild features (Carreras, 2007) in the MST parsing algorithm. In this section, we propose an approach that uses the bilingual subtree constraints to help parse source sentences that have translations on the target side. We use large-scale aut"
P10-1003,D07-1101,0,0.602173,"word alignment links, and the directed links between words indicate that they have a (candidate) dependency relation. In the English side, it is difficult for a parser to determine the head of word “with” because there is a PP-attachment problem. However, in Chinese it is unambiguous. Therefore, we can use the information on the Chinese side to help disambigua3 Dependency parsing For dependency parsing, there are two main types of parsing models (Nivre and McDonald, 2008; Nivre and Kubler, 2006): transition-based (Nivre, 2003; Yamada and Matsumoto, 2003) and graphbased (McDonald et al., 2005; Carreras, 2007). Our approach can be applied to both parsing models. In this paper, we employ the graph-based MST parsing model proposed by McDonald and Pereira 22 4 Bilingual subtree constraints (2006), which is an extension of the projective parsing algorithm of Eisner (1996). To use richer second-order information, we also implement parent-child-grandchild features (Carreras, 2007) in the MST parsing algorithm. In this section, we propose an approach that uses the bilingual subtree constraints to help parse source sentences that have translations on the target side. We use large-scale auto-parsed data to"
P10-1003,2006.iwslt-evaluation.9,0,0.0149817,"res gold standard trees on the both sides. Compared to the reordering constraint model, which requires the same training data as ours, our method achieved higher accuracy because of richer bilingual constraints. Experiments on the translated portion of the Chinese Treebank show that our system outperforms monolingual parsers by 2.93 points for Chinese and 1.64 points for English. 1 Introduction Parsing bilingual texts (bitexts) is crucial for training machine translation systems that rely on syntactic structures on either the source side or the target side, or the both (Ding and Palmer, 2005; Nakazawa et al., 2006). Bitexts could provide more information, which is useful in parsing, than a usual monolingual texts that can be called “bilingual constraints”, and we expect to obtain more accurate parsing results that can be effectively used in the training of MT systems. With this motivation, there are several studies aiming at highly 21 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 21–29, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics the target side that might be useful for ambiguity resolution. Our method achieves much"
P10-1003,D09-1060,1,0.951248,"sign the bilingual subtree features based on the mapping rules for the parsing model. These features indicate the information of the constraints between bilingual subtrees, that are called bilingual subtree constraints. 3.1 Parsing with monolingual features Figure 3 shows an example of dependency parsing. In the graph-based parsing model, features are represented for all the possible relations on single edges (two words) or adjacent edges (three words). The parsing algorithm chooses the tree with the highest score in a bottom-up fashion. 4.1 Subtree extraction ROOT He ate the meat with a fork Chen et al. (2009) propose a simple method to extract subtrees from large-scale monolingual data and use them as features to improve monolingual parsing. Following their method, we parse large unannotated data with a monolingual parser and obtain a set of subtrees (STt ) in the target language. We encode the subtrees into string format that is expressed as st = w : hid(−w : hid)+1 , where w refers to a word in the subtree and hid refers to the word ID of the word’s head (hid=0 means that this word is the root of a subtree). Here, word ID refers to the ID (starting from 1) of a word in the subtree (words are ord"
P10-1003,P08-1108,0,0.0267451,"have an input sentence pair as shown in Figure 1, where the source sentence is in English, the target is in Chinese, the dashed undirected links are word alignment links, and the directed links between words indicate that they have a (candidate) dependency relation. In the English side, it is difficult for a parser to determine the head of word “with” because there is a PP-attachment problem. However, in Chinese it is unambiguous. Therefore, we can use the information on the Chinese side to help disambigua3 Dependency parsing For dependency parsing, there are two main types of parsing models (Nivre and McDonald, 2008; Nivre and Kubler, 2006): transition-based (Nivre, 2003; Yamada and Matsumoto, 2003) and graphbased (McDonald et al., 2005; Carreras, 2007). Our approach can be applied to both parsing models. In this paper, we employ the graph-based MST parsing model proposed by McDonald and Pereira 22 4 Bilingual subtree constraints (2006), which is an extension of the projective parsing algorithm of Eisner (1996). To use richer second-order information, we also implement parent-child-grandchild features (Carreras, 2007) in the MST parsing algorithm. In this section, we propose an approach that uses the bil"
P10-1003,P07-1003,0,0.141246,"or example, in Figure 6, trigram-subtree “在(NULL):3-上(at):1-说(say):0” is mapped onto bigram-subtree “said:0-at:1”. Since asking linguists to define the mapping rules is very expensive, we propose a simple method to easily obtain the mapping rules. 4.2.2 Bilingual subtree mapping 4.2.3 Generalized mapping rules To solve the mapping problems, we use a bilingual corpus, which includes sentence pairs, to automatically generate the mapping rules. First, the sentence pairs are parsed by monolingual parsers on both sides. Then we perform word alignment using a word-level aligner (Liang et al., 2006; DeNero and Klein, 2007). Figure 8 shows an example of a processed sentence pair that has tree structures on both sides and word alignment links. ROOT ROOT ԆԜ ༴Ҿ ⽮Պ 䗩㕈 To increase the mapping coverage, we generalize the mapping rules from the extracted subtree pairs by using the following procedure. The rules are divided by “=&gt;” into two parts: source (left) and target (right). The source part is from the source subtree and the target part is from the target subtree. For the source part, we replace nouns and verbs using their POS tags (coarse grained tags). For the target part, we use the word alignment information t"
P10-1003,W03-3017,0,0.118478,"ntence is in English, the target is in Chinese, the dashed undirected links are word alignment links, and the directed links between words indicate that they have a (candidate) dependency relation. In the English side, it is difficult for a parser to determine the head of word “with” because there is a PP-attachment problem. However, in Chinese it is unambiguous. Therefore, we can use the information on the Chinese side to help disambigua3 Dependency parsing For dependency parsing, there are two main types of parsing models (Nivre and McDonald, 2008; Nivre and Kubler, 2006): transition-based (Nivre, 2003; Yamada and Matsumoto, 2003) and graphbased (McDonald et al., 2005; Carreras, 2007). Our approach can be applied to both parsing models. In this paper, we employ the graph-based MST parsing model proposed by McDonald and Pereira 22 4 Bilingual subtree constraints (2006), which is an extension of the projective parsing algorithm of Eisner (1996). To use richer second-order information, we also implement parent-child-grandchild features (Carreras, 2007) in the MST parsing algorithm. In this section, we propose an approach that uses the bilingual subtree constraints to help parse source sentence"
P10-1003,P05-1067,0,0.0374513,"sing model, which requires gold standard trees on the both sides. Compared to the reordering constraint model, which requires the same training data as ours, our method achieved higher accuracy because of richer bilingual constraints. Experiments on the translated portion of the Chinese Treebank show that our system outperforms monolingual parsers by 2.93 points for Chinese and 1.64 points for English. 1 Introduction Parsing bilingual texts (bitexts) is crucial for training machine translation systems that rely on syntactic structures on either the source side or the target side, or the both (Ding and Palmer, 2005; Nakazawa et al., 2006). Bitexts could provide more information, which is useful in parsing, than a usual monolingual texts that can be called “bilingual constraints”, and we expect to obtain more accurate parsing results that can be effectively used in the training of MT systems. With this motivation, there are several studies aiming at highly 21 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 21–29, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics the target side that might be useful for ambiguity resolution. O"
P10-1003,W04-3207,0,0.350187,"Missing"
P10-1003,C02-1145,0,0.142006,"ata on the target side. The main problem to be addressed is mapping words on the source side to the target subtree because there are many to many mappings and reordering problems that often occur in translation (Koehn et al., 2003). We use an automatic way for generating mapping rules to solve the problems. Based on the mapping rules, we design a set of features for parsing models. The basic idea is as follows: if the words form a subtree on one side, their corresponding words on the another side will also probably form a subtree. Experiments on the translated portion of the Chinese Treebank (Xue et al., 2002; Bies et al., 2007) show that our system outperforms state-ofthe-art monolingual parsers by 2.93 points for Chinese and 1.64 points for English. The results also show that our system provides higher accuracies than the parser of Huang et al. (2009). The rest of the paper is organized as follows: Section 2 introduces the motivation of our idea. Section 3 introduces the background of dependency parsing. Section 4 proposes an approach of constructing bilingual subtree constraints. Section 5 explains the experimental results. Finally, in Section 6 we draw conclusions and discuss future work. 2 He"
P10-1003,C96-1058,0,0.00990233,"e it is unambiguous. Therefore, we can use the information on the Chinese side to help disambigua3 Dependency parsing For dependency parsing, there are two main types of parsing models (Nivre and McDonald, 2008; Nivre and Kubler, 2006): transition-based (Nivre, 2003; Yamada and Matsumoto, 2003) and graphbased (McDonald et al., 2005; Carreras, 2007). Our approach can be applied to both parsing models. In this paper, we employ the graph-based MST parsing model proposed by McDonald and Pereira 22 4 Bilingual subtree constraints (2006), which is an extension of the projective parsing algorithm of Eisner (1996). To use richer second-order information, we also implement parent-child-grandchild features (Carreras, 2007) in the MST parsing algorithm. In this section, we propose an approach that uses the bilingual subtree constraints to help parse source sentences that have translations on the target side. We use large-scale auto-parsed data to obtain subtrees on the target side. Then we generate the mapping rules to map the source subtrees onto the extracted target subtrees. Finally, we design the bilingual subtree features based on the mapping rules for the parsing model. These features indicate the i"
P10-1003,W03-3023,0,0.2085,"English, the target is in Chinese, the dashed undirected links are word alignment links, and the directed links between words indicate that they have a (candidate) dependency relation. In the English side, it is difficult for a parser to determine the head of word “with” because there is a PP-attachment problem. However, in Chinese it is unambiguous. Therefore, we can use the information on the Chinese side to help disambigua3 Dependency parsing For dependency parsing, there are two main types of parsing models (Nivre and McDonald, 2008; Nivre and Kubler, 2006): transition-based (Nivre, 2003; Yamada and Matsumoto, 2003) and graphbased (McDonald et al., 2005; Carreras, 2007). Our approach can be applied to both parsing models. In this paper, we employ the graph-based MST parsing model proposed by McDonald and Pereira 22 4 Bilingual subtree constraints (2006), which is an extension of the projective parsing algorithm of Eisner (1996). To use richer second-order information, we also implement parent-child-grandchild features (Carreras, 2007) in the MST parsing algorithm. In this section, we propose an approach that uses the bilingual subtree constraints to help parse source sentences that have translations on t"
P10-1003,P09-1007,0,0.117236,"to a subtree in the corresponding target-language sentence by using word alignment and mapping rules that are automatically learned. The target subtree is verified by checking the subtree list that is collected from unlabeled sentences in the target language parsed by a usual monolingual parser. The result is used as additional features for the source side dependency parser. In this paper, our task is to improve the source side parser with the help of the translations on the target side. Many researchers have investigated the use of bilingual constraints for parsing (Burkett and Klein, 2008; Zhao et al., 2009; Huang et al., 2009). For example, Burkett and Klein (2008) show that parsing with joint models on bitexts improves performance on either or both sides. However, their methods require that the training data have tree structures on both sides, which are hard to obtain. Our method only requires dependency annotation on the source side and is much simpler and faster. Huang et al. (2009) proposes a method, bilingual-constrained monolingual parsing, in which a source-language parser is extended to use the re-ordering of words between two sides’ sentences as additional information. The input of the"
P10-1003,D09-1127,0,0.136762,"Missing"
P10-1026,P94-1038,0,0.168634,"roblem and tackled in the context of language modeling and supervised machine learning. However, to our knowledge, there Introduction The semantic similarity of words is a longstanding topic in computational linguistics because it is theoretically intriguing and has many applications in the ﬁeld. Many researchers have conducted studies based on the distributional hypothesis (Harris, 1954), which states that words that occur in the same contexts tend to have similar meanings. A number of semantic similarity measures have been proposed based on this hypothesis (Hindle, 1990; Grefenstette, 1994; Dagan et al., 1994; Dagan et al., 1995; Lin, 1998; Dagan et al., 1999). ∗ (1) The work was done while the author was at NICT. 247 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 247–256, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics The rest of the paper is organized as follows. In Section 2, we brieﬂy introduce the Bayesian estimation and the Bhattacharyya coefﬁcient. Section 3 proposes our new Bayesian Bhattacharyya coefﬁcient for robust similarity calculation. Section 4 mentions some implementation issues and the solutions. T"
P10-1026,I08-1025,0,0.0193862,"and set the output variable to the default value. Then, we iterate over the sparse vectors c(w1 , fk ) and c(w2 , fk ). If We used the GNU (www.gnu.org/software/gsl/), function. = T 1 X δ(wi ∈ ans), T i=1 AP = N 1 X δ(wi ∈ ans)P@i. R i=1 δ(wi ∈ ans) returns 1 if the output word wi is in the answers, and 0 otherwise. N is the number of outputs and R is the number of the answers. MP@T and MAP are the averages of these values over all input words. For each k: (D): exp(2(lnΓ(αk + 12 )). 2 P@T 5.2 Collecting context proﬁles Dependency relations are used as context proﬁles as in Kazama and Torisawa (2008) and Kazama et al. (2009). From a large corpus of Japanese Web documents (Shinzato et al., 2008) (100 million Scientiﬁc Library (GSL) which implements this 250 “B” is for the validation of the parameter tuning. documents), where each sentence has a dependency parse, we extracted noun-verb and nounnoun dependencies with relation types and then calculated their frequencies in the corpus. If a noun, n, depends on a word, w, with a relation, r, we collect a dependency pair, (n, 〈w, r〉). That is, a context fk , is 〈w, r〉 here. For noun-verb dependencies, postpositions in Japanese represent relation"
P10-1026,P97-1008,0,0.0366308,"known that p(φ|D) is also a Dirichlet distribution for this simplest case, and it can be analytically calculated as follows. p(φ|D) = Dir(φ|{αk + c(k)}), (6) where c(k) is the frequency of choice k in data D. For example, c(k) = c(wi , fk ) in the estimation of p(fk |wi ). This is very simple: we just need to add the observed counts to the hyperparameters. 248 2.2 Bhattacharyya coefﬁcient When the context proﬁles are probability distributions, we usually utilize the measures on probability distributions such as the Jensen-Shannon (JS) divergence to calculate similarities (Dagan et al., 1994; Dagan et al., 1997). The JS divergence is deﬁned as follows. JS(p1 ||p2 ) = = 2 where pavg = p1 +p is a point-wise average of p1 2 and p2 and KL(.) is the Kullback-Leibler divergence. Although we found that the JS divergence is a good measure, it is difﬁcult to derive an efﬁcient calculation of Eq. 2, even in the Dirichlet prior case.1 In this study, we employ the Bhattacharyya coefﬁcient (Bhattacharyya, 1943) (BC for short), which is deﬁned as follows: K X √ BCb (w1 , w2 ) = Γ(α0 + a0 )Γ(β0 + b0 ) × Γ(α0 + a0 + 12 )Γ(β0 + b0 + 12 ) (8) K X Γ(αk + c(w1 , fk ) + 21 )Γ(βk + c(w2 , fk ) + 12 ) , Γ(αk + c(w1 , fk ))"
P10-1026,P06-1124,0,0.012983,"3 proposes our new Bayesian Bhattacharyya coefﬁcient for robust similarity calculation. Section 4 mentions some implementation issues and the solutions. Then, Section 5 reports the experimental results. has been no study that seriously dealt with data sparseness in the context of semantic similarity calculation. The data sparseness problem is usually solved by smoothing, regularization, margin maximization and so on (Chen and Goodman, 1998; Chen and Rosenfeld, 2000; Cortes and Vapnik, 1995). Recently, the Bayesian approach has emerged and achieved promising results with a clearer formulation (Teh, 2006; Mochihashi et al., 2009). In this paper, we apply the Bayesian framework to the calculation of distributional similarity. The method is straightforward: Instead of using the point estimation of v(wi ), we ﬁrst estimate the distribution of the context proﬁle, p(v(wi )), by Bayesian estimation and then take the expectation of the original similarity under this distribution as follows: simb (w1 , w2 ) 2 Background 2.1 Bayesian estimation with Dirichlet prior Assume that we estimate a probabilistic model for the observed data D, p(D|φ), which is parameterized with parameters φ. In the maximum li"
P10-1026,P90-1034,0,0.43827,"has been recognized as a serious problem and tackled in the context of language modeling and supervised machine learning. However, to our knowledge, there Introduction The semantic similarity of words is a longstanding topic in computational linguistics because it is theoretically intriguing and has many applications in the ﬁeld. Many researchers have conducted studies based on the distributional hypothesis (Harris, 1954), which states that words that occur in the same contexts tend to have similar meanings. A number of semantic similarity measures have been proposed based on this hypothesis (Hindle, 1990; Grefenstette, 1994; Dagan et al., 1994; Dagan et al., 1995; Lin, 1998; Dagan et al., 1999). ∗ (1) The work was done while the author was at NICT. 247 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 247–256, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics The rest of the paper is organized as follows. In Section 2, we brieﬂy introduce the Bayesian estimation and the Bhattacharyya coefﬁcient. Section 3 proposes our new Bayesian Bhattacharyya coefﬁcient for robust similarity calculation. Section 4 mentions some im"
P10-1026,P08-1047,1,0.897456,"all c(wi , fk ) = 0 and set the output variable to the default value. Then, we iterate over the sparse vectors c(w1 , fk ) and c(w2 , fk ). If We used the GNU (www.gnu.org/software/gsl/), function. = T 1 X δ(wi ∈ ans), T i=1 AP = N 1 X δ(wi ∈ ans)P@i. R i=1 δ(wi ∈ ans) returns 1 if the output word wi is in the answers, and 0 otherwise. N is the number of outputs and R is the number of the answers. MP@T and MAP are the averages of these values over all input words. For each k: (D): exp(2(lnΓ(αk + 12 )). 2 P@T 5.2 Collecting context proﬁles Dependency relations are used as context proﬁles as in Kazama and Torisawa (2008) and Kazama et al. (2009). From a large corpus of Japanese Web documents (Shinzato et al., 2008) (100 million Scientiﬁc Library (GSL) which implements this 250 “B” is for the validation of the parameter tuning. documents), where each sentence has a dependency parse, we extracted noun-verb and nounnoun dependencies with relation types and then calculated their frequencies in the corpus. If a noun, n, depends on a word, w, with a relation, r, we collect a dependency pair, (n, 〈w, r〉). That is, a context fk , is 〈w, r〉 here. For noun-verb dependencies, postpositions in Japanese represent relation"
P10-1026,P98-2127,0,0.309119,"guage modeling and supervised machine learning. However, to our knowledge, there Introduction The semantic similarity of words is a longstanding topic in computational linguistics because it is theoretically intriguing and has many applications in the ﬁeld. Many researchers have conducted studies based on the distributional hypothesis (Harris, 1954), which states that words that occur in the same contexts tend to have similar meanings. A number of semantic similarity measures have been proposed based on this hypothesis (Hindle, 1990; Grefenstette, 1994; Dagan et al., 1994; Dagan et al., 1995; Lin, 1998; Dagan et al., 1999). ∗ (1) The work was done while the author was at NICT. 247 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 247–256, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics The rest of the paper is organized as follows. In Section 2, we brieﬂy introduce the Bayesian estimation and the Bhattacharyya coefﬁcient. Section 3 proposes our new Bayesian Bhattacharyya coefﬁcient for robust similarity calculation. Section 4 mentions some implementation issues and the solutions. Then, Section 5 reports the expe"
P10-1026,D09-1098,0,\N,Missing
P10-1026,P09-1012,0,\N,Missing
P10-1026,C98-2122,0,\N,Missing
P10-1026,P08-1000,0,\N,Missing
P11-1109,P05-1074,0,0.327239,"Missing"
P11-1109,N03-1003,0,0.822953,"he same information in many ways, which makes natural language processing (NLP) a challenging area. Accordingly, many researchers have recognized that automatic paraphrasing is an indispensable component of intelligent NLP systems (Iordanskaja et al., 1991; McKeown et al., 2002; Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Kauchak and Barzilay, 2006; Callison-Burch et al., 2006) and have tried to acquire a large amount of paraphrase knowledge, which is a key to achieving robust automatic paraphrasing, from corpora (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003). We propose a method to extract phrasal paraphrases from pairs of sentences that define the same 1087 concept. The method is based on our observation that two sentences defining the same concept can be regarded as a parallel corpus since they largely convey the same information using different expressions. Such definition sentences abound on the Web. This suggests that we may be able to extract a large amount of phrasal paraphrase knowledge from the definition sentences on the Web. For instance, the following two sentences, both of which define the same concept “osteoporosis”, include two pai"
P11-1109,P01-1008,0,0.454546,"ntroduction Natural language allows us to express the same information in many ways, which makes natural language processing (NLP) a challenging area. Accordingly, many researchers have recognized that automatic paraphrasing is an indispensable component of intelligent NLP systems (Iordanskaja et al., 1991; McKeown et al., 2002; Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Kauchak and Barzilay, 2006; Callison-Burch et al., 2006) and have tried to acquire a large amount of paraphrase knowledge, which is a key to achieving robust automatic paraphrasing, from corpora (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003). We propose a method to extract phrasal paraphrases from pairs of sentences that define the same 1087 concept. The method is based on our observation that two sentences defining the same concept can be regarded as a parallel corpus since they largely convey the same information using different expressions. Such definition sentences abound on the Web. This suggests that we may be able to extract a large amount of phrasal paraphrase knowledge from the definition sentences on the Web. For instance, the following two sentences, both of which define"
P11-1109,D07-1017,0,0.0267206,"cription of related work on the same research issue. Section 2 describes related works. Section 3 presents our proposed method. Section 4 reports on evaluation results. Section 5 concludes the paper. 2 Related Work The existing work for paraphrase extraction is categorized into two groups. The first involves a distributional similarity approach pioneered by Lin and Pantel (2001). Basically, this approach assumes that two expressions that have a large distributional similarity are paraphrases. There are also variants of this approach that address entailment acquisition (Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor and Dagan, 2008; Hashimoto et al., 2009). These methods can be applied to a normal monolingual corpus, and it has been shown that a large number of paraphrases or entailment rules could be extracted. How1088 ever, the precision of these methods has been relatively low. This is due to the fact that the evidence, i.e., distributional similarity, is just indirect evidence of paraphrase/entailment. Accordingly, these methods occasionally mistake antonymous pairs for paraphrases/entailment pairs, since an expression and its antonymous counterpart are also likely to have a large distribut"
P11-1109,N06-1003,0,0.101957,"Missing"
P11-1109,C04-1051,0,0.665037,". Our objective is to extract phrasal paraphrases from pairs of sentences that define the same concept. We propose a supervised method that exploits various kinds of lexical similarity features and contextual features. Sentences defining certain concepts are acquired automatically on a large scale from the Web by applying a quite simple supervised method. Previous methods most relevant to our work used parallel corpora such as multiple translations of the same source text (Barzilay and McKeown, 2001) or automatically acquired parallel news texts (Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004). The former requires a large amount of manual labor to translate the same texts Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1087–1097, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics in several ways. The latter would suffer from the fact that it is not easy to automatically retrieve large bodies of parallel news text with high accuracy. On the contrary, recognizing definition sentences for the same concept is quite an easy task at least for Japanese, as we will show, and we were able to find a huge amount"
P11-1109,P05-1014,0,0.0131216,"same cuisine, or the description of related work on the same research issue. Section 2 describes related works. Section 3 presents our proposed method. Section 4 reports on evaluation results. Section 5 concludes the paper. 2 Related Work The existing work for paraphrase extraction is categorized into two groups. The first involves a distributional similarity approach pioneered by Lin and Pantel (2001). Basically, this approach assumes that two expressions that have a large distributional similarity are paraphrases. There are also variants of this approach that address entailment acquisition (Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor and Dagan, 2008; Hashimoto et al., 2009). These methods can be applied to a normal monolingual corpus, and it has been shown that a large number of paraphrases or entailment rules could be extracted. How1088 ever, the precision of these methods has been relatively low. This is due to the fact that the evidence, i.e., distributional similarity, is just indirect evidence of paraphrase/entailment. Accordingly, these methods occasionally mistake antonymous pairs for paraphrases/entailment pairs, since an expression and its antonymous counterpart are also likely to h"
P11-1109,D09-1122,1,0.885272,"Missing"
P11-1109,N06-1058,0,0.0634,"Missing"
P11-1109,D07-1073,1,0.816716,"We describe them below. 3.1 Definition sentence acquisition We acquire sentences that define a concept (definition sentences) as in Example (2), which defines “骨 粗鬆症” (osteoporosis), from the 6 × 108 Web pages (Akamine et al., 2010) and the Japanese Wikipedia. 92.2, and 91.4, respectively. Using the classifier, we acquired 1,925,052 positive sentences from all of the collected sentences. After adding definition (2) 骨粗鬆症とは、骨がもろくなってしまう病気だ。 sentences from Wikipedia articles, which are typi(Osteoporosis is a disease that makes bones fragile.) cally the first sentence of the body of each article (Kazama and Torisawa, 2007), we obtained a total Fujii and Ishikawa (2002) developed an unsuper- of 2,141,878 definition sentence candidates, which vised method to find definition sentences from the covered 867,321 concepts ranging from weapons to Web using 18 sentential templates and a language rules of baseball. Then, we coupled two definition model constructed from an encyclopedia. On the sentences whose defined concepts were the same other hand, we developed a supervised method to and obtained 29,661,812 definition sentence pairs. achieve a higher precision. Obviously, our method is tailored to Japanese. For We use"
P11-1109,P08-1047,1,0.825091,"e defined similarly. 1090 Figure 1: Illustration of features f8-12. didate phrase of s2 but do appear in the other part of s2 , i.e. they are extra morphemes for s1 ’s candidate phrase. On the other hand, f9 is zero since there is no such extra morpheme in s2 ’s candidate phrase. Also, features f10-12 have positive values since the two candidate phrases share two parent dependency tree fragments, (that increases) and (of fracture). We have also tried the following features, which we do not detail due to space limitation: the similarity of candidate phrases based on semantically similar nouns (Kazama and Torisawa, 2008), entailing/entailed verbs (Hashimoto et al., 2009), and the identity of the pronunciation and base form of the head morpheme; N -grams (N =1,2,3) of child and parent contexts represented by either the inflected form, base form, pronunciation, or POS of morOriginal definition sentence pair (s1 , s2 ) s1 : Osteoporosis is a disease that reduces bone mass and makes bones fragile. s2 : Osteoporosis is a disease that decreases the quantity of bone and increases the risk of bone fracture. Paraphrased definition sentence pair (s01 , s02 ) s01 : Osteoporosis is a disease that decreases the quantity o"
P11-1109,D08-1084,0,0.0310218,"Missing"
P11-1109,J10-3003,0,0.0405564,"unt of parallel corpora, as noted before. We avoid this by using definition sentences, which can be easily acquired on a large scale from the Web, as parallel corpora. Murata et al. (2004) used definition sentences in two manually compiled dictionaries, which are considerably fewer in the number of definition sentences than those on the Web. Thus, the coverage of their method should be quite limited. Furthermore, the precision of their method is much poorer than ours as we report in Section 4. For a more extensive survey on paraphrasing methods, see Androutsopoulos and Malakasiotis (2010) and Madnani and Dorr (2010). 3 Proposed method Our method, targeting the Japanese language, consists of two steps: definition sentence acquisition and paraphrase extraction. We describe them below. 3.1 Definition sentence acquisition We acquire sentences that define a concept (definition sentences) as in Example (2), which defines “骨 粗鬆症” (osteoporosis), from the 6 × 108 Web pages (Akamine et al., 2010) and the Japanese Wikipedia. 92.2, and 91.4, respectively. Using the classifier, we acquired 1,925,052 positive sentences from all of the collected sentences. After adding definition (2) 骨粗鬆症とは、骨がもろくなってしまう病気だ。 sentences f"
P11-1109,P10-1134,0,0.0253121,"ences from the covered 867,321 concepts ranging from weapons to Web using 18 sentential templates and a language rules of baseball. Then, we coupled two definition model constructed from an encyclopedia. On the sentences whose defined concepts were the same other hand, we developed a supervised method to and obtained 29,661,812 definition sentence pairs. achieve a higher precision. Obviously, our method is tailored to Japanese. For We use one sentential template and an SVM clas- a language-independent method of definition acquisifier. Specifically, we first collect definition sen- sition, see Navigli and Velardi (2010) as an example. tence candidates by a template “ˆNP とは.*”, where ˆ is the beginning of sentence and NP is the noun 3.2 Paraphrase extraction phrase expressing the concept to be defined followed Paraphrase extraction proceeds as follows. First, by a particle sequence, “と” (comitative) and “は” each sentence in a pair is parsed by the depen(topic) (and optionally followed by comma), as ex- dency parser KNP2 and dependency tree fragemplified in (2). As a result, we collected 3,027,101 ments that constitute linguistically well-formed consentences. Although the particle sequence tends stituents are"
P11-1109,W04-3219,0,0.024729,"Missing"
P11-1109,P02-1006,0,0.0207055,"uire sentences that define a concept (definition sentences) as in Example (2), which defines “骨 粗鬆症” (osteoporosis), from the 6 × 108 Web pages (Akamine et al., 2010) and the Japanese Wikipedia. 92.2, and 91.4, respectively. Using the classifier, we acquired 1,925,052 positive sentences from all of the collected sentences. After adding definition (2) 骨粗鬆症とは、骨がもろくなってしまう病気だ。 sentences from Wikipedia articles, which are typi(Osteoporosis is a disease that makes bones fragile.) cally the first sentence of the body of each article (Kazama and Torisawa, 2007), we obtained a total Fujii and Ishikawa (2002) developed an unsuper- of 2,141,878 definition sentence candidates, which vised method to find definition sentences from the covered 867,321 concepts ranging from weapons to Web using 18 sentential templates and a language rules of baseball. Then, we coupled two definition model constructed from an encyclopedia. On the sentences whose defined concepts were the same other hand, we developed a supervised method to and obtained 29,661,812 definition sentence pairs. achieve a higher precision. Obviously, our method is tailored to Japanese. For We use one sentential template and an SVM clas- a lang"
P11-1109,I05-5011,0,0.0171765,"Missing"
P11-1109,P07-1058,0,0.0128043,"the Web are a treasure trove of paraphrase knowledge (Section 4.2). II. Our method of paraphrase acquisition from definition sentences is more accurate than wellknown competing methods (Section 4.1). We first verify claim II by comparing our method with that of Barzilay and McKeown (2001) (BM method), Moses7 (Koehn et al., 2007) (SMT method), and that of Murata et al. (2004) (Mrt method). The first two methods are well known for accurately extracting semantically equivalent phrase pairs from parallel corpora.8 Then, we verify claim This scheme is similar to the one proposed by Szpektor et al. (2007). We adopt this scheme since paraphrase judgment might be unstable between annotators unless they are given a particular context based on which they make a judgment. As de6 The remaining 36 pairs were discarded as they contained scribed below, we use definition sentences as congarbled characters of Japanese. texts. We admit that annotators might be biased by 7 http://www.statmt.org/moses/ 8 this in some unexpected way, but we believe that As anonymous reviewers pointed out, they are unsuperthis is a more stable method than that without con- vised methods and thus unable to be adapted to defini"
P11-1109,C08-1107,0,\N,Missing
P11-1109,P07-2045,0,\N,Missing
W02-0301,J96-1002,0,0.00235563,"ta et al., 2002) has been developed, and at this time it is the largest biomedical annotated corpus available to public, containing 670 annotated abstracts of the MEDLINE database. Another reason for low accuracies is that biomedical named entities are essentially hard to recognize using standard feature sets compared with the named entities in newswire articles (Nobata et al., 2000). Thus, we need to employ powerful machine learning techniques which can incorporate various and complex features in a consistent way. Support Vector Machines (SVMs) (Vapnik, 1995) and Maximum Entropy (ME) method (Berger et al., 1996) are powerful learning methods that satisfy such requirements, and are applied successfully to other NLP tasks (Kudo and Matsumoto, 2000; Nakagawa et al., 2001; Ratnaparkhi, 1996). In this paper, we apply Support Vector Machines to biomedical named entity recognition and train them with the GENIA corpus. We formulate the named entity recognition as the classification of each word with context to one of the classes that represent region and named entity’s semantic class. Although there is a previous work that applied SVMs to biomedical named entity task in this formulation (Yamada et al., 2000)"
W02-0301,C00-1030,1,0.436132,"al IE systems. Conceptually, named entity recognition consists of two tasks: identification, which finds the region of a named entity in a text, and classification, which determines the semantic class of that named entity. The following illustrates biomedical named entity recognition. “Thus, CIITAPROTEIN not only activates the expression of class II genes DNA but recruits another B cell-specific coactivator to increase transcriptional activity of class II promoters in DNA B cellsCELLTYPE .” Machine learning approach has been applied to biomedical named entity recognition (Nobata et al., 1999; Collier et al., 2000; Yamada et al., 2000; Shimpuku, 2002). However, no work has achieved sufficient recognition accuracy. One reason is the lack of annotated corpora for training as is often the case of a new domain. Nobata et al. (1999) and Collier et al. (2000) trained their model with only 100 annotated paper abstracts from the MEDLINE database (National Library of Medicine, 1999), and Yamada et al. (2000) used only 77 annotated paper abstracts. In addition, it is difficult to compare the techniques used in each study because they used a closed and different corpus. To overcome such a situation, the GENIA cor"
W02-0301,W00-0730,0,0.559664,"670 annotated abstracts of the MEDLINE database. Another reason for low accuracies is that biomedical named entities are essentially hard to recognize using standard feature sets compared with the named entities in newswire articles (Nobata et al., 2000). Thus, we need to employ powerful machine learning techniques which can incorporate various and complex features in a consistent way. Support Vector Machines (SVMs) (Vapnik, 1995) and Maximum Entropy (ME) method (Berger et al., 1996) are powerful learning methods that satisfy such requirements, and are applied successfully to other NLP tasks (Kudo and Matsumoto, 2000; Nakagawa et al., 2001; Ratnaparkhi, 1996). In this paper, we apply Support Vector Machines to biomedical named entity recognition and train them with the GENIA corpus. We formulate the named entity recognition as the classification of each word with context to one of the classes that represent region and named entity’s semantic class. Although there is a previous work that applied SVMs to biomedical named entity task in this formulation (Yamada et al., 2000), their method to construct a classifier using SVMs, one-vs-rest, fails to train a classifier with entire GENIA corpus, since the cost o"
W02-0301,N01-1025,0,0.28155,"“temp”. 2 Table 1: Basic statistics of the GENIA corpus # of sentences # of words # of named entities # of words in NEs # of words not in NEs Av. length of NEs (σ) 5,109 152,216 23,793 50,229 101,987 2.11 (1.40) 3 Named Entity Recognition Using SVMs 3.1 Named Entity Recognition as Classification We formulate the named entity task as the classification of each word with context to one of the classes that represent region information and named entity’s semantic class. Several representations to encode region information are proposed and examined (Ramshaw and Marcus, 1995; Uchimoto et al., 2000; Kudo and Matsumoto, 2001). In this paper, we employ the simplest BIO representation, which is also used in (Yamada et al., 2000). We modify this representation in Section 5.1 in order to accelerate the SVM training. In the BIO representation, the region information is represented as the class prefixes “B-” and “I-”, and a class “O”. B- means that the current word is at the beginning of a named entity, I- means that the current word is in a named entity (but not at the beginning), and O means the word is not in a named entity. For each named entity class C, class B-C and I-C are produced. Therefore, if we have N named"
W02-0301,W00-0904,1,0.737839,"Yamada et al. (2000) used only 77 annotated paper abstracts. In addition, it is difficult to compare the techniques used in each study because they used a closed and different corpus. To overcome such a situation, the GENIA corpus (Ohta et al., 2002) has been developed, and at this time it is the largest biomedical annotated corpus available to public, containing 670 annotated abstracts of the MEDLINE database. Another reason for low accuracies is that biomedical named entities are essentially hard to recognize using standard feature sets compared with the named entities in newswire articles (Nobata et al., 2000). Thus, we need to employ powerful machine learning techniques which can incorporate various and complex features in a consistent way. Support Vector Machines (SVMs) (Vapnik, 1995) and Maximum Entropy (ME) method (Berger et al., 1996) are powerful learning methods that satisfy such requirements, and are applied successfully to other NLP tasks (Kudo and Matsumoto, 2000; Nakagawa et al., 2001; Ratnaparkhi, 1996). In this paper, we apply Support Vector Machines to biomedical named entity recognition and train them with the GENIA corpus. We formulate the named entity recognition as the classificat"
W02-0301,W95-0107,0,0.0356952,"such expressions are annotated as a dummy class “temp”. 2 Table 1: Basic statistics of the GENIA corpus # of sentences # of words # of named entities # of words in NEs # of words not in NEs Av. length of NEs (σ) 5,109 152,216 23,793 50,229 101,987 2.11 (1.40) 3 Named Entity Recognition Using SVMs 3.1 Named Entity Recognition as Classification We formulate the named entity task as the classification of each word with context to one of the classes that represent region information and named entity’s semantic class. Several representations to encode region information are proposed and examined (Ramshaw and Marcus, 1995; Uchimoto et al., 2000; Kudo and Matsumoto, 2001). In this paper, we employ the simplest BIO representation, which is also used in (Yamada et al., 2000). We modify this representation in Section 5.1 in order to accelerate the SVM training. In the BIO representation, the region information is represented as the class prefixes “B-” and “I-”, and a class “O”. B- means that the current word is at the beginning of a named entity, I- means that the current word is in a named entity (but not at the beginning), and O means the word is not in a named entity. For each named entity class C, class B-C an"
W02-0301,W96-0213,0,0.284109,"Another reason for low accuracies is that biomedical named entities are essentially hard to recognize using standard feature sets compared with the named entities in newswire articles (Nobata et al., 2000). Thus, we need to employ powerful machine learning techniques which can incorporate various and complex features in a consistent way. Support Vector Machines (SVMs) (Vapnik, 1995) and Maximum Entropy (ME) method (Berger et al., 1996) are powerful learning methods that satisfy such requirements, and are applied successfully to other NLP tasks (Kudo and Matsumoto, 2000; Nakagawa et al., 2001; Ratnaparkhi, 1996). In this paper, we apply Support Vector Machines to biomedical named entity recognition and train them with the GENIA corpus. We formulate the named entity recognition as the classification of each word with context to one of the classes that represent region and named entity’s semantic class. Although there is a previous work that applied SVMs to biomedical named entity task in this formulation (Yamada et al., 2000), their method to construct a classifier using SVMs, one-vs-rest, fails to train a classifier with entire GENIA corpus, since the cost of SVM training is super-linear to the size"
W02-0301,P00-1042,0,0.00524534,"tated as a dummy class “temp”. 2 Table 1: Basic statistics of the GENIA corpus # of sentences # of words # of named entities # of words in NEs # of words not in NEs Av. length of NEs (σ) 5,109 152,216 23,793 50,229 101,987 2.11 (1.40) 3 Named Entity Recognition Using SVMs 3.1 Named Entity Recognition as Classification We formulate the named entity task as the classification of each word with context to one of the classes that represent region information and named entity’s semantic class. Several representations to encode region information are proposed and examined (Ramshaw and Marcus, 1995; Uchimoto et al., 2000; Kudo and Matsumoto, 2001). In this paper, we employ the simplest BIO representation, which is also used in (Yamada et al., 2000). We modify this representation in Section 5.1 in order to accelerate the SVM training. In the BIO representation, the region information is represented as the class prefixes “B-” and “I-”, and a class “O”. B- means that the current word is at the beginning of a named entity, I- means that the current word is in a named entity (but not at the beginning), and O means the word is not in a named entity. For each named entity class C, class B-C and I-C are produced. The"
W03-1018,J96-1002,0,0.0110736,"n feature expectations. However, the equality constraint is inappropriate for sparse and therefore unreliable features. This study explores an ME model with box-type inequality constraints, where the equality can be violated to reflect this unreliability. We evaluate the inequality ME model using text categorization datasets. We also propose an extension of the inequality ME model, which results in a natural integration with the Gaussian MAP estimation. Experimental results demonstrate the advantage of the inequality models and the proposed extension. 1 Introduction The maximum entropy model (Berger et al., 1996; Pietra et al., 1997) has attained great popularity in the NLP field due to its power, robustness, and successful performance in various NLP tasks (Ratnaparkhi, 1996; Nigam et al., 1999; Borthwick, 1999). In the ME estimation, an event is decomposed into features, which indicate the strength of certain aspects in the event, and the most uniform model among the models that satisfy: to the model being estimated. A powerful and robust estimation is possible since the features can be as specific or general as required and does not need to be independent of each other, and since the most uniform m"
W03-1018,W02-2018,0,0.0935761,"(x, y)), (7) pλ (y|x) = Z(x) i  where Z(x) = y exp( i λi fi (x, y)). The dual objective function becomes:    L(λ) = x p˜(x) y p˜(y|x) i λi fi (x, y) (8)    − x p˜(x) log y exp( i λi fi (x, y)).  The ME estimation becomes the maximization of L(λ). And it is equivalent to the  maximization of the log-likelihood: LL(λ) = log x,y pλ (y|x)p˜(x,y) . This optimization can be solved using algorithms such as the GIS algorithm (Darroch and Ratcliff, 1972) and the IIS algorithm (Pietra et al., 1997). In addition, gradient-based algorithms can be applied since the objective function is concave. Malouf (2002) compares several algorithms for the ME estimation including GIS, IIS, and the limitedmemory variable metric (LMVM) method, which is a gradient-based method, and shows that the LMVM method requires much less time to converge for real NLP datasets. We also observed that the LMVM method converges very quickly for the text categorization datasets with an improvement in accuracy. Therefore, we use the LMVM method (and its variant for the inequality models) throughout the experiments. Thus, we only show the gradient when mentioning the training. The gradient of the objective function (8) is compute"
W03-1018,W96-0213,0,0.04206,"equality constraints, where the equality can be violated to reflect this unreliability. We evaluate the inequality ME model using text categorization datasets. We also propose an extension of the inequality ME model, which results in a natural integration with the Gaussian MAP estimation. Experimental results demonstrate the advantage of the inequality models and the proposed extension. 1 Introduction The maximum entropy model (Berger et al., 1996; Pietra et al., 1997) has attained great popularity in the NLP field due to its power, robustness, and successful performance in various NLP tasks (Ratnaparkhi, 1996; Nigam et al., 1999; Borthwick, 1999). In the ME estimation, an event is decomposed into features, which indicate the strength of certain aspects in the event, and the most uniform model among the models that satisfy: to the model being estimated. A powerful and robust estimation is possible since the features can be as specific or general as required and does not need to be independent of each other, and since the most uniform model avoids overfitting the training data. In spite of these advantages, the ME model still suffers from a lack of data as long as it imposes the equality constraint"
W06-2908,W05-0620,0,0.0416402,"Missing"
W06-2908,J02-3001,0,0.0131431,"e give a solution that uses an efﬁcient DP updating procedure applicable in argument recognition. We demonstrate that the WMOLT kernel improves the accuracy, and our speed-up method makes the recognition more than 40 times faster than the naive classiﬁcation. 1 Introduction Semantic role labeling (SRL) is a task that recognizes the arguments of a predicate (verb) in a sentence and assigns the correct role to each argument. As this task is recognized as an important step after (or the last step of) syntactic analysis, many studies have been conducted to achieve accurate semantic role labeling (Gildea and Jurafsky, 2002; Moschitti, 2004; Hacioglu et al., 2004; Punyakanok et al., 2004; Pradhan et al., 2005a; Pradhan et al., 2005b; Toutanova et al., 2005). Most of the studies have focused on machine learning because of the availability of standard datasets, such as PropBank (Kingsbury and Palmer, 2002). Naturally, the usefulness of parse trees in this task can be anticipated. For example, the recent CoNLL 2005 shared task (Carreras and M`arquez, 2005) provided parse trees for use and their usefulness was ensured. Most of the methods heuristically extract features from parse trees, and from other sources, and u"
W06-2908,W04-2416,0,0.0213929,"pdating procedure applicable in argument recognition. We demonstrate that the WMOLT kernel improves the accuracy, and our speed-up method makes the recognition more than 40 times faster than the naive classiﬁcation. 1 Introduction Semantic role labeling (SRL) is a task that recognizes the arguments of a predicate (verb) in a sentence and assigns the correct role to each argument. As this task is recognized as an important step after (or the last step of) syntactic analysis, many studies have been conducted to achieve accurate semantic role labeling (Gildea and Jurafsky, 2002; Moschitti, 2004; Hacioglu et al., 2004; Punyakanok et al., 2004; Pradhan et al., 2005a; Pradhan et al., 2005b; Toutanova et al., 2005). Most of the studies have focused on machine learning because of the availability of standard datasets, such as PropBank (Kingsbury and Palmer, 2002). Naturally, the usefulness of parse trees in this task can be anticipated. For example, the recent CoNLL 2005 shared task (Carreras and M`arquez, 2005) provided parse trees for use and their usefulness was ensured. Most of the methods heuristically extract features from parse trees, and from other sources, and use them in machine learning methods base"
W06-2908,H05-1018,1,0.0719837,"s Jun’ichi Kazama and Kentaro Torisawa Japan Advanced Institute of Science and Technology (JAIST) Asahidai 1-1, Nomi, Ishikawa, 923-1292 Japan {kazama, torisawa}@jaist.ac.jp Abstract We present a method for recognizing semantic role arguments using a kernel on weighted marked ordered labeled trees (the WMOLT kernel). We extend the kernels on marked ordered labeled trees (Kazama and Torisawa, 2005) so that the mark can be weighted according to its importance. We improve the accuracy by giving more weights on subtrees that contain the predicate and the argument nodes with this ability. Although Kazama and Torisawa (2005) presented fast training with tree kernels, the slow classiﬁcation during runtime remained to be solved. In this paper, we give a solution that uses an efﬁcient DP updating procedure applicable in argument recognition. We demonstrate that the WMOLT kernel improves the accuracy, and our speed-up method makes the recognition more than 40 times faster than the naive classiﬁcation. 1 Introduction Semantic role labeling (SRL) is a task that recognizes the arguments of a predicate (verb) in a sentence and assigns the correct role to each argument. As this task is recognized as an important step afte"
W06-2908,kingsbury-palmer-2002-treebank,0,0.0533911,"Missing"
W06-2908,W05-0407,0,0.029547,"Missing"
W06-2908,P04-1043,0,0.0414869,"an efﬁcient DP updating procedure applicable in argument recognition. We demonstrate that the WMOLT kernel improves the accuracy, and our speed-up method makes the recognition more than 40 times faster than the naive classiﬁcation. 1 Introduction Semantic role labeling (SRL) is a task that recognizes the arguments of a predicate (verb) in a sentence and assigns the correct role to each argument. As this task is recognized as an important step after (or the last step of) syntactic analysis, many studies have been conducted to achieve accurate semantic role labeling (Gildea and Jurafsky, 2002; Moschitti, 2004; Hacioglu et al., 2004; Punyakanok et al., 2004; Pradhan et al., 2005a; Pradhan et al., 2005b; Toutanova et al., 2005). Most of the studies have focused on machine learning because of the availability of standard datasets, such as PropBank (Kingsbury and Palmer, 2002). Naturally, the usefulness of parse trees in this task can be anticipated. For example, the recent CoNLL 2005 shared task (Carreras and M`arquez, 2005) provided parse trees for use and their usefulness was ensured. Most of the methods heuristically extract features from parse trees, and from other sources, and use them in machin"
W06-2908,P05-1072,0,0.0129971,"tion. We demonstrate that the WMOLT kernel improves the accuracy, and our speed-up method makes the recognition more than 40 times faster than the naive classiﬁcation. 1 Introduction Semantic role labeling (SRL) is a task that recognizes the arguments of a predicate (verb) in a sentence and assigns the correct role to each argument. As this task is recognized as an important step after (or the last step of) syntactic analysis, many studies have been conducted to achieve accurate semantic role labeling (Gildea and Jurafsky, 2002; Moschitti, 2004; Hacioglu et al., 2004; Punyakanok et al., 2004; Pradhan et al., 2005a; Pradhan et al., 2005b; Toutanova et al., 2005). Most of the studies have focused on machine learning because of the availability of standard datasets, such as PropBank (Kingsbury and Palmer, 2002). Naturally, the usefulness of parse trees in this task can be anticipated. For example, the recent CoNLL 2005 shared task (Carreras and M`arquez, 2005) provided parse trees for use and their usefulness was ensured. Most of the methods heuristically extract features from parse trees, and from other sources, and use them in machine learning methods based on feature vector representation. As a result"
W06-2908,C04-1197,0,0.0707445,"of the methods heuristically extract features from parse trees, and from other sources, and use them in machine learning methods based on feature vector representation. As a result, these methods depend on feature engineering, which is time-consuming. Tree kernels (Collins and Duffy, 2001; Kashima and Koyanagi, 2002) have been proposed to directly handle trees in kernel-based methods, such as SVMs (Vapnik, 1995). Tree kernels calculate the similarity between trees, taking into consideration all of the subtrees, and, therefore there is no need for such feature engineering. Moschitti and Bejan (2004) extensively studied tree kernels for semantic role labeling. However, they reported that they could not successfully build an accurate argument recognizer, although the role assignment was improved. Although Moschitti et al. (2005) reported on argument recognition using tree kernels, it was a preliminary evaluation because they used oracle parse trees. Kazama and Torisawa (2005) proposed a new tree kernel for node relation labeling, as which SRL can be cast. This kernel is deﬁned on marked ordered labeled trees, where a node can have a mark to indicate the existence of a relation. We refer to"
W06-2908,W03-1012,0,0.0610052,"Missing"
W06-2908,P05-1073,0,0.0642226,"d Kentaro Torisawa Japan Advanced Institute of Science and Technology (JAIST) Asahidai 1-1, Nomi, Ishikawa, 923-1292 Japan {kazama, torisawa}@jaist.ac.jp Abstract We present a method for recognizing semantic role arguments using a kernel on weighted marked ordered labeled trees (the WMOLT kernel). We extend the kernels on marked ordered labeled trees (Kazama and Torisawa, 2005) so that the mark can be weighted according to its importance. We improve the accuracy by giving more weights on subtrees that contain the predicate and the argument nodes with this ability. Although Kazama and Torisawa (2005) presented fast training with tree kernels, the slow classiﬁcation during runtime remained to be solved. In this paper, we give a solution that uses an efﬁcient DP updating procedure applicable in argument recognition. We demonstrate that the WMOLT kernel improves the accuracy, and our speed-up method makes the recognition more than 40 times faster than the naive classiﬁcation. 1 Introduction Semantic role labeling (SRL) is a task that recognizes the arguments of a predicate (verb) in a sentence and assigns the correct role to each argument. As this task is recognized as an important step afte"
W09-1209,burchardt-etal-2006-salsa,0,0.0198371,"Missing"
W09-1209,I08-1012,1,0.714872,"ods. As for the semantic parser, a group of well selected feature templates are used with n-best syntactic features. 1 Our thanks give to the following corpus providers, (Taul´e et al., 2008; Palmer and Xue, 2009; Hajiˇc et al., 2006; Surdeanu et al., 2008; Burchardt et al., 2006) and (Kawahara et al., 2002). 61 Basically, we build our syntactic dependency parsers based on the MSTParser, a freely available implementation2 , whose details are presented in the paper of McDonald and Pereira (2006). Moreover, we exploit rich features for the parsers. We represent features by following the work of Chen et al. (2008) and Koo et al. (2008) and use features based on dependency relations predicted by transition-based parsers (Nivre and McDonald, 2008). Chen et al. (2008) and Koo et al. (2008) proposed the methods to obtain new features from large-scale unlabeled data. In our system, we perform their methods on training data because the closed challenge does not allow to use unlabeled data. In this paper, we call these new additional features rich features. 2.1 Basic Features Firstly, we use all the features presented by McDonald et al. (2006), if they are available in data. Then we add new features for the l"
W09-1209,D07-1097,0,0.04916,"Missing"
W09-1209,kawahara-etal-2002-construction,0,0.011934,"hnology (NICT) and City University of Hong Kong (CityU) for the joint learning task of CoNLL-2009 shared task (Hajiˇc et al., 2009)1 . The system is basically a pipeline of syntactic parser and semantic parser. We use a syntactic parser that uses very rich features and integrates graph- and transition-based methods. As for the semantic parser, a group of well selected feature templates are used with n-best syntactic features. 1 Our thanks give to the following corpus providers, (Taul´e et al., 2008; Palmer and Xue, 2009; Hajiˇc et al., 2006; Surdeanu et al., 2008; Burchardt et al., 2006) and (Kawahara et al., 2002). 61 Basically, we build our syntactic dependency parsers based on the MSTParser, a freely available implementation2 , whose details are presented in the paper of McDonald and Pereira (2006). Moreover, we exploit rich features for the parsers. We represent features by following the work of Chen et al. (2008) and Koo et al. (2008) and use features based on dependency relations predicted by transition-based parsers (Nivre and McDonald, 2008). Chen et al. (2008) and Koo et al. (2008) proposed the methods to obtain new features from large-scale unlabeled data. In our system, we perform their metho"
W09-1209,P08-1068,0,0.10612,"c parser, a group of well selected feature templates are used with n-best syntactic features. 1 Our thanks give to the following corpus providers, (Taul´e et al., 2008; Palmer and Xue, 2009; Hajiˇc et al., 2006; Surdeanu et al., 2008; Burchardt et al., 2006) and (Kawahara et al., 2002). 61 Basically, we build our syntactic dependency parsers based on the MSTParser, a freely available implementation2 , whose details are presented in the paper of McDonald and Pereira (2006). Moreover, we exploit rich features for the parsers. We represent features by following the work of Chen et al. (2008) and Koo et al. (2008) and use features based on dependency relations predicted by transition-based parsers (Nivre and McDonald, 2008). Chen et al. (2008) and Koo et al. (2008) proposed the methods to obtain new features from large-scale unlabeled data. In our system, we perform their methods on training data because the closed challenge does not allow to use unlabeled data. In this paper, we call these new additional features rich features. 2.1 Basic Features Firstly, we use all the features presented by McDonald et al. (2006), if they are available in data. Then we add new features for the languages having FEAT i"
W09-1209,E06-1011,0,0.013162,"rser and semantic parser. We use a syntactic parser that uses very rich features and integrates graph- and transition-based methods. As for the semantic parser, a group of well selected feature templates are used with n-best syntactic features. 1 Our thanks give to the following corpus providers, (Taul´e et al., 2008; Palmer and Xue, 2009; Hajiˇc et al., 2006; Surdeanu et al., 2008; Burchardt et al., 2006) and (Kawahara et al., 2002). 61 Basically, we build our syntactic dependency parsers based on the MSTParser, a freely available implementation2 , whose details are presented in the paper of McDonald and Pereira (2006). Moreover, we exploit rich features for the parsers. We represent features by following the work of Chen et al. (2008) and Koo et al. (2008) and use features based on dependency relations predicted by transition-based parsers (Nivre and McDonald, 2008). Chen et al. (2008) and Koo et al. (2008) proposed the methods to obtain new features from large-scale unlabeled data. In our system, we perform their methods on training data because the closed challenge does not allow to use unlabeled data. In this paper, we call these new additional features rich features. 2.1 Basic Features Firstly, we use"
W09-1209,W06-2932,0,0.0235357,"atures for the parsers. We represent features by following the work of Chen et al. (2008) and Koo et al. (2008) and use features based on dependency relations predicted by transition-based parsers (Nivre and McDonald, 2008). Chen et al. (2008) and Koo et al. (2008) proposed the methods to obtain new features from large-scale unlabeled data. In our system, we perform their methods on training data because the closed challenge does not allow to use unlabeled data. In this paper, we call these new additional features rich features. 2.1 Basic Features Firstly, we use all the features presented by McDonald et al. (2006), if they are available in data. Then we add new features for the languages having FEAT information (Hajiˇc et al., 2009). FEAT is a set of morphological-features, e.g. more detailed part of speech, number, gender, etc. We try to align different types of morphological-features. For example, 2 http://mstparser.sourceforge.net Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 61–66, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics we can obtain a sequence of gender tags of all words from a head h to its d"
W09-1209,P08-1108,0,0.0780057,"ks give to the following corpus providers, (Taul´e et al., 2008; Palmer and Xue, 2009; Hajiˇc et al., 2006; Surdeanu et al., 2008; Burchardt et al., 2006) and (Kawahara et al., 2002). 61 Basically, we build our syntactic dependency parsers based on the MSTParser, a freely available implementation2 , whose details are presented in the paper of McDonald and Pereira (2006). Moreover, we exploit rich features for the parsers. We represent features by following the work of Chen et al. (2008) and Koo et al. (2008) and use features based on dependency relations predicted by transition-based parsers (Nivre and McDonald, 2008). Chen et al. (2008) and Koo et al. (2008) proposed the methods to obtain new features from large-scale unlabeled data. In our system, we perform their methods on training data because the closed challenge does not allow to use unlabeled data. In this paper, we call these new additional features rich features. 2.1 Basic Features Firstly, we use all the features presented by McDonald et al. (2006), if they are available in data. Then we add new features for the languages having FEAT information (Hajiˇc et al., 2009). FEAT is a set of morphological-features, e.g. more detailed part of speech, nu"
W09-1209,W03-3017,0,0.0110636,"graph-based and transition-based parsers. Here, we represent features based on dependency relations predicted by transition-based parsers for graphbased parser. Based on the results on development data, we choose the MaltParser for Catalan, Czech, German, and Spanish, and choose another MaxEntbased parser for Chinese, English, and Japanese. 2.4.1 A Transition-based Parser: MaltParser For Catalan, Czech, German, and Spanish, we use the MaltParser, a freely available implementa4 http://www.cs.berkeley.edu/˜pliang/software/browncluster-1.2.zip tion5 , whose details are presented in the paper of Nivre (2003). More information about the parser can be available in the paper (Nivre, 2003). Due to computational cost, we do not select new feature templates for the MaltParser. Following the features settings of Hall et al. (2007), we use their Czech feature file and Catalan feature file. To simply, we apply Czech feature file for German too, and apply Catalan feature file for Spanish. 2.4.2 Another Transition-based Parser: MaxEnt-based Parser In three highly projective language, Chinese, English and Japanese, we use the maximum entropy syntactic dependency parser as in Zhao and Kit (2008). We still use"
W09-1209,W08-2121,0,0.0249233,"Missing"
W09-1209,taule-etal-2008-ancora,0,0.0239121,"Missing"
W09-1209,W08-2127,1,0.761054,"nted in the paper of Nivre (2003). More information about the parser can be available in the paper (Nivre, 2003). Due to computational cost, we do not select new feature templates for the MaltParser. Following the features settings of Hall et al. (2007), we use their Czech feature file and Catalan feature file. To simply, we apply Czech feature file for German too, and apply Catalan feature file for Spanish. 2.4.2 Another Transition-based Parser: MaxEnt-based Parser In three highly projective language, Chinese, English and Japanese, we use the maximum entropy syntactic dependency parser as in Zhao and Kit (2008). We still use the similar feature notations of that work. We use the same greedy feature selection of Zhao et al. (2009) to determine an optimal feature template set for each language. Full feature sets for the three languages can be found at website, http://bcmi.sjtu.edu.cn/˜zhaohai. 2.4.3 Feature Representation For training data, we use 2-way jackknifing to generate predicted dependency parsing trees by two transition-based parsers. Following the features of Nivre and McDonald (2008), we define features for a head h and its dependent d with label l as shown in table 2, where GT ran refers t"
W09-1209,W09-1208,1,0.895734,"omputational cost, we do not select new feature templates for the MaltParser. Following the features settings of Hall et al. (2007), we use their Czech feature file and Catalan feature file. To simply, we apply Czech feature file for German too, and apply Catalan feature file for Spanish. 2.4.2 Another Transition-based Parser: MaxEnt-based Parser In three highly projective language, Chinese, English and Japanese, we use the maximum entropy syntactic dependency parser as in Zhao and Kit (2008). We still use the similar feature notations of that work. We use the same greedy feature selection of Zhao et al. (2009) to determine an optimal feature template set for each language. Full feature sets for the three languages can be found at website, http://bcmi.sjtu.edu.cn/˜zhaohai. 2.4.3 Feature Representation For training data, we use 2-way jackknifing to generate predicted dependency parsing trees by two transition-based parsers. Following the features of Nivre and McDonald (2008), we define features for a head h and its dependent d with label l as shown in table 2, where GT ran refers to dependency parsing trees generated by the MaltParser or MaxEnt-base Parser and ∗ refers to any label. All features are"
W10-3907,P90-1034,0,0.480261,"he type counts of dependency relations, which is roughly equated with the “frequencies” of the terms. Each base term in B is associated with up to 500 of the most distributionally similar terms. This defines T . For M, we used the Jensen-Shannon divergence (JS-divergence) base on the probability distributions derived by an EM-based soft clustering (Kazama and Torisawa, 2008). For convenience, some relevant details of the data construction are described in Appendix A, but in a nutshell, we used dependency relations as distributional information. This makes our method comparable to that used in Hindle (1990). The statistics of the distributional data used were as follows: roughly 920 million types of dependency relations1) were automatically acquired 1) The 920 million types come in two kinds of context triples: 590 million types of (t, p, v) and 320 million types 41 from a large-scale Japanese Web-corpus called the Tsubaki corpus (Shinzato et al., 2008) which consists of roughly 100 million Japanese pages with six billion sentences. After excluding hapax nouns, we had about 33 million types of nouns (in terms of string) and 27 million types of verbs. These nouns were ranked by type count of the"
W10-3907,P08-1047,1,0.851243,"list of n terms T = [ti,1 , ti,2 ,. . . , ti, j , . . . , ti,n ] where ti, j denotes the jth most similarity term in T against bi ∈ B. P(k) are pairs of bi and ti,k , i.e., the k-th most similar term to bi . c. Human raters classify a portion Q of the pairs in P(k) with reference to a classification guideline prepared for the task. Note that the selection of base set B can be independent of the selection of T . Note also that T is indexed by terms in B. To encode this, we write: T [bi ] = [ti,1 , ti,2 ,. . . , ti, j , . . . , ti,n ]. 2.2 Data For T , we used Kazama’s nominal term clustering (Kazama and Torisawa, 2008; Kazama et al., 2009). In this data, base set B for T is one million terms defined by the type counts of dependency relations, which is roughly equated with the “frequencies” of the terms. Each base term in B is associated with up to 500 of the most distributionally similar terms. This defines T . For M, we used the Jensen-Shannon divergence (JS-divergence) base on the probability distributions derived by an EM-based soft clustering (Kazama and Torisawa, 2008). For convenience, some relevant details of the data construction are described in Appendix A, but in a nutshell, we used dependency re"
W10-3907,P98-2127,0,0.390989,"Missing"
W10-3907,P99-1014,0,0.0288406,"Missing"
W10-3907,I08-1025,0,0.0162148,"ering (Kazama and Torisawa, 2008). For convenience, some relevant details of the data construction are described in Appendix A, but in a nutshell, we used dependency relations as distributional information. This makes our method comparable to that used in Hindle (1990). The statistics of the distributional data used were as follows: roughly 920 million types of dependency relations1) were automatically acquired 1) The 920 million types come in two kinds of context triples: 590 million types of (t, p, v) and 320 million types 41 from a large-scale Japanese Web-corpus called the Tsubaki corpus (Shinzato et al., 2008) which consists of roughly 100 million Japanese pages with six billion sentences. After excluding hapax nouns, we had about 33 million types of nouns (in terms of string) and 27 million types of verbs. These nouns were ranked by type count of the two context triples, i.e., (t, p, v) and (n∗ , p∗ ,t). B was determined by selecting the top one million terms with the most variations of context triples. 2.2.1 Sample of T [b] For illustration, we present examples of the Web-derived distributional similar terms. (2) shows the 10 most distributionally similar terms (i.e., [t1070,1 , t1070,2 , . . . ,"
W10-3907,C98-2122,0,\N,Missing
W11-0328,P04-1015,0,0.0634212,"scores on test data). clearly demonstrate that the lookahead search boosts parsing accuracy. As expected, training and test speed decreases, almost by a factor of three, which is the branching factor of the dependency parser. The table also lists accuracy figures reported in the literature on shift-reduce dependency parsing. Most of the latest studies on shift-reduce dependency parsing employ dynamic programing or beam search, which implies that deterministic methods were not as competitive as those methods. It should also be noted that all of the listed studies learn structured perceptrons (Collins and Roark, 2004), while our parser learns locally optimized perceptrons. In this table, our parser without lookahead search (i.e. depth = 0) resulted in significantly lower accuracy than the previous studies. In fact, it is worse than the deterministic parser of Huang et al. (2009), which uses (almost) the same set of features. This is presumably due to the difference between locally optimized perceptrons and globally optimized structured perceptrons. However, our parser with lookahead search is significantly better than their deterministic parser, and its accuracy is close to the levels of the parsers with b"
W11-0328,W02-1001,0,0.356736,"3 in Figure 3). The only difference between our algorithm and the standard algorithm for margin perceptrons is that we use the states and their scores obtained from lookahead searches (Line 11 in Figure 3), which are backed up from the leaves of the search trees. In Appendix A, we provide a proof of the convergence of our training algorithm and show that the margin will approach at least half the true margin (assuming that the training data are linearly separable). As in many studies using perceptrons, we average the weight vector over the whole training iterations at the end of the training (Collins, 2002). 4 Experiments This section presents four sets of experimental results to show how the lookahead process improves the accuracy of history-based models in common NLP tasks. 4.1 Sequence prediction tasks First, we evaluate our framework with three sequence prediction tasks: POS tagging, chunking, and named entity recognition. We compare our method with the CRF model, which is one of the de facto standard machine learning models for such sequence prediction tasks. We trained L1-regularized first-order CRF models using the efficient stochastic gradient descent (SGD)-based training method presente"
W11-0328,N10-1115,0,0.0164406,"search. In that case, the search queue is not necessarily truncated. 244 (Tesauro, 2001; Hoki, 2006) in that the parameters are optimized based on the differences of the feature vectors realized by the correct and incorrect actions. In history-based models, the order of actions is often very important. For example, backward tagging is considerably more accurate than forward tagging in biomedical named entity recognition. Our lookahead method is orthogonal to more elaborate techniques for determining the order of actions such as easy-first tagging/parsing strategies (Tsuruoka and Tsujii, 2005; Elhadad, 2010). We expect that incorporating such elaborate techniques in our framework will lead to improved accuracy, but we leave it for future work. 6 Conclusion We have presented a simple and general framework for incorporating a lookahead process in historybased models and a perceptron-based training algorithm for the framework. We have conducted experiments using standard data sets for POS tagging, chunking, named entity recognition and dependency parsing, and obtained very promising results—the accuracy achieved by the history-based models enhanced with lookahead was as competitive as globally optim"
W11-0328,P10-1110,0,0.0167726,"subsection to assign the POS tags for the development and test data. Unlabeled attachment scores for all words excluding punctuations are reported. The development set is used for tuning the meta parameters, while the test set is used for evaluating the final accuracy. The parsing algorithm is the “arc-standard” method (Nivre, 2004), which is briefly described in Section 2. With this algorithm, state S corresponds to a parser configuration, i.e., the stack and the queue, and action a corresponds to shift, reduceL , and reduceR . In this experiment, we use the same set of feature templates as Huang and Sagae (2010). Table 4 shows training time, test time, and parsing accuracy. In this table, “No lookahead (depth = 0)” corresponds to a conventional shift-reduce parsing method without any lookahead search. The results 3 Penn2Malt is applied for this conversion, while dependency labels are removed. CRF (L1 regularization & SGD training) No lookahead (depth = 0) Lookahead (depth = 1) Lookahead (depth = 2) No lookahead (depth = 0) + tag trigram features Lookahead (depth = 1) + tag trigram features Lookahead (depth = 2) + tag trigram features Structured perceptron (Collins, 2002) Guided learning (Shen et al.,"
W11-0328,D09-1127,0,0.0172615,"Missing"
W11-0328,W04-1213,0,0.0354731,"Missing"
W11-0328,N01-1025,0,0.0492284,"l., 2007; Lavergne et al., 2010). The second set of experiments is about chunking. We used the data set for the CoNLL 2000 shared task, which contains 8,936 sentences where each token is annotated with the “IOB” tags representing text chunks. The experimental results are shown in Table 2. Again, our history-based models with lookahead were slightly more accurate than the CRF model using exactly the same set of features. The accuracy achieved by the lookahead model with a search depth of 2 was comparable to the accuracy achieved by a computationally heavy combination of max-margin classifiers (Kudo and Matsumoto, 2001). We also tested the effectiveness of additional features of tag trigrams using the development data, but there was no improvement in the accuracy. The third set of experiments is about named entity recognition. We used the data provided for the BioNLP/NLPBA 2004 shared task (Kim et al., 242 2004), which contains 18,546 sentences where each token is annotated with the “IOB” tags representing biomedical named entities. We performed the tagging in the right-to-left fashion because it is known that backward tagging is more accurate than forward tagging on this data set (Yoshida and Tsujii, 2007)."
W11-0328,P10-1052,0,0.0421723,"Missing"
W11-0328,W04-2407,0,0.00845323,"ed on partof-speech tagging, chunking, named entity recognition and dependency parsing, using standard data sets and features. Experimental results demonstrate that history-based models with lookahead are as competitive as globally optimized models including conditional random fields (CRFs) and structured perceptrons. 1 Introduction History-based models have been a popular approach in a variety of natural language processing (NLP) tasks including part-of-speech (POS) tagging, named entity recognition, and syntactic parsing (Ratnaparkhi, 1996; McCallum et al., 2000; Yamada and Matsumoto, 2003; Nivre et al., 2004). The idea is to decompose the complex structured prediction problem into a series of simple classification problems and use a machine learning-based classifier to make each decision using the information about the past decisions and partially completed structures as features. In this paper, we argue that history-based models are not something that should be left behind in research history, by demonstrating that their accuracy can be significantly improved by incorporating a lookahead mechanism into their decisionmaking process. It should be emphasized that we use the word “lookahead” differen"
W11-0328,W04-0308,0,0.0142111,"ead rules of Yamada and Matsumoto (2003).3 The data is split into training (section 02-21), development (section 22), and test (section 23) sets. The parsing accuracy was evaluated with auto-POS data, i.e., we used our lookahead POS tagger (depth = 2) presented in the previous subsection to assign the POS tags for the development and test data. Unlabeled attachment scores for all words excluding punctuations are reported. The development set is used for tuning the meta parameters, while the test set is used for evaluating the final accuracy. The parsing algorithm is the “arc-standard” method (Nivre, 2004), which is briefly described in Section 2. With this algorithm, state S corresponds to a parser configuration, i.e., the stack and the queue, and action a corresponds to shift, reduceL , and reduceR . In this experiment, we use the same set of feature templates as Huang and Sagae (2010). Table 4 shows training time, test time, and parsing accuracy. In this table, “No lookahead (depth = 0)” corresponds to a conventional shift-reduce parsing method without any lookahead search. The results 3 Penn2Malt is applied for this conversion, while dependency labels are removed. CRF (L1 regularization & S"
W11-0328,P06-1059,1,0.636231,"Missing"
W11-0328,W96-0213,0,0.382157,"and show its convergence properties. The proposed framework is evaluated on partof-speech tagging, chunking, named entity recognition and dependency parsing, using standard data sets and features. Experimental results demonstrate that history-based models with lookahead are as competitive as globally optimized models including conditional random fields (CRFs) and structured perceptrons. 1 Introduction History-based models have been a popular approach in a variety of natural language processing (NLP) tasks including part-of-speech (POS) tagging, named entity recognition, and syntactic parsing (Ratnaparkhi, 1996; McCallum et al., 2000; Yamada and Matsumoto, 2003; Nivre et al., 2004). The idea is to decompose the complex structured prediction problem into a series of simple classification problems and use a machine learning-based classifier to make each decision using the information about the past decisions and partially completed structures as features. In this paper, we argue that history-based models are not something that should be left behind in research history, by demonstrating that their accuracy can be significantly improved by incorporating a lookahead mechanism into their decisionmaking pr"
W11-0328,P07-1096,0,0.0216958,"e experimental results are shown in Table 1. Note that the models in the top four rows use exactly the same feature set. It is clearly seen that the lookahead improves tagging accuracy, and our historybased models with lookahead is as accurate as the CRF model. We also created another set of models by simply adding tag trigram features, which cannot be employed by first-order CRF models. These features have slightly improved the tagging accuracy, and the final accuracy achieved by a search depth of 3 was comparable to some of the best results achieved by pure supervised learning in this task (Shen et al., 2007; Lavergne et al., 2010). The second set of experiments is about chunking. We used the data set for the CoNLL 2000 shared task, which contains 8,936 sentences where each token is annotated with the “IOB” tags representing text chunks. The experimental results are shown in Table 2. Again, our history-based models with lookahead were slightly more accurate than the CRF model using exactly the same set of features. The accuracy achieved by the lookahead model with a search depth of 2 was comparable to the accuracy achieved by a computationally heavy combination of max-margin classifiers (Kudo and"
W11-0328,H05-1059,1,0.67598,"ch strategies such as beam search. In that case, the search queue is not necessarily truncated. 244 (Tesauro, 2001; Hoki, 2006) in that the parameters are optimized based on the differences of the feature vectors realized by the correct and incorrect actions. In history-based models, the order of actions is often very important. For example, backward tagging is considerably more accurate than forward tagging in biomedical named entity recognition. Our lookahead method is orthogonal to more elaborate techniques for determining the order of actions such as easy-first tagging/parsing strategies (Tsuruoka and Tsujii, 2005; Elhadad, 2010). We expect that incorporating such elaborate techniques in our framework will lead to improved accuracy, but we leave it for future work. 6 Conclusion We have presented a simple and general framework for incorporating a lookahead process in historybased models and a perceptron-based training algorithm for the framework. We have conducted experiments using standard data sets for POS tagging, chunking, named entity recognition and dependency parsing, and obtained very promising results—the accuracy achieved by the history-based models enhanced with lookahead was as competitive a"
W11-0328,P09-1054,1,0.841931,"xperiments This section presents four sets of experimental results to show how the lookahead process improves the accuracy of history-based models in common NLP tasks. 4.1 Sequence prediction tasks First, we evaluate our framework with three sequence prediction tasks: POS tagging, chunking, and named entity recognition. We compare our method with the CRF model, which is one of the de facto standard machine learning models for such sequence prediction tasks. We trained L1-regularized first-order CRF models using the efficient stochastic gradient descent (SGD)-based training method presented in Tsuruoka et al. (2009). Since our main interest is not in achieving the state-of-the-art results for those tasks, we did not conduct feature engineering to come up with elaborate features—we simply adopted the feature sets described in their paper (with an exception being tag trigram features tested in the POS tagging experiments). The experiments for these sequence prediction tasks were carried out using one core of a 3.33GHz Intel Xeon W5590 processor. The first set of experiments is about POS tagging. The training and test data were created from the Wall Street Journal corpus of the Penn Treebank (Marcus et al.,"
W11-0328,W03-3023,0,0.114016,"roposed framework is evaluated on partof-speech tagging, chunking, named entity recognition and dependency parsing, using standard data sets and features. Experimental results demonstrate that history-based models with lookahead are as competitive as globally optimized models including conditional random fields (CRFs) and structured perceptrons. 1 Introduction History-based models have been a popular approach in a variety of natural language processing (NLP) tasks including part-of-speech (POS) tagging, named entity recognition, and syntactic parsing (Ratnaparkhi, 1996; McCallum et al., 2000; Yamada and Matsumoto, 2003; Nivre et al., 2004). The idea is to decompose the complex structured prediction problem into a series of simple classification problems and use a machine learning-based classifier to make each decision using the information about the past decisions and partially completed structures as features. In this paper, we argue that history-based models are not something that should be left behind in research history, by demonstrating that their accuracy can be significantly improved by incorporating a lookahead mechanism into their decisionmaking process. It should be emphasized that we use the word"
W11-0328,W07-1033,0,0.0609087,"(Kudo and Matsumoto, 2001). We also tested the effectiveness of additional features of tag trigrams using the development data, but there was no improvement in the accuracy. The third set of experiments is about named entity recognition. We used the data provided for the BioNLP/NLPBA 2004 shared task (Kim et al., 242 2004), which contains 18,546 sentences where each token is annotated with the “IOB” tags representing biomedical named entities. We performed the tagging in the right-to-left fashion because it is known that backward tagging is more accurate than forward tagging on this data set (Yoshida and Tsujii, 2007). Table 3 shows the experimental results, together with some previous performance reports achieved by pure machine leaning methods (i.e. without rulebased post processing or external resources such as gazetteers). Our history-based model with no lookahead was considerably worse than the CRF model using the same set of features, but it was significantly improved by the introduction of lookahead and resulted in accuracy figures better than that of the CRF model. 4.2 Dependency parsing We also evaluate our method in dependency parsing. We follow the most standard experimental setting for English"
W11-0328,D08-1059,0,0.029985,"Missing"
W11-0328,J93-2004,0,\N,Missing
wang-etal-2010-adapting,C02-1148,0,\N,Missing
wang-etal-2010-adapting,P09-1058,1,\N,Missing
wang-etal-2010-adapting,W08-0336,0,\N,Missing
wang-etal-2010-adapting,N03-1017,0,\N,Missing
wang-etal-2010-adapting,P08-1115,0,\N,Missing
wang-etal-2010-adapting,I08-1033,0,\N,Missing
wang-etal-2010-adapting,D08-1076,0,\N,Missing
Y10-1075,I08-1007,1,0.872999,"Missing"
Y10-1075,P94-1038,0,0.256331,"Missing"
Y10-1075,P08-1047,1,0.887762,"Missing"
Y10-1075,D08-1047,0,0.0534269,"Missing"
Y10-1075,I08-1025,0,0.0329099,"Missing"
Y10-1075,C02-1119,0,0.053663,"Missing"
Y10-1075,J01-2002,0,0.052682,"Missing"
Y10-1075,P97-1017,0,\N,Missing
