2020.coling-main.493,D19-5402,0,0.175257,"s part of our model. We will The code is available at https://github.com/Ruifeng-paper/FactExsum-coling2020 This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http: //creativecommons.org/licenses/by/4.0/. 5629 Proceedings of the 28th International Conference on Computational Linguistics, pages 5629–5639 Barcelona, Spain (Online), December 8-13, 2020 show that such a fact-sentence-document hierarchical text modeling facilitates to capture more structured contextual information for effective fact extraction. Recent works (Liu and Lapata, 2019; Bae et al., 2019; Zhang et al., 2019) have demonstrated that it is highly beneficial to apply pretrained language models such as BERT to extractive summarization models. Following this trend, we adopt a BERT-based encoder to generate contextualized representations for further extraction. It is challenging to impose the structure information when fine-tuning the BERT model in a downstream task. (Fang et al., 2019; Zhang et al., 2019) separately encode representations for each granularity with BERT and then capture the structure information with a graph network stacked upon the encoder. However, this method not"
2020.coling-main.493,P16-1046,0,0.0260563,"training objective. Since most datasets do not contain sentence-level labels, oracle summaries are normally generated by a greedy algorithm, which maximizes the ROUGE score between the sentences in the source text and in the gold summary (Nallapati et al., 2017; Liu and Lapata, 2019). As pointed out by (Narayan et al., 2018), sentences with high ROUGE scores may not necessarily lead to an optimal summary. Such discrepancy may be due to the overlapping contents and over extraction. Therefore, some researches try to perform extraction at a lower level such as words or phrases (Bui et al., 2016; Cheng and Lapata, 2016; Gehrmann et al., 2018). Although these models are able to learn which phrases or words are more important directly from the gold summary, it is hard to maintain semantic integrity when information is scattered. Inspired by (Cao et al., 2018), we propose to extract facts, a granularity between phrases and sentences, as the primary textual units to generate summaries. In this work, we first develop a heuristic algorithm to split a sentence into several smaller units, where each unit is considered as a single fact description. Then we apply a one-to-one strategy to match each fact in the gold s"
2020.coling-main.493,D18-1409,0,0.0226691,"Missing"
2020.coling-main.493,D18-1443,0,0.0149419,"e most datasets do not contain sentence-level labels, oracle summaries are normally generated by a greedy algorithm, which maximizes the ROUGE score between the sentences in the source text and in the gold summary (Nallapati et al., 2017; Liu and Lapata, 2019). As pointed out by (Narayan et al., 2018), sentences with high ROUGE scores may not necessarily lead to an optimal summary. Such discrepancy may be due to the overlapping contents and over extraction. Therefore, some researches try to perform extraction at a lower level such as words or phrases (Bui et al., 2016; Cheng and Lapata, 2016; Gehrmann et al., 2018). Although these models are able to learn which phrases or words are more important directly from the gold summary, it is hard to maintain semantic integrity when information is scattered. Inspired by (Cao et al., 2018), we propose to extract facts, a granularity between phrases and sentences, as the primary textual units to generate summaries. In this work, we first develop a heuristic algorithm to split a sentence into several smaller units, where each unit is considered as a single fact description. Then we apply a one-to-one strategy to match each fact in the gold summary to one fact in th"
2020.coling-main.493,N19-1238,0,0.021249,"Zhang et al., 2019), especially BERT, have made a step forward. 2.2 Graph Based Summarization Graph-based summarization methods aim to utilize the structure information of the text. Based on the assumption that important sentences are often linked with each other, unsupervised models like TextRank (Mihalcea and Tarau, 2004) and LexRank (Erkan and Radev, 2004) show great ability to identify the salient information. (Liu et al., 2018) proposes an abstractive summarization framework based on the Abstract Meaning Representation (AMR) graph, which captures the propositional semantic information. (Koncel-Kedziorski et al., 2019) presents a graph transformer to generate one-sentence summaries from a knowledge graph. Meanwhile, other researches focus on learning latent tree structures while optimizing summarization models. (Williams et al., 2018) regards the tree structure learning problem as a reinforcement learning problem and (Liu et al., 2019) generates a multi-root dependency tree where each root is a summary sentence. 5630 conj cc appos appos nsubj dobj advmod nmod det compound compound case det det compound ccomp det dobj nsubj neg nmod conj case cc Ahmadinejad essentially called Yukiya Amano , the director gene"
2020.coling-main.493,W04-1013,0,0.0178115,"ation diversity and richness. 5 5.1 Experiments Dataset and Evaluation Metric We conduct experiments on the non-anonymized version of the CNN/DailyMail dataset (Hermann et al., 2015). The dataset is composed of news articles and their corresponding highlights that give brief overviews of articles. We apply the standard splits for training, validation, and testing (CNN: 90,266/1,220/1,093 and DailyMail: 196,961/12,148/10,397). Due to the length limitation of BERT, we follow the common practice to truncate all input documents to 512 tokens. To evaluate the summarization quality, we apply ROUGE (Lin, 2004) for automatic evaluation. Following the convention, we report ROUGE-1 (unigram), ROUGE-2 (bi-gram) and ROUGE-L (longest common subsequence) F1-score in the following experiments. Moreover, we manually evaluate the fact-level recall, precision and F1-score between system-generally summaries and human-written gold summaries. 5.2 Details We build our model using PyTorch and the BERT-base-uncased version of BERT. All the input documents are tokenized by BERT’s sub-words tokenizer. We train the model for at most 50000 steps and the batch size in each step is 32. After training for 10000 steps, the"
2020.coling-main.493,D19-1387,0,0.0717653,"Missing"
2020.coling-main.493,N19-1173,0,0.0114892,"k (Erkan and Radev, 2004) show great ability to identify the salient information. (Liu et al., 2018) proposes an abstractive summarization framework based on the Abstract Meaning Representation (AMR) graph, which captures the propositional semantic information. (Koncel-Kedziorski et al., 2019) presents a graph transformer to generate one-sentence summaries from a knowledge graph. Meanwhile, other researches focus on learning latent tree structures while optimizing summarization models. (Williams et al., 2018) regards the tree structure learning problem as a reinforcement learning problem and (Liu et al., 2019) generates a multi-root dependency tree where each root is a summary sentence. 5630 conj cc appos appos nsubj dobj advmod nmod det compound compound case det det compound ccomp det dobj nsubj neg nmod conj case cc Ahmadinejad essentially called Yukiya Amano , the director general of the IAEA , a U.S. puppet and said the U.N.A has no jurisdiction in Iran and Irap. Figure 1: A dependency tree example. We extract the following two fact descriptions: Ahmadinejad no jurisdiction in Iran and Irap 2.3 Pretrained Language Models Pretrained language models (Radford et al., 2018; Devlin et al., 2018) ha"
2020.coling-main.493,W04-3252,0,0.0572985,"a decoder that jointly learns to score and select sentences, while (Zhang et al., 2018) presents a latent variable extractive summarization model, which directly maximizes the likelihood of human summaries. In recent works, models based on pretrained models (Liu and Lapata, 2019; Bae et al., 2019; Zhang et al., 2019), especially BERT, have made a step forward. 2.2 Graph Based Summarization Graph-based summarization methods aim to utilize the structure information of the text. Based on the assumption that important sentences are often linked with each other, unsupervised models like TextRank (Mihalcea and Tarau, 2004) and LexRank (Erkan and Radev, 2004) show great ability to identify the salient information. (Liu et al., 2018) proposes an abstractive summarization framework based on the Abstract Meaning Representation (AMR) graph, which captures the propositional semantic information. (Koncel-Kedziorski et al., 2019) presents a graph transformer to generate one-sentence summaries from a knowledge graph. Meanwhile, other researches focus on learning latent tree structures while optimizing summarization models. (Williams et al., 2018) regards the tree structure learning problem as a reinforcement learning pr"
2020.coling-main.493,N18-1158,0,0.102969,"from the source text. As we know, sentences are regarded as the basic textual units making up the final summaries in most extractive summarization models. One of the problems with sentence-level extractive summarization is that there exists a gap between the human-written gold summary and the training objective. Since most datasets do not contain sentence-level labels, oracle summaries are normally generated by a greedy algorithm, which maximizes the ROUGE score between the sentences in the source text and in the gold summary (Nallapati et al., 2017; Liu and Lapata, 2019). As pointed out by (Narayan et al., 2018), sentences with high ROUGE scores may not necessarily lead to an optimal summary. Such discrepancy may be due to the overlapping contents and over extraction. Therefore, some researches try to perform extraction at a lower level such as words or phrases (Bui et al., 2016; Cheng and Lapata, 2016; Gehrmann et al., 2018). Although these models are able to learn which phrases or words are more important directly from the gold summary, it is hard to maintain semantic integrity when information is scattered. Inspired by (Cao et al., 2018), we propose to extract facts, a granularity between phrases"
2020.coling-main.493,D14-1162,0,0.097836,"mpound ccomp det dobj nsubj neg nmod conj case cc Ahmadinejad essentially called Yukiya Amano , the director general of the IAEA , a U.S. puppet and said the U.N.A has no jurisdiction in Iran and Irap. Figure 1: A dependency tree example. We extract the following two fact descriptions: Ahmadinejad no jurisdiction in Iran and Irap 2.3 Pretrained Language Models Pretrained language models (Radford et al., 2018; Devlin et al., 2018) have been proved to be extremely successful for making great progress in various nature language tasks. These models are originated from the idea of word embeddings (Pennington et al., 2014) and further extend it by pretraining a sentence encoder on the huge unlabeled corpus using language modeling objectives. Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2018), one of the state-of-art language models, is trained with a masked language model and a next-sentence-predicting task. Recently, pretrained language models have been widely used to improve performance in language understanding tasks (Dong et al., 2019). As for the extractive summarization task, it provides the powerful sentence embeddings and the contextualized information among sentences ("
2020.coling-main.493,Q18-1019,0,0.0149974,"es are often linked with each other, unsupervised models like TextRank (Mihalcea and Tarau, 2004) and LexRank (Erkan and Radev, 2004) show great ability to identify the salient information. (Liu et al., 2018) proposes an abstractive summarization framework based on the Abstract Meaning Representation (AMR) graph, which captures the propositional semantic information. (Koncel-Kedziorski et al., 2019) presents a graph transformer to generate one-sentence summaries from a knowledge graph. Meanwhile, other researches focus on learning latent tree structures while optimizing summarization models. (Williams et al., 2018) regards the tree structure learning problem as a reinforcement learning problem and (Liu et al., 2019) generates a multi-root dependency tree where each root is a summary sentence. 5630 conj cc appos appos nsubj dobj advmod nmod det compound compound case det det compound ccomp det dobj nsubj neg nmod conj case cc Ahmadinejad essentially called Yukiya Amano , the director general of the IAEA , a U.S. puppet and said the U.N.A has no jurisdiction in Iran and Irap. Figure 1: A dependency tree example. We extract the following two fact descriptions: Ahmadinejad no jurisdiction in Iran and Irap 2"
2020.coling-main.493,D18-1088,0,0.0153916,"lly formulate it as a sentence binary classification problem. SummaRuNNer (Nallapati et al., 2017), one of the earliest neural summarization models, applies an RNN-based encoder to generate sentence representations and a neural classifier to determine which sentences should be included in the summary. (Narayan et al., 2018) further extends SummaRuNNer with a reinforcement model, which optimizes the summary-level ROUGE metric. Some other works achieve better performance through more complex models. (Zhou et al., 2018) proposes a decoder that jointly learns to score and select sentences, while (Zhang et al., 2018) presents a latent variable extractive summarization model, which directly maximizes the likelihood of human summaries. In recent works, models based on pretrained models (Liu and Lapata, 2019; Bae et al., 2019; Zhang et al., 2019), especially BERT, have made a step forward. 2.2 Graph Based Summarization Graph-based summarization methods aim to utilize the structure information of the text. Based on the assumption that important sentences are often linked with each other, unsupervised models like TextRank (Mihalcea and Tarau, 2004) and LexRank (Erkan and Radev, 2004) show great ability to iden"
2020.coling-main.493,P19-1499,0,0.193037,"l. We will The code is available at https://github.com/Ruifeng-paper/FactExsum-coling2020 This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http: //creativecommons.org/licenses/by/4.0/. 5629 Proceedings of the 28th International Conference on Computational Linguistics, pages 5629–5639 Barcelona, Spain (Online), December 8-13, 2020 show that such a fact-sentence-document hierarchical text modeling facilitates to capture more structured contextual information for effective fact extraction. Recent works (Liu and Lapata, 2019; Bae et al., 2019; Zhang et al., 2019) have demonstrated that it is highly beneficial to apply pretrained language models such as BERT to extractive summarization models. Following this trend, we adopt a BERT-based encoder to generate contextualized representations for further extraction. It is challenging to impose the structure information when fine-tuning the BERT model in a downstream task. (Fang et al., 2019; Zhang et al., 2019) separately encode representations for each granularity with BERT and then capture the structure information with a graph network stacked upon the encoder. However, this method not only results in a la"
2020.coling-main.493,P19-1100,0,0.103193,"and further extend it by pretraining a sentence encoder on the huge unlabeled corpus using language modeling objectives. Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2018), one of the state-of-art language models, is trained with a masked language model and a next-sentence-predicting task. Recently, pretrained language models have been widely used to improve performance in language understanding tasks (Dong et al., 2019). As for the extractive summarization task, it provides the powerful sentence embeddings and the contextualized information among sentences (Zhong et al., 2019), which have been proven to be critical to extractive summarization. 3 Extraction of Facts and Alignment with Gold Summary We propose to explicitly align a gold summary with its corresponding facts descriptions in the source text. For this purpose, we develop a heuristic algorithm to break up the source text and the summary into smaller granularities as introduced below. When splitting sentences into smaller semantic units, we need to ensure each unit has a proper size while maintaining the integrity of information. An intuitive idea might be to split sentences with commas and other conjunctio"
2020.coling-main.493,P18-1061,0,0.0660875,"ext and subsequently concatenate them as a summary. With neural network models, researchers usually formulate it as a sentence binary classification problem. SummaRuNNer (Nallapati et al., 2017), one of the earliest neural summarization models, applies an RNN-based encoder to generate sentence representations and a neural classifier to determine which sentences should be included in the summary. (Narayan et al., 2018) further extends SummaRuNNer with a reinforcement model, which optimizes the summary-level ROUGE metric. Some other works achieve better performance through more complex models. (Zhou et al., 2018) proposes a decoder that jointly learns to score and select sentences, while (Zhang et al., 2018) presents a latent variable extractive summarization model, which directly maximizes the likelihood of human summaries. In recent works, models based on pretrained models (Liu and Lapata, 2019; Bae et al., 2019; Zhang et al., 2019), especially BERT, have made a step forward. 2.2 Graph Based Summarization Graph-based summarization methods aim to utilize the structure information of the text. Based on the assumption that important sentences are often linked with each other, unsupervised models like T"
2021.emnlp-main.334,2020.emnlp-main.750,0,0.031886,"bjective function consists of three terms: L = Lseq + Lkl + Lnode . 3.5 Faithful Beam Search Inspired by (Scialom et al., 2020), we propose faithful beam search to reduce possible factual errors at the inference stage. Given a factual consistency checking model F and a sentence fusion model G, the goal is to re-rank every generated token based on both the generation probability calculated by 3.4 Training G and the faithful score derived from F . In our Generation Loss. With the generation loss, the training goal is to maximize the estimated proba- work, we adopt the FactCC model developed by (Kryscinski et al., 2020), a BERT-based faithfulbility of the reference sequence. Following most ness checking model, to evaluate faithfulness. The current works, we adopt the maximum likelihood training objective function that minimizes the fol- input to FactCC consists of a hypothesis sentence and several source sentences, while the output from lowing loss. FactCC is a probability that refers to whether the X 1 hypothesis sentence is faithful to the source senLseq = − logp(y|x, g; θ) (12) |D |(x,y,g)∈D tences. Since what we need here is to verify the 4079 pcopy = sigmoid(Wcopy [yt−1 ; st ; cst ; cgt ; cft ]) (11) wh"
2021.emnlp-main.334,P99-1071,0,0.721098,"Missing"
2021.emnlp-main.334,W11-1607,0,0.0709535,"Missing"
2021.emnlp-main.334,P19-1102,0,0.0189564,"at connect these event components together. Compared to the word graph or the dependency tree, the event graph provides more informative event-level (or to say entity-level) information. Meanwhile, it maintains the semantic integrity of each node, which allows us to add additional edges to represent some crucial relationships in disparate sentence fusion like co-reference. Such a structured representation is capable of preserving inherent event information and meanwhile formulating cross-sentence information such as entity interactions and proximity of relevant concepts. 2015) and Multi-News (Fabbri et al., 2019). The experiments show that our proposed model indeed improves Rouges and the other metrics like faithfulness and the fusion rate. The contribution of our work can be summarized as follows: (1) We propose a model to address both similar sentence fusion and disparate sentence fusion, which are critical for abstractive summarization. (2) We build an event graph to guide sentence fusion, which allows our model to utilize the structural event information and various cross-sentence relations. (3) We innovatively apply a graph flow attention to control the fusion process via the graph structure. 2 2"
2021.emnlp-main.334,D08-1019,0,0.0653821,"1 Related Work Sentence Fusion in Text Summarization Sentence fusion has been considered as an essential step for generating abstractive summaries. Its With the target to guide sentence fusion, we de- importance has long been recognized in the tradivelop a decoder that utilizes the information from tional text summarization research (Barzilay et al., both the sentence sequence and the event graph 1999). The early attempts mainly focus on fusing a equipped with different attention mechanisms. We set of similar sentences (Marsi and Krahmer, 2005; employ sequence attention and graph attention to Filippova and Strube, 2008; Elsner and Santhanam, determine what information is important to be se- 2011; Thadani and McKeown, 2013). They oflect to generate the appropriate word token at each ten build a dependency graph or a word graph decoding step. Note that sentence fusion requires from multiple similar sentences, and then adopt not only selecting the right salient information but linear programming to generate the fused sentence also organizing the selected information logically from the graph. Recently, (Lebanoff et al., 2019a) and orderly. Otherwise, the models may tend to conducts a comprehensive analysis of s"
2021.emnlp-main.334,2020.emnlp-main.338,0,0.0430875,"Missing"
2021.emnlp-main.334,D19-5413,0,0.202596,"ntences (Marsi and Krahmer, 2005; employ sequence attention and graph attention to Filippova and Strube, 2008; Elsner and Santhanam, determine what information is important to be se- 2011; Thadani and McKeown, 2013). They oflect to generate the appropriate word token at each ten build a dependency graph or a word graph decoding step. Note that sentence fusion requires from multiple similar sentences, and then adopt not only selecting the right salient information but linear programming to generate the fused sentence also organizing the selected information logically from the graph. Recently, (Lebanoff et al., 2019a) and orderly. Otherwise, the models may tend to conducts a comprehensive analysis of sentence furandomly combine the key event components or sion in neural abstractive summarization and finds simply copy the most important text span. To this that it remains a challenge for current state-of-theend, we develop a graph flow attention to explore art models. To address this problem, (Lebanoff potential fusion paths via the graph structure and et al., 2020a,b) propose to utilize points of correcontrol the fusion process. Moreover, how to avoid spondence between sentences to fuse disparate senfactu"
2021.emnlp-main.334,2020.acl-srw.26,0,0.0217464,"xt of text summarization. candidate output sequence during the generation Moving beyond sentence fusion alone, (Mehdad process by refining the generation probability with et al., 2013; Lebanoff et al., 2019b) discusses the a faithful score. potential application scenarios for enhancing text Since there is no available dataset to evaluate the summarization with sentence fusion. Their models effectiveness of the sentence fusion models in the follow a similar framework that first extracts a few context of text summarization, following previous related sentences from the source document and work (Lebanoff et al., 2020b), we automatically then fuses them to obtain a summary sentence. Our generate sentence fusion data from summarization model can be considered as a better replacement of datasets including CNN/DaliyMail (Hermann et al., the fusion model in such a framework. 4076 Attentions on Event Graph 2 Joint Encoder Event Graph 1 1 4 3 5 Graph Attention 2 1 4 3 5 LSTM Decoder 4 3 2 KL loss St 5 1 4 Graph Flow Attention 3 2 Source Cbs news correspondent julianna goldman reports from washington that president obama didn ' t talk military planning friday night when he met with democratic donors ... 5 .. . 1"
2021.emnlp-main.334,P19-1209,0,0.30272,"ntences (Marsi and Krahmer, 2005; employ sequence attention and graph attention to Filippova and Strube, 2008; Elsner and Santhanam, determine what information is important to be se- 2011; Thadani and McKeown, 2013). They oflect to generate the appropriate word token at each ten build a dependency graph or a word graph decoding step. Note that sentence fusion requires from multiple similar sentences, and then adopt not only selecting the right salient information but linear programming to generate the fused sentence also organizing the selected information logically from the graph. Recently, (Lebanoff et al., 2019a) and orderly. Otherwise, the models may tend to conducts a comprehensive analysis of sentence furandomly combine the key event components or sion in neural abstractive summarization and finds simply copy the most important text span. To this that it remains a challenge for current state-of-theend, we develop a graph flow attention to explore art models. To address this problem, (Lebanoff potential fusion paths via the graph structure and et al., 2020a,b) propose to utilize points of correcontrol the fusion process. Moreover, how to avoid spondence between sentences to fuse disparate senfactu"
2021.emnlp-main.334,D19-1387,0,0.0244394,"e 48.63 44.60 32.95 27.04 36.56 37.16 100 100 100 - 71.28 31.48 Pointer-Generator BERT+LSTM BERTSUMABS 49.01 50.93 51.85 31.57 33.99 31.60 40.65 43.00 44.62 81.45 85.84 78.32 44.39 48.30 56.48 29.28 26.16 26.32 Our Model 53.06 36.02 45.40 89.31 59.82 25.36 Multi-News Fusion Table 2: Automatic evaluation on Rouge, faithfulness(Fai), fusion rate(Fus), and generated sentence length (Len). erence relationships between entities to enhance sentence fusion. Since our data can be approximately regarded as multi-sentence summarization, we also adopt BERT based document summarization model, BERTSUMABS (Liu and Lapata, 2019), for comparisons. Most of these models are trained on the two sentence fusion datasets by ourselves except that the output result of TransformerLinking is directly obtained from its author. As shown in Table 2, our proposed model obtains the highest Rouge scores on the Multi-News Fusion dataset and the competitive Rouge scores on the CNN/DaliyMail Fusion dataset. Meanwhile, our model achieves the best performance in fusion rate and faithfulness on both datasets. These suggest the effectiveness of our model in fusing sentences and its ability to reduce factual errors. We also notice that the t"
2021.emnlp-main.334,W05-1612,0,0.128465,"flow attention to control the fusion process via the graph structure. 2 2.1 Related Work Sentence Fusion in Text Summarization Sentence fusion has been considered as an essential step for generating abstractive summaries. Its With the target to guide sentence fusion, we de- importance has long been recognized in the tradivelop a decoder that utilizes the information from tional text summarization research (Barzilay et al., both the sentence sequence and the event graph 1999). The early attempts mainly focus on fusing a equipped with different attention mechanisms. We set of similar sentences (Marsi and Krahmer, 2005; employ sequence attention and graph attention to Filippova and Strube, 2008; Elsner and Santhanam, determine what information is important to be se- 2011; Thadani and McKeown, 2013). They oflect to generate the appropriate word token at each ten build a dependency graph or a word graph decoding step. Note that sentence fusion requires from multiple similar sentences, and then adopt not only selecting the right salient information but linear programming to generate the fused sentence also organizing the selected information logically from the graph. Recently, (Lebanoff et al., 2019a) and orde"
2021.emnlp-main.334,W13-2117,0,0.0185986,"Missing"
2021.emnlp-main.334,N19-1236,0,0.0509824,"Missing"
2021.emnlp-main.334,N19-1348,0,0.0375562,"Missing"
2021.emnlp-main.334,P17-1099,0,0.0516777,"sional hidden states as the decoder. We truncate the input sentences to 150 tokens and limit the decoder to a maximum of 60 steps. The batch size is set to 32 and we train the model for 20 epochs. After training, we select top-3 checkpoints on the validation dataset, and report the one with the best record on the test set among the three. For inference, the beam size is set to 5 in CNN/DaliyMail Fusion and 2 in Multi-News Fusion. 4.2 Automatic Evaluation To examine the effectiveness of our model, we compare our model with two widely adopted seq2seq baseline models. They are Pointer-Generator (See et al., 2017) and BERT+LSTM, which is Evaluation Metrics: Sentence fusion can be ap- our basic encoder-decoder architecture before inproximately regarded as multi-sentence summa- tegrating the graph information. We also implerization. Following the common practice, we adopt ment the state-of-the-art sentence fusion model ROUGE F1 as the basic evaluation metric. We for comparisons. Tranformer-Linking (Lebanoff also apply FactCC (Kryscinski et al., 2020) to et al., 2020a) is a BERT based model proposed evaluate faithfulness (Fai) automatically. FactCC for disparate sentence fusion. It utilizes coref4080 CNN/"
2021.emnlp-main.334,N18-1081,0,0.0179121,"es extracted with OpenIE to an event graph to acquire semantic interpretation over input to assist text summarization. (Zheng and Kordjamshidi, 2020) adopts an event graph to understand the path of multi-hop reasoning in question answering. To control the generation process and avoid factual errors, (Cao et al., 2017) proposes an additional event relation encoder to produce representations of event triples. Considering the importance of the relations between events in sentence fusion and inspired by the above-mentioned works, we adopt the event graph to guide sentence fusion. AllenNLP-OpenIE (Stanovsky et al., 2018) to extract a set of events, where each event is composed of a predicate and an arbitrary number of arguments. When there is an overlap between two events, only the longer one is retained. These predicates and arguments are represented as the nodes in the event graph. When two nodes share the same content, we merge them into one. The graph is a directed graph. Two types of edges are considered. (1) Directional edges connect a predicate and its corresponding arguments in an event and the direction follows the order of subject to predicate and predicate to other arguments. (2) Bi-directional edg"
2021.emnlp-main.334,I13-1198,0,0.0192103,"tep for generating abstractive summaries. Its With the target to guide sentence fusion, we de- importance has long been recognized in the tradivelop a decoder that utilizes the information from tional text summarization research (Barzilay et al., both the sentence sequence and the event graph 1999). The early attempts mainly focus on fusing a equipped with different attention mechanisms. We set of similar sentences (Marsi and Krahmer, 2005; employ sequence attention and graph attention to Filippova and Strube, 2008; Elsner and Santhanam, determine what information is important to be se- 2011; Thadani and McKeown, 2013). They oflect to generate the appropriate word token at each ten build a dependency graph or a word graph decoding step. Note that sentence fusion requires from multiple similar sentences, and then adopt not only selecting the right salient information but linear programming to generate the fused sentence also organizing the selected information logically from the graph. Recently, (Lebanoff et al., 2019a) and orderly. Otherwise, the models may tend to conducts a comprehensive analysis of sentence furandomly combine the key event components or sion in neural abstractive summarization and finds"
2021.emnlp-main.334,2020.coling-main.493,1,0.74561,"corporates the information node in the attention layer of BERT. To distinguish from the source sentences and the event graph to the two kinds of tokens, we assign two different generate a fused sentence. segment embeddings to sentence tokens and node 3.1 Event Graph Construction tokens. Since there is no sequential relationship The event graph is built to capture the semantic between nodes, we initialize the positional embedrelationships in the source sentences. We utilize ding for node tokens as a special pad embedding. 4077 We use an additional mask matrix M similar to the one presented in (Yuan et al., 2020) to control the attention of the BERT-based encoder. Mij = 0 means token i is allowed to attend to j, while Mij = −∞ prohibits i from attending to j. In our model, three possible situations can happen: (1) a sentence token attends to all other sentence tokens; (2) a sentence token attends to its corresponding graph node token; (3) a node token attends to other adjacent nodes on the event graph. After defining the mask matrix M , we calculate attention with Equation (1) below, where Q, K and V refer to the query matrix, the key matrix and the value matrix, respectively, dk is a scaling factor."
2021.emnlp-main.334,2020.emnlp-main.714,0,0.0429553,"tokens or nodes. 2.2 Event-aware Generation Model Currently, in the conditional generation tasks like text summarization and question answering, most of the source documents are usually composed of a series of events. Understanding how to leverage event information in these generation models becomes crucial. (Moryossef et al., 2019) learns to generate a fluent sentence with an input subjectverb-object triple that describes an event. (Huang et al., 2020) transfers event triples extracted with OpenIE to an event graph to acquire semantic interpretation over input to assist text summarization. (Zheng and Kordjamshidi, 2020) adopts an event graph to understand the path of multi-hop reasoning in question answering. To control the generation process and avoid factual errors, (Cao et al., 2017) proposes an additional event relation encoder to produce representations of event triples. Considering the importance of the relations between events in sentence fusion and inspired by the above-mentioned works, we adopt the event graph to guide sentence fusion. AllenNLP-OpenIE (Stanovsky et al., 2018) to extract a set of events, where each event is composed of a predicate and an arbitrary number of arguments. When there is a"
2021.findings-emnlp.48,P14-1093,0,0.076267,"Missing"
2021.findings-emnlp.48,2020.acl-main.703,0,0.0880048,"Missing"
2021.findings-emnlp.48,C16-1316,0,0.0177212,"ll components of ei . input cause and rewriting the predicted effect event The final hidden vector eL i (i = 1, · · · , NCG ) 1 https://spacy.io/ of events are used to select the guided effect event 528 T · h is eY by eY = maxi csi , where csi = eL X i the causal score between each candidate event ei 1 Pm and X, hX = m k=1 hxk is the mean-pooling representation of X. Effect Event Rewriter: The predicted eY contains the skeleton information, we want retain all tokens of eY when generating the effect sentence to avoiding the causal information carried by eY degrading to word-level. Inspired by (Mou et al., 2016; Martin et al., 2018), we rewrite eY = (s, v, o, m) into the effect sentence which conforms to the format of [_s][_v][_o][_m], where blanks indicate the place words should be added to in order to make a sentence richer in content. We use a decoder with attention mechanism (Bahdanau et al., 2014) to generate words in each blank until generating the ""&lt;eos&gt;"" token. 4.2 Baselines and Evaluation Baselines: We compare our method with state-ofthe-art text generation methods, including GPT2 (Radford et al., 2019), BART(Lewis et al., 2019), CopyNet(Zhu et al., 2017) and CausalBERT(Li et al., 2020). De"
2021.findings-emnlp.48,P02-1040,0,0.114136,"ormat of [_s][_v][_o][_m], where blanks indicate the place words should be added to in order to make a sentence richer in content. We use a decoder with attention mechanism (Bahdanau et al., 2014) to generate words in each blank until generating the ""&lt;eos&gt;"" token. 4.2 Baselines and Evaluation Baselines: We compare our method with state-ofthe-art text generation methods, including GPT2 (Radford et al., 2019), BART(Lewis et al., 2019), CopyNet(Zhu et al., 2017) and CausalBERT(Li et al., 2020). Details can be seen in Appendix A. Metrics: For automatic evaluation, we use metrics including BLEU-4 (Papineni et al., 2002), Distinct-n (Li et al., 2015) to evaluate the generated effect sentences. Abstraction-Matching (AbsMat) evaluates the percentage of the generated effect sequences that have the same abstraction as the corresponding gold effect sequences. For the manual evaluation, we examine whether the generated sequence is a plausible effect of the input, which is denoted as plausibility (Plau). Details can be seen in Appendix B. Result: The result is shown in Table 1, where 4 Experiments EGCER achieves the best results. BART per4.1 Datasets forms better than GPT2 due to the adopted encoderEnglish Wikipedia"
2021.findings-emnlp.48,speer-havasi-2012-representing,0,0.0479168,"Missing"
2021.semeval-1.70,Q17-1010,0,0.0473349,"ery distinct path as a single feature. In total, we generated 267 dependency paths features with this mechanism. Another feature was based on Word Embedding similarity: first, we computed the sum of the embeddings for all the words preceding the target, as a sort of general representation of the sentence context 1 , and then we measured the cosine similarity with the embedding of the target word. If the target was a multiword expression, we summed the embeddings of the words composing it. As word embeddings, we used the publicly available FastText vectors, pre-trained on the Wikipedia corpus (Bojanowski et al., 2017). 2 We added one feature based on the BERT Transformer Model (Devlin et al., 2019) 3 by masking the target word in the original sentence and taking the probability value provided in output by the Softmax. For multiword expressions, we sequentially masked the words composing the target and took the average value. Similarly, we used the GPT-2 Transformer Model (Radford et al., 2019) 4 to obtain a probability score for the full sentence, computed as the product of the probabilities of the single tokens. The total number of extracted features is 300. Finally, we decided to generate polynomial feat"
2021.semeval-1.70,W16-4102,1,0.826572,"meanings. In linguistic typology, for example, complexity is generally studied as a property of the language system as a whole, it is conceived as the number of (morphological, syntactic, semantic etc.) distinctions that a speaker has to master, and it is assessed by comparing different languages (McWhorter, 2001; Parkvall, 2008). On the other hand, in the perspective of psycholinguistics and cognitive science, the notion of complexity can be described as the difficulty encountered by language users while processing concrete linguistic realizations (sentences, utterances etc.) (Blache, 2011; Chersoni et al., 2016, 2017, 2021; Iavarone et al., 2021; Sarti et al., 2021). Finally, in the Computational Linguistics community, the assessment of complexity at the lexical level is often related to readability applications (Shardlow et al., 2020), with the goal of determining if a word in a given text will be difficult to understand for the language users. Such applications are extremely useful for second language learners, for speakers with relatively low literacy and for people with reading disabilities, helping to tailor the difficult level of the texts to the needs of the target users. Task 1 of SemEval 20"
2021.semeval-1.70,S17-1021,1,0.866426,"Missing"
2021.semeval-1.70,N19-1423,0,0.0167303,"tures with this mechanism. Another feature was based on Word Embedding similarity: first, we computed the sum of the embeddings for all the words preceding the target, as a sort of general representation of the sentence context 1 , and then we measured the cosine similarity with the embedding of the target word. If the target was a multiword expression, we summed the embeddings of the words composing it. As word embeddings, we used the publicly available FastText vectors, pre-trained on the Wikipedia corpus (Bojanowski et al., 2017). 2 We added one feature based on the BERT Transformer Model (Devlin et al., 2019) 3 by masking the target word in the original sentence and taking the probability value provided in output by the Softmax. For multiword expressions, we sequentially masked the words composing the target and took the average value. Similarly, we used the GPT-2 Transformer Model (Radford et al., 2019) 4 to obtain a probability score for the full sentence, computed as the product of the probabilities of the single tokens. The total number of extracted features is 300. Finally, we decided to generate polynomial features from our set, in order to exploit potential interactions. We used the Polynom"
2021.semeval-1.70,2021.cmcl-1.23,0,0.0651991,"Missing"
2021.semeval-1.70,2005.mtsummit-papers.11,0,0.0360983,"ressions track. Examples of the instances are shown in Table 1. 4 Evaluation For both the single words and the multiword expressions track, we used the same set of features as input for a regression algorithm. In the multiword expressions track, we computed the value of the features for each of the two words in the target expression and then we took the average. 4.1 Datasets The datasets for the shared task are part of the CompLex corpus, which has been published and described by Shardlow et al. (2020). The annotated sentences were collected using three different corpora: the Europarl corpus (Koehn, 2005), which includes the proceedings of the European Parliament; the CRAFT biomedical corpus (Bada et al., 2012); and the Bible, in the modern version of the World English Bible translation (Christodouloupoulos and Steedman, 2015). The organizers selected targets as either single words (Sub-Task 1) or multiword expressions (SubTask 2), and the datasets include also multiple examples with the same target, as different contexts can determine different complexity values. As for the multiword expressions, they were identified via syntactic patterns, being either adjective-noun or noun-noun phrases. 20"
2021.semeval-1.70,P14-5010,0,0.00258051,"CompLex. In total, we obtained 6 features (4 frequency + 2 length features) for each instance. We also added two Boolean features for Capitalization: the first was equal to 1 if the first letter of the target word was upper case and 0 otherwise; the second one was equal to 1 if all the letters of the target word were upper case and 0 otherwise. The latter feature was added because we noticed that some of the target words in the dataset are acronyms. Apart from the lexical information, Syntactic Features were explored for both single words and 566 multiword expressions. The StanfordNLP tools (Manning et al., 2014) were first used to acquire both the part-of-speech (POS) tags and dependency trees. POS tags of target words were manipulated using one-hot encoding, for a total of 20 POS-based features. On the other hand, directed and path from the target word to the root were extracted as dependency features. We concatenated all dependency tags to the root, using one-hot encoding once again to encode every distinct path as a single feature. In total, we generated 267 dependency paths features with this mechanism. Another feature was based on Word Embedding similarity: first, we computed the sum of the embe"
2021.semeval-1.70,2021.cmcl-1.5,0,0.0626278,"Missing"
2021.semeval-1.70,2020.readi-1.9,0,0.14381,"ker has to master, and it is assessed by comparing different languages (McWhorter, 2001; Parkvall, 2008). On the other hand, in the perspective of psycholinguistics and cognitive science, the notion of complexity can be described as the difficulty encountered by language users while processing concrete linguistic realizations (sentences, utterances etc.) (Blache, 2011; Chersoni et al., 2016, 2017, 2021; Iavarone et al., 2021; Sarti et al., 2021). Finally, in the Computational Linguistics community, the assessment of complexity at the lexical level is often related to readability applications (Shardlow et al., 2020), with the goal of determining if a word in a given text will be difficult to understand for the language users. Such applications are extremely useful for second language learners, for speakers with relatively low literacy and for people with reading disabilities, helping to tailor the difficult level of the texts to the needs of the target users. Task 1 of SemEval 2021 (Shardlow et al., 2021) aims at the development of systems for the estimation of lexical complexity in context, both for single words and for multiword expressions. The organizers provided two datasets with the target words in"
2021.semeval-1.70,D18-1499,0,0.0547684,"Missing"
2021.semeval-1.70,W18-0507,0,0.0594112,"Missing"
2021.semeval-1.70,W17-5910,0,0.0184583,"variable: given a word in context, the word will be judged as complex or not. Of course, this was a simplifying assumption, since there might be many situations where the boundary is not a clear-cut one, and annotators would rather indicate a value in a continuous scale. Moreover, the ”complex” words in the data only needed to be categorized as such by just one 565 Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021), pages 565–570 Bangkok, Thailand (online), August 5–6, 2021. ©2021 Association for Computational Linguistics of the annotators. A further study by Zampieri et al. (2017) analyzed the output of the participating systems, showing that modeling complexity as binary actually hindered their performance. A second iteration of the shared task was organized in 2018 (Yimam et al., 2018), this time features two separate subtasks: the traditional binary classification task, where systems had to predict whether one word was complex or not, and a regression task, where systems had to estimate the probability that an annotator would have considered a given word as complex. Recently, Shardlow et al. (2020) have introduced CompLex, a new gold standard for the estimation of l"
C04-1101,P90-1016,0,0.178631,"le. 1 Introduction Temporal information describes changes and time of the changes. In a language, the time of an event may be specified explicitly, for example “ 他 们 在 1997 年解决了该市的交通问题 (They solved the traffic problem of the city in 1997)”; or it may be related to the time of another event, for example “修成立交 桥以后, 他们解决了该市的交通问题 (They solved the traffic problem of the city after the street bridge had been built”. Temporal reference describes how events relate to one another, which is essential to natural language processing (NLP). Its major applications cover syntactic structural disambiguation (Brent, 1990), information extraction and question answering (Li, 2002), language generation and machine translation (Dorr, 2002). Many researchers have attempted to characterize the nature of temporal reference in a discourse. Identifying temporal relations 1 between two events de1 The relations under examined include both intra-sentence and interWenjie Li Department of Computing The Hong Kong Polytechnic University, Hong Kong cswjli@comp.polyu.edu.hk Chunfa Yuan Department of Computer Science and Technology Tsinghua University, Beijing, China. cfyuan@tsinghua.edu.cn pends on a combination of information"
C04-1101,P91-1008,0,0.0849163,"Missing"
C04-1101,J88-2005,0,0.132459,"se structure regardless of the lexical words included in the sentence. Generally, constraints were used to support syntactic disambiguation (Brent, 1990) or to generate acceptable sentences (Dorr, 2002). In a given CTS, a past perfect clause should precede the event described by a simple past clause. However, the order of two events in CTS does not necessarily correspond to the order imposed by the interpretation of the connective (Dorr, 2002). Temporal/casual connective, such as “after”, “before” or “because”, can supply explicit information about the temporal ordering of events. Passonneau (Passonneau, 1988), Brent (Brent, 1990 and Sing (Sing, 1997) determined intra-sentential relations by accounting for temporal or causal connectives. Dorr and Gaasterland (Dorr, 2002), on the other hand, studied how to generate the sentences which reflect event temporal relations by selecting proper connecting words. However, temporal connectives can be ambiguous. For instance, a “when” clause permits many possible temporal relations. Several researchers have developed the models that incorporated aspectual types (such as those distinct from states, processes and events) to interpret temporal relations between c"
C04-1101,J00-4004,0,0.0410779,"Missing"
C04-1101,P94-1013,0,0.0219908,"be jointly affected by auxiliary words (e.g. 过 , were/was), trend verbs (起来, begin to), and so on. Obviously, it is not a simple task to map the combined effects of the thirteen linguistic features to the corresponding relations. Therefore, a machine learning approach is proposed, which investigates how these features contribute to the task and how they should be combined. 4 Combining Linguistic Features with Machine Learning Approach Previous efforts in corpus-based NLP have incorporated machine learning methods to coordinate multiple linguistic features, for example, in accent restoration (Yarowsky, 1994) and event classification (Siegel, 1998). Temporal relation determination can be modeled as a relation classification task. We formulate the thirteen temporal relations (see Figure 1) as the classes to be decided by a classifier. The classification process is to assign an event pair to one class according to their linguistic features. There existed numerous classification algorithms based upon supervised learning principle. One of the most effective classifiers is Bayesian Classifier, introduced by Duda Effect Not Applicable Tense Aspect Discourse Structure/Aspect Discourse Structure Tense/Asp"
C04-1101,J88-2003,0,\N,Missing
C04-1101,J88-2006,0,\N,Missing
C04-1101,E95-1035,0,\N,Missing
C08-1062,W04-3247,0,0.249297,"(4) rank SB given that SA is provided. Among them, (4) is of most concern. It should be noting that both (2) and (4) need to consider the influence from the sentences in the same and different collections. In this study, we made an attempt to capture the intuition that “A sentence receives a positive influence from the sentences that correlate to it in the same collection, whereas a sentence receives a negative influence from the sentences that correlates to it in the different collection.” We represent the sentences in A or B as a text graph constructed using the same approach as was used in Erkan and Radev (2004a, 2004b). Different from the existing PageRank-like algorithms adopted in document summarization, we propose a novel sentence ranking algorithm, called PNR2 (Ranking with Positive and Negative Reinforcement). While PageRank models the positive mutual reinforcement among the sentences in the graph, PNR2 is capable of modeling both positive and negative reinforcement in the ranking process. The remainder of this paper is organized as follows. Section 2 introduces the background of the work presented in this paper, including existing graph-based summarization models, descriptions of update summa"
C08-1062,P06-1047,1,0.861538,"hich was then used as the criterion to rank and select summary sentences. Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year. Besides, they reported experimental comparison of three different graph-based sentence ranking algorithms obtained from Positional Power Function, HITS and PageRank (Mihalcea and Tarau, 2005). Both HITS and PageRank performed excellently. Likewise, the use of PageRank family was also very popular in event-based summarization approaches (Leskovec et al., 2004; Vanderwende et al., 2004; Yoshioka and Haraguchi, 2004; Li et al., 2006). In contrast to conventional sentencebased approaches, newly emerged event-based approaches took event terms, such as verbs and action nouns and their associated named entities as graph nodes, and connected nodes according to their co-occurrence information or semantic dependency relations. They were able to provide finer text representation and thus could be in favor of sentence compression which was targeted to include more informative contents in a fixed-length summary. Nevertheless, these advantages lied on appropriately defining and selecting event terms. All above-mentioned representati"
C08-1062,N03-1020,0,0.205294,"summary the highest ranked sentence of concern if it doesn’t significantly repeat the information already included in the summary until the word limitation is reached. Average number of documents Average number of sentences A 10 237.6 B 10 177.3 Table 1. Basic Statistics of DUC2007 Update Data Set As for the evaluation metric, it is difficult to come up with a universally accepted method that can measure the quality of machine-generated summaries accurately and effectively. Many literatures have addressed different methods for automatic evaluations other than human judges. Among them, ROUGE5 (Lin and Hovy, 2003) is supposed to produce the most reliable scores in correspondence with human evaluations. Given the fact that judgments by humans are timeconsuming and labor-intensive, and more important, ROUGE has been officially adopted for the DUC evaluations since 2005, like the other researchers, we also choose it as the evaluation criteria. In the following experiments, the sentences and the queries are all represented as the vectors of words. The relevance of a sentence to the query is calculated by cosine similarity. Notice that the word weights are normally measured by the document-level TF*IDF sche"
C08-1062,P04-3020,0,0.0697616,"Missing"
C08-1062,W04-3252,0,\N,Missing
C08-1062,H05-1115,0,\N,Missing
C08-1124,W97-0703,0,0.0174732,"ng centroid (Radev et al., 2004), signature terms (Lin and Hovy, 2000) and high frequency words (Nenkova e t al., 2006). Radev et al. (2004) defined centroid words as those whose average tf*idf score were higher than a threshold. Lin and Hovy (2000) identified signature terms that were strongly associated with documents based on statistics measures. Nenkova et al. (2006) later reported that high frequency words were crucial in reflecting the focus of the document. Bag of words is somewhat loose and omits structural information. Document structure is another possible feature for summarization. Barzilay and Elhadad (1997) constructed lexical chains and extracted strong chains in summaries. Marcu (1997) parsed documents as rhetorical trees and identified important sentences based on the trees. However, only moderate results were reported. On the other hand, Dejong (1978) represented documents using predefined templates. The procedure to create and fill the templates was time consuming and it was hard to adapt the method to different domains. Recently, semi-structure events (Filatovia and Hatzivassiloglou, 2004; Li et al., 2006; Wu, 2006) have been investigated by many researchers as they balanced document repre"
C08-1124,W04-3247,0,0.0136217,"uctures. They defined events as verbs (or action nouns) plus the associated named entities. For instance, given the sentence “Yasser Arafat on Tuesday accused the United States of threatening to kill PLO officials”, they first identified “accused”, “threatening” and “kill” as event terms; and “Yasser Arafat”, “United States”, “PLO” and “Tuesday” as event elements. Encouraging results based on events were reported for news stories. From another point of view, sentences in a document are somehow connected. Sentence relevance has been used as an alternative means to identify important sentences. Erkan and Radev (2004) and Yoshioka (2004) evaluate the relevance (similarity) between any two sentences first. Then a web analysis approach, PageRank, was used to select important sentences from a sentence map built on relevance. Promising results were reported. However, the combination of these features is not well studied. Wu et al. (2007) conducted preliminary research on this problem, but event features were not considered. Normally labeling procedure in supervised learning is very time consuming. Blum and Mitchell (1998) proposed co-training approach to exploit labeled and unlabeled data. Promising results we"
C08-1124,W04-1017,0,0.0250281,"Missing"
C08-1124,P04-1074,1,0.897441,"research on this problem, but event features were not considered. Normally labeling procedure in supervised learning is very time consuming. Blum and Mitchell (1998) proposed co-training approach to exploit labeled and unlabeled data. Promising results were reported from their experiments on web page classification. A number of successful studies emerged thereafter for other natural language processing tasks, such as text classification (Denis and Gilleron, 2003), noun phrase chunking (Pierce and Cardie, 2001), parsing (Sarkar, 2001) and reference or relation resolution (Muller et al., 2001; Li et al., 2004). To our knowledge, there is little research in the application of co-training techniques to extractive summarization. 3 The Framework for Extractive Summarization Extractive summarization can be regarded as a classification problem. Given the features of a sentence, a machine-learning based classification model will judge how likely the sentence is important. The classification model can be supervised or semi-supervised learning. Supervised approaches normally perform better, but require more labeled training data. SVMs perform well in many classification problems. Thus we employ it for super"
C08-1124,P06-1047,1,0.809104,"ly. Although the evaluation results are encouraging, supervised learning approach requires much labeled data. Therefore we investigate co-training by combining labeled and unlabeled data. Experiments show that this semisupervised learning approach achieves comparable performance to its supervised counterpart and saves about half of the labeling time cost. 1 Introduction 1 Automatic text summarization involves condensing a document or a document set to produce a human comprehensible summary. Two kinds of summarization approaches were suggested in the past, i.e., extractive (Radev et al., 2004; Li et al., 2006) and abstractive summarization (Dejong, 1978). The abstractive approaches typically need © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0/). Some rights reserved. to “understand” and then paraphrase the salient concepts across documents. Due to the limitations in natural language processing technology, abstractive approaches are restricted to specific domains. In contrast, extractive approaches commonly select sentences that contain the most significant concepts in the documents. These approa"
C08-1124,C00-1072,0,0.00947415,"zation framework. Section 4 outlines the various sentence features and Section 5 describes supervised/semi-supervised learning approaches. Section 6 presents experiments and results. Finally, Section 7 concludes the paper. 2 Related Work Traditionally, features for summarization were studied separately. Radev et al. (2004) reported that position and length are useful surface features. They observed that sentences located at the document head most likely contained important information. Recently, content features were also well studied, including centroid (Radev et al., 2004), signature terms (Lin and Hovy, 2000) and high frequency words (Nenkova e t al., 2006). Radev et al. (2004) defined centroid words as those whose average tf*idf score were higher than a threshold. Lin and Hovy (2000) identified signature terms that were strongly associated with documents based on statistics measures. Nenkova et al. (2006) later reported that high frequency words were crucial in reflecting the focus of the document. Bag of words is somewhat loose and omits structural information. Document structure is another possible feature for summarization. Barzilay and Elhadad (1997) constructed lexical chains and extracted s"
C08-1124,N03-1020,0,0.043526,"ters of relevant documents and 308 documents in total. Each cluster deals with a specific topic (e.g. a hurricane) and comes with model summaries created by NIST assessors. 50, 100, 200 and 400 word summaries are provided. Twenty-five of the thirty document clusters are used as training data and the remaining five are used as testing. The training/testing configuration is same in experiments of supervised learning and semi-supervised learning, while the difference is that some sentences in training data are not tagged for semi-supervised learning. An automatic evaluation package, i.e., ROUGE (Lin and Hovy, 2003) is employed to evaluate the summarization performance. It compares machine-generated summaries with model summaries based on the overlap. Precision and recall measures are used to evaluate the classification performance. For comparison, we evaluate our approaches on DUC 2004 data set also. It contains 50 clusters of documents. Only 665-character summaries are given by assessors for each cluster. 6.1 Experiments on Supervised Learning Approach We use LibSVM3 as our classification model for SVM classifiers normally perform better. Types of features presented in previous section are evaluated in"
C08-1124,W01-0501,0,0.00800588,"rted. However, the combination of these features is not well studied. Wu et al. (2007) conducted preliminary research on this problem, but event features were not considered. Normally labeling procedure in supervised learning is very time consuming. Blum and Mitchell (1998) proposed co-training approach to exploit labeled and unlabeled data. Promising results were reported from their experiments on web page classification. A number of successful studies emerged thereafter for other natural language processing tasks, such as text classification (Denis and Gilleron, 2003), noun phrase chunking (Pierce and Cardie, 2001), parsing (Sarkar, 2001) and reference or relation resolution (Muller et al., 2001; Li et al., 2004). To our knowledge, there is little research in the application of co-training techniques to extractive summarization. 3 The Framework for Extractive Summarization Extractive summarization can be regarded as a classification problem. Given the features of a sentence, a machine-learning based classification model will judge how likely the sentence is important. The classification model can be supervised or semi-supervised learning. Supervised approaches normally perform better, but require more l"
C08-1124,radev-etal-2004-mead,0,0.0471075,"Missing"
C08-1124,N01-1023,0,0.00741574,"hese features is not well studied. Wu et al. (2007) conducted preliminary research on this problem, but event features were not considered. Normally labeling procedure in supervised learning is very time consuming. Blum and Mitchell (1998) proposed co-training approach to exploit labeled and unlabeled data. Promising results were reported from their experiments on web page classification. A number of successful studies emerged thereafter for other natural language processing tasks, such as text classification (Denis and Gilleron, 2003), noun phrase chunking (Pierce and Cardie, 2001), parsing (Sarkar, 2001) and reference or relation resolution (Muller et al., 2001; Li et al., 2004). To our knowledge, there is little research in the application of co-training techniques to extractive summarization. 3 The Framework for Extractive Summarization Extractive summarization can be regarded as a classification problem. Given the features of a sentence, a machine-learning based classification model will judge how likely the sentence is important. The classification model can be supervised or semi-supervised learning. Supervised approaches normally perform better, but require more labeled training data. SV"
C08-1124,P06-3007,1,0.728649,"structure is another possible feature for summarization. Barzilay and Elhadad (1997) constructed lexical chains and extracted strong chains in summaries. Marcu (1997) parsed documents as rhetorical trees and identified important sentences based on the trees. However, only moderate results were reported. On the other hand, Dejong (1978) represented documents using predefined templates. The procedure to create and fill the templates was time consuming and it was hard to adapt the method to different domains. Recently, semi-structure events (Filatovia and Hatzivassiloglou, 2004; Li et al., 2006; Wu, 2006) have been investigated by many researchers as they balanced document representation with words and structures. They defined events as verbs (or action nouns) plus the associated named entities. For instance, given the sentence “Yasser Arafat on Tuesday accused the United States of threatening to kill PLO officials”, they first identified “accused”, “threatening” and “kill” as event terms; and “Yasser Arafat”, “United States”, “PLO” and “Tuesday” as event elements. Encouraging results based on events were reported for news stories. From another point of view, sentences in a document are someho"
C08-1124,P97-1013,0,\N,Missing
C08-1124,P02-1045,0,\N,Missing
C10-1016,N06-2046,0,0.0147249,"growth of the Internet and information explosion. It aims to condense the original text into its essential content and to assist in filtering and selection of necessary information. So far extractive summarization that directly extracts sentences from documents to compose summaries is still the mainstream in this field. Under this framework, sentence ranking is the issue of most concern. Though traditional feature-based ranking approaches and graph-based approaches In order to enhance the performance of summarization, recently cluster-based ranking approaches were explored in the literature (Wan and Yang, 2006; Sun et al, 2007; Wang et al, 2008a,b; Qazvinian and Radev, 2008). Normally these approaches applied a clustering algorithm to obtain the theme clusters first and then ranked the sentences within each cluster or by exploring the interaction between sentences and obtained clusters (referring to Figure 1(b)). In other words, clustering and ranking are regarded as two independent processes in these approaches although the cluster-level information has been incorporated into the sentence ranking process. As a result, 134 Proceedings of the 23rd International Conference on Computational Linguistic"
C10-1016,C00-1072,0,0.172423,"Missing"
C10-1016,P09-1083,0,0.0323645,"Missing"
C10-1016,H05-1115,0,0.0241509,"Missing"
C10-1016,C08-1087,0,0.364052,"to condense the original text into its essential content and to assist in filtering and selection of necessary information. So far extractive summarization that directly extracts sentences from documents to compose summaries is still the mainstream in this field. Under this framework, sentence ranking is the issue of most concern. Though traditional feature-based ranking approaches and graph-based approaches In order to enhance the performance of summarization, recently cluster-based ranking approaches were explored in the literature (Wan and Yang, 2006; Sun et al, 2007; Wang et al, 2008a,b; Qazvinian and Radev, 2008). Normally these approaches applied a clustering algorithm to obtain the theme clusters first and then ranked the sentences within each cluster or by exploring the interaction between sentences and obtained clusters (referring to Figure 1(b)). In other words, clustering and ranking are regarded as two independent processes in these approaches although the cluster-level information has been incorporated into the sentence ranking process. As a result, 134 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 134–142, Beijing, August 2010 the ranking p"
C10-2106,A97-1042,0,0.0961085,". The MEAD system performed very well in the generic multi-document summarization task of the DUC 2004 competition. Later, position information is also applied to more summarization tasks. For example, in queryfocused task, sentence position features are widely used in learning-based summarization systems as a component feature for calculating the composite sentence score (Ouyang et al, 2007; Toutanova et al, 2007). However, the effect of position features alone was not studied in these works. There were also studies aimed at analyzing and explaining the effectiveness of position information. Lin and Hovy (1997) provided an empirical validation on the sentence position hypothesis. For each position, the sentence position yield was defined as the average value of the significance of the sentences with the fixed position. It was observed that the average significance at earlier positions was indeed larger. Nenkova (2005) did a conclusive overview on the DUC 2001-2004 evaluation results. It was reported that position information is very effective in generic summarization. In generic single-document summarization, a leadbased baseline that simply takes the leading sentences as the summary can outperform"
C10-2106,P08-2052,0,0.0188063,"position. It was observed that the average significance at earlier positions was indeed larger. Nenkova (2005) did a conclusive overview on the DUC 2001-2004 evaluation results. It was reported that position information is very effective in generic summarization. In generic single-document summarization, a leadbased baseline that simply takes the leading sentences as the summary can outperform most submitted summarization system in DUC 2001 and 2002. As in multi-document summarization, the position-based baseline system is competitive in generating short summaries but not in longer summaries. Schilder and Kondadadi (2008) analyzed the effectiveness of the features that are used in their learning-based sentence scoring model for query-focused summarization. By comparing the ROUGE-2 results of each individual feature, it was reported that position-based features are less effective than frequency-based features. In (Gillick et al., 2009), the effect of position information in the update summarization task was studied. By using ROUGE to measure the density of valuable words at each sentence position, it was observed that the first sentence of newswire document was especially important for composing update summarie"
C10-2170,P05-1018,0,0.0472047,"ith directions to future work. 2 Related Work In MDS, information ordering is often realized on the sentence level and treated as a coherence enhancement task. A simple ordering criterion is the chronological order of the events represented in the sentences, which is often augmented with other ordering criteria such as lexical overlap (Conroy et al., 2006), lexical cohesion (Barzilay et al., 2002) or syntactic features (Lapata 2003). A different way to capture local coherence in sentence ordering is the Centering Theory (CT, Grosz et al. 1995)-inspired entity-transition approach, advocated by Barzilay and Lapata (2005, 2008). In their entity grid model, syntactic roles played by entities and transitions between these syntactic roles underlie the coherence patterns between sentences and in the 1489 Coling 2010: Poster Volume, pages 1489–1497, Beijing, August 2010 whole text. An entity-parsed corpus can be used to train a model that prefers the sentence orderings that comply with the optimal entity transition patterns. Another important clue to sentence ordering is the sentence positional information in a source document, or “precedence relation”, which is utilized by Okazaki et al. (2004) in combination wit"
C10-2170,P03-1069,0,0.427224,"luding layered clustering and cluster-based ordering. The performance of the event-enriched model will be extensively evaluated in section 6. Section 7 will conclude the work with directions to future work. 2 Related Work In MDS, information ordering is often realized on the sentence level and treated as a coherence enhancement task. A simple ordering criterion is the chronological order of the events represented in the sentences, which is often augmented with other ordering criteria such as lexical overlap (Conroy et al., 2006), lexical cohesion (Barzilay et al., 2002) or syntactic features (Lapata 2003). A different way to capture local coherence in sentence ordering is the Centering Theory (CT, Grosz et al. 1995)-inspired entity-transition approach, advocated by Barzilay and Lapata (2005, 2008). In their entity grid model, syntactic roles played by entities and transitions between these syntactic roles underlie the coherence patterns between sentences and in the 1489 Coling 2010: Poster Volume, pages 1489–1497, Beijing, August 2010 whole text. An entity-parsed corpus can be used to train a model that prefers the sentence orderings that comply with the optimal entity transition patterns. Ano"
C10-2170,P06-1047,1,0.90623,"ences by enhancing coherence since incoherence is the source of disorder. Recent researches in this direction mostly focus on local coherence by studying lexical cohesion (Conroy et al., 2006) or entity overlap and transition (Barzilay and Lapata, 2008). But global coherence, i.e., coherence between sentence groups with the whole text in view, is largely unaccounted for and few efforts are made at levels higher than entity or word in measuring sentence coherence. On the other hand, event as a high-level construct has proved useful in MDS content selection (Filatova and Hatzivassiloglou, 2004; Li et al., 2006). But the potential of event in summarization has not been fully gauged and few publications report using event in MDS information ordering. We will argue that event is instrumental for MDS information ordering, especially multi-document news summarization (MDNS). Ordering algorithms based on event and entity information outperform those based only on entity information. After related works are surveyed in section 2, we will discuss in section 3 the problem of semantic deficiency in IR-based text processing, which motivates building event information into sentence representation. The details o"
C10-2170,P07-2047,1,0.832055,"nt), but the extracted event-enriched representations help to alleviate the semantic deficiency problem in IR. 4.2 Event Relations The relations between two events include event term relation and event entity relation. Two events are similar if their event terms are similar and/or their event entities are similar. Such similarities are in turn defined on the word level. For event terms, we first find the root verbs of deverbal nouns and then measure verb similarity by using the fine-grained relations provided by VerbOcean (Chklovski and Pantel, 2004), which has proved useful in summarization (Liu et al., 2007). But unlike (Liu et al., 2007), we count in all the verb relations except antonymy because considering two antonymous verbs as similar is counterintuitive. The other four relations – similarity, strength, enablement, before – are all considered in our measurement of verb similarity. If we denote the normalized score of two verbs on relation i as VOi(V1, V2) with i = 1, 2, 3, 4 corresponding to the above four relations, the term similarity of two events ȝt(E1, E2) is defined as in Eq. 2, where İ is a small number to suppress zeroes. İ = 0.01 if VOi(V1, V2) = 1 and otherwise İ = 0. ȝt(E1, E2) ="
C10-2170,P06-1049,0,0.162047,"rsions, marked by * (p &lt; .05) and ** (p &lt; .01) on a two-tailed t-test. Peer Code A B C D E F G H I 200w Kendall’s Ĳ AC 0.014** 0.009** 0.387 0.151* 0.369* 0.128* 0.380 0.163 0.375* 0.156* 0.388 0.159* 0.385 0.158* 0.384 0.164 0.395 0.170 400w Kendall’s Ĳ AC -0.019** 0.004** 0.259** 0.151* 0.264* 0.156* 0.270* 0.158* 0.267* 0.157* 0.264* 0.157* 0.269* 0.162 0.292* 0.170 0.350 0.176 Table 3. Evaluation Result Table 2. Peer Orderings 6.3 where m is the number of inversions described above and n is the total number of sentences. The second metric we use is the Average Continuity (AC) developed by Bollegala et al. (2006), which captures the intuition that the ordering quality can be estimated by the number of correctly arranged continuous sentences. ଵ σ log(ܲ + ߝ)  = ܥܣexp( (Eq. 17) ିଵ ୀଶ where k is the maximum number of continuous sentences, ߝ is a small value in case Pn = 1. Pn, the proportion of continuous sentences of length n in an ordering, is defined as m/(N – n + 1) where m is the number of continuous sentences of length n in both the test and reference orderings and N is the total number of sentences. We set k = 4 and ߝ = 0.01. Metrics A popular metric used in sequence evaluation is Kendall’s Ĳ"
C10-2170,C04-1108,0,0.654885,", advocated by Barzilay and Lapata (2005, 2008). In their entity grid model, syntactic roles played by entities and transitions between these syntactic roles underlie the coherence patterns between sentences and in the 1489 Coling 2010: Poster Volume, pages 1489–1497, Beijing, August 2010 whole text. An entity-parsed corpus can be used to train a model that prefers the sentence orderings that comply with the optimal entity transition patterns. Another important clue to sentence ordering is the sentence positional information in a source document, or “precedence relation”, which is utilized by Okazaki et al. (2004) in combination with topical clustering. Those works are all relevant to the current work because we seek ordering clues from chronological order, lexical cohesion, entity transition, and sentence precedence. But we also add an important member to the panoply – event. Despite its intuitive and conceptual appeal, event is not as extensively used in summarization as term or entity. Filatova and Hatzivassiloglou (2004) use “atomic events” as conceptual representations in MDS content selection, followed by Li et al. (2006) who treat event terms and named entities as graph nodes in their PageRank a"
C10-2170,W04-3205,0,0.0169161,"admittedly coarse-grade (e.g., “storm” is missing from the “moving” event), but the extracted event-enriched representations help to alleviate the semantic deficiency problem in IR. 4.2 Event Relations The relations between two events include event term relation and event entity relation. Two events are similar if their event terms are similar and/or their event entities are similar. Such similarities are in turn defined on the word level. For event terms, we first find the root verbs of deverbal nouns and then measure verb similarity by using the fine-grained relations provided by VerbOcean (Chklovski and Pantel, 2004), which has proved useful in summarization (Liu et al., 2007). But unlike (Liu et al., 2007), we count in all the verb relations except antonymy because considering two antonymous verbs as similar is counterintuitive. The other four relations – similarity, strength, enablement, before – are all considered in our measurement of verb similarity. If we denote the normalized score of two verbs on relation i as VOi(V1, V2) with i = 1, 2, 3, 4 corresponding to the above four relations, the term similarity of two events ȝt(E1, E2) is defined as in Eq. 2, where İ is a small number to suppress zeroes."
C10-2170,W06-2407,0,0.0155647,"to introduce event. 4 Event-Enriched SentenceRepresentation In summarization, an event is an activity or episode associated with participants, time, place, and manner. Conceptually, event bridges sentence and term/entity and partially fills the semantic gap in the sentence representation. 4.1 Event Structure and Extraction Following (Li et al. 2006), we define an event E as a structured semantic unit consisting of one event term Term(E) and a set of event entities Entity(E). In the news domain, event terms are typically action verbs or deverbal nouns. Light verbs such as “take”, “give”, etc. (Tan et al., 2006) are removed. Event entities include named entities and high-frequency entities. Named entities denote people, locations, organizations, dates, etc. High-frequency entities are common nouns or NPs that frequently participate in news events. Filatova and Hatzivassiloglou (2004) take the top 10 most frequent entities and Li et al. (2006) take the entities with frequency &gt; 10. Rather than using a fixed threshold, we reformulate “high-frequency” as relative statistics based on (assumed) Gaussian distribution of the entities and consider those with z-score &gt; 1 as candidate event entities. Event ext"
C10-2170,W04-1017,0,0.179351,"A sensible solution is ordering sentences by enhancing coherence since incoherence is the source of disorder. Recent researches in this direction mostly focus on local coherence by studying lexical cohesion (Conroy et al., 2006) or entity overlap and transition (Barzilay and Lapata, 2008). But global coherence, i.e., coherence between sentence groups with the whole text in view, is largely unaccounted for and few efforts are made at levels higher than entity or word in measuring sentence coherence. On the other hand, event as a high-level construct has proved useful in MDS content selection (Filatova and Hatzivassiloglou, 2004; Li et al., 2006). But the potential of event in summarization has not been fully gauged and few publications report using event in MDS information ordering. We will argue that event is instrumental for MDS information ordering, especially multi-document news summarization (MDNS). Ordering algorithms based on event and entity information outperform those based only on entity information. After related works are surveyed in section 2, we will discuss in section 3 the problem of semantic deficiency in IR-based text processing, which motivates building event information into sentence representat"
C10-2170,J95-2003,0,0.057497,"xtensively evaluated in section 6. Section 7 will conclude the work with directions to future work. 2 Related Work In MDS, information ordering is often realized on the sentence level and treated as a coherence enhancement task. A simple ordering criterion is the chronological order of the events represented in the sentences, which is often augmented with other ordering criteria such as lexical overlap (Conroy et al., 2006), lexical cohesion (Barzilay et al., 2002) or syntactic features (Lapata 2003). A different way to capture local coherence in sentence ordering is the Centering Theory (CT, Grosz et al. 1995)-inspired entity-transition approach, advocated by Barzilay and Lapata (2005, 2008). In their entity grid model, syntactic roles played by entities and transitions between these syntactic roles underlie the coherence patterns between sentences and in the 1489 Coling 2010: Poster Volume, pages 1489–1497, Beijing, August 2010 whole text. An entity-parsed corpus can be used to train a model that prefers the sentence orderings that comply with the optimal entity transition patterns. Another important clue to sentence ordering is the sentence positional information in a source document, or “precede"
C10-2170,J08-1001,0,\N,Missing
C12-1168,P12-1007,0,0.0222664,"ed relations as training data and concluded that removing discourse markers may lead to a meaning shift in the examples. Sporleder and Lascarides (2008) p ro moted the other research line that used the human annotated training data. The develop ment of various discourse banks also made the u se of human-annotated data feasible. Based on Rhetorical Structure Theory Discourse Treebank (RSTDT) (Carlson et al. 2001), Soricut and Marcu (2003) developed two probabilistic models to identify elementary discourse units and generate discourse trees at the sentence level. Further Hernault et al. (2010); Feng and Hirst (2012) explo re various features for discourse tree building on RST-DT. With the Discourse Graphbank (Wolf and Gibson, 2005), Wellner et al.(2006) integrated mult iple knowledge sources to produce syntactic and lexical semantic features, which were then used to automatically identify and classify exp licit and imp licit d iscourse relations. Especially after the release of the second version of the PDTB v2.0 (Prasad et al., 2008), more research began to take the advantage of the annotated implicit relat ions for training purpose and were dedicated to explo iting various linguistic features in the su"
C12-1168,D09-1036,0,0.2519,"to each typical examp le in the set Ai . Each examp le in the union A0 =∪Bi (1≤i≤n) is labelled as R0 . Then the set of ordered pairs &lt;Ai , Ri > (0≤i≤n) can be used to train an implicit relat ion classifier for labelling Ri (1 ≤ i ≤ n). Both clustering and classification require representing the annotated argument pairs with feature vectors. We introduce the feature selection in subsection 3.1. 3.1 Feature Selection Various linguistic features have been experimented for recognizing imp licit discourse relations in previous studies (Marcu and Echihabi, 2002; Pit ler, Lou is and Nenkova, 2009; Lin et al., 2009). Learning from them, we consider the following 7 types of features. Polarity: The polarity of each s entiment word is tagged as positive, negative or neutral according to Multi-perspective Question Answering Op inion Co rpus (Wilson et al., 2005). Note that the sentiment words preceded by negated words would be assigned an opposite tag. For example, &quot;good&quot; would be assigned as positive while “not good” is negative. Negated neutral is ignored. The occurrence of negative, positive and neutral polarities in each argu ment and their cross product are used as features. Inquirer tags: General Inqui"
C12-1168,P02-1047,0,0.541136,"Missing"
C12-1168,C08-2022,0,0.703215,"Missing"
C12-1168,P09-1077,0,0.564315,"relations. Especially after the release of the second version of the PDTB v2.0 (Prasad et al., 2008), more research began to take the advantage of the annotated implicit relat ions for training purpose and were dedicated to explo iting various linguistic features in the supervised framework (Pitler, Louis and Nenkova, 2009; Lin, Kan and Ng, 2009; Wang, Su and Tan, 2010). L in, Kan and Ng (2009) conducted a thorough performance analysis for four classes of features including contextual relations, constituent parse features, dependency parse features and cross -argument lexical pairs, while Pit ler et al. (2009) applied several linguistically informed features, such as word polarity, verb classes, and word pairs. Wang, Su and Tan (2010) adopted the tree kernel approach to mine more structure informat ion and got better results. These efforts of feature selection have achieved better performance though not that satisfying. The quality of training data are partly responsible for the difficulty of improving the performance of implicit relation recognition . To better recognize the imp licit discourse relations, we propose to review the annotated discourse corpora available at hand, identify and choose t"
C12-1168,prasad-etal-2008-penn,0,0.784226,"gue that an effective train ing se t is co mposed of typical examples, wh ich have distinct characteristics to signify their discourse relations. These typical examples, however, can be either the natively implicit relations or the created imp licit relations with connectives removed fro m the exp licit relat ions. Using the typical examp les as training data, 2758 an implicit relation classifier with higher discrimination power can be built according to the linguistic features in the two arguments. We provide three Comparison relat ion examp les fro m the Penn Discourse TreeBank (PDTB) v2.0 (Prasad et al., 2008) wh ich is widely used in the research of relation recognition as follows to illustrate what the possible typical examples are like. (1) Arg 1: 44 North Koreans oppose the plan, Arg 2: (while) South Koreans, Japanese and Taiwanese accept it or are neutral. (2) Arg 1: In such situations, you cannot write rules in advance. Arg 2: you can only make sure the President takes the responsibility. (3) Arg 1: Columbia Savings is a major holder of so-called junk bonds. Arg 2: New federal leg islation requires that all thrifts divest themselves of such speculative securities over a period of years. Here,"
C12-1168,N06-2034,0,0.0224094,"Missing"
C12-1168,N03-1030,0,0.0878457,"ion. However, Sporleder and Lascarides (2008) d iscovered that the models of Marcu and Echihabi (2002) did not perform well on imp lic it relations recognition with artificially created relations as training data and concluded that removing discourse markers may lead to a meaning shift in the examples. Sporleder and Lascarides (2008) p ro moted the other research line that used the human annotated training data. The develop ment of various discourse banks also made the u se of human-annotated data feasible. Based on Rhetorical Structure Theory Discourse Treebank (RSTDT) (Carlson et al. 2001), Soricut and Marcu (2003) developed two probabilistic models to identify elementary discourse units and generate discourse trees at the sentence level. Further Hernault et al. (2010); Feng and Hirst (2012) explo re various features for discourse tree building on RST-DT. With the Discourse Graphbank (Wolf and Gibson, 2005), Wellner et al.(2006) integrated mult iple knowledge sources to produce syntactic and lexical semantic features, which were then used to automatically identify and classify exp licit and imp licit d iscourse relations. Especially after the release of the second version of the PDTB v2.0 (Prasad et al."
C12-1168,P10-1073,0,0.0778405,"Missing"
C12-1168,H05-1044,0,0.00261514,"lustering and classification require representing the annotated argument pairs with feature vectors. We introduce the feature selection in subsection 3.1. 3.1 Feature Selection Various linguistic features have been experimented for recognizing imp licit discourse relations in previous studies (Marcu and Echihabi, 2002; Pit ler, Lou is and Nenkova, 2009; Lin et al., 2009). Learning from them, we consider the following 7 types of features. Polarity: The polarity of each s entiment word is tagged as positive, negative or neutral according to Multi-perspective Question Answering Op inion Co rpus (Wilson et al., 2005). Note that the sentiment words preceded by negated words would be assigned an opposite tag. For example, &quot;good&quot; would be assigned as positive while “not good” is negative. Negated neutral is ignored. The occurrence of negative, positive and neutral polarities in each argu ment and their cross product are used as features. Inquirer tags: General Inquirer lexicon (Stone et al., 1966) divides each word into fine-g rained semantic categories described by the inquirer tags. Fro m all the categories, we select 21 pairs of complementary categories, such as: Rise versus Fall, or Pleasure versus Pain,"
C12-1168,W06-1317,0,0.26788,"Missing"
C12-1168,P95-1026,0,0.113132,"Missing"
C12-1168,W10-4326,0,0.0401837,"one 2759 presented by Marcu and Echihabi (2002) who applied massive amounts of unannotated explicit relations and lexical features to train the Naïve Bayes classifier for both exp licit and implicit discourse relation recognition. Following the same idea, Saito, Yamamoto and Sekine (2006) conducted the experiments with the co mb ination of cross -argument wo rd pairs and phrasal patterns as features on Japanese sentences. Blair-Go ldensohn (2007) further extended the work of Marcu and Echihabi (2002) by involving syntactic filtering and topic segmentation. Another interesting work is that of Zhou et al. (2010), which predicted discourse connectives between arguments via a language model. Then the generated connectives plus other linguistic features were combined in a supervised framework to determine the implicit discourse relation. However, Sporleder and Lascarides (2008) d iscovered that the models of Marcu and Echihabi (2002) did not perform well on imp lic it relations recognition with artificially created relations as training data and concluded that removing discourse markers may lead to a meaning shift in the examples. Sporleder and Lascarides (2008) p ro moted the other research line that u"
C12-2068,C02-1130,0,0.112702,"Missing"
C12-2068,C08-1034,0,0.0166596,"ks whether the context of entity mention Wi contains the word in the relevant classspecific word set. If context words surrounding Wi hit the word in the class-specific feature set of Cj, the binary feature corresponding to Cj is set to 1. 697 3 3.1 Experiments Experimental Settings We test our approach on UKWAC3( M. Baroni et al., 2009), a 2 billion word English corpora constructed from the Web limiting the crawl to the .uk domain which has been PoS-tagged and lemmatized. The input person instances for each class are the same as used by Giuliano (2009) based on the People Ontology defined by Giuliano and Gliozzo (2008). The ontology extracted from WordNet is arranged in a multi-level taxonomy with 21 fine-grained classes, containing 1,657 distinct person instances. The taxonomy has a maximum depth of 4. We extract all entity mentions together with their contexts in the entire corpus. All the contexts in which NEs occur are randomly partitioned into two equally sized subsets. One is used for training and the other for testing, and vice versa. Like other hierarchical classification tasks, the hypernym classes contain all instances of their hyponym classes when constructing the datasets. For example, Mozart is"
C12-2068,W09-1125,0,0.0795897,"amed entity categories defined by the classic Named Entity Classification (NEC) task are coarse grained, typically PERS, LOC, ORG, MISC. The results obtained from coarse grained NEC are insufficient for complex applications such as Information Retrieval, QuestionAnswering or Ontology Population. Consequently, some researchers turn to address the problem of recognizing and categorizing fine-grained NE classes. Fleischman (2001) presents a preliminary study on the subcategorization of location names, and more recent work focuses on the subcategorization of person names (Fleischman et al., 2002; Giuliano, 2009; Asif Ekbal et al., 2010). Fine-grained NEC (FG-NEC) is a more difficult task than classic NEC, due to the increase in the number of classes and the decrease in the semantic differences between classes. The classic NEC can yield a good classification performance using only simple local context features. While for the FG-NEC, just using these features is far from enough to meet the requirements. Take the following sentence for example, “Dennis Rodman, a close friend of Pippen&apos;s who won three NBA Champions with Jordan&apos;s Bulls, was shocked to hear of Pippen&apos;s comments.”, Based on the context inf"
C12-2068,W10-2415,0,0.0688523,"s defined by the classic Named Entity Classification (NEC) task are coarse grained, typically PERS, LOC, ORG, MISC. The results obtained from coarse grained NEC are insufficient for complex applications such as Information Retrieval, QuestionAnswering or Ontology Population. Consequently, some researchers turn to address the problem of recognizing and categorizing fine-grained NE classes. Fleischman (2001) presents a preliminary study on the subcategorization of location names, and more recent work focuses on the subcategorization of person names (Fleischman et al., 2002; Giuliano, 2009; Asif Ekbal et al., 2010). Fine-grained NEC (FG-NEC) is a more difficult task than classic NEC, due to the increase in the number of classes and the decrease in the semantic differences between classes. The classic NEC can yield a good classification performance using only simple local context features. While for the FG-NEC, just using these features is far from enough to meet the requirements. Take the following sentence for example, “Dennis Rodman, a close friend of Pippen&apos;s who won three NBA Champions with Jordan&apos;s Bulls, was shocked to hear of Pippen&apos;s comments.”, Based on the context information “NBA Champions”,"
C12-2068,P11-1053,0,0.0465885,"Missing"
C12-2068,J92-4003,0,0.444087,"e this cluster-based features into our model. Combining these motivations, we present a method exploiting Multi-features for fine-grained classification of NEs in this paper. The only input data for our algorithm is a few manually annotated entities for each class. In addition to adopting the context word features and the word sense disambiguation features proposed by prior work, this paper puts forward three new features: the cluster-based features, the entity-related features and the class-specific features. 1. Cluster-based features are generated by the Brown clustering algorithm (Peter F. Brown et al., 1992) from a large unlabeled corpus. 2. Entity-related features are context features introduced by other related entities. 3. Class-specific features are words extracted for each class. Each word is given a classspecific score denoting its ability to indicate the relevant class. 694 Our work presented here concentrates on the subcategorization of person names, since the previous researches have indicated that the classification of person names which relies on much more contextual information are often more challenging. The person instances are already identified as entities, and only being classifi"
C12-2068,N04-1043,0,0.0179531,"n a window for each entity mention. Only three individual word tokens and their PoS tags before and after the occurrence of the mention will be added into the feature set. In this paper, a context word and its PoS tag are tied together as an ensemble feature. For an entity mention Wi, its context words will be represented as: fcii33 ( wi 3& posi 3) ( wi 3& posi 3) . 2.2 Cluster-based Features Bag-of-words model cannot deal with synonyms. To address this flaw, some work took advantage of the cluster-based features. The preliminary idea of using word clusters as features was presented by Miller et al. (2004), who augmented name tagging training data with hierarchical word clusters generated by the Brown clustering algorithm (Peter F. Brown et al., 1992) from a large unlabeled corpus. Ang Sun et al. (2011) use the Brown algorithm to generate the word clusters as additional features which are applying to improve the performance of the relation extraction system. They use the English portion of the TDT5 corpora as their unlabeled data for inducing word clusters. The result of this word clusters is a binary tree. A particular word can be assigned a binary string by following the path from the root to"
C12-2068,W02-2002,0,0.0969582,"Missing"
C12-3019,N03-4017,0,0.0609519,"Missing"
C16-1053,P15-2136,1,0.147777,"y useless in the summary because it is unable to answer the query need. Therefore, the surface features are inadequate to measure the query relevance, which further augments the error of the whole summarization system. This drawback partially explains why it might achieve acceptable performance to adopt generic summarization models in the query-focused summarization task (e.g., (Gillick and Favre, 2009)). Intuitively, the isolation problem can be solved with a joint model. Meanwhile, neural networks have shown to generate better representations than surface features in the summarization task (Cao et al., 2015b; Yin and Pei, 2015). Thus, a joint neural network model should be a nice solution to extractive query-focused summarization. To this end, we propose a novel summarization system called AttSum, which joints query relevance ranking and sentence saliency ranking with a neural attention model. The attention mechanism has been successfully applied to learn alignment between various modalities (Chorowski et al., 2014; Xu et al., 2015; Bahdanau et al., 2014). In addition, the work of (Kobayashi et al., 2015) demonstrates that it is reasonably good to use the similarity between the sentence embeddin"
C16-1053,P16-1046,0,0.0295447,"th manual and system summaries for the task of summary evaluation. Their method, however, did not surpass ROUGE. Recently, some works (Cao et al., 2015a; Cao et al., 2015b) have tried to use neural networks to complement sentence ranking features. Although these models achieved the state-of-the-art performance, they still heavily relied on hand-crafted features. A few researches explored to directly measure similarity based on distributed representations. (Yin and Pei, 2015) trained a language model based on convolutional neural networks to project sentences onto distributed representations. (Cheng and Lapata, 2016) treated single document summarization as a sequence labeling task and modeled it by the recurrent neural networks. Others like (Kobayashi et al., 2015; K˚ageb¨ack et al., 2014) just used the sum of trained word embeddings to represent sentences or documents. In addition to extractive summarization, deep learning technologies have also been applied to compressive and abstractive summarization. (Filippova et al., 2015) used word embeddings and Long Short Term Memory models (LSTMs) to output readable and informative sentence compressions. (Rush et al., 2015; Hu et al., 2015) leveraged the neural"
C16-1053,D15-1042,0,0.00650885,"based on distributed representations. (Yin and Pei, 2015) trained a language model based on convolutional neural networks to project sentences onto distributed representations. (Cheng and Lapata, 2016) treated single document summarization as a sequence labeling task and modeled it by the recurrent neural networks. Others like (Kobayashi et al., 2015; K˚ageb¨ack et al., 2014) just used the sum of trained word embeddings to represent sentences or documents. In addition to extractive summarization, deep learning technologies have also been applied to compressive and abstractive summarization. (Filippova et al., 2015) used word embeddings and Long Short Term Memory models (LSTMs) to output readable and informative sentence compressions. (Rush et al., 2015; Hu et al., 2015) leveraged the neural attention model (Bahdanau et al., 2014) in the machine translation area to generate one-sentence summaries. We have described these methods in Section 2.2. 6 Conclusion and Future Work This paper proposes a novel query-focused summarization system called AttSum which jointly handles saliency ranking and relevance ranking. It automatically generates distributed representations for sentences as well as the document clu"
C16-1053,W06-1643,0,0.0186371,"o find the optimal solution (McDonald, 2007; Gillick and Favre, 2009). Graph-based models played a leading role in the extractive summarization area, due to its ability to reflect various sentence relationships. For example, (Wan and Xiao, 2009) adopted manifold ranking to make use of the within-document sentence relationships, the cross-document sentence relationships and the sentence-to-query relationships. In contrast to these unsupervised approaches, there are also various learning-based summarization systems. Different classifiers have been explored, e.g., conditional random field (CRF) (Galley, 2006), Support Vector Regression (SVR) (Ouyang et al., 2011), and Logistic Regression (Li et al., 2013), etc. Many query-focused summarizers are heuristic extensions of generic summarization methods by incorporating the information of the given query. A variety of query-dependent features were defined to measure the relevance, including TF-IDF cosine similarity (Wan and Xiao, 2009), WordNet similarity (Ouyang et al., 2011), and word co-occurrence (Prasad Pingali and Varma, 2007), etc. However, these features usually reward sentences similar to the query, which fail to meet the query need. 5.2 Deep"
C16-1053,W09-1802,0,0.148632,"document cluster embeddings. “⊕” stands for a pooling operation, while “⊗” represents a relevance measurement function. for reference. Apparently, even if a sentence is exactly the same as the query, it is still totally useless in the summary because it is unable to answer the query need. Therefore, the surface features are inadequate to measure the query relevance, which further augments the error of the whole summarization system. This drawback partially explains why it might achieve acceptable performance to adopt generic summarization models in the query-focused summarization task (e.g., (Gillick and Favre, 2009)). Intuitively, the isolation problem can be solved with a joint model. Meanwhile, neural networks have shown to generate better representations than surface features in the summarization task (Cao et al., 2015b; Yin and Pei, 2015). Thus, a joint neural network model should be a nice solution to extractive query-focused summarization. To this end, we propose a novel summarization system called AttSum, which joints query relevance ranking and sentence saliency ranking with a neural attention model. The attention mechanism has been successfully applied to learn alignment between various modaliti"
C16-1053,D15-1229,0,0.108359,"and, if a sentence is salient in the document cluster, its embedding should be representative. As a result, the weighted-sum pooling generates the document representation which is automatically biased to embeddings of sentences match both documents and the query. AttSum simulates human attentive reading behavior, and the attention mechanism in it has actual meaning. The experiments to be presented in Section 4.6 will demonstrate its strong ability to catch query relevant sentences. Actually, the attention mechanism has been applied in one-sentence summary generation before (Rush et al., 2015; Hu et al., 2015). The success of these works, however, heavily depends on the hand-crafted features. We believe that the attention mechanism may not be able to play its anticipated role if it is not used appropriately. 2.3 Ranking Layer Since the semantics directly lies in sentence and document embeddings, we rank a sentence according to its embedding similarity to the document cluster, following the work of (Kobayashi et al., 2015). Here we adopt cosine similarity: v(s) • v(d|q)T cos(d, s|q) = (5) ||v(s) ||• ||v(d|q)|| Compared with Euclidean distance, one advantage of cosine similarity is that it is automat"
C16-1053,W14-1504,0,0.158097,"Missing"
C16-1053,D15-1232,0,0.0981039,"rks have shown to generate better representations than surface features in the summarization task (Cao et al., 2015b; Yin and Pei, 2015). Thus, a joint neural network model should be a nice solution to extractive query-focused summarization. To this end, we propose a novel summarization system called AttSum, which joints query relevance ranking and sentence saliency ranking with a neural attention model. The attention mechanism has been successfully applied to learn alignment between various modalities (Chorowski et al., 2014; Xu et al., 2015; Bahdanau et al., 2014). In addition, the work of (Kobayashi et al., 2015) demonstrates that it is reasonably good to use the similarity between the sentence embedding and document embedding for saliency measurement, where the document embedding is derived from the sum pooling of sentence embeddings. In order to consider the relevance and saliency simultaneously, we introduce the weighted-sum pooling over sentence embeddings to represent the document, where the weight is the automatically learned query relevance of a sentence. In this way, the document representation will be biased to the sentence embeddings which match the meaning of both query and documents. The w"
C16-1053,P13-1099,0,0.185236,"Introduction Query-focused summarization (Dang, 2005) aims to create a brief, well-organized and fluent summary that answers the need of the query. It is useful in many scenarios like news services and search engines, etc. Nowadays, most summarization systems are under the extractive framework which directly selects existing sentences to form the summary. Basically, there are two major tasks in extractive query-focused summarization, i.e., to measure the saliency of a sentence and its relevance to a user’s query. After a long period of research, learning-based models like Logistic Regression (Li et al., 2013) etc. have become growingly popular in this area. However, most current supervised summarization systems often perform the two tasks in isolation. Usually, they design query-dependent features (e.g., query word overlap) to learn the relevance ranking, and query-independent features (e.g., term frequency) to learn the saliency ranking. Then, the two types of features are combined to train an overall ranking model. Note that the only supervision available is the reference summaries. Humans write summaries with the trade-off between relevance and saliency. Some salient content may not appear in r"
C16-1053,W04-1013,0,0.0254933,"according to its embedding similarity to the document cluster, following the work of (Kobayashi et al., 2015). Here we adopt cosine similarity: v(s) • v(d|q)T cos(d, s|q) = (5) ||v(s) ||• ||v(d|q)|| Compared with Euclidean distance, one advantage of cosine similarity is that it is automatically scaled. According to (K˚ageb¨ack et al., 2014), cosine similarity is the best metrics to measure the embedding similarity for summarization. In the training process, we apply the pairwise ranking strategy (Collobert et al., 2011) to tune model parameters. Specifically, we calculate the ROUGE-2 scores (Lin, 2004) of all the sentences in the training dataset. Those sentences with high ROUGE-2 scores are regarded as positive samples, and the rest as negative samples. Afterwards, we randomly choose a pair of positive and negative sentences which are denoted as s+ and s− , respectively. Through the CNN Layer and Pooling Layer, we generate the embeddings of v(s+ ), v(s− ) and v(d|q). We can then obtain the ranking scores of s+ and s− according to Eq. 5. With the pairwise ranking criterion, AttSum should give a positive sample a higher score in comparison with a negative sample. The cost function is defined"
C16-1053,W12-2601,0,0.0619964,"= 50 for all the rest experiments. It is the same dimension as the word embeddings. During the training of pairwise ranking, we set the margin Ω = 0.5. The initial learning rate is 0.1 and batch size is 100. 4.3 Evaluation Metric For evaluation, we adopt the widely-used automatic evaluation metric ROUGE (Lin, 2004) 3 . It measures the summary quality by counting the overlapping units such as the n-grams, word sequences and word pairs between the peer summary and reference summaries. We take ROUGE-2 as the main measures due to its high capability of evaluating automatic summarization systems (Owczarzak et al., 2012). During the training data of pairwise ranking, we also rank the sentences according to ROUGE-2 scores. 4.4 Baselines To evaluate the summarization performance of AttSum, we implement rich extractive summarization methods. Above all, we introduce two common baselines. The first one just selects the leading sentences to form a summary. It is often used as an official baseline of DUC, and we name it “LEAD”. The other system is called “QUERY SIM”, which directly ranks sentences according to its TF-IDF cosine similarity to the query. In addition, we implement two popular extractive query-focused s"
C16-1053,D15-1044,0,0.141032,"rge. On the other hand, if a sentence is salient in the document cluster, its embedding should be representative. As a result, the weighted-sum pooling generates the document representation which is automatically biased to embeddings of sentences match both documents and the query. AttSum simulates human attentive reading behavior, and the attention mechanism in it has actual meaning. The experiments to be presented in Section 4.6 will demonstrate its strong ability to catch query relevant sentences. Actually, the attention mechanism has been applied in one-sentence summary generation before (Rush et al., 2015; Hu et al., 2015). The success of these works, however, heavily depends on the hand-crafted features. We believe that the attention mechanism may not be able to play its anticipated role if it is not used appropriately. 2.3 Ranking Layer Since the semantics directly lies in sentence and document embeddings, we rank a sentence according to its embedding similarity to the document cluster, following the work of (Kobayashi et al., 2015). Here we adopt cosine similarity: v(s) • v(d|q)T cos(d, s|q) = (5) ||v(s) ||• ||v(d|q)|| Compared with Euclidean distance, one advantage of cosine similarity is"
C16-1208,D14-1179,0,0.0178585,"Missing"
C16-1208,C12-1113,0,0.0540984,"Missing"
C16-1208,P14-1146,0,0.0457107,"n the DeGroot model, which first 2208 introduces the negative influence and proves the effectiveness of the proposed model on the social media dataset. However, existing models fail to consider the effects of content information on the opinion formation problem. Our work is the first try to integrate semantic information into opinion behavior modeling. 2.2 Neural Network in NLP Tasks Recently, neural network has received great achievements in Natural Language Processing tasks, such as language modeling (Bengio et al., 2003), machine translation (Cho et al., 2014) and sentiment classification (Tang et al., 2014). One of the most useful neural network techniques for NLP is the word embedding, which learns vector representations of words (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013). The neural language model proposed by Bengio et al., (Bengio et al., 2003) uses the concatenation of several previous words (context) as the input of the feed-forward neural network, and then the encoded context vector is used to predict the next word (target word). Following the word embedding techniques, several models are extended to achieve the phrase-level and sentence-level representations b"
C16-1208,C10-1142,0,0.0238881,"03; Collobert and Weston, 2008; Mikolov et al., 2013). The neural language model proposed by Bengio et al., (Bengio et al., 2003) uses the concatenation of several previous words (context) as the input of the feed-forward neural network, and then the encoded context vector is used to predict the next word (target word). Following the word embedding techniques, several models are extended to achieve the phrase-level and sentence-level representations by composing all vectors of words in the phrase or sentence together. The basic composition method is using weighted average of all word vectors (Zanzotto et al., 2010; Mikolov et al., 2013). In (Mikolov et al., 2013), they use a simple data-driven approach, where phrases are formed based on the unigram and bigram counts of the words. Furthermore, considering the syntactic structure of the phrases or sentences, a method combining the words by their orders in the syntactic tree is proposed (Socher et al., 2011). The proposed content-based social influence model bears similarities with the neural language model. In the opinion formation tasks, we regard the neighboring opinions and one’s previous opinion as the ”contexts”, and the ”target” is one’s future opi"
chen-etal-2006-study,J90-1003,0,\N,Missing
chen-etal-2006-study,W03-1725,0,\N,Missing
chen-etal-2006-study,W02-1407,0,\N,Missing
chen-etal-2006-study,C02-1125,0,\N,Missing
chen-etal-2006-study,I05-3009,0,\N,Missing
chen-etal-2008-chinese,J04-2002,0,\N,Missing
chen-etal-2008-chinese,huang-etal-2004-sinica,0,\N,Missing
chen-etal-2008-chinese,I08-7003,1,\N,Missing
cui-etal-2008-corpus,P98-1013,0,\N,Missing
cui-etal-2008-corpus,C98-1013,0,\N,Missing
D15-1098,W13-3512,0,0.154906,"Missing"
D15-1098,P14-2131,0,0.0662531,"Missing"
D15-1098,D14-1082,0,0.0976709,"Missing"
D15-1098,cheng-etal-2014-parsing,0,0.0275981,"Missing"
D15-1098,C14-1015,0,0.0585293,"Missing"
D15-1098,P14-1146,0,0.0260875,"Missing"
D15-1098,P12-1092,0,0.014382,"work, we innovatively develop two component-enhanced Chinese character embedding models and their bigram extensions. Distinguished from English word embeddings, our models explore the compositions of Chinese characters, which often serve as semantic indictors inherently. The evaluations on both word similarity and text classification demonstrate the effectiveness of our models. 1 Introduction Due to its advantage over traditional one-hot representation, distributed word representation has demonstrated its benefit for semantic representation in various NLP tasks. Among the existing approaches (Huang et al, 2012; Levy and Goldberg, 2014; Yang and Eisenstein, 2015), the continuous bag-of-words model (CBOW) and the continuous skip-gram model (SkipGram) remain the most popular ones that one can use to build word embeddings efficiently (Mikolov et al, 2013a; Mikolov et al, 2013b). These two models learn the distributed representation of a word based on its context. The context defined by the window of surrounding words may unavoidably include certain less semantically-relevant words and/or miss the words with important and relevant meanings (Levy and Goldberg, 2014). To overcome this shortcoming, a line"
D15-1098,N15-1142,0,0.0132362,"Missing"
D15-1098,D14-1108,0,0.0310523,"Missing"
D15-1098,D13-1136,0,0.0522196,"Missing"
D15-1098,P14-2050,0,0.111025,"ly develop two component-enhanced Chinese character embedding models and their bigram extensions. Distinguished from English word embeddings, our models explore the compositions of Chinese characters, which often serve as semantic indictors inherently. The evaluations on both word similarity and text classification demonstrate the effectiveness of our models. 1 Introduction Due to its advantage over traditional one-hot representation, distributed word representation has demonstrated its benefit for semantic representation in various NLP tasks. Among the existing approaches (Huang et al, 2012; Levy and Goldberg, 2014; Yang and Eisenstein, 2015), the continuous bag-of-words model (CBOW) and the continuous skip-gram model (SkipGram) remain the most popular ones that one can use to build word embeddings efficiently (Mikolov et al, 2013a; Mikolov et al, 2013b). These two models learn the distributed representation of a word based on its context. The context defined by the window of surrounding words may unavoidably include certain less semantically-relevant words and/or miss the words with important and relevant meanings (Levy and Goldberg, 2014). To overcome this shortcoming, a line of research deploys the o"
D15-1098,N15-1069,0,0.0148434,"enhanced Chinese character embedding models and their bigram extensions. Distinguished from English word embeddings, our models explore the compositions of Chinese characters, which often serve as semantic indictors inherently. The evaluations on both word similarity and text classification demonstrate the effectiveness of our models. 1 Introduction Due to its advantage over traditional one-hot representation, distributed word representation has demonstrated its benefit for semantic representation in various NLP tasks. Among the existing approaches (Huang et al, 2012; Levy and Goldberg, 2014; Yang and Eisenstein, 2015), the continuous bag-of-words model (CBOW) and the continuous skip-gram model (SkipGram) remain the most popular ones that one can use to build word embeddings efficiently (Mikolov et al, 2013a; Mikolov et al, 2013b). These two models learn the distributed representation of a word based on its context. The context defined by the window of surrounding words may unavoidably include certain less semantically-relevant words and/or miss the words with important and relevant meanings (Levy and Goldberg, 2014). To overcome this shortcoming, a line of research deploys the order information of the word"
D15-1098,Q15-1016,0,0.0671812,"Missing"
D15-1098,P14-2089,0,0.0295662,"Missing"
D15-1098,P14-1140,0,0.035612,"Missing"
D15-1098,P13-1013,0,0.0493386,"Missing"
D15-1305,P12-2018,0,0.0697402,"Missing"
D15-1305,P12-1092,0,0.0376354,"einforce topical and sentimental information. TPD has a theoretical guarantee that the word dependency is pure, i.e., the dependence pattern has the integral meaning whose underlying distribution can not be conditionally factorized. Our method outperforms the state-of-the-art performance on text classification tasks. 1 Introduction Word embeddings can be learned by training a neural probabilistic language model or a unified neural network architecture for various NLP tasks (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011). In global context-aware neural language model (Huang et al., 2012), the global context vector is a weighted average of all word embeddings of a single document/paragraph. After trained with all word embeddings belonging to the current paragraph, a resulting Paragraph Vector can be obtained. Actually, Le and Mikolov’s Paragraph Vector (Le and Mikolov, 2014) is trained based on the log-linear neural language model (Mikolov et al., 2013a). For text classification, using a straightforward extension of language model (e.g. Le and Mikolov’s Paragraph Vector) is considered not to be sensible. Embeddings learned for text classification should be very different from"
D15-1305,P11-1015,0,\N,Missing
D18-1354,K16-1002,0,0.244028,"al., 2016b; Xing et al., 2017; Zhou et al., 2017b) and modifying the architecture of existing models (Li et al., 2016a; Xu et al., 2017; Zhou et al., 2017a). Another solution to address this problem is to add stochastic latent variables in order to change the deterministic structure of Seq2Seq models. VAE (Kingma and Welling, 2013) is one of the most successful models (Serban et al., 2017; Zhao et al., 2017; Shen et al., 2017; Cao and Clark, 2017). However, VAE-based models only use a single latent variable to encode the whole response sequence, thus suffering from the model collapse problem (Bowman et al., 2016). To overcome this problem, we propose a novel model that based on the variational autoregressive decoder to better represent highly structural latent variables. 2.2 Variational Autoregressive Models Recently, some works attempted to combine VAE with autoregressive models to better process input sequences. Broadly speaking, they can be categorized into two groups. Methods in the first group leverage autoregressive models to improve the inference of traditional VAEs. The most well-known model is Inverse Autoregressive Flow (IAF), which used a series of invertible transformations based on the au"
D18-1354,E17-2029,0,0.0370672,"eneric responses, such as I don’t know (Li et al., 2016a). Various approaches have been proposed to address this problem, including adding additional information (Li et al., 2016b; Xing et al., 2017; Zhou et al., 2017b) and modifying the architecture of existing models (Li et al., 2016a; Xu et al., 2017; Zhou et al., 2017a). Another solution to address this problem is to add stochastic latent variables in order to change the deterministic structure of Seq2Seq models. VAE (Kingma and Welling, 2013) is one of the most successful models (Serban et al., 2017; Zhao et al., 2017; Shen et al., 2017; Cao and Clark, 2017). However, VAE-based models only use a single latent variable to encode the whole response sequence, thus suffering from the model collapse problem (Bowman et al., 2016). To overcome this problem, we propose a novel model that based on the variational autoregressive decoder to better represent highly structural latent variables. 2.2 Variational Autoregressive Models Recently, some works attempted to combine VAE with autoregressive models to better process input sequences. Broadly speaking, they can be categorized into two groups. Methods in the first group leverage autoregressive models to imp"
D18-1354,N16-1014,0,0.0894923,"associating latent variables to different time steps of autoregressive decoder and approximating the posterior of latent variables by augmenting the hidden states of a backward RNN. • A BOW based auxiliary objective is proposed to help preserving the diversity of generated responses. 2 2.1 Related Work Conversational Systems As neural network based models dominate the research in natural language processing, Seq2Seq models have been widely used for response generation (Sordoni et al., 2015). However, Seq2seq models suffer from the problem of generating generic responses, such as I don’t know (Li et al., 2016a). Various approaches have been proposed to address this problem, including adding additional information (Li et al., 2016b; Xing et al., 2017; Zhou et al., 2017b) and modifying the architecture of existing models (Li et al., 2016a; Xu et al., 2017; Zhou et al., 2017a). Another solution to address this problem is to add stochastic latent variables in order to change the deterministic structure of Seq2Seq models. VAE (Kingma and Welling, 2013) is one of the most successful models (Serban et al., 2017; Zhao et al., 2017; Shen et al., 2017; Cao and Clark, 2017). However, VAE-based models only us"
D18-1354,P16-1094,0,0.131353,"associating latent variables to different time steps of autoregressive decoder and approximating the posterior of latent variables by augmenting the hidden states of a backward RNN. • A BOW based auxiliary objective is proposed to help preserving the diversity of generated responses. 2 2.1 Related Work Conversational Systems As neural network based models dominate the research in natural language processing, Seq2Seq models have been widely used for response generation (Sordoni et al., 2015). However, Seq2seq models suffer from the problem of generating generic responses, such as I don’t know (Li et al., 2016a). Various approaches have been proposed to address this problem, including adding additional information (Li et al., 2016b; Xing et al., 2017; Zhou et al., 2017b) and modifying the architecture of existing models (Li et al., 2016a; Xu et al., 2017; Zhou et al., 2017a). Another solution to address this problem is to add stochastic latent variables in order to change the deterministic structure of Seq2Seq models. VAE (Kingma and Welling, 2013) is one of the most successful models (Serban et al., 2017; Zhao et al., 2017; Shen et al., 2017; Cao and Clark, 2017). However, VAE-based models only us"
D18-1354,D17-1222,1,0.881935,"Missing"
D18-1354,D16-1230,0,0.071479,"Missing"
D18-1354,N15-1020,0,0.0998218,"Missing"
D18-1354,D17-1065,0,0.0351809,"e diversity of generated responses. 2 2.1 Related Work Conversational Systems As neural network based models dominate the research in natural language processing, Seq2Seq models have been widely used for response generation (Sordoni et al., 2015). However, Seq2seq models suffer from the problem of generating generic responses, such as I don’t know (Li et al., 2016a). Various approaches have been proposed to address this problem, including adding additional information (Li et al., 2016b; Xing et al., 2017; Zhou et al., 2017b) and modifying the architecture of existing models (Li et al., 2016a; Xu et al., 2017; Zhou et al., 2017a). Another solution to address this problem is to add stochastic latent variables in order to change the deterministic structure of Seq2Seq models. VAE (Kingma and Welling, 2013) is one of the most successful models (Serban et al., 2017; Zhao et al., 2017; Shen et al., 2017; Cao and Clark, 2017). However, VAE-based models only use a single latent variable to encode the whole response sequence, thus suffering from the model collapse problem (Bowman et al., 2016). To overcome this problem, we propose a novel model that based on the variational autoregressive decoder to better"
D18-1354,P17-1061,0,0.404009,"te-of-the-art baselines. 1 Figure 1: Distributions of latent variable Introduction Recently, variational Bayesian models have shown attractive merits from both theoretical and practical perspectives (Kingma and Welling, 2013). As one of the most successful variational Bayesian models, Conditional Variational Auto-Encoder (CVAE) (Kingma et al., 2014) was proposed to improve upon the traditional Sequence-to-Sequence (Seq2Seq) dialogue models. The CVAE based models incorporate stochastic latent variables into decoders in order to generate more relevant and diverse responses (Serban et al., 2017; Zhao et al., 2017; Shen et al., 2017). However, existing CVAE ∗ Corresponding author As illustrated in Figure 1, the unimodal latent variable z used in the conventional VAE usually captures simple unimodal pattern of responses. However, in open-domain conversations, an utterance may have various responses which form complex multimodal distributions. To overcome this problem and improve the quality of generated responses, we propose a novel model, named Variational Autoregressive Decoder (VAD) to iteratively incorporate a series of latent variables into the autoregressive decoder. In particular, a distinct late"
D18-1463,K16-1002,0,0.324347,"I|c) pφ (Fi+1 Fi+1 i+1 (3) ˜ ˜ where Hi−1 and Fi+1 are also assumed to be Gaussian distributed given c with mean and covariance estimated from multi-layer perceptrons. We infer the encoded vectors instead of the original sequences for three reasons. Firstly, inferring dense vectors is parallelizable and computationally much cheaper than autoregressive decoding, especially when the context sequences could be unlimitedly long. Secondly, sequence vectors can capture more holistic semantic-level similarity than individual tokens. Lastly, It can also help alleviate the posterior collapsing issue (Bowman et al., 2016) when training variational inference models on text (Chen et al., 2017; Shen et al., 2018), which we will use later. It can be shown that the above objective maximizes a lower bound of λ1 I(Hi−1 , c) + λ2 I(c, Fi+1 ), given the conditional probability pφ (c|Hi−1 , Fi ). The proof is a direct extension of the derivation in (Chen et al., 2016), followed by the Data Processing Inequality (Beaudry and Renner, 2012) that the encoding function can only reduce the mutual information. As the sampling process contains only Gaussian continuous variables, the above objective can be trained through the re"
D18-1463,N18-2081,0,0.0162528,"ence on Empirical Methods in Natural Language Processing, pages 4316–4327 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics with the surrounding context. To enable efficient training, two challenges exist. The first challenge comes from the discrete nature of language tokens, hindering efficient gradient descent. One strategy is to estimate the gradient by methods like Gumbel-Softmax (Maddison et al., 2017; Jang et al., 2017) or REINFORCE algorithm (Williams, 1992), which has been applied in many NLP tasks (He et al., 2016; Shetty et al., 2017; Gu et al., 2018; Paulus et al., 2018), but the trade-off between bias and variance of the estimated gradient is hard to reconcile. The resulting model usually strongly relies on sensitive hyper-parameter tuning, careful pre-train and taskspecific tricks. Li et al. (2016a); Wang et al. (2017) avoid this non-differentiability problem by learning a separate backward model to rerank candidate responses in the testing phase while still adhering to the MLE objective for training. However, the candidate set normally suffers from low diversity and a huge sample size is needed for good performance (Li et al., 2016b)."
D18-1463,W04-3250,0,0.029811,"Missing"
D18-1463,N16-1014,0,0.293431,"we introduce an auxiliary continuous code space and map such code space to a learnable prior distribution for generation purpose. Experiments on two dialogue datasets validate the effectiveness of our model, where the generated responses are closely related to the dialogue context and lead to more interactive conversations. 1 Figure 1: A conversation in real life suitable for modeling dialogues. Recent research has found that while the seq2seq model generates syntactically well-formed responses, they are prone to being off-context, short, and generic. (e.g., “I dont know” or “I am not sure”) (Li et al., 2016a; Serban et al., 2016). The reason lies in the one-to-many alignments in human conversations, where one dialogue context is open to multiple potential responses. When optimizing with the MLE objective, the model tends to have a strong bias towards safe responses as they can be literally paired with arbitrary dialogue context without semantical or grammatical contradictions. These safe responses break the dialogue flow without bringing any useful information and people will easily lose interest in continuing the conversation. Introduction With the availability of massive online conversational"
D18-1463,D16-1127,0,0.35394,"we introduce an auxiliary continuous code space and map such code space to a learnable prior distribution for generation purpose. Experiments on two dialogue datasets validate the effectiveness of our model, where the generated responses are closely related to the dialogue context and lead to more interactive conversations. 1 Figure 1: A conversation in real life suitable for modeling dialogues. Recent research has found that while the seq2seq model generates syntactically well-formed responses, they are prone to being off-context, short, and generic. (e.g., “I dont know” or “I am not sure”) (Li et al., 2016a; Serban et al., 2016). The reason lies in the one-to-many alignments in human conversations, where one dialogue context is open to multiple potential responses. When optimizing with the MLE objective, the model tends to have a strong bias towards safe responses as they can be literally paired with arbitrary dialogue context without semantical or grammatical contradictions. These safe responses break the dialogue flow without bringing any useful information and people will easily lose interest in continuing the conversation. Introduction With the availability of massive online conversational"
D18-1463,D17-1230,0,0.403999,"ned and the training stage is unstable due to the huge search space. In contrast, our model maximizes the mutual information in the continuous space and trains the prior distribution through the reparamaterization trick. As a result, our model can be more easily trained with a lower variance. Throughout our experiment, the training process of NEXUS network is rather stable and much less data-hungry. The MMI objective of our model is theoretically more sound and no manually-defined rules need to be specified. 4 4.1 Experiments Dataset and Training Details We run experiments on the DailyDialog (Li et al., 2017b) and Twitter corpus (Ritter et al., 2011). DailyDialog contains 13118 daily conversations under ten different topics. This dataset is crawled from various websites for English learner to practice English in daily life, which is high-quality, less noisy but relatively smaller. In contrast, the Twitter corpus is significantly larger but contains more noise. We obtain the dataset as used in Serban et al. (2017) and filter out tweets that have already been deleted, resulting in about 750,000 multi-turn dialogues. The contents have more informal, colloquial expressions which makes the generation"
D18-1463,I17-1099,1,0.939152,"ned and the training stage is unstable due to the huge search space. In contrast, our model maximizes the mutual information in the continuous space and trains the prior distribution through the reparamaterization trick. As a result, our model can be more easily trained with a lower variance. Throughout our experiment, the training process of NEXUS network is rather stable and much less data-hungry. The MMI objective of our model is theoretically more sound and no manually-defined rules need to be specified. 4 4.1 Experiments Dataset and Training Details We run experiments on the DailyDialog (Li et al., 2017b) and Twitter corpus (Ritter et al., 2011). DailyDialog contains 13118 daily conversations under ten different topics. This dataset is crawled from various websites for English learner to practice English in daily life, which is high-quality, less noisy but relatively smaller. In contrast, the Twitter corpus is significantly larger but contains more noise. We obtain the dataset as used in Serban et al. (2017) and filter out tweets that have already been deleted, resulting in about 750,000 multi-turn dialogues. The contents have more informal, colloquial expressions which makes the generation"
D18-1463,W05-0908,0,0.0713008,"Missing"
D18-1463,C04-1072,0,0.0453311,"Missing"
D18-1463,D11-1054,0,0.0393555,"due to the huge search space. In contrast, our model maximizes the mutual information in the continuous space and trains the prior distribution through the reparamaterization trick. As a result, our model can be more easily trained with a lower variance. Throughout our experiment, the training process of NEXUS network is rather stable and much less data-hungry. The MMI objective of our model is theoretically more sound and no manually-defined rules need to be specified. 4 4.1 Experiments Dataset and Training Details We run experiments on the DailyDialog (Li et al., 2017b) and Twitter corpus (Ritter et al., 2011). DailyDialog contains 13118 daily conversations under ten different topics. This dataset is crawled from various websites for English learner to practice English in daily life, which is high-quality, less noisy but relatively smaller. In contrast, the Twitter corpus is significantly larger but contains more noise. We obtain the dataset as used in Serban et al. (2017) and filter out tweets that have already been deleted, resulting in about 750,000 multi-turn dialogues. The contents have more informal, colloquial expressions which makes the generation task harder. These two datasets are randoml"
D18-1463,D16-1230,0,0.223757,"Missing"
D18-1463,P02-1040,0,0.101349,"Missing"
D18-1463,W12-2018,0,0.0743031,"Missing"
D18-1463,N15-1020,0,0.058439,"Missing"
D18-1463,D17-1228,0,0.0630013,"Missing"
D18-1463,P17-1061,0,0.462004,"., 2016b). The second challenge relates to the unknown future context in the testing phase. In our framework, both the history and future context need to be explicitly observed in order to compute the mutual information. When applying it to generating tasks where only the history context is given, there is no way to explicitly take into account the future information. Therefore, reranking-based models do not apply here. (Li et al., 2016c) addresses future information by policy learning, but the model suffers from high variance due to the enormous sequential search space. Serban et al. (2017); Zhao et al. (2017); Shen et al. (2017) adopt the variational inference strategy to reduce the training variance by optimizing over latent continuous variables. However, they all stick to the original MLE objective and no connection with the surrounding context is considered. In this work, we address both challenges by introducing an auxiliary continuous code space which is learned from the whole dialogue flow. At each time step, instead of directly optimizing discrete utterances, the current, past and future utterances are all trained to maximize the mutual information with this code space. Furthermore, a learn"
I05-1037,C02-1150,0,0.0766252,"Missing"
I05-1037,A00-1023,1,\N,Missing
I05-1037,W01-1203,0,\N,Missing
I05-1037,N01-1005,0,\N,Missing
I05-1037,W03-1208,0,\N,Missing
I05-1037,P04-1072,0,\N,Missing
I05-1037,H01-1069,0,\N,Missing
I05-1037,A00-1041,0,\N,Missing
I05-1061,W01-1313,0,0.0207662,"ed by the potential applications, temporal information processing has absorbed more attention recently than ever, such as ACL 2001 workshop on temporal and spatial information processing, LREC 2002 and TERN 2004 [14]. Mani [10] gives a good review about the recent trend. Research works in this area can be classified into four types: designing annotation scheme for temporal information representation [4, 6, 12]; developing temporal ontology which covers temporal objects and their relationships between each other [2, 7]; Identifying time-stamps of events or temporal relationships between events [5, 9]; Identifying and normalizing temporal expressions from different languages [1, 3, 8, 11, 13, 15]. Temporal annotation, temporal ontology and temporal reasoning are not the focuses in this paper. Among the research works on temporal expression extraction and normalization, most of them are based on hand-written rules or machine-learnt rules. Mani and Wilson [11] resolve temporal expressions by hand-crafted and machinelearnt rules. Their focus is resolving temporal expressions, especially indexical expressions, which designate times that are dependent on the speaker and some reference time. We"
I05-1061,W01-1305,1,0.823148,"ed by the potential applications, temporal information processing has absorbed more attention recently than ever, such as ACL 2001 workshop on temporal and spatial information processing, LREC 2002 and TERN 2004 [14]. Mani [10] gives a good review about the recent trend. Research works in this area can be classified into four types: designing annotation scheme for temporal information representation [4, 6, 12]; developing temporal ontology which covers temporal objects and their relationships between each other [2, 7]; Identifying time-stamps of events or temporal relationships between events [5, 9]; Identifying and normalizing temporal expressions from different languages [1, 3, 8, 11, 13, 15]. Temporal annotation, temporal ontology and temporal reasoning are not the focuses in this paper. Among the research works on temporal expression extraction and normalization, most of them are based on hand-written rules or machine-learnt rules. Mani and Wilson [11] resolve temporal expressions by hand-crafted and machinelearnt rules. Their focus is resolving temporal expressions, especially indexical expressions, which designate times that are dependent on the speaker and some reference time. We"
I05-1061,P00-1010,0,0.0532695,"e attention recently than ever, such as ACL 2001 workshop on temporal and spatial information processing, LREC 2002 and TERN 2004 [14]. Mani [10] gives a good review about the recent trend. Research works in this area can be classified into four types: designing annotation scheme for temporal information representation [4, 6, 12]; developing temporal ontology which covers temporal objects and their relationships between each other [2, 7]; Identifying time-stamps of events or temporal relationships between events [5, 9]; Identifying and normalizing temporal expressions from different languages [1, 3, 8, 11, 13, 15]. Temporal annotation, temporal ontology and temporal reasoning are not the focuses in this paper. Among the research works on temporal expression extraction and normalization, most of them are based on hand-written rules or machine-learnt rules. Mani and Wilson [11] resolve temporal expressions by hand-crafted and machinelearnt rules. Their focus is resolving temporal expressions, especially indexical expressions, which designate times that are dependent on the speaker and some reference time. We concentrate on the procedure of extraction and normalization, and try to cover more temporal expr"
I05-1061,W01-1309,0,0.0850976,"e attention recently than ever, such as ACL 2001 workshop on temporal and spatial information processing, LREC 2002 and TERN 2004 [14]. Mani [10] gives a good review about the recent trend. Research works in this area can be classified into four types: designing annotation scheme for temporal information representation [4, 6, 12]; developing temporal ontology which covers temporal objects and their relationships between each other [2, 7]; Identifying time-stamps of events or temporal relationships between events [5, 9]; Identifying and normalizing temporal expressions from different languages [1, 3, 8, 11, 13, 15]. Temporal annotation, temporal ontology and temporal reasoning are not the focuses in this paper. Among the research works on temporal expression extraction and normalization, most of them are based on hand-written rules or machine-learnt rules. Mani and Wilson [11] resolve temporal expressions by hand-crafted and machinelearnt rules. Their focus is resolving temporal expressions, especially indexical expressions, which designate times that are dependent on the speaker and some reference time. We concentrate on the procedure of extraction and normalization, and try to cover more temporal expr"
I05-1061,W01-1314,0,0.162426,"e attention recently than ever, such as ACL 2001 workshop on temporal and spatial information processing, LREC 2002 and TERN 2004 [14]. Mani [10] gives a good review about the recent trend. Research works in this area can be classified into four types: designing annotation scheme for temporal information representation [4, 6, 12]; developing temporal ontology which covers temporal objects and their relationships between each other [2, 7]; Identifying time-stamps of events or temporal relationships between events [5, 9]; Identifying and normalizing temporal expressions from different languages [1, 3, 8, 11, 13, 15]. Temporal annotation, temporal ontology and temporal reasoning are not the focuses in this paper. Among the research works on temporal expression extraction and normalization, most of them are based on hand-written rules or machine-learnt rules. Mani and Wilson [11] resolve temporal expressions by hand-crafted and machinelearnt rules. Their focus is resolving temporal expressions, especially indexical expressions, which designate times that are dependent on the speaker and some reference time. We concentrate on the procedure of extraction and normalization, and try to cover more temporal expr"
I05-3012,C02-1097,0,0.0266704,"-based approaches, and hybrid approaches. Among these approaches, the supervised corpus-based approach had been applied and discussed by many researches ([2, 3, 4, 5, 6, 7, 8]). According to [1], the corpusbased supervised machine learning methods are the most successful approaches to WSD where contextual features have been used mainly to distinguish ambiguous words in these methods. However, word occurrences in the context are too diverse to capture the right pattern, which means that the dimension of contextual words will be very large when all words in the training samples are used for WSD [14]. Certain uninformative features will weaken the discriminative power of a classifier resulting in a lower precision rate. To narrow down the context, we propose to use collocations as contextual information as defined in Section 3.1.2. It is generally understood that the sense of an ambiguous word is unique in a given collocation [19]. For example, “ࣙ㺅” means “burden” but not “baggage” when it appears in the collocation “ᗱᛇࣙ㺅” (“ burden of thought”). In this paper, we apply a classifier to combine the local features of collocations which contain the target word with other contextual features"
I05-3012,C96-1005,0,0.139751,"Missing"
I05-3012,P91-1019,0,0.115845,"Missing"
I05-3012,W03-1302,0,0.0457119,"to distinguish ambiguous words in these methods. However, word occurrences in the context are too diverse to capture the right pattern, which means that the dimension of contextual words will be very large when all words in the training samples are used for WSD [14]. Certain uninformative features will weaken the discriminative power of a classifier resulting in a lower precision rate. To narrow down the context, we propose to use collocations as contextual information as defined in Section 3.1.2. It is generally understood that the sense of an ambiguous word is unique in a given collocation [19]. For example, “ࣙ㺅” means “burden” but not “baggage” when it appears in the collocation “ᗱᛇࣙ㺅” (“ burden of thought”). In this paper, we apply a classifier to combine the local features of collocations which contain the target word with other contextual features to discriminate the ambiguous words. The intuition is that when the target context captures a collocation, the influence of other dimensions of Abstract The selection of features is critical in providing discriminative information for classifiers in Word Sense Disambiguation (WSD). Uninformative features will degrade the performance of"
I05-3012,W97-0201,0,0.116005,"Missing"
I05-3012,P03-1058,0,0.0738081,"lyu.ed u.hk du.hk du.hk Through Time”, or the Chinese word “ഄᮍ” in “ഄᮍᬓᑰ”(“local government”) and “Ҫгᇍ ⱘഄᮍ”(“He is also partly right”). WSD tries to automatically assign an appropriate sense to an occurrence of a word in a given context. Various approaches have been proposed to deal with the word sense disambiguation problem including rule-based approaches, knowledge or dictionary based approaches, corpus-based approaches, and hybrid approaches. Among these approaches, the supervised corpus-based approach had been applied and discussed by many researches ([2, 3, 4, 5, 6, 7, 8]). According to [1], the corpusbased supervised machine learning methods are the most successful approaches to WSD where contextual features have been used mainly to distinguish ambiguous words in these methods. However, word occurrences in the context are too diverse to capture the right pattern, which means that the dimension of contextual words will be very large when all words in the training samples are used for WSD [14]. Certain uninformative features will weaken the discriminative power of a classifier resulting in a lower precision rate. To narrow down the context, we propose to use collocations as conte"
I05-3012,H93-1051,0,0.117697,"Missing"
I05-3012,J98-1006,0,0.0972472,"SENSEVAL-3 Chinese training data. Even though in both approaches, statistically significant bi-gram co-occurrence information is used, they are not necessarily true collocations. For example, in the express “ᥠᦵⲥ㾚ᴀᎲ ഄᮄ㒇㊍ߚᄤⱘ⌏ᚙ”މ, the bi-grams in their system are (ᥠᦵ , ⲥ㾚 , ⲥ㾚ᴀ Related Work Automating word sense disambiguation tasks based on annotated corpora have been proposed. Examples of supervised learning methods for WSD appear in [2, 3, 4], [7, 8]. The learning algorithms applied including: decision tree, decisionlist [15], neural networks [7], naïve Bayesian learning ([5],[11]) and maximum entropy [10]. Among these leaning methods, the most important issue is what features will be used to construct the classifier. It is common in WSD to use contextual information that can be found in the neighborhood of the ambiguous word in training data ([6], [16, 17, 18]). It is generally true that when Ꮂ , ᴀᎲഄ , ഄᮄ㒇㊍ , ᮄ㒇㊍ⱘ  ⱘ⌏ , ⌏ᚙ މSome bi-grams such as ⌏   ᚙ   މmay have higher frequency but may introduce noise when considering it as features in disambiguating the sense “human|Ҏ” and “symbol|ヺো” like in the example case of “∈ߚᄤ⌏ᚙ”މ. In our system,"
I05-3012,J98-1004,0,0.170264,"ᥠᦵ , ⲥ㾚 , ⲥ㾚ᴀ Related Work Automating word sense disambiguation tasks based on annotated corpora have been proposed. Examples of supervised learning methods for WSD appear in [2, 3, 4], [7, 8]. The learning algorithms applied including: decision tree, decisionlist [15], neural networks [7], naïve Bayesian learning ([5],[11]) and maximum entropy [10]. Among these leaning methods, the most important issue is what features will be used to construct the classifier. It is common in WSD to use contextual information that can be found in the neighborhood of the ambiguous word in training data ([6], [16, 17, 18]). It is generally true that when Ꮂ , ᴀᎲഄ , ഄᮄ㒇㊍ , ᮄ㒇㊍ⱘ  ⱘ⌏ , ⌏ᚙ މSome bi-grams such as ⌏   ᚙ   މmay have higher frequency but may introduce noise when considering it as features in disambiguating the sense “human|Ҏ” and “symbol|ヺো” like in the example case of “∈ߚᄤ⌏ᚙ”މ. In our system, we do not rely on co-occurrence information. Instead, we utilize true collocation information (ᮄ㒇㊍, ߚᄤ) which fall in the window size of (-5, +5) as fea88 the contextual window size as 10 in our system. Each of the Chinese words except the stop words inside the window range wi"
I05-3012,J98-1005,0,0.0690024,"n 5 is the conclusion. 2 words are used in the same sense, they have similar context and co-occurrence information [13]. It is also generally true that the nearby context words of an ambiguous word give more effective patterns and features values than those far from it [12]. The existing methods consider features selection for context representation including both local and topic features where local features refer to the information pertained only to the given context and topical features are statistically obtained from a training corpus. Most of the recent works for English corpus including [7] and [8], which combine both local and topical information in order to improve their performance. An interesting study on feature selection for Chinese [10] has considered topical features as well as local collocational, syntactic, and semantic features using the maximum entropy model. In Dang’s [10] work, collocational features refer to the local PoS information and bi-gram co-occurrences of words within 2 positions of the ambiguous word. A useful result from this work based on (about one million words) the tagged People’s Daily News shows that adding more features from richer levels of lingu"
I05-3012,P94-1013,0,0.167045,"Missing"
I05-3012,P95-1026,0,0.359684,"Missing"
I05-3012,C02-1143,0,0.240681,"ata. Even though in both approaches, statistically significant bi-gram co-occurrence information is used, they are not necessarily true collocations. For example, in the express “ᥠᦵⲥ㾚ᴀᎲ ഄᮄ㒇㊍ߚᄤⱘ⌏ᚙ”މ, the bi-grams in their system are (ᥠᦵ , ⲥ㾚 , ⲥ㾚ᴀ Related Work Automating word sense disambiguation tasks based on annotated corpora have been proposed. Examples of supervised learning methods for WSD appear in [2, 3, 4], [7, 8]. The learning algorithms applied including: decision tree, decisionlist [15], neural networks [7], naïve Bayesian learning ([5],[11]) and maximum entropy [10]. Among these leaning methods, the most important issue is what features will be used to construct the classifier. It is common in WSD to use contextual information that can be found in the neighborhood of the ambiguous word in training data ([6], [16, 17, 18]). It is generally true that when Ꮂ , ᴀᎲഄ , ഄᮄ㒇㊍ , ᮄ㒇㊍ⱘ  ⱘ⌏ , ⌏ᚙ މSome bi-grams such as ⌏   ᚙ   މmay have higher frequency but may introduce noise when considering it as features in disambiguating the sense “human|Ҏ” and “symbol|ヺো” like in the example case of “∈ߚᄤ⌏ᚙ”މ. In our system, we do not rely on co-occurrence"
I05-3012,W04-0847,0,0.276723,"as fully fixed collocations, fixed collocations, strong collocations and loose collocations. Fixed collocations means the appearance of one word implies the co-occurrence of another one such as “ग़ࣙ㺅” (“burden of history”), while strong collocations allows very limited substitution of the components, for example, “ഄᮍ䰶” (“local college”), or ” ഄᮍ ᄺ” (“local university”). The sense of ambiguous words can be uniquely determined in these two types of collocations, therefore are the collocations applied in our system. The sources of the collocations will be explained in Section 4.1. In both Niu [11] and Dang’s [10] work, topical features as well as the so called collocational features were used. However, as discussed in Section 2, they both used bi-gram cooccurrences as the additional local features. However, bi-gram co-occurrences only indicate statistical significance which may not actually satisfy the conceptual definition of collocations. Thus instead of using co-occurrences of bigrams, we take the true bi-gram collocations extracted from our system and use this data to compare with bi-gram co-occurrences to test the usefulness of collocation for WSD. The local features in our system"
I05-3012,P97-1007,0,\N,Missing
I08-7003,N03-1033,0,0.0108382,"eriments show that this method is quite effective in giving good precision and minimal computing time. The remaining of this paper is organized as follows. Section 2 reviews the related work. Section 3 gives the observations to the task and corresponding corpus, then presents our method for TPOS tagging. Section 4 gives the evaluation details and discussions on the proposed method and reference methods. Section 5 concludes this paper. 2 Some existing methods are based on the analysis of word morphology. They exploited more features besides morphology or took morphology as supplementary means (Toutanova et al., 2003; Huihsin Tseng et al., 2005; Samuelsson, Christer, 1993). Toutanova et al. demonstrated the use of both preceding and following tag contexts via a dependency network representation and made use of some additional features such as lexical features including jointly conditioning on multiple consecutive words and other fine-grained modeling of word features (Toutanova et al., 2003). Huihisin et al. proposed a variety of morphological word features, such as the tag sequence features from both left and right side of the current word for POS tagging and implemented them in a Maximum Entropy Markov"
I08-7003,I05-3005,0,0.0135145,"s quite effective in giving good precision and minimal computing time. The remaining of this paper is organized as follows. Section 2 reviews the related work. Section 3 gives the observations to the task and corresponding corpus, then presents our method for TPOS tagging. Section 4 gives the evaluation details and discussions on the proposed method and reference methods. Section 5 concludes this paper. 2 Some existing methods are based on the analysis of word morphology. They exploited more features besides morphology or took morphology as supplementary means (Toutanova et al., 2003; Huihsin Tseng et al., 2005; Samuelsson, Christer, 1993). Toutanova et al. demonstrated the use of both preceding and following tag contexts via a dependency network representation and made use of some additional features such as lexical features including jointly conditioning on multiple consecutive words and other fine-grained modeling of word features (Toutanova et al., 2003). Huihisin et al. proposed a variety of morphological word features, such as the tag sequence features from both left and right side of the current word for POS tagging and implemented them in a Maximum Entropy Markov model (Huihsin Tseng et al.,"
I08-7003,O05-4006,1,0.881949,"Missing"
I08-7003,A00-1031,0,0.0173915,"Missing"
I08-7003,E99-1018,0,0.090738,"Missing"
I08-7003,A97-1018,0,0.0617779,"Missing"
I08-7003,W03-1730,0,0.0254164,"Missing"
I08-7003,J97-3003,0,\N,Missing
I08-7003,P96-1043,0,\N,Missing
I11-1055,J02-4001,0,0.0322537,"a user’s complex information need. In order to solve this problem, multi-document summarization, which reduces the size of documents while preserves their important semantic content is highly demanded. Most of the summarization work done till date follow the sentence extraction framework, which ranks sentences according to various pre-specified criteria and selects the most salient sentences from the original documents to form summaries. In addition to sentence salience, the other fundamental issues that must be concerned in summarization are information redundancy and information diversity (Radev et al., 2002). When the given documents are all supposed to be about the same topic, they are very likely to repeat some important information in different documents or in different places of the same document. Therefore, effectively recognizing the sentences with the same or very similar content is necessary for reducing redundancy and covering more diverse informative content in a summary. This is normally achieved by clustering highly related sentences into topical themes. Summaries can then be produced, e.g., by extracting the representative sentence(s) from each theme cluster. Thus, good sentence clus"
I11-1055,C08-1087,0,0.178061,"tion (either generic or query-oriented). With the advancement of information technologies and the explosion of information on the Internet, clustering has become increasingly important in text mining and knowledge discovery. Recently it has been successfully applied in theme-based (a.k.a. clustering-based) summarization. In terms of the roles of clustering in summarization, one could take the advantage of the clustering results to select the representative sentences in order to generate diverse summaries. The typical examples of such use are C-RR and C-LexRank proposed by Qazvinian and Radev (Qazvinian and Radev, 2008), which selected the important citation sentences from the sentence clusters generated by a hierarchical agglomeration algorithm. Alternatively, the clustering results could be used to improve or refine the sentence ranking results. Most of the clusteringbased summarization approaches are of this nature. For example, Wan and Yang (Wan and Yang, 2008) presented a cluster-based conditional Markov random walk model and a clusterbased HITS model to incorporate the clusterlevel information into the process of sentence ranking. Wang et al. (Wang et al., 2008a) also proposed a language model to clust"
I11-1055,N03-1020,0,0.0801513,"teration is repeated until the length of sentences in the summary reaches the length limitation. In our experiment, the threshold is set to 0.9. 495 5 Experiments and Evaluation We conduct a series of experiments on the DUC2004 generic summarization dataset and the DUC2007 query-based summarization dataset. According to task definitions, systems are required to produce a concise summary for each document set (without or with a given query description) and the length of summaries is limited to 665 bytes in DUC 2004 and 250 words in DUC2007. A well-recognized automatic evaluation toolkit ROUGE (Lin and Hovy, 2003) is used for evaluation. We report two common ROUGE scores in this paper, namely ROUGE-1 and ROUGE-2, which base on the Uni-gram match and the Bi-gram match, respectively. Documents and queries are pre-processed by segmenting sentences and splitting words. Stop words are removed and the remaining words are stemmed using Porter stemmer. 5.1 Summarization Evaluation To evaluate the performance of the noise detection enhanced spectral clustering approach, we compare the ROUGE scores of it with the ROUGE scores of the LexRank approach for generic summarization and query-oriented LexRank approach,"
I13-1073,W04-1102,0,0.0904726,"Missing"
I13-1073,W02-1001,0,0.105237,"the precision, recall, and F-score based on a single query. Hence, we finally have six metrics: macro-precision, macro-recall, macro-Fscore, micro-precision, micro-recall, and micro-Fscore. We use the novel training method, adaptive online gradient descent based on feature frequency information (ADF) (Sun et al., 2012), for fast and accurate training of the CRF model. To study the performance of other machine learning models, we also implement on other wellknown sequential labeling models, including maximum entropy Markov models (MEMMs) (McCallum et al., 2000) and averaged perceptrons (Perc) (Collins, 2002). 3.2 Results on Abbreviation Prediction The experimental results are shown in Table 2. In the table, the overall accuracy is most important and it means the final accuracy achieved by the systems in generalized abbreviation prediction with NFFs. For the completeness of experimental information, we also show the discriminate accuracy. The discriminate accuracy checks the accuracy of discriminating positive and negative full forms, without comparing the generated abbreviations with the gold-standard abbreviations. As we can see from Table 2, first, the best system is the system Unified-Assum.1-"
I13-1073,P08-2016,0,0.646784,"Missing"
I13-1073,P02-1021,0,0.0505622,"Missing"
I13-1073,P09-1102,1,0.611546,"Missing"
I13-1073,P12-1027,1,0.813122,"inally, the CRF model outperforms the MEMM and averaged perceptron models. To summarize, the unified system with assumption-1, global information, and CRF model has the best performance. For evaluating web search quality based on a set of queries, we use the macro-averaging and microaveraging of the precision, recall, and F-score based on a single query. Hence, we finally have six metrics: macro-precision, macro-recall, macro-Fscore, micro-precision, micro-recall, and micro-Fscore. We use the novel training method, adaptive online gradient descent based on feature frequency information (ADF) (Sun et al., 2012), for fast and accurate training of the CRF model. To study the performance of other machine learning models, we also implement on other wellknown sequential labeling models, including maximum entropy Markov models (MEMMs) (McCallum et al., 2000) and averaged perceptrons (Perc) (Collins, 2002). 3.2 Results on Abbreviation Prediction The experimental results are shown in Table 2. In the table, the overall accuracy is most important and it means the final accuracy achieved by the systems in generalized abbreviation prediction with NFFs. For the completeness of experimental information, we also s"
I13-1073,N09-2069,0,0.541149,"Missing"
I13-1073,W06-0103,0,\N,Missing
I17-1099,L16-1017,0,0.0313815,"Missing"
I17-1099,W14-4012,0,0.0107437,"Missing"
I17-1099,D16-1127,0,0.291469,"s contextaware modeling ability, HRED has shown better performances in previous work (Sordoni et al., 2015). Intention and Emotion-enhanced To utilize the intention and emotion labels, we follow Zhou et al. (2017) to incorporate the label information during decoding. The intention and emotion labels are characterized as one-hot vectors. We denote the label-enhanced approaches as {L+} and the performances are given in the second box in Table 4. Pretrained We also examine whether pre-training with other dataset will boost the performance of the first three generation-based approaches. Following Li et al. (2016, 2017), we use the OpenSubtitle dataset (J¨org Tiedemann, 2009)7 . Because it has no clear and concise segmentation for each conversation, we treat each of three consecutive utterances as context, and the foregoing one as response. Finally, 3,000,000 three-turn dialogs are randomly sampled and used to pre-train the compared models for 12 epochs. We denote the approaches using pre-training as {Pre+}. According to BLEU scores from Table 4 (last four columns), we can see that in general attentionbased approaches are better than vanilla Seq2Seq model. Among the three compared approaches, HREDs ac"
I17-1099,D17-1230,0,0.0402712,"Missing"
I17-1099,D16-1230,0,0.347091,"Missing"
I17-1099,W15-4640,0,0.113364,"et along very well. It makes me feel that I’m someone special. It makes me feel that I’m someone special.” The context history for this response is “oh, really? so you just took home a stray cat? // Yes. It was starving and looking for something to eat when I saw it. // Poor cat.” whose emotion history is {6, 0, 0}. 4.3 4.3.1 Generation-based Approaches Compared Approaches Seq2Seq The simplest generation-based approach we adopt is a vanilla Seq2Seq with GRU as basic cell, as described in Section 4.1. Such approach is widely selected as baseline models in dialog generation Shang et al. (2015); Lowe et al. (2015); Al-Rfou’ et al. (2016). Attention-based Seq2Seq We then evaluate the Seq2Seq approach with attention mechanism (Bahdanau et al., 2014) which has shown its effectiveness on various NLP tasks including dialog response (Hermann et al., 2015; Luong et al., 2015; Mei et al., 2017). We denote this approach as {AttnSeq2Seq}. HRED The third generation-based approach we evaluate is hierarchical encoder-decoder (HRED) (Sordoni et al., 2015). Due to its contextaware modeling ability, HRED has shown better performances in previous work (Sordoni et al., 2015). Intention and Emotion-enhanced To utilize th"
I17-1099,D15-1166,0,0.00437513,"saw it. // Poor cat.” whose emotion history is {6, 0, 0}. 4.3 4.3.1 Generation-based Approaches Compared Approaches Seq2Seq The simplest generation-based approach we adopt is a vanilla Seq2Seq with GRU as basic cell, as described in Section 4.1. Such approach is widely selected as baseline models in dialog generation Shang et al. (2015); Lowe et al. (2015); Al-Rfou’ et al. (2016). Attention-based Seq2Seq We then evaluate the Seq2Seq approach with attention mechanism (Bahdanau et al., 2014) which has shown its effectiveness on various NLP tasks including dialog response (Hermann et al., 2015; Luong et al., 2015; Mei et al., 2017). We denote this approach as {AttnSeq2Seq}. HRED The third generation-based approach we evaluate is hierarchical encoder-decoder (HRED) (Sordoni et al., 2015). Due to its contextaware modeling ability, HRED has shown better performances in previous work (Sordoni et al., 2015). Intention and Emotion-enhanced To utilize the intention and emotion labels, we follow Zhou et al. (2017) to incorporate the label information during decoding. The intention and emotion labels are characterized as one-hot vectors. We denote the label-enhanced approaches as {L+} and the performances are"
I17-1099,petukhova-etal-2014-dbox,0,0.377808,"A good conversational agent enables enterprises to provide automatic customer services and thus reduce human labor costs. For academia, it is challenging yet appealing to build up such an intelligent chatbot which involves a series of high-level natural language processing techniques, such as understanding the underlying semantics of user input utterance, and generating coherent and meaningful responses. However, the training datasets for this research area are still deficient. Traditional dialogue systems are often trained with domain-specific spoken dialogue datasets (Ringger et al., 1996; Petukhova et al., 2014), which are often small-scale and oriented to complete a specific task. More recent work feed their conversational models with open-domain datasets. Switchboard (Godfrey et al., 1992) and OpenSubtitles (J¨org Tiedemann, 2009) datasets Figure 1: An example in DailyDialog dataset. Some text is shortened for space. Best viewed in color. comprise approximately 150 turns in a “conversation” and thus are too disperse to capture the main topic. Twitter Dialog Corpus (Ritter et al., 2011) and Chinese Weibo dataset (Wang et al., 2013) are comprised of posts and replies on social networks, which are noi"
I17-1099,P96-1009,0,0.367918,"al and academic camps. A good conversational agent enables enterprises to provide automatic customer services and thus reduce human labor costs. For academia, it is challenging yet appealing to build up such an intelligent chatbot which involves a series of high-level natural language processing techniques, such as understanding the underlying semantics of user input utterance, and generating coherent and meaningful responses. However, the training datasets for this research area are still deficient. Traditional dialogue systems are often trained with domain-specific spoken dialogue datasets (Ringger et al., 1996; Petukhova et al., 2014), which are often small-scale and oriented to complete a specific task. More recent work feed their conversational models with open-domain datasets. Switchboard (Godfrey et al., 1992) and OpenSubtitles (J¨org Tiedemann, 2009) datasets Figure 1: An example in DailyDialog dataset. Some text is shortened for space. Best viewed in color. comprise approximately 150 turns in a “conversation” and thus are too disperse to capture the main topic. Twitter Dialog Corpus (Ritter et al., 2011) and Chinese Weibo dataset (Wang et al., 2013) are comprised of posts and replies on socia"
I17-1099,D11-1054,0,0.629177,"itional dialogue systems are often trained with domain-specific spoken dialogue datasets (Ringger et al., 1996; Petukhova et al., 2014), which are often small-scale and oriented to complete a specific task. More recent work feed their conversational models with open-domain datasets. Switchboard (Godfrey et al., 1992) and OpenSubtitles (J¨org Tiedemann, 2009) datasets Figure 1: An example in DailyDialog dataset. Some text is shortened for space. Best viewed in color. comprise approximately 150 turns in a “conversation” and thus are too disperse to capture the main topic. Twitter Dialog Corpus (Ritter et al., 2011) and Chinese Weibo dataset (Wang et al., 2013) are comprised of posts and replies on social networks, which are noisy, informal and different from real conversations. ∗ Authors contributed equally. Correspondence should be sent to Y. Li (csyli@comp.polyu.edu.hk). 1 The dataset is available on http://yanran.li/ dailydialog In this work, we develop a high-quality multiturn dialogue dataset, which contains conversations about our daily life. We refer to it as DailyDialog. In our daily life, we communicate with others by two main reasons: exchanging information and enhancing social bonding. To exc"
I17-1099,P15-1152,0,0.222928,"onsistent with our real experience that we often invite people for social activities (Relationship), talk about what happened recently (Ordinary Life) and what happened at work (Work). 3.2 Bi-turn Dialog Flow Because the dialogues are assumed to happen in daily life, they follow natural dialog flow. It makes DailyDialog dataset quite different from existing QA datasets such as SubTle dataset (Dodge et al., 2015)which are improperly used for training dialog systems. DailyDialog dataset also distinguishes from those post-reply datasets such as Reddit comment (Al-Rfou’ et al., 2016), Sina Weibo (Shang et al., 2015) and Twitter (Ritter et al., 2011) datasets. The latter datasets comprise postreply pairs on social networks where people interact with others more freely (often more than two speakers) and results in ambiguous dialog flows. Instead, the dialog act flows in Dailydialog are more consistent with our daily communication. For example, we usually do not leave alone others’ question and just tersely change the topic. Instead, we will answer others’ questions politely. By the definitions we introduce in Section 2.2, this reflects a Questions-Inform bi-turn dialog flow. This is a frequent circle pheno"
I17-1099,D13-1096,0,0.571416,"domain-specific spoken dialogue datasets (Ringger et al., 1996; Petukhova et al., 2014), which are often small-scale and oriented to complete a specific task. More recent work feed their conversational models with open-domain datasets. Switchboard (Godfrey et al., 1992) and OpenSubtitles (J¨org Tiedemann, 2009) datasets Figure 1: An example in DailyDialog dataset. Some text is shortened for space. Best viewed in color. comprise approximately 150 turns in a “conversation” and thus are too disperse to capture the main topic. Twitter Dialog Corpus (Ritter et al., 2011) and Chinese Weibo dataset (Wang et al., 2013) are comprised of posts and replies on social networks, which are noisy, informal and different from real conversations. ∗ Authors contributed equally. Correspondence should be sent to Y. Li (csyli@comp.polyu.edu.hk). 1 The dataset is available on http://yanran.li/ dailydialog In this work, we develop a high-quality multiturn dialogue dataset, which contains conversations about our daily life. We refer to it as DailyDialog. In our daily life, we communicate with others by two main reasons: exchanging information and enhancing social bonding. To exchange and share ideas, we often communicate wi"
I17-1099,P16-1049,0,0.0592118,"nsorFlow (Abadi et al., 2015). 4.1 Experimental Setup We randomly separate the DailyDialog datasets into training/validation/test sets with 11,118/1,000/1,000 conversations. We tune the parameters on validation set and report the performance on test sets. In all experiments, the vocabulary size is set as 25,000 and all the OOV words are mapped to a special token UNK. We Retrieval-based Approaches Compared Approaches First, we choose three categories of four retrievalbased approaches, i.e., (1) Embedding-based Similarity (Luo and Li, 2016); (2) Feature-based Similarity (Jafarpour et al., 2010; Yan et al., 2016); (3)(4) Feature-based Similarity with Intention and Emotion Reranking (Luo and Li, 2016; Otsuka et al., 2017). We aim to see whether classical embeddings-based, feature-based and rerankingenhanced approaches are effective on DailyDialog. Embedding-based The embedding-based approach is using basic neural networks as described in Section 4.1 and denoted as {Embedding} below. We measure the distance between embeddings as the average of cosine similarity, Jaccard distance and Euclidean distance. At test time, candidates whose context embedding is closer to the test context embedding are ranked hi"
J14-3004,P07-1056,0,0.0481432,"Missing"
J14-3004,W02-1001,0,0.609391,"g rate or so-called decaying rate, and Lstoch (zzi , w t ) is the stochastic loss function based on a training sample z i . (More details of SGD are described in Bottou [1998], Tsuruoka, Tsujii, and Ananiadou [2009], and Sun et al. [2013].) Following the most recent work of SGD, the exponential decaying rate works the best for natural language processing tasks, and it is adopted in our implementation of the SGD (Tsuruoka, Tsujii, and Ananiadou 2009; Sun et al. 2013). Other well-known on-line training methods include perceptron training (Freund and Schapire 1999), averaged perceptron training (Collins 2002), more recent development/extensions of stochastic gradient descent (e.g., the second-order stochastic gradient descent training methods like stochastic meta descent) (Vishwanathan et al. 2006; Hsu et al. 2009), and so on. However, the second-order stochastic gradient descent method requires the computation or approximation of the inverse of the Hessian matrix of the objective function, which is typically slow, especially for heavily structured classification models. Usually the convergence speed based on number of training iterations is moderately faster, but the time cost per iteration is sl"
J14-3004,W04-1217,0,0.0112629,"tion (Bio-NER) task is from the BIONLP-2004 shared task. The task is to recognize five kinds of biomedical named entities, including DNA, RNA, protein, cell line, and cell type, on the MEDLINE biomedical text mining corpus (Kim et al. 2004). A typical approach to this problem is to cast it as a sequential labeling task with the BIO encoding. This data set consists of 20,546 training samples (from 2,000 MEDLINE article abstracts, with 472,006 word tokens) and 4,260 test samples. The properties of the data are summarized in Table 1. State-of-the-art systems for this task include Settles (2004), Finkel et al. (2004), Okanohara et al. (2006), Hsu et al. (2009), Sun, Matsuzaki, et al. (2009), and Tsuruoka, Tsujii, and Ananiadou (2009). Following previous studies for this task (Okanohara et al. 2006; Sun, Matsuzaki, et al. 2009), we use word token–based features, part-of-speech (POS) based features, and orthography pattern–based features (prefix, uppercase/lowercase, etc.), as listed in Table 2. With the traditional implementation of CRF systems (e.g., the HCRF package), the edges features usually contain only the information of yi−1 and yi , and ignore the 572 Sun et al. Feature-Frequency–Adaptive On-line"
J14-3004,P07-1104,0,0.0145497,"e word segmentation aims to automatically segment character sequences into word sequences. Chinese word segmentation is important because it is the first step for most Chinese language information processing systems. Our experiments are based on the Microsoft Research data provided by The Second International Chinese Word Segmentation Bakeoff. In this data set, there are 8.8 × 104 word-types, 2.4 × 106 wordtokens, 5 × 103 character-types, and 4.1 × 106 character-tokens. State-of-the-art systems for this task include Tseng et al. (2005), Zhang, Kikui, and Sumita (2006), Zhang and Clark (2007), Gao et al. (2007), Sun, Zhang, et al. (2009), Sun (2010), Zhao et al. (2010), and Zhao and Kit (2011). The feature engineering follows previous work on word segmentation (Sun, Wang, and Li 2012). Rich edge features are used. For the classification label yi and the label transition yi−1 yi on position i, we use the feature templates as follows (Sun, Wang, and Li 2012): r Character unigrams located at positions i − 2, i − 1, i, i + 1, and i + 2. 573 Computational Linguistics r r r r r r r Volume 40, Number 3 Character bigrams located at positions i − 2, i − 1, i and i + 1. Whether xj and xj+1 are identical, for"
J14-3004,W04-1213,0,0.0135585,"also perform experiments on a nonstructured binary classification task: sentiment-based text classification. For the nonstructured classification task, the ADF training is based on the maximum entropy model (Berger, Della Pietra, and Della Pietra 1996; Ratnaparkhi 1996). 4.1 Biomedical Named Entity Recognition (Structured Classification) The biomedical named entity recognition (Bio-NER) task is from the BIONLP-2004 shared task. The task is to recognize five kinds of biomedical named entities, including DNA, RNA, protein, cell line, and cell type, on the MEDLINE biomedical text mining corpus (Kim et al. 2004). A typical approach to this problem is to cast it as a sequential labeling task with the BIO encoding. This data set consists of 20,546 training samples (from 2,000 MEDLINE article abstracts, with 472,006 word tokens) and 4,260 test samples. The properties of the data are summarized in Table 1. State-of-the-art systems for this task include Settles (2004), Finkel et al. (2004), Okanohara et al. (2006), Hsu et al. (2009), Sun, Matsuzaki, et al. (2009), and Tsuruoka, Tsujii, and Ananiadou (2009). Following previous studies for this task (Okanohara et al. 2006; Sun, Matsuzaki, et al. 2009), we u"
J14-3004,N01-1025,0,0.136481,"ssification) In the phrase chunking task, the non-recursive cores of noun phrases, called base NPs, are identified. The phrase chunking data is extracted from the data of the CoNLL-2000 shallow-parsing shared task (Sang and Buchholz 2000). The training set consists of 8,936 sentences, and the test set consists of 2,012 sentences. We use the feature templates based on word n-grams and part-of-speech n-grams, and feature templates are shown in Table 3. Rich edge features are used. Using the feature templates, we extract 4.8 × 105 features in total. State-of-the-art systems for this task include Kudo and Matsumoto (2001), Collins (2002), McDonald, Crammer, and Pereira (2005), Vishwanathan et al. (2006), Sun et al. (2008), and Tsuruoka, Tsujii, and Ananiadou (2009). Following prior studies, the evaluation metric for this task is the balanced F-score. 4.4 Sentiment Classification (Non-Structured Classification) To demonstrate that the proposed method is not limited to structured classification, we select a well-known sentiment classification task for evaluating the proposed method on non-structured classification. Table 3 Feature templates used for the phrase chunking task. wi , ti , and yi are defined as befor"
J14-3004,H05-1124,0,0.245783,"Missing"
J14-3004,P06-1059,0,0.0295941,"from the BIONLP-2004 shared task. The task is to recognize five kinds of biomedical named entities, including DNA, RNA, protein, cell line, and cell type, on the MEDLINE biomedical text mining corpus (Kim et al. 2004). A typical approach to this problem is to cast it as a sequential labeling task with the BIO encoding. This data set consists of 20,546 training samples (from 2,000 MEDLINE article abstracts, with 472,006 word tokens) and 4,260 test samples. The properties of the data are summarized in Table 1. State-of-the-art systems for this task include Settles (2004), Finkel et al. (2004), Okanohara et al. (2006), Hsu et al. (2009), Sun, Matsuzaki, et al. (2009), and Tsuruoka, Tsujii, and Ananiadou (2009). Following previous studies for this task (Okanohara et al. 2006; Sun, Matsuzaki, et al. 2009), we use word token–based features, part-of-speech (POS) based features, and orthography pattern–based features (prefix, uppercase/lowercase, etc.), as listed in Table 2. With the traditional implementation of CRF systems (e.g., the HCRF package), the edges features usually contain only the information of yi−1 and yi , and ignore the 572 Sun et al. Feature-Frequency–Adaptive On-line Training for Natural Lang"
J14-3004,W96-0213,0,0.366727,"xisting gold-standard systems, which are complicated and use extra resources. 2. Related Work Our main focus is on structured classification models with high dimensional features. For structured classification, the conditional random fields model is widely used. To illustrate that the proposed method is a general-purpose training method not limited to a specific classification task or model, we also evaluate the proposal for non-structured classification tasks like binary classification. For non-structured classification, the maximum entropy model (Berger, Della Pietra, and Della Pietra 1996; Ratnaparkhi 1996) is widely used. Here, we review the conditional random fields model and the related work of on-line training methods. 2.1 Conditional Random Fields The conditional random field (CRF) model is a representative structured classification model and it is well known for its high accuracy in real-world applications. The CRF model is proposed for structured classification by solving “the label bias problem” (Lafferty, McCallum, and Pereira 2001). Assuming a feature function that maps a pair of observation sequence x and label sequence y to a global feature vector f, the probability of a label sequen"
J14-3004,W00-0726,0,0.0354294,"nstraints on j and k. All feature templates are instantiated with values that occurred in training samples. The extracted feature set is large, and there are 2.4 × 107 features in total. Our evaluation is based on a closed test, and we do not use extra resources. Following prior studies, the evaluation metric for this task is the balanced F-score. 4.3 Phrase Chunking (Structured Classification) In the phrase chunking task, the non-recursive cores of noun phrases, called base NPs, are identified. The phrase chunking data is extracted from the data of the CoNLL-2000 shallow-parsing shared task (Sang and Buchholz 2000). The training set consists of 8,936 sentences, and the test set consists of 2,012 sentences. We use the feature templates based on word n-grams and part-of-speech n-grams, and feature templates are shown in Table 3. Rich edge features are used. Using the feature templates, we extract 4.8 × 105 features in total. State-of-the-art systems for this task include Kudo and Matsumoto (2001), Collins (2002), McDonald, Crammer, and Pereira (2005), Vishwanathan et al. (2006), Sun et al. (2008), and Tsuruoka, Tsujii, and Ananiadou (2009). Following prior studies, the evaluation metric for this task is t"
J14-3004,W04-1221,0,0.0458102,"Missing"
J14-3004,C10-2139,0,0.0102007,"ent character sequences into word sequences. Chinese word segmentation is important because it is the first step for most Chinese language information processing systems. Our experiments are based on the Microsoft Research data provided by The Second International Chinese Word Segmentation Bakeoff. In this data set, there are 8.8 × 104 word-types, 2.4 × 106 wordtokens, 5 × 103 character-types, and 4.1 × 106 character-tokens. State-of-the-art systems for this task include Tseng et al. (2005), Zhang, Kikui, and Sumita (2006), Zhang and Clark (2007), Gao et al. (2007), Sun, Zhang, et al. (2009), Sun (2010), Zhao et al. (2010), and Zhao and Kit (2011). The feature engineering follows previous work on word segmentation (Sun, Wang, and Li 2012). Rich edge features are used. For the classification label yi and the label transition yi−1 yi on position i, we use the feature templates as follows (Sun, Wang, and Li 2012): r Character unigrams located at positions i − 2, i − 1, i, i + 1, and i + 2. 573 Computational Linguistics r r r r r r r Volume 40, Number 3 Character bigrams located at positions i − 2, i − 1, i and i + 1. Whether xj and xj+1 are identical, for j = i − 2, . . . , i + 1. Whether xj an"
J14-3004,C08-1106,1,0.654936,"fied. The phrase chunking data is extracted from the data of the CoNLL-2000 shallow-parsing shared task (Sang and Buchholz 2000). The training set consists of 8,936 sentences, and the test set consists of 2,012 sentences. We use the feature templates based on word n-grams and part-of-speech n-grams, and feature templates are shown in Table 3. Rich edge features are used. Using the feature templates, we extract 4.8 × 105 features in total. State-of-the-art systems for this task include Kudo and Matsumoto (2001), Collins (2002), McDonald, Crammer, and Pereira (2005), Vishwanathan et al. (2006), Sun et al. (2008), and Tsuruoka, Tsujii, and Ananiadou (2009). Following prior studies, the evaluation metric for this task is the balanced F-score. 4.4 Sentiment Classification (Non-Structured Classification) To demonstrate that the proposed method is not limited to structured classification, we select a well-known sentiment classification task for evaluating the proposed method on non-structured classification. Table 3 Feature templates used for the phrase chunking task. wi , ti , and yi are defined as before. Word-Token–based Features: {wi−2 , wi−1 , wi , wi+1 , wi+2 , wi−1 wi , wi wi+1 } ×{yi , yi−1 yi } P"
J14-3004,P12-1027,1,0.678739,"Missing"
J14-3004,N09-1007,1,0.809977,"Missing"
J14-3004,I05-3027,0,0.0133462,"R is recall. 4.2 Chinese Word Segmentation (Structured Classification) Chinese word segmentation aims to automatically segment character sequences into word sequences. Chinese word segmentation is important because it is the first step for most Chinese language information processing systems. Our experiments are based on the Microsoft Research data provided by The Second International Chinese Word Segmentation Bakeoff. In this data set, there are 8.8 × 104 word-types, 2.4 × 106 wordtokens, 5 × 103 character-types, and 4.1 × 106 character-tokens. State-of-the-art systems for this task include Tseng et al. (2005), Zhang, Kikui, and Sumita (2006), Zhang and Clark (2007), Gao et al. (2007), Sun, Zhang, et al. (2009), Sun (2010), Zhao et al. (2010), and Zhao and Kit (2011). The feature engineering follows previous work on word segmentation (Sun, Wang, and Li 2012). Rich edge features are used. For the classification label yi and the label transition yi−1 yi on position i, we use the feature templates as follows (Sun, Wang, and Li 2012): r Character unigrams located at positions i − 2, i − 1, i, i + 1, and i + 2. 573 Computational Linguistics r r r r r r r Volume 40, Number 3 Character bigrams located at"
J14-3004,P09-1054,0,0.0266947,"Missing"
J14-3004,N06-2049,0,0.0453143,"Missing"
J14-3004,P07-1106,0,0.0122276,"d Classification) Chinese word segmentation aims to automatically segment character sequences into word sequences. Chinese word segmentation is important because it is the first step for most Chinese language information processing systems. Our experiments are based on the Microsoft Research data provided by The Second International Chinese Word Segmentation Bakeoff. In this data set, there are 8.8 × 104 word-types, 2.4 × 106 wordtokens, 5 × 103 character-types, and 4.1 × 106 character-tokens. State-of-the-art systems for this task include Tseng et al. (2005), Zhang, Kikui, and Sumita (2006), Zhang and Clark (2007), Gao et al. (2007), Sun, Zhang, et al. (2009), Sun (2010), Zhao et al. (2010), and Zhao and Kit (2011). The feature engineering follows previous work on word segmentation (Sun, Wang, and Li 2012). Rich edge features are used. For the classification label yi and the label transition yi−1 yi on position i, we use the feature templates as follows (Sun, Wang, and Li 2012): r Character unigrams located at positions i − 2, i − 1, i, i + 1, and i + 2. 573 Computational Linguistics r r r r r r r Volume 40, Number 3 Character bigrams located at positions i − 2, i − 1, i and i + 1. Whether xj and xj+1"
J14-3004,J96-1002,0,\N,Missing
J15-1002,baccianella-etal-2010-sentiwordnet,0,0.0149854,"e only five steps between the word good and the word bad in WordNet (Hassan et al. 2011). Takamura et al. (2005) construct a word graph with the gloss of WordNet. Words are connected if a word appears in the gloss of another. The word sentiment polarity is determined by the weight of its connections on the word graph. Based on WordNet, Rao and Ravichandran (2009) exploit several graph-based semi-supervised learning methods like Mincuts and Label Propagation. The word polarity orientations are induced by initializing some sentiment seed words in the WordNet graph. Esuli et al. (2006, 2007) and Baccianella et al. (2010) treat sentiment word learning as a machine learning problem, that is, to classify the polarity orientations of the words in WordNet. They select seven positive words and seven negative words and expand them through the see-also and antonym relations in WordNet. These expanded words are then used for training. They train a ternary classifier to predict the sentiment polarities of all the words in WordNet and use the glosses (textual definitions of the words in WordNet) as the features of classification. The sentiment lexicon generated is the well-known SentiWordNet.2 2.2 Cross-Lingual Sentimen"
J15-1002,C10-1004,0,0.0343341,"Missing"
J15-1002,D10-1005,0,0.149484,"a. The terms in each review are leveraged as the features for training, which has proven to be effective in sentiment classification (Pang and Lee 2008). We can regard the task of sentiment lexicon learning as word-level sentiment classification. However, for wordlevel sentiment classification, it is not straightforward to extract features for a single word. Without sufficient features, it is difficult for these approaches to perform well in learning. Another line of cross-lingual sentiment classification uses Latent Dirichlet Allocation (LDA) (Blei, Ng, and Jordan 2003) or its variants, like Boyd-Graber and Resnik (2010) or He, Alani, and Zhou (2010). These studies assume that each review is a mixture of sentiments and each sentiment is a probability over words. Then they apply the LDA-like approach to model the sentiment polarity of each review. Nonetheless, this assumption may not be applicable in sentiment lexicon learning because a single word can be regarded as the minimal semantic unit, and it is difficult, if not impossible, to infer the latent topics from a single word. Recall that different from the sentiment classification of product reviews where the instances are normally independent, words in sen"
J15-1002,P11-1061,0,0.0255781,"essary because it is relatively easy to collect from the Web. Consequentially, the novel sentiment information inferred from the parallel corpus can easily update the existing sentiment lexicons. These advantages can greatly improve the coverage of the generated sentiment lexicon, as demonstrated later in our experiments. 3.2 Bilingual Word Graph Label Propagation As commonly used semi-supervised approaches, label propagation (Zhu and Ghahramani 2002) and its variants (Zhu, Ghahramani, and Lafferty 2003; Zhou et al. 2004) have been applied to many applications, such as part-of-speech tagging (Das and Petrov 2011; Li, Graca, and Taskar 2012), image annotation (Wang, Huang, and Ding 2011), protein function prediction (Jiang 2011; Jiang and McQuay 2012), and so forth. 4 http://www.statmt.org/moses/giza/GIZA++.html. 5 http://nlp.cs.berkeley.edu. 27 Computational Linguistics Volume 41, Number 1 The underlying idea of label propagation is that the connected nodes in the graph tend to share the same sentiment labels. In bilingual word graph label propagation, the words tend to share same sentiment labels if they are connected by synonym relations or word alignment and tend to belong to different sentiment l"
J15-1002,P11-2075,0,0.0525109,"Missing"
J15-1002,esuli-sebastiani-2006-sentiwordnet,0,0.0560392,"Missing"
J15-1002,P11-2104,0,0.0862085,"ords to the words in the target language. The few existing approaches first build word relations between English and the target language. Then, based on the word relation and English sentiment seed words, they determine the sentiment polarities of the words in the target language. In these two steps, relation-building plays a fundamental role because it is responsible for the transfer of sentiment information between the two languages. Two approaches are often used to connect the words in different languages in the literature. One is based on translation entries in cross-lingual dictionaries (Hassan et al. 2011). The other relies on a machine translation (MT) engine as a black box to translate the sentiment words in English to the target language (Steinberger et al. 2011). The two approaches in Duh, Fujino, and Nagata (2011) and Mihalcea, Banea, and Weibe (2007) tend to use a small set of vocabularies to translate the natural language, which leads to a low coverage of generated sentiment lexicons for the target language. To solve this problem, we propose a generic approach to addressing the task of cross-lingual sentiment lexicon learning. Specifically, we model this task with a bilingual word graph,"
J15-1002,P97-1023,0,0.0418632,"ely leverages the inter-language relations and both types (synonym and antonym) of the intra-language relations in sentiment lexicon learning. 3. We leverage the word alignment information derived from a large number of parallel sentences in sentiment lexicon learning. We build the inter-language relation in the bilingual word graph upon word alignment, and achieve significant results. 2. Related Work 2.1 English Sentiment Lexicon Learning In general, the work on sentiment lexicon learning focuses mainly on English and can be categorized as co-occurrence–based approaches (Hatzivassiloglou and McKeown 1997; Riloff, Wiebe, and Wilson 2003; Qiu et al. 2011) and semantic-based approaches (Mihalcea, Banea, and Wiebe 2007; Takamura, Inui, and Okumura 2005; Kim and Hovy 2004). The co-occurrence-based approaches determine the sentiment polarity of a given word according to the statistical information, like the co-occurrence of the word to predefined sentiment seed words or the co-occurrence to product features. The statistical information is mainly derived from certain corpora. One of the earliest work conducted by Hatzivassiloglou and McKeown (1997) assumes that the conjunction words can convey the p"
J15-1002,W10-4116,0,0.0466903,"Missing"
J15-1002,C04-1200,0,0.955319,"mputing, the Hong Kong Polytechnic University. E-mail: gaodehong polyu@163.com, cswjli@comp.polyu.edu.hk. Submission received: 28 April 2013; revised submission received: 25 February 2014; accepted for publication: 12 May 2014. doi:10.1162/COLI a 00207 © 2015 Association for Computational Linguistics Computational Linguistics Volume 41, Number 1 1. Introduction A sentiment lexicon is regarded as the most valuable resource for sentiment analysis (Pang and Lee 2008), and lays the groundwork of much sentiment analysis research, for example, sentiment classification (Yu and Hatzivassiloglou 2003; Kim and Hovy 2004) and opinion summarization (Hu and Liu 2004). To avoid manually annotating sentiment words, an automatically learning sentiment lexicon has attracted considerable attention in the community of sentiment analysis. The existing work determines word sentiment polarities either by the statistical information (e.g., the co-occurrence of words with predefined sentiment seed words) derived from a large corpus (Riloff, Wiebe, and Wilson 2003; Hu and Liu 2006) or by the word semantic information (e.g., synonym relations) found in existing human-created resources (e.g., WordNet) (Takamura, Inui, and Oku"
J15-1002,N06-1014,0,0.0196801,"Missing"
J15-1002,P11-1033,0,0.014246,"gle Translator. They believe that those words in the third language that appear in both translation lists are likely to be sentiment words. These approaches connect the words in two languages based on MT engines. The main concern of these approaches is the low overlapping between the vocabularies of natural documents and the vocabularies of the documents translated by MT engines (Duh, Fujino, and Nagata 2011; Meng et al. 2012a). The shortcoming of these MT-based approaches inevitably leads to low coverage. Our task resembles the task of cross-lingual sentiment classification, like Wan (2009), Lu et al. (2011), and Meng et al. (2012a), which classifies the sentiment polarities of product reviews. Generally, these studies use semi-supervised learning approaches and regard translations from labeled English sentiment reviews as the training data. The terms in each review are leveraged as the features for training, which has proven to be effective in sentiment classification (Pang and Lee 2008). We can regard the task of sentiment lexicon learning as word-level sentiment classification. However, for wordlevel sentiment classification, it is not straightforward to extract features for a single word. Wit"
J15-1002,P12-1060,1,0.855189,"They manually produce two high-level gold-standard sentiment lexicons for two languages (e.g., English and Spanish) and then translate them into the third language (e.g., Italian) via Google Translator. They believe that those words in the third language that appear in both translation lists are likely to be sentiment words. These approaches connect the words in two languages based on MT engines. The main concern of these approaches is the low overlapping between the vocabularies of natural documents and the vocabularies of the documents translated by MT engines (Duh, Fujino, and Nagata 2011; Meng et al. 2012a). The shortcoming of these MT-based approaches inevitably leads to low coverage. Our task resembles the task of cross-lingual sentiment classification, like Wan (2009), Lu et al. (2011), and Meng et al. (2012a), which classifies the sentiment polarities of product reviews. Generally, these studies use semi-supervised learning approaches and regard translations from labeled English sentiment reviews as the training data. The terms in each review are leveraged as the features for training, which has proven to be effective in sentiment classification (Pang and Lee 2008). We can regard the task"
J15-1002,C12-2081,1,0.923795,"They manually produce two high-level gold-standard sentiment lexicons for two languages (e.g., English and Spanish) and then translate them into the third language (e.g., Italian) via Google Translator. They believe that those words in the third language that appear in both translation lists are likely to be sentiment words. These approaches connect the words in two languages based on MT engines. The main concern of these approaches is the low overlapping between the vocabularies of natural documents and the vocabularies of the documents translated by MT engines (Duh, Fujino, and Nagata 2011; Meng et al. 2012a). The shortcoming of these MT-based approaches inevitably leads to low coverage. Our task resembles the task of cross-lingual sentiment classification, like Wan (2009), Lu et al. (2011), and Meng et al. (2012a), which classifies the sentiment polarities of product reviews. Generally, these studies use semi-supervised learning approaches and regard translations from labeled English sentiment reviews as the training data. The terms in each review are leveraged as the features for training, which has proven to be effective in sentiment classification (Pang and Lee 2008). We can regard the task"
J15-1002,P07-1123,0,0.137069,"expand them through the see-also and antonym relations in WordNet. These expanded words are then used for training. They train a ternary classifier to predict the sentiment polarities of all the words in WordNet and use the glosses (textual definitions of the words in WordNet) as the features of classification. The sentiment lexicon generated is the well-known SentiWordNet.2 2.2 Cross-Lingual Sentiment Lexicon Learning The work on cross-lingual sentiment lexicon learning is still at an early stage and can be categorized into two types, according to how they bridge the words in two languages. Mihalcea et al. (2007) generate sentiment lexicon for Romanian by directly translating the English sentiment words into Romanian through bilingual English–Romanian dictionaries. When confronting multiword translations, they translate the multiwords word by word. Then the validated translations must occur at least three times on the Web. The approach proposed by Hassan et al. (2011) learns sentiment words based on English WordNet and WordNets in the target languages (e.g., Hindi and Arabic). Crosslingual dictionaries are used to connect the words in two languages and the polarity of a given word is determined by the"
J15-1002,J05-4003,0,0.0143604,"garded as negative. 4. Experiment 4.1 Data Sets We conduct experiments on Chinese sentiment lexicon learning. As in previous work (Baccianella, Esuli, and Sebastiani 2010), the sentiment words in General Inquirer lexicon are selected as the English seeds (Stone 1997). From the GI lexicon we collect 2,005 positive words and 1,635 negative words. To build the bilingual word graph, we adopt the Chinese–English parallel corpus, which is obtained from the news articles published by Xinhua News Agency in Chinese and English collections, using the automatic parallel sentence identification approach (Munteanu and Marcu 2005). Altogether, we collect more than 25M parallel sentence pairs in English and (of) and am) Chinese. We remove all the stopwords in Chinese and English (e.g., together with the low-frequency words that occur fewer than 5 times. After preprocessing, we finally have more than 174,000 English words, among which 3,519 words have sentiment labels and more than 146,000 Chinese words for which we need to predict the sentiment labels. To transfer sentiment information to Chinese unlabeled words more efficiently, we remove the unlabeled English words in the word graph (i.e., XU E = Φ). The unsupervised"
J15-1002,J11-1002,0,0.101363,"both types (synonym and antonym) of the intra-language relations in sentiment lexicon learning. 3. We leverage the word alignment information derived from a large number of parallel sentences in sentiment lexicon learning. We build the inter-language relation in the bilingual word graph upon word alignment, and achieve significant results. 2. Related Work 2.1 English Sentiment Lexicon Learning In general, the work on sentiment lexicon learning focuses mainly on English and can be categorized as co-occurrence–based approaches (Hatzivassiloglou and McKeown 1997; Riloff, Wiebe, and Wilson 2003; Qiu et al. 2011) and semantic-based approaches (Mihalcea, Banea, and Wiebe 2007; Takamura, Inui, and Okumura 2005; Kim and Hovy 2004). The co-occurrence-based approaches determine the sentiment polarity of a given word according to the statistical information, like the co-occurrence of the word to predefined sentiment seed words or the co-occurrence to product features. The statistical information is mainly derived from certain corpora. One of the earliest work conducted by Hatzivassiloglou and McKeown (1997) assumes that the conjunction words can convey the polarity relation of the two words they connect. Fo"
J15-1002,E09-1077,0,0.0937027,"n summarization (Hu and Liu 2004). To avoid manually annotating sentiment words, an automatically learning sentiment lexicon has attracted considerable attention in the community of sentiment analysis. The existing work determines word sentiment polarities either by the statistical information (e.g., the co-occurrence of words with predefined sentiment seed words) derived from a large corpus (Riloff, Wiebe, and Wilson 2003; Hu and Liu 2006) or by the word semantic information (e.g., synonym relations) found in existing human-created resources (e.g., WordNet) (Takamura, Inui, and Okumura 2005; Rao and Ravichandran 2009). However, current work mainly focuses on English sentiment lexicon generation or expansion, while sentiment lexicon learning for other languages has not been well studied. In this article, we address the issue of cross-lingual sentiment lexicon learning, which aims to generate sentiment lexicons for a non-English language (hereafter referred to as “the target language”) with the help of the available English sentiment lexicons. The underlying motivation of this task is to leverage the existing English sentiment lexicons and substantial linguistic resources to label the sentiment polarities of"
J15-1002,W03-0404,0,0.0971793,"ds or the co-occurrence to product features. The statistical information is mainly derived from certain corpora. One of the earliest work conducted by Hatzivassiloglou and McKeown (1997) assumes that the conjunction words can convey the polarity relation of the two words they connect. For example, the conjunction word and tends to link two words with the same polarity, whereas the conjunction word but is likely to link two words with opposite polarities. Their approach only considers adjectives, not nouns or verbs, and it is unable to extract adjectives that are not conjoined by conjunctions. Riloff et al. (2003) define several pattern templates and extract sentiment words by two bootstrapping approaches. Turney and Littman (2003) calculate the pointwise mutual information (PMI) of a given word with positive and negative sets of sentiment words. The sentiment polarity of the word is determined by average PMI values of the positive and negative sets. To obtain PMI, they provide queries (consisting of the given word and the sentiment word) to the search engine. The number of hits and the position (if the given word is near the sentiment word) are used to estimate the association of the given word to the"
J15-1002,W11-1704,0,0.104368,"ord relation and English sentiment seed words, they determine the sentiment polarities of the words in the target language. In these two steps, relation-building plays a fundamental role because it is responsible for the transfer of sentiment information between the two languages. Two approaches are often used to connect the words in different languages in the literature. One is based on translation entries in cross-lingual dictionaries (Hassan et al. 2011). The other relies on a machine translation (MT) engine as a black box to translate the sentiment words in English to the target language (Steinberger et al. 2011). The two approaches in Duh, Fujino, and Nagata (2011) and Mihalcea, Banea, and Weibe (2007) tend to use a small set of vocabularies to translate the natural language, which leads to a low coverage of generated sentiment lexicons for the target language. To solve this problem, we propose a generic approach to addressing the task of cross-lingual sentiment lexicon learning. Specifically, we model this task with a bilingual word graph, which is composed of two intra-language subgraphs and an interlanguage subgraph. The intra-language subgraphs are used to model the semantic relations among the w"
J15-1002,P05-1017,0,0.0288596,"gative) word are positive (negative) and its antonyms are negative (positive). Initializing with a set of sentiment words, they expand sentiment lexicons based on these two kinds of word relations. Kamps et al. (2004) build a synonym graph according to the synonym relation (synset) derived from WordNet. The sentiment polarity of a word is calculated by the shortest path to two sentiment words good and bad. However, the shortest path cannot precisely describe the sentiment orientation, considering there are only five steps between the word good and the word bad in WordNet (Hassan et al. 2011). Takamura et al. (2005) construct a word graph with the gloss of WordNet. Words are connected if a word appears in the gloss of another. The word sentiment polarity is determined by the weight of its connections on the word graph. Based on WordNet, Rao and Ravichandran (2009) exploit several graph-based semi-supervised learning methods like Mincuts and Label Propagation. The word polarity orientations are induced by initializing some sentiment seed words in the WordNet graph. Esuli et al. (2006, 2007) and Baccianella et al. (2010) treat sentiment word learning as a machine learning problem, that is, to classify the"
J15-1002,P09-1027,0,0.0759673,"ian) via Google Translator. They believe that those words in the third language that appear in both translation lists are likely to be sentiment words. These approaches connect the words in two languages based on MT engines. The main concern of these approaches is the low overlapping between the vocabularies of natural documents and the vocabularies of the documents translated by MT engines (Duh, Fujino, and Nagata 2011; Meng et al. 2012a). The shortcoming of these MT-based approaches inevitably leads to low coverage. Our task resembles the task of cross-lingual sentiment classification, like Wan (2009), Lu et al. (2011), and Meng et al. (2012a), which classifies the sentiment polarities of product reviews. Generally, these studies use semi-supervised learning approaches and regard translations from labeled English sentiment reviews as the training data. The terms in each review are leveraged as the features for training, which has proven to be effective in sentiment classification (Pang and Lee 2008). We can regard the task of sentiment lexicon learning as word-level sentiment classification. However, for wordlevel sentiment classification, it is not straightforward to extract features for"
J15-1002,W03-1017,0,0.230886,"are from the Department of Computing, the Hong Kong Polytechnic University. E-mail: gaodehong polyu@163.com, cswjli@comp.polyu.edu.hk. Submission received: 28 April 2013; revised submission received: 25 February 2014; accepted for publication: 12 May 2014. doi:10.1162/COLI a 00207 © 2015 Association for Computational Linguistics Computational Linguistics Volume 41, Number 1 1. Introduction A sentiment lexicon is regarded as the most valuable resource for sentiment analysis (Pang and Lee 2008), and lays the groundwork of much sentiment analysis research, for example, sentiment classification (Yu and Hatzivassiloglou 2003; Kim and Hovy 2004) and opinion summarization (Hu and Liu 2004). To avoid manually annotating sentiment words, an automatically learning sentiment lexicon has attracted considerable attention in the community of sentiment analysis. The existing work determines word sentiment polarities either by the statistical information (e.g., the co-occurrence of words with predefined sentiment seed words) derived from a large corpus (Riloff, Wiebe, and Wilson 2003; Hu and Liu 2006) or by the word semantic information (e.g., synonym relations) found in existing human-created resources (e.g., WordNet) (Tak"
J15-1002,D12-1127,0,\N,Missing
J15-1002,kamps-etal-2004-using,0,\N,Missing
li-etal-2006-mining,I05-1037,1,\N,Missing
li-etal-2006-mining,hutchinson-2004-mining,0,\N,Missing
N18-1018,P16-1046,0,0.0723454,"Missing"
N18-1018,N16-1012,0,0.0446173,"lt to understand. Compared with the statistic model, WEAN generates a more fluent sentence. Besides, WEAN can capture the semantic meaning of the word by querying the word embeddings, so the generated sentence is semantically correct, 5 Related Work Our work is related to the encoder-decoder framework (Cho et al., 2014) and the attention mechanism (Bahdanau et al., 2014). Encoderdecoder framework, like sequence-to-sequence model, has achieved success in machine translation (Sutskever et al., 2014; Jean et al., 2015; Luong et al., 2015; Lin et al., 2018), text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; Wang et al., 2017; Ma and Sun, 2017), and other natural language processing tasks (Liu et al., 2017). There are many other methods to improve neural attention model (Jean et al., 2015; Luong et al., 2015). Zhu et al. (2010) constructs a wikipedia dataset, and proposes a tree-based simplification model. Woodsend and Lapata (2011) introduces a datadriven model based on quasi-synchronous grammar, which captures structural mismatches and complex rewrite operations. Wubben et al. (2012) 203 Acknowledgements presents a method for text simplification using phrase based machi"
N18-1018,P16-1154,0,0.23065,"-SEW test set, so that we can compare our model with these systems. • Seq2seq is our implementation of the sequence-to-sequence model with attention mechanism, which is the most popular neural model for text generation. • NTS and NTS-w2v (Nisioi et al., 2017) are two sequence-to-sequence model with extra mechanism like prediction ranking, and NTS-w2v uses a pretrain word2vec. • DRESS and DRESS-LS (Zhang and Lapata, 2017) are two deep reinforcement learning 200 LCSTS RNN-W(Hu et al., 2015) RNN(Hu et al., 2015) RNN-cont-W(Hu et al., 2015) RNN-cont(Hu et al., 2015) SRB(Ma et al., 2017) CopyNet-W(Gu et al., 2016) CopyNet(Gu et al., 2016) RNN-dist(Chen et al., 2016) DRGD(Li et al., 2017) Seq2seq WEAN R-1 17.7 21.5 26.8 29.9 33.3 35.0 34.4 35.2 37.0 32.1 37.8 R-2 8.5 8.9 16.1 17.4 20.0 22.3 21.6 22.6 24.2 19.9 25.6 R-L 15.8 18.6 24.1 27.2 30.1 32.0 31.3 32.5 34.2 29.2 35.2 website called Sina Weibo.4 It is split into three parts, with 2,400,591 pairs in PART I, 10,666 pairs in PART II and 1,106 pairs in PART III. All the text-summary pairs in PART II and PART III are manually annotated with relevant scores ranged from 1 to 5. We only reserve pairs with scores no less than 3, leaving 8,685 pairs in PART"
N18-1018,D15-1229,0,0.48478,"contains 359 sentence pairs. Besides, each complex sentence is paired with 8 reference simplified sentences provided by Amazon Mechanical Turk workers. i We use Adam optimization method to train the model, with the default hyper-parameters: the learning rate α = 0.001, and β1 = 0.9, β2 = 0.999,  = 1e − 8. 3 Experiments Following the previous work (Cao et al., 2017), we test our model on the following two paraphrase orientated tasks: text simplification and short text abstractive summarization. 3.1 Text Simplification 3.1.2 Evaluation Metrics Following the previous work (Nisioi et al., 2017; Hu et al., 2015), we evaluate our model with different metrics on two tasks. 3.1.1 Datasets The datasets are both from the alignments between English Wikipedia website2 and Simple English Wikipedia website.3 The Simple English Wikipedia is built for “the children and adults who are learning the English language”, and the articles are composed with “easy words and short sentences”. Therefore, Simple English Wikipedia is a natural public simplified text corpus. • Automatic evaluation. We use the BLEU score (Papineni et al., 2002) as the automatic evaluation metric. BLEU is a widely used metric for machine trans"
N18-1018,P17-2100,1,0.925978,"f PBMTR and SBMT-SARI on EW-SEW test set, so that we can compare our model with these systems. • Seq2seq is our implementation of the sequence-to-sequence model with attention mechanism, which is the most popular neural model for text generation. • NTS and NTS-w2v (Nisioi et al., 2017) are two sequence-to-sequence model with extra mechanism like prediction ranking, and NTS-w2v uses a pretrain word2vec. • DRESS and DRESS-LS (Zhang and Lapata, 2017) are two deep reinforcement learning 200 LCSTS RNN-W(Hu et al., 2015) RNN(Hu et al., 2015) RNN-cont-W(Hu et al., 2015) RNN-cont(Hu et al., 2015) SRB(Ma et al., 2017) CopyNet-W(Gu et al., 2016) CopyNet(Gu et al., 2016) RNN-dist(Chen et al., 2016) DRGD(Li et al., 2017) Seq2seq WEAN R-1 17.7 21.5 26.8 29.9 33.3 35.0 34.4 35.2 37.0 32.1 37.8 R-2 8.5 8.9 16.1 17.4 20.0 22.3 21.6 22.6 24.2 19.9 25.6 R-L 15.8 18.6 24.1 27.2 30.1 32.0 31.3 32.5 34.2 29.2 35.2 website called Sina Weibo.4 It is split into three parts, with 2,400,591 pairs in PART I, 10,666 pairs in PART II and 1,106 pairs in PART III. All the text-summary pairs in PART II and PART III are manually annotated with relevant scores ranged from 1 to 5. We only reserve pairs with scores no less than 3, l"
N18-1018,N15-1022,0,0.189994,"ioi et al. (2017) and Zhang et al. (2017), we ask the human raters to rate the simplified text in three dimensions: Fluency, Adequacy and Simplicity. Fluency assesses whether the outputs are grammatically right and well formed. Adequacy represents the meaning preservation of the simplified text. Both the scores of fluency and adequacy range from 1 to 5 (1 is very bad and 5 is very good). Simplicity shows how simpler the model outputs are than the source text, which ranges from 1 to 5. • English Wikipedia and Simple English Wikipedia (EW-SEW). EW-SEW is a publicly available dataset provided by Hwang et al. (2015). To build the corpus, they first align the complex-simple sentence pairs, score the semantic similarity between the complex sentence and the simple sentence, and classify 2 3 3.1.3 Settings Our proposed model is based on the encoderdecoder framework. The encoder is implemented on LSTM, and the decoder is based on LSTM with Luong style attention (Luong et al., 2015). We http://en.wikipedia.org http://simple.wikipedia.org 199 PWKP PBMT (Wubben et al., 2012) Hybrid (Narayan and Gardent, 2014) EncDecA (Zhang and Lapata, 2017) DRESS (Zhang and Lapata, 2017) DRESS-LS (Zhang and Lapata, 2017) Seq2se"
N18-1018,P15-1001,0,0.0312775,"erate a fluent sentence, but the meaning is different from the source text, and even more difficult to understand. Compared with the statistic model, WEAN generates a more fluent sentence. Besides, WEAN can capture the semantic meaning of the word by querying the word embeddings, so the generated sentence is semantically correct, 5 Related Work Our work is related to the encoder-decoder framework (Cho et al., 2014) and the attention mechanism (Bahdanau et al., 2014). Encoderdecoder framework, like sequence-to-sequence model, has achieved success in machine translation (Sutskever et al., 2014; Jean et al., 2015; Luong et al., 2015; Lin et al., 2018), text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; Wang et al., 2017; Ma and Sun, 2017), and other natural language processing tasks (Liu et al., 2017). There are many other methods to improve neural attention model (Jean et al., 2015; Luong et al., 2015). Zhu et al. (2010) constructs a wikipedia dataset, and proposes a tree-based simplification model. Woodsend and Lapata (2011) introduces a datadriven model based on quasi-synchronous grammar, which captures structural mismatches and complex rewrite operations. Wubben et"
N18-1018,K16-1028,0,0.124083,"SARI. 3.1.5 3.1.4 Baselines We compare our model with several neural text simplification systems. Results We compare WEAN with state-of-the-art models for text simplification. Table 1 and Table 2 summarize the results of the automatic evaluation. On PWKP dataset, we compare WEAN with PBMT, Hybrid, EncDecA, DRESS and DRESSLS. WEAN achieves a BLEU score of 54.54, outperforming all of the previous systems. On EWSEW dataset, we compare WEAN with PBMT-R, Hybrid, SBMT-SARI, and the neural models described above. We do not find any public release code of PBMT-R and SBMT-SARI. Fortunately, Xu et al. (2016) provides the predictions of PBMTR and SBMT-SARI on EW-SEW test set, so that we can compare our model with these systems. • Seq2seq is our implementation of the sequence-to-sequence model with attention mechanism, which is the most popular neural model for text generation. • NTS and NTS-w2v (Nisioi et al., 2017) are two sequence-to-sequence model with extra mechanism like prediction ranking, and NTS-w2v uses a pretrain word2vec. • DRESS and DRESS-LS (Zhang and Lapata, 2017) are two deep reinforcement learning 200 LCSTS RNN-W(Hu et al., 2015) RNN(Hu et al., 2015) RNN-cont-W(Hu et al., 2015) RNN"
N18-1018,P13-1151,0,0.0185862,"Ma and Sun, 2017), and other natural language processing tasks (Liu et al., 2017). There are many other methods to improve neural attention model (Jean et al., 2015; Luong et al., 2015). Zhu et al. (2010) constructs a wikipedia dataset, and proposes a tree-based simplification model. Woodsend and Lapata (2011) introduces a datadriven model based on quasi-synchronous grammar, which captures structural mismatches and complex rewrite operations. Wubben et al. (2012) 203 Acknowledgements presents a method for text simplification using phrase based machine translation with re-ranking the outputs. Kauchak (2013) proposes a text simplification corpus, and evaluates language modeling for text simplification on the proposed corpus. Narayan and Gardent (2014) propose a hybrid approach to sentence simplification which combines deep semantics and monolingual machine translation. Hwang et al. (2015) introduces a parallel simplification corpus by evaluating the similarity between the source text and the simplified ˇ text based on WordNet. Glavaˇs and Stajner (2015) propose an unsupervised approach to lexical simplification that makes use of word vectors and require only regular corpora. Xu et al. (2016) desi"
N18-1018,P14-1041,0,0.190927,". • English Wikipedia and Simple English Wikipedia (EW-SEW). EW-SEW is a publicly available dataset provided by Hwang et al. (2015). To build the corpus, they first align the complex-simple sentence pairs, score the semantic similarity between the complex sentence and the simple sentence, and classify 2 3 3.1.3 Settings Our proposed model is based on the encoderdecoder framework. The encoder is implemented on LSTM, and the decoder is based on LSTM with Luong style attention (Luong et al., 2015). We http://en.wikipedia.org http://simple.wikipedia.org 199 PWKP PBMT (Wubben et al., 2012) Hybrid (Narayan and Gardent, 2014) EncDecA (Zhang and Lapata, 2017) DRESS (Zhang and Lapata, 2017) DRESS-LS (Zhang and Lapata, 2017) Seq2seq (our implementation) WEAN (our proposal) BLEU 46.31 53.94 47.93 34.53 36.32 48.26 54.54 PWKP NTS-w2v DRESS-LS WEAN Reference EW-SEW Fluency Adequacy Simplicity All PBMT-R 3.36 2.92 3.37 3.22 SBMT-SARI 3.41 3.63 3.25 3.43 NTS-w2v 3.56 3.52 3.42 3.50 DRESS-LS 3.59 3.43 3.65 3.56 WEAN 3.61 3.56 3.65 3.61 Reference 3.71 3.64 3.45 3.60 Table 1: Automatic evaluation of our model and other related systems on PWKP datasets. The results are reported on the test sets. EW-SEW PBMT-R (Wubben et al.,"
N18-1018,P17-2014,0,0.141044,"e decoder input and the predicted output share the same vocabulary and word embeddings. Besides, we do not use any pretrained word embeddings in our model, so that all of the parameters are learned from scratch. 2.5 Training Although our generator is a retrieval style, WEAN is as differentiable as the sequence-to-sequence model. The objective of training is to minimize the cross entropy between the predicted word probability distribution and the golden one-hot distribution: X L=− yˆi log p(yi ) (7) each sentence pair as a good, good partial, partial, or bad match. Following the previous work (Nisioi et al., 2017), we discard the unclassified matches, and use the good matches and partial matches with a scaled threshold greater than 0.45. The corpus contains about 150K good matches and 130K good partial matches. We use this corpus as the training set, and the dataset provided by Xu et al. (Xu et al., 2016) as the validation set and the test set. The validation set consists of 2,000 sentence pairs, and the test set contains 359 sentence pairs. Besides, each complex sentence is paired with 8 reference simplified sentences provided by Amazon Mechanical Turk workers. i We use Adam optimization method to tra"
N18-1018,D17-1222,0,0.497871,"eq is our implementation of the sequence-to-sequence model with attention mechanism, which is the most popular neural model for text generation. • NTS and NTS-w2v (Nisioi et al., 2017) are two sequence-to-sequence model with extra mechanism like prediction ranking, and NTS-w2v uses a pretrain word2vec. • DRESS and DRESS-LS (Zhang and Lapata, 2017) are two deep reinforcement learning 200 LCSTS RNN-W(Hu et al., 2015) RNN(Hu et al., 2015) RNN-cont-W(Hu et al., 2015) RNN-cont(Hu et al., 2015) SRB(Ma et al., 2017) CopyNet-W(Gu et al., 2016) CopyNet(Gu et al., 2016) RNN-dist(Chen et al., 2016) DRGD(Li et al., 2017) Seq2seq WEAN R-1 17.7 21.5 26.8 29.9 33.3 35.0 34.4 35.2 37.0 32.1 37.8 R-2 8.5 8.9 16.1 17.4 20.0 22.3 21.6 22.6 24.2 19.9 25.6 R-L 15.8 18.6 24.1 27.2 30.1 32.0 31.3 32.5 34.2 29.2 35.2 website called Sina Weibo.4 It is split into three parts, with 2,400,591 pairs in PART I, 10,666 pairs in PART II and 1,106 pairs in PART III. All the text-summary pairs in PART II and PART III are manually annotated with relevant scores ranged from 1 to 5. We only reserve pairs with scores no less than 3, leaving 8,685 pairs in PART II and 725 pairs in PART III. Following the previous work (Hu et al., 2015)"
N18-1018,P02-1040,0,0.104711,"implification 3.1.2 Evaluation Metrics Following the previous work (Nisioi et al., 2017; Hu et al., 2015), we evaluate our model with different metrics on two tasks. 3.1.1 Datasets The datasets are both from the alignments between English Wikipedia website2 and Simple English Wikipedia website.3 The Simple English Wikipedia is built for “the children and adults who are learning the English language”, and the articles are composed with “easy words and short sentences”. Therefore, Simple English Wikipedia is a natural public simplified text corpus. • Automatic evaluation. We use the BLEU score (Papineni et al., 2002) as the automatic evaluation metric. BLEU is a widely used metric for machine translation and text simplification, which measures the agreement between the model outputs and the gold references. The references can be either single or multiple. In our experiments, the references are single on PWKP, and multiple on EW-SEW. • Parallel Wikipedia Simplification Corpus (PWKP). PWKP (Zhu et al., 2010) is a widely used benchmark for evaluating text simplification systems. It consists of aligned complex text from English WikiPedia (as of Aug. 22nd, 2009) and simple text from Simple Wikipedia (as of Aug"
N18-1018,N03-1020,0,0.390902,"pairs in PART II and PART III are manually annotated with relevant scores ranged from 1 to 5. We only reserve pairs with scores no less than 3, leaving 8,685 pairs in PART II and 725 pairs in PART III. Following the previous work (Hu et al., 2015), we use PART I as training set, PART II as validation set, and PART III as test set. 3.2.2 Table 4: ROUGE F1 score on the LCSTS test set. R1, R-2, and R-L denote ROUGE-1, ROUGE-2, and ROUGE-L, respectively. The models with a suffix of ‘W’ in the table are word-based, while the rest of models are character-based. Our evaluation metric is ROUGE score (Lin and Hovy, 2003), which is popular for summarization evaluation. The metrics compare an automatically produced summary against the reference summaries, by computing overlapping lexical units, including unigram, bigram, trigram, and longest common subsequence (LCS). Following previous work (Rush et al., 2015; Hu et al., 2015), we use ROUGE-1 (unigram), ROUGE-2 (bi-gram) and ROUGE-L (LCS) as the evaluation metrics in the reported experimental results. It shows that the neural models have better performance in BLEU, and WEAN achieves the best BLEU score with 94.45. We perform the human evaluation of WEAN and oth"
N18-1018,C16-1275,0,0.226628,"Missing"
N18-1018,D15-1044,0,0.294351,"idation set, and PART III as test set. 3.2.2 Table 4: ROUGE F1 score on the LCSTS test set. R1, R-2, and R-L denote ROUGE-1, ROUGE-2, and ROUGE-L, respectively. The models with a suffix of ‘W’ in the table are word-based, while the rest of models are character-based. Our evaluation metric is ROUGE score (Lin and Hovy, 2003), which is popular for summarization evaluation. The metrics compare an automatically produced summary against the reference summaries, by computing overlapping lexical units, including unigram, bigram, trigram, and longest common subsequence (LCS). Following previous work (Rush et al., 2015; Hu et al., 2015), we use ROUGE-1 (unigram), ROUGE-2 (bi-gram) and ROUGE-L (LCS) as the evaluation metrics in the reported experimental results. It shows that the neural models have better performance in BLEU, and WEAN achieves the best BLEU score with 94.45. We perform the human evaluation of WEAN and other related systems, and the results are shown in Table 3. DRESS-LS is based on the reinforcement learning, and it encourages the fluency, simplicity and relevance of the outputs. Therefore, it achieves a high score in our human evaluation. WEAN gains a even better score than DRESS-LS. Beside"
N18-1018,D17-1062,0,0.19844,"he gold references. The references can be either single or multiple. In our experiments, the references are single on PWKP, and multiple on EW-SEW. • Parallel Wikipedia Simplification Corpus (PWKP). PWKP (Zhu et al., 2010) is a widely used benchmark for evaluating text simplification systems. It consists of aligned complex text from English WikiPedia (as of Aug. 22nd, 2009) and simple text from Simple Wikipedia (as of Aug. 17th, 2009). The dataset contains 108,016 sentence pairs, with 25.01 words on average per complex sentence and 20.87 words per simple sentence. Following the previous work (Zhang and Lapata, 2017), we remove the duplicate sentence pairs, and split the corpus with 89,042 pairs for training, 205 pairs for validation and 100 pairs for test. • Human evaluation. Human evaluation is essential to evaluate the quality of the model outputs. Following Nisioi et al. (2017) and Zhang et al. (2017), we ask the human raters to rate the simplified text in three dimensions: Fluency, Adequacy and Simplicity. Fluency assesses whether the outputs are grammatically right and well formed. Adequacy represents the meaning preservation of the simplified text. Both the scores of fluency and adequacy range from"
N18-1018,C10-1152,0,0.182865,", and the articles are composed with “easy words and short sentences”. Therefore, Simple English Wikipedia is a natural public simplified text corpus. • Automatic evaluation. We use the BLEU score (Papineni et al., 2002) as the automatic evaluation metric. BLEU is a widely used metric for machine translation and text simplification, which measures the agreement between the model outputs and the gold references. The references can be either single or multiple. In our experiments, the references are single on PWKP, and multiple on EW-SEW. • Parallel Wikipedia Simplification Corpus (PWKP). PWKP (Zhu et al., 2010) is a widely used benchmark for evaluating text simplification systems. It consists of aligned complex text from English WikiPedia (as of Aug. 22nd, 2009) and simple text from Simple Wikipedia (as of Aug. 17th, 2009). The dataset contains 108,016 sentence pairs, with 25.01 words on average per complex sentence and 20.87 words per simple sentence. Following the previous work (Zhang and Lapata, 2017), we remove the duplicate sentence pairs, and split the corpus with 89,042 pairs for training, 205 pairs for validation and 100 pairs for test. • Human evaluation. Human evaluation is essential to ev"
N18-1018,D16-1112,0,0.160819,"Missing"
N18-1018,D17-1020,0,0.0247035,"model, WEAN generates a more fluent sentence. Besides, WEAN can capture the semantic meaning of the word by querying the word embeddings, so the generated sentence is semantically correct, 5 Related Work Our work is related to the encoder-decoder framework (Cho et al., 2014) and the attention mechanism (Bahdanau et al., 2014). Encoderdecoder framework, like sequence-to-sequence model, has achieved success in machine translation (Sutskever et al., 2014; Jean et al., 2015; Luong et al., 2015; Lin et al., 2018), text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; Wang et al., 2017; Ma and Sun, 2017), and other natural language processing tasks (Liu et al., 2017). There are many other methods to improve neural attention model (Jean et al., 2015; Luong et al., 2015). Zhu et al. (2010) constructs a wikipedia dataset, and proposes a tree-based simplification model. Woodsend and Lapata (2011) introduces a datadriven model based on quasi-synchronous grammar, which captures structural mismatches and complex rewrite operations. Wubben et al. (2012) 203 Acknowledgements presents a method for text simplification using phrase based machine translation with re-ranking the outputs."
N18-1018,D11-1038,0,0.0306716,"(Bahdanau et al., 2014). Encoderdecoder framework, like sequence-to-sequence model, has achieved success in machine translation (Sutskever et al., 2014; Jean et al., 2015; Luong et al., 2015; Lin et al., 2018), text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; Wang et al., 2017; Ma and Sun, 2017), and other natural language processing tasks (Liu et al., 2017). There are many other methods to improve neural attention model (Jean et al., 2015; Luong et al., 2015). Zhu et al. (2010) constructs a wikipedia dataset, and proposes a tree-based simplification model. Woodsend and Lapata (2011) introduces a datadriven model based on quasi-synchronous grammar, which captures structural mismatches and complex rewrite operations. Wubben et al. (2012) 203 Acknowledgements presents a method for text simplification using phrase based machine translation with re-ranking the outputs. Kauchak (2013) proposes a text simplification corpus, and evaluates language modeling for text simplification on the proposed corpus. Narayan and Gardent (2014) propose a hybrid approach to sentence simplification which combines deep semantics and monolingual machine translation. Hwang et al. (2015) introduces"
N18-1018,P12-1107,0,0.0647051,"Missing"
P04-1074,W99-0613,0,0.0160745,"Missing"
P04-1074,P92-1001,0,0.296954,"Missing"
P04-1074,J00-4004,0,0.0513276,"Missing"
P04-1074,P94-1013,0,0.0601715,"ts. Notice that the mapping is not one-to-one. For example, adverbs affect tense/aspect as well as discourse structure. For another example, tense/aspect can be affected by auxiliary words, trend verbs, etc. This shows that classification of temporal indicators based on partof-speech (POS) information alone cannot determine relative temporal relations. 3 Machine Learning Approaches for Relative Relation Resolution Previous efforts in corpus-based natural language processing have incorporated machine learning methods to coordinate multiple linguistic features for example in accent restoration (Yarowsky, 1994) and event classification (Siegel and McKeown, 1998), etc. Relative relation resolution can be modeled as a relation classification task. We model the thirteen relative temporal relations (see Figure 1) as the classes to be decided by a classifier. The resolution process is to assign an event pair (i.e. the two events under concern)2 to one class according to their linguistic features. For this purpose, we train two classifiers, a Probabilistic Decision Tree Classifier (PDT) and a Naïve Bayesian Classifier (NBC). We then combine the results by the Collaborative Bootstrapping (CB) technique whi"
P04-1074,E95-1035,0,\N,Missing
P06-1047,W04-1017,0,0.225201,"Missing"
P06-1047,W03-0502,0,0.0620901,"Missing"
P06-1047,P05-3013,0,0.00625203,"ir term vectors was used to generate links and define link strength. The same idea was followed and investigated exten370 occurrences. They roughly relate to “did What”. One or more associated named entities are considered as what are denoted by linguists as event arguments. Four types of named entities are currently under the consideration. These are &lt;Person&gt;, &lt;Organization&gt;, &lt;Location&gt; and &lt;Date&gt;. They convey the information of “Who”, “Whom”, “When” and “Where”. A verb or an action noun is deemed as an event term only when it presents itself at least once between two named entities. sively (Mihalcea, 2005). Yoshioka and Haraguchi (2004) went one step further toward eventbased summarization. Two sentences were linked if they shared similar events. When tested on TSC-3, the approach favoured longer summaries. In contrast, the importance of the verbs and nouns constructing events was evaluated with PageRank as individual nodes aligned by their dependence relations (Vanderwende, 2004; Leskovec, 2004). Although we agree that the fabric of event constitutions constructed by their syntactic relations can help dig out the important events, we have two comments. First, not all verbs denote event happeni"
P06-1047,P05-1018,0,0.0200939,"Missing"
P06-1047,N04-3012,0,0.0634733,"Missing"
P06-1047,N03-1020,0,\N,Missing
P06-1125,J90-2002,0,0.342873,"Missing"
P06-1125,W06-2808,1,0.654156,"Missing"
P06-1125,xia-etal-2006-constructing,1,0.754567,"estimation method with Chinese character trigram model on NIL corpus. In our implementation, Katz Backoff smoothing technique (Katz, 1987) is used to handle the sparse data problem, and Viterbi algorithm is employed to find the optimal solution in XSCM. 6 6.1 Evaluation Data Description Training Sets Two types of training data are used in our experiments. We use news from Xinhua News Agency in LDC Chinese Gigaword v.2 (CNGIGA) (Graf et al., 2005) as standard Chinese corpus to construct phonetic mapping models because of its excellent coverage of standard Simplified Chinese. We use NIL corpus (Xia et al., 2006b) as chat language corpus. To evaluate our methods on size-varying training data, six chat language corpora are created based on NIL corpus. We select 6056 sentences from NIL corpus randomly to make the first chat language corpus, i.e. C#1. In every next corpus, we add extra 1,211 random sentences. So 7,267 sentences are contained in C#2, 8,478 in C#3, 9,689 in C#4, 10,200 in C#5, and 12,113 in C#6. Test Sets Test sets are used to prove that chat language is dynamic and XSCM is effective and robust in normalizing dynamic chat language terms. Six time-varying test sets, i.e. T#1 ~ T#6, are cre"
P06-1125,I05-3013,1,\N,Missing
P07-2047,W04-1017,0,0.0566291,"Missing"
P07-2047,W04-3205,0,0.223325,"Missing"
P07-2047,P06-1047,1,0.858338,"Science and Technology mfliu_china@hotmail.com Most existing event-based summarization approaches rely on the statistical features derived from documents and generally associated with Event-based summarization extracts and single events, but they neglect the relations among organizes summary sentences in terms of events. However, events are commonly related the events that the sentences describe. In with one another especially when the documents to this work, we focus on semantic relations be summarized are about the same or very similar among event terms. By connecting terms topics. Li et al (2006) report that the improved with relations, we build up event term performance can be achieved by taking into graph, upon which relevant terms are account of event distributional similarities, but it grouped into clusters. We assume that each does not benefit much from semantic similarities. cluster represents a topic of documents. This motivated us to further investigate whether Then two summarization strategies are event-based summarization can take advantage of investigated, i.e. selecting one term as the the semantic relations of event terms, and most representative of each topic so as to co"
P07-2047,N03-1020,0,\N,Missing
P08-2023,H05-1091,0,0.0999416,"ck of necessary co-referenced mentions might be the main reason. 2 Related Work Many approaches have been proposed in the literature of relation extraction. Among them, feature-based and kernel-based approaches are most popular. Kernel-based approaches exploit the structure of the tree that connects two entities. Zelenko et al (2003) proposed a kernel over two parse trees, which recursively matched nodes from roots to leaves in a top-down manner. Culotta and Sorensen (2004) extended this work to estimate similarity between augmented dependency trees. The above two work was further advanced by Bunescu and Mooney (2005) who argued that the information to extract a relation between two entities can be typically captured by the shortest path between them in the dependency graph. Later, Zhang et al (2006) developed a composite kernel that combined parse tree kernel with entity kernel and Zhou et al (2007) experimented with a context-sensitive kernel by automatically determining context-sensitive tree spans. In the feature-based framework, Kambhatla (2004) employed ME models to combine diverse lexical, syntactic and semantic features derived from word, entity type, mention level, overlap, dependency and parse tr"
P08-2023,P04-1054,0,0.1814,"less effort than applying deep natural language processing. But unfortunately, entity co-reference does not help as much as we have expected. The lack of necessary co-referenced mentions might be the main reason. 2 Related Work Many approaches have been proposed in the literature of relation extraction. Among them, feature-based and kernel-based approaches are most popular. Kernel-based approaches exploit the structure of the tree that connects two entities. Zelenko et al (2003) proposed a kernel over two parse trees, which recursively matched nodes from roots to leaves in a top-down manner. Culotta and Sorensen (2004) extended this work to estimate similarity between augmented dependency trees. The above two work was further advanced by Bunescu and Mooney (2005) who argued that the information to extract a relation between two entities can be typically captured by the shortest path between them in the dependency graph. Later, Zhang et al (2006) developed a composite kernel that combined parse tree kernel with entity kernel and Zhou et al (2007) experimented with a context-sensitive kernel by automatically determining context-sensitive tree spans. In the feature-based framework, Kambhatla (2004) employed ME"
P08-2023,N07-1015,0,0.0958831,"y determining context-sensitive tree spans. In the feature-based framework, Kambhatla (2004) employed ME models to combine diverse lexical, syntactic and semantic features derived from word, entity type, mention level, overlap, dependency and parse tree. Based on his work, Zhou et al (2005) 89 Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 89–92, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list. Jiang and Zhai (2007) then systematically explored a large space of features and evaluated the effectiveness of different feature subspaces corresponding to sequence, syntactic parse tree and dependency parse tree. Their experiments showed that using only the basic unit features within each feature subspace can already achieve state-of-art performance, while over-inclusion of complex features might hurt the performance. Previous approaches mainly focused on English relations. Most of them were evaluated on the ACE 2004 data set (or a sub set of it) which defined 7 relation types and 23 subtypes. Although Chinese p"
P08-2023,P05-1053,0,0.169738,"formation to extract a relation between two entities can be typically captured by the shortest path between them in the dependency graph. Later, Zhang et al (2006) developed a composite kernel that combined parse tree kernel with entity kernel and Zhou et al (2007) experimented with a context-sensitive kernel by automatically determining context-sensitive tree spans. In the feature-based framework, Kambhatla (2004) employed ME models to combine diverse lexical, syntactic and semantic features derived from word, entity type, mention level, overlap, dependency and parse tree. Based on his work, Zhou et al (2005) 89 Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 89–92, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list. Jiang and Zhai (2007) then systematically explored a large space of features and evaluated the effectiveness of different feature subspaces corresponding to sequence, syntactic parse tree and dependency parse tree. Their experiments showed that using only the basic unit features within each"
P08-2023,D07-1076,0,0.0581352,"ects two entities. Zelenko et al (2003) proposed a kernel over two parse trees, which recursively matched nodes from roots to leaves in a top-down manner. Culotta and Sorensen (2004) extended this work to estimate similarity between augmented dependency trees. The above two work was further advanced by Bunescu and Mooney (2005) who argued that the information to extract a relation between two entities can be typically captured by the shortest path between them in the dependency graph. Later, Zhang et al (2006) developed a composite kernel that combined parse tree kernel with entity kernel and Zhou et al (2007) experimented with a context-sensitive kernel by automatically determining context-sensitive tree spans. In the feature-based framework, Kambhatla (2004) employed ME models to combine diverse lexical, syntactic and semantic features derived from word, entity type, mention level, overlap, dependency and parse tree. Based on his work, Zhou et al (2005) 89 Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 89–92, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics further incorporated the base phrase chunking information and semi-automatically collecte"
P08-2023,I05-2023,0,0.101769,"Missing"
P08-2023,P06-1104,0,\N,Missing
P09-2029,N03-1020,0,0.0990589,"For the words in the hierarchical tree, set the initial states of the top n words3 as “activated” and the states of other words as “inactivated”. 2: For all the sentences in the document set, 3 4 Experiment Experiments are conducted on the DUC 2007 data set which contains 45 document sets. Each document set consists of 25 documents and a topic description as the query. In the task definition, the length of the summary is limited to 250 words. In our summarization system, preprocessing includes stop-word removal and word stemming (conducted by GATE4). One of the DUC evaluation methods, ROUGE (Lin and Hovy, 2003), is used to evaluate the content of the generated summaries. ROUGE is a state-of-the-art automatic evaluation method based on N-gram matching between system summaries and human summaries. In the experiment, our system is compared to the top systems in DUC 2007. Moreover, a baseline system which considers only the frequencies of words but ignores the relations between words is included for comparison. Table 1 below shows the average recalls of ROUGE-1, ROUGE-2 and ROUGE-SU4 over the 45 DUC 2007document sets. In the experiment, the proposed summarization system outperforms the baseline system,"
P09-2030,C00-1072,0,0.0223101,"elaborately designed to characterize the different aspects of the sentences. They have been extensively investigated in the past due to their easy implementation and the ability to achieve promising results. The use of featurebased ranking has led to many successful (e.g. top five) systems in DUC 2005-2007 queryfocused summarization (Over et al., 2007). A variety of statistical and linguistic features, such as term distribution, sentence length, sentence position, and named entity, etc., can be found in literature. Among them, query relevance, centroid (Radev et al., 2004) and signature term (Lin and Hovy, 2000) are most remarkable. There are two alternative approaches to integrate the features. One is to combine features into a unified representation first, and then use it to rank the sentences. The other is to utilize rank fusion or rank aggregation techniques to combine the ranking results (orders, ranks or scores) produced by the multiple ranking functions into a unified rank. The most popular implementation of the latter approaches is to linearly combine the features to obtain an overall score which is then used as the ranking criterion. The weights of the features are either experimentally tune"
P09-2030,N03-1020,0,0.315524,"Missing"
P10-2055,H05-1115,0,0.0842511,"Missing"
P12-1027,W06-1655,0,0.141199,"entation. 2.2 Online Training Related Work First, we review related work on word segmentation and new word detection. Then, we review popular online training methods, in particular stochastic gradient descent (SGD). 2.1 Word Segmentation and New Word Detection Conventional approaches to Chinese word segmentation treat the problem as a sequential labeling task (Xue, 2003; Peng et al., 2004; Tseng et al., 2005; Asahara et al., 2005; Zhao et al., 2010). To achieve high accuracy, most of the stateof-the-art systems are heavy probabilistic systems using semi-Markov assumptions or latent variables (Andrew, 2006; Sun et al., 2009b). For example, one of the state-of-the-art CWS system is the latent variable conditional random field (Sun et al., 2008; Sun and Tsujii, 2009) system presented in Sun et al. (2009b). It is a heavy probabilistic model and it is slow in training. A few other state-of-the-art CWS systems are using semi-Markov perceptron methods or voting systems based on multiple semi-Markov 254 The most representative online training method is the SGD method. The SGD uses a small randomly-selected subset of the training samples to approximate the gradient of an objective function. The number"
P12-1027,I05-3018,0,0.160445,"nefit both segmentation and new word detection. Our method provides a convenient framework for doing this. Our new word detection is not a standalone process, but an integral part of segmentation. 2.2 Online Training Related Work First, we review related work on word segmentation and new word detection. Then, we review popular online training methods, in particular stochastic gradient descent (SGD). 2.1 Word Segmentation and New Word Detection Conventional approaches to Chinese word segmentation treat the problem as a sequential labeling task (Xue, 2003; Peng et al., 2004; Tseng et al., 2005; Asahara et al., 2005; Zhao et al., 2010). To achieve high accuracy, most of the stateof-the-art systems are heavy probabilistic systems using semi-Markov assumptions or latent variables (Andrew, 2006; Sun et al., 2009b). For example, one of the state-of-the-art CWS system is the latent variable conditional random field (Sun et al., 2008; Sun and Tsujii, 2009) system presented in Sun et al. (2009b). It is a heavy probabilistic model and it is slow in training. A few other state-of-the-art CWS systems are using semi-Markov perceptron methods or voting systems based on multiple semi-Markov 254 The most representativ"
P12-1027,O98-3002,0,0.582016,"accuracies on both word segmentation and new word detection. 2 perceptron segmenters (Zhang and Clark, 2007; Sun, 2010). Those semi-Markov perceptron systems are moderately faster than the heavy probabilistic systems using semi-Markov conditional random fields or latent variable conditional random fields. However, a disadvantage of the perceptron style systems is that they can not provide probabilistic information. On the other hand, new word detection is also one of the important problems in Chinese information processing. Many statistical approaches have been proposed (J. Nie and Jin, 1995; Chen and Bai, 1998; Wu and Jiang, 2000; Peng et al., 2004; Chen and Ma, 2002; Zhou, 2005; Goh et al., 2003; Fu and Luke, 2004; Wu et al., 2011). New word detection is normally considered as a separate process from segmentation. There were studies trying to solve this problem jointly with CWS. However, the current studies are limited. Integrating the two tasks would benefit both segmentation and new word detection. Our method provides a convenient framework for doing this. Our new word detection is not a standalone process, but an integral part of segmentation. 2.2 Online Training Related Work First, we review r"
P12-1027,C02-1049,0,0.113216,". 2 perceptron segmenters (Zhang and Clark, 2007; Sun, 2010). Those semi-Markov perceptron systems are moderately faster than the heavy probabilistic systems using semi-Markov conditional random fields or latent variable conditional random fields. However, a disadvantage of the perceptron style systems is that they can not provide probabilistic information. On the other hand, new word detection is also one of the important problems in Chinese information processing. Many statistical approaches have been proposed (J. Nie and Jin, 1995; Chen and Bai, 1998; Wu and Jiang, 2000; Peng et al., 2004; Chen and Ma, 2002; Zhou, 2005; Goh et al., 2003; Fu and Luke, 2004; Wu et al., 2011). New word detection is normally considered as a separate process from segmentation. There were studies trying to solve this problem jointly with CWS. However, the current studies are limited. Integrating the two tasks would benefit both segmentation and new word detection. Our method provides a convenient framework for doing this. Our new word detection is not a standalone process, but an integral part of segmentation. 2.2 Online Training Related Work First, we review related work on word segmentation and new word detection. T"
P12-1027,I05-3019,0,0.188076,"Missing"
P12-1027,I05-3017,0,0.499942,"R), City University of Hongkong (CU), and Peking University (PKU). Details of the corpora are listed in Table 1. We did not use any extra resources such as common surnames, parts-of-speech, and semantics. Four metrics were used to evaluate segmentation results: recall (R, the percentage of gold standard output words that are correctly segmented by the decoder), precision (P , the percentage of words in the decoder output that are segmented correctly), balanced F-score defined by 2P R/(P + R), and recall of new word detection (NWD recall). For more detailed information on the corpora, refer to Emerson (2005). 5.2 Features, Training, and Tuning We employed the feature templates defined in Section 3.2. The feature sets are huge. There are 2.4 × 107 features for the MSR data, 4.1 × 107 features for the CU data, and 4.7 × 107 features for the PKU data. To generate word-based features, we extracted high-frequency word-based unigram and bigram lists from the training data. As for training, we performed gradient descent MSR CU ADF SGD 96 LBFGS (batch) 95.5 94 F−score 96.5 F−score F−score 95.5 94.5 97 95 93.5 93 94.5 0 10 20 30 40 Number of Passes 92 50 0 10 20 30 40 Number of Passes MSR 94 50 0 CU 10 20"
P12-1027,P07-1104,0,0.0508865,"Missing"
P12-1027,P03-2039,0,0.03267,"g and Clark, 2007; Sun, 2010). Those semi-Markov perceptron systems are moderately faster than the heavy probabilistic systems using semi-Markov conditional random fields or latent variable conditional random fields. However, a disadvantage of the perceptron style systems is that they can not provide probabilistic information. On the other hand, new word detection is also one of the important problems in Chinese information processing. Many statistical approaches have been proposed (J. Nie and Jin, 1995; Chen and Bai, 1998; Wu and Jiang, 2000; Peng et al., 2004; Chen and Ma, 2002; Zhou, 2005; Goh et al., 2003; Fu and Luke, 2004; Wu et al., 2011). New word detection is normally considered as a separate process from segmentation. There were studies trying to solve this problem jointly with CWS. However, the current studies are limited. Integrating the two tasks would benefit both segmentation and new word detection. Our method provides a convenient framework for doing this. Our new word detection is not a standalone process, but an integral part of segmentation. 2.2 Online Training Related Work First, we review related work on word segmentation and new word detection. Then, we review popular online"
P12-1027,C04-1081,0,0.706516,"new word detection. 2 perceptron segmenters (Zhang and Clark, 2007; Sun, 2010). Those semi-Markov perceptron systems are moderately faster than the heavy probabilistic systems using semi-Markov conditional random fields or latent variable conditional random fields. However, a disadvantage of the perceptron style systems is that they can not provide probabilistic information. On the other hand, new word detection is also one of the important problems in Chinese information processing. Many statistical approaches have been proposed (J. Nie and Jin, 1995; Chen and Bai, 1998; Wu and Jiang, 2000; Peng et al., 2004; Chen and Ma, 2002; Zhou, 2005; Goh et al., 2003; Fu and Luke, 2004; Wu et al., 2011). New word detection is normally considered as a separate process from segmentation. There were studies trying to solve this problem jointly with CWS. However, the current studies are limited. Integrating the two tasks would benefit both segmentation and new word detection. Our method provides a convenient framework for doing this. Our new word detection is not a standalone process, but an integral part of segmentation. 2.2 Online Training Related Work First, we review related work on word segmentation and ne"
P12-1027,E09-1088,1,0.801001,"ning methods, in particular stochastic gradient descent (SGD). 2.1 Word Segmentation and New Word Detection Conventional approaches to Chinese word segmentation treat the problem as a sequential labeling task (Xue, 2003; Peng et al., 2004; Tseng et al., 2005; Asahara et al., 2005; Zhao et al., 2010). To achieve high accuracy, most of the stateof-the-art systems are heavy probabilistic systems using semi-Markov assumptions or latent variables (Andrew, 2006; Sun et al., 2009b). For example, one of the state-of-the-art CWS system is the latent variable conditional random field (Sun et al., 2008; Sun and Tsujii, 2009) system presented in Sun et al. (2009b). It is a heavy probabilistic model and it is slow in training. A few other state-of-the-art CWS systems are using semi-Markov perceptron methods or voting systems based on multiple semi-Markov 254 The most representative online training method is the SGD method. The SGD uses a small randomly-selected subset of the training samples to approximate the gradient of an objective function. The number of training samples used for this approximation is called the batch size. By using a smaller batch size, one can update the parameters more frequently and speed u"
P12-1027,C08-1106,1,0.0330946,"opular online training methods, in particular stochastic gradient descent (SGD). 2.1 Word Segmentation and New Word Detection Conventional approaches to Chinese word segmentation treat the problem as a sequential labeling task (Xue, 2003; Peng et al., 2004; Tseng et al., 2005; Asahara et al., 2005; Zhao et al., 2010). To achieve high accuracy, most of the stateof-the-art systems are heavy probabilistic systems using semi-Markov assumptions or latent variables (Andrew, 2006; Sun et al., 2009b). For example, one of the state-of-the-art CWS system is the latent variable conditional random field (Sun et al., 2008; Sun and Tsujii, 2009) system presented in Sun et al. (2009b). It is a heavy probabilistic model and it is slow in training. A few other state-of-the-art CWS systems are using semi-Markov perceptron methods or voting systems based on multiple semi-Markov 254 The most representative online training method is the SGD method. The SGD uses a small randomly-selected subset of the training samples to approximate the gradient of an objective function. The number of training samples used for this approximation is called the batch size. By using a smaller batch size, one can update the parameters more"
P12-1027,N09-1007,1,0.647152,"Online Training Related Work First, we review related work on word segmentation and new word detection. Then, we review popular online training methods, in particular stochastic gradient descent (SGD). 2.1 Word Segmentation and New Word Detection Conventional approaches to Chinese word segmentation treat the problem as a sequential labeling task (Xue, 2003; Peng et al., 2004; Tseng et al., 2005; Asahara et al., 2005; Zhao et al., 2010). To achieve high accuracy, most of the stateof-the-art systems are heavy probabilistic systems using semi-Markov assumptions or latent variables (Andrew, 2006; Sun et al., 2009b). For example, one of the state-of-the-art CWS system is the latent variable conditional random field (Sun et al., 2008; Sun and Tsujii, 2009) system presented in Sun et al. (2009b). It is a heavy probabilistic model and it is slow in training. A few other state-of-the-art CWS systems are using semi-Markov perceptron methods or voting systems based on multiple semi-Markov 254 The most representative online training method is the SGD method. The SGD uses a small randomly-selected subset of the training samples to approximate the gradient of an objective function. The number of training sample"
P12-1027,C10-2139,0,0.725279,"Missing"
P12-1027,I05-3027,0,0.248309,"Missing"
P12-1027,W00-1207,0,0.0506628,"ord segmentation and new word detection. 2 perceptron segmenters (Zhang and Clark, 2007; Sun, 2010). Those semi-Markov perceptron systems are moderately faster than the heavy probabilistic systems using semi-Markov conditional random fields or latent variable conditional random fields. However, a disadvantage of the perceptron style systems is that they can not provide probabilistic information. On the other hand, new word detection is also one of the important problems in Chinese information processing. Many statistical approaches have been proposed (J. Nie and Jin, 1995; Chen and Bai, 1998; Wu and Jiang, 2000; Peng et al., 2004; Chen and Ma, 2002; Zhou, 2005; Goh et al., 2003; Fu and Luke, 2004; Wu et al., 2011). New word detection is normally considered as a separate process from segmentation. There were studies trying to solve this problem jointly with CWS. However, the current studies are limited. Integrating the two tasks would benefit both segmentation and new word detection. Our method provides a convenient framework for doing this. Our new word detection is not a standalone process, but an integral part of segmentation. 2.2 Online Training Related Work First, we review related work on word"
P12-1027,O11-2013,0,0.0152595,"semi-Markov perceptron systems are moderately faster than the heavy probabilistic systems using semi-Markov conditional random fields or latent variable conditional random fields. However, a disadvantage of the perceptron style systems is that they can not provide probabilistic information. On the other hand, new word detection is also one of the important problems in Chinese information processing. Many statistical approaches have been proposed (J. Nie and Jin, 1995; Chen and Bai, 1998; Wu and Jiang, 2000; Peng et al., 2004; Chen and Ma, 2002; Zhou, 2005; Goh et al., 2003; Fu and Luke, 2004; Wu et al., 2011). New word detection is normally considered as a separate process from segmentation. There were studies trying to solve this problem jointly with CWS. However, the current studies are limited. Integrating the two tasks would benefit both segmentation and new word detection. Our method provides a convenient framework for doing this. Our new word detection is not a standalone process, but an integral part of segmentation. 2.2 Online Training Related Work First, we review related work on word segmentation and new word detection. Then, we review popular online training methods, in particular stoch"
P12-1027,O03-4002,0,0.825038,"es are limited. Integrating the two tasks would benefit both segmentation and new word detection. Our method provides a convenient framework for doing this. Our new word detection is not a standalone process, but an integral part of segmentation. 2.2 Online Training Related Work First, we review related work on word segmentation and new word detection. Then, we review popular online training methods, in particular stochastic gradient descent (SGD). 2.1 Word Segmentation and New Word Detection Conventional approaches to Chinese word segmentation treat the problem as a sequential labeling task (Xue, 2003; Peng et al., 2004; Tseng et al., 2005; Asahara et al., 2005; Zhao et al., 2010). To achieve high accuracy, most of the stateof-the-art systems are heavy probabilistic systems using semi-Markov assumptions or latent variables (Andrew, 2006; Sun et al., 2009b). For example, one of the state-of-the-art CWS system is the latent variable conditional random field (Sun et al., 2008; Sun and Tsujii, 2009) system presented in Sun et al. (2009b). It is a heavy probabilistic model and it is slow in training. A few other state-of-the-art CWS systems are using semi-Markov perceptron methods or voting sys"
P12-1027,P07-1106,0,0.741552,"ning algorithm later. We will show in experiments that our solution is an order magnitude faster compared with exiting learning methods, and can achieve equal or even higher accuracies. The contribution of this work is as follows: • We propose a general purpose fast online training method, ADF. The proposed training method requires only a few passes to complete the training. • We propose a joint model for Chinese word segmentation and new word detection. • Compared with prior work, our system achieves better accuracies on both word segmentation and new word detection. 2 perceptron segmenters (Zhang and Clark, 2007; Sun, 2010). Those semi-Markov perceptron systems are moderately faster than the heavy probabilistic systems using semi-Markov conditional random fields or latent variable conditional random fields. However, a disadvantage of the perceptron style systems is that they can not provide probabilistic information. On the other hand, new word detection is also one of the important problems in Chinese information processing. Many statistical approaches have been proposed (J. Nie and Jin, 1995; Chen and Bai, 1998; Wu and Jiang, 2000; Peng et al., 2004; Chen and Ma, 2002; Zhou, 2005; Goh et al., 2003;"
P12-1027,N06-2049,0,0.698283,"Missing"
P12-1027,I05-1047,0,0.0837938,"enters (Zhang and Clark, 2007; Sun, 2010). Those semi-Markov perceptron systems are moderately faster than the heavy probabilistic systems using semi-Markov conditional random fields or latent variable conditional random fields. However, a disadvantage of the perceptron style systems is that they can not provide probabilistic information. On the other hand, new word detection is also one of the important problems in Chinese information processing. Many statistical approaches have been proposed (J. Nie and Jin, 1995; Chen and Bai, 1998; Wu and Jiang, 2000; Peng et al., 2004; Chen and Ma, 2002; Zhou, 2005; Goh et al., 2003; Fu and Luke, 2004; Wu et al., 2011). New word detection is normally considered as a separate process from segmentation. There were studies trying to solve this problem jointly with CWS. However, the current studies are limited. Integrating the two tasks would benefit both segmentation and new word detection. Our method provides a convenient framework for doing this. Our new word detection is not a standalone process, but an integral part of segmentation. 2.2 Online Training Related Work First, we review related work on word segmentation and new word detection. Then, we revi"
P13-2101,N10-1100,0,0.346748,"Missing"
P13-2101,C12-1047,0,0.0289564,"Missing"
P13-2101,W04-1013,0,0.0151021,"Missing"
P13-2101,W11-0709,0,0.324012,"Missing"
P13-2101,J06-4002,0,\N,Missing
P14-1003,W05-0613,0,0.2812,"otated as either nucleus or satellite depending on how salient they are for interpretation. It is attractive and challenging to parse the whole text into one tree. Since such a hierarchical discourse tree is analogous to a constituency based syntactic tree except that the constituents in the discourse trees are text spans, previous researches have explored different constituency based syntactic parsing techniques (eg. CKY and chart parsing) and various features (eg. length, position et al.) for discourse parsing (Soricut and Marcu, 2003; Joty et al., 2012; Reitter, 2003; LeThanh et al., 2004; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Sagae, 2009; Hernault et al., 2010b; Feng and Hirst, 2012). However, the existing approaches suffer from at least one of the following three problems. First, it is difficult to design a set of production rules as in syntactic parsing, since there are no determinate generative rules for the interior text spans. Second, the different levels of discourse units (e.g. EDUs or larger text spans) occurring in the generative process are better represented with different features, and thus a uniform framework for discourse analysis is hard to develop. Third, to reduce the"
P14-1003,D09-1036,0,0.124101,"Missing"
P14-1003,W01-1605,0,0.196768,"ency parsing is to parse an optimal spanning tree from VRV-0. Here we follow the arc factored method and define the score of a dependency tree as the sum of the scores of all the arcs in the tree. Thus, the optimal dependency tree for T is a spanning tree with the highest score and obtained through the function DT(T,w): DT (T , w )  argmaxGT V  RV0 score(T , GT ) To automatically conduct discourse dependency parsing, constructing a discourse dependency treebank is fundamental. It is costly to manually construct such a treebank from scratch. Fortunately, RST Discourse Treebank (RST-DT) (Carlson et al., 2001) is an available resource to help with. A RST tree constitutes a hierarchical structure for one document through rhetorical relations. A total of 110 fine-grained relations (e.g. Elaboration-part-whole and List) were used for tagging RST-DT. They can be categorized into 18 classes (e.g. Elaboration and Joint). All these relations can be hypotactic (“mononuclear”) or paratactic (“multi-nuclear”). A hypotactic relation holds between a nucleus span and an adjacent satellite span, while a paratactic relation connects two or more equally important adjacent nucleus spans. For convenience of computat"
P14-1003,C96-1058,0,0.471564,"out worrying about any interior text spans. Since dependency trees contain much fewer nodes and on average they are simpler than constituency based trees, the current dependency parsers can have a relatively low computational complexity. Moreover, concerning linearization, it is well known that dependency structures can deal with non-projective relations, while constituency-based models need the addition of complex mechanisms like transformations, movements and so on. In our work, we adopt the graph based dependency parsing techniques learned from large sets of annotated dependency trees. The Eisner (1996) algorithm e3 e2 1 e3 e2 e1 3 e1*-e2-e3 *-e e1 e2 e1 5 e1 e3 e1 6 e3 e2 4 2-e3 e1-e2*-e3 e2-e3* e2*-e3 e3 e1*-e2 2 e2-e3* e2 e3 e2 e1 e1-e2-e3* e1-e2-e3* e1*-e2 e1-e2* e1-e2* e1 e1*-e2-e3 e1-e2-e3* e1-e2*-e3 e1 and maximum spanning tree (MST) algorithm are used respectively to parse the optimal projective and non-projective dependency trees with the large-margin learning technique (Crammer and Singer, 2003). To the best of our knowledge, we are the first to apply the dependency structure and introduce the dependency parsing techniques into discourse analysis. The rest of this paper is organize"
P14-1003,H05-1066,0,0.128654,"1 to n 3 For i := 1 to n 4 j=i+m 5 if j> n then break; 6 # Create subgraphs with c=0 by adding arcs 7 E[i, j, 0, 0]=maxiqj (E[i,q,1,1]+E[q+1,j,0,1]+(ej,ei)) 8 E[i, j, 1, 0]=maxiqj (E[i,q,1,1]+E[q+1,j,0,1]+(ei,ej)) 9 # Add corresponding left/right subgraphs 10 E[i, j, 0, 1]=maxiqj (E[i,q,0,1]+E[q,j,0,0] 11 E[i, j, 1, 1]=maxiqj (E[i,q,1,0]+E[q,j,1,1]) A ... ... B A B Figure 4: Pictorial Diagram of Non-projective Trees Chu and Liu (1965) and Edmonds (1967) independently proposed the virtually identical algorithm named the Chu-Liu/Edmonds algorithm, for finding MSTs on directed graphs (McDonald et al. 2005b). Figure 5 shows the details of the Chu-Liu/Edmonds algorithm for discourse parsing. Each node in the graph greedily selects the incoming arc with the highest score. If one tree results, the algorithm ends. Otherwise, there must exist a cycle. The algorithm contracts the identified cycle into a single node and recalculates the scores of the arcs which go in and out of the cycle. Next, the algorithm recursively call itself on the contracted graph. Finally, those arcs which go in or out of one cycle will recover themselves to connect with the original nodes in V. Like McDonald et al. (2005b),"
P14-1003,P12-1007,0,0.54857,"attractive and challenging to parse the whole text into one tree. Since such a hierarchical discourse tree is analogous to a constituency based syntactic tree except that the constituents in the discourse trees are text spans, previous researches have explored different constituency based syntactic parsing techniques (eg. CKY and chart parsing) and various features (eg. length, position et al.) for discourse parsing (Soricut and Marcu, 2003; Joty et al., 2012; Reitter, 2003; LeThanh et al., 2004; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Sagae, 2009; Hernault et al., 2010b; Feng and Hirst, 2012). However, the existing approaches suffer from at least one of the following three problems. First, it is difficult to design a set of production rules as in syntactic parsing, since there are no determinate generative rules for the interior text spans. Second, the different levels of discourse units (e.g. EDUs or larger text spans) occurring in the generative process are better represented with different features, and thus a uniform framework for discourse analysis is hard to develop. Third, to reduce the time complexity of the state-of-the-art constituency based parsing techniques, the appro"
P14-1003,P09-1077,0,0.0249236,"hniques are mainly based on two well-known treebanks. One is the Penn Discourse TreeBank (PDTB) (Prasad et al., 2007) and the other is RST-DT. PDTB adopts the predicate-arguments representation by taking an implicit/explicit connective as a predication of two adjacent sentences (arguments). Then the discourse relation between each pair of sentences is annotated independently to characterize its predication. A majority of researches regard discourse parsing as a classification task and mainly focus on exploiting various linguistic features and classifiers when using PDTB (Wellner et al., 2006; Pitler et al., 2009; Wang et al., 2010). However, the predicatearguments annotation scheme itself has such a limitation that one can only obtain the local discourse relations without knowing the rich context. In contrast, RST and its treebank enable people to derive a complete representation of the whole discourse. Researches have begun to investigate how to construct a RST tree for the given text. Since the RST tree is similar to the constituency based syntactic tree except that the constituent nodes are different, the syntactic parsing techniques have been borrowed for discourse parsing (Soricut and Marcu, 200"
P14-1003,D10-1039,0,0.106064,"or interpretation. It is attractive and challenging to parse the whole text into one tree. Since such a hierarchical discourse tree is analogous to a constituency based syntactic tree except that the constituents in the discourse trees are text spans, previous researches have explored different constituency based syntactic parsing techniques (eg. CKY and chart parsing) and various features (eg. length, position et al.) for discourse parsing (Soricut and Marcu, 2003; Joty et al., 2012; Reitter, 2003; LeThanh et al., 2004; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Sagae, 2009; Hernault et al., 2010b; Feng and Hirst, 2012). However, the existing approaches suffer from at least one of the following three problems. First, it is difficult to design a set of production rules as in syntactic parsing, since there are no determinate generative rules for the interior text spans. Second, the different levels of discourse units (e.g. EDUs or larger text spans) occurring in the generative process are better represented with different features, and thus a uniform framework for discourse analysis is hard to develop. Third, to reduce the time complexity of the state-of-the-art constituency based parsi"
P14-1003,W09-3813,0,0.332613,"nt they are for interpretation. It is attractive and challenging to parse the whole text into one tree. Since such a hierarchical discourse tree is analogous to a constituency based syntactic tree except that the constituents in the discourse trees are text spans, previous researches have explored different constituency based syntactic parsing techniques (eg. CKY and chart parsing) and various features (eg. length, position et al.) for discourse parsing (Soricut and Marcu, 2003; Joty et al., 2012; Reitter, 2003; LeThanh et al., 2004; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Sagae, 2009; Hernault et al., 2010b; Feng and Hirst, 2012). However, the existing approaches suffer from at least one of the following three problems. First, it is difficult to design a set of production rules as in syntactic parsing, since there are no determinate generative rules for the interior text spans. Second, the different levels of discourse units (e.g. EDUs or larger text spans) occurring in the generative process are better represented with different features, and thus a uniform framework for discourse analysis is hard to develop. Third, to reduce the time complexity of the state-of-the-art c"
P14-1003,N09-1064,0,0.0711676,"Missing"
P14-1003,P10-1073,0,0.0122735,"sed on two well-known treebanks. One is the Penn Discourse TreeBank (PDTB) (Prasad et al., 2007) and the other is RST-DT. PDTB adopts the predicate-arguments representation by taking an implicit/explicit connective as a predication of two adjacent sentences (arguments). Then the discourse relation between each pair of sentences is annotated independently to characterize its predication. A majority of researches regard discourse parsing as a classification task and mainly focus on exploiting various linguistic features and classifiers when using PDTB (Wellner et al., 2006; Pitler et al., 2009; Wang et al., 2010). However, the predicatearguments annotation scheme itself has such a limitation that one can only obtain the local discourse relations without knowing the rich context. In contrast, RST and its treebank enable people to derive a complete representation of the whole discourse. Researches have begun to investigate how to construct a RST tree for the given text. Since the RST tree is similar to the constituency based syntactic tree except that the constituent nodes are different, the syntactic parsing techniques have been borrowed for discourse parsing (Soricut and Marcu, 2003; Baldridge and Las"
P14-1003,W06-1317,0,0.0276017,"discourse parsing techniques are mainly based on two well-known treebanks. One is the Penn Discourse TreeBank (PDTB) (Prasad et al., 2007) and the other is RST-DT. PDTB adopts the predicate-arguments representation by taking an implicit/explicit connective as a predication of two adjacent sentences (arguments). Then the discourse relation between each pair of sentences is annotated independently to characterize its predication. A majority of researches regard discourse parsing as a classification task and mainly focus on exploiting various linguistic features and classifiers when using PDTB (Wellner et al., 2006; Pitler et al., 2009; Wang et al., 2010). However, the predicatearguments annotation scheme itself has such a limitation that one can only obtain the local discourse relations without knowing the rich context. In contrast, RST and its treebank enable people to derive a complete representation of the whole discourse. Researches have begun to investigate how to construct a RST tree for the given text. Since the RST tree is similar to the constituency based syntactic tree except that the constituent nodes are different, the syntactic parsing techniques have been borrowed for discourse parsing (S"
P14-1003,C04-1048,0,\N,Missing
P14-1003,P05-1012,0,\N,Missing
P14-1003,N03-1030,0,\N,Missing
P14-1003,prasad-etal-2008-penn,0,\N,Missing
P15-1041,D08-1014,0,0.0893355,"Missing"
P15-1041,S13-1032,0,0.0658767,"Missing"
P15-1041,D08-1083,0,0.017363,"red through experience or education by perceiving, discovering or learning2 . It can be implicit or explicit. In machine learning, natural language knowledge is a continuously improving hypothesis that consists of both semantic and significant domain 2 Definition from Oxford Dictionary of English, available at: http://oxforddictionaries.com/view/ entry/m_en_us126. 422 (t)− characters. While language is the expression of semantic, semantic is the carrier of sentiment. Using another word, two texts with more smaller semantic distance have higher probability to share the same sentiment polarity. Choi and Cardie (2008) assert that the sentiment polarity of natural language can be better inferred by compositional semantics. They also suggest that incorporating compositional semantics into learning can improve the performance of sentiment classifiers. Saif et al. (2012) also demonstrate that the addition of extra semantic features can further improve performance. In order to filter out noisy and incorrect sentiment labels, we propose a knowledge validation approach to reduce these noisy data that hinder the improvement of learning performance. Knowledge validation is a way to identify the acquired knowledge i"
P15-1041,P11-2075,0,0.0379683,"Missing"
P15-1041,C04-1121,0,0.0633962,"arning based approaches (Liu, 2012). Lexicon-based approaches highly depend on sentiment lexicons. Turney (2002) derives the overall phrase and document sentiment scores by averaging the sentiment scores provided in a lexicon over the words included. Similar idea is adopted in (Hiroshi et al., 2004; Kennedy and Inkpen, 2006). Machine learning based approaches, on the other hand, apply classification models. The task-specific features are designed to train sentiment polarity classifiers. Pang et al. (2002) compare the performance of NB, SVM and ME on movie reviews. SVM is found more effective. Gamon (2004) shows that SVM with deep linguistic 420 pseudo-reviews LT rCN = {(xlicnT r , yi )}M i=1 and UCN into English to obtain extra unlabeled English pseudo-reviews UT rEN = {xljenT r }N j=1 . Thereby, we obtain a pair of pseudo-parallel data (UCN , UT rEN ). The task is to use LEN and UCN to train a Chinese classifier to predict sentiment polarity for the test data TCN . It is a standard transfer learning problem. We consider two language views, i.e., source language view DS and target language view Dτ . DS boosts itself with the labeled English data and recommend translated knowledge to Dτ , while"
P15-1041,P12-1060,0,0.043262,"ranslated languages are combined by voting. Most researches, on the other hand, explore transfer learning and focus on knowledge adaptation. For example, Wan (2009) applies a supervised cotraining framework to iteratively adapt knowledge learned from the two languages by transferring translated texts to each other. Other similar work includes (Wei and Pal, 2010) and (He, 2011b). All these approaches rely on MT to build language connection. Meanwhile, the unlabeled parallel data is also employed to fill the gap between two languages. To solve the feature coverage problem with the EM algorithm, Meng et al. (2012) leverage the unlabeled parallel data to learn unseen sentiment words. Similarly, Popat et al. (2013) use the unlabeled parallel data to cluster features in order to reduce the data sparsity problem. Meng et al. (2012) and Popat et al. (2013) also use the unlabeled parallel data to reduce the negative influence of the noisy and incorrect sentiment labels introduced by machine translation and knowledge transfer. However, the parallel data is also a scarce resource. Related Work Sentiment Analysis Sentiment has been analyzed in different language granularity, e.g., entity, aspect, sentence and d"
P15-1041,W04-3253,0,0.0409393,"ious research has focused on knowledge validation to filter out the noisy knowledge having sentiment changes caused by wrong translations during knowledge transfer. 1 419 http://translate.google.com Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 419–429, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics features can further improve the performance. A variety of other machine learning approaches are also proposed to sentiment classification (Mullen and Collier, 2004; Read, 2005; Hassan and Radev, 2010; Socher et al., 2013). Cross-domain sentiment classification (CDSC) shares certain common characteristics with crosslingual sentiment classification (CLSC) (Tan et al., 2007; Li et al., 2009; Pan and Yang, 2010; He et al., 2011a; Glorot et al., 2011). Notice that the gap between source domain and target domain is the main difference between CDSC and CLSC. CLSC copes with two different datasets in two different languages. This difference makes CLSC a new challenge, drawing specific attention to researcher recently. To reduce the noisy sentiment knowledge int"
P15-1041,W02-1011,0,0.0215305,"views in the document level. Existing approaches are generally categorized into lexicon-based and machine learning based approaches (Liu, 2012). Lexicon-based approaches highly depend on sentiment lexicons. Turney (2002) derives the overall phrase and document sentiment scores by averaging the sentiment scores provided in a lexicon over the words included. Similar idea is adopted in (Hiroshi et al., 2004; Kennedy and Inkpen, 2006). Machine learning based approaches, on the other hand, apply classification models. The task-specific features are designed to train sentiment polarity classifiers. Pang et al. (2002) compare the performance of NB, SVM and ME on movie reviews. SVM is found more effective. Gamon (2004) shows that SVM with deep linguistic 420 pseudo-reviews LT rCN = {(xlicnT r , yi )}M i=1 and UCN into English to obtain extra unlabeled English pseudo-reviews UT rEN = {xljenT r }N j=1 . Thereby, we obtain a pair of pseudo-parallel data (UCN , UT rEN ). The task is to use LEN and UCN to train a Chinese classifier to predict sentiment polarity for the test data TCN . It is a standard transfer learning problem. We consider two language views, i.e., source language view DS and target language vie"
P15-1041,P10-1041,0,0.0103337,"validation to filter out the noisy knowledge having sentiment changes caused by wrong translations during knowledge transfer. 1 419 http://translate.google.com Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 419–429, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics features can further improve the performance. A variety of other machine learning approaches are also proposed to sentiment classification (Mullen and Collier, 2004; Read, 2005; Hassan and Radev, 2010; Socher et al., 2013). Cross-domain sentiment classification (CDSC) shares certain common characteristics with crosslingual sentiment classification (CLSC) (Tan et al., 2007; Li et al., 2009; Pan and Yang, 2010; He et al., 2011a; Glorot et al., 2011). Notice that the gap between source domain and target domain is the main difference between CDSC and CLSC. CLSC copes with two different datasets in two different languages. This difference makes CLSC a new challenge, drawing specific attention to researcher recently. To reduce the noisy sentiment knowledge introduced into the target language, we"
P15-1041,P11-1013,0,0.0158888,"Missing"
P15-1041,P05-2008,0,0.0441804,"on knowledge validation to filter out the noisy knowledge having sentiment changes caused by wrong translations during knowledge transfer. 1 419 http://translate.google.com Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 419–429, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics features can further improve the performance. A variety of other machine learning approaches are also proposed to sentiment classification (Mullen and Collier, 2004; Read, 2005; Hassan and Radev, 2010; Socher et al., 2013). Cross-domain sentiment classification (CDSC) shares certain common characteristics with crosslingual sentiment classification (CLSC) (Tan et al., 2007; Li et al., 2009; Pan and Yang, 2010; He et al., 2011a; Glorot et al., 2011). Notice that the gap between source domain and target domain is the main difference between CDSC and CLSC. CLSC copes with two different datasets in two different languages. This difference makes CLSC a new challenge, drawing specific attention to researcher recently. To reduce the noisy sentiment knowledge introduced into"
P15-1041,D08-1058,0,0.0678622,"dible. This is why our approach can outperform state-ofthe-art CLSA approaches. The rest of this paper is organized as follows. Section 2 summarizes the related work. Section 3 explains the proposed model. Section 4 presents experimental results. Finally, Section 5 concludes the paper and suggests future work. 2 2.1 2.2 Cross-lingual Sentiment Analysis There are two alternative solutions to cross-lingual sentiment analysis. One is ensemble learning that combines multiple classifiers. The other is transfer learning that develops strategies to adapt the knowledge from one language to the other. Wan (2008) is among the pioneers to develop the ensemble learning solutions, where multiple classifiers learned from different training datasets including those in original languages and translated languages are combined by voting. Most researches, on the other hand, explore transfer learning and focus on knowledge adaptation. For example, Wan (2009) applies a supervised cotraining framework to iteratively adapt knowledge learned from the two languages by transferring translated texts to each other. Other similar work includes (Wei and Pal, 2010) and (He, 2011b). All these approaches rely on MT to build"
P15-1041,P09-1027,0,0.693701,"ent Analysis There are two alternative solutions to cross-lingual sentiment analysis. One is ensemble learning that combines multiple classifiers. The other is transfer learning that develops strategies to adapt the knowledge from one language to the other. Wan (2008) is among the pioneers to develop the ensemble learning solutions, where multiple classifiers learned from different training datasets including those in original languages and translated languages are combined by voting. Most researches, on the other hand, explore transfer learning and focus on knowledge adaptation. For example, Wan (2009) applies a supervised cotraining framework to iteratively adapt knowledge learned from the two languages by transferring translated texts to each other. Other similar work includes (Wei and Pal, 2010) and (He, 2011b). All these approaches rely on MT to build language connection. Meanwhile, the unlabeled parallel data is also employed to fill the gap between two languages. To solve the feature coverage problem with the EM algorithm, Meng et al. (2012) leverage the unlabeled parallel data to learn unseen sentiment words. Similarly, Popat et al. (2013) use the unlabeled parallel data to cluster f"
P15-1041,P10-2048,0,0.0886585,"strategies to adapt the knowledge from one language to the other. Wan (2008) is among the pioneers to develop the ensemble learning solutions, where multiple classifiers learned from different training datasets including those in original languages and translated languages are combined by voting. Most researches, on the other hand, explore transfer learning and focus on knowledge adaptation. For example, Wan (2009) applies a supervised cotraining framework to iteratively adapt knowledge learned from the two languages by transferring translated texts to each other. Other similar work includes (Wei and Pal, 2010) and (He, 2011b). All these approaches rely on MT to build language connection. Meanwhile, the unlabeled parallel data is also employed to fill the gap between two languages. To solve the feature coverage problem with the EM algorithm, Meng et al. (2012) leverage the unlabeled parallel data to learn unseen sentiment words. Similarly, Popat et al. (2013) use the unlabeled parallel data to cluster features in order to reduce the data sparsity problem. Meng et al. (2012) and Popat et al. (2013) also use the unlabeled parallel data to reduce the negative influence of the noisy and incorrect sentim"
P15-1041,W11-1724,0,0.0356086,"Missing"
P15-1041,C04-1071,0,\N,Missing
P15-1041,P14-2139,0,\N,Missing
P15-1041,P13-1041,0,\N,Missing
P15-2102,P12-1092,0,0.0590827,"Missing"
P15-2102,P14-2131,0,0.0541362,"Missing"
P15-2102,D14-1108,0,0.0262901,"Missing"
P15-2102,D14-1082,0,0.0385828,"Missing"
P15-2102,P14-2050,0,0.0356419,"Missing"
P15-2102,cheng-etal-2014-parsing,0,0.025317,"Missing"
P15-2102,Q15-1016,0,0.0389344,"Missing"
P15-2102,P14-1140,0,0.0298355,"Missing"
P15-2102,W13-3512,0,0.0456091,"Missing"
P15-2102,P14-1146,0,0.0385086,"Missing"
P15-2102,N15-1142,0,0.0259363,"Missing"
P15-2102,C14-1015,0,0.025463,"Missing"
P15-2102,D13-1136,0,0.052079,"Missing"
P15-2102,N15-1069,0,0.0263371,"Missing"
P15-2102,D09-1026,0,0.0521129,"xpertise and finding experts on social media, which facilitates the services of user recommendation and questionanswering, etc. Despite the demand to access expertise, the challenges of identifying domain experts on social media exist. Social media often contains plenty of noises such as the tags with which users describe themselves. Noises impose the inherent drawback on the feature-based learning methods (Krishnamurthy et al, 2008). Data imbalance and sparseness also limits the performance of the promising latent semantic analysis methods such as the LDA-like topic models (Blei et al, 2003; Ramage et al, 2009). When some topics co-occur more frequently than others, the strict assumption of these topic models cannot be met and consequently many nonsensical topics will be generated (Zhao and Jiang, 2011; Pal et al, 2011; 616 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 616–622, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics mains’ knowledge. Using such trees allows us to flexibly incorporate more information into the data repres"
P15-2102,P14-2089,0,0.0368303,"Missing"
P15-2102,P13-1013,0,0.0606307,"Missing"
P15-2102,D14-1162,0,\N,Missing
P15-2136,D14-1181,0,0.00276888,"Missing"
P15-2136,W04-1013,0,0.0525643,"ver, he reserves all the representations generated by filters to a fully connected output layer. This practice greatly enlarges following parameters and ignores the relation among phrases with different lengths. Hence we use the two-stage max-over-time pooling to associate all these filters. Besides the features xp obtained through the CNNs, we also extract several documentdependent features notated as xe , shown in Table 1. In the end, xp is combined with xe to conduct sentence ranking. Here we follow the regression framework of Li et al. (2007). The sentence saliency y is scored by ROUGE-2 (Lin, 2004) (stopwords removed) and the model tries to estimate this saliency. φ = [xp , xe ] (3) wrT (4) yˆ = ×φ AVG-CF Description The position of the sentence. The averaged term frequency values of words in the sentence. The averaged cluster frequency values of words in the sentence. 3.2 Comparison with Baseline Methods To evaluate the summarization performance of PriorSum, we compare it with the best peer systems (PeerT, Peer26 and Peer65 in Table 2) participating DUC evaluations. We also choose as baselines those state-of-the-art summarization results on DUC (2001, 2002, and 2004) data. To our knowl"
P15-2136,W02-0401,0,0.431087,"res beyond word level (e.g., phrases) are seldom involved in current research. The CTSUM system developed by Wan and Zhang (2014) is the most relevant to ours. It attempted to explore a context-free measure named certainty which is critical to ranking sentences in summarization. To calculate the certainty score, four dictionaries are manually built as features and a corpus is annotated to train the feature weights using Support Vector Regression (SVR). HowIntroduction Sentence ranking, the vital part of extractive summarization, has been extensively investigated. Regardless of ranking models (Osborne, 2002; Galley, 2006; Conroy et al., 2004; Li et al., 2007), feature engineering largely determines the final summarization performance. Features often fall into two types: document-dependent features (e.g., term frequency or position) and documentindependent features (e.g., stopword ratio or word polarity). The latter type of features take effects due to the fact that, a sentence can often be judged by itself whether it is appropriate to be included in a summary no matter which document it lies in. Take the following two sentences as an example: 1. Hurricane Emily slammed into Dominica on September"
P15-2136,P14-2105,0,0.013617,"The underlined phrases greatly reduce the certainty of this sentence according to Wan and Zhang (2014)’s model. But, in fact, this sentence can summarize the government’s attitude and is salient enough in the related documents. Thus, in our opinion, certainty can just be viewed as a specific aspect of the summary prior nature. To this end, we develop a novel summarization system called PriorSum to automatically exploit all possible semantic aspects latent in the summary prior nature. Since the Convolutional Neural Networks (CNNs) have shown promising progress in latent feature representation (Yih et al., 2014; Shen et al., 2014; Zeng et al., 2014), PriorSum applies CNNs with multiple filters to capture a comprehensive set of document-independent features derived from length-variable phrases. Then we adopt a two-stage max-over-time pooling operation to associate these filters since phrases with different lengths may express the same aspect of summary prior. PriorSum generates the document-independent features, and concatenates them with document-dependent ones to work for sentence regression (Section 2.1). We conduct extensive experiments on the DUC 2001, 2002 and 2004 generic multi-document summar"
P15-2136,C14-1220,0,0.00457144,"the certainty of this sentence according to Wan and Zhang (2014)’s model. But, in fact, this sentence can summarize the government’s attitude and is salient enough in the related documents. Thus, in our opinion, certainty can just be viewed as a specific aspect of the summary prior nature. To this end, we develop a novel summarization system called PriorSum to automatically exploit all possible semantic aspects latent in the summary prior nature. Since the Convolutional Neural Networks (CNNs) have shown promising progress in latent feature representation (Yih et al., 2014; Shen et al., 2014; Zeng et al., 2014), PriorSum applies CNNs with multiple filters to capture a comprehensive set of document-independent features derived from length-variable phrases. Then we adopt a two-stage max-over-time pooling operation to associate these filters since phrases with different lengths may express the same aspect of summary prior. PriorSum generates the document-independent features, and concatenates them with document-dependent ones to work for sentence regression (Section 2.1). We conduct extensive experiments on the DUC 2001, 2002 and 2004 generic multi-document summarization datasets. The experimental resu"
P15-2136,W06-1643,0,\N,Missing
P15-2136,E14-1075,0,\N,Missing
P17-1169,D07-1043,0,0.0288184,"Ls and Email addresses, delete documents with less than ten words. 20Newsgroups have roughly comparable sizes of categories. (D5) BBCSports. (D6) Ohsumed and Ohsumed-full: Documents are medical abstracts from the MeSH categories of the year 1991. Specifically, there are 23 cardiovascular diseases categories. 4.1 Datasets and Evaluation Metrics Evaluating clustering results is known to be nontrivial. We use the following three sets of quantitative metrics to assess the quality of clusters by knowing the ground truth categorical labels of documents: (i) Homogeneity, Completeness, and V-measure (Rosenberg and Hirschberg, 2007); (ii) Adjusted Mutual Information (AMI) (Vinh et al., 2010); and (iii) Adjusted Rand Index (ARI) (Rand, 1971). For sensitivity analysis, we use the homogeneity score (Rosenberg and Hirschberg, 2007) as a projection dimension of other metrics, creating a 2D plot to visualize the metrics of a method along different homogeneity levels. Generally speaking, more clusters leads to higher homogeneity by chance. We prepare six datasets to conduct a set of experiments. Two short-text datasets are created as follows. (D1) BBCNews abstract: We concatenate 1 BBCNews and BBCSport are downloaded from http:"
P17-1169,P11-1080,0,0.0179697,"ein barycenters (Cuturi and Doucet, 2014; Ye and Li, 2014; Benamou et al., 2015; Ye et al., 2017), one can now perform document clustering by directly treating them as empirical measures over a word embedding space. Although the computational cost is still higher than the usual vector-based clustering methods, we believe that the new clustering approach has reached a level of efficiency to justify its usage given how important it is to obtain high-quality clustering of unstructured text data. For instance, clustering is a crucial step performed ahead of cross-document co-reference resolution (Singh et al., 2011), document summarization, retrospective events detection, and opinion mining (Zhai et al., 2011). 1.1 Contributions Our work has two main contributions. First, we create a basic tool of document clustering, which is easy to use and scalable. The new method leverages the latest numerical toolbox developed for optimal transport. It achieves state-of-theart clustering performance across heterogeneous text data—an advantage over other methods in the literature. Second, the method enables us to quantitatively inspect how well a word-embedding model can fit the data and how much gain it can produce"
P17-1169,P15-4023,1,0.851383,"Missing"
P17-1169,N13-1090,0,0.0272571,"r clustering empirical distributions. By using the Wasserstein distance between distributions, the word-to-word semantic relationship is taken into account in a principled way. The new clustering method is easy to use and consistently outperforms other methods on a variety of data sets. More importantly, the method provides an effective framework for determining when and how much word embeddings contribute to document analysis. Experimental results with multiple embedding models are reported. 1 Introduction Word embeddings (a.k.a. word vectors) have been broadly adopted for document analysis (Mikolov et al., 2013a,b). The embeddings can be trained from external large-scale corpus and then easily utilized for different data. To a certain degree, the knowledge mined from the corpus, possibly in very intricate ways, is coded in the vector space, Correspondence should be sent to J. Ye (jxy198@psu.edu) and J. Li (jiali@psu.edu). The work was done when Z. Wu was with Penn State. the samples of which are easy to describe and ready for mathematical modeling. Despite the appeal, researchers will be interested in knowing how much gain an embedding can bring forth over the performance achievable by existing bag-"
P17-1169,D14-1162,0,0.0950646,"Missing"
P17-2080,D17-1066,0,0.0359631,"ther specific areas. 2 Models higher-level context vector is the concatenation of both status vectors. We will show later that SPHRED not only well keeps individual features, but also provides a better holistic representation for the response decoder than normal HRED. To provide a better dialog context, we build a hierarchical recurrent encoder-decoder with separated context models (SPHRED). This section first introduces the concept of SPHRED, then explains the conditional variational framework and two application scenarios. 2.2 VAEs have been used for text generation in (Bowman et al., 2015; Semeniuta et al., 2017), where texts are synthesized from latent variables. Starting from this idea, we assume every utterance wn comes with a corresponding label yn and latent variable zn . The generation of zn and wn are conditioned on the dialog context provided by SPHRED, and this additional class label yn . This includes 2 situations, where the label of the next sequence is known (like for Scenario 1 in Section 2.3) or not (Section 2.4). For each utterance, the latent variable zn is first sampled from a prior distribution. The whole dialog can be explained by the generative process: 2.1 SPHRED We decomposes a d"
P17-2080,P15-1152,0,0.0834311,"Missing"
P17-2080,P15-2073,0,0.00814574,"Missing"
P17-2080,N15-1020,0,0.0192508,"Missing"
P17-2080,D16-1230,0,0.00490343,"also experimented by substituting the encoder with a normal HRED, the resulting model cannot predict the correct sentiment at all because the context information is highly mingled for both speakers. The embedding based scores of our framework are still comparable with SPHRED and even better than VHRED. Imposing an external label didn’t bring any significant quality decline. Evaluation Accurate automatic evaluation of dialog generation is difficult (Galley et al., 2015; Pietquin and Hastie, 2013). In our experiment, we conducted three embedding-based evaluations (average, greedy and extrema) (Liu et al., 2016) on all our models, which map responses into vector space and compute the cosine similarity. Though not necessarily accurate, the embedding-based metrics can to a large extent measure the semantic similarity and test the ability of successfully generating a response sharing a similar topic with the golden answer. The results of a GRU language model (LM), HRED and VHRED were also provided for comparison. For the two scenarios of our framework, we further measured the percentage of generated responses matching the correct labels (accuracy). In (Liu et al., 2016), current popular metrics are show"
P17-2080,W15-4640,0,0.00336433,"nt tag of next utterance as the average of the preceding two ones. Namely, if one is positive and the other is negative, the next response would be neutral. The label y represents the sentiment tag, which is unknown at test time and needs to be predicted from the context. The probability qφ (yn |w1n−1 ) is modeled by feedforward neural networks. This scenario is designed to demonstrate our framework can successfully learn the manually defined rules to predict the proper label and decode responses conforming to this label. 3 Experiments We conducted our experiments on the Ubuntu dialog Corpus (Lowe et al., 2015), which contains about 500,000 multi-turn dialogs. The vocabulary was set as the most frequent 20,000 words. All the letters are transferred to lowercase and the Outof-Vocabulary (OOV) words were preprocessed as &lt;unk> tokens. 3.1 Training Procedures Model hyperparameters were set the same as in VHRED model except that we reduced by half the context RNN dimension. The encoder, context and decoder RNNs all make use of the Gated Recurrent Unit (GRU) structure (Cho et al., 2014). Labels were mapped to embeddings with size 100 and word vectors were initialized with the pubic Word2Vec embeddings tra"
P17-2100,N16-1012,0,0.185993,"Missing"
P17-2100,P16-1154,0,0.0428289,"Missing"
P17-2100,D16-1009,0,0.0234952,"vectors, and the decoder generates summaries and produces semantic vectors of the generated summaries. Finally, the similarity function evaluates the relevance between the sematic vectors of source texts and generated summaries. Our training objective is to maximize the similarity score so that the generated summaries have high semantic relevance with source texts. (1) 3.1 Text Representation There are several methods to represent a text or a sentence, such as mean pooling of RNN output or reserving the last state of RNN. In our model, source text is represented by a gated attention encoder (Hahn and Keller, 2016). Every upcoming word is fed into a gated attention network, which measures its importance. The gated attention network outputs the important score with a feedforward network. At each time step, it inputs a word vector et and its previous context vector ht , then outputs the score βt . Then the word vector et is multiplied by the score βt , and fed into RNN encoder. We select the last output hN of RNN encoder as the semantic vector of the source text Vt . A natural idea to get the semantic vector of a summary is to feed it into the encoder as well. However, this method wastes much time because"
P17-2100,D15-1229,0,0.633711,"literally, but low semantic relevance. Introduction Text summarization is to produce a brief summary of the main ideas of the text. For long and normal documents, extractive summarization achieves satisfying performance by selecting a few sentences from source texts (Radev et al., 2004; Woodsend and Lapata, 2010; Cheng and Lapata, 2016). However, it does not apply to Chinese social media text summarization, where texts are comparatively short and often full of noise. Therefore, abstractive text summarization, which is based on encoder-decoder framework, is a better choice (Rush et al., 2015; Hu et al., 2015). For extractive summarization, the selected sentences often have high semantic relevance to the text. However, for abstractive text summarization, current models tend to produce grammatical and coherent summaries regardless of its semantic relevance with source texts. Figure 1 shows that the summary generated by a current model (RNN encoder-decoder) is similar to the source text literally, but it has low semantic relevance. In this work, our goal is to improve the semantic relevance between source texts and generated summaries for Chinese social media text summarization. To achieve this goal,"
P17-2100,N03-1020,0,0.426438,"Missing"
P17-2100,D15-1166,0,0.236244,"Missing"
P17-2100,P16-1046,0,0.0589016,"联航空机场发生爆炸致多人死亡。 China United Airlines exploded in the airport, leaving several people dead. Gold: 航班多人吸烟机组人员与乘客冲突。 Several people smoked on a flight which led to a collision between crew and passengers. Figure 1: An example of RNN generated summary. It has high similarity to the text literally, but low semantic relevance. Introduction Text summarization is to produce a brief summary of the main ideas of the text. For long and normal documents, extractive summarization achieves satisfying performance by selecting a few sentences from source texts (Radev et al., 2004; Woodsend and Lapata, 2010; Cheng and Lapata, 2016). However, it does not apply to Chinese social media text summarization, where texts are comparatively short and often full of noise. Therefore, abstractive text summarization, which is based on encoder-decoder framework, is a better choice (Rush et al., 2015; Hu et al., 2015). For extractive summarization, the selected sentences often have high semantic relevance to the text. However, for abstractive text summarization, current models tend to produce grammatical and coherent summaries regardless of its semantic relevance with source texts. Figure 1 shows that the summary generated by a curren"
P17-2100,K16-1028,0,0.0653475,"Missing"
P17-2100,radev-etal-2004-mead,0,0.073768,"Missing"
P17-2100,D15-1044,0,0.0636919,"ilarity to the text literally, but low semantic relevance. Introduction Text summarization is to produce a brief summary of the main ideas of the text. For long and normal documents, extractive summarization achieves satisfying performance by selecting a few sentences from source texts (Radev et al., 2004; Woodsend and Lapata, 2010; Cheng and Lapata, 2016). However, it does not apply to Chinese social media text summarization, where texts are comparatively short and often full of noise. Therefore, abstractive text summarization, which is based on encoder-decoder framework, is a better choice (Rush et al., 2015; Hu et al., 2015). For extractive summarization, the selected sentences often have high semantic relevance to the text. However, for abstractive text summarization, current models tend to produce grammatical and coherent summaries regardless of its semantic relevance with source texts. Figure 1 shows that the summary generated by a current model (RNN encoder-decoder) is similar to the source text literally, but it has low semantic relevance. In this work, our goal is to improve the semantic relevance between source texts and generated summaries for Chinese social media text summarization. To"
P17-2100,C16-1019,1,0.802479,"we use PART I as training set, PART II as development set, and PART III as test set. (4) Previous work has proved that it is effective to represent a span of words without encoding them once more (Wang and Chang, 2016). 3.2 Semantic Relevance 4.2 Experiment Setting Our goal is to compute the semantic relevance of source text and generated summary given semantic vector Vt and Vs . Here, we use cosine similarity to measure the semantic relevance, which is represented with a dot product and magnitude: cos(Vs , Vt ) = Vs · Vt ∥Vs ∥∥Vt ∥ To alleviate the risk of word segmentation mistakes (Xu and Sun, 2016), we use Chinese character sequences as both source inputs and target outputs. We limit the model vocabulary size to 4000, which covers most of the common characters. Each character is represented by a random initialized word embedding. We tune our parameter on the development set. In our model, the embedding size is 400, the hidden state size of encoderdecoder is 500, and the size of gated attention network is 1000. We use Adam optimizer to learn the model parameters, and the batch size is set as 32. The parameter λ is 0.0001. Both the encoder and decoder are based on LSTM unit. Following the"
P17-2100,P16-1218,0,0.0181009,"summary pairs, constructed from a famous Chinese social media website called Sina Weibo1 . It is split into three parts, with 2,400,591 pairs in PART I, 10,666 pairs in PART II and 1,106 pairs in PART III. All the textsummary pairs in PART II and PART III are manually annotated with relevant scores ranged from 1 to 5, and we only reserve pairs with scores no less than 3. Following the previous work, we use PART I as training set, PART II as development set, and PART III as test set. (4) Previous work has proved that it is effective to represent a span of words without encoding them once more (Wang and Chang, 2016). 3.2 Semantic Relevance 4.2 Experiment Setting Our goal is to compute the semantic relevance of source text and generated summary given semantic vector Vt and Vs . Here, we use cosine similarity to measure the semantic relevance, which is represented with a dot product and magnitude: cos(Vs , Vt ) = Vs · Vt ∥Vs ∥∥Vt ∥ To alleviate the risk of word segmentation mistakes (Xu and Sun, 2016), we use Chinese character sequences as both source inputs and target outputs. We limit the model vocabulary size to 4000, which covers most of the common characters. Each character is represented by a random"
P17-2100,P10-1058,0,0.0357048,"cial media corpus. 1 RNN: 中联航空机场发生爆炸致多人死亡。 China United Airlines exploded in the airport, leaving several people dead. Gold: 航班多人吸烟机组人员与乘客冲突。 Several people smoked on a flight which led to a collision between crew and passengers. Figure 1: An example of RNN generated summary. It has high similarity to the text literally, but low semantic relevance. Introduction Text summarization is to produce a brief summary of the main ideas of the text. For long and normal documents, extractive summarization achieves satisfying performance by selecting a few sentences from source texts (Radev et al., 2004; Woodsend and Lapata, 2010; Cheng and Lapata, 2016). However, it does not apply to Chinese social media text summarization, where texts are comparatively short and often full of noise. Therefore, abstractive text summarization, which is based on encoder-decoder framework, is a better choice (Rush et al., 2015; Hu et al., 2015). For extractive summarization, the selected sentences often have high semantic relevance to the text. However, for abstractive text summarization, current models tend to produce grammatical and coherent summaries regardless of its semantic relevance with source texts. Figure 1 shows that the summ"
P17-2100,P16-2092,1,0.794027,"s work, we use PART I as training set, PART II as development set, and PART III as test set. (4) Previous work has proved that it is effective to represent a span of words without encoding them once more (Wang and Chang, 2016). 3.2 Semantic Relevance 4.2 Experiment Setting Our goal is to compute the semantic relevance of source text and generated summary given semantic vector Vt and Vs . Here, we use cosine similarity to measure the semantic relevance, which is represented with a dot product and magnitude: cos(Vs , Vt ) = Vs · Vt ∥Vs ∥∥Vt ∥ To alleviate the risk of word segmentation mistakes (Xu and Sun, 2016), we use Chinese character sequences as both source inputs and target outputs. We limit the model vocabulary size to 4000, which covers most of the common characters. Each character is represented by a random initialized word embedding. We tune our parameter on the development set. In our model, the embedding size is 400, the hidden state size of encoderdecoder is 500, and the size of gated attention network is 1000. We use Adam optimizer to learn the model parameters, and the batch size is set as 32. The parameter λ is 0.0001. Both the encoder and decoder are based on LSTM unit. Following the"
P17-2100,P15-1001,0,\N,Missing
P18-1015,P16-1154,0,0.0665386,"(Banko et al., 2000). Recently, the application of the attentional seq2seq framework has attracted growing attention and achieved state-of-the-art performance on this task (Rush et al., 2015a; Chopra et al., 2016; Nallapati et al., 2016). In addition to the direct application of the general seq2seq framework, researchers attempted to integrate various properties of summarization. For example, Nallapati et al. (2016) enriched the encoder with hand-crafted features such as named entities and POS tags. These features have played important roles in traditional feature based summarization systems. Gu et al. (2016) found that a large proportion of the words in the summary were copied from the source text. Therefore, they proposed CopyNet which considered the copying mechanism during generation. Recently, See et al. (2017) used the coverage mechanism to discourage repetition. Cao et al. (2017b) encoded facts extracted from the source sentence to enhance the summary faithfulness. There were also studies to modify the loss function to fit the evaluation metrics. For instance, Ayana et al. (2016) applied the Minimum Risk Training strategy to maximize the ROUGE scores of generated sum5 Conclusion and Future"
P18-1015,W17-3204,0,0.0762296,"Missing"
P18-1015,W04-1013,0,0.062659,"seq model to generate more faithful and informative summaries. Specifically, since the input of our system consists of both the sentence and soft template, we use the concatenation function3 to combine the hidden states of the sentence and template: 2.2.1 Rerank In Retrieve, the template candidates are ranked according to the text similarity between the corresponding indexed sentences and the input sentence. However, for the summarization task, we expect the soft template r resembles the actual summary y∗ as much as possible. Here we use the widely-used summarization evaluation metrics ROUGE (Lin, 2004) to measure the actual saliency s∗ (r, y∗ ) (see Section 3.2). We utilize the hidden states of x and r to predict the saliency s of the template. Specifically, we regard the output of the BiRNN as the representation of the sentence or template: ← − → − hx = [ h x1 ; h x−1 ] ← − → − hr = [ h r1 ; h r−1 ] Hc = [hx1 ; · · · ; hx−1 ; hr1 ; · · · ; hr−1 ] The combined hidden states are fed into the prevailing attentional RNN decoder (Bahdanau et al., 2014) to generate the decoding hidden state at the position t: st = Att-RNN(st−1 , yt−1 , Hc ), s(r, x) = (2) ot = sof tmax(st Wo ), (3) + bs ), (6) w"
P18-1015,D15-1166,0,0.0773642,"eNLP (Manning et al., 2014) to recognize named entities. 3.3 the sentence. ABS+ Rush et al. (2015a) further tuned the ABS model with additional hand-crafted features to balance between abstraction and extraction. RAS-Elman As the extension of the ABS model, it used a convolutional attention-based encoder and a RNN decoder (Chopra et al., 2016). Featseq2seq Nallapati et al. (2016) used a complete seq2seq RNN model and added the hand-crafted features such as POS tag and NER, to enhance the encoder representation. Luong-NMT Chopra et al. (2016) implemented the neural machine translation model of Luong et al. (2015) for summarization. This model contained two-layer LSTMs with 500 hidden units in each layer. OpenNMT We also implement the standard attentional seq2seq model with OpenNMT. All the settings are the same as our system. It is noted that OpenNMT officially examined the Gigaword dataset. We distinguish the official result6 and our experimental result with suffixes “O” and “I” respectively. FTSum Cao et al. (2017b) encoded the facts extracted from the source sentence to improve both the faithfulness and informativeness of generated summaries. In addition, to evaluate the effectiveness of our joint"
P18-1015,P00-1041,0,0.523097,"of neural text generation. However, they handled the task of Language Modeling and randomly picked an existing sentence in the training corpus. In comparison, we develop an IR system to find proper existing summaries as soft templates. Moreover, Guu et al. (2017) used a general seq2seq framework while we extend the seq2seq framework to conduct template reranking and template-aware summary generation simultaneously. tion include template-based methods (Zhou and Hovy, 2004), syntactic tree pruning (Knight and Marcu, 2002; Clarke and Lapata, 2008) and statistical machine translation techniques (Banko et al., 2000). Recently, the application of the attentional seq2seq framework has attracted growing attention and achieved state-of-the-art performance on this task (Rush et al., 2015a; Chopra et al., 2016; Nallapati et al., 2016). In addition to the direct application of the general seq2seq framework, researchers attempted to integrate various properties of summarization. For example, Nallapati et al. (2016) enriched the encoder with hand-crafted features such as named entities and POS tags. These features have played important roles in traditional feature based summarization systems. Gu et al. (2016) fou"
P18-1015,P14-5010,0,0.00357672,"LESS 3 The number of the generated summaries, which contains less than three tokens. These extremely short summaries are usually unreadable. COPY The proportion of the summary words (without stopwords) copied from the source sentence. A seriously large copy ratio indicates that the summarization system pays more attention to compression rather than required abstraction. NEW NE The number of the named entities that do not appear in the source sentence or actual summary. Intuitively, the appearance of new named entities in the summary is likely to bring unfaithfulness. We use Stanford CoreNLP (Manning et al., 2014) to recognize named entities. 3.3 the sentence. ABS+ Rush et al. (2015a) further tuned the ABS model with additional hand-crafted features to balance between abstraction and extraction. RAS-Elman As the extension of the ABS model, it used a convolutional attention-based encoder and a RNN decoder (Chopra et al., 2016). Featseq2seq Nallapati et al. (2016) used a complete seq2seq RNN model and added the hand-crafted features such as POS tag and NER, to enhance the encoder representation. Luong-NMT Chopra et al. (2016) implemented the neural machine translation model of Luong et al. (2015) for sum"
P18-1015,K16-1028,0,0.407324,"ng online information has necessitated the development of effective automatic summarization systems. In this paper, we focus on an increasingly intriguing task, i.e., abstractive sentence summarization (Rush et al., 2015a), which generates a shorter version of a given sentence while attempting to preserve its original meaning. It can be used to design or refine appealing headlines. Recently, the application of the attentional sequence-to-sequence (seq2seq) framework has attracted growing attention and achieved state-of-the-art performance on this task (Rush et al., 2015a; Chopra et al., 2016; Nallapati et al., 2016). 152 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 152–161 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics and Rewrite. Given the input sentence x, the Retrieve module filters candidate soft templates C = {ri } from the training corpus. For validation and test, we regard the candidate template with the highest predicted saliency (a.k.a informativeness) score as the actual soft template r. For training, we choose the one with the maximal actual saliency score in C, which speeds up converge"
P18-1015,P16-1223,0,0.0259771,"Missing"
P18-1015,D15-1044,0,0.0589636,"and templateaware summary generation (Rewriting). Experiments show that, in terms of informativeness, our model significantly outperforms the state-of-the-art methods, and even soft templates themselves demonstrate high competitiveness. In addition, the import of high-quality external summaries improves the stability and readability of generated summaries. 1 Introduction The exponentially growing online information has necessitated the development of effective automatic summarization systems. In this paper, we focus on an increasingly intriguing task, i.e., abstractive sentence summarization (Rush et al., 2015a), which generates a shorter version of a given sentence while attempting to preserve its original meaning. It can be used to design or refine appealing headlines. Recently, the application of the attentional sequence-to-sequence (seq2seq) framework has attracted growing attention and achieved state-of-the-art performance on this task (Rush et al., 2015a; Chopra et al., 2016; Nallapati et al., 2016). 152 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 152–161 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computati"
P18-1015,N16-1012,0,0.53363,"e exponentially growing online information has necessitated the development of effective automatic summarization systems. In this paper, we focus on an increasingly intriguing task, i.e., abstractive sentence summarization (Rush et al., 2015a), which generates a shorter version of a given sentence while attempting to preserve its original meaning. It can be used to design or refine appealing headlines. Recently, the application of the attentional sequence-to-sequence (seq2seq) framework has attracted growing attention and achieved state-of-the-art performance on this task (Rush et al., 2015a; Chopra et al., 2016; Nallapati et al., 2016). 152 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 152–161 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics and Rewrite. Given the input sentence x, the Retrieve module filters candidate soft templates C = {ri } from the training corpus. For validation and test, we regard the candidate template with the highest predicted saliency (a.k.a informativeness) score as the actual soft template r. For training, we choose the one with the maximal actual saliency score in C,"
P18-1015,P17-1099,0,0.146846,"16; Nallapati et al., 2016). In addition to the direct application of the general seq2seq framework, researchers attempted to integrate various properties of summarization. For example, Nallapati et al. (2016) enriched the encoder with hand-crafted features such as named entities and POS tags. These features have played important roles in traditional feature based summarization systems. Gu et al. (2016) found that a large proportion of the words in the summary were copied from the source text. Therefore, they proposed CopyNet which considered the copying mechanism during generation. Recently, See et al. (2017) used the coverage mechanism to discourage repetition. Cao et al. (2017b) encoded facts extracted from the source sentence to enhance the summary faithfulness. There were also studies to modify the loss function to fit the evaluation metrics. For instance, Ayana et al. (2016) applied the Minimum Risk Training strategy to maximize the ROUGE scores of generated sum5 Conclusion and Future Work This paper proposes to introduce soft templates as additional input to guide the seq2seq summarization. We use the popular IR platform Lucene to retrieve proper existing summaries as candidate soft template"
P18-1015,W04-1000,0,0.59971,"ctive function of likelihood and ROUGE scores. Guu et al. (2017) also proposed to encode human-written sentences to improvement the performance of neural text generation. However, they handled the task of Language Modeling and randomly picked an existing sentence in the training corpus. In comparison, we develop an IR system to find proper existing summaries as soft templates. Moreover, Guu et al. (2017) used a general seq2seq framework while we extend the seq2seq framework to conduct template reranking and template-aware summary generation simultaneously. tion include template-based methods (Zhou and Hovy, 2004), syntactic tree pruning (Knight and Marcu, 2002; Clarke and Lapata, 2008) and statistical machine translation techniques (Banko et al., 2000). Recently, the application of the attentional seq2seq framework has attracted growing attention and achieved state-of-the-art performance on this task (Rush et al., 2015a; Chopra et al., 2016; Nallapati et al., 2016). In addition to the direct application of the general seq2seq framework, researchers attempted to integrate various properties of summarization. For example, Nallapati et al. (2016) enriched the encoder with hand-crafted features such as na"
P18-1090,I17-1064,1,0.782224,"nal sentence. A related study is “back reconstruction” in machine translation (He et al., 2016; Tu et al., 2017). They couple two inverse tasks: one is for translating a sentence in language A to a sentence in language B; the other is for translating a sentence in language B to a sentence in language A. Different from the previous work, we do not introduce the inverse task, but use collaboration between the neutralization module and the emotionalization module. Sentiment analysis is also related to our work (Socher et al., 2011; Pontiki et al., 2015; Rosenthal et al., 2017; Chen et al., 2017; Ma et al., 2017, 2018b). The task usually involves detecting whether a piece of text expresses positive, negative, or neutral sentiment. The sentiment can be general or about a specific topic. one of the reinforcement learning methods, to reward the output of the neutralization module based on the feedback from the emotionalization module. We add different sentiment to the semantic content and use the quality of the generated text as reward. The quality is evaluated by two useful metrics: one for identifying whether the generated text matches the target sentiment; one for evaluating the content preservation"
P18-1090,N18-1018,1,0.88559,"Missing"
P18-1090,E17-1059,0,0.055843,"Missing"
P18-1090,P02-1040,0,0.103498,"dataset contains 230K, 10K, and 3K pairs for training, validation, and testing, respectively. (9) where Rc is calculated as Rc = R1 + R2 (10) Based on Eq. 8 and Eq. 9, we use the sampling approach to estimate the expected reward. This cycled process is repeated until converge. 3.4.1 Reward The reward consists of two parts, sentiment confidence and BLEU. Sentiment confidence evaluates whether the generated text matches the target sentiment. We use a pre-trained classifier to make the judgment. Specially, we use the proposed selfattention based sentiment classifier for implementation. The BLEU (Papineni et al., 2002) score is used to measure the content preservation performance. Considering that the reward should encourage the model to improve both metrics, we use the harmonic mean of sentiment confidence and BLEU as reward, which is formulated as R = (1 + β 2 ) 2 · BLEU · Conf id (β 2 · BLEU ) + Conf id 4.2 We tune hyper-parameters based on the performance on the validation sets. The self-attention based sentiment classifier is trained for 10 epochs on two datasets. We set β for calculating reward to 0.5, hidden size to 256, embedding size to 128, vocabulary size to 50K, learning rate to 0.6, and batch s"
P18-1090,S15-2082,0,0.0806921,"Missing"
P18-1090,S17-2088,0,0.0733652,"Missing"
P18-1090,W17-3526,0,0.0284011,"Missing"
P19-1201,W13-2322,0,0.0255861,"om semantic parser) and pd (x, y) (learned from NL generator) to an underlying unknown distribution P(x, y). Introduction Semantic parsing studies the task of translating natural language (NL) utterances into formal meaning representations (MRs) (Zelle and Mooney, 1996; Tang and Mooney, 2000). NL generation models can be designed to learn the reverse: mapping MRs to their NL descriptions (Wong and Mooney, 2007). Generally speaking, MR often takes a logical form that captures the semantic meaning, including λcalculus (Zettlemoyer and Collins, 2005, 2007), Abstract Meaning Representation (AMR) (Banarescu et al., 2013; Misra and Artzi, 2016), and general-purpose computer programs, such as 1 Code for this paper is available at: github.com/oceanypt/DIM ⟷ https:// Python (Yin and Neubig, 2017) or SQL (Zhong et al., 2017). Recently, NL generation models have been proposed to automatically construct humanreadable descriptions from MRs, for code summarization (Hu et al., 2018; Allamanis et al., 2016; Iyer et al., 2016) that predicts the function of code snippets, and for AMR-to-text generation (Song et al., 2018; Konstas et al., 2017; Flanigan et al., 2016). Specifically, a common objective that semantic parsers"
P19-1201,P17-1005,0,0.019457,"performance, and a high correlation is observed between them (Fig. 7). 5 Related Work Semantic Parsing and NL Generation. Neural sequence-to-sequence models have achieved promising results on semantic parsing (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016; Dong and Lapata, 2018) and natural language generation (Iyer et al., 2016; Konstas et al., 2017; Hu et al., 2018). To better model structured MRs, tree structures and more complicated graphs are explored for both parsing and generation (Dong and Lapata, 2016; Rabinovich et al., 2017; Yin and Neubig, 2017; Song et al., 2018; Cheng et al., 2017; Alon et al., 2018). Semisupervised learning has been widely studied for semantic parsing (Yin et al., 2018b; Kocisk´y et al., 2016; Jia and Liang, 2016). Similar to our work, Chen and Zhou (2018) and Allamanis et al. (2015) study code retrieval and code summarization jointly to enhance both tasks. Here, we focus on the more challenging task of code generation instead of retrieval, and we also aim for generalpurpose MRs. Joint Learning in NLP. There has been growing interests in leveraging related NLP problems to enhance primal tasks (Collobert et al., 2011; Peng et al., 2017; Liu et al., 201"
P19-1201,P16-1004,0,0.594396,"l information learning. With the objective of Eq. 3, we jointly learn a parser and a generator, as well as maximize the dual information between the two. LDIM serves as a regularization term to influence the learning process, whose detailed algorithm is described in §3. Our method of DIM is model-independent. If the learning objectives for semantic parser and NL generator are subject to Eq. 1 and Eq. 2, we can always adopt DIM to conduct joint learning. Out of most commonly used seq2seq models for the parser and generator, more complex tree and graph structures have been adopted to model MRs (Dong and Lapata, 2016; Song et al., 2018). In this paper, without loss of generality, we study our joint-learning method on the widely-used seq2seq LeDIM LdDIM frameworks mentioned above (§2.1). 3 Dual Information Maximization In this section, we first introduce dual information in §3.1, followed by its maximization (§3.2). §3.3 discusses its extension with semi-supervision. 3.1 Dual Information As discussed above, we treat semantic parsing and NL generation as the dual tasks and exploit the duality between the two tasks for our joint learning. With conditional distributions pθ (y|x) for the parser and qφ (x|y) fo"
P19-1201,P17-1097,0,0.0233711,"to obtain the final lower bound. To learn the lower bound of LeDIM , we provide the following method to calculate its gradients: Gradient Estimation. We adopt Monte Carlo samples using the REINFORCE policy (Williams, 1992) to approximate the gradient of LeDIM (θ, φ) with regard to θ: ∇θ LeDIM (θ, φ) = Epθ (y|x) ∇θ log pθ (y|x) · [log qφ (x|y) + log q(y) − b] = Epθ (y|x) ∇θ log pθ (y|x) · l(x, y; φ) (7) X 1 ˆ i ; φ) ∇θ log pθ (ˆ yi |x) · l(x, y ≈ |S| ˆ i ∈S y l(x, y; φ) can be seen as the learning signal from the dual model, which is similar to the reward in reinforcement learning algorithms (Guu et al., 2017; Paulus et al., 2017). To handle the highvariance of learning signals, we adopt the baseline function b by empirically averaging the signals to stabilize the learning process (Williams, 1992). With prior pθ (·|x), we use beam search to generate a pool of MR candidates (y), denoted as S, for the input of x. The gradient with regard to φ is then calculated as: ∇φ LeDIM (θ, φ) = Epθ (y|x) ∇φ log qφ (x|y) 1 X ≈ ∇φ log qφ (x|ˆ yi ) (8) |S| ˆ i ∈S y The above maximization procedure for LeDIM is analogous to the EM algorithm: Step 1: Freeze φ and find the optimal θ∗ = arg maxθ LeDIM (θ, φ) with Eq."
P19-1201,P16-1195,0,0.158342,", 2007). Generally speaking, MR often takes a logical form that captures the semantic meaning, including λcalculus (Zettlemoyer and Collins, 2005, 2007), Abstract Meaning Representation (AMR) (Banarescu et al., 2013; Misra and Artzi, 2016), and general-purpose computer programs, such as 1 Code for this paper is available at: github.com/oceanypt/DIM ⟷ https:// Python (Yin and Neubig, 2017) or SQL (Zhong et al., 2017). Recently, NL generation models have been proposed to automatically construct humanreadable descriptions from MRs, for code summarization (Hu et al., 2018; Allamanis et al., 2016; Iyer et al., 2016) that predicts the function of code snippets, and for AMR-to-text generation (Song et al., 2018; Konstas et al., 2017; Flanigan et al., 2016). Specifically, a common objective that semantic parsers aim to estimate is pθ (y|x), the conditional distribution between NL input x and the corresponding MR output y, as demonstrated in Fig. 1. Similarly, for NL generation from MRs, the goal is to learn a generator of qφ (x|y). As demonstrated in Fig. 2, there is a clear duality between the two tasks, given that one task’s input is the other task’s output, and vice versa. However, such duality remains l"
P19-1201,P16-1002,0,0.256039,"competitive comparmore, we leverage pointer network (Vinyals et al., ison models trained for each task separately. 2015) to copy tokens from the input to handle outof-vocabulary (OOV) words. The structured MRs Overall, we have the following contributions in are linearized for the sequential encoder and dethis work: 2091 082 083 084 085 086 087 088 089 090 091 092 093 094 095 096 097 098 099 coder. More details of the parser and the generator can be found in Appendix A. Briefly speaking, our models differ from existing work as follows: PARSER: Our architecture is similar to the one proposed in Jia and Liang (2016) for semantic parsing; G ENERATOR: Our model improves upon the D EEP C OM coder summarization system (Hu et al., 2018) by: 1) replacing LSTM with biLSTM for the encoder to better model context, and 2) adding copying mechanism. 2.2 Jointly Learning Parser and Generator Our joint learning framework is designed to model the duality between a parser and a generator. To incorporate the duality into our learning process, we design the framework to encourage the expected joint distributions pe (x, y) and pd (x, y) to both approximate the unknown joint distribution of x and y (shown in Fig. 1). To ach"
P19-1201,D16-1116,0,0.0408621,"Missing"
P19-1201,P17-1014,0,0.106667,"(Zettlemoyer and Collins, 2005, 2007), Abstract Meaning Representation (AMR) (Banarescu et al., 2013; Misra and Artzi, 2016), and general-purpose computer programs, such as 1 Code for this paper is available at: github.com/oceanypt/DIM ⟷ https:// Python (Yin and Neubig, 2017) or SQL (Zhong et al., 2017). Recently, NL generation models have been proposed to automatically construct humanreadable descriptions from MRs, for code summarization (Hu et al., 2018; Allamanis et al., 2016; Iyer et al., 2016) that predicts the function of code snippets, and for AMR-to-text generation (Song et al., 2018; Konstas et al., 2017; Flanigan et al., 2016). Specifically, a common objective that semantic parsers aim to estimate is pθ (y|x), the conditional distribution between NL input x and the corresponding MR output y, as demonstrated in Fig. 1. Similarly, for NL generation from MRs, the goal is to learn a generator of qφ (x|y). As demonstrated in Fig. 2, there is a clear duality between the two tasks, given that one task’s input is the other task’s output, and vice versa. However, such duality remains largely unstudied, even though joint modeling has been demonstrated effective in various NLP problems, e.g. question a"
P19-1201,P16-1057,0,0.13213,"Missing"
P19-1201,D15-1166,0,0.0192631,"mized (§3.3). pata, 2016; Hu et al., 2018), and without loss We experiment with three datasets from two difof generality, we adopt it as the basic frameferent domains: ATIS for dialogue management; work for both tasks in this work. Specifically, for D JANGO and C O NA L A for code generation and both pθ (y|x) and qφ (x|y), we use a two-layer bisummarization. Experimental results show that directional LSTM (bi-LSTM) as the encoder and both the semantic parser and generator can be conanother one-layer LSTM as the decoder with atsistently improved with joint learning using DIM tention mechanism (Luong et al., 2015). Furtherand S EMI DIM, compared to competitive comparmore, we leverage pointer network (Vinyals et al., ison models trained for each task separately. 2015) to copy tokens from the input to handle outof-vocabulary (OOV) words. The structured MRs Overall, we have the following contributions in are linearized for the sequential encoder and dethis work: 2091 082 083 084 085 086 087 088 089 090 091 092 093 094 095 096 097 098 099 coder. More details of the parser and the generator can be found in Appendix A. Briefly speaking, our models differ from existing work as follows: PARSER: Our architectur"
P19-1201,P18-1068,0,0.417535,"the dual information and unsupervised objectives equally for simplicity, so the lower bounds over them are combined for joint optimization. We combine the labeled and unlabeled data to calculate the lower bounds to optimize the variational lower bounds of dual information and unsupervised objectives. 4 4.1 Experiments Datasets S EMANTIC PARSING (in Acc.) Pro. S UPER DIM S EMI DIM 1/4 64.7 69.0 71.9 1/2 78.1 78.8 80.8 full 84.6 85.3 – Previous Supervised Methods (Pro. = full) S EQ 2T REE (Dong and Lapata, 2016) ASN (Rabinovich et al., 2017) ASN+S UPATT (Rabinovich et al., 2017) C OARSE 2F INE (Dong and Lapata, 2018) S ELF T RAIN 66.3 79.2 – Acc. 84.6 85.3 85.9 87.7 NL G ENERATION (in BLEU) Pro. S UPER DIM S EMI DIM 1/4 36.9 37.7 39.1 1/2 39.1 40.7 40.9 full 39.3 40.6 – Previous Supervised Methods (Pro. = full) D EEP C OM (Hu et al., 2018) BACK B OOST 40.9 39.3 – BLEU 42.3 Table 2: Semantic parsing and NL generation results on ATIS. Pro.: proportion of the training samples used for training. Best result in each row is highlighted in bold. |full |= 4,434. are lowercased and tokenized and the tokens in code snippets are separated with space. Statistics of the datasets are summarized in Table 1. 4.2 Experime"
P19-1201,N16-1087,0,0.0212966,"ns, 2005, 2007), Abstract Meaning Representation (AMR) (Banarescu et al., 2013; Misra and Artzi, 2016), and general-purpose computer programs, such as 1 Code for this paper is available at: github.com/oceanypt/DIM ⟷ https:// Python (Yin and Neubig, 2017) or SQL (Zhong et al., 2017). Recently, NL generation models have been proposed to automatically construct humanreadable descriptions from MRs, for code summarization (Hu et al., 2018; Allamanis et al., 2016; Iyer et al., 2016) that predicts the function of code snippets, and for AMR-to-text generation (Song et al., 2018; Konstas et al., 2017; Flanigan et al., 2016). Specifically, a common objective that semantic parsers aim to estimate is pθ (y|x), the conditional distribution between NL input x and the corresponding MR output y, as demonstrated in Fig. 1. Similarly, for NL generation from MRs, the goal is to learn a generator of qφ (x|y). As demonstrated in Fig. 2, there is a clear duality between the two tasks, given that one task’s input is the other task’s output, and vice versa. However, such duality remains largely unstudied, even though joint modeling has been demonstrated effective in various NLP problems, e.g. question answering and generation"
P19-1201,D16-1183,0,0.0214721,"pd (x, y) (learned from NL generator) to an underlying unknown distribution P(x, y). Introduction Semantic parsing studies the task of translating natural language (NL) utterances into formal meaning representations (MRs) (Zelle and Mooney, 1996; Tang and Mooney, 2000). NL generation models can be designed to learn the reverse: mapping MRs to their NL descriptions (Wong and Mooney, 2007). Generally speaking, MR often takes a logical form that captures the semantic meaning, including λcalculus (Zettlemoyer and Collins, 2005, 2007), Abstract Meaning Representation (AMR) (Banarescu et al., 2013; Misra and Artzi, 2016), and general-purpose computer programs, such as 1 Code for this paper is available at: github.com/oceanypt/DIM ⟷ https:// Python (Yin and Neubig, 2017) or SQL (Zhong et al., 2017). Recently, NL generation models have been proposed to automatically construct humanreadable descriptions from MRs, for code summarization (Hu et al., 2018; Allamanis et al., 2016; Iyer et al., 2016) that predicts the function of code snippets, and for AMR-to-text generation (Song et al., 2018; Konstas et al., 2017; Flanigan et al., 2016). Specifically, a common objective that semantic parsers aim to estimate is pθ ("
P19-1201,P17-1186,0,0.0225296,"g et al., 2018; Cheng et al., 2017; Alon et al., 2018). Semisupervised learning has been widely studied for semantic parsing (Yin et al., 2018b; Kocisk´y et al., 2016; Jia and Liang, 2016). Similar to our work, Chen and Zhou (2018) and Allamanis et al. (2015) study code retrieval and code summarization jointly to enhance both tasks. Here, we focus on the more challenging task of code generation instead of retrieval, and we also aim for generalpurpose MRs. Joint Learning in NLP. There has been growing interests in leveraging related NLP problems to enhance primal tasks (Collobert et al., 2011; Peng et al., 2017; Liu et al., 2016), e.g. sequence tagging (Collobert et al., 2011), dependency parsing (Peng et al., 2017), discourse analysis (Liu et al., 2016). Among those, multi-task learning (MTL) (Ando and Zhang, 2005) is a common method for joint learning, especially for neural networks where parameter sharing is utilized for representation learning. We follow the recent work on dual learning (Xia et al., 2017) to train dual tasks, where interactions can be employed to enhance both models. Dual learning has been successfully applied in NLP and computer vision problems, such as neural machine translati"
P19-1201,P17-1105,0,0.104424,"i ∼qφ (·|y) (13) where Dx = Ux ∪ Lx and Dy = Uy ∪ Ly . In this work, we weight the dual information and unsupervised objectives equally for simplicity, so the lower bounds over them are combined for joint optimization. We combine the labeled and unlabeled data to calculate the lower bounds to optimize the variational lower bounds of dual information and unsupervised objectives. 4 4.1 Experiments Datasets S EMANTIC PARSING (in Acc.) Pro. S UPER DIM S EMI DIM 1/4 64.7 69.0 71.9 1/2 78.1 78.8 80.8 full 84.6 85.3 – Previous Supervised Methods (Pro. = full) S EQ 2T REE (Dong and Lapata, 2016) ASN (Rabinovich et al., 2017) ASN+S UPATT (Rabinovich et al., 2017) C OARSE 2F INE (Dong and Lapata, 2018) S ELF T RAIN 66.3 79.2 – Acc. 84.6 85.3 85.9 87.7 NL G ENERATION (in BLEU) Pro. S UPER DIM S EMI DIM 1/4 36.9 37.7 39.1 1/2 39.1 40.7 40.9 full 39.3 40.6 – Previous Supervised Methods (Pro. = full) D EEP C OM (Hu et al., 2018) BACK B OOST 40.9 39.3 – BLEU 42.3 Table 2: Semantic parsing and NL generation results on ATIS. Pro.: proportion of the training samples used for training. Best result in each row is highlighted in bold. |full |= 4,434. are lowercased and tokenized and the tokens in code snippets are separated w"
P19-1201,P16-1009,0,0.0407195,"on task in C O NA L A, we use BLEU-4 following the setup in Yin et al. (2018a). Baselines. We compare our methods of DIM and S EMI DIM with the following baselines: S UPER: Train the parser or generator separately without joint learning. The models for the parser and generator are the same as DIM. S ELF T RAIN (Lee, 2013): We use the pre-trained parser or generator to generate pseudo labels for the unlabeled sources, then the constructed pseudo samples will be mixed with the labeled data to fine-tune the pre-trained parser or generator. BACK B OOST: Adopted from the back translation method in Sennrich et al. (2016), which generates sources from unlabeled targets. The training process for BACK B OOST is the same as in S ELF T RAIN. In addition to the above baselines, we also compare with popular supervised methods for each task, shown in the corresponding result tables. 4.3 BACK B OOST 9.0 – Results and Further Analysis Main Results with Full- and Semi-supervision. Results on the three datasets with supervised and semi-supervised setups are presented in Tables 2, 3, and 4. For semi-supervised experiments on ATIS, we use the NL part as extra unlabeled samTable 4: Code generation and code summarization res"
P19-1201,P18-1150,0,0.182968,"ncluding λcalculus (Zettlemoyer and Collins, 2005, 2007), Abstract Meaning Representation (AMR) (Banarescu et al., 2013; Misra and Artzi, 2016), and general-purpose computer programs, such as 1 Code for this paper is available at: github.com/oceanypt/DIM ⟷ https:// Python (Yin and Neubig, 2017) or SQL (Zhong et al., 2017). Recently, NL generation models have been proposed to automatically construct humanreadable descriptions from MRs, for code summarization (Hu et al., 2018; Allamanis et al., 2016; Iyer et al., 2016) that predicts the function of code snippets, and for AMR-to-text generation (Song et al., 2018; Konstas et al., 2017; Flanigan et al., 2016). Specifically, a common objective that semantic parsers aim to estimate is pθ (y|x), the conditional distribution between NL input x and the corresponding MR output y, as demonstrated in Fig. 1. Similarly, for NL generation from MRs, the goal is to learn a generator of qφ (x|y). As demonstrated in Fig. 2, there is a clear duality between the two tasks, given that one task’s input is the other task’s output, and vice versa. However, such duality remains largely unstudied, even though joint modeling has been demonstrated effective in various NLP pro"
P19-1201,D17-1090,0,0.0912481,"Missing"
P19-1201,W00-1317,0,0.118976,"ed and semi-supervised setups1 . 1 x y qϕ pθ x e → y p (x, y) = p(x)pθ (y|x) y − → x d p (x, y) = q(y)qϕ (x|y) Figure 1: Illustration of our joint learning model. x: NL; y: MRs. pθ (y|x): semantic parser; qφ (x|y): NL generator. We model the duality of the two tasks by matching the joint distributions of pe (x, y) (learned from semantic parser) and pd (x, y) (learned from NL generator) to an underlying unknown distribution P(x, y). Introduction Semantic parsing studies the task of translating natural language (NL) utterances into formal meaning representations (MRs) (Zelle and Mooney, 1996; Tang and Mooney, 2000). NL generation models can be designed to learn the reverse: mapping MRs to their NL descriptions (Wong and Mooney, 2007). Generally speaking, MR often takes a logical form that captures the semantic meaning, including λcalculus (Zettlemoyer and Collins, 2005, 2007), Abstract Meaning Representation (AMR) (Banarescu et al., 2013; Misra and Artzi, 2016), and general-purpose computer programs, such as 1 Code for this paper is available at: github.com/oceanypt/DIM ⟷ https:// Python (Yin and Neubig, 2017) or SQL (Zhong et al., 2017). Recently, NL generation models have been proposed to automaticall"
P19-1201,P18-1070,0,0.377635,"x y x y x y E XAMPLE can you list all flights from chicago to milwaukee ( lambda $0 e ( and ( flight $0) ( from $0 chicago: ci) ( to $0 milwaukee: ci) ) ) convert max entries into a string, substitute it for self. max entries. self. max entries = int(max entries) more pythonic alternative for getting a value in range not using min and max a = 1 if x < 1 else 10 if x > 10 else x Ave. Token 10.6 26.5 11.9 8.2 9.7 14.1 Figure 2: Sample natural language utterances and meaning representations from datasets used in this work: ATIS for dialogue management; D JANGO (Oda et al., 2015) and C O NA L A (Yin et al., 2018a) for code generation and summarization. 033 034 035 036 037 038 039 040 041 042 043 044 045 046 047 048 049 073 074 075 076 077 078 079 080 081 031 032 072 • We are the first to jointly study semantic parsation (Xia et al., 2017). ing and natural language generation by exIn this paper, we propose to jointly model seploiting the duality between the two tasks; mantic parsing and NL generation by exploiting • We propose DIM to capture the duality and the interaction between the two tasks. Followadopt variational approximation to maximize ing previous work on dual learning (Xia et al., the dual"
P19-1201,D07-1071,0,0.0807189,"ous Supervised Methods (Pro. = full) D EEP C OM (Hu et al., 2018) BACK B OOST 40.9 39.3 – BLEU 42.3 Table 2: Semantic parsing and NL generation results on ATIS. Pro.: proportion of the training samples used for training. Best result in each row is highlighted in bold. |full |= 4,434. are lowercased and tokenized and the tokens in code snippets are separated with space. Statistics of the datasets are summarized in Table 1. 4.2 Experiments are conducted on three datasets with sample pairs shown in Fig. 2: one for dialogue management which studies semantic parsing and generation from λ-calculus (Zettlemoyer and Collins, 2007) (ATIS) and two for code generation and summarization (D JANGO, C O NA L A). ATIS . This dataset has 5,410 pairs of queries (NL) from a flight booking system and corresponding λcalculus representation (MRs). The anonymized version from Dong and Lapata (2016) is used. D JANGO. It contains 18,805 lines of Python code snippets (Oda et al., 2015). Each snippet is annotated with a piece of human-written pseudo code. Similar to Yin and Neubig (2017), we replace strings separated by quotation marks with indexed place holder in NLs and MRs. C O N A L A. This is another Python-related corpus containing"
P19-1201,N07-1022,0,0.0604602,": Illustration of our joint learning model. x: NL; y: MRs. pθ (y|x): semantic parser; qφ (x|y): NL generator. We model the duality of the two tasks by matching the joint distributions of pe (x, y) (learned from semantic parser) and pd (x, y) (learned from NL generator) to an underlying unknown distribution P(x, y). Introduction Semantic parsing studies the task of translating natural language (NL) utterances into formal meaning representations (MRs) (Zelle and Mooney, 1996; Tang and Mooney, 2000). NL generation models can be designed to learn the reverse: mapping MRs to their NL descriptions (Wong and Mooney, 2007). Generally speaking, MR often takes a logical form that captures the semantic meaning, including λcalculus (Zettlemoyer and Collins, 2005, 2007), Abstract Meaning Representation (AMR) (Banarescu et al., 2013; Misra and Artzi, 2016), and general-purpose computer programs, such as 1 Code for this paper is available at: github.com/oceanypt/DIM ⟷ https:// Python (Yin and Neubig, 2017) or SQL (Zhong et al., 2017). Recently, NL generation models have been proposed to automatically construct humanreadable descriptions from MRs, for code summarization (Hu et al., 2018; Allamanis et al., 2016; Iyer et"
P19-1201,P17-1041,0,0.442857,"l language (NL) utterances into formal meaning representations (MRs) (Zelle and Mooney, 1996; Tang and Mooney, 2000). NL generation models can be designed to learn the reverse: mapping MRs to their NL descriptions (Wong and Mooney, 2007). Generally speaking, MR often takes a logical form that captures the semantic meaning, including λcalculus (Zettlemoyer and Collins, 2005, 2007), Abstract Meaning Representation (AMR) (Banarescu et al., 2013; Misra and Artzi, 2016), and general-purpose computer programs, such as 1 Code for this paper is available at: github.com/oceanypt/DIM ⟷ https:// Python (Yin and Neubig, 2017) or SQL (Zhong et al., 2017). Recently, NL generation models have been proposed to automatically construct humanreadable descriptions from MRs, for code summarization (Hu et al., 2018; Allamanis et al., 2016; Iyer et al., 2016) that predicts the function of code snippets, and for AMR-to-text generation (Song et al., 2018; Konstas et al., 2017; Flanigan et al., 2016). Specifically, a common objective that semantic parsers aim to estimate is pθ (y|x), the conditional distribution between NL input x and the corresponding MR output y, as demonstrated in Fig. 1. Similarly, for NL generation from MR"
W01-1305,P92-1030,0,0.0332149,"Missing"
W01-1305,M93-1023,0,\N,Missing
W07-1511,W03-2405,0,0.024716,"lysis. However, the performances of automatic collocation extraction systems are not satisfactory (Pecina 2005). A problem is that collocations are word combinations that co-occur within a short context, but not all such co-occurrences are true collocations. Further examinations is needed to filter out pseudo-collocations once co-occurred word pairs are identified. A collocation bank with true collocations annotated is naturally an indispensable resource for collocation research. (Kosho et al. 2000) presented their works of collocation annotation on Japanese text. Also, the Turkish treebank, (Bedin 2003) included collocation annotation as one step in its annotation. These two collocation banks provided collocation identification and co-occurrence verification information. (Tutin 2005) used shallow analysis based on finite state transducers and lexicon-grammar to identify and annotate collocations in a French corpus. This collocation bank further provided the lexical functions of the collocations. However to this day, there is no reported Chinese collocation bank available. 61 Proceedings of the Linguistic Annotation Workshop, pages 61–68, c Prague, June 2007. 2007 Association for Computationa"
W07-1511,shudo-etal-2000-collocations,0,0.0790794,"Missing"
W07-1511,J93-1007,0,\N,Missing
W07-1511,P05-2003,0,\N,Missing
W10-2416,P02-1060,0,0.0506863,"and knowledge bases. Later, Collins (1999) adopted the AdaBoost algorithm to find a weighted combination of simple classifiers. They reported that the combination of simple classifiers can yield some powerful systems with much better performances. As a matter of fact, these methods all need manual studies on the construction of the rule set or the simple classifiers. Machine learning models attract more attentions recently. Usually, they train classification models based on context features. Various lexical and syntactic features are considered, such as N-grams, Part-Of-Speech (POS), and etc. Zhou and Su (2002) integrated four different kinds of features, which convey different semantic information, for a classification model based on the Hidden Markov Model (HMM). Koen (2006) built a classifier with the Conditional Random Field (CRF) model to classify noun phrases in a text with the WordNet SynSet. Isozaki and Kazawa (2002) studied the use of SVM instead. There were fewer studies in Chinese entity categorization. Guo and Jiang (2005) applied Robust Risk Minimization to classify the named entities. The features include seven traditional lexical features and two external-NE-hints based features. An i"
W10-2416,C02-1054,0,0.0364763,"construction of the rule set or the simple classifiers. Machine learning models attract more attentions recently. Usually, they train classification models based on context features. Various lexical and syntactic features are considered, such as N-grams, Part-Of-Speech (POS), and etc. Zhou and Su (2002) integrated four different kinds of features, which convey different semantic information, for a classification model based on the Hidden Markov Model (HMM). Koen (2006) built a classifier with the Conditional Random Field (CRF) model to classify noun phrases in a text with the WordNet SynSet. Isozaki and Kazawa (2002) studied the use of SVM instead. There were fewer studies in Chinese entity categorization. Guo and Jiang (2005) applied Robust Risk Minimization to classify the named entities. The features include seven traditional lexical features and two external-NE-hints based features. An important result they reported is that character-based features can be as good as wordbased features since they avoid the Chinese word segmentation errors. In (Jing et al., 2003), it was further reported that pure character-based models can even outperform word-based models with character combination features. Deep Beli"
W10-2416,W06-0505,0,0.0667592,"Missing"
W10-2416,W99-0613,0,0.195004,"Missing"
W10-2416,A97-1030,0,0.171859,"Missing"
W10-2416,W03-1026,0,\N,Missing
W10-4115,I05-2023,0,0.0688774,"Missing"
W10-4115,N07-1015,0,0.0695042,"Missing"
W10-4115,P06-1017,0,0.049374,"Missing"
W10-4115,P08-2023,1,0.826889,"Missing"
W10-4115,W02-1010,0,0.0822543,"Missing"
W10-4115,zhang-etal-2008-exploiting,1,0.52438,"ul. Kernel-based approaches utilize kernel functions on structures between two entities, such as sequences and trees, to measure the similarity between two relation instances. Zelenok (2003) applied parsing tree kernel function to distinguish whether there was an existing relationship between two entities. However, they limited their task on Personaffiliation and organization-location. The previous work mainly concentrated on relation extraction in English. Relatively, less attention was drawn on Chinese relation extraction. However, its importance is being gradually recognized. For instance, Zhang et al. (2008) combined position information, entity type and context features in a feature-based approach and Che (2005) introduced the edit distance kernel over the original Chinese string representation. DBN is a new feature-based approach for NLP tasks. According to the work by Hinton (2006), DBN consisted of several layers including multiple Restricted Boltzmann Machine (RBM) layers and a Back Propagation (BP) layer. It was reported to perform very well in many classification problems (Ackley, 1985), which is from the origin of its ability to scale gracefully and be computationally tractable when appli"
W10-4115,W03-1026,0,\N,Missing
W10-4152,S07-1024,1,\N,Missing
W10-4152,P98-1012,0,\N,Missing
W10-4152,C98-1012,0,\N,Missing
W10-4152,S07-1012,0,\N,Missing
W12-0603,P07-1125,0,0.0267557,"ith different training/unlabeled data ratios for three major Twitter categories and a mixed-type category provides solid evidential support for future research. 19 As in many practical applications, sufficient annotated data are hard to obtain. Therefore, unsupervised and semi-supervised learning methods are actively pursued. While unsupervised sentence classification is rule-based and domain-dependent (Deshpande et al. 2010), semi-supervised methods that both alleviate the data deficiency problem and leverage the power of state-of-the-art classifiers hold more promises for different domains (Medlock and Briscoe 2007; Erkan et al. 2007). In the machine learning literature, a classic semi-supervised learning scheme is proposed by Yarowsky (1995), which is a classical selfteaching process that makes no use of labeled data before they are classified. More theoretical analyses are made by (Culp and Michailidis 2007) and (Haffari and Sarkar 2007). Transductive SVM (Joachims 1999) extends the state-of-the-art inductive SVM by explicitly considering the relationship between labeled and unlabeled data. The graph-based label propagation model (Zhu et al. 2003; Zhou et al. 2004) using a harmonic function also accom"
W12-0603,W04-3240,0,0.599104,"N 2. Related Work The automatic recognition of speech act, also known as “dialogue act”, has attracted sustained interest in computational linguistics and speech technology for over a decade (Searle 1975; Stolcke et al. 2000). A few annotated corpora such as Switchboard-DAMSL (Jurafsky et al. 1997) and Meeting Recorder Dialog Act (Dhillon et al. 2004) are widely used, with data transcribed from telephone or face-to-face conversation. Prior to the flourish of microblogging services such as Twitter, speech act recognition has been extended to electronic media such as email and discussion forum (Cohen et al. 2004; Feng et al. 2006) in order to study the behavior of email or message senders. The annotated corpora for ordinary verbal communications and the methods developed for email, or discussion forum cannot be directly used for our task because Twitter text has a distinctive Netspeak style that is situated between speech and text but resembles neither (Crystal 2006, 2011). Compared with email or forum post, it is rife with linguistic noises such as spelling mistakes, random coinages, mixed use of letters and symbols. Speech act recognition in Twitter is a fairly new task. In our pioneering work (Zha"
W12-0603,J00-3003,0,0.0558906,", Section 6 concludes the paper and outlines future directions. Speech Act Libya Releases 4 Times Statement Journalists http://www.photozz.com/?104k #sincewebeinghonest why u so Question obsessed with what me n her do?? Don't u got ya own man???? Oh wait..... RT @NaonkaMixon: I will Suggestion donate 10 $ to the Red Cross Japan Earthquake fund for every person that retweets this! #PRAYFORJAPAN 2. Related Work The automatic recognition of speech act, also known as “dialogue act”, has attracted sustained interest in computational linguistics and speech technology for over a decade (Searle 1975; Stolcke et al. 2000). A few annotated corpora such as Switchboard-DAMSL (Jurafsky et al. 1997) and Meeting Recorder Dialog Act (Dhillon et al. 2004) are widely used, with data transcribed from telephone or face-to-face conversation. Prior to the flourish of microblogging services such as Twitter, speech act recognition has been extended to electronic media such as email and discussion forum (Cohen et al. 2004; Feng et al. 2006) in order to study the behavior of email or message senders. The annotated corpora for ordinary verbal communications and the methods developed for email, or discussion forum cannot be dire"
W12-0603,P95-1026,0,0.0326151,"for future research. 19 As in many practical applications, sufficient annotated data are hard to obtain. Therefore, unsupervised and semi-supervised learning methods are actively pursued. While unsupervised sentence classification is rule-based and domain-dependent (Deshpande et al. 2010), semi-supervised methods that both alleviate the data deficiency problem and leverage the power of state-of-the-art classifiers hold more promises for different domains (Medlock and Briscoe 2007; Erkan et al. 2007). In the machine learning literature, a classic semi-supervised learning scheme is proposed by Yarowsky (1995), which is a classical selfteaching process that makes no use of labeled data before they are classified. More theoretical analyses are made by (Culp and Michailidis 2007) and (Haffari and Sarkar 2007). Transductive SVM (Joachims 1999) extends the state-of-the-art inductive SVM by explicitly considering the relationship between labeled and unlabeled data. The graph-based label propagation model (Zhu et al. 2003; Zhou et al. 2004) using a harmonic function also accommodates the knowledge about unlabeled data. We will adapt both of them to our multiclass classification task. Jeong et al. (2009)"
W12-0603,D07-1024,0,0.011887,"abeled data ratios for three major Twitter categories and a mixed-type category provides solid evidential support for future research. 19 As in many practical applications, sufficient annotated data are hard to obtain. Therefore, unsupervised and semi-supervised learning methods are actively pursued. While unsupervised sentence classification is rule-based and domain-dependent (Deshpande et al. 2010), semi-supervised methods that both alleviate the data deficiency problem and leverage the power of state-of-the-art classifiers hold more promises for different domains (Medlock and Briscoe 2007; Erkan et al. 2007). In the machine learning literature, a classic semi-supervised learning scheme is proposed by Yarowsky (1995), which is a classical selfteaching process that makes no use of labeled data before they are classified. More theoretical analyses are made by (Culp and Michailidis 2007) and (Haffari and Sarkar 2007). Transductive SVM (Joachims 1999) extends the state-of-the-art inductive SVM by explicitly considering the relationship between labeled and unlabeled data. The graph-based label propagation model (Zhu et al. 2003; Zhou et al. 2004) using a harmonic function also accommodates the knowledg"
W12-0603,N06-1027,0,0.258877,"Missing"
W12-0603,D09-1130,0,0.262709,"d by Yarowsky (1995), which is a classical selfteaching process that makes no use of labeled data before they are classified. More theoretical analyses are made by (Culp and Michailidis 2007) and (Haffari and Sarkar 2007). Transductive SVM (Joachims 1999) extends the state-of-the-art inductive SVM by explicitly considering the relationship between labeled and unlabeled data. The graph-based label propagation model (Zhu et al. 2003; Zhou et al. 2004) using a harmonic function also accommodates the knowledge about unlabeled data. We will adapt both of them to our multiclass classification task. Jeong et al. (2009) report a semi-supervised approach to classifying speech acts in emails and online forums. But their subtree-based method is not applicable to our task because Twitter’s noisy textual quality cannot be found in the much cleaner email or forum texts. 3. Supervised Learning of Speech Act Types Supervised learning of speech act types in Twitter relies heavily on a good set of features that capture the textual characteristics of both Twitter and speech act utterances. As in our previous work, we use speech act-specific cues, special words (abbreviations and acronyms, opinion words, vulgar words, a"
W12-6307,I05-3017,0,0.0339892,"w experiments of evaluation methods. The final corpora consist of 5000 sentences (or articles, strictly. For simplicity, we refer to the individual article as a sentence, since most of the MicroBlog articles consist of only one sentence.) For evaluation, we adopt the evaluation method used in previous bake-off tasks, and use precision, recall and F-measure to measure the overPreface After years of intensive researches, Chinese word segmentation has achieved a quite high precision. Five prior word segmentation bakeoffs, have been successfully conducted in 2003 (Sproat and Emerson, 2003), 2005 (Emerson, 2005), 2006 (Levow, 2006), 2007 (Jin and Chen, 2007) and 2012 (Zhao and Liu, 2010). These evaluations have established benchmarks for word segmentation with which researchers could evaluate their segmentation system. However, the performance of segmentation is not so satisfying for the MicroBlog corpora. The corpus of a specific domain may have its characteristics in vocabulary, sentence pattern and style. MicroBlog makes no exception. The MicroBlog texts are much similar to oral expression, with a casual style and less deliberation in writing, resulting in a simple and comfortable style: the Micro"
W12-6307,W06-0115,0,0.078264,"ation methods. The final corpora consist of 5000 sentences (or articles, strictly. For simplicity, we refer to the individual article as a sentence, since most of the MicroBlog articles consist of only one sentence.) For evaluation, we adopt the evaluation method used in previous bake-off tasks, and use precision, recall and F-measure to measure the overPreface After years of intensive researches, Chinese word segmentation has achieved a quite high precision. Five prior word segmentation bakeoffs, have been successfully conducted in 2003 (Sproat and Emerson, 2003), 2005 (Emerson, 2005), 2006 (Levow, 2006), 2007 (Jin and Chen, 2007) and 2012 (Zhao and Liu, 2010). These evaluations have established benchmarks for word segmentation with which researchers could evaluate their segmentation system. However, the performance of segmentation is not so satisfying for the MicroBlog corpora. The corpus of a specific domain may have its characteristics in vocabulary, sentence pattern and style. MicroBlog makes no exception. The MicroBlog texts are much similar to oral expression, with a casual style and less deliberation in writing, resulting in a simple and comfortable style: the MicroBlog style. Like oth"
W12-6307,W03-1719,0,\N,Missing
W12-6307,I08-4010,0,\N,Missing
W12-6307,W03-1726,0,\N,Missing
xia-etal-2006-constructing,brants-plaehn-2000-interactive,0,\N,Missing
xia-etal-2006-constructing,W03-1730,0,\N,Missing
xu-etal-2008-opinion,W03-1017,0,\N,Missing
xu-etal-2008-opinion,W03-0404,0,\N,Missing
xu-etal-2008-opinion,P03-1027,0,\N,Missing
xu-etal-2008-opinion,J04-3002,0,\N,Missing
xu-etal-2008-opinion,W02-1011,0,\N,Missing
xu-etal-2008-opinion,P97-1023,0,\N,Missing
Y06-1014,P97-1065,0,0.0367446,"Missing"
Y06-1014,W03-1704,0,0.0611892,"Missing"
Y06-1014,W01-0513,0,\N,Missing
Y06-1014,W03-1026,0,\N,Missing
Y06-1014,C96-1009,0,\N,Missing
zhang-etal-2008-exploiting,N07-1015,0,\N,Missing
zhang-etal-2008-exploiting,P05-1053,0,\N,Missing
zhang-etal-2008-exploiting,P06-2060,0,\N,Missing
zhang-etal-2008-exploiting,P04-1054,0,\N,Missing
zhang-etal-2008-exploiting,N06-1038,0,\N,Missing
zhang-etal-2008-exploiting,D07-1076,0,\N,Missing
zhang-etal-2008-exploiting,H05-1091,0,\N,Missing
zhang-etal-2008-exploiting,P06-1104,0,\N,Missing
