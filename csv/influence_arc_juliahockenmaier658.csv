2020.acl-main.232,D15-1138,0,0.024753,"he yellow blocks are pointing B picks up 2 red blocks, puts down a red block 2.4 Related Work There is growing interest in situated collaborative scenarios involving instruction givers/followers with one-way (Hu et al., 2019; Suhr et al., 2019) and two-way (Kim et al., 2019; Ilinykh et al., 2019) communication. Here, we compare our task to related work on instruction following, both generally and within Blocks World and Minecraft. Instruction following: Prior approaches to instruction comprehension typically take a semantic parsing approach (Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013; Andreas and Klein, 2015). Semantic parsing components enable human-robot understanding (Tellex et al., 2011; Matuszek et al., 2013); some approaches to interactive robot design combine these architectures with physical robot exploration to enable online learning (Thomason et al., 2015, 2016, 2017). The SCONE corpus (Long et al., 2016) features tasks in three domains requiring context-dependent sequential instruction understanding, in which a system is given a world containing several predefined objects and properties and has to predict the final world state by parsing instructions to intermediate logical forms. Some"
2020.acl-main.232,Q13-1005,0,0.0302578,"red block A: towards where the yellow blocks are pointing B picks up 2 red blocks, puts down a red block 2.4 Related Work There is growing interest in situated collaborative scenarios involving instruction givers/followers with one-way (Hu et al., 2019; Suhr et al., 2019) and two-way (Kim et al., 2019; Ilinykh et al., 2019) communication. Here, we compare our task to related work on instruction following, both generally and within Blocks World and Minecraft. Instruction following: Prior approaches to instruction comprehension typically take a semantic parsing approach (Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013; Andreas and Klein, 2015). Semantic parsing components enable human-robot understanding (Tellex et al., 2011; Matuszek et al., 2013); some approaches to interactive robot design combine these architectures with physical robot exploration to enable online learning (Thomason et al., 2015, 2016, 2017). The SCONE corpus (Long et al., 2016) features tasks in three domains requiring context-dependent sequential instruction understanding, in which a system is given a world containing several predefined objects and properties and has to predict the final world state by parsing instructions to interme"
2020.acl-main.232,N19-1423,0,0.012193,"ctures. To this end, we define the challenging subtask of Builder Action Prediction, tasking models with generating appropriate action sequences learned from the actions of human Builders. Our models process the game history along with a 3D representation of the evolving world to predict actions in a sequence-to-sequence fashion. We show that these models, especially when conditioned on a suitable amount of game history and trained on larger amounts of synthetically generated data, improve over naive baselines. In the future, richer representations of the dialogue history (e.g. by using BERT (Devlin et al., 2019) or of past Builder actions) combined with de-noising of the human data and perhaps more exhaustive data augmentation should produce better output sequences. For true interactivity, the Builder must be augmented with the capability to determine when and how to respond when it is too uncertain to act. And, finally, an approach like the SpeakerFollower Models of Fried et al. (2018) could be used to train our Builder model and the Architect model of Narayan-Chen et al. (2019) jointly. Acknowledgements We would like to thank the reviewers for their valuable comments. This work was supported by Con"
2020.acl-main.232,P19-1651,0,0.0210482,"ways correspond to a complete execution of the previous instruction, e.g. when B is interrupted by A or stops to ask a question: A: now it will be a diagonal staircase with 4 steps angling towards the middle A: if that makes sense B puts down a red block B: diagonal staircase with this orientation? B puts down a red block A: towards where the yellow blocks are pointing B picks up 2 red blocks, puts down a red block 2.4 Related Work There is growing interest in situated collaborative scenarios involving instruction givers/followers with one-way (Hu et al., 2019; Suhr et al., 2019) and two-way (Kim et al., 2019; Ilinykh et al., 2019) communication. Here, we compare our task to related work on instruction following, both generally and within Blocks World and Minecraft. Instruction following: Prior approaches to instruction comprehension typically take a semantic parsing approach (Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013; Andreas and Klein, 2015). Semantic parsing components enable human-robot understanding (Tellex et al., 2011; Matuszek et al., 2013); some approaches to interactive robot design combine these architectures with physical robot exploration to enable online learning (Thomason e"
2020.acl-main.232,P19-1537,1,0.884941,"im et al., 2016b,a; Budzianowski et al., 2018) which instead require grounding to entities in a knowledge base, or operate within static environments, such as images (Das et al., 2017) or videos (Pasunuru and Bansal, 2018). Relevant efforts in robotics have largely focused on single-shot instruction following, and are mostly constrained to simple language (Roy and Reiter, 2005; Tellex et al., 2011) with limited resources (Thomason et al., 2015; Misra et al., 2016; Chai et al., 2018). The recently introduced Minecraft Collaborative Building Task and the corresponding Minecraft Dialogue Corpus (Narayan-Chen et al., 2019) is one attempt to bridge this gap within the simulated game world of Minecraft. In this task, two players, an Architect (A) instructs a Builder (B) to construct a target structure out of multi-colored building blocks. The corpus consists of 509 game logs between humans that perform this task. NarayanChen et al. (2019) focus on generating Architect utterances. In this paper, we explore models for building an automated Builder agent.1 We focus on the subtask of predicting the Builder’s block placements, and leave the back-and-forth dialogue aspect of the overall task required of a fully interac"
2020.acl-main.232,D18-1012,0,0.0251971,"rad (1971)). The goal for agents in this scenario is to not only be able to engage in rich natural language discourse with their human conversation partners, but also to ground that discourse to physical objects, and execute instructions in the real world. Traditional dialogue scenarios are either completely ungrounded (Ritter et al., 2010; Schrading et al., 2015), focus on slot-value filling tasks (Kim et al., 2016b,a; Budzianowski et al., 2018) which instead require grounding to entities in a knowledge base, or operate within static environments, such as images (Das et al., 2017) or videos (Pasunuru and Bansal, 2018). Relevant efforts in robotics have largely focused on single-shot instruction following, and are mostly constrained to simple language (Roy and Reiter, 2005; Tellex et al., 2011) with limited resources (Thomason et al., 2015; Misra et al., 2016; Chai et al., 2018). The recently introduced Minecraft Collaborative Building Task and the corresponding Minecraft Dialogue Corpus (Narayan-Chen et al., 2019) is one attempt to bridge this gap within the simulated game world of Minecraft. In this task, two players, an Architect (A) instructs a Builder (B) to construct a target structure out of multi-co"
2020.acl-main.232,D14-1162,0,0.0840709,"e sequence of tokens. Similar to Narayan-Chen et al. (2019), we encode the dialogue history as a sequence of tokens in which each player’s utterances are contained within speaker-specific start and end tokens (hAi . . . hAi or (hBi . . . hBi.). We also represent B’s prior actions naively as tokens that capture the action type (placement or removal) and block color (e.g. as “builder putdown red”). The 2 × 6 = 12 action tokens as well as the speaker tokens are encoded using 300-dimensional random vectors, while all other tokens are encoded as 300-dimensional pre-trained GloVe word embeddings (Pennington et al., 2014). The token embeddings are passed through a GRU to produce a H-dim embedding (H ∈ {200, 300}) of the dialogue history in the GRU’s final hidden state. 3.3 World state encoder The world state is the current grid configuration that is fed into the action prediction model at each time step. We first describe how we represent the raw world state, before we explain how this representation is then encoded via a CNN-based architecture. Input: the raw world state Minecraft blocks are unit cubes that can be placed at integer-valued 2593 hx, y, zi locations in a 3D grid; the Collaborative Building Task"
2020.acl-main.232,N10-1020,0,0.0591656,"pective leads to a significant improvement in performance on this challenging language understanding problem. 1 Introduction There is a long-standing interest in building interactive agents that can communicate with humans about and operate within the physical world (e.g. Winograd (1971)). The goal for agents in this scenario is to not only be able to engage in rich natural language discourse with their human conversation partners, but also to ground that discourse to physical objects, and execute instructions in the real world. Traditional dialogue scenarios are either completely ungrounded (Ritter et al., 2010; Schrading et al., 2015), focus on slot-value filling tasks (Kim et al., 2016b,a; Budzianowski et al., 2018) which instead require grounding to entities in a knowledge base, or operate within static environments, such as images (Das et al., 2017) or videos (Pasunuru and Bansal, 2018). Relevant efforts in robotics have largely focused on single-shot instruction following, and are mostly constrained to simple language (Roy and Reiter, 2005; Tellex et al., 2011) with limited resources (Thomason et al., 2015; Misra et al., 2016; Chai et al., 2018). The recently introduced Minecraft Collaborative"
2020.acl-main.232,D15-1309,0,0.0114759,"gnificant improvement in performance on this challenging language understanding problem. 1 Introduction There is a long-standing interest in building interactive agents that can communicate with humans about and operate within the physical world (e.g. Winograd (1971)). The goal for agents in this scenario is to not only be able to engage in rich natural language discourse with their human conversation partners, but also to ground that discourse to physical objects, and execute instructions in the real world. Traditional dialogue scenarios are either completely ungrounded (Ritter et al., 2010; Schrading et al., 2015), focus on slot-value filling tasks (Kim et al., 2016b,a; Budzianowski et al., 2018) which instead require grounding to entities in a knowledge base, or operate within static environments, such as images (Das et al., 2017) or videos (Pasunuru and Bansal, 2018). Relevant efforts in robotics have largely focused on single-shot instruction following, and are mostly constrained to simple language (Roy and Reiter, 2005; Tellex et al., 2011) with limited resources (Thomason et al., 2015; Misra et al., 2016; Chai et al., 2018). The recently introduced Minecraft Collaborative Building Task and the cor"
2020.acl-main.232,P16-1138,0,0.0185877,"we compare our task to related work on instruction following, both generally and within Blocks World and Minecraft. Instruction following: Prior approaches to instruction comprehension typically take a semantic parsing approach (Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013; Andreas and Klein, 2015). Semantic parsing components enable human-robot understanding (Tellex et al., 2011; Matuszek et al., 2013); some approaches to interactive robot design combine these architectures with physical robot exploration to enable online learning (Thomason et al., 2015, 2016, 2017). The SCONE corpus (Long et al., 2016) features tasks in three domains requiring context-dependent sequential instruction understanding, in which a system is given a world containing several predefined objects and properties and has to predict the final world state by parsing instructions to intermediate logical forms. Some papers have also applied neural action prediction models (Suhr and Artzi, 2018; Huang et al., 2019) to SCONE. More recently, Vision-and-Language Navigation (VLN), (Anderson et al., 2018), and its dialog counterpart, Cooperative Vision-and-Dialog Navigation (CVDN) (Thomason et al., 2019), focus on instruction fo"
2020.acl-main.232,P18-1193,0,0.0713533,"ding (Tellex et al., 2011; Matuszek et al., 2013); some approaches to interactive robot design combine these architectures with physical robot exploration to enable online learning (Thomason et al., 2015, 2016, 2017). The SCONE corpus (Long et al., 2016) features tasks in three domains requiring context-dependent sequential instruction understanding, in which a system is given a world containing several predefined objects and properties and has to predict the final world state by parsing instructions to intermediate logical forms. Some papers have also applied neural action prediction models (Suhr and Artzi, 2018; Huang et al., 2019) to SCONE. More recently, Vision-and-Language Navigation (VLN), (Anderson et al., 2018), and its dialog counterpart, Cooperative Vision-and-Dialog Navigation (CVDN) (Thomason et al., 2019), focus on instruction following and cooperative interactions in photorealistic navigation settings. Since our dataset does not contain any logical forms, we also cannot use semantic parsing approaches, and have to resort to neural action prediction models. However, Minecraft instructions are more challenging than the SCONE tasks because our action space is significantly larger and our ut"
2020.acl-main.232,D19-1218,0,0.159022,"human action sequences do not always correspond to a complete execution of the previous instruction, e.g. when B is interrupted by A or stops to ask a question: A: now it will be a diagonal staircase with 4 steps angling towards the middle A: if that makes sense B puts down a red block B: diagonal staircase with this orientation? B puts down a red block A: towards where the yellow blocks are pointing B picks up 2 red blocks, puts down a red block 2.4 Related Work There is growing interest in situated collaborative scenarios involving instruction givers/followers with one-way (Hu et al., 2019; Suhr et al., 2019) and two-way (Kim et al., 2019; Ilinykh et al., 2019) communication. Here, we compare our task to related work on instruction following, both generally and within Blocks World and Minecraft. Instruction following: Prior approaches to instruction comprehension typically take a semantic parsing approach (Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013; Andreas and Klein, 2015). Semantic parsing components enable human-robot understanding (Tellex et al., 2011; Matuszek et al., 2013); some approaches to interactive robot design combine these architectures with physical robot exploration to enab"
2020.acl-main.232,P17-1086,0,0.0207328,"ruction instructions frequently refer to objects that need to be built by the agent. And although more recent navigation tasks require real vision, their underlying world state space (as defined by fixed viewpoints and the underlying navigation graph) is just as highly discretized. Our task does not require vision, but poses an arguably more challenging planning problem, since its action space is much larger (7623 possible actions vs. six actions in the vision-language navigation work). Blocks World: There is a renewed interest in instruction comprehension in Blocks World scenarios. Voxelurn (Wang et al., 2017) interfaces with human users and learns to understand descriptions of voxel structures of increasing complexity, but does so by mapping them down to a core programmatic language. Bisk et al. (2016a,b, 2018) build models for understanding single-shot instructions that transform one world state to another using simulated 3D blocks. Blocks are viewed from a fixed bird’s-eye perspective, initialized randomly in the initial world state, and uniquely identifiable. The varying Builder perspective and lack of easily identifiable referents, along with the need to understand utterances in a dialogue con"
2020.acl-main.758,Q17-1010,0,0.0095194,", 2017), and our MP-CAT model that combines both MP and CAT architectures. 3.1 CT: A Baseline Code and Text Model Our baseline model (CT) is based on Gu et al. (2018)’s CODEnn model. It maps both code and natural language descriptions to vectors in the same embedding space and then computes the similarity between these vectors using the L2 distance metric. These vectors are computed by two sets of three layers (one set per modality): The Word Embedding Module consists of two independently pre-trained lookup tables that map code tokens or natural language tokens to embeddings. We use FastText (Bojanowski et al., 2017)) for all embeddings in this paper. The Context Representation Module consists of bi-directional LSTM layers (one for code, one for text) that map the word embedding sequences into another pair of sequences of embeddings that contain contextual information. The Maxpool Layer performs max pool (separately per dimension) over the Context Representation embedding sequences to obtain a single vector. The Similarity Module computes the similarity of the two vectors vc and vc produced by the Maxpool Layers as d(v1 , v2 ) = d X (v1i − v2i )2 i=1 sim(vc , vd ) = 1 − d( vc vd , ) kvc k2 kvd k2 where d"
2020.acl-main.758,P18-1007,0,0.0154128,"s the other similarity modules. Figure 1 shows the pipeline of the MP-CAT framework. 4 Experiments The CoNaLa Dataset The CoNaLa dataset (Yin et al., 2018) has two parts, a manually curated parallel corpus of 2,379 training and 500 test examples, and a large automatically-mined dataset with 600k examples (which we ignore here). Each example consists of a snippet of Python code and its corresponding English description. Pre-processing We pre-process the text representing both the source code and the natural language descriptions using sub-word regularization based on unigram language modeling (Kudo, 2018) transforms the original tokens into sequences of shorter (and hence more common) substrings. We use the sentencepiece library (Kudo and Richardson, 2018) and follow the same approach as used by Yin et al. (2018) for the CoNaLa dataset. Training procedure During training, we use triplets consisting of a code snippet, a correct description, and an incorrect description (obtained by 8565 Figure 1: The MP-CAT framework that contains both global-level and local-level features for code–text matching Framework CT CAT MP MP-CAT Training Time (s) 4663.10 6702.69 183393.47 240062.38 Evaluation Time (s)"
2020.acl-main.758,D18-2012,0,0.0127508,"in et al., 2018) has two parts, a manually curated parallel corpus of 2,379 training and 500 test examples, and a large automatically-mined dataset with 600k examples (which we ignore here). Each example consists of a snippet of Python code and its corresponding English description. Pre-processing We pre-process the text representing both the source code and the natural language descriptions using sub-word regularization based on unigram language modeling (Kudo, 2018) transforms the original tokens into sequences of shorter (and hence more common) substrings. We use the sentencepiece library (Kudo and Richardson, 2018) and follow the same approach as used by Yin et al. (2018) for the CoNaLa dataset. Training procedure During training, we use triplets consisting of a code snippet, a correct description, and an incorrect description (obtained by 8565 Figure 1: The MP-CAT framework that contains both global-level and local-level features for code–text matching Framework CT CAT MP MP-CAT Training Time (s) 4663.10 6702.69 183393.47 240062.38 Evaluation Time (s) 6755.62 11050.68 17374.14 25306.97 Table 1: Training and Evaluation times for all our models. The models were trained for 100 epochs and the evaluation t"
2020.sigmorphon-1.15,K18-3001,0,0.0398268,"Missing"
2020.sigmorphon-1.15,K17-2001,0,0.139684,"Missing"
2021.findings-acl.356,P06-1060,0,0.0910227,"nodes and edge types. “[s]” or [SEP] is a virtual edge type, representing the end of each BFS level. Introduction Information Extraction (IE) can be viewed as a Text-to-Graph extraction task that aims to extract an information graph (Li et al., 2014; Shi et al., 2017) consisting of mentions and types from unstructured texts, where the nodes of the graph are mentions or entity types and the edges are relation types that indicate the relations between the nodes. A typical approach towards graph extraction is to break the extraction process into sub-tasks, such as Named Entity Recognition (NER) (Florian et al., 2006, 2010) and Relation Extraction (RE) (Sun et al., 2011; Jiang and Zhai, 2007), and either perform them separately (Chan and Roth, 2011) or jointly (Li and Ji, 2014; Eberts and Ulges, 2019). Recent joint IE models (Wadden et al., 2019; Wang and Lu, 2020; Lin et al., 2020) have shown 1 Our code is publicly available at https://github. com/renll/HySPA impressive performance on various IE tasks, since they can mitigate error propagation and leverage inter-dependencies between the tasks. Previous work often uses pairwise scoring techniques to identify relation types between entities. However, this"
2021.findings-acl.356,D10-1033,0,0.0154093,"Missing"
2021.findings-acl.356,C16-1239,0,0.0527608,"Missing"
2021.findings-acl.356,N07-1015,0,0.0256623,"e end of each BFS level. Introduction Information Extraction (IE) can be viewed as a Text-to-Graph extraction task that aims to extract an information graph (Li et al., 2014; Shi et al., 2017) consisting of mentions and types from unstructured texts, where the nodes of the graph are mentions or entity types and the edges are relation types that indicate the relations between the nodes. A typical approach towards graph extraction is to break the extraction process into sub-tasks, such as Named Entity Recognition (NER) (Florian et al., 2006, 2010) and Relation Extraction (RE) (Sun et al., 2011; Jiang and Zhai, 2007), and either perform them separately (Chan and Roth, 2011) or jointly (Li and Ji, 2014; Eberts and Ulges, 2019). Recent joint IE models (Wadden et al., 2019; Wang and Lu, 2020; Lin et al., 2020) have shown 1 Our code is publicly available at https://github. com/renll/HySPA impressive performance on various IE tasks, since they can mitigate error propagation and leverage inter-dependencies between the tasks. Previous work often uses pairwise scoring techniques to identify relation types between entities. However, this approach is computationally inefficient because it needs to enumerate all pos"
2021.findings-acl.356,P17-1085,0,0.0397803,"Missing"
2021.findings-acl.356,2020.acl-main.521,0,0.0282496,"s fail to capture interrelations between relation types for different pairs of mentions. Another approach is to treat the joint information extraction task as a table filling problem (Zhang et al., 2017; Wang and Lu, 2020), and generate twodimensional tables with a Multi-Dimensional Recurrent Neural Network (Graves et al., 2007). This can capture interrelations among entities and relations, but the space complexity grows quadratically with respect to the length of the input text, making this approach impractical for long sequences. Some attempts, such as Seq2RDF (Liu et al., 2018) and IMoJIE (Kolluru et al., 2020), leverage the power of Seq2seq models (Cho et al., 2014) 4066 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4066–4078 August 1–6, 2021. ©2021 Association for Computational Linguistics to capture the interrelations among mentions and types with first-order complexity, but they all use a pre-defined vocabulary for mention prediction, which largely depends on the distribution of the target words and will not be able to handle unseen out-of-vocabulary words. To solve these problems, we propose a first-order approach that invertibly maps the target graph to an a"
2021.findings-acl.356,P14-1038,1,0.811113,"-Graph extraction task that aims to extract an information graph (Li et al., 2014; Shi et al., 2017) consisting of mentions and types from unstructured texts, where the nodes of the graph are mentions or entity types and the edges are relation types that indicate the relations between the nodes. A typical approach towards graph extraction is to break the extraction process into sub-tasks, such as Named Entity Recognition (NER) (Florian et al., 2006, 2010) and Relation Extraction (RE) (Sun et al., 2011; Jiang and Zhai, 2007), and either perform them separately (Chan and Roth, 2011) or jointly (Li and Ji, 2014; Eberts and Ulges, 2019). Recent joint IE models (Wadden et al., 2019; Wang and Lu, 2020; Lin et al., 2020) have shown 1 Our code is publicly available at https://github. com/renll/HySPA impressive performance on various IE tasks, since they can mitigate error propagation and leverage inter-dependencies between the tasks. Previous work often uses pairwise scoring techniques to identify relation types between entities. However, this approach is computationally inefficient because it needs to enumerate all possible entity pairs in a document, and the relation type is a null value for most of th"
2021.findings-acl.356,D14-1198,1,0.88614,"ties. Extensive experiments on the ACE05 dataset show that our approach also significantly outperforms state-ofthe-art on the joint entity and relation extraction task.1 1 Figure 1: We represent directed multigraphs as alternating sequences of nodes (blue) and edges (orange). Here, the graph is traversed by Breadth First Search (BFS) with an ascending ordering of nodes and edge types. “[s]” or [SEP] is a virtual edge type, representing the end of each BFS level. Introduction Information Extraction (IE) can be viewed as a Text-to-Graph extraction task that aims to extract an information graph (Li et al., 2014; Shi et al., 2017) consisting of mentions and types from unstructured texts, where the nodes of the graph are mentions or entity types and the edges are relation types that indicate the relations between the nodes. A typical approach towards graph extraction is to break the extraction process into sub-tasks, such as Named Entity Recognition (NER) (Florian et al., 2006, 2010) and Relation Extraction (RE) (Sun et al., 2011; Jiang and Zhai, 2007), and either perform them separately (Chan and Roth, 2011) or jointly (Li and Ji, 2014; Eberts and Ulges, 2019). Recent joint IE models (Wadden et al.,"
2021.findings-acl.356,N19-1308,0,0.0184322,"elation between tasks. One line of approaches is to treat the joint task as a squared table filling problem (Miwa and Sasaki, 2014; Gupta et al., 2016; Wang and Lu, 2020), where the i-th column or row represents the i-th token. The table has diagonals indicating sequential tags for entities and other entries as relations between pairs of tokens. Another line of work is by performing RE after NER. In the work by Miwa and Bansal (2016), the authors used BiLSTM (Graves et al., 2013) for NER and consequently a Tree-LSTM (Tai et al., 2015) based on dependency graph for RE. Wadden et al. (2019) and Luan et al. (2019), on the other hand, takes the approach of constructing dynamic text span graphs to detect entities and relations. Extending on Wadden et al. (2019), Lin et al. (2020) introduced O NEIE, which further incorporates global features based on cross subtask and instance constraints, aiming to extract IE results as a graph. Note that our model differs from O NEIE (Lin et al., 2020) in that our model captures global relationships automatically through autoregressive generation while O NEIE uses feature engineered templates; Moreover, O NEIE needs to do pairwise classification for relation extraction,"
2021.findings-acl.356,P16-1105,0,0.0213187,"(e.g., military bases) can be both locations and organizations, or facilities. 5 Related Work NER is often done jointly with RE in order to mitigate error propagation and learn inter-relation between tasks. One line of approaches is to treat the joint task as a squared table filling problem (Miwa and Sasaki, 2014; Gupta et al., 2016; Wang and Lu, 2020), where the i-th column or row represents the i-th token. The table has diagonals indicating sequential tags for entities and other entries as relations between pairs of tokens. Another line of work is by performing RE after NER. In the work by Miwa and Bansal (2016), the authors used BiLSTM (Graves et al., 2013) for NER and consequently a Tree-LSTM (Tai et al., 2015) based on dependency graph for RE. Wadden et al. (2019) and Luan et al. (2019), on the other hand, takes the approach of constructing dynamic text span graphs to detect entities and relations. Extending on Wadden et al. (2019), Lin et al. (2020) introduced O NEIE, which further incorporates global features based on cross subtask and instance constraints, aiming to extract IE results as a graph. Note that our model differs from O NEIE (Lin et al., 2020) in that our model captures global relati"
2021.findings-acl.356,D14-1200,0,0.0294044,"l part-of United Nations). Such mistakes could be avoided by consulting a knowledge base such as DBpedia (Bizer et al., 2009) or by performing entity linking. Inherent ambiguity Many examples have inherent ambiguity, e.g. European Union can be typed as organization or political entity, while some entities (e.g., military bases) can be both locations and organizations, or facilities. 5 Related Work NER is often done jointly with RE in order to mitigate error propagation and learn inter-relation between tasks. One line of approaches is to treat the joint task as a squared table filling problem (Miwa and Sasaki, 2014; Gupta et al., 2016; Wang and Lu, 2020), where the i-th column or row represents the i-th token. The table has diagonals indicating sequential tags for entities and other entries as relations between pairs of tokens. Another line of work is by performing RE after NER. In the work by Miwa and Bansal (2016), the authors used BiLSTM (Graves et al., 2013) for NER and consequently a Tree-LSTM (Tai et al., 2015) based on dependency graph for RE. Wadden et al. (2019) and Luan et al. (2019), on the other hand, takes the approach of constructing dynamic text span graphs to detect entities and relation"
2021.findings-acl.356,N19-4009,0,0.0289626,"to our hybrid span decoder during training. We set the maximum span length, m = 16, the hidden size of our model, dm = 256, and the number of the decoder blocks, N = 12. Even though theoretically the beamsearch should help us reduce the exposure bias, we do not observe any performance gain during grid search of the beam size and the length penalty on the validation set (detailed grid search setting is in Appendix A). Thus we set a vanilla beam size of 1 and the length penalty of 1, and leave this theoryexperiment contradiction for future research. Our model is built with the FAIRSEQ toolkit (Ott et al., 2019) for efficient distributed training and all the experiments are conducted on two NVIDIA TITAN X GPUs. 4 https://catalog.ldc.upenn.edu/ LDC2006T06 4072 IE Models PointerNet (Katiyar and Cardie, 2017) SpanRE (Dixit and Al-Onaizan, 2019) Dygie++ (Wadden et al., 2019) OneIE (Lin et al., 2020) TabSeq (Wang and Lu, 2020) HySPA (ours) w/ RoBERTa w/ ALBERT Space Complexity O(n) O(n) O(n) O(n) O(n2 ) Time Complexity O(n2 ) O(n2 ) O(n2 ) O(n2 ) O(n) O(n) O(n) NER 82.6 86.0 88.6 88.8 89.5 88.9 89.9 RE 55.9 62.8 63.4 67.5 67.6 68.2 68.0 Table 1: Joint NER and RE F1 scores of the IE models on the ACE05 tes"
2021.findings-acl.356,D14-1162,0,0.0850407,"U |] U ˆ = [Q1 , ..., Q|Q |] Q where ⊕ means the concatenation operator beˆ U ˆ, Q ˆ are the lists of the tween two lists, and R, type names in the sets R, U, Q, respectively (e.g. ˆ = [“Geopolitics”, “Person”, ...]). Note that the Q concatenation order between the lists of type names can be arbitrary as long as it is kept consistent throughout the whole model. Then, as in the embedding part of the table-sequence encoder (Wang and Lu, 2020), for each type, vi , we embed the label tokens of the types with the contextualized word embedding from a pre-trained language model, the GloVe embedding (Pennington et al., 2014) and the character embedding, E1 = ContextualizedEmbed(v), ∈ R E2 = GloveEmbed(v), ∈ R vi ∈ v, we take the average of these token vectors as the representation of vi and freeze its update during training. More details of the embedding pipeline can be found in Appendix A. This embedding pipeline is also used to embed the words in the input text, x. Unlike the pipeline for the type embedding, we represent the word as the contextualized embedding of its first sub-token from the pre-trained Language Model (LM, e.g. BERT (Devlin et al., 2018)), and finetune the LM in an end-to-end fashion. After ob"
2021.findings-acl.356,P11-1053,0,0.0335572,"e, representing the end of each BFS level. Introduction Information Extraction (IE) can be viewed as a Text-to-Graph extraction task that aims to extract an information graph (Li et al., 2014; Shi et al., 2017) consisting of mentions and types from unstructured texts, where the nodes of the graph are mentions or entity types and the edges are relation types that indicate the relations between the nodes. A typical approach towards graph extraction is to break the extraction process into sub-tasks, such as Named Entity Recognition (NER) (Florian et al., 2006, 2010) and Relation Extraction (RE) (Sun et al., 2011; Jiang and Zhai, 2007), and either perform them separately (Chan and Roth, 2011) or jointly (Li and Ji, 2014; Eberts and Ulges, 2019). Recent joint IE models (Wadden et al., 2019; Wang and Lu, 2020; Lin et al., 2020) have shown 1 Our code is publicly available at https://github. com/renll/HySPA impressive performance on various IE tasks, since they can mitigate error propagation and leverage inter-dependencies between the tasks. Previous work often uses pairwise scoring techniques to identify relation types between entities. However, this approach is computationally inefficient because it nee"
2021.findings-acl.356,P15-1150,0,0.0265745,"done jointly with RE in order to mitigate error propagation and learn inter-relation between tasks. One line of approaches is to treat the joint task as a squared table filling problem (Miwa and Sasaki, 2014; Gupta et al., 2016; Wang and Lu, 2020), where the i-th column or row represents the i-th token. The table has diagonals indicating sequential tags for entities and other entries as relations between pairs of tokens. Another line of work is by performing RE after NER. In the work by Miwa and Bansal (2016), the authors used BiLSTM (Graves et al., 2013) for NER and consequently a Tree-LSTM (Tai et al., 2015) based on dependency graph for RE. Wadden et al. (2019) and Luan et al. (2019), on the other hand, takes the approach of constructing dynamic text span graphs to detect entities and relations. Extending on Wadden et al. (2019), Lin et al. (2020) introduced O NEIE, which further incorporates global features based on cross subtask and instance constraints, aiming to extract IE results as a graph. Note that our model differs from O NEIE (Lin et al., 2020) in that our model captures global relationships automatically through autoregressive generation while O NEIE uses feature engineered templates;"
2021.findings-acl.356,D19-1585,0,0.220401,"Li et al., 2014; Shi et al., 2017) consisting of mentions and types from unstructured texts, where the nodes of the graph are mentions or entity types and the edges are relation types that indicate the relations between the nodes. A typical approach towards graph extraction is to break the extraction process into sub-tasks, such as Named Entity Recognition (NER) (Florian et al., 2006, 2010) and Relation Extraction (RE) (Sun et al., 2011; Jiang and Zhai, 2007), and either perform them separately (Chan and Roth, 2011) or jointly (Li and Ji, 2014; Eberts and Ulges, 2019). Recent joint IE models (Wadden et al., 2019; Wang and Lu, 2020; Lin et al., 2020) have shown 1 Our code is publicly available at https://github. com/renll/HySPA impressive performance on various IE tasks, since they can mitigate error propagation and leverage inter-dependencies between the tasks. Previous work often uses pairwise scoring techniques to identify relation types between entities. However, this approach is computationally inefficient because it needs to enumerate all possible entity pairs in a document, and the relation type is a null value for most of the cases due to the sparsity of relations between entities. Also, pairw"
2021.findings-acl.356,2020.emnlp-main.133,0,0.0335547,"Missing"
2021.findings-acl.356,D16-1137,0,0.0468413,"Missing"
2021.findings-acl.356,P18-1047,0,0.0123911,"adden et al. (2019), Lin et al. (2020) introduced O NEIE, which further incorporates global features based on cross subtask and instance constraints, aiming to extract IE results as a graph. Note that our model differs from O NEIE (Lin et al., 2020) in that our model captures global relationships automatically through autoregressive generation while O NEIE uses feature engineered templates; Moreover, O NEIE needs to do pairwise classification for relation extraction, while our method efficiently generates existing relations and entities. While several Seq2Seq-based models (Zhang et al., 2020; Zeng et al., 2018, 2020; Wei et al., 2019; Zhang et al., 2019) have been proposed to generate triples (i.e., node-edge-node), our model is fundamentally different from them in that: (1) it is generating a BFS/DFS traversal of the target graph, which captures dependencies between nodes and edges and has a shorter target sequence, (2) we model the nodes as the spans in the text, which is independent of the vocabulary, so even if the tokens of the nodes are rare or unseen words, we can still generate spans on them based on the context information. 6 Conclusion In this work, we propose the Hybrid Span Generation ("
2021.findings-acl.356,D17-1182,0,0.0276987,"Missing"
2021.findings-acl.356,D19-1392,0,0.041141,"Missing"
C04-1180,briscoe-carroll-2002-robust,0,0.0181404,"Missing"
C04-1180,E99-1042,0,0.00750892,"Missing"
C04-1180,2003.mtsummit-papers.6,0,0.0315977,"Missing"
C04-1180,A00-2018,0,0.0299319,"Missing"
C04-1180,W03-1013,1,0.567339,"mma and returns a sentential modifier of the same type. Type-raising is applied to the categories NP, PP and S adj NP (adjectival phrase), and is implemented by adding the relevant set of type-raised categories to the chart whenever an NP, PP or S adj NP is present. The sets of type-raised categories are based on the most commonly used typeraising rule instantiations in sections 2-21 of CCGbank, and currently contain 8 type-raised categories for NP and 1 each for PP and S adj NP. For a given sentence, the automatically extracted grammar can produce a very large number of derivations. Clark and Curran (2003) and Clark and Curran (2004b) describe how a packed chart can be used to efficiently represent the derivation space, and also efficient algorithms for finding the most probable derivation. The parser uses a log-linear model over normal-form derivations.3 Features are defined in terms of the local trees in the derivation, including lexical head information and word-word dependencies. The normal-form derivations in CCGbank provide the gold standard training data. For a given sentence, the output of the parser is a set of syntactic dependencies corresponding to the 3 most probable derivation. How"
C04-1180,C04-1041,1,0.441407,"es are suppressed, since these are developed beWhile the proliferation of surface constituents allowed by CCG adds to derivational ambiguity (since the constituent taxpayers £15 million to install is also allowed in the non-coordinate sentence It could cost taxpayers £15 million to install), previous work has shown that standard techniques from the statistical parsing literature can be used for practical widecoverage parsing with state-of-the-art performance. 3 The Parser A number of statistical parsers have recently been developed for CCG (Clark et al., 2002; Hockenmaier and Steedman, 2002b; Clark and Curran, 2004b). All of these parsers use a grammar derived from CCGbank (Hockenmaier and Steedman, 2002a; Hockenmaier, 2003), a treebank of normalform CCG derivations derived semi-automatically from the Penn Treebank. In this paper we use the Clark and Curran (2004b) parser, which uses a loglinear model of normal-form derivations to select an analysis. The parser takes a POS tagged sentence as input with a set of lexical categories assigned to each word. A CCG supertagger (Clark and Curran, 2004a) is used to assign the categories. The supertagger uses a log-linear model of the target word’s context to dec"
C04-1180,P04-1014,1,0.669575,"es are suppressed, since these are developed beWhile the proliferation of surface constituents allowed by CCG adds to derivational ambiguity (since the constituent taxpayers £15 million to install is also allowed in the non-coordinate sentence It could cost taxpayers £15 million to install), previous work has shown that standard techniques from the statistical parsing literature can be used for practical widecoverage parsing with state-of-the-art performance. 3 The Parser A number of statistical parsers have recently been developed for CCG (Clark et al., 2002; Hockenmaier and Steedman, 2002b; Clark and Curran, 2004b). All of these parsers use a grammar derived from CCGbank (Hockenmaier and Steedman, 2002a; Hockenmaier, 2003), a treebank of normalform CCG derivations derived semi-automatically from the Penn Treebank. In this paper we use the Clark and Curran (2004b) parser, which uses a loglinear model of normal-form derivations to select an analysis. The parser takes a POS tagged sentence as input with a set of lexical categories assigned to each word. A CCG supertagger (Clark and Curran, 2004a) is used to assign the categories. The supertagger uses a log-linear model of the target word’s context to dec"
C04-1180,P02-1042,1,0.779112,"of the derivation and of the semantics of noun phrases are suppressed, since these are developed beWhile the proliferation of surface constituents allowed by CCG adds to derivational ambiguity (since the constituent taxpayers £15 million to install is also allowed in the non-coordinate sentence It could cost taxpayers £15 million to install), previous work has shown that standard techniques from the statistical parsing literature can be used for practical widecoverage parsing with state-of-the-art performance. 3 The Parser A number of statistical parsers have recently been developed for CCG (Clark et al., 2002; Hockenmaier and Steedman, 2002b; Clark and Curran, 2004b). All of these parsers use a grammar derived from CCGbank (Hockenmaier and Steedman, 2002a; Hockenmaier, 2003), a treebank of normalform CCG derivations derived semi-automatically from the Penn Treebank. In this paper we use the Clark and Curran (2004b) parser, which uses a loglinear model of normal-form derivations to select an analysis. The parser takes a POS tagged sentence as input with a set of lexical categories assigned to each word. A CCG supertagger (Clark and Curran, 2004a) is used to assign the categories. The supertagger us"
C04-1180,hockenmaier-steedman-2002-acquiring,1,0.878485,"nd of the semantics of noun phrases are suppressed, since these are developed beWhile the proliferation of surface constituents allowed by CCG adds to derivational ambiguity (since the constituent taxpayers £15 million to install is also allowed in the non-coordinate sentence It could cost taxpayers £15 million to install), previous work has shown that standard techniques from the statistical parsing literature can be used for practical widecoverage parsing with state-of-the-art performance. 3 The Parser A number of statistical parsers have recently been developed for CCG (Clark et al., 2002; Hockenmaier and Steedman, 2002b; Clark and Curran, 2004b). All of these parsers use a grammar derived from CCGbank (Hockenmaier and Steedman, 2002a; Hockenmaier, 2003), a treebank of normalform CCG derivations derived semi-automatically from the Penn Treebank. In this paper we use the Clark and Curran (2004b) parser, which uses a loglinear model of normal-form derivations to select an analysis. The parser takes a POS tagged sentence as input with a set of lexical categories assigned to each word. A CCG supertagger (Clark and Curran, 2004a) is used to assign the categories. The supertagger uses a log-linear model of the tar"
C04-1180,P02-1043,1,0.826238,"nd of the semantics of noun phrases are suppressed, since these are developed beWhile the proliferation of surface constituents allowed by CCG adds to derivational ambiguity (since the constituent taxpayers £15 million to install is also allowed in the non-coordinate sentence It could cost taxpayers £15 million to install), previous work has shown that standard techniques from the statistical parsing literature can be used for practical widecoverage parsing with state-of-the-art performance. 3 The Parser A number of statistical parsers have recently been developed for CCG (Clark et al., 2002; Hockenmaier and Steedman, 2002b; Clark and Curran, 2004b). All of these parsers use a grammar derived from CCGbank (Hockenmaier and Steedman, 2002a; Hockenmaier, 2003), a treebank of normalform CCG derivations derived semi-automatically from the Penn Treebank. In this paper we use the Clark and Curran (2004b) parser, which uses a loglinear model of normal-form derivations to select an analysis. The parser takes a POS tagged sentence as input with a set of lexical categories assigned to each word. A CCG supertagger (Clark and Curran, 2004a) is used to assign the categories. The supertagger uses a log-linear model of the tar"
C04-1180,N04-1013,0,0.0413167,"Missing"
C04-1180,C02-1105,0,0.0414764,"Missing"
C04-1180,J03-4003,0,\N,Missing
C10-1053,P09-1005,0,0.0117988,"y of Illinois at Urbana-Champaign {juliahmr, bisk1}@illinois.edu Abstract We propose and implement a modification of the Eisner (1996) normal form to account for generalized composition of bounded degree, and an extension to deal with grammatical type-raising. 1 Introduction Combinatory Categorial Grammar (Steedman, 2000) is a linguistically expressive grammar formalism that has been used for many NLP applications, including wide-coverage parsing (Clark and Curran, 2007; Hockenmaier, 2003) and semantic interpretation (Curran et al., 2007), semantic role-labeling (Gildea and Hockenmaier, 2003; Boxwell et al., 2009), semantic parsing (Zettlemoyer and Collins, 2005) and natural language generation (Espinosa et al., 2008). An essential feature of CCG is its flexible constituent structure, licensed by type-raising and composition rules which can create “nonstandard” constituents such as “John saw”, or “Mary talked to”, required in constructions involving non-local dependencies, such as whextraction (Fig. 1) or right-node raising. Since “John saw” can now also be a constituent in “John saw Mary”, this leads to a combinatorial explosion of spurious ambiguities, i.e. multiple syntactic derivations of the same"
C10-1053,2005.mtsummit-papers.11,0,0.00948755,"Y, head (both atomic and complex), and punctuation and conjunction categories. The comma can act as a conjunction or to set off modifiers (requiring punctuation rules 10 The restriction of categories to a fixed arity means that we could generate cross-serial dependencies N1 ...Nn V1 ...Vn only up to n = Acat . of the form X|X , ⇒ X|X and , X|X ⇒ X|X). We furthermore define coarse-grained parts of speech (noun, verb, function word, conj, other) and decide for each part of speech which lexical categories it can take. We compare different NF settings for sentences of lengths 15–30 from Europarl (Koehn, 2005). At each length, we compare 100 sentences that our grammar can parse. All NFs can parse all sentences the full grammar can parse. Results (Fig. 9(a)) show that our NF reduces the number of derivations significantly over Eisner’s NF, even though our (full) grammar only allows a restricted set of type-raising rules. Fig. 9(b) illustrates the combinatorial explosion of spurious derivations as the sentence length increases. 6 Conclusions We have proposed a modification and extension of Eisner (1996)’s normal form that is more appropriate for commonly used variants of CCG with grammatical type-rai"
C10-1053,P97-1069,0,0.0186053,"nly categories X ∈ Carg to be type-raised.4 Instantia3 In X|Y1..n or X|α=X|Y1...|α |, we do not assume the slash variable |∈ {/, } to be instantiated the same way for all Yi . We will therefore only distinguish between forward and backward generalized composition Bn&gt;1 . 4 We stipulate that it may be further necessary to only allow those argument categories to type-raise that are not used to project unbounded dependencies, such as S/NP in 466 tions of the variable T should also be restricted to categories of finite arity NT in oder to prevent an increase in generative capacity (Hoffman, 1995; Komagata, 1997). We refer to the arity of T as the degree of any particular instantation of T . We follow Steedman (2000) and assume NT = NB . Coordination requires a ternary rule (Φ) which can be binarized (Φ&gt;, Φ<) to simplify parsing:5 Punctuation and Type-changing rules CCGbank (Hockenmaier and Steedman, 2007) uses special punctuation rules such as S . ⇒ S or , NPNP ⇒ NPNP, and a small number of (non-recursive) type-changing rules (with idiosyncratic semantics) such as N ⇒ NP (for determiner-less NPs) or S[pss]NP ⇒ NPNP (for complex adjuncts, here passive VPs being used as NP postmodifiers): (&gt;P) X:φ"
C10-1053,J07-4004,0,0.16206,"Missing"
C10-1053,P07-2009,0,0.0473389,"alized composition and type-raising Julia Hockenmaier Yonatan Bisk University of Illinois at Urbana-Champaign {juliahmr, bisk1}@illinois.edu Abstract We propose and implement a modification of the Eisner (1996) normal form to account for generalized composition of bounded degree, and an extension to deal with grammatical type-raising. 1 Introduction Combinatory Categorial Grammar (Steedman, 2000) is a linguistically expressive grammar formalism that has been used for many NLP applications, including wide-coverage parsing (Clark and Curran, 2007; Hockenmaier, 2003) and semantic interpretation (Curran et al., 2007), semantic role-labeling (Gildea and Hockenmaier, 2003; Boxwell et al., 2009), semantic parsing (Zettlemoyer and Collins, 2005) and natural language generation (Espinosa et al., 2008). An essential feature of CCG is its flexible constituent structure, licensed by type-raising and composition rules which can create “nonstandard” constituents such as “John saw”, or “Mary talked to”, required in constructions involving non-local dependencies, such as whextraction (Fig. 1) or right-node raising. Since “John saw” can now also be a constituent in “John saw Mary”, this leads to a combinatorial explos"
C10-1053,P94-1018,0,0.620275,"n now also be a constituent in “John saw Mary”, this leads to a combinatorial explosion of spurious ambiguities, i.e. multiple syntactic derivations of the same semantic interpretation (Wittenburg, 1986). This can create problems for applications based on CCG, e.g. for the induction of stochastic CCGs from text annotated with logical forms (Zettlemoyer and Collins, 2007), where spreading probability mass over equivalent derivations should be avoided. A number of normal-form (NF) parsing algorithms that aim to produce only one derivation per interpretation have been proposed (Wittenburg, 1986; Niv, 1994; Pareschi and Steedman, 1987; Hepple and Morrill, 1989; Eisner, 1996). Computationally, such algorithms are very attractive since they do not require costly semantic equivalence checks (Karttunen, 1989; Komagata, 2004) during parsing. Eisner’s (1996) normal form is the most developed and well-known of these approaches, but is only defined for a variant of CCG where type-raising is a lexical operation and where the degree of composition is unbounded. Therefore, it and its equivalent reformulation by Hoyt and Baldridge (2008) in a multimodal variant of CCG are not safe (preserve all interpretat"
C10-1053,P96-1011,0,0.851386,"mbinatorial explosion of spurious ambiguities, i.e. multiple syntactic derivations of the same semantic interpretation (Wittenburg, 1986). This can create problems for applications based on CCG, e.g. for the induction of stochastic CCGs from text annotated with logical forms (Zettlemoyer and Collins, 2007), where spreading probability mass over equivalent derivations should be avoided. A number of normal-form (NF) parsing algorithms that aim to produce only one derivation per interpretation have been proposed (Wittenburg, 1986; Niv, 1994; Pareschi and Steedman, 1987; Hepple and Morrill, 1989; Eisner, 1996). Computationally, such algorithms are very attractive since they do not require costly semantic equivalence checks (Karttunen, 1989; Komagata, 2004) during parsing. Eisner’s (1996) normal form is the most developed and well-known of these approaches, but is only defined for a variant of CCG where type-raising is a lexical operation and where the degree of composition is unbounded. Therefore, it and its equivalent reformulation by Hoyt and Baldridge (2008) in a multimodal variant of CCG are not safe (preserve all interpretations) and complete (remove all spurious ambiguities) for more commonly"
C10-1053,P08-1022,0,0.0220368,"fication of the Eisner (1996) normal form to account for generalized composition of bounded degree, and an extension to deal with grammatical type-raising. 1 Introduction Combinatory Categorial Grammar (Steedman, 2000) is a linguistically expressive grammar formalism that has been used for many NLP applications, including wide-coverage parsing (Clark and Curran, 2007; Hockenmaier, 2003) and semantic interpretation (Curran et al., 2007), semantic role-labeling (Gildea and Hockenmaier, 2003; Boxwell et al., 2009), semantic parsing (Zettlemoyer and Collins, 2005) and natural language generation (Espinosa et al., 2008). An essential feature of CCG is its flexible constituent structure, licensed by type-raising and composition rules which can create “nonstandard” constituents such as “John saw”, or “Mary talked to”, required in constructions involving non-local dependencies, such as whextraction (Fig. 1) or right-node raising. Since “John saw” can now also be a constituent in “John saw Mary”, this leads to a combinatorial explosion of spurious ambiguities, i.e. multiple syntactic derivations of the same semantic interpretation (Wittenburg, 1986). This can create problems for applications based on CCG, e.g. f"
C10-1053,W03-1008,1,0.786924,"enmaier Yonatan Bisk University of Illinois at Urbana-Champaign {juliahmr, bisk1}@illinois.edu Abstract We propose and implement a modification of the Eisner (1996) normal form to account for generalized composition of bounded degree, and an extension to deal with grammatical type-raising. 1 Introduction Combinatory Categorial Grammar (Steedman, 2000) is a linguistically expressive grammar formalism that has been used for many NLP applications, including wide-coverage parsing (Clark and Curran, 2007; Hockenmaier, 2003) and semantic interpretation (Curran et al., 2007), semantic role-labeling (Gildea and Hockenmaier, 2003; Boxwell et al., 2009), semantic parsing (Zettlemoyer and Collins, 2005) and natural language generation (Espinosa et al., 2008). An essential feature of CCG is its flexible constituent structure, licensed by type-raising and composition rules which can create “nonstandard” constituents such as “John saw”, or “Mary talked to”, required in constructions involving non-local dependencies, such as whextraction (Fig. 1) or right-node raising. Since “John saw” can now also be a constituent in “John saw Mary”, this leads to a combinatorial explosion of spurious ambiguities, i.e. multiple syntactic d"
C10-1053,P87-1012,0,0.792071,"be a constituent in “John saw Mary”, this leads to a combinatorial explosion of spurious ambiguities, i.e. multiple syntactic derivations of the same semantic interpretation (Wittenburg, 1986). This can create problems for applications based on CCG, e.g. for the induction of stochastic CCGs from text annotated with logical forms (Zettlemoyer and Collins, 2007), where spreading probability mass over equivalent derivations should be avoided. A number of normal-form (NF) parsing algorithms that aim to produce only one derivation per interpretation have been proposed (Wittenburg, 1986; Niv, 1994; Pareschi and Steedman, 1987; Hepple and Morrill, 1989; Eisner, 1996). Computationally, such algorithms are very attractive since they do not require costly semantic equivalence checks (Karttunen, 1989; Komagata, 2004) during parsing. Eisner’s (1996) normal form is the most developed and well-known of these approaches, but is only defined for a variant of CCG where type-raising is a lexical operation and where the degree of composition is unbounded. Therefore, it and its equivalent reformulation by Hoyt and Baldridge (2008) in a multimodal variant of CCG are not safe (preserve all interpretations) and complete (remove al"
C10-1053,J93-4002,0,0.095257,"tation of an input string consisting of a sequence of words and their lexical categories. Since the presence of both pre- and postmodifiers (as in “intentionally knock twice”7 ) introduces a genuine ambiguity, Eisner proves that the only kind of spurious ambiguity that can arise in his variant of CCG is due to associative chains of composition such as A/B B/C C/D or A/B B/C CD, which can be derived as either 6 Since composition allows the arity of derived (≈ nonterminal) CCG categories to grow with the length of the input string, worst-case complexity of this naive algorithm is exponential. (Vijay-Shanker and Weir, 1993)’s O(n6 ) algorithm has a more compact representation of categories. 7 This can mean λx.intentionally  (twice  (knock  (x))) or λx.twice  (intentionally  (knock  (x))). 467 Eisner NF (A|B1..b )/C (C|D1..d )/E (E|F1..f )/G G|H1..h &gt;Bh Not Eisner NF (A|B1..b )/C (C|D1..d )/E (E|F1..f )/G G|H1..h &gt;Bd+1 (E|F1..f )|H1..h ((A|B1..b )|D1..d )/E &gt;Bf +h &gt;Bf +1 ((C|D1..d )|F1..f )|H1..h (((A|B1..b )|D1..d )|F1..f )|G &gt;Bd+f +h (((A|B1..b )|D1..d )|F1..f )|H1..h (((A|B1..b )|D1..d )|F1..f )|H1..h &gt;Bh Figure 3: Eisner NF and generalized composition Bn&gt;1 Left branching Right branching A/B (B|D0..m )/C"
C10-1053,E89-1002,0,0.295515,"Mary”, this leads to a combinatorial explosion of spurious ambiguities, i.e. multiple syntactic derivations of the same semantic interpretation (Wittenburg, 1986). This can create problems for applications based on CCG, e.g. for the induction of stochastic CCGs from text annotated with logical forms (Zettlemoyer and Collins, 2007), where spreading probability mass over equivalent derivations should be avoided. A number of normal-form (NF) parsing algorithms that aim to produce only one derivation per interpretation have been proposed (Wittenburg, 1986; Niv, 1994; Pareschi and Steedman, 1987; Hepple and Morrill, 1989; Eisner, 1996). Computationally, such algorithms are very attractive since they do not require costly semantic equivalence checks (Karttunen, 1989; Komagata, 2004) during parsing. Eisner’s (1996) normal form is the most developed and well-known of these approaches, but is only defined for a variant of CCG where type-raising is a lexical operation and where the degree of composition is unbounded. Therefore, it and its equivalent reformulation by Hoyt and Baldridge (2008) in a multimodal variant of CCG are not safe (preserve all interpretations) and complete (remove all spurious ambiguities) fo"
C10-1053,J07-3004,1,0.874311,"ate that it may be further necessary to only allow those argument categories to type-raise that are not used to project unbounded dependencies, such as S/NP in 466 tions of the variable T should also be restricted to categories of finite arity NT in oder to prevent an increase in generative capacity (Hoffman, 1995; Komagata, 1997). We refer to the arity of T as the degree of any particular instantation of T . We follow Steedman (2000) and assume NT = NB . Coordination requires a ternary rule (Φ) which can be binarized (Φ&gt;, Φ<) to simplify parsing:5 Punctuation and Type-changing rules CCGbank (Hockenmaier and Steedman, 2007) uses special punctuation rules such as S . ⇒ S or , NPNP ⇒ NPNP, and a small number of (non-recursive) type-changing rules (with idiosyncratic semantics) such as N ⇒ NP (for determiner-less NPs) or S[pss]NP ⇒ NPNP (for complex adjuncts, here passive VPs being used as NP postmodifiers): (&gt;P) X:φ [., ; ] ⇒ X:φ (<P) [., ; ] X:φ ⇒ X:φ TypeChanging (TCR) X:φ ⇒ Y:ψ(φ) (Φ) X conj X ⇒X (Φ&gt;) X X[conj] ⇒ X (Φ<) conj X ⇒ X[conj] Uses of type-raising and composition In English, type-raising and composition are required for wh-extraction and right node raising of arguments as well as so-called argumen"
C10-1053,P08-1038,0,0.0154196,"produce only one derivation per interpretation have been proposed (Wittenburg, 1986; Niv, 1994; Pareschi and Steedman, 1987; Hepple and Morrill, 1989; Eisner, 1996). Computationally, such algorithms are very attractive since they do not require costly semantic equivalence checks (Karttunen, 1989; Komagata, 2004) during parsing. Eisner’s (1996) normal form is the most developed and well-known of these approaches, but is only defined for a variant of CCG where type-raising is a lexical operation and where the degree of composition is unbounded. Therefore, it and its equivalent reformulation by Hoyt and Baldridge (2008) in a multimodal variant of CCG are not safe (preserve all interpretations) and complete (remove all spurious ambiguities) for more commonly used variants of CCG. In particular, this NF is not safe when the degree of composition is bounded,1 and not complete when type-raising is a grammatical operation. This paper defines a NF for CCG with bounded composition and grammatical type-raising. 2 Combinatory Categorial Grammar In CCG, every constituent (“John saw”) has a syntactic category (S/NP) and a semantic interpretation (λx.saw(john , x)).2 Constituents combine according to a small set of lan"
C10-1053,D07-1071,0,0.053711,"d by type-raising and composition rules which can create “nonstandard” constituents such as “John saw”, or “Mary talked to”, required in constructions involving non-local dependencies, such as whextraction (Fig. 1) or right-node raising. Since “John saw” can now also be a constituent in “John saw Mary”, this leads to a combinatorial explosion of spurious ambiguities, i.e. multiple syntactic derivations of the same semantic interpretation (Wittenburg, 1986). This can create problems for applications based on CCG, e.g. for the induction of stochastic CCGs from text annotated with logical forms (Zettlemoyer and Collins, 2007), where spreading probability mass over equivalent derivations should be avoided. A number of normal-form (NF) parsing algorithms that aim to produce only one derivation per interpretation have been proposed (Wittenburg, 1986; Niv, 1994; Pareschi and Steedman, 1987; Hepple and Morrill, 1989; Eisner, 1996). Computationally, such algorithms are very attractive since they do not require costly semantic equivalence checks (Karttunen, 1989; Komagata, 2004) during parsing. Eisner’s (1996) normal form is the most developed and well-known of these approaches, but is only defined for a variant of CCG w"
C10-2133,A00-1011,0,0.0331826,"Missing"
C10-2133,W02-1011,0,0.0109152,"ork on extraction is that we extract sentences as units, which is shallower but presumably more robust. Heinze et al. (2002) state that the current state-of-theart in NLP is suitable for mining information of moderate content depth across a diverse collection of medical settings and specialties. Zhou et al. (2006), the authors perform information extraction from clinical medical records using a decision tree based classifier using resources such as WordNet 1 , UMLS 2 etc. They extract past medical history and social behaviour from the records. In other related works, sentiment classification (Pang et al., 2002; Prabowo and Thelwall, 2009; Cui et al., 2006; Dave et al., 2003) attempts to categorize text based on polarity of sentiments and is often applied at the sentence level (Kim and Zhai, 2009). Some work has also been done on extracting content from forum data. This includes finding question answer pairs (Cong et al., 2008) from online forums, auto-answering queries on a technical forum (Feng et al., 2006), ranking answers (Harabagiu and Hickl, 2006) etc. To the best of our knowledge, this is the first work on shallow extraction from medical forum data. a patient (i.e., roughly symptoms) (2) Med"
C10-2133,P06-1114,0,0.0124286,"s such as WordNet 1 , UMLS 2 etc. They extract past medical history and social behaviour from the records. In other related works, sentiment classification (Pang et al., 2002; Prabowo and Thelwall, 2009; Cui et al., 2006; Dave et al., 2003) attempts to categorize text based on polarity of sentiments and is often applied at the sentence level (Kim and Zhai, 2009). Some work has also been done on extracting content from forum data. This includes finding question answer pairs (Cong et al., 2008) from online forums, auto-answering queries on a technical forum (Feng et al., 2006), ranking answers (Harabagiu and Hickl, 2006) etc. To the best of our knowledge, this is the first work on shallow extraction from medical forum data. a patient (i.e., roughly symptoms) (2) Medications (MED), which includes sentences mentioning medications (i.e., roughly treatment). These sentences provide a basic description of a medical case and can already be very useful if we can extract them. We chose to analyze at the sentence level because a sentence provides enough context to detect the category accurately. For example, detecting the categories at word level will not help us to mark a sentence like “I get very uncomfortable after"
C10-2145,bird-etal-2008-acl,0,0.0487662,"aper proposed a novel author topic model, CAT, which extends the existing author topic model with additional cited author information. We applied it to the domain of expert retrieval and demonstrated the effectiveness of our model in improving coherence in topic clustering and author topic association. The proposed model also provides an effective solution to the problem of community mining as shown by the promising retrieval results derived in our expert search system. One immediate improvement would result from extending our corpus. For example, we can apply our model to the ACL ARC corpus (Bird et al., 2008) to check the model’s robustness and enhance the ranking by learning from more data. We can also apply our model to data sets with rich linkage structure, such as the TREC benchmark data set or ACL Anthology Network (Radev et al., 2009) and try to enhance our model with the appropriate network analysis. Acknowledgments Table 6: Recall comparison between our proposed model and the model without cited author information. Since we do not have a gold standard experts pool for our queries, to evaluate recall, we collected a pool of authors returned from an academic search engine, ArnetMiner (Tang e"
C10-2145,W10-1202,1,0.444365,"rch at the TREC enterprise track from 2005 to 2007, which focus on enterprise scale search and discovering relationships between entities. In that setting, the task is to find the experts, given a web domain, a list of candidate experts and a set of topics 1 . The task defined in our paper is different in the sense that our topics are hidden and our document repositories are more homogeneous since our documents are all research papers authored by the experts. Within this setting, we can explore in depth the influence of the hidden topics and contents to the ranking of our experts. Similar to (Johri et al., 2010), in this paper we apply CAT in a semantic retrieval scenario, where searching people is associated with a set of hidden semantically meaningful topics instead of their personal names. In recent literature, there are three main lines of work that extend author topic analyses. One line of work is to relax the model’s “bag-of-words” assumption by automatically discovering multiword phrases and adding them into the original model (Johri et al., 2010). Similar work has also been proposed for other topic models such as Ngram topic models (Wallach, 2006; Wang and McCallum, 2005; Wang et al., 2007; G"
C10-2145,W09-1119,1,0.356308,"icitly lists each paper together with its title and author information. Therefore, the author information of each paper can be obtained accurately without extracting it from the original paper. However, many author names are not represented consistently. For example, the same author may have his/her middle name listed in some papers, but not in others. We therefore normalized all author names by eliminating middle names from all authors. Cited authors of each paper are extracted from the reference section and automatically identified by a named entity recognizer tuned for citation extraction (Ratinov and Roth, 2009). Similar to regular authors, all cited authors are also normalized 3 http://www.arnetminer.org Conf. ACL EMNLP CONLL Total Year 03-09 93-09 97-09 93-09 Paper 1,326 912 495 2,733 Author 2,084 1,453 833 2,911 uni. 34,012 40,785 27,312 62,958 Vocab. 205,260 219,496 123,176 366,565 Table 1: Statistics about our data set. Uni. denotes unigram words and Vocab. denotes all unigrams and multiword phrases discovered in the data set. with their first name initial and their full last name. We extracted about 20,000 cited authors from our corpus. However, for the sake of efficiency, we only keep those ci"
D16-1214,D13-1160,0,0.126124,"realization is potentially a correct derivation. For example, the Freebase logical form for the example sentence in Figure 1 is shown below, and none of its derivations are penalized if they could result in this logical form. λe. business.acquisition(e) ∧ acquiring company(e, G OOGLE) ∧ company acquired(e, N EST) (1) ∧ date(e, 2014) Since grammar induction systems are traditionally trained on declarative sentences, we would ideally require declarative sentences paired with Freebase logical forms. But such datasets do not exist in the Freebase semantic parsing literature (Cai and Yates, 2013; Berant et al., 2013). To alleviate this prob1 Please see Bisk and Hockenmaier (2013) for more details. 2023 lem, and yet perform Freebase semantic parsing, we propose an entity slot-filling task. Entity Slot-Filling Task. Given a declarative sentence containing mentions of Freebase entities, we randomly remove one of the mentions to create a blank slot. The task is to fill this slot by translating the declarative sentence into a Freebase query. Consider the following sentence where the entity Nest has been removed: Google acquired which was founded in Palo Alto To correctly fill in the blank, one has to query Fre"
D16-1214,P15-1135,1,0.941398,"nois.edu, steedman@inf.ed.ac.uk, Abstract We compare the effectiveness of four different syntactic CCG parsers for a semantic slotfilling task to explore how much syntactic supervision is required for downstream semantic analysis. This extrinsic, task-based evaluation also provides a unique window into the semantics captured (or missed) by unsupervised grammar induction systems. 1 Introduction The past several years have seen significant progress in unsupervised grammar induction (Carroll and Charniak, 1992; Yuret, 1998; Klein and Manning, 2004; Spitkovsky et al., 2010; Garrette et al., 2015; Bisk and Hockenmaier, 2015). But how useful are unsupervised syntactic parsers for downstream NLP tasks? What phenomena are they able to capture, and where would additional annotation be required? Instead of standard intrinsic evaluations – attachment scores that depend strongly on the particular annotation styles of the gold treebank – we examine the utility of unsupervised and weakly supervised parsers for semantics. We perform an extrinsic evaluation of unsupervised and weakly supervised CCG parsers on a grounded semantic parsing task that will shed light on the extent to which these systems recover semantic informat"
D16-1214,C04-1180,1,0.682913,"gold answer is the same as the predicted answer. 4 Sentences to Freebase Logical Forms CCG provides a clean interface between syntax and semantics, i.e. each argument of a words syntactic category corresponds to an argument of the lambda expression that defines its semantic interpretation (e.g., the lambda expression corresponding to the category (SNP)/NP of the verb acquired is λf.λg.λe.∃x.∃y.acquired(e) ∧ f (x) ∧ g(y) ∧ arg1 (e, y) ∧ arg2 (e, x)), and the logical form for the complete sentence can be constructed by composing word level lambda expressions following the syntactic derivation (Bos et al., 2004). In Figure 2 we show two syntactic derivations for the same sentence, and the corresponding logical forms and equivalent graph representations derived by G RAPH PARSER (Reddy et al., 2014). The graph representations are possible because G RAPH PARSER assumes access to coindexations of input CCG categories. We provide acquired NP (SNP)/NP hblanki NP which was founded in PA (NPNP)/(SNP) SNP founded. in.arg2 > NPNP < NP > SNP x target < S e2 fo u in n d .a e d rg . 1 Google acquired. arg2 e1 Palo Alto acquired. arg1 Google λe1 .∃xe2 . TARGET(x) ∧ acquired(e1 ) ∧ arg1 (e1 , Google) ∧ arg2 ("
D16-1214,S13-1045,0,0.0184229,"at could lead to this realization is potentially a correct derivation. For example, the Freebase logical form for the example sentence in Figure 1 is shown below, and none of its derivations are penalized if they could result in this logical form. λe. business.acquisition(e) ∧ acquiring company(e, G OOGLE) ∧ company acquired(e, N EST) (1) ∧ date(e, 2014) Since grammar induction systems are traditionally trained on declarative sentences, we would ideally require declarative sentences paired with Freebase logical forms. But such datasets do not exist in the Freebase semantic parsing literature (Cai and Yates, 2013; Berant et al., 2013). To alleviate this prob1 Please see Bisk and Hockenmaier (2013) for more details. 2023 lem, and yet perform Freebase semantic parsing, we propose an entity slot-filling task. Entity Slot-Filling Task. Given a declarative sentence containing mentions of Freebase entities, we randomly remove one of the mentions to create a blank slot. The task is to fill this slot by translating the declarative sentence into a Freebase query. Consider the following sentence where the entity Nest has been removed: Google acquired which was founded in Palo Alto To correctly fill in the blank"
D16-1214,P04-1061,0,0.063815,"ign ybisk@isi.edu, siva.reddy@ed.ac.uk, blitzer@google.com, juliahmr@illinois.edu, steedman@inf.ed.ac.uk, Abstract We compare the effectiveness of four different syntactic CCG parsers for a semantic slotfilling task to explore how much syntactic supervision is required for downstream semantic analysis. This extrinsic, task-based evaluation also provides a unique window into the semantics captured (or missed) by unsupervised grammar induction systems. 1 Introduction The past several years have seen significant progress in unsupervised grammar induction (Carroll and Charniak, 1992; Yuret, 1998; Klein and Manning, 2004; Spitkovsky et al., 2010; Garrette et al., 2015; Bisk and Hockenmaier, 2015). But how useful are unsupervised syntactic parsers for downstream NLP tasks? What phenomena are they able to capture, and where would additional annotation be required? Instead of standard intrinsic evaluations – attachment scores that depend strongly on the particular annotation styles of the gold treebank – we examine the utility of unsupervised and weakly supervised parsers for semantics. We perform an extrinsic evaluation of unsupervised and weakly supervised CCG parsers on a grounded semantic parsing task that w"
D16-1214,D14-1107,1,0.890059,"Missing"
D16-1214,N10-1116,0,0.0189303,"eddy@ed.ac.uk, blitzer@google.com, juliahmr@illinois.edu, steedman@inf.ed.ac.uk, Abstract We compare the effectiveness of four different syntactic CCG parsers for a semantic slotfilling task to explore how much syntactic supervision is required for downstream semantic analysis. This extrinsic, task-based evaluation also provides a unique window into the semantics captured (or missed) by unsupervised grammar induction systems. 1 Introduction The past several years have seen significant progress in unsupervised grammar induction (Carroll and Charniak, 1992; Yuret, 1998; Klein and Manning, 2004; Spitkovsky et al., 2010; Garrette et al., 2015; Bisk and Hockenmaier, 2015). But how useful are unsupervised syntactic parsers for downstream NLP tasks? What phenomena are they able to capture, and where would additional annotation be required? Instead of standard intrinsic evaluations – attachment scores that depend strongly on the particular annotation styles of the gold treebank – we examine the utility of unsupervised and weakly supervised parsers for semantics. We perform an extrinsic evaluation of unsupervised and weakly supervised CCG parsers on a grounded semantic parsing task that will shed light on the ext"
D16-1214,P12-1063,0,0.0558732,"Missing"
D19-1515,P82-1020,0,0.723842,"Missing"
D19-1515,N16-1030,0,0.0260421,"ained to pick only the best candidate region while rejecting all the inferior-than-best candidate regions, which is intuitively not a good behavior. Yu et al. (2018b) proposes a soft-label target distribution which gives weighted credit to all good candidate regions (i.e. those with above-threshold IoU with the gold region), and uses Kullback-Leibler (KL) divergence loss as training objective. Conditional Random Fields. CRFs (Lafferty et al., 2001) are discriminative probabilistic models that have been found useful in sequence labeling tasks by capturing label dependencies (Ma and Hovy, 2016; Lample et al., 2016). We summarize some works relevant to CRFs learned in softlabel or multi-label settings. Multi-CRFs (Dredze et al., 2009) learn CRFs with noisy annotated data, where annotators may disagree on the label for input tokens. The assumption is that there is always only one gold label for each token, so the model favors single label while conforming to the prior distribution of labels set by annotators. To work with soft-label targets, it employs a mode-seeking, exclusive KL divergence definition, which does not imply moment-matching, a desired property of CRFs (and in general, exponential family mo"
D19-1515,P16-1101,0,0.023089,"me, the model is trained to pick only the best candidate region while rejecting all the inferior-than-best candidate regions, which is intuitively not a good behavior. Yu et al. (2018b) proposes a soft-label target distribution which gives weighted credit to all good candidate regions (i.e. those with above-threshold IoU with the gold region), and uses Kullback-Leibler (KL) divergence loss as training objective. Conditional Random Fields. CRFs (Lafferty et al., 2001) are discriminative probabilistic models that have been found useful in sequence labeling tasks by capturing label dependencies (Ma and Hovy, 2016; Lample et al., 2016). We summarize some works relevant to CRFs learned in softlabel or multi-label settings. Multi-CRFs (Dredze et al., 2009) learn CRFs with noisy annotated data, where annotators may disagree on the label for input tokens. The assumption is that there is always only one gold label for each token, so the model favors single label while conforming to the prior distribution of labels set by annotators. To work with soft-label targets, it employs a mode-seeking, exclusive KL divergence definition, which does not imply moment-matching, a desired property of CRFs (and in general,"
D19-1515,D14-1162,0,0.0906426,"Missing"
D19-1515,N18-1202,0,0.0215956,"ases to ground is not part of our task. Following Plummer et al. (2017b), we merge all regions that are ground to the same phrase into one larger bounding box, and split the dataset into 29, 783 training images, 1k validation images and 1k test images. We do not apply our method to RefCOCO (Yu et al., 2016) or Visual Genome (Krishna et al., 2017) because they consist of independently grounded entity phrases without any entity dependencies that CRFs could leverage. Implementation details. For text feature extraction, we use the 1024-d contextualized word embeddings from the last layer of ELMo (Peters et al., 2018), followed by a bi-directional LSTM (Hochreiter and Schmidhuber, 1997) encoder with hidden dimension dhidden = 512 for each direction, so that the text feature vector has dimension dtext = 1024. We use the Bottom-Up Attention model (Anderson et al., 2018) to generate region proposals and extract visual features, as in the state-of-the-art BAN (Kim et al., 2018) and DDPN (Yu et al., 2018b) models. K = 100 region proposals are generated for each image. Each candidate region with coordinates (xmin , ymin ), (xmax , ymax ) is represented by a dvis = 2053 feature vector that consists of 2048-d visu"
E03-1008,H91-1060,0,0.0320313,"Missing"
E03-1008,W99-0613,0,0.0609689,"Missing"
E03-1008,W01-0521,0,0.161063,"Missing"
E03-1008,J93-2004,0,0.0278308,"Missing"
E03-1008,W01-0501,0,0.185139,"Missing"
E03-1008,N01-1023,1,0.616825,"Missing"
E03-1008,P95-1026,0,0.323358,"Missing"
E03-1008,J03-4003,0,\N,Missing
E03-1008,P02-1046,0,\N,Missing
E17-1068,marelli-etal-2014-sick,0,0.0608943,"information is known or unknown. However, they only evaluate on lexical semantic tasks such as hyponymy detection. Other approaches explore the idea that it may be more appropriate to represent a word as a region in space instead of a single point. Erk (2009) presents a word vector representation in which the hyponyms of a word are mapped to vectors that exist within the boundaries of that word vector’s region. Vilnis and McCallum (2015) use Gaussian functions to map a word to a density over a latent space. Both papers evaluate their models only on lexical relationships. edge (SICK) dataset (Marelli et al., 2014) and the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015), both of which involve a 3-way classification for textual entailment. SICK was created for SemEval 2014 based on image caption data and video descriptions. The premises and hypotheses are automatically generated from the original captions and so contain some unintentional systematic patterns. Most approaches to SICK involve hand-engineered features (Lai and Hockenmaier, 2014) or large collections of entailment rules (Beltagy et al., 2015). SNLI is the largest textual entailment dataset by several orders of magnitu"
E17-1068,J12-1003,0,0.0278892,"e captions. In order to compute visual denotations from the corpus, they define a set of normalization and reduction rules (e.g. lemmatization, dropping modifiers, replacing nouns with their hypernyms, dropping PPs, extracting NPs) that augment the original F LICKR 30K captions with a large number of shorter, more generic phrases that are each associated with a subset of the F LICKR 30K images. The result is a large subsumption hierarchy over phrases, which Young et al. call a denotation graph (see Figure 1). The structure of the denotation graph is similar to the idea of an entailment graph (Berant et al., 2012). Each node in the denotation graph corresponds to a phrase s, associated with its denotation JsK, i.e. the set of images that correspond to the original captions from which this phrase could be derived. For example, the denotation of a phrase “woman jog on beach” is the set of images in the corpus that depict a woman jogging on a beach. Note that the denoVector space representations Several related works have explored different approaches to learning vector space representations that express entailment more directly. Kruszewski et al. (2015) learn a mapping from an existing distributional vec"
E17-1068,D15-1075,0,0.373444,"tasks such as hyponymy detection. Other approaches explore the idea that it may be more appropriate to represent a word as a region in space instead of a single point. Erk (2009) presents a word vector representation in which the hyponyms of a word are mapped to vectors that exist within the boundaries of that word vector’s region. Vilnis and McCallum (2015) use Gaussian functions to map a word to a density over a latent space. Both papers evaluate their models only on lexical relationships. edge (SICK) dataset (Marelli et al., 2014) and the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015), both of which involve a 3-way classification for textual entailment. SICK was created for SemEval 2014 based on image caption data and video descriptions. The premises and hypotheses are automatically generated from the original captions and so contain some unintentional systematic patterns. Most approaches to SICK involve hand-engineered features (Lai and Hockenmaier, 2014) or large collections of entailment rules (Beltagy et al., 2015). SNLI is the largest textual entailment dataset by several orders of magnitude. It was created with the goal of training neural network models for textual e"
E17-1068,E17-1038,0,0.0293769,"ent dataset by several orders of magnitude. It was created with the goal of training neural network models for textual entailment. The premises in SNLI are captions from the F LICKR 30K corpus (Young et al., 2014). The hypotheses (entailed, contradictory, or neutral in relation to the premise) were solicited from workers on Mechanical Turk. Bowman et al. (2015) initially illustrated the effectiveness of LSTMs (Hochreiter and Schmidhuber, 1997) on SNLI, and recent approaches have focused on improvements in neural network architectures. These include sentence embedding models (Liu et al., 2016; Munkhdalai and Yu, 2017a), neural attention models (Rockt¨aschel et al., 2016; Parikh et al., 2016), and neural tree-based models (Munkhdalai and Yu, 2017b; Chen et al., 2016). In contrast, in this paper we focus on using a different input representation, and demonstrate its effectiveness when added to a standard neural network model for textual entailment. We demonstrate that the results of the LSTM model of Bowman et al. (2015) can be improved by adding a single feature based on our predicted denotational probabilities. We expect to see similar improvements when our predicted probabilities are added to more comple"
E17-1068,E17-1002,0,0.0113562,"ent dataset by several orders of magnitude. It was created with the goal of training neural network models for textual entailment. The premises in SNLI are captions from the F LICKR 30K corpus (Young et al., 2014). The hypotheses (entailed, contradictory, or neutral in relation to the premise) were solicited from workers on Mechanical Turk. Bowman et al. (2015) initially illustrated the effectiveness of LSTMs (Hochreiter and Schmidhuber, 1997) on SNLI, and recent approaches have focused on improvements in neural network architectures. These include sentence embedding models (Liu et al., 2016; Munkhdalai and Yu, 2017a), neural attention models (Rockt¨aschel et al., 2016; Parikh et al., 2016), and neural tree-based models (Munkhdalai and Yu, 2017b; Chen et al., 2016). In contrast, in this paper we focus on using a different input representation, and demonstrate its effectiveness when added to a standard neural network model for textual entailment. We demonstrate that the results of the LSTM model of Bowman et al. (2015) can be improved by adding a single feature based on our predicted denotational probabilities. We expect to see similar improvements when our predicted probabilities are added to more comple"
E17-1068,D16-1244,0,0.066901,"Missing"
E17-1068,D14-1162,0,0.0911795,"0. In fact, in our embedding space, the joint probability represented by the vector z will always be greater than or equal to the product of the probabilities represented by the vectors x and y. For any pair x = (x1 , ..., xN ) and y = (y1 , ..., yN ), PJK (X, Y ) ≥ PJK (X)PJK (Y ): PJK (X, Y ) = exp − 512D FF LSTM RNN X xi − i 6 Our model for PJK (x) and PJK (x, y) We train a neural network model to predict PJK (x), PJK (y), and PJK (x|y) for phrases x and y. This model consists of an LSTM that outputs a 512d vector which is passed through an additional 512d layer. We use 300d GloVe vectors (Pennington et al., 2014) trained on 840B tokens as the word embedding input to the LSTM. We use the same model to represent both x and y regardless of which phrase is the premise or the hypothesis. Thus, we pass the sequence of word embeddings for phrase x through the model to get x, and we do the same for phrase y to get y. As previously described, we sum the elements of x and y to get the predicted denotational probabilities PJK (x) and PJK (y). From x and y, we find the joint vector z, which we use to compute the predicted denotational conditional probability PJK (x|y) according to the equation in Section 5. Figur"
E17-1068,W09-1109,0,0.457932,"ssociation for Computational Linguistics strict SICK to a binary task and their sentence vectors result from simple composition functions (e.g. addition) over their word representations. Henderson and Popa (2016) learn a mapping from an existing distributional vector representation to an entailment-based vector representation that expresses whether information is known or unknown. However, they only evaluate on lexical semantic tasks such as hyponymy detection. Other approaches explore the idea that it may be more appropriate to represent a word as a region in space instead of a single point. Erk (2009) presents a word vector representation in which the hyponyms of a word are mapped to vectors that exist within the boundaries of that word vector’s region. Vilnis and McCallum (2015) use Gaussian functions to map a word to a density over a latent space. Both papers evaluate their models only on lexical relationships. edge (SICK) dataset (Marelli et al., 2014) and the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015), both of which involve a 3-way classification for textual entailment. SICK was created for SemEval 2014 based on image caption data and video descriptions. Th"
E17-1068,P16-1193,0,0.0188794,"and instead focus on the purely semantic problem of how to represent the meaning of sentences. This version of the textual entailment task has been popularized by two datasets, the Sentences Involving Compositional Knowl721 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 721–730, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics strict SICK to a binary task and their sentence vectors result from simple composition functions (e.g. addition) over their word representations. Henderson and Popa (2016) learn a mapping from an existing distributional vector representation to an entailment-based vector representation that expresses whether information is known or unknown. However, they only evaluate on lexical semantic tasks such as hyponymy detection. Other approaches explore the idea that it may be more appropriate to represent a word as a region in space instead of a single point. Erk (2009) presents a word vector representation in which the hyponyms of a word are mapped to vectors that exist within the boundaries of that word vector’s region. Vilnis and McCallum (2015) use Gaussian functi"
E17-1068,Q15-1027,0,0.0258943,"on graph is similar to the idea of an entailment graph (Berant et al., 2012). Each node in the denotation graph corresponds to a phrase s, associated with its denotation JsK, i.e. the set of images that correspond to the original captions from which this phrase could be derived. For example, the denotation of a phrase “woman jog on beach” is the set of images in the corpus that depict a woman jogging on a beach. Note that the denoVector space representations Several related works have explored different approaches to learning vector space representations that express entailment more directly. Kruszewski et al. (2015) learn a mapping from an existing distributional vector representation to a structured Boolean vector representation that expresses entailment as feature inclusion. They evaluate the resulting representation on lexical entailment tasks and on sentence entailment in SICK, but they re722 x2 x2 P(X,Y) P(X) P(X,Y) P(X) child play y x z P(Y) x y child play guitar girl play on playground girl play child play on beach girl play on beach o child play soccer x1 o x1 Figure 2: An embedding space that expresses the individual probability of events X and Y and the joint probability P (X, Y ). child in red"
E17-1068,S14-2055,1,0.950284,"n functions to map a word to a density over a latent space. Both papers evaluate their models only on lexical relationships. edge (SICK) dataset (Marelli et al., 2014) and the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015), both of which involve a 3-way classification for textual entailment. SICK was created for SemEval 2014 based on image caption data and video descriptions. The premises and hypotheses are automatically generated from the original captions and so contain some unintentional systematic patterns. Most approaches to SICK involve hand-engineered features (Lai and Hockenmaier, 2014) or large collections of entailment rules (Beltagy et al., 2015). SNLI is the largest textual entailment dataset by several orders of magnitude. It was created with the goal of training neural network models for textual entailment. The premises in SNLI are captions from the F LICKR 30K corpus (Young et al., 2014). The hypotheses (entailed, contradictory, or neutral in relation to the premise) were solicited from workers on Mechanical Turk. Bowman et al. (2015) initially illustrated the effectiveness of LSTMs (Hochreiter and Schmidhuber, 1997) on SNLI, and recent approaches have focused on impr"
hockenmaier-steedman-2002-acquiring,J91-3003,0,\N,Missing
hockenmaier-steedman-2002-acquiring,E89-1002,0,\N,Missing
hockenmaier-steedman-2002-acquiring,J03-4003,0,\N,Missing
hockenmaier-steedman-2002-acquiring,P96-1011,0,\N,Missing
hockenmaier-steedman-2002-acquiring,P90-1024,0,\N,Missing
I17-1011,D14-1162,0,0.0839258,"h premise). Each premise vector pi is concatenated with its hypothesis vector hi and passed through a feedforward layer to produce logit prediction li . We sum l1 ... l4 to obtain the final prediction, which we use to compute the cross-entropy loss. When training on SNLI, we apply the conditional LSTM only once to read the premise and hypothesis and produce p1 and h1 . We pass the concatenation of p1 and h1 through the feedforward layer to produce l1 , which we use to compute the cross-entropy loss. 7 Experimental Results Training Details For the LSTM and SE models, we use 300d GloVe vectors (Pennington et al., 2014) trained on 840B tokens as the input. The attention model uses word2vec vectors (Mikolov et al., 2013) (replacing with GloVe had almost no effect on performance). We use the Adam optimizer (Kingma and Ba, 2014) with the default configuration. We train each model for 10 epochs based on convergence on dev. For joint SNLI+MPE training, we use the same parameters and pretrain for 10 epochs on SNLI, then train for 10 epochs on MPE. This was 8.2 Performance by Pair Agreement To get a better understanding of how our task differs from standard entailment, we analyze how 2 Dropout: 0.8, learning rate:"
I17-1011,S12-1051,0,0.0376075,"h International Joint Conference on Natural Language Processing, pages 100–109, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP related/unrelated sentence as the hypothesis, but we restrict the hypothesis to have low word overlap with the premises, and we collect human judgments to label the items as entailing, contradictory, or neutral. that describe concrete, everyday activities. The SICK dataset (Marelli et al., 2014) consists of 10K sentence pairs. The premise sentences come from the F LICKR 8K image caption corpus (Rashtchian et al., 2010) and the MSR Video Paraphrase Corpus (Agirre et al., 2012), while the hypotheses were automatically generated. This process introduced some errors (e.g. “A motorcycle is riding standing up on the seat of the vehicle”) and an uneven distribution of phenomena across entailment classes that is easy to exploit (e.g. negation (Lai and Hockenmaier, 2014)). The SNLI dataset (Bowman et al., 2015) contains over 570K sentence pairs. The premises come from the F LICKR 30K image caption corpus (Young et al., 2014) and VisualGenome (Krishna et al., 2016). The hypotheses were written by Mechanical Turk workers who were given the premise and asked to write one defi"
I17-1011,W10-0721,1,0.75831,"from image or video caption data 100 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 100–109, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP related/unrelated sentence as the hypothesis, but we restrict the hypothesis to have low word overlap with the premises, and we collect human judgments to label the items as entailing, contradictory, or neutral. that describe concrete, everyday activities. The SICK dataset (Marelli et al., 2014) consists of 10K sentence pairs. The premise sentences come from the F LICKR 8K image caption corpus (Rashtchian et al., 2010) and the MSR Video Paraphrase Corpus (Agirre et al., 2012), while the hypotheses were automatically generated. This process introduced some errors (e.g. “A motorcycle is riding standing up on the seat of the vehicle”) and an uneven distribution of phenomena across entailment classes that is easy to exploit (e.g. negation (Lai and Hockenmaier, 2014)). The SNLI dataset (Bowman et al., 2015) contains over 570K sentence pairs. The premises come from the F LICKR 30K image caption corpus (Young et al., 2014) and VisualGenome (Krishna et al., 2016). The hypotheses were written by Mechanical Turk work"
I17-1011,D15-1075,0,0.670606,"tly written sentences, all describing the same scene (see examples in Figure 1). The task is to decide whether the hypothesis sentence 1) can be used to describe the same scene (entailment), 2) cannot be used to describe the same scene (contradiction), or 3) may or may not describe the same scene (neutral). The main challenge is to infer what happened in the scene from the multiple premise statements, in some cases aggregating information across multiple sentences into a coherent whole. Figure 1: The Multiple Premise Entailment Task Similar to the SICK and SNLI datasets (Marelli et al., 2014; Bowman et al., 2015), each premise sentence in our data is a single sentence describing everyday events, rather than news paragraphs as in the RTE datasets (Dagan et al., 2006), which require named entity recognition and coreference resolution. Instead of soliciting humans to write new hypotheses, as SNLI did, we use simplified versions of existing image captions, and use a word overlap filter and the structure of the denotation graph of Young et al. (2014) to minimize the presence of trivial lexical relationships. 2 Related Standard Entailment Tasks In the following datasets, premises are single sentences drawn"
I17-1011,S14-2055,1,0.83102,"ments to label the items as entailing, contradictory, or neutral. that describe concrete, everyday activities. The SICK dataset (Marelli et al., 2014) consists of 10K sentence pairs. The premise sentences come from the F LICKR 8K image caption corpus (Rashtchian et al., 2010) and the MSR Video Paraphrase Corpus (Agirre et al., 2012), while the hypotheses were automatically generated. This process introduced some errors (e.g. “A motorcycle is riding standing up on the seat of the vehicle”) and an uneven distribution of phenomena across entailment classes that is easy to exploit (e.g. negation (Lai and Hockenmaier, 2014)). The SNLI dataset (Bowman et al., 2015) contains over 570K sentence pairs. The premises come from the F LICKR 30K image caption corpus (Young et al., 2014) and VisualGenome (Krishna et al., 2016). The hypotheses were written by Mechanical Turk workers who were given the premise and asked to write one definitely true sentence, one possibly true sentence, and one definitely false sentence. The task design prompted workers to write hypotheses that frequently parallel the premise in structure and vocabulary, and therefore the semantic relationships between premise and hypothesis are often limite"
I17-1011,marelli-etal-2014-sick,0,0.0939548,"of multiple independently written sentences, all describing the same scene (see examples in Figure 1). The task is to decide whether the hypothesis sentence 1) can be used to describe the same scene (entailment), 2) cannot be used to describe the same scene (contradiction), or 3) may or may not describe the same scene (neutral). The main challenge is to infer what happened in the scene from the multiple premise statements, in some cases aggregating information across multiple sentences into a coherent whole. Figure 1: The Multiple Premise Entailment Task Similar to the SICK and SNLI datasets (Marelli et al., 2014; Bowman et al., 2015), each premise sentence in our data is a single sentence describing everyday events, rather than news paragraphs as in the RTE datasets (Dagan et al., 2006), which require named entity recognition and coreference resolution. Instead of soliciting humans to write new hypotheses, as SNLI did, we use simplified versions of existing image captions, and use a word overlap filter and the structure of the denotation graph of Young et al. (2014) to minimize the presence of trivial lexical relationships. 2 Related Standard Entailment Tasks In the following datasets, premises are s"
I17-1011,Q14-1006,1,\N,Missing
J07-3004,P90-1024,0,0.228488,"ence, modifiers of used would also receive different categories depending on what occurrence of used they modify. This is undesirable, because we are only guaranteed to acquire a complete lexicon if we have seen all participles (and their possible modifiers) in all their possible surface positions. Similar regularities have been recognized and given a categorial analysis by Carpenter (1992), who advocates lexical rules to account for the use of predicatives as adjuncts. In a statistical model, the parameters for such lexical rules are difficult to estimate. We therefore follow the approach of Aone and Wittenburg (1990) and implement these type-changing 367 Computational Linguistics Volume 33, Number 3 Figure 3 Type-changing rules reduce the number of lexical category types required for complex adjuncts. operations in the derivational syntax, where these generalizations are captured in a few rules. If these rules apply recursively to their own output, they can generate an infinite set of category types, leading to a shift in generative power from context-free to recursively enumerable (Carpenter 1991, 1992). Like Aone and Wittenburg, we therefore consider only a finite number of instantiations of these type-"
J07-3004,A00-2031,0,0.0116897,"inguistic variation exhibits a Zipfian distribution, where a very small proportion of the available alternatives accounts for most of the data. This creates a temptation to concentrate on capturing the few high-frequency cases at the top end of the distribution, and to ignore the “long tail” of rare events such as non-local dependencies. Despite the fact that these occur in a large number of sentences, they affect only a small number of words, and have thus a small impact on overall dependency recovery. Although there is now a sizable literature on trace and function-tag insertion algorithms (Blaheta and Charniak 2000; Johnson 2002; Campbell 2004), and integrated parsing with function tags or null elements (Dienes and Dubey 2003a, 2003b; Merlo and Musillo 2005; Gabbard, Kulick, and Marcus 2006), such approaches typically require additional pre- or postprocessing steps that are likely to add further noise and errors to the parser output. A completely integrated approach that is based on a syntactic representation which allows direct recovery of the underlying predicate–argument structure might therefore be preferable. Such representations are provided by grammar formalisms that are more expressive than simp"
J07-3004,C04-1180,1,0.711358,"rresponding predicate–argument structure or logical form can therefore be directly obtained from any derivation if the semantic interpretation of each lexical entry is known. In this article and in CCGbank, we approximate such semantic interpretations with dependency graphs that include most semantically relevant non-anaphoric local and long-range dependencies. Although certain decisions taken by the builders of the original Penn Treebank mean that the syntactic derivations that can be obtained from the Penn Treebank are not always semantically correct (as we will discuss), subsequent work by Bos et al. (2004) and Bos (2005) has demonstrated that the output of parsers trained on CCGbank can also be directly translated into logical forms such as Discourse Representation Theory structures (Kamp and Reyle 1993), which can then be used as input to a theorem prover in applications like question answering and textual entailment recognition. Translating the Treebank into this more demanding formalism has revealed certain sources of noise and inconsistency in the original annotation that have had to be corrected in order to permit induction of a linguistically correct grammar. Because of this preprocessing"
J07-3004,H05-1079,0,0.0174005,"ier and Steedman CCGbank ing those that arise under relativization and coordination. Although these dependencies are only an approximation of the full semantic interpretation that can in principle be obtained from a CCG, they may prove useful for tasks such as summarization and question answering (Clark, Steedman, and Curran 2004). Furthermore, Bos et al. (2004) and Bos (2005) have demonstrated that the output of CCGbank parsers can be successfully translated into Kamp and Reyle’s (1993) Discourse Representation Theory structures, to support question answering and the textual entailment task (Bos and Markert 2005). We hope that these results can be ported to other corpora and other similarly expressive grammar formalisms. We also hope that our experiences will be useful in designing guidelines for future treebanks. Although implementational details will differ across formalisms, similar problems and questions to those that arose in our work will be encountered in any attempt to extract expressive grammars from annotated corpora. Because CCGbank preserves most of the linguistic information in the Treebank in a somewhat less noisy form, we hope that others will find it directly helpful for inducing gramm"
J07-3004,P98-1025,0,0.0261249,"eveloped as a “near-contextfree” theory of natural language grammar, with a very free definition of derivational structure adapted to the analysis of coordination and unbounded dependency without movement or deletion transformations. It has been successfully applied to the analysis of coordination, relative clauses and related constructions, intonation structure, binding and control, and quantifier scope alternation, in a number of languages—see Steedman and Baldridge (2006) for a recent review. Extensions of CCG to other languages and word-orders are discussed by Hoffman (1995), Kang (1995), Bozsahin (1998), Komagata (1999), Steedman (2000), Trechsel (2000), Baldridge (2002), and C ¸ akıcı (2005). The derivations in CCGbank follow the analyses of Steedman (1996, 2000), except where noted. 2.1 Lexical Categories Categorial Grammars are strongly lexicalized, in the sense that the grammar is entirely defined by a lexicon in which words (and other lexical items) are associated with one or more specific categories which completely define their syntactic behavior. The set of categories consists of basic categories (e.g., S, NP, PP) and complex categories of the form X/Y or XY, representing functors w"
J07-3004,P04-1041,0,0.00757058,"Missing"
J07-3004,P05-2013,0,0.18086,"on of derivational structure adapted to the analysis of coordination and unbounded dependency without movement or deletion transformations. It has been successfully applied to the analysis of coordination, relative clauses and related constructions, intonation structure, binding and control, and quantifier scope alternation, in a number of languages—see Steedman and Baldridge (2006) for a recent review. Extensions of CCG to other languages and word-orders are discussed by Hoffman (1995), Kang (1995), Bozsahin (1998), Komagata (1999), Steedman (2000), Trechsel (2000), Baldridge (2002), and C ¸ akıcı (2005). The derivations in CCGbank follow the analyses of Steedman (1996, 2000), except where noted. 2.1 Lexical Categories Categorial Grammars are strongly lexicalized, in the sense that the grammar is entirely defined by a lexicon in which words (and other lexical items) are associated with one or more specific categories which completely define their syntactic behavior. The set of categories consists of basic categories (e.g., S, NP, PP) and complex categories of the form X/Y or XY, representing functors with (basic or complex) argument category Y and result category X. Functor categories of the"
J07-3004,P04-1082,0,0.00611989,"stribution, where a very small proportion of the available alternatives accounts for most of the data. This creates a temptation to concentrate on capturing the few high-frequency cases at the top end of the distribution, and to ignore the “long tail” of rare events such as non-local dependencies. Despite the fact that these occur in a large number of sentences, they affect only a small number of words, and have thus a small impact on overall dependency recovery. Although there is now a sizable literature on trace and function-tag insertion algorithms (Blaheta and Charniak 2000; Johnson 2002; Campbell 2004), and integrated parsing with function tags or null elements (Dienes and Dubey 2003a, 2003b; Merlo and Musillo 2005; Gabbard, Kulick, and Marcus 2006), such approaches typically require additional pre- or postprocessing steps that are likely to add further noise and errors to the parser output. A completely integrated approach that is based on a syntactic representation which allows direct recovery of the underlying predicate–argument structure might therefore be preferable. Such representations are provided by grammar formalisms that are more expressive than simple phrase-structure grammar, l"
J07-3004,J91-3003,0,0.0255629,"parameters for such lexical rules are difficult to estimate. We therefore follow the approach of Aone and Wittenburg (1990) and implement these type-changing 367 Computational Linguistics Volume 33, Number 3 Figure 3 Type-changing rules reduce the number of lexical category types required for complex adjuncts. operations in the derivational syntax, where these generalizations are captured in a few rules. If these rules apply recursively to their own output, they can generate an infinite set of category types, leading to a shift in generative power from context-free to recursively enumerable (Carpenter 1991, 1992). Like Aone and Wittenburg, we therefore consider only a finite number of instantiations of these type-changing rules, namely those which arise when we extend the category assignment procedure in the following way: For any sentential or verb phrase modifier (an adjunct with label S or SBAR with null complementizer, or VP) to which the original algorithm assigns category X|X, apply the following type-changing rule (given in bottom-up notation) in reverse: S$ ⇒ X|X (16) where S$ is the category that this constituent obtains if it is treated like a head node by the basic algorithm. S$ has"
J07-3004,A00-2018,0,0.155441,"that has become the de facto standard training and test data for statistical parsers. Its annotation, which is based on generic phrasestructure grammar (with coindexed traces and other null elements indicating non-local dependencies) and function tags on nonterminal categories providing (a limited degree of) syntactic role information, is designed to facilitate the extraction of the underlying predicate–argument structure. Statistical parsing on the Penn Treebank has made great progress by focusing on the machine-learning or algorithmic aspects (Magerman 1994; Ratnaparkhi 1998; Collins 1999; Charniak 2000; Henderson 2004; McDonald, Crammer, and Pereira 2005). However, this has often resulted in parsing models and evaluation measures that are both based on reduced representations which simplify or ignore the linguistic information represented by function tags and null elements in the original Treebank. (One exception is Collins 1999, whose Model 2 includes a distinction between arguments and adjuncts, and whose Model 3 additionally captures wh-movement in relative clauses with a GPSG-like “slash-feature-passing” mechanism.) The reasons for this shift away from linguistic adequacy are easy to tr"
J07-3004,P00-1058,0,0.00798912,"; XTAG-group 1999; Copestake and Flickinger 2000; OpenCCG1 [White and Baldridge 2003; White 2006]). Because treebank annotation for individual formalisms is prohibitively expensive, there have been a number of efforts to extract TAGs, LFGs, and, more recently, HPSGs, from the Penn Treebank (Xia 1999; Chen and Vijay-Shanker 2000; Xia, Palmer, and Joshi 2000; Xia 2001; Cahill et al. 2002; Miyao, Ninomiya, and Tsujii 2004; O’Donovan et al. 2005; Shen and Joshi 2005; Chen, Bangalore, and Vijay-Shanker 2006). Statistical parsers that are trained on these TAG and HPSG corpora have been presented by Chiang (2000) and Miyao and Tsujii (2005), whereas the LFG parsing system of Cahill et al. (2004) uses 1 Open CCG, the successor of Grok (Hockenmaier et al. 2004), is available from http://openccg. sourceforge.net. 356 Hockenmaier and Steedman CCGbank a postprocessing step on the output of a Treebank parser to recover predicate–argument dependencies. In this article we present an algorithmic method for obtaining a corpus of CCG derivations and dependency structures from the Penn Treebank, together with some observations that we believe carry wider implications for similar attempts with other grammar formal"
J07-3004,P04-1014,0,0.0192347,"tprocessing step on the output of a Treebank parser to recover predicate–argument dependencies. In this article we present an algorithmic method for obtaining a corpus of CCG derivations and dependency structures from the Penn Treebank, together with some observations that we believe carry wider implications for similar attempts with other grammar formalisms and corpora. Earlier versions of the resulting corpus, CCGbank, have already been used to build a number of wide-coverage statistical parsers (Clark, Hockenmaier, and Steedman 2002; Hockenmaier and Steedman 2002; Hockenmaier 2003b, 2003a; Clark and Curran 2004, 2007), which recover both local and long-range dependencies directly and in a single pass. CCG is a linguistically expressive, but efficiently parseable, lexicalized grammar formalism that was specifically designed to provide a base-generative account of coordinate and relativized constructions like the following: a. pay HealthVest $5 million right away and additional amounts in the future b. the parched Franco years, the everyday poverty and stagnant atmosphere of which he described in brutally direct, vivid prose c. Who is, and who should be, making the criminal law here? (1) CCG directly"
J07-3004,P07-1032,0,0.103266,"Missing"
J07-3004,P02-1042,1,0.905724,"Missing"
J07-3004,W04-3215,1,0.810022,"Missing"
J07-3004,P97-1003,0,0.016668,"Missing"
J07-3004,copestake-flickinger-2000-open,0,0.00491162,"ions are provided by grammar formalisms that are more expressive than simple phrase-structure grammar, like Lexical-Functional Grammar (LFG) (Kaplan and Bresnan 1982), Head-driven Phrase-Structure Grammar (HPSG) (Pollard and Sag 1994), Tree-Adjoining Grammar (TAG) (Joshi and Schabes 1992), Minimalist Program–related Grammars (Stabler 2004), or Combinatory Categorial Grammar (CCG) (Steedman 1996, 2000). However, until very recently, only handwritten grammars, which lack the wide coverage and robustness of Treebank parsers, were available for these formalisms (Butt et al. 1999; XTAG-group 1999; Copestake and Flickinger 2000; OpenCCG1 [White and Baldridge 2003; White 2006]). Because treebank annotation for individual formalisms is prohibitively expensive, there have been a number of efforts to extract TAGs, LFGs, and, more recently, HPSGs, from the Penn Treebank (Xia 1999; Chen and Vijay-Shanker 2000; Xia, Palmer, and Joshi 2000; Xia 2001; Cahill et al. 2002; Miyao, Ninomiya, and Tsujii 2004; O’Donovan et al. 2005; Shen and Joshi 2005; Chen, Bangalore, and Vijay-Shanker 2006). Statistical parsers that are trained on these TAG and HPSG corpora have been presented by Chiang (2000) and Miyao and Tsujii (2005), where"
J07-3004,W03-1005,0,0.0232639,"Missing"
J07-3004,P03-1055,0,0.0123941,"unts for most of the data. This creates a temptation to concentrate on capturing the few high-frequency cases at the top end of the distribution, and to ignore the “long tail” of rare events such as non-local dependencies. Despite the fact that these occur in a large number of sentences, they affect only a small number of words, and have thus a small impact on overall dependency recovery. Although there is now a sizable literature on trace and function-tag insertion algorithms (Blaheta and Charniak 2000; Johnson 2002; Campbell 2004), and integrated parsing with function tags or null elements (Dienes and Dubey 2003a, 2003b; Merlo and Musillo 2005; Gabbard, Kulick, and Marcus 2006), such approaches typically require additional pre- or postprocessing steps that are likely to add further noise and errors to the parser output. A completely integrated approach that is based on a syntactic representation which allows direct recovery of the underlying predicate–argument structure might therefore be preferable. Such representations are provided by grammar formalisms that are more expressive than simple phrase-structure grammar, like Lexical-Functional Grammar (LFG) (Kaplan and Bresnan 1982), Head-driven Phrase-"
J07-3004,P96-1011,0,0.230637,"yxz : them NP : I >T S/(SNP ) : λf .f I (S/NP )/NP : λxλy.give yxI >B 2 > S/NP : λy.give y them I NN : λqλx.give x them I ∧ qx N : λx.give x them I ∧ money x > < We will see further examples of their use later. Such rules induce additional derivational ambiguity, even in canonical sentences like (4). However, our translation 4 Application of the two rules is indicated by underlines distinguished as < and >, respectively. 359 Computational Linguistics Volume 33, Number 3 algorithm yields normal form derivations (Hepple and Morrill 1989; Wittenburg and ¨ Wall 1991; Konig 1994; Eisner 1996), which use composition and type-raising only when syntactically necessary. For coordination, we will use a binarized version of the following ternary rule schema:5 Coordination: X: f conj X: g ⇒& X: λx.fx ∧ gx (7) For further explanation and linguistics and computational motivation for this theory of grammar, the reader is directed to Steedman (1996, 2000). 2.3 Head-Dependency Structure in CCGbank The syntactic derivations in CCGbank are accompanied with bilexical head-dependency structures, which are defined in terms of the lexical heads of functor categories and their arguments. The derivat"
J07-3004,N06-1024,0,0.0162215,"Missing"
J07-3004,P04-1013,0,0.00860915,"e the de facto standard training and test data for statistical parsers. Its annotation, which is based on generic phrasestructure grammar (with coindexed traces and other null elements indicating non-local dependencies) and function tags on nonterminal categories providing (a limited degree of) syntactic role information, is designed to facilitate the extraction of the underlying predicate–argument structure. Statistical parsing on the Penn Treebank has made great progress by focusing on the machine-learning or algorithmic aspects (Magerman 1994; Ratnaparkhi 1998; Collins 1999; Charniak 2000; Henderson 2004; McDonald, Crammer, and Pereira 2005). However, this has often resulted in parsing models and evaluation measures that are both based on reduced representations which simplify or ignore the linguistic information represented by function tags and null elements in the original Treebank. (One exception is Collins 1999, whose Model 2 includes a distinction between arguments and adjuncts, and whose Model 3 additionally captures wh-movement in relative clauses with a GPSG-like “slash-feature-passing” mechanism.) The reasons for this shift away from linguistic adequacy are easy to trace. The very he"
J07-3004,E89-1002,0,0.313491,"λpλqλx.px ∧ qx N give them (6) ((SNP )/NP )/NP NP : λxλyλz.give yxz : them NP : I >T S/(SNP ) : λf .f I (S/NP )/NP : λxλy.give yxI >B 2 > S/NP : λy.give y them I NN : λqλx.give x them I ∧ qx N : λx.give x them I ∧ money x > < We will see further examples of their use later. Such rules induce additional derivational ambiguity, even in canonical sentences like (4). However, our translation 4 Application of the two rules is indicated by underlines distinguished as < and >, respectively. 359 Computational Linguistics Volume 33, Number 3 algorithm yields normal form derivations (Hepple and Morrill 1989; Wittenburg and ¨ Wall 1991; Konig 1994; Eisner 1996), which use composition and type-raising only when syntactically necessary. For coordination, we will use a binarized version of the following ternary rule schema:5 Coordination: X: f conj X: g ⇒& X: λx.fx ∧ gx (7) For further explanation and linguistics and computational motivation for this theory of grammar, the reader is directed to Steedman (1996, 2000). 2.3 Head-Dependency Structure in CCGbank The syntactic derivations in CCGbank are accompanied with bilexical head-dependency structures, which are defined in terms of the lexical heads"
J07-3004,P03-1046,1,0.56553,"and Steedman CCGbank a postprocessing step on the output of a Treebank parser to recover predicate–argument dependencies. In this article we present an algorithmic method for obtaining a corpus of CCG derivations and dependency structures from the Penn Treebank, together with some observations that we believe carry wider implications for similar attempts with other grammar formalisms and corpora. Earlier versions of the resulting corpus, CCGbank, have already been used to build a number of wide-coverage statistical parsers (Clark, Hockenmaier, and Steedman 2002; Hockenmaier and Steedman 2002; Hockenmaier 2003b, 2003a; Clark and Curran 2004, 2007), which recover both local and long-range dependencies directly and in a single pass. CCG is a linguistically expressive, but efficiently parseable, lexicalized grammar formalism that was specifically designed to provide a base-generative account of coordinate and relativized constructions like the following: a. pay HealthVest $5 million right away and additional amounts in the future b. the parched Franco years, the everyday poverty and stagnant atmosphere of which he described in brutally direct, vivid prose c. Who is, and who should be, making the crimi"
J07-3004,P06-1064,1,0.606323,"grammars Mel’ˇcuk and Pertsov 1987; Hudson 1984) and parsers (McDonald, Crammer, and Pereira 2005) could be trained and tested with little extra work on the dependencies in CCGbank. Finally, we believe that existing methods for translating the Penn Treebank from scratch into other grammar formalisms will benefit from including preprocessing similar to that described here. As some indication of the relative ease with which these techniques transfer, we offer the observation that the 900K-word German Tiger dependency corpus has recently been translated into CCG using very similar techniques by Hockenmaier (2006), and C ¸ akıcı (2005) has derived a Turkish lexicon from the a similarly preprocessed version of the METU-Sabanc¸ı Turkish dependency treebank (Oflazer et al. 2003). A fundamental assumption behind attempts at the automatic translation of syntactically annotated corpora into different grammatical formalisms such as CCG, TAG, HPSG, or LFG is that the analyses that are captured in the original annotation can be mapped directly (or, at least, without too much additional work) into the desired analyses in the target formalism. This can only hold if all constructions that are treated in a similar"
J07-3004,P02-1043,1,0.79729,"urceforge.net. 356 Hockenmaier and Steedman CCGbank a postprocessing step on the output of a Treebank parser to recover predicate–argument dependencies. In this article we present an algorithmic method for obtaining a corpus of CCG derivations and dependency structures from the Penn Treebank, together with some observations that we believe carry wider implications for similar attempts with other grammar formalisms and corpora. Earlier versions of the resulting corpus, CCGbank, have already been used to build a number of wide-coverage statistical parsers (Clark, Hockenmaier, and Steedman 2002; Hockenmaier and Steedman 2002; Hockenmaier 2003b, 2003a; Clark and Curran 2004, 2007), which recover both local and long-range dependencies directly and in a single pass. CCG is a linguistically expressive, but efficiently parseable, lexicalized grammar formalism that was specifically designed to provide a base-generative account of coordinate and relativized constructions like the following: a. pay HealthVest $5 million right away and additional amounts in the future b. the parched Franco years, the everyday poverty and stagnant atmosphere of which he described in brutally direct, vivid prose c. Who is, and who should be"
J07-3004,P02-1018,0,0.00643982,"s a Zipfian distribution, where a very small proportion of the available alternatives accounts for most of the data. This creates a temptation to concentrate on capturing the few high-frequency cases at the top end of the distribution, and to ignore the “long tail” of rare events such as non-local dependencies. Despite the fact that these occur in a large number of sentences, they affect only a small number of words, and have thus a small impact on overall dependency recovery. Although there is now a sizable literature on trace and function-tag insertion algorithms (Blaheta and Charniak 2000; Johnson 2002; Campbell 2004), and integrated parsing with function tags or null elements (Dienes and Dubey 2003a, 2003b; Merlo and Musillo 2005; Gabbard, Kulick, and Marcus 2006), such approaches typically require additional pre- or postprocessing steps that are likely to add further noise and errors to the parser output. A completely integrated approach that is based on a syntactic representation which allows direct recovery of the underlying predicate–argument structure might therefore be preferable. Such representations are provided by grammar formalisms that are more expressive than simple phrase-stru"
J07-3004,C92-2066,0,0.247061,"aches typically require additional pre- or postprocessing steps that are likely to add further noise and errors to the parser output. A completely integrated approach that is based on a syntactic representation which allows direct recovery of the underlying predicate–argument structure might therefore be preferable. Such representations are provided by grammar formalisms that are more expressive than simple phrase-structure grammar, like Lexical-Functional Grammar (LFG) (Kaplan and Bresnan 1982), Head-driven Phrase-Structure Grammar (HPSG) (Pollard and Sag 1994), Tree-Adjoining Grammar (TAG) (Joshi and Schabes 1992), Minimalist Program–related Grammars (Stabler 2004), or Combinatory Categorial Grammar (CCG) (Steedman 1996, 2000). However, until very recently, only handwritten grammars, which lack the wide coverage and robustness of Treebank parsers, were available for these formalisms (Butt et al. 1999; XTAG-group 1999; Copestake and Flickinger 2000; OpenCCG1 [White and Baldridge 2003; White 2006]). Because treebank annotation for individual formalisms is prohibitively expensive, there have been a number of efforts to extract TAGs, LFGs, and, more recently, HPSGs, from the Penn Treebank (Xia 1999; Chen a"
J07-3004,kinyon-prolo-2002-identifying,0,0.022997,"Missing"
J07-3004,H94-1020,0,0.012887,"fy the correct analysis among the many alternatives that such a wide-coverage grammar will generate even for the simplest sentences. Given our current machine learning techniques, such parsing models typically need to be trained on relatively large treebanks—that is, text corpora hand-labeled with detailed syntactic structures. Because such annotation requires linguistic expertise, and is therefore difficult to produce, we are currently limited to at most a few treebanks per language. One of the largest and earliest such efforts is the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993; Marcus et al. 1994), which contains a one-million word ∗ Institute for Research in Cognitive Science, University of Pennsylvania, 3401 Walnut Street, Suite 400A, Philadelphia, PA 19104-6228, USA. E-mail: juliahr@cis.upenn.edu. ∗∗ School of Informatics, University of Edinburgh, 2 Buccleuch Place, Edinburgh EH8 9LW, UK. E-mail: steedman@inf.ed.ac.uk. Submission received: 16 July 2005; revised submission received: 24 January 2007; accepted for publication: 21 February 2007. © 2007 Association for Computational Linguistics Computational Linguistics Volume 33, Number 3 subcorpus of Wall Street Journal text that has b"
J07-3004,J93-2004,0,0.0463466,"Missing"
J07-3004,P05-1012,0,0.0304631,"Missing"
J07-3004,H05-1078,0,0.00695377,"creates a temptation to concentrate on capturing the few high-frequency cases at the top end of the distribution, and to ignore the “long tail” of rare events such as non-local dependencies. Despite the fact that these occur in a large number of sentences, they affect only a small number of words, and have thus a small impact on overall dependency recovery. Although there is now a sizable literature on trace and function-tag insertion algorithms (Blaheta and Charniak 2000; Johnson 2002; Campbell 2004), and integrated parsing with function tags or null elements (Dienes and Dubey 2003a, 2003b; Merlo and Musillo 2005; Gabbard, Kulick, and Marcus 2006), such approaches typically require additional pre- or postprocessing steps that are likely to add further noise and errors to the parser output. A completely integrated approach that is based on a syntactic representation which allows direct recovery of the underlying predicate–argument structure might therefore be preferable. Such representations are provided by grammar formalisms that are more expressive than simple phrase-structure grammar, like Lexical-Functional Grammar (LFG) (Kaplan and Bresnan 1982), Head-driven Phrase-Structure Grammar (HPSG) (Pollar"
J07-3004,P05-1011,0,0.00465607,"Copestake and Flickinger 2000; OpenCCG1 [White and Baldridge 2003; White 2006]). Because treebank annotation for individual formalisms is prohibitively expensive, there have been a number of efforts to extract TAGs, LFGs, and, more recently, HPSGs, from the Penn Treebank (Xia 1999; Chen and Vijay-Shanker 2000; Xia, Palmer, and Joshi 2000; Xia 2001; Cahill et al. 2002; Miyao, Ninomiya, and Tsujii 2004; O’Donovan et al. 2005; Shen and Joshi 2005; Chen, Bangalore, and Vijay-Shanker 2006). Statistical parsers that are trained on these TAG and HPSG corpora have been presented by Chiang (2000) and Miyao and Tsujii (2005), whereas the LFG parsing system of Cahill et al. (2004) uses 1 Open CCG, the successor of Grok (Hockenmaier et al. 2004), is available from http://openccg. sourceforge.net. 356 Hockenmaier and Steedman CCGbank a postprocessing step on the output of a Treebank parser to recover predicate–argument dependencies. In this article we present an algorithmic method for obtaining a corpus of CCG derivations and dependency structures from the Penn Treebank, together with some observations that we believe carry wider implications for similar attempts with other grammar formalisms and corpora. Earlier ve"
J07-3004,J05-3003,0,0.00595653,"Missing"
J07-3004,J05-1004,0,0.134314,"Missing"
J07-3004,W03-2316,0,0.00769266,"hat are more expressive than simple phrase-structure grammar, like Lexical-Functional Grammar (LFG) (Kaplan and Bresnan 1982), Head-driven Phrase-Structure Grammar (HPSG) (Pollard and Sag 1994), Tree-Adjoining Grammar (TAG) (Joshi and Schabes 1992), Minimalist Program–related Grammars (Stabler 2004), or Combinatory Categorial Grammar (CCG) (Steedman 1996, 2000). However, until very recently, only handwritten grammars, which lack the wide coverage and robustness of Treebank parsers, were available for these formalisms (Butt et al. 1999; XTAG-group 1999; Copestake and Flickinger 2000; OpenCCG1 [White and Baldridge 2003; White 2006]). Because treebank annotation for individual formalisms is prohibitively expensive, there have been a number of efforts to extract TAGs, LFGs, and, more recently, HPSGs, from the Penn Treebank (Xia 1999; Chen and Vijay-Shanker 2000; Xia, Palmer, and Joshi 2000; Xia 2001; Cahill et al. 2002; Miyao, Ninomiya, and Tsujii 2004; O’Donovan et al. 2005; Shen and Joshi 2005; Chen, Bangalore, and Vijay-Shanker 2006). Statistical parsers that are trained on these TAG and HPSG corpora have been presented by Chiang (2000) and Miyao and Tsujii (2005), whereas the LFG parsing system of Cahill"
J07-3004,W00-1307,0,0.0633456,"Missing"
J07-3004,W97-1010,0,\N,Missing
J07-3004,W96-0213,0,\N,Missing
J07-3004,J03-4003,0,\N,Missing
J07-3004,C98-1025,0,\N,Missing
N03-1031,A00-2018,0,0.0290607,"s are iteratively re-trained on each other’s output; and a semi-supervised approach, corrected co-training, in which a human corrects each parser’s output before adding it to the training data. The selection of labeled training examples is an integral part of both frameworks. We propose several selection methods based on the criteria of minimizing errors in the data and maximizing training utility. We show that incorporating the utility criterion into the selection method results in better parsers for both frameworks. 1 Introduction Current state-of-the-art statistical parsers (Collins, 1999; Charniak, 2000) are trained on large annotated corpora such as the Penn Treebank (Marcus et al., 1993). However, the production of such corpora is expensive and labor-intensive. Given this bottleneck, there is considerable interest in (partially) automating the annotation process. To overcome this bottleneck, two approaches from machine learning have been applied to training parsers. One is sample selection (Thompson et al., 1999; Hwa, 2000; Tang et al., 2002), a variant of active learning (Cohn et al., 1994), which tries to identify a small set of unlabeled sentences with high training utility for the human"
N03-1031,P96-1042,0,0.012772,"Missing"
N03-1031,N01-1023,1,0.558581,"e. Given this bottleneck, there is considerable interest in (partially) automating the annotation process. To overcome this bottleneck, two approaches from machine learning have been applied to training parsers. One is sample selection (Thompson et al., 1999; Hwa, 2000; Tang et al., 2002), a variant of active learning (Cohn et al., 1994), which tries to identify a small set of unlabeled sentences with high training utility for the human to label1 . Sentences with high training utility are those most likely to improve the parser. The other approach, and the focus of this paper, is co-training (Sarkar, 2001), a mostlyunsupervised algorithm that replaces the human by having two (or more) parsers label training examples for each other. The goal is for both parsers to improve by bootstrapping off each other’s strengths. Because the parsers may label examples incorrectly, only a subset of their output, chosen by some selection mechanism, is used in order to minimize errors. The choice of selection method significantly affects the quality of the resulting parsers. We investigate a novel approach of selecting training examples for co-training parsers by incorporating the idea of maximizing training uti"
N03-1031,E03-1008,1,0.765705,"Missing"
N03-1031,W00-1306,1,0.314843,"ility criterion into the selection method results in better parsers for both frameworks. 1 Introduction Current state-of-the-art statistical parsers (Collins, 1999; Charniak, 2000) are trained on large annotated corpora such as the Penn Treebank (Marcus et al., 1993). However, the production of such corpora is expensive and labor-intensive. Given this bottleneck, there is considerable interest in (partially) automating the annotation process. To overcome this bottleneck, two approaches from machine learning have been applied to training parsers. One is sample selection (Thompson et al., 1999; Hwa, 2000; Tang et al., 2002), a variant of active learning (Cohn et al., 1994), which tries to identify a small set of unlabeled sentences with high training utility for the human to label1 . Sentences with high training utility are those most likely to improve the parser. The other approach, and the focus of this paper, is co-training (Sarkar, 2001), a mostlyunsupervised algorithm that replaces the human by having two (or more) parsers label training examples for each other. The goal is for both parsers to improve by bootstrapping off each other’s strengths. Because the parsers may label examples inc"
N03-1031,P02-1016,0,0.00757357,"rion into the selection method results in better parsers for both frameworks. 1 Introduction Current state-of-the-art statistical parsers (Collins, 1999; Charniak, 2000) are trained on large annotated corpora such as the Penn Treebank (Marcus et al., 1993). However, the production of such corpora is expensive and labor-intensive. Given this bottleneck, there is considerable interest in (partially) automating the annotation process. To overcome this bottleneck, two approaches from machine learning have been applied to training parsers. One is sample selection (Thompson et al., 1999; Hwa, 2000; Tang et al., 2002), a variant of active learning (Cohn et al., 1994), which tries to identify a small set of unlabeled sentences with high training utility for the human to label1 . Sentences with high training utility are those most likely to improve the parser. The other approach, and the focus of this paper, is co-training (Sarkar, 2001), a mostlyunsupervised algorithm that replaces the human by having two (or more) parsers label training examples for each other. The goal is for both parsers to improve by bootstrapping off each other’s strengths. Because the parsers may label examples incorrectly, only a sub"
N03-1031,J93-2004,0,0.0255646,"h, corrected co-training, in which a human corrects each parser’s output before adding it to the training data. The selection of labeled training examples is an integral part of both frameworks. We propose several selection methods based on the criteria of minimizing errors in the data and maximizing training utility. We show that incorporating the utility criterion into the selection method results in better parsers for both frameworks. 1 Introduction Current state-of-the-art statistical parsers (Collins, 1999; Charniak, 2000) are trained on large annotated corpora such as the Penn Treebank (Marcus et al., 1993). However, the production of such corpora is expensive and labor-intensive. Given this bottleneck, there is considerable interest in (partially) automating the annotation process. To overcome this bottleneck, two approaches from machine learning have been applied to training parsers. One is sample selection (Thompson et al., 1999; Hwa, 2000; Tang et al., 2002), a variant of active learning (Cohn et al., 1994), which tries to identify a small set of unlabeled sentences with high training utility for the human to label1 . Sentences with high training utility are those most likely to improve the"
N03-1031,P00-1016,0,0.0133509,"Missing"
N03-1031,W01-0501,0,0.0173865,"have different goals, their selection methods focus on different criteria: co-training typically favors selecting accurately labeled examples, while sample selection typically favors selecting examples with high training utility, which often are not sentences that the parsers already label accurately. In this work, we investigate selection methods for co-training that explore the trade-off between maximizing training utility and minimizing errors. Empirical studies were conducted to compare selection methods under both co-training and a semi-supervised framework called corrected co-training (Pierce and Cardie, 2001), in which the selected examples are manually checked and corrected before being added to the 1 In the context of training parsers, a labeled example is a sentence with its parse tree. Throughout this paper, we use the term “label” and “parse” interchangeably. training data. For co-training, we show that the benefit of selecting examples with high training utility can offset the additional errors they contain. For corrected co-training, we show that selecting examples with high training utility reduces the number of sentences the human annotator has to check. For both frameworks, we show that"
N03-1031,J03-4003,0,\N,Missing
N03-1031,P02-1046,0,\N,Missing
P02-1042,J99-2004,0,\N,Missing
P02-1042,A00-2018,0,\N,Missing
P02-1042,J97-4005,0,\N,Missing
P02-1042,C96-1058,0,\N,Missing
P02-1042,hockenmaier-steedman-2002-acquiring,1,\N,Missing
P02-1042,J03-4003,0,\N,Missing
P02-1042,P02-1043,1,\N,Missing
P02-1042,P96-1011,0,\N,Missing
P02-1042,P00-1058,0,\N,Missing
P02-1042,P96-1025,0,\N,Missing
P02-1042,1997.iwpt-1.17,0,\N,Missing
P02-1042,P99-1069,0,\N,Missing
P02-1043,J97-4005,0,0.14797,"nce our approach makes so much use of the POS-tag information for unknown words. However, a POS-tagger trained on CCGbank might yield slightly better results. 5.5 Limitations of the current model Unlike Clark et al. (2002), our parser does not always model the dependencies in the logical form. For example, in the interpretation of a coordinate structure like “buy and sell shares”, shares will head an object of both buy and sell. Similarly, in examples like “buy the company that wins”, the relative construction makes company depend upon both buy as object and wins as subject. As is well known (Abney, 1997), DAG-like dependencies cannot in general be modeled with a generative approach of the kind taken here3 . 5.6 Comparison with Clark et al. (2002) Clark et al. (2002) presents another statistical CCG parser, which is based on a conditional (rather than generative) model of the derived dependency structure, including non-surface dependencies. The following table compares the two parsers according to the evaluation of surface and deep dependencies given in Clark et al. (2002). We use Clark et al.’s parser to generate these dependencies from the output of our parser (see Clark and Hockenmaier (200"
P02-1043,P02-1042,1,0.374333,"ssary to acquire correct features on categories). It is reasonable to assume that this input is of higher quality than can be produced by a POS-tagger. We therefore ran the dependency model on a test corpus tagged with the POS-tagger of Ratnaparkhi (1996), which is trained on the original Penn Treebank (see HWDep (+ tagger) in Table 3). Performance degrades slightly, which is to be expected, since our approach makes so much use of the POS-tag information for unknown words. However, a POS-tagger trained on CCGbank might yield slightly better results. 5.5 Limitations of the current model Unlike Clark et al. (2002), our parser does not always model the dependencies in the logical form. For example, in the interpretation of a coordinate structure like “buy and sell shares”, shares will head an object of both buy and sell. Similarly, in examples like “buy the company that wins”, the relative construction makes company depend upon both buy as object and wins as subject. As is well known (Abney, 1997), DAG-like dependencies cannot in general be modeled with a generative approach of the kind taken here3 . 5.6 Comparison with Clark et al. (2002) Clark et al. (2002) presents another statistical CCG parser, whi"
P02-1043,P97-1003,0,0.320625,"the training corpus does not contain all the entries required to parse the test corpus. We discuss a simple, but imperfect, solution to this problem in section 7. 5 Extending the baseline model State-of-the-art statistical parsers use many other features, or conditioning variables, such as head words, subcategorization frames, distance measures and grandparent nodes. We too can extend the baseline model described in the previous section by including more features. Like the models of Goodman (1997), the additional features in our model are generated probabilistically, whereas in the parser of Collins (1997) distance measures are assumed to be a function of the already generated structure and are not generated explicitly. In order to estimate the conditional probabilities of our model, we recursively smooth empirical estimates eˆi of specific conditional distributions with (possible smoothed) estimates of less specific distributions e˜i 1 , using linear interpolation: e˜i = λeˆi + (1 λ)e˜i 1 λ is a smoothing weight which depends on the particular distribution.2 When defining models, we will indicate a backoff level with a # sign between conditioning variables, eg. A; B # C # D means that we inter"
P02-1043,W01-0521,0,0.0598412,"Missing"
P02-1043,hockenmaier-steedman-2002-acquiring,1,0.563785,"Missing"
P02-1043,1997.iwpt-1.13,0,0.102221,"Missing"
P02-1043,J98-4004,0,0.0521291,"Missing"
P02-1043,A00-2018,0,\N,Missing
P02-1043,W96-0213,0,\N,Missing
P02-1043,J03-4003,0,\N,Missing
P02-1043,P00-1058,0,\N,Missing
P03-1046,A00-2018,0,0.249232,"ner, and our experimental results for English demonstrate that their inclusion improves parsing performance. However, since the predicate-argument structure itself depends only to a degree on the grammar formalism, it is likely that parsers that are based on other grammar formalisms could equally benefit from such a model. The conditional model used by the CCG parser of Clark et al. (2002) also captures dependencies in the predicate-argument structure; however, their model is inconsistent. State-of-the-art statistical parsers for Penn Treebank-style phrase-structure grammars (Collins, 1999), (Charniak, 2000), but also for Categorial Grammar (Hockenmaier and Steedman, 2002b), include models of bilexical dependencies defined in terms of local trees. However, this paper demonstrates that such models would be inadequate for languages with freer word order. We use the example of Dutch ditransitives, but our argument equally applies to other languages such as Czech (see Collins et al. (1999)). We argue that this problem can be avoided if instead the bilexical dependencies in the predicate-argument structure are captured, and propose a generative model for these dependencies. The focus of this paper is"
P03-1046,P02-1042,1,0.904063,"te access to the predicate-argument structure, which includes not only local, but also long-range dependencies arising through coordination, extraction and control. These dependencies can be captured by our model in a sound manner, and our experimental results for English demonstrate that their inclusion improves parsing performance. However, since the predicate-argument structure itself depends only to a degree on the grammar formalism, it is likely that parsers that are based on other grammar formalisms could equally benefit from such a model. The conditional model used by the CCG parser of Clark et al. (2002) also captures dependencies in the predicate-argument structure; however, their model is inconsistent. State-of-the-art statistical parsers for Penn Treebank-style phrase-structure grammars (Collins, 1999), (Charniak, 2000), but also for Categorial Grammar (Hockenmaier and Steedman, 2002b), include models of bilexical dependencies defined in terms of local trees. However, this paper demonstrates that such models would be inadequate for languages with freer word order. We use the example of Dutch ditransitives, but our argument equally applies to other languages such as Czech (see Collins et al"
P03-1046,P99-1065,0,0.0242622,"et al. (2002) also captures dependencies in the predicate-argument structure; however, their model is inconsistent. State-of-the-art statistical parsers for Penn Treebank-style phrase-structure grammars (Collins, 1999), (Charniak, 2000), but also for Categorial Grammar (Hockenmaier and Steedman, 2002b), include models of bilexical dependencies defined in terms of local trees. However, this paper demonstrates that such models would be inadequate for languages with freer word order. We use the example of Dutch ditransitives, but our argument equally applies to other languages such as Czech (see Collins et al. (1999)). We argue that this problem can be avoided if instead the bilexical dependencies in the predicate-argument structure are captured, and propose a generative model for these dependencies. The focus of this paper is on models for Combinatory Categorial Grammar (CCG, Steedman (2000)). First, we review the dependency model proposed by Hockenmaier and Steedman (2002b). We then use the example of Dutch ditransitives to demonstrate its inadequacy for languages with a freer word order. This leads us to define a new generative model of CCG derivations, which captures word-word dependencies in the unde"
P03-1046,hockenmaier-steedman-2002-acquiring,1,0.651437,"nstrate that their inclusion improves parsing performance. However, since the predicate-argument structure itself depends only to a degree on the grammar formalism, it is likely that parsers that are based on other grammar formalisms could equally benefit from such a model. The conditional model used by the CCG parser of Clark et al. (2002) also captures dependencies in the predicate-argument structure; however, their model is inconsistent. State-of-the-art statistical parsers for Penn Treebank-style phrase-structure grammars (Collins, 1999), (Charniak, 2000), but also for Categorial Grammar (Hockenmaier and Steedman, 2002b), include models of bilexical dependencies defined in terms of local trees. However, this paper demonstrates that such models would be inadequate for languages with freer word order. We use the example of Dutch ditransitives, but our argument equally applies to other languages such as Czech (see Collins et al. (1999)). We argue that this problem can be avoided if instead the bilexical dependencies in the predicate-argument structure are captured, and propose a generative model for these dependencies. The focus of this paper is on models for Combinatory Categorial Grammar (CCG, Steedman (2000"
P03-1046,P02-1043,1,0.732634,"nstrate that their inclusion improves parsing performance. However, since the predicate-argument structure itself depends only to a degree on the grammar formalism, it is likely that parsers that are based on other grammar formalisms could equally benefit from such a model. The conditional model used by the CCG parser of Clark et al. (2002) also captures dependencies in the predicate-argument structure; however, their model is inconsistent. State-of-the-art statistical parsers for Penn Treebank-style phrase-structure grammars (Collins, 1999), (Charniak, 2000), but also for Categorial Grammar (Hockenmaier and Steedman, 2002b), include models of bilexical dependencies defined in terms of local trees. However, this paper demonstrates that such models would be inadequate for languages with freer word order. We use the example of Dutch ditransitives, but our argument equally applies to other languages such as Czech (see Collins et al. (1999)). We argue that this problem can be avoided if instead the bilexical dependencies in the predicate-argument structure are captured, and propose a generative model for these dependencies. The focus of this paper is on models for Combinatory Categorial Grammar (CCG, Steedman (2000"
P03-1046,J03-4003,0,\N,Missing
P06-1064,P04-1042,0,0.0235123,"Missing"
P06-1064,J93-2004,0,0.0394401,"Missing"
P06-1064,P05-1011,0,0.0626451,"Missing"
P06-1064,P05-2013,0,0.136567,"Missing"
P06-1064,moortgat-moot-2002-using,0,0.0460665,"Missing"
P06-1064,J05-3003,0,0.0476712,"Missing"
P06-1064,P04-1014,0,0.0885078,"Missing"
P06-1064,H05-1102,0,0.0398262,"Missing"
P06-1064,P03-1013,0,0.0816126,"Missing"
P06-1064,A97-1014,0,0.0321951,"of these problems may be mitigated. Here, we present an algorithm which translates the German Tiger corpus (Brants et al., 2002) into CCG derivations. Similar algorithms have been developed by Hockenmaier and Steedman (2002a) to create CCGbank, a corpus of CCG derivations (Hockenmaier and Steedman, 2005) from the Penn Treebank, by C ¸ akıcı (2005) to extract a CCG lexicon from a Turkish dependency corpus, and by Moortgat and Moot (2002) to induce a type-logical grammar for Dutch. The annotation scheme used in Tiger is an extension of that used in the earlier, and smaller, German Negra corpus (Skut et al., 1997). Tiger is better suited for the extraction of subcategorization information (and thus the translation into “deep” grammars of any kind), since it distinguishes between PP complements and modifiers, and includes “secondary” edges to indicate shared arguments in coordinate constructions. Tiger also includes morphology and lemma information. Negra is also provided with a “Penn Treebank”style representation, which uses flat phrase structure trees instead of the crossing dependency structures in the original corpus. This version has been used by Cahill et al. (2005) to extract a German LFG. Howeve"
P06-1064,hockenmaier-steedman-2002-acquiring,1,0.830229,"Missing"
P06-1064,P02-1043,1,0.888803,"Missing"
P10-5001,J08-1003,1,0.889847,"Missing"
P10-5001,J08-1002,1,0.800767,"Missing"
P10-5001,J07-3004,1,0.850258,"Missing"
P15-1135,Q13-1007,1,0.19794,"analysis is the fact that most of the work in this domain has focused on parsers that return unlabeled dependencies, which cannot easily be assigned a linguistic interpretation. This paper shows that approaches that are based on categorial grammar (Steedman, 2000) are amenable to more stringent evaluation metrics, which enable detailed analyses of the constructions they capture, while the commonly used unlabeled directed attachment scores hide linguistically important errors. Any categorial grammar based system, whether deriving its grammar from seed knowledge distinguishing nouns and verbs (Bisk and Hockenmaier, 2013), from a lexicon constructed from a simple questionnaire for linguists (Boonkwan and Steedman, 2011), or from sections of a treebank (Garrette et al., 2015), will attach linguistically expressive categories to individual words, and can therefore produce labeled dependencies. We provide a simple proof of concept for how these labeled dependencies can be used to isolate problem areas in CCG induction algorithms. We illustrate how they make the linguistic assumptions and mistakes of the model transparent, and are easily comparable to a treebank where available. They also allow us to identify ling"
P15-1135,D10-1117,0,0.0579177,"rogress in grammar induction. 1 Introduction Grammar induction aims to develop algorithms that can automatically discover the latent syntactic structure of language from raw or part-of-speech tagged text. While such algorithms would have the greatest utility for low-resource languages for which no treebank is available to train supervised parsers, most work in this area has focused on languages where existing treebanks can be used to measure and compare the performance of the resultant parsers. Despite significant progress in the last decade (Klein and Manning, 2004; Headden III et al., 2009; Blunsom and Cohn, 2010; Spitkovsky et al., 2013; Mareˇcek and Straka, 2013), there has been little analysis performed on the types of errors these induction systems make, and our understanding of what kinds of constructions these parsers can or cannot recover is still rather limited. One likely reason for this lack of analysis is the fact that most of the work in this domain has focused on parsers that return unlabeled dependencies, which cannot easily be assigned a linguistic interpretation. This paper shows that approaches that are based on categorial grammar (Steedman, 2000) are amenable to more stringent evalua"
P15-1135,I11-1049,0,0.34643,"eled dependencies, which cannot easily be assigned a linguistic interpretation. This paper shows that approaches that are based on categorial grammar (Steedman, 2000) are amenable to more stringent evaluation metrics, which enable detailed analyses of the constructions they capture, while the commonly used unlabeled directed attachment scores hide linguistically important errors. Any categorial grammar based system, whether deriving its grammar from seed knowledge distinguishing nouns and verbs (Bisk and Hockenmaier, 2013), from a lexicon constructed from a simple questionnaire for linguists (Boonkwan and Steedman, 2011), or from sections of a treebank (Garrette et al., 2015), will attach linguistically expressive categories to individual words, and can therefore produce labeled dependencies. We provide a simple proof of concept for how these labeled dependencies can be used to isolate problem areas in CCG induction algorithms. We illustrate how they make the linguistic assumptions and mistakes of the model transparent, and are easily comparable to a treebank where available. They also allow us to identify linguistic phenomena that require additional supervision or training signal to master. Our analysis will"
P15-1135,W98-1505,0,0.0490183,"line model to capture lexicalization and punctuation, and how increasing the complexity of the induced grammars affect performance (Table 2). 6.1 Modeling Lexicalization In keeping with most work in grammar induction from part-of-speech tagged text, Bisk and Hockenmaier’s (2013) HDP-CCG treats POS tags t rather than words w as the terminals it generates based on their lexical categories c. The advantage of this approach is that tag-based emissions p(t|c) are a lot less sparse than word-based emissions p(w|c). It is therefore beneficial to first train a model that emits tags rather than words (Carroll and Rooth, 1998), and then to use this simpler model to initialize a lexicalized model that generates words instead of tags. To perform the switch we simply estimate counts for the parse forests using the unlexicalized model during the E-Step and then apply those counts to the lexicalized model during the M-Step. Inside-Outside then continues as before. Many words, like prepositions, differ systematically in their preferred syntactic role from that of their part-of-speech tags. This change benefits all settings of the model (Column 2 of Table 2). 6.2 Modeling Punctuation Spitkovsky et al. (2011) performed a d"
P15-1135,P02-1042,1,0.842746,"Missing"
P15-1135,P10-1035,0,0.048103,"Missing"
P15-1135,N09-1012,0,0.0579749,"Missing"
P15-1135,J07-3004,1,0.808296,"of trees), and that have motivated the use of categorial grammarbased approaches for supervised parsing. 1395 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1395–1404, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics First, we provide a brief introduction to CCG. Next, we define a labeled evaluation metric that allows us to compare the labeled dependencies produced by Bisk and Hockenmaier (2013)’s unsupervised parser with those in CCGbank (Hockenmaier and Steedman, 2007). Third, we extend their induction algorithm to allow it to induce more complex categories, and refine their probability model to handle punctuation and lexicalization, which we show to be necessary when handling the larger grammars induced by our variant of their algorithm. While we also perform a traditional dependency evaluation for comparison to the non-CCG based literature, we focus on our CCG-based labeled evaluation metrics to perform a comparative analysis of Bisk and Hockenmaier (2013)’s parser and our extensions. 2 Combinatory Categorial Grammar CCG categories CCG (Steedman, 2000) is"
P15-1135,P04-1061,0,0.308217,"hinting at new research directions necessary for progress in grammar induction. 1 Introduction Grammar induction aims to develop algorithms that can automatically discover the latent syntactic structure of language from raw or part-of-speech tagged text. While such algorithms would have the greatest utility for low-resource languages for which no treebank is available to train supervised parsers, most work in this area has focused on languages where existing treebanks can be used to measure and compare the performance of the resultant parsers. Despite significant progress in the last decade (Klein and Manning, 2004; Headden III et al., 2009; Blunsom and Cohn, 2010; Spitkovsky et al., 2013; Mareˇcek and Straka, 2013), there has been little analysis performed on the types of errors these induction systems make, and our understanding of what kinds of constructions these parsers can or cannot recover is still rather limited. One likely reason for this lack of analysis is the fact that most of the work in this domain has focused on parsers that return unlabeled dependencies, which cannot easily be assigned a linguistic interpretation. This paper shows that approaches that are based on categorial grammar (Ste"
P15-1135,D07-1072,0,0.0409228,"l follow the standard practice in supervised parsing of using WSJ Sections 02-21 for training, Section 22 for development and error analysis, and a final evaluation of the best models on Section 23. Because the induced lexicons are overly general, the memory footprint grows rapidly as the complexity of the grammar increases. For this reason, we only train on sentences that contain up to 20 words (as well as an arbitrary number of punctuation marks). All analyses and evaluation are performed with sentences of all lengths unless otherwise indicated. Finally, Bisk and Hockenmaier (2013) followed Liang et al. (2007) in setting the values of the hyperparameters α to powers (eg. the square) of the number of observed outcomes in the distribution. But when the output consists of words rather than POS tags, the concentration parameter α = V 2 is too large to allow the model to learn. For this reason, experiments will be reported with all hyperparameters set to a constant of 2500.1 1 We tested three values (1000, 2500, 5000) and found that the basic model at 2500 performed closest to the previously 1398 Base + Lexicalization + Punctuation + Punc & Lex + Allow (X|X)|X Only Atomic Arguments (S, N) B1 B3 34.2 34."
P15-1135,P13-1028,0,0.0285623,"Missing"
P15-1135,D10-1120,0,0.483772,"CCG categories the model’s to be question for reading future work as to whether annotation Unsupervised CCG induction Thebeyond induction performance on CCGbank dependencies (TaIIwith saw her from afar systems directly to an unsupervised dependency Unsupervised CCG induction The induction used as NP modifiers NPNP) that are the output and make mistake can even be easily easily diagnosed. A saw her andfrom learning more burdensome. 9 standards Conclusions and mistake can be diagnosed. A similar picture emerges, threeafar of five at- output 2 . This also ble 10) allows usout to compare parser (Naseem et al., 2010), who report directedour model algorithm needs to identify the set of lexical algorithm needs to identify the set of lexical producing this analysis analysis is not not learning learning the the scope of our induction algorithm. The usedirectly of categories as predicate dependency labels model producing is The use of categories as dependency These dependencies dependencies are the complete artachments are deemed correct: systems to complete an dependency) unsupervised dependency In 9this Conclusions paper, we havethis touched upon many linguisattachment (unlabeled scores of alabels These are"
P15-1135,W11-0303,0,0.0224194,"r than words (Carroll and Rooth, 1998), and then to use this simpler model to initialize a lexicalized model that generates words instead of tags. To perform the switch we simply estimate counts for the parse forests using the unlexicalized model during the E-Step and then apply those counts to the lexicalized model during the M-Step. Inside-Outside then continues as before. Many words, like prepositions, differ systematically in their preferred syntactic role from that of their part-of-speech tags. This change benefits all settings of the model (Column 2 of Table 2). 6.2 Modeling Punctuation Spitkovsky et al. (2011) performed a detailed analysis of punctuation for dependency-based grammar induction, and proposed a number of constraints that aimed to capture the different ways in which dependencies might cross constituent boundaries implied by punctuation marks. A constituency-based formalism like CCG allows us instead to define a very simple, but effective Dirichlet Process (DP) based Markov gramreported dependency evaluation comparison with the work of Naseem et al. (2010). We fixed this hyperparameter setting for experimental simplicity but a more rigorous grid search might find better parameters for t"
P15-1135,D13-1204,0,0.0380025,"tion. 1 Introduction Grammar induction aims to develop algorithms that can automatically discover the latent syntactic structure of language from raw or part-of-speech tagged text. While such algorithms would have the greatest utility for low-resource languages for which no treebank is available to train supervised parsers, most work in this area has focused on languages where existing treebanks can be used to measure and compare the performance of the resultant parsers. Despite significant progress in the last decade (Klein and Manning, 2004; Headden III et al., 2009; Blunsom and Cohn, 2010; Spitkovsky et al., 2013; Mareˇcek and Straka, 2013), there has been little analysis performed on the types of errors these induction systems make, and our understanding of what kinds of constructions these parsers can or cannot recover is still rather limited. One likely reason for this lack of analysis is the fact that most of the work in this domain has focused on parsers that return unlabeled dependencies, which cannot easily be assigned a linguistic interpretation. This paper shows that approaches that are based on categorial grammar (Steedman, 2000) are amenable to more stringent evaluation metrics, which enabl"
P15-1135,W03-3023,0,0.356046,"Missing"
P15-2143,N13-1014,0,0.0322673,"nformation (Christodoulopoulos et al., 2012) may close this gap. on the class label proves a beneficial signal. We have also marked two classes with * to draw the reader’s attention to a fully noun cluster in English and an other cluster in Chinese which are highly ambiguous. Specifically, in both of these cases the frequent words also occur frequently as verbs, providing additional motivation for a better soft-clustering algorithm in future work. How to most effectively use seed knowledge and annotation is still an open question. Approaches range from labeling frequent words like the work of Garrette and Baldridge (2013) to the recently introduced active learning approach of Stratos and Collins (2015). In this work, we were able to demonstrate high noun and verb recall with the use of a very small set of labeled words because they correspond to an existing clustering. In contrast, we found that labeling even the 1000 most frequent words led to very few clusters being correctly identified; e.g. in English, using the 1000 most frequent words results in identifying 2 verb and 5 noun clusters, compared to our method’s 9 verb and 16 noun clusters. This is because the most frequent words tend to be clustered in a f"
P15-2143,P15-1135,1,0.900225,"ince the soft clustering they provide may be advantageous over the hard Brown and BMMM clusters. But it is known that unsupervised HMMs may not find good POS tags (Johnson, 2007), and in future work, more sophisticated models (e.g. Blunsom and Cohn (2011)), might outperform the systems we use here. In all cases, we assume that we can identify punctuation marks, which are moved to their own cluster and ignored for the purposes of tagging and parsing evaluation. Our Models Our goal in this work will be to produce labeled dependencies from raw text. Our approach is based on the HDP-CCG parser of Bisk and Hockenmaier (2015) with their extensions to capture lexicalization and punctuation, which, to our knowledge, is the only unsupervised approach to produce labeled dependencies. It first induces a CCG from POS-tagged text, and then estimates a model based on Hierarchical Dirichlet Processes (Teh et al., 2006) over the induced parse forests. The HDP model uses a hyperparameter which controls the amount of smoothing to the base measure of the HDP. Setting this value will prove important when moving between datasets of drastically different sizes. The induction algorithm assumes that a) verbs may be predicates (with"
P15-2143,D10-1117,0,0.0299658,"lusters are very noisy. We show here, for the first time, that very limited human supervision (three frequent words per cluster) may be required to induce labeled dependencies from automatically induced word clusters. 1 Introduction Despite significant progress on inducing part-of-speech (POS) tags from raw text (Christodoulopoulos et al., 2010; Blunsom and Cohn, 2011) and a small number of notable exceptions (Seginer, 2007; Spitkovsky et al., 2011; Christodoulopoulos et al., 2012), most approaches to grammar induction or unsupervised parsing (Klein and Manning, 2004; Spitkovsky et al., 2013; Blunsom and Cohn, 2010) are based on the assumption that gold POS tags are available to the induction system. Although most approaches treat these POS tags as arbitrary, if relatively clean, clusters, it has also been shown that the linguistic knowledge implicit in these tags can be exploited in a more explicit fashion (Naseem et al., 2010). The presence of POS tags is also essential for approaches that aim to return richer structures than the standard unlabeled dependencies. Boonkwan and Steedman (2011) train a parser that uses a semi-automatically constructed Combinatory Categorial Grammar (CCG, Steedman (2000)) l"
P15-2143,W12-1909,0,0.113004,"verb or other, and that this small amount of knowledge is sufficient to produce labeled syntactic structures from raw text, something that has not yet been proposed in the literature. Specifically, we will provide a labeled evaluation of induced CCG parsers against the English (Hockenmaier and Steedman, 2007) and Chinese (Tse, 2013) CCGbanks. To provide a direct comparison to the dependency induction literature, we will also provide an unlabeled evaluation on the 10 dependency corpora that were used for the task of grammar induction from raw text in the PASCAL Challenge on Grammar Induction (Gelling et al., 2012). The system of Christodoulopoulos et al. (2012) was the only participant competing in the PASCAL Challenge that operated over raw text (instead of gold POS tags). However, their approach did not outperform the six baseline systems provided. These baselines were two versions of the DMV model (Klein and Manning, 2004; Gillenwater et al., 2011) run on varying numbers of induced Brown clusters (described in section 2.1). We will therefore compare against these baselines in our evaluation. Outside of the shared task, Spitkovsky et al. (2011) demonstrated impressive performance using Brown clusters"
P15-2143,P11-1087,0,0.352693,"aluation Nearly all work in unsupervised grammar induction aims to induce unlabeled dependency trees from gold part-of-speechtagged text. These clean linguistic classes provide a very important, though unrealistic, inductive bias. Conversely, induced clusters are very noisy. We show here, for the first time, that very limited human supervision (three frequent words per cluster) may be required to induce labeled dependencies from automatically induced word clusters. 1 Introduction Despite significant progress on inducing part-of-speech (POS) tags from raw text (Christodoulopoulos et al., 2010; Blunsom and Cohn, 2011) and a small number of notable exceptions (Seginer, 2007; Spitkovsky et al., 2011; Christodoulopoulos et al., 2012), most approaches to grammar induction or unsupervised parsing (Klein and Manning, 2004; Spitkovsky et al., 2013; Blunsom and Cohn, 2010) are based on the assumption that gold POS tags are available to the induction system. Although most approaches treat these POS tags as arbitrary, if relatively clean, clusters, it has also been shown that the linguistic knowledge implicit in these tags can be exploited in a more explicit fashion (Naseem et al., 2010). The presence of POS tags is"
P15-2143,I11-1049,0,0.0144577,"2012), most approaches to grammar induction or unsupervised parsing (Klein and Manning, 2004; Spitkovsky et al., 2013; Blunsom and Cohn, 2010) are based on the assumption that gold POS tags are available to the induction system. Although most approaches treat these POS tags as arbitrary, if relatively clean, clusters, it has also been shown that the linguistic knowledge implicit in these tags can be exploited in a more explicit fashion (Naseem et al., 2010). The presence of POS tags is also essential for approaches that aim to return richer structures than the standard unlabeled dependencies. Boonkwan and Steedman (2011) train a parser that uses a semi-automatically constructed Combinatory Categorial Grammar (CCG, Steedman (2000)) lexicon for POS tags, 870 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 870–876, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics for languages other than English. The system we propose here will use a coarsegrained labeling comprised of three classes, which makes it substantially simpler than traditional tagsets,"
P15-2143,P06-1111,0,0.0389663,"ar (CCG, Steedman (2000)) lexicon for POS tags, 870 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 870–876, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics for languages other than English. The system we propose here will use a coarsegrained labeling comprised of three classes, which makes it substantially simpler than traditional tagsets, and uses far fewer labeled tokens than is customary for weakly-supervised approaches (Haghighi and Klein, 2006; Garrette et al., 2015). 2 to incorporate multiple types of features either at a token level (e.g. ±1 context word) or at a type level (e.g. morphology features derived from the Morfessor system (Creutz and Lagus, 2006)). The combination of these features allows BMMM to better capture morphosyntactic information. Bigram HMM We also evaluate unsupervised bigram HMMs, since the soft clustering they provide may be advantageous over the hard Brown and BMMM clusters. But it is known that unsupervised HMMs may not find good POS tags (Johnson, 2007), and in future work, more sophisticated models (e."
P15-2143,J92-4003,0,0.376418,"he universal tagset (Petrov et al., 2012) as our source of labels for the most frequent three words per cluster (we map the tags N OUN, N UM, P RON to noun, V ERB to verb, and all others to other). The final labeling is a majority vote, where each word type contributes a vote for each label it can take (see Table 4 for some examples). This approach could easily be scaled to allow more words per cluster to vote. But we will see that three per cluster is sufficient to label most tokens correctly. Inducing Word Clusters We will evaluate three clustering approaches: Brown Clusters Brown clusters (Brown et al., 1992) assign each word to a single cluster using an aglomerative clustering that maximizes the probability of the corpus under a bigram class conditional model. We use Liang’s implementation1 . BMMM The Bayesian Multinomial Mixture Model2 (BMMM, Christodoulopoulos et al. 2011) is also a hard clustering system, but has the ability 1 2 Identifying Noun and Verb Clusters 3 Experimental Setup We will focus first on producing CCG labeled predicate-argument dependencies for English and Chinese and will then apply our best settings to produce a comparison with the tree structures of the languages of the P"
P15-2143,C08-1042,0,0.0274284,"e importantly, we see that, at least for English, despite clear differences in tagging performance, the parsing results (LF1) are much more similar. In Chinese, we see that the performance of the two hard clustering systems is almost identical, again, not representative of the differences in the tagging scores. The N/V/O recall scores in both languages are equally poor predictors of parsing performance. However, these scores show that having only three labeled tokens per class is sufficient to capture most of the necessary distinctions for the HDP-CCG. All of this confirms the observations of Headden et al. (2008) that POS tagging metrics are not correlated with parsing performance. However, since BMMM seems to have a slight overall advantage, we will be using it as our clustering system for the remaining experiments. Since the goal of this work was to produce labeled syntactic structures, we also wanted to evaluate our performance against that of the HDPCCG system that uses gold-standard POS tags. As we can see in the last two columns of our development results in Table 1 and in the final test results of Table 2, our system is within 2/3 of the labeled performance of the gold-POS-based HDP-CCG3 . Figu"
P15-2143,D10-1056,1,0.955774,"n clusters but did not provide evaluation Nearly all work in unsupervised grammar induction aims to induce unlabeled dependency trees from gold part-of-speechtagged text. These clean linguistic classes provide a very important, though unrealistic, inductive bias. Conversely, induced clusters are very noisy. We show here, for the first time, that very limited human supervision (three frequent words per cluster) may be required to induce labeled dependencies from automatically induced word clusters. 1 Introduction Despite significant progress on inducing part-of-speech (POS) tags from raw text (Christodoulopoulos et al., 2010; Blunsom and Cohn, 2011) and a small number of notable exceptions (Seginer, 2007; Spitkovsky et al., 2011; Christodoulopoulos et al., 2012), most approaches to grammar induction or unsupervised parsing (Klein and Manning, 2004; Spitkovsky et al., 2013; Blunsom and Cohn, 2010) are based on the assumption that gold POS tags are available to the induction system. Although most approaches treat these POS tags as arbitrary, if relatively clean, clusters, it has also been shown that the linguistic knowledge implicit in these tags can be exploited in a more explicit fashion (Naseem et al., 2010). Th"
P15-2143,D11-1059,1,0.933502,"Missing"
P15-2143,P02-1043,1,0.613983,"y used because undirected dependencies do not penalize argument vs. adjunct distinctions, e.g. for prepositional phrases. For this reason we will include UF1 in the final test set evaluation (Table 2). We use the published train/dev/test splits, using the dev set for choosing a cluster induction algorithm, and then present final performance on the test data. We induce 36 tags for English and 37 for Chinese to match the number of tags present in the treebanks (excluding symbol and punctuation tags). 3 To put this result into its full perspective, the LF1 performance of a supervised CCG system (Hockenmaier and Steedman, 2002), HWDep model, trained on the same length-20 dataset and tested on the simplified CCGbank test set is 80.3. 872 English Chinese This Gold VM N / V / O 26.0 / 51.1 10.3 / 33.5 37.1 / 64.9 15.6 / 39.8 This ST @15 BH Czech2500 42 86 / 67 / 67 9.49 33.2 12.2 50.7 English2500 59 87 / 76 / 85 43.8 24.4 51.6 62.9 CHILDES2500 68 84 / 97 / 89 47.2 42.2 47.5 73.3 Portuguese2500 55 88 / 81 / 69 55.5 31.7 55.8 70.5 Table 2: CCG parsing performance (LF1/UF1) on Dutch1000 50 81 / 81 / 82 39.9 33.7 43.8 54.4 the test set with and without gold tags. Basque1000 52 2 / 78 / 95 31.1 28.7 35.2 45.0 hertz equipmen"
P15-2143,W12-1913,1,0.944329,"mount of knowledge is sufficient to produce labeled syntactic structures from raw text, something that has not yet been proposed in the literature. Specifically, we will provide a labeled evaluation of induced CCG parsers against the English (Hockenmaier and Steedman, 2007) and Chinese (Tse, 2013) CCGbanks. To provide a direct comparison to the dependency induction literature, we will also provide an unlabeled evaluation on the 10 dependency corpora that were used for the task of grammar induction from raw text in the PASCAL Challenge on Grammar Induction (Gelling et al., 2012). The system of Christodoulopoulos et al. (2012) was the only participant competing in the PASCAL Challenge that operated over raw text (instead of gold POS tags). However, their approach did not outperform the six baseline systems provided. These baselines were two versions of the DMV model (Klein and Manning, 2004; Gillenwater et al., 2011) run on varying numbers of induced Brown clusters (described in section 2.1). We will therefore compare against these baselines in our evaluation. Outside of the shared task, Spitkovsky et al. (2011) demonstrated impressive performance using Brown clusters but did not provide evaluation Nearly all work"
P15-2143,J07-3004,1,0.822289,"er, assuming clean POS tags is highly unrealistic for most scenarios in which one would wish to use an otherwise unsupervised parser. In this paper we demonstrate that the simple “universal” knowledge of Bisk and Hockenmaier (2013) can be easily applied to induced clusters given a small number of words labeled as noun, verb or other, and that this small amount of knowledge is sufficient to produce labeled syntactic structures from raw text, something that has not yet been proposed in the literature. Specifically, we will provide a labeled evaluation of induced CCG parsers against the English (Hockenmaier and Steedman, 2007) and Chinese (Tse, 2013) CCGbanks. To provide a direct comparison to the dependency induction literature, we will also provide an unlabeled evaluation on the 10 dependency corpora that were used for the task of grammar induction from raw text in the PASCAL Challenge on Grammar Induction (Gelling et al., 2012). The system of Christodoulopoulos et al. (2012) was the only participant competing in the PASCAL Challenge that operated over raw text (instead of gold POS tags). However, their approach did not outperform the six baseline systems provided. These baselines were two versions of the DMV mod"
P15-2143,D07-1031,0,0.093085,"tomary for weakly-supervised approaches (Haghighi and Klein, 2006; Garrette et al., 2015). 2 to incorporate multiple types of features either at a token level (e.g. ±1 context word) or at a type level (e.g. morphology features derived from the Morfessor system (Creutz and Lagus, 2006)). The combination of these features allows BMMM to better capture morphosyntactic information. Bigram HMM We also evaluate unsupervised bigram HMMs, since the soft clustering they provide may be advantageous over the hard Brown and BMMM clusters. But it is known that unsupervised HMMs may not find good POS tags (Johnson, 2007), and in future work, more sophisticated models (e.g. Blunsom and Cohn (2011)), might outperform the systems we use here. In all cases, we assume that we can identify punctuation marks, which are moved to their own cluster and ignored for the purposes of tagging and parsing evaluation. Our Models Our goal in this work will be to produce labeled dependencies from raw text. Our approach is based on the HDP-CCG parser of Bisk and Hockenmaier (2015) with their extensions to capture lexicalization and punctuation, which, to our knowledge, is the only unsupervised approach to produce labeled depende"
P15-2143,P02-1042,1,0.575205,"dard POS tags. As we can see in the last two columns of our development results in Table 1 and in the final test results of Table 2, our system is within 2/3 of the labeled performance of the gold-POS-based HDP-CCG3 . Figure 1 shows an example labeled syntactic structure induced by the model. We can see the system successfully learns to attach the final Experiment 1: CCG-based Evaluation Experimental Setup For our primary experiments, we train and test our systems on the English and Chinese CCGbanks, and report directed labeled F1 (LF1) and undirected unlabeled F1 (UF1) over CCG dependencies (Clark et al., 2002). For the labeled evaluation, we follow the simplification of CCGbank categories proposed by Bisk and Hockenmaier (2015): for English to remove morphosyntactic features, map NP to N and change VP modifiers (SNP)|(SNP) to sentential modifiers (S|S); for Chinese we map both M and QP to N. In the CCG literature, UF1 is commonly used because undirected dependencies do not penalize argument vs. adjunct distinctions, e.g. for prepositional phrases. For this reason we will include UF1 in the final test set evaluation (Table 2). We use the published train/dev/test splits, using the dev set for choos"
P15-2143,D10-1120,0,0.0620728,"doulopoulos et al., 2010; Blunsom and Cohn, 2011) and a small number of notable exceptions (Seginer, 2007; Spitkovsky et al., 2011; Christodoulopoulos et al., 2012), most approaches to grammar induction or unsupervised parsing (Klein and Manning, 2004; Spitkovsky et al., 2013; Blunsom and Cohn, 2010) are based on the assumption that gold POS tags are available to the induction system. Although most approaches treat these POS tags as arbitrary, if relatively clean, clusters, it has also been shown that the linguistic knowledge implicit in these tags can be exploited in a more explicit fashion (Naseem et al., 2010). The presence of POS tags is also essential for approaches that aim to return richer structures than the standard unlabeled dependencies. Boonkwan and Steedman (2011) train a parser that uses a semi-automatically constructed Combinatory Categorial Grammar (CCG, Steedman (2000)) lexicon for POS tags, 870 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 870–876, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics for languages othe"
P15-2143,petrov-etal-2012-universal,0,0.02056,"and then use very minimal supervision to identify noun and verb clusters. 2.1 2.2 To induce CCGs from induced clusters, we need to label them as {noun, verb, other}. This needs to be done judiciously; providing every cluster the verb label, for example, leads to the model identifying prepositions as the main sentential predicates. We demonstrate here that labeling three frequent words per cluster is sufficient to outperform state-of-the-art performance on grammar induction from raw text in many languages. We emulate having a native speaker annotate words for us by using the universal tagset (Petrov et al., 2012) as our source of labels for the most frequent three words per cluster (we map the tags N OUN, N UM, P RON to noun, V ERB to verb, and all others to other). The final labeling is a majority vote, where each word type contributes a vote for each label it can take (see Table 4 for some examples). This approach could easily be scaled to allow more words per cluster to vote. But we will see that three per cluster is sufficient to label most tokens correctly. Inducing Word Clusters We will evaluate three clustering approaches: Brown Clusters Brown clusters (Brown et al., 1992) assign each word to a"
P15-2143,D07-1043,0,0.0218062,"g punctuation). All cluster induction algorithms are treated as black boxes https://github.com/percyliang/brown-cluster https://github.com/christos-c/bmmm 871 and run over the complete datasets in advance. This alleviates having to handle tagging of unknown words. To provide an intuition for the performance of the induced word clusters, we provide two standard metrics for unsupervised tagging: Many-to-one (M-1) A commonly used measure, M-1 relies on mapping each cluster to the most common POS tag of its words. However, M1 can be easily inflated by inducing more clusters. V-Measure Proposed by Rosenberg and Hirschberg (2007), V-Measure (VM) measures the information-theoretic distance between two clusterings and has been shown to be robust to the number of induced clusters (Christodoulopoulos et al., 2010). Both of these metrics are known to be highly dependent on the gold annotation standards they are compared against, and may not correlate with downstream performance at parsing. Of more immediate relevance to our task is the ability to accurately identify nouns and verbs: Noun, Verb, and Other Recall We measure the (token-based) recall of our three-way labeling scheme of clusters as noun/verb/other against the u"
P15-2143,P07-1049,0,0.0352674,"induce unlabeled dependency trees from gold part-of-speechtagged text. These clean linguistic classes provide a very important, though unrealistic, inductive bias. Conversely, induced clusters are very noisy. We show here, for the first time, that very limited human supervision (three frequent words per cluster) may be required to induce labeled dependencies from automatically induced word clusters. 1 Introduction Despite significant progress on inducing part-of-speech (POS) tags from raw text (Christodoulopoulos et al., 2010; Blunsom and Cohn, 2011) and a small number of notable exceptions (Seginer, 2007; Spitkovsky et al., 2011; Christodoulopoulos et al., 2012), most approaches to grammar induction or unsupervised parsing (Klein and Manning, 2004; Spitkovsky et al., 2013; Blunsom and Cohn, 2010) are based on the assumption that gold POS tags are available to the induction system. Although most approaches treat these POS tags as arbitrary, if relatively clean, clusters, it has also been shown that the linguistic knowledge implicit in these tags can be exploited in a more explicit fashion (Naseem et al., 2010). The presence of POS tags is also essential for approaches that aim to return richer"
P15-2143,D11-1118,0,0.0615173,"from raw text in the PASCAL Challenge on Grammar Induction (Gelling et al., 2012). The system of Christodoulopoulos et al. (2012) was the only participant competing in the PASCAL Challenge that operated over raw text (instead of gold POS tags). However, their approach did not outperform the six baseline systems provided. These baselines were two versions of the DMV model (Klein and Manning, 2004; Gillenwater et al., 2011) run on varying numbers of induced Brown clusters (described in section 2.1). We will therefore compare against these baselines in our evaluation. Outside of the shared task, Spitkovsky et al. (2011) demonstrated impressive performance using Brown clusters but did not provide evaluation Nearly all work in unsupervised grammar induction aims to induce unlabeled dependency trees from gold part-of-speechtagged text. These clean linguistic classes provide a very important, though unrealistic, inductive bias. Conversely, induced clusters are very noisy. We show here, for the first time, that very limited human supervision (three frequent words per cluster) may be required to induce labeled dependencies from automatically induced word clusters. 1 Introduction Despite significant progress on ind"
P15-2143,D13-1204,0,0.0177016,"as. Conversely, induced clusters are very noisy. We show here, for the first time, that very limited human supervision (three frequent words per cluster) may be required to induce labeled dependencies from automatically induced word clusters. 1 Introduction Despite significant progress on inducing part-of-speech (POS) tags from raw text (Christodoulopoulos et al., 2010; Blunsom and Cohn, 2011) and a small number of notable exceptions (Seginer, 2007; Spitkovsky et al., 2011; Christodoulopoulos et al., 2012), most approaches to grammar induction or unsupervised parsing (Klein and Manning, 2004; Spitkovsky et al., 2013; Blunsom and Cohn, 2010) are based on the assumption that gold POS tags are available to the induction system. Although most approaches treat these POS tags as arbitrary, if relatively clean, clusters, it has also been shown that the linguistic knowledge implicit in these tags can be exploited in a more explicit fashion (Naseem et al., 2010). The presence of POS tags is also essential for approaches that aim to return richer structures than the standard unlabeled dependencies. Boonkwan and Steedman (2011) train a parser that uses a semi-automatically constructed Combinatory Categorial Grammar"
P15-2143,W15-1511,0,0.017097,"roves a beneficial signal. We have also marked two classes with * to draw the reader’s attention to a fully noun cluster in English and an other cluster in Chinese which are highly ambiguous. Specifically, in both of these cases the frequent words also occur frequently as verbs, providing additional motivation for a better soft-clustering algorithm in future work. How to most effectively use seed knowledge and annotation is still an open question. Approaches range from labeling frequent words like the work of Garrette and Baldridge (2013) to the recently introduced active learning approach of Stratos and Collins (2015). In this work, we were able to demonstrate high noun and verb recall with the use of a very small set of labeled words because they correspond to an existing clustering. In contrast, we found that labeling even the 1000 most frequent words led to very few clusters being correctly identified; e.g. in English, using the 1000 most frequent words results in identifying 2 verb and 5 noun clusters, compared to our method’s 9 verb and 16 noun clusters. This is because the most frequent words tend to be clustered in a few very large clusters resulting in low coverage. Stratos and Collins (2015) demon"
P15-2143,Q13-1007,1,\N,Missing
P19-1537,N16-1089,0,0.138347,"Missing"
P19-1537,D18-1547,0,0.0313547,"Missing"
P19-1537,P12-3009,0,0.0313016,"context dialogue where users interact in a chat room while viewing a live-streamed video (Pasunuru and Bansal, 2018). It requires the ability to refer to real-world objects and spatial relations that depend on the current position of the speakers as well as changes in the environment. Due to the expense of actual human-robot communication (e.g. Tellex et al., 2011; Thomason et al., ∗ Both authors equally contributed to the paper. 2015; Misra et al., 2016; Chai et al., 2018), simulated environments that allow easier experimentation are commonly used (Koller et al., 2010; Chen and Mooney, 2011; Janarthanam et al., 2012). In this paper, we therefore introduce the Minecraft Collaborative Building Task, in which pairs of users control avatars in the Minecraft virtual environment and collaboratively build 3D structures in a Blocks World-like scenario while communicating solely via text chat (Section 3). We have built a data collection platform and have used it to collect the Minecraft Dialogue Corpus, consisting of 509 human-human written dialogues, screenshots and complete game logs for this task (Section 4). While our ultimate goal is to develop fully interactive agents that can collaborate with humans success"
P19-1537,W15-4640,0,0.0151846,"s for this task, we consider the subtask of Architect utterance generation, and show how challenging it is. 1 Introduction Building interactive agents that can successfully communicate with humans about the physical world around them to collaboratively solve tasks in this environment is a long-sought goal of AI (e.g. Winograd, 1971). Such situated dialogue poses challenges that go beyond what is required for the slot-value filling tasks performed by standard dialogue systems (e.g. Kim et al., 2016, 2017; Budzianowski et al., 2018) or chatbots (e.g. Ritter et al., 2010; Schrading et al., 2015; Lowe et al., 2015), as well as for so-called visual dialogue where users talk about a static image (Das et al., 2017) or video-context dialogue where users interact in a chat room while viewing a live-streamed video (Pasunuru and Bansal, 2018). It requires the ability to refer to real-world objects and spatial relations that depend on the current position of the speakers as well as changes in the environment. Due to the expense of actual human-robot communication (e.g. Tellex et al., 2011; Thomason et al., ∗ Both authors equally contributed to the paper. 2015; Misra et al., 2016; Chai et al., 2018), simulated e"
P19-1537,P02-1040,0,0.103683,"ch on the validation set for our best model. 9 Results and Analysis We evaluate our models in three ways: we use automated metrics to assess how closely the generated utterances match the human utterances. For a random sample of 100 utterances per model, we use human evaluators to identify dialogue acts and to evaluate whether the generated utterances are correct in the given game context. Finally, we perform a qualitative analysis of our best model. 9.1 Automated Evaluation Metrics To evaluate how closely the generated utterances resemble the human utterances, we report standard BLEU scores (Papineni et al., 2002). We also compute (modified) precision and recall of a number of lists of domain-specific keywords that are instrumental to task success: colors, spatial relations, and other words that are highly indicative of dialogue acts (e.g., responding “yes” vs. “no”, instructing to “place” vs. “remove”, etc.). These lists also capture synonyms that are common in our data (e.g. “yes”/“yeah”), and were obtained by curating non-overlapping lists of words (with a frequency ≥ 10 across all data splits) that are appropriate to each category.2 We report precision and recall scores per category, and for an “al"
P19-1537,P17-1086,0,0.0511733,"n the creation of structures rather than navigation around existing ones. Koller et al. (2010) design a challenge where systems with access to symbolic world rep1 http://juliahmr.cs.illinois.edu/Minecraft 5405 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5405–5415 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics resentations and a route planner generate real-time instructions to guide users through a treasure hunt in a virtual 3D world. There is a resurgence of interest in Blocks World-like scenarios. Wang et al. (2017) let users define 3D voxel structures via a highly programmatic natural language. The interface learns to understand descriptions of increasing complexity, but does not engage in a back-and-forth dialogue with the user. Most closely related to our work are the corpora of Bisk et al. (2018, 2016a,b), which feature pairs of scenes involving simulated, uniquely labeled, 3D blocks annotated with single-shot instructions aimed at guiding an (imaginary) partner on how to transform an input scene into the target. In their scenario, the building area is always viewed from a fixed bird’s-eye perspectiv"
P19-1537,D18-1012,0,0.0648787,"orld around them to collaboratively solve tasks in this environment is a long-sought goal of AI (e.g. Winograd, 1971). Such situated dialogue poses challenges that go beyond what is required for the slot-value filling tasks performed by standard dialogue systems (e.g. Kim et al., 2016, 2017; Budzianowski et al., 2018) or chatbots (e.g. Ritter et al., 2010; Schrading et al., 2015; Lowe et al., 2015), as well as for so-called visual dialogue where users talk about a static image (Das et al., 2017) or video-context dialogue where users interact in a chat room while viewing a live-streamed video (Pasunuru and Bansal, 2018). It requires the ability to refer to real-world objects and spatial relations that depend on the current position of the speakers as well as changes in the environment. Due to the expense of actual human-robot communication (e.g. Tellex et al., 2011; Thomason et al., ∗ Both authors equally contributed to the paper. 2015; Misra et al., 2016; Chai et al., 2018), simulated environments that allow easier experimentation are commonly used (Koller et al., 2010; Chen and Mooney, 2011; Janarthanam et al., 2012). In this paper, we therefore introduce the Minecraft Collaborative Building Task, in which"
P19-1537,D14-1162,0,0.0846312,"ing them to the word embedding vector that is fed into the decoder at each time step (Figure 2). 8 Experimental Setup Data Our training, test and dev splits contain 6,548, 2,855, and 2,251 Architect utterances. Training We trained for a maximum of 40 epochs using the Adam optimizer (Kingma and Ba, 2015). During training, we minimize the sum of the cross entropy losses between each predicted and ground truth token. We stop training early when perplexity on the held-out validation set had increased monotonically for two epochs. All word embeddings were initialized with pretrained GloVe vectors (Pennington et al., 2014). We first performed grid search over model architecture hyperparameters (embedding layer sizes and RNN layer depths). Once the best-performing architecture was found, we then varied dropout parameters (Srivastava et al., 2014). More details can be found in the supplementary materials. Decoding We use beam search decoding to generate the utterance with the maximum loglikelihood score according to our model normalized by utterance length (beam size = 10). In order to promote diversity of generated utterances, we use a γ penalty (Li et al., 2016) of γ = 0.8. These parameters were found by a grid"
P19-1537,N10-1020,0,0.0847923,"ur goal of developing fully interactive agents for this task, we consider the subtask of Architect utterance generation, and show how challenging it is. 1 Introduction Building interactive agents that can successfully communicate with humans about the physical world around them to collaboratively solve tasks in this environment is a long-sought goal of AI (e.g. Winograd, 1971). Such situated dialogue poses challenges that go beyond what is required for the slot-value filling tasks performed by standard dialogue systems (e.g. Kim et al., 2016, 2017; Budzianowski et al., 2018) or chatbots (e.g. Ritter et al., 2010; Schrading et al., 2015; Lowe et al., 2015), as well as for so-called visual dialogue where users talk about a static image (Das et al., 2017) or video-context dialogue where users interact in a chat room while viewing a live-streamed video (Pasunuru and Bansal, 2018). It requires the ability to refer to real-world objects and spatial relations that depend on the current position of the speakers as well as changes in the environment. Due to the expense of actual human-robot communication (e.g. Tellex et al., 2011; Thomason et al., ∗ Both authors equally contributed to the paper. 2015; Misra e"
P19-1537,D15-1309,0,0.022292,"fully interactive agents for this task, we consider the subtask of Architect utterance generation, and show how challenging it is. 1 Introduction Building interactive agents that can successfully communicate with humans about the physical world around them to collaboratively solve tasks in this environment is a long-sought goal of AI (e.g. Winograd, 1971). Such situated dialogue poses challenges that go beyond what is required for the slot-value filling tasks performed by standard dialogue systems (e.g. Kim et al., 2016, 2017; Budzianowski et al., 2018) or chatbots (e.g. Ritter et al., 2010; Schrading et al., 2015; Lowe et al., 2015), as well as for so-called visual dialogue where users talk about a static image (Das et al., 2017) or video-context dialogue where users interact in a chat room while viewing a live-streamed video (Pasunuru and Bansal, 2018). It requires the ability to refer to real-world objects and spatial relations that depend on the current position of the speakers as well as changes in the environment. Due to the expense of actual human-robot communication (e.g. Tellex et al., 2011; Thomason et al., ∗ Both authors equally contributed to the paper. 2015; Misra et al., 2016; Chai et al."
Q13-1007,P10-1131,0,0.208844,"Missing"
Q13-1007,W12-1912,1,0.0650657,"5 Transactions of the Association for Computational Linguistics, 1 (2013) 75–88. Action Editor: Sharon Goldwater. c Submitted 11/2012; Revised 1/2013; Published 3/2013. 2013 Association for Computational Linguistics. the algorithm has identified the basic syntactic properties of the language, it is hence sufficient to inspect the induced lexicon. Conversely, Boonkwan and Steedman (2011) show that knowledge of these basic syntactic properties makes it very easy to create a language-specific lexicon for accurate unsupervised CCG parsing. We have recently proposed an algorithm for inducing CCGs (Bisk and Hockenmaier, 2012b) that has been shown to be competitive with other approaches even when paired with a very simple probability model (Gelling et al., 2012). In this paper, we pair this induction algorithm with a novel nonparametric Bayesian model that is based on a different factorization of CCG derivations, and show that it outperforms our original model and many other approaches on a large number of languages. Our results indicate that the use of CCG yields grammars that are significantly more robust when dealing with longer sentences than most dependency grammar-based approaches. 2 Spanish word ordering di"
Q13-1007,D10-1117,0,0.623291,"t al., 2011; Cohen and Smith, 2010) and learning regimes (Spitkovsky et al., 2010), as well as the incorporation of prior linguistic knowledge (Cohen and Smith, 2009; BergKirkpatrick and Klein, 2010; Naseem et al., 2010) can lead to significant improvement over Klein and Manning’s baseline model. The use of dependency grammars circumvents the question of how to obtain an appropriate inventory of categories, since dependency parses are simply defined by unlabeled edges between the lexical items in the sentence. But dependency grammars make it also difficult to capture non-local structures, and Blunsom and Cohn (2010) show that it may be advantageous to reformulate the underlying dependency grammar in terms of a tree-substitution grammar (TSG) which pairs words with treelets that specify the number of left and right dependents they have. In this paper, we explore yet another option: instead of dependency grammars, we use Combinatory Categorial Grammar (CCG, Steedman (1996; 2000)), a linguistically expressive formalism that pairs lexical items with rich categories that capture all language-specific information. This may seem a puzzling choice, since CCG requires a significantly larger inventory of categorie"
Q13-1007,I11-1049,0,0.839559,"ile the elementary trees of Blunsom and Cohn (2010)’s TSG and their internal nodel labels have no obvious linguistic interpretation, the syntactic behavior of any CCG constituent can be directly inferred from its category. To see whether 75 Transactions of the Association for Computational Linguistics, 1 (2013) 75–88. Action Editor: Sharon Goldwater. c Submitted 11/2012; Revised 1/2013; Published 3/2013. 2013 Association for Computational Linguistics. the algorithm has identified the basic syntactic properties of the language, it is hence sufficient to inspect the induced lexicon. Conversely, Boonkwan and Steedman (2011) show that knowledge of these basic syntactic properties makes it very easy to create a language-specific lexicon for accurate unsupervised CCG parsing. We have recently proposed an algorithm for inducing CCGs (Bisk and Hockenmaier, 2012b) that has been shown to be competitive with other approaches even when paired with a very simple probability model (Gelling et al., 2012). In this paper, we pair this induction algorithm with a novel nonparametric Bayesian model that is based on a different factorization of CCG derivations, and show that it outperforms our original model and many other approa"
Q13-1007,W06-2920,0,0.033859,"ng and testing our models on the corpora containing sentences up to length 15 used in this paper takes between one minute to at most three hours on a single 12-core machine depending on their size. 6 Evaluation As is standard for this task, we evaluate our systems against a number of different dependency treebanks, 80 and measure performance in terms of the accuracy of directed dependencies (i.e. the percentage of words in the test corpus that are correctly attached). We use the data from the PASCAL challenge for grammar induction (Gelling et al., 2012), the data from the CoNLL-X shared task (Buchholz and Marsi, 2006) and Goldberg (2011)’s Hebrew corpus. Converting CCG derivations into dependencies is mostly straightforward, since the CCG derivation identifies the root word of each sentence, and headargument and head-modifier dependencies are easily read off of CCG derivations, since the lexicon defines them explicitly. Unlike dependency grammar, CCG is designed to recover non-local dependencies that arise in control and binding constructions as well as in wh-extraction and non-standard coordination, but since this requires re-entrancies, or coindexation of arguments (Hockenmaier and Steedman, 2007), withi"
Q13-1007,P07-1032,0,0.0218822,"ned to recover non-local dependencies that arise in control and binding constructions as well as in wh-extraction and non-standard coordination, but since this requires re-entrancies, or coindexation of arguments (Hockenmaier and Steedman, 2007), within the lexical categories that trigger these constructions, our current system returns only local dependencies. But since dependency grammars also captures only local dependencies, this has no negative influence on our current evaluation. However, a direct comparison between dependency treebanks and dependencies produced by CCG is more difficult (Clark and Curran, 2007), since dependency grammars allow considerable freedom in how to analyze specific constructions such as verb clusters (which verb is the head?) prepositional phrases and particles (is the head the noun or the preposition/particle?), subordinating conjunctions (is the conjunction a dependent of the head of the main clause and the head of the embedded clause a dependent of the conjunction, or vice versa?) and this is reflected in the fact that the treebanks we consider often apply different conventions for these cases. Although remedying this issue is beyond the scope of this work, these discrep"
Q13-1007,P09-2001,0,0.0183375,"uch as X-bar theory (Jackendoff, 1977), these nonterminals are essentially arbitrary labels that can be combined in arbitrary ways. While further CFG-based approaches have been proposed (Clark, 2001; Kurihara and Sato, 2004), most recent work has followed Klein and Manning (2004) in developing models for the induction of projective dependency grammars. It has been shown that more sophisticated probability models (Headden III et al., 2009; Gillenwater et al., 2011; Cohen and Smith, 2010) and learning regimes (Spitkovsky et al., 2010), as well as the incorporation of prior linguistic knowledge (Cohen and Smith, 2009; BergKirkpatrick and Klein, 2010; Naseem et al., 2010) can lead to significant improvement over Klein and Manning’s baseline model. The use of dependency grammars circumvents the question of how to obtain an appropriate inventory of categories, since dependency parses are simply defined by unlabeled edges between the lexical items in the sentence. But dependency grammars make it also difficult to capture non-local structures, and Blunsom and Cohn (2010) show that it may be advantageous to reformulate the underlying dependency grammar in terms of a tree-substitution grammar (TSG) which pairs w"
Q13-1007,J03-4003,0,0.0471421,"). Since our system has explicit rules for coordination, we transform its output into the desired target representation that is specific to each language. 7 Experiments We evaluate our system on 13 different languages. In each case, we follow the test and training regimes that were used to obtain previously published results in order to allow a direct comparison. We compare our system to the results presented at the PASCAL Challenge on Grammar Induction (Gelling et al., 2012)6 , as well as to Gillenwater et al. (2011) and Naseem et al. (2012). We use Nivre (2006)’s Penn2Malt implementation of Collins (2003)’s head rules to translate the WSJ Penn Treebank (Marcus et al., 1993) into dependencies. Finally, when training the MLE version of our model we use a simple smoothing scheme which defines a small rule probability (e−15 ) to prevent any rule used during training from going to zero. 7.1 PASCAL Challenge on Grammar Induction In Table 1, we compare the performance of the basic Argument model (MLE), of our HDP model with four different settings of the hyperparameters (as explained above) and of the systems presented in the PASCAL Challenge on Grammar Induction (Gelling et al., 2012). The systems i"
Q13-1007,P09-1041,0,0.0262232,"Three of these constraints correspond to our rules that verbs are the roots of sentences and may take nouns as dependents, but the other ten constraints (e.g. that adjectives modify nouns, adverbs modify adjectives or verbs, etc.) have no equivalent in our system. Although our system has less prior knowledge, it still performs competitively. On the WSJ, Naseem et al. demonstrate the importance and effect of the specific choice of syntactic rules by comparing the performance of their system with hand crafted universal rules (71.9), with English specific rules (73.8), and with rules proposed by Druck et al. (2009) (64.9). The performance of Naseem et al.’s system drops very significantly as sentence length (and presumable parse complexity) Sl 3.8K Es 4.2K Da 9.5K Pt 15K Sv 24K #Tokens Sl 3,857 Es 4,230 Da 9,549 Pt Sv 15,015 24,021 50.9 56.6 67.2 62.1 51.9 51.5 71.5 74.7 63.3 69.8 G10 HDP 51.2 57.9 62.4 65.4 47.2 49.3 54.3 73.5 Table 2: A comparison of our system with Naseem et al. (2010), both trained and tested on the length 10 training data from the CoNLL-X Shared Task. #Tokens Bg WSJ Nl Ja De 38,220 42,442 43,405 43,501 77,705 G10 HDP 59.8 66.1 increases, whereas our system shows significantly less"
Q13-1007,P96-1011,0,0.0442545,"good , the, eating, ...}pendencies, and our grammar is thus weakly equivalent to the dependency grammar representations that SN : {sleeps, ate, eating, ...} (SN)/N : {sees, ate, ...} are commonly used for grammar induction. The SS : {quickly, today...} (SN)/(SN) : {good , the, ...} main role of composition in our fragment is that it To draw a simple contrast, in Spanish we would allows sentential and verb modifiers to both take catexpect adjectives to take the category NN because egories of the form SS and S/S. Composition in76 troduces spurious ambiguities, which we eliminate by using Eisner (1996)’s normal form.1 Coordinating conjunctions have a special category conj, and we binarize coordination as follows (Hockenmaier and Steedman, 2007): X conj 3 X[conj] X ⇒&1 ⇒&2 X (&1) X[conj] (&2) The DT N/N Category induction Unlike dependency grammars, CCG requires an inventory of lexical categories. Given a set of lexical categories, the combinatory rules define the set of parses for each sentence. We follow the algorithm proposed by Bisk and Hockenmaier (2012b) to automatically induce these categories. The lexicon is initialized by pairing all nominal tags (nouns, pronouns and determiners) wi"
Q13-1007,W12-1909,0,0.545152,"/2013; Published 3/2013. 2013 Association for Computational Linguistics. the algorithm has identified the basic syntactic properties of the language, it is hence sufficient to inspect the induced lexicon. Conversely, Boonkwan and Steedman (2011) show that knowledge of these basic syntactic properties makes it very easy to create a language-specific lexicon for accurate unsupervised CCG parsing. We have recently proposed an algorithm for inducing CCGs (Bisk and Hockenmaier, 2012b) that has been shown to be competitive with other approaches even when paired with a very simple probability model (Gelling et al., 2012). In this paper, we pair this induction algorithm with a novel nonparametric Bayesian model that is based on a different factorization of CCG derivations, and show that it outperforms our original model and many other approaches on a large number of languages. Our results indicate that the use of CCG yields grammars that are significantly more robust when dealing with longer sentences than most dependency grammar-based approaches. 2 Spanish word ordering dictates that the adjective follow the noun. The lexical categories also capture word-word dependencies: head-argument relations are captured"
Q13-1007,P10-2036,0,0.101782,"e complexity) Sl 3.8K Es 4.2K Da 9.5K Pt 15K Sv 24K #Tokens Sl 3,857 Es 4,230 Da 9,549 Pt Sv 15,015 24,021 50.9 56.6 67.2 62.1 51.9 51.5 71.5 74.7 63.3 69.8 G10 HDP 51.2 57.9 62.4 65.4 47.2 49.3 54.3 73.5 Table 2: A comparison of our system with Naseem et al. (2010), both trained and tested on the length 10 training data from the CoNLL-X Shared Task. #Tokens Bg WSJ Nl Ja De 38,220 42,442 43,405 43,501 77,705 G10 HDP 59.8 66.1 increases, whereas our system shows significantly less decline, and outperforms their universal system by a significant margin.7 Table 3: A comparison of our system with Gillenwater et al. (2010), both trained on the length 10 training data, and tested on the length 10 test data, from the CoNLL-X Shared task. ∼#Tokens N10 HDP Naseem Universal Rules Naseem English Rules HDP-CCG HDP-CCG (train ≤ 20) ≤ 10 71.9 73.8 68.2 64.4 70.3 47.5 56.2 60.2 64.1 48.6 73.2 47.4 68.4 ≤ 20 50.4 66.1 64.2 71.9 In contrast to Spitkovsky et al. (2010), who reported that performance of their dependency based system degrades when trained on longer sentences, our performance on length 10 sentences increases to 71.9 when we train on sentences up to length 20. Another system that is also based on CCG, but captu"
Q13-1007,N09-1012,0,0.532861,"Missing"
Q13-1007,C10-1053,1,0.878206,"ordinating conjunctions that appear in the middle of sentences) to modify an immediate neighbor that has the category S or N or is a modifier (S|S or N|N) itself. quickly RB - Complex lexical categories are induced by considering the local context in which tokens appear. Given an input sentence, and a current lexicon which assigns categories to at least some of the tokens in the sentence, we apply the following two rules to add new categories to the lexicon: The argument rule allows any lexical tokens that have categories other than N and conj to take immediately adjacent 1 The normal-form of Hockenmaier and Bisk (2010) is not required for this fragment of CCG. 2 This distinction was suggested by the authors (p.c.) 77 man NNS N, S/S ate VBD S, NN SN quickly RB SS These rules can be applied iteratively to form more complex categories. We restrict lexical categories to a maximal arity of 2, and disallow the category (S/N)N, since it is equivalent to (SN)/N. The man ate quickly DT NNS VBD RB N/N, N, S/S S, NN, SS, (S/S)/(S/S)(NN)/(NN) SN (NN)(NN) (N/N)(N/N) (S/S)(S/S) (SS)/(SS) The resultant, overly general, lexicon is then used to parse the training data. Each complete parse has to be of categ"
Q13-1007,P02-1043,1,0.687165,"uivalent to (SN)/N. The man ate quickly DT NNS VBD RB N/N, N, S/S S, NN, SS, (S/S)/(S/S)(NN)/(NN) SN (NN)(NN) (N/N)(N/N) (S/S)(S/S) (SS)/(SS) The resultant, overly general, lexicon is then used to parse the training data. Each complete parse has to be of category S or N, with the constraint that sentences that contain a main verb can only form parses of category S. 4 A new probability model for CCG Generative models define the probability of a parse tree τ as the product of individual rule probabilities. Our previous work (Bisk and Hockenmaier, 2012b) uses the most basic model of Hockenmaier and Steedman (2002), which first generates the head direction (left, right, unary, or lexical), followed by the head category, and finally the sister category. 3 This factorization does not take advantage of the unique functional nature of CCG. We therefore introduce a new factorization we call the Argument Model. It exploits the fact that CCG imposes strong constraints on a category’s left and right children, since these must combine to create the parent type via one of the combinators. In practice this means that given the parent X/Z, the choice of combinator4 c and an argument Y we can uniquely determine the"
Q13-1007,J07-3004,1,0.544489,"N : {sleeps, ate, eating, ...} (SN)/N : {sees, ate, ...} are commonly used for grammar induction. The SS : {quickly, today...} (SN)/(SN) : {good , the, ...} main role of composition in our fragment is that it To draw a simple contrast, in Spanish we would allows sentential and verb modifiers to both take catexpect adjectives to take the category NN because egories of the form SS and S/S. Composition in76 troduces spurious ambiguities, which we eliminate by using Eisner (1996)’s normal form.1 Coordinating conjunctions have a special category conj, and we binarize coordination as follows (Hockenmaier and Steedman, 2007): X conj 3 X[conj] X ⇒&1 ⇒&2 X (&1) X[conj] (&2) The DT N/N Category induction Unlike dependency grammars, CCG requires an inventory of lexical categories. Given a set of lexical categories, the combinatory rules define the set of parses for each sentence. We follow the algorithm proposed by Bisk and Hockenmaier (2012b) to automatically induce these categories. The lexicon is initialized by pairing all nominal tags (nouns, pronouns and determiners) with the category N, all verb tags with the category S, and coordinating conjunctions with the category conj: CONJ DET, NOUN, NUM, PRON VERB → → →"
Q13-1007,C12-1077,0,0.0113964,"eft, right, unary, or lexical), followed by the head category, and finally the sister category. 3 This factorization does not take advantage of the unique functional nature of CCG. We therefore introduce a new factorization we call the Argument Model. It exploits the fact that CCG imposes strong constraints on a category’s left and right children, since these must combine to create the parent type via one of the combinators. In practice this means that given the parent X/Z, the choice of combinator4 c and an argument Y we can uniquely determine the categories of the left and right children: 3 Huang et al. (2012) present a (deficient) variant and Bayesian extension of the Bisk and Hockenmaier (2012b) model without k-best smoothing that both underperform our published results. 4 If X is an atomic category, only application is possible. Parent c X/Z B0&gt; B0< B1&gt; B1< ⇒ Left Right (X/Z)/Y Y X/Y Y/Z Y (X/Z)Y Y/Z XY and correspondingly for X: Parent c X B0&gt; B0< B1&gt; B1< ⇒ Left Right (X)/Y Y X/Y Y Y (X)Y Y XY While type-changing and raising are not used in this work the model’s treatment of root productions extends easily to handle these other unary cases. We simply treat the argument Y as the"
Q13-1007,P04-1061,0,0.653834,"tion is appropriate for unsupervised grammar induction? Initial attempts with context-free grammars (CFGs) were not very successful (Carroll and Charniak, 1992; Charniak, 1993). One reason may be that CFGs require the specification of a finite inventory of nonterminal categories and rewrite rules, but unless one adopts linguistic principles such as X-bar theory (Jackendoff, 1977), these nonterminals are essentially arbitrary labels that can be combined in arbitrary ways. While further CFG-based approaches have been proposed (Clark, 2001; Kurihara and Sato, 2004), most recent work has followed Klein and Manning (2004) in developing models for the induction of projective dependency grammars. It has been shown that more sophisticated probability models (Headden III et al., 2009; Gillenwater et al., 2011; Cohen and Smith, 2010) and learning regimes (Spitkovsky et al., 2010), as well as the incorporation of prior linguistic knowledge (Cohen and Smith, 2009; BergKirkpatrick and Klein, 2010; Naseem et al., 2010) can lead to significant improvement over Klein and Manning’s baseline model. The use of dependency grammars circumvents the question of how to obtain an appropriate inventory of categories, since depende"
Q13-1007,E12-1024,0,0.0255517,"responds to the mean of the DP, and a concentration or shape parameter α. In a Hierarchical Dirichlet Process (Teh et al., 2006), there is a hierarchy of DPs, such that the base distribution of a DP at level n is a DP at level n − 1. The HDP-CCG (Figure 1) is a reformulation of the Argument Model introduced above in terms of Hierarchical Dirichlet Processes.5 At the heart of the model is a distribution over CCG categories. By combining a stick breaking process with a multinomial over categories we can define a DP over CCG 5 An alternative HDP model for semantic parsing with CCG is proposed by Kwiatkowski et al. (2012). HDP-CCG 1) Draw global parameters Define MLE root parameter θTOP Draw top-level symbol weights β Y ∼ GEM(αY ) Draw top-level lexical weights β L ∼ GEM(αL ) For each grammar symbol z ∈ {1, 2, ...}: Define MLE rule type parameters θzT Draw argument parameters φYz ∼ DP(αY , β Y ) L L Draw lexical emission parameters φL z ∼ DP(α , β ) For each grammar symbol y ∈ {1, 2, ...}: C Define MLE combinator parameters θz,y βY θT zTOP ∼ Binomial(θTOP ) ti ∼ Multinomial(θzTi ) xi ∼ Multinomial(φL zi ) yi ∼ Multinomial(φYzi ) ci ∼ Multinomial(θzCi ,yi ) (and zR(i) if binary) yi ci zL(i) zR(i) xL(i) xR(i) θC"
Q13-1007,D07-1072,0,0.0550937,"ears to be superlinear but subquadratic in n, we present results where p takes the values 0, 1.0, 1.5, and 2.0 to explore the range from uniform to quadratic. This setting for α is the only free parameter in the model. By controlling precision we can tell the model to what extent global corpus statistics should be trusted. We believe this has a similar effect to Bisk and Hockenmaier (2012b)’s top-k upweighting and smoothing scheme. One advantage of the argument model is that it only requires a single distribution over categories for each binary tree. In contrast to similar proposals for CFGs (Liang et al., 2007), which impose no formal restrictions on the nonterminals X, Y, Z that can appear in a rewrite rule X → Y Z, this greatly simplifies the modeling problem (yielding effectively a model that is more akin to nonparametric HMMs), since it avoids the need to capture correlations between different base distributions for Y and Z. Variational Inference HDPs need to be estimated with approximate techniques. As an alternative to Gibbs sampling (Teh et al., 2006), which is exact, but typically very slow and has no clear convergence criteria, variational inference algorithms (Bishop, 2006; Blei and Jordan"
Q13-1007,J93-2004,0,0.0435704,"sform its output into the desired target representation that is specific to each language. 7 Experiments We evaluate our system on 13 different languages. In each case, we follow the test and training regimes that were used to obtain previously published results in order to allow a direct comparison. We compare our system to the results presented at the PASCAL Challenge on Grammar Induction (Gelling et al., 2012)6 , as well as to Gillenwater et al. (2011) and Naseem et al. (2012). We use Nivre (2006)’s Penn2Malt implementation of Collins (2003)’s head rules to translate the WSJ Penn Treebank (Marcus et al., 1993) into dependencies. Finally, when training the MLE version of our model we use a simple smoothing scheme which defines a small rule probability (e−15 ) to prevent any rule used during training from going to zero. 7.1 PASCAL Challenge on Grammar Induction In Table 1, we compare the performance of the basic Argument model (MLE), of our HDP model with four different settings of the hyperparameters (as explained above) and of the systems presented in the PASCAL Challenge on Grammar Induction (Gelling et al., 2012). The systems in this competition were instructed to train over the full dataset, inc"
Q13-1007,W12-1911,0,0.0285689,"Missing"
Q13-1007,D10-1120,0,0.756918,"ls are essentially arbitrary labels that can be combined in arbitrary ways. While further CFG-based approaches have been proposed (Clark, 2001; Kurihara and Sato, 2004), most recent work has followed Klein and Manning (2004) in developing models for the induction of projective dependency grammars. It has been shown that more sophisticated probability models (Headden III et al., 2009; Gillenwater et al., 2011; Cohen and Smith, 2010) and learning regimes (Spitkovsky et al., 2010), as well as the incorporation of prior linguistic knowledge (Cohen and Smith, 2009; BergKirkpatrick and Klein, 2010; Naseem et al., 2010) can lead to significant improvement over Klein and Manning’s baseline model. The use of dependency grammars circumvents the question of how to obtain an appropriate inventory of categories, since dependency parses are simply defined by unlabeled edges between the lexical items in the sentence. But dependency grammars make it also difficult to capture non-local structures, and Blunsom and Cohn (2010) show that it may be advantageous to reformulate the underlying dependency grammar in terms of a tree-substitution grammar (TSG) which pairs words with treelets that specify the number of left and"
Q13-1007,P12-1066,0,0.00979494,"coordinating conjunctions which use different styles (not all shown here). Since our system has explicit rules for coordination, we transform its output into the desired target representation that is specific to each language. 7 Experiments We evaluate our system on 13 different languages. In each case, we follow the test and training regimes that were used to obtain previously published results in order to allow a direct comparison. We compare our system to the results presented at the PASCAL Challenge on Grammar Induction (Gelling et al., 2012)6 , as well as to Gillenwater et al. (2011) and Naseem et al. (2012). We use Nivre (2006)’s Penn2Malt implementation of Collins (2003)’s head rules to translate the WSJ Penn Treebank (Marcus et al., 1993) into dependencies. Finally, when training the MLE version of our model we use a simple smoothing scheme which defines a small rule probability (e−15 ) to prevent any rule used during training from going to zero. 7.1 PASCAL Challenge on Grammar Induction In Table 1, we compare the performance of the basic Argument model (MLE), of our HDP model with four different settings of the hyperparameters (as explained above) and of the systems presented in the PASCAL Ch"
Q13-1007,petrov-etal-2012-universal,0,0.0158812,"Missing"
Q13-1007,W12-1910,0,0.0113558,"different settings of the hyperparameters (as explained above) and of the systems presented in the PASCAL Challenge on Grammar Induction (Gelling et al., 2012). The systems in this competition were instructed to train over the full dataset, including the unlabelled test data, and include Bisk and Hockenmaier (2012a)’s CCG-based system (BH) to Cohn et al. (2010)’s reimplementation of Klein and Manning (2004)’s DMV model in a tree-substitution grammar framework (BC), as well as three other dependency based systems which either incorporate Naseem et al. (2010)’s rules in a deterministic fashion (Søgaard, 2012), rely on extensive tuning on 6 Numbers are from personal correspondence with the organizers. The previously published numbers are not comparable to literature due to an error in the evaluation. http://wiki. cs.ox.ac.uk/InducingLinguisticStructure/ ResultsDepComparable 81 the development set (Tu, 2012) or incorporate millions of additional tokens from Wikipedia to estimate model parameters (Marecek and Zabokrtsky, 2012). We ignore punctuation for all experiments reported in this paper, but since the training data (but not the evaluation) includes punctuation marks, participants were free to ch"
Q13-1007,N10-1116,0,0.156409,"onterminal categories and rewrite rules, but unless one adopts linguistic principles such as X-bar theory (Jackendoff, 1977), these nonterminals are essentially arbitrary labels that can be combined in arbitrary ways. While further CFG-based approaches have been proposed (Clark, 2001; Kurihara and Sato, 2004), most recent work has followed Klein and Manning (2004) in developing models for the induction of projective dependency grammars. It has been shown that more sophisticated probability models (Headden III et al., 2009; Gillenwater et al., 2011; Cohen and Smith, 2010) and learning regimes (Spitkovsky et al., 2010), as well as the incorporation of prior linguistic knowledge (Cohen and Smith, 2009; BergKirkpatrick and Klein, 2010; Naseem et al., 2010) can lead to significant improvement over Klein and Manning’s baseline model. The use of dependency grammars circumvents the question of how to obtain an appropriate inventory of categories, since dependency parses are simply defined by unlabeled edges between the lexical items in the sentence. But dependency grammars make it also difficult to capture non-local structures, and Blunsom and Cohn (2010) show that it may be advantageous to reformulate the underl"
Q13-1007,P06-1124,0,0.0419046,"ut uniquely defines the two children of a parent node. We will see below that this greatly simplifies the development of non-parametric extensions. 5 HDP-CCG: a nonparametric model Simple generative models such as PCFGs or Bisk and Hockenmaier (2012b)’s CCG model are not robust in the face of sparsity, since they assign zero probability to any unseen event. Sparsity is a particular problem for formalisms like CCG that have a rich inventory of object types. Nonparametric Bayesian models, e.g. Dirichlet Processes (Teh, 2010) or their hierarchical variants (Teh et al., 2006) and generalizations (Teh, 2006) overcome this problem in a very elegant manner, and are used by many state-of-the-art grammar induction systems 78 (Naseem et al., 2010; Blunsom and Cohn, 2010; Boonkwan and Steedman, 2011). They also impose a rich-getting-richer behavior that seems to be advantageous in many modeling applications. By contrast, Bisk and Hockenmaier (2012b) propose a weighted top-k scheme to address these issues in an ad-hoc manner. The argument model introduced above lends itself particularly well to nonparametric extensions such as the standard Hierarchical Dirichlet Processes (HDP). In this work the size of"
Q13-1007,W12-1915,0,0.0303865,"Missing"
Q14-1006,S12-1051,0,0.0063742,"s purely syntactic and lexical rules to produce simpler captions (which have a larger denotation). But since each image is originally associated with several captions, the graph can also capture similarities between syntactically and lexically unrelated descriptions. We apply these similarities to two different tasks (Sections 6 and 7): an approximate entailment recognition task for our domain, where the goal is to decide whether the hypothesis (a brief image caption) refers to the same image as the premises (four longer captions), and the recently introduced Semantic Textual Similarity task (Agirre et al., 2012), which can be viewed as a graded (rather than binary) version of paraphrase detection. Both tasks require semantic inference, and our results indicate that denotational similarities are at least as effective as standard approaches to similarity. Our code and data set, as well as the denotation graph itself and the lexical similarities we define over it are available for research purposes at http://nlp.cs.illinois.edu/ Denotation.html. 2 2.1 Towards Denotational Similarities Distributional Similarities The distributional hypothesis posits that linguistic expressions that appear in similar cont"
Q14-1006,P13-4021,0,0.00686106,"Missing"
Q14-1006,J12-1003,0,0.0578116,"mages or video with descriptive language, but we have not yet applied our approach to them. Chen and Dolan (2011)’s MSR Video Description Corpus (of which the STS data is a subset) is most similar to ours, but its curated part is significantly smaller. Instead of several independent captions, Grubinger et al. (2006)’s IAPR TC-12 data set contains longer descriptions. Ordonez et al. (2011) harvested 1 million images and their user-generated captions from Flickr to create the SBU Captioned Photo Dataset. These captions tend to be less descriptive of the image. The denotation graph is similar to Berant et al. (2012)’s ‘entailment graph’, but differs from it in two ways: first, entailment relations in the denotation graph are defined extensionally in terms of the images described by the expressions at each node, and second, nodes in Berant et al.’s entailment graph correspond to generic propositional templates (X treats Y), whereas nodes in our denotation graph correspond to complete propositions (a dog runs). Acknowledgements We gratefully acknowledge the support of the National Science Foundation under NSF awards 0803603 “INT2-Medium: Understanding the meaning of images”, 1053856 “CAREER: Bayesian Model"
Q14-1006,P11-1020,0,0.496628,"we believe that they should also be of great interest for people who aim to build systems that automatically associate image with sentences that describe them (Farhadi et al., 2010; Kulkarni et al., 2011; Li et al., 2011; Yang et al., 2011; Mitchell et al., 2012; Kuznetsova et al., 2012; Gupta et al., 2012; Hodosh et al., 2013). Related Work and Resources We believe that the work reported in this paper has the potential to open up promising new research directions. There are other data sets that pair images or video with descriptive language, but we have not yet applied our approach to them. Chen and Dolan (2011)’s MSR Video Description Corpus (of which the STS data is a subset) is most similar to ours, but its curated part is significantly smaller. Instead of several independent captions, Grubinger et al. (2006)’s IAPR TC-12 data set contains longer descriptions. Ordonez et al. (2011) harvested 1 million images and their user-generated captions from Flickr to create the SBU Captioned Photo Dataset. These captions tend to be less descriptive of the image. The denotation graph is similar to Berant et al. (2012)’s ‘entailment graph’, but differs from it in two ways: first, entailment relations in the de"
Q14-1006,W04-3205,0,0.0398013,"ity (Lin): cos(w, w0 ) Lin(w, w0 ) = = P w·w0 kwkkw0 k w(i)+w0 (i) P 0 w(i)+ i i w (i) i:w(i)>0∧w0 (i)>0 P We use two directed lexical similarities: Clarke (2009)’s similarity (Clk), and Szpektor and Dagan (2008)’s balanced precision (Bal), which builds on Lin and on Weeds and Weir (2003)’s similarity (W): 0 = 0 Bal(w |w ) = W(w |w0 ) = Clk(w |w ) P i:w(i)>0∧w0 (i)>0 P min(w(i), w0 (i)) i w(i) p 0 W(w |w ) × Lin(w, w0 ) P i:w(i)>0∧w0 (i)>0 w(i) P i w(i) We also use two publicly available resources that provide precomputed similarities, Kotlerman et al. (2010)’s D IRECT noun and verb rules and Chklovski and Pantel (2004)’s V ERB O CEAN rules. Both are motivated by the need for numerically quantifiable semantic inferences between predicates. We only use entries that correspond to single tokens (ignoring e.g. phrasal verbs). Each lexical similarity results in the following features: words in the output are represented by a max-simw feature which captures its maximum similarity with any word in the premises (max-simw = maxw0 ∈P sim(w, w0 )) and by a sum-simw feature which captures the sum of its similarities to the words in the premises (sum-simw = P 0 w0 ∈P sim(w, w )). Global max sim and sum sim features captu"
Q14-1006,J90-1003,0,0.121366,"that reduces the string s0 to s (i.e. ω(s0 ) = s) and its inverse ω −1 expands the string s to s0 (i.e. ω −1 (s) = s0 ). 2.4 Denotational Similarities Given a denotation graph over N images, we estimate the denotational probability of an expression s with a denotation of size |JsK |as PJK (s) = |JsK|/N , and the joint probability of two expressions analogously as PJK (s, s0 ) = |JsK ∩ Js0 K|/N . The conditional probability PJK (s |s0 ) indicates how likely s is to be true when s0 holds, and yields a simple directed denotational similarity. The (normalized) pointwise mutual information (PMI) (Church and Hanks, 1990) defines a symmetric similarity:   PJK (s,s0 ) log P (s)P 0 JK JK (s ) nPMI JK (s, s0 ) = − log(PJK (s, s0 )) We set PJK (s|s) = nPMI JK (s, s) = 1, and, if s or are not in the denotation graph, nPMI JK (s, s0 ) = PJK (s, s0 ) = 0. s0 3 Our Data Set Our data set (Figure 1) consists of 31,783 photographs of everyday activities, events and scenes (all harvested from Flickr) and 158,915 captions (obtained via crowdsourcing). It contains and extends Hodosh et al. (2013)’s corpus of 8,092 images. We followed Hodosh et al. (2013)’s approach to collect images. We also use their annotation guideline"
Q14-1006,W09-0215,0,0.0132178,"rom our data set and their five captions similar meaning (Harris, 1954). This has led to the definition of vector-based distributional similarities, which represent each word w as a vector w derived from counts of w’s co-occurrence with other words. These vectors can be used directly to compute the lexical similarities of words, either via the cosine of the angle between them, or via other, more complex metrics (Lin, 1998). More recently, asymmetric similarities have been proposed as more suitable for semantic inference tasks such as entailment (Weeds and Weir, 2003; Szpektor and Dagan, 2008; Clarke, 2009; Kotlerman et al., 2010). Distributional word vectors can also be used to define the compositional similarity of longer strings (Mitchell and Lapata, 2010). To compute the similarity of two strings, the lexical vectors of the words in each string are first combined into a single vector (e.g. by element-wise addition or multiplication), and then an appropriate vector similarity (e.g. cosine) is applied to the resulting pair of vectors. 2.2 Visual Denotations Our approach is inspired by truth-conditional semantic theories in which the denotation of a declarative sentence is assumed to be the se"
Q14-1006,W10-2920,1,0.20528,"Missing"
Q14-1006,P12-1038,0,0.0906233,"ptive captions, and a denotation graph that pairs generalized versions of these captions with their visual denotations, i.e. the sets of images they describe. Both of these resources are freely available (http://nlp.cs.illinois.edu/ Denotation.html) Although the aim of this paper is to show their utility for a purely linguistic task, we believe that they should also be of great interest for people who aim to build systems that automatically associate image with sentences that describe them (Farhadi et al., 2010; Kulkarni et al., 2011; Li et al., 2011; Yang et al., 2011; Mitchell et al., 2012; Kuznetsova et al., 2012; Gupta et al., 2012; Hodosh et al., 2013). Related Work and Resources We believe that the work reported in this paper has the potential to open up promising new research directions. There are other data sets that pair images or video with descriptive language, but we have not yet applied our approach to them. Chen and Dolan (2011)’s MSR Video Description Corpus (of which the STS data is a subset) is most similar to ours, but its curated part is significantly smaller. Instead of several independent captions, Grubinger et al. (2006)’s IAPR TC-12 data set contains longer descriptions. Ordonez et"
Q14-1006,W11-0326,0,0.509884,"ew resources: a large data set of images paired with descriptive captions, and a denotation graph that pairs generalized versions of these captions with their visual denotations, i.e. the sets of images they describe. Both of these resources are freely available (http://nlp.cs.illinois.edu/ Denotation.html) Although the aim of this paper is to show their utility for a purely linguistic task, we believe that they should also be of great interest for people who aim to build systems that automatically associate image with sentences that describe them (Farhadi et al., 2010; Kulkarni et al., 2011; Li et al., 2011; Yang et al., 2011; Mitchell et al., 2012; Kuznetsova et al., 2012; Gupta et al., 2012; Hodosh et al., 2013). Related Work and Resources We believe that the work reported in this paper has the potential to open up promising new research directions. There are other data sets that pair images or video with descriptive language, but we have not yet applied our approach to them. Chen and Dolan (2011)’s MSR Video Description Corpus (of which the STS data is a subset) is most similar to ours, but its curated part is significantly smaller. Instead of several independent captions, Grubinger et al. (2"
Q14-1006,C08-1066,0,0.0125432,"tation of s0 (JsK ⊆ Js0 K), and we say that s0 subsumes the more specific s (s0 v s). In our domain of descriptive sentences, we can obtain more generic descriptions by simple syntactic and lexical operations ω ∈ O ⊂ S × S that preserve upward entailment, so that if ω(s) = s0 , JsK ⊆ Js0 K. We consider three types of operations: the removal of optional material (e.g PPs like on the beach), the extraction of simpler constituents (NPs, VPs, or simple Ss), and lexical substitutions of nouns by their hypernyms (poodle → dog). These operations are akin to the atomic edits of MacCartney and Manning (2008)’s NatLog system, and allow us to construct large subsumption hierarchies over image descriptions, which we call denotation graphs. Given a set of (upward entailment-preserving) operations O ⊂ S × S, the denotation graph DG = hE, V i of a set of images I and a set of strings S represents a subsumption hierarchy in which each node V = hs, JsKi corresponds to a string s ∈ S and its denotation JsK ⊆ I. Directed edges e = (s, s0 ) ∈ E ⊆ V × V indicate a subsumption relation s v s0 between a more generic expression s and its child s0 . An edge from s to s0 exists if there is an operation ω ∈ O that"
Q14-1006,E09-1064,0,0.0230646,"re on when constituent x in h is equal to the string s and whose value is equal to the constituent-based feature. For PJK , each constituent (and each constituent-node Q pair) has an additional feature P (h|P ) = 1 − n (1 − PJK (h|pn )) that estimates the probability that h is generated by some node in the premise. Lexical Similarity Features We use two symmetric lexical similarities: standard cosine distance (cos), and Lin (1998)’s similarity (Lin): cos(w, w0 ) Lin(w, w0 ) = = P w·w0 kwkkw0 k w(i)+w0 (i) P 0 w(i)+ i i w (i) i:w(i)>0∧w0 (i)>0 P We use two directed lexical similarities: Clarke (2009)’s similarity (Clk), and Szpektor and Dagan (2008)’s balanced precision (Bal), which builds on Lin and on Weeds and Weir (2003)’s similarity (W): 0 = 0 Bal(w |w ) = W(w |w0 ) = Clk(w |w ) P i:w(i)>0∧w0 (i)>0 P min(w(i), w0 (i)) i w(i) p 0 W(w |w ) × Lin(w, w0 ) P i:w(i)>0∧w0 (i)>0 w(i) P i w(i) We also use two publicly available resources that provide precomputed similarities, Kotlerman et al. (2010)’s D IRECT noun and verb rules and Chklovski and Pantel (2004)’s V ERB O CEAN rules. Both are motivated by the need for numerically quantifiable semantic inferences between predicates. We only use"
Q14-1006,E12-1076,0,0.609917,"ages paired with descriptive captions, and a denotation graph that pairs generalized versions of these captions with their visual denotations, i.e. the sets of images they describe. Both of these resources are freely available (http://nlp.cs.illinois.edu/ Denotation.html) Although the aim of this paper is to show their utility for a purely linguistic task, we believe that they should also be of great interest for people who aim to build systems that automatically associate image with sentences that describe them (Farhadi et al., 2010; Kulkarni et al., 2011; Li et al., 2011; Yang et al., 2011; Mitchell et al., 2012; Kuznetsova et al., 2012; Gupta et al., 2012; Hodosh et al., 2013). Related Work and Resources We believe that the work reported in this paper has the potential to open up promising new research directions. There are other data sets that pair images or video with descriptive language, but we have not yet applied our approach to them. Chen and Dolan (2011)’s MSR Video Description Corpus (of which the STS data is a subset) is most similar to ours, but its curated part is significantly smaller. Instead of several independent captions, Grubinger et al. (2006)’s IAPR TC-12 data set contains longer"
Q14-1006,nivre-etal-2006-maltparser,0,0.0176897,"Missing"
Q14-1006,S01-1035,0,0.0386914,"es as “persons”, we only allow terms that are likely to describe adults or teenagers (including occupations) to be replaced by the term “person”. This means that the term “girl” has two senses: a female child (the default) or a younger woman. We distinguish the two senses in a preprocessing step: if the other captions of the same image do not mention children, but refer to teenaged or adult women, we assign girl the woman-sense. Some nouns that end in -er (e.g. “diner”, “pitcher” also violate our monosemy assumption. 2 Coreference resolution has also been used for word sense disambiguation by Preiss (2001) and Hu and Liu (2011). 70 ating the denotation graph. In order to distinguish between frequently occurring homonyms where the noun is unrelated to the verb, we change all forms of the verb dress to dressed, all forms of the verb stand to standing and all forms of the verb park to parking. Finally, we drop sentence-initial there/here/this is/are (as in there is a dog splashing in the water), and normalize the expressions in X and dressed (up) in X (where X is an article of clothing or a color) to wear X. We reduce plural determiners to {two, three, some}, and drop singular determiners except f"
Q14-1006,C08-1107,0,0.0424251,"or. Figure 1: Two images from our data set and their five captions similar meaning (Harris, 1954). This has led to the definition of vector-based distributional similarities, which represent each word w as a vector w derived from counts of w’s co-occurrence with other words. These vectors can be used directly to compute the lexical similarities of words, either via the cosine of the angle between them, or via other, more complex metrics (Lin, 1998). More recently, asymmetric similarities have been proposed as more suitable for semantic inference tasks such as entailment (Weeds and Weir, 2003; Szpektor and Dagan, 2008; Clarke, 2009; Kotlerman et al., 2010). Distributional word vectors can also be used to define the compositional similarity of longer strings (Mitchell and Lapata, 2010). To compute the similarity of two strings, the lexical vectors of the words in each string are first combined into a single vector (e.g. by element-wise addition or multiplication), and then an appropriate vector similarity (e.g. cosine) is applied to the resulting pair of vectors. 2.2 Visual Denotations Our approach is inspired by truth-conditional semantic theories in which the denotation of a declarative sentence is assume"
Q14-1006,S12-1060,0,0.0165303,"Missing"
Q14-1006,W03-1011,0,0.0240154,"he other hoses the floor. Figure 1: Two images from our data set and their five captions similar meaning (Harris, 1954). This has led to the definition of vector-based distributional similarities, which represent each word w as a vector w derived from counts of w’s co-occurrence with other words. These vectors can be used directly to compute the lexical similarities of words, either via the cosine of the angle between them, or via other, more complex metrics (Lin, 1998). More recently, asymmetric similarities have been proposed as more suitable for semantic inference tasks such as entailment (Weeds and Weir, 2003; Szpektor and Dagan, 2008; Clarke, 2009; Kotlerman et al., 2010). Distributional word vectors can also be used to define the compositional similarity of longer strings (Mitchell and Lapata, 2010). To compute the similarity of two strings, the lexical vectors of the words in each string are first combined into a single vector (e.g. by element-wise addition or multiplication), and then an appropriate vector similarity (e.g. cosine) is applied to the resulting pair of vectors. 2.2 Visual Denotations Our approach is inspired by truth-conditional semantic theories in which the denotation of a decl"
Q14-1006,D11-1041,0,0.110381,"arge data set of images paired with descriptive captions, and a denotation graph that pairs generalized versions of these captions with their visual denotations, i.e. the sets of images they describe. Both of these resources are freely available (http://nlp.cs.illinois.edu/ Denotation.html) Although the aim of this paper is to show their utility for a purely linguistic task, we believe that they should also be of great interest for people who aim to build systems that automatically associate image with sentences that describe them (Farhadi et al., 2010; Kulkarni et al., 2011; Li et al., 2011; Yang et al., 2011; Mitchell et al., 2012; Kuznetsova et al., 2012; Gupta et al., 2012; Hodosh et al., 2013). Related Work and Resources We believe that the work reported in this paper has the potential to open up promising new research directions. There are other data sets that pair images or video with descriptive language, but we have not yet applied our approach to them. Chen and Dolan (2011)’s MSR Video Description Corpus (of which the STS data is a subset) is most similar to ours, but its curated part is significantly smaller. Instead of several independent captions, Grubinger et al. (2006)’s IAPR TC-12 d"
S14-2055,D10-1115,0,0.047225,"e vector space, each dimension corresponds to one of the 1000 most frequent lemmas (contexts). The jth entry of the vector of wi is the positive normalized pointwise mutual information (pnPMI) between target wi and context wj :    P (wi ,wj ) log P (wi )P (wj )  pnPMI(wi , wj ) = max 0, − log (P (wi , wj )) We define P (wi ) as the fraction of images with at least one caption containing wi , and P (wi , wj ) as the fraction of images whose captions contain both wi and wj . Following recent work that extends distributional similarities to phrases and sentences (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, Our System Our system combines different sources of semantic similarity to predict semantic relatedness and This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/ by/4.0/ 1 http://nlp.stanford.edu/software/ corenlp.shtml 329 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 329–334, Dublin, Ireland, August 23-24, 2014. Features Description Negation Word overlap Denotational co"
S14-2055,D11-1129,0,0.0237952,"Missing"
S14-2055,S14-2001,0,0.150366,"onym relations. 1 2.1 We lemmatize all sentences with the Stanford CoreNLP system1 and extract syntactic chunks with the Illinois Chunker (Punyakanok and Roth, 2001). Like Young et al. (2014), we use the Malt parser (Nivre et al., 2006) to identify 5 sets of constituents for each sentence: subject NPs, verbs, VPs, direct object NPs, and other NPs. For stopwords, we use the NLTK English stopword list of 127 high-frequency words. We remove negation words (no, not, and nor) from the stopword list since their presence is informative for this dataset and task. Task Description SemEval 2014 Task 1 (Marelli et al., 2014a) evaluates system predictions of semantic relatedness (SR) and textual entailment (TE) relations on sentence pairs from the SICK dataset (Marelli et al., 2014b). The dataset is intended to test compositional knowledge without requiring the world knowledge that is often required for paraphrase classification or Recognizing Textual Entailment tasks. SR scores range from 1 to 5. TE relations are ‘entailment,’ ‘contradiction,’ and ‘neutral.’ Our system uses features that depend on the amount of word overlap and alignment between the two sentences, the presence of negation, and the semantic simil"
S14-2055,marelli-etal-2014-sick,0,0.255951,"onym relations. 1 2.1 We lemmatize all sentences with the Stanford CoreNLP system1 and extract syntactic chunks with the Illinois Chunker (Punyakanok and Roth, 2001). Like Young et al. (2014), we use the Malt parser (Nivre et al., 2006) to identify 5 sets of constituents for each sentence: subject NPs, verbs, VPs, direct object NPs, and other NPs. For stopwords, we use the NLTK English stopword list of 127 high-frequency words. We remove negation words (no, not, and nor) from the stopword list since their presence is informative for this dataset and task. Task Description SemEval 2014 Task 1 (Marelli et al., 2014a) evaluates system predictions of semantic relatedness (SR) and textual entailment (TE) relations on sentence pairs from the SICK dataset (Marelli et al., 2014b). The dataset is intended to test compositional knowledge without requiring the world knowledge that is often required for paraphrase classification or Recognizing Textual Entailment tasks. SR scores range from 1 to 5. TE relations are ‘entailment,’ ‘contradiction,’ and ‘neutral.’ Our system uses features that depend on the amount of word overlap and alignment between the two sentences, the presence of negation, and the semantic simil"
S14-2055,P81-1022,0,0.67366,"Missing"
S14-2055,D12-1110,0,0.00546096,"unaligned chunk labels to number of chunks; number of matched labels; ratio of matched to unmatched chunk labels Number of matched synonym pairs (w1 , w2 ) Number of matched hypernym pairs (w1 , w2 ), number of matched hypernym pairs (w2 , w1 ) Number of matched antonym pairs (w1 , w2 ) Distributional constituent similarity Alignment Unaligned matching Chunk alignment Synonym Hypernym Antonym # of features 1 1 30 30 23 31 17 1 2 1 Table 1: Summary of features. and f2 are the features of the shorter sentence. These directional features are specified in the following feature descriptions. 2011; Socher et al., 2012), we define a phrase vector p to be the pointwise multiplication product of the vectors of the words in the phrase: p = w1 ... wn Negation In this dataset, contradictory sentence pairs are often marked by explicit negation, e.g. s1 = “The man is stirring the sauce for the chicken” and s2 = “The man is not stirring the sauce for the chicken.” A binary feature is set to 1 if either sentence contains not, no, or nobody, and set to 0 otherwise. where is the multiplication of corresponding vector components, i.e. pi = ui · vi . 2.3 Denotational Similarities In Young et al. (2014), we introduce deno"
S14-2055,Q14-1006,1,0.897792,"linois at Urbana-Champaign {aylai2, juliahmr}@illinois.edu Abstract textual entailment. We use distributional similarity features, denotational similarity features, and alignment features based on shallow syntactic structure. This paper describes and analyzes our SemEval 2014 Task 1 system. Its features are based on distributional and denotational similarities; word alignment; negation; and hypernym/hyponym, synonym, and antonym relations. 1 2.1 We lemmatize all sentences with the Stanford CoreNLP system1 and extract syntactic chunks with the Illinois Chunker (Punyakanok and Roth, 2001). Like Young et al. (2014), we use the Malt parser (Nivre et al., 2006) to identify 5 sets of constituents for each sentence: subject NPs, verbs, VPs, direct object NPs, and other NPs. For stopwords, we use the NLTK English stopword list of 127 high-frequency words. We remove negation words (no, not, and nor) from the stopword list since their presence is informative for this dataset and task. Task Description SemEval 2014 Task 1 (Marelli et al., 2014a) evaluates system predictions of semantic relatedness (SR) and textual entailment (TE) relations on sentence pairs from the SICK dataset (Marelli et al., 2014b). The dat"
S14-2055,nivre-etal-2006-maltparser,0,\N,Missing
W03-1008,P02-1043,1,0.469137,"res. The PropBank (Kingsbury and Palmer, 2002) and the FrameNet (Baker et al., 1998) projects both document the variation in syntactic realization of the arguments of predicates in general English text. Gildea and Palmer (2002) developed a system to predict semantic roles (as defined in PropBank) from sentences and their parse trees as determined by the statistical parser of Collins (1999). In this paper, we examine how the syntactic representations used by different statistical parsers affect the performance of such a system. We compare a parser based on Combinatory Categorial Grammar (CCG) (Hockenmaier and Steedman, 2002b) with the Collins parser. As the CCG parser is trained and tested on a corpus of CCG derivations that have been obtained by automatic conversion from the Penn Treebank, we are able to compare performance using both goldstandard and automatic parses for both CCG and the traditional Treebank representation. The Treebankparser returns skeletal phrase-structure trees without the traces or functional tags in the original Penn Treebank, whereas the CCG parser returns wordword dependencies that correspond to the underlying predicate-argument structure, including longrange dependencies arising throu"
W03-1008,P02-1018,0,0.0264799,"Missing"
W03-1008,kingsbury-palmer-2002-treebank,0,0.112779,"e semantic roles of sentence constituents is a crucial part of interpreting text, and in addition to forming an important part of the information extraction problem, can serve as an intermediate step in machine translation or automatic summarization. Even for a single predicate, semantic arguments can have multiple syntactic realizations, as shown by the following paraphrases: (1) (2) John will meet with Mary. John will meet Mary. John and Mary will meet. The door opened. Mary opened the door. Recently, attention has turned to creating corpora annotated with argument structures. The PropBank (Kingsbury and Palmer, 2002) and the FrameNet (Baker et al., 1998) projects both document the variation in syntactic realization of the arguments of predicates in general English text. Gildea and Palmer (2002) developed a system to predict semantic roles (as defined in PropBank) from sentences and their parse trees as determined by the statistical parser of Collins (1999). In this paper, we examine how the syntactic representations used by different statistical parsers affect the performance of such a system. We compare a parser based on Combinatory Categorial Grammar (CCG) (Hockenmaier and Steedman, 2002b) with the Coll"
W03-1008,P98-1013,0,0.0402206,"crucial part of interpreting text, and in addition to forming an important part of the information extraction problem, can serve as an intermediate step in machine translation or automatic summarization. Even for a single predicate, semantic arguments can have multiple syntactic realizations, as shown by the following paraphrases: (1) (2) John will meet with Mary. John will meet Mary. John and Mary will meet. The door opened. Mary opened the door. Recently, attention has turned to creating corpora annotated with argument structures. The PropBank (Kingsbury and Palmer, 2002) and the FrameNet (Baker et al., 1998) projects both document the variation in syntactic realization of the arguments of predicates in general English text. Gildea and Palmer (2002) developed a system to predict semantic roles (as defined in PropBank) from sentences and their parse trees as determined by the statistical parser of Collins (1999). In this paper, we examine how the syntactic representations used by different statistical parsers affect the performance of such a system. We compare a parser based on Combinatory Categorial Grammar (CCG) (Hockenmaier and Steedman, 2002b) with the Collins parser. As the CCG parser is train"
W03-1008,J02-3001,1,0.53218,"are available, which can be thought of as choosing the topmost distributions available from a backoff lattice, shown in Figure 3. P(r |h, pt, p) P(r |pt, path, p) P(r |h, p) P(r |h) P(r |pt, p) P(r |pt, pos, v, p) P(r |pt, pos, v) P(r |p) Figure 3: Backoff lattice with more specific distributions towards the top. The probabilities P (ri |Fi , p) are combined with the probabilities P ({r1..n }|p) for a set of roles appearing in a sentence given a predicate, using the following formula: P (r1..n |F1..n , p) ≈ P ({r1..n }|p) Y P (ri |Fi , p) i P (ri |p) This approach, described in more detail in Gildea and Jurafsky (2002), allows interaction between the role assignments for individual constituents while making certain independence assumptions necessary for efficient probability estimation. In particular, we assume that sets of roles appear independent of their linear order, and that the features F of a constituents are independent of other constituents’ features given the constituent’s role. 5.2 The model for CCG derivations In the CCG version, we replace the features above with corresponding features based on both the sentence’s CCG derivation tree (shown in Figure 1) and the CCG predicate-argument relations"
W03-1008,P02-1031,1,0.632647,"termediate step in machine translation or automatic summarization. Even for a single predicate, semantic arguments can have multiple syntactic realizations, as shown by the following paraphrases: (1) (2) John will meet with Mary. John will meet Mary. John and Mary will meet. The door opened. Mary opened the door. Recently, attention has turned to creating corpora annotated with argument structures. The PropBank (Kingsbury and Palmer, 2002) and the FrameNet (Baker et al., 1998) projects both document the variation in syntactic realization of the arguments of predicates in general English text. Gildea and Palmer (2002) developed a system to predict semantic roles (as defined in PropBank) from sentences and their parse trees as determined by the statistical parser of Collins (1999). In this paper, we examine how the syntactic representations used by different statistical parsers affect the performance of such a system. We compare a parser based on Combinatory Categorial Grammar (CCG) (Hockenmaier and Steedman, 2002b) with the Collins parser. As the CCG parser is trained and tested on a corpus of CCG derivations that have been obtained by automatic conversion from the Penn Treebank, we are able to compare per"
W03-1008,hockenmaier-steedman-2002-acquiring,1,\N,Missing
W03-1008,J03-4003,0,\N,Missing
W03-1008,C98-1013,0,\N,Missing
W06-1635,J98-2004,0,\N,Missing
W06-1635,N03-1016,0,\N,Missing
W06-1637,H94-1020,0,0.0505219,"rom the fact that categories merely encode unsatisfied subcategorized arguments. Given that a transitive verb has the same category as the constituent formed by a ditransitive verb and its direct object, we would expect that both categories can prime each other, if they are cognitive units. More generally, we would expect that lexical (terminal) and phrasal (non-terminal) categories of the same syntactic type may prime each other. The interaction of such conditions with the priming effect can be quantified in the statistical model. 3.3 4 Corpus Data 4.1 The Switchboard Corpus The Switchboard (Marcus et al., 1994) corpus contains transcriptions of spoken, spontaneous conversation annotated with phrase-structure trees. Dialogues were recorded over the telephone among randomly paired North American speakers, who were just given a general topic to talk about. 80,000 utterances of the corpus have been annotated with syntactic structure. This portion, included in the Penn Treebank, has been timealigned (per word) in the Paraphrase project (Carletta et al., 2004). Using the same regression technique as employed here, Reitter et al. (2006b) found a marked structural priming effect for Penn-Treebank style phra"
W06-1637,P94-1018,0,0.0435542,"syntactic structure. This portion, included in the Penn Treebank, has been timealigned (per word) in the Paraphrase project (Carletta et al., 2004). Using the same regression technique as employed here, Reitter et al. (2006b) found a marked structural priming effect for Penn-Treebank style phrase structure rules in Switchboard. Incrementality of Analyses Type-raising and composition allow derivations that are mostly left-branching, or incremental. Adopting a left-to-right processing order for a sentence is important, if the syntactic theory is to make psycholinguistically viable predictions (Niv, 1994; Steedman, 2000). Pickering et al. (2002) present priming experiments that suggest that, in production, structural dominance and linearization do not take place in different stages. Their argument involves verbal phrases with a shifted prepositional object such as showed to the mechanic a torn overall. At a dominance-only level, such phrases are equivalent to non-shifted prepositional constructions (showed a torn overall to the mechanic), but the two variants may be differentiated at a linearization stage. Shifted primes do not prime prepositional objects in their canonical position, thus pri"
W06-1637,carletta-etal-2004-using,0,0.0285793,"of such conditions with the priming effect can be quantified in the statistical model. 3.3 4 Corpus Data 4.1 The Switchboard Corpus The Switchboard (Marcus et al., 1994) corpus contains transcriptions of spoken, spontaneous conversation annotated with phrase-structure trees. Dialogues were recorded over the telephone among randomly paired North American speakers, who were just given a general topic to talk about. 80,000 utterances of the corpus have been annotated with syntactic structure. This portion, included in the Penn Treebank, has been timealigned (per word) in the Paraphrase project (Carletta et al., 2004). Using the same regression technique as employed here, Reitter et al. (2006b) found a marked structural priming effect for Penn-Treebank style phrase structure rules in Switchboard. Incrementality of Analyses Type-raising and composition allow derivations that are mostly left-branching, or incremental. Adopting a left-to-right processing order for a sentence is important, if the syntactic theory is to make psycholinguistically viable predictions (Niv, 1994; Steedman, 2000). Pickering et al. (2002) present priming experiments that suggest that, in production, structural dominance and lineariza"
W06-1637,P04-1014,0,0.0138117,"gle right-branching normal form derivation (Eisner, 1996) for each possible semantic interpretation. Such normal form derivations only use composition and type-raising where syntactically necessary (eg. in relative clauses). CCG (Steedman, 2000) is a mildly contextsensitive, lexicalized grammar formalism with a transparent syntax-semantics interface and a flexible constituent structure that is of particular interest to psycholinguistics, since it allows the construction of incremental derivations. CCG has also enjoyed the interest of the NLP community, with high-accuracy wide-coverage parsers(Clark and Curran, 2004; Hockenmaier and Steedman, 2002) and generators1 available (White and Baldridge, 2003). Words are associated with lexical categories which specify their subcategorization behaviour, eg. ((S[dcl]NP)/NP)/NP is the lexical category for (tensed) ditransitive verbs in English such as gives or send, which expect two NP objects to their right, and one NP subject to their left. Complex categories X/Y or XY are functors which yield a constituent with category X, if they are applied to a constituent with category Y to their right (/Y) or to their left (Y). Constituents are combined via a small set o"
W06-1637,W03-2316,0,0.0830081,"c interpretation. Such normal form derivations only use composition and type-raising where syntactically necessary (eg. in relative clauses). CCG (Steedman, 2000) is a mildly contextsensitive, lexicalized grammar formalism with a transparent syntax-semantics interface and a flexible constituent structure that is of particular interest to psycholinguistics, since it allows the construction of incremental derivations. CCG has also enjoyed the interest of the NLP community, with high-accuracy wide-coverage parsers(Clark and Curran, 2004; Hockenmaier and Steedman, 2002) and generators1 available (White and Baldridge, 2003). Words are associated with lexical categories which specify their subcategorization behaviour, eg. ((S[dcl]NP)/NP)/NP is the lexical category for (tensed) ditransitive verbs in English such as gives or send, which expect two NP objects to their right, and one NP subject to their left. Complex categories X/Y or XY are functors which yield a constituent with category X, if they are applied to a constituent with category Y to their right (/Y) or to their left (Y). Constituents are combined via a small set of combinatory rule schemata: X/Y Y X X/Z X X/Z T/(TX) X the man SNP Combinatory Cat"
W06-1637,P06-1053,1,0.789682,"Grammar David Reitter Julia Hockenmaier Frank Keller School of Informatics Inst. for Res. in Cognitive Science School of Informatics University of Edinburgh University of Pennsylvania University of Edinburgh 2 Buccleuch Place 3401 Walnut Street 2 Buccleuch Place Edinburgh EH8 9LW, UK Philadelphia PA 19104, USA Edinburgh EH8 9LW, UK dreitter@inf.ed.ac.uk juliahr@cis.upenn.edu keller@inf.ed.ac.uk Abstract recsanyi, 2005), consistent with the experimental literature, but also generalize to syntactic rules across the board, which repeated more often than expected by chance (Reitter et al., 2006b; Dubey et al., 2006). In the present paper, we build on this corpus-based approach to priming, but focus on the role of the underlying syntactic representations. In particular, we use priming to evaluate claims resulting from a particular syntactic theory, which is a way of testing the representational assumptions it makes. Using priming effects to inform syntactic theory is a novel idea; previous corpus-based priming studies have simply worked with uncontroversial classes of constructions (e.g., passive/active). The contribution of this paper is to overcome this limitation by defining a computational model of pr"
W06-1637,P96-1011,0,0.604915,"NP I saw and NPNP you NP (SNP)/NP conj NP &gt;T &gt; heard the man (SNP)/NP &gt;T S/(SNP) S/(SNP) S/NP &gt;B S/NP &gt;B &lt;Φ&gt; S &gt; The combinatory rules of CCG allow multiple, semantically equivalent, syntactic derivations of the same sentence. This spurious ambiguity is the result of CCG’s flexible constituent structure, which can account for long-range dependencies and coordination (as in the above example), and also for interaction with information structure. CCG parsers often limit the use of the combinatory rules (in particular: type-raising) to obtain a single right-branching normal form derivation (Eisner, 1996) for each possible semantic interpretation. Such normal form derivations only use composition and type-raising where syntactically necessary (eg. in relative clauses). CCG (Steedman, 2000) is a mildly contextsensitive, lexicalized grammar formalism with a transparent syntax-semantics interface and a flexible constituent structure that is of particular interest to psycholinguistics, since it allows the construction of incremental derivations. CCG has also enjoyed the interest of the NLP community, with high-accuracy wide-coverage parsers(Clark and Curran, 2004; Hockenmaier and Steedman, 2002) a"
W06-1637,P02-1043,1,0.852294,"al form derivation (Eisner, 1996) for each possible semantic interpretation. Such normal form derivations only use composition and type-raising where syntactically necessary (eg. in relative clauses). CCG (Steedman, 2000) is a mildly contextsensitive, lexicalized grammar formalism with a transparent syntax-semantics interface and a flexible constituent structure that is of particular interest to psycholinguistics, since it allows the construction of incremental derivations. CCG has also enjoyed the interest of the NLP community, with high-accuracy wide-coverage parsers(Clark and Curran, 2004; Hockenmaier and Steedman, 2002) and generators1 available (White and Baldridge, 2003). Words are associated with lexical categories which specify their subcategorization behaviour, eg. ((S[dcl]NP)/NP)/NP is the lexical category for (tensed) ditransitive verbs in English such as gives or send, which expect two NP objects to their right, and one NP subject to their left. Complex categories X/Y or XY are functors which yield a constituent with category X, if they are applied to a constituent with category Y to their right (/Y) or to their left (Y). Constituents are combined via a small set of combinatory rule schemata: X/Y"
W06-1637,N06-2031,1,\N,Missing
W06-1637,P04-1005,0,\N,Missing
W07-2205,W02-1503,1,0.818483,"Baldwin University of Melbourne tim@csse.unimelb.edu.au Julia Hockenmaier University of Pennsylvania juliahr@cis.upenn.edu Mark Dras Macquarie University madras@ics.mq.edu.au Tracy Holloway King PARC thking@parc.com Abstract Gertjan van Noord University of Groningen vannoord@let.rug.nl dependencies or the underlying predicate-argument structure directly. Aspects of this research have often had their own separate fora, such as the ACL 2005 workshop on deep lexical acquisition (Baldwin et al., 2005), as well as the TAG+ (Kallmeyer and Becker, 2006), Alpino (van der Beek et al., 2005), ParGram (Butt et al., 2002) and DELPH-IN (Oepen et al., 2002) projects and meetings. However, the fundamental approaches to building a linguistically-founded system and many of the techniques used to engineer efficient systems are common across these projects and independent of the specific grammar formalism chosen. As such, we felt the need for a common meeting in which experiences could be shared among a wider community, similar to the role played by recent meetings on grammar engineering (Wintner, 2006; Bender and King, 2007). As the organizers of the ACL 2007 Deep Linguistic Processing workshop (Baldwin et al., 2007"
W08-2306,E91-1005,0,0.167709,"this book to the children. b. dass der Rat dem Pfarrer die Menschen der Opfer gedenken zu lassen versprochen hat. that the council the priest the people the victims commemorate let promised has. that the council has promised the priest to let the citizens commemorate the victims. c. dass die Menschen der Opfer dem Pfarrer der Rat gedenken zu lassen versprochen hat. that the people the victims the priest the council commemorate let promised has. that the council has promised the priest to let the citizens commemorate the victims. Figure 2: Non-local scrambling examples (from Rambow (1994) and Becker et al. (1991). The treatment of subjects Unlike Hockenmaier (2006), we treat subjects as arguments of main verb, and assume auxiliaries are categories of the form S/S and SS (with appropriate features to avoid overgeneration). Evidence for this analysis (which is similar to the standard analysis of subjects in TAG) comes from coordinations that would otherwise not be derivable (see Figure 7). Local Scrambling In the so-called “Mittelfeld” all orders of arguments and adjuncts are potentially possible. In the following example, all 5! permutations are grammatical (Rambow, 1994): (6) dass [eine Firma] [meine"
W08-2306,P06-1064,1,0.907381,"nker and Weir (1994) show that a TAG can be translated into a CCG that uses only function application and composition, but does not require type-raising. and Schabes, 1997). Weir (1988) gives a CCG for this language, but since his grammar assumes that the string contains n empty strings ǫ with lexical categories, we give in Figure 2 a different analysis. This grammar assigns the lexical category (SA)/D)4 S)/C to any but the leftmost b, where we have used 4 to indicate a modality which requires backward crossed 4-ary composition.2 3 CCG for a fragment of German We follow Steedman (2000) and Hockenmaier (2006) in most of our basic analyses. German has three different word orders that depend on the clause type. Main clauses (3) are verb-second. Imperatives and questions are verb-initial (4). If a modifier or one of the objects is moved to the front, the word order becomes verb-initial (4). Subordinate and relative clauses are verb-final (5): (3) (4) (5) a. Peter gibt ihm ein Buch. Peter gives him a book. b. Ein Buch gibt Peter ihm. c. dann gibt Peter ihm ein Buch. a. Gibt Peter ihm ein Buch? b. Gib ihm ein Buch! a. dass Peter ihm das Buch gibt. b. das Buch, das Peter ihm gibt. Deriving an bn cn dn T"
W08-2306,P93-1045,0,0.0764939,"A)/D)4 S ... (...(SA1 )/D1 )....An )/Dn &gt; Figure 1: Type-raising is not required to derive an bn cn dn in CCG. Both the maximal arity n up to which generalized composition Bn is allowed and the maximal arity k of the variable T that results from typeraising are assumed to be bounded (typically to the maximal arity of lexical categories required by a language (Steedman, 2000)). These bounds are known to be important: Weir (1988) shows that if there is no bound on generalized composition, CCG can generate {an a′m bn cn b′m c′m d′m dn }, which cannot be generated by a TAG or LIG, and Hoffman (Hoffman, 1993) shows that a CCG with B× 2 and no bounds on the arity of type-raised categories can derive an bn cn dn en , which also cannot be generated by a TAG or LIG. In English, type-raising and composition allow derivations of wh-extraction, right node raising and argument cluster coordination in which the verbs involved have the same lexical categories as in standard sentences that do not involve nonlocal dependencies. In TAG, these constructions either require either additional elementary trees, or non-standard coordination rules (Sarkar and Joshi, 1996) that were not taken into account in the origi"
W08-2306,E95-1034,0,0.0654269,"ts of arguments: (7) Dir gibt Maria den Ball und Peter das Buch. to-you gives Maria the ball and Peter the book. To you, Maria gives the ball and Peter the book. Dir gibt den Ball Maria und das Buch Peter. (8) Das Buch gibt Maria dir und Peter mir. Das Buch gibt dir Maria und mir Peter. (9) Peter gibt mir das Buch und dir den Ball. Peter gibt das Buch mir und den Ball dir. Like in a TAG analysis of local scrambling, we will therefore assume separate lexical categories for each possible permutation3 . 3 To avoid this combinatorial explosion of the lexicon, extensions of CCG have been proposed (Hoffman, 1995b; Baldridge, 2002); albeit, at least in Hoffman’s case, these raise its generative capacity beyond that of standard CCG Partial VP fronting requires an analysis in which the remnant arguments in the Mittelfeld for a constituent, similar to argument cluster coordination (here TVv1 = (Sv1 NPn )NPa ): Gelesen hat Sdcl /TVv1 Sv1 /Spt Peter NPn das Buch NPa &gt;T &gt;T S/(SNPn )(SNP)/((SNP)NPa) S/((SNPn )NPa ) (Sv1 NPn )NPa &gt;B &gt;B Other constructions If verbs like versprechen (promise) have lexical categories of the form ((SNPn )NPd )/(S[zu]NPn ), with a suitable modality on the S[zu]NP tha"
W08-2306,C96-2103,0,0.0465581,"hich cannot be generated by a TAG or LIG, and Hoffman (Hoffman, 1993) shows that a CCG with B× 2 and no bounds on the arity of type-raised categories can derive an bn cn dn en , which also cannot be generated by a TAG or LIG. In English, type-raising and composition allow derivations of wh-extraction, right node raising and argument cluster coordination in which the verbs involved have the same lexical categories as in standard sentences that do not involve nonlocal dependencies. In TAG, these constructions either require either additional elementary trees, or non-standard coordination rules (Sarkar and Joshi, 1996) that were not taken into account in the original equivalence proof. On the other hand, the Dutch cross-serial dependencies (without extraction or coordination1 ) and the weakly equivalent an bn , can easily by a CCG with bounded generalized composition and without type-raising. In fact, Vijay-Shanker and Weir (1994) show that a TAG can be translated into a CCG that uses only function application and composition, but does not require type-raising. and Schabes, 1997). Weir (1988) gives a CCG for this language, but since his grammar assumes that the string contains n empty strings ǫ with lexical"
W08-2306,1985.tmi-1.17,0,0.385281,"Introduction Vijay-Shanker and Weir (1994) proved that TreeAdjoining Grammar (TAG (Joshi and Schabes, 1997)), Combinatory Categorial Grammar (CCG (Steedman, 2000)) and Linear Indexed Grammars (LIG, (Gazdar, 1988)) are weakly equivalent, i.e. can generate the same sets of strings. All of these grammars can generate the languages {an bn cn dn } (which does not correspond to any known construction in natural language), and {an bn } with cross-serial dependencies (i.e. a1 ...an b1 ...bn ), corresponding to the cross-serial dependencies that arise in Dutch (Bresnan et al., 1982) and Swiss German (Shieber, 1985). Although this result has important algorithmic consequences (Vijay-Shanker and Weir, 1993), it is easy to overestimate its linguistic relevance. Weak equivalence does, of course, not necessarily imply that two formalisms are capable of recovering the same set of dependencies between the elements of a string. Since the notion of strong equivalence is often hard to define, strong equivalene proofs are rarely found in the literature. But examples of structures that can only be analyzed in one formalism can provide insight into where their strong generative capacities differ. 2 Combinatory Categ"
W08-2306,J93-4002,0,0.401539,"(TAG (Joshi and Schabes, 1997)), Combinatory Categorial Grammar (CCG (Steedman, 2000)) and Linear Indexed Grammars (LIG, (Gazdar, 1988)) are weakly equivalent, i.e. can generate the same sets of strings. All of these grammars can generate the languages {an bn cn dn } (which does not correspond to any known construction in natural language), and {an bn } with cross-serial dependencies (i.e. a1 ...an b1 ...bn ), corresponding to the cross-serial dependencies that arise in Dutch (Bresnan et al., 1982) and Swiss German (Shieber, 1985). Although this result has important algorithmic consequences (Vijay-Shanker and Weir, 1993), it is easy to overestimate its linguistic relevance. Weak equivalence does, of course, not necessarily imply that two formalisms are capable of recovering the same set of dependencies between the elements of a string. Since the notion of strong equivalence is often hard to define, strong equivalene proofs are rarely found in the literature. But examples of structures that can only be analyzed in one formalism can provide insight into where their strong generative capacities differ. 2 Combinatory Categorial Grammar In addition to function application (&gt; and <), CCG allows the combinatory rule"
W10-0721,D09-1030,0,0.0137824,"om flickr-social, 916 from outdoor, 1257 from strangers and 1773 from wild-child. 8 Note that the annotation process scaled pretty well, considering that annotating more than eight times the number of images took only 31 hours longer. 145 collected question and answer pairs by presenting Turkers with a question and telling them to copy and paste from a document of text they know to contain the answer. They achieve a good but far from perfect interannotator agreement based on the extracted answers. We speculate that the quality would be much worse if the Turkers wrote the sentences themselves. Callison-Burch (2009) asks Turkers to produce translations when given reference sentences in other languages. Overall, he finds find that Turkers produce better translations than machine translation systems. To eliminate translations from Turkers who simply put the reference sentence into an online translation website, he performs a follow-up task, where he asks other Turkers to vote on if they believe that sentences were generated using an online translation system. Mihalcea and Strapparava (2009) ask Turkers to produce 4-5 sentence opinion paragraphs about the death penalty, about abortion and describing a frien"
W10-0721,kaisser-lowe-2008-creating,0,0.0864038,"s about different aspects of Wikipedia articles. At first they receive very noisy results, due to Turkers’ not paying attention when completing the task or specifically trying to cheat the requester. They remade the task, this time starting by asking the Turkers verifiable questions, speculating that the users would produce better quality responses when they suspect their answers will be checked. They also added a question that required the Turkers to comprehend the content of the Wikipedia article. With this new setup, they find that the quality greatly increases and carelessness is reduced. Kaisser and Lowe (2008) 7 Our final data set consists of 1482 pictures from action photography, 1904 from dogs, 776 from flickr-social, 916 from outdoor, 1257 from strangers and 1773 from wild-child. 8 Note that the annotation process scaled pretty well, considering that annotating more than eight times the number of images took only 31 hours longer. 145 collected question and answer pairs by presenting Turkers with a question and telling them to copy and paste from a document of text they know to contain the answer. They achieve a good but far from perfect interannotator agreement based on the extracted answers. We"
W10-0721,P09-2078,0,0.0234745,"ased on the extracted answers. We speculate that the quality would be much worse if the Turkers wrote the sentences themselves. Callison-Burch (2009) asks Turkers to produce translations when given reference sentences in other languages. Overall, he finds find that Turkers produce better translations than machine translation systems. To eliminate translations from Turkers who simply put the reference sentence into an online translation website, he performs a follow-up task, where he asks other Turkers to vote on if they believe that sentences were generated using an online translation system. Mihalcea and Strapparava (2009) ask Turkers to produce 4-5 sentence opinion paragraphs about the death penalty, about abortion and describing a friend. They report that aside from a small number of invalid responses, all of the paragraphs were of good quality and followed their instructions. Their success is surprising to us because they do not report using a qualification test, and when we did this our responses contained a large amount of incorrect English spelling and grammar. The TurKit toolkit (Little et al., 2009) provides another approach to improving the quality of MTurk annotations. Their iterative framework allows"
W10-0721,D08-1027,0,0.0317351,"Missing"
W10-0721,W09-0619,0,0.0122283,"Missing"
W10-0721,D09-1006,0,0.0725857,"Missing"
W10-2920,P08-1032,0,0.0218923,"reference resolution for automatic image understanding Micah Hodosh Peter Young Cyrus Rashtchian Julia Hockenmaier Department of Computer Science University of Illinois at Urbana-Champaign {mhodosh2, pyoung2, crashtc2, juliahmr}@illinois.edu Abstract Photo sharing sites such as Flickr allow users to annotate images with keywords and other descriptions, and vision researchers have access to large collections of images annotated with keywords (e.g. the Corel collection). A lot of recent work in computer vision has been aimed at predicting these keywords (Blei et al., 2003; Barnard et al., 2003; Feng and Lapata, 2008; Deschacht and Moens, 2007; Jeon et al., 2003). But keywords alone are not expressive enough to capture relations between entities. Some research has used the text that surrounds an image in a news article as a proxy (Feng and Lapata, 2008; Deschacht and Moens, 2007). However, in many cases, the surrounding text or a user-provided caption does not simply describe what is depicted in the image (since this is usually obvious to the human reader for which this text is intended), but provides additional information. We have collected a corpus of 8108 images associated with several simple descript"
W10-2920,P07-1107,0,0.197847,"e represent the number of times entity type e is generated by topic t. Each iteration consists of two steps: first, each Zi is resampled, fixing T; and then each Ti is resampled based on Z12 . 1. For each noun, choose the synset that appears in the most number of captions of an image, and break ties by choosing the synset that covers the fewest distinct lemmatized nouns. 2. Group all of the noun phrase chunks that share a synset into a single coreference chain. 6 Bayesian coreference models Since we cannot afford to manually annotate our entire data set with coreference information, we follow Haghighi and Klein (2007)’s work on unsupervised coreference resolution, and develop a series of generative Bayesian models for our task. 6.1 1. Sampling Zj : P (Zj = e|wj ∈ wi ,Z−j ,T) ∝ P (wj |Zj = e)P (Zj = e|Ti ) „ « n−j e,x + α P (wj = x|Zj = e) ∝ P δxe −j x0 ne,x0 + α Model 0: Simple Mixture Model In our first model, based on Haghighi and Klein’s baseline Dirichlet Process model, each image i corresponds to the set of observed mentions wi from across its captions. Image i has a hidden global topic Ti , drawn from a distribution with a GEM prior with hyperparameter γ as explained by Teh et al. (2006). In a Dirich"
W10-2920,D09-1120,0,0.0143671,"Recall 28.4 48.3 51.7 50.9 52.2 52.3 Precision 20.6 43.9 42.8 45.4 42.7 46.0 F-score 23.9 46.0 46.8 48.0 47.0 49.0 sociated with several simple descriptive captions, which provide more detailed information about the image than simple keywords. We plan to make this data set available for further research in computer vision and natural language processing. In order to enable the creation of a semantic representation of the image content that is consistent with the captions in our data set, we use WordNet and a series of Bayesian models to perform cross-caption coreference resolution. Similar to Haghighi and Klein (2009), who find that linguistic heuristics can provide very strong baselines for standard coreference resultion, relatively simple heuristics based on WordNet alone perform surprisingly well on our task, although they are outperformed by our Bayesian models for overall entity prediction. Since our generative models are based on Dirichlet Process priors, they are designed to favor a small number of unique entities per image. In the heuristic algorithm, this bias is built in explicitly, resulting in slightly higher performance on the coreference resolution task. However, while the generative models c"
W10-2920,W09-2206,0,0.023155,"aspects of the entity to describe it (for example, “skier” and “a skiing man”). 164 from an entity type-specific multinomial φz over all possible words V, drawn from a finite Dirichlet prior with hyperparameter α. The set of all images belonging to the same topic is analogous to an individual document in Haghighi and Klein’s baseline model.11 All headwords of the same entity type are assumed to be coreferent, similar to Haghighi and Klein’s model. As described in section 4, we use WordNet to identify the subset of types that can actually produce the given words. Therefore, similar to the way Andrzejewski and Zhu (2009) handled a priori knowledge of topics, we will define an indicator variable δij that is 1 iff the WordNet information allows word i to be produced from entity set j and 0 otherwise. the lexicon of n2 (since if n1 is using the sense s1 , then n2 must be using the sense s2 ) and if n1 occurs at least five times in the corpus, we add s1 to the lexicon of n2 . 5 A heuristic coreference algorithm Based on WordNet candidate synsets, we define a heuristic algorithm that finds the optimal entity assignment for the mentions associated with each image. This algorithm is based on the principles driving o"
W10-2920,P01-1008,0,0.0437552,"Feng and Lapata, 2008; Deschacht and Moens, 2007). However, in many cases, the surrounding text or a user-provided caption does not simply describe what is depicted in the image (since this is usually obvious to the human reader for which this text is intended), but provides additional information. We have collected a corpus of 8108 images associated with several simple descriptive captions. In contrast to the text near an image on the web, the captions in our corpus provide direct, if partial and slightly noisy, descriptions of the image content. Our data set differs from paraphrase corpora (Barzilay and McKeown, 2001; Dolan et al., 2004) in that the different captions of an image are produced independently by different writers. There are many ways of describing the same image, because it is often possible to focus on different aspects of the depicted situation, and because certain aspects of the situation may be unclear to the human viewer. Recent work in computer vision has aimed to associate image regions with keywords describing the depicted entities, but actual image ‘understanding’ would also require identifying their attributes, relations and activities. Since this information cannot be conveyed by"
W10-2920,J07-4004,0,0.0117978,"classifier, implemented in the M ALLET toolkit (McCallum, 2002), and define the following text features: NP Chunk: We include all the words in the NP chunk, unfiltered. WordNet Synsets and Hypernyms: The most likely synset is either the first one that appears in WordNet or one of the ones predicted by our coreference system. For each of these possibilities, we include all of that synset’s hypernyms. A baseline model: In our baseline model, two noun phrases in captions of the same image are coreferent if they share the same head noun. Syntactic Role: We parsed our captions with the C&C parser (Clark and Curran, 2007), and record whether the word appears as a direct object of a verb, as the object of a preposition, as the subject of the sentence, or as a modifier. If it is a modifier, we also add the head word of the phrase being modified. Upper bound on performance: Although WordNet synsets provide a good indication of whether two mentions can refer to the same entity or not, they may also be overly restrictive in other cases. We measure the upper bound on performance that our reliance on WordNet imposes by finding the best-scoring coreference assignment that is consistent with our lexicon. 13 Model and F"
W10-2920,P07-1126,0,0.0303573,"r automatic image understanding Micah Hodosh Peter Young Cyrus Rashtchian Julia Hockenmaier Department of Computer Science University of Illinois at Urbana-Champaign {mhodosh2, pyoung2, crashtc2, juliahmr}@illinois.edu Abstract Photo sharing sites such as Flickr allow users to annotate images with keywords and other descriptions, and vision researchers have access to large collections of images annotated with keywords (e.g. the Corel collection). A lot of recent work in computer vision has been aimed at predicting these keywords (Blei et al., 2003; Barnard et al., 2003; Feng and Lapata, 2008; Deschacht and Moens, 2007; Jeon et al., 2003). But keywords alone are not expressive enough to capture relations between entities. Some research has used the text that surrounds an image in a news article as a proxy (Feng and Lapata, 2008; Deschacht and Moens, 2007). However, in many cases, the surrounding text or a user-provided caption does not simply describe what is depicted in the image (since this is usually obvious to the human reader for which this text is intended), but provides additional information. We have collected a corpus of 8108 images associated with several simple descriptive captions. In contrast t"
W10-2920,C04-1051,0,0.0406701,"acht and Moens, 2007). However, in many cases, the surrounding text or a user-provided caption does not simply describe what is depicted in the image (since this is usually obvious to the human reader for which this text is intended), but provides additional information. We have collected a corpus of 8108 images associated with several simple descriptive captions. In contrast to the text near an image on the web, the captions in our corpus provide direct, if partial and slightly noisy, descriptions of the image content. Our data set differs from paraphrase corpora (Barzilay and McKeown, 2001; Dolan et al., 2004) in that the different captions of an image are produced independently by different writers. There are many ways of describing the same image, because it is often possible to focus on different aspects of the depicted situation, and because certain aspects of the situation may be unclear to the human viewer. Recent work in computer vision has aimed to associate image regions with keywords describing the depicted entities, but actual image ‘understanding’ would also require identifying their attributes, relations and activities. Since this information cannot be conveyed by simple keywords, we h"
W10-2920,W10-0721,1,0.770068,"e or sepia, as well as images with watermarks, signatures, borders or other obvious editing. Since our collection focuses on images depicting actions, we then filtered out images of scenery, portraits, and mood photography. This was done independently by two members of our group and adjudicated by a third. We paid Turk workers $0.10 to write one descriptive sentence for each of five distinct and randomly chosen images that were displayed one at a time. We required a small qualification test that examined the workers’ English grammar and spelling and we restricted the task to U.S. workers (see Rashtchian et al. (2010) for more details). Our final corpus contains five sentences for each of our 8108 images, totaling 478,317 word tokens, and an average sentence length of 11.8 words. We first spell-checked3 these sentences, and used OpenNLP4 to POS-tag them. We identified NPs using OpenNLP’s chunker, followed by a semiOntological annotation of entities In order to understand the role entities mentioned in the sentences play in the image, we have defined a simple ontology of entity classes (Table 1). We distinguish entities that constitute the background of an image from those that appear in the foreground. The"
W10-2920,W10-0707,0,\N,Missing
W12-1912,I11-1049,0,0.0898555,"n Ave. Urbana IL, 61801 {bisk1,juliahmr}@illinois.edu Abstract Our system consists of a simple, EM-based induction algorithm (Bisk and Hockenmaier, 2012), which induces a language-specific Combinatory Categorial grammar (CCG) and lexicon based on a small number of linguistic principles, e.g. that verbs may be the roots of sentences and can take nouns as arguments. 1 Introduction Much of the recent work on grammar induction has focused on the development of sophisticated statistical models that incorporate expressive priors (Cohen and Smith, 2010) or linguistic universals (Naseem et al., 2010; Boonkwan and Steedman, 2011) that have all been shown to be very helpful. But, with some notable exceptions, such as (Cohn et al., 2011), the question of what underlying linguistic representation to use has received considerably less attention. Our induction algorithm is based on Combinatory Categorial Grammar (Steedman, 2000), a linguistically expressive, lexicalized grammar formalism which associates words with rich syntactic categories that capture language-specific facts about basic word order and subcategorization. While Boonkwan and Steedman (2011) have shown that linguists can easily devise a language-specific inv"
W12-1912,C10-1053,1,0.795448,"e lexicon of the language. This restricted set of combinatory rules provides sufficient power for reasonable parse accuracy but does not allow us to capture non-projective (crossing) dependencies. Coordination is handled by a ternary rule X conj X ⇒X (>) ⇒X ⇒ X[conj] (&lt; &) (> &) which we binarize as: X conj X[conj] X Punctuation, when present, can be absorbed by rules of the form X Pct Pct X ⇒X ⇒X (&lt; p) (> p) The iterative combination of these categories resulting in S or N is considered a successful parse. In order to avoid spurious ambiguities, we restrict our derivations to be normal-form (Hockenmaier and Bisk, 2010). 3 Category induction We assume there are two atomic categories, N (nouns or noun phrases) and S (sentences), a special conjunction category conj, and a special start symbol TOP. We assume that all strings we encounter are either nouns or sentences: N ⇒ TOP &lt;B N 3.1 An algorithm for unsupervised CCG induction We now describe our induction algorithm, which consists of two stages: category induction (creation of the grammar), followed by parameter estimation for the probability model. 91 S ⇒ TOP We also assume that we can group POS-tags into four groups: nominal tags, verbal tags, conjunctions,"
W12-1912,P02-1043,1,0.742915,"complex categories, leading us to complete, but incorrect parses such as The DT man NNS eats VBZ with IN friends NNS N/N N SN SS SS > &lt;B N SN &lt;B SN S &lt; During the second iteration, we can discover additional simple, as well as more complex, categories. We now discover transitive verb categories: 92 Parameter estimation After constructing the lexicon, we parse the training corpus, and use the Inside-Outside algorithm (Lari and Young, 1991), a variant of the ExpectationMaximization algorithm for probabilistic contextfree grammars, to estimate model parameters. We use the baseline model of Hockenmaier and Steedman (2002), which is a simple generative model that is equivalent to an unlexicalized PCFG. In a CFG, the set of terminals and non-terminals is disjoint, but in CCG, not every category can be lexical. Since this model is also the basis of a lexicalized model that captures dependencies, it distinguishes between lexical expansions (which produce words), unary expansions (which are the result of type-raising or the TOP rules), binary expansions where the head is the left child, and binary expansions whose head is the right child. Each tree is generated top-down from the start category TOP. For each (parent"
W12-1912,J07-3004,1,0.847305,"types are of the form X/Y or XY, and represent functions which combine with an argument of type Y to yield a constituent of type X as result. The slash indicates whether the Y precedes () or follows (/) the functor. An English lexicon should contain categories such as SN and (SN)/N for verbs: both transitive and intransitive verbs subcategorize for a preceding subject, and the transitive verb additionally takes an object to its right. In this manner, the argument slots of lexical categories also define word-word dependencies between heads and their arguments (Clark and Hockenmaier, 2002; Hockenmaier and Steedman, 2007). Modifiers are generally of the form X|X: in English, pre-nominal adjectives are N/N, whereas adverbs may be (N/N)/(N/N), S/S, or SS, and prepositions can have categories such as (NN)/N or (SS)/N. That is, CCG assumes that the direction of the corresponding dependency goes from the modifier to the head. This discrepancy between CCG and most other analyses can easily be removed under the assumption that all categories of the form X|X are modifiers whose dependencies should be reversed when comparing against other frameworks. Adjacent constituents can be combined according to a small, univer"
W12-1912,W07-2416,0,0.0688783,"|P, exp = Unary) Left pe (exp = Left |P) × pH (H |P, exp = Left) Language Arabic Basque Childes Czech Danish Dutch Slovene Swedish PTB Portuguese × pS (S |P, H, exp = Left) Right pe (exp = Right |P) × pH (H |P, exp = Right) × pS (S |P, H, exp = Right) 3.3 Dependency generation We use the following regime for generating dependencies from the resulting CCG derivations: 1. Arguments Y are dependents of their heads X|Y 2. Modifiers X|X are dependents of their heads X or X|Y. 3. The head of the entire string is a dependent of the root node (0) 4. Following the CoNLL-07 shared task representation (Johansson and Nugues, 2007), we analyze coordinations (X1 conj X2 ) as creating a dependency from the first conjunct, X1 , to the conjunction conj, and from conj to the second conjunct X2 . In the case of parse failures we return a rightbranching dependency tree. 3.4 Training details The data provided includes fine, coarse and universal part-of-speech tags. Additionally, the data was split into train, test and development sets though the organizers encouraged merging the data for training. Finally, while punctuation was present, it was not evaluated but potentially provided an additional source of signal during training"
W12-1912,J93-2004,0,0.0410266,"al start symbol TOP. We assume that all strings we encounter are either nouns or sentences: N ⇒ TOP &lt;B N 3.1 An algorithm for unsupervised CCG induction We now describe our induction algorithm, which consists of two stages: category induction (creation of the grammar), followed by parameter estimation for the probability model. 91 S ⇒ TOP We also assume that we can group POS-tags into four groups: nominal tags, verbal tags, conjunctions, and others. This allows us to create an initial lexicon L(0) , which only contains entries for atomic categories, e.g. for the English Penn Treebank tag set (Marcus et al., 1993): N : {NN, NNS, NNP, PRP, DT} S : {MD, VB, VBZ, VBG, VBN, VBD} conj : {CC} We force any string that contains one or more verbs (besides VBG in English), to be parsed with the S ⇒ TOP rule. Since the initial lexicon would only allow us to parse single word utterances (or coordinations thereof), we need to induce complex functor categories. The lexicon entries for atomic categories remain, but all POS-tags, including nouns and conjunctions, will be able to acquire complex categories during induction. We impose the following constraints on the lexical categories we induce: 1. Nouns (N) do not tak"
W12-1912,D10-1120,0,0.118632,"hampaign 201 N Goodwin Ave. Urbana IL, 61801 {bisk1,juliahmr}@illinois.edu Abstract Our system consists of a simple, EM-based induction algorithm (Bisk and Hockenmaier, 2012), which induces a language-specific Combinatory Categorial grammar (CCG) and lexicon based on a small number of linguistic principles, e.g. that verbs may be the roots of sentences and can take nouns as arguments. 1 Introduction Much of the recent work on grammar induction has focused on the development of sophisticated statistical models that incorporate expressive priors (Cohen and Smith, 2010) or linguistic universals (Naseem et al., 2010; Boonkwan and Steedman, 2011) that have all been shown to be very helpful. But, with some notable exceptions, such as (Cohn et al., 2011), the question of what underlying linguistic representation to use has received considerably less attention. Our induction algorithm is based on Combinatory Categorial Grammar (Steedman, 2000), a linguistically expressive, lexicalized grammar formalism which associates words with rich syntactic categories that capture language-specific facts about basic word order and subcategorization. While Boonkwan and Steedman (2011) have shown that linguists can easily"
W16-3203,W14-3348,0,0.0400967,"re each system produces novel captions, see e.g. Kulkarni et al. (2011)), and as a ranking task (where each system is required to rank the same pool of unseen test captions for each test image, see e.g. Hodosh et al. (2013)). But although the numbers reported in the literature make it seem as though this task is quickly approaching being solved (on the recent MSCOCO challenge,1 the best models outperformed humans according to some metrics), evaluation remains problematic for both approaches (Hodosh, 2015). Caption generation requires either automated metrics (Papineni et al., 2002; Lin, 2004; Denkowski and Lavie, 2014; Vedantam et al., 2015), most of which have been shown to correlate poorly with human judgments (Hodosh et al., 2013; Elliott and Keller, 2014; Hodosh, 2015) and fail to capture the variety in human captions, while human evaluation is subjective (especially when reduced to simple questions such as “Which is a better caption?”), expensive, and difficult to replicate. Ranking-based evaluation suffers from the 1 http://mscoco.org/dataset/#captions-challenge2015 problem that the pool of candidate captions may, on the one hand, be too small to contain many meaningful and interesting distractors, a"
W16-3203,P15-2017,0,0.0126346,"e word embeddings trained on large corpora). However, RNNs (Elman, 1990) and LSTMs offer convenient ways to define a probability distribution across the space of all possible image captions that cannot be modeled as easily with a bag-of-words style approach. The question remains if that convenience comes at a cost of no longer being able to easily train a model that understands the language to an 26 acceptable amount of detail. It is also important to note that we were unable to evaluate a model that combines a generation model with a reranker such Fang et al. (2014) and the follow up work in Devlin et al. (2015). In theory, if the generation models are able produce a significantly enough diverse set of captions, the reranking can make up the gap in performance while still being able to generate novel captions easily. 6 Conclusion It is clear that evaluation still remains a difficult issue for image description. The community needs to develop metrics that are more sensitive than the ranking task while being more directly correlated to human judgement than current automated metrics used for generation. In this paper, we developed a sequence of binary forced-choice tasks to evaluate and compare differen"
W16-3203,P14-2074,0,0.0782044,"pool of unseen test captions for each test image, see e.g. Hodosh et al. (2013)). But although the numbers reported in the literature make it seem as though this task is quickly approaching being solved (on the recent MSCOCO challenge,1 the best models outperformed humans according to some metrics), evaluation remains problematic for both approaches (Hodosh, 2015). Caption generation requires either automated metrics (Papineni et al., 2002; Lin, 2004; Denkowski and Lavie, 2014; Vedantam et al., 2015), most of which have been shown to correlate poorly with human judgments (Hodosh et al., 2013; Elliott and Keller, 2014; Hodosh, 2015) and fail to capture the variety in human captions, while human evaluation is subjective (especially when reduced to simple questions such as “Which is a better caption?”), expensive, and difficult to replicate. Ranking-based evaluation suffers from the 1 http://mscoco.org/dataset/#captions-challenge2015 problem that the pool of candidate captions may, on the one hand, be too small to contain many meaningful and interesting distractors, and may, on the other hand, contain other sentences that are equally valid descriptions of the image. To illustrate just how much is still to be"
W16-3203,P82-1020,0,0.631222,"Missing"
W16-3203,W04-1013,0,0.0297805,"roblem (where each system produces novel captions, see e.g. Kulkarni et al. (2011)), and as a ranking task (where each system is required to rank the same pool of unseen test captions for each test image, see e.g. Hodosh et al. (2013)). But although the numbers reported in the literature make it seem as though this task is quickly approaching being solved (on the recent MSCOCO challenge,1 the best models outperformed humans according to some metrics), evaluation remains problematic for both approaches (Hodosh, 2015). Caption generation requires either automated metrics (Papineni et al., 2002; Lin, 2004; Denkowski and Lavie, 2014; Vedantam et al., 2015), most of which have been shown to correlate poorly with human judgments (Hodosh et al., 2013; Elliott and Keller, 2014; Hodosh, 2015) and fail to capture the variety in human captions, while human evaluation is subjective (especially when reduced to simple questions such as “Which is a better caption?”), expensive, and difficult to replicate. Ranking-based evaluation suffers from the 1 http://mscoco.org/dataset/#captions-challenge2015 problem that the pool of candidate captions may, on the one hand, be too small to contain many meaningful and"
W16-3203,P02-1040,0,0.0950773,"l language generation problem (where each system produces novel captions, see e.g. Kulkarni et al. (2011)), and as a ranking task (where each system is required to rank the same pool of unseen test captions for each test image, see e.g. Hodosh et al. (2013)). But although the numbers reported in the literature make it seem as though this task is quickly approaching being solved (on the recent MSCOCO challenge,1 the best models outperformed humans according to some metrics), evaluation remains problematic for both approaches (Hodosh, 2015). Caption generation requires either automated metrics (Papineni et al., 2002; Lin, 2004; Denkowski and Lavie, 2014; Vedantam et al., 2015), most of which have been shown to correlate poorly with human judgments (Hodosh et al., 2013; Elliott and Keller, 2014; Hodosh, 2015) and fail to capture the variety in human captions, while human evaluation is subjective (especially when reduced to simple questions such as “Which is a better caption?”), expensive, and difficult to replicate. Ranking-based evaluation suffers from the 1 http://mscoco.org/dataset/#captions-challenge2015 problem that the pool of candidate captions may, on the one hand, be too small to contain many mea"
W16-3203,D14-1162,0,0.0824065,"learn the embedding of the captions. While the Flickr30K trained model should be more appropriate for our test data, the MSCOCO trained model might be more directly comparable to the generation model of Vinyals et al. A comparison between the two variants can offer insight into the degree of domain shift between the two datasets. Our ranking baseline model (BOW Ranking) replaces the LSTM of Kiros et al. (2014) with a simple bag-of-words text representation, allowing us to examine whether the expressiveness of LSTMs is required for this task. We use the average of the tokens’ GloVe embeddings (Pennington et al., 2014) as input to a fully connected neural network layer that produces the final learned text embedding4 . More formally, for a sentence consisting of tokens w1 ...wn , GloVe embeddings φ(), and a non-linear activation function σw , we define the learned sentence embedding as F (w1 ...wn ) = P σw (Ww · ( n1 ) i φ(wi ) + bw ). Similarly, the embedding of an image represented as a vector p is defined as G(p) = σi (Wi · p + bi ). We use a ranking loss similar to Kiros et al. (2014) to train the parameters of our model, θ = (Ww , Wi , bw , bi ). We define the distance of the embeddings of image i and s"
W16-3203,Q14-1006,1,0.748658,"and”, and how models trained for generation differ from models trained for ranking. The models we compare consist of a number of simple baselines, as well as some publicly available models that each had close to state-ofthe-art performance on standard tasks when they were published. More details and discussion can be found in Hodosh (2015). 2 A framework for focused evaluation In this paper, we evaluate image description systems with a series of binary (two-alternative) forced choice tasks. The items in each task consist of one image from the test or development part of the Flickr30K dataset (Young et al., 2014), paired 19 Proceedings of the 5th Workshop on Vision and Language, pages 19–28, c Berlin, Germany, August 12 2016. 2016 Association for Computational Linguistics “Switch People” Task Image “Replace Scene” Task Gold Caption Distractor a man holding and kissing a crying little boy on the cheek a crying little boy holding and kissing a man on the cheek a woman is hula hooping in front of an audience Image Gold Caption two dogs playing on a beach Distractor two dogs playing on frozen tundra a brown dog is a brown dog is bending down bending down trying to drink from trying to drink from your loca"
W17-2812,W10-2903,1,0.74867,"e language in the context of that task (i.e. to map between utterances and meaning representations the problem solving components of the agent can act on in a particular situation). The agent may also need to initiate clarification requests when communication fails, and to learn new domain (or conversation) specific vocabulary and its meaning. This kind of symmetric, grounded communication with a problem-solving agent goes significantly beyond the one-step, single direction understanding tasks considered in standard semantic parsing (e.g. Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Clarke et al., 2010) or even short, simple instructions to robots (e.g. Tellex et al., 2011). In order to focus on these concept learning and communication issues, we deliberately limit ourselves here to a simple, simulated environment. 95 Proceedings of the First Workshop on Language Grounding for Robotics, pages 95–103, c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics Figure 1: A complex shape, which can be viewed as conjunction of simpler known shapes: a row (dark, width = 4) and a square (light, size = 3). 2 Domain and Problem Setup We consider a two-dimensional (2"
W17-2812,P16-1004,0,0.0294175,"= h{hsi , idi , ∧k di i}i∈S , ∧j∈[S×S] fj i, where In this section, we describe the current implementations of the different modules (language comprehension, memory, problem solving, language production, and dialogue mediation) in COG, noting that the architecture is flexible and allows for us to plug-in other implementations as needed. 4.1 Language Comprehension The LC module consists of semantic parsing and language grounding components. 4.1.1 Language Grounding Semantic Parsing Our semantic parser is implemented as a neural sequence-to-sequence model with attention (Bahdanau et al., 2014; Dong and Lapata, 2016; Jia and Liang, 2016). The model consists of two LSTMs. The first LSTM (the encoder) processes the input sentence x = (x1 , . . . , xm ) token-by-token, producing a sequence of hidden states hs = (hs1 , . . . , hsm ) as output. The second LSTM (the decoder) models a distribution P (yi |y1 , . . . , yi−1 ; hs ) at each time step over output tokens as a function of the encoder hidden states and the previous outputs. The final parse y = (y1 , . . . , yn ) is obtained by selecting the token at each time step that maximizes this probability and feeding a learned embedding for it into (k) S is the"
W17-2812,W03-2316,0,0.0595628,"expect to see a large number of examples. We will consider the use of probabilistic logic models which can handle both issues by explicitly including the trade-off in the optimization function (Odom et al., 2015). A final challenge is the application of our agent to new domains. Currently, the memory module contains all the knowledge required to plan and produce comprehensible responses. This declarative approach should generalize well to some simple enough domains, but will need to be extended to deal with more involved tasks and domains. veloped a grammar-based realizer inspired by OpenCCG (White and Baldridge, 2003; White, 2006) that operates over the first-order semantic representations used by our agent. We plan to augment the realizer’s semantic lexicon with the learned definitions of predicates for new shapes, allowing our system to generate natural language instructions describing the new configurations. One of the key challenges of the scenario we envision (and a fundamental problem in language acquisition) is the necessity to generalize across situations. This is required in order to learn general concepts from a few specific instances. At this point, our agent is able to generalize from a single"
W17-2812,P16-1154,0,0.0142285,"ere the initial encoder hidden state for a given sentence is set to the final encoder hidden state for the previous sentence, and the initial encoder hidden state for the first sentence is the zero vector. The final logical form is the conjunction of the logical forms for each individual sentence. The parser is trained with a small fixed-size vocabulary; however, to represent new shapes it needs to be able to output new predicates for shapes that it has not encountered during training. We accomplish this by using an attentionbased copying mechanism (Jia and Liang, 2016; Gulcehre et al., 2016; Gu et al., 2016). At every time step, the decoder may either output a token from the training vocabulary or copy a word from the input sentence. Hence, when new shapes are encountered in the input, the parser is able to copy the shape name from the input sentence to define a new predicate. also in this language. If grounding fails, LC produces queries that are sent to the Language Production (LP) module. The LP takes these queries or the plans produced by PS and returns natural language output. The Memory (M) module stores lifted representations of built-in and acquired predicates that are used by LC and LP."
W17-2812,P16-1014,0,0.0162333,"entence-by-sentence, where the initial encoder hidden state for a given sentence is set to the final encoder hidden state for the previous sentence, and the initial encoder hidden state for the first sentence is the zero vector. The final logical form is the conjunction of the logical forms for each individual sentence. The parser is trained with a small fixed-size vocabulary; however, to represent new shapes it needs to be able to output new predicates for shapes that it has not encountered during training. We accomplish this by using an attentionbased copying mechanism (Jia and Liang, 2016; Gulcehre et al., 2016; Gu et al., 2016). At every time step, the decoder may either output a token from the training vocabulary or copy a word from the input sentence. Hence, when new shapes are encountered in the input, the parser is able to copy the shape name from the input sentence to define a new predicate. also in this language. If grounding fails, LC produces queries that are sent to the Language Production (LP) module. The LP takes these queries or the plans produced by PS and returns natural language output. The Memory (M) module stores lifted representations of built-in and acquired predicates that are u"
W17-2812,P16-1002,0,0.050247,"nputs are processed sentence-by-sentence, where the initial encoder hidden state for a given sentence is set to the final encoder hidden state for the previous sentence, and the initial encoder hidden state for the first sentence is the zero vector. The final logical form is the conjunction of the logical forms for each individual sentence. The parser is trained with a small fixed-size vocabulary; however, to represent new shapes it needs to be able to output new predicates for shapes that it has not encountered during training. We accomplish this by using an attentionbased copying mechanism (Jia and Liang, 2016; Gulcehre et al., 2016; Gu et al., 2016). At every time step, the decoder may either output a token from the training vocabulary or copy a word from the input sentence. Hence, when new shapes are encountered in the input, the parser is able to copy the shape name from the input sentence to define a new predicate. also in this language. If grounding fails, LC produces queries that are sent to the Language Production (LP) module. The LP takes these queries or the plans produced by PS and returns natural language output. The Memory (M) module stores lifted representations of built-in and acquire"
Y98-1021,J95-4004,0,0.268351,"Missing"
Y98-1021,P97-1041,0,0.213475,"nt by the system is the final output of the learning process' . - We now have a sequence of transformations which can be applied any text which has been passed through the initial state annotator. This scheme is sufficiently general to apply to many tasks, including Chinese word segmentation, although each task will require the choice of a suitable set of rule templates, which serve to define the space of transformations to be searched by the algorithm. Performance will hinge crucially on the choice of appropriate templates. 3 Transformation-Based Learning applied to word segmentation Palmer ([4]) reports four experiments on Chinese word segmentation. Adopting Brill's scheme, he used four distinct initial-state annotators: a naive character-as-word segmentation - an initial segmentation obtained by maximum-matching - an initial segmentation by maximum matching where unknown character sequences were treated by a character-as-word segmentation. - the output of a high-accuracy word segmenter For all experiments, Palmer used a set of rule templates with three types of actions: - concatenate two characters. - spl i t two characters - s 1 i de a word boundary to the adjacent character. Rule"
Y98-1021,J96-3004,0,0.254686,"Missing"
