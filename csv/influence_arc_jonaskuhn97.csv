2020.acl-main.134,W11-2832,0,0.0387203,"an Problem (TSP), and use a biaffine attention model to calculate the edge costs. We facilitate the decoding by solving the TSP for each subtree and combining the solution into a projective tree. We then design a transition system as post-processing, inspired by non-projective transition-based parsing, to obtain non-projective sentences. Our proposed method outperforms the state-of-the-art linearizer while being 10 times faster in training and decoding. 1 Introduction Surface realization is the task of generating a sentence from a syntactic or semantic representation. In several shared tasks (Belz et al., 2011; Mille et al., 2018, 2019), the input representations are unordered dependency trees. The state-of-the-art system (Yu et al., 2019a) in the Surface Realization Shared Task 2019 (SR’19) takes a pipeline approach, where the first step is linearization, namely ordering the tokens in the dependency tree. They use a Tree-LSTM to encode each token with awareness of the whole tree, then apply the divide-andconquer strategy to split the full tree into subtrees and find the optimal order for each subtree using beam search. Finally, the linearized subtrees are combined into a full projective tree. The"
2020.acl-main.134,D12-1085,1,0.791573,"Missing"
2020.acl-main.134,C10-1012,0,0.31125,"input representations are unordered dependency trees. The state-of-the-art system (Yu et al., 2019a) in the Surface Realization Shared Task 2019 (SR’19) takes a pipeline approach, where the first step is linearization, namely ordering the tokens in the dependency tree. They use a Tree-LSTM to encode each token with awareness of the whole tree, then apply the divide-andconquer strategy to split the full tree into subtrees and find the optimal order for each subtree using beam search. Finally, the linearized subtrees are combined into a full projective tree. The general strategy is adapted from Bohnet et al. (2010). In this work, we tackle linearization decoding in a different way, by casting it as a Traveling Salesman Problem (TSP). Knight (1999) first formulated the word ordering of the target language in word-based machine translation as a TSP, where the words are the nodes to traverse, and the log probabilities of the bigrams are the edge costs. Several works have followed this formulation. Among others, Zaslavskiy et al. (2009) formulate the word ordering in phrase-based machine translation as a TSP, and show that it achieves better performance and speed than beam search decoding with the same bigr"
2020.acl-main.134,P81-1022,0,0.697299,"Missing"
2020.acl-main.134,P01-1030,0,0.37145,"e word ordering in phrase-based machine translation as a TSP, and show that it achieves better performance and speed than beam search decoding with the same bigram language model. Horvat and Byrne (2014) explore higher-order n-gram language models for TSP-based word ordering, which transforms into a much larger TSP graph. All of the aforementioned works operate on a bag of words without syntax, which is a TSP graph of non-trivial size with little information about the internal structure. Much effort has been put into incorporating more powerful decoding algorithms such as Integer Programming (Germann et al., 2001) and Dynamic Programming (Tillmann and Ney, 2003). Our work differs from the previous work on TSP-based word ordering in several aspects. (1) Linearization is a special case of word ordering with syntax, where we can use a tree-structured encoder to provide better representation of the tokens. (2) We adopt the divide-and-conquer strategy to break down the full tree into subtrees and order each subtree separately, which is faster and more reliable with an approximate decoder. (3) We apply deep biaffine attention (Dozat and Manning, 2016), which has yielded great improvements in dependency parsi"
2020.acl-main.134,E14-3010,0,0.0162235,"tackle linearization decoding in a different way, by casting it as a Traveling Salesman Problem (TSP). Knight (1999) first formulated the word ordering of the target language in word-based machine translation as a TSP, where the words are the nodes to traverse, and the log probabilities of the bigrams are the edge costs. Several works have followed this formulation. Among others, Zaslavskiy et al. (2009) formulate the word ordering in phrase-based machine translation as a TSP, and show that it achieves better performance and speed than beam search decoding with the same bigram language model. Horvat and Byrne (2014) explore higher-order n-gram language models for TSP-based word ordering, which transforms into a much larger TSP graph. All of the aforementioned works operate on a bag of words without syntax, which is a TSP graph of non-trivial size with little information about the internal structure. Much effort has been put into incorporating more powerful decoding algorithms such as Integer Programming (Germann et al., 2001) and Dynamic Programming (Tillmann and Ney, 2003). Our work differs from the previous work on TSP-based word ordering in several aspects. (1) Linearization is a special case of word"
2020.acl-main.134,Q16-1023,0,0.482967,"matrix by a margin: X X ( max(0, 1 + si,j 0 − si,j ) L= (i,j)∈z + j 0 6=j X max(0, 1 + si0 ,j − si,j )) This objective aims to maximizing the score of each correct bigram (i, j) in both directions, essentially log P (j|i) and log P (i|j), where the cells in the same row corresponds to all possible tokens following i, and the cells in the column corresponds to all possible tokens preceding j. The objective is greedy in the sense that it updates more than “necessary” to decode the correct path. We contrast it to the structured loss in most graph-based dependency parsers (McDonald et al., 2005; Kiperwasser and Goldberg, 2016), which updates the scores of the correct path z against the highest scoring incorrect path z 0 : (6) L0 = max(0, 1+ max 0 z 6=z Finally, we turn the score matrix into a nonnegative cost matrix for the TSP solver: C = max (S) − S (7) Our model is inspired by the biaffine dependency parser of Dozat and Manning (2016), but stands in contrast in many aspects. They use a bidirectional LSTM to encode the sequential information of the tokens, and the biaffine attention itself does not (8) i0 6=i X (i0 ,j 0 )∈z 0 si0 ,j 0 − X si,j ) (i,j)∈z (9) The greedy objective for the TSP has two main advantages"
2020.acl-main.134,J99-4005,0,0.652196,"19 (SR’19) takes a pipeline approach, where the first step is linearization, namely ordering the tokens in the dependency tree. They use a Tree-LSTM to encode each token with awareness of the whole tree, then apply the divide-andconquer strategy to split the full tree into subtrees and find the optimal order for each subtree using beam search. Finally, the linearized subtrees are combined into a full projective tree. The general strategy is adapted from Bohnet et al. (2010). In this work, we tackle linearization decoding in a different way, by casting it as a Traveling Salesman Problem (TSP). Knight (1999) first formulated the word ordering of the target language in word-based machine translation as a TSP, where the words are the nodes to traverse, and the log probabilities of the bigrams are the edge costs. Several works have followed this formulation. Among others, Zaslavskiy et al. (2009) formulate the word ordering in phrase-based machine translation as a TSP, and show that it achieves better performance and speed than beam search decoding with the same bigram language model. Horvat and Byrne (2014) explore higher-order n-gram language models for TSP-based word ordering, which transforms in"
2020.acl-main.134,H05-1066,0,0.422268,"Missing"
2020.acl-main.134,W18-3601,0,0.0548761,"nd use a biaffine attention model to calculate the edge costs. We facilitate the decoding by solving the TSP for each subtree and combining the solution into a projective tree. We then design a transition system as post-processing, inspired by non-projective transition-based parsing, to obtain non-projective sentences. Our proposed method outperforms the state-of-the-art linearizer while being 10 times faster in training and decoding. 1 Introduction Surface realization is the task of generating a sentence from a syntactic or semantic representation. In several shared tasks (Belz et al., 2011; Mille et al., 2018, 2019), the input representations are unordered dependency trees. The state-of-the-art system (Yu et al., 2019a) in the Surface Realization Shared Task 2019 (SR’19) takes a pipeline approach, where the first step is linearization, namely ordering the tokens in the dependency tree. They use a Tree-LSTM to encode each token with awareness of the whole tree, then apply the divide-andconquer strategy to split the full tree into subtrees and find the optimal order for each subtree using beam search. Finally, the linearized subtrees are combined into a full projective tree. The general strategy is"
2020.acl-main.134,D19-6301,0,0.312681,"Missing"
2020.acl-main.134,P09-1040,0,0.5824,"nstead of any other bigram in the same row or column. 2.5 Table 1: The shift-swap transition system. Generating Non-Projective Trees 1 If we directly linearize the full tree, the output is naturally unrestricted, i.e., possibly non-projective. However, when we linearize each subtree separately in order to reduce the search space, as in the proposed method, the reconstructed output is restricted to be projective (Bohnet et al., 2010). To relax the projectivity restriction, we design a transition system to reorder projective trees into non-projective trees as a post-processing step, inspired by Nivre (2009) but working in the opposite way. It is essentially a reduced version of their transition system, removing the attachment transitions and keeping only swap and shift. In the transition system (as shown in Table 1), a configuration consists of a stack σ, which is initially empty, and a buffer β, which initially holds all input tokens. The shift transition moves the front of the buffer to the top of the stack, and the swap transition moves the top of the stack back to the second place in the buffer. When all tokens are moved from the buffer to the stack, the procedure terminates. To prevent the"
2020.acl-main.134,L16-1262,0,0.0276742,"Missing"
2020.acl-main.134,P05-1013,0,0.108146,"sitions (cf. Vinyals et al. (2015) for the discussion on encoding a set with an LSTM). In contrast, when we only use this sys1454 tem to reorder a linearized projective tree as postprocessing, where input sequence is meaningful and consistent, it is much easier to learn. Using the swap system as a post-processing step stands in contrast to Bohnet et al. (2012), where they pre-process the tree by lifting the arcs so that the correct word order could form a projective tree. These two approaches draw inspiration from the non-projective parsing in Nivre (2009) and the pseudo-projective parsing in Nivre and Nilsson (2005) respectively. We argue that our post-processing approach is more convenient since there is no need to change the syntactic annotation in the original tree, and it is much easier to evaluate the effectiveness of the sorting model. 2.6 Relative Word Order Constraints In the SR’19 dataset, some relative word order information is given, which indicates e.g. the order of the conjuncts in the coordination. Since the order in a coordination is generally arbitrary (at least syntactically), it will thus introduce randomness in the single reference evaluation. We believe that using such information lea"
2020.acl-main.134,P02-1040,0,0.107753,"Missing"
2020.acl-main.134,N16-1058,0,0.0583907,"Baselines We use the datasets from the Surface Realization 2019 Shared Task (Mille et al., 2019) in our experiments, which includes 11 languages in 20 treebanks from the Universal Dependencies (Nivre et al., 2016). We experiment on the shallow track, i.e., all tokens in the output are present in the input tree. We only report the BLEU score (Papineni et al., 2002) as the evaluation metric, since we mostly evaluate on the lemma level, where the metrics involving word forms are irrelevant. As baselines for the final evaluation, we use several available linearizers by Bohnet et al. (2010) (B10), Puduppully et al. (2016) (P16) and Yu et al. (2019a) (Y19). B10, P16 and our linearizer all use the same inflection and contraction models, trained with the same hyperparameters as in Y19, and we compare to the reported shared task results of Y19. 3.2 Main Results Table 2 shows the performance of different linearizers, where beam is the baseline beam-search linearizer as in Yu et al. (2019b) with default hyperparameters, full is the TSP decoder on the full tree level, sub is the TSP decoder on the subtree level, and +swap is sub post-processed with reordering. We test the decoders under two conditions: without word o"
2020.acl-main.134,J03-1005,0,0.173486,"tion as a TSP, and show that it achieves better performance and speed than beam search decoding with the same bigram language model. Horvat and Byrne (2014) explore higher-order n-gram language models for TSP-based word ordering, which transforms into a much larger TSP graph. All of the aforementioned works operate on a bag of words without syntax, which is a TSP graph of non-trivial size with little information about the internal structure. Much effort has been put into incorporating more powerful decoding algorithms such as Integer Programming (Germann et al., 2001) and Dynamic Programming (Tillmann and Ney, 2003). Our work differs from the previous work on TSP-based word ordering in several aspects. (1) Linearization is a special case of word ordering with syntax, where we can use a tree-structured encoder to provide better representation of the tokens. (2) We adopt the divide-and-conquer strategy to break down the full tree into subtrees and order each subtree separately, which is faster and more reliable with an approximate decoder. (3) We apply deep biaffine attention (Dozat and Manning, 2016), which has yielded great improvements in dependency parsing, and reinterpret it as a bigram language model"
2020.acl-main.134,D19-6306,1,0.820137,"ach subtree and combining the solution into a projective tree. We then design a transition system as post-processing, inspired by non-projective transition-based parsing, to obtain non-projective sentences. Our proposed method outperforms the state-of-the-art linearizer while being 10 times faster in training and decoding. 1 Introduction Surface realization is the task of generating a sentence from a syntactic or semantic representation. In several shared tasks (Belz et al., 2011; Mille et al., 2018, 2019), the input representations are unordered dependency trees. The state-of-the-art system (Yu et al., 2019a) in the Surface Realization Shared Task 2019 (SR’19) takes a pipeline approach, where the first step is linearization, namely ordering the tokens in the dependency tree. They use a Tree-LSTM to encode each token with awareness of the whole tree, then apply the divide-andconquer strategy to split the full tree into subtrees and find the optimal order for each subtree using beam search. Finally, the linearized subtrees are combined into a full projective tree. The general strategy is adapted from Bohnet et al. (2010). In this work, we tackle linearization decoding in a different way, by castin"
2020.acl-main.134,W19-8636,1,0.70168,"ach subtree and combining the solution into a projective tree. We then design a transition system as post-processing, inspired by non-projective transition-based parsing, to obtain non-projective sentences. Our proposed method outperforms the state-of-the-art linearizer while being 10 times faster in training and decoding. 1 Introduction Surface realization is the task of generating a sentence from a syntactic or semantic representation. In several shared tasks (Belz et al., 2011; Mille et al., 2018, 2019), the input representations are unordered dependency trees. The state-of-the-art system (Yu et al., 2019a) in the Surface Realization Shared Task 2019 (SR’19) takes a pipeline approach, where the first step is linearization, namely ordering the tokens in the dependency tree. They use a Tree-LSTM to encode each token with awareness of the whole tree, then apply the divide-andconquer strategy to split the full tree into subtrees and find the optimal order for each subtree using beam search. Finally, the linearized subtrees are combined into a full projective tree. The general strategy is adapted from Bohnet et al. (2010). In this work, we tackle linearization decoding in a different way, by castin"
2020.acl-main.134,P09-1038,0,0.0336679,"es and find the optimal order for each subtree using beam search. Finally, the linearized subtrees are combined into a full projective tree. The general strategy is adapted from Bohnet et al. (2010). In this work, we tackle linearization decoding in a different way, by casting it as a Traveling Salesman Problem (TSP). Knight (1999) first formulated the word ordering of the target language in word-based machine translation as a TSP, where the words are the nodes to traverse, and the log probabilities of the bigrams are the edge costs. Several works have followed this formulation. Among others, Zaslavskiy et al. (2009) formulate the word ordering in phrase-based machine translation as a TSP, and show that it achieves better performance and speed than beam search decoding with the same bigram language model. Horvat and Byrne (2014) explore higher-order n-gram language models for TSP-based word ordering, which transforms into a much larger TSP graph. All of the aforementioned works operate on a bag of words without syntax, which is a TSP graph of non-trivial size with little information about the internal structure. Much effort has been put into incorporating more powerful decoding algorithms such as Integer"
2020.coling-main.353,W19-8015,0,0.0269396,".e. potential candidates for universals) generated by a search algorithm. The availability of comparable treebanks – syntactically annotated corpora – for a growing number of typologically distinct languages (most prominently in the collaborative Universal Dependencies project (Nivre et al., 2016)) has led to a recent surge of interest in computational work aiming to detect systematic patterns in the grammatical systems of natural languages and/or to test hypotheses from theoretical work in language typology against empirical evidence. The treebank-based approach (Liu, 2010; Lochbihler, 2017; Gerdes et al., 2019; Bjerva et al., 2019c; Hahn et al., 2020) adds a more data-driven perspective to a strand of research in computational typology (Daum´e and Campbell, 2007; Malaviya et al., 2017; Oncevay et al., 2019; Bjerva et al., 2019a; Bjerva et al., 2019b) that is based on carefully curated typological databases such as WALS1 (Dryer and Haspelmath, 2013) or URIEL2 (Littell et al., 2017). The research strand in computational typology which relies on databases essentially builds on the language features that the long tradition of typological research has identified as most relevant for identifying language"
2020.coling-main.353,D17-1268,0,0.0199201,"ypologically distinct languages (most prominently in the collaborative Universal Dependencies project (Nivre et al., 2016)) has led to a recent surge of interest in computational work aiming to detect systematic patterns in the grammatical systems of natural languages and/or to test hypotheses from theoretical work in language typology against empirical evidence. The treebank-based approach (Liu, 2010; Lochbihler, 2017; Gerdes et al., 2019; Bjerva et al., 2019c; Hahn et al., 2020) adds a more data-driven perspective to a strand of research in computational typology (Daum´e and Campbell, 2007; Malaviya et al., 2017; Oncevay et al., 2019; Bjerva et al., 2019a; Bjerva et al., 2019b) that is based on carefully curated typological databases such as WALS1 (Dryer and Haspelmath, 2013) or URIEL2 (Littell et al., 2017). The research strand in computational typology which relies on databases essentially builds on the language features that the long tradition of typological research has identified as most relevant for identifying language universals. Examples of such features are the relative order of verbs and their objects, and the order of nouns and their dependents such as adjectives, numerals and genitives."
2020.coling-main.353,L16-1262,0,0.0759724,"Missing"
2020.iwpt-1.4,P19-1012,1,0.879036,"0 Token position (b) GB head 20 (c) MTL Figure 4: The average IMPACT of tokens on BiLSTM vectors with respect to the token position and the structural (gold-standard) relation between them (heads vs. non-heads of the analyzed vector). rates slower. Since this model is globally-trained, the influence of heads does not depend on the side – the plot is almost symmetrical, suggesting that representations encode as much information about syntactic relations on the left as on the right. ticular part of the architecture is with respect to changes in input. Specifically, we use our metric IMPACT from Falenska and Kuhn (2019) that mea sures how every BiLSTM representation xi is influenced by every word representation xj from the sentence. Intuitively, IMPACT can be thought of as a percentage distributed over all words of the sentence – the higher the percentage of xj the more it  influenced the representation of xi . For every sentence from the development set  and every vector xi we calculate IMPACT values  of all words xj on xi and bucket those values according to the distance between j and i. Figure 4 shows the average impact of tokens at particular positions. We see the same two general patterns as Gaddy e"
2020.iwpt-1.4,K17-3005,0,0.0165218,"ecific ensemble dependency parsers. Since neural network training can be sensitive to initialization (Reimers and Gurevych, 2017), recent ensemble dependency parsers are rather combining models trained with different random seeds than different paradigms. For example, out of 24 teams participating in the CoNLL 2018 Shared Task on dependency parsing (Zeman et al., 2018), five employed ensemble techniques. However, all of them took advantage of either diversity coming from random seeds or different languages. Neural parsers of the same type can be combined by taking the sum of their MLP scores (Che et al., 2017), averaging softmax scores (Che et al., 2018), or through re-parsing (Kuncoro et al., 2016). The last authors also showed that such an ensemble could be distilled into a single graphbased parser. Finally, Shi et al. (2017b) used MTL in a similar way to ours. They shared BiLSTMs between three parsers to speed up their training time. However, all the models where globallytrained and the authors did not evaluate if the combination improved their performance. 7 Introduction of BiLSTMs into dependency parsers had another consequence, i.e., it enabled the use of exact search algorithms for transitio"
2020.iwpt-1.4,N18-1091,0,0.0624582,"Missing"
2020.iwpt-1.4,K18-2005,0,0.384612,"ictions for the silver-standard resource preparation (Schweitzer et al., 2018), or as analysis tools (de Lhoneux et al., 2019). One of the most significant recent developments in dependency parsing is based on encoding rich sentential context into word representations, such as BiLSTM vectors (Hochreiter and Schmidhuber, 1997; Graves and Schmidhuber, 2005) and deep contextualized word embeddings (Peters et al., 2018; Devlin et al., 2019). Including these representations as features has set a new state of the art for both graph-based and transition-based parsers (Kiperwasser and Goldberg, 2016; Che et al., 2018). However, it also brought the two architectures closer. Kulmizev et al. (2019) showed that after including deep contextualized word embeddings, the average error profiles of graph- and transition-based parsers converge, potentially reducing gains from combining them. On the other hand, the authors also noticed that the underlying trade-off between the parsing paradigms is still visible in their results. Thus, it is an open question to what extent the differences between the parsing paradigms could still be leveraged. In this paper, we fill the gaps left in understanding the behavior of transi"
2020.iwpt-1.4,C12-1059,0,0.0143524,"ers in sequence so that the second (level-1) parser can use the output of the first (level-0) parser as features (denoted STACKlevel-1 level-0 ). To generate training data for the level-1 parser, we apply 10-fold cross-validation on the training sets with the level-0 parser. Then, we follow Ouchi et al. (2014) and extract stacking features from the level-0 parser’s predictions in the form 3 We performed multiple experiments within the K&G architecture by differentiating transition-systems (e.g., removing SWAP, using the arc-hybrid system (Kuhlmann et al., 2011), and adding the dynamic oracle (Goldberg and Nivre, 2012, 2013)), graph-decoders (testing Eisner’s (1996) algorithm), feature sets (also extended feature set from Kiperwasser and Goldberg (2016)), and word representations. In all the tested scenarios, the general picture was essentially the same. Therefore, we present results only for the bestperforming configurations. 27 of supertags. More precisely, for every word wi , we build its supertag by filling the template label/hdir+hasLdep hasRdep, where label is the dependency relation, hdir denotes relative head direction, and hasLdep/hasRdep mark presence of left/right dependents. Such supertags are"
2020.iwpt-1.4,Q13-1033,0,0.0392737,"Missing"
2020.iwpt-1.4,P11-1068,0,0.0535802,"Missing"
2020.iwpt-1.4,P18-1248,0,0.0223401,"Missing"
2020.iwpt-1.4,Q16-1023,0,0.374838,"parsers1 , achieving robust predictions for the silver-standard resource preparation (Schweitzer et al., 2018), or as analysis tools (de Lhoneux et al., 2019). One of the most significant recent developments in dependency parsing is based on encoding rich sentential context into word representations, such as BiLSTM vectors (Hochreiter and Schmidhuber, 1997; Graves and Schmidhuber, 2005) and deep contextualized word embeddings (Peters et al., 2018; Devlin et al., 2019). Including these representations as features has set a new state of the art for both graph-based and transition-based parsers (Kiperwasser and Goldberg, 2016; Che et al., 2018). However, it also brought the two architectures closer. Kulmizev et al. (2019) showed that after including deep contextualized word embeddings, the average error profiles of graph- and transition-based parsers converge, potentially reducing gains from combining them. On the other hand, the authors also noticed that the underlying trade-off between the parsing paradigms is still visible in their results. Thus, it is an open question to what extent the differences between the parsing paradigms could still be leveraged. In this paper, we fill the gaps left in understanding the"
2020.iwpt-1.4,P05-1012,0,0.624344,"ained with different random seeds. Thus, an integration no longer leads to increased accuracy. When both parsers depend on BiLSTMs, the graph-based architecture has a consistent advantage. This advantage stems from globally-trained BiLSTM representations, which capture more distant look-ahead syntactic relations. Such representations can be exploited through multi-task learning, which improves the transition-based parser, especially on treebanks with a high ratio of right-headed dependencies. 1 Introduction Dependency parsers can roughly be divided into two classes: graph-based (Eisner, 1996; McDonald et al., 2005) and transition-based (Yamada and Matsumoto, 2003; Nivre, 2003). The two paradigms differ in their approach to the trade-off between access to contextual features in the output dependency tree and exactness of search (McDonald and Nivre, 2007). The complementary strengths of those paradigms have given grounds to numerous diversity-based methods for integrating parsing models (Nivre and McDonald, 2008; Sagae and Lavie, 2006, among others). To date, the methods are commonly used for improving the 1 See results from the CoNLL 2018 shared task on dependency parsing (Zeman et al., 2018) 25 Proceedi"
2020.iwpt-1.4,D19-1279,0,0.0234439,"results from the CoNLL 2018 shared task on dependency parsing (Zeman et al., 2018) 25 Proceedings of the 16th International Conference on Parsing Technologies and the IWPT 2020 Shared Task, pages 25–39 c Virtual Meeting, July 9, 2020. 2020 Association for Computational Linguistics [3] TB scores: LAlbl RAlbl SH SW s1 s0 score arclbl b0 Deep contextualized word representations. The two most popular models of deep contextualized representations are ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). Both models have been used with dependency parsers, either for multi-lingual applications (Kondratyuk and Straka, 2019; Schuster et al., 2019) or to improve parsing accuracy (Che et al., 2018; Jawahar et al., 2018; Lim et al., 2018). Recently, Kulmizev et al. (2019) analyzed the influence of both of the models on the K&G architecture and showed that they give similar results, BERT being slightly ahead. Since the scope of our experiments is to analyze the influence of contextualized embeddings on parser integration, and not to analyze differences between different embedding models, we use ELMo, which is more accessible. ELMo representations encode words within the context of the entire sentence. The representa"
2020.iwpt-1.4,D07-1013,0,0.387957,"epresentations, which capture more distant look-ahead syntactic relations. Such representations can be exploited through multi-task learning, which improves the transition-based parser, especially on treebanks with a high ratio of right-headed dependencies. 1 Introduction Dependency parsers can roughly be divided into two classes: graph-based (Eisner, 1996; McDonald et al., 2005) and transition-based (Yamada and Matsumoto, 2003; Nivre, 2003). The two paradigms differ in their approach to the trade-off between access to contextual features in the output dependency tree and exactness of search (McDonald and Nivre, 2007). The complementary strengths of those paradigms have given grounds to numerous diversity-based methods for integrating parsing models (Nivre and McDonald, 2008; Sagae and Lavie, 2006, among others). To date, the methods are commonly used for improving the 1 See results from the CoNLL 2018 shared task on dependency parsing (Zeman et al., 2018) 25 Proceedings of the 16th International Conference on Parsing Technologies and the IWPT 2020 Shared Task, pages 25–39 c Virtual Meeting, July 9, 2020. 2020 Association for Computational Linguistics [3] TB scores: LAlbl RAlbl SH SW s1 s0 score arclbl b0"
2020.iwpt-1.4,N18-1202,0,0.332659,"uage Processing 2 Lund University, Department of Astronomy and Theoretical Physics {falenska,jonas}@ims.uni-stuttgart.de anders.bjorkelund@thep.lu.se accuracy of single parsers1 , achieving robust predictions for the silver-standard resource preparation (Schweitzer et al., 2018), or as analysis tools (de Lhoneux et al., 2019). One of the most significant recent developments in dependency parsing is based on encoding rich sentential context into word representations, such as BiLSTM vectors (Hochreiter and Schmidhuber, 1997; Graves and Schmidhuber, 2005) and deep contextualized word embeddings (Peters et al., 2018; Devlin et al., 2019). Including these representations as features has set a new state of the art for both graph-based and transition-based parsers (Kiperwasser and Goldberg, 2016; Che et al., 2018). However, it also brought the two architectures closer. Kulmizev et al. (2019) showed that after including deep contextualized word embeddings, the average error profiles of graph- and transition-based parsers converge, potentially reducing gains from combining them. On the other hand, the authors also noticed that the underlying trade-off between the parsing paradigms is still visible in their re"
2020.iwpt-1.4,D13-1032,0,0.076945,"Missing"
2020.iwpt-1.4,D17-1035,0,0.121474,"all models achieve accuracy higher than 95 LAS, proving that they are capable of learning from the stacking representations. Influence of representations. Next, we consider the models which were trained without BiLSTMs and ELMo (left, lightest bars). Surprisingly, for both TB (green) and GB (blue), small improvements can be noticed in the self-application scenario S, which was not the case for non-neural models (Martins et al., 2008; Fale´nska et al., 2015). One explanation for this is the diversity of the models coming with random seeds, which was less prominent in their non-neural versions (Reimers and Gurevych, 2017). However, clearer improvements are visible in scenario O, which combines models of different types. Both GB STACK TB GB and STACK TB surpass both of the single baselines, proving that integration is beneficial when BiLSTMs and ELMo are not used. Considering the case where BiLSTMs are included (middle) changes the picture. SelfParsing Architectures and Diversity In this section, we investigate which aspects of the K&G architecture are responsible for no gains from the integration. For this purpose, we run ablation experiments and apply blending and stack29 100 –BiLSTM 100 –ELMo blendn×tb m×gb"
2020.iwpt-1.4,N06-2033,0,0.570443,"specially on treebanks with a high ratio of right-headed dependencies. 1 Introduction Dependency parsers can roughly be divided into two classes: graph-based (Eisner, 1996; McDonald et al., 2005) and transition-based (Yamada and Matsumoto, 2003; Nivre, 2003). The two paradigms differ in their approach to the trade-off between access to contextual features in the output dependency tree and exactness of search (McDonald and Nivre, 2007). The complementary strengths of those paradigms have given grounds to numerous diversity-based methods for integrating parsing models (Nivre and McDonald, 2008; Sagae and Lavie, 2006, among others). To date, the methods are commonly used for improving the 1 See results from the CoNLL 2018 shared task on dependency parsing (Zeman et al., 2018) 25 Proceedings of the 16th International Conference on Parsing Technologies and the IWPT 2020 Shared Task, pages 25–39 c Virtual Meeting, July 9, 2020. 2020 Association for Computational Linguistics [3] TB scores: LAlbl RAlbl SH SW s1 s0 score arclbl b0 Deep contextualized word representations. The two most popular models of deep contextualized representations are ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). Both models"
2020.iwpt-1.4,W03-3017,0,0.104293,"o increased accuracy. When both parsers depend on BiLSTMs, the graph-based architecture has a consistent advantage. This advantage stems from globally-trained BiLSTM representations, which capture more distant look-ahead syntactic relations. Such representations can be exploited through multi-task learning, which improves the transition-based parser, especially on treebanks with a high ratio of right-headed dependencies. 1 Introduction Dependency parsers can roughly be divided into two classes: graph-based (Eisner, 1996; McDonald et al., 2005) and transition-based (Yamada and Matsumoto, 2003; Nivre, 2003). The two paradigms differ in their approach to the trade-off between access to contextual features in the output dependency tree and exactness of search (McDonald and Nivre, 2007). The complementary strengths of those paradigms have given grounds to numerous diversity-based methods for integrating parsing models (Nivre and McDonald, 2008; Sagae and Lavie, 2006, among others). To date, the methods are commonly used for improving the 1 See results from the CoNLL 2018 shared task on dependency parsing (Zeman et al., 2018) 25 Proceedings of the 16th International Conference on Parsing Technologie"
2020.iwpt-1.4,N19-1162,0,0.0243401,"hared task on dependency parsing (Zeman et al., 2018) 25 Proceedings of the 16th International Conference on Parsing Technologies and the IWPT 2020 Shared Task, pages 25–39 c Virtual Meeting, July 9, 2020. 2020 Association for Computational Linguistics [3] TB scores: LAlbl RAlbl SH SW s1 s0 score arclbl b0 Deep contextualized word representations. The two most popular models of deep contextualized representations are ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). Both models have been used with dependency parsers, either for multi-lingual applications (Kondratyuk and Straka, 2019; Schuster et al., 2019) or to improve parsing accuracy (Che et al., 2018; Jawahar et al., 2018; Lim et al., 2018). Recently, Kulmizev et al. (2019) analyzed the influence of both of the models on the K&G architecture and showed that they give similar results, BERT being slightly ahead. Since the scope of our experiments is to analyze the influence of contextualized embeddings on parser integration, and not to analyze differences between different embedding models, we use ELMo, which is more accessible. ELMo representations encode words within the context of the entire sentence. The representations are built from a l"
2020.iwpt-1.4,P09-1040,0,0.0234427,"er is colored red in Figure 1. For every configuration consisting of a stack, buffer, and the current set of arcs, the parser builds a feature set of three items: the two top-most items of the stack and the first item on the buffer (denoted s0 , s1 , and b0 ). Next, it concatenates their BiLSTM vectors and passes on to a multi-layer perceptron (MLP, level [3] in Figure 1). The MLP scores all possible transitions, and the highest-scoring one is applied to proceed to the next configuration. Our implementation (denoted TB) uses the arcstandard transition system extended with the SWAP transition (Nivre, 2009) and can thus handle nonprojective trees.3 We use Nivre et al.’s (2009) lazy SWAP oracle for training. Labels are predicted together with the transitions. For analysis, we also use variants of TB trained without BiLSTMs. In these cases, vectors xi are passed directly to the MLP layer (similarly to Chen and Manning (2014)), and the implicit context encoded by the BiLSTMs is lost. We compensate for it by using Kiperwasser and Goldberg’s (2016) extended feature set, which adds the embedding information of eight additional tokens in the structural context of the parser state. sentence (a) Blending"
2020.iwpt-1.4,L18-1457,1,0.887996,"Missing"
2020.iwpt-1.4,W09-3811,0,0.0450792,"Missing"
2020.iwpt-1.4,P81-1022,0,0.669256,"Missing"
2020.iwpt-1.4,P08-1108,0,0.29929,"transition-based parser, especially on treebanks with a high ratio of right-headed dependencies. 1 Introduction Dependency parsers can roughly be divided into two classes: graph-based (Eisner, 1996; McDonald et al., 2005) and transition-based (Yamada and Matsumoto, 2003; Nivre, 2003). The two paradigms differ in their approach to the trade-off between access to contextual features in the output dependency tree and exactness of search (McDonald and Nivre, 2007). The complementary strengths of those paradigms have given grounds to numerous diversity-based methods for integrating parsing models (Nivre and McDonald, 2008; Sagae and Lavie, 2006, among others). To date, the methods are commonly used for improving the 1 See results from the CoNLL 2018 shared task on dependency parsing (Zeman et al., 2018) 25 Proceedings of the 16th International Conference on Parsing Technologies and the IWPT 2020 Shared Task, pages 25–39 c Virtual Meeting, July 9, 2020. 2020 Association for Computational Linguistics [3] TB scores: LAlbl RAlbl SH SW s1 s0 score arclbl b0 Deep contextualized word representations. The two most popular models of deep contextualized representations are ELMo (Peters et al., 2018) and BERT (Devlin et"
2020.iwpt-1.4,P16-2038,0,0.03112,"wo parsers that can be applied independently: one transition-based GB (denoted MTL TB GB ) and one graph-based ( MTL TB ). We use a straightforward MTL training protocol: for every sentence, we calculate the Bi LSTM representations xi and collect all local losses from both tasks (TB and GB). Then, the losses are summed and the model parameters are updated through backpropagation. We note in passing that this training protocol leaves many options for improvements, such as adding weights to losses from different tasks (Shi et al., 2017b), sharing representations on different levels of BiLSTMs (Søgaard and Goldberg, 2016), or employing stack-propagation (Zhang and Weiss, 2016). We abstain from such extensions as they are orthogonal to the central points of our analysis. 2.3 Evaluation and Analysis 3 Diversity-Based Integration We start by evaluating the two integration methods (STACK and BLEND ) and applying them to our transition- and graph-based parsers (TB and GB). Average results. The first column in Table 1 gives the average results. In the case of stacking, the performance of combined models is almost the same as that of the baseline models. Small improvements are noticeable for STACK TB GB vs. TB , but"
2020.iwpt-1.4,N19-1077,0,0.200427,"Missing"
2020.iwpt-1.4,W03-3023,0,0.135372,"ntegration no longer leads to increased accuracy. When both parsers depend on BiLSTMs, the graph-based architecture has a consistent advantage. This advantage stems from globally-trained BiLSTM representations, which capture more distant look-ahead syntactic relations. Such representations can be exploited through multi-task learning, which improves the transition-based parser, especially on treebanks with a high ratio of right-headed dependencies. 1 Introduction Dependency parsers can roughly be divided into two classes: graph-based (Eisner, 1996; McDonald et al., 2005) and transition-based (Yamada and Matsumoto, 2003; Nivre, 2003). The two paradigms differ in their approach to the trade-off between access to contextual features in the output dependency tree and exactness of search (McDonald and Nivre, 2007). The complementary strengths of those paradigms have given grounds to numerous diversity-based methods for integrating parsing models (Nivre and McDonald, 2008; Sagae and Lavie, 2006, among others). To date, the methods are commonly used for improving the 1 See results from the CoNLL 2018 shared task on dependency parsing (Zeman et al., 2018) 25 Proceedings of the 16th International Conference on Parsi"
2020.iwpt-1.4,K18-2001,0,0.0475706,"Missing"
2020.iwpt-1.4,P16-1147,0,0.0575075,"Missing"
2020.iwpt-1.4,D08-1059,0,0.0496401,"standard deviations are provided in Table 5 in Appendix A. traditional integration methods. These methods are known for exploiting diversity in the strengths and weaknesses of transition- and graph-based parsing paradigms. We found out that when models use BiLSTMs, such diversity is on the level of different random seeds. Adding deep contextualized representations on top of BiLSTMs improves the performance of both parsers but does not change the picture regarding the integration. ing, blending, or beam search-based transitionbased parsers with features strongly inspired by graph-based models (Zhang and Clark, 2008; Bohnet and Kuhn, 2012). However, combining parsers that process input left-to-right and rightto-left (Hall et al., 2007; Attardi and Dell’Orletta, 2009), or even parsers and sequence labelers (Fale´nska et al., 2015), was also proposed. Blending was usually applied to a mixture of graphbased and transition-based left-to-right and rightto-left parsers (Sagae and Lavie, 2006; Surdeanu and Manning, 2010; Bj¨orkelund et al., 2017, among others). Moreover, in the case of stacking, integrating two parsers of the same type gives at most minor improvements (Martins et al., 2008). Rich-feature sets u"
2020.lrec-1.115,P19-3018,1,0.352915,"nd the construction of border installations as a solution to the immigration problem. The claims are highlighted in colors in the text, and give rise to the corresponding parts of the network representation to the right. The actors are represented by red squares in the discourse network. Blue edges indicate support towards a claim category (Merkel supports the &quot;Refugees Welcome&quot; claim), red edges indicate opposition to it (the demonstrators stand against the claim &quot;Controlling migration with border 919 installations&quot;). Methodologically, this paper follows our previous work (Padó et al., 2019; Blessing et al., 2019). The novel contribution of this paper are: (a) the release and the documentation of the complete dataset, including a quantitative and qualitative analysis with corpus linguistic tools such as keywords and collocations (Baker et al., 2008); and (b) a quantitative/qualitative analysis of the concrete discourse network structures that arise from the annotation as well as the temporal dynamics of these structures. In Section 2, we provide background on the discourse network analysis framework from political science that we build on, and on the role of and challenges for NLP in such a study. In S"
2020.lrec-1.115,P19-1273,1,0.793254,"olation of Europe and the construction of border installations as a solution to the immigration problem. The claims are highlighted in colors in the text, and give rise to the corresponding parts of the network representation to the right. The actors are represented by red squares in the discourse network. Blue edges indicate support towards a claim category (Merkel supports the &quot;Refugees Welcome&quot; claim), red edges indicate opposition to it (the demonstrators stand against the claim &quot;Controlling migration with border 919 installations&quot;). Methodologically, this paper follows our previous work (Padó et al., 2019; Blessing et al., 2019). The novel contribution of this paper are: (a) the release and the documentation of the complete dataset, including a quantitative and qualitative analysis with corpus linguistic tools such as keywords and collocations (Baker et al., 2008); and (b) a quantitative/qualitative analysis of the concrete discourse network structures that arise from the annotation as well as the temporal dynamics of these structures. In Section 2, we provide background on the discourse network analysis framework from political science that we build on, and on the role of and challenges for N"
2020.lrec-1.115,J17-3005,0,0.0205038,"ile the projection on the concept side yields the argumentative clusters present in the debate. Clearly, manual annotation of such claims and claim-actor relations is a resource intensive process. It it therefore natural to ask if Natural Language Processing can help: What are the potentials, limitations, and the practical issues of applying NLP to the automatic construction of discourse networks? At a general level, the NLP take on debate modeling can build on the insights from argumentation mining and subjectivity analysis (Peldszus and Stede, 2013; Ceron et al., 2014; Swanson et al., 2015; Stab and Gurevych, 2017; Vilares and He, 2017). An ideal NLP tool would automatically identify the actors and their contributions to the debate, and analyze such contributions at a structural level (identifying argumentative structure in their statements), at a semantic level (classifying statements into relevant categories), and at a pragmatic level (detecting the polarity of the statements). In our concrete experience (Padó et al., 2019), however, this task cannot be completely automated, at least not if the target is to acquire representations at the level of granularity and at the level of quality which are requ"
2020.lrec-1.115,W15-4631,0,0.0243993,"on the actor side, while the projection on the concept side yields the argumentative clusters present in the debate. Clearly, manual annotation of such claims and claim-actor relations is a resource intensive process. It it therefore natural to ask if Natural Language Processing can help: What are the potentials, limitations, and the practical issues of applying NLP to the automatic construction of discourse networks? At a general level, the NLP take on debate modeling can build on the insights from argumentation mining and subjectivity analysis (Peldszus and Stede, 2013; Ceron et al., 2014; Swanson et al., 2015; Stab and Gurevych, 2017; Vilares and He, 2017). An ideal NLP tool would automatically identify the actors and their contributions to the debate, and analyze such contributions at a structural level (identifying argumentative structure in their statements), at a semantic level (classifying statements into relevant categories), and at a pragmatic level (detecting the polarity of the statements). In our concrete experience (Padó et al., 2019), however, this task cannot be completely automated, at least not if the target is to acquire representations at the level of granularity and at the level"
2020.lrec-1.115,D17-1165,0,0.028332,"concept side yields the argumentative clusters present in the debate. Clearly, manual annotation of such claims and claim-actor relations is a resource intensive process. It it therefore natural to ask if Natural Language Processing can help: What are the potentials, limitations, and the practical issues of applying NLP to the automatic construction of discourse networks? At a general level, the NLP take on debate modeling can build on the insights from argumentation mining and subjectivity analysis (Peldszus and Stede, 2013; Ceron et al., 2014; Swanson et al., 2015; Stab and Gurevych, 2017; Vilares and He, 2017). An ideal NLP tool would automatically identify the actors and their contributions to the debate, and analyze such contributions at a structural level (identifying argumentative structure in their statements), at a semantic level (classifying statements into relevant categories), and at a pragmatic level (detecting the polarity of the statements). In our concrete experience (Padó et al., 2019), however, this task cannot be completely automated, at least not if the target is to acquire representations at the level of granularity and at the level of quality which are required for the political"
2020.lrec-1.636,brants-hansen-2002-developments,0,0.0749346,"sage of categories S+ and S-. Figure 1: Examples of sentences with insertions (marked in bold). For readability reasons only annotations for top-level constituents are presented. solved under supervision of an expert. To facilitate the attachment of tokens to their correct phrases, the annotators were allowed to listen to the original sound files of the interviews to gather intonational information, if necessary. The annotation tool PhiTag3 was used for both creating and merging the annotations. Regarding the guidelines of the annotation, we followed the annotation scheme of the TIGER corpus (Brants and Hansen, 2002).4 The objective was to stay as close as possible to the original guidelines but in cases where domainrelated phenomena were not captured by them. In such cases we had to adapt the framework to our task (compare also the set-up of the NoSta-D annotations spanning several non-standard varieties of German (Dipper et al., 2013)). TIGER contains newspaper articles which represent written 3 https://phitag.de Conversion from TIGER-style trees to Universal Dependencies is not straightforward and requires manual work. Therefore, we leave it for future work. 4 and edited usage of language. By contrast,"
2020.lrec-1.636,K18-2005,0,0.0229868,"rser from Bohnet (2010) which is a component of mate-tools. To compare this model with a more state-of-the-art tool, we take the BiLSTM-based graph-based parser from IMSnPars9 described in Falenska and Kuhn (2019). The parser does not use lemmas and morphological tags. It builds token representations by concatenating pretrained word embeddings, character-based embeddings, part-of-speech tags, and ELMO deep contextualized word representations (Peters et al., 2018). For the pretrained word and ELMO representations we use the fastText vectors (Grave et al., 2018) and the German model provided by Che et al. (2018) respectively. We use default hyperparameters for both of the parsers and provide averages from three runs with different random seeds. Results. Table 3 presents parsing performance in terms of unlabeled attachment score (UAS) and labeled attachment score (LAS) for both of the parsers. As expected parsing out-of-domain datasets is more difficult than the indomain test set of TIGER. Similarly to the results of Seeker and Kuhn (2014), for models trained on newspaper articles the most challenging domains are DVD manuals and economy news. Interestingly, for the mate parser interviews are as proble"
2020.lrec-1.636,K18-2001,0,0.0128765,"ohnet (2010) which is a component of mate-tools. To compare this model with a more state-of-the-art tool, we take the BiLSTM-based graph-based parser from IMSnPars9 described in Falenska and Kuhn (2019). The parser does not use lemmas and morphological tags. It builds token representations by concatenating pretrained word embeddings, character-based embeddings, part-of-speech tags, and ELMO deep contextualized word representations (Peters et al., 2018). For the pretrained word and ELMO representations we use the fastText vectors (Grave et al., 2018) and the German model provided by Che et al. (2018) respectively. We use default hyperparameters for both of the parsers and provide averages from three runs with different random seeds. Results. Table 3 presents parsing performance in terms of unlabeled attachment score (UAS) and labeled attachment score (LAS) for both of the parsers. As expected parsing out-of-domain datasets is more difficult than the indomain test set of TIGER. Similarly to the results of Seeker and Kuhn (2014), for models trained on newspaper articles the most challenging domains are DVD manuals and economy news. Interestingly, for the mate parser interviews are as proble"
2020.lrec-1.636,P19-1012,1,0.82571,".19 89.21 85.26 86.81 82.15 81.54 86.52 interviews all interviews int. interviews guest 82.77 83.76 82.48 79.31 80.38 79.00 87.17 87.25 87.15 84.68 84.11 84.84 Table 3: Parsing performance for two dependency parsers: mate and IMSnPars. The models are trained on the training part of TIGER and applied to the out-of-domain test sets. Parsers. Following Seeker and Kuhn (2014) we use the graph-based dependency parser from Bohnet (2010) which is a component of mate-tools. To compare this model with a more state-of-the-art tool, we take the BiLSTM-based graph-based parser from IMSnPars9 described in Falenska and Kuhn (2019). The parser does not use lemmas and morphological tags. It builds token representations by concatenating pretrained word embeddings, character-based embeddings, part-of-speech tags, and ELMO deep contextualized word representations (Peters et al., 2018). For the pretrained word and ELMO representations we use the fastText vectors (Grave et al., 2018) and the German model provided by Che et al. (2018) respectively. We use default hyperparameters for both of the parsers and provide averages from three runs with different random seeds. Results. Table 3 presents parsing performance in terms of un"
2020.lrec-1.636,L18-1348,1,0.896088,"Missing"
2020.lrec-1.636,foth-etal-2014-size,0,0.0686843,"Missing"
2020.lrec-1.636,P19-1339,0,0.0582477,"Missing"
2020.lrec-1.636,L18-1550,0,0.0178882,"and Kuhn (2014) we use the graph-based dependency parser from Bohnet (2010) which is a component of mate-tools. To compare this model with a more state-of-the-art tool, we take the BiLSTM-based graph-based parser from IMSnPars9 described in Falenska and Kuhn (2019). The parser does not use lemmas and morphological tags. It builds token representations by concatenating pretrained word embeddings, character-based embeddings, part-of-speech tags, and ELMO deep contextualized word representations (Peters et al., 2018). For the pretrained word and ELMO representations we use the fastText vectors (Grave et al., 2018) and the German model provided by Che et al. (2018) respectively. We use default hyperparameters for both of the parsers and provide averages from three runs with different random seeds. Results. Table 3 presents parsing performance in terms of unlabeled attachment score (UAS) and labeled attachment score (LAS) for both of the parsers. As expected parsing out-of-domain datasets is more difficult than the indomain test set of TIGER. Similarly to the results of Seeker and Kuhn (2014), for models trained on newspaper articles the most challenging domains are DVD manuals and economy news. Interest"
2020.lrec-1.636,D13-1032,0,0.05182,"Missing"
2020.lrec-1.636,L16-1262,0,0.0176749,"deduced from first names of the speakers. While Seeker and Kuhn (2014) provide dependency trees from a conversion step (Seeker and Kuhn, 2012), NoStaD is directly annotated with dependencies. Regarding further spoken primary data, the DIRNDL corpus (Eckart et al., 2012) comes with automatically annotated constituency trees based on the German LFG-grammar by Rohrer and Forst (2006). However, the primary data are also from the news domain (read radio news) and the syntactically sound manuscripts have been used for the syntactic annotation. Nevertheless, approaches such as from Dannenberg et al. (2016) show, that there is an interest in syntactic analysis of spontaneous speech. They compare syntactic trees of American English data to the respective prosodic tree structures. However, they opt at a mostly automatic setting, thus also make use of automatically created syntactic analyses. 3. textual versions. A more detailed description of this process can be found in (Eckart and G¨artner, 2016). Apart from textual unnormalization, the gold-standard part of GRAIN consists of manually annotated part-ofspeech tags, referential information status (Riester and Baumann, 2017), questions-under-discus"
2020.lrec-1.636,N18-1202,0,0.0102137,"on the training part of TIGER and applied to the out-of-domain test sets. Parsers. Following Seeker and Kuhn (2014) we use the graph-based dependency parser from Bohnet (2010) which is a component of mate-tools. To compare this model with a more state-of-the-art tool, we take the BiLSTM-based graph-based parser from IMSnPars9 described in Falenska and Kuhn (2019). The parser does not use lemmas and morphological tags. It builds token representations by concatenating pretrained word embeddings, character-based embeddings, part-of-speech tags, and ELMO deep contextualized word representations (Peters et al., 2018). For the pretrained word and ELMO representations we use the fastText vectors (Grave et al., 2018) and the German model provided by Che et al. (2018) respectively. We use default hyperparameters for both of the parsers and provide averages from three runs with different random seeds. Results. Table 3 presents parsing performance in terms of unlabeled attachment score (UAS) and labeled attachment score (LAS) for both of the parsers. As expected parsing out-of-domain datasets is more difficult than the indomain test set of TIGER. Similarly to the results of Seeker and Kuhn (2014), for models tr"
2020.lrec-1.636,rohrer-forst-2006-improving,0,0.0823489,"848 Evelyn Seibert (f) 2015-06-20 87/23/64 2025 Evi Seibert (f) 2015-08-08 109/32/77 1633 Rebecca L¨uer (f) 2015-09-19 101/25/76 1920 Uwe Lueb (m) Table 1: GRAIN-S annotated interviews, total number of sentences: 626. Gender information is deduced from first names of the speakers. While Seeker and Kuhn (2014) provide dependency trees from a conversion step (Seeker and Kuhn, 2012), NoStaD is directly annotated with dependencies. Regarding further spoken primary data, the DIRNDL corpus (Eckart et al., 2012) comes with automatically annotated constituency trees based on the German LFG-grammar by Rohrer and Forst (2006). However, the primary data are also from the news domain (read radio news) and the syntactically sound manuscripts have been used for the syntactic annotation. Nevertheless, approaches such as from Dannenberg et al. (2016) show, that there is an interest in syntactic analysis of spontaneous speech. They compare syntactic trees of American English data to the respective prosodic tree structures. However, they opt at a mostly automatic setting, thus also make use of automatically created syntactic analyses. 3. textual versions. A more detailed description of this process can be found in (Eckart"
2020.lrec-1.636,L18-1457,1,0.839918,"Missing"
2020.lrec-1.636,seeker-kuhn-2012-making,1,0.757049,"erman Environment Agency Ingo Kramer (m) Arbeitgeberpr¨asident (BDA) President of the Confederation of German Employers’ Associations 2014-05-24 94/29/65 1894 Rebecca L¨uer (f) 2014-12-06 107/27/80 1954 Jan Seidel (m) 2015-01-24 128/41/87 1848 Evelyn Seibert (f) 2015-06-20 87/23/64 2025 Evi Seibert (f) 2015-08-08 109/32/77 1633 Rebecca L¨uer (f) 2015-09-19 101/25/76 1920 Uwe Lueb (m) Table 1: GRAIN-S annotated interviews, total number of sentences: 626. Gender information is deduced from first names of the speakers. While Seeker and Kuhn (2014) provide dependency trees from a conversion step (Seeker and Kuhn, 2012), NoStaD is directly annotated with dependencies. Regarding further spoken primary data, the DIRNDL corpus (Eckart et al., 2012) comes with automatically annotated constituency trees based on the German LFG-grammar by Rohrer and Forst (2006). However, the primary data are also from the news domain (read radio news) and the syntactically sound manuscripts have been used for the syntactic annotation. Nevertheless, approaches such as from Dannenberg et al. (2016) show, that there is an interest in syntactic analysis of spontaneous speech. They compare syntactic trees of American English data to t"
2020.lrec-1.636,seeker-kuhn-2014-domain,1,0.567568,"man treebanks are based on primary data from the news domain, such as TIGER (Brants et al., 2004), T¨uBa-D/Z (Hinrichs et al., 2004), or HDT (Foth et al., 2014). More specifically, TIGER and T¨uBaD/Z contain German newspaper data and HDT online newscasts from a technical news service. More recent approaches, such as the Universal Dependencies Project (Nivre et al., 2016), introduce German treebanks containing articles from Wikipedia and historic literary text (see the latest release v2.5 of the Universal Dependencies (Zeman et al., 2019)). NoSta-D (Dipper et al., 2013) and the test suite from Seeker and Kuhn (2014) provide common syntactic annotations for several domains. NoSta-D includes historical, chat and learner data, literary prose, newspaper texts and also spoken data from a map task. Seeker and Kuhn (2014) include DVD manuals, alpine hiking stories, text from a novel, proceedings from the European Parliament and economy news. Both datasets are based on the TIGER annotation. 5169 #sentences all/int./guest #tokens interviewer (gender) guest (gender) Karl-Josef Laumann (m) Pflege- und Patientenbeauftragter der Bundesregierung State Secretary in the Federal Ministry of Health Michael H¨uther (m) Dir"
2020.lrec-1.636,A97-1015,0,0.519061,"rovide the signal in supervised training of machine learning models, or inform processes of adaptation, generation of synthetic data, etc. Therefore, for more and more languages corpora annotated for syntactic structure have been provided to the research community – not least in response to the Universal Dependencies initiative (Nivre et al., 2016). It is known that for language-technological systems trained with supervised machine learning, there is a relatively strong dependency on the text genre, language register, content domain and other dimensions of the material in the training corpus (Sekine, 1997). For research into techniques for model adaptation and for building more corpusindependent tools, it is important to have test data that represent relevant variations of existing treebanks for the same language. For example, since adaptation of text-processing tools to spoken language is of central importance to many research and application contexts, the availability of manually annotated syntactic structures on samples of spoken utterances is crucial. In this contribution we present GRAIN-S(yntax) – a set of manually created syntactic annotations for GRAIN, a corpus of German RAdio INtervie"
2020.lrec-1.859,baroni-etal-2008-cleaneval,0,0.027994,"gchen, 2019). Over the years, there have been various attempts to clean corpora for both specific and general use in NLP with some contributions aiming to automate the process (Reynaert, 2006). In the field of machine translation, Imamura and Sumita (2002) present a method for cleaning bilingual corpora based on translation literality as measured by word-level and phrase-level correspondence in sentence pairs. As for more general applications, the special interest group of the Association for Computational Linguistics (ACL) on the Web as Corpus (ACL SIGWAC) released the shared task CLEANEVAL (Baroni et al., 2008), which aimed to clean web data for use as corpora in NLP. More recent efforts include Gra¨en et al. (2014) who cleaned the Europarl Corpus, a collection of the European Parliament’s debates. Similarly, Faaß and Eckart (2013) cleaned the German web corpus deWaC of the WaCky project (Baroni et al., 2009). Our work is close to that of Faaß and Eckart as we adopt a similar approach that requires several passes over the data with a measure to test the corpus quality. 6958 3. COHA The Corpus of Historical American English (COHA), developed by Brigham Young University, is a structured collection of"
2020.lrec-1.859,P04-3031,0,0.246734,"d to clean the annotated format first and then generate the dependent parts of the other formats using the cleaned corpus. Accordingly, the steps described in this section were performed on the annotated corpus. Annotated data CCOHA Clean data Pass 1 Pass 2 – – – – – – – – – – – – – – – – – – Evaluation – – – – – – – – – Cleaning Algorithm Adjust By the same is assied to Summer for the placid. Annotated Text Figure 2: Diagram of the annotated corpus clean-up. 4.1. Annotated Corpus clean-up The corpus clean-up was implemented using Python (Rossum, 1995) and the natural language toolkit (NLTK) (Bird and Loper, 2004). Specifically, the NLTK “Averaged Perceptron Tagger” was used to tag tokens, and NLTK “Punkt Sentence Tokenizer” was used to segment the data into sentences. The cleaning process, illustrated in Figure 2, was performed iteratively such that data were first cleaned and then manually evaluated. Based on the results of the evaluation, the cleaning algorithm would be updated and a new iteration would start where the original annotated corpus is cleaned and then evaluated. The cycle is repeated until the results of the evaluation reveal that no further improvements are needed. We explain the clean"
2020.lrec-1.859,W19-4706,0,0.0197824,"rs such as cultural changes and technological advances (Blank, 1999; Fromkin et al., 2018). The field of historical or diachronic linguistics is concerned with the study and analysis of language change over time. Over the past two decades, researchers have shown an increased interest in the various aspects of diachronic language change. This can be attributed to the advances in technology such as the digitization of historical texts, improved computational power and availability of large-scale historical corpora designed specifically for diachronic studies (Tahmasebi et al., 2018; Tang, 2018; Bowern, 2019). Large historical corpora first appeared a decade ago and quickly gained popularity because they allow researchers to test hypotheses using computational approaches that are only possible with corpora of such volume (Kutuzov et al., 2018; Dubossarsky et al., 2019; Perrone et al., 2019; Schlechtweg et al., 2019). The Corpus of Historical American English (COHA) (Davies, 2012) is a popular large-scale resource for studying lexical, syntactic and semantic change in English. Despite its many features and advantages, COHA is not without its limitations. These shortcomings, which include inconsiste"
2020.lrec-1.859,P19-1044,1,0.804653,"ave shown an increased interest in the various aspects of diachronic language change. This can be attributed to the advances in technology such as the digitization of historical texts, improved computational power and availability of large-scale historical corpora designed specifically for diachronic studies (Tahmasebi et al., 2018; Tang, 2018; Bowern, 2019). Large historical corpora first appeared a decade ago and quickly gained popularity because they allow researchers to test hypotheses using computational approaches that are only possible with corpora of such volume (Kutuzov et al., 2018; Dubossarsky et al., 2019; Perrone et al., 2019; Schlechtweg et al., 2019). The Corpus of Historical American English (COHA) (Davies, 2012) is a popular large-scale resource for studying lexical, syntactic and semantic change in English. Despite its many features and advantages, COHA is not without its limitations. These shortcomings, which include inconsistent lemmas and malformed tokens, can complicate certain tasks and increase the required time and effort to complete them. As a case in point, let us consider the original task for which we needed COHA. The task required sentence-level context extraction for a set o"
2020.lrec-1.859,E03-1076,0,0.109138,"ting corpus CCOHA offers more word tokens, less non-words, and less invalid tokens than the original COHA. While the annotated and linear text formats are available in CCOHA, the database format should be generated by interested parties. In conclusion, we discuss some of the possible improvements and steps that can be taken to further clean the corpus. First, malformed tokens that contain the pattern “P1X1 X2 ” may be cleaned using regular expressions. 6964 Second, malformed tokens that consist of one or more words could be cleaned using one of the many approaches for compound word splitting (Koehn and Knight, 2003; Norvig, 2009; Macherey et al., 2011). Third, if one wishes to use the more fine-grained POS tags of CLAWS7, it is feasible to extract tokens tagged using the coarse-grained tags and then retag them using CLAWS tagger or some heuristics. Last, by following the steps in Section 4.3. the database format of the clean corpus can be generated from the annotated data. 7. Acknowledgements We especially thank Mark Davies for his comments during the cleaning process. We would also like to thank our reviewers for their insightful feedback. The first and second authors were supported by the CRETA center"
2020.lrec-1.859,C18-1117,0,0.137912,"decades, researchers have shown an increased interest in the various aspects of diachronic language change. This can be attributed to the advances in technology such as the digitization of historical texts, improved computational power and availability of large-scale historical corpora designed specifically for diachronic studies (Tahmasebi et al., 2018; Tang, 2018; Bowern, 2019). Large historical corpora first appeared a decade ago and quickly gained popularity because they allow researchers to test hypotheses using computational approaches that are only possible with corpora of such volume (Kutuzov et al., 2018; Dubossarsky et al., 2019; Perrone et al., 2019; Schlechtweg et al., 2019). The Corpus of Historical American English (COHA) (Davies, 2012) is a popular large-scale resource for studying lexical, syntactic and semantic change in English. Despite its many features and advantages, COHA is not without its limitations. These shortcomings, which include inconsistent lemmas and malformed tokens, can complicate certain tasks and increase the required time and effort to complete them. As a case in point, let us consider the original task for which we needed COHA. The task required sentence-level cont"
2020.lrec-1.859,P11-1140,0,0.021557,"ens, less non-words, and less invalid tokens than the original COHA. While the annotated and linear text formats are available in CCOHA, the database format should be generated by interested parties. In conclusion, we discuss some of the possible improvements and steps that can be taken to further clean the corpus. First, malformed tokens that contain the pattern “P1X1 X2 ” may be cleaned using regular expressions. 6964 Second, malformed tokens that consist of one or more words could be cleaned using one of the many approaches for compound word splitting (Koehn and Knight, 2003; Norvig, 2009; Macherey et al., 2011). Third, if one wishes to use the more fine-grained POS tags of CLAWS7, it is feasible to extract tokens tagged using the coarse-grained tags and then retag them using CLAWS tagger or some heuristics. Last, by following the steps in Section 4.3. the database format of the clean corpus can be generated from the annotated data. 7. Acknowledgements We especially thank Mark Davies for his comments during the cleaning process. We would also like to thank our reviewers for their insightful feedback. The first and second authors were supported by the CRETA center funded by the German Ministry for Edu"
2020.lrec-1.859,H94-1020,0,0.454512,"read and split into sentences using NLTK Punkt sentence tokenizer. Next, all occurrences of the ‘NUL’ control character in the lemma field were replaced with the special string “<nul>”. Then, all tokens away from sentence boundaries where the lemma was either “<nul>” or “<temp>” were tagged and lemmatized given the full sentence as context. The only exception was the special token “@” which has a “<nul>” lemma. Similarly all tokens where the POS tag was “<nul>” were tagged and lemmatized in the same fashion. Considering that the NLTK “Averaged Perceptron Tagger” uses the Penn Treebank tagset (Marcus et al., 1994), the resulting POS tags were mapped to their CLAWS7 counterparts and appended with the special string “ <sub>” to help identify cleaned tokens. The mapping was manually created by the first author of this paper. In order to detect the malformed tokens around sentence boundaries, sentences were reconstructed using the NLTK segmentation results as a guide. Specifically, upon reading each token in the annotated file, it would be appended to a list of tokens that were not part of the previous NLTK sentence. This list or “partial sentence” was then compared to the current NLTK sentence and when th"
2020.lrec-1.859,W19-4707,0,0.0811786,"erest in the various aspects of diachronic language change. This can be attributed to the advances in technology such as the digitization of historical texts, improved computational power and availability of large-scale historical corpora designed specifically for diachronic studies (Tahmasebi et al., 2018; Tang, 2018; Bowern, 2019). Large historical corpora first appeared a decade ago and quickly gained popularity because they allow researchers to test hypotheses using computational approaches that are only possible with corpora of such volume (Kutuzov et al., 2018; Dubossarsky et al., 2019; Perrone et al., 2019; Schlechtweg et al., 2019). The Corpus of Historical American English (COHA) (Davies, 2012) is a popular large-scale resource for studying lexical, syntactic and semantic change in English. Despite its many features and advantages, COHA is not without its limitations. These shortcomings, which include inconsistent lemmas and malformed tokens, can complicate certain tasks and increase the required time and effort to complete them. As a case in point, let us consider the original task for which we needed COHA. The task required sentence-level context extraction for a set of target words, but wa"
2020.lrec-1.859,reynaert-2006-corpus,0,0.0714817,"next section, we describe the related work on data clean-up. Further, we give an overview of COHA and describe its features and limitations. Then, we discuss the approach taken to clean COHA and overcome its limitations in Section 4. The resulting clean corpus is presented and compared to the original corpus in Section 5. 2. Related Work Data clean-up is an essential yet time consuming process in research (Hill and Hengchen, 2019). Over the years, there have been various attempts to clean corpora for both specific and general use in NLP with some contributions aiming to automate the process (Reynaert, 2006). In the field of machine translation, Imamura and Sumita (2002) present a method for cleaning bilingual corpora based on translation literality as measured by word-level and phrase-level correspondence in sentence pairs. As for more general applications, the special interest group of the Association for Computational Linguistics (ACL) on the Web as Corpus (ACL SIGWAC) released the shared task CLEANEVAL (Baroni et al., 2008), which aimed to clean web data for use as corpora in NLP. More recent efforts include Gra¨en et al. (2014) who cleaned the Europarl Corpus, a collection of the European Pa"
2020.lrec-1.859,P19-1072,1,0.47303,"Missing"
2020.msr-1.4,P17-1183,0,0.0271995,"tation of the stack and the buffer will be updated. 2.4 Subsequent Modules The subsequent modules are exactly the same as in previous year, we only very briefly describe them, and refer the reader to Yu et al. (2019) for the details. The function word completion module takes the ordered deep dependency tree as input, and generates function words to the left and right for each content word. Only when a special symbol signifying stopping generation is predicted, we move to generate function words to the next content word. The inflection module is similar to the hard-monotonic attention model by Aharoni and Goldberg (2017), which uses a pointer to indicate the current input character in the lemma and predicts edit operations to generate the inflected word. To increase the robustness on irregular inflection, we combine the model with a rule-based system, which takes precedence if the lemma and morphological tag combination is frequent and unique in the training data. The contraction module has two steps. We first assign BIO tags to each word, which group the words that need to be contracted together. The grouped words are then concatenated as a sequence, and passed to a standard Seq2Seq model to generate the con"
2020.msr-1.4,C10-1012,0,0.0351028,"nally use a bidirectional LSTM to model the sequence, and the output vector is also added into the final representation. (seq) vi x0i = 2.2 (token) ...vn(token) )i (6) (tree) vi (seq) vi (7) = BiLSTM(v0 (token) vi + + Graph-based Linearization Similar to our previous system, we take a divide-and-conquer approach by breaking each dependency tree into subtrees, consisting of a head and several dependents, and order the subtrees individually. Finally, we combine the ordered subtrees into a full sentence. In our previous system, we used beam search to build up the sequence incrementally following Bohnet et al. (2010). It achieves good performance, however, the nature of beam search restricts the training and decoding speed, since the model has to make a calculation at every step. Instead, we use a graph-based decoding method proposed in Yu et al. (2020), in which we model the linearization task as a Traveling Salesman Problem (TSP): each word is treated as a city, the score of a bigram in the output sequence is the traveling cost from a city to another, and the optimal sequence of words is the shortest route that visits each city exactly once. In this formulation, the model only needs to calculate the cos"
2020.msr-1.4,2020.acl-main.665,0,0.0308116,"Missing"
2020.msr-1.4,D19-6301,0,0.101047,"token representation and a better linearizer, as well as a simple ensembling approach. We also experiment with data augmentation, which brings some additional performance gain. The system is available at https://github.com/EggplantElf/IMSurReal. 1 Introduction This paper presents our submission to the Surface Realization Shared Task 2020 (Mille et al., 2020). As in the previous year, we participate in both the shallow and the deep track. Additionally, we also participate in the new setting, where additional unlabeled data is permitted to train the models. Since last year’s SR’19 shared task (Mille et al., 2019), we have made some improvements over the winning system, mostly described in Yu et al. (2020). In particular, we designed a new linearization module, where we cast the task of word ordering as a Traveling Salesman Problem (TSP), and use the biaffine attention model (Dozat and Manning, 2016) to calculate the score. The new linearizer greatly reduces the decoding time and improves the performance. We also addressed a problem in the previous system which restricted the output to projective trees by employing a transition-based reordering system, which is inspired by transition-based non-projecti"
2020.msr-1.4,2020.msr-1.1,0,0.656591,"ermany firstname.lastname@ims.uni-stuttgart.de Abstract We introduce the IMS contribution to the Surface Realization Shared Task 2020. Our current system achieves substantial improvement over the state-of-the-art system from last year, mainly due to a better token representation and a better linearizer, as well as a simple ensembling approach. We also experiment with data augmentation, which brings some additional performance gain. The system is available at https://github.com/EggplantElf/IMSurReal. 1 Introduction This paper presents our submission to the Surface Realization Shared Task 2020 (Mille et al., 2020). As in the previous year, we participate in both the shallow and the deep track. Additionally, we also participate in the new setting, where additional unlabeled data is permitted to train the models. Since last year’s SR’19 shared task (Mille et al., 2019), we have made some improvements over the winning system, mostly described in Yu et al. (2020). In particular, we designed a new linearization module, where we cast the task of word ordering as a Traveling Salesman Problem (TSP), and use the biaffine attention model (Dozat and Manning, 2016) to calculate the score. The new linearizer greatl"
2020.msr-1.4,P09-1040,0,0.237158,"de some improvements over the winning system, mostly described in Yu et al. (2020). In particular, we designed a new linearization module, where we cast the task of word ordering as a Traveling Salesman Problem (TSP), and use the biaffine attention model (Dozat and Manning, 2016) to calculate the score. The new linearizer greatly reduces the decoding time and improves the performance. We also addressed a problem in the previous system which restricted the output to projective trees by employing a transition-based reordering system, which is inspired by transition-based non-projective parsing (Nivre, 2009). Inspired by the large improvement from data augmentation in Elder et al. (2020), we utilize extra training data for all languages. We parse unlabeled sentences from news and Wikipedia text, then convert the parse trees into the shared task format, and train our system on the augmented data. Finally, we apply simple ensembling by majority voting using ten models to improve the stability of the system and further push towards the performance upper bound. 2 Surface Realization System Our system largely follows our submission from the previous year (Yu et al., 2019), with some major change in th"
2020.msr-1.4,D19-6306,1,0.930467,"ion-based non-projective parsing (Nivre, 2009). Inspired by the large improvement from data augmentation in Elder et al. (2020), we utilize extra training data for all languages. We parse unlabeled sentences from news and Wikipedia text, then convert the parse trees into the shared task format, and train our system on the augmented data. Finally, we apply simple ensembling by majority voting using ten models to improve the stability of the system and further push towards the performance upper bound. 2 Surface Realization System Our system largely follows our submission from the previous year (Yu et al., 2019), with some major change in the linearization module. It consists of up to five modules working in a pipeline fashion. The first module is linearization, which orders the input dependency tree (shallow or deep alike) into a sequence of lemmata. The second module is reordering, which adjusts the sequence from the previous step such that the new sequence could be non-projective. The third module is function word completion (for the deep track only), which generates function words as the dependents of the content words from the previous step. The fourth module is morphological inflection, which t"
2020.msr-1.4,2020.acl-main.134,1,0.859074,"xperiment with data augmentation, which brings some additional performance gain. The system is available at https://github.com/EggplantElf/IMSurReal. 1 Introduction This paper presents our submission to the Surface Realization Shared Task 2020 (Mille et al., 2020). As in the previous year, we participate in both the shallow and the deep track. Additionally, we also participate in the new setting, where additional unlabeled data is permitted to train the models. Since last year’s SR’19 shared task (Mille et al., 2019), we have made some improvements over the winning system, mostly described in Yu et al. (2020). In particular, we designed a new linearization module, where we cast the task of word ordering as a Traveling Salesman Problem (TSP), and use the biaffine attention model (Dozat and Manning, 2016) to calculate the score. The new linearizer greatly reduces the decoding time and improves the performance. We also addressed a problem in the previous system which restricted the output to projective trees by employing a transition-based reordering system, which is inspired by transition-based non-projective parsing (Nivre, 2009). Inspired by the large improvement from data augmentation in Elder et"
2020.sigmorphon-1.5,Q16-1022,0,0.0616144,"Missing"
2020.sigmorphon-1.5,P17-1183,0,0.021931,"models with different inductive biases. The first type is the Finite-State-Transducer (FST) baseline by Lee et al. (2020), based on the pair n-gram model (Novak et al., 2016). The other three types are all variants of Seq2Seq models, where we use the same BiLSTM encoder to encode the input grapheme sequence. The first one is a vanilla Seq2Seq model with attention (attn), similar to Luong et al. (2015), where the decoder applies attention on the encoded input and use the attended input vector to predict the output phonemes. The second one is a hard monotonic attention model (mono), similar to Aharoni and Goldberg (2017), where the decoder uses a pointer to select the input vector to make a prediction: either producGrapheme-to-Phoneme Conversion Task and Data We first apply our framework on the grapheme-tophoneme conversion task (Gorman et al., 2020), which includes 15 languages from the WikiPron project (Lee et al., 2020) with a diverse typological spectrum: Armenian (arm), Bulgarian (bul), French (fre), Georgian (geo), Hindi (hin), Hungarian (hun), Icelandic (ice), Korean (kor), Lithuanian (lit), Modern Greek (gre), Adyghe (ady), Dutch (dut), Japanese hiragana (jpn), Romanian (rum), and Vietnamese (vie). As"
2020.sigmorphon-1.5,D19-1091,0,0.06609,"Missing"
2020.sigmorphon-1.5,K17-2002,0,0.0345938,"Missing"
2020.sigmorphon-1.5,K17-2010,0,0.0349667,"em is available at https://www.ims.uni-stuttgart.de/ en/institute/team/Yu-00010/. 1 Introduction The vast majority of languages in the world have very few annotated dataset available for training natural language processing models, if at all. Dealing with the low-resource languages has sparked much interest in the NLP community (Garrette et al., 2013; Agi´c et al., 2016; Zoph et al., 2016). When annotation is difficult to obtain, data augmentation is a common practice to increase training data size with reasonable quality to feed to powerful models (Ragni et al., 2014; Bergmanis et al., 2017; Silfverberg et al., 2017). For example, the data hallucination method by Anastasopoulos and Neubig (2019) automatically creates non-existing “words” to augment morphological inflection data, which alleviates the label bias problem in the generation model. However, the data created by such method can only help regularize the model, but cannot be viewed as valid words of a language. Orthogonal to the data augmentation approach, another commonly used method to boost model performance without changing the architecture is ensembling, i.e., by training several models of the same kind and selecting the output by majority vot"
2020.sigmorphon-1.5,W03-0407,0,0.18492,"ramework to search for the optimal ensemble and simultaneously annotate unlabeled data. The proposed method is an iterative process, which uses an ensemble of heterogeneous models to select and annotate unlabeled data based on the agreement of the ensemble, and use the annotated data to train new models, which are in turn potential members of the new ensemble. The ensemble is a subset of all trained models that maximizes the accuracy on the development set, and we use a genetic algorithm to find such combination of models. This approach can be viewed as a type of selftraining (Yarowsky, 1995; Clark et al., 2003), but instead of using the confidence of one model, we use the agreement of many models to annotate new data. The key difference is that the model diversity in the ensemble can alleviate the confirmation bias of typical self-training approaches. We apply the framework on two of the SIGMORPHON 2020 Shared Tasks: grapheme-to-phoneme conversion (Gorman et al., 2020) and morphological inflection (Vylomova et al., 2020). Our system rank the first in the former and the fourth in the latter. While analyzing the contribution of each component of our framework, we found that the data augmentation metho"
2020.sigmorphon-1.5,D13-1021,0,0.0299244,", Romanian (rum), and Vietnamese (vie). As preprocessing, we romanize the scripts of 1 https://pypi.org/project/pykakasi/ https://pypi.org/project/ hangul-romanize/ 3 https://github.com/hermitdave/ FrequencyWords/ 4 https://github.com/timarkh/ uniparser-grammar-adyghe 5 https://github.com/akalongman/ geo-words 6 Georgian is actually in OpenSubtitles, but we accidentally missed it because of a confusion with the language code. 2 72 ing a phoneme, or moving the pointer to the next position. The monotonic alignment of the input and output is obtained with the Chinese Restaurant Process following Sudoh et al. (2013), which is provided in the baseline model of the SIGMORPHON 2016 Shared Task (Cotterell et al., 2016). The third one is essentially a hybrid of hard monotonic attention model and tagging model (tag), i.e., for each grapheme we predict a short sequence of phonemes that is aligned to it. It relies on the same monotonic alignment for training. This model is different from the previous one in that it can potentially alleviate the error propagation problem, since the short sequences are nonautoregressive and independent of each other, much like tagging. For each of the three models, we further crea"
2020.sigmorphon-1.5,P13-1057,0,0.0301993,"shared tasks: graphemeto-phoneme conversion and morphological inflection. With very simple base models in the ensemble, we rank the first and the fourth in these two tasks. We show in the analysis that our system works especially well on lowresource languages. The system is available at https://www.ims.uni-stuttgart.de/ en/institute/team/Yu-00010/. 1 Introduction The vast majority of languages in the world have very few annotated dataset available for training natural language processing models, if at all. Dealing with the low-resource languages has sparked much interest in the NLP community (Garrette et al., 2013; Agi´c et al., 2016; Zoph et al., 2016). When annotation is difficult to obtain, data augmentation is a common practice to increase training data size with reasonable quality to feed to powerful models (Ragni et al., 2014; Bergmanis et al., 2017; Silfverberg et al., 2017). For example, the data hallucination method by Anastasopoulos and Neubig (2019) automatically creates non-existing “words” to augment morphological inflection data, which alleviates the label bias problem in the generation model. However, the data created by such method can only help regularize the model, but cannot be viewe"
2020.sigmorphon-1.5,2020.sigmorphon-1.2,0,0.578796,"he ensemble is a subset of all trained models that maximizes the accuracy on the development set, and we use a genetic algorithm to find such combination of models. This approach can be viewed as a type of selftraining (Yarowsky, 1995; Clark et al., 2003), but instead of using the confidence of one model, we use the agreement of many models to annotate new data. The key difference is that the model diversity in the ensemble can alleviate the confirmation bias of typical self-training approaches. We apply the framework on two of the SIGMORPHON 2020 Shared Tasks: grapheme-to-phoneme conversion (Gorman et al., 2020) and morphological inflection (Vylomova et al., 2020). Our system rank the first in the former and the fourth in the latter. While analyzing the contribution of each component of our framework, we found that the data augmentation method does not significantly improve the results for languages with medium or large training data in the shared tasks, i.e., the advantage of our system mainly comes from the massive ensemble of a variety of base models. However, when we simulate the low-resource scenario or consider only the low-resource languages, the benefit of data augmentation becomes prominent."
2020.sigmorphon-1.5,P15-2111,0,0.0646665,"Missing"
2020.sigmorphon-1.5,2020.lrec-1.521,0,0.324908,"the middle path, in which we keep half of all additional data from the previous iteration together with the selected data in the current iteration. For example, there are 3600 additional instances produced in iteration 0, 3600/2 + 3600 = 5400 in iteration 1, 5400/2 + 3600 = 6300 in iteration 2, and the size eventually converges to 3600 × 2 = 7200. 3 3.1 3.2 Models As the framework desires the models to be as diverse as possible to maximize its benefit, we employ four different types of base models with different inductive biases. The first type is the Finite-State-Transducer (FST) baseline by Lee et al. (2020), based on the pair n-gram model (Novak et al., 2016). The other three types are all variants of Seq2Seq models, where we use the same BiLSTM encoder to encode the input grapheme sequence. The first one is a vanilla Seq2Seq model with attention (attn), similar to Luong et al. (2015), where the decoder applies attention on the encoded input and use the attended input vector to predict the output phonemes. The second one is a hard monotonic attention model (mono), similar to Aharoni and Goldberg (2017), where the decoder uses a pointer to select the input vector to make a prediction: either prod"
2020.sigmorphon-1.5,L16-1147,0,0.0706111,"Missing"
2020.sigmorphon-1.5,P19-1148,0,0.0347615,"n the previous task, and each model has about 0.5M parameters. AVG 54.2 35.5 35.6 25.2 54.7 53.4 35.9 29.2 Table 3: WER on the development set for the simulated low-resource experiment in the scenarios with and without data augmentation. In each scenario, we show the average model performance and the ensemble performance in the first iteration and the best iteration. 4.3 Experiments Table 4 compares the average test accuracy between our system (IMS-00-0) and the systems of the winning teams as well as the baselines. The baselines include a hard monotonic attention model with latent alignment (Wu and Cotterell, 2019) and a carefully tuned transformer (Vaswani et al., 2017; Wu et al., 2020), noted as mono and trm. They are additionally trained with augmented data by Anastasopoulos and Neubig (2019), noted as mono-aug and trm-aug. On average, our system ranks the fourth among the participating teams and the third in the restricted setting (without external data source or cross-lingual methods). It outperforms the hard monotonic attention baseline, but not the transformer baseline. More details on the systems and their comparisons are described in Vylomova et al. (2020). Compared to the previous task, we use"
2020.sigmorphon-1.5,P95-1026,0,0.741894,"y developing a framework to search for the optimal ensemble and simultaneously annotate unlabeled data. The proposed method is an iterative process, which uses an ensemble of heterogeneous models to select and annotate unlabeled data based on the agreement of the ensemble, and use the annotated data to train new models, which are in turn potential members of the new ensemble. The ensemble is a subset of all trained models that maximizes the accuracy on the development set, and we use a genetic algorithm to find such combination of models. This approach can be viewed as a type of selftraining (Yarowsky, 1995; Clark et al., 2003), but instead of using the confidence of one model, we use the agreement of many models to annotate new data. The key difference is that the model diversity in the ensemble can alleviate the confirmation bias of typical self-training approaches. We apply the framework on two of the SIGMORPHON 2020 Shared Tasks: grapheme-to-phoneme conversion (Gorman et al., 2020) and morphological inflection (Vylomova et al., 2020). Our system rank the first in the former and the fourth in the latter. While analyzing the contribution of each component of our framework, we found that the da"
2020.sigmorphon-1.5,D16-1163,0,0.0302611,"and morphological inflection. With very simple base models in the ensemble, we rank the first and the fourth in these two tasks. We show in the analysis that our system works especially well on lowresource languages. The system is available at https://www.ims.uni-stuttgart.de/ en/institute/team/Yu-00010/. 1 Introduction The vast majority of languages in the world have very few annotated dataset available for training natural language processing models, if at all. Dealing with the low-resource languages has sparked much interest in the NLP community (Garrette et al., 2013; Agi´c et al., 2016; Zoph et al., 2016). When annotation is difficult to obtain, data augmentation is a common practice to increase training data size with reasonable quality to feed to powerful models (Ragni et al., 2014; Bergmanis et al., 2017; Silfverberg et al., 2017). For example, the data hallucination method by Anastasopoulos and Neubig (2019) automatically creates non-existing “words” to augment morphological inflection data, which alleviates the label bias problem in the generation model. However, the data created by such method can only help regularize the model, but cannot be viewed as valid words of a language. Orthogon"
2020.udw-1.8,2020.tlt-1.9,0,0.0845961,"Missing"
2020.udw-1.8,W17-7624,0,0.0446874,"Missing"
2020.udw-1.8,W18-6013,0,0.0897531,"et al., 2008; de Marneffe et al., 2017) or calculating the fitness scores of the dependency relations in a tree (Alzetta et al., 2017). In contrast to these studies, we focus on finding inconsistencies between treebanks that are supposed to be annotated in the same way according to a common guideline. Our method builds on the assumption that each individual treebank is already largely consistent within itself, and seeks to identify dependency patterns with contradicting statistics across treebanks. So far, annotation inconsistencies have been reported for some specific languages, e.g. Korean (Noh et al., 2018) and Russian (Droganova et al., 2018), but there seems to be no comprehensive study addressing these cross-treebank inconsistencies within the UD collection. In our pilot study, we experiment with a dependency-based measure for inconsistency detection that makes no specific language-typological assumptions and appears to be quite effective for detecting various types of inconsistencies. In two experiments, we demonstrate that inconsistencies detected this way can be used as the trigger for a comparatively simple, but effective conversion process on one of the treebanks: a parser trained on the"
2020.udw-1.8,L16-1680,0,0.0564055,"Missing"
2021.acl-long.543,D16-1250,0,0.0167507,"2020) and DIACR-Ita (Basile et al., 2020), we use the Skip-gram with Negative Sampling model (SGNS, Mikolov et al., 2013a,b) to create static word embeddings. SGNS is a shallow neural language model trained on pairs of word co-occurrences extracted from a corpus with a symmetric window. The optimized parameters can be interpreted as a semantic vector space that contains the word vectors for all words in the vocabulary. In our case, we obtain two separately trained vector spaces, one for each subcorpus (C1 and C2 ). Following standard practice, both spaces are length-normalized, mean-centered (Artetxe et al., 2016; Schlechtweg et al., 2019) and then aligned by applying Orthogonal Procrustes (OP), because columns from different vector spaces may not correspond to the same coordinate axes (Hamilton et al., 2016). The change between two time-specific embeddings is measured by calculating their Cosine Distance (CD) (Salton and McGill, 1983). The strength of SGNS+OP+CD has been shown in two recent shared tasks with this sub-system combination ranking among the best submissions (Arefyev and Zhikov, 2020; Kaiser et al., 2020b; P¨omsl and Lyapin, 2020; Praˇza´ k et al., 2020). 4.2 Token-based approach Bidirect"
2021.acl-long.543,N19-1423,0,0.00972412,"g Orthogonal Procrustes (OP), because columns from different vector spaces may not correspond to the same coordinate axes (Hamilton et al., 2016). The change between two time-specific embeddings is measured by calculating their Cosine Distance (CD) (Salton and McGill, 1983). The strength of SGNS+OP+CD has been shown in two recent shared tasks with this sub-system combination ranking among the best submissions (Arefyev and Zhikov, 2020; Kaiser et al., 2020b; P¨omsl and Lyapin, 2020; Praˇza´ k et al., 2020). 4.2 Token-based approach Bidirectional Encoder Representations from Transformers (BERT, Devlin et al., 2019) is a transformer-based neural language model designed to find contextualized representations for text by analyzing left and right contexts. The base version processes text in 12 different layers. In each layer, a contextualized token vector representation is created for every word. A layer, or a combination of multiple layers (we use the average), then serves as a representation for a token. For every target word we extract usages (i.e., sentences in which the word appears) by randomly sub-sampling up to 100 sentences from both subcorpora C1 and C2 .1 These are then fed into BERT to create co"
2021.acl-long.543,P19-1044,1,0.884283,"Missing"
2021.acl-long.543,D17-1118,0,0.0958213,"can successfully be applied to discover new words undergoing meaning change. Furthermore, we provide an almost fully automated framework for both evaluation and discovery. 1 Introduction There has been considerable progress in Lexical Semantic Change Detection (LSCD) in recent years (Kutuzov et al., 2018; Tahmasebi et al., 2018; Hengchen et al., 2021), with milestones such as the first approaches using neural language models (Kim et al., 2014; Kulkarni et al., 2015), the introduction of Orthogonal Procrustes alignment (Kulkarni et al., 2015; Hamilton et al., 2016), detecting sources of noise (Dubossarsky et al., 2017, 2019), the formulation of continuous models (Frermann and Lapata, 2016; Rosenfeld and Erk, 2018; Tsakalidis and Liakata, 2020), the first uses of contextualized embeddings (Hu et al., 2019; Giulianelli et al., 2020), the development of solid annotation and evaluation frameworks (Schlechtweg et al., 2018, 2019; Shoemark et al., 2019) and shared tasks (Basile et al., 2020; Schlechtweg et al., 2020). However, only a very limited amount of work applies the methods to discover novel instances of semantic change and to evaluate the usefulness of such discovered senses for external fields. That is,"
2021.acl-long.543,D19-1006,0,0.0250897,"ance (Dubossarsky et al., 2017; Schlechtweg and Schulte im Walde, 2020). 4 Models Type-based models generate a single vector for each word from a pre-defined vocabulary. In contrast, token-based models generate one vector for each usage of a word. While the former do not take into account that most words have multiple senses, the latter are able to capture this particular aspect and are thus presumably more suited for the task of LSCD (Martinc et al., 2020). Even though contextualized approaches have indeed significantly outperformed static approaches in several NLP tasks over the past years (Ethayarajh, 2019), the field of LSCD is still dominated by type-based models (Schlechtweg et al., 2020). Kutuzov and Giulianelli (2020) yet show that the performance of tokenbased models (especially ELMo) can be increased by fine-tuning on the target corpora. Laicher et al. (2020, 2021) drastically improve the performance of BERT by reducing the influence of target word morphology. In this paper, we compare both families of approaches for change discovery. 6986 4.1 Type-based approach Most type-based approaches in LSCD combine three sub-systems: (i) creating semantic word representations, (ii) aligning them ac"
2021.acl-long.543,Q16-1003,0,0.0215972,"ange. Furthermore, we provide an almost fully automated framework for both evaluation and discovery. 1 Introduction There has been considerable progress in Lexical Semantic Change Detection (LSCD) in recent years (Kutuzov et al., 2018; Tahmasebi et al., 2018; Hengchen et al., 2021), with milestones such as the first approaches using neural language models (Kim et al., 2014; Kulkarni et al., 2015), the introduction of Orthogonal Procrustes alignment (Kulkarni et al., 2015; Hamilton et al., 2016), detecting sources of noise (Dubossarsky et al., 2017, 2019), the formulation of continuous models (Frermann and Lapata, 2016; Rosenfeld and Erk, 2018; Tsakalidis and Liakata, 2020), the first uses of contextualized embeddings (Hu et al., 2019; Giulianelli et al., 2020), the development of solid annotation and evaluation frameworks (Schlechtweg et al., 2018, 2019; Shoemark et al., 2019) and shared tasks (Basile et al., 2020; Schlechtweg et al., 2020). However, only a very limited amount of work applies the methods to discover novel instances of semantic change and to evaluate the usefulness of such discovered senses for external fields. That is, the majority of research focuses on the introduction of novel LSCD mode"
2021.acl-long.543,2020.acl-main.365,0,0.0284746,"Missing"
2021.acl-long.543,W11-2508,0,0.0169691,"et al., 2018; Perrone et al., 2019; Basile et al., 2020; Rodina and Kutuzov, 2020; Schlechtweg et al., 2020). Contrary to this, our goal is to find ‘undiscovered’ changing words and validate the predictions of our models by human annotators. Few studies focus on this task. Kim et al. (2014), Hamilton et al. (2016), Basile et al. (2016), Basile and Mcgillivray (2018), Takamura et al. (2017) and Tsakalidis et al. (2019) evaluate their approaches by validating the top ranked words through author intuitions or known historical data. The only approaches applying a systematic annotation process are Gulordava and Baroni (2011) and Cook et al. (2013). Gulordava and Baroni ask human annotators to rate 100 randomly sampled words on a 4-point scale from 0 (no change) to 3 (changed significantly), however without relating this to a data set. Cook et al. work closely with a professional lexicographer to inspect 20 lemmas predicted by their models plus 10 randomly selected ones. Gulordava and Baroni and Cook et al. evaluate their predictions on the (macro) lemma level. We, however, annotate our predictions on the (micro) usage level, enabling us to better control the criteria for annotation and their inter-subjectivity. I"
2021.acl-long.543,P16-1141,0,0.324875,"lished German data, we demonstrate that both models can successfully be applied to discover new words undergoing meaning change. Furthermore, we provide an almost fully automated framework for both evaluation and discovery. 1 Introduction There has been considerable progress in Lexical Semantic Change Detection (LSCD) in recent years (Kutuzov et al., 2018; Tahmasebi et al., 2018; Hengchen et al., 2021), with milestones such as the first approaches using neural language models (Kim et al., 2014; Kulkarni et al., 2015), the introduction of Orthogonal Procrustes alignment (Kulkarni et al., 2015; Hamilton et al., 2016), detecting sources of noise (Dubossarsky et al., 2017, 2019), the formulation of continuous models (Frermann and Lapata, 2016; Rosenfeld and Erk, 2018; Tsakalidis and Liakata, 2020), the first uses of contextualized embeddings (Hu et al., 2019; Giulianelli et al., 2020), the development of solid annotation and evaluation frameworks (Schlechtweg et al., 2018, 2019; Shoemark et al., 2019) and shared tasks (Basile et al., 2020; Schlechtweg et al., 2020). However, only a very limited amount of work applies the methods to discover novel instances of semantic change and to evaluate the usefulness o"
2021.acl-long.543,P19-1379,0,0.0131143,"en considerable progress in Lexical Semantic Change Detection (LSCD) in recent years (Kutuzov et al., 2018; Tahmasebi et al., 2018; Hengchen et al., 2021), with milestones such as the first approaches using neural language models (Kim et al., 2014; Kulkarni et al., 2015), the introduction of Orthogonal Procrustes alignment (Kulkarni et al., 2015; Hamilton et al., 2016), detecting sources of noise (Dubossarsky et al., 2017, 2019), the formulation of continuous models (Frermann and Lapata, 2016; Rosenfeld and Erk, 2018; Tsakalidis and Liakata, 2020), the first uses of contextualized embeddings (Hu et al., 2019; Giulianelli et al., 2020), the development of solid annotation and evaluation frameworks (Schlechtweg et al., 2018, 2019; Shoemark et al., 2019) and shared tasks (Basile et al., 2020; Schlechtweg et al., 2020). However, only a very limited amount of work applies the methods to discover novel instances of semantic change and to evaluate the usefulness of such discovered senses for external fields. That is, the majority of research focuses on the introduction of novel LSCD models, and on analyzing and evaluating existing models. Up to now, these preferences for development and analysis vs. app"
2021.acl-long.543,2021.eacl-main.10,1,0.752004,"Missing"
2021.acl-long.543,2020.semeval-1.8,1,0.859634,"C2 ). Following standard practice, both spaces are length-normalized, mean-centered (Artetxe et al., 2016; Schlechtweg et al., 2019) and then aligned by applying Orthogonal Procrustes (OP), because columns from different vector spaces may not correspond to the same coordinate axes (Hamilton et al., 2016). The change between two time-specific embeddings is measured by calculating their Cosine Distance (CD) (Salton and McGill, 1983). The strength of SGNS+OP+CD has been shown in two recent shared tasks with this sub-system combination ranking among the best submissions (Arefyev and Zhikov, 2020; Kaiser et al., 2020b; P¨omsl and Lyapin, 2020; Praˇza´ k et al., 2020). 4.2 Token-based approach Bidirectional Encoder Representations from Transformers (BERT, Devlin et al., 2019) is a transformer-based neural language model designed to find contextualized representations for text by analyzing left and right contexts. The base version processes text in 12 different layers. In each layer, a contextualized token vector representation is created for every word. A layer, or a combination of multiple layers (we use the average), then serves as a representation for a token. For every target word we extract usages (i."
2021.acl-long.543,C18-1117,0,0.0170949,"of existing models. In this paper, we propose a shift of focus from change detection to change discovery, i.e., discovering novel word senses over time from the full corpus vocabulary. By heavily fine-tuning a type-based and a token-based approach on recently published German data, we demonstrate that both models can successfully be applied to discover new words undergoing meaning change. Furthermore, we provide an almost fully automated framework for both evaluation and discovery. 1 Introduction There has been considerable progress in Lexical Semantic Change Detection (LSCD) in recent years (Kutuzov et al., 2018; Tahmasebi et al., 2018; Hengchen et al., 2021), with milestones such as the first approaches using neural language models (Kim et al., 2014; Kulkarni et al., 2015), the introduction of Orthogonal Procrustes alignment (Kulkarni et al., 2015; Hamilton et al., 2016), detecting sources of noise (Dubossarsky et al., 2017, 2019), the formulation of continuous models (Frermann and Lapata, 2016; Rosenfeld and Erk, 2018; Tsakalidis and Liakata, 2020), the first uses of contextualized embeddings (Hu et al., 2019; Giulianelli et al., 2020), the development of solid annotation and evaluation frameworks"
2021.acl-long.543,2021.eacl-srw.25,1,0.832016,"Missing"
2021.acl-long.543,W14-2517,0,0.181317,"r time from the full corpus vocabulary. By heavily fine-tuning a type-based and a token-based approach on recently published German data, we demonstrate that both models can successfully be applied to discover new words undergoing meaning change. Furthermore, we provide an almost fully automated framework for both evaluation and discovery. 1 Introduction There has been considerable progress in Lexical Semantic Change Detection (LSCD) in recent years (Kutuzov et al., 2018; Tahmasebi et al., 2018; Hengchen et al., 2021), with milestones such as the first approaches using neural language models (Kim et al., 2014; Kulkarni et al., 2015), the introduction of Orthogonal Procrustes alignment (Kulkarni et al., 2015; Hamilton et al., 2016), detecting sources of noise (Dubossarsky et al., 2017, 2019), the formulation of continuous models (Frermann and Lapata, 2016; Rosenfeld and Erk, 2018; Tsakalidis and Liakata, 2020), the first uses of contextualized embeddings (Hu et al., 2019; Giulianelli et al., 2020), the development of solid annotation and evaluation frameworks (Schlechtweg et al., 2018, 2019; Shoemark et al., 2019) and shared tasks (Basile et al., 2020; Schlechtweg et al., 2020). However, only a ver"
2021.acl-long.543,2020.semeval-1.14,0,0.0605088,"ate a single vector for each word from a pre-defined vocabulary. In contrast, token-based models generate one vector for each usage of a word. While the former do not take into account that most words have multiple senses, the latter are able to capture this particular aspect and are thus presumably more suited for the task of LSCD (Martinc et al., 2020). Even though contextualized approaches have indeed significantly outperformed static approaches in several NLP tasks over the past years (Ethayarajh, 2019), the field of LSCD is still dominated by type-based models (Schlechtweg et al., 2020). Kutuzov and Giulianelli (2020) yet show that the performance of tokenbased models (especially ELMo) can be increased by fine-tuning on the target corpora. Laicher et al. (2020, 2021) drastically improve the performance of BERT by reducing the influence of target word morphology. In this paper, we compare both families of approaches for change discovery. 6986 4.1 Type-based approach Most type-based approaches in LSCD combine three sub-systems: (i) creating semantic word representations, (ii) aligning them across corpora, and (iii) measuring differences between the aligned representations (Schlechtweg et al., 2019). Motivate"
2021.acl-long.543,2021.naacl-main.369,0,0.0262622,". ‘It is true that with the emergence of the manufactory, in contrast to the handicraft, traces of child labor are showing.’ (2) Sie wissen, daß wir f¨ur das Vieh mehr Futter aus eigenem Aufkommen brauchen. ‘They know that we need more feed from our own production for the cattle.’ The annotated data of a word is represented in a Word Usage Graph (WUG), where vertices represent word usages, and weights on edges represent 3 In a practical setting where predictions have to be generated only once, a much larger number may be chosen. Also, possibilities to scale up BERT performance can be applied (Montariol et al., 2021). 6988 C1 full C2 Figure 1: Word Usage Graph of German Aufkommen (left), subgraphs for first time period C1 (middle) and for second time period C2 (right). black/gray lines indicate high/low edge weights. the (median) semantic relatedness judgment of a pair of usages such as (1) and (2). The final WUGs are clustered with a variation of correlation clustering (Bansal et al., 2004; Schlechtweg et al., 2020) (see Figure 1, left) and split into two subgraphs representing nodes from subcorpora C1 and C2 , respectively (middle and right). Clusters are then interpreted as word senses and changes in c"
2021.acl-long.543,W19-4707,0,0.0189371,"elated Work State-of-the-art semantic change detection models are Vector Space Models (VSMs) (Schlechtweg et al., 2020). These can be divided into type-based (static) (Turney and Pantel, 2010) and token-based (contextualized) (Sch¨utze, 1998) approaches. For our study, we use both a static and a contextualized model. As mentioned above, previous work mostly focuses on creating data sets or developing, evaluating and analyzing models. A common approach for evaluation is to annotate target words selected from dictionaries in specific corpora (Tahmasebi and Risse, 2017; Schlechtweg et al., 2018; Perrone et al., 2019; Basile et al., 2020; Rodina and Kutuzov, 2020; Schlechtweg et al., 2020). Contrary to this, our goal is to find ‘undiscovered’ changing words and validate the predictions of our models by human annotators. Few studies focus on this task. Kim et al. (2014), Hamilton et al. (2016), Basile et al. (2016), Basile and Mcgillivray (2018), Takamura et al. (2017) and Tsakalidis et al. (2019) evaluate their approaches by validating the top ranked words through author intuitions or known historical data. The only approaches applying a systematic annotation process are Gulordava and Baroni (2011) and Co"
2021.acl-long.543,2020.semeval-1.21,0,0.0234657,"Missing"
2021.acl-long.543,2020.coling-main.90,0,0.0280782,"e detection models are Vector Space Models (VSMs) (Schlechtweg et al., 2020). These can be divided into type-based (static) (Turney and Pantel, 2010) and token-based (contextualized) (Sch¨utze, 1998) approaches. For our study, we use both a static and a contextualized model. As mentioned above, previous work mostly focuses on creating data sets or developing, evaluating and analyzing models. A common approach for evaluation is to annotate target words selected from dictionaries in specific corpora (Tahmasebi and Risse, 2017; Schlechtweg et al., 2018; Perrone et al., 2019; Basile et al., 2020; Rodina and Kutuzov, 2020; Schlechtweg et al., 2020). Contrary to this, our goal is to find ‘undiscovered’ changing words and validate the predictions of our models by human annotators. Few studies focus on this task. Kim et al. (2014), Hamilton et al. (2016), Basile et al. (2016), Basile and Mcgillivray (2018), Takamura et al. (2017) and Tsakalidis et al. (2019) evaluate their approaches by validating the top ranked words through author intuitions or known historical data. The only approaches applying a systematic annotation process are Gulordava and Baroni (2011) and Cook et al. (2013). Gulordava and Baroni ask huma"
2021.acl-long.543,N18-1044,0,0.0178523,"de an almost fully automated framework for both evaluation and discovery. 1 Introduction There has been considerable progress in Lexical Semantic Change Detection (LSCD) in recent years (Kutuzov et al., 2018; Tahmasebi et al., 2018; Hengchen et al., 2021), with milestones such as the first approaches using neural language models (Kim et al., 2014; Kulkarni et al., 2015), the introduction of Orthogonal Procrustes alignment (Kulkarni et al., 2015; Hamilton et al., 2016), detecting sources of noise (Dubossarsky et al., 2017, 2019), the formulation of continuous models (Frermann and Lapata, 2016; Rosenfeld and Erk, 2018; Tsakalidis and Liakata, 2020), the first uses of contextualized embeddings (Hu et al., 2019; Giulianelli et al., 2020), the development of solid annotation and evaluation frameworks (Schlechtweg et al., 2018, 2019; Shoemark et al., 2019) and shared tasks (Basile et al., 2020; Schlechtweg et al., 2020). However, only a very limited amount of work applies the methods to discover novel instances of semantic change and to evaluate the usefulness of such discovered senses for external fields. That is, the majority of research focuses on the introduction of novel LSCD models, and on analyzing and"
2021.acl-long.543,P19-1072,1,0.778778,"Missing"
2021.acl-long.543,2020.semeval-1.1,1,0.439261,"using neural language models (Kim et al., 2014; Kulkarni et al., 2015), the introduction of Orthogonal Procrustes alignment (Kulkarni et al., 2015; Hamilton et al., 2016), detecting sources of noise (Dubossarsky et al., 2017, 2019), the formulation of continuous models (Frermann and Lapata, 2016; Rosenfeld and Erk, 2018; Tsakalidis and Liakata, 2020), the first uses of contextualized embeddings (Hu et al., 2019; Giulianelli et al., 2020), the development of solid annotation and evaluation frameworks (Schlechtweg et al., 2018, 2019; Shoemark et al., 2019) and shared tasks (Basile et al., 2020; Schlechtweg et al., 2020). However, only a very limited amount of work applies the methods to discover novel instances of semantic change and to evaluate the usefulness of such discovered senses for external fields. That is, the majority of research focuses on the introduction of novel LSCD models, and on analyzing and evaluating existing models. Up to now, these preferences for development and analysis vs. application represented a well-motivated choice, because the quality of state-of-the-art models had not been established yet, and because no tuning and testing data were available. But with recent advances in evalu"
2021.acl-long.543,N18-2027,1,0.910751,"Tahmasebi et al., 2018; Hengchen et al., 2021), with milestones such as the first approaches using neural language models (Kim et al., 2014; Kulkarni et al., 2015), the introduction of Orthogonal Procrustes alignment (Kulkarni et al., 2015; Hamilton et al., 2016), detecting sources of noise (Dubossarsky et al., 2017, 2019), the formulation of continuous models (Frermann and Lapata, 2016; Rosenfeld and Erk, 2018; Tsakalidis and Liakata, 2020), the first uses of contextualized embeddings (Hu et al., 2019; Giulianelli et al., 2020), the development of solid annotation and evaluation frameworks (Schlechtweg et al., 2018, 2019; Shoemark et al., 2019) and shared tasks (Basile et al., 2020; Schlechtweg et al., 2020). However, only a very limited amount of work applies the methods to discover novel instances of semantic change and to evaluate the usefulness of such discovered senses for external fields. That is, the majority of research focuses on the introduction of novel LSCD models, and on analyzing and evaluating existing models. Up to now, these preferences for development and analysis vs. application represented a well-motivated choice, because the quality of state-of-the-art models had not been establishe"
2021.acl-long.543,2021.emnlp-main.567,1,0.783221,"Missing"
2021.acl-long.543,J98-1004,0,0.474416,"Missing"
2021.acl-long.543,D19-1007,0,0.0346962,"Missing"
2021.acl-long.543,tahmasebi-risse-2017-finding,0,0.0168705,". ©2021 Association for Computational Linguistics 2 Related Work State-of-the-art semantic change detection models are Vector Space Models (VSMs) (Schlechtweg et al., 2020). These can be divided into type-based (static) (Turney and Pantel, 2010) and token-based (contextualized) (Sch¨utze, 1998) approaches. For our study, we use both a static and a contextualized model. As mentioned above, previous work mostly focuses on creating data sets or developing, evaluating and analyzing models. A common approach for evaluation is to annotate target words selected from dictionaries in specific corpora (Tahmasebi and Risse, 2017; Schlechtweg et al., 2018; Perrone et al., 2019; Basile et al., 2020; Rodina and Kutuzov, 2020; Schlechtweg et al., 2020). Contrary to this, our goal is to find ‘undiscovered’ changing words and validate the predictions of our models by human annotators. Few studies focus on this task. Kim et al. (2014), Hamilton et al. (2016), Basile et al. (2016), Basile and Mcgillivray (2018), Takamura et al. (2017) and Tsakalidis et al. (2019) evaluate their approaches by validating the top ranked words through author intuitions or known historical data. The only approaches applying a systematic annotatio"
2021.acl-long.543,E17-1112,0,0.0248711,"tly focuses on creating data sets or developing, evaluating and analyzing models. A common approach for evaluation is to annotate target words selected from dictionaries in specific corpora (Tahmasebi and Risse, 2017; Schlechtweg et al., 2018; Perrone et al., 2019; Basile et al., 2020; Rodina and Kutuzov, 2020; Schlechtweg et al., 2020). Contrary to this, our goal is to find ‘undiscovered’ changing words and validate the predictions of our models by human annotators. Few studies focus on this task. Kim et al. (2014), Hamilton et al. (2016), Basile et al. (2016), Basile and Mcgillivray (2018), Takamura et al. (2017) and Tsakalidis et al. (2019) evaluate their approaches by validating the top ranked words through author intuitions or known historical data. The only approaches applying a systematic annotation process are Gulordava and Baroni (2011) and Cook et al. (2013). Gulordava and Baroni ask human annotators to rate 100 randomly sampled words on a 4-point scale from 0 (no change) to 3 (changed significantly), however without relating this to a data set. Cook et al. work closely with a professional lexicographer to inspect 20 lemmas predicted by their models plus 10 randomly selected ones. Gulordava an"
2021.acl-long.543,R19-1139,0,0.0224395,"a sets or developing, evaluating and analyzing models. A common approach for evaluation is to annotate target words selected from dictionaries in specific corpora (Tahmasebi and Risse, 2017; Schlechtweg et al., 2018; Perrone et al., 2019; Basile et al., 2020; Rodina and Kutuzov, 2020; Schlechtweg et al., 2020). Contrary to this, our goal is to find ‘undiscovered’ changing words and validate the predictions of our models by human annotators. Few studies focus on this task. Kim et al. (2014), Hamilton et al. (2016), Basile et al. (2016), Basile and Mcgillivray (2018), Takamura et al. (2017) and Tsakalidis et al. (2019) evaluate their approaches by validating the top ranked words through author intuitions or known historical data. The only approaches applying a systematic annotation process are Gulordava and Baroni (2011) and Cook et al. (2013). Gulordava and Baroni ask human annotators to rate 100 randomly sampled words on a 4-point scale from 0 (no change) to 3 (changed significantly), however without relating this to a data set. Cook et al. work closely with a professional lexicographer to inspect 20 lemmas predicted by their models plus 10 randomly selected ones. Gulordava and Baroni and Cook et al. eval"
2021.acl-long.543,2020.emnlp-main.682,0,0.0358132,"ted framework for both evaluation and discovery. 1 Introduction There has been considerable progress in Lexical Semantic Change Detection (LSCD) in recent years (Kutuzov et al., 2018; Tahmasebi et al., 2018; Hengchen et al., 2021), with milestones such as the first approaches using neural language models (Kim et al., 2014; Kulkarni et al., 2015), the introduction of Orthogonal Procrustes alignment (Kulkarni et al., 2015; Hamilton et al., 2016), detecting sources of noise (Dubossarsky et al., 2017, 2019), the formulation of continuous models (Frermann and Lapata, 2016; Rosenfeld and Erk, 2018; Tsakalidis and Liakata, 2020), the first uses of contextualized embeddings (Hu et al., 2019; Giulianelli et al., 2020), the development of solid annotation and evaluation frameworks (Schlechtweg et al., 2018, 2019; Shoemark et al., 2019) and shared tasks (Basile et al., 2020; Schlechtweg et al., 2020). However, only a very limited amount of work applies the methods to discover novel instances of semantic change and to evaluate the usefulness of such discovered senses for external fields. That is, the majority of research focuses on the introduction of novel LSCD models, and on analyzing and evaluating existing models. Up"
2021.conll-1.41,W10-3001,0,0.100659,"Missing"
2021.conll-1.41,2020.blackboxnlp-1.16,0,0.243528,"co, tion include sentiment analysis (Wiegand et al., 2012), building robust computational models is far from being a solved task, in part due to a lack of 2010; Moore and Barnes, 2021) and information annotation standards (Jiménez-Zafra et al., 2020b). extraction. Negation is also still a challenge in machine translation (Fancellu and Webber, 2015; Negation resolution has traditionally been adBentivogli et al., 2016; Hossain et al., 2020a) and dressed by heavily relying on syntactic parses (e.g. natural language inference (Hossain et al., 2020c; Sanchez Graillet and Poesio, 2007; Sohn et al., Geiger et al., 2020). 2012; Mehrabi et al., 2015). Recently, end-to-end Negation Resolution (Morante and Blanco, neural approaches to modeling negation resolu2021) refers to the task of automatically retriev- tion (Fancellu et al., 2016, 2018; Khandelwal and ing the elements of a sentence that are affected by Sawant, 2020; Kurtz et al., 2020) have claimed 528 Proceedings of the 25th Conference on Computational Natural Language Learning (CoNLL), pages 528–543 November 10–11, 2021. ©2021 Association for Computational Linguistics superior performance. A significant problem in the field of negation resolution is that"
2021.conll-1.41,C10-1076,0,0.0150901,"rules over syntactic 3.1 Metrics used in Shared Tasks structures to resolve negation or speculation scopes Previously proposed metrics for negation resolu(e.g., Velldal et al., 2012; Packard et al., 2014; tion can be divided into metrics requiring exact cue McKenna and Steedman, 2020) or training systems matches and metrics requiring partial cue matches with explicit or learned syntactic features (Read (perhaps confusingly called No Cue Match). et al., 2012; Lapponi et al., 2012; Enger et al., 2017; Cue Detection. In this step, gold standard and Ren et al., 2018; Jiménez-Zafra et al., 2020a). Li et al. (2010) frame scope resolution as a shal- predicted cue annotations are matched to each other. 2 low semantic parsing task backed up by syntactic Morante and Blanco (2012) also use scope resolution to parses. In the neural age, Kurtz et al. (2020) frame refer to the entire task of negation resolution. 530 When matching the gold standard cue cg and the predicted cue cp , we can either require an exact match (cg = cp ) or a partial match (cg ∩ cp 6= ∅). The CoNLL 2010 Shared Task on detecting hedges and their scope in text (Farkas et al., 2010) required exact cue matches, while the *SEM 2012 Shared Tas"
2021.conll-1.41,D16-1078,0,0.0163922,"n “Your kids don’t hate school” vs. “Your kids don’t hate school” (Blanco and Moldovan, 2011b). Correct identification of a negation’s focus is key to natural language understanding. However, to date, no corpora annotating both negation scopes and focus exist. For some ideas on integrating focus identification into our proposed evaluation framework, see Sec. 6. 2.2 Automatic Negation Resolution negation resolution as a dependency parsing task. Several works using neural networks (e.g., Fancellu et al., 2016, 2018; Lazib et al., 2020) train BiLSTMs, or syntactically structured BiLSTMs or GCNs. Qian et al. (2016) propose a CNN-based architecture combined with some path/position information. Recently, a range of papers has explored BERT-based models for negation resolution (Khandelwal and Sawant, 2020; Khandelwal and Britto, 2020; Britto and Khandelwal, 2020; Shaitarova and Rinaldi, 2021). Further related work includes datasets annotated for focus (Blanco and Moldovan, 2011a; Altuna et al., 2017), and the computational modeling thereof (e.g., Hossain et al., 2020b). In addition, there is a growing body of work addressing negation within the context of neural language models and commonsense reasoning us"
2021.conll-1.41,S12-1041,0,0.0711798,"Missing"
2021.conll-1.41,2021.naacl-srw.3,0,0.0321635,"ideas on integrating focus identification into our proposed evaluation framework, see Sec. 6. 2.2 Automatic Negation Resolution negation resolution as a dependency parsing task. Several works using neural networks (e.g., Fancellu et al., 2016, 2018; Lazib et al., 2020) train BiLSTMs, or syntactically structured BiLSTMs or GCNs. Qian et al. (2016) propose a CNN-based architecture combined with some path/position information. Recently, a range of papers has explored BERT-based models for negation resolution (Khandelwal and Sawant, 2020; Khandelwal and Britto, 2020; Britto and Khandelwal, 2020; Shaitarova and Rinaldi, 2021). Further related work includes datasets annotated for focus (Blanco and Moldovan, 2011a; Altuna et al., 2017), and the computational modeling thereof (e.g., Hossain et al., 2020b). In addition, there is a growing body of work addressing negation within the context of neural language models and commonsense reasoning using them (e.g., Hossain et al., 2020c; Geiger et al., 2020; Hosseini et al., 2021; Jiang et al., 2021). 3 Evaluation Metrics and Settings In this section, we first give an overview of the various evaluation metrics used in shared tasks and research publications. We then propose a"
2021.conll-1.41,silveira-etal-2014-gold,0,0.0167996,"Missing"
2021.conll-1.41,W08-0606,0,0.359635,"ub.com/ based approaches, providing a competitive range boschresearch/negation_resolution_ of baselines for future work to compare with. evaluation_conll2021. 529 event is factual. Thus, in cases such as “He may not know the answer,” no event is annotated. To avoid terminological confusion, in this paper, we ignore Event annotations and call “know” in the example above the main predicate of the negation. The scope is annotated as the longest relevant part of the sentence, i.e., as the main predicate referring to the negated event and all its arguments and complements. In contrast to BioScope (Szarvas et al., 2008), CD-neg includes the subject, but not the cue in the scope. In constituent negation, the negation marker is attached to the object as in “Mary came to the lecture with no books.” Still, the negation scopes over the entire sentence and is marked accordingly in CD-neg to achieve representational equality with the sentence “Mary did not come to the lecture with books.” A constituentnegated subject receives the same treatment. One element of the scope is singled out as the negation’s focus, i.e., the part that is intended to be interpreted as false (Huddleston and Pullum, 2002). Detecting the foc"
2021.conll-1.41,taboada-etal-2006-methods,0,0.0757208,"Missing"
2021.conll-1.41,J12-2005,0,0.0198733,"onal work on negation resolution is generally based on small- to medium-scale corpora, which in addition are often not very compatible due to differences in the employed annotation schemes and underlying tokenization. JiménezZafra et al. (2020b) provide a comprehensive survey of datasets annotated for negation. As the problem of negation resolution is closely tied to syntax, there are many works leveraging syntactic information, using rules over syntactic 3.1 Metrics used in Shared Tasks structures to resolve negation or speculation scopes Previously proposed metrics for negation resolu(e.g., Velldal et al., 2012; Packard et al., 2014; tion can be divided into metrics requiring exact cue McKenna and Steedman, 2020) or training systems matches and metrics requiring partial cue matches with explicit or learned syntactic features (Read (perhaps confusingly called No Cue Match). et al., 2012; Lapponi et al., 2012; Enger et al., 2017; Cue Detection. In this step, gold standard and Ren et al., 2018; Jiménez-Zafra et al., 2020a). Li et al. (2010) frame scope resolution as a shal- predicted cue annotations are matched to each other. 2 low semantic parsing task backed up by syntactic Morante and Blanco (2012)"
2021.conll-1.41,W10-3111,0,0.120875,"Missing"
2021.conll-1.41,P15-1064,0,0.015943,"relying on exact cue matches. We argue that this metric is well-motivated and intuitively interpretable and should hence be adopted by future studies or shared tasks. In addition, our experimental study, comparing a set of recent neural architectures on a similar basis, will serve as a reference for future work. Besides implementing a variety of linguistically motivated extensions with the aim of deeper system analyses using our framework as suggested above, an important next step is to evaluate the suite of models used in this paper on further datasets in languages other than English (e.g., Zou et al., 2015; Liu et al., 2018; Jiménez-Zafra et al., 2018). References Begoña Altuna, Anne-Lyse Minard, and Manuela Speranza. 2017. The scope and focus of negation: A complete annotation framework for Italian. In Proceedings of the Workshop Computational Semantics Beyond Events and Roles, pages 34–42, Valencia, Spain. Association for Computational Linguistics. Luisa Bentivogli, Arianna Bisazza, Mauro Cettolo, and Marcello Federico. 2016. Neural versus phrasebased machine translation quality: a case study. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 257"
2021.eacl-srw.25,2020.semeval-1.24,0,0.0500821,"Missing"
2021.eacl-srw.25,2020.semeval-1.4,0,0.0237961,"ance Related work Traditional approaches for LSC detection are typebased (Dubossarsky et al., 2019; Schlechtweg et al., 2019). This means that not every word occurrence is considered individually (token-based); instead, a general vector representation that summarizes every occurrence of a word (including polysemous words) is created. The results of SemEval2020 Task 1 and DIACR-Ita (Basile et al., 2020; Schlechtweg et al., 2020) demonstrated that overall type-based approaches (Asgari et al., 2020; Kaiser et al., 2020; Praˇza´ k et al., 2020) achieved better results than token-based approaches (Beck, 2020; Kutuzov and Giulianelli, 2020; Laicher et al., 2020). This is surprising, however, for two main reasons: (i) contextualized token-based approaches have significantly outperformed static type-based approaches in several NLP tasks over the past years (Ethayarajh, 2019). (ii) SemEval-2020 Task 1 and DIACR-Ita both include a subtask on binary change detection that requires to discover small sets of contextualized usages with the same sense. Typebased embeddings do not infer usage-based (or token-based) representations and are therefore not expected to be able to find such sets (Schlechtweg et al"
2021.eacl-srw.25,N19-1423,0,0.188567,"he past years (Kutuzov et al., 2018; Tahmasebi et al., 2018; Hengchen et al., 2021). Recently, SemEval-2020 Task 1 and the Italian follow-up task DIACR-Ita provided a multi-lingual evaluation framework to compare the variety of proposed model architectures (Schlechtweg et al., 2020; Basile et al., 2020). Both tasks demonstrated that type-based embeddings outperform token-based embeddings. This is surprising given that contextualised token-based approaches have achieved significant improvements over the static type-based approaches in several NLP tasks over the past years (Peters et al., 2018; Devlin et al., 2019). In this study, we relate model results on LSC detection to results on the word sense disambiguation data set underlying SemEval-2020 Task 1. This allows us to test the performance of different methods more rigorously, and to thoroughly analyze results of clustering-based methods. We investigate the influence of a range of variables on clusterings of BERT vectors and show that its low performance Related work Traditional approaches for LSC detection are typebased (Dubossarsky et al., 2019; Schlechtweg et al., 2019). This means that not every word occurrence is considered individually (token-b"
2021.eacl-srw.25,W17-1202,0,0.0142805,"those two clusters whose merging maximizes a predefined criterion. We use Ward’s method, where clusters with the lowest loss of information are merged (Ward Jr, 1963). Following Giulianelli et al. (2020) and Martinc et al. (2020a), we estimate the number of clusters k with the Silhouette Method (Rousseeuw, 1987): we perform a cluster analysis for each 2 ≤ k ≤ 10 and calculate the silhouette index for each k. The number of clusters with the largest index is used for the final clustering. The Jensen-Shannon Distance (JSD) measures the difference between two probability distributions (Lin, 1991; Donoso and Sanchez, 2017). We convert two time specific clusterings into probability distribution P and Q and measure their distance JSD(P, Q) to obtain graded change values (Giulianelli et al., 2020; Kutuzov and Giulianelli, 2020). If P and Q are very similar, the JSD returns a value close to 0. If the distributions are very different, the JSD returns a value close to 1. Spearman’s Rank-Order Correlation Coefficient ρ measures the strength and the direction of the relationship between two variables (Bolboaca and J¨antschi, 2006) by correlating the rank order of two variables. Its values range from -1 to 1, where 1 de"
2021.eacl-srw.25,P19-1044,1,0.843352,"ovements over the static type-based approaches in several NLP tasks over the past years (Peters et al., 2018; Devlin et al., 2019). In this study, we relate model results on LSC detection to results on the word sense disambiguation data set underlying SemEval-2020 Task 1. This allows us to test the performance of different methods more rigorously, and to thoroughly analyze results of clustering-based methods. We investigate the influence of a range of variables on clusterings of BERT vectors and show that its low performance Related work Traditional approaches for LSC detection are typebased (Dubossarsky et al., 2019; Schlechtweg et al., 2019). This means that not every word occurrence is considered individually (token-based); instead, a general vector representation that summarizes every occurrence of a word (including polysemous words) is created. The results of SemEval2020 Task 1 and DIACR-Ita (Basile et al., 2020; Schlechtweg et al., 2020) demonstrated that overall type-based approaches (Asgari et al., 2020; Kaiser et al., 2020; Praˇza´ k et al., 2020) achieved better results than token-based approaches (Beck, 2020; Kutuzov and Giulianelli, 2020; Laicher et al., 2020). This is surprising, however, for"
2021.eacl-srw.25,D19-1006,0,0.0303503,"rizes every occurrence of a word (including polysemous words) is created. The results of SemEval2020 Task 1 and DIACR-Ita (Basile et al., 2020; Schlechtweg et al., 2020) demonstrated that overall type-based approaches (Asgari et al., 2020; Kaiser et al., 2020; Praˇza´ k et al., 2020) achieved better results than token-based approaches (Beck, 2020; Kutuzov and Giulianelli, 2020; Laicher et al., 2020). This is surprising, however, for two main reasons: (i) contextualized token-based approaches have significantly outperformed static type-based approaches in several NLP tasks over the past years (Ethayarajh, 2019). (ii) SemEval-2020 Task 1 and DIACR-Ita both include a subtask on binary change detection that requires to discover small sets of contextualized usages with the same sense. Typebased embeddings do not infer usage-based (or token-based) representations and are therefore not expected to be able to find such sets (Schlechtweg et al., 2020). Yet, they show better performance on binary change detection than clusterings of tokenbased embeddings (Kutuzov and Giulianelli, 2020). 3 Data and evaluation We utilize the annotated English, German and Swedish datasets (ENG, GER, SWE) underlying 192 Proceedi"
2021.eacl-srw.25,2020.acl-main.365,0,0.0663183,"Missing"
2021.eacl-srw.25,N18-1202,0,0.0498661,"easing attention in the past years (Kutuzov et al., 2018; Tahmasebi et al., 2018; Hengchen et al., 2021). Recently, SemEval-2020 Task 1 and the Italian follow-up task DIACR-Ita provided a multi-lingual evaluation framework to compare the variety of proposed model architectures (Schlechtweg et al., 2020; Basile et al., 2020). Both tasks demonstrated that type-based embeddings outperform token-based embeddings. This is surprising given that contextualised token-based approaches have achieved significant improvements over the static type-based approaches in several NLP tasks over the past years (Peters et al., 2018; Devlin et al., 2019). In this study, we relate model results on LSC detection to results on the word sense disambiguation data set underlying SemEval-2020 Task 1. This allows us to test the performance of different methods more rigorously, and to thoroughly analyze results of clustering-based methods. We investigate the influence of a range of variables on clusterings of BERT vectors and show that its low performance Related work Traditional approaches for LSC detection are typebased (Dubossarsky et al., 2019; Schlechtweg et al., 2019). This means that not every word occurrence is considered"
2021.eacl-srw.25,2020.semeval-1.30,0,0.0611414,"Missing"
2021.eacl-srw.25,P19-1072,1,0.921379,"Missing"
2021.eacl-srw.25,2020.semeval-1.1,1,0.944414,"and show that its low performance is largely due to orthographic information on the target word, which is encoded even in the higher layers of BERT representations. By reducing the influence of orthography we considerably improve BERT’s performance. 1 2 Introduction Lexical Semantic Change (LSC) Detection has drawn increasing attention in the past years (Kutuzov et al., 2018; Tahmasebi et al., 2018; Hengchen et al., 2021). Recently, SemEval-2020 Task 1 and the Italian follow-up task DIACR-Ita provided a multi-lingual evaluation framework to compare the variety of proposed model architectures (Schlechtweg et al., 2020; Basile et al., 2020). Both tasks demonstrated that type-based embeddings outperform token-based embeddings. This is surprising given that contextualised token-based approaches have achieved significant improvements over the static type-based approaches in several NLP tasks over the past years (Peters et al., 2018; Devlin et al., 2019). In this study, we relate model results on LSC detection to results on the word sense disambiguation data set underlying SemEval-2020 Task 1. This allows us to test the performance of different methods more rigorously, and to thoroughly analyze results of clust"
2021.eacl-srw.25,N18-2027,1,0.902244,"rds, else 1.3 For proper names a sentence receives label 0, if no proper names are in the sentence, 1, if one proper name occurs, else 2.4 The hypothesis that proper names may influence the clustering was suggested in Martinc et al. (2020b). For corpora, a sentence is labeled 0, if it occurs in the first target corpus, else 1. Average measures Given two sets of token vectors V1 and V2 from t1 and t2 , Average Pairwise Distance (APD) is calculated by randomly picking n vectors from both sets, calculating their pairwise cosine distances d(x, y) where x ∈ V1 and y ∈ V2 and averaging over these. (Schlechtweg et al., 2018; Giulianelli et al., 2020). We determine n as the minimum size of V1 and V2 . APDOLD/NEW measure the average of pairwise distances within V1 and V2 , respectively. They are calculated as the average distance of max. 10, 000 randomly sampled unique combinations of vectors from either V1 or V2 . COS is calculated as the cosine distance of the respective mean vectors for V1 and V2 (Kutuzov and Giulianelli, 2020). 5 5.1 Results Clustering Because of the high computational load, we apply the clustering only to the ENG and the GER part of the SemEval data set. For this, we use BERT to create token"
2021.iwpt-1.13,C12-1015,0,0.0858586,"Missing"
2021.iwpt-1.13,2020.iwpt-1.16,0,0.541538,"ddings such as BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019). Both characteristics are present in recent top-performing systems (Che et al., 2018; Straka et al., 2019; Kondratyuk and Straka, 2019; Kanerva et al., 2018, 2020; Che et al., 2018). However, there remain a considerable number of implementation and configuration choices whose impact on parser performance is less well understood. This is evidenced by the many different model configurations (see Table 1) present in parsers that have achieved top results in recent shared tasks addressing UD parsing (Zeman et al., 2017, 2018; Bouma et al., 2020). The choices include (a) the particular pre-trained word embeddings or language model to use, (b) whether to utilize an LSTM in addition to (fine-tuned) contextualized word embeddings; and (c) whether to use a multi-task training setup simultaneously predicting additional UD features (such as morphology or parts of speech) during parsing. 131 Proceedings of the 17th International Conference on Parsing Technologies (IWPT 2021), pages 131–144 Bangkok, Thailand (online), August 6, 2021. ©2021 Association for Computational Linguistics The aim of this paper is to disentangle the effects of the abo"
2021.iwpt-1.13,K18-2005,0,0.078768,"ovements can in large part be attributed to two developments: (1) the introduction of deep biaffine classifiers (Dozat and Manning, 2017), which now constitute the de-facto standard approach for graph-based dependency parsing, and 1 We release our code and pre-trained models on github.com/boschresearch/steps-parser. nsubj Arc Scorer Label Scorer (2) the rise of pre-trained distributed word representations, particularly transformer-based contextualized embeddings such as BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019). Both characteristics are present in recent top-performing systems (Che et al., 2018; Straka et al., 2019; Kondratyuk and Straka, 2019; Kanerva et al., 2018, 2020; Che et al., 2018). However, there remain a considerable number of implementation and configuration choices whose impact on parser performance is less well understood. This is evidenced by the many different model configurations (see Table 1) present in parsers that have achieved top results in recent shared tasks addressing UD parsing (Zeman et al., 2017, 2018; Bouma et al., 2020). The choices include (a) the particular pre-trained word embeddings or language model to use, (b) whether to utilize an LSTM in addition"
2021.iwpt-1.13,L18-1347,0,0.0598518,"Missing"
2021.iwpt-1.13,2020.acl-main.747,0,0.587552,"(the Stuttgart Transformer-based Extensible Parsing System), a modular graph-based dependency parser which implements commonly used modules such as biaffine scorers (Dozat et al., 2017; Kondratyuk and Straka, 2019) or LSTM layers (Straka, 2018) (see Figure 1). Using STEPS, we perform a series of experiments on the UD treebanks of a diverse set of languages. Our setup facilitates estimating the impact of the various architectures and configuration decisions in a comparable way. Our most important insight is that a relatively simple architecture using biaffine heads on top of fine-tuned XLM-R (Conneau et al., 2020) leads to the highest parsing accuracy for almost all languages in our study, outperforming prior systems on most languages. Our analysis indicates that LSTM layers do not lead to benefits. Simplifying the architecture even further by using a single scorer for edge and label prediction results in similar performance but on average leads to longer training times. Our contributions are as follows: (1) We introduce STEPS, a new implementation of a graph-based dependency parser designed to be modular and easily extensible. STEPS achieves new state-of-the-art UD parsing performance (in terms of LAS"
2021.iwpt-1.13,W19-3712,0,0.0612874,"Missing"
2021.iwpt-1.13,P18-2077,0,0.359346,"fine classifiers. The first classifier (the “arc scorer”) is responsible for predicting which (unlabeled) edges exist in the output structure. It predicts, for each token, a probability distribution over potential syntactic heads (i.e., all other tokens in the sentence). We then feed the log-probabilities to the ChuLiu/Edmonds maximum spanning tree algorithm (Chu and Liu, 1965; Edmonds, 1967) and label the resulting tree using the label scorer. The second classifier (the “label scorer”) then assigns dependency labels to edges predicted in the first step. The unfactorized approach, proposed by Dozat and Manning (2018) for semantic graph parsing, uses only a single biaffine classifier (namely the label scorer). Non-existence of dependencies is encoded using simply another label (∅). We adapt this approach to tree parsing by discarding the arc scorer and computing the edge weights for the ChuLiu/Edmonds MST algorithm as log(1 − P (∅)) in order to extract a labeled dependency tree directly. To the best of our knowledge, this is the first time that the unfactorized architecture has been applied to the parsing of dependency tree structures. 3.2 We study the effects of a multi-task training setup by implementing"
2021.iwpt-1.13,K17-3002,0,0.086566,"sing Technologies (IWPT 2021), pages 131–144 Bangkok, Thailand (online), August 6, 2021. ©2021 Association for Computational Linguistics The aim of this paper is to disentangle the effects of the above factors and determine their impact on parser performance. We appeal to the concept of Occam’s razor by ways of avoiding architectural elements that do not bring about a testable advantage. With this idea in mind, we introduce STEPS (the Stuttgart Transformer-based Extensible Parsing System), a modular graph-based dependency parser which implements commonly used modules such as biaffine scorers (Dozat et al., 2017; Kondratyuk and Straka, 2019) or LSTM layers (Straka, 2018) (see Figure 1). Using STEPS, we perform a series of experiments on the UD treebanks of a diverse set of languages. Our setup facilitates estimating the impact of the various architectures and configuration decisions in a comparable way. Our most important insight is that a relatively simple architecture using biaffine heads on top of fine-tuned XLM-R (Conneau et al., 2020) leads to the highest parsing accuracy for almost all languages in our study, outperforming prior systems on most languages. Our analysis indicates that LSTM layers"
2021.iwpt-1.13,P15-1033,0,0.0134848,"experiments (Sec. 4). Sec. 5 presents the adaption of our system to Enhanced UD. Finally, we discuss implications for parser choice and future parser design (Sec. 6). 2 Related Work This section provides a brief outline of the use of contextualized word embeddings in syntactic parsers, recently developed graph-based dependency parsers, and related work on dependency parser analysis. Contextualized Word Embeddings in Dependency Parsing. Like in other sub-fields of natural language processing, using contextualized word embeddings has become the de-facto standard when building syntactic parsers. Dyer et al. (2015) use LSTM-based contextual representations for the stack and buffer in transition-based parsing, while Kiperwasser and Goldberg (2016) use BiLSTMbased feature representations for individual tokens in both graph-based and transition-based parsing. In both of these cases, the underlying LSTM is trained simultaneously with the target task. In contrast, recently the predominant approach towards contextualized word representations has been to pre-train systems on large-scale language modeling objectives, then taking their representations as 132 input for a target task, optionally while continuing t"
2021.iwpt-1.13,P19-1012,1,0.847438,"e of studies comparing different language models for dependency parsing (e.g., Kanerva et al., 2018; Pyysalo et al., 2020; Smith et al., 2018). Additionally, several studies have investigated the amount of implicit syntactic information captured in pre-trained LMs such as ELMo and BERT (Tenney et al., 2019a,b; Hewitt and Manning, 2019). Conversely, several studies have investigated the utility of structural features for dependency parsing in the presence of LSTMs and/or contextualized word embeddings, generally finding that their impact is diminished in the presence of contextual information (Falenska and Kuhn, 2019; Fonseca and Martins, 2020). Kulmizev et al. (2019) compare the effect of deep contextualized word embeddings on transition-based and graph-based dependency parsers, showing that their inclusion makes the two approaches virtually equivalent in terms of parsing accuracy. Our work is similar to theirs in the sense that we also evaluate several very different dimensions of parser architecture at the same time, utilizing the same underlying backbone and thus ensuring comparability across experiments. Multi-Purpose Parsers. Other parsers with modular or extensible architectures include Alto (Gontr"
2021.iwpt-1.13,2020.acl-main.776,0,0.0228099,"fferent language models for dependency parsing (e.g., Kanerva et al., 2018; Pyysalo et al., 2020; Smith et al., 2018). Additionally, several studies have investigated the amount of implicit syntactic information captured in pre-trained LMs such as ELMo and BERT (Tenney et al., 2019a,b; Hewitt and Manning, 2019). Conversely, several studies have investigated the utility of structural features for dependency parsing in the presence of LSTMs and/or contextualized word embeddings, generally finding that their impact is diminished in the presence of contextual information (Falenska and Kuhn, 2019; Fonseca and Martins, 2020). Kulmizev et al. (2019) compare the effect of deep contextualized word embeddings on transition-based and graph-based dependency parsers, showing that their inclusion makes the two approaches virtually equivalent in terms of parsing accuracy. Our work is similar to theirs in the sense that we also evaluate several very different dimensions of parser architecture at the same time, utilizing the same underlying backbone and thus ensuring comparability across experiments. Multi-Purpose Parsers. Other parsers with modular or extensible architectures include Alto (Gontrum et al., 2017), a prototyp"
2021.iwpt-1.13,E17-3008,0,0.013435,"2019; Fonseca and Martins, 2020). Kulmizev et al. (2019) compare the effect of deep contextualized word embeddings on transition-based and graph-based dependency parsers, showing that their inclusion makes the two approaches virtually equivalent in terms of parsing accuracy. Our work is similar to theirs in the sense that we also evaluate several very different dimensions of parser architecture at the same time, utilizing the same underlying backbone and thus ensuring comparability across experiments. Multi-Purpose Parsers. Other parsers with modular or extensible architectures include Alto (Gontrum et al., 2017), a prototyping tool for new 133 3 STEPS: A Modular Graph-Based Dependency Parser In this section, we describe our modular dependency parser STEPS (Stuttgart Transformer-based Extensible Parsing System). Each subsection focuses on a particular aspect of the parser setup, providing background on its usage and its potential impact on parser performance. 3.1 Input Token Representation STEPS provides a number of different options for input token representation. As Table 1 shows, parsers have made use of a variety of pre-trained embeddings, with transformer-based language models having become the p"
2021.iwpt-1.13,2020.iwpt-1.26,1,0.710515,"Missing"
2021.iwpt-1.13,N19-1419,0,0.0236409,"transition-based dependency parsing. In contrast to these two, STEPS is a graph-based dependency parser that focuses on easy configuration of different transformer-based language models and neural architecture variants. Parser Analyses and Comparisons. Recent years have seen a wide range of studies comparing different language models for dependency parsing (e.g., Kanerva et al., 2018; Pyysalo et al., 2020; Smith et al., 2018). Additionally, several studies have investigated the amount of implicit syntactic information captured in pre-trained LMs such as ELMo and BERT (Tenney et al., 2019a,b; Hewitt and Manning, 2019). Conversely, several studies have investigated the utility of structural features for dependency parsing in the presence of LSTMs and/or contextualized word embeddings, generally finding that their impact is diminished in the presence of contextual information (Falenska and Kuhn, 2019; Fonseca and Martins, 2020). Kulmizev et al. (2019) compare the effect of deep contextualized word embeddings on transition-based and graph-based dependency parsers, showing that their inclusion makes the two approaches virtually equivalent in terms of parsing accuracy. Our work is similar to theirs in the sense"
2021.iwpt-1.13,K18-2013,0,0.0615884,"e introduction of deep biaffine classifiers (Dozat and Manning, 2017), which now constitute the de-facto standard approach for graph-based dependency parsing, and 1 We release our code and pre-trained models on github.com/boschresearch/steps-parser. nsubj Arc Scorer Label Scorer (2) the rise of pre-trained distributed word representations, particularly transformer-based contextualized embeddings such as BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019). Both characteristics are present in recent top-performing systems (Che et al., 2018; Straka et al., 2019; Kondratyuk and Straka, 2019; Kanerva et al., 2018, 2020; Che et al., 2018). However, there remain a considerable number of implementation and configuration choices whose impact on parser performance is less well understood. This is evidenced by the many different model configurations (see Table 1) present in parsers that have achieved top results in recent shared tasks addressing UD parsing (Zeman et al., 2017, 2018; Bouma et al., 2020). The choices include (a) the particular pre-trained word embeddings or language model to use, (b) whether to utilize an LSTM in addition to (fine-tuned) contextualized word embeddings; and (c) whether to use"
2021.iwpt-1.13,2020.iwpt-1.17,0,0.0928859,"Missing"
2021.iwpt-1.13,Q16-1023,0,0.0279331,"ser choice and future parser design (Sec. 6). 2 Related Work This section provides a brief outline of the use of contextualized word embeddings in syntactic parsers, recently developed graph-based dependency parsers, and related work on dependency parser analysis. Contextualized Word Embeddings in Dependency Parsing. Like in other sub-fields of natural language processing, using contextualized word embeddings has become the de-facto standard when building syntactic parsers. Dyer et al. (2015) use LSTM-based contextual representations for the stack and buffer in transition-based parsing, while Kiperwasser and Goldberg (2016) use BiLSTMbased feature representations for individual tokens in both graph-based and transition-based parsing. In both of these cases, the underlying LSTM is trained simultaneously with the target task. In contrast, recently the predominant approach towards contextualized word representations has been to pre-train systems on large-scale language modeling objectives, then taking their representations as 132 input for a target task, optionally while continuing to fine-tune them. This approach was initially proposed using an LSTM-based system (ELMo; Peters et al., 2018) and has since been trans"
2021.iwpt-1.13,D19-1279,0,0.377139,"d to two developments: (1) the introduction of deep biaffine classifiers (Dozat and Manning, 2017), which now constitute the de-facto standard approach for graph-based dependency parsing, and 1 We release our code and pre-trained models on github.com/boschresearch/steps-parser. nsubj Arc Scorer Label Scorer (2) the rise of pre-trained distributed word representations, particularly transformer-based contextualized embeddings such as BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019). Both characteristics are present in recent top-performing systems (Che et al., 2018; Straka et al., 2019; Kondratyuk and Straka, 2019; Kanerva et al., 2018, 2020; Che et al., 2018). However, there remain a considerable number of implementation and configuration choices whose impact on parser performance is less well understood. This is evidenced by the many different model configurations (see Table 1) present in parsers that have achieved top results in recent shared tasks addressing UD parsing (Zeman et al., 2017, 2018; Bouma et al., 2020). The choices include (a) the particular pre-trained word embeddings or language model to use, (b) whether to utilize an LSTM in addition to (fine-tuned) contextualized word embeddings; a"
2021.iwpt-1.13,D19-1277,0,0.0311765,"Missing"
2021.iwpt-1.13,2020.iwpt-1.3,0,0.0600199,"Missing"
2021.iwpt-1.13,2021.ccl-1.108,0,0.049919,"Missing"
2021.iwpt-1.13,de-marneffe-etal-2014-universal,0,0.382257,"Missing"
2021.iwpt-1.13,2021.eacl-demos.10,0,0.401074,"Missing"
2021.iwpt-1.13,N18-1202,0,0.329878,"arsing, while Kiperwasser and Goldberg (2016) use BiLSTMbased feature representations for individual tokens in both graph-based and transition-based parsing. In both of these cases, the underlying LSTM is trained simultaneously with the target task. In contrast, recently the predominant approach towards contextualized word representations has been to pre-train systems on large-scale language modeling objectives, then taking their representations as 132 input for a target task, optionally while continuing to fine-tune them. This approach was initially proposed using an LSTM-based system (ELMo; Peters et al., 2018) and has since been transferred to transformers (e.g., BERT; Devlin et al., 2019). Transformer-based pre-trained language models have proven wildly successful and have become a standard method for a wide range of NLP tasks, including syntactic dependency parsing. Recent Graph-based Parsers. Table 1 shows the configurations of three parsers that were among the best-performing systems in the CoNLL 2017 and CoNLL 2018 Shared Tasks on UD parsing, as well as the more recent UDify and Trankit parsers. StanfordNLP (Dozat et al., 2017) was one of the first systems to apply the biaffine graph-based par"
2021.iwpt-1.13,2020.emnlp-demos.7,0,0.078802,"Missing"
2021.iwpt-1.13,2020.emnlp-main.617,0,0.0613614,"Missing"
2021.iwpt-1.13,2020.acl-demos.14,0,0.0207677,"re our results against TurkuNLP, a modified version of UDify which scored 1st in the official evaluation of the IWPT 2020 Shared Task, and ShanghaiTech, which scored 1st in the unofficial post-evaluation. We evaluate in terms of ELAS (Enhanced LAS, i.e., F1 score over the set of enhanced dependencies in the system output and the gold standard) using the official evaluation script for the IWPT 2020 Shared Task6 and report per-treebank results for TurkuNLP and ShanghaiTech as submitted.7 To ensure comparability with previous work, we compute our results using raw text as input and using Stanza (Qi et al., 2020) for tokenization and sentence segmentation. Table 6 reports our results. Our parser achieves very high accuracy, outperforming TurkuNLP and ShanghaiTech on all evaluated languages except Arabic and Czech. Notably, the latter system also uses XLM-R embeddings, but with a more complex parser architecture. Unlike in tree parsing, the unfactorized system actually slightly outperforms the factorized system on a number of languages, with the largest margins 6 https://universaldependencies.org/ iwpt20/iwpt20_xud_eval.py 7 https://universaldependencies.org/ iwpt20/Results.html ar-PADT cs-PDT en-EWT f"
2021.iwpt-1.13,2020.semeval-1.271,0,0.0989717,"Missing"
2021.iwpt-1.13,L16-1376,0,0.371779,"Missing"
2021.iwpt-1.13,2020.emnlp-main.610,0,0.0228611,"Missing"
2021.iwpt-1.13,silveira-etal-2014-gold,0,0.0693114,"Missing"
2021.iwpt-1.13,D18-1291,0,0.0459986,"Missing"
2021.iwpt-1.13,P19-1452,0,0.0206129,"a modular framework for transition-based dependency parsing. In contrast to these two, STEPS is a graph-based dependency parser that focuses on easy configuration of different transformer-based language models and neural architecture variants. Parser Analyses and Comparisons. Recent years have seen a wide range of studies comparing different language models for dependency parsing (e.g., Kanerva et al., 2018; Pyysalo et al., 2020; Smith et al., 2018). Additionally, several studies have investigated the amount of implicit syntactic information captured in pre-trained LMs such as ELMo and BERT (Tenney et al., 2019a,b; Hewitt and Manning, 2019). Conversely, several studies have investigated the utility of structural features for dependency parsing in the presence of LSTMs and/or contextualized word embeddings, generally finding that their impact is diminished in the presence of contextual information (Falenska and Kuhn, 2019; Fonseca and Martins, 2020). Kulmizev et al. (2019) compare the effect of deep contextualized word embeddings on transition-based and graph-based dependency parsers, showing that their inclusion makes the two approaches virtually equivalent in terms of parsing accuracy. Our work is"
2021.iwpt-1.13,2020.emnlp-main.180,0,0.0433389,"Missing"
2021.iwpt-1.13,2020.lrec-1.494,0,0.0245564,"ven the fact that both are built on the same underlying language model (XLM-R-large). The slight advantage for STEPSXLM-R observed on most languages may stem from the fact that it fine-tunes the entire transformer model instead of merely adding Adapter layers, and that it does not use a multitask training setup (cf. Sec. 4.6). Interestingly, on Finnish and Latvian, both systems outperform other existing parsers by very large margins (around 4.9 and 6.5 LAS, respectively). We assume that there are two main reasons for this. First, XLM-R is pre-trained on CommonCrawl data (Conneau et al., 2020; Wenzek et al., 2020) as opposed to Wikipedia dumps, which results not only in several orders 136 3 In a similar study comparing mBERT- and langBERTbased parsers, Kanerva et al. (2020) also found Latvian to be one of the few languages for which mBERT outperformed the language-specific (WikiBERT) version. Both the Latvian and the Hindi Wikipedias are rather small, consisting of only 21M and 35M tokens, respectively (Pyysalo et al., 2020). ar PADT cs PDT de GSD en EWT fi TDT hi HDTB it ISDT ja GSD ko Kaist lv LVTB ru STR zh GSD UDPipe+ UDify UDifymono Trankitlarge 84.62 82.88 83.34 86.51 92.56 92.88 91.58 93.11 84.0"
2021.iwpt-1.13,2020.emnlp-demos.6,0,0.0540762,"Missing"
2021.iwpt-1.13,2020.acl-main.577,0,0.0256568,"rce scenario, assuming that we know the application language of a parser and thus training a single parser per language. Future work may address multilingual approaches such as the training setup used by UDify or the recently proposed UDapter (Üstün et al., 2020), which aims at boosting performance of low-resource languages while keeping performance of high-resource languages high. Furthermore, it would be interesting to see if our results about biaffine achitectures also hold for non-syntactic tasks that have recently been framed as dependency parsing tasks, such as Named Entity Recognition (Yu et al., 2020), negation scope detection (Kurtz et al., 2020) or Semantic Role Labeling (Shi et al., 2020). To sum up, in this paper we have applied “Occam’s razor” to graph-based dependency parsing. We believe that the insights from our study will foster further research on dependency parsing and on framing other tasks as dependency parsing, taking our simplified but robustly performing STEPS parser as a starting point. Acknowledgments We thank Agnieszka Falenska, Heike Adel, Lukas Lange, Hendrik Schuff, Jannik Strötgen, as well as the anonymous reviewers for their valuable comments with regard to this wor"
2021.iwpt-1.13,K18-2001,0,0.0605947,"Missing"
2021.konvens-1.24,J90-1003,0,0.381999,"nce resolution; Guillaume et al. (2016) design ZombiLingo, a game for syntactic dependency annotation. We present a GWAP-style game implementation called W ORD G UESS1 where associations of a target word are offered to players in order to guess the target word. For example, associations such as winter, white, cold provide hints to players when guessing the target word snow. Our game is a web-based and mobile-based application whose aim is to learn and understand word-association and word-context relationships: previous research has shown that associations and corpus co-occurrence are related (Church and Hanks, 1990; de Deyne and Storms, 2008a; Schulte im Walde et al., 2008, i.a.); we plan to explore their connections and differences in more depth. In this vein, (i) we vary associations obtained from humans, and context-based words induced from corpus co-occurrence; (ii) we provide a multilingual gaming environment in order to understand the conditions across languages and relational patterns between native and second languages; and (iii) we offer the players to choose between levels of difficulty (i.e., providing more or less cues). The obtained data enables us to induce conditions and weights for word"
2021.konvens-1.24,C02-1113,0,0.0853929,"ul for many NLP purposes such as ontology induction and anaphora resolution. 1 Introduction Games-with-a-purpose (GWAP) offer enjoyable entertainment to players and at the same time allow researchers in Natural Language Processing (NLP) to collect and explore cognitive and (computational) linguistic facets of human-generated data. While the term GWAP has been coined by Von Ahn and Dabbish (2008), the underlying idea has been pursued across linguistic levels and across NLP purposes for much longer. To provide a few examples across research fields, the adventure and interactive fiction games by Gabsdil et al. (2002) rely on natural-language question-answering dialogues to explore inference systems with reference resolution, syntactic ambiguities, and scripted dialogues; Chamberlain et al. (2008) exploit collaborative work to identify relationships between words and phrases in web data; OntoGame (Siorpaes and Hepp, 2008) matches classes in an ontology with Wikipedia articles; Hladk´a et al. (2009) propose a gamified annotation approach for coreference resolution; Guillaume et al. (2016) design ZombiLingo, a game for syntactic dependency annotation. We present a GWAP-style game implementation called W ORD"
2021.konvens-1.24,C16-1286,0,0.0123894,"purposes for much longer. To provide a few examples across research fields, the adventure and interactive fiction games by Gabsdil et al. (2002) rely on natural-language question-answering dialogues to explore inference systems with reference resolution, syntactic ambiguities, and scripted dialogues; Chamberlain et al. (2008) exploit collaborative work to identify relationships between words and phrases in web data; OntoGame (Siorpaes and Hepp, 2008) matches classes in an ontology with Wikipedia articles; Hladk´a et al. (2009) propose a gamified annotation approach for coreference resolution; Guillaume et al. (2016) design ZombiLingo, a game for syntactic dependency annotation. We present a GWAP-style game implementation called W ORD G UESS1 where associations of a target word are offered to players in order to guess the target word. For example, associations such as winter, white, cold provide hints to players when guessing the target word snow. Our game is a web-based and mobile-based application whose aim is to learn and understand word-association and word-context relationships: previous research has shown that associations and corpus co-occurrence are related (Church and Hanks, 1990; de Deyne and St"
2021.konvens-1.24,P09-2053,0,0.0394785,"Missing"
2021.konvens-1.24,C02-1007,0,0.290666,"ne et al., 2019), among others. For many NLP purposes such as ontology induction and anaphora resolution, it is crucial to define and induce semantic relations between words or contexts, and according to the co-occurrence hypothesis (Miller, 1969; Spence and Owens, 1990) semantic association is related to the textual cooccurrence of stimulus-associate pairs. Therefore, a number of studies have exploited the connection between co-occurrence distributions and semantic relatedness, and used association norms as a test-bed for distributional models of semantic relatedness (Church and Hanks, 1990; Rapp, 2002; de Deyne and Storms, 2008a; Schulte im Walde et al., 2008, i.a.). Game Idea The aim of W ORD G UESS is to exploit a gamification environment in order to deepen the understanding of associative relatedness. Differently to previous approaches, we do not directly Figure 1: Sample page views of the game: (a) On the left you can see the decision page where the player can choose a predefined game, the number of targets and the difficulty level. (b) On the right you can see a game page at some point during the game where (top row: left to right) the player is currently working on the third out of f"
2021.spnlp-1.6,P19-1273,1,0.880202,"Missing"
2021.spnlp-1.6,N19-1423,0,0.00748564,"a single-layer LSTM. The final hidden state is used as input to a fully connected layer. 3 BiLSTM A single-layer Bidirectional LSTM (Graves et al., 2013) traverses the input. The final hidden states in both directions are concatenated and fed to a fully connected layer. BiLSTM+Attention This model combines the BiLSTM architecture with the attention mechanism described in Shimaoka et al. (2017a). The input is fed to a single-layer BiLSTM. Then, the attention-weighted sum of the hidden states corresponding to the input sequence is fed to a fully connected layer. BERT This is a pretrained BERT (Devlin et al., 2019) model trained solely on German corpora 3 and a fully connected layer which is trained while the BERT encoder is fine-tuned. After each input is encoded, we use the final hidden state of the first token, corresponding to the special token [CLS], as the contextualized representation of the input which serves as input to a fully connected layer. Basic Claim Classification Given the properties described above, we model claim classification as multi-label classification. We follow previous work on coarse-grained claim classification (Pad´o et al., 2019) in comparing a set 2 Further details regardi"
2021.spnlp-1.6,C04-1197,0,0.44746,"t developing practically useful models of fine-grained claim classification. Its main proposal is to exploit the hierarchical nature of claim ontologies by jointly predicting (frequent) supercategories and (informative) subcategories. By enforcing consistency between the levels, the predictions can profit off each other. We experiment with two operationalizations of this idea. The first one, Hierarchical Label Encoding (HLE, Shimaoka et al. (2017a)) introduces “soft” constraints through parameter sharing between classes in the classifier. The second one, Integer Linear Programming (ILP, e.g., Punyakanok et al. (2004)) introduces “hard” constraints in a post-processing step. Both methods can be applied to a range of claim classifier architectures. We present experiments with four architectures on a German manually annotated corpus from the so-called refugee crisis in Germany in 2015. We answer the following questions: Do HLE and ILP improve the performance in our experimental setup? (Yes.) Is there complementarity between them? (Yes.) Does the effect depend on the underlying architectures. (Broadly, no.) What types of classes is the improvement most pronounced for. (Low-frequency ones.) The analysis of pub"
2021.spnlp-1.6,W06-1616,0,0.0155168,"i) ≤ 0 The second constraint is that each predicted supercategory is accompanied by at least one if its subcategories. Let subs(i) denote the set of subcategories for supercategory i. The constraint is: X for each supercategory xi : xi − xj ≤ 0 j∈subs(i) ILP has a complementary profile to HLE in enforcing hard constraints on the output, without propagating the errors back to representation learning. Integer Linear Programming (ILP). ILP has been applied to enforce linguistically motivated constraints on predicted structures such as semantic roles (Punyakanok et al., 2004), dependency parsing (Riedel and Clarke, 2006), or entailment graphs (Berant et al., 2011). Formally, an integer linear program is an optimization problem over a set of integer variables x, given a linear objective function with a set of coefficients c and a set of linear inequality (and equality) constraints (Schrijver, 1984): max c |x 5 Experimental Evaluation Setup. We remove very infrequent subcategories in the dataset by applying a threshold of 20 instances. Smaller categories are merged with the preexisting subcategory x99, which exists for each supercategory as a ‘catch-all’ category for outlier cases. After filtering, there are 8"
2021.spnlp-1.6,W07-1509,0,0.0627328,"Specifically, HLE+ILP models achieves better Recall scores than HLE models (+7 points on average) and better Precision (+8 points on average) scores than ILP models. The effect is least pronounced for the best architecture (BERT); nevertheless, BERT with HLE and ILP achieves the overall highest Recall (0.59) and F-Score (0.60), corresponding to an improvement of 13 points F1 compared to the ‘plain’ version. The fact that the F1 boost is fueled mainly by Recall is particularly promising because optimizing for Recall is the best strategy when NLP tools are employed in semiautomatic annotation (Ganchev et al., 2007; Ambati et al., 2011). Qualitative Considerations. Finally, we investigate which subcategories benefit most from HLE and ILP in our best model (BERT). Table 4 again shows complementarity between HLE and ILP, indicating that a better combination of the two methods could lead to further improvements. HLE+ILP overlaps largely with HLE, mirroring the larger impact of HLE. Analysis of these classes shows that they belong to the mid and low frequency bands. However, not all low and mid frequency classes profit equally. To explain this, we note that the finegrained classes in the migration ontology"
2021.spnlp-1.6,P19-4004,0,0.0219905,"Missing"
2021.spnlp-1.6,E17-1119,0,0.13595,"he set of classes, the more data would be desirable, while in actuality, the number of instances per class shrinks (Mai et al., 2018; Chang et al., 2020). Our paper aims at developing practically useful models of fine-grained claim classification. Its main proposal is to exploit the hierarchical nature of claim ontologies by jointly predicting (frequent) supercategories and (informative) subcategories. By enforcing consistency between the levels, the predictions can profit off each other. We experiment with two operationalizations of this idea. The first one, Hierarchical Label Encoding (HLE, Shimaoka et al. (2017a)) introduces “soft” constraints through parameter sharing between classes in the classifier. The second one, Integer Linear Programming (ILP, e.g., Punyakanok et al. (2004)) introduces “hard” constraints in a post-processing step. Both methods can be applied to a range of claim classifier architectures. We present experiments with four architectures on a German manually annotated corpus from the so-called refugee crisis in Germany in 2015. We answer the following questions: Do HLE and ILP improve the performance in our experimental setup? (Yes.) Is there complementarity between them? (Yes.)"
2021.starsem-1.23,Q16-1003,0,0.0196507,"erally falls within the area of Bayesian probabilistic modeling (Koch, 2007). More specifically, it is related to model-based graph clustering techniques, e.g., Latent Space models such as Gaussian Mixture Models (Hoff et al., 2002; Duda and Hart, 1973). These methods are common in the field of community detection (Abbe, 2017). Within computational linguistics our approach is most strongly related to generative probabilistic topic models, where words in documents are modeled as being drawn from a latent topic distribution (Steyvers and Griffiths, 2007). Topics are often interpreted as senses (Frermann and Lapata, 2016; Perrone et al., 2019). Another common, yet non-probabilistic, modeling approach for word senses is to group word uses expressing similar meanings into clusters based on contextual features (Sch¨utze, 1998; Biemann, 2006). As to our knowledge, only a small set of studies is concerned with the modeling of human-annotated WUGs (McCarthy et al., 2016; Schlechtweg et al., 2020, 2021). This research line is motivated by insights from lexical semantics that word senses are no discrete objects (Kilgarriff, 1997; Erk et al., 2013). Most important to note is the pioneering work of McCarthy et al. (201"
2021.starsem-1.23,W06-3812,0,0.116123,"Duda and Hart, 1973). These methods are common in the field of community detection (Abbe, 2017). Within computational linguistics our approach is most strongly related to generative probabilistic topic models, where words in documents are modeled as being drawn from a latent topic distribution (Steyvers and Griffiths, 2007). Topics are often interpreted as senses (Frermann and Lapata, 2016; Perrone et al., 2019). Another common, yet non-probabilistic, modeling approach for word senses is to group word uses expressing similar meanings into clusters based on contextual features (Sch¨utze, 1998; Biemann, 2006). As to our knowledge, only a small set of studies is concerned with the modeling of human-annotated WUGs (McCarthy et al., 2016; Schlechtweg et al., 2020, 2021). This research line is motivated by insights from lexical semantics that word senses are no discrete objects (Kilgarriff, 1997; Erk et al., 2013). Most important to note is the pioneering work of McCarthy et al. (2016) as the first to represent human-annotated word uses within graphs and then clustering the uses based on heuristics such as connected components and cliques. McCarthy et al. derived edge weights from human lexical substi"
2021.starsem-1.23,P08-2063,0,0.0652567,"connection with a global threshold to group vertices with high edge weights and developed an efficient iterative sampling strategy for edges to reduce annotation load. However, these approaches are ad-hoc clustering methods which do not provide a probabilistic model for WUGs. 3 Data Table 1: DURel relatedness scale (Schlechtweg et al., 2018). represent word uses and weights w ∈ W represent the semantic proximity of a pair of uses (u1 , u2 ) ∈ E (Schlechtweg and Schulte im Walde, submitted). In practice, semantic proximity can be measured by human annotator judgments on a scale of relatedness (Brown, 2008; Schlechtweg et al., 2018) or similarity (Erk et al., 2013). Humanannotated WUGs are often sparsely observed and noisy, i.e., only a small percentage of edges from the full graph are annotated, and annotators often show disagreements, e.g. for ambiguous uses, as can be seen in Figure 1. Recently, Schlechtweg et al. (2020, 2021) developed a large-scale multi-lingual resource of WUGs. Annotators were asked to judge the semantic relatedness of pairs of word uses (such as the two uses of grasp in (1) and (2)) according to the scale in Table 1.1 (1) He continued to grasp, between forefinger and th"
2021.starsem-1.23,J16-2003,0,0.281058,"mulation of the Weighted Stochastic Block Model, a generative model for random graphs popular in biology, physics and social sciences. By providing a probabilistic model of graded word meaning we aim to approach the slippery and yet widely used notion of word sense in a novel way. The proposed framework enables us to rigorously compare models of word senses with respect to their fit to the data. We perform extensive experiments and select the empirically most adequate model. 1 Introduction Word Usage Graphs (WUGs) are a relatively new model of graded word meaning in context (Erk et al., 2013; McCarthy et al., 2016; Schlechtweg et al., 2021). They represent word uses (i.e., words in context) within a weighted undirected graph, with edge weights reflecting the semantic proximity between uses. WUGs may be obtained via human annotation by presenting annotators with pairs of words uses and asking them for proximity judgments. The WUGs may then be clustered into sets of uses exhibiting high semantic proximity, in order to reflect traditional word sense distinctions (McCarthy et al., 2016), and to provide insight into key aspects of word meaning such as polysemy, vagueness, and lexical semantic change (Schlec"
2021.starsem-1.23,W19-4707,0,0.0131734,"a of Bayesian probabilistic modeling (Koch, 2007). More specifically, it is related to model-based graph clustering techniques, e.g., Latent Space models such as Gaussian Mixture Models (Hoff et al., 2002; Duda and Hart, 1973). These methods are common in the field of community detection (Abbe, 2017). Within computational linguistics our approach is most strongly related to generative probabilistic topic models, where words in documents are modeled as being drawn from a latent topic distribution (Steyvers and Griffiths, 2007). Topics are often interpreted as senses (Frermann and Lapata, 2016; Perrone et al., 2019). Another common, yet non-probabilistic, modeling approach for word senses is to group word uses expressing similar meanings into clusters based on contextual features (Sch¨utze, 1998; Biemann, 2006). As to our knowledge, only a small set of studies is concerned with the modeling of human-annotated WUGs (McCarthy et al., 2016; Schlechtweg et al., 2020, 2021). This research line is motivated by insights from lexical semantics that word senses are no discrete objects (Kilgarriff, 1997; Erk et al., 2013). Most important to note is the pioneering work of McCarthy et al. (2016) as the first to repr"
2021.starsem-1.23,2020.semeval-1.1,1,0.799905,", 2016; Schlechtweg et al., 2021). They represent word uses (i.e., words in context) within a weighted undirected graph, with edge weights reflecting the semantic proximity between uses. WUGs may be obtained via human annotation by presenting annotators with pairs of words uses and asking them for proximity judgments. The WUGs may then be clustered into sets of uses exhibiting high semantic proximity, in order to reflect traditional word sense distinctions (McCarthy et al., 2016), and to provide insight into key aspects of word meaning such as polysemy, vagueness, and lexical semantic change (Schlechtweg et al., 2020, 2021). We suggest to model WUGs with a Bayesian formulation of the Weighted Stochastic Block Model (WSBM), a generative model for random graphs popular in biology, physics and social sciences (Aicher et al., 2014; Peixoto, 2017). The basic assumption of WSBMs is that vertices belong to latent blocks (clusters), and that vertices in the same block are stochastically equivalent (i.e., they have edges drawn from the same distribution). Fitting the model is equivalent to determining the optimal latent block structure providing a clustering of word uses. By using a Bayesian probabilistic model of"
2021.starsem-1.23,N18-2027,1,0.852785,"espective target words and binarized them according to a threshold. This idea was recently modified and extended by Schlechtweg et al. (2020, 2021). Schlechtweg et al. used semantic proximity judgments to annotate edges. They applied correlation clustering (Bansal et al., 2004) in connection with a global threshold to group vertices with high edge weights and developed an efficient iterative sampling strategy for edges to reduce annotation load. However, these approaches are ad-hoc clustering methods which do not provide a probabilistic model for WUGs. 3 Data Table 1: DURel relatedness scale (Schlechtweg et al., 2018). represent word uses and weights w ∈ W represent the semantic proximity of a pair of uses (u1 , u2 ) ∈ E (Schlechtweg and Schulte im Walde, submitted). In practice, semantic proximity can be measured by human annotator judgments on a scale of relatedness (Brown, 2008; Schlechtweg et al., 2018) or similarity (Erk et al., 2013). Humanannotated WUGs are often sparsely observed and noisy, i.e., only a small percentage of edges from the full graph are annotated, and annotators often show disagreements, e.g. for ambiguous uses, as can be seen in Figure 1. Recently, Schlechtweg et al. (2020, 2021) d"
2021.starsem-1.23,2021.emnlp-main.567,1,0.725882,"d Stochastic Block Model, a generative model for random graphs popular in biology, physics and social sciences. By providing a probabilistic model of graded word meaning we aim to approach the slippery and yet widely used notion of word sense in a novel way. The proposed framework enables us to rigorously compare models of word senses with respect to their fit to the data. We perform extensive experiments and select the empirically most adequate model. 1 Introduction Word Usage Graphs (WUGs) are a relatively new model of graded word meaning in context (Erk et al., 2013; McCarthy et al., 2016; Schlechtweg et al., 2021). They represent word uses (i.e., words in context) within a weighted undirected graph, with edge weights reflecting the semantic proximity between uses. WUGs may be obtained via human annotation by presenting annotators with pairs of words uses and asking them for proximity judgments. The WUGs may then be clustered into sets of uses exhibiting high semantic proximity, in order to reflect traditional word sense distinctions (McCarthy et al., 2016), and to provide insight into key aspects of word meaning such as polysemy, vagueness, and lexical semantic change (Schlechtweg et al., 2020, 2021)."
2021.starsem-1.23,J98-1004,0,0.880279,"Missing"
2021.starsem-1.24,W14-1207,0,0.0682729,"Missing"
2021.starsem-1.24,2020.lrec-1.537,1,0.830408,"Missing"
2021.starsem-1.24,Q17-1010,0,0.0343167,"er productivity (domain) modifier productivity (general) head productivity (domain) head productivity (general) Tertiles and Ranges low mid high 3–4 4–8 8–444 0 0–17 17–53,569 1–14 14–55 55–665 0–101 103–588 590–4,976 1–14 14–61 62–1,157 0–119 119–786 786–8,293 Micro-F1 low high 0.773 0.722 0.779 0.722 0.863 0.658 0.884 0.661 0.802 0.652 0.812 0.693 Table 8: Ranges of selected properties across tertiles, and results on binary classification for extreme ‘low’ and ‘high’ tertiles when using all features (cf. All in Table 2 with Micro-F1=0.732). ing: word2vec (Mikolov et al., 2013) and fastText (Bojanowski et al., 2017).5 We use the word2vec model, because it is a standard model for natural language processing applications. The fastText model works on character n-grams and not on words, and Bojanowski et al. (2017) argues that it performs well on closed compounds. This model is particularly interesting for us because a compound embedding is learned partially from the same n-grams as the embeddings of its constituents. Thus, we implicitly have a representation of the constituents in the compound embedding, which we expect to be beneficial for our classification task. Inspecting some words and their nearest ne"
2021.starsem-1.24,bonin-etal-2010-contrastive,0,0.0594013,"Missing"
2021.starsem-1.24,L16-1366,0,0.0223248,"tion, a major strand of methodologies are contrastive techniques, where a term candidate’s distribution in a domain-specific text corpus is compared to the distribution in a reference corpus, for example a general-language corpus (Ahmad et al., 1994; Rayson and Garside, 2000; Drouin, 2003; Kit and Liu, 2008; Bonin et al., 2010; Kochetkova, 2015; Lopes et al., 2016; Mykowiecka et al., 2018, i.a.). Many term difficulty prediction studies rely on some variant of contrastive approaches, mostly frequency-based; notable exceptions are Zeng-Treitler et al. (2008), who apply a contextual network, and Bouamor et al. (2016), who use a likelihood ratio test based on two language models. Most studies fall into the medical, biomedical or health domain. They rely on classical readability features such as frequency, term length, syllable count, the Dale-Chall readability formula or affixes (Zeng et al., 2005; Zeng-Treitler et al., 2008; Vydiswaran et al., 2014; Grabar et al., 2014). Some features are tailored to the medical domain, for example relying on neo-classical word 1 Termhood refers to the degree to which a lexical unit can be considered a domain-specific concept (Kageura and Umino, 1996). components, since m"
2021.starsem-1.24,W14-5702,0,0.0710014,"Missing"
2021.starsem-1.24,W09-3102,0,0.0654895,"Missing"
2021.starsem-1.24,W14-4812,0,0.131141,"Missing"
2021.starsem-1.24,W14-1202,0,0.0197135,"2016; Mykowiecka et al., 2018, i.a.). Many term difficulty prediction studies rely on some variant of contrastive approaches, mostly frequency-based; notable exceptions are Zeng-Treitler et al. (2008), who apply a contextual network, and Bouamor et al. (2016), who use a likelihood ratio test based on two language models. Most studies fall into the medical, biomedical or health domain. They rely on classical readability features such as frequency, term length, syllable count, the Dale-Chall readability formula or affixes (Zeng et al., 2005; Zeng-Treitler et al., 2008; Vydiswaran et al., 2014; Grabar et al., 2014). Some features are tailored to the medical domain, for example relying on neo-classical word 1 Termhood refers to the degree to which a lexical unit can be considered a domain-specific concept (Kageura and Umino, 1996). components, since medical terminology is considered to be highly influenced by Greek and Latin (Del´eger and Zweigenbaum, 2009; Bouamor et al., 2016). As to our knowledge, there is no previous work that investigated term difficulty prediction for complex phrases. Regarding the more general task of automatic term extraction, a few studies included complex phrases and their cons"
2021.starsem-1.24,E17-4012,1,0.771859,"Missing"
2021.starsem-1.24,W18-4909,1,0.868072,"Missing"
2021.starsem-1.24,W00-0901,0,0.336295,"a decision tree classifier using manually designed features to characterize termhood and compound formation, and neural classifiers using word embeddings. 2 Related Work Term difficulty prediction (also referred to as term familiarity or term technicality prediction) can be seen as a subtask of automatic term extraction. For automatic term extraction, a major strand of methodologies are contrastive techniques, where a term candidate’s distribution in a domain-specific text corpus is compared to the distribution in a reference corpus, for example a general-language corpus (Ahmad et al., 1994; Rayson and Garside, 2000; Drouin, 2003; Kit and Liu, 2008; Bonin et al., 2010; Kochetkova, 2015; Lopes et al., 2016; Mykowiecka et al., 2018, i.a.). Many term difficulty prediction studies rely on some variant of contrastive approaches, mostly frequency-based; notable exceptions are Zeng-Treitler et al. (2008), who apply a contextual network, and Bouamor et al. (2016), who use a likelihood ratio test based on two language models. Most studies fall into the medical, biomedical or health domain. They rely on classical readability features such as frequency, term length, syllable count, the Dale-Chall readability formul"
2021.starsem-1.24,I11-1024,0,0.0448059,"s for the decision tree classification using all features. The classification models significantly outperform the respective baselines in the binary classification tasks, but in the four-class distinctions this only applies to the Automotive domain and across all domains (non-significant results are in italics). For the binary task, the results for Automotive are better than for Cooking and DIY. We assume that this divergence is due to a higher imbalance of class sizes across the domains, cf. figure 1. Note that we decided against a direct computation of compound–constituent compositionality (Reddy et al., 2011; Schulte im Walde et al., 2013, 2016) as a feature, because the compound dataset was balanced for frequency. It includes infrequent compounds for which word embeddings and compositionality measures would be imprecise. Method: Decision Trees. Decision tree classifiers (DTs) are supervised machine learning methods that are represented as tree structures. DTs were chosen for this task because they are easy to 4 Note that for all but one of these features we have a balanced set of compounds in the gold standard, see section 3.3. 255 Results by feature group. Having looked at the results when usin"
2021.starsem-1.24,S16-2020,1,0.903299,"Missing"
2021.starsem-1.24,S13-1038,1,0.787126,"Missing"
2021.starsem-1.24,W14-4814,0,0.039444,"Missing"
2021.starsem-1.24,W17-1722,0,0.0584491,"Missing"
blessing-kuhn-2014-textual,C10-1011,0,\N,Missing
blessing-kuhn-2014-textual,zesch-etal-2008-extracting,0,\N,Missing
blessing-kuhn-2014-textual,hinrichs-etal-2010-weblicht,0,\N,Missing
bouma-etal-2010-towards,nivre-etal-2006-talbanken05,0,\N,Missing
bouma-etal-2010-towards,ritz-etal-2008-annotation,0,\N,Missing
bouma-etal-2010-towards,nivre-etal-2006-maltparser,0,\N,Missing
bouma-etal-2010-towards,J93-2004,0,\N,Missing
bouma-etal-2010-towards,W06-2920,0,\N,Missing
bouma-etal-2010-towards,2006.jeptalnrecital-invite.2,0,\N,Missing
bouma-etal-2010-towards,W07-2416,0,\N,Missing
bouma-etal-2010-towards,2005.mtsummit-papers.11,0,\N,Missing
C10-2129,W06-2922,0,0.0131657,"ble combinations of arc labels with a single head (Section 8). Section 9 offers a brief conclusion. 2 Related Work and Data Basis We quickly review the situation in data-driven dependency parsing in general and on applying it to German specifically. The two main approaches to data-driven dependency parsing are transition based dependency parsing (Nivre, 2003; Yamada and Matsumoto, 2003; Titov and Henderson, 2007) and maximum spanning tree based dependency parsing (Eisner, 1996; Eisner, 2000; McDonald and Pereira, 2006). Transition based parsers typically have a linear or quadratic complexity (Attardi, 2006). Nivre (2009) introduced a transition based nonprojective parsing algorithm that has a worst case quadratic complexity and an expected linear parsing time. Titov and Henderson (2007) combined a transition based parsing algorithm, using beam search, with a latent variable machine learning technique. Maximum spanning tree based dependency parsers decompose a dependency structure into factors. The factors of the first order maximum spanning tree parsing algorithm are edges consisting of the head, the dependent (child) and the edge label. This algorithm has a quadratic complexity. The second orde"
C10-2129,W09-1210,1,0.879438,"Missing"
C10-2129,W06-2920,0,0.0226861,"over the labels. This algorithm therefore has a complexity of O(n4 ). Johansson and Nugues (2008) reduced the required number of loops over the edge labels by considering only the edges that existed in the training corpus for a distinct head and child part-of-speech tag combination. Predating the surge of interest in data-based dependency parsing, there is a relatively long tradition of dependency parsing work on German, including for instance Menzel and Schr¨oder (1998) and Duchier and Debusmann (2001). German was included in the CoNLL shared tasks in 2006 (Multilingual Dependency Parsing, (Buchholz and Marsi, 2006)) and in 2009 (Syntactic and Semantic Dependencies in Multiple Languages, (Hajiˇc et al., 2009)) with data based on the TIGER 1123 corpus (Brants et al., 2002) in both cases. Since the original TIGER treebank is in a hybrid phrasestructural/dependency format with a relatively flat hierarchical structure, conversion to a pure dependency format involves some non-trivial steps. The 2008 ACL Workshop on Parsing German included a specific shared task on dependency parsing of German (K¨ubler, 2008), based on two sets of data: again the TIGER corpus – however with a different conversion routine than"
C10-2129,D07-1101,0,0.0256126,"learning technique. Maximum spanning tree based dependency parsers decompose a dependency structure into factors. The factors of the first order maximum spanning tree parsing algorithm are edges consisting of the head, the dependent (child) and the edge label. This algorithm has a quadratic complexity. The second order parsing algorithm of McDonald and Pereira (2006) uses a separate algorithm for edge labeling. In addition to the first order factors, this algorithm uses the edges to those children which are closest to the dependent and has a complexity of O(n3 ). The second order algorithm of Carreras (2007) uses in addition to McDonald and Pereira (2006) the child of the dependent occurring in the sentence between the head and the dependent as well as the edge from the dependents to a grandchild. The edge labeling is an integral part of the algorithm which requires an additional loop over the labels. This algorithm therefore has a complexity of O(n4 ). Johansson and Nugues (2008) reduced the required number of loops over the edge labels by considering only the edges that existed in the training corpus for a distinct head and child part-of-speech tag combination. Predating the surge of interest i"
C10-2129,P01-1024,0,0.132315,"he dependents to a grandchild. The edge labeling is an integral part of the algorithm which requires an additional loop over the labels. This algorithm therefore has a complexity of O(n4 ). Johansson and Nugues (2008) reduced the required number of loops over the edge labels by considering only the edges that existed in the training corpus for a distinct head and child part-of-speech tag combination. Predating the surge of interest in data-based dependency parsing, there is a relatively long tradition of dependency parsing work on German, including for instance Menzel and Schr¨oder (1998) and Duchier and Debusmann (2001). German was included in the CoNLL shared tasks in 2006 (Multilingual Dependency Parsing, (Buchholz and Marsi, 2006)) and in 2009 (Syntactic and Semantic Dependencies in Multiple Languages, (Hajiˇc et al., 2009)) with data based on the TIGER 1123 corpus (Brants et al., 2002) in both cases. Since the original TIGER treebank is in a hybrid phrasestructural/dependency format with a relatively flat hierarchical structure, conversion to a pure dependency format involves some non-trivial steps. The 2008 ACL Workshop on Parsing German included a specific shared task on dependency parsing of German (K"
C10-2129,W98-0509,0,0.562004,"Missing"
C10-2129,P05-1013,0,0.02721,"al., 2006) is a languageindependent system for data-driven dependency parsing which is freely available.7 It is based on a deterministic parsing strategy in combination with treebank-induced classifiers for predicting parsing actions. MaltParser employs a rich feature representation in order to guide parsing. For the training of the Malt parser model that we use in the stacking experiments, we use learner and parser settings identical to the ones optimized for German in the CoNLL-X shared task (Nivre et al., 2006). Furthermore, we employ the technique of pseudo-projective parsing described in Nilsson and Nivre (2005) and a split prediction strategy for predicting parse transitions and arc labels (Nivre and Hall, 2008).8 In order to obtain automatic parses for the whole data set, we perform a 10fold split. For the parser stacking, we follow the approach of Nivre and McDonald (2008), using MaltParser as a guide for the MST parser with the hash kernel, i.e., providing the arcs and labels assigned by MaltParser as features. Table 5 shows the scores we obtain by parser stacking. Although our version of MaltParser does not quite have the same performance as for instance the version of Hall and Nivre (2008), its"
C10-2129,C96-1058,0,0.125123,"pilot study (Section 7), and finally, we apply Integer Linear Programming in a targeted way to add some global constraints on possible combinations of arc labels with a single head (Section 8). Section 9 offers a brief conclusion. 2 Related Work and Data Basis We quickly review the situation in data-driven dependency parsing in general and on applying it to German specifically. The two main approaches to data-driven dependency parsing are transition based dependency parsing (Nivre, 2003; Yamada and Matsumoto, 2003; Titov and Henderson, 2007) and maximum spanning tree based dependency parsing (Eisner, 1996; Eisner, 2000; McDonald and Pereira, 2006). Transition based parsers typically have a linear or quadratic complexity (Attardi, 2006). Nivre (2009) introduced a transition based nonprojective parsing algorithm that has a worst case quadratic complexity and an expected linear parsing time. Titov and Henderson (2007) combined a transition based parsing algorithm, using beam search, with a latent variable machine learning technique. Maximum spanning tree based dependency parsers decompose a dependency structure into factors. The factors of the first order maximum spanning tree parsing algorithm a"
C10-2129,W08-1007,0,0.257232,"It is based on a deterministic parsing strategy in combination with treebank-induced classifiers for predicting parsing actions. MaltParser employs a rich feature representation in order to guide parsing. For the training of the Malt parser model that we use in the stacking experiments, we use learner and parser settings identical to the ones optimized for German in the CoNLL-X shared task (Nivre et al., 2006). Furthermore, we employ the technique of pseudo-projective parsing described in Nilsson and Nivre (2005) and a split prediction strategy for predicting parse transitions and arc labels (Nivre and Hall, 2008).8 In order to obtain automatic parses for the whole data set, we perform a 10fold split. For the parser stacking, we follow the approach of Nivre and McDonald (2008), using MaltParser as a guide for the MST parser with the hash kernel, i.e., providing the arcs and labels assigned by MaltParser as features. Table 5 shows the scores we obtain by parser stacking. Although our version of MaltParser does not quite have the same performance as for instance the version of Hall and Nivre (2008), its guidance leads to a small improvement in the overall parsing results. MaltParser our parser +stacking"
C10-2129,P08-1108,0,0.0752809,"ts significantly. This seems to support our intuition that number helps in disambiguating case values. However, adding gender information does not further increase this effect but hurts parser performance even more than case annotation alone. This leaves us with a puzzle here. Annotating case and number helps the parser, but case alone or having case, number and gender together affects performance negatively. A possible explanation might be that the effect of the gender information is masked by the increased number of feature values (24) which confuses the parsing algorithm. 7 Parser Stacking Nivre and McDonald (2008) show how two different approaches to data-driven dependency pars6 Person would be another syntactically relevant information. However, since we are dealing with a newspaper corpus, first and second person features appear very rarely. 1127 ing, the graph-based and transition-based approaches, may be combined and subsequently learn to complement each other to achieve improved parsing results for different languages. MaltParser (Nivre et al., 2006) is a languageindependent system for data-driven dependency parsing which is freely available.7 It is based on a deterministic parsing strategy in com"
C10-2129,W06-2933,0,0.0343479,"Missing"
C10-2129,W03-3017,0,0.265855,"nation of different parsing strategies is advantageous; we include a relatively simple parser stacking procedure in our pilot study (Section 7), and finally, we apply Integer Linear Programming in a targeted way to add some global constraints on possible combinations of arc labels with a single head (Section 8). Section 9 offers a brief conclusion. 2 Related Work and Data Basis We quickly review the situation in data-driven dependency parsing in general and on applying it to German specifically. The two main approaches to data-driven dependency parsing are transition based dependency parsing (Nivre, 2003; Yamada and Matsumoto, 2003; Titov and Henderson, 2007) and maximum spanning tree based dependency parsing (Eisner, 1996; Eisner, 2000; McDonald and Pereira, 2006). Transition based parsers typically have a linear or quadratic complexity (Attardi, 2006). Nivre (2009) introduced a transition based nonprojective parsing algorithm that has a worst case quadratic complexity and an expected linear parsing time. Titov and Henderson (2007) combined a transition based parsing algorithm, using beam search, with a latent variable machine learning technique. Maximum spanning tree based dependency parser"
C10-2129,P09-1040,0,0.0510405,"of arc labels with a single head (Section 8). Section 9 offers a brief conclusion. 2 Related Work and Data Basis We quickly review the situation in data-driven dependency parsing in general and on applying it to German specifically. The two main approaches to data-driven dependency parsing are transition based dependency parsing (Nivre, 2003; Yamada and Matsumoto, 2003; Titov and Henderson, 2007) and maximum spanning tree based dependency parsing (Eisner, 1996; Eisner, 2000; McDonald and Pereira, 2006). Transition based parsers typically have a linear or quadratic complexity (Attardi, 2006). Nivre (2009) introduced a transition based nonprojective parsing algorithm that has a worst case quadratic complexity and an expected linear parsing time. Titov and Henderson (2007) combined a transition based parsing algorithm, using beam search, with a latent variable machine learning technique. Maximum spanning tree based dependency parsers decompose a dependency structure into factors. The factors of the first order maximum spanning tree parsing algorithm are edges consisting of the head, the dependent (child) and the edge label. This algorithm has a quadratic complexity. The second order parsing algo"
C10-2129,W08-2123,0,0.0137423,") uses a separate algorithm for edge labeling. In addition to the first order factors, this algorithm uses the edges to those children which are closest to the dependent and has a complexity of O(n3 ). The second order algorithm of Carreras (2007) uses in addition to McDonald and Pereira (2006) the child of the dependent occurring in the sentence between the head and the dependent as well as the edge from the dependents to a grandchild. The edge labeling is an integral part of the algorithm which requires an additional loop over the labels. This algorithm therefore has a complexity of O(n4 ). Johansson and Nugues (2008) reduced the required number of loops over the edge labels by considering only the edges that existed in the training corpus for a distinct head and child part-of-speech tag combination. Predating the surge of interest in data-based dependency parsing, there is a relatively long tradition of dependency parsing work on German, including for instance Menzel and Schr¨oder (1998) and Duchier and Debusmann (2001). German was included in the CoNLL shared tasks in 2006 (Multilingual Dependency Parsing, (Buchholz and Marsi, 2006)) and in 2009 (Syntactic and Semantic Dependencies in Multiple Languages,"
C10-2129,P95-1037,0,0.0587336,"s of the noun phrase directly at PP level. This annotation was kept in the dependency version and it can cause problems for the parser since there are two different ways of annotating NPs: (i) for normal NPs where all dependents of the noun are attached as daughters of the head noun and (ii) for NPs in PPs where all dependents of the noun are attached as daughters to the preposition thus being sisters to their head noun. We changed the annotation of PPs by identifying the head noun in the PP and attaching all of its siblings to it. To find the correct head, we used a heuristic in the style of Magerman (1995). The head is chosen by taking the rightmost daughter of the preposition that has a category label according to the heuristic and is labeled with NK (noun kernel element). Table 1 shows the parser performance on the data after PP-restructuring.4 The explanation for the benefit of the restructuring is of course that 4 Note that we are evaluating against a gold standard here (and in the rest of the paper) which has been restructured as well. With a different gold standard one could argue that the absolute figures we obtain are not fully comparable with the original CoNLL shared task. However, si"
C10-2129,W96-0213,0,0.25313,"y likely to mislead the parser in its decision process. A lot of the parser’s features include PoS tags and reducing the amount of errors during PoS tagging will therefore reduce misleading feature values as well. Since the quality of the automatically assigned PoS tags in the German CoNLL ’09 data is not state-of-the-art (see Table 2 below), we decided to retag the data with our own tagger which uses additional information from a symbolic morphological analyzer to direct a statistical classifier. For the assignment of PoS tags, we apply a standard maximum entropy classification approach (see Ratnaparkhi (1996)). The classes of the classifier are the PoS categories defined in the Stuttgart-T¨ubingen Tag Set (STTS) (Schiller et al., 1999). We use standard binarized features like the word itself, its last three letters, whether the word is capitalized, contains a hyphen, a digit or whether it consists of digits only. As the only nonbinary feature, word length is recorded. These standard features are augmented by a number of binary features that support the classification process by providing a preselection of possible PoS tags. Every word is analyzed by DMOR, a finite state morphological analyzer, fro"
C10-2129,P10-1111,1,0.801761,"Missing"
C10-2129,W07-2218,0,0.0457627,"advantageous; we include a relatively simple parser stacking procedure in our pilot study (Section 7), and finally, we apply Integer Linear Programming in a targeted way to add some global constraints on possible combinations of arc labels with a single head (Section 8). Section 9 offers a brief conclusion. 2 Related Work and Data Basis We quickly review the situation in data-driven dependency parsing in general and on applying it to German specifically. The two main approaches to data-driven dependency parsing are transition based dependency parsing (Nivre, 2003; Yamada and Matsumoto, 2003; Titov and Henderson, 2007) and maximum spanning tree based dependency parsing (Eisner, 1996; Eisner, 2000; McDonald and Pereira, 2006). Transition based parsers typically have a linear or quadratic complexity (Attardi, 2006). Nivre (2009) introduced a transition based nonprojective parsing algorithm that has a worst case quadratic complexity and an expected linear parsing time. Titov and Henderson (2007) combined a transition based parsing algorithm, using beam search, with a latent variable machine learning technique. Maximum spanning tree based dependency parsers decompose a dependency structure into factors. The fac"
C10-2129,W03-3023,0,0.0797297,"ferent parsing strategies is advantageous; we include a relatively simple parser stacking procedure in our pilot study (Section 7), and finally, we apply Integer Linear Programming in a targeted way to add some global constraints on possible combinations of arc labels with a single head (Section 8). Section 9 offers a brief conclusion. 2 Related Work and Data Basis We quickly review the situation in data-driven dependency parsing in general and on applying it to German specifically. The two main approaches to data-driven dependency parsing are transition based dependency parsing (Nivre, 2003; Yamada and Matsumoto, 2003; Titov and Henderson, 2007) and maximum spanning tree based dependency parsing (Eisner, 1996; Eisner, 2000; McDonald and Pereira, 2006). Transition based parsers typically have a linear or quadratic complexity (Attardi, 2006). Nivre (2009) introduced a transition based nonprojective parsing algorithm that has a worst case quadratic complexity and an expected linear parsing time. Titov and Henderson (2007) combined a transition based parsing algorithm, using beam search, with a latent variable machine learning technique. Maximum spanning tree based dependency parsers decompose a dependency str"
C10-2129,E06-1011,0,0.041155,"finally, we apply Integer Linear Programming in a targeted way to add some global constraints on possible combinations of arc labels with a single head (Section 8). Section 9 offers a brief conclusion. 2 Related Work and Data Basis We quickly review the situation in data-driven dependency parsing in general and on applying it to German specifically. The two main approaches to data-driven dependency parsing are transition based dependency parsing (Nivre, 2003; Yamada and Matsumoto, 2003; Titov and Henderson, 2007) and maximum spanning tree based dependency parsing (Eisner, 1996; Eisner, 2000; McDonald and Pereira, 2006). Transition based parsers typically have a linear or quadratic complexity (Attardi, 2006). Nivre (2009) introduced a transition based nonprojective parsing algorithm that has a worst case quadratic complexity and an expected linear parsing time. Titov and Henderson (2007) combined a transition based parsing algorithm, using beam search, with a latent variable machine learning technique. Maximum spanning tree based dependency parsers decompose a dependency structure into factors. The factors of the first order maximum spanning tree parsing algorithm are edges consisting of the head, the depend"
C10-2129,W08-1008,0,\N,Missing
C10-2129,W09-1201,0,\N,Missing
C10-2163,W02-1503,1,0.801455,"tween a verbal, predicative or adverbial use. 2.1 Participles in the German LFG In order to account for sentences like (1-c), an intuitive approach would be to generally allow for adverb conversion of participles in the grammar. However, in Zarrieß et al. (2010), we show that such a rule can have a strong negative effect on the overall performance of the parsing system, despite the fact that it produces the desired syntactic and semantic analysis for specific sentences. This problem was illustrated using a German LFG grammar (Rohrer and Forst, 2006) constructed as part of the ParGram project (Butt et al., 2002). The grammar is implemented in the XLE, a grammar development environment which includes a very efficient LFG parser and a stochastic disambiguation component which is based on a loglinear probability model (Riezler et al., 2002). In Zarrieß et al. (2010), we found that the naive implementation of adverbial participles in the German LFG, i.e. in terms of a general grammar rule that allows for participles-adverb conversion, leads to spurious ambiguities that mislead the disambiguation component of the grammar. Moreover, the rule increases the number of timeouts, i.e. sentences that cannot be p"
C10-2163,H01-1035,0,0.0430209,"se study on cross-lingual induction of lexical resources for deep, broad-coverage syntactic analysis of German. We use a parallel corpus to induce a classifier for German participles which can predict their syntactic category. By means of this classifier, we induce a resource of adverbial participles from a huge monolingual corpus of German. We integrate the resource into a German LFG grammar and show that it improves parsing coverage while maintaining accuracy. 1 Introduction Parallel corpora are currently exploited in a wide range of induction scenarios, including projection of morphologic (Yarowsky et al., 2001), syntactic (Hwa et al., 2005) and semantic (Pad´o and Lapata, 2009) resources. In this paper, we use crosslingual data to learn to predict whether a lexical item belongs to a specific syntactic category that cannot easily be learned from monolingual resources. In an application test scenario, we show that this prediction method can be used to obtain a lexical resource that improves deep, grammarbased parsing. The general idea of cross-lingual induction is that linguistic annotations or structures, which are not available or explicit in a given language, can be inferred from another language w"
C10-2163,W10-2106,1,0.214349,"However, this latter perspective has been less prominent in the NLP community so far. This paper investigates a cross-lingual induction method based on an exemplary problem arising in the deep syntactic analysis of German. This showcase is the syntactic flexibility of German participles, being morphologically ambiguous between verbal, adjectival and adverbial readings, and it is instructive for several reasons: first, the phenomenon is a notorious problem for linguistic analysis and annotation of German, such that standard German resources do not represent the underlying analysis. Second, in Zarrieß et al. (2010), we showed that integrating the phenomenon of adverbial participles in a naive way into a broadcoverage grammar of German leads to significant parsing problems, due to spurious ambiguities. Third, it is completely straightforward to detect adverbial participles in cross-lingual data since in other languages, e.g. English or French, adverbs are often morphologically marked. In this paper, we use instances of adverbially translated participles in a parallel corpus to bootstrap a classifier that is able to identify an adverbially used participle based on its monolingual syntactic context. In con"
C10-2163,W02-2018,0,0.0205207,"To do this, we use the filtering mechanisms already proposed in Zarrieß et al. (2010). These filters apply on the type level, such that we first identify the positive types (46 total) and then use all instances of these types in the 4891 sentences as positive instances of adverbial participles (1978 instances). The remaining sentences are used as negative instances. For the training of the classifier, we use maximum-entropy classification, which is also commonly used for the general task of tagging (Ratnaparkhi, 1996). In particular, we use the open source TADM tool for parameter estimation (Malouf, 2002). The tags of the words surrounding the participles are used as features in the classification task. We explore different sizes of the context window, where the trigram window is the most succesful (see Table 1). Beyond the trigram window, the results of the classifier start decreasing again, probably because of too many misleading features. Generally, this experiment shows that the grammar-based identification is more precise, but that the classifier still performs surprisingly well. Compared to the results from the grammar-based identification, the high accuracy of the classifier suggests th"
C10-2163,P07-1123,0,0.0605871,"Missing"
C10-2163,W96-0213,0,0.182566,"nces from this training set, and then divide it into a set of positive and negative instances. To do this, we use the filtering mechanisms already proposed in Zarrieß et al. (2010). These filters apply on the type level, such that we first identify the positive types (46 total) and then use all instances of these types in the 4891 sentences as positive instances of adverbial participles (1978 instances). The remaining sentences are used as negative instances. For the training of the classifier, we use maximum-entropy classification, which is also commonly used for the general task of tagging (Ratnaparkhi, 1996). In particular, we use the open source TADM tool for parameter estimation (Malouf, 2002). The tags of the words surrounding the participles are used as features in the classification task. We explore different sizes of the context window, where the trigram window is the most succesful (see Table 1). Beyond the trigram window, the results of the classifier start decreasing again, probably because of too many misleading features. Generally, this experiment shows that the grammar-based identification is more precise, but that the classifier still performs surprisingly well. Compared to the resul"
C10-2163,P02-1035,0,0.0239769,". However, in Zarrieß et al. (2010), we show that such a rule can have a strong negative effect on the overall performance of the parsing system, despite the fact that it produces the desired syntactic and semantic analysis for specific sentences. This problem was illustrated using a German LFG grammar (Rohrer and Forst, 2006) constructed as part of the ParGram project (Butt et al., 2002). The grammar is implemented in the XLE, a grammar development environment which includes a very efficient LFG parser and a stochastic disambiguation component which is based on a loglinear probability model (Riezler et al., 2002). In Zarrieß et al. (2010), we found that the naive implementation of adverbial participles in the German LFG, i.e. in terms of a general grammar rule that allows for participles-adverb conversion, leads to spurious ambiguities that mislead the disambiguation component of the grammar. Moreover, the rule increases the number of timeouts, i.e. sentences that cannot be parsed in a predefined amount of time (20 seconds). Therefore, we observe a drop in parsing accuracy although grammar coverage is improved. As a solution, we induced a lexical resource of adverbial participles based on their adverb"
C10-2163,rohrer-forst-2006-improving,1,0.844851,"t inflected, the surface form of a German participle is ambiguous between a verbal, predicative or adverbial use. 2.1 Participles in the German LFG In order to account for sentences like (1-c), an intuitive approach would be to generally allow for adverb conversion of participles in the grammar. However, in Zarrieß et al. (2010), we show that such a rule can have a strong negative effect on the overall performance of the parsing system, despite the fact that it produces the desired syntactic and semantic analysis for specific sentences. This problem was illustrated using a German LFG grammar (Rohrer and Forst, 2006) constructed as part of the ParGram project (Butt et al., 2002). The grammar is implemented in the XLE, a grammar development environment which includes a very efficient LFG parser and a stochastic disambiguation component which is based on a loglinear probability model (Riezler et al., 2002). In Zarrieß et al. (2010), we found that the naive implementation of adverbial participles in the German LFG, i.e. in terms of a general grammar rule that allows for participles-adverb conversion, leads to spurious ambiguities that mislead the disambiguation component of the grammar. Moreover, the rule in"
C12-2014,W09-1210,0,0.025382,"approximation algorithm of McDonald and Pereira (2006). Non-projective edges have also been handled by applying pre- and post-processing steps to the training and test data, allowing for the use of any labeled projective parsing algorithm, only to recover the non-projective edges after parsing, e.g. pseudo-projective parsing (Nivre and Nilsson, 2005). In the CoNLL 2008 and 2009 Shared Tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009), some of the best systems used the chart-based parsing algorithm. Besides using slightly different feature sets, non-projective edges were handled differently – Bohnet (2009) used the non-projective approximation algorithm, while Johansson and Nugues (2008) and Che et al. (2009) used pseudo-projective parsing. Handling non-projective edges is unarguably an important aspect of a parser, however, little is known about whether one of the methods mentioned above is better than the other. With a fixed feature set, we compare pseudo-projective parsing with nonprojective approximation using a state-of-the-art chart-based dependency parser (Bohnet, 2010). We also evaluate different encoding schemes for pseudo-projective parsing. More recently, highly accurate parsers that"
C12-2014,C10-1011,0,0.163906,"based parsing algorithm. Besides using slightly different feature sets, non-projective edges were handled differently – Bohnet (2009) used the non-projective approximation algorithm, while Johansson and Nugues (2008) and Che et al. (2009) used pseudo-projective parsing. Handling non-projective edges is unarguably an important aspect of a parser, however, little is known about whether one of the methods mentioned above is better than the other. With a fixed feature set, we compare pseudo-projective parsing with nonprojective approximation using a state-of-the-art chart-based dependency parser (Bohnet, 2010). We also evaluate different encoding schemes for pseudo-projective parsing. More recently, highly accurate parsers that model non-projective edges directly in the parsing algorithm have been proposed, such as the ILP-based parser of Martins et al. (2010) as well as algorithms relying on non-projective head automata (Koo et al., 2010). It would be interesting to include these parsers in our study, however they only provide unlabeled trees. For now, we leave the extension of these parsers to the labeled case and the comparison to future work. All experiments are performed on three languages tha"
C12-2014,D07-1101,0,0.0186739,"st scoring dependency tree for a sentence: ˆy = arg max y F (x, y), given a scoring function F . To make the search for the optimal tree tractable, the scoring function is decomposed into a sum over factors of the tree (McDonald et al., 2005a): X F (x, y) = ψ( f ) · w f ∈ f ac t ors(x, y) where ψ is a feature-mapping function that maps a factor f to a vector in high-dimensional feature space and w a weight vector. The chart-based algorithm of Eisner (1996) has the advantage that it can incorporate secondorder factors while still remaining computationally feasible. The version we use is due to Carreras (2007) and makes use of second-order factors including sibling and grandchild relations. This factorization offers access to valuable features but comes at the cost of a time complexity of O(Ln4 ), where L is the number of edge labels. To reduce the impact of the factor L, edge filters are applied (Bohnet, 2010), constraining the search of edge labels to those observed in training for the same head and dependent POS-tags; this reduces execution time considerably. The Non-projective Approximation algorithm (McDonald and Pereira, 2006) exploits the observation that, although the chart-based parsing al"
C12-2014,W09-1207,0,0.0131477,"y applying pre- and post-processing steps to the training and test data, allowing for the use of any labeled projective parsing algorithm, only to recover the non-projective edges after parsing, e.g. pseudo-projective parsing (Nivre and Nilsson, 2005). In the CoNLL 2008 and 2009 Shared Tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009), some of the best systems used the chart-based parsing algorithm. Besides using slightly different feature sets, non-projective edges were handled differently – Bohnet (2009) used the non-projective approximation algorithm, while Johansson and Nugues (2008) and Che et al. (2009) used pseudo-projective parsing. Handling non-projective edges is unarguably an important aspect of a parser, however, little is known about whether one of the methods mentioned above is better than the other. With a fixed feature set, we compare pseudo-projective parsing with nonprojective approximation using a state-of-the-art chart-based dependency parser (Bohnet, 2010). We also evaluate different encoding schemes for pseudo-projective parsing. More recently, highly accurate parsers that model non-projective edges directly in the parsing algorithm have been proposed, such as the ILP-based p"
C12-2014,C96-1058,0,0.112341,"tforward way, without the use of traces or secondary edges. Informally, a dependency tree is said to be non-projective if it cannot be drawn without crossing edges. An example is shown in Figure 1. OBJ PRD ROOT SBJ root It OPRD NMOD is what federal SBJ support VC should MNR try hardest IM to achieve Figure 1: A non-projective sentence Although there are decoding algorithms for graph-based parsers that are able to output nonprojective trees directly (e.g. spanning tree algorithms (McDonald et al., 2005b) and ILP-based parsers (Riedel and Clarke, 2006, inter alia)), the chart-based algorithm of Eisner (1996), which is restricted to projective output, has shown very promising results in recent years. It typically outperforms the non-projective algorithms since it allows access to features involving pairs of edges. A notable extension to the chart-based parsing algorithm that is able to output non-projective dependencies while still including edge-pair features is the non-projective approximation algorithm of McDonald and Pereira (2006). Non-projective edges have also been handled by applying pre- and post-processing steps to the training and test data, allowing for the use of any labeled projectiv"
C12-2014,W09-1201,0,0.0589034,"Missing"
C12-2014,W08-2123,0,0.0894516,"e edges have also been handled by applying pre- and post-processing steps to the training and test data, allowing for the use of any labeled projective parsing algorithm, only to recover the non-projective edges after parsing, e.g. pseudo-projective parsing (Nivre and Nilsson, 2005). In the CoNLL 2008 and 2009 Shared Tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009), some of the best systems used the chart-based parsing algorithm. Besides using slightly different feature sets, non-projective edges were handled differently – Bohnet (2009) used the non-projective approximation algorithm, while Johansson and Nugues (2008) and Che et al. (2009) used pseudo-projective parsing. Handling non-projective edges is unarguably an important aspect of a parser, however, little is known about whether one of the methods mentioned above is better than the other. With a fixed feature set, we compare pseudo-projective parsing with nonprojective approximation using a state-of-the-art chart-based dependency parser (Bohnet, 2010). We also evaluate different encoding schemes for pseudo-projective parsing. More recently, highly accurate parsers that model non-projective edges directly in the parsing algorithm have been proposed, s"
C12-2014,D10-1125,0,0.0136582,"ct of a parser, however, little is known about whether one of the methods mentioned above is better than the other. With a fixed feature set, we compare pseudo-projective parsing with nonprojective approximation using a state-of-the-art chart-based dependency parser (Bohnet, 2010). We also evaluate different encoding schemes for pseudo-projective parsing. More recently, highly accurate parsers that model non-projective edges directly in the parsing algorithm have been proposed, such as the ILP-based parser of Martins et al. (2010) as well as algorithms relying on non-projective head automata (Koo et al., 2010). It would be interesting to include these parsers in our study, however they only provide unlabeled trees. For now, we leave the extension of these parsers to the labeled case and the comparison to future work. All experiments are performed on three languages that exhibit different typological properties and frequency of non-projective dependencies: Czech, English, and German. We find that non-projective approximation performs better than pseudo-projective parsing, although both methods clearly outperform a projective baseline. While similar studies have been carried out for transition-based"
C12-2014,D10-1004,0,0.0135542,"udo-projective parsing. Handling non-projective edges is unarguably an important aspect of a parser, however, little is known about whether one of the methods mentioned above is better than the other. With a fixed feature set, we compare pseudo-projective parsing with nonprojective approximation using a state-of-the-art chart-based dependency parser (Bohnet, 2010). We also evaluate different encoding schemes for pseudo-projective parsing. More recently, highly accurate parsers that model non-projective edges directly in the parsing algorithm have been proposed, such as the ILP-based parser of Martins et al. (2010) as well as algorithms relying on non-projective head automata (Koo et al., 2010). It would be interesting to include these parsers in our study, however they only provide unlabeled trees. For now, we leave the extension of these parsers to the labeled case and the comparison to future work. All experiments are performed on three languages that exhibit different typological properties and frequency of non-projective dependencies: Czech, English, and German. We find that non-projective approximation performs better than pseudo-projective parsing, although both methods clearly outperform a proje"
C12-2014,P05-1012,0,0.356251,"yntax, can represent long-distance dependencies between words through non-projective dependencies in a more straightforward way, without the use of traces or secondary edges. Informally, a dependency tree is said to be non-projective if it cannot be drawn without crossing edges. An example is shown in Figure 1. OBJ PRD ROOT SBJ root It OPRD NMOD is what federal SBJ support VC should MNR try hardest IM to achieve Figure 1: A non-projective sentence Although there are decoding algorithms for graph-based parsers that are able to output nonprojective trees directly (e.g. spanning tree algorithms (McDonald et al., 2005b) and ILP-based parsers (Riedel and Clarke, 2006, inter alia)), the chart-based algorithm of Eisner (1996), which is restricted to projective output, has shown very promising results in recent years. It typically outperforms the non-projective algorithms since it allows access to features involving pairs of edges. A notable extension to the chart-based parsing algorithm that is able to output non-projective dependencies while still including edge-pair features is the non-projective approximation algorithm of McDonald and Pereira (2006). Non-projective edges have also been handled by applying"
C12-2014,E06-1011,0,0.469971,"ut nonprojective trees directly (e.g. spanning tree algorithms (McDonald et al., 2005b) and ILP-based parsers (Riedel and Clarke, 2006, inter alia)), the chart-based algorithm of Eisner (1996), which is restricted to projective output, has shown very promising results in recent years. It typically outperforms the non-projective algorithms since it allows access to features involving pairs of edges. A notable extension to the chart-based parsing algorithm that is able to output non-projective dependencies while still including edge-pair features is the non-projective approximation algorithm of McDonald and Pereira (2006). Non-projective edges have also been handled by applying pre- and post-processing steps to the training and test data, allowing for the use of any labeled projective parsing algorithm, only to recover the non-projective edges after parsing, e.g. pseudo-projective parsing (Nivre and Nilsson, 2005). In the CoNLL 2008 and 2009 Shared Tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009), some of the best systems used the chart-based parsing algorithm. Besides using slightly different feature sets, non-projective edges were handled differently – Bohnet (2009) used the non-projective approximation al"
C12-2014,H05-1066,0,0.351756,"yntax, can represent long-distance dependencies between words through non-projective dependencies in a more straightforward way, without the use of traces or secondary edges. Informally, a dependency tree is said to be non-projective if it cannot be drawn without crossing edges. An example is shown in Figure 1. OBJ PRD ROOT SBJ root It OPRD NMOD is what federal SBJ support VC should MNR try hardest IM to achieve Figure 1: A non-projective sentence Although there are decoding algorithms for graph-based parsers that are able to output nonprojective trees directly (e.g. spanning tree algorithms (McDonald et al., 2005b) and ILP-based parsers (Riedel and Clarke, 2006, inter alia)), the chart-based algorithm of Eisner (1996), which is restricted to projective output, has shown very promising results in recent years. It typically outperforms the non-projective algorithms since it allows access to features involving pairs of edges. A notable extension to the chart-based parsing algorithm that is able to output non-projective dependencies while still including edge-pair features is the non-projective approximation algorithm of McDonald and Pereira (2006). Non-projective edges have also been handled by applying"
C12-2014,P05-1013,0,0.493786,"tperforms the non-projective algorithms since it allows access to features involving pairs of edges. A notable extension to the chart-based parsing algorithm that is able to output non-projective dependencies while still including edge-pair features is the non-projective approximation algorithm of McDonald and Pereira (2006). Non-projective edges have also been handled by applying pre- and post-processing steps to the training and test data, allowing for the use of any labeled projective parsing algorithm, only to recover the non-projective edges after parsing, e.g. pseudo-projective parsing (Nivre and Nilsson, 2005). In the CoNLL 2008 and 2009 Shared Tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009), some of the best systems used the chart-based parsing algorithm. Besides using slightly different feature sets, non-projective edges were handled differently – Bohnet (2009) used the non-projective approximation algorithm, while Johansson and Nugues (2008) and Che et al. (2009) used pseudo-projective parsing. Handling non-projective edges is unarguably an important aspect of a parser, however, little is known about whether one of the methods mentioned above is better than the other. With a fixed feature set"
C12-2014,W06-1616,0,0.0160108,"between words through non-projective dependencies in a more straightforward way, without the use of traces or secondary edges. Informally, a dependency tree is said to be non-projective if it cannot be drawn without crossing edges. An example is shown in Figure 1. OBJ PRD ROOT SBJ root It OPRD NMOD is what federal SBJ support VC should MNR try hardest IM to achieve Figure 1: A non-projective sentence Although there are decoding algorithms for graph-based parsers that are able to output nonprojective trees directly (e.g. spanning tree algorithms (McDonald et al., 2005b) and ILP-based parsers (Riedel and Clarke, 2006, inter alia)), the chart-based algorithm of Eisner (1996), which is restricted to projective output, has shown very promising results in recent years. It typically outperforms the non-projective algorithms since it allows access to features involving pairs of edges. A notable extension to the chart-based parsing algorithm that is able to output non-projective dependencies while still including edge-pair features is the non-projective approximation algorithm of McDonald and Pereira (2006). Non-projective edges have also been handled by applying pre- and post-processing steps to the training an"
C12-2014,W08-2121,0,0.0190783,"eatures involving pairs of edges. A notable extension to the chart-based parsing algorithm that is able to output non-projective dependencies while still including edge-pair features is the non-projective approximation algorithm of McDonald and Pereira (2006). Non-projective edges have also been handled by applying pre- and post-processing steps to the training and test data, allowing for the use of any labeled projective parsing algorithm, only to recover the non-projective edges after parsing, e.g. pseudo-projective parsing (Nivre and Nilsson, 2005). In the CoNLL 2008 and 2009 Shared Tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009), some of the best systems used the chart-based parsing algorithm. Besides using slightly different feature sets, non-projective edges were handled differently – Bohnet (2009) used the non-projective approximation algorithm, while Johansson and Nugues (2008) and Che et al. (2009) used pseudo-projective parsing. Handling non-projective edges is unarguably an important aspect of a parser, however, little is known about whether one of the methods mentioned above is better than the other. With a fixed feature set, we compare pseudo-projective parsing with nonprojective approx"
C12-2015,W12-4503,1,0.893178,"nt building blocks one should consider – even from the theoretical perspective. The results from an end-to-end evaluation of real-life coreference systems using off-the-shelf phrase structure and dependency parsers will of course by no means allow us to differentiate between the theoretical paradigms; but we believe that a systematic comparison will help increase awareness of how different syntactic paradigms emphasize different syntactic properties in their core representations and how this may affect downstream processing tasks. 3 Coreference System We use our in-house coreference resolver (Björkelund and Farkas, 2012), which obtained the second best result in the CoNLL 2012 shared task. At the core, the system is similar to the pair-wise model proposed by Soon et al. (2001), which has become a de facto standard in coreference research during the last decade. However, the system features some extensions, including the use of multiple decoders that are combined through stacking. It also uses a rich feature set that includes both lexical information and syntax paths. The system is parametrized to allow for flexible experimentation with different feature sets. Since the system relies on a linear classifier, th"
C12-2015,C10-1011,0,0.0347253,"Missing"
C12-2015,de-marneffe-etal-2006-generating,0,0.0865919,"Missing"
C12-2015,N06-2015,0,0.0411759,"Missing"
C12-2015,H05-1004,0,0.200781,"cture-based features. Systems 4 and 5 allow us to measure if the combination of features from both syntactic paradigms improves the performance of the system. Finally, system 2 is a purely phrase-structure-based system with an already optimized feature set. This is the reference system, and it provides an upper bound for using the standard CoNLL annotation layers alone (i.e., not using any dependency-based features).5 Results. To evaluate the systems we use the official CoNLL scorer,6 which computes several metrics including MUC (Vilain et al., 1995), BCUB (Bagga and Baldwin, 1998), and CEAF (Luo, 2005). For completeness we also present end-to-end mention detection (MD) F-measure and the CoNLL average, i.e., the unweighted arithmetic mean of MUC, BCUB and the entity-based CEAF (CEAFE). To avoid clutter, and since precision and recall do not provide additional insights for the discussion at hand, we only present the F-measures of the corresponding metrics. The results of all systems on the CoNLL development set are presented in Table 1. Sys. 1 2 3 4 5 Feature set ❇▲ ❇▲✰P❙ ❇▲✰❉❚Choi ❇▲✰P❙✰❉❚Choi ❇▲✰P❙✰❉❚S t an f MD 73.64 74.96 74.54 75.23 75.23 MUC 65.64 67.12 66.74 67.69 67.46 BCUB 70.45 71.1"
C12-2015,P02-1014,0,0.0352244,"on a linear classifier, the parametrization also supports conjunctions between basic features. The system works in thee stages: First, mentions are extracted by a set of rules that work on a phrase structure tree and extract all pronouns and noun phrases. Additionally, a statistical classifier is applied to filter out non-referential instances of certain pronouns (such as expletive it). The second stage is a cluster-based coreference algorithm that relies on a pairwise classifier. This resolver gives relatively small, but consistent clusters. The third stage is a standard best-first resolver (Ng and Cardie, 2002) that, in addition to the features used by the previous resolver, also encodes the output of the previous resolver into its feature space. For a more detailed description we refer to (Björkelund and Farkas, 2012). The system relies on a phrase structure tree for two purposes: 1) For mention extraction; 2) As features for the pair-wise classifier. Since our systematic comparison focuses on the latter, we keep a phrase-structure-based mention extraction module fixed throughout the experiments. Syntax-based features. To provide the “building blocks” for picking up machine-learned variants of the"
C12-2015,W12-4501,0,0.0324236,"Missing"
C12-2015,W11-1901,0,0.0331436,"Missing"
C12-2015,S10-1001,0,0.0165933,"for the standard coreference task focusing on noun phrase (NP) and pronoun resolution. But also for the subsequent subtask, coreference resolution, syntactic information has proven useful in data-driven approaches – as one might expect from the rich linguistic work on Binding Theory, which targets the grammatical constraints on possible interpretations of referential phrases. It is this second subtask that we will parametrize systematically in this paper. Most coreference work has built on phrase structure syntax, although dependency syntax was, for instance, used in the SemEval 2010 Task 1 (Recasens et al., 2010). To our knowledge, effects of the two main alternatives have not been studied systematically. The choice typically seems to be driven by external factors (such as availability in shared task data). The fact that mention detection is so straightforward with phrase structure input also creates a practical bias affecting the full pipeline, but since both the phrase structure and the dependency parsing research paradigms are at mature stages, with parsers available for many languages, a more informed decision would be desirable. We here intend to shed some initial light on how the two different s"
C12-2015,J01-4004,0,0.336701,"shelf phrase structure and dependency parsers will of course by no means allow us to differentiate between the theoretical paradigms; but we believe that a systematic comparison will help increase awareness of how different syntactic paradigms emphasize different syntactic properties in their core representations and how this may affect downstream processing tasks. 3 Coreference System We use our in-house coreference resolver (Björkelund and Farkas, 2012), which obtained the second best result in the CoNLL 2012 shared task. At the core, the system is similar to the pair-wise model proposed by Soon et al. (2001), which has become a de facto standard in coreference research during the last decade. However, the system features some extensions, including the use of multiple decoders that are combined through stacking. It also uses a rich feature set that includes both lexical information and syntax paths. The system is parametrized to allow for flexible experimentation with different feature sets. Since the system relies on a linear classifier, the parametrization also supports conjunctions between basic features. The system works in thee stages: First, mentions are extracted by a set of rules that work"
C12-2015,M95-1005,0,0.105196,"es. Hence, this system will reveal the importance of phrase-structure-based features. Systems 4 and 5 allow us to measure if the combination of features from both syntactic paradigms improves the performance of the system. Finally, system 2 is a purely phrase-structure-based system with an already optimized feature set. This is the reference system, and it provides an upper bound for using the standard CoNLL annotation layers alone (i.e., not using any dependency-based features).5 Results. To evaluate the systems we use the official CoNLL scorer,6 which computes several metrics including MUC (Vilain et al., 1995), BCUB (Bagga and Baldwin, 1998), and CEAF (Luo, 2005). For completeness we also present end-to-end mention detection (MD) F-measure and the CoNLL average, i.e., the unweighted arithmetic mean of MUC, BCUB and the entity-based CEAF (CEAFE). To avoid clutter, and since precision and recall do not provide additional insights for the discussion at hand, we only present the F-measures of the corresponding metrics. The results of all systems on the CoNLL development set are presented in Table 1. Sys. 1 2 3 4 5 Feature set ❇▲ ❇▲✰P❙ ❇▲✰❉❚Choi ❇▲✰P❙✰❉❚Choi ❇▲✰P❙✰❉❚S t an f MD 73.64 74.96 74.54 75.23 7"
C12-2098,N12-1049,0,0.115077,"932: (move hamster, (toLoc oﬃce)) (play hamster) (neg (sleep sister, (loc bedroom))) GET’(+) GTOY’ their toy FromLOC(CLOSET)’ from the storage (-+) GET’(-) didn’t (+-) fail to GET’(+) get Figure 1: Ambiguous training examples from the extended corpus. The latent semantic analysis on the right is the representation we aim to learn from the observable context. makes the learning task much harder. The overall aim is to use the ambiguous contexts and event polarity to construct a latent semantic analysis (see Figure 1), that derives the appropriate relation and inference (for a similar idea, see (Angeli et al., 2012)). In other words, we want to learn, merely from ambiguous supervision, how to map novel sentences to their correct semantic representations (the typical goal in Semantic Parsing), while also making the correct inferences. Notice that the target analysis is a kind of syntactic analysis, keeping to the idea that such inferences are visible from the surface. 2.2 Method Many approaches to Semantic Parsing start by assigning rich structure to the target semantic representations, which can be used for finding alignments with latent structures in the language. Well known work by (Wong and Mooney, 20"
C12-2098,N12-1050,0,0.0178452,"the domain lexicon is properly inferred. Despite these issues, the results are encouraging and show that learning light inference can be done using standard Semantic Parsing techniques with loose ambiguous supervision. This result is not altogether surprising, given that the inference patterns we consider are types of syntactic patterns, and are therefore similar to the other patterns we induce. Future work will look at scaling this up to more complex types of inference in an open-domain. One particular direction might be looking at more complex forms of negation, as studied in, for example, (Blanco and Moldovan, 2012). Another direction is using these techniques, which require very little supervision, to help learn inference patterns for unresourced languages and domains. 3 Conclusions This work complements recent work on Semantic Parsing, specifically within the ambiguous learning paradigm, and shows how to integrate light syntactic inference into the learning using event polarity and context as loose supervision. The main focus has been on learning implicative verb constructions, which have well-understood semantic properties relating to speaker commitment. The strategy we adopted follows that of (Nairn"
C12-2098,D11-1131,0,0.23449,"nt interest in Semantic Parsing, centering on using data-driven techniques for mapping natural language to full semantic representations (Mooney, 2007). One particular focus has been on learning with ambiguous supervision (Chen and Mooney, 2008; Kim and Mooney, 2012), where the goal is to model language learning within broader perceptual contexts (Mooney, 2008). We look at learning light inference patterns for Semantic Parsing within this paradigm, focusing on detecting speaker commitments about events under discussion (Nairn et al., 2006; Karttunen, 2012). We adapt PCFG induction techniques (Börschinger et al., 2011; Johnson et al., 2012) for learning inference using event polarity and context as supervision, and demonstrate the effectiveness of our approach on a modified portion of the Grounded World corpus (Bordes et al., 2010). KEYWORDS: Semantic Parsing, Computational Semantics, Detecting Textual Entailment, Grammar Induction. Proceedings of COLING 2012: Posters, pages 1007–1018, COLING 2012, Mumbai, December 2012. 1007 1 Overview and Motivation Semantic Parsing is a subfield in NLP that looks at using data-driven techniques for mapping language expressions to complete semantic representations (Moone"
C12-2098,H05-1079,0,0.0287429,"atives can be productively stacked together as shown in Example 2. Determining the resulting inference for an arbitrary nesting of implicatives requires computing the relative polarity of each smaller phrase, which is the idea behind the polarity propagation algorithm (Nairn et al., 2006). This can be done directly from syntax by traversing a tree annotated with polarity information and calculating the polarity interactions incrementally. This general strategy for doing inference, which relies on syntactic and lexical features alone, avoids a full semantic analysis and translation into logic (Bos and Markert, 2005), and has been successfully applied to more general textual entailment tasks (MacCartney and Manning, 2007, 2008). One problem with the approach of (Nairn et al., 2006), however, is that the implicative signatures of verbs must be manually compiled, as there are no standard datasets available for doing learning. To our knowledge, there has been little work on learning these specific patterns (some related studies (Danescu-Niculescu-Mizil et al., 2009; Cheung and Penn, 2012)), which would be useful for applying these methods to languages and domains where resources are not available. Further, t"
C12-2098,E12-1071,0,0.0151743,"erence, which relies on syntactic and lexical features alone, avoids a full semantic analysis and translation into logic (Bos and Markert, 2005), and has been successfully applied to more general textual entailment tasks (MacCartney and Manning, 2007, 2008). One problem with the approach of (Nairn et al., 2006), however, is that the implicative signatures of verbs must be manually compiled, as there are no standard datasets available for doing learning. To our knowledge, there has been little work on learning these specific patterns (some related studies (Danescu-Niculescu-Mizil et al., 2009; Cheung and Penn, 2012)), which would be useful for applying these methods to languages and domains where resources are not available. Further, their algorithm encodes the lexical properties as hard facts, making it hard to model potential uncertainty and ambiguity associated with these inferences (e.g. if John was able to do X, how certain are we that he actually did X?) The semantics of implicative expressions can often be inferred from non-linguistic context. Knowing that managed to X implies X is something we can learn from hearing this utterance in contexts where X holds. Recent studies on learning from ambiguo"
C12-2098,W10-3001,0,0.0211716,"y these methods to tasks like Detecting Textual Entailment. In this work, we show how to learn light (syntactic) inference patterns for textual entailment using loosely-supervised Semantic Parsing methods. Detecting Textual Entailment is a topic that has received considerable attention in NLP, largely because of its connection to applications such as question answering, summarization, paraphrase generation, and many others. The goal, loosely speaking, is to detect entailment inference relationships between pairs of sentences (Dagan et al., 2005). More recent work on Hedge and Event Detection (Farkas et al., 2010) has focused on similar issues related to determining event certainty, especially in the biomedical domain (Example 3 (Thompson et al., 2011)). Four inferences are shown in Examples 1-4, and relate to implied speaker commitments (Karttunen, 2012; Nairn et al., 2006) about events under discussion. 1. John forgot to help Mary organize the meeting (a) |= John didn’t help Mary organize the meeting 2. John remembered (to not neglect) to turn off the lights before leaving work (a) |= John turned off some lights 3. NF-kappa B p50 alone fails to (=doesn’t) stimulate kappa B-directed transcription 4. T"
C12-2098,P12-1093,0,0.235566,"rsing, centering on using data-driven techniques for mapping natural language to full semantic representations (Mooney, 2007). One particular focus has been on learning with ambiguous supervision (Chen and Mooney, 2008; Kim and Mooney, 2012), where the goal is to model language learning within broader perceptual contexts (Mooney, 2008). We look at learning light inference patterns for Semantic Parsing within this paradigm, focusing on detecting speaker commitments about events under discussion (Nairn et al., 2006; Karttunen, 2012). We adapt PCFG induction techniques (Börschinger et al., 2011; Johnson et al., 2012) for learning inference using event polarity and context as supervision, and demonstrate the effectiveness of our approach on a modified portion of the Grounded World corpus (Bordes et al., 2010). KEYWORDS: Semantic Parsing, Computational Semantics, Detecting Textual Entailment, Grammar Induction. Proceedings of COLING 2012: Posters, pages 1007–1018, COLING 2012, Mumbai, December 2012. 1007 1 Overview and Motivation Semantic Parsing is a subfield in NLP that looks at using data-driven techniques for mapping language expressions to complete semantic representations (Mooney, 2007). A variety of"
C12-2098,P12-1051,0,0.0503107,"g data-driven techniques for mapping language expressions to complete semantic representations (Mooney, 2007). A variety of corpora and learning techniques have been developed for these purposes, both for doing supervised learning (Kate et al., 2005; Kwiatkowski et al., 2010) and learning in more complex (ambiguous) settings (Chen and Mooney, 2008, 2011). In many studies, the learning is done by finding alignments between (latent) syntactic patterns in language and parts of the target semantic representations, often using techniques from Statistical Machine Translation (Wong and Mooney, 2006; Jones et al., 2012). Despite achieving impressive results in different domains, learning semantic inference patterns is often not addressed, making it unclear how to apply these methods to tasks like Detecting Textual Entailment. In this work, we show how to learn light (syntactic) inference patterns for textual entailment using loosely-supervised Semantic Parsing methods. Detecting Textual Entailment is a topic that has received considerable attention in NLP, largely because of its connection to applications such as question answering, summarization, paraphrase generation, and many others. The goal, loosely spe"
C12-2098,S12-1020,0,0.260596,"uni-stuttgart.de ABSTRACT There has been a lot of recent interest in Semantic Parsing, centering on using data-driven techniques for mapping natural language to full semantic representations (Mooney, 2007). One particular focus has been on learning with ambiguous supervision (Chen and Mooney, 2008; Kim and Mooney, 2012), where the goal is to model language learning within broader perceptual contexts (Mooney, 2008). We look at learning light inference patterns for Semantic Parsing within this paradigm, focusing on detecting speaker commitments about events under discussion (Nairn et al., 2006; Karttunen, 2012). We adapt PCFG induction techniques (Börschinger et al., 2011; Johnson et al., 2012) for learning inference using event polarity and context as supervision, and demonstrate the effectiveness of our approach on a modified portion of the Grounded World corpus (Bordes et al., 2010). KEYWORDS: Semantic Parsing, Computational Semantics, Detecting Textual Entailment, Grammar Induction. Proceedings of COLING 2012: Posters, pages 1007–1018, COLING 2012, Mumbai, December 2012. 1007 1 Overview and Motivation Semantic Parsing is a subfield in NLP that looks at using data-driven techniques for mapping la"
C12-2098,D12-1040,0,0.119209,"nslation methods for finding alignments between semantic representations structured as trees and syntactic patterns in language. These alignments constitute the domain lexicon, and can be modeled using synchronous grammars. A number of such alignment-based learning methods have been proposed, using a variety of tools (Kate and Mooney, 2006; Jones et al., 2012; Wong and Mooney, 2006; Liang et al., 2011; Kwiatkowski et al., 2010). (Börschinger et al., 2011) recast the problem in terms of an unsupervised PCFG induction problem, an idea also explored in (Johnson et al., 2012; Angeli et al., 2012; Kim and Mooney, 2012). They develop a method for automatically generating PCFGs from semantic relations, by decomposing parts of the relations into rewrite rules. Formally, a semantic PCFG G is VN on , VTer m , C on, SR , R, P , where SR ∈ VN on is the set of start symbols corresponding to the full semantic representations in a corpus, C on ∈ VN on is the set of contexts, R is the set of productions X → β for X ∈ VN on , β ∈ V ∗ , and P is a probability function over R. A schema of the rules in R is shown at the top of Figure 2. Words in the training data (in VTer m ) are assigned to all pre-terminals (i.e. semant"
C12-2098,D10-1119,0,0.0891505,"modified portion of the Grounded World corpus (Bordes et al., 2010). KEYWORDS: Semantic Parsing, Computational Semantics, Detecting Textual Entailment, Grammar Induction. Proceedings of COLING 2012: Posters, pages 1007–1018, COLING 2012, Mumbai, December 2012. 1007 1 Overview and Motivation Semantic Parsing is a subfield in NLP that looks at using data-driven techniques for mapping language expressions to complete semantic representations (Mooney, 2007). A variety of corpora and learning techniques have been developed for these purposes, both for doing supervised learning (Kate et al., 2005; Kwiatkowski et al., 2010) and learning in more complex (ambiguous) settings (Chen and Mooney, 2008, 2011). In many studies, the learning is done by finding alignments between (latent) syntactic patterns in language and parts of the target semantic representations, often using techniques from Statistical Machine Translation (Wong and Mooney, 2006; Jones et al., 2012). Despite achieving impressive results in different domains, learning semantic inference patterns is often not addressed, making it unclear how to apply these methods to tasks like Detecting Textual Entailment. In this work, we show how to learn light (synt"
C12-2098,P11-1060,0,0.0318252,"igning rich structure to the target semantic representations, which can be used for finding alignments with latent structures in the language. Well known work by (Wong and Mooney, 2006) uses Statistical Machine Translation methods for finding alignments between semantic representations structured as trees and syntactic patterns in language. These alignments constitute the domain lexicon, and can be modeled using synchronous grammars. A number of such alignment-based learning methods have been proposed, using a variety of tools (Kate and Mooney, 2006; Jones et al., 2012; Wong and Mooney, 2006; Liang et al., 2011; Kwiatkowski et al., 2010). (Börschinger et al., 2011) recast the problem in terms of an unsupervised PCFG induction problem, an idea also explored in (Johnson et al., 2012; Angeli et al., 2012; Kim and Mooney, 2012). They develop a method for automatically generating PCFGs from semantic relations, by decomposing parts of the relations into rewrite rules. Formally, a semantic PCFG G is VN on , VTer m , C on, SR , R, P , where SR ∈ VN on is the set of start symbols corresponding to the full semantic representations in a corpus, C on ∈ VN on is the set of contexts, R is the set of productions X"
C12-2098,C08-1066,0,0.0520724,"Missing"
C12-2098,W07-1431,0,0.182445,"e is true in Example 2. This is triggered by the implicative phrases Forget to X and Remember to X, which affect the polarity of the modified event X. These inferences relate to the semantics of English complement constructions, a topic well studied in Linguistics (Karttunen, 1971; Kiparsky and Kiparsky, 1970). They are also part of a wider range of inference patterns that are syntactic in nature, or visible from language surface form (Dowty, 1994). They have been of interest to studies in proof-theoretic semantics and Natural Logic, which look at doing inference on natural language directly (MacCartney and Manning, 2007; Moss, 2010; Valencia, 1991). We aim to learn these implicative patterns, building on existing computational work. (Nairn et al., 2006; Karttunen, 2012) provide a classification of implicative verbs according to the effect they have on their surrounding context. They observe that implicative constructions differ in terms of the polarity contexts they occur in, and the effect they have in these contexts. 1008 As illustrated in Table 1, one-way implicatives occur in a single polarity, whereas two-way implicatives occur in both. For example, Forget to X in Example 1 switches polarity in a positi"
C12-2098,W06-3907,0,0.271767,"rt {kyle,jonas}@ims.uni-stuttgart.de ABSTRACT There has been a lot of recent interest in Semantic Parsing, centering on using data-driven techniques for mapping natural language to full semantic representations (Mooney, 2007). One particular focus has been on learning with ambiguous supervision (Chen and Mooney, 2008; Kim and Mooney, 2012), where the goal is to model language learning within broader perceptual contexts (Mooney, 2008). We look at learning light inference patterns for Semantic Parsing within this paradigm, focusing on detecting speaker commitments about events under discussion (Nairn et al., 2006; Karttunen, 2012). We adapt PCFG induction techniques (Börschinger et al., 2011; Johnson et al., 2012) for learning inference using event polarity and context as supervision, and demonstrate the effectiveness of our approach on a modified portion of the Grounded World corpus (Bordes et al., 2010). KEYWORDS: Semantic Parsing, Computational Semantics, Detecting Textual Entailment, Grammar Induction. Proceedings of COLING 2012: Posters, pages 1007–1018, COLING 2012, Mumbai, December 2012. 1007 1 Overview and Motivation Semantic Parsing is a subfield in NLP that looks at using data-driven techniq"
C12-2098,N06-1056,0,0.14073,"NLP that looks at using data-driven techniques for mapping language expressions to complete semantic representations (Mooney, 2007). A variety of corpora and learning techniques have been developed for these purposes, both for doing supervised learning (Kate et al., 2005; Kwiatkowski et al., 2010) and learning in more complex (ambiguous) settings (Chen and Mooney, 2008, 2011). In many studies, the learning is done by finding alignments between (latent) syntactic patterns in language and parts of the target semantic representations, often using techniques from Statistical Machine Translation (Wong and Mooney, 2006; Jones et al., 2012). Despite achieving impressive results in different domains, learning semantic inference patterns is often not addressed, making it unclear how to apply these methods to tasks like Detecting Textual Entailment. In this work, we show how to learn light (syntactic) inference patterns for textual entailment using loosely-supervised Semantic Parsing methods. Detecting Textual Entailment is a topic that has received considerable attention in NLP, largely because of its connection to applications such as question answering, summarization, paraphrase generation, and many others."
C12-2105,P04-1082,0,0.203759,"rve the parallelism. The German example on the bottom shows a coordination of two sentences that share the finite and the passive auxiliary with each other, both represented as a phonetically empty head in the structure. By introducing empty nodes into the annotation, the parallelism in the underlying syntactic structure of the two conjuncts is preserved. We would like to stress that the problem of empty heads in dependency syntax is rather different from the problem of introducing trace elements previously addressed by work on the English Penn Treebank (Johnson, 2002; Dienes and Dubey, 2003; Campbell, 2004). The PTB encodes a lot of different elements that do not show on the surface, but most of these would be leaf nodes in dependency representation.1 1 There is a small number of cases, where the PTB annotates a missing verb (marked as *?*, see Section 4.6 in Bies et al. (1995)). We found 581 instances of those in the whole corpus, 293 of which were dominated by a VP node. Only those empty elements correspond to empty heads in a dependency representation since they would normally have dependents on their own. But in contrast to dependency formalisms, it is not a problem to annotate a head-less p"
C12-2105,W11-0417,0,0.208415,"ian. We conclude with an error analysis and a discussion of the results. 2 Related Work We are aware of two previous papers where the issue of empty heads has been addressed in the context of dependency parsing: One is Dukes and Habash (2011) who present a parser for the Quranic Arabic Dependency Treebank, which also contains empty heads. Unfortunately, they do not evaluate or discuss the role of empty heads. Their solution of the problem – introducing a new transition into a transition-based parser – is similar to one of our proposed procedures (the in-parsing approach). The other work is by Chaitanya et al. (2011), who use hand-crafted rules to recover empty nodes from the output of a rule-based dependency parser for the Hindi Dependency Treebank. They achieve good results on some phenomena and a bit lower results on others, proving that it is indeed possible to treat this problem in syntactic processing. However, given that their data base is very small, and they used a rule-based, language-specific approach, the question remains if we can use statistical learning to address this problem. 3 Approaches for Parsing with Empty Heads The parser that we use for our experiment is basically a best-first pars"
C12-2105,P03-1055,0,0.710356,"ead can be used to preserve the parallelism. The German example on the bottom shows a coordination of two sentences that share the finite and the passive auxiliary with each other, both represented as a phonetically empty head in the structure. By introducing empty nodes into the annotation, the parallelism in the underlying syntactic structure of the two conjuncts is preserved. We would like to stress that the problem of empty heads in dependency syntax is rather different from the problem of introducing trace elements previously addressed by work on the English Penn Treebank (Johnson, 2002; Dienes and Dubey, 2003; Campbell, 2004). The PTB encodes a lot of different elements that do not show on the surface, but most of these would be leaf nodes in dependency representation.1 1 There is a small number of cases, where the PTB annotates a missing verb (marked as *?*, see Section 4.6 in Bies et al. (1995)). We found 581 instances of those in the whole corpus, 293 of which were dominated by a VP node. Only those empty elements correspond to empty heads in a dependency representation since they would normally have dependents on their own. But in contrast to dependency formalisms, it is not a problem to annot"
C12-2105,W11-2912,0,0.42051,"oach where the presence of empty heads is determined by a classifier run prior to parsing. The paper is structured as follows: we first review some related work and continue with the presentation of the three different methods. We then define the metric that we use to measure the quality of the empty head prediction and use it to evaluate parsing experiments on German and Hungarian. We conclude with an error analysis and a discussion of the results. 2 Related Work We are aware of two previous papers where the issue of empty heads has been addressed in the context of dependency parsing: One is Dukes and Habash (2011) who present a parser for the Quranic Arabic Dependency Treebank, which also contains empty heads. Unfortunately, they do not evaluate or discuss the role of empty heads. Their solution of the problem – introducing a new transition into a transition-based parser – is similar to one of our proposed procedures (the in-parsing approach). The other work is by Chaitanya et al. (2011), who use hand-crafted rules to recover empty nodes from the output of a rule-based dependency parser for the Hindi Dependency Treebank. They achieve good results on some phenomena and a bit lower results on others, pro"
C12-2105,N10-1115,0,0.0348517,"recover empty nodes from the output of a rule-based dependency parser for the Hindi Dependency Treebank. They achieve good results on some phenomena and a bit lower results on others, proving that it is indeed possible to treat this problem in syntactic processing. However, given that their data base is very small, and they used a rule-based, language-specific approach, the question remains if we can use statistical learning to address this problem. 3 Approaches for Parsing with Empty Heads The parser that we use for our experiment is basically a best-first parser like the ones described in (Goldberg and Elhadad, 2010; Tratz and Hovy, 2011), which is trained with the Guided Learning technique proposed in Shen et al. (2007) embedded in a MIRA framework (Crammer et al., 2003). The best-first parsing approach has the advantage that it is easy to modify in order to allow for the introduction of empty heads, while for graph-based parsers (McDonald et al., 2005) it is not even clear how to do it. The approach is also more suitable here than the standard transition-based approach (Nivre et al., 2004), since it can build context on both sides of the current attachment site while it achieves competitive results. In"
C12-2105,P02-1018,0,0.449938,"ces, an empty head can be used to preserve the parallelism. The German example on the bottom shows a coordination of two sentences that share the finite and the passive auxiliary with each other, both represented as a phonetically empty head in the structure. By introducing empty nodes into the annotation, the parallelism in the underlying syntactic structure of the two conjuncts is preserved. We would like to stress that the problem of empty heads in dependency syntax is rather different from the problem of introducing trace elements previously addressed by work on the English Penn Treebank (Johnson, 2002; Dienes and Dubey, 2003; Campbell, 2004). The PTB encodes a lot of different elements that do not show on the surface, but most of these would be leaf nodes in dependency representation.1 1 There is a small number of cases, where the PTB annotates a missing verb (marked as *?*, see Section 4.6 in Bies et al. (1995)). We found 581 instances of those in the whole corpus, 293 of which were dominated by a VP node. Only those empty elements correspond to empty heads in a dependency representation since they would normally have dependents on their own. But in contrast to dependency formalisms, it i"
C12-2105,P06-2066,0,0.0283657,"resort to a swap operation as is done in Tratz and Hovy (2011). It also increases theoretical decoding complexity to O(n2 ). However, there are non-projective structures that cannot be produced by this approach.2 To allow the derivation of these structures, we reintroduce the swap operation from the parser in Tratz and Hovy (2011), but during training, the parser is only allowed to apply the swap operation in case of an ill-nested structure, which leads to a very small number of swaps. 2 These structures do not fulfill the well-nestedness condition that is described in Bodirsky et al. (2005); Kuhlmann and Nivre (2006) and appear for example in German centerfield scrambling structures. 1083 The feature set of the parser uses the word forms, lemmata, POS tags, and already predicted dependency labels for the head and its prospective dependent, as well as combinations thereof for up to three surrounding tokens in the sentence. The same features and combinations are extracted for up to three surrounding partially built structures. We also add features for the left-most and right-most dependent of a token, the labels of the dependents, distance features, and valency features as proposed by Zhang and Nivre (2011)"
C12-2105,P05-1012,0,0.0104219,"guage-specific approach, the question remains if we can use statistical learning to address this problem. 3 Approaches for Parsing with Empty Heads The parser that we use for our experiment is basically a best-first parser like the ones described in (Goldberg and Elhadad, 2010; Tratz and Hovy, 2011), which is trained with the Guided Learning technique proposed in Shen et al. (2007) embedded in a MIRA framework (Crammer et al., 2003). The best-first parsing approach has the advantage that it is easy to modify in order to allow for the introduction of empty heads, while for graph-based parsers (McDonald et al., 2005) it is not even clear how to do it. The approach is also more suitable here than the standard transition-based approach (Nivre et al., 2004), since it can build context on both sides of the current attachment site while it achieves competitive results. In contrast to the best-first parser in Goldberg and Elhadad (2010), the decoding algorithm is modified so that it works like the LTAG dependency parser described in Shen and Joshi (2008), which allows an edge to attach to an inside node of an already built structure. This difference makes it possible to directly produce a large portion of non-p"
C12-2105,W04-2407,0,0.0203665,"eads The parser that we use for our experiment is basically a best-first parser like the ones described in (Goldberg and Elhadad, 2010; Tratz and Hovy, 2011), which is trained with the Guided Learning technique proposed in Shen et al. (2007) embedded in a MIRA framework (Crammer et al., 2003). The best-first parsing approach has the advantage that it is easy to modify in order to allow for the introduction of empty heads, while for graph-based parsers (McDonald et al., 2005) it is not even clear how to do it. The approach is also more suitable here than the standard transition-based approach (Nivre et al., 2004), since it can build context on both sides of the current attachment site while it achieves competitive results. In contrast to the best-first parser in Goldberg and Elhadad (2010), the decoding algorithm is modified so that it works like the LTAG dependency parser described in Shen and Joshi (2008), which allows an edge to attach to an inside node of an already built structure. This difference makes it possible to directly produce a large portion of non-projective structures (e. g. sentence extraposition or WH-extraction) without having to resort to a swap operation as is done in Tratz and Ho"
C12-2105,W01-0708,0,0.0254884,"before to-infinitive constructions) whereas our empty heads can occur more freely due to the free word order of German and Hungarian. We therefore pursue here a clause-based empty head preinsertion procedure since we think that the decision about inserting an empty head (which is basically the identification of the absence of the verb) can be made on the clause-level. For this, we implemented a clause boundary identification module and a classifier that predicts whether an empty word form should be inserted into a particular clause. Clause boundary identification is a difficult problem (cf. (Sang and Déjean, 2001)) as clauses usually form a hierarchy – and this hierarchy is important for predicting the insertion of empty heads. Our clause boundary detector achieves f-scores of 92.6 and 86.8 on the German and Hungarian development datasets respectively. These results are in line with the state-of-the-art results on the English Penn Treebank (Carreras et al., 2005; Ram and Lalitha Devi, 2008). If we evaluate only the in-sentence clauses, we get f-scores of 85.4 and 78.2 for German and Hungarian respectively. In order to decide whether to insert an empty head, we implemented a classifier that decides for"
C12-2105,seeker-kuhn-2012-making,1,0.832743,"e of the verb in the verb-second word order of German). For Hungarian, the manual annotation of the position of the empty word forms is quite irregular and we insert them at the beginning of the clause. Finally, we train the best-first parser on the original training dataset containing the gold standard empty heads and use it to parse the sentences that contain the automatically inserted empty heads from the preinserter. 4 Experiments In order to test the parsing methods, we performed two experiments each: we trained the parser on the German TiGer corpus using the dependency representation by Seeker and Kuhn (2012) , and on the Szeged Dependency Treebank of Hungarian (Vincze et al., 2010), both of which data sets explicitly represent empty heads in the dependency trees. Table 1 shows the data sizes and the splits we used for the experiment. The German data was preprocessed (lemma, POS, morphology) with the mate-tools,4 the Hungarian data comes with automatic annotation.5 data set German Hungarian # sentences 50,474 81,960 training # sents # empty 36,000 2,618 61,034 14,850 development # sents # empty 2,000 117 11,688 2,536 # sents 10,472 9,238 test # empty 722 2,106 Table 1: Data sets 4.1 Evaluation Met"
C12-2105,D08-1052,0,0.0558341,"best-first parsing approach has the advantage that it is easy to modify in order to allow for the introduction of empty heads, while for graph-based parsers (McDonald et al., 2005) it is not even clear how to do it. The approach is also more suitable here than the standard transition-based approach (Nivre et al., 2004), since it can build context on both sides of the current attachment site while it achieves competitive results. In contrast to the best-first parser in Goldberg and Elhadad (2010), the decoding algorithm is modified so that it works like the LTAG dependency parser described in Shen and Joshi (2008), which allows an edge to attach to an inside node of an already built structure. This difference makes it possible to directly produce a large portion of non-projective structures (e. g. sentence extraposition or WH-extraction) without having to resort to a swap operation as is done in Tratz and Hovy (2011). It also increases theoretical decoding complexity to O(n2 ). However, there are non-projective structures that cannot be produced by this approach.2 To allow the derivation of these structures, we reintroduce the swap operation from the parser in Tratz and Hovy (2011), but during training"
C12-2105,P07-1096,0,0.0170372,"eve good results on some phenomena and a bit lower results on others, proving that it is indeed possible to treat this problem in syntactic processing. However, given that their data base is very small, and they used a rule-based, language-specific approach, the question remains if we can use statistical learning to address this problem. 3 Approaches for Parsing with Empty Heads The parser that we use for our experiment is basically a best-first parser like the ones described in (Goldberg and Elhadad, 2010; Tratz and Hovy, 2011), which is trained with the Guided Learning technique proposed in Shen et al. (2007) embedded in a MIRA framework (Crammer et al., 2003). The best-first parsing approach has the advantage that it is easy to modify in order to allow for the introduction of empty heads, while for graph-based parsers (McDonald et al., 2005) it is not even clear how to do it. The approach is also more suitable here than the standard transition-based approach (Nivre et al., 2004), since it can build context on both sides of the current attachment site while it achieves competitive results. In contrast to the best-first parser in Goldberg and Elhadad (2010), the decoding algorithm is modified so th"
C12-2105,D11-1116,0,0.134076,"e output of a rule-based dependency parser for the Hindi Dependency Treebank. They achieve good results on some phenomena and a bit lower results on others, proving that it is indeed possible to treat this problem in syntactic processing. However, given that their data base is very small, and they used a rule-based, language-specific approach, the question remains if we can use statistical learning to address this problem. 3 Approaches for Parsing with Empty Heads The parser that we use for our experiment is basically a best-first parser like the ones described in (Goldberg and Elhadad, 2010; Tratz and Hovy, 2011), which is trained with the Guided Learning technique proposed in Shen et al. (2007) embedded in a MIRA framework (Crammer et al., 2003). The best-first parsing approach has the advantage that it is easy to modify in order to allow for the introduction of empty heads, while for graph-based parsers (McDonald et al., 2005) it is not even clear how to do it. The approach is also more suitable here than the standard transition-based approach (Nivre et al., 2004), since it can build context on both sides of the current attachment site while it achieves competitive results. In contrast to the best-f"
C12-2105,vincze-etal-2010-hungarian,0,0.105132,"nual annotation of the position of the empty word forms is quite irregular and we insert them at the beginning of the clause. Finally, we train the best-first parser on the original training dataset containing the gold standard empty heads and use it to parse the sentences that contain the automatically inserted empty heads from the preinserter. 4 Experiments In order to test the parsing methods, we performed two experiments each: we trained the parser on the German TiGer corpus using the dependency representation by Seeker and Kuhn (2012) , and on the Szeged Dependency Treebank of Hungarian (Vincze et al., 2010), both of which data sets explicitly represent empty heads in the dependency trees. Table 1 shows the data sizes and the splits we used for the experiment. The German data was preprocessed (lemma, POS, morphology) with the mate-tools,4 the Hungarian data comes with automatic annotation.5 data set German Hungarian # sentences 50,474 81,960 training # sents # empty 36,000 2,618 61,034 14,850 development # sents # empty 2,000 117 11,688 2,536 # sents 10,472 9,238 test # empty 722 2,106 Table 1: Data sets 4.1 Evaluation Method Since the number of edges in the gold standard does not always equal th"
C12-2105,P11-2033,0,0.0210148,"lmann and Nivre (2006) and appear for example in German centerfield scrambling structures. 1083 The feature set of the parser uses the word forms, lemmata, POS tags, and already predicted dependency labels for the head and its prospective dependent, as well as combinations thereof for up to three surrounding tokens in the sentence. The same features and combinations are extracted for up to three surrounding partially built structures. We also add features for the left-most and right-most dependent of a token, the labels of the dependents, distance features, and valency features as proposed by Zhang and Nivre (2011) but adapted to the best-first decoder. For internal feature representation and combination the parser implements the hash kernel method by Bohnet (2010). 3.1 Empty Head Introduction during Parsing For the first method, we change the parser so that it can decide for an empty head during the parsing itself. To the three moves that the standard parser can perform – attach_left(label), attach_right(label), and swap – we add a fourth move (see Figure 2), that allows the parser to introduce an empty head for a particular dependent (together with a dependency label). This is similar in spirit to the"
C16-1140,P13-1035,0,0.0626801,"Missing"
C16-1140,P14-1035,0,0.016233,"them available in Wikipedia. Li et al. (2013) use information from Wikipedia and an external source (websites referring to entities in Wikipedia obtained by crawling the web). They conduct experiments on two datasets, TAC-KBP 2009 and twitter data about 25 ambiguous and randomly picked entities, however they filter this data to only include entities that occur in Wikipedia, because their approach is also limited to entities in Wikipedia. Bamman et al. (2013) extend a topic model to learn character types (e.g., {dark, major, henchman} or {shoot, aim, overpower}) in movies. In subsequent work, Bamman et al. (2014) apply an extended topic model to learn character types in English novels of the 18th and 19th century. They do not identify and link the individual names to their real world entities. Topic models have also been used in named entity recognition (NER). Guo et al. (2009) use LDA for NER in query. Ritter et al. (2011) extend LDA and take information from Freebase for NER on twitter data. They have a similar problem with ambiguous expressions which they need to solve to determine the correct class (e.g., China can belong to several classes such as LOCATION or PERSON). However, they do not identif"
C16-1140,W06-0803,0,0.0368402,"as determining for each textual mention of a proper name, which of (typically) several entries in a knowledge base (such as DBpedia or Wikipedia) representing unique referents is the correct one in the given context. The standard approach to this task is to view it as a supervised classification problem, i.e., training data of textual mentions labeled with the correct disambiguation target are used to induce knowledge about indicative contextual clues for each candidate. A considerable amount of research has gone into the development of effective models (Mann and Yarowsky, 2003; Malin, 2005; Bollegala et al., 2006; Chen and Martin, 2007), and particularly into weakly supervised or distant supervision techniques, i.e., finding ways of exploiting explicit or implicit indications for the correct name references in real-life data (Cohen, 2005; Bunescu and Pasca, 2006; Cucerzan, 2007; Mihalcea and Csomai, 2007; Han et al., 2011; Hoffart et al., 2011). Indeed, for medium-to-high frequency name/referent combinations, it is not hard to harvest the web for suitable training material. The contribution that we present in this paper is motivated by a type of name/referent pairs that falls outside of the standard t"
C16-1140,E06-1002,0,0.0425744,"is to view it as a supervised classification problem, i.e., training data of textual mentions labeled with the correct disambiguation target are used to induce knowledge about indicative contextual clues for each candidate. A considerable amount of research has gone into the development of effective models (Mann and Yarowsky, 2003; Malin, 2005; Bollegala et al., 2006; Chen and Martin, 2007), and particularly into weakly supervised or distant supervision techniques, i.e., finding ways of exploiting explicit or implicit indications for the correct name references in real-life data (Cohen, 2005; Bunescu and Pasca, 2006; Cucerzan, 2007; Mihalcea and Csomai, 2007; Han et al., 2011; Hoffart et al., 2011). Indeed, for medium-to-high frequency name/referent combinations, it is not hard to harvest the web for suitable training material. The contribution that we present in this paper is motivated by a type of name/referent pairs that falls outside of the standard training scenario and has so far received little attention from the research comThis work is licenced under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 1481 Proceedings of COLING"
C16-1140,D07-1020,0,0.0335363,"textual mention of a proper name, which of (typically) several entries in a knowledge base (such as DBpedia or Wikipedia) representing unique referents is the correct one in the given context. The standard approach to this task is to view it as a supervised classification problem, i.e., training data of textual mentions labeled with the correct disambiguation target are used to induce knowledge about indicative contextual clues for each candidate. A considerable amount of research has gone into the development of effective models (Mann and Yarowsky, 2003; Malin, 2005; Bollegala et al., 2006; Chen and Martin, 2007), and particularly into weakly supervised or distant supervision techniques, i.e., finding ways of exploiting explicit or implicit indications for the correct name references in real-life data (Cohen, 2005; Bunescu and Pasca, 2006; Cucerzan, 2007; Mihalcea and Csomai, 2007; Han et al., 2011; Hoffart et al., 2011). Indeed, for medium-to-high frequency name/referent combinations, it is not hard to harvest the web for suitable training material. The contribution that we present in this paper is motivated by a type of name/referent pairs that falls outside of the standard training scenario and has"
C16-1140,W05-1303,0,0.0463273,"to this task is to view it as a supervised classification problem, i.e., training data of textual mentions labeled with the correct disambiguation target are used to induce knowledge about indicative contextual clues for each candidate. A considerable amount of research has gone into the development of effective models (Mann and Yarowsky, 2003; Malin, 2005; Bollegala et al., 2006; Chen and Martin, 2007), and particularly into weakly supervised or distant supervision techniques, i.e., finding ways of exploiting explicit or implicit indications for the correct name references in real-life data (Cohen, 2005; Bunescu and Pasca, 2006; Cucerzan, 2007; Mihalcea and Csomai, 2007; Han et al., 2011; Hoffart et al., 2011). Indeed, for medium-to-high frequency name/referent combinations, it is not hard to harvest the web for suitable training material. The contribution that we present in this paper is motivated by a type of name/referent pairs that falls outside of the standard training scenario and has so far received little attention from the research comThis work is licenced under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 14"
C16-1140,D07-1074,0,0.0646905,"ised classification problem, i.e., training data of textual mentions labeled with the correct disambiguation target are used to induce knowledge about indicative contextual clues for each candidate. A considerable amount of research has gone into the development of effective models (Mann and Yarowsky, 2003; Malin, 2005; Bollegala et al., 2006; Chen and Martin, 2007), and particularly into weakly supervised or distant supervision techniques, i.e., finding ways of exploiting explicit or implicit indications for the correct name references in real-life data (Cohen, 2005; Bunescu and Pasca, 2006; Cucerzan, 2007; Mihalcea and Csomai, 2007; Han et al., 2011; Hoffart et al., 2011). Indeed, for medium-to-high frequency name/referent combinations, it is not hard to harvest the web for suitable training material. The contribution that we present in this paper is motivated by a type of name/referent pairs that falls outside of the standard training scenario and has so far received little attention from the research comThis work is licenced under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 1481 Proceedings of COLING 2016, the 26th I"
C16-1140,hahm-etal-2014-named,0,0.0485454,"Missing"
C16-1140,D12-1010,0,0.0412379,"Missing"
C16-1140,D11-1072,0,0.0989437,"Missing"
C16-1140,W03-0405,0,0.0946814,"Csomai, 2007) – is typically construed as determining for each textual mention of a proper name, which of (typically) several entries in a knowledge base (such as DBpedia or Wikipedia) representing unique referents is the correct one in the given context. The standard approach to this task is to view it as a supervised classification problem, i.e., training data of textual mentions labeled with the correct disambiguation target are used to induce knowledge about indicative contextual clues for each candidate. A considerable amount of research has gone into the development of effective models (Mann and Yarowsky, 2003; Malin, 2005; Bollegala et al., 2006; Chen and Martin, 2007), and particularly into weakly supervised or distant supervision techniques, i.e., finding ways of exploiting explicit or implicit indications for the correct name references in real-life data (Cohen, 2005; Bunescu and Pasca, 2006; Cucerzan, 2007; Mihalcea and Csomai, 2007; Han et al., 2011; Hoffart et al., 2011). Indeed, for medium-to-high frequency name/referent combinations, it is not hard to harvest the web for suitable training material. The contribution that we present in this paper is motivated by a type of name/referent pairs"
C16-1140,U08-1016,0,0.0329746,"son. In other cases the number of snippets is more evenly distributed over all entities with the same name. The last column in Table 3 (Avg) lists the average numbers of snippets that were extracted for each entity. We use the extracted link information as gold labels to disambiguate the person. For example, if we find the link Michael Jackson (English singer) in an article, we know that the extracted name and context around it refers to Michael Jackson, the English singer. By using this link information we do not have to annotate a dataset manually. Similar datasets have been created before (Nothman et al., 2008; Nothman et al., 2013; Hahm et al., 2014). 5 Experiments and Discussion For obtaining topic information we use the MALLET toolkit (McCallum, 2002) which contains several machine learning applications, for example, document classification, clustering, and information extraction. For topic modeling it provides implementations of Latent Dirichlet Allocation (LDA), Pachinko Allocation, and Hierarchical LDA. We use the ParallelTopicModel class which is a simple parallel threaded implementation of LDA based on Newman et al. (2009) and Yao et al. (2009). We trained different topic models using the c"
C16-1140,D11-1141,0,0.022232,"this data to only include entities that occur in Wikipedia, because their approach is also limited to entities in Wikipedia. Bamman et al. (2013) extend a topic model to learn character types (e.g., {dark, major, henchman} or {shoot, aim, overpower}) in movies. In subsequent work, Bamman et al. (2014) apply an extended topic model to learn character types in English novels of the 18th and 19th century. They do not identify and link the individual names to their real world entities. Topic models have also been used in named entity recognition (NER). Guo et al. (2009) use LDA for NER in query. Ritter et al. (2011) extend LDA and take information from Freebase for NER on twitter data. They have a similar problem with ambiguous expressions which they need to solve to determine the correct class (e.g., China can belong to several classes such as LOCATION or PERSON). However, they do not identify the actual real world entity of the expression (e.g., there are several cities called China in the US and other countries, but they all belong to the class LOCATION). 3 Approach Our system does not rely on having textual training data for a specific person (e.g., the Canadian law professor Michael Jackson). Instea"
C18-1298,P14-1005,1,0.839074,"Missing"
C18-1298,W12-1632,1,0.784093,"cooccurring – phenomena being defined as bridging, namely referential and lexical bridging, which is why we have included a rather extensive review of bridging definitions in the next section. 2 Defining bridging Bridging has been examined in many theoretical studies (Clark, 1975; Hawkins, 1978; Hobbs et al., 1993; Asher and Lascarides, 1998; Baumann and Riester, 2012) as well as in corpus and computational studies (Fraurud, 1990; Poesio et al., 1997; Vieira and Teufel, 1997; Poesio and Vieira, 1998; Poesio et al., 2004; Nissim et al., 2004; Nedoluzhko et al., 2009; Lassalle and Denis, 2011; Cahill and Riester, 2012; Markert et al., 2012; Hou et al., 2013b; Hou et al., 2013a; Hou, 2016b; Zik´anov´a et al., 2015; Grishina, 2016; Roitberg and Nedoluzhko, 2016; Riester and Baumann, 2017). Unlike in work on coreference resolution, these studies do not follow an agreed upon definition of bridging. On the contrary, many different phenomena have been described as bridging. While some of the issues have been controversial for a long time, e.g. the question of definiteness, the importance of the distinction between referential and lexical bridging, inspired by the two-level RefLex annotation scheme by Baumann and"
C18-1298,W03-2410,0,0.47367,"Missing"
C18-1298,W16-0702,0,0.541236,"a rather extensive review of bridging definitions in the next section. 2 Defining bridging Bridging has been examined in many theoretical studies (Clark, 1975; Hawkins, 1978; Hobbs et al., 1993; Asher and Lascarides, 1998; Baumann and Riester, 2012) as well as in corpus and computational studies (Fraurud, 1990; Poesio et al., 1997; Vieira and Teufel, 1997; Poesio and Vieira, 1998; Poesio et al., 2004; Nissim et al., 2004; Nedoluzhko et al., 2009; Lassalle and Denis, 2011; Cahill and Riester, 2012; Markert et al., 2012; Hou et al., 2013b; Hou et al., 2013a; Hou, 2016b; Zik´anov´a et al., 2015; Grishina, 2016; Roitberg and Nedoluzhko, 2016; Riester and Baumann, 2017). Unlike in work on coreference resolution, these studies do not follow an agreed upon definition of bridging. On the contrary, many different phenomena have been described as bridging. While some of the issues have been controversial for a long time, e.g. the question of definiteness, the importance of the distinction between referential and lexical bridging, inspired by the two-level RefLex annotation scheme by Baumann and Riester (2012), became evident in our experiments. The two terms describe two different phenomena which are curr"
C18-1298,P94-1002,0,0.0379888,"Missing"
C18-1298,D13-1077,0,0.809571,"ons and annotations. The resolution of bridging links is important because it can prove beneficial in tasks which use the concept of textual coherence, for example Barzilay and Lapata’s (2008) entity grid or Hearst’s (1994) text segmentation. Note that while a benchmark dataset for bridging has not yet been established, most recent work is based on the ISNotes corpus (Markert et al., 2012), which contains Wall Street Journal articles. Full bridging resolution on this corpus has been investigated in Hou et al. (2014), following earlier experiments on the subtasks of bridging anaphor detection (Hou et al., 2013a) and antecedent selection (Hou et al., 2013b). Apart from this, there is some work on bridging detection as a subclass of information status classification, where bridging is typically a category with low annotator agreement and low detection accuracy (Markert et al., 2012; Rahman and Ng, 2012; Hou, 2016a). In the meantime, a few other corpora This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/. 1 Anaphors are marked in bold face, their antecedents are underlined. 3516 Proceedings of the 27th Inter"
C18-1298,N13-1111,0,0.556415,"ons and annotations. The resolution of bridging links is important because it can prove beneficial in tasks which use the concept of textual coherence, for example Barzilay and Lapata’s (2008) entity grid or Hearst’s (1994) text segmentation. Note that while a benchmark dataset for bridging has not yet been established, most recent work is based on the ISNotes corpus (Markert et al., 2012), which contains Wall Street Journal articles. Full bridging resolution on this corpus has been investigated in Hou et al. (2014), following earlier experiments on the subtasks of bridging anaphor detection (Hou et al., 2013a) and antecedent selection (Hou et al., 2013b). Apart from this, there is some work on bridging detection as a subclass of information status classification, where bridging is typically a category with low annotator agreement and low detection accuracy (Markert et al., 2012; Rahman and Ng, 2012; Hou, 2016a). In the meantime, a few other corpora This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/. 1 Anaphors are marked in bold face, their antecedents are underlined. 3516 Proceedings of the 27th Inter"
C18-1298,D14-1222,0,0.112242,"al Language Processing University of Stuttgart, Germany {roesigia,arndt,jonas}@ims.uni-stuttgart.de Abstract Recent work on bridging resolution has so far been based on the corpus ISNotes (Markert et al., 2012), as this was the only corpus available with unrestricted bridging annotation. Hou et al.’s (2014) rule-based system currently achieves state-of-the-art performance on this corpus, as learning-based approaches suffer from the lack of available training data. Recently, a number of new corpora with bridging annotations have become available. To test the generalisability of the approach by Hou et al. (2014), we apply a slightly extended rule-based system to these corpora. Besides the expected out-of-domain effects, we also observe low performance on some of the indomain corpora. Our analysis shows that this is the result of two very different phenomena being defined as bridging, which we call referential and lexical bridging. We also report that filtering out gold or predicted coreferent anaphors before applying the bridging resolution system helps improve bridging resolution. 1 Introduction Bridging is an anaphoric phenomenon where the interpretation of a bridging anaphor, sometimes also called"
C18-1298,C16-1177,0,0.55501,"lished, most recent work is based on the ISNotes corpus (Markert et al., 2012), which contains Wall Street Journal articles. Full bridging resolution on this corpus has been investigated in Hou et al. (2014), following earlier experiments on the subtasks of bridging anaphor detection (Hou et al., 2013a) and antecedent selection (Hou et al., 2013b). Apart from this, there is some work on bridging detection as a subclass of information status classification, where bridging is typically a category with low annotator agreement and low detection accuracy (Markert et al., 2012; Rahman and Ng, 2012; Hou, 2016a). In the meantime, a few other corpora This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/. 1 Anaphors are marked in bold face, their antecedents are underlined. 3516 Proceedings of the 27th International Conference on Computational Linguistics, pages 3516–3528 Santa Fe, New Mexico, USA, August 20-26, 2018. have been made available (some of the same domain as ISNotes, newspaper, some of other domains) which make it possible to assess how generalisable the rule-based approach is. The focus of this p"
C18-1298,P12-1084,0,0.148776,"NLP tasks, the progress in bridging resolution is much slower. The main issue for most researchers aiming to apply statistical algorithms to this task is the lack of training data, as well as the rather diverse bridging definitions and annotations. The resolution of bridging links is important because it can prove beneficial in tasks which use the concept of textual coherence, for example Barzilay and Lapata’s (2008) entity grid or Hearst’s (1994) text segmentation. Note that while a benchmark dataset for bridging has not yet been established, most recent work is based on the ISNotes corpus (Markert et al., 2012), which contains Wall Street Journal articles. Full bridging resolution on this corpus has been investigated in Hou et al. (2014), following earlier experiments on the subtasks of bridging anaphor detection (Hou et al., 2013a) and antecedent selection (Hou et al., 2013b). Apart from this, there is some work on bridging detection as a subclass of information status classification, where bridging is typically a category with low annotator agreement and low detection accuracy (Markert et al., 2012; Rahman and Ng, 2012; Hou, 2016a). In the meantime, a few other corpora This work is licensed under"
C18-1298,nissim-etal-2004-annotation,0,0.19271,"lysis shows that this is the result of two very different – though often cooccurring – phenomena being defined as bridging, namely referential and lexical bridging, which is why we have included a rather extensive review of bridging definitions in the next section. 2 Defining bridging Bridging has been examined in many theoretical studies (Clark, 1975; Hawkins, 1978; Hobbs et al., 1993; Asher and Lascarides, 1998; Baumann and Riester, 2012) as well as in corpus and computational studies (Fraurud, 1990; Poesio et al., 1997; Vieira and Teufel, 1997; Poesio and Vieira, 1998; Poesio et al., 2004; Nissim et al., 2004; Nedoluzhko et al., 2009; Lassalle and Denis, 2011; Cahill and Riester, 2012; Markert et al., 2012; Hou et al., 2013b; Hou et al., 2013a; Hou, 2016b; Zik´anov´a et al., 2015; Grishina, 2016; Roitberg and Nedoluzhko, 2016; Riester and Baumann, 2017). Unlike in work on coreference resolution, these studies do not follow an agreed upon definition of bridging. On the contrary, many different phenomena have been described as bridging. While some of the issues have been controversial for a long time, e.g. the question of definiteness, the importance of the distinction between referential and lexica"
C18-1298,poesio-artstein-2008-anaphoric,0,0.175188,"he other with lexical proximity between two words and the relation between entities in the real world, although the two types of bridging often co-occur within one and the same pair of expressions, such as in (5), where we have a relation of meronymy between the content words sea urchin(s) and spine(s), but also an anaphoric relation between the referring expressions most sea urchins and the spines, i.e. a case of referential bridging. (5) In most sea urchins, touch elicits a prompt reaction from the spines. The second release of the ARRAU corpus (Uryupina et al., to appear, first released in Poesio and Artstein, 2008), as used in the first shared task on bridging resolution, for example, contains instances of both referential and lexical bridging, with the majority of the bridging links being purely lexical bridging pairs, i.e. most expressions labeled as bridging are actually not context-dependent. 3 Note that for proper nouns (names), like Spain, there is a one-to-one mapping between the word and its referent in the real world, which is not the case for common nouns, cf. Kripke (1972). 3518 2.3 Subset relations and lexical givenness Another relation often brought up in connection with (lexical) bridging"
C18-1298,J98-2001,0,0.916235,"mance on some of the indomain corpora. Our analysis shows that this is the result of two very different – though often cooccurring – phenomena being defined as bridging, namely referential and lexical bridging, which is why we have included a rather extensive review of bridging definitions in the next section. 2 Defining bridging Bridging has been examined in many theoretical studies (Clark, 1975; Hawkins, 1978; Hobbs et al., 1993; Asher and Lascarides, 1998; Baumann and Riester, 2012) as well as in corpus and computational studies (Fraurud, 1990; Poesio et al., 1997; Vieira and Teufel, 1997; Poesio and Vieira, 1998; Poesio et al., 2004; Nissim et al., 2004; Nedoluzhko et al., 2009; Lassalle and Denis, 2011; Cahill and Riester, 2012; Markert et al., 2012; Hou et al., 2013b; Hou et al., 2013a; Hou, 2016b; Zik´anov´a et al., 2015; Grishina, 2016; Roitberg and Nedoluzhko, 2016; Riester and Baumann, 2017). Unlike in work on coreference resolution, these studies do not follow an agreed upon definition of bridging. On the contrary, many different phenomena have been described as bridging. While some of the issues have been controversial for a long time, e.g. the question of definiteness, the importance of the"
C18-1298,W97-1301,0,0.858609,"-of-domain effects, we also observe low performance on some of the indomain corpora. Our analysis shows that this is the result of two very different – though often cooccurring – phenomena being defined as bridging, namely referential and lexical bridging, which is why we have included a rather extensive review of bridging definitions in the next section. 2 Defining bridging Bridging has been examined in many theoretical studies (Clark, 1975; Hawkins, 1978; Hobbs et al., 1993; Asher and Lascarides, 1998; Baumann and Riester, 2012) as well as in corpus and computational studies (Fraurud, 1990; Poesio et al., 1997; Vieira and Teufel, 1997; Poesio and Vieira, 1998; Poesio et al., 2004; Nissim et al., 2004; Nedoluzhko et al., 2009; Lassalle and Denis, 2011; Cahill and Riester, 2012; Markert et al., 2012; Hou et al., 2013b; Hou et al., 2013a; Hou, 2016b; Zik´anov´a et al., 2015; Grishina, 2016; Roitberg and Nedoluzhko, 2016; Riester and Baumann, 2017). Unlike in work on coreference resolution, these studies do not follow an agreed upon definition of bridging. On the contrary, many different phenomena have been described as bridging. While some of the issues have been controversial for a long time, e.g. th"
C18-1298,P04-1019,0,0.852015,"main corpora. Our analysis shows that this is the result of two very different – though often cooccurring – phenomena being defined as bridging, namely referential and lexical bridging, which is why we have included a rather extensive review of bridging definitions in the next section. 2 Defining bridging Bridging has been examined in many theoretical studies (Clark, 1975; Hawkins, 1978; Hobbs et al., 1993; Asher and Lascarides, 1998; Baumann and Riester, 2012) as well as in corpus and computational studies (Fraurud, 1990; Poesio et al., 1997; Vieira and Teufel, 1997; Poesio and Vieira, 1998; Poesio et al., 2004; Nissim et al., 2004; Nedoluzhko et al., 2009; Lassalle and Denis, 2011; Cahill and Riester, 2012; Markert et al., 2012; Hou et al., 2013b; Hou et al., 2013a; Hou, 2016b; Zik´anov´a et al., 2015; Grishina, 2016; Roitberg and Nedoluzhko, 2016; Riester and Baumann, 2017). Unlike in work on coreference resolution, these studies do not follow an agreed upon definition of bridging. On the contrary, many different phenomena have been described as bridging. While some of the issues have been controversial for a long time, e.g. the question of definiteness, the importance of the distinction between r"
C18-1298,E12-1081,0,0.244605,"as not yet been established, most recent work is based on the ISNotes corpus (Markert et al., 2012), which contains Wall Street Journal articles. Full bridging resolution on this corpus has been investigated in Hou et al. (2014), following earlier experiments on the subtasks of bridging anaphor detection (Hou et al., 2013a) and antecedent selection (Hou et al., 2013b). Apart from this, there is some work on bridging detection as a subclass of information status classification, where bridging is typically a category with low annotator agreement and low detection accuracy (Markert et al., 2012; Rahman and Ng, 2012; Hou, 2016a). In the meantime, a few other corpora This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/. 1 Anaphors are marked in bold face, their antecedents are underlined. 3516 Proceedings of the 27th International Conference on Computational Linguistics, pages 3516–3528 Santa Fe, New Mexico, USA, August 20-26, 2018. have been made available (some of the same domain as ISNotes, newspaper, some of other domains) which make it possible to assess how generalisable the rule-based approach is. The focu"
C18-1298,recasens-etal-2010-typology,0,0.0227719,"Piontek, 2015, 242f.) and the two expressions are marked as two contrastive elements from the same alternative set (Rooth, 1992). Comparative anaphors can be considered cases of referential bridging where the implicit argument is the implicit or explicit alternative set, i.e. another dog (from a specific or unspecific set dogs). (11) I saw a small pug two days ago and another dog yesterday. 3519 2.4 Near-identity While many approaches distinguish only between coreferent anaphors, which refer to the same referent as their antecedent, and bridging anaphors, which refer to a different referent, Recasens and Hovy (2010) and Recasens et al. (2012) have introduced a third concept, the concept of near-identity which has been picked up by others (e.g. Grishina, 2016). Near-identity is defined to hold between an anaphor and an antecedent whose referents are almost identical, but differ in one of four respects: name metonomy, meronymy, class or spatio-temporal functions. (12) On homecoming night Postville feels like Hometown, USA, but a look around this town of 2,000 shows its become a miniature Ellis Island . . . For those who prefer the old Postville, Mayor John Hyman has a simple answer. We believe that the int"
C18-1298,recasens-etal-2012-annotating,0,0.0200627,"e two expressions are marked as two contrastive elements from the same alternative set (Rooth, 1992). Comparative anaphors can be considered cases of referential bridging where the implicit argument is the implicit or explicit alternative set, i.e. another dog (from a specific or unspecific set dogs). (11) I saw a small pug two days ago and another dog yesterday. 3519 2.4 Near-identity While many approaches distinguish only between coreferent anaphors, which refer to the same referent as their antecedent, and bridging anaphors, which refer to a different referent, Recasens and Hovy (2010) and Recasens et al. (2012) have introduced a third concept, the concept of near-identity which has been picked up by others (e.g. Grishina, 2016). Near-identity is defined to hold between an anaphor and an antecedent whose referents are almost identical, but differ in one of four respects: name metonomy, meronymy, class or spatio-temporal functions. (12) On homecoming night Postville feels like Hometown, USA, but a look around this town of 2,000 shows its become a miniature Ellis Island . . . For those who prefer the old Postville, Mayor John Hyman has a simple answer. We believe that the introduction of this additiona"
C18-1298,W16-0709,0,0.0619697,"ve review of bridging definitions in the next section. 2 Defining bridging Bridging has been examined in many theoretical studies (Clark, 1975; Hawkins, 1978; Hobbs et al., 1993; Asher and Lascarides, 1998; Baumann and Riester, 2012) as well as in corpus and computational studies (Fraurud, 1990; Poesio et al., 1997; Vieira and Teufel, 1997; Poesio and Vieira, 1998; Poesio et al., 2004; Nissim et al., 2004; Nedoluzhko et al., 2009; Lassalle and Denis, 2011; Cahill and Riester, 2012; Markert et al., 2012; Hou et al., 2013b; Hou et al., 2013a; Hou, 2016b; Zik´anov´a et al., 2015; Grishina, 2016; Roitberg and Nedoluzhko, 2016; Riester and Baumann, 2017). Unlike in work on coreference resolution, these studies do not follow an agreed upon definition of bridging. On the contrary, many different phenomena have been described as bridging. While some of the issues have been controversial for a long time, e.g. the question of definiteness, the importance of the distinction between referential and lexical bridging, inspired by the two-level RefLex annotation scheme by Baumann and Riester (2012), became evident in our experiments. The two terms describe two different phenomena which are currently both defined and annotate"
C18-1298,W18-0705,1,0.867419,"Missing"
C18-1298,L16-1275,1,0.631622,"Missing"
C18-1298,L18-1058,0,0.2723,"Missing"
C18-1298,W18-0703,1,0.455527,"Missing"
C18-1298,W16-5304,0,0.0219548,"e have simply re-implemented the system from the original paper without any hand-tuning on the development set, we also report the numbers on the whole ISNotes corpus. Here, our re-implementation yields a precision of 57.7, a recall of 10.1 and an F1 score of 17.2 for full bridging resolution. Compared to the original numbers in Hou et al. (2014), we achieve higher precision, but lower recall, resulting in an overall lower F1 measure. 5.3 New rule and final performance In order to include more general information and to increase recall, we apply the distributional (DS) classifier described in Shwartz and Dagan (2016) to distinguish certain semantic relations, e.g. hyponyms and meronyms. The classifiers input are word embeddings, taken from ConceptNet (Speer et al., 2017). As we expect the prototypical bridging relation to be the relation of meronymy (part-whole), we include this information in our bridging resolver, in the form of the following rule: the anaphor has to be a definite, unmodified expression in the form of the N. We search for an antecedent within the last three sentences 3523 Corpus ISNotes (gold markables) ISNotes (pred markables) BASHI (pred) ARRAU (original, gold mark.) ARRAU (adapted, g"
C18-1298,P97-1072,0,0.699389,"e also observe low performance on some of the indomain corpora. Our analysis shows that this is the result of two very different – though often cooccurring – phenomena being defined as bridging, namely referential and lexical bridging, which is why we have included a rather extensive review of bridging definitions in the next section. 2 Defining bridging Bridging has been examined in many theoretical studies (Clark, 1975; Hawkins, 1978; Hobbs et al., 1993; Asher and Lascarides, 1998; Baumann and Riester, 2012) as well as in corpus and computational studies (Fraurud, 1990; Poesio et al., 1997; Vieira and Teufel, 1997; Poesio and Vieira, 1998; Poesio et al., 2004; Nissim et al., 2004; Nedoluzhko et al., 2009; Lassalle and Denis, 2011; Cahill and Riester, 2012; Markert et al., 2012; Hou et al., 2013b; Hou et al., 2013a; Hou, 2016b; Zik´anov´a et al., 2015; Grishina, 2016; Roitberg and Nedoluzhko, 2016; Riester and Baumann, 2017). Unlike in work on coreference resolution, these studies do not follow an agreed upon definition of bridging. On the contrary, many different phenomena have been described as bridging. While some of the issues have been controversial for a long time, e.g. the question of definitenes"
C18-2026,C16-2024,0,0.0586206,"Missing"
C18-2026,P14-5010,0,0.00436528,"ries of design sessions with interaction designers and NLP professionals we found that a web application is best to implement our Natural Language Analysis Tool (NLATool). Furthermore, by using well established interaction patters we can further support the usability. 4 System The CL community today already offers a quite wide range of very mature tools for numerous specific NLP tasks. This web-based solution is designed in a modular way to make use of existing analysis infrastructure and enable easy integration of other tools. As the main module, we used Stanford dependency parser (CoreNLP) (Manning et al., 2014) and its’ features for text analysis. Additionally we used the Google Knowledge Graph to obtain information about the named entities beyond the text itself. We implemented the NLATool as a web application . The main view is the text view where the user can view, edit, and delete named entities, the text component. However, it also presents an overview of all additional information beyond the text, the research component. We followed the metaphor of a split screen to enable the user to see both side by side. The text component takes up the left side of the screen 2 3 https://gate.ac.uk https://"
C18-2026,C16-2031,0,0.0149071,"1 http://demo.dbpedia-spotlight.org 118 Proceedings of the 27th International Conference on Computational Linguistics: System Demonstrations, pages 118–122 Santa Fe, New Mexico, USA, August 20-26, 2018. et al., 2013) or the YODIE (Gorrell et al., 2015) module for GATE2 . They rarely provide more than rudimentary highlighting of the found entities and generally are not aimed at more assistive functions or visualizations. Similar to this, tools for wikification (i.e., annotating mentions with the Wikipedia link of the respective entity) include for instance the Illinois Cross-Lingual Wikifier (Tsai and Roth, 2016). More advanced in terms of visualization, TASTY (Arnold et al., 2016) implements an as-you-type approach to interactive entity linking. It allows users to write a text through the application’s own interface and be provided with a live outline of complementary information, such as a picture or article link. Its design is however limited in the scale of how much of this complementary information is visible at once. 3 Design To determine requirements and desired features of text analysis and information extraction, we performed a qualitative user study with six computational linguistics (CL) ex"
C96-2113,C96-1057,0,0.053427,"Missing"
D12-1085,P11-2040,0,0.0226499,"Missing"
D12-1085,W11-2832,0,0.241299,"algorithm for tree linearization. We obtain statistically significant improvements on six typologically different languages: English, German, Dutch, Danish, Hungarian, and Czech. 1 (1) a. *It is federal support should try to what achieve b. *It is federal support should try to achieve what c. *It is try to achieve what federal support should Introduction There is a growing interest in language-independent data-driven approaches to natural language generation (NLG). An important subtask of NLG is surface realization, which was recently addressed in the 2011 Shared Task on Surface Realisation (Belz et al., 2011). Here, the input is a linguistic representation, such as a syntactic dependency tree lacking all precedence information, and the task is to determine a natural, coherent linearization of the words. The standard data-driven approach is to traverse the dependency tree deciding locally at each node on the relative order of the head and its children. The shared task results have proven this approach to be both effective and efficient when applied to English. ROOT SBJ OBJ PRD NMOD SBJ VC OPRD IM It is what federal support should try to achieve Figure 1: A non-projective example from the CoNLL 2009"
D12-1085,C10-1012,1,0.879608,"s correspond to the lower and upper bound from the nonlifted and the gold-lifted baseline. It clearly emerges from this figure that the range of improvements obtainable from lifting is closely tied to the general 936 We also evaluated our linearizer on the data of 2011 Shared Task on Surface Realisation, which is based on the English CoNLL 2009 data (like our previous evaluations) but excludes information on morphological realization. For training and evaluation, we used the exact set up of the Shared Task. For the morphological realization, we used the morphological realizer of Bohnet et al. (2010) that predicts the word form using shortest edit scripts. For the language model (LM), we use a 5-gram model with Kneser-Ney (Kneser and Ney, 1995) smoothing derived from 11 million sentences of the Wikipedia. In Table 6, we compare our two linearizers (with and without lifting) to the two top systems of the 2011 Shared Task on Surface Realisation, (Bohnet et al., 2011) and (Guo et al., 2011). Without the lifting, our system reaches a score comparable to the topranked system in the Shared Task. With the lifting, we get a small7 but statistically significant improvement in BLEU such that our sy"
D12-1085,W11-2835,1,0.79302,"method to obtain the word order domains from dependency trees and to order the words in the tree is to use each word and its children as domain and then to order the domains and contained words recursively. As outlined in the introduction, the direct mapping of syntactic trees to domains does not provide the possibility to obtain all possible correct word orders. Linearization systems can be roughly distinguished as either rule-based or statistical systems. In the 2011 Shared Task on Surface Realisation (Belz et al., 2011), the top performing systems were all statistical dependency realizers (Bohnet et al., 2011; Guo et al., 2011; Stent, 2011). Grammar-based approaches map dependency structures or phrase structures to a tree that represents the linear precedence. These approaches are mostly able to generate non-projective word orders. Early work was nearly exclusively applied to phrase structure grammars (e.g. (Kathol and Pollard, 1995; Rambow and Joshi, 1994; Langkilde and Knight, 1998)). Concerning dependency-based frameworks, Br¨oker (1998) used the concept of word order domains to separate surface realization from linear precedence trees. Similarly, Duchier and Debusmann (2001) differentiate Imme"
D12-1085,P98-1026,0,0.161246,"Missing"
D12-1085,W06-2920,0,0.0163117,"Missing"
D12-1085,W07-2303,0,0.0304587,"surface realization from linear precedence trees. Similarly, Duchier and Debusmann (2001) differentiate Immediate Dominance trees (ID-trees) from Linear Precedence trees (LPtrees). Gerdes and Kahane (2001) apply a hierarchical topological model for generating German word order. Bohnet (2004) employs graph grammars to map between dependency trees and linear precedence trees represented as hierarchical graphs. In the frameworks of HPSG, LFG, and CCG, a grammarbased generator produces word order candidates that might be non-projective, and a ranker is used to select the best surface realization (Cahill et al., 2007; White and Rajkumar, 2009). Statistical methods for linearization have recently become more popular (Langkilde and Knight, 1998; Ringger et al., 2004; Filippova and Strube, 2009; Wan et al., 2009; He et al., 2009; Bohnet et al., 2010; Guo et al., 2011). They typically work by traversing the syntactic structure either bottom-up (Filippova and Strube, 2007; Bohnet et al., 2010) or topdown (Guo et al., 2011; Bohnet et al., 2011). These linearizers are mostly applied to English and do not deal with non-projective word orders. An exception is Filippova and Strube (2007), who contribute a study on"
D12-1085,P01-1024,0,0.0448278,"tical dependency realizers (Bohnet et al., 2011; Guo et al., 2011; Stent, 2011). Grammar-based approaches map dependency structures or phrase structures to a tree that represents the linear precedence. These approaches are mostly able to generate non-projective word orders. Early work was nearly exclusively applied to phrase structure grammars (e.g. (Kathol and Pollard, 1995; Rambow and Joshi, 1994; Langkilde and Knight, 1998)). Concerning dependency-based frameworks, Br¨oker (1998) used the concept of word order domains to separate surface realization from linear precedence trees. Similarly, Duchier and Debusmann (2001) differentiate Immediate Dominance trees (ID-trees) from Linear Precedence trees (LPtrees). Gerdes and Kahane (2001) apply a hierarchical topological model for generating German word order. Bohnet (2004) employs graph grammars to map between dependency trees and linear precedence trees represented as hierarchical graphs. In the frameworks of HPSG, LFG, and CCG, a grammarbased generator produces word order candidates that might be non-projective, and a ranker is used to select the best surface realization (Cahill et al., 2007; White and Rajkumar, 2009). Statistical methods for linearization hav"
D12-1085,P07-1041,0,0.0267414,"d linear precedence trees represented as hierarchical graphs. In the frameworks of HPSG, LFG, and CCG, a grammarbased generator produces word order candidates that might be non-projective, and a ranker is used to select the best surface realization (Cahill et al., 2007; White and Rajkumar, 2009). Statistical methods for linearization have recently become more popular (Langkilde and Knight, 1998; Ringger et al., 2004; Filippova and Strube, 2009; Wan et al., 2009; He et al., 2009; Bohnet et al., 2010; Guo et al., 2011). They typically work by traversing the syntactic structure either bottom-up (Filippova and Strube, 2007; Bohnet et al., 2010) or topdown (Guo et al., 2011; Bohnet et al., 2011). These linearizers are mostly applied to English and do not deal with non-projective word orders. An exception is Filippova and Strube (2007), who contribute a study on the treatment of preverbal and postverbal constituents for German focusing on constituent order at the sentence level. The work most similar to ours is that of Gamon et al. (2002). They use machine-learning techniques to lift edges in a preprocessing step to a surface realizer. Their objective is the same as ours: by lifting, they avoid crossing edges. Ho"
D12-1085,N09-2057,0,0.133446,"LPtrees). Gerdes and Kahane (2001) apply a hierarchical topological model for generating German word order. Bohnet (2004) employs graph grammars to map between dependency trees and linear precedence trees represented as hierarchical graphs. In the frameworks of HPSG, LFG, and CCG, a grammarbased generator produces word order candidates that might be non-projective, and a ranker is used to select the best surface realization (Cahill et al., 2007; White and Rajkumar, 2009). Statistical methods for linearization have recently become more popular (Langkilde and Knight, 1998; Ringger et al., 2004; Filippova and Strube, 2009; Wan et al., 2009; He et al., 2009; Bohnet et al., 2010; Guo et al., 2011). They typically work by traversing the syntactic structure either bottom-up (Filippova and Strube, 2007; Bohnet et al., 2010) or topdown (Guo et al., 2011; Bohnet et al., 2011). These linearizers are mostly applied to English and do not deal with non-projective word orders. An exception is Filippova and Strube (2007), who contribute a study on the treatment of preverbal and postverbal constituents for German focusing on constituent order at the sentence level. The work most similar to ours is that of Gamon et al. (2002"
D12-1085,C02-1036,0,0.215683,"a and Strube, 2009; Wan et al., 2009; He et al., 2009; Bohnet et al., 2010; Guo et al., 2011). They typically work by traversing the syntactic structure either bottom-up (Filippova and Strube, 2007; Bohnet et al., 2010) or topdown (Guo et al., 2011; Bohnet et al., 2011). These linearizers are mostly applied to English and do not deal with non-projective word orders. An exception is Filippova and Strube (2007), who contribute a study on the treatment of preverbal and postverbal constituents for German focusing on constituent order at the sentence level. The work most similar to ours is that of Gamon et al. (2002). They use machine-learning techniques to lift edges in a preprocessing step to a surface realizer. Their objective is the same as ours: by lifting, they avoid crossing edges. However, contrary to our work, they use phrase-structure syntax and focus on a limited number of cases of crossing branches in German only. 3 Lifting Dependency Edges In this section, we describe the first of the two stages in our approach, namely the classifier that lifts edges in dependency trees. The classifier we aim to train is meant to predict liftings on a given unordered dependency tree, yielding a tree that, wit"
D12-1085,P01-1029,0,0.0320148,"structures or phrase structures to a tree that represents the linear precedence. These approaches are mostly able to generate non-projective word orders. Early work was nearly exclusively applied to phrase structure grammars (e.g. (Kathol and Pollard, 1995; Rambow and Joshi, 1994; Langkilde and Knight, 1998)). Concerning dependency-based frameworks, Br¨oker (1998) used the concept of word order domains to separate surface realization from linear precedence trees. Similarly, Duchier and Debusmann (2001) differentiate Immediate Dominance trees (ID-trees) from Linear Precedence trees (LPtrees). Gerdes and Kahane (2001) apply a hierarchical topological model for generating German word order. Bohnet (2004) employs graph grammars to map between dependency trees and linear precedence trees represented as hierarchical graphs. In the frameworks of HPSG, LFG, and CCG, a grammarbased generator produces word order candidates that might be non-projective, and a ranker is used to select the best surface realization (Cahill et al., 2007; White and Rajkumar, 2009). Statistical methods for linearization have recently become more popular (Langkilde and Knight, 1998; Ringger et al., 2004; Filippova and Strube, 2009; Wan et"
D12-1085,W11-2833,0,0.0579172,"Missing"
D12-1085,W09-1201,0,0.070125,"Missing"
D12-1085,P09-1091,0,0.0665074,"rarchical topological model for generating German word order. Bohnet (2004) employs graph grammars to map between dependency trees and linear precedence trees represented as hierarchical graphs. In the frameworks of HPSG, LFG, and CCG, a grammarbased generator produces word order candidates that might be non-projective, and a ranker is used to select the best surface realization (Cahill et al., 2007; White and Rajkumar, 2009). Statistical methods for linearization have recently become more popular (Langkilde and Knight, 1998; Ringger et al., 2004; Filippova and Strube, 2009; Wan et al., 2009; He et al., 2009; Bohnet et al., 2010; Guo et al., 2011). They typically work by traversing the syntactic structure either bottom-up (Filippova and Strube, 2007; Bohnet et al., 2010) or topdown (Guo et al., 2011; Bohnet et al., 2011). These linearizers are mostly applied to English and do not deal with non-projective word orders. An exception is Filippova and Strube (2007), who contribute a study on the treatment of preverbal and postverbal constituents for German focusing on constituent order at the sentence level. The work most similar to ours is that of Gamon et al. (2002). They use machine-learning techni"
D12-1085,P98-1106,0,0.0820264,"eration has addressed certain aspects of the problem. – OA# NK SB OC – Das Mandat will er zur¨ uckgeben . the.ACC mandate.ACC want.3SG he.NOM return.INF . ’He wants to return the mandate.’ Figure 2: German object fronting with complex verb introducing a non-projective edge. In this paper, we aim for a general data-driven approach that can deal with various causes for nonprojectivity and will work for typologically different languages. Our technique is inspired by work in data-driven multilingual parsing, where non-projectivity has received considerable attention. In pseudo-projective parsing (Kahane et al., 1998; Nivre and Nilsson, 2005), the parsing algorithm is restricted to projective structures, but the issue is side-stepped by converting non-projective structures to projective ones prior to training and application, and then restoring the original structure afterwards. Similarly, we split the linearization task in two stages: initially, the input tree is modified by lifting certain edges in such a way that new orderings become possible even under a projectivity constraint; the second stage is the original, projective linearization step. In parsing, projectivization is a deterministic process tha"
D12-1085,P95-1024,0,0.0216643,"obtain all possible correct word orders. Linearization systems can be roughly distinguished as either rule-based or statistical systems. In the 2011 Shared Task on Surface Realisation (Belz et al., 2011), the top performing systems were all statistical dependency realizers (Bohnet et al., 2011; Guo et al., 2011; Stent, 2011). Grammar-based approaches map dependency structures or phrase structures to a tree that represents the linear precedence. These approaches are mostly able to generate non-projective word orders. Early work was nearly exclusively applied to phrase structure grammars (e.g. (Kathol and Pollard, 1995; Rambow and Joshi, 1994; Langkilde and Knight, 1998)). Concerning dependency-based frameworks, Br¨oker (1998) used the concept of word order domains to separate surface realization from linear precedence trees. Similarly, Duchier and Debusmann (2001) differentiate Immediate Dominance trees (ID-trees) from Linear Precedence trees (LPtrees). Gerdes and Kahane (2001) apply a hierarchical topological model for generating German word order. Bohnet (2004) employs graph grammars to map between dependency trees and linear precedence trees represented as hierarchical graphs. In the frameworks of HPSG,"
D12-1085,kow-belz-2012-lg,0,0.0286802,"percentage of nonprojective edges in our data sets, which are however important to linearize correctly (see Figure 1). linearizer. In particular, we wanted to check whether the lifting-based linearizer produces more natural word orders for sentences that had a non-projective tree in the corpus, and maybe less natural word orders on originally projective sentences. Therefore, we divided the evaluated items into originally projective and non-projective sentences. We asked four annotators to judge 60 sentence pairs comparing the lifting-based against the nonlifted linearizer using the toolkit by Kow and Belz (2012). All annotators are students, two of them have a background in linguistics. The items were randomly sampled from the subset of the development set containing those sentences where the linearizers produced different surface realizations. The items are subdivided into 30 originally projective and 30 originally non-projective sentences. For each item, we presented the original context sentence from the corpus and the pair of automatically produced linearizations for the current sentence. The annotators had to decide on two criteria: (i) which sentence do they prefer? (ii) how fluent is that sent"
D12-1085,P98-1116,0,0.302823,"tion systems can be roughly distinguished as either rule-based or statistical systems. In the 2011 Shared Task on Surface Realisation (Belz et al., 2011), the top performing systems were all statistical dependency realizers (Bohnet et al., 2011; Guo et al., 2011; Stent, 2011). Grammar-based approaches map dependency structures or phrase structures to a tree that represents the linear precedence. These approaches are mostly able to generate non-projective word orders. Early work was nearly exclusively applied to phrase structure grammars (e.g. (Kathol and Pollard, 1995; Rambow and Joshi, 1994; Langkilde and Knight, 1998)). Concerning dependency-based frameworks, Br¨oker (1998) used the concept of word order domains to separate surface realization from linear precedence trees. Similarly, Duchier and Debusmann (2001) differentiate Immediate Dominance trees (ID-trees) from Linear Precedence trees (LPtrees). Gerdes and Kahane (2001) apply a hierarchical topological model for generating German word order. Bohnet (2004) employs graph grammars to map between dependency trees and linear precedence trees represented as hierarchical graphs. In the frameworks of HPSG, LFG, and CCG, a grammarbased generator produces word"
D12-1085,P05-1013,0,0.592937,"certain aspects of the problem. – OA# NK SB OC – Das Mandat will er zur¨ uckgeben . the.ACC mandate.ACC want.3SG he.NOM return.INF . ’He wants to return the mandate.’ Figure 2: German object fronting with complex verb introducing a non-projective edge. In this paper, we aim for a general data-driven approach that can deal with various causes for nonprojectivity and will work for typologically different languages. Our technique is inspired by work in data-driven multilingual parsing, where non-projectivity has received considerable attention. In pseudo-projective parsing (Kahane et al., 1998; Nivre and Nilsson, 2005), the parsing algorithm is restricted to projective structures, but the issue is side-stepped by converting non-projective structures to projective ones prior to training and application, and then restoring the original structure afterwards. Similarly, we split the linearization task in two stages: initially, the input tree is modified by lifting certain edges in such a way that new orderings become possible even under a projectivity constraint; the second stage is the original, projective linearization step. In parsing, projectivization is a deterministic process that lifts edges based on the"
D12-1085,E89-1014,0,0.400442,"onent. This classifier has to be trained on suitable data, and it is an empirical question whether the projective linearizer can take advantage of this preceding lifting step. We present experiments on six languages with varying degrees of non-projective structures: English, German, Dutch, Danish, Czech and Hungarian, which exhibit substantially different word order properties. Our approach achieves significant improvements on all six languages. On German, we also report results of a pilot human evaluation. 929 2 Related Work An important concept for tree linearization are word order domains (Reape, 1989). The domains are bags of words (constituents) that are not allowed to be discontinuous. A straightforward method to obtain the word order domains from dependency trees and to order the words in the tree is to use each word and its children as domain and then to order the domains and contained words recursively. As outlined in the introduction, the direct mapping of syntactic trees to domains does not provide the possibility to obtain all possible correct word orders. Linearization systems can be roughly distinguished as either rule-based or statistical systems. In the 2011 Shared Task on Surf"
D12-1085,C04-1097,0,0.282171,"ear Precedence trees (LPtrees). Gerdes and Kahane (2001) apply a hierarchical topological model for generating German word order. Bohnet (2004) employs graph grammars to map between dependency trees and linear precedence trees represented as hierarchical graphs. In the frameworks of HPSG, LFG, and CCG, a grammarbased generator produces word order candidates that might be non-projective, and a ranker is used to select the best surface realization (Cahill et al., 2007; White and Rajkumar, 2009). Statistical methods for linearization have recently become more popular (Langkilde and Knight, 1998; Ringger et al., 2004; Filippova and Strube, 2009; Wan et al., 2009; He et al., 2009; Bohnet et al., 2010; Guo et al., 2011). They typically work by traversing the syntactic structure either bottom-up (Filippova and Strube, 2007; Bohnet et al., 2010) or topdown (Guo et al., 2011; Bohnet et al., 2011). These linearizers are mostly applied to English and do not deal with non-projective word orders. An exception is Filippova and Strube (2007), who contribute a study on the treatment of preverbal and postverbal constituents for German focusing on constituent order at the sentence level. The work most similar to ours i"
D12-1085,seeker-kuhn-2012-making,1,0.896576,"Missing"
D12-1085,W11-2834,0,0.0293792,"from dependency trees and to order the words in the tree is to use each word and its children as domain and then to order the domains and contained words recursively. As outlined in the introduction, the direct mapping of syntactic trees to domains does not provide the possibility to obtain all possible correct word orders. Linearization systems can be roughly distinguished as either rule-based or statistical systems. In the 2011 Shared Task on Surface Realisation (Belz et al., 2011), the top performing systems were all statistical dependency realizers (Bohnet et al., 2011; Guo et al., 2011; Stent, 2011). Grammar-based approaches map dependency structures or phrase structures to a tree that represents the linear precedence. These approaches are mostly able to generate non-projective word orders. Early work was nearly exclusively applied to phrase structure grammars (e.g. (Kathol and Pollard, 1995; Rambow and Joshi, 1994; Langkilde and Knight, 1998)). Concerning dependency-based frameworks, Br¨oker (1998) used the concept of word order domains to separate surface realization from linear precedence trees. Similarly, Duchier and Debusmann (2001) differentiate Immediate Dominance trees (ID-trees)"
D12-1085,vincze-etal-2010-hungarian,0,0.062248,"Missing"
D12-1085,E09-1097,0,0.0255443,"(2001) apply a hierarchical topological model for generating German word order. Bohnet (2004) employs graph grammars to map between dependency trees and linear precedence trees represented as hierarchical graphs. In the frameworks of HPSG, LFG, and CCG, a grammarbased generator produces word order candidates that might be non-projective, and a ranker is used to select the best surface realization (Cahill et al., 2007; White and Rajkumar, 2009). Statistical methods for linearization have recently become more popular (Langkilde and Knight, 1998; Ringger et al., 2004; Filippova and Strube, 2009; Wan et al., 2009; He et al., 2009; Bohnet et al., 2010; Guo et al., 2011). They typically work by traversing the syntactic structure either bottom-up (Filippova and Strube, 2007; Bohnet et al., 2010) or topdown (Guo et al., 2011; Bohnet et al., 2011). These linearizers are mostly applied to English and do not deal with non-projective word orders. An exception is Filippova and Strube (2007), who contribute a study on the treatment of preverbal and postverbal constituents for German focusing on constituent order at the sentence level. The work most similar to ours is that of Gamon et al. (2002). They use machin"
D12-1085,D09-1043,0,0.0262488,"rom linear precedence trees. Similarly, Duchier and Debusmann (2001) differentiate Immediate Dominance trees (ID-trees) from Linear Precedence trees (LPtrees). Gerdes and Kahane (2001) apply a hierarchical topological model for generating German word order. Bohnet (2004) employs graph grammars to map between dependency trees and linear precedence trees represented as hierarchical graphs. In the frameworks of HPSG, LFG, and CCG, a grammarbased generator produces word order candidates that might be non-projective, and a ranker is used to select the best surface realization (Cahill et al., 2007; White and Rajkumar, 2009). Statistical methods for linearization have recently become more popular (Langkilde and Knight, 1998; Ringger et al., 2004; Filippova and Strube, 2009; Wan et al., 2009; He et al., 2009; Bohnet et al., 2010; Guo et al., 2011). They typically work by traversing the syntactic structure either bottom-up (Filippova and Strube, 2007; Bohnet et al., 2010) or topdown (Guo et al., 2011; Bohnet et al., 2011). These linearizers are mostly applied to English and do not deal with non-projective word orders. An exception is Filippova and Strube (2007), who contribute a study on the treatment of preverbal"
D12-1085,C98-1112,0,\N,Missing
D12-1085,C98-1102,0,\N,Missing
D12-1085,C98-1026,0,\N,Missing
D13-1033,D12-1133,0,0.0148322,"umber marks if the form contains a digit. After preprocessing the data, our baseline system is trained using the feature set shown in Table 1. The baseline system does not make use of any syntactic information but predicts morphological information based solely on tokens and their linear context. The features are divided into static features, which can be computed on the input, and dynamic features, which are computed also on previous output of the system (cf. two passes in Section 2.2). 6 Lemma and part-of-speech prediction may also profit from syntactic information, see e.g. Prins (2004) or Bohnet and Nivre (2012). 336 The feature sets in Table 1 were developed specifically for our experiments and are the result of an automatic forward/backward feature selection process. The purpose of the feature selection was to arrive at a baseline system that performs well without any syntactic information. With such an optimized baseline system, we can measure the contribution of syntactic features more reliably. The last-verb/next-verb and pos+case features are variants of the features proposed in Votrubec (2006). They extract information about the first verb within the last 10/the next 30 tokens in the sentence."
D13-1033,carreras-etal-2004-freeling,0,0.0100495,"morphological lexicons. Lexicons encode important knowledge that is difficult to pick up in a purely statistical system, e. g. the gender of nouns, which often cannot be deduced from the word form (Corbett, 1991).7 We extend our system from the previous experiment to include information from a morphological dictionaries. For Czech, we use the morphological analyzer distributed with the Prague Dependency Treebank 2 (Hajiˇc et al., 2006). For German, we use DMor (Schiller, 1994). For Hungarian, we use (Tr´on et al., 2006), and for Spanish, we use the morphological analyzer included in Freeling (Carreras et al., 2004). The output of the analyzers is given to the system as features that simply record the presence of a particular morphological analysis for the current word. The system can thus use the output of any tool regardless of its annotation scheme, especially if the annotation scheme of the treebank is different from the one of the morphological analyzer. Table 4 presents the results of experiments where we add the output of the morphological analyzers to our system. Again, we run experiments with and without syntactic features. For Czech, we also show results from featurama8 with the feature set dev"
D13-1033,D07-1022,0,0.0250798,"ce. Morphologically rich languages pose different challenges for automatic systems. In this paper, we work with European languages, where the problem of predicting morphology can be reduced to a tagging problem. In languages like Arabic, Hebrew, or Turkish, widespread ambiguity in segmentation of single words into meaningful morphemes adds an additional complexity. Given a good segmentation tool that takes care of this, our approach is applicable to these languages as well. For Hebrew, this problem has also been addressed by jointly modeling segmentation, morphological prediction, and syntax (Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2013). 6 Conclusion In this paper, we have demonstrated that using syntactic information for predicting morphological information is helpful if the language shows form syncretism in combination with morphosyntactic phenomena like agreement. A model that uses syntactic information is superior to a sequence model because it leverages the syntactic dependencies that may hold between morphologically dependent words as suggested by linguistic theory. We also showed that only small amounts of training data for a statistical parser would be needed"
D13-1033,W02-1001,0,0.0181228,"ny interactions between morphology and syntax. For example, languages with a case system use different forms of the same word to mark different syntactic (or semantic) relations (Blake, 2001). In many languages, two words that participate in a syntactic relation show covariance in some or all of their morphological features (so-called agreement, Corbett (2006)).3 Automatic annotation of morphology assigns morphological descriptions (e. g., nominativesingular-masculine) to word forms. It is usually modeled as a sequence model, often in combination with part-of-speech tagging and lemmatization (Collins, 2002; Hajiˇc, 2004; Smith et al., 2005; Chrupała et al., 2008, and others). Sequence models achieve high accuracy and coverage but since they only use linear context they only approximate some of the underlying hierarchical relationships. As an example for these hierarchical relationships, 2 And also by prosodic means, which we will not discuss since text-based tools rarely have access to this information. 3 For example, in English, the subject of a sentence and the finite verb agree with respect to their number and person feature. 333 Proceedings of the 2013 Conference on Empirical Methods in Nat"
D13-1033,E12-1007,0,0.0160997,"d genitive case (out of about 18 case suffixes). All languages show agreement between subject and verb, and within the noun phrase. The word order in Czech and Hungarian is very variable whereas it is more restrictive in Spanish and German. As our data, we use the CoNLL 2009 Shared Task data sets (Hajiˇc et al., 2009) for Czech and Spanish. For German, we use the dependency conversion of the TiGer treebank by Seeker and Kuhn (2012), splitting it into 40k/5k/5k sentences for training/development/test. For Hungarian, we use the Szeged Dependency Treebank (Vincze et al., 2010), with the split of Farkas et al. (2012). 2.2 System Description To test our hypotheses, we implemented a tagger that assigns full morphological descriptions to each token in a sentence. The system was inspired by the morphological tagger included in mate-tools.5 Like the tagger provided with mate-tools, it is a classifier that tags each token using the surrounding tokens in 5 A collection of language independent, data-driven analysis tools for lemmatization, pos-tagging, morphological analysis, and dependency parsing: http://code.google.com/p/mate-tools 335 its feature model. Models are trained using passiveaggressive online traini"
D13-1033,W10-1412,0,0.0699872,"the motivation that joint models should model the problem more faithfully. They demonstrate that both sides can use information from each other. However, their model is computationally quite demanding and its overall performance falls far behind the standard pipeline approach where both tasks are done in sequence. The problem of modeling the interaction between morphology and syntax has recently attracted some attention in the SPMRL workshops (Tsarfaty et al., 2010). Modeling morphosyntactic relations explicitly has been shown to improve statistical parsing models (Tsarfaty and Sima’an, 2010; Goldberg and Elhadad, 2010; Seeker and Kuhn, 2013), but the codependency between morphology and syntax makes it a difficult problem, and linguistic intuition is often contradicted by the empirical findings. For example, Marton et al. (2013) show that case information is the most helpful morphological feature for parsing Arabic, but only if it is given as gold information, whereas using case information from an automatic system may even harm the performance. Morphologically rich languages pose different challenges for automatic systems. In this paper, we work with European languages, where the problem of predicting morp"
D13-1033,J13-1007,0,0.123268,"allenges for automatic systems. In this paper, we work with European languages, where the problem of predicting morphology can be reduced to a tagging problem. In languages like Arabic, Hebrew, or Turkish, widespread ambiguity in segmentation of single words into meaningful morphemes adds an additional complexity. Given a good segmentation tool that takes care of this, our approach is applicable to these languages as well. For Hebrew, this problem has also been addressed by jointly modeling segmentation, morphological prediction, and syntax (Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2013). 6 Conclusion In this paper, we have demonstrated that using syntactic information for predicting morphological information is helpful if the language shows form syncretism in combination with morphosyntactic phenomena like agreement. A model that uses syntactic information is superior to a sequence model because it leverages the syntactic dependencies that may hold between morphologically dependent words as suggested by linguistic theory. We also showed that only small amounts of training data for a statistical parser would be needed to improve the morphological tagger. Making use of the imp"
D13-1033,P08-1043,0,0.0196924,"h languages pose different challenges for automatic systems. In this paper, we work with European languages, where the problem of predicting morphology can be reduced to a tagging problem. In languages like Arabic, Hebrew, or Turkish, widespread ambiguity in segmentation of single words into meaningful morphemes adds an additional complexity. Given a good segmentation tool that takes care of this, our approach is applicable to these languages as well. For Hebrew, this problem has also been addressed by jointly modeling segmentation, morphological prediction, and syntax (Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2013). 6 Conclusion In this paper, we have demonstrated that using syntactic information for predicting morphological information is helpful if the language shows form syncretism in combination with morphosyntactic phenomena like agreement. A model that uses syntactic information is superior to a sequence model because it leverages the syntactic dependencies that may hold between morphologically dependent words as suggested by linguistic theory. We also showed that only small amounts of training data for a statistical parser would be needed to improve the morphological"
D13-1033,A00-2013,0,0.440398,"Missing"
D13-1033,W09-1201,0,0.0572687,"Missing"
D13-1033,N12-1032,0,0.0209379,"87.61 88.06 88.15 90.24 90.34 Table 9: Impact of the improved morphology on the quality of the dependency parser for Czech and German. tween the two parsing models with respect to grammatical functions that are morphologically marked. For example, in German, performance on subjects and accusative objects improves while performance for dative objects and genitives decreases. This suggests different strengths in the two parsing models. However, the question how to make use of the improved morphology in parsing clearly needs more research in the future. A promising avenue may be the approach by Hohensee and Bender (2012). 5 Related Work Morphological taggers have been developed for many languages. The most common approach is the combination of a morphological lexicon with a statistical disambiguation model (Hakkani-T¨ur et al., 2002; Hajiˇc, 2004; Smith et al., 2005; Spoustov´a et al., 2009; Zsibrita et al., 2013). Our work has been inspired by Versley et al. (2010), who annotate a treebank with morphological information after the syntax had been annotated already. The system used a finite-state morphology to propose a set of candidate tags for each word, which is then further restricted using hand-crafted ru"
D13-1033,P11-1089,0,0.013736,"been developed for many languages. The most common approach is the combination of a morphological lexicon with a statistical disambiguation model (Hakkani-T¨ur et al., 2002; Hajiˇc, 2004; Smith et al., 2005; Spoustov´a et al., 2009; Zsibrita et al., 2013). Our work has been inspired by Versley et al. (2010), who annotate a treebank with morphological information after the syntax had been annotated already. The system used a finite-state morphology to propose a set of candidate tags for each word, which is then further restricted using hand-crafted rules over the already available syntax tree. Lee et al. (2011) pursue the idea of jointly predicting syntax and morphology, out of the motivation that joint models should model the problem more faithfully. They demonstrate that both sides can use information from each other. However, their model is computationally quite demanding and its overall performance falls far behind the standard pipeline approach where both tasks are done in sequence. The problem of modeling the interaction between morphology and syntax has recently attracted some attention in the SPMRL workshops (Tsarfaty et al., 2010). Modeling morphosyntactic relations explicitly has been show"
D13-1033,J13-1008,0,0.0263949,"formance falls far behind the standard pipeline approach where both tasks are done in sequence. The problem of modeling the interaction between morphology and syntax has recently attracted some attention in the SPMRL workshops (Tsarfaty et al., 2010). Modeling morphosyntactic relations explicitly has been shown to improve statistical parsing models (Tsarfaty and Sima’an, 2010; Goldberg and Elhadad, 2010; Seeker and Kuhn, 2013), but the codependency between morphology and syntax makes it a difficult problem, and linguistic intuition is often contradicted by the empirical findings. For example, Marton et al. (2013) show that case information is the most helpful morphological feature for parsing Arabic, but only if it is given as gold information, whereas using case information from an automatic system may even harm the performance. Morphologically rich languages pose different challenges for automatic systems. In this paper, we work with European languages, where the problem of predicting morphology can be reduced to a tagging problem. In languages like Arabic, Hebrew, or Turkish, widespread ambiguity in segmentation of single words into meaningful morphemes adds an additional complexity. Given a good s"
D13-1033,P04-2011,0,0.0366751,"conjunctions. number marks if the form contains a digit. After preprocessing the data, our baseline system is trained using the feature set shown in Table 1. The baseline system does not make use of any syntactic information but predicts morphological information based solely on tokens and their linear context. The features are divided into static features, which can be computed on the input, and dynamic features, which are computed also on previous output of the system (cf. two passes in Section 2.2). 6 Lemma and part-of-speech prediction may also profit from syntactic information, see e.g. Prins (2004) or Bohnet and Nivre (2012). 336 The feature sets in Table 1 were developed specifically for our experiments and are the result of an automatic forward/backward feature selection process. The purpose of the feature selection was to arrive at a baseline system that performs well without any syntactic information. With such an optimized baseline system, we can measure the contribution of syntactic features more reliably. The last-verb/next-verb and pos+case features are variants of the features proposed in Votrubec (2006). They extract information about the first verb within the last 10/the next"
D13-1033,C08-1098,0,0.0197015,"simply record the presence of a particular morphological analysis for the current word. The system can thus use the output of any tool regardless of its annotation scheme, especially if the annotation scheme of the treebank is different from the one of the morphological analyzer. Table 4 presents the results of experiments where we add the output of the morphological analyzers to our system. Again, we run experiments with and without syntactic features. For Czech, we also show results from featurama8 with the feature set developed by Votrubec (2006). For German, we show results for RFTagger (Schmid and Laws, 2008). As expected, the information from the morphological lexicon improves the overall performance 7 Lexicons are also often used to speed up processing considerably by restricting the search space of the statistical model. 8 http://sourceforge.net/projects/featurama/ 338 considerably compared to the results in Table 3, especially on unknown tokens. This shows that even with the considerable amounts of training data available nowadays, rule-based morphological analyzers are important resources for morphological description (cf. Hajiˇc (2000)). The contribution of syntactic features in German and C"
D13-1033,seeker-kuhn-2012-making,1,0.824277,"shows syncretism in the verbal inflection paradigms. In Hungarian, form syncretisms are much less frequent. The case paradigm of Hungarian only shows one form syncretism between dative and genitive case (out of about 18 case suffixes). All languages show agreement between subject and verb, and within the noun phrase. The word order in Czech and Hungarian is very variable whereas it is more restrictive in Spanish and German. As our data, we use the CoNLL 2009 Shared Task data sets (Hajiˇc et al., 2009) for Czech and Spanish. For German, we use the dependency conversion of the TiGer treebank by Seeker and Kuhn (2012), splitting it into 40k/5k/5k sentences for training/development/test. For Hungarian, we use the Szeged Dependency Treebank (Vincze et al., 2010), with the split of Farkas et al. (2012). 2.2 System Description To test our hypotheses, we implemented a tagger that assigns full morphological descriptions to each token in a sentence. The system was inspired by the morphological tagger included in mate-tools.5 Like the tagger provided with mate-tools, it is a classifier that tags each token using the surrounding tokens in 5 A collection of language independent, data-driven analysis tools for lemmat"
D13-1033,J13-1004,1,0.80519,"dels should model the problem more faithfully. They demonstrate that both sides can use information from each other. However, their model is computationally quite demanding and its overall performance falls far behind the standard pipeline approach where both tasks are done in sequence. The problem of modeling the interaction between morphology and syntax has recently attracted some attention in the SPMRL workshops (Tsarfaty et al., 2010). Modeling morphosyntactic relations explicitly has been shown to improve statistical parsing models (Tsarfaty and Sima’an, 2010; Goldberg and Elhadad, 2010; Seeker and Kuhn, 2013), but the codependency between morphology and syntax makes it a difficult problem, and linguistic intuition is often contradicted by the empirical findings. For example, Marton et al. (2013) show that case information is the most helpful morphological feature for parsing Arabic, but only if it is given as gold information, whereas using case information from an automatic system may even harm the performance. Morphologically rich languages pose different challenges for automatic systems. In this paper, we work with European languages, where the problem of predicting morphology can be reduced to"
D13-1033,H05-1060,0,0.606227,"ology and syntax. For example, languages with a case system use different forms of the same word to mark different syntactic (or semantic) relations (Blake, 2001). In many languages, two words that participate in a syntactic relation show covariance in some or all of their morphological features (so-called agreement, Corbett (2006)).3 Automatic annotation of morphology assigns morphological descriptions (e. g., nominativesingular-masculine) to word forms. It is usually modeled as a sequence model, often in combination with part-of-speech tagging and lemmatization (Collins, 2002; Hajiˇc, 2004; Smith et al., 2005; Chrupała et al., 2008, and others). Sequence models achieve high accuracy and coverage but since they only use linear context they only approximate some of the underlying hierarchical relationships. As an example for these hierarchical relationships, 2 And also by prosodic means, which we will not discuss since text-based tools rarely have access to this information. 3 For example, in English, the subject of a sentence and the finite verb agree with respect to their number and person feature. 333 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 33"
D13-1033,E09-1087,0,0.184171,"Missing"
D13-1033,tron-etal-2006-morphdb,0,0.0742457,"Missing"
D13-1033,W10-1405,0,0.0308968,"Missing"
D13-1033,W10-1401,0,0.0414481,"Missing"
D13-1033,vincze-etal-2010-hungarian,0,0.0641366,"Missing"
D13-1033,chrupala-etal-2008-learning,0,\N,Missing
D13-1194,W09-1206,0,0.0764022,"Missing"
D13-1194,C08-1031,0,0.337529,"SRL system out of the box. Both approaches consider only nouns and pronouns for entities and aspects, we use all POS and allow for multi-word arguments. Jindal and Liu (2006b) base the recognition of comparative predi1893 cates on a list of manually compiled keywords. We use this as our baseline. Our approach is not dependent on a set of keywords and is therefore more easily adaptable to a new domain. All works label the entities according to their position with respect to the predicate. This requires the identification of the preferred entity in a non-equal comparison as an additional step. Ganapathibhotla and Liu (2008) use hand-crafted rules based on the polarity of the predicate for this task. As we label the entities with their roles from the start, we solve both problems at the same time. Xu et al. (2011) cast the task as a relation extraction problem. They present an approach that uses conditional random fields to extract relations (better, worse, same and no comparison) between two entitites, an attribute and a predicate phrase. The approach of Hou and Li (2008) is most related to our approach. They use SRL with standard SRL features to extract comparative relations from Chinese sentences. We confirm t"
D13-1194,P13-1094,0,0.0250735,"Missing"
D13-1194,W03-2102,0,0.0224883,"Missing"
D13-1194,P09-2039,0,0.508686,"to comparison detection. 2 Related Work The syntax and semantics of comparative sentences have been the topic of research in linguistics for a long time (Moltmann, 1992; Kennedy, 1999). However, our focus is on computational methods and we also treat comparisons that are not comparative sentences in a linguistic sense. In sentiment analysis, some studies have been presented to identify comparison sentences. Jindal and Liu (2006a) report good results on English using class sequential rules based on keywords as features for a Naive Bayes classifier. A similar approach for Korean is presented by Yang and Ko (2009; 2011b; 2011a). In our work, we do not address the task of identifying comparison sentences, we assume that we are given a set of such sentences. The step we are concerned with is the detection of relevant parts of a comparison. To identify entities and aspect, Jindal and Liu (2006b) use an involved pattern mining process to mine label sequential rules from annotated English sentences. A similar approach is again presented by Yang and Ko (2011a) for Korean. In contrast to their complicated processing, we simply use an existing SRL system out of the box. Both approaches consider only nouns and"
D13-1194,P11-1164,0,0.0947295,"sults on English using class sequential rules based on keywords as features for a Naive Bayes classifier. A similar approach for Korean is presented by Yang and Ko (2009; 2011b; 2011a). In our work, we do not address the task of identifying comparison sentences, we assume that we are given a set of such sentences. The step we are concerned with is the detection of relevant parts of a comparison. To identify entities and aspect, Jindal and Liu (2006b) use an involved pattern mining process to mine label sequential rules from annotated English sentences. A similar approach is again presented by Yang and Ko (2011a) for Korean. In contrast to their complicated processing, we simply use an existing SRL system out of the box. Both approaches consider only nouns and pronouns for entities and aspects, we use all POS and allow for multi-word arguments. Jindal and Liu (2006b) base the recognition of comparative predi1893 cates on a list of manually compiled keywords. We use this as our baseline. Our approach is not dependent on a set of keywords and is therefore more easily adaptable to a new domain. All works label the entities according to their position with respect to the predicate. This requires the ide"
D17-1288,L16-1153,0,0.40293,"7 the corpus was untraceable and no recent research relating to the data could be found. OCR post-correction is applied in a diversity of fields in order to compile high-quality datasets. This is not merely reflected in the homogeneity of techniques but in the metric of evaluation as well. While accuracy has been widely used as evaluation measure in OCR post-correction research, Reynaert (2008a) advocates the use of precision and recall in order to improve transparency in evaluations. Dependent on the paradigm of the applied technique even evaluation measures like BLEU score can be found (cf. Afli et al. (2016)). Since shared tasks are a good opportunity to establish certain standards and facilitate the comparability of techniques, the Competition on Post-OCR Text Correction3 organized in the context of ICDAR 2017 could mark a milestone for more unified OCR post-correction research efforts. Regarding techniques used for OCR postcorrection, there are two main trends to be mentioned: statistical approaches utilizing error distributions inferred from training data and lexical approaches oriented towards the comparison of source words to a canonical form. Combinations of the two approaches are also avai"
D17-1288,L16-1155,0,0.303563,"crucial part of Digital Humanities collaborations, we describe the workflow we suggest for efficient text recognition and subsequent automatic and manual postcorrection. 1 Introduction Humanities are no longer just the realm of scholars turning pages of thick books. As the worlds of humanists and computer scientists begin to intertwine, new methods to revisit known ground emerge and options to widen the scope of research questions are available. Moreover, the nature of language encountered in such research attracts the attention of the NLP community (Kao and Jurafsky (2015), Milli and Bamman (2016)). Yet, the basic requirement for the successful implementation of such projects often poses a stumbling block: large digital corpora comprising the textual material of interest are rare. Archives and individual scholars are in the process of improving this situation by applying Optical Character Recognition (OCR) to the physical resources. In the Google Books1 project books are being digitized on a large scale. But even though collections of literary texts like Project Gutenberg2 exist, these collections often lack the texts of interest to a specific question. As an example, we describe the c"
D17-1288,R13-1024,1,0.891283,"Missing"
D17-1288,N13-1037,0,0.0635648,"Missing"
D17-1288,W11-2123,0,0.00984297,"esseract4 (Smith and Inc, 2007) which comes with recognition models for Gothic font. 4.3 Gutenberg data for language modeling Since the output of our system is supposed to consist of well-formed German sentences, we need a method to assess the quality of the output language. This task is generally tackled by language modeling. We compiled a collection of 500 randomly chosen texts from Project Gutenberg5 comprising 28,528,078 tokens. With its relative closeness to our target domain it constitutes the best approximation of a target language. The language model is trained with the KenLM toolkit (Heafield, 2011) with an order of 5 on token level and 10 on character level following De Clercq et al. (2013). 4 Considering the open source aspect of our resulting system, we decided to use the open source OCR software tesseract and move away from Abbyy some time after our project started: https://github.com/tesseract-ocr. 5 Project Gutenberg. Retrieved January 21, 2017, from www.gutenberg.org. 5 Why OCR post-correction is hard In tasks like the normalization of historical text (Bollmann et al., 2012) or social media, one can take advantage of regularities in the deviations from the standard form that appea"
D17-1288,2015.lilt-12.3,0,0.0223976,"e accessibility of the resulting tool as a crucial part of Digital Humanities collaborations, we describe the workflow we suggest for efficient text recognition and subsequent automatic and manual postcorrection. 1 Introduction Humanities are no longer just the realm of scholars turning pages of thick books. As the worlds of humanists and computer scientists begin to intertwine, new methods to revisit known ground emerge and options to widen the scope of research questions are available. Moreover, the nature of language encountered in such research attracts the attention of the NLP community (Kao and Jurafsky (2015), Milli and Bamman (2016)). Yet, the basic requirement for the successful implementation of such projects often poses a stumbling block: large digital corpora comprising the textual material of interest are rare. Archives and individual scholars are in the process of improving this situation by applying Optical Character Recognition (OCR) to the physical resources. In the Google Books1 project books are being digitized on a large scale. But even though collections of literary texts like Project Gutenberg2 exist, these collections often lack the texts of interest to a specific question. As an e"
D17-1288,P07-2045,0,0.0120415,"uggestion modules In the following, we give an outline of techniques included into our system. 6.1.1 Word level suggestion modules • Original: the majority of words do not contain any kind of error, thus we want to have 6 Tokenizer of TreeTagger (Schmid, 1997). • • • the initial token available in our suggestion pool Spell checker: spelling correction suggestion for misspelled words with hunspell7 Compounder: merges two tokens into one token if it is evaluated as an existing word by hunspell Word splitter: splits two tokens into two words using compound-splitter module from the Moses toolkit (Koehn et al., 2007) Text-Internal Vocabulary: extracts highfrequent words from the input texts and suggests them as correction of words with small adjusted Levenshtein distance8 The compound and word split techniques react to the variance in manual typesetting, where the distances between letters vary. This means that the word boundary recognition becomes difficult (cf. Figure 3). A problem related to the spell-checking approach is the limited coverage of the dictionary since it uses a modern German lexicon. Related to this is the difficulty of out-of-vocabulary words above average for literature text. Archaic w"
D17-1288,D16-1218,0,0.0284517,"sulting tool as a crucial part of Digital Humanities collaborations, we describe the workflow we suggest for efficient text recognition and subsequent automatic and manual postcorrection. 1 Introduction Humanities are no longer just the realm of scholars turning pages of thick books. As the worlds of humanists and computer scientists begin to intertwine, new methods to revisit known ground emerge and options to widen the scope of research questions are available. Moreover, the nature of language encountered in such research attracts the attention of the NLP community (Kao and Jurafsky (2015), Milli and Bamman (2016)). Yet, the basic requirement for the successful implementation of such projects often poses a stumbling block: large digital corpora comprising the textual material of interest are rare. Archives and individual scholars are in the process of improving this situation by applying Optical Character Recognition (OCR) to the physical resources. In the Google Books1 project books are being digitized on a large scale. But even though collections of literary texts like Project Gutenberg2 exist, these collections often lack the texts of interest to a specific question. As an example, we describe the c"
D17-1288,P03-1021,0,0.0177621,"-generated contents. In the first stage, a set of specialized modules (Section 6.1) suggest corrected versions for the tokenized6 OCR text lines. Those modules can be context-independent (work on just one word at a time) or context-dependent (an entire text line is processed at a time). The second stage is the decision phase. After the collection of various suggestions per input token, these have to be ranked to enable a decision for the most probable output token given the context. We achieve this by assigning weights the different modules with the help of Minimal Error Rate Training (MERT) (Och, 2003). 6.1 Suggestion modules In the following, we give an outline of techniques included into our system. 6.1.1 Word level suggestion modules • Original: the majority of words do not contain any kind of error, thus we want to have 6 Tokenizer of TreeTagger (Schmid, 1997). • • • the initial token available in our suggestion pool Spell checker: spelling correction suggestion for misspelled words with hunspell7 Compounder: merges two tokens into one token if it is evaluated as an existing word by hunspell Word splitter: splits two tokens into two words using compound-splitter module from the Moses to"
D17-1288,W96-0108,0,0.62377,"organized in the context of ICDAR 2017 could mark a milestone for more unified OCR post-correction research efforts. Regarding techniques used for OCR postcorrection, there are two main trends to be mentioned: statistical approaches utilizing error distributions inferred from training data and lexical approaches oriented towards the comparison of source words to a canonical form. Combinations of the two approaches are also available. Techniques residing in this statistical domain have the advantage that they can model specific distributions of the target domain if training data is available. Tong and Evans (1996) approach post-correction as a statistical language modeling problem, taking context into account. P´erezCortes et al. (2000) employ stochastic finite-state automaton along with a modified version of the Viterbi Algorithm to perform a stochastic error correcting parsing. Extending the simpler stochastic context-sensitive models, Kolak and 3 https://sites.google.com/view/ icdar2017-postcorrectionocr/home, 3.07.2017. 2717 Resnik (2002) apply the first noisy channel model, using edit distance from noisy to corrected text on character level. In order to train such a model, manually generated train"
D17-1288,reynaert-2008-errors,0,0.15938,"resource is available. There have been attempts towards shared datasets for evaluation. Mihov et al. (2005) released a corpus covering four different kinds of OCRed text comprising German and Bulgarian. However, in 2017 the corpus was untraceable and no recent research relating to the data could be found. OCR post-correction is applied in a diversity of fields in order to compile high-quality datasets. This is not merely reflected in the homogeneity of techniques but in the metric of evaluation as well. While accuracy has been widely used as evaluation measure in OCR post-correction research, Reynaert (2008a) advocates the use of precision and recall in order to improve transparency in evaluations. Dependent on the paradigm of the applied technique even evaluation measures like BLEU score can be found (cf. Afli et al. (2016)). Since shared tasks are a good opportunity to establish certain standards and facilitate the comparability of techniques, the Competition on Post-OCR Text Correction3 organized in the context of ICDAR 2017 could mark a milestone for more unified OCR post-correction research efforts. Regarding techniques used for OCR postcorrection, there are two main trends to be mentioned:"
D17-2012,P17-1148,1,0.914217,"s DependencyGraph ( o b j e c t ) : &quot;&quot;&quot;A container ....for a dependency structure&quot;&quot;&quot; In this paper, we describe Function Assistant, a lightweight Python-based toolkit for querying and exploring source code repositories using natural language. The toolkit is designed to help end-users of a target API quickly find information about functions through high-level natural language queries and descriptions. For a given text query and background API, the tool finds candidate functions by performing a translation from the text to known representations in the API using the semantic parsing approach of Richardson and Kuhn (2017). Translations are automatically learned from example textcode pairs in example APIs. The toolkit includes features for building translation pipelines and query engines for arbitrary source code projects. To explore this last feature, we perform new experiments on 27 well-known Python projects hosted on Github. 1 def remove by address ( s e l f , address ) : &quot;&quot;&quot; Removes the node with the given address. &quot;&quot;&quot; # => implementation def add arc ( s e l f , head address , mod address ) : &quot;&quot;&quot;Adds an arc from the node specified by head_address to the node specified by the mod address.... &quot;&quot;&quot; Figure 1: E"
D17-2012,N06-1056,0,0.171317,"ger code from source code annotations for natural language programming (Allamanis et al., 2015), often focusing narrowly on a specific programming language (e.g., Java) or set of APIs. To our knowledge, none of these approaches include companion software that facilitate building custom pipelines for specific APIs and querying. Technically, our approach is related to work on semantic parsing, which looks at generating formal representations from text input for natural language understanding applications, notably question-answering. Many existing methods take direct inspiration from work on MT (Wong and Mooney, 2006) and parsing (Zettlemoyer and Collins, 2009). Please see RK for more discussion and pointers to related work. 3 Technical Approach In this paper, we focus on learning to generate function representations from textual descriptions inside of source code collections, or APIs. We will refer to these target function representations as API components. Each component specifies a function name, a list of arguments, and other optional information such as a namespace. Given a set of example text-component pairs from an example API, D = {(xi , zi )}ni=1 , the goal 2 github.com/vinta/awesome-python 68 e.g"
D17-2012,P09-1110,0,\N,Missing
D19-6306,W19-7911,1,0.408633,"tgart.de Abstract 2 Our system takes a pipeline approach, which consists of up to five steps to produce the final detokenized text. The steps are: linearization (§2.2), completion (§2.3), inflection (§2.4), contraction (§2.5), and detokenization (§2.6), among which completion is used only in the deep track. All the steps except for the rule-based detokenization use the same Tree-LSTM encoder architecture (§2.1). As the multi-task style training hurt performance in the preliminary experiments, all the steps are trained separately. Since the submission is mostly based on our system described in Yu et al. (2019b), here we mainly focus on the changes introduced for this shared task, and we refer the reader to Yu et al. (2019b) for more details, especially on the explanation and ablation experiments of the TreeLSTM encoder and the linearization decoder. We introduce the IMS contribution to the Surface Realization Shared Task 2019. Our submission achieves the state-of-the-art performance without using any external resources. The system takes a pipeline approach consisting of five steps: linearization, completion, inflection, contraction, and detokenization. We compare the performance of our linearizati"
D19-6306,P17-1183,0,0.0415336,"e input vectors and predicts an output, which could be a symbol 3 to copy the current input character, a symbol 7 to ignore the current input character, or a character from the alphabet to generate a new one. When 3 or 7 is predicted, the input pointer will move one step forward, while if a character is generated, the input pointer does not move. The ground truth of such sequence is calculated from the Levenshtein edit operations between the lemma and the word form, where only insertion and deletion is allowed (no substitution). Our model is in a way similar to the hard monotonic attention in Aharoni and Goldberg (2017), but we use a much simpler source-target alignment (Levenshtein edit operations), and we use copy as an edit operation to avoid completion errors while they do not. Furthermore, our edit operations are associated with the moving of the pointer, while they treat moving the pointer as an atomic operation, which lead to longer prediction sequences. Generally, our model performs on a par with theirs, see the comparison in Yu et al. (2019b). (1) (move pointer) h d(+1) d(+2) $ (2) (new token f1) f1 h d(+1) d(+2) f1 $ (3) f2 h d(+1) f1 (new token f2) d(+2) f2 $ (4) (move pointer) h d(+1) f1 f2 d(+2)"
D19-6306,W19-8636,1,0.201402,"tgart.de Abstract 2 Our system takes a pipeline approach, which consists of up to five steps to produce the final detokenized text. The steps are: linearization (§2.2), completion (§2.3), inflection (§2.4), contraction (§2.5), and detokenization (§2.6), among which completion is used only in the deep track. All the steps except for the rule-based detokenization use the same Tree-LSTM encoder architecture (§2.1). As the multi-task style training hurt performance in the preliminary experiments, all the steps are trained separately. Since the submission is mostly based on our system described in Yu et al. (2019b), here we mainly focus on the changes introduced for this shared task, and we refer the reader to Yu et al. (2019b) for more details, especially on the explanation and ablation experiments of the TreeLSTM encoder and the linearization decoder. We introduce the IMS contribution to the Surface Realization Shared Task 2019. Our submission achieves the state-of-the-art performance without using any external resources. The system takes a pipeline approach consisting of five steps: linearization, completion, inflection, contraction, and detokenization. We compare the performance of our linearizati"
D19-6306,C16-1274,0,0.0667815,"based on its lemma, UPOS, morphological features, and dependency label. We use embeddings for the lemma, UPOS and dependency label, and employ an LSTM to process the list of morphological features.1 We then concatenate all of the obtained vectors as the representation of each token (v◦ ). The representation is further processed by a bidirectional Tree-LSTM to encode the tree structure information. The encoder is generally the same as described in Yu et al. (2019b), consisting of two passes of information: a bottom-up pass followed by a top-down pass. In the bottom-up pass, we use a Tree-LSTM (Zhou et al., 2016) to compose the bottom-up vector of the head from the vectors of the dependents, attended by the 1 There could be better treatment of the morphological features, since they are not sequences in nature. 50 Proceedings of the 2nd Workshop on Multilingual Surface Realisation (MSR 2019), pages 50–58 c Hong Kong, China, November 3rd, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 token-level vector of the head, denoted as v↑ . The bottom-up vectors are then fed into a sequential LSTM for the top-down pass from the root to each leaf token, so that every token ha"
D19-6306,D12-1085,1,0.838767,"Missing"
D19-6306,C10-1012,0,0.705967,"tep (3) another token f2 is created and attached to f1 ; in step (4) the pointer is moved to d(+2) ; in step (5) the pointer is moved again to $, which terminates the process and outputs the sequence [h, d(+1) , f1 , f2 , d(+2) ]. The left and right completion processes are independent of each other, since both forward LSTMs are only aware of the initial linearized tokens on both sides but not the newly generated tokens. We tried several variations in the prelimiLinearization The linearization algorithm is the same as in Yu et al. (2019b), which is in turn based on the linearizer described by Bohnet et al. (2010). The algorithm takes an divide-and-conquer strategy, which orders each subtree (a head and its dependents) individually, and then combines them into a fully linearized tree.2 The main improvement of our algorithm to Bohnet et al. (2010) is that instead of ordering the subtrees from left to right, we start from the head (thus called the head-first decoder), and add the dependents on both sides of the head incrementally. We also train a left-to-right and a right-toleft decoders to form an ensemble with a shared encoder, which is shown in Yu et al. (2019b) to achieve the best performance. We use"
D19-6306,D19-6301,0,0.0978772,"state-of-the-art performance without using any external resources. The system takes a pipeline approach consisting of five steps: linearization, completion, inflection, contraction, and detokenization. We compare the performance of our linearization algorithm with two external baselines and report results for each step in the pipeline. Furthermore, we perform detailed error analysis revealing correlation between word order freedom and difficulty of the linearization task. 1 Surface Realization System Introduction This paper presents our submission to the Surface Realization Shared Task 2019 (Mille et al., 2019). We participate in both shallow and deep track of the shared task, where the shallow track requires the recovery of the linear order and inflection of a dependency tree, and the deep track additionally requires the completion of function words. We approach both tasks with very similar pipelines, consisting of linearizing the unordered dependency trees, completing function words (for the deep track only), inflecting lemmata to word forms, and contracting several words as one token, and finally detokenizing to obtain the natural written text. We use machine learning models for the first four st"
D19-6306,L16-1262,0,0.0429906,"Missing"
D19-6306,N16-1058,0,0.0805804,"Missing"
E12-1009,W06-2922,0,0.0623484,"a set which fits to the elements composed by the dy79 namic programming approach. This is a trade-off between an exhaustive search and a unrestricted (rich) feature set and the question which provides a higher accuracy is still an open research question, cf. (Kuhlmann et al., 2011). Parsing of non-projective dependency trees is an important feature for many languages. At first most algorithms were restricted to projective dependency trees and used pseudo-projective parsing (Kahane et al., 1998; Nivre and Nilsson, 2005). Later, additional transitions were introduced to handle non-projectivity (Attardi, 2006; Nivre, 2009). The most common strategy uses the swap transition (Nivre, 2009; Nivre et al., 2009), an alternative solution uses two planes and a switch transition to switch between the two planes (G´omez-Rodr´ıguez and Nivre, 2010). Since we use the scoring model of a graphbased parser, we briefly review releated work on graph-based parsing. The most well known graph-based parser is the MST (maximum spanning tree) parser, cf. (McDonald et al., 2005; McDonald and Pereira, 2006). The idea of the MST parser is to find the highest scoring tree in a graph that contains all possible edges. Eisner"
E12-1009,W09-1210,1,0.861553,"Missing"
E12-1009,C10-1011,1,0.415151,"the position of the nodes relative to the other nodes of the part and a factor identifier. Training. For the training of our parser, we use a variant of the perceptron algorithm that uses the Passive-Aggressive update function, cf. (Freund and Schapire, 1998; Collins, 2002; Crammer et al., 2006). The Passive-Aggressive perceptron uses an aggressive update strategy by modifying the weight vector by as much as needed to classify correctly the current example, cf. (Crammer et al., 2006). We apply a random function (hash function) to retrieve the weights from the weight vector instead of a table. Bohnet (2010) showed that the Hash Kernel improves parsing speed and accuracy since the parser uses additionaly negative features. Ganchev and Dredze (2008) used 82 this technique for structured prediction in NLP to reduce the needed space, cf. (Shi et al., 2009). We use as weight vector size 800 million. After the training, we counted 65 millions non zero weights for English (penn2malt), 83 for Czech and 87 millions for German. The feature vectors are the union of features originating from the transition sequence of a sentence and the features of the factors over all edges of a dependency tree (e.g. G2a ,"
E12-1009,W08-2102,0,0.0715818,"Missing"
E12-1009,D07-1101,0,0.438734,"et al., 2009), an alternative solution uses two planes and a switch transition to switch between the two planes (G´omez-Rodr´ıguez and Nivre, 2010). Since we use the scoring model of a graphbased parser, we briefly review releated work on graph-based parsing. The most well known graph-based parser is the MST (maximum spanning tree) parser, cf. (McDonald et al., 2005; McDonald and Pereira, 2006). The idea of the MST parser is to find the highest scoring tree in a graph that contains all possible edges. Eisner (1996) introduced a dynamic programming algorithm to solve this problem efficiently. Carreras (2007) introduced the left-most and right-most grandchild as factors. We use the factor model of Carreras (2007) as starting point for our experiments, cf. Section 4. We extend Carreras (2007) graphbased model with factors involving three edges similar to that of Koo and Collins (2010). 3 Transition-based Parser with a Beam This section specifies the transition-based beamsearch parser underlying the combined approach more formally. Sec. 4 will discuss the graphbased scoring model that we are adding. The input to the parser is a word string x, the goal is to find the optimal set y of labeled edges xi"
E12-1009,P04-1015,0,0.430939,"umoto (2003) carried over the idea for deterministic parsing by chunks from Abney (1991) to dependency parsing. Nivre (2003) describes in a more strict sense the first incremental parser that tries to find the most appropriate dependency tree by a sequence of local transitions. In order to optimize the results towards a more globally optimal solution, Johansson and Nugues (2006) first applied beam search, which leads to a substantial improvment of the results (cf. also (Titov and Henderson, 2007)). Zhang and Clark (2008) augment the beam-search algorithm, adapting the early update strategy of Collins and Roark (2004) to dependency parsing. In this approach, the parser stops and updates the model when the oracle transition sequence drops out of the beam. In contrast to most other approaches, the training procedure of Zhang and Clark (2008) takes the complete transition sequence into account as it is calculating the update. Zhang and Clark compare aspects of transition-based and graph-based parsing, and end up using a transition-based parser with a combined transition-based/second-order graph-based scoring model (Zhang and Clark, 2008, 567), which is similar to the approach we describe in this paper. Howeve"
E12-1009,W02-1001,0,0.0950514,"hree-, and four-grams for the first order and second order. The algorithm includes these features only the words left and right do not overlap with the factor (e.g. the head, dependent, etc.). We use feature extraction procedure for second order, and third order factors. Each feature extracted in this procedure includes information about the position of the nodes relative to the other nodes of the part and a factor identifier. Training. For the training of our parser, we use a variant of the perceptron algorithm that uses the Passive-Aggressive update function, cf. (Freund and Schapire, 1998; Collins, 2002; Crammer et al., 2006). The Passive-Aggressive perceptron uses an aggressive update strategy by modifying the weight vector by as much as needed to classify correctly the current example, cf. (Crammer et al., 2006). We apply a random function (hash function) to retrieve the weights from the weight vector instead of a table. Bohnet (2010) showed that the Hash Kernel improves parsing speed and accuracy since the parser uses additionaly negative features. Ganchev and Dredze (2008) used 82 this technique for structured prediction in NLP to reduce the needed space, cf. (Shi et al., 2009). We use a"
E12-1009,C96-1058,0,0.408336,"ed features and show that the completion model leads to a substantial increase in accuracy. We apply the new transition-based parser on typologically different languages such as English, Chinese, Czech, and German and report competitive labeled and unlabeled attachment scores. 1 Introduction Background. A considerable amount of recent research has gone into data-driven dependency parsing, and interestingly throughout the continuous process of improvements, two classes of parsing algorithms have stayed at the centre of attention, the transition-based (Nivre, 2003) vs. the graph-based approach (Eisner, 1996; McDonald et al., 2005).1 The two approaches apply fundamentally different strategies to solve the task of finding the optimal labeled dependency tree over the words of an input sentence (where supervised machine learning is used to estimate the scoring parameters on a treebank). The transition-based approach is based on the conceptually (and cognitively) compelling idea 1 More references will be provided in sec. 2. that machine learning, i.e., a model of linguistic experience, is used in exactly those situations when there is an attachment choice in an otherwise deterministic incremental lef"
E12-1009,W08-0804,0,0.0157914,", we use a variant of the perceptron algorithm that uses the Passive-Aggressive update function, cf. (Freund and Schapire, 1998; Collins, 2002; Crammer et al., 2006). The Passive-Aggressive perceptron uses an aggressive update strategy by modifying the weight vector by as much as needed to classify correctly the current example, cf. (Crammer et al., 2006). We apply a random function (hash function) to retrieve the weights from the weight vector instead of a table. Bohnet (2010) showed that the Hash Kernel improves parsing speed and accuracy since the parser uses additionaly negative features. Ganchev and Dredze (2008) used 82 this technique for structured prediction in NLP to reduce the needed space, cf. (Shi et al., 2009). We use as weight vector size 800 million. After the training, we counted 65 millions non zero weights for English (penn2malt), 83 for Czech and 87 millions for German. The feature vectors are the union of features originating from the transition sequence of a sentence and the features of the factors over all edges of a dependency tree (e.g. G2a , etc.). To prevent over-fitting, we use averaging to cope with this problem, cf. (Freund and Schapire, 1998; Collins, 2002). We calculate the e"
E12-1009,W09-1205,0,0.06999,"Missing"
E12-1009,N10-1115,0,0.0312009,"transition and the beam has to be sorted. The parser sorts the beam when it exceeds the maximal beam size, in order to discard superfluous parses or when the parsing algorithm terminates in order to select the best parse tree. The complexity of the transition-based parser is quadratic due to swap operation in the worse case, which is rare, and O(n) in the best case, cf. (Nivre, 2009). The beam size B is constant. Hence, the complexity is in the worst case O(n2 ). The parsing time is to a large degree determined by the feature extraction, the score calculation and the implementation, cf. also (Goldberg and Elhadad, 2010). The transition-based parser is able to parse 30 sentences per second. The parser with completion model processes about 5 sentences per second with a beam size of 80. Note, we use a rich feature set, a completion model with third order factors, negative features, and a large beam. 3 We implemented the following optimizations: (1) We use a parallel feature extraction for the beam elements. Each process extracts the features, scores the possible transitions and computes the score of the completion model. After the extension step, the beam is sorted and the best elements are selected according t"
E12-1009,P10-1151,0,0.0272703,"Missing"
E12-1009,W09-1201,0,0.0368525,"Missing"
E12-1009,P10-1110,0,0.656844,"th a combined transition-based/second-order graph-based scoring model (Zhang and Clark, 2008, 567), which is similar to the approach we describe in this paper. However, their approach does not involve beam rescoring as the partial structures built by the transition-based parser are subsequently augmented; hence, there are cases in which our approach is able to differentiate based on higher-order factors that go unnoticed by the combined model of (Zhang and Clark, 2008, 567). One step beyond the use of a beam is a dynamic programming approach to carry out a full search in the state space, cf. (Huang and Sagae, 2010; Kuhlmann et al., 2011). However, in this case one has to restrict the employed features to a set which fits to the elements composed by the dy79 namic programming approach. This is a trade-off between an exhaustive search and a unrestricted (rich) feature set and the question which provides a higher accuracy is still an open research question, cf. (Kuhlmann et al., 2011). Parsing of non-projective dependency trees is an important feature for many languages. At first most algorithms were restricted to projective dependency trees and used pseudo-projective parsing (Kahane et al., 1998; Nivre a"
E12-1009,W06-2930,0,0.248217,"issue and solution strategy. In order to preserve the conceptual (and complexity) advantages of the transition-based strategy, the integrated algorithm we are looking for has to be transition-based at the top level. The advantages of the graph-based approach – a more globally informed basis for the decision among different attachment options – have to be included as part of the scoring procedure. As a prerequisite, our algorithm will require a memory for storing alternative analyses among which to choose. This has been previously introduced in transitionbased approaches in the form of a beam (Johansson and Nugues, 2006): rather than representing only the best-scoring history of transitions, the k best-scoring alternative histories are kept around. As we will indicate in the following, the mere addition of beam search does not help overcome a representational key issue of transition-based parsing: in many situations, a transition-based parser is forced to make an attachment decision for a given input word at a point where no or only partial information about the word’s own dependents (and further decendents) is available. Figure 1 illustrates such a case. Figure 1: The left set of brackets indicates material"
E12-1009,W08-2123,0,0.0286038,"Missing"
E12-1009,P98-1106,0,0.115036,", cf. (Huang and Sagae, 2010; Kuhlmann et al., 2011). However, in this case one has to restrict the employed features to a set which fits to the elements composed by the dy79 namic programming approach. This is a trade-off between an exhaustive search and a unrestricted (rich) feature set and the question which provides a higher accuracy is still an open research question, cf. (Kuhlmann et al., 2011). Parsing of non-projective dependency trees is an important feature for many languages. At first most algorithms were restricted to projective dependency trees and used pseudo-projective parsing (Kahane et al., 1998; Nivre and Nilsson, 2005). Later, additional transitions were introduced to handle non-projectivity (Attardi, 2006; Nivre, 2009). The most common strategy uses the swap transition (Nivre, 2009; Nivre et al., 2009), an alternative solution uses two planes and a switch transition to switch between the two planes (G´omez-Rodr´ıguez and Nivre, 2010). Since we use the scoring model of a graphbased parser, we briefly review releated work on graph-based parsing. The most well known graph-based parser is the MST (maximum spanning tree) parser, cf. (McDonald et al., 2005; McDonald and Pereira, 2006)."
E12-1009,P10-1001,0,0.195548,"l known graph-based parser is the MST (maximum spanning tree) parser, cf. (McDonald et al., 2005; McDonald and Pereira, 2006). The idea of the MST parser is to find the highest scoring tree in a graph that contains all possible edges. Eisner (1996) introduced a dynamic programming algorithm to solve this problem efficiently. Carreras (2007) introduced the left-most and right-most grandchild as factors. We use the factor model of Carreras (2007) as starting point for our experiments, cf. Section 4. We extend Carreras (2007) graphbased model with factors involving three edges similar to that of Koo and Collins (2010). 3 Transition-based Parser with a Beam This section specifies the transition-based beamsearch parser underlying the combined approach more formally. Sec. 4 will discuss the graphbased scoring model that we are adding. The input to the parser is a word string x, the goal is to find the optimal set y of labeled edges xi →l xj forming a dependency tree over x ∪{root}. We characterize the state of a transitionbased parser as πi =hσi , βi , yi , hi i, πi ∈ Π, the set of possible states. σi is a stack of words from x that are still under consideration; βi is the input buffer, the suffix of x yet to"
E12-1009,P08-1068,0,0.0443972,"Missing"
E12-1009,W02-2016,0,0.0284448,"drops out of the beam before complete information becomes available. But as our experiments show, this does not seem to be a serious issue empirically. parser requires only one training phase (without jackknifing) and it uses only a single transitionbased decoder. The structure of this paper is as follows. In Section 2, we discuss related work. In Section 3, we introduce our transition-based parser and in Section 4 the completion model as well as the implementation of third order models. In Section 5, we describe experiments and provide evaluation results on selected data sets. 2 Related Work Kudo and Matsumoto (2002) and Yamada and Matsumoto (2003) carried over the idea for deterministic parsing by chunks from Abney (1991) to dependency parsing. Nivre (2003) describes in a more strict sense the first incremental parser that tries to find the most appropriate dependency tree by a sequence of local transitions. In order to optimize the results towards a more globally optimal solution, Johansson and Nugues (2006) first applied beam search, which leads to a substantial improvment of the results (cf. also (Titov and Henderson, 2007)). Zhang and Clark (2008) augment the beam-search algorithm, adapting the early"
E12-1009,P11-1068,0,0.0308158,"Missing"
E12-1009,D10-1004,0,0.0466354,"Missing"
E12-1009,E06-1011,0,0.45361,"parsing (Kahane et al., 1998; Nivre and Nilsson, 2005). Later, additional transitions were introduced to handle non-projectivity (Attardi, 2006; Nivre, 2009). The most common strategy uses the swap transition (Nivre, 2009; Nivre et al., 2009), an alternative solution uses two planes and a switch transition to switch between the two planes (G´omez-Rodr´ıguez and Nivre, 2010). Since we use the scoring model of a graphbased parser, we briefly review releated work on graph-based parsing. The most well known graph-based parser is the MST (maximum spanning tree) parser, cf. (McDonald et al., 2005; McDonald and Pereira, 2006). The idea of the MST parser is to find the highest scoring tree in a graph that contains all possible edges. Eisner (1996) introduced a dynamic programming algorithm to solve this problem efficiently. Carreras (2007) introduced the left-most and right-most grandchild as factors. We use the factor model of Carreras (2007) as starting point for our experiments, cf. Section 4. We extend Carreras (2007) graphbased model with factors involving three edges similar to that of Koo and Collins (2010). 3 Transition-based Parser with a Beam This section specifies the transition-based beamsearch parser u"
E12-1009,P05-1012,0,0.726541,"d show that the completion model leads to a substantial increase in accuracy. We apply the new transition-based parser on typologically different languages such as English, Chinese, Czech, and German and report competitive labeled and unlabeled attachment scores. 1 Introduction Background. A considerable amount of recent research has gone into data-driven dependency parsing, and interestingly throughout the continuous process of improvements, two classes of parsing algorithms have stayed at the centre of attention, the transition-based (Nivre, 2003) vs. the graph-based approach (Eisner, 1996; McDonald et al., 2005).1 The two approaches apply fundamentally different strategies to solve the task of finding the optimal labeled dependency tree over the words of an input sentence (where supervised machine learning is used to estimate the scoring parameters on a treebank). The transition-based approach is based on the conceptually (and cognitively) compelling idea 1 More references will be provided in sec. 2. that machine learning, i.e., a model of linguistic experience, is used in exactly those situations when there is an attachment choice in an otherwise deterministic incremental left-to-right parsing proce"
E12-1009,P08-1108,0,0.06918,"of larger spans are more cumbersome to integrate into the model (since the combination algorithm has to be adjusted), in particular for third-order factors or higher. Empirically, i.e., when applied in supervised machine learning experiments based on existing treebanks for various languages, both strategies (and further refinements of them not mentioned here) turn out roughly equal in their capability of picking up most of the relevant patterns well; some subtle strengths and weaknesses are complementary, such that stacking of two parsers representing both strategies yields the best results (Nivre and McDonald, 2008): in training and application, one of the parsers is run on each sentence prior to the other, providing additional feature information for the other parser. Another successful technique to combine parsers is voting as carried out by Sagae and Lavie (2006). The present paper addresses the question if and how a more integrated combination of the strengths of the two strategies can be achieved and implemented efficiently to warrant competitive results. The main issue and solution strategy. In order to preserve the conceptual (and complexity) advantages of the transition-based strategy, the integr"
E12-1009,P05-1013,0,0.122173,"e, 2010; Kuhlmann et al., 2011). However, in this case one has to restrict the employed features to a set which fits to the elements composed by the dy79 namic programming approach. This is a trade-off between an exhaustive search and a unrestricted (rich) feature set and the question which provides a higher accuracy is still an open research question, cf. (Kuhlmann et al., 2011). Parsing of non-projective dependency trees is an important feature for many languages. At first most algorithms were restricted to projective dependency trees and used pseudo-projective parsing (Kahane et al., 1998; Nivre and Nilsson, 2005). Later, additional transitions were introduced to handle non-projectivity (Attardi, 2006; Nivre, 2009). The most common strategy uses the swap transition (Nivre, 2009; Nivre et al., 2009), an alternative solution uses two planes and a switch transition to switch between the two planes (G´omez-Rodr´ıguez and Nivre, 2010). Since we use the scoring model of a graphbased parser, we briefly review releated work on graph-based parsing. The most well known graph-based parser is the MST (maximum spanning tree) parser, cf. (McDonald et al., 2005; McDonald and Pereira, 2006). The idea of the MST parser"
E12-1009,W09-3811,0,0.03112,"rade-off between an exhaustive search and a unrestricted (rich) feature set and the question which provides a higher accuracy is still an open research question, cf. (Kuhlmann et al., 2011). Parsing of non-projective dependency trees is an important feature for many languages. At first most algorithms were restricted to projective dependency trees and used pseudo-projective parsing (Kahane et al., 1998; Nivre and Nilsson, 2005). Later, additional transitions were introduced to handle non-projectivity (Attardi, 2006; Nivre, 2009). The most common strategy uses the swap transition (Nivre, 2009; Nivre et al., 2009), an alternative solution uses two planes and a switch transition to switch between the two planes (G´omez-Rodr´ıguez and Nivre, 2010). Since we use the scoring model of a graphbased parser, we briefly review releated work on graph-based parsing. The most well known graph-based parser is the MST (maximum spanning tree) parser, cf. (McDonald et al., 2005; McDonald and Pereira, 2006). The idea of the MST parser is to find the highest scoring tree in a graph that contains all possible edges. Eisner (1996) introduced a dynamic programming algorithm to solve this problem efficiently. Carreras (2007"
E12-1009,W03-3017,0,0.506576,"tion that allows for the use of sophisticated features and show that the completion model leads to a substantial increase in accuracy. We apply the new transition-based parser on typologically different languages such as English, Chinese, Czech, and German and report competitive labeled and unlabeled attachment scores. 1 Introduction Background. A considerable amount of recent research has gone into data-driven dependency parsing, and interestingly throughout the continuous process of improvements, two classes of parsing algorithms have stayed at the centre of attention, the transition-based (Nivre, 2003) vs. the graph-based approach (Eisner, 1996; McDonald et al., 2005).1 The two approaches apply fundamentally different strategies to solve the task of finding the optimal labeled dependency tree over the words of an input sentence (where supervised machine learning is used to estimate the scoring parameters on a treebank). The transition-based approach is based on the conceptually (and cognitively) compelling idea 1 More references will be provided in sec. 2. that machine learning, i.e., a model of linguistic experience, is used in exactly those situations when there is an attachment choice in"
E12-1009,P09-1040,0,0.475175,"s to the elements composed by the dy79 namic programming approach. This is a trade-off between an exhaustive search and a unrestricted (rich) feature set and the question which provides a higher accuracy is still an open research question, cf. (Kuhlmann et al., 2011). Parsing of non-projective dependency trees is an important feature for many languages. At first most algorithms were restricted to projective dependency trees and used pseudo-projective parsing (Kahane et al., 1998; Nivre and Nilsson, 2005). Later, additional transitions were introduced to handle non-projectivity (Attardi, 2006; Nivre, 2009). The most common strategy uses the swap transition (Nivre, 2009; Nivre et al., 2009), an alternative solution uses two planes and a switch transition to switch between the two planes (G´omez-Rodr´ıguez and Nivre, 2010). Since we use the scoring model of a graphbased parser, we briefly review releated work on graph-based parsing. The most well known graph-based parser is the MST (maximum spanning tree) parser, cf. (McDonald et al., 2005; McDonald and Pereira, 2006). The idea of the MST parser is to find the highest scoring tree in a graph that contains all possible edges. Eisner (1996) introdu"
E12-1009,N06-2033,0,0.0409521,"ng treebanks for various languages, both strategies (and further refinements of them not mentioned here) turn out roughly equal in their capability of picking up most of the relevant patterns well; some subtle strengths and weaknesses are complementary, such that stacking of two parsers representing both strategies yields the best results (Nivre and McDonald, 2008): in training and application, one of the parsers is run on each sentence prior to the other, providing additional feature information for the other parser. Another successful technique to combine parsers is voting as carried out by Sagae and Lavie (2006). The present paper addresses the question if and how a more integrated combination of the strengths of the two strategies can be achieved and implemented efficiently to warrant competitive results. The main issue and solution strategy. In order to preserve the conceptual (and complexity) advantages of the transition-based strategy, the integrated algorithm we are looking for has to be transition-based at the top level. The advantages of the graph-based approach – a more globally informed basis for the decision among different attachment options – have to be included as part of the scoring pro"
E12-1009,D09-1058,0,0.0832753,"Missing"
E12-1009,W07-2218,0,0.100726,"xperiments and provide evaluation results on selected data sets. 2 Related Work Kudo and Matsumoto (2002) and Yamada and Matsumoto (2003) carried over the idea for deterministic parsing by chunks from Abney (1991) to dependency parsing. Nivre (2003) describes in a more strict sense the first incremental parser that tries to find the most appropriate dependency tree by a sequence of local transitions. In order to optimize the results towards a more globally optimal solution, Johansson and Nugues (2006) first applied beam search, which leads to a substantial improvment of the results (cf. also (Titov and Henderson, 2007)). Zhang and Clark (2008) augment the beam-search algorithm, adapting the early update strategy of Collins and Roark (2004) to dependency parsing. In this approach, the parser stops and updates the model when the oracle transition sequence drops out of the beam. In contrast to most other approaches, the training procedure of Zhang and Clark (2008) takes the complete transition sequence into account as it is calculating the update. Zhang and Clark compare aspects of transition-based and graph-based parsing, and end up using a transition-based parser with a combined transition-based/second-order"
E12-1009,W03-3023,0,0.815496,"omplete information becomes available. But as our experiments show, this does not seem to be a serious issue empirically. parser requires only one training phase (without jackknifing) and it uses only a single transitionbased decoder. The structure of this paper is as follows. In Section 2, we discuss related work. In Section 3, we introduce our transition-based parser and in Section 4 the completion model as well as the implementation of third order models. In Section 5, we describe experiments and provide evaluation results on selected data sets. 2 Related Work Kudo and Matsumoto (2002) and Yamada and Matsumoto (2003) carried over the idea for deterministic parsing by chunks from Abney (1991) to dependency parsing. Nivre (2003) describes in a more strict sense the first incremental parser that tries to find the most appropriate dependency tree by a sequence of local transitions. In order to optimize the results towards a more globally optimal solution, Johansson and Nugues (2006) first applied beam search, which leads to a substantial improvment of the results (cf. also (Titov and Henderson, 2007)). Zhang and Clark (2008) augment the beam-search algorithm, adapting the early update strategy of Collins and"
E12-1009,D08-1059,0,0.846364,"tion results on selected data sets. 2 Related Work Kudo and Matsumoto (2002) and Yamada and Matsumoto (2003) carried over the idea for deterministic parsing by chunks from Abney (1991) to dependency parsing. Nivre (2003) describes in a more strict sense the first incremental parser that tries to find the most appropriate dependency tree by a sequence of local transitions. In order to optimize the results towards a more globally optimal solution, Johansson and Nugues (2006) first applied beam search, which leads to a substantial improvment of the results (cf. also (Titov and Henderson, 2007)). Zhang and Clark (2008) augment the beam-search algorithm, adapting the early update strategy of Collins and Roark (2004) to dependency parsing. In this approach, the parser stops and updates the model when the oracle transition sequence drops out of the beam. In contrast to most other approaches, the training procedure of Zhang and Clark (2008) takes the complete transition sequence into account as it is calculating the update. Zhang and Clark compare aspects of transition-based and graph-based parsing, and end up using a transition-based parser with a combined transition-based/second-order graph-based scoring mode"
E12-1009,P11-2033,0,0.688368,"n, w is the corresponding weight vector. In order to add the factor of Figure 4 to our model, we have to add the scoring function (2a) the sum: (2b) scoreG2b (x, y) = scoreG2a (x, y) P + (h,c,cmi)∈y w · fgra (x,h,c,cmi) In order to build a scoring function for combination of the factors shown in Figure 5 to 7, we have to add to the equation 2b one or more of the following sums: (3a) P (3b) P w · fgra (x,h,c,cm1,cm2) (3c) P w · fgra (x,h,c,cmo,tmo) (h,c,ch1,ch2)∈y (h,c,cm1,cm2)∈y (h,c,cmo,tmo)∈y w · fgra (x,h,c,ch1,ch2) Feature Set. The feature set of the transition model is similar to that of Zhang and Nivre (2011). In addition, we use the cross product of morphologic features between the head and the dependent since we apply also the parser on morphologic rich languages. The feature sets of the completion model described above are mostly based on previous work (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). The models denoted with + use all combinations of words before and after the head, dependent, sibling, grandchilrden, etc. These are respectively three-, and four-grams for the first order and second order. The algorithm includes these features only the wo"
E12-1009,C98-1102,0,\N,Missing
E12-1078,J08-1001,0,0.0278889,"on German surface realisation has highlighted the role of the initial position in the German sentence, the so-called Vorfeld (or “prefield”). Filippova and Strube (2007) show that once the Vorfeld (i.e. the constituent that precedes the finite verb) is correctly determined, the prediction of the order in the Mittelfeld (i.e. the constituents that follow the finite verb) is very easy. Cheung and Penn (2010) extend the approach of Filippova and Strube (2007) and augment a sentence-internal constituent ordering model with sentence-external features inspired from the entity grid model proposed by Barzilay and Lapata (2008). 2 3 Related Work In the generation literature, most works on exploiting sentence-external discourse information are set in a summarisation or content ordering framework. Barzilay and Lee (2004) propose an account for constraints on topic selection based on probabilistic content models. Barzilay and Lapata (2008) propose an entity grid model which represents the distribution of referents in a discourse for sentence ordering. Karamanis et al. (2009) use Centering-based metrics to assess coherence in an information ordering system. Clarke and LaMotivation While there would be many ways to const"
E12-1078,N04-1015,0,0.0247948,"i.e. the constituent that precedes the finite verb) is correctly determined, the prediction of the order in the Mittelfeld (i.e. the constituents that follow the finite verb) is very easy. Cheung and Penn (2010) extend the approach of Filippova and Strube (2007) and augment a sentence-internal constituent ordering model with sentence-external features inspired from the entity grid model proposed by Barzilay and Lapata (2008). 2 3 Related Work In the generation literature, most works on exploiting sentence-external discourse information are set in a summarisation or content ordering framework. Barzilay and Lee (2004) propose an account for constraints on topic selection based on probabilistic content models. Barzilay and Lapata (2008) propose an entity grid model which represents the distribution of referents in a discourse for sentence ordering. Karamanis et al. (2009) use Centering-based metrics to assess coherence in an information ordering system. Clarke and LaMotivation While there would be many ways to construe or represent discourse context (e.g. in terms of the global discourse or information structure), we concentrate on capturing local coherence through the distribution of discourse referents in"
E12-1078,E06-1040,0,0.021211,"gests that sentenceinternal realisation implicitly carries a lot of imformation about discourse context. On average, the morphosyntactic properties of constituents in a text are better approximates of their discourse status than actual coreference relations. This result feeds into a number of research questions concerning the representation of discourse and its application in generation systems. Although we should certainly not expect a computational model to achieve a perfect accuracy in the constituent ordering task – even humans only agree to a certain extent in rating word order variants (Belz and Reiter, 2006; Cahill, 2009) – the average accuracy in the 60’s for prediction of Vorfeld occupance is still moderate. An obvious direction would be to further investigate more complex representations of discourse that take into account the relations between utterances, such as topic shifts. Moreover, it is not clear whether the effects we find for linearisation in this paper carry over to other levels of generation such as tactical generation where syntactic functions are not fully specified. In a broader perspective, our results underline the need for better formalisations of discourse that can be transl"
E12-1078,W10-1834,0,0.0221106,"Missing"
E12-1078,P09-1092,1,0.943535,"actual lexical choice, including functional categories such as determiners, is fully fixed (which is of course not always the case), one can take advantage of 767 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 767–776, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics these reflexes. This explains in part the fairly high baseline performance of n-gram language models in the surface realization task. And the effect can indeed be taken much further: the discriminative training experiments of Cahill and Riester (2009) show how effective it is to systematically take advantage of asymmetry patterns in the morphosyntactic reflexes of the discourse notion of information status (i.e., using a feature set with well-chosen purely sentence-bound features). These observations give rise to the question: in the light of the difficulty in obtaining reliable discourse information on the one hand and the effectiveness of exploiting the reflexes of discourse in the sentence-internal material on the other – can we nevertheless expect to gain something from adding sentence-external feature information? We propose two scena"
E12-1078,W07-2303,1,0.935476,"pata (2010) have improved a sentence compression system by capturing prominence of phrases or referents in terms of lexical chain information inspired by Morris and Hirst (1991) and Centering (Grosz et al., 1995). In their system, discourse context is represented in terms of hard constraints modelling whether a certain constituent can be deleted or not. In the linearisation or surface realisation domain, there is a considerable body of work approximating information structure in terms of sentence-internal realisation (Ringger et al., 2004; Filippova and Strube, 2009; Velldal and Oepen, 2005; Cahill et al., 2007). Cahill and Riester (2009) improve realisation ranking for German – which mainly deals with word order variation – by representing precedence patterns of constituents in terms of asymmetries in their morphosyntactic properties. As a simple example, a pattern exploited by Cahill and Riester (2009) is the tendency of definite elements tend to precede indefinites, which, on a discourse level, reflects that given entities in a sentence tend to precede new entities. Other work on German surface realisation has highlighted the role of the initial position in the German sentence, the so-called Vorfe"
E12-1078,P09-2025,1,0.85901,"rnal realisation implicitly carries a lot of imformation about discourse context. On average, the morphosyntactic properties of constituents in a text are better approximates of their discourse status than actual coreference relations. This result feeds into a number of research questions concerning the representation of discourse and its application in generation systems. Although we should certainly not expect a computational model to achieve a perfect accuracy in the constituent ordering task – even humans only agree to a certain extent in rating word order variants (Belz and Reiter, 2006; Cahill, 2009) – the average accuracy in the 60’s for prediction of Vorfeld occupance is still moderate. An obvious direction would be to further investigate more complex representations of discourse that take into account the relations between utterances, such as topic shifts. Moreover, it is not clear whether the effects we find for linearisation in this paper carry over to other levels of generation such as tactical generation where syntactic functions are not fully specified. In a broader perspective, our results underline the need for better formalisations of discourse that can be translated into featu"
E12-1078,P10-1020,0,0.0149304,"ited by Cahill and Riester (2009) is the tendency of definite elements tend to precede indefinites, which, on a discourse level, reflects that given entities in a sentence tend to precede new entities. Other work on German surface realisation has highlighted the role of the initial position in the German sentence, the so-called Vorfeld (or “prefield”). Filippova and Strube (2007) show that once the Vorfeld (i.e. the constituent that precedes the finite verb) is correctly determined, the prediction of the order in the Mittelfeld (i.e. the constituents that follow the finite verb) is very easy. Cheung and Penn (2010) extend the approach of Filippova and Strube (2007) and augment a sentence-internal constituent ordering model with sentence-external features inspired from the entity grid model proposed by Barzilay and Lapata (2008). 2 3 Related Work In the generation literature, most works on exploiting sentence-external discourse information are set in a summarisation or content ordering framework. Barzilay and Lee (2004) propose an account for constraints on topic selection based on probabilistic content models. Barzilay and Lapata (2008) propose an entity grid model which represents the distribution of r"
E12-1078,J10-3005,0,0.0482,"Missing"
E12-1078,N09-2057,0,0.0249541,"btained from coreference and lexical chain relations. pata (2010) have improved a sentence compression system by capturing prominence of phrases or referents in terms of lexical chain information inspired by Morris and Hirst (1991) and Centering (Grosz et al., 1995). In their system, discourse context is represented in terms of hard constraints modelling whether a certain constituent can be deleted or not. In the linearisation or surface realisation domain, there is a considerable body of work approximating information structure in terms of sentence-internal realisation (Ringger et al., 2004; Filippova and Strube, 2009; Velldal and Oepen, 2005; Cahill et al., 2007). Cahill and Riester (2009) improve realisation ranking for German – which mainly deals with word order variation – by representing precedence patterns of constituents in terms of asymmetries in their morphosyntactic properties. As a simple example, a pattern exploited by Cahill and Riester (2009) is the tendency of definite elements tend to precede indefinites, which, on a discourse level, reflects that given entities in a sentence tend to precede new entities. Other work on German surface realisation has highlighted the role of the initial posit"
E12-1078,J95-2003,0,0.940218,"ference relations. As we get the same effect in both setups – the sentenceexternal features do not improve over a baseline that captures basic morphosyntactic properties of the constituents – we conclude that sentenceinternal realisation is actually a relatively accurate predictor of discourse context, even more accurate than information that can be obtained from coreference and lexical chain relations. pata (2010) have improved a sentence compression system by capturing prominence of phrases or referents in terms of lexical chain information inspired by Morris and Hirst (1991) and Centering (Grosz et al., 1995). In their system, discourse context is represented in terms of hard constraints modelling whether a certain constituent can be deleted or not. In the linearisation or surface realisation domain, there is a considerable body of work approximating information structure in terms of sentence-internal realisation (Ringger et al., 2004; Filippova and Strube, 2009; Velldal and Oepen, 2005; Cahill et al., 2007). Cahill and Riester (2009) improve realisation ranking for German – which mainly deals with word order variation – by representing precedence patterns of constituents in terms of asymmetries i"
E12-1078,J09-1003,0,0.017767,"007) and augment a sentence-internal constituent ordering model with sentence-external features inspired from the entity grid model proposed by Barzilay and Lapata (2008). 2 3 Related Work In the generation literature, most works on exploiting sentence-external discourse information are set in a summarisation or content ordering framework. Barzilay and Lee (2004) propose an account for constraints on topic selection based on probabilistic content models. Barzilay and Lapata (2008) propose an entity grid model which represents the distribution of referents in a discourse for sentence ordering. Karamanis et al. (2009) use Centering-based metrics to assess coherence in an information ordering system. Clarke and LaMotivation While there would be many ways to construe or represent discourse context (e.g. in terms of the global discourse or information structure), we concentrate on capturing local coherence through the distribution of discourse referents in a text. These discourse referents basically correspond to the constituents that our surface realisation model has to put in the right order. As the order of referents or constituents is arguably influenced by the information structure of a sentence given th"
E12-1078,W05-0311,0,0.0198734,"nd Zinsmeister, 2009) have made some good progress towards a coherenceoriented account of at least the left edge of the German clause structure, the Vorfeld constituent. What makes the technological application of theoretical insights even harder is that for most relevant factors, automatic recognition cannot be performed with high accuracy (e.g., a coreference accuracy in the 70’s means there is a good deal of noise) and for the higher-level notions such as the information-structural focus, interannotator agreement on real corpus data tends to be much lower than for core-grammatical notions (Poesio and Artstein, 2005; Ritz et al., 2008). On the other hand, many of the relevant discourse factors are reflected indirectly in properties of the sentence-internal material. Most notably, knowing the shape of referring expressions narrows down many aspects of givenness and salience of its referent; pronominal realizations indicate givenness, and in German there are even two variants of the personal pronoun (er and der) for distinguishing salience. So, if the generation task is set in such a way that the actual lexical choice, including functional categories such as determiners, is fully fixed (which is of course"
E12-1078,J04-3003,0,0.0644019,"Missing"
E12-1078,C04-1097,0,0.267231,"ormation that can be obtained from coreference and lexical chain relations. pata (2010) have improved a sentence compression system by capturing prominence of phrases or referents in terms of lexical chain information inspired by Morris and Hirst (1991) and Centering (Grosz et al., 1995). In their system, discourse context is represented in terms of hard constraints modelling whether a certain constituent can be deleted or not. In the linearisation or surface realisation domain, there is a considerable body of work approximating information structure in terms of sentence-internal realisation (Ringger et al., 2004; Filippova and Strube, 2009; Velldal and Oepen, 2005; Cahill et al., 2007). Cahill and Riester (2009) improve realisation ranking for German – which mainly deals with word order variation – by representing precedence patterns of constituents in terms of asymmetries in their morphosyntactic properties. As a simple example, a pattern exploited by Cahill and Riester (2009) is the tendency of definite elements tend to precede indefinites, which, on a discourse level, reflects that given entities in a sentence tend to precede new entities. Other work on German surface realisation has highlighted t"
E12-1078,ritz-etal-2008-annotation,0,0.025282,"Missing"
E12-1078,rohrer-forst-2006-improving,0,0.0276056,"nt that investigates sentence-external context in a surface realisation task. The sentence-external context is represented in terms of lexical chain features and compared to sentence-internal models which are based on morphosyntactic features. The experiment thus targets a generation scenario where no coreference information is available and aims at assessing whether relatively naive context information is also useful. 4.1 System Description We carry out our first experiment in a regeneration set-up with two components: a) a largescale hand-crafted Lexical Functional Grammar (LFG) for German (Rohrer and Forst, 2006), used to parse and regenerate a corpus sentence, b) a stochastic ranker that selects the most appropriate regenerated sentence in context according to an underlying, linguistically motivated feature model. In contrast to fully statistical linearisation methods, our system first generates the full set of sentences that correspond to the grammatically well-formed realisations of the intermediate syntactic representation.1 This representation is an f-structure, which underspecifies the order of constituents and, to some extent, their morphological realisation, such that the output sentences cont"
E12-1078,2005.mtsummit-papers.15,0,0.019652,"lexical chain relations. pata (2010) have improved a sentence compression system by capturing prominence of phrases or referents in terms of lexical chain information inspired by Morris and Hirst (1991) and Centering (Grosz et al., 1995). In their system, discourse context is represented in terms of hard constraints modelling whether a certain constituent can be deleted or not. In the linearisation or surface realisation domain, there is a considerable body of work approximating information structure in terms of sentence-internal realisation (Ringger et al., 2004; Filippova and Strube, 2009; Velldal and Oepen, 2005; Cahill et al., 2007). Cahill and Riester (2009) improve realisation ranking for German – which mainly deals with word order variation – by representing precedence patterns of constituents in terms of asymmetries in their morphosyntactic properties. As a simple example, a pattern exploited by Cahill and Riester (2009) is the tendency of definite elements tend to precede indefinites, which, on a discourse level, reflects that given entities in a sentence tend to precede new entities. Other work on German surface realisation has highlighted the role of the initial position in the German sentenc"
E12-1078,J91-1002,0,\N,Missing
E14-2015,agarwal-etal-2012-gui,0,0.0343763,"Missing"
E14-2015,ambati-etal-2010-high,0,0.0699051,"Missing"
E14-2015,E03-1068,0,0.661013,"Missing"
E14-2015,P13-4010,1,0.431846,"Missing"
E14-2015,seeker-kuhn-2012-making,1,0.859235,"Missing"
ghayoomi-kuhn-2014-converting,C10-1026,0,\N,Missing
ghayoomi-kuhn-2014-converting,candito-etal-2010-statistical,0,\N,Missing
ghayoomi-kuhn-2014-converting,nivre-etal-2006-maltparser,0,\N,Missing
ghayoomi-kuhn-2014-converting,J93-2004,0,\N,Missing
ghayoomi-kuhn-2014-converting,W12-5205,0,\N,Missing
ghayoomi-kuhn-2014-converting,W09-1210,0,\N,Missing
ghayoomi-kuhn-2014-converting,C10-2162,0,\N,Missing
ghayoomi-kuhn-2014-converting,W09-0806,0,\N,Missing
ghayoomi-kuhn-2014-converting,W08-1301,0,\N,Missing
ghayoomi-kuhn-2014-converting,P05-2013,0,\N,Missing
ghayoomi-kuhn-2014-converting,N13-1031,0,\N,Missing
ghayoomi-kuhn-2014-converting,rambow-etal-2002-dependency,0,\N,Missing
ghayoomi-kuhn-2014-converting,W98-0141,0,\N,Missing
glaser-kuhn-2014-exploring,N04-1002,0,\N,Missing
glaser-kuhn-2014-exploring,E06-1002,0,\N,Missing
glaser-kuhn-2014-exploring,N03-1033,0,\N,Missing
glaser-kuhn-2014-exploring,D08-1029,0,\N,Missing
glaser-kuhn-2014-exploring,W11-1902,0,\N,Missing
glaser-kuhn-2014-exploring,J13-4004,0,\N,Missing
glaser-kuhn-2014-exploring,W03-0405,0,\N,Missing
glaser-kuhn-2014-exploring,N13-1071,0,\N,Missing
glaser-kuhn-2014-exploring,D07-1074,0,\N,Missing
glaser-kuhn-2014-exploring,D10-1048,0,\N,Missing
glaser-kuhn-2014-exploring,W04-0701,0,\N,Missing
glaser-kuhn-2014-exploring,P05-1045,0,\N,Missing
J13-1004,W06-2920,0,0.0156525,"ogical paradigms, in our case Czech and German (e. g., for neuter nouns, nominative and accusative case have the same surface form). In contrast, due to its mostly unambiguous case system, we find a much smaller effect for Hungarian. Although the parser itself profits much from morphological information as our experiments with gold standard morphology show, errors in automatically predicted morphological information frequently cause errors in the syntactic analysis. 1 Compare, for example, the various Shared Tasks on parsing multiple languages, such as the CoNLL Shared Tasks 2006, 2007, 2009 (Buchholz and Marsi 2006; Nivre et al. 2007a; Hajiˇc et al. 2009), or the PaGe ¨ Shared Task on parsing German (Kubler 2008). 2 Two or more different feature values are signaled by the same form. 24 Seeker and Kuhn Morphological and Syntactic Case in Statistical Dependency Parsing In order to better handle syncretism in the morphological description, we then propose a different way of integrating morphology into the parsing process. We develop an alternative architecture that circumvents the strict separation of morphological and syntactic analysis in the pipeline model. We adopt the integer linear programming (hence"
J13-1004,D07-1101,0,0.10434,"es separated by a vertical bar. The parser itself consists of two main modules, the decoder and the feature model. It is a maximum-spanning-tree5 parser (McDonald et al. 2005; McDonald and Pereira 2006) that searches for the best-scoring tree using a chart-based dynamic programming approach similar to the one proposed by Eisner (1997). The substructures are scored by a statistical feature model that has been trained on treebank data; the best-scoring tree is the tree with the highest sum over the scores of all substructures in the tree. The actual implementation is derived from the decoder by Carreras (2007), which was shown to be efficient even for very rich feature models (Carreras 2007; Johansson and Nugues 2008; Bohnet 2009). The features used in the statistical model are combinations of basic features, namely, word form, lemma, POS, and morphological features. In addition, the distance between two nodes, the direction of the edge, and the words between head and dependent are included. Every feature is combined with the function label on the edge. A detailed description of the feature model is beyond the scope of this article, but the interested reader can find it in Bohnet (2009, 2010). Beca"
J13-1004,D07-1022,0,0.126504,"Missing"
J13-1004,P99-1065,0,0.0863118,"Missing"
J13-1004,W04-1903,0,0.0119856,"v´ a et al. 2000; Hajiˇc et al. 2006). The German data set (Seeker and Kuhn 2012) is a semi-automatically corrected recreation of the data set that was used in the CoNLL 2009 Shared Task (36,017 sentences). It uses the exact6 same raw (surface) data but contains a different syntactic annotation. It was semi-automatically derived from the original TIGER treebank (Brants et al. 2002) and some time was spent on manually correcting incorrect function labels and POS tags. The Hungarian data consist of the general newspaper subcorpus (10,188 sentences) of the Szeged Treebank ´ (Csendes, Csirik, and Gyimothy 2004), which was converted from the original constituent structure annotation to dependency annotation and manually checked by four trained linguists (Vincze et al. 2010). For the experiments in the following sections, we use the training splits for Czech and German, and the whole set for Hungarian. For the Czech and the Hungarian data, we kept the predicted information for lemmata, POS, and morphology that was already provided with the data. For both 6 Except for three sentences that for some reason were missing in the 2006 version of the TiGer treebank, from which this corpus was derived. The ori"
J13-1004,1997.iwpt-1.10,0,0.238347,", predicted function label. Semantic information is not displayed. The gold standard columns are used for evaluation purposes. human-annotated gold standard and the predicted value by an automatic tool is represented. The morphology columns contain several morphological features separated by a vertical bar. The parser itself consists of two main modules, the decoder and the feature model. It is a maximum-spanning-tree5 parser (McDonald et al. 2005; McDonald and Pereira 2006) that searches for the best-scoring tree using a chart-based dynamic programming approach similar to the one proposed by Eisner (1997). The substructures are scored by a statistical feature model that has been trained on treebank data; the best-scoring tree is the tree with the highest sum over the scores of all substructures in the tree. The actual implementation is derived from the decoder by Carreras (2007), which was shown to be efficient even for very rich feature models (Carreras 2007; Johansson and Nugues 2008; Bohnet 2009). The features used in the statistical model are combinations of basic features, namely, word form, lemma, POS, and morphological features. In addition, the distance between two nodes, the direction"
J13-1004,J08-3003,0,0.0121532,"Missing"
J13-1004,P06-1041,0,0.0256585,"cal model still use the same predicted and fully disambiguated morphological information from the pipeline architecture as the Bohnet parser. As we saw in the first experiment, using no morphological information in the statistical model is very harmful to the performance on Czech and Hungarian, though not so much for German. One advantage of the proposed architecture is the fact that the ILP parser is still mainly driven by the statistical model. Krivanek and Meurers (2011) compared a datadriven, transition-based dependency parser (Nivre et al. 2007b) and a constraint-based dependency parser (Foth and Menzel 2006) on learner and newspaper corpora and found that whereas the former is better on modifier functions (e.g., PP-attachment), the latter performs better on argument functions. Their explanation is that where the data-driven parser has access to lots of data and can pick up statistical effects in the data like semantic or selectional preferences, the constraint-based parser has access to deep lexical and grammatical information and is thus able to model argument structure in a better way. In the ILP parser, we can combine both strengths, letting the statistical model learn preferences but forcing"
J13-1004,W09-1205,0,0.0422141,"Missing"
J13-1004,W10-1412,0,0.201064,"Missing"
J13-1004,P08-1043,0,0.052872,"on is predicted as a preprocessing step to the actual parsing. Although Goldberg and Elhadad (2010) and Marton, Habash, and Rambow (2010) find improvements for hand-annotated (gold) morphological features, automatically predicted morphological information has none or even negative effects on their parsing models. Goldberg and Elhadad (2010) also show that linguistically grounded, carefully designed features (here agreement between adjectives and nouns in Hebrew) can contribute a considerable improvement, however. Finally, the pipeline approach itself can be questioned. Cohen and Smith (2007), Goldberg and Tsarfaty (2008), and Lee, Naradowsky, and Smith (2011) present joint models where the processes of predicting morphological information and syntactic information are performed at the same time. All three approaches acknowledge the fact that syntax and morphology are heavily intertwined and interact with each other. Our attempt at tackling the technical and linguistic challenges can be characterized as follows: In Section 6, we propose a system architecture that at the basic level follows a pipeline approach, where local data-driven models are used to predict the highest scoring output in each step. But this"
J13-1004,W09-1201,0,0.0188074,"Missing"
J13-1004,W08-2123,0,0.111765,"feature model. It is a maximum-spanning-tree5 parser (McDonald et al. 2005; McDonald and Pereira 2006) that searches for the best-scoring tree using a chart-based dynamic programming approach similar to the one proposed by Eisner (1997). The substructures are scored by a statistical feature model that has been trained on treebank data; the best-scoring tree is the tree with the highest sum over the scores of all substructures in the tree. The actual implementation is derived from the decoder by Carreras (2007), which was shown to be efficient even for very rich feature models (Carreras 2007; Johansson and Nugues 2008; Bohnet 2009). The features used in the statistical model are combinations of basic features, namely, word form, lemma, POS, and morphological features. In addition, the distance between two nodes, the direction of the edge, and the words between head and dependent are included. Every feature is combined with the function label on the edge. A detailed description of the feature model is beyond the scope of this article, but the interested reader can find it in Bohnet (2009, 2010). Because we are interested in the way the parser handles morphological information, we will briefly discuss the in"
J13-1004,W09-3816,0,0.0253093,"Missing"
J13-1004,P07-2051,0,0.0736356,"Missing"
J13-1004,P11-1089,0,0.112575,"Missing"
J13-1004,P09-1039,0,0.00892267,"Missing"
J13-1004,W10-1402,0,0.0465328,"Missing"
J13-1004,E06-1011,0,0.0845869,", predicted lemma, gold POS, predicted POS, gold morphology, predicted morphology, gold head position, predicted head position, gold function label, predicted function label. Semantic information is not displayed. The gold standard columns are used for evaluation purposes. human-annotated gold standard and the predicted value by an automatic tool is represented. The morphology columns contain several morphological features separated by a vertical bar. The parser itself consists of two main modules, the decoder and the feature model. It is a maximum-spanning-tree5 parser (McDonald et al. 2005; McDonald and Pereira 2006) that searches for the best-scoring tree using a chart-based dynamic programming approach similar to the one proposed by Eisner (1997). The substructures are scored by a statistical feature model that has been trained on treebank data; the best-scoring tree is the tree with the highest sum over the scores of all substructures in the tree. The actual implementation is derived from the decoder by Carreras (2007), which was shown to be efficient even for very rich feature models (Carreras 2007; Johansson and Nugues 2008; Bohnet 2009). The features used in the statistical model are combinations of"
J13-1004,H05-1066,0,0.0325737,"Missing"
J13-1004,W06-1616,0,0.019228,"ical specification of the words. The parser then needs to choose a different option. In order to implement the constrained parser, we use a parsing approach by Martins, Smith, and Xing (2009) using integer linear programming. It is related to the Bohnet parser in the sense that it is also a graph-based approach, but it allows us to elegantly augment the basic decoder with linguistically motivated constraints 43 Computational Linguistics Volume 39, Number 1 (Klenner 2007; Seeker et al. 2010). ILP is a mathematical tool for optimizing linear functions and was first used in dependency parsing by Riedel and Clarke (2006), who performed experiments on Dutch using linguistically motivated constraints as we will do. Martins, Smith, and Xing improved the formulation considerably so that the parser would output well-formed dependency trees without the need for iterative solving. In our ILP parser, we use the formulation by Martins, Smith, and Xing extended to labeled dependency parsing. Like the Bohnet parser, the ILP parser consists of a decoder and a statistical feature model. Whereas the feature model remains basically the same, the decoder is implemented using ILP. The formulation represents every possible arc"
J13-1004,C04-1056,0,0.0266917,"f technical challenges often go hand in hand, as a change in architecture effectively means a change in the interface representations, and vice versa. Collins et al. (1999) reduce the tag set for the Czech treebank, which consists of a combination of POS tags and a detailed morphological specification, in order to tackle data sparseness. A combination of POS and case features turns out to be best for their parsing models. In statistical constituent parsing, many investigations devise treebank transformations that allow the parsing models to access morphological information higher in the tree (Schiehlen 2004; Versley 2005; Versley and Rehbein 2009). These transformations apply category splits by decorating category symbols with morphological information like case. Whereas these approaches change traditional models to cope with morphological information, others approach the problem by devising new models tailored to the special requirements of morphologically rich languages. Tsarfaty and Sima’an (2008, 2010) introduce an additional layer into the parsing process that directly models the subcategorization of a non-terminal symbol without taking word order into consideration. The parser thus separat"
J13-1004,seeker-kuhn-2012-making,1,0.0821034,"nce between the two fusional languages and the agglutinating language and directly reflect this typological difference. We will thus show that whereas the morphological preprocessing for Czech and German makes mistakes because of the syncretism in the morphological paradigms, the morphological preprocessing for Hungarian suffers from a different problem. All the data sets come from the newspaper domain. The Czech data set is the CoNLL 2009 Shared Task data set consisting of 38,727 sentences from the Prague ¨ Dependency Treebank (Bohmov´ a et al. 2000; Hajiˇc et al. 2006). The German data set (Seeker and Kuhn 2012) is a semi-automatically corrected recreation of the data set that was used in the CoNLL 2009 Shared Task (36,017 sentences). It uses the exact6 same raw (surface) data but contains a different syntactic annotation. It was semi-automatically derived from the original TIGER treebank (Brants et al. 2002) and some time was spent on manually correcting incorrect function labels and POS tags. The Hungarian data consist of the general newspaper subcorpus (10,188 sentences) of the Szeged Treebank ´ (Csendes, Csirik, and Gyimothy 2004), which was converted from the original constituent structure annot"
J13-1004,W11-2908,1,0.927835,"Missing"
J13-1004,P10-1111,1,0.89407,"Missing"
J13-1004,E09-1087,0,0.0415708,"Missing"
J13-1004,tron-etal-2006-morphdb,0,0.169171,"Missing"
J13-1004,W10-1401,0,0.341952,"Missing"
J13-1004,C08-1112,0,0.186944,"Missing"
J13-1004,W10-1405,0,0.240971,"Missing"
J13-1004,W09-3820,0,0.0926442,"o hand in hand, as a change in architecture effectively means a change in the interface representations, and vice versa. Collins et al. (1999) reduce the tag set for the Czech treebank, which consists of a combination of POS tags and a detailed morphological specification, in order to tackle data sparseness. A combination of POS and case features turns out to be best for their parsing models. In statistical constituent parsing, many investigations devise treebank transformations that allow the parsing models to access morphological information higher in the tree (Schiehlen 2004; Versley 2005; Versley and Rehbein 2009). These transformations apply category splits by decorating category symbols with morphological information like case. Whereas these approaches change traditional models to cope with morphological information, others approach the problem by devising new models tailored to the special requirements of morphologically rich languages. Tsarfaty and Sima’an (2008, 2010) introduce an additional layer into the parsing process that directly models the subcategorization of a non-terminal symbol without taking word order into consideration. The parser thus separates the functional subcategorization of a"
J13-1004,vincze-etal-2010-hungarian,0,0.062977,"Missing"
J13-1004,W09-1210,0,\N,Missing
J13-1004,W08-1008,0,\N,Missing
J13-1004,J08-4010,0,\N,Missing
J13-1004,D07-1096,0,\N,Missing
J13-1004,R09-1033,0,\N,Missing
K17-3004,P13-2107,0,0.0237642,"Missing"
K17-3004,P13-2109,0,0.0166876,"LassySmall since we found that this lead to better sentence segmentation results on the development sets. 2.3 Part-of-Speech and Morphological Tagging We used MarMoT to jointly predict POS tags and morphological features. We annotated the training sets via 5-fold jackknifing. All parsers for all languages except the surprise ones were trained on jackknifed features. We did not use XPOS tags and lemmas. We used MarMoT with default hyperparameters. 2.4 3 Baseline parsers Surdeanu and Manning (2010) show that combining a set of parsers with a simple voting scheme can improve parsing performance. Martins et al. (2013) demonstrate that self-application, i.e., stacking a parser on its own output, only leads to minuscule improvements.2 Therefore to profit from combining components one of the most significant factor is their diversity. Thus we experimented with three parsers with quite different arSupertags Supertags (Joshi and Bangalore, 1994) are labels for tokens which encode syntactic information, e.g., the head direction or the subcategorization frame. Supertagging has recently been proposed to provide syntactic information to the feature model of statistical dependency parsers (Ambati et al. (2013; 2014)"
K17-3004,W15-2210,1,0.928805,"Missing"
K17-3004,E06-1011,0,0.0402545,"settings for beam size (20) and number of training epochs (also 20). Similarly to GP, we employ different seeds for the random number generator used during shuffling of the training instances in order to obtain multiple different models. Graph-based perceptron parser As the graph-based parser we used mate3 (Bohnet, 2010), henceforth referred to as GP. This is a state-of-the-art graph- and perceptron-based parser. The parser uses the Carreras (2007) extension of the Eisner (1997) decoding algorithm to build a projective parse tree. It then applies the non-projective approximation algorithm of McDonald and Pereira (2006) to recover nonprojective dependencies. We train the parser using the default number of training epochs (10). We modified the publicly available sources of this parser in two ways. First, we extended the feature set with features based on the supertags following Fale´nska et al. (2015). Second, we changed the perceptron implementation to shuffle the training instances between epochs.4 Shuffling enables us to obtain different instances of the parser trained with different random seeds, which are used in the blending step. Since the time complexity of the Carreras (2007) decoder is quite high (O"
K17-3004,C10-1011,0,0.0214778,"In fact, even supertagging can be regarded as a form of stacking. Also in this case, the key ingredient is that the suppertagger is architecturally sufficiently different from the parser (Fale´nska et al., 2015). 42 chitectures and additionally varied their settings. 3.1 We use the default settings for beam size (20) and number of training epochs (also 20). Similarly to GP, we employ different seeds for the random number generator used during shuffling of the training instances in order to obtain multiple different models. Graph-based perceptron parser As the graph-based parser we used mate3 (Bohnet, 2010), henceforth referred to as GP. This is a state-of-the-art graph- and perceptron-based parser. The parser uses the Carreras (2007) extension of the Eisner (1997) decoding algorithm to build a projective parse tree. It then applies the non-projective approximation algorithm of McDonald and Pereira (2006) to recover nonprojective dependencies. We train the parser using the default number of training epochs (10). We modified the publicly available sources of this parser in two ways. First, we extended the feature set with features based on the supertags following Fale´nska et al. (2015). Second,"
K17-3004,D13-1032,0,0.0760477,"Missing"
K17-3004,D07-1101,0,0.0473344,"is architecturally sufficiently different from the parser (Fale´nska et al., 2015). 42 chitectures and additionally varied their settings. 3.1 We use the default settings for beam size (20) and number of training epochs (also 20). Similarly to GP, we employ different seeds for the random number generator used during shuffling of the training instances in order to obtain multiple different models. Graph-based perceptron parser As the graph-based parser we used mate3 (Bohnet, 2010), henceforth referred to as GP. This is a state-of-the-art graph- and perceptron-based parser. The parser uses the Carreras (2007) extension of the Eisner (1997) decoding algorithm to build a projective parse tree. It then applies the non-projective approximation algorithm of McDonald and Pereira (2006) to recover nonprojective dependencies. We train the parser using the default number of training epochs (10). We modified the publicly available sources of this parser in two ways. First, we extended the feature set with features based on the supertags following Fale´nska et al. (2015). Second, we changed the perceptron implementation to shuffle the training instances between epochs.4 Shuffling enables us to obtain differe"
K17-3004,P09-1040,0,0.0608383,"nces further split into two groups: two parse from left to right (TP-l2r) and two parse from right to left (TP-r2l). The four TN instances differ not only in the parsing direction, but also in the word embeddings, two use pretrained embeddings Transition-based beam-perceptron parser We apply an in-house transition-based beam search parser trained with the perceptron (Bj¨orkelund and Nivre, 2015), henceforth referred to as TP.6 We have previously extended this parser to accommodate features from supertags (Fale´nska et al., 2015). It uses the ArcStandard system extended with a Swap transition (Nivre, 2009) and is trained using the improved oracle by Nivre et al. (2009). The parser is trained with a globally optimized structured perceptron (Zhang and Clark, 2008) using max-violation updates (Huang et al., 2012). 3 http://code.google.com/p/mate-tools The publicly available version does not shuffle. 5 For the baseline results on the development sets (Tables 3 and 4), the parser was applied to all sentences. 6 This parser as well as the variant that we applied for sentence segmentation (TPSeg) is available on the first author’s website. 4 7 This parser as well as the neural tagger used for supertag"
K17-3004,1997.iwpt-1.10,0,0.452706,"different from the parser (Fale´nska et al., 2015). 42 chitectures and additionally varied their settings. 3.1 We use the default settings for beam size (20) and number of training epochs (also 20). Similarly to GP, we employ different seeds for the random number generator used during shuffling of the training instances in order to obtain multiple different models. Graph-based perceptron parser As the graph-based parser we used mate3 (Bohnet, 2010), henceforth referred to as GP. This is a state-of-the-art graph- and perceptron-based parser. The parser uses the Carreras (2007) extension of the Eisner (1997) decoding algorithm to build a projective parse tree. It then applies the non-projective approximation algorithm of McDonald and Pereira (2006) to recover nonprojective dependencies. We train the parser using the default number of training epochs (10). We modified the publicly available sources of this parser in two ways. First, we extended the feature set with features based on the supertags following Fale´nska et al. (2015). Second, we changed the perceptron implementation to shuffle the training instances between epochs.4 Shuffling enables us to obtain different instances of the parser trai"
K17-3004,W09-3811,0,0.0242323,"o right (TP-l2r) and two parse from right to left (TP-r2l). The four TN instances differ not only in the parsing direction, but also in the word embeddings, two use pretrained embeddings Transition-based beam-perceptron parser We apply an in-house transition-based beam search parser trained with the perceptron (Bj¨orkelund and Nivre, 2015), henceforth referred to as TP.6 We have previously extended this parser to accommodate features from supertags (Fale´nska et al., 2015). It uses the ArcStandard system extended with a Swap transition (Nivre, 2009) and is trained using the improved oracle by Nivre et al. (2009). The parser is trained with a globally optimized structured perceptron (Zhang and Clark, 2008) using max-violation updates (Huang et al., 2012). 3 http://code.google.com/p/mate-tools The publicly available version does not shuffle. 5 For the baseline results on the development sets (Tables 3 and 4), the parser was applied to all sentences. 6 This parser as well as the variant that we applied for sentence segmentation (TPSeg) is available on the first author’s website. 4 7 This parser as well as the neural tagger used for supertagging (TAG NN) is available on the third author’s website. 43 Usi"
K17-3004,W15-2215,1,0.846371,"Missing"
K17-3004,E14-4030,0,0.127624,"to its components. Concretely with the French example, we take 1 41 http://cistern.cis.lmu.de/marmot/ Preprocessing UDPipe Words, Sentences langs1 langs2 CRF UDPipe Words Sentences UDPipe TPSeg Words Sentences CRF TPSeg Words Sentences surprise UDPipe UPOS, Feats CRF TagNN UPOS, STags PREPROCESSED default Parsing GP TP Blend-Opt OUT TN Feats langs3 Figure 1: System architecture, where langs1 : he, ja, fr, fr sequoia, fr partut, fr pud, vi, zh; langs2 : cu, en, et, got, grc proiel, la, la ittb, la proiel, nl lassysmall, sl sst; langs3 : ar, ar pud. We follow the definition of supertagging from Ouchi et al. (2014) and extract supertag tag sets from the treebanks. We use their Model 1 to design our supertags. That is, we encode the dependency relation (label), the relative head direction (hdir) and the presence of left and right dependents (hasLdep, hasRdep) and follow the template label/hdir+hasLdep hasRdep. We used an in-house neural-based tagger (TAG NN) to predict the supertags (Yu et al., 2017). It takes the context of a word within a window size of 15. The input word representations are concatenations of three components: output of a character-based Convolutional Neural Network (CNN), pretrained w"
K17-3004,P15-2040,0,0.0290492,"Missing"
K17-3004,N06-2033,0,0.227525,"{anders,falenska,xiangyu,jonas}@ims.uni-stuttgart.de Abstract 2014) with a neural tagger. Additionally, we performed our own word and/or sentence segmentation on a subset of the test sets. For the parsing step we applied an ensemble approach using three different parsers, sometimes using multiple instances of the same parser: one graph-based parser trained with the perceptron; one transition-based beam search parser also trained with the perceptron; and one greedy transition-based parser trained with neural networks. The parser outputs were combined through blending (also known as reparsing; Sagae and Lavie, 2006) using the Chu-Liu-Edmonds algorithm (Chu and Liu, 1965; Edmonds, 1967). The final test runs were carried out on the TIRA platform (Potthast et al., 2014) where participants were assigned a virtual machine. To ensure that our final test run would finish on time on the VM, we established a time budget for each treebank and set a goal that a full test run should finish within 24 hours. Thus we applied a combination search under a time constraint to limit the maximal number of instances of the individual parsers. An interesting aspect of the ST was the introduction of four surprise languages. The"
K17-3004,I08-3008,0,0.0501808,"Missing"
K17-3004,L16-1680,0,0.0500934,"Missing"
K17-3004,P16-1147,0,0.0776238,"Missing"
K17-3004,D08-1059,0,0.0326841,"only in the parsing direction, but also in the word embeddings, two use pretrained embeddings Transition-based beam-perceptron parser We apply an in-house transition-based beam search parser trained with the perceptron (Bj¨orkelund and Nivre, 2015), henceforth referred to as TP.6 We have previously extended this parser to accommodate features from supertags (Fale´nska et al., 2015). It uses the ArcStandard system extended with a Swap transition (Nivre, 2009) and is trained using the improved oracle by Nivre et al. (2009). The parser is trained with a globally optimized structured perceptron (Zhang and Clark, 2008) using max-violation updates (Huang et al., 2012). 3 http://code.google.com/p/mate-tools The publicly available version does not shuffle. 5 For the baseline results on the development sets (Tables 3 and 4), the parser was applied to all sentences. 6 This parser as well as the variant that we applied for sentence segmentation (TPSeg) is available on the first author’s website. 4 7 This parser as well as the neural tagger used for supertagging (TAG NN) is available on the third author’s website. 43 Using the TPSeg parser to predict sentence boundaries results in an average improvement of 9.32 po"
K17-3004,P17-2106,1,0.795112,"parser trained with different random seeds, which are used in the blending step. Since the time complexity of the Carreras (2007) decoder is quite high (O(n4 )) this parser required a considerable amount of time to parse long sentences. Therefore, while applying this parser in the blending scenario, we skipped all sentences longer than 50 tokens.5 We additionally made sure that for each treebank we had at least one parser that was not GP, so that all sentences would be parsed. 3.2 3.3 Transition-based greedy neural parser We use an in-house transition-based greedy parser with neural networks (Yu and Vu, 2017), henceforth referred to as TN.7 The parser uses a CNN to compose word representations from characters, it also takes the embeddings of word forms, universal POS tags and supertags and concatenates all of them as input features. The input is then fed into two hidden layers with ReLU non-linearity, and finally predicts the transition with a softmax layer. The parser uses the same Swap transition system and oracle as TP. We use the default hyperparameters during training and testing. During training the parser additionally predicts the supertag of the top token in the stack and includes the tagg"
kessler-kuhn-2014-corpus,C08-1031,0,\N,Missing
kessler-kuhn-2014-corpus,P11-1164,0,\N,Missing
kessler-kuhn-2014-corpus,D13-1194,1,\N,Missing
kessler-kuhn-2014-corpus,P08-1031,0,\N,Missing
kuhn-mateo-toledo-2004-applying,E99-1010,0,\N,Missing
L16-1024,S10-1022,0,0.0424313,"compound head match - F3: GermaNet - F4: Distributional information - F5: Animacy: gender match names - F6: Animacy: person match Evaluation On the newest dataset available (T¨uBa-D/Z, version 10), our resolver currently achieves a CoNLL score of 65.765 . Table 1 compares the performance of our system using gold annotations with our system trained on predicted annotations (Section 7. lists the tools involved). 5 compares our scores with the three best performing systems in the shared task, BART (Broscheit et al., 2010a; Broscheit et al., 2010b), SUCRE (Kobdani and Sch¨utze, 2010) and TANL-1 (Attardi et al., 2010) as well as with CorZu (Klenner and Tuggener, 2011; Tuggener and Klenner, 2014).7 The CoNLL scores for all systems have been computed using the official CoNLL scorer v8.01 and the system outputs provided on the SemEval webpage. The scores differ from those published on the SemEval website due to the newer, improved scorer script and because we did not include singletons in the evaluation. CoNLL 65.76 63.80* 65.03* 65.72 65.32** 65.76 65.59** 65.58** Table 3: Performance of IMS HotCoref DE on T¨uBa-D/Z version 10: ablation experiments Table 3 shows the results when leaving out one of the previo"
L16-1024,P06-1005,0,0.0201218,"Missing"
L16-1024,P14-1005,1,0.886336,"Missing"
L16-1024,D12-1133,0,0.0529307,"Missing"
L16-1024,S10-1021,0,0.144014,"orming version. IMS HotCoref DE Best performing version - lemma-based - F1: gender agreement - F2: compound head match - F3: GermaNet - F4: Distributional information - F5: Animacy: gender match names - F6: Animacy: person match Evaluation On the newest dataset available (T¨uBa-D/Z, version 10), our resolver currently achieves a CoNLL score of 65.765 . Table 1 compares the performance of our system using gold annotations with our system trained on predicted annotations (Section 7. lists the tools involved). 5 compares our scores with the three best performing systems in the shared task, BART (Broscheit et al., 2010a; Broscheit et al., 2010b), SUCRE (Kobdani and Sch¨utze, 2010) and TANL-1 (Attardi et al., 2010) as well as with CorZu (Klenner and Tuggener, 2011; Tuggener and Klenner, 2014).7 The CoNLL scores for all systems have been computed using the official CoNLL scorer v8.01 and the system outputs provided on the SemEval webpage. The scores differ from those published on the SemEval website due to the newer, improved scorer script and because we did not include singletons in the evaluation. CoNLL 65.76 63.80* 65.03* 65.72 65.32** 65.76 65.59** 65.58** Table 3: Performance of IMS HotCoref DE on T¨uBa-"
L16-1024,broscheit-etal-2010-extending,0,0.165505,"orming version. IMS HotCoref DE Best performing version - lemma-based - F1: gender agreement - F2: compound head match - F3: GermaNet - F4: Distributional information - F5: Animacy: gender match names - F6: Animacy: person match Evaluation On the newest dataset available (T¨uBa-D/Z, version 10), our resolver currently achieves a CoNLL score of 65.765 . Table 1 compares the performance of our system using gold annotations with our system trained on predicted annotations (Section 7. lists the tools involved). 5 compares our scores with the three best performing systems in the shared task, BART (Broscheit et al., 2010a; Broscheit et al., 2010b), SUCRE (Kobdani and Sch¨utze, 2010) and TANL-1 (Attardi et al., 2010) as well as with CorZu (Klenner and Tuggener, 2011; Tuggener and Klenner, 2014).7 The CoNLL scores for all systems have been computed using the official CoNLL scorer v8.01 and the system outputs provided on the SemEval webpage. The scores differ from those published on the SemEval website due to the newer, improved scorer script and because we did not include singletons in the evaluation. CoNLL 65.76 63.80* 65.03* 65.72 65.32** 65.76 65.59** 65.58** Table 3: Performance of IMS HotCoref DE on T¨uBa-"
L16-1024,P15-1136,0,0.0181868,"able for download. Keywords: Co-reference Resolution, German, Adaptation, Tool 1. Introduction Noun phrase co-reference resolution is the task of determining which noun phrases (NPs) in a text or dialogue refer to the same discourse entities (Ng, 2010). Coreference resolution has been extensively addressed in NLP research, e.g. in the CoNLL shared task 2012 and 2011 (Pradhan et al., 2012; Pradhan et al., 2011) or in the SemEval shared task 2010 (Recasens et al., 2010). A lot of research focuses on English co-reference, resulting in a number of high performing English coreference systems, e.g. Clark and Manning (2015), Durrett and Klein (2014) or Bj¨orkelund and Kuhn (2014). However, there has been less work on German coreference resolution. Since the SemEval shared task 2010, only a few systems have been improved or developed, such as the rule-based CorZu system (Klenner and Tuggener, 2011; Tuggener and Klenner, 2014) or Krug et al. (2015)’s system which is tailored to the domain of historic novels. This paper presents a data-driven co-reference resolution system that is based on the English IMS HotCoref system (Bj¨orkelund and Kuhn, 2014). It describes the adaptation process, the specific requirements fo"
L16-1024,P13-4006,0,0.0154263,"eate our list of noun-noun pairs and their similarity values. To get the similarity values, we built a vector space from the SdeWaC corpus (Faaß and Eckart, 2013), part-of-speech tagged and lemmatised using TreeTagger (Schmid, 1994). From the corpus, we extracted lemmatised sentences and trained a CBOW model (Mikolov et al., 2013). This model builds distributed word vectors by learning to predict the current word based on a context. We use lemma-POS pairs as both target and context elements, 300 dimensions, negative sampling set to 15, and no hierarchical softmax. We used the DISSECT toolkit (Dinu et al., 2013) to compute the cosine similarity scores between all nouns of the corpus. F5/F6: Animacy and name information Three knowledge sources have been integrated that are taken from Klenner and Tuggener (2011): a list of words which refer to people, e.g. Politiker (politician) or Mutti (Mummy), a list of names which refer to females, e.g. Laura, Anne, and a list of names which refer to males, e.g. Michael, Thomas, etc. We use this information in two features: The first feature, called person match, is true if the anaphor is a masculine or feminine pronoun and the antecedent is on the people list. It"
L16-1024,Q14-1037,0,0.0127333,"s: Co-reference Resolution, German, Adaptation, Tool 1. Introduction Noun phrase co-reference resolution is the task of determining which noun phrases (NPs) in a text or dialogue refer to the same discourse entities (Ng, 2010). Coreference resolution has been extensively addressed in NLP research, e.g. in the CoNLL shared task 2012 and 2011 (Pradhan et al., 2012; Pradhan et al., 2011) or in the SemEval shared task 2010 (Recasens et al., 2010). A lot of research focuses on English co-reference, resulting in a number of high performing English coreference systems, e.g. Clark and Manning (2015), Durrett and Klein (2014) or Bj¨orkelund and Kuhn (2014). However, there has been less work on German coreference resolution. Since the SemEval shared task 2010, only a few systems have been improved or developed, such as the rule-based CorZu system (Klenner and Tuggener, 2011; Tuggener and Klenner, 2014) or Krug et al. (2015)’s system which is tailored to the domain of historic novels. This paper presents a data-driven co-reference resolution system that is based on the English IMS HotCoref system (Bj¨orkelund and Kuhn, 2014). It describes the adaptation process, the specific requirements for co-reference resolution"
L16-1024,R11-1025,0,0.577063,"has been extensively addressed in NLP research, e.g. in the CoNLL shared task 2012 and 2011 (Pradhan et al., 2012; Pradhan et al., 2011) or in the SemEval shared task 2010 (Recasens et al., 2010). A lot of research focuses on English co-reference, resulting in a number of high performing English coreference systems, e.g. Clark and Manning (2015), Durrett and Klein (2014) or Bj¨orkelund and Kuhn (2014). However, there has been less work on German coreference resolution. Since the SemEval shared task 2010, only a few systems have been improved or developed, such as the rule-based CorZu system (Klenner and Tuggener, 2011; Tuggener and Klenner, 2014) or Krug et al. (2015)’s system which is tailored to the domain of historic novels. This paper presents a data-driven co-reference resolution system that is based on the English IMS HotCoref system (Bj¨orkelund and Kuhn, 2014). It describes the adaptation process, the specific requirements for co-reference resolution in German text as well as the tool that is freely available for download1 . 2. Noun Phrase Coreference Resolution Coreferent links exist between two NPs if the first NP refers back to a discourse entity that has already been introduced in the discourse"
L16-1024,S10-1018,0,0.03582,"Missing"
L16-1024,W15-0711,0,0.51385,"e CoNLL shared task 2012 and 2011 (Pradhan et al., 2012; Pradhan et al., 2011) or in the SemEval shared task 2010 (Recasens et al., 2010). A lot of research focuses on English co-reference, resulting in a number of high performing English coreference systems, e.g. Clark and Manning (2015), Durrett and Klein (2014) or Bj¨orkelund and Kuhn (2014). However, there has been less work on German coreference resolution. Since the SemEval shared task 2010, only a few systems have been improved or developed, such as the rule-based CorZu system (Klenner and Tuggener, 2011; Tuggener and Klenner, 2014) or Krug et al. (2015)’s system which is tailored to the domain of historic novels. This paper presents a data-driven co-reference resolution system that is based on the English IMS HotCoref system (Bj¨orkelund and Kuhn, 2014). It describes the adaptation process, the specific requirements for co-reference resolution in German text as well as the tool that is freely available for download1 . 2. Noun Phrase Coreference Resolution Coreferent links exist between two NPs if the first NP refers back to a discourse entity that has already been introduced in the discourse and is thereby known to the reader. The referring"
L16-1024,P10-1142,0,0.0149622,"dataset T¨uBa-D/Z and include a post-task SemEval 2010 evaluation, showing that the resolver achieves state-of-the-art performance. We also include ablation experiments that indicate that integrating linguistic features increases results. The paper also describes the steps and the format necessary to use the resolver on new texts. The tool is freely available for download. Keywords: Co-reference Resolution, German, Adaptation, Tool 1. Introduction Noun phrase co-reference resolution is the task of determining which noun phrases (NPs) in a text or dialogue refer to the same discourse entities (Ng, 2010). Coreference resolution has been extensively addressed in NLP research, e.g. in the CoNLL shared task 2012 and 2011 (Pradhan et al., 2012; Pradhan et al., 2011) or in the SemEval shared task 2010 (Recasens et al., 2010). A lot of research focuses on English co-reference, resulting in a number of high performing English coreference systems, e.g. Clark and Manning (2015), Durrett and Klein (2014) or Bj¨orkelund and Kuhn (2014). However, there has been less work on German coreference resolution. Since the SemEval shared task 2010, only a few systems have been improved or developed, such as the r"
L16-1024,P06-1055,0,0.00729599,"s as well as a set of named entity labels are extracted. Note that most parsers for German do not annotate NPs inside PPs, i.e. they are flat, so these need to be inserted before running the tool. The tool works best on new texts if the same tools are used with which the training corpus has been preprocessed. There are two models available: one trained on the gold annotations (this one is preferable if you can find a way to create similar annotations to the T¨uBa gold annotations for your own texts.). We have also uploaded a model trained on predicted annotations: we used the Berkeley parser (Petrov et al., 2006) (out of the box, standard models trained on Tiger) to create the parses, the Stanford NER system for German (Faruqui and Pad´o, 2010) to find named entities and mate10 to lemmatise, tag part-of-speech and produce the morphological information. Two example documents for the annotations are provided on the webpage. Format The tool takes input in CoNLL-12 format. The CoNLL-12 format is a standardised, tab-separated format in a one-word-per-line setup. Table 1 shows the information contained in the respective columns. An example document can be found on the webpage. Column 1 2 3 4 5 6 7 8 9 10 11"
L16-1024,W11-1901,0,0.0465187,"blation experiments that indicate that integrating linguistic features increases results. The paper also describes the steps and the format necessary to use the resolver on new texts. The tool is freely available for download. Keywords: Co-reference Resolution, German, Adaptation, Tool 1. Introduction Noun phrase co-reference resolution is the task of determining which noun phrases (NPs) in a text or dialogue refer to the same discourse entities (Ng, 2010). Coreference resolution has been extensively addressed in NLP research, e.g. in the CoNLL shared task 2012 and 2011 (Pradhan et al., 2012; Pradhan et al., 2011) or in the SemEval shared task 2010 (Recasens et al., 2010). A lot of research focuses on English co-reference, resulting in a number of high performing English coreference systems, e.g. Clark and Manning (2015), Durrett and Klein (2014) or Bj¨orkelund and Kuhn (2014). However, there has been less work on German coreference resolution. Since the SemEval shared task 2010, only a few systems have been improved or developed, such as the rule-based CorZu system (Klenner and Tuggener, 2011; Tuggener and Klenner, 2014) or Krug et al. (2015)’s system which is tailored to the domain of historic novels"
L16-1024,W12-4501,0,0.138625,"nce. We also include ablation experiments that indicate that integrating linguistic features increases results. The paper also describes the steps and the format necessary to use the resolver on new texts. The tool is freely available for download. Keywords: Co-reference Resolution, German, Adaptation, Tool 1. Introduction Noun phrase co-reference resolution is the task of determining which noun phrases (NPs) in a text or dialogue refer to the same discourse entities (Ng, 2010). Coreference resolution has been extensively addressed in NLP research, e.g. in the CoNLL shared task 2012 and 2011 (Pradhan et al., 2012; Pradhan et al., 2011) or in the SemEval shared task 2010 (Recasens et al., 2010). A lot of research focuses on English co-reference, resulting in a number of high performing English coreference systems, e.g. Clark and Manning (2015), Durrett and Klein (2014) or Bj¨orkelund and Kuhn (2014). However, there has been less work on German coreference resolution. Since the SemEval shared task 2010, only a few systems have been improved or developed, such as the rule-based CorZu system (Klenner and Tuggener, 2011; Tuggener and Klenner, 2014) or Krug et al. (2015)’s system which is tailored to the do"
L16-1024,W09-2411,0,0.15904,"Missing"
L16-1024,S10-1019,0,0.058237,"Missing"
L16-1024,S10-1001,0,\N,Missing
L16-1684,W13-3520,0,0.0265152,"nnotated data. Thus, abstraction from the surface form is preferable. In the context of language modeling with the help of neural networks, it has been shown helpful to train so called word embeddings (e.g., Mikolov et al 2013, Lebret et al 2013). These embeddings are high dimensional vectors representing features of words in a high feature space and are able to capture syntactic and semantic regularities (Mikolov et al., 2013). These characteristics make them a good departure point for a scenario in which one faces data sparsity. We train 64-dimensional word embeddings using word2embeddings (Al-Rfou et al., 2013) and a window size of five tokens on our entire corpus. Although this oversteps the clear division between training and test data because those vectors summerize the context of 1 Middle High German texts are characterized by their high Based on the Gotha manuscript edited by Samuel Singer, Berlin 1906. Digitalized version from http://www.mhgta. uni-trier.de (G¨artner, 2002). 4317 STTS tags ADJA, ADJD APPR, APPRART, APPO ADV, PWAV CARD ART, PDAT, PDS, PIAT, PIDAT PIAT APZR, TRUNC, PTK, PTKZU, PTKNEG, PTKVZ, PTKANT, PTKA DRELS, PRELAT, PRELS PPOSAT, PPOSS PIS, PPER, PRF PWAT, PWS PWAT, PWS KON,"
L16-1684,P06-3002,0,0.0765752,"The aim of this study is to evaluate the performance of PoS tagging considering different supply conditions of data and related external resources. This can serve as a reference point for DH projects. We strive for a better idea of how one can gain performance in such a context by investing time in developing resources. The approximation of such a gain is an important consideration given that those texts often have very specific characteristics and developed resources might not be reused in another context. 2. Related Work Completely unsupervised PoS tagging is still in its very early stages. Biemann (2006) relies on a graph clustering method. Unlike in current state-of-the-art approaches, the kind and number of different tags are generated by the method itself. Contrary to this, Haghighi and Klein (2006) use distributional prototypes in the learning process of their log-linear model. This way they inform the algorithm indirectly about the PoS classes. These unsupervised or semisupervised approaches make use of distributional semantics (Turian et al., 2010). In this context, the use of word embeddings have to be mentioned. Their ability to capture syntactic and semantic regularities (Mikolov et"
L16-1684,W06-1615,0,0.216707,"Missing"
L16-1684,W13-2302,0,0.144973,"mple PoS taggers developed for closely-related languages can be applied as done in Zeman and Resnik (2008). This requires a proper mapping from one tag set to another. In the field of low resource language processing, not just parallel data of closely-related languages is used, but the task is often tackled as domain-adaptation of tools developed for a related language. Blitzer et al. (2006) introduce structural correspondence learning for domain adaptation from newspaper text to the biomedical domain also for the setting when there is no labeled data from the target domain. Dipper (2010) and Bollmann (2013) concentrate on PoS tagging of historical German text using normalization to modern German as a preprocessing step before applying a PoS tagger for standard German. This approach requires the availability of a normalizer and tagging quality is highly dependent on the normalization success. Moreover, the heterogeneity of texts even within the same time period is a crucial issue for an approach using text normalization. Being confronted with a diversity of methods to tackle PoS tagging for underresourced languages, we investigate those being feasible regarding our data situation. Therefore, we f"
L16-1684,P11-1061,0,0.0313453,"mber of sentences and tokens in train, development and test set of our gold standard. (2007) who use aligned text to compensate for the lack of annotated data in the language under investigation. S´anchez-Mart´ınez et al. (2007) unsupervisedly train a HMM-based Occitan part-of-speech tagger used within an MT system using translation probabilities given tag assignments to inform the HMM. Agic et al. (2015) introduce an approach using the bible as a parallel corpus aggregating over the tags from annotated languages. This way, they train PoS taggers for 100 languages like Cakchiquel and Akawaio. Das and Petrov (2011) locate their approach on the unsupervised side, however, they use translated text in a resource-rich language for cross-lingual knowledge transfer. Several other approaches utilize lexicons providing the learning algorithm with possible valid PoS for a part of the vocabulary (Ravi and Knight, 2009). Garrette and Baldridge (2013) show that there is no need for huge annotated corpora but that reasonable results can be achieved by generalizing from just a little amount of annotated data. Moreover, simple PoS taggers developed for closely-related languages can be applied as done in Zeman and Resn"
L16-1684,N13-1037,0,0.0218705,"Missing"
L16-1684,W13-4811,0,0.0382174,"Missing"
L16-1684,N13-1014,0,0.284664,"es of German. In this paper, we investigate PoS tagging for historical texts sharing a lot of the challenges of PoS tagging for low resourced languages. We do this by means of what we call expanding exploration. We compare different approaches towards boosting performance of PoS tagging of text for which no suitable PoS tagger is available and no or really limited annotated data is available. Departing from the assumption that we have no text-external resources to our disposal, we experiment with unsupervised and weakly supervised learning methods. Moreover, we follow experiments performed by Garrette and Baldridge (2013) who describe PoS tagging research for low resourced languages using really small amounts of annotated data. Expanding to textexternal resources, we include taggers that have been developed for related languages into our experiments. The aim of this study is to evaluate the performance of PoS tagging considering different supply conditions of data and related external resources. This can serve as a reference point for DH projects. We strive for a better idea of how one can gain performance in such a context by investing time in developing resources. The approximation of such a gain is an impor"
L16-1684,N06-1041,0,0.555743,"projects. We strive for a better idea of how one can gain performance in such a context by investing time in developing resources. The approximation of such a gain is an important consideration given that those texts often have very specific characteristics and developed resources might not be reused in another context. 2. Related Work Completely unsupervised PoS tagging is still in its very early stages. Biemann (2006) relies on a graph clustering method. Unlike in current state-of-the-art approaches, the kind and number of different tags are generated by the method itself. Contrary to this, Haghighi and Klein (2006) use distributional prototypes in the learning process of their log-linear model. This way they inform the algorithm indirectly about the PoS classes. These unsupervised or semisupervised approaches make use of distributional semantics (Turian et al., 2010). In this context, the use of word embeddings have to be mentioned. Their ability to capture syntactic and semantic regularities (Mikolov et al., 2013) can be utilized to compensate for the high number of hapax legomena in sparse data. Word embeddings have been used by Lin et al. (2015) for unsupervised PoS induction. Weakly supervised techn"
L16-1684,N15-1144,0,0.0223553,"nerated by the method itself. Contrary to this, Haghighi and Klein (2006) use distributional prototypes in the learning process of their log-linear model. This way they inform the algorithm indirectly about the PoS classes. These unsupervised or semisupervised approaches make use of distributional semantics (Turian et al., 2010). In this context, the use of word embeddings have to be mentioned. Their ability to capture syntactic and semantic regularities (Mikolov et al., 2013) can be utilized to compensate for the high number of hapax legomena in sparse data. Word embeddings have been used by Lin et al. (2015) for unsupervised PoS induction. Weakly supervised techniques can involve supervision of different degrees and of different kinds. There exist approaches using parallel data like Moon and Baldridge 4316 set train dev test # sentences av. # tokens 100 100 50 1374 1372 688 Table 1: Average number of sentences and tokens in train, development and test set of our gold standard. (2007) who use aligned text to compensate for the lack of annotated data in the language under investigation. S´anchez-Mart´ınez et al. (2007) unsupervisedly train a HMM-based Occitan part-of-speech tagger used within an MT"
L16-1684,melero-etal-2012-holaaa,0,0.0608663,"Missing"
L16-1684,N13-1090,0,0.0591527,"Missing"
L16-1684,D07-1041,0,0.0694361,"Missing"
L16-1684,P09-1057,0,0.03077,"thin an MT system using translation probabilities given tag assignments to inform the HMM. Agic et al. (2015) introduce an approach using the bible as a parallel corpus aggregating over the tags from annotated languages. This way, they train PoS taggers for 100 languages like Cakchiquel and Akawaio. Das and Petrov (2011) locate their approach on the unsupervised side, however, they use translated text in a resource-rich language for cross-lingual knowledge transfer. Several other approaches utilize lexicons providing the learning algorithm with possible valid PoS for a part of the vocabulary (Ravi and Knight, 2009). Garrette and Baldridge (2013) show that there is no need for huge annotated corpora but that reasonable results can be achieved by generalizing from just a little amount of annotated data. Moreover, simple PoS taggers developed for closely-related languages can be applied as done in Zeman and Resnik (2008). This requires a proper mapping from one tag set to another. In the field of low resource language processing, not just parallel data of closely-related languages is used, but the task is often tackled as domain-adaptation of tools developed for a related language. Blitzer et al. (2006) in"
L16-1684,P10-1040,0,0.0242359,"developed resources might not be reused in another context. 2. Related Work Completely unsupervised PoS tagging is still in its very early stages. Biemann (2006) relies on a graph clustering method. Unlike in current state-of-the-art approaches, the kind and number of different tags are generated by the method itself. Contrary to this, Haghighi and Klein (2006) use distributional prototypes in the learning process of their log-linear model. This way they inform the algorithm indirectly about the PoS classes. These unsupervised or semisupervised approaches make use of distributional semantics (Turian et al., 2010). In this context, the use of word embeddings have to be mentioned. Their ability to capture syntactic and semantic regularities (Mikolov et al., 2013) can be utilized to compensate for the high number of hapax legomena in sparse data. Word embeddings have been used by Lin et al. (2015) for unsupervised PoS induction. Weakly supervised techniques can involve supervision of different degrees and of different kinds. There exist approaches using parallel data like Moon and Baldridge 4316 set train dev test # sentences av. # tokens 100 100 50 1374 1372 688 Table 1: Average number of sentences and"
L16-1684,W11-2205,0,0.0162612,"atures. 4.3. Tritraining As another implementation of self-learning, this time using external classifiers, we use tritraining (Zhou and Li, 2005). We use two classifiers, our external taggers for NHG and MHG, to inform our third classifier about which sentence from the unlabeled data set to add to the training process. For this decision, we choose simple agreement of both classifiers. If they agree on the tagging of an entire sentence, this sentence is added to the training data. standard for PoS tagging seems counter-intuitive given that the clustering is not informed about the task at hand. Vlachos (2011) advocates the evaluation as clustering-based word representation induction. Extrinsic evaluation is suggested as a solution to this problem. Having all these drawbacks in mind, we evaluate the overlap between the clustering results and the gold standard data without drawing strict conclusions about the usefulness of the clustering results for downstream tasks. To facilitate the mapping and weakly inform the clustering about the task at hand, we use a typical word for each PoS as seed for each cluster inspired by prototype learning introduced by Haghighi and Klein (2006). This leaves us with f"
L16-1684,I08-3008,0,0.0608463,"Petrov (2011) locate their approach on the unsupervised side, however, they use translated text in a resource-rich language for cross-lingual knowledge transfer. Several other approaches utilize lexicons providing the learning algorithm with possible valid PoS for a part of the vocabulary (Ravi and Knight, 2009). Garrette and Baldridge (2013) show that there is no need for huge annotated corpora but that reasonable results can be achieved by generalizing from just a little amount of annotated data. Moreover, simple PoS taggers developed for closely-related languages can be applied as done in Zeman and Resnik (2008). This requires a proper mapping from one tag set to another. In the field of low resource language processing, not just parallel data of closely-related languages is used, but the task is often tackled as domain-adaptation of tools developed for a related language. Blitzer et al. (2006) introduce structural correspondence learning for domain adaptation from newspaper text to the biomedical domain also for the setting when there is no labeled data from the target domain. Dipper (2010) and Bollmann (2013) concentrate on PoS tagging of historical German text using normalization to modern German"
L18-1176,P06-4018,0,0.0231969,"(Eckart de Castilho and Gurevych, 2014) framework have been developed. They are mainly focused on building shareable analysis pipelines and build on a strongly typed model, where linguistic types or theories are directly encoded. This can be helpful for data exchange between collections of predefined processing components such as parsers. However, it made this approach unsuited for us as alternative to designing our own modeling toolkit. Other text processing pipelines include OpenNLP5 , the General Architecture for Text Engineering (Cunningham et al., 2002) and the Natural Language Toolkit (Bird, 2006) for Python. They provide solutions for various NLP tasks and also feature modeling approaches of varying expressiveness. Their general focus on text data however keeps them limited and unfit as basis for a middleware system that attempts to unify access to multiple modalities. 4. 4 3 Architecture An implementation of our modeling approach is provided as an open-source Java toolkit (further information on availability is provided in Section 6) which requires version 8 of the Java Runtime Environment (JRE). It offers a set of abstract building blocks and other components to compose the data str"
L18-1176,P02-1022,0,0.182902,"nents for natural language processing around the DKPro Core (Eckart de Castilho and Gurevych, 2014) framework have been developed. They are mainly focused on building shareable analysis pipelines and build on a strongly typed model, where linguistic types or theories are directly encoded. This can be helpful for data exchange between collections of predefined processing components such as parsers. However, it made this approach unsuited for us as alternative to designing our own modeling toolkit. Other text processing pipelines include OpenNLP5 , the General Architecture for Text Engineering (Cunningham et al., 2002) and the Natural Language Toolkit (Bird, 2006) for Python. They provide solutions for various NLP tasks and also feature modeling approaches of varying expressiveness. Their general focus on text data however keeps them limited and unfit as basis for a middleware system that attempts to unify access to multiple modalities. 4. 4 3 Architecture An implementation of our modeling approach is provided as an open-source Java toolkit (further information on availability is provided in Section 6) which requires version 8 of the Java Runtime Environment (JRE). It offers a set of abstract building block"
L18-1176,W14-5201,0,0.0616075,"Missing"
L18-1176,francopoulo-etal-2006-lexical,0,0.0434817,"us to designing and implementing a new dedicated framework. With “modeling corpus resources” we refer to the task of representing the structure and content of a corpus or similar resource by means of a data model in memory, i.e. during an application’s runtime. As a corpus or equivalent resource we treat any collection of utterances in arbitrary form, be they written, spoken or presented in yet another modality. Note that presently this definition excludes the modeling of other types of linguistic resources like lexicons or dictionaries as realized for example in the Lexical Markup Framework (Francopoulo et al., 2006). In the remainder of this paper we present our approach for providing unified access to very different corpus resources. Section 2 discusses the underlying issue of a heterogeneous format multiverse and Section 3 contextualizes our work. We present a detailed overview of various aspects of our framework in Section 4 and then proceed to illustrate its usage with code examples in Section 5. Information on the availability of the software and its documentation is provided in Section 6 and Section 7 concludes. 2. Format Multiverse and Middleware Extensive work has already been invested into desig"
L18-1176,P13-4010,1,0.844718,"Missing"
L18-1176,P14-5002,1,0.909909,"Missing"
L18-1176,P15-4005,1,0.889635,"Missing"
L18-1176,W09-1201,0,0.0168097,"Missing"
L18-1176,P10-4005,0,0.0327022,"Logic Data Model A Data Model B Converter A Converter B Format A Format B Software Software Application Logic Data Model Converter Physical Format (c) Middleware solution Figure 1: Bottom-up data flows in several resource processing applications depicted as software stacks. Grey filled boxes represent data to be processed, blue bordered ones are parts of an application’s own codebase and green shows the placement of our middleware solution in such an architecture in (c). Grid (Ide et al., 2016). Less flexible exchange formats include for instance the Text Corpus Format (TCF) of the WebLicht (Hinrichs et al., 2010) environment, where the coverage of different annotation layers or hierarchical structures remains fixed. Albeit having very expressive and standardized formats available, the reality is that more often than not corpora or system outputs used in day-to-day research work are stored in specialized or proprietary formats. Especially tools used for a specific task or field of research often use formats that have emerged as a kind of “local” standard for the particular field, such as Praat TextGrid files (Boersma, 2001) for phonetic analysis. Tools designed in the context of a particular task not u"
L18-1176,W07-1501,0,0.027866,"present a detailed overview of various aspects of our framework in Section 4 and then proceed to illustrate its usage with code examples in Section 5. Information on the availability of the software and its documentation is provided in Section 6 and Section 7 concludes. 2. Format Multiverse and Middleware Extensive work has already been invested into designing and establishing interoperable formats for linguistic resources and annotations based on various common transport formats. Notable examples are the NLP Interchange Format (NIF) (Hellmann et al., 2013), an RDF/OWL-based format, or GrAF (Ide and Suderman, 2007), a pivot XMLbased serialization format for the Linguistic annotation framework (LAF) (Ide and Suderman, 2014). GrAF has also been standardized by ISO1 as part of the “Language resource management” committee (ISO/TC 37/SC 4). The LAPPS Interchange Format (Verhagen et al., 2016) is a JSON-LD2 format and used for data transfer between service implementations in the The Language Applications 1087 1 2 ISO 24612:2012 https://json-ld.org/ Application Logic Data Model + Metadata Converter Framework Formats A...Z Data Source X (b) Multiple supported format stacks Software (a) Monolithic design Applica"
L18-1176,W03-0804,0,0.10868,"or segmentation, structure and data storage maximizes generality but sacrifices any direct knowledge about linguistic specificity (equivalent to aforementioned generalpurpose graph models). Independent of this the other part acts as metadata and carries the information describing composition and dependencies of a specific resource. We outline the two aforementioned parts and how they interact with each other in Sections 4.3 to 4.5. 4.2. Design Foundations Our model design builds on several observations and requirements for modeling diverse corpus resources, some of which have been proposed by Ide et al. (2003) in an early iteration of LAF. We list these requirements and show how our modeling approach fulfills them: Expressive adequacy and Openness: We do not impose any limitations on the nature of linguistic information or theories that are expressed or stored in our framework to keep it as flexible as possible. Uniformity: We utilize a compact set of universal “building blocks” (see Section 4.3) to model all kinds of structural or hierarchical information in a corpus (equal in expressiveness to a graph, but closer to linguistic universals). Granularity: For every resource there is at least one man"
L18-1176,L16-1073,0,0.0196131,"Framework Formats A...Z Data Source X (b) Multiple supported format stacks Software (a) Monolithic design Application Logic Data Model A Data Model B Converter A Converter B Format A Format B Software Software Application Logic Data Model Converter Physical Format (c) Middleware solution Figure 1: Bottom-up data flows in several resource processing applications depicted as software stacks. Grey filled boxes represent data to be processed, blue bordered ones are parts of an application’s own codebase and green shows the placement of our middleware solution in such an architecture in (c). Grid (Ide et al., 2016). Less flexible exchange formats include for instance the Text Corpus Format (TCF) of the WebLicht (Hinrichs et al., 2010) environment, where the coverage of different annotation layers or hierarchical structures remains fixed. Albeit having very expressive and standardized formats available, the reality is that more often than not corpora or system outputs used in day-to-day research work are stored in specialized or proprietary formats. Especially tools used for a specific task or field of research often use formats that have emerged as a kind of “local” standard for the particular field, su"
L18-1176,W11-1901,0,0.0374717,"s. Especially tools used for a specific task or field of research often use formats that have emerged as a kind of “local” standard for the particular field, such as Praat TextGrid files (Boersma, 2001) for phonetic analysis. Tools designed in the context of a particular task not uncommonly motivate the design and limitations of their data model with the original data format provided. In the popular CoNLL Shared Tasks for example data sets consistently are made available in a tabular format tailored to the individual tasks (e.g. parsing 2009 (Hajiˇc et al., 2009), coreference resolution 2011 (Pradhan et al., 2011) and again parsing to Universal Dependencies in the 2017 shared task3 ). Tools developed for those tasks often made the respective CoNLL format their exclusive input and output representation, leading to a plethora of classic monolithic designs of the form shown in Figure 1a. Processing applications such as search or visualization tools are then required to either support multiple formats and associated abstract models (see Figure 1b) or force the user into converting input data into a pivot format (further encouraging designs as in Figure 1a). This leads to increased workload for either the d"
L18-1176,P15-2014,0,0.0136657,"layers on varying linguistic levels and granularities, such as the SFB732 Silver Standard Collection, of which a first overview has been provided by Eckart and G¨artner (2016). A strong focus lies hereby on connecting linguistic layers or modalities which so far have not received much attention in their particular combination. This is to support interesting research questions, such as the interaction of information status and prosodic realization(Baumann and Riester, 2013) or the incorporation of prosodic information into the task of automatic coreference resolution on spoken text as done by Roesiger and Riester (2015). As a result we faced the challenge of modeling access from a single processing software to a very diverse set of corpus resources. And while several standards around the Linguistic Annotation Framework (LAF)(Ide and Suderman, 2014) exist, they primarily take the point of view of preparation or curation of data. The ongoing standardization effort for a Corpus Query Lingua Franca (CQLF, ISO/DIS 24623-1)(Banski et al., 2016) correctly identifies the lack of standards which address a quite different view, that is, the one of querying or processing of corpus resources. As such there is no standar"
L18-1348,bjorkelund-etal-2014-extended,1,0.862232,"ulness of the new annotation layers from TIGER 2.2-doc. Keywords: Corpus Annotation, Treebank, Document Structure 1. Introduction The TIGER corpus (Brants et al., 2004) is an important treebank for German. It has been frequently employed over the years: for several shared tasks on multilingual dependency parsing (Buchholz and Marsi, 2006) and parsing morphologically rich languages (Seddah et al., 2013; Seddah et al., 2014); as part of the HamleDT compilation (Zeman et al., 2012); and as training data for processing pipelines for variety of tasks, including coreference and bridging resolution (Björkelund et al., 2014), theoretical linguistics (Haselbach et al., 2012), and building morphological word-embeddings (Cotterell and Schütze, 2015). TIGER consists of roughly 900,000 words or 50,000 sentences from German newspaper articles and its focus is on token and sentence level: For the first release of the TIGER treebank, the corpus was semi-automatically annotated and corrected with part-of-speech tags and constituent structures. The following versions enhanced the corpus with respect to the number of sentences and introduced morphological and lemma annotations. Additionally, parts of the corpus were release"
L18-1348,P16-1181,1,0.890172,"Missing"
L18-1348,W06-2920,0,0.554934,"nt baseline experiments for lemmatization, part-of-speech and morphological tagging, and dependency parsing. Finally, we present two example use cases: sentence boundary detection and authorship attribution. These use cases take the data from TIGER into account and illustrate the usefulness of the new annotation layers from TIGER 2.2-doc. Keywords: Corpus Annotation, Treebank, Document Structure 1. Introduction The TIGER corpus (Brants et al., 2004) is an important treebank for German. It has been frequently employed over the years: for several shared tasks on multilingual dependency parsing (Buchholz and Marsi, 2006) and parsing morphologically rich languages (Seddah et al., 2013; Seddah et al., 2014); as part of the HamleDT compilation (Zeman et al., 2012); and as training data for processing pipelines for variety of tasks, including coreference and bridging resolution (Björkelund et al., 2014), theoretical linguistics (Haselbach et al., 2012), and building morphological word-embeddings (Cotterell and Schütze, 2015). TIGER consists of roughly 900,000 words or 50,000 sentences from German newspaper articles and its focus is on token and sentence level: For the first release of the TIGER treebank, the corp"
L18-1348,N15-1140,0,0.0316116,"roduction The TIGER corpus (Brants et al., 2004) is an important treebank for German. It has been frequently employed over the years: for several shared tasks on multilingual dependency parsing (Buchholz and Marsi, 2006) and parsing morphologically rich languages (Seddah et al., 2013; Seddah et al., 2014); as part of the HamleDT compilation (Zeman et al., 2012); and as training data for processing pipelines for variety of tasks, including coreference and bridging resolution (Björkelund et al., 2014), theoretical linguistics (Haselbach et al., 2012), and building morphological word-embeddings (Cotterell and Schütze, 2015). TIGER consists of roughly 900,000 words or 50,000 sentences from German newspaper articles and its focus is on token and sentence level: For the first release of the TIGER treebank, the corpus was semi-automatically annotated and corrected with part-of-speech tags and constituent structures. The following versions enhanced the corpus with respect to the number of sentences and introduced morphological and lemma annotations. Additionally, parts of the corpus were released as dependency treebanks, e.g. a dependency version of TIGER 2.2 (Seeker and Kuhn, 2012). During the early development of t"
L18-1348,D13-1146,0,0.0752313,"Missing"
L18-1348,C12-1068,1,0.840008,"doc. Keywords: Corpus Annotation, Treebank, Document Structure 1. Introduction The TIGER corpus (Brants et al., 2004) is an important treebank for German. It has been frequently employed over the years: for several shared tasks on multilingual dependency parsing (Buchholz and Marsi, 2006) and parsing morphologically rich languages (Seddah et al., 2013; Seddah et al., 2014); as part of the HamleDT compilation (Zeman et al., 2012); and as training data for processing pipelines for variety of tasks, including coreference and bridging resolution (Björkelund et al., 2014), theoretical linguistics (Haselbach et al., 2012), and building morphological word-embeddings (Cotterell and Schütze, 2015). TIGER consists of roughly 900,000 words or 50,000 sentences from German newspaper articles and its focus is on token and sentence level: For the first release of the TIGER treebank, the corpus was semi-automatically annotated and corrected with part-of-speech tags and constituent structures. The following versions enhanced the corpus with respect to the number of sentences and introduced morphological and lemma annotations. Additionally, parts of the corpus were released as dependency treebanks, e.g. a dependency versi"
L18-1348,D13-1032,0,0.0807082,"rforming, i.a., sentence segmentation, POS tagging, and dependency parsing. From all the functionalities of UDP IPE we employ only the sentence segmenter. It uses a single-layer bidirectional GRU network. The segmenter works on character level, i.e., for each character in text it predicts if a sentence ends after it. We use UDP IPE as a baseline following the recent CoNLL Shared Task 2017 (Zeman et al., 2017). M AR M OT: is the best performing baseline from Björkelund et al. (2016). It augments POS tags with information if a token starts a new sentence. Then, it trains the sequence labeler of Müller et al. (2013) on whole documents annotated with the augmented tags. The method applied to new documents jointly predicts sentence boundaries and POS tags. For POS tagging, morphological analysis and parsing we apply the methods from Section 4 – namely CRF-based tagger by Müller et al. (2013) and transition-based parser by Björkelund and Nivre (2015). For the re-ordered pipeline we train the same POS tagger but on whole documents instead of sentences. We employ the aforementioned parser extended to predict sentence boundaries (Björkelund et al., 2016) (referred to as J OINT). It predicts sentence boundaries"
L18-1348,P09-1040,0,0.0836806,"Missing"
L18-1348,W13-4917,0,0.0705691,"Missing"
L18-1348,seeker-kuhn-2012-making,1,0.856669,"rphological word-embeddings (Cotterell and Schütze, 2015). TIGER consists of roughly 900,000 words or 50,000 sentences from German newspaper articles and its focus is on token and sentence level: For the first release of the TIGER treebank, the corpus was semi-automatically annotated and corrected with part-of-speech tags and constituent structures. The following versions enhanced the corpus with respect to the number of sentences and introduced morphological and lemma annotations. Additionally, parts of the corpus were released as dependency treebanks, e.g. a dependency version of TIGER 2.2 (Seeker and Kuhn, 2012). During the early development of the corpus parts of the information about document boundaries were lost. The current distribution 2.2 does not contain any grouping of sentences into documents, i.e., the corpus consists of one long sequence of sentences. Therefore, it is not possible to annotate this dataset with any document-level information, such as anaphora and coreference relations, or discourse trees.1 However, as opposed to datasets where contiguous documents are not available due to technical design or copyright reasons (Faaß and Eckart, 2013; Schäfer, 2015), TIGER does contain full d"
L18-1348,seeker-kuhn-2014-domain,1,0.620377,"etric differences or content of the documents (e.g. authors tending to write documents on the same topics). We present pilot experiments for two authorship attributions tasks: predicting the author of the text and gender of the author. Methodology and Tools In both of the tasks we use only sentences not labeled as M ETA, as they contain implicit information on authors (names, surnames and cities). For gender prediction we use all 533 documents with author id (for a detailed distribution of documents and their authors 6 We also applied the two described pipelines to an out-ofdomain test suite (Seeker and Kuhn, 2014). Results showed that J OINT trained on the new TIGER 2.2-doc has an advantage over standard pipeline also for other diverse datasets – for example dvd player manuals. 2207 see Figure 3). For author attribution we use only authors who wrote at least two documents (87 authors and 377 documents). We use two classification methods: D ELTA: the Burrows’s Delta (Burrows, 2002) is the most established method for capturing stylometric differences. We calculate D ELTA and perform classification with the R stylo package (Eder et al., 2013) with default parameters. To establish if the method captures st"
L18-1348,L16-1680,0,0.0712223,"Missing"
L18-1348,L16-1258,0,0.0695382,"Missing"
L18-1348,zeman-etal-2012-hamledt,0,0.0735739,"Missing"
L18-1348,K17-3001,0,0.0233355,"tain any grouping of sentences into documents, i.e., the corpus consists of one long sequence of sentences. Therefore, it is not possible to annotate this dataset with any document-level information, such as anaphora and coreference relations, or discourse trees.1 However, as opposed to datasets where contiguous documents are not available due to technical design or copyright reasons (Faaß and Eckart, 2013; Schäfer, 2015), TIGER does contain full documents and the only information lost is a mapping between sentences and documents to which they belong. 1 Cf. guidelines like Riester and Baumann (2017) and Riester et al. (to appear) which have been applied to German text, taking documents into account. The purpose of this paper is to overcome shortcomings of the current TIGER distribution and to enable application of document-level tasks to it. We present TIGER 2.2-doc2 , a new set of annotations that divides the sentences of the corpus into documents, i.e., newspaper articles, and realigns articles with their categories and approximate publication dates using original sources. We also semi-automatically enrich documents with structure annotations (differentiating meta-level information fro"
L18-1457,W15-2210,0,0.0350935,"Missing"
L18-1457,W13-4916,0,0.0641374,"Missing"
L18-1457,bjorkelund-etal-2014-extended,1,0.896131,"Missing"
L18-1457,P16-1181,1,0.906874,"Missing"
L18-1457,K17-3004,1,0.917064,"e refer to the documentation that is part of the release. Part-of-speech tagging. The interviews were annotated with part-of-speech labels based on the STTS guidelines 5 This term is inspired by a step called normalization often applied to map non-canonical representations to some sort of standard or processable forms. (Schiller et al., 1999) including the modifications from the TIGER corpus (Albert et al., 2003). Some additional guidelines were set up for the interview corpus (Seeker, 2016) but due to the specificities of the interviews no further categories were needed (cf. Westpfahl et al. (2017) for a broader set for spoken data). Three annotators were involved in the process and each interview was annotated by two of them independently, applying the Synpathy tool6 . The annotators achieved pair-wise agreement with a Cohen’s κ of 0.97, ranging between 0.96 and 0.98. In an adjudication step all three annotators then decided on the annotation, and remaining hard cases were discussed in the project context and documented separately. After all interviews had been manually annotated and discussed, an implementation of the DECCA-Tools (Dickinson and Meurers, 2003) in ICARUS (Thiele et al.,"
L18-1457,D12-1133,0,0.0321613,"(IMS Festival, 2010) to retrieve some of the features needed in the automatic annotation process for prosodic events. We release some of the syllable-based features, for convenience. We provide the duration of each syllable, its position in the word, and the number of phonemes in onset and rhyme, as well as the Van Santen/Hirschberg Classification (van Santen and Hirschberg, 1994) of onset and rhyme. Morpho-syntax. On the morpho-syntactic level we employed a series of very different pipeline implementations (BitPar (Schmid, 2006; Schmid, 2004), IMS-SZEGEDCIS (Bj¨orkelund et al., 2013), Mate (Bohnet and Nivre, 2012; Bohnet, 2010), IMSTrans (Bj¨orkelund and Nivre, 2015; Bj¨orkelund et al., 2016) and Stanford CoreNLP components such as the Stanford Parser (Chen and Manning, 2014)) to generate automatic parses and underlying morpho-syntactic annotations for the entire data set (see Table 3). Since we do not have (morpho-)syntactic gold standard annotations for this release, we post-processed the automatic system output to improve its usability, but without actually changing or correcting it. That is, following the idea of a silver-standard, we introduced additional confidence estimations as meta annotation"
L18-1457,C10-1011,0,0.0390573,"retrieve some of the features needed in the automatic annotation process for prosodic events. We release some of the syllable-based features, for convenience. We provide the duration of each syllable, its position in the word, and the number of phonemes in onset and rhyme, as well as the Van Santen/Hirschberg Classification (van Santen and Hirschberg, 1994) of onset and rhyme. Morpho-syntax. On the morpho-syntactic level we employed a series of very different pipeline implementations (BitPar (Schmid, 2006; Schmid, 2004), IMS-SZEGEDCIS (Bj¨orkelund et al., 2013), Mate (Bohnet and Nivre, 2012; Bohnet, 2010), IMSTrans (Bj¨orkelund and Nivre, 2015; Bj¨orkelund et al., 2016) and Stanford CoreNLP components such as the Stanford Parser (Chen and Manning, 2014)) to generate automatic parses and underlying morpho-syntactic annotations for the entire data set (see Table 3). Since we do not have (morpho-)syntactic gold standard annotations for this release, we post-processed the automatic system output to improve its usability, but without actually changing or correcting it. That is, following the idea of a silver-standard, we introduced additional confidence estimations as meta annotations for individua"
L18-1457,D14-1082,0,0.0467379,", for convenience. We provide the duration of each syllable, its position in the word, and the number of phonemes in onset and rhyme, as well as the Van Santen/Hirschberg Classification (van Santen and Hirschberg, 1994) of onset and rhyme. Morpho-syntax. On the morpho-syntactic level we employed a series of very different pipeline implementations (BitPar (Schmid, 2006; Schmid, 2004), IMS-SZEGEDCIS (Bj¨orkelund et al., 2013), Mate (Bohnet and Nivre, 2012; Bohnet, 2010), IMSTrans (Bj¨orkelund and Nivre, 2015; Bj¨orkelund et al., 2016) and Stanford CoreNLP components such as the Stanford Parser (Chen and Manning, 2014)) to generate automatic parses and underlying morpho-syntactic annotations for the entire data set (see Table 3). Since we do not have (morpho-)syntactic gold standard annotations for this release, we post-processed the automatic system output to improve its usability, but without actually changing or correcting it. That is, following the idea of a silver-standard, we introduced additional confidence estimations as meta annotations for individual predictions based on the agreement between different systems (see Figure 2a for an example of trees predicted by three different parsing systems for"
L18-1457,L18-1304,1,0.882906,"Missing"
L18-1457,E03-1068,0,0.0828341,"er categories were needed (cf. Westpfahl et al. (2017) for a broader set for spoken data). Three annotators were involved in the process and each interview was annotated by two of them independently, applying the Synpathy tool6 . The annotators achieved pair-wise agreement with a Cohen’s κ of 0.97, ranging between 0.96 and 0.98. In an adjudication step all three annotators then decided on the annotation, and remaining hard cases were discussed in the project context and documented separately. After all interviews had been manually annotated and discussed, an implementation of the DECCA-Tools (Dickinson and Meurers, 2003) in ICARUS (Thiele et al., 2014) was applied to the interviews, automatically finding potential cases of inconsistent annotation. Referential information status. From the gold part 20 interviews and the three training interviews were annotated with referential information status (Baumann and Riester, 2012), following the guidelines in Riester and Baumann (2017). This means that all referring expressions in the interviews (and a number of verb phrases and sentences functioning as antecedents for abstract anaphors) were categorized as to whether they are given/coreferential, bridging anaphors, d"
L18-1457,P13-4010,1,0.873529,"Missing"
L18-1457,W09-1201,0,0.105048,"Missing"
L18-1457,W12-4501,0,0.0556463,"where available. Figures 3 and 4 show (condensed) instances of this metadata for manual and automatic processing steps, respectively. The entire process metadata is available as part of the corpus release together with the overall documentation. 6. Availability Due to the high number of different tools involved in the creation of GRAIN and in order to accommodate researchers from different communities, various representation formats are part of this release. They contain, for instance, popular tabular formats such as those used in the CoNLL Shared Tasks of 2009 (Hajiˇc et al., 2009) and 2012 (Pradhan et al., 2012) and extended versions (Bj¨orkelund et al., 2014), XML-based formats such as TIGER XML (K¨onig et al., 2003), or Praat TextGrids (Boersma, 2001). For an exhaustive list of formats used and annotation layers contained in them we refer to the official documentation [result: [""swr2-interview-der-woche-20150620.txt-mod""], input: [""swr2-interview-der-woche-20150620.txt"",""swr2interview-der-woche-20150620.6444m.mp3""], workflowSteps:[ [description: ""transcription modification"", mode: ""manual"", operators:[ [name: ""OP01"", components:[ [name: ""transcription-change-guidelines"", version: ""1.1.0"", type: ""gu"
L18-1457,rebholz-schuhmann-etal-2010-calbc,0,0.342589,"Missing"
L18-1457,N06-2033,0,0.112192,"following annotations in subsequent releases of the silver standard part of the corpus. Merged dependency parses. As part of the parsing outputs described in Section 4.1 several parallel dependency trees are available for every sentence. While this provides a rich foundation for comparison, it can also be challenging for users to work with multiple concurrent trees. We will therefore provide additional merged versions of dependency trees. That is, we will include a majority decision of the dependency parsers under tree constraints. This is done by employing blending, also known as reparsing (Sagae and Lavie, 2006). We combine all the silver standard trees for a sentence into one graph and assign scores to arcs depending on the confidence estimations (see Figure 2b for an example of a combined graph). We then use the Chu-Liu-Edmonds algorithm (Chu and Liu, 1965; Edmonds, 1967) to find the maximum spanning tree in the combined graph (see Figure 2c for the resulting maximum spanning tree). For every resulting arc we select the most frequent label across all the labels previously assigned to it. This additional layer of automatic annotations has two purposes. It improves the usability of the syntactic laye"
L18-1457,C04-1024,0,0.0830128,"e Festival (Black, 1997) version of the University of Stuttgart (IMS Festival, 2010) to retrieve some of the features needed in the automatic annotation process for prosodic events. We release some of the syllable-based features, for convenience. We provide the duration of each syllable, its position in the word, and the number of phonemes in onset and rhyme, as well as the Van Santen/Hirschberg Classification (van Santen and Hirschberg, 1994) of onset and rhyme. Morpho-syntax. On the morpho-syntactic level we employed a series of very different pipeline implementations (BitPar (Schmid, 2006; Schmid, 2004), IMS-SZEGEDCIS (Bj¨orkelund et al., 2013), Mate (Bohnet and Nivre, 2012; Bohnet, 2010), IMSTrans (Bj¨orkelund and Nivre, 2015; Bj¨orkelund et al., 2016) and Stanford CoreNLP components such as the Stanford Parser (Chen and Manning, 2014)) to generate automatic parses and underlying morpho-syntactic annotations for the entire data set (see Table 3). Since we do not have (morpho-)syntactic gold standard annotations for this release, we post-processed the automatic system output to improve its usability, but without actually changing or correcting it. That is, following the idea of a silver-stan"
L18-1457,P06-1023,0,0.0361631,"essed using the Festival (Black, 1997) version of the University of Stuttgart (IMS Festival, 2010) to retrieve some of the features needed in the automatic annotation process for prosodic events. We release some of the syllable-based features, for convenience. We provide the duration of each syllable, its position in the word, and the number of phonemes in onset and rhyme, as well as the Van Santen/Hirschberg Classification (van Santen and Hirschberg, 1994) of onset and rhyme. Morpho-syntax. On the morpho-syntactic level we employed a series of very different pipeline implementations (BitPar (Schmid, 2006; Schmid, 2004), IMS-SZEGEDCIS (Bj¨orkelund et al., 2013), Mate (Bohnet and Nivre, 2012; Bohnet, 2010), IMSTrans (Bj¨orkelund and Nivre, 2015; Bj¨orkelund et al., 2016) and Stanford CoreNLP components such as the Stanford Parser (Chen and Manning, 2014)) to generate automatic parses and underlying morpho-syntactic annotations for the entire data set (see Table 3). Since we do not have (morpho-)syntactic gold standard annotations for this release, we post-processed the automatic system output to improve its usability, but without actually changing or correcting it. That is, following the idea o"
L18-1457,E14-2015,1,0.931714,"Missing"
N18-1066,P17-2098,1,0.837602,"grams from text (Yin and Neubig, 2017; Rabinovich et al., 2017), or solving the inverse problem of code to text generation (Iyer et al., 2016; Richardson et al., 2017). In contrast to our work, these studies do not look explicitly at translating to target APIs, or at non-English documentation. The idea of polyglot modeling has gained some traction in recent years for a variety of problems (Tsvetkov et al., 2016) and has appeared within work in SP under the heading of multilingual SP (Jie and Lu, 2014; Duong et al., 2017). A related topic is learning from multiple knowledge sources or domains (Herzig and Berant, 2017), which is related to our idea of learning from multiple APIs. To deal with these issues, we aim to learn more general text-to-code translation models that are trained on multiple datasets simultaneously. Our ultimate goal is to build polyglot translation models (cf. Johnson et al. (2016)), or models with shared representations that can translate any input text to any output programming language, regardless of whether such language pairs were encountered explicitly during training. Inherent in this task is the challenge of building an efficient polyglot decoder, or a translation mechanism that"
N18-1066,P13-2009,0,0.183632,"and 112,107 paths or component representations over an output vocabulary of 9,324 words. 5.2 Experimental Setup For the technical datasets, the goal is to see if our model generates correct signature representations from unobserved descriptions using exact match. We follow exactly the experimental setup and data splits from Richardson and Kuhn (2017b), and measure the accuracy at 1 (Acc@1), accuracy in top 10 (Acc@10), and MRR. For the GeoQuery and Sportscaster experiments, the goal is to see if our models can generate correct meaning representations for unseen input. For GeoQuery, we follow Andreas et al. (2013) in evaluating extrinsically by checking that each representation evaluates to the same answer as the gold representation when executed against the Geobase database. For Sportscaster, we evaluate by exact match to a gold representation. Mixed GeoQuery and Sportscaster We run experiments on the GeoQuery 880 corpus using the splits from Andreas et al. (2013), which includes geography queries for English, Greek, Thai, and German paired with formal database queries, as well as a seed lexicon or NP list for each language. In addition to training models on each individual dataset, we also learn poly"
N18-1066,C08-5001,0,0.0346971,"olyglot decoding, i.e., generating any output language by starting at the initial state 0 (e.g., C and Clojure). We formulate the decoding problem using a variant of the well-known single source shortest path (SSSP) algorithm for directed acyclic graphs (DAGs) (Johnson (1977)). This involves a graph G = (V, E) (nodes V and labeled edges E, see graph in Figure 2), and taking an off-line topological sort of the graph’s vertices. Using a data structure d ∈ R|V |(initialized as ∞|V |, as shown in Figure 2), the standard SSSP algorithm (which is the forward update variant of the Viterbi algorithm (Huang, 2008)) works by searching forward through the graph in sorted order and finding for each node v an incoming labeled edge u, with label z, that solves the following recurrence: n o d(v) = min d(u) + w(u, v, z) (2) Input: Input x of size n, DAG G = (V, E), lexical translation function pt , source node b with initial score o. Output: Shortest component path 1: d[V [G]] ← ∞, π[V [G]] ← N il, d[b] ← o 2: s[V [G], n] ← 0.0 . Shortest path sums at each node 3: for each vertex u ≥ b ∈ V [G] in sorted order do 4: for each vertex and label  Qn(v, z) ∈ Adj[u] do  5: score ← −log i pt (xi |z) + s[u, i] 6: if"
N18-1066,E09-2008,0,0.0321984,"r English, Greek, Thai, and German paired with formal database queries, as well as a seed lexicon or NP list for each language. In addition to training models on each individual dataset, we also learn polyglot models trained on all datasets concatenated together. We also created a new mixed language test set that was built by re726 Sportscaster 5.3 Mixed Table 1: Test results on the Stdlib and Py27 tasks averaged over all datasets and compared against the best monolingual results from Richardson and Kuhn (2017b,a), or RK Implementation and Model Details We use the Foma finite-state toolkit of Hulden (2009) to construct all graphs used in our experiments. We also use the Cython version of Dynet (Neubig et al., 2017) to implement all the neural models (see supp. materials for more details). In the results tables, we refer to the lexical and neural models introduced in Section 4 as Lexical Shortest Path and Neural Shortest Path, where models that use copying (+ copy) and lexical biasing (+ bias) are marked accordingly. We also experimented with adding a discriminative reranker to our lexical models (+ rerank), using the approach from Richardson and Kuhn (2017b), which uses additional lexical (e.g."
N18-1066,P16-1195,0,0.0302132,"; Cheng et al., 2017) these traditional models, the latter idea we look at in this paper. Work in NLP on software documentation has accelerated in recent years due in large part to the availability of new data resources through websites such as StackOverflow and Github (cf. Allamanis et al. (2017)). Most of this recent work focuses on processing large amounts of API data in bulk (Gu et al., 2016; Miceli Barone and Sennrich, 2017), either for learning longer executable programs from text (Yin and Neubig, 2017; Rabinovich et al., 2017), or solving the inverse problem of code to text generation (Iyer et al., 2016; Richardson et al., 2017). In contrast to our work, these studies do not look explicitly at translating to target APIs, or at non-English documentation. The idea of polyglot modeling has gained some traction in recent years for a variety of problems (Tsvetkov et al., 2016) and has appeared within work in SP under the heading of multilingual SP (Jie and Lu, 2014; Duong et al., 2017). A related topic is learning from multiple knowledge sources or domains (Herzig and Berant, 2017), which is related to our idea of learning from multiple APIs. To deal with these issues, we aim to learn more genera"
N18-1066,D13-1160,1,0.747,"idual APIs may be too small or poorly documented to build individual models or QA applications, and will in some way need to bootstrap off of more general models or resources. 2 Related Work Our approach builds on the baseline models introduced in Richardson and Kuhn (2017b) (see also Deng and Chrupała (2014)). Their work is positioned within the broader SP literature, where traditionally SMT (Wong and Mooney, 2006a) and parsing (Zettlemoyer and Collins, 2009) methods are used to study the problem of translating text to formal meaning representations, usually centering around QA applications (Berant et al., 2013). More recently, there has been interest in using neural network approaches either in place of (Dong and Lapata, 2016; Koˇcisk´y et al., 2016) or in combination with (Misra and Artzi, 2016; Jia and Liang, 2016; Cheng et al., 2017) these traditional models, the latter idea we look at in this paper. Work in NLP on software documentation has accelerated in recent years due in large part to the availability of new data resources through websites such as StackOverflow and Github (cf. Allamanis et al. (2017)). Most of this recent work focuses on processing large amounts of API data in bulk (Gu et al"
N18-1066,P16-1002,0,0.181603,"on the baseline models introduced in Richardson and Kuhn (2017b) (see also Deng and Chrupała (2014)). Their work is positioned within the broader SP literature, where traditionally SMT (Wong and Mooney, 2006a) and parsing (Zettlemoyer and Collins, 2009) methods are used to study the problem of translating text to formal meaning representations, usually centering around QA applications (Berant et al., 2013). More recently, there has been interest in using neural network approaches either in place of (Dong and Lapata, 2016; Koˇcisk´y et al., 2016) or in combination with (Misra and Artzi, 2016; Jia and Liang, 2016; Cheng et al., 2017) these traditional models, the latter idea we look at in this paper. Work in NLP on software documentation has accelerated in recent years due in large part to the availability of new data resources through websites such as StackOverflow and Github (cf. Allamanis et al. (2017)). Most of this recent work focuses on processing large amounts of API data in bulk (Gu et al., 2016; Miceli Barone and Sennrich, 2017), either for learning longer executable programs from text (Yin and Neubig, 2017; Rabinovich et al., 2017), or solving the inverse problem of code to text generation ("
N18-1066,D11-1131,0,0.0737932,"Missing"
N18-1066,C14-1122,0,0.103952,"API data in bulk (Gu et al., 2016; Miceli Barone and Sennrich, 2017), either for learning longer executable programs from text (Yin and Neubig, 2017; Rabinovich et al., 2017), or solving the inverse problem of code to text generation (Iyer et al., 2016; Richardson et al., 2017). In contrast to our work, these studies do not look explicitly at translating to target APIs, or at non-English documentation. The idea of polyglot modeling has gained some traction in recent years for a variety of problems (Tsvetkov et al., 2016) and has appeared within work in SP under the heading of multilingual SP (Jie and Lu, 2014; Duong et al., 2017). A related topic is learning from multiple knowledge sources or domains (Herzig and Berant, 2017), which is related to our idea of learning from multiple APIs. To deal with these issues, we aim to learn more general text-to-code translation models that are trained on multiple datasets simultaneously. Our ultimate goal is to build polyglot translation models (cf. Johnson et al. (2016)), or models with shared representations that can translate any input text to any output programming language, regardless of whether such language pairs were encountered explicitly during trai"
N18-1066,P12-1051,0,0.57328,"Missing"
N18-1066,P17-1005,0,0.0813129,"ls introduced in Richardson and Kuhn (2017b) (see also Deng and Chrupała (2014)). Their work is positioned within the broader SP literature, where traditionally SMT (Wong and Mooney, 2006a) and parsing (Zettlemoyer and Collins, 2009) methods are used to study the problem of translating text to formal meaning representations, usually centering around QA applications (Berant et al., 2013). More recently, there has been interest in using neural network approaches either in place of (Dong and Lapata, 2016; Koˇcisk´y et al., 2016) or in combination with (Misra and Artzi, 2016; Jia and Liang, 2016; Cheng et al., 2017) these traditional models, the latter idea we look at in this paper. Work in NLP on software documentation has accelerated in recent years due in large part to the availability of new data resources through websites such as StackOverflow and Github (cf. Allamanis et al. (2017)). Most of this recent work focuses on processing large amounts of API data in bulk (Gu et al., 2016; Miceli Barone and Sennrich, 2017), either for learning longer executable programs from text (Yin and Neubig, 2017; Rabinovich et al., 2017), or solving the inverse problem of code to text generation (Iyer et al., 2016; Ri"
N18-1066,J99-4005,0,0.145502,"hin this 721 pendent word generation process: When building models that can translate between unobserved language pairs, we use the term zeroshot translation from Johnson et al. (2016). 3 |x ||z| 1 YX p(x |z) = pt (xj |zi ) |A| Baseline Semantic Translator where each pt defines a multinomial distribution over a given component term z for all words x. The decoding problem for the above translation model involves finding the most likely output zˆ, which requires solving an arg maxz over Equation 1. In the general case, this problem is known to be N P-complete for the models under consideration (Knight, 1999) largely due to the large space of possible predictions z. Richardson and Kuhn (2017b) avoid these issues by exploiting the finiteness of the target component search space (an idea we also pursue here and discuss more below), and describe a constrained decoding algorithm that runs in time O(|C |log |C|). While this works well for small APIs, it becomes less feasible when dealing with large sets of APIs, as in the polyglot case, or with more complex semantic languages typically used in SP (Liang, 2013). Problem Formulation Throughout the paper, we refer to target code representations as API com"
N18-1066,knight-al-onaizan-1998-translation,0,0.0889234,"our first model, we use the lexical translation model and probability function pt in Equation 1 as 723 ← − ← − den states ( h , ..., h |x |) for the input sequence (x1 , ..., x|x |). Standardly, each word is then represented as the concatenation of its forward and → − ← − backward states: hj = [ h j , h j ]. the weighting function, which can be learned efficiently off-line using the EM algorithm. When attempting to use the SSSP procedure to compute this equation for a given source input x, we immediately have the problem that such a computation requires a complete component representation z (Knight and Al-Onaizan, 1998). We use an approximation1 that involves ignoring the normalizer |A |and exploiting the word independence assumption of the model, which allows us to incrementally compute translation scores for individual source words given output translations corresponding to shortest paths during the SSSP search. The full decoding algorithm in shown in Algorithm 1, where the red highlights the adjustments made to the standard SSSP search as presented in Cormen et al. (2009). The main modification involves adding a data structure s ∈ R|V |× |x |(initialized as 0.0|V |×|x |at line 2) that stores a running sum"
N18-1066,deng-chrupala-2014-semantic,0,0.0242857,"not observed during training. While software documentation is easy to find in bulk, if a particular API is not already documented in a language other than English (e.g., Haskell in de), it is unlikely that such a translation will appear without considerable effort by experienced translators. Similarly, many individual APIs may be too small or poorly documented to build individual models or QA applications, and will in some way need to bootstrap off of more general models or resources. 2 Related Work Our approach builds on the baseline models introduced in Richardson and Kuhn (2017b) (see also Deng and Chrupała (2014)). Their work is positioned within the broader SP literature, where traditionally SMT (Wong and Mooney, 2006a) and parsing (Zettlemoyer and Collins, 2009) methods are used to study the problem of translating text to formal meaning representations, usually centering around QA applications (Berant et al., 2013). More recently, there has been interest in using neural network approaches either in place of (Dong and Lapata, 2016; Koˇcisk´y et al., 2016) or in combination with (Misra and Artzi, 2016; Jia and Liang, 2016; Cheng et al., 2017) these traditional models, the latter idea we look at in thi"
N18-1066,D16-1116,0,0.0350275,"Missing"
N18-1066,P16-1004,0,0.0550629,"need to bootstrap off of more general models or resources. 2 Related Work Our approach builds on the baseline models introduced in Richardson and Kuhn (2017b) (see also Deng and Chrupała (2014)). Their work is positioned within the broader SP literature, where traditionally SMT (Wong and Mooney, 2006a) and parsing (Zettlemoyer and Collins, 2009) methods are used to study the problem of translating text to formal meaning representations, usually centering around QA applications (Berant et al., 2013). More recently, there has been interest in using neural network approaches either in place of (Dong and Lapata, 2016; Koˇcisk´y et al., 2016) or in combination with (Misra and Artzi, 2016; Jia and Liang, 2016; Cheng et al., 2017) these traditional models, the latter idea we look at in this paper. Work in NLP on software documentation has accelerated in recent years due in large part to the availability of new data resources through websites such as StackOverflow and Github (cf. Allamanis et al. (2017)). Most of this recent work focuses on processing large amounts of API data in bulk (Gu et al., 2016; Miceli Barone and Sennrich, 2017), either for learning longer executable programs from text (Yin and Neubig,"
N18-1066,D10-1119,0,0.180033,"Missing"
N18-1066,K17-1038,0,0.093166,"Gu et al., 2016; Miceli Barone and Sennrich, 2017), either for learning longer executable programs from text (Yin and Neubig, 2017; Rabinovich et al., 2017), or solving the inverse problem of code to text generation (Iyer et al., 2016; Richardson et al., 2017). In contrast to our work, these studies do not look explicitly at translating to target APIs, or at non-English documentation. The idea of polyglot modeling has gained some traction in recent years for a variety of problems (Tsvetkov et al., 2016) and has appeared within work in SP under the heading of multilingual SP (Jie and Lu, 2014; Duong et al., 2017). A related topic is learning from multiple knowledge sources or domains (Herzig and Berant, 2017), which is related to our idea of learning from multiple APIs. To deal with these issues, we aim to learn more general text-to-code translation models that are trained on multiple datasets simultaneously. Our ultimate goal is to build polyglot translation models (cf. Johnson et al. (2016)), or models with shared representations that can translate any input text to any output programming language, regardless of whether such language pairs were encountered explicitly during training. Inherent in thi"
N18-1066,J13-2005,0,0.0524802,"e general case, this problem is known to be N P-complete for the models under consideration (Knight, 1999) largely due to the large space of possible predictions z. Richardson and Kuhn (2017b) avoid these issues by exploiting the finiteness of the target component search space (an idea we also pursue here and discuss more below), and describe a constrained decoding algorithm that runs in time O(|C |log |C|). While this works well for small APIs, it becomes less feasible when dealing with large sets of APIs, as in the polyglot case, or with more complex semantic languages typically used in SP (Liang, 2013). Problem Formulation Throughout the paper, we refer to target code representations as API components. In all cases, components will consist of formal representations of functions, or function signatures (e.g., long max(int a, int b)), which include a function name (max), a sequence of arguments (int a, int b), and other information such as a return value (long) and namespace (for more details, see Richardson (2018)). For a given API dataset D = {(xi , zi )}ni=1 of size n, the goal is to learn a model that can generate exactly a correct component sequence z = (z1 , .., z|z |), within a finite"
N18-1066,D15-1166,0,0.0188769,"gi is the decoder’s hidden state at step i, and ci is a contextvector that encodes information about the input x and the encoder annotations. Each context vector ci in turn is a weighted sum of each annotation hj against an attention vector αi,j , or ci = P|x| j=1 αi,j hj , which is jointly learned using an additional single layered multi-layer perceptron defined in the following way: Neural Shortest Path αi,j ∝ exp(ei,j ); Our second set of models use neural networks to compute the weighting function in Equation 2. We use an encoder-decoder model with global attention (Bahdanau et al., 2014; Luong et al., 2015), which has the following two components: ei,j = MLP(gi−1 , hj ) (8) Lexical Bias and Copying In contrast to standard MT tasks, we are dealing with a relatively low-resource setting where the sparseness of the target vocabulary is an issue. For this reason, we experimented with integrating lexical translation scores using a biasing technique from Arthur et al. (2016). Their method is based on the following computation for each token zi : Encoder Model The first is an encoder network, which uses a bi-directional recurrent neural network architecture with LSTM units (Hochreiter and Schmidhuber,"
N18-1066,W17-3516,1,0.852695,"7) these traditional models, the latter idea we look at in this paper. Work in NLP on software documentation has accelerated in recent years due in large part to the availability of new data resources through websites such as StackOverflow and Github (cf. Allamanis et al. (2017)). Most of this recent work focuses on processing large amounts of API data in bulk (Gu et al., 2016; Miceli Barone and Sennrich, 2017), either for learning longer executable programs from text (Yin and Neubig, 2017; Rabinovich et al., 2017), or solving the inverse problem of code to text generation (Iyer et al., 2016; Richardson et al., 2017). In contrast to our work, these studies do not look explicitly at translating to target APIs, or at non-English documentation. The idea of polyglot modeling has gained some traction in recent years for a variety of problems (Tsvetkov et al., 2016) and has appeared within work in SP under the heading of multilingual SP (Jie and Lu, 2014; Duong et al., 2017). A related topic is learning from multiple knowledge sources or domains (Herzig and Berant, 2017), which is related to our idea of learning from multiple APIs. To deal with these issues, we aim to learn more general text-to-code translation"
N18-1066,I17-2053,0,0.0975531,"problem of text to function signature translation. Initially, these datasets were proposed as a resource for studying semantic parser induction (Mooney, 2007), or for building models that learn to translate text to formal meaning representations from parallel data. In followup work (Richardson and Kuhn, 2017a), they proposed using the resulting models to do automated question-answering (QA) and code retrieval on target APIs, and experimented with an additional set of software datasets built from 27 open-source Python projects. Introduction Recent work by Richardson and Kuhn (2017a,b); Miceli Barone and Sennrich (2017) considers the problem of translating source code documentation to lower-level code template representations as part of an effort to model the meaning of such documentation. Example documentation for a number of programming languages is shown in Figure 1, where each docstring description in red describes a given function (blue) in the library. While capturing the semantics of docstrings is in general a difficult task, learning the translation from descriptions to formal code representations (e.g., formal representations of functions) is proposed as a reasonable first step towards learning more"
N18-1066,N16-1161,0,0.0280835,"(cf. Allamanis et al. (2017)). Most of this recent work focuses on processing large amounts of API data in bulk (Gu et al., 2016; Miceli Barone and Sennrich, 2017), either for learning longer executable programs from text (Yin and Neubig, 2017; Rabinovich et al., 2017), or solving the inverse problem of code to text generation (Iyer et al., 2016; Richardson et al., 2017). In contrast to our work, these studies do not look explicitly at translating to target APIs, or at non-English documentation. The idea of polyglot modeling has gained some traction in recent years for a variety of problems (Tsvetkov et al., 2016) and has appeared within work in SP under the heading of multilingual SP (Jie and Lu, 2014; Duong et al., 2017). A related topic is learning from multiple knowledge sources or domains (Herzig and Berant, 2017), which is related to our idea of learning from multiple APIs. To deal with these issues, we aim to learn more general text-to-code translation models that are trained on multiple datasets simultaneously. Our ultimate goal is to build polyglot translation models (cf. Johnson et al. (2016)), or models with shared representations that can translate any input text to any output programming l"
N18-1066,N06-1056,0,0.487003,"already documented in a language other than English (e.g., Haskell in de), it is unlikely that such a translation will appear without considerable effort by experienced translators. Similarly, many individual APIs may be too small or poorly documented to build individual models or QA applications, and will in some way need to bootstrap off of more general models or resources. 2 Related Work Our approach builds on the baseline models introduced in Richardson and Kuhn (2017b) (see also Deng and Chrupała (2014)). Their work is positioned within the broader SP literature, where traditionally SMT (Wong and Mooney, 2006a) and parsing (Zettlemoyer and Collins, 2009) methods are used to study the problem of translating text to formal meaning representations, usually centering around QA applications (Berant et al., 2013). More recently, there has been interest in using neural network approaches either in place of (Dong and Lapata, 2016; Koˇcisk´y et al., 2016) or in combination with (Misra and Artzi, 2016; Jia and Liang, 2016; Cheng et al., 2017) these traditional models, the latter idea we look at in this paper. Work in NLP on software documentation has accelerated in recent years due in large part to the avai"
N18-1066,P17-1041,0,0.0277633,"nd Lapata, 2016; Koˇcisk´y et al., 2016) or in combination with (Misra and Artzi, 2016; Jia and Liang, 2016; Cheng et al., 2017) these traditional models, the latter idea we look at in this paper. Work in NLP on software documentation has accelerated in recent years due in large part to the availability of new data resources through websites such as StackOverflow and Github (cf. Allamanis et al. (2017)). Most of this recent work focuses on processing large amounts of API data in bulk (Gu et al., 2016; Miceli Barone and Sennrich, 2017), either for learning longer executable programs from text (Yin and Neubig, 2017; Rabinovich et al., 2017), or solving the inverse problem of code to text generation (Iyer et al., 2016; Richardson et al., 2017). In contrast to our work, these studies do not look explicitly at translating to target APIs, or at non-English documentation. The idea of polyglot modeling has gained some traction in recent years for a variety of problems (Tsvetkov et al., 2016) and has appeared within work in SP under the heading of multilingual SP (Jie and Lu, 2014; Duong et al., 2017). A related topic is learning from multiple knowledge sources or domains (Herzig and Berant, 2017), which is re"
N18-1066,J03-1002,0,0.0145505,"entations in all languages. Word-based Translation Model The translation models investigated in Richardson and Kuhn (2017b) use a noisy-channel formulation where p(z |x) ∝ p(x |z)p(z) via Bayes rule. By assuming a uniform prior on output components, p(z), the model therefore involves estimating p(x |z), which under a word-translation model is computed using the following formula: P p(x |z) = a∈A p(x, a |z), where the summation ranges over the set of all many-to-one word alignments A from x → z, with |A |equal to (|z |+ 1)|x |. They investigate various types of sequence-based alignment models (Och and Ney, 2003), and find that the classic IBM Model 1 outperforms more complex word models. This model factors in the following way and assumes an inde722 ceil s0 2C 0.00 2Clojure s5 numeric s1 s7 math ∞7 ∞6 ∞5 ∞1 s6 algo ∞2 s2 ∞3 math s3 atan2 atan2 ceil s9 s8 ∞9 ∞8 x ∞4 ∞10 s4 s10 arg s11 y ∞11 x Figure 2: A DAFSA representation for a portion of the component sequence search space C that includes math functions in C and Clojure, and an example path/translation (in bold): 2C numeric math ceil arg. Algorithm 1 Lexical Shortest Path Search Decoding reduces to the problem of finding a path for a given text in"
N18-1066,P09-1110,0,0.0929458,"Missing"
N18-1066,P17-1105,0,0.0280205,"sk´y et al., 2016) or in combination with (Misra and Artzi, 2016; Jia and Liang, 2016; Cheng et al., 2017) these traditional models, the latter idea we look at in this paper. Work in NLP on software documentation has accelerated in recent years due in large part to the availability of new data resources through websites such as StackOverflow and Github (cf. Allamanis et al. (2017)). Most of this recent work focuses on processing large amounts of API data in bulk (Gu et al., 2016; Miceli Barone and Sennrich, 2017), either for learning longer executable programs from text (Yin and Neubig, 2017; Rabinovich et al., 2017), or solving the inverse problem of code to text generation (Iyer et al., 2016; Richardson et al., 2017). In contrast to our work, these studies do not look explicitly at translating to target APIs, or at non-English documentation. The idea of polyglot modeling has gained some traction in recent years for a variety of problems (Tsvetkov et al., 2016) and has appeared within work in SP under the heading of multilingual SP (Jie and Lu, 2014; Duong et al., 2017). A related topic is learning from multiple knowledge sources or domains (Herzig and Berant, 2017), which is related to our idea of learn"
N18-1066,D16-1163,0,0.0234111,".2 78.0 78.9 79.6 4.2 71.1 75.2 Acc@10 – – – 92.4 94.1 91.1 92.8 92.1 92.9 94.7 91.4 91.7 91.9 18.2 94.3 90.0 74.2 86.0 40.3 70.3 81.9 83.4 83.3 – – 86.8 90.2 94.8 93.9 90.5 Table 2: Test results for the standard (above) and mixed (middle) GeoQuery tasks averaged over all languages, and results for the English Sportscaster task (below). The neural models are strongly outperformed by all other models both in the monolingual and polyglot case (only the latter results shown), even when lexical biasing is applied. While surprising, this is consistent with other studies on low¨ resource neural MT (Zoph et al., 2016; Ostling and Tiedemann, 2017), where datasets of comparable size to ours (e.g., 1 million tokens or less) typically fail against classical SMT models. This result has also been found in relation to neural AMR semantic parsing, where similar issues of sparsity are encountered (Peng et al., 2017). Even by doubling the amount of training data by training on all datasets (results not shown), this did not improve the accuracy, suggesting that much more data is needed (more discussion below). Beyond increases in accuracy, our polyglot models support zero-shot translation as shown in Figure 4, which"
N18-1066,D17-2012,1,0.0587922,"t.de ‡ Tel-Aviv University, Israel joberant@cs.tau.ac.il Abstract 1. (en, Java) Documentation *Returns the greater of two long values public static long max(long a, long b) Traditional approaches to semantic parsing (SP) work by training individual models for each available parallel dataset of text-meaning pairs. In this paper, we explore the idea of polyglot semantic translation, or learning semantic parsing models that are trained on multiple datasets and natural languages. In particular, we focus on translating text to code signature representations using the software component datasets of Richardson and Kuhn (2017a,b). The advantage of such models is that they can be used for parsing a wide variety of input natural languages and output programming languages, or mixed input languages, using a single unified model. To facilitate modeling of this type, we develop a novel graph-based decoding framework that achieves state-of-the-art performance on the above datasets, and apply this method to two other benchmark SP tasks. 1 2. (en, Python) Documentation max(self, a, b): """"""Compares two values numerically and returns the maximum"""""" 3. (en, Haskell) Documentation -- |""The largest element of a non-empty struct"
N18-1066,D16-1162,0,\N,Missing
N18-1066,E17-1035,0,\N,Missing
N18-1066,P16-1162,0,\N,Missing
P00-1046,C00-1062,0,\N,Missing
P00-1046,C88-2128,0,\N,Missing
P00-1046,P83-1021,0,\N,Missing
P00-1046,P96-1027,0,\N,Missing
P00-1061,P99-1035,1,0.867771,"Missing"
P00-1061,P97-1003,0,0.125027,"Missing"
P00-1061,A94-1009,0,0.0611326,"Missing"
P00-1061,A00-2021,1,0.883041,"Missing"
P00-1061,P99-1069,1,0.492297,"for estimating the parameters of the stochastic grammar from unannotated data. Our usage of EM was initiated by the current lack of large unicationbased treebanks for German. However, our experimental results also show an exception to the common wisdom of the insuciency of EM for highly accurate statistical modeling. Our approach to lexicalized stochastic modeling is based on the parametric family of loglinear probability models, which is used to dene a probability distribution on the parses of a Lexical-Functional Grammar (LFG) for German. In previous work on log-linear models for LFG by Johnson et al. (1999), pseudolikelihood estimation from annotated corpora has been introduced and experimented with on a small scale. However, to our knowledge, to date no large LFG annotated corpora of unrestricted German text are available. Fortunately, algorithms exist for statistical inference of log-linear models from unannotated data (Riezler, 1999). We apply this algorithm to estimate log-linear LFG models from large corpora of newspaper text. In our largest experiment, we used 250,000 parses which were produced by parsing 36,000 newspaper sentences with the German LFG. Experimental evaluation of our models"
P00-1061,J93-2004,0,0.0541982,"Missing"
P00-1061,P92-1017,0,0.134096,"Missing"
P00-1061,C00-2094,1,0.846315,"Missing"
P00-1061,W97-0301,0,0.0177929,"Missing"
P02-1007,P97-1040,0,0.0320603,"the optimization task for unrestricted OT is undecidable too. 3 However, it is not in the spirit of OT to have extremely powerful individual constraints; the explanatory power should rather arise from interaction of simple constraints. 3 OT-LFG Following (Bresnan, 2000; Kuhn, 2000; Kuhn, 2001), we define a restricted OT system based on Lexical-Functional Grammar (LFG) representations: c(ategory) structure/f(unctional) structure 2 Most computational OT work so far focuses on candidates and constraints expressible as regular languages/rational relations, based on (Frank and Satta, 1998) (e.g., (Eisner, 1997; Karttunen, 1998; Gerdemann and van Noord, 2000)). 3 Cf. also (Johnson, 1998) for the sketch of an undecidability argument and (Kuhn, 2001, 4.2, 6.3) for further constructions.   pairs like (4),(5) . Each c-structure tree node is mapped to a node in the f-structure graph by the function . The mapping is specified by fannotations in the grammar rules (below category symbols, cf. (2)) and lexicon entries (3). 4 (2) ROOT FP  FP (NP)  F       SUBJ    F ( SUBJ )= (4) NP ( ( that F had F ( seen V ( ( thought V ( ( laughed V ( (  V   = NP  COMP   JONPJO are descript"
P02-1007,J98-2006,0,0.0223955,"is known to be undecidable, so the optimization task for unrestricted OT is undecidable too. 3 However, it is not in the spirit of OT to have extremely powerful individual constraints; the explanatory power should rather arise from interaction of simple constraints. 3 OT-LFG Following (Bresnan, 2000; Kuhn, 2000; Kuhn, 2001), we define a restricted OT system based on Lexical-Functional Grammar (LFG) representations: c(ategory) structure/f(unctional) structure 2 Most computational OT work so far focuses on candidates and constraints expressible as regular languages/rational relations, based on (Frank and Satta, 1998) (e.g., (Eisner, 1997; Karttunen, 1998; Gerdemann and van Noord, 2000)). 3 Cf. also (Johnson, 1998) for the sketch of an undecidability argument and (Kuhn, 2001, 4.2, 6.3) for further constructions.   pairs like (4),(5) . Each c-structure tree node is mapped to a node in the f-structure graph by the function . The mapping is specified by fannotations in the grammar rules (below category symbols, cf. (2)) and lexicon entries (3). 4 (2) ROOT FP  FP (NP)  F       SUBJ    F ( SUBJ )= (4) NP ( ( that F had F ( seen V ( ( thought V ( ( laughed V ( (  V   = NP  COMP"
P02-1007,W00-1804,0,0.0694324,"Missing"
P02-1007,P99-1069,0,0.0333419,"a form is defined as grammatical if it is optimal (most harmonic) within a set of generation alternatives for an underlying logical form. The harmony of a candidate analysis depends  on a language-specific ranking ( ) of violable constraints, thus the learning task amounts to adjusting the ranking over a given set of constraints.   is more harmonic than iff it incurs fewer Candidate in violations of the highest-ranking constraint and differ. which (1)     The comparison-based setup of OT learning is closely related to discriminative learning approaches in probabilistic parsing (Johnson et al., 1999; Riezler et al., 2000; Riezler et al., 2002), 1 however the comparison of generation alternatives – rather than parsing alternatives – adds the possibility of systematically learning the basic language-specific grammatical principles (which in probabilistic parsing are typically fixed a priori, using either a treebankderived or a manually written grammar for the given  This work was supported by a postdoctoral fellowship of the German Academic Exchange Service (DAAD). 1 This is for instance pointed out by (Johnson, 1998). language). The “base grammar” assumed as given can be highly unrestric"
P02-1007,C00-1062,0,0.1441,"Missing"
P02-1007,W98-1301,0,0.0147268,"on task for unrestricted OT is undecidable too. 3 However, it is not in the spirit of OT to have extremely powerful individual constraints; the explanatory power should rather arise from interaction of simple constraints. 3 OT-LFG Following (Bresnan, 2000; Kuhn, 2000; Kuhn, 2001), we define a restricted OT system based on Lexical-Functional Grammar (LFG) representations: c(ategory) structure/f(unctional) structure 2 Most computational OT work so far focuses on candidates and constraints expressible as regular languages/rational relations, based on (Frank and Satta, 1998) (e.g., (Eisner, 1997; Karttunen, 1998; Gerdemann and van Noord, 2000)). 3 Cf. also (Johnson, 1998) for the sketch of an undecidability argument and (Kuhn, 2001, 4.2, 6.3) for further constructions.   pairs like (4),(5) . Each c-structure tree node is mapped to a node in the f-structure graph by the function . The mapping is specified by fannotations in the grammar rules (below category symbols, cf. (2)) and lexicon entries (3). 4 (2) ROOT FP  FP (NP)  F       SUBJ    F ( SUBJ )= (4) NP ( ( that F had F ( seen V ( ( thought V ( ( laughed V ( (  V   = NP  COMP   JONPJO are descriptions of nontermin"
P02-1007,P00-1046,1,0.769667,"a priori, using either a treebankderived or a manually written grammar for the given  This work was supported by a postdoctoral fellowship of the German Academic Exchange Service (DAAD). 1 This is for instance pointed out by (Johnson, 1998). language). The “base grammar” assumed as given can be highly unrestricted in the OT setup. Using a linguistically motivated set of constraints, learning proceeds with a bias for unmarked linguistic structures (cf. e.g., (Bresnan et al., 2001)). For computational OT syntax, an interleaving of candidate generation and constraint checking has been proposed (Kuhn, 2000). But the decidability of the optimization task in OT syntax, i.e., the identification of the optimal candidate(s) in a potentially infinite candidate set, has not been proven yet. 2 2 Undecidability for unrestricted OT Assume that the candidate set is characterized by a context-free grammar (cfg)  , plus one additional candidate ‘yes’. There are two constraints     ):   is violated if the candidate is neither ( ‘yes’ nor a structure generated by a cfg  ;   is vi olated only by ‘yes’. Now, ‘yes’ is in the language defined by this system iff there are no structures in  that are"
P02-1007,P00-1061,1,0.855653,"rammatical if it is optimal (most harmonic) within a set of generation alternatives for an underlying logical form. The harmony of a candidate analysis depends  on a language-specific ranking ( ) of violable constraints, thus the learning task amounts to adjusting the ranking over a given set of constraints.   is more harmonic than iff it incurs fewer Candidate in violations of the highest-ranking constraint and differ. which (1)     The comparison-based setup of OT learning is closely related to discriminative learning approaches in probabilistic parsing (Johnson et al., 1999; Riezler et al., 2000; Riezler et al., 2002), 1 however the comparison of generation alternatives – rather than parsing alternatives – adds the possibility of systematically learning the basic language-specific grammatical principles (which in probabilistic parsing are typically fixed a priori, using either a treebankderived or a manually written grammar for the given  This work was supported by a postdoctoral fellowship of the German Academic Exchange Service (DAAD). 1 This is for instance pointed out by (Johnson, 1998). language). The “base grammar” assumed as given can be highly unrestricted in the OT setup. U"
P02-1007,P02-1035,0,0.0122174,"timal (most harmonic) within a set of generation alternatives for an underlying logical form. The harmony of a candidate analysis depends  on a language-specific ranking ( ) of violable constraints, thus the learning task amounts to adjusting the ranking over a given set of constraints.   is more harmonic than iff it incurs fewer Candidate in violations of the highest-ranking constraint and differ. which (1)     The comparison-based setup of OT learning is closely related to discriminative learning approaches in probabilistic parsing (Johnson et al., 1999; Riezler et al., 2000; Riezler et al., 2002), 1 however the comparison of generation alternatives – rather than parsing alternatives – adds the possibility of systematically learning the basic language-specific grammatical principles (which in probabilistic parsing are typically fixed a priori, using either a treebankderived or a manually written grammar for the given  This work was supported by a postdoctoral fellowship of the German Academic Exchange Service (DAAD). 1 This is for instance pointed out by (Johnson, 1998). language). The “base grammar” assumed as given can be highly unrestricted in the OT setup. Using a linguistically m"
P03-1025,W03-3006,0,0.0427677,"Missing"
P03-1025,P97-1058,0,0.0212298,"n the initially mentioned compound Verkehrs.wege.planungs.beschleunigungs.gesetz (‘law for speeding up the planning of traffic routes’), we might assume that Verkehrs.wege (‘traffic routes’) is stored as a unit, but the remainder of the analysis is rule-based. With this depth of recursion (and a realistic morphological grammar), we get an unmanagable explosion of the number of states in the compiled (intermediate) FSA. 4 Proposed strategy We propose a refinement of finite-state approximation techniques for context-free grammars, as they have been developed for syntax (Pereira and Wright 1997, Grimley-Evans 1997, Johnson 1998, Nederhof 2000). Our strategy assumes that we want to express and develop the morphological grammar at the linguistically satisfactory level of a (contextfree-equivalent) unification grammar. In processing, a finite-state approximation of this grammar is used. Exploiting specific facts about morphology, the number of states for the constructed FSA can be kept relatively low, while still being in a position to cover realistic corpus example in an exact way. The construction is based on the following observation: Intuitively, context-free expressiveness is not needed to constrain"
P03-1025,P98-1101,0,0.0208502,"ioned compound Verkehrs.wege.planungs.beschleunigungs.gesetz (‘law for speeding up the planning of traffic routes’), we might assume that Verkehrs.wege (‘traffic routes’) is stored as a unit, but the remainder of the analysis is rule-based. With this depth of recursion (and a realistic morphological grammar), we get an unmanagable explosion of the number of states in the compiled (intermediate) FSA. 4 Proposed strategy We propose a refinement of finite-state approximation techniques for context-free grammars, as they have been developed for syntax (Pereira and Wright 1997, Grimley-Evans 1997, Johnson 1998, Nederhof 2000). Our strategy assumes that we want to express and develop the morphological grammar at the linguistically satisfactory level of a (contextfree-equivalent) unification grammar. In processing, a finite-state approximation of this grammar is used. Exploiting specific facts about morphology, the number of states for the constructed FSA can be kept relatively low, while still being in a position to cover realistic corpus example in an exact way. The construction is based on the following observation: Intuitively, context-free expressiveness is not needed to constrain grammaticality"
P03-1025,2000.iwpt-1.15,0,0.0740488,"Missing"
P03-1025,J00-1003,0,0.0907457,"Verkehrs.wege.planungs.beschleunigungs.gesetz (‘law for speeding up the planning of traffic routes’), we might assume that Verkehrs.wege (‘traffic routes’) is stored as a unit, but the remainder of the analysis is rule-based. With this depth of recursion (and a realistic morphological grammar), we get an unmanagable explosion of the number of states in the compiled (intermediate) FSA. 4 Proposed strategy We propose a refinement of finite-state approximation techniques for context-free grammars, as they have been developed for syntax (Pereira and Wright 1997, Grimley-Evans 1997, Johnson 1998, Nederhof 2000). Our strategy assumes that we want to express and develop the morphological grammar at the linguistically satisfactory level of a (contextfree-equivalent) unification grammar. In processing, a finite-state approximation of this grammar is used. Exploiting specific facts about morphology, the number of states for the constructed FSA can be kept relatively low, while still being in a position to cover realistic corpus example in an exact way. The construction is based on the following observation: Intuitively, context-free expressiveness is not needed to constrain grammaticality for most of the"
P03-1025,C98-1098,0,\N,Missing
P03-1025,P91-1032,0,\N,Missing
P04-1060,P99-1065,0,0.0192693,"is method are very encouraging. Will the information projection approach also work for less shallow analysis tools, in particular full syntactic parsers? An obvious issue is that one does not expect the phrase structure representation of English (as produced by state-of-the-art treebank parsers) to carry over to less configurational languages. Therefore, (Hwa et al., 2002) extract a more language-independent dependency structure from the English parse as the basis for projection to Chinese. From the resulting (noisy) dependency treebank, a dependency parser is trained using the techniques of (Collins, 1999). (Hwa et al., 2002) report that the noise in the projected treebank is still a major challenge, suggesting that a future research focus should be on the filtering of (parts of) unreliable trees and statistical word alignment models sensitive to the syntactic projection framework. Our hypothesis is that the quality of the resulting parser/grammar for language can be significantly improved if the training method for the parser is changed to accomodate for training data which are in part unreliable. The experiments we report in this paper focus on a specific part of the problem: we replace stand"
P04-1060,P02-1017,0,0.0729344,"ge can be significantly improved if the training method for the parser is changed to accomodate for training data which are in part unreliable. The experiments we report in this paper focus on a specific part of the problem: we replace standard treebank training with an Expectation-Maximization (EM) algorithm for PCFGs, augmented by weighting factors for the reliability of training data, following the approach of (Nigam et al., 2000), who apply it for EM training of a text classifier. The factors are only sensitive to the constituent/distituent (C/D) status of each span of the string in (cp. (Klein and Manning, 2002)). The C/D status is derived from an aligned parallel corpus in a way discussed in section 2. We use the Europarl corpus (Koehn, 2002), and the statistical word alignment was performed with the GIZA++ toolkit (Al-Onaizan et al., 1999; Och and Ney, 2003).1 For the current experiments we assume no preexisting parser for any of the languages, contrary to the information projection scenario. While better absolute results could be expected using one or more parsers for the languages involved, we think that it is important to isolate the usefulness of exploiting just crosslinguistic word order diver"
P04-1060,N03-1017,0,0.00607272,"images is     which do not form a contiguous string in   . The sequences     or       are  -induced blocks. Let us define a maximal  -block as an  -block  $ 35353  * , such that adding  $#= at the beginning or  *> at the end is either (i) impossible (because it would lead to a non-block, or  $?= or  *> do not exist as we are at the beginning or end of the string), or (ii) it would introduce a new crossing alignment 2 The block notion we are defining in this section is indirectly related to the concept of a “phrase” in recent work in Statistical Machine Translation. (Koehn et al., 2003) show that exploiting all contiguous word blocks in phrase-based alignment is better than focusing on syntactic constituents only. In our context, we are interested in inducing syntactic constituents based on alignment information; given the observations from Statistical MT, it does not come as a surprise that there is no direct link from blocks to constituents. Our work can be seen as an attempt to zero in on the distinction between the concepts; we find that it is most useful to keep track of the boundaries between blocks. (Wu, 1997) also includes a brief discussion of crossing constraints t"
P04-1060,J03-1002,0,0.00717426,"ing with an Expectation-Maximization (EM) algorithm for PCFGs, augmented by weighting factors for the reliability of training data, following the approach of (Nigam et al., 2000), who apply it for EM training of a text classifier. The factors are only sensitive to the constituent/distituent (C/D) status of each span of the string in (cp. (Klein and Manning, 2002)). The C/D status is derived from an aligned parallel corpus in a way discussed in section 2. We use the Europarl corpus (Koehn, 2002), and the statistical word alignment was performed with the GIZA++ toolkit (Al-Onaizan et al., 1999; Och and Ney, 2003).1 For the current experiments we assume no preexisting parser for any of the languages, contrary to the information projection scenario. While better absolute results could be expected using one or more parsers for the languages involved, we think that it is important to isolate the usefulness of exploiting just crosslinguistic word order divergences in order to obtain partial prior knowledge about the constituent structure of a language, which is then exploited in an EM learning approach (section 3). Not using a parser for some languages also makes it possible to compare various language pai"
P04-1060,C00-2139,0,0.0526754,"Missing"
P04-1060,J97-3002,0,0.0599752,"recent work in Statistical Machine Translation. (Koehn et al., 2003) show that exploiting all contiguous word blocks in phrase-based alignment is better than focusing on syntactic constituents only. In our context, we are interested in inducing syntactic constituents based on alignment information; given the observations from Statistical MT, it does not come as a surprise that there is no direct link from blocks to constituents. Our work can be seen as an attempt to zero in on the distinction between the concepts; we find that it is most useful to keep track of the boundaries between blocks. (Wu, 1997) also includes a brief discussion of crossing constraints that can be derived from phrase structure correspondences. to the block.3 String     in (1) is not a maximal  -block, because       is an  -block; but       is maximal since   is the final word of the sentence and         is a non-block. We can now make the initial observation precise that (1) and (2) have the same block structure, but the constituent structures are different (and this is not due to an incorrect alignment).    is a maximal block in both cases, but while it is a constituent in (1), it isn’t"
P04-1060,N01-1026,0,0.043065,"xts for the identification of string spans that cannot be constituents in one of the languages. This information is exploited in monolingual PCFG grammar induction for that language, within an augmented version of the inside-outside algorithm. Besides the aligned corpus, no other resources are required. We discuss an implemented system and present experimental results with an evaluation against the Penn Treebank. 1 Introduction There have been a number of recent studies exploiting parallel corpora in bootstrapping of monolingual analysis tools. In the “information projection” approach (e.g., (Yarowsky and Ngai, 2001)), statistical word alignment is applied to a parallel corpus of English and some other language for which no tagger/morphological analyzer/chunker etc. (henceforth simply: analysis tool) exists. A high-quality analysis tool is applied to the English text, and the statistical word alignment is used to project a (noisy) target annotation to the version of the text. Robust learning techniques are then applied to bootstrap an analysis tool for , using the annotations projected with high confidence as the initial training data. (Confidence of both the English analysis tool and the statistical word"
P06-2048,P93-1005,0,0.0751732,"data set. P(NP → NP PP, NP → DT NN, DT → that, NN → money, PP → IN NP, IN → in, NP → DT NN, DT → the, NN → market) Obviously this joint distribution is just as difficult to assess and compute with as P (tree). However there exist cubic-time dynamic programming algorithms to find the most likely parse if we assume that all CFG rule applications are marginally 369 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 369–376, c Sydney, July 2006. 2006 Association for Computational Linguistics The major alternative to PCFG-based approaches are so-called history-based parsers (Black et al., 1993). These parsers differ from PCFG parsers in that they incorporate context by using a more complex probability model, rather than by modifying the data itself. The tradeoff to using a more powerful probabilistic model is that one can no longer employ dynamic programming to find the most probable parse. Thus one trades assurances of polynomial running time for greater modeling flexibility. work that is viable not only for parsing, but for other application areas that current rely on dynamic programming, like phrase-based machine translation. 2 Preliminaries For the following discussion, it will"
P06-2048,A00-2018,0,0.238607,"f one another. The problem, of course, with this simplification is that although it is computationally attractive, it is usually too strong of an independence assumption. To mitigate this loss of context, without sacrificing algorithmic tractability, typically researchers annotate the nodes of the parse tree with contextual information. A simple example is the annotation of nodes with their parent labels (Johnson, 1998). 1 Introduction Much of the current research into probabilistic parsing is founded on probabilistic contextfree grammars (PCFGs) (Collins, 1996; Charniak, 1997; Collins, 1999; Charniak, 2000; Charniak, 2001; Klein and Manning, 2003). For instance, consider the parse tree in Figure 1. One way to decompose this parse tree is to view it as a sequence of applications of CFG rules. For this particular tree, we could view it as the application of rule “NP → NP PP,” followed by rule “NP → DT NN,” followed by rule “DT → that,” and so forth. Hence instead of analyzing P (tree), we deal with the more modular: The choice of which annotations to use is one of the main features that distinguish parsers based on this approach. Generally, this approach has proven quite effective in producing En"
P06-2048,P01-1017,0,0.0269819,"he problem, of course, with this simplification is that although it is computationally attractive, it is usually too strong of an independence assumption. To mitigate this loss of context, without sacrificing algorithmic tractability, typically researchers annotate the nodes of the parse tree with contextual information. A simple example is the annotation of nodes with their parent labels (Johnson, 1998). 1 Introduction Much of the current research into probabilistic parsing is founded on probabilistic contextfree grammars (PCFGs) (Collins, 1996; Charniak, 1997; Collins, 1999; Charniak, 2000; Charniak, 2001; Klein and Manning, 2003). For instance, consider the parse tree in Figure 1. One way to decompose this parse tree is to view it as a sequence of applications of CFG rules. For this particular tree, we could view it as the application of rule “NP → NP PP,” followed by rule “NP → DT NN,” followed by rule “DT → that,” and so forth. Hence instead of analyzing P (tree), we deal with the more modular: The choice of which annotations to use is one of the main features that distinguish parsers based on this approach. Generally, this approach has proven quite effective in producing English phrase-str"
P06-2048,P96-1025,0,0.0767039,"et Figure 1: Example parse tree. independent of one another. The problem, of course, with this simplification is that although it is computationally attractive, it is usually too strong of an independence assumption. To mitigate this loss of context, without sacrificing algorithmic tractability, typically researchers annotate the nodes of the parse tree with contextual information. A simple example is the annotation of nodes with their parent labels (Johnson, 1998). 1 Introduction Much of the current research into probabilistic parsing is founded on probabilistic contextfree grammars (PCFGs) (Collins, 1996; Charniak, 1997; Collins, 1999; Charniak, 2000; Charniak, 2001; Klein and Manning, 2003). For instance, consider the parse tree in Figure 1. One way to decompose this parse tree is to view it as a sequence of applications of CFG rules. For this particular tree, we could view it as the application of rule “NP → NP PP,” followed by rule “NP → DT NN,” followed by rule “DT → that,” and so forth. Hence instead of analyzing P (tree), we deal with the more modular: The choice of which annotations to use is one of the main features that distinguish parsers based on this approach. Generally, this appr"
P06-2048,J98-4004,0,0.0435103,"nk, compares favorably with other stateof-the-art approaches, in terms of both accuracy and speed. PP NP DT NN IN that money in NP DT NN the market Figure 1: Example parse tree. independent of one another. The problem, of course, with this simplification is that although it is computationally attractive, it is usually too strong of an independence assumption. To mitigate this loss of context, without sacrificing algorithmic tractability, typically researchers annotate the nodes of the parse tree with contextual information. A simple example is the annotation of nodes with their parent labels (Johnson, 1998). 1 Introduction Much of the current research into probabilistic parsing is founded on probabilistic contextfree grammars (PCFGs) (Collins, 1996; Charniak, 1997; Collins, 1999; Charniak, 2000; Charniak, 2001; Klein and Manning, 2003). For instance, consider the parse tree in Figure 1. One way to decompose this parse tree is to view it as a sequence of applications of CFG rules. For this particular tree, we could view it as the application of rule “NP → NP PP,” followed by rule “NP → DT NN,” followed by rule “DT → that,” and so forth. Hence instead of analyzing P (tree), we deal with the more m"
P06-2048,P03-1054,0,0.222459,"ourse, with this simplification is that although it is computationally attractive, it is usually too strong of an independence assumption. To mitigate this loss of context, without sacrificing algorithmic tractability, typically researchers annotate the nodes of the parse tree with contextual information. A simple example is the annotation of nodes with their parent labels (Johnson, 1998). 1 Introduction Much of the current research into probabilistic parsing is founded on probabilistic contextfree grammars (PCFGs) (Collins, 1996; Charniak, 1997; Collins, 1999; Charniak, 2000; Charniak, 2001; Klein and Manning, 2003). For instance, consider the parse tree in Figure 1. One way to decompose this parse tree is to view it as a sequence of applications of CFG rules. For this particular tree, we could view it as the application of rule “NP → NP PP,” followed by rule “NP → DT NN,” followed by rule “DT → that,” and so forth. Hence instead of analyzing P (tree), we deal with the more modular: The choice of which annotations to use is one of the main features that distinguish parsers based on this approach. Generally, this approach has proven quite effective in producing English phrase-structure grammar parsers tha"
P06-2048,P95-1037,0,0.0791806,"hat maps a subset W of the variables of X to values in their respective domains. We define dom(w) = W. When W = X, then we say that w is a full assignment of X. The trivial assignment of X makes no variable assignments. Let w(X) denote the value that partial assignment w assigns to variable X. For value x ∈ dom(X), let w[X = x] denote the assignment identical to w except that w[X = x](X) = x. For a set Y of variables, let w|Y denote the restriction of partial assignment w to the variables in dom(w) ∩ Y. There are two canonical parsers that fall into this category: the decision-tree parser of (Magerman, 1995), and the maximum-entropy parser of (Ratnaparkhi, 1997). Both showed decent results on parsing the Penn Treebank, but in the decade since these papers were published, history-based parsers have been largely ignored by the research community in favor of PCFG-based approaches. There are several reasons why this may be. First is naturally the matter of time efficiency. Magerman reports decent parsing times, but for the purposes of efficiency, must restrict his results to sentences of length 40 or less. Furthermore, his twophase stack decoder is a bit complicated and is acknowledged to require too"
P06-2048,W97-0301,0,0.0285936,"in their respective domains. We define dom(w) = W. When W = X, then we say that w is a full assignment of X. The trivial assignment of X makes no variable assignments. Let w(X) denote the value that partial assignment w assigns to variable X. For value x ∈ dom(X), let w[X = x] denote the assignment identical to w except that w[X = x](X) = x. For a set Y of variables, let w|Y denote the restriction of partial assignment w to the variables in dom(w) ∩ Y. There are two canonical parsers that fall into this category: the decision-tree parser of (Magerman, 1995), and the maximum-entropy parser of (Ratnaparkhi, 1997). Both showed decent results on parsing the Penn Treebank, but in the decade since these papers were published, history-based parsers have been largely ignored by the research community in favor of PCFG-based approaches. There are several reasons why this may be. First is naturally the matter of time efficiency. Magerman reports decent parsing times, but for the purposes of efficiency, must restrict his results to sentences of length 40 or less. Furthermore, his twophase stack decoder is a bit complicated and is acknowledged to require too much memory to handle certain sentences. Ratnaparkhi i"
P06-2048,J03-4003,0,\N,Missing
P09-2010,W06-2920,0,0.130123,"ezler et al., 2002). 1 Introduction The divide between grammar-driven and datadriven approaches to parsing has become less pronounced in recent years due to extensive work on robustness and efficiency for the grammar-driven approaches (Riezler et al., 2002; Cahill et al., 2008b). The linguistic generalizations captured in such knowledge-based resources are thus increasingly available for use in practical applications. The NLP-community has in recent years witnessed a surge of interest in dependency-based approaches to syntactic parsing, spurred by the CoNLL shared tasks of dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007). Nivre and McDonald (2008) show how two different approaches to dependency parsing, the graphbased and transition-based approaches, may be combined and subsequently learn to complement each other to achieve improved parse results for a range of different languages. In this paper, we show how a data-driven dependency parser may straightforwardly be modified to learn directly from a grammar-driven parser. We evaluate on English and German and show significant improvements for both languages. Like Nivre 3 Dependency conversion and feature extraction In extracting information"
P09-2010,W02-1503,0,0.0983016,"nglish and German and show significant improvements stemming from the proposed dependency structure as well as various other, deep linguistic features derived from the respective grammars. 2 Grammar-driven LFG-parsing The XLE system (Crouch et al., 2007) performs unification-based parsing using hand-crafted LFG grammars. It processes raw text and assigns to it both a phrase-structural (‘c-structure’) and a feature structural, functional (‘f-structure’). In the work described in this paper, we employ the XLE platform using the grammars available for English and German from the ParGram project (Butt et al., 2002). In order to increase the coverage of the grammars, we employ the robustness techniques of fragment parsing and ‘skimming’ available in XLE (Riezler et al., 2002). 1 Introduction The divide between grammar-driven and datadriven approaches to parsing has become less pronounced in recent years due to extensive work on robustness and efficiency for the grammar-driven approaches (Riezler et al., 2002; Cahill et al., 2008b). The linguistic generalizations captured in such knowledge-based resources are thus increasingly available for use in practical applications. The NLP-community has in recent ye"
P09-2010,J08-1003,0,0.0471193,"Missing"
P09-2010,W08-1705,0,0.0186163,"ructural, functional (‘f-structure’). In the work described in this paper, we employ the XLE platform using the grammars available for English and German from the ParGram project (Butt et al., 2002). In order to increase the coverage of the grammars, we employ the robustness techniques of fragment parsing and ‘skimming’ available in XLE (Riezler et al., 2002). 1 Introduction The divide between grammar-driven and datadriven approaches to parsing has become less pronounced in recent years due to extensive work on robustness and efficiency for the grammar-driven approaches (Riezler et al., 2002; Cahill et al., 2008b). The linguistic generalizations captured in such knowledge-based resources are thus increasingly available for use in practical applications. The NLP-community has in recent years witnessed a surge of interest in dependency-based approaches to syntactic parsing, spurred by the CoNLL shared tasks of dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007). Nivre and McDonald (2008) show how two different approaches to dependency parsing, the graphbased and transition-based approaches, may be combined and subsequently learn to complement each other to achieve improved parse results f"
P09-2010,J93-2004,0,0.033081,"htforwardly be modified to learn directly from a grammar-driven parser. We evaluate on English and German and show significant improvements for both languages. Like Nivre 3 Dependency conversion and feature extraction In extracting information from the output of the deep grammars we wish to capture as much of the precise, linguistic generalizations embodied in the grammars as possible, whilst keeping with the requirements posed by the dependency parser. The process is illustrated in Figure 1. 3.1 Data The English data set consists of the Wall Street Journal sections 2-24 of the Penn treebank (Marcus et al., 1993), converted to dependency format. The treebank data used for German is the Tiger 37 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 37–40, c Suntec, Singapore, 4 August 2009. 2009 ACL and AFNLP PRED V TYPE S UBJ ‘halteh. . .i’ predicative “pro” P RED ‘Verhalten’ C ASE acc S PEC f3“das”            OBJ     A DJUNCT f4“damalige”   f2      P RED ‘f¨urh. . .i’   i  XCOMP-PRED PTYPE hnosem P RED ‘richtig’ O BJ S UBJ converted: X COMP -P RED S UBJ - OBJ S PEC S UBJ A DJCT O BJ Ich halte das damalige Verhalten f¨ur richtig. 1sg pred. acc SB nosem NK N"
P09-2010,P08-1108,0,0.553644,"between grammar-driven and datadriven approaches to parsing has become less pronounced in recent years due to extensive work on robustness and efficiency for the grammar-driven approaches (Riezler et al., 2002; Cahill et al., 2008b). The linguistic generalizations captured in such knowledge-based resources are thus increasingly available for use in practical applications. The NLP-community has in recent years witnessed a surge of interest in dependency-based approaches to syntactic parsing, spurred by the CoNLL shared tasks of dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007). Nivre and McDonald (2008) show how two different approaches to dependency parsing, the graphbased and transition-based approaches, may be combined and subsequently learn to complement each other to achieve improved parse results for a range of different languages. In this paper, we show how a data-driven dependency parser may straightforwardly be modified to learn directly from a grammar-driven parser. We evaluate on English and German and show significant improvements for both languages. Like Nivre 3 Dependency conversion and feature extraction In extracting information from the output of the deep grammars we wish to"
P09-2010,nivre-etal-2006-maltparser,0,0.310635,"DJUNCT f4“damalige”   f2      P RED ‘f¨urh. . .i’   i  XCOMP-PRED PTYPE hnosem P RED ‘richtig’ O BJ S UBJ converted: X COMP -P RED S UBJ - OBJ S PEC S UBJ A DJCT O BJ Ich halte das damalige Verhalten f¨ur richtig. 1sg pred. acc SB nosem NK NK OA NK MO gold: f1 Figure 1: Treebank enrichment with LFG output; German example: I consider the past behaviour correct. treebank (Brants et al., 2004), where we employ the version released with the CoNLL-X shared task on dependency parsing (Buchholz and Marsi, 2006). use during parsing from the German and English XLE-parses. 3.2 MaltParser (Nivre et al., 2006a) is a languageindependent system for data-driven dependency parsing which is freely available.1 MaltParser is based on a deterministic parsing strategy in combination with treebank-induced classifiers for predicting parse transitions. MaltParser constructs parsing as a set of transitions between parse configurations. A parse configuration is a triple hS, I, Gi, where S represents the parse stack, I is the queue of remaining input tokens, and G represents the dependency graph defined thus far. The feature model in MaltParser defines the relevant attributes of tokens in a parse configuration."
P09-2010,W06-2933,0,0.051148,"Missing"
P09-2010,P02-1035,0,0.0355219,"rom the respective grammars. 2 Grammar-driven LFG-parsing The XLE system (Crouch et al., 2007) performs unification-based parsing using hand-crafted LFG grammars. It processes raw text and assigns to it both a phrase-structural (‘c-structure’) and a feature structural, functional (‘f-structure’). In the work described in this paper, we employ the XLE platform using the grammars available for English and German from the ParGram project (Butt et al., 2002). In order to increase the coverage of the grammars, we employ the robustness techniques of fragment parsing and ‘skimming’ available in XLE (Riezler et al., 2002). 1 Introduction The divide between grammar-driven and datadriven approaches to parsing has become less pronounced in recent years due to extensive work on robustness and efficiency for the grammar-driven approaches (Riezler et al., 2002; Cahill et al., 2008b). The linguistic generalizations captured in such knowledge-based resources are thus increasingly available for use in practical applications. The NLP-community has in recent years witnessed a surge of interest in dependency-based approaches to syntactic parsing, spurred by the CoNLL shared tasks of dependency parsing (Buchholz and Marsi,"
P09-2010,D07-1096,0,\N,Missing
P10-1111,W97-0307,0,0.140606,"Missing"
P10-1111,W02-1503,0,0.0974823,"Missing"
P10-1111,P04-1041,1,0.830825,"Missing"
P10-1111,J08-1003,1,0.872782,"Missing"
P10-1111,P06-2018,1,0.937482,"Missing"
P10-1111,W04-1905,0,0.0139116,"). Due to the constraints imposed on the classification, the function labeller can no longer assign two subjects to the same S node. Faced with two nodes whose most probable label is SB, it has to decide on one of them taking the next best label for the other. This way, it outputs the optimal solution with respect to the set of constraints. Note that this requires the feature model not only to rank the correct label highest but also to provide a reasonable ranking of the other labels as well. 4 Evaluation We conducted a number of experiments using 1,866 sentences of the TiGer Dependency Bank (Forst et al., 2004) as our test set. The TiGerDB is a part of the TiGer Treebank semi-automatically converted into a dependency representation. We use the manually labelled TiGer trees corresponding to the sentences in the TiGerDB for assessing the labelling quality in the intrinsic evaluation, and precision f-score 83.60 83.20 recall tagging acc. 82.81 97.97 Table 1: evalb unlabelled parsing scores on test set for Berkeley Parser trained on 48,000 sentences (sentence length ≤ 40) 4.1 Intrinsic Evaluation In the intrinsic evaluation, we measured the quality of the labelling itself. We used the node span evaluati"
P10-1111,W07-1203,0,0.18101,"Missing"
P10-1111,P03-1054,0,0.0158506,"e functions constitute the core meaning of a sentence (as in: who did what to whom), it is important to get them right. We present a system that adds grammatical function labels to constituent parser output for German in a postprocessing step. We combine a statistical classifier with an integer linear program (ILP) to model non-violable global linguistic constraints, restricting the solution space of the classifier to those labellings that comply with our set of global constraints. There are, of course, many other ways of including functional information into the output of a syntactic parser. Klein and Manning (2003) show that merging some linguistically motivated function labels with specific syntactic categories can improve the performance of a PCFG model on Penn-II En1 Coordinate subjects/objects form a constituent that functions as a joint subject/object. 1087 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1087–1097, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics glish data.2 Tsarfaty and Sim’aan (2008) present a statistical model (Relational-Realizational Parsing) that alternates between functional and configurational"
P10-1111,P07-2051,0,0.192396,"be labelled OG or AG9 . 7 Note that some of these constraints are language specific in that they represent linguistic facts about German and do not necessarily hold for other languages. Furthermore, the constraints are treebank specific to a certain degree in that they use a TiGer-specific set of labels and are conditioned on TiGer-specific configurations and categories. 8 SB = subject, OA = accusative object, OA2 = second accusative object, DA = dative, OG = genitive object, OP = prepositional object, PD = predicate, OC = clausal object, EP = expletive es 9 AG = genitive adjunct 1090 Unlike Klenner (2007), we do not use predefined subcategorization frames, instead letting the statistical model choose arguments. In TiGer, sentences whose main verbs are formed from auxiliary-participle combinations, are annotated by embedding the participle under an extra VP node and non-subject arguments are sisters to the participle. Therefore we add an extension of the constraint in (6) to the constraint set in order to also include the daughters of an embedded VP node in such a case. Because of the particulars of the annotation scheme of TiGer, we can decide some labels in advance. As mentioned before, punct"
P10-1111,H05-1064,0,0.0761706,"Missing"
P10-1111,P95-1037,0,0.0370794,"number of daughters the number of terminals covered the lemma of the left/right corner terminal the category of the left/right corner terminal the category of the mother node the category of the mother’s head node the lemma of the mother’s head node the category of the grandmother node the category of the grandmother’s head node the lemma of the grandmother’s head node the case features for noun phrases the category for PP objects the lemma for PP objects (if terminal node) These features are also computed for the head of the phrase, determined using a set of headfinding rules in the style of Magerman (1995) adapted to TiGer. For lemmatisation, we use TreeTagger (Schmid, 1994) and case features of noun 1089 phrases are obtained from a full German morphological analyser based on (Schiller, 1994). If a noun phrase consists of a single word (e. g. pronouns, but also bare common nouns and proper nouns), all case values output by the analyser are used to reflect the case syncretism. For multi-word noun phrases, the case feature is computed by taking the intersection of all case-bearing words inside the noun phrase, i. e. determiners, pronouns, adjectives, common nouns and proper nouns. If, for some re"
P10-1111,P09-1039,0,0.0685521,"Missing"
P10-1111,E06-1011,0,0.201487,"Missing"
P10-1111,P06-1055,0,0.069168,"tension of the constraint in (6) to the constraint set in order to also include the daughters of an embedded VP node in such a case. Because of the particulars of the annotation scheme of TiGer, we can decide some labels in advance. As mentioned before, punctuation does not get a label in TiGer. We set the label for those nodes to −− (no label). Other examples are: the dependencies from TiGerDB for assessing the quality and coverage of the automatically acquired LFG resources in the extrinsic evaluation. In order to test on real parser output, the test set was parsed with the Berkeley Parser (Petrov et al., 2006) trained on 48k sentences of the TiGer corpus (Table 1), excluding the test set. Since the Berkeley Parser assumes projective structures, the training data and test data were made projective by raising non-projective nodes in the tree (K¨ubler, 2005). • If a node’s category is PTKVZ (separated verb particle), it is labeled SVP (separable verb particle). The maximum entropy classifier of the function labeller was trained on 46,473 sentences of the TiGer Treebank (excluding the test set) which yields about 1.2 million nodes as training samples. For training the Maximum Entropy Model, we used the"
P10-1111,C04-1197,0,0.0154566,"of German and does not allow for unary branching. The annotations use non-projective trees modelling long distance dependencies directly by crossing branches. Words are lemmatised and part-of-speech tagged with the Stuttgart-T¨ubingen Tag Set (STTS) (Schiller et al., 1999) and contain morphological annotations (Release 2). TiGer uses 25 syntactic categories and a set of 42 function labels to annotate the grammatical function of a phrase. The function labeller consists of two main components, a maximum entropy classifier and an integer linear program. This basic architecture was introduced by Punyakanok et al. (2004) for the task of semantic role labelling and since then has been applied to different NLP tasks without significant changes. In our case, its input is a bare tree 4 Although the classifier may, of course, still identify the wrong phrase as subject or object. structure (as obtained by a standard phrase structure parser) and it outputs a tree structure where every node is labelled with the grammatical relation it bears to its mother node. For each possible label and for each node, the classifier assigns a probability that this node is labelled by this label. This results in a complete probabilit"
P10-1111,W04-2401,0,0.0408733,"Missing"
P10-1111,C08-1112,0,0.0700124,"Missing"
P10-1111,A00-2031,0,\N,Missing
P10-1111,J08-2005,0,\N,Missing
P10-1111,J96-1002,0,\N,Missing
P11-1101,W10-4201,0,0.0187591,"no positive effect on the voice and precedence accuracy. The n-best evaluations even suggest that the LM scores negatively impact the ranker: the accuracy for the top 3 sentences increases much less as compared to the model that does not integrate LM scores.6 The n-best performance of a realisation ranker is practically relevant for re-ranking applications such as Velldal (2008). We think that it is also conceptually interesting. Previous evaluation studies suggest that the original corpus sentence is not always the only optimal realisation of a given linguistic input (Cahill and Forst, 2010; Belz and Kow, 2010). Humans seem to have varying preferences for word order contrasts in certain contexts. The n-best evaluation could reflect the behaviour of a ranking model with respect to the range of variations encountered in real discourse. The pilot human evaluation in the next Section deals with this question. 6 Human Evaluation Our experiment in Section 5.3 has shown that the accuracy of our linguistically informed ranking model dramatically increases when we consider the three 6 (Nakanishi et al., 2005) also note a negative effect of including LM scores in their model, pointing out that the LM was not"
P11-1101,W10-4237,0,0.0289516,"Missing"
P11-1101,W05-1601,0,0.202581,"ecture and the design of the input representation. Section 4 describes the setup for the experiments in Section 5. In Section 1008 6, we present the results from the human evaluation. 2 Related Work 2.1 Generation Background The first widely known data-driven approach to surface realisation, or tactical generation, (Langkilde and Knight, 1998) used language-model ngram statistics on a word lattice of candidate realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Marciniak and Strube, 2005; Belz, 2005, a.o.). Work on data-driven approaches has led to insights into the importance of linguistic features for sentence linearisation decisions (Ringger et al., 2004; Filippova and Strube, 2009). The availability of discriminative learning techniques for the ranking of candidate analyses output by broad-coverage grammars with rich linguistic representations, originally in parsing (Riezler et al., 2000; Riezler et al., 2002), has also led to a revival of interest in linguistically sophisticated reversible grammars as the basis for surface realisation (Velldal and Oepen, 2006; Cahill et al., 2007)."
P11-1101,C10-1012,0,0.0336633,"in linguistically sophisticated reversible grammars as the basis for surface realisation (Velldal and Oepen, 2006; Cahill et al., 2007). The grammar generates candidate analyses for an underlying representation and the ranker’s task is to predict the contextually appropriate realisation. The work that is most closely related to ours is Velldal (2008). He uses an MRS representation derived by an HPSG grammar that can be underspecified for information status. In his case, the underspecification is encoded in the grammar and not directly controlled. In multilingually oriented linearisation work, Bohnet et al. (2010) generate from semantic corpus annotations included in the CoNLL’09 shared task data. However, they note that these annotations are not suitable for full generation since they are often incomplete. Thus, it is not clear to which degree these annotations are actually underspecified for certain paraphrases. 2.2 Linguistic Background In competition-based linguistic theories (Optimality Theory and related frameworks), the use of argument alternations is construed as an effect of markedness hierarchies (Aissen, 1999; Aissen, 2003). Argument functions (subject, object, . . . ) on the one hand and th"
P11-1101,P09-1092,1,0.897952,"grammatically possible. Bresnan et al. (2007) correlate the use of the English dative alternation to a number of features such as givenness, pronominalisation, definiteness, constituent length, animacy of the involved verb arguments. These features are assumed to reflect the discourse acessibility of the arguments. Interestingly, the properties that have been used to model argument alternations in strict word order languages like English have been identified as factors that influence word order in free word order languages like German, see Filippova and Strube (2007) for a number of pointers. Cahill and Riester (2009) implement a model for German word order variation that approximates the information status of constituents through morphological features like definiteness, pronominalisation etc. We are not aware of any corpus-based generation studies investigating how these properties relate to argument alternations in free word order languages. 3 Generation Architecture Our data-driven methodology for investigating factors relevant to surface realisation uses a regeneration set-up2 with two main components: a) a grammar-based component used to parse a corpus sentence and map it to all its meaning-equivalen"
P11-1101,W07-2303,1,0.906034,"rube, 2005; Belz, 2005, a.o.). Work on data-driven approaches has led to insights into the importance of linguistic features for sentence linearisation decisions (Ringger et al., 2004; Filippova and Strube, 2009). The availability of discriminative learning techniques for the ranking of candidate analyses output by broad-coverage grammars with rich linguistic representations, originally in parsing (Riezler et al., 2000; Riezler et al., 2002), has also led to a revival of interest in linguistically sophisticated reversible grammars as the basis for surface realisation (Velldal and Oepen, 2006; Cahill et al., 2007). The grammar generates candidate analyses for an underlying representation and the ranker’s task is to predict the contextually appropriate realisation. The work that is most closely related to ours is Velldal (2008). He uses an MRS representation derived by an HPSG grammar that can be underspecified for information status. In his case, the underspecification is encoded in the grammar and not directly controlled. In multilingually oriented linearisation work, Bohnet et al. (2010) generate from semantic corpus annotations included in the CoNLL’09 shared task data. However, they note that these"
P11-1101,D09-1046,0,0.0256955,"Missing"
P11-1101,P07-1041,0,0.351701,"on the person scale than the agent, but an active is grammatically possible. Bresnan et al. (2007) correlate the use of the English dative alternation to a number of features such as givenness, pronominalisation, definiteness, constituent length, animacy of the involved verb arguments. These features are assumed to reflect the discourse acessibility of the arguments. Interestingly, the properties that have been used to model argument alternations in strict word order languages like English have been identified as factors that influence word order in free word order languages like German, see Filippova and Strube (2007) for a number of pointers. Cahill and Riester (2009) implement a model for German word order variation that approximates the information status of constituents through morphological features like definiteness, pronominalisation etc. We are not aware of any corpus-based generation studies investigating how these properties relate to argument alternations in free word order languages. 3 Generation Architecture Our data-driven methodology for investigating factors relevant to surface realisation uses a regeneration set-up2 with two main components: a) a grammar-based component used to parse a cor"
P11-1101,N09-2057,0,0.0127313,"valuation. 2 Related Work 2.1 Generation Background The first widely known data-driven approach to surface realisation, or tactical generation, (Langkilde and Knight, 1998) used language-model ngram statistics on a word lattice of candidate realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Marciniak and Strube, 2005; Belz, 2005, a.o.). Work on data-driven approaches has led to insights into the importance of linguistic features for sentence linearisation decisions (Ringger et al., 2004; Filippova and Strube, 2009). The availability of discriminative learning techniques for the ranking of candidate analyses output by broad-coverage grammars with rich linguistic representations, originally in parsing (Riezler et al., 2000; Riezler et al., 2002), has also led to a revival of interest in linguistically sophisticated reversible grammars as the basis for surface realisation (Velldal and Oepen, 2006; Cahill et al., 2007). The grammar generates candidate analyses for an underlying representation and the ranker’s task is to predict the contextually appropriate realisation. The work that is most closely related"
P11-1101,W07-1203,0,0.135806,"g-equivalent surface realisations, b) a statistical ranking component used to select the correct, i.e. contextually most appropriate surface realisation. Two variants of this set-up that we use are sketched in Figure 1. We generally use a hand-crafted, broad-coverage LFG for German (Rohrer and Forst, 2006) to parse a corpus sentence into a f(unctional) structure3 and generate all surface realisations from a given 2 Compare the bidirectional competition set-up in some Optimality-Theoretic work, e.g., (Kuhn, 2003). 3 The choice among alternative f-structures is done with a discriminative model (Forst, 2007). 1009 Sntx Snty SVM Ranker SVM Ranker Snta1 Snta2 ... Sntam LFG grammar Sntb1 Snta1 Snta2 ... Sntbn LFG Grammar FSa FSb Reverse Sem. Rules SEM FSa LFG grammar Sem. Rules FS1 LFG Grammar Snti Snti Figure 1: Generation pipelines f-structure, following the generation approach of Cahill et al. (2007). F-structures are attributevalue matrices representing grammatical functions and morphosyntactic features; their theoretical motivation lies in the abstraction over details of surface realisation. The grammar is implemented in the XLE framework (Crouch et al., 2006), which allows for reversible use o"
P11-1101,P10-1160,0,0.0208723,"ntation for Sentence (4) in Figure 2, the realiser will not generate an active realisation since the agent role cannot be instantiated by any phrase in the grammar. However, depending on the exact context there are typically options for realising the subject phrase in an active with very little descriptive content. Ideally, one would like to account for these phenomena in a meaning representation that underspecifies the lexicalisation of discourse referents, and also captures the reference of implicit arguments. Especially the latter task has hardly been addressed in NLP applications (but see Gerber and Chai (2010)). In order to work around that problem, we implemented some simple heuristics which underspecify the realisation of certain verb arguments. These rules define: 1. a set of pronouns (generic and neutral pronouns, universal quantifiers) that correspond to “trivial” agents in active and implicit agents 1010 in passive sentences; 2. a set of prepositional adjuncts in passive sentences that correspond to subjects in active sentence (e.g. causative and instrumental prepositions like durch “by means of”); 3. certain syntactic contexts where special underspecification devices are needed, e.g. coordin"
P11-1101,J95-2003,0,0.192951,"nt” features (ScalAl.): combinations of voice and role properties with morphological properties, e.g. “subject is singular”, “agent is 3rd person in active voice” (these are surface-independent, identical for each alternation candidate). The model for which we present our results is based on sentence-internal features only; as Cahill and Riester (2009) showed, these feature carry a considerable amount of implicit information about the discourse context (e.g. in the shape of referring expressions). We also implemented a set of explicitly inter-sentential features, inspired by Centering Theory (Grosz et al., 1995). This model did not improve over the intra-sentential model. Evaluation Measures In order to assess the general quality of our generation ranking models, we 4 The language model is trained on the German data release for the 2009 ACL Workshop on Machine Translation shared task, 11,991,277 total sentences. LM Ling. Model Match BLEU NIST Match BLEU NIST SEMn 68.2 10.72 15.04 0.68 12.95 27.66 0.759 13.14 SEMh 75.8 7.28 11.89 0.65 12.69 26.38 0.747 13.01 Table 2: Evaluation of Experiment 1 use several standard measures: a) exact match: how often does the model select the original corpus sentence,"
P11-1101,P98-1116,0,0.0627281,"king the risk of occasional overgeneration. The paper is structured as follows: Section 2 situates our methodology with respect to other work on surface realisation and briefly summarises the relevant theoretical linguistic background. In Section 3, we present our generation architecture and the design of the input representation. Section 4 describes the setup for the experiments in Section 5. In Section 1008 6, we present the results from the human evaluation. 2 Related Work 2.1 Generation Background The first widely known data-driven approach to surface realisation, or tactical generation, (Langkilde and Knight, 1998) used language-model ngram statistics on a word lattice of candidate realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Marciniak and Strube, 2005; Belz, 2005, a.o.). Work on data-driven approaches has led to insights into the importance of linguistic features for sentence linearisation decisions (Ringger et al., 2004; Filippova and Strube, 2009). The availability of discriminative learning techniques for the ranking of candidate analyses output by broad-coverage grammars with rich lingui"
P11-1101,W05-1510,0,0.0227506,"sentence is not always the only optimal realisation of a given linguistic input (Cahill and Forst, 2010; Belz and Kow, 2010). Humans seem to have varying preferences for word order contrasts in certain contexts. The n-best evaluation could reflect the behaviour of a ranking model with respect to the range of variations encountered in real discourse. The pilot human evaluation in the next Section deals with this question. 6 Human Evaluation Our experiment in Section 5.3 has shown that the accuracy of our linguistically informed ranking model dramatically increases when we consider the three 6 (Nakanishi et al., 2005) also note a negative effect of including LM scores in their model, pointing out that the LM was not trained on enough data. The corpus used for training our LM might also have been too small or distinct in genre. 1014 best sentences rather than only the top-ranked sentence. This means that the model sometimes predicts almost equal naturalness for different voice realisations. Moreover, in the case of word order, we know from previous evaluation studies, that humans sometimes prefer different realisations than the original corpus sentences. This Section investigates agreement in human judgemen"
P11-1101,A00-2026,0,0.142822,"In Section 3, we present our generation architecture and the design of the input representation. Section 4 describes the setup for the experiments in Section 5. In Section 1008 6, we present the results from the human evaluation. 2 Related Work 2.1 Generation Background The first widely known data-driven approach to surface realisation, or tactical generation, (Langkilde and Knight, 1998) used language-model ngram statistics on a word lattice of candidate realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Marciniak and Strube, 2005; Belz, 2005, a.o.). Work on data-driven approaches has led to insights into the importance of linguistic features for sentence linearisation decisions (Ringger et al., 2004; Filippova and Strube, 2009). The availability of discriminative learning techniques for the ranking of candidate analyses output by broad-coverage grammars with rich linguistic representations, originally in parsing (Riezler et al., 2000; Riezler et al., 2002), has also led to a revival of interest in linguistically sophisticated reversible grammars as the basis for surface realisation (Velldal"
P11-1101,P00-1061,1,0.519387,"rd lattice of candidate realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Marciniak and Strube, 2005; Belz, 2005, a.o.). Work on data-driven approaches has led to insights into the importance of linguistic features for sentence linearisation decisions (Ringger et al., 2004; Filippova and Strube, 2009). The availability of discriminative learning techniques for the ranking of candidate analyses output by broad-coverage grammars with rich linguistic representations, originally in parsing (Riezler et al., 2000; Riezler et al., 2002), has also led to a revival of interest in linguistically sophisticated reversible grammars as the basis for surface realisation (Velldal and Oepen, 2006; Cahill et al., 2007). The grammar generates candidate analyses for an underlying representation and the ranker’s task is to predict the contextually appropriate realisation. The work that is most closely related to ours is Velldal (2008). He uses an MRS representation derived by an HPSG grammar that can be underspecified for information status. In his case, the underspecification is encoded in the grammar and not direc"
P11-1101,P02-1035,0,0.0210818,"e realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Marciniak and Strube, 2005; Belz, 2005, a.o.). Work on data-driven approaches has led to insights into the importance of linguistic features for sentence linearisation decisions (Ringger et al., 2004; Filippova and Strube, 2009). The availability of discriminative learning techniques for the ranking of candidate analyses output by broad-coverage grammars with rich linguistic representations, originally in parsing (Riezler et al., 2000; Riezler et al., 2002), has also led to a revival of interest in linguistically sophisticated reversible grammars as the basis for surface realisation (Velldal and Oepen, 2006; Cahill et al., 2007). The grammar generates candidate analyses for an underlying representation and the ranker’s task is to predict the contextually appropriate realisation. The work that is most closely related to ours is Velldal (2008). He uses an MRS representation derived by an HPSG grammar that can be underspecified for information status. In his case, the underspecification is encoded in the grammar and not directly controlled. In mult"
P11-1101,C04-1097,0,0.120095,"sults from the human evaluation. 2 Related Work 2.1 Generation Background The first widely known data-driven approach to surface realisation, or tactical generation, (Langkilde and Knight, 1998) used language-model ngram statistics on a word lattice of candidate realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Marciniak and Strube, 2005; Belz, 2005, a.o.). Work on data-driven approaches has led to insights into the importance of linguistic features for sentence linearisation decisions (Ringger et al., 2004; Filippova and Strube, 2009). The availability of discriminative learning techniques for the ranking of candidate analyses output by broad-coverage grammars with rich linguistic representations, originally in parsing (Riezler et al., 2000; Riezler et al., 2002), has also led to a revival of interest in linguistically sophisticated reversible grammars as the basis for surface realisation (Velldal and Oepen, 2006; Cahill et al., 2007). The grammar generates candidate analyses for an underlying representation and the ranker’s task is to predict the contextually appropriate realisation. The work"
P11-1101,rohrer-forst-2006-improving,0,0.198238,"elate to argument alternations in free word order languages. 3 Generation Architecture Our data-driven methodology for investigating factors relevant to surface realisation uses a regeneration set-up2 with two main components: a) a grammar-based component used to parse a corpus sentence and map it to all its meaning-equivalent surface realisations, b) a statistical ranking component used to select the correct, i.e. contextually most appropriate surface realisation. Two variants of this set-up that we use are sketched in Figure 1. We generally use a hand-crafted, broad-coverage LFG for German (Rohrer and Forst, 2006) to parse a corpus sentence into a f(unctional) structure3 and generate all surface realisations from a given 2 Compare the bidirectional competition set-up in some Optimality-Theoretic work, e.g., (Kuhn, 2003). 3 The choice among alternative f-structures is done with a discriminative model (Forst, 2007). 1009 Sntx Snty SVM Ranker SVM Ranker Snta1 Snta2 ... Sntam LFG grammar Sntb1 Snta1 Snta2 ... Sntbn LFG Grammar FSa FSb Reverse Sem. Rules SEM FSa LFG grammar Sem. Rules FS1 LFG Grammar Snti Snti Figure 1: Generation pipelines f-structure, following the generation approach of Cahill et al. (20"
P11-1101,W06-1661,0,0.236129,"i, 2000; Marciniak and Strube, 2005; Belz, 2005, a.o.). Work on data-driven approaches has led to insights into the importance of linguistic features for sentence linearisation decisions (Ringger et al., 2004; Filippova and Strube, 2009). The availability of discriminative learning techniques for the ranking of candidate analyses output by broad-coverage grammars with rich linguistic representations, originally in parsing (Riezler et al., 2000; Riezler et al., 2002), has also led to a revival of interest in linguistically sophisticated reversible grammars as the basis for surface realisation (Velldal and Oepen, 2006; Cahill et al., 2007). The grammar generates candidate analyses for an underlying representation and the ranker’s task is to predict the contextually appropriate realisation. The work that is most closely related to ours is Velldal (2008). He uses an MRS representation derived by an HPSG grammar that can be underspecified for information status. In his case, the underspecification is encoded in the grammar and not directly controlled. In multilingually oriented linearisation work, Bohnet et al. (2010) generate from semantic corpus annotations included in the CoNLL’09 shared task data. However"
P11-1101,W09-2602,1,0.842156,"oach of Cahill et al. (2007). F-structures are attributevalue matrices representing grammatical functions and morphosyntactic features; their theoretical motivation lies in the abstraction over details of surface realisation. The grammar is implemented in the XLE framework (Crouch et al., 2006), which allows for reversible use of the same declarative grammar in the parsing and generation direction. To obtain a more abstract underlying representation (in the pipeline on the right-hand side of Figure 1), the present work uses an additional semantic construction component (Crouch and King, 2006; Zarrieß, 2009) to map LFG f-structures to meaning representations. For the reverse direction, the meaning representations are mapped to f-structures which can then be mapped to surface strings by the XLE generator (Zarrieß and Kuhn, 2010). For the final realisation ranking step in both pipelines, we used SVMrank, a Support Vector Machine-based learning tool (Joachims, 1996). The ranking step is thus technically independent from the LFG-based component. However, the grammar is used to produce the training data, pairs of corpus sentences and the possible alternations. The two pipelines allow us to vary the de"
P11-1101,C98-1112,0,\N,Missing
P13-1152,W10-4226,0,0.214049,"off (1992) mentions Example (1) where the sentence planning component needs to have access to the lexical knowledge that “order” and not “home” can be realized as a verb in English. (1) a. b. *John homed him with an order. John ordered him home. In recent data-driven generation research, the focus has somewhat shifted from full data-to-text systems to approaches that isolate well-defined subproblems from the NLG pipeline. In particular, the tasks of surface realization and referring expression generation (REG) have received increasing attention using a number of available annotated data sets (Belz and Kow, 2010; Belz et al., 2011). While these single-task approaches have given rise to many insights about algorithms and corpus-based modelling for specific phenomena, they can hardly deal with aspects of the architecture and interaction between generation levels. This paper suggests a middle ground between full data-to-text and single-task generation, combining two well-studied NLG problems. We integrate a discourse-level approach to REG with sentence-level surface realization in a data-driven framework. We address this integrated task with a set of components that can be trained on flexible inputs whi"
P13-1152,W07-2302,0,0.0189498,"lippova and Strube, 2007; Cahill and Riester, 2009). (Zarrieß et al., 2012) have recently argued that the good performance of these linguistically motivated word order models, which exploit morphosyntactic features of noun phrases (i.e. referents), is related to the fact that these morphosyntactic features implicitly encode a lot of knowledge about the underlying discourse or information structure. A considerable body of REG research has been done in the paradigm established by Dale (1989; 1995). More closely related to our work are approaches in the line of Siddarthan and Copestake (2004) or Belz and Varges (2007) who generate contextually appropriate REs for instances of a referent in a text. Belz and Varges (2007)’s GREC data set includes annotations of implicit subjects in coordinations. Zarrieß et al. (2011) deal with implicit subjects in passives, proposing a set of heuristics for adding these agents to the generation input. Roth and Frank (2012) acquire automatic annotations of implicit roles for the purpose of studying coherence patterns in texts. Implicit referents have also received attention for the analysis of semantic roles (Gerber and Chai, 2010; Ruppenhofer et al., 2010). Statistical meth"
P13-1152,W11-2832,0,0.265471,"Example (1) where the sentence planning component needs to have access to the lexical knowledge that “order” and not “home” can be realized as a verb in English. (1) a. b. *John homed him with an order. John ordered him home. In recent data-driven generation research, the focus has somewhat shifted from full data-to-text systems to approaches that isolate well-defined subproblems from the NLG pipeline. In particular, the tasks of surface realization and referring expression generation (REG) have received increasing attention using a number of available annotated data sets (Belz and Kow, 2010; Belz et al., 2011). While these single-task approaches have given rise to many insights about algorithms and corpus-based modelling for specific phenomena, they can hardly deal with aspects of the architecture and interaction between generation levels. This paper suggests a middle ground between full data-to-text and single-task generation, combining two well-studied NLG problems. We integrate a discourse-level approach to REG with sentence-level surface realization in a data-driven framework. We address this integrated task with a set of components that can be trained on flexible inputs which allows us to syst"
P13-1152,W05-1601,0,0.0230272,"Cahill et al., 1999; Bateman and Zock, 2003). In recent data-driven research on NLG, many single tasks have been addressed with corpusbased methods. For surface realization, the standard set-up is to regenerate from syntactic representations that have been produced for realistic corpus sentences. The first widely known statistical approach by Langkilde and Knight (1998) used language-model n-gram statistics on a word lattice of candidate realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Belz, 2005). Work on data-driven approaches has led to insights about the importance of linguistic features for sentence 1548 linearization decisions (Ringger et al., 2004; Filippova and Strube, 2007; Cahill and Riester, 2009). (Zarrieß et al., 2012) have recently argued that the good performance of these linguistically motivated word order models, which exploit morphosyntactic features of noun phrases (i.e. referents), is related to the fact that these morphosyntactic features implicitly encode a lot of knowledge about the underlying discourse or information structure. A considerable body of REG researc"
P13-1152,C10-1012,0,0.0218968,"nment between units in the non-linguistic representation and the linguistic utterance can be learned. Marciniak and Strube (2005) propose an ILP model for global optimization in a generation task that is decomposed into a set of classifiers. Bohnet et al. (2011) deal with multi-level generation in a statistical framework and in a less restricted domain. They adopt a standard sequential pipeline approach. Recent corpus-based generation approaches faced the problem that existing standard treebank representations for parsing or other analysis tasks do not necessarily fit the needs of generation (Bohnet et al., 2010; Wanner et al., 2012). Zarrieß et al. (2011) discuss the problem of an input representation that is appropriately underspecified for the realistic generation of voice alternations. 3 The Data Set The data set for our generation experiments consists of 200 newspaper articles about robbery events. The articles were extracted from a large German newspaper corpus. A complete example text with RE annotations is given in Figure 2, Table 1 summarizes some data set statistics. 3.1 RE annotation The RE annotations mark explicit and implicit mentions of referents involved in the robbery event described"
P13-1152,W11-2835,0,0.025502,"alysis of semantic roles (Gerber and Chai, 2010; Ruppenhofer et al., 2010). Statistical methods for data-to-text generation have been explored only recently. Belz (2008) trains a probabilistic CFG to generate weather forecasts, Chen et al. (2010) induce a synchronous grammar to generate sportcaster text. Both address a restricted domain where a direct alignment between units in the non-linguistic representation and the linguistic utterance can be learned. Marciniak and Strube (2005) propose an ILP model for global optimization in a generation task that is decomposed into a set of classifiers. Bohnet et al. (2011) deal with multi-level generation in a statistical framework and in a less restricted domain. They adopt a standard sequential pipeline approach. Recent corpus-based generation approaches faced the problem that existing standard treebank representations for parsing or other analysis tasks do not necessarily fit the needs of generation (Bohnet et al., 2010; Wanner et al., 2012). Zarrieß et al. (2011) discuss the problem of an input representation that is appropriately underspecified for the realistic generation of voice alternations. 3 The Data Set The data set for our generation experiments co"
P13-1152,D12-1085,1,0.883203,"Missing"
P13-1152,C10-1011,0,0.0710366,"threatened the two men p:0   dem Ehemann v:1 , ihn v:1 zusammenzuschlagen. beat up. him  the husband    Er v:1 gab deshalb seine v:1 Brieftasche ohne Gegenwehrag:v,the:p heraus. gave therefore his wallet without resistanceag:v,the:p out. He    Anschließend nahmen ihm v:1 die R¨auber p:0 noch die Armbanduhrposs:v ab und fl¨uchtetenag:p . Afterwards took also the watchposs:v off and fleedag:p . him  the robbers Figure 2: Example text with RE annotations, oval boxes mark victim mentions, square boxes mark perp mentions, heads of implicit arguments are underlined the Bohnet (2010) dependency parser to obtain an automatic annotation of shallow or surface dependencies for the corpus sentences. The deep syntactic dependencies are derived from the shallow layer by a set of hand-written transformation rules. The goal is to link referents to their main predicate in a uniform way, independently of the surface-syntactic realization of the verb. We address passives, nominalizations and possessives corresponding to the contexts where we annotated implicit referents (see above). The transformations are defined as follows: 1. remove auxiliary nodes, verb morphology and finiteness,"
P13-1152,P09-1092,0,0.0280708,"egenerate from syntactic representations that have been produced for realistic corpus sentences. The first widely known statistical approach by Langkilde and Knight (1998) used language-model n-gram statistics on a word lattice of candidate realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Belz, 2005). Work on data-driven approaches has led to insights about the importance of linguistic features for sentence 1548 linearization decisions (Ringger et al., 2004; Filippova and Strube, 2007; Cahill and Riester, 2009). (Zarrieß et al., 2012) have recently argued that the good performance of these linguistically motivated word order models, which exploit morphosyntactic features of noun phrases (i.e. referents), is related to the fact that these morphosyntactic features implicitly encode a lot of knowledge about the underlying discourse or information structure. A considerable body of REG research has been done in the paradigm established by Dale (1989; 1995). More closely related to our work are approaches in the line of Siddarthan and Copestake (2004) or Belz and Varges (2007) who generate contextually ap"
P13-1152,P89-1009,0,0.315072,"ights about the importance of linguistic features for sentence 1548 linearization decisions (Ringger et al., 2004; Filippova and Strube, 2007; Cahill and Riester, 2009). (Zarrieß et al., 2012) have recently argued that the good performance of these linguistically motivated word order models, which exploit morphosyntactic features of noun phrases (i.e. referents), is related to the fact that these morphosyntactic features implicitly encode a lot of knowledge about the underlying discourse or information structure. A considerable body of REG research has been done in the paradigm established by Dale (1989; 1995). More closely related to our work are approaches in the line of Siddarthan and Copestake (2004) or Belz and Varges (2007) who generate contextually appropriate REs for instances of a referent in a text. Belz and Varges (2007)’s GREC data set includes annotations of implicit subjects in coordinations. Zarrieß et al. (2011) deal with implicit subjects in passives, proposing a set of heuristics for adding these agents to the generation input. Roth and Frank (2012) acquire automatic annotations of implicit roles for the purpose of studying coherence patterns in texts. Implicit referents ha"
P13-1152,P84-1107,0,0.138315,"and syntax can be modelled in sequential generation architecture where the RE component has access to information about syntactic realization and an approximative, intermediate linearization. Such a system is reminiscent of earlier work in rulebased generation that implements an interactive or revision-based feedback between discourse-level planning and linguistic realisation (Hovy, 1988; Robin, 1993). 2 Related Work Despite the common view of NLG as a pipeline process, it is a well-known problem that highlevel, conceptual knowledge and low-level linguistic knowledge are tightly interleaved (Danlos, 1984; Mellish et al., 2000). In rule-based, strictly sequential generators these interactions can lead to a so-called generation gap, where a downstream module cannot realize a text or sentence plan generated by the preceding modules (Meteer, 1991; Wanner, 1994). For this reason, a number of other architectures has been proposed, see De Smedt et al. (1996) for an overview. For reasons of tractability and scalability, many practical NLG systems still have been designed as sequential pipelines that follow the basic layout of macroplanning-microplanning-linguistic realization (Reiter, 1994; Cahill et"
P13-1152,P07-1041,0,0.018174,"the standard set-up is to regenerate from syntactic representations that have been produced for realistic corpus sentences. The first widely known statistical approach by Langkilde and Knight (1998) used language-model n-gram statistics on a word lattice of candidate realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Belz, 2005). Work on data-driven approaches has led to insights about the importance of linguistic features for sentence 1548 linearization decisions (Ringger et al., 2004; Filippova and Strube, 2007; Cahill and Riester, 2009). (Zarrieß et al., 2012) have recently argued that the good performance of these linguistically motivated word order models, which exploit morphosyntactic features of noun phrases (i.e. referents), is related to the fact that these morphosyntactic features implicitly encode a lot of knowledge about the underlying discourse or information structure. A considerable body of REG research has been done in the paradigm established by Dale (1989; 1995). More closely related to our work are approaches in the line of Siddarthan and Copestake (2004) or Belz and Varges (2007) w"
P13-1152,P10-1160,0,0.0256792,"line of Siddarthan and Copestake (2004) or Belz and Varges (2007) who generate contextually appropriate REs for instances of a referent in a text. Belz and Varges (2007)’s GREC data set includes annotations of implicit subjects in coordinations. Zarrieß et al. (2011) deal with implicit subjects in passives, proposing a set of heuristics for adding these agents to the generation input. Roth and Frank (2012) acquire automatic annotations of implicit roles for the purpose of studying coherence patterns in texts. Implicit referents have also received attention for the analysis of semantic roles (Gerber and Chai, 2010; Ruppenhofer et al., 2010). Statistical methods for data-to-text generation have been explored only recently. Belz (2008) trains a probabilistic CFG to generate weather forecasts, Chen et al. (2010) induce a synchronous grammar to generate sportcaster text. Both address a restricted domain where a direct alignment between units in the non-linguistic representation and the linguistic utterance can be learned. Marciniak and Strube (2005) propose an ILP model for global optimization in a generation task that is decomposed into a set of classifiers. Bohnet et al. (2011) deal with multi-level gene"
P13-1152,J95-2003,0,0.12689,"alians]p on [a young man]v . [Two italians]p are on trial because of an attack on [a young man]v . Sentence (2-a) is incoherent because the syntactic surface obscurs the intended meaning that “two italians” and “the two men” refer to the same referent. In order to generate the natural Sentence (2-b), the RE component needs information about linear precedence of the two perp instances and the nominalization of “attack”. These types of interactions between referential and syntactic realization have been thoroughly discussed in theoretical accounts of textual coherence, as e.g. Centering Theory (Grosz et al., 1995). The integrated modelling of REG and surface realization leads to a considerable expansion of the choice space. In a sentence with 3 referents that each have 10 RE candidates and can be freely ordered, the number of surface realizations increases from 6 to 6·103 , assuming that the remaining words can not be syntactically varied. Thus, even when the generation problem is restricted to these tasks, a fully integrated architecture faces scalability issues on realistic corpus data. In this work, we assume a modular set-up of the generation system that allows for a flexible ordering of the single"
P13-1152,P88-1020,0,0.263536,"rameters of the generation architecture: 1) the sequential order of the modules, 2) parallelization of modules, 3) joint vs. separate modelling of implicit referents. Our results suggest that the interactions between RE and syntax can be modelled in sequential generation architecture where the RE component has access to information about syntactic realization and an approximative, intermediate linearization. Such a system is reminiscent of earlier work in rulebased generation that implements an interactive or revision-based feedback between discourse-level planning and linguistic realisation (Hovy, 1988; Robin, 1993). 2 Related Work Despite the common view of NLG as a pipeline process, it is a well-known problem that highlevel, conceptual knowledge and low-level linguistic knowledge are tightly interleaved (Danlos, 1984; Mellish et al., 2000). In rule-based, strictly sequential generators these interactions can lead to a so-called generation gap, where a downstream module cannot realize a text or sentence plan generated by the preceding modules (Meteer, 1991; Wanner, 1994). For this reason, a number of other architectures has been proposed, see De Smedt et al. (1996) for an overview. For rea"
P13-1152,P98-1116,0,0.120748,"medt et al. (1996) for an overview. For reasons of tractability and scalability, many practical NLG systems still have been designed as sequential pipelines that follow the basic layout of macroplanning-microplanning-linguistic realization (Reiter, 1994; Cahill et al., 1999; Bateman and Zock, 2003). In recent data-driven research on NLG, many single tasks have been addressed with corpusbased methods. For surface realization, the standard set-up is to regenerate from syntactic representations that have been produced for realistic corpus sentences. The first widely known statistical approach by Langkilde and Knight (1998) used language-model n-gram statistics on a word lattice of candidate realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Belz, 2005). Work on data-driven approaches has led to insights about the importance of linguistic features for sentence 1548 linearization decisions (Ringger et al., 2004; Filippova and Strube, 2007; Cahill and Riester, 2009). (Zarrieß et al., 2012) have recently argued that the good performance of these linguistically motivated word order models, which exploit morphos"
P13-1152,W05-0618,0,0.187792,"ations of implicit roles for the purpose of studying coherence patterns in texts. Implicit referents have also received attention for the analysis of semantic roles (Gerber and Chai, 2010; Ruppenhofer et al., 2010). Statistical methods for data-to-text generation have been explored only recently. Belz (2008) trains a probabilistic CFG to generate weather forecasts, Chen et al. (2010) induce a synchronous grammar to generate sportcaster text. Both address a restricted domain where a direct alignment between units in the non-linguistic representation and the linguistic utterance can be learned. Marciniak and Strube (2005) propose an ILP model for global optimization in a generation task that is decomposed into a set of classifiers. Bohnet et al. (2011) deal with multi-level generation in a statistical framework and in a less restricted domain. They adopt a standard sequential pipeline approach. Recent corpus-based generation approaches faced the problem that existing standard treebank representations for parsing or other analysis tasks do not necessarily fit the needs of generation (Bohnet et al., 2010; Wanner et al., 2012). Zarrieß et al. (2011) discuss the problem of an input representation that is appropria"
P13-1152,A00-1017,0,0.059807,"n be modelled in sequential generation architecture where the RE component has access to information about syntactic realization and an approximative, intermediate linearization. Such a system is reminiscent of earlier work in rulebased generation that implements an interactive or revision-based feedback between discourse-level planning and linguistic realisation (Hovy, 1988; Robin, 1993). 2 Related Work Despite the common view of NLG as a pipeline process, it is a well-known problem that highlevel, conceptual knowledge and low-level linguistic knowledge are tightly interleaved (Danlos, 1984; Mellish et al., 2000). In rule-based, strictly sequential generators these interactions can lead to a so-called generation gap, where a downstream module cannot realize a text or sentence plan generated by the preceding modules (Meteer, 1991; Wanner, 1994). For this reason, a number of other architectures has been proposed, see De Smedt et al. (1996) for an overview. For reasons of tractability and scalability, many practical NLG systems still have been designed as sequential pipelines that follow the basic layout of macroplanning-microplanning-linguistic realization (Reiter, 1994; Cahill et al., 1999; Bateman and"
P13-1152,A00-2026,0,0.0370846,"ion (Reiter, 1994; Cahill et al., 1999; Bateman and Zock, 2003). In recent data-driven research on NLG, many single tasks have been addressed with corpusbased methods. For surface realization, the standard set-up is to regenerate from syntactic representations that have been produced for realistic corpus sentences. The first widely known statistical approach by Langkilde and Knight (1998) used language-model n-gram statistics on a word lattice of candidate realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Belz, 2005). Work on data-driven approaches has led to insights about the importance of linguistic features for sentence 1548 linearization decisions (Ringger et al., 2004; Filippova and Strube, 2007; Cahill and Riester, 2009). (Zarrieß et al., 2012) have recently argued that the good performance of these linguistically motivated word order models, which exploit morphosyntactic features of noun phrases (i.e. referents), is related to the fact that these morphosyntactic features implicitly encode a lot of knowledge about the underlying discourse or information structure. A considerable body o"
P13-1152,W94-0319,0,0.166386,"terleaved (Danlos, 1984; Mellish et al., 2000). In rule-based, strictly sequential generators these interactions can lead to a so-called generation gap, where a downstream module cannot realize a text or sentence plan generated by the preceding modules (Meteer, 1991; Wanner, 1994). For this reason, a number of other architectures has been proposed, see De Smedt et al. (1996) for an overview. For reasons of tractability and scalability, many practical NLG systems still have been designed as sequential pipelines that follow the basic layout of macroplanning-microplanning-linguistic realization (Reiter, 1994; Cahill et al., 1999; Bateman and Zock, 2003). In recent data-driven research on NLG, many single tasks have been addressed with corpusbased methods. For surface realization, the standard set-up is to regenerate from syntactic representations that have been produced for realistic corpus sentences. The first widely known statistical approach by Langkilde and Knight (1998) used language-model n-gram statistics on a word lattice of candidate realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000"
P13-1152,C04-1097,0,0.0334446,"r surface realization, the standard set-up is to regenerate from syntactic representations that have been produced for realistic corpus sentences. The first widely known statistical approach by Langkilde and Knight (1998) used language-model n-gram statistics on a word lattice of candidate realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Belz, 2005). Work on data-driven approaches has led to insights about the importance of linguistic features for sentence 1548 linearization decisions (Ringger et al., 2004; Filippova and Strube, 2007; Cahill and Riester, 2009). (Zarrieß et al., 2012) have recently argued that the good performance of these linguistically motivated word order models, which exploit morphosyntactic features of noun phrases (i.e. referents), is related to the fact that these morphosyntactic features implicitly encode a lot of knowledge about the underlying discourse or information structure. A considerable body of REG research has been done in the paradigm established by Dale (1989; 1995). More closely related to our work are approaches in the line of Siddarthan and Copestake (2004)"
P13-1152,S12-1030,0,0.0118512,"about the underlying discourse or information structure. A considerable body of REG research has been done in the paradigm established by Dale (1989; 1995). More closely related to our work are approaches in the line of Siddarthan and Copestake (2004) or Belz and Varges (2007) who generate contextually appropriate REs for instances of a referent in a text. Belz and Varges (2007)’s GREC data set includes annotations of implicit subjects in coordinations. Zarrieß et al. (2011) deal with implicit subjects in passives, proposing a set of heuristics for adding these agents to the generation input. Roth and Frank (2012) acquire automatic annotations of implicit roles for the purpose of studying coherence patterns in texts. Implicit referents have also received attention for the analysis of semantic roles (Gerber and Chai, 2010; Ruppenhofer et al., 2010). Statistical methods for data-to-text generation have been explored only recently. Belz (2008) trains a probabilistic CFG to generate weather forecasts, Chen et al. (2010) induce a synchronous grammar to generate sportcaster text. Both address a restricted domain where a direct alignment between units in the non-linguistic representation and the linguistic ut"
P13-1152,W09-2417,0,0.0358082,"Missing"
P13-1152,seeker-kuhn-2012-making,1,0.802563,"n 3.2). (4) a. b. (x,lemma,VV,y) → (x,lemma,VVFIN,y) (x,¨uberfallen/attack,VV,y) → (x,bei/at,PREP,y), ¨ (z,Uberfall/attack,NN,x),(q,der/the,ART,z) The baseline for the verb transformation component is a two-step procedure: 1) pick a lexical4.1.3 LIN: Linearization For linearization, we use the state-of-the-art dependency linearizer described in Bohnet et al. (2012). We train the linearizer on an automatically parsed version of the German TIGER treebank (Brants et al., 2002). This version was produced with the dependency parser by Bohnet (2010), trained on the dependency conversion of TIGER by Seeker and Kuhn (2012). 4.2 Architectures Depending on the way the generation components are combined in an architecture, they will have access to different layers of the input representation. The following definitions of architectures recur to the layers introduced in Section 3.3. 4.2.1 First Pipeline The first pipeline corresponds most closely to a standard generation pipeline in the sense of (Reiter and Dale, 1997). REG is carried out prior to surface realization such that the RE component does not have access to surface syntax or word order whereas the SYN component has access to fully specified RE slots. 1551"
P13-1152,P04-1052,0,0.0926647,"Missing"
P13-1152,E12-2021,0,0.0583566,"Missing"
P13-1152,W12-1506,0,0.0114807,"n the non-linguistic representation and the linguistic utterance can be learned. Marciniak and Strube (2005) propose an ILP model for global optimization in a generation task that is decomposed into a set of classifiers. Bohnet et al. (2011) deal with multi-level generation in a statistical framework and in a less restricted domain. They adopt a standard sequential pipeline approach. Recent corpus-based generation approaches faced the problem that existing standard treebank representations for parsing or other analysis tasks do not necessarily fit the needs of generation (Bohnet et al., 2010; Wanner et al., 2012). Zarrieß et al. (2011) discuss the problem of an input representation that is appropriately underspecified for the realistic generation of voice alternations. 3 The Data Set The data set for our generation experiments consists of 200 newspaper articles about robbery events. The articles were extracted from a large German newspaper corpus. A complete example text with RE annotations is given in Figure 2, Table 1 summarizes some data set statistics. 3.1 RE annotation The RE annotations mark explicit and implicit mentions of referents involved in the robbery event described in an article. Explic"
P13-1152,W94-0316,0,0.050514,"tion that implements an interactive or revision-based feedback between discourse-level planning and linguistic realisation (Hovy, 1988; Robin, 1993). 2 Related Work Despite the common view of NLG as a pipeline process, it is a well-known problem that highlevel, conceptual knowledge and low-level linguistic knowledge are tightly interleaved (Danlos, 1984; Mellish et al., 2000). In rule-based, strictly sequential generators these interactions can lead to a so-called generation gap, where a downstream module cannot realize a text or sentence plan generated by the preceding modules (Meteer, 1991; Wanner, 1994). For this reason, a number of other architectures has been proposed, see De Smedt et al. (1996) for an overview. For reasons of tractability and scalability, many practical NLG systems still have been designed as sequential pipelines that follow the basic layout of macroplanning-microplanning-linguistic realization (Reiter, 1994; Cahill et al., 1999; Bateman and Zock, 2003). In recent data-driven research on NLG, many single tasks have been addressed with corpusbased methods. For surface realization, the standard set-up is to regenerate from syntactic representations that have been produced f"
P13-1152,P11-1101,1,0.876126,"ic features of noun phrases (i.e. referents), is related to the fact that these morphosyntactic features implicitly encode a lot of knowledge about the underlying discourse or information structure. A considerable body of REG research has been done in the paradigm established by Dale (1989; 1995). More closely related to our work are approaches in the line of Siddarthan and Copestake (2004) or Belz and Varges (2007) who generate contextually appropriate REs for instances of a referent in a text. Belz and Varges (2007)’s GREC data set includes annotations of implicit subjects in coordinations. Zarrieß et al. (2011) deal with implicit subjects in passives, proposing a set of heuristics for adding these agents to the generation input. Roth and Frank (2012) acquire automatic annotations of implicit roles for the purpose of studying coherence patterns in texts. Implicit referents have also received attention for the analysis of semantic roles (Gerber and Chai, 2010; Ruppenhofer et al., 2010). Statistical methods for data-to-text generation have been explored only recently. Belz (2008) trains a probabilistic CFG to generate weather forecasts, Chen et al. (2010) induce a synchronous grammar to generate sportc"
P13-1152,E12-1078,1,0.848057,"resentations that have been produced for realistic corpus sentences. The first widely known statistical approach by Langkilde and Knight (1998) used language-model n-gram statistics on a word lattice of candidate realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Belz, 2005). Work on data-driven approaches has led to insights about the importance of linguistic features for sentence 1548 linearization decisions (Ringger et al., 2004; Filippova and Strube, 2007; Cahill and Riester, 2009). (Zarrieß et al., 2012) have recently argued that the good performance of these linguistically motivated word order models, which exploit morphosyntactic features of noun phrases (i.e. referents), is related to the fact that these morphosyntactic features implicitly encode a lot of knowledge about the underlying discourse or information structure. A considerable body of REG research has been done in the paradigm established by Dale (1989; 1995). More closely related to our work are approaches in the line of Siddarthan and Copestake (2004) or Belz and Varges (2007) who generate contextually appropriate REs for instan"
P13-1152,W09-0629,0,\N,Missing
P13-1152,S10-1008,0,\N,Missing
P13-1152,C98-1112,0,\N,Missing
P13-4010,heid-etal-2010-corpus,0,0.0235429,"e possible to easily extend ICARUS to work with constituent trees. ICARUS comes with a dedicated plugin that enables access to web services provided by CLARIN-D. The project aims to provide tools and services for language-centered research in the humanities and social sciences. In contrast to the integration of, e.g., mate-tools, where the tool chain is executed locally, the user can define a tool chain by chaining several web services (e.g., lemmatizers, part-of-speech taggers etc.) together and apply them to his own data. To do this, ICARUS is able to read and write the TCF exchange format (Heid et al., 2010) that is used by CLARIN-D web services. The output can then be inspected and searched using ICARUS. As new NLP tools are added as CLARIN-D web services they can be immediately employed by ICARUS. 5 An upcoming release includes the following extensions: • Currently, treebanks are assumed to fit into the executing computer’s main memory. The new implementation will support asynchronous loading of data, with notifications passed to the query engine or a plugin when required data is available. Treebanks with millions of entries can then be loaded in less Extensibility ICARUS relies on the Java Plu"
P13-4010,nilsson-nivre-2008-malteval,0,0.0331078,"and Research tools, University of Stuttgart 55 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 55–60, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics word order, such as the Prague Dependency Treebank for Czech (Hajiˇc et al., 2000) or SynTagRus for Russian (Boguslavsky et al., 2000). A simple tool for visualization of dependency trees is What’s wrong with my NLP? (Riedel, 2008). Its querying functionality is however limited to simple string-searching on surface forms. A somewhat more advanced tool is MaltEval (Nilsson and Nivre, 2008), which offers a number of predefined search patterns ranging from part-ofspeech tag to branching degree. On the other hand, powerful tools such as PMLˇ ep´anek, 2009) or INESS (Meurer, TQ (Pajas and Stˇ 2012) offer expressive query languages and can facilitate cross-layer queries (e.g., involving both syntactic and semantic structures). They also accommodate both constituent and dependency structures. In terms of complexity in usage and expressivity, we believe ICARUS constitutes a middle way between highly expressive and very simple visualization tools. It is easy to use, requires no install"
P13-4010,P09-4009,0,0.100209,"Missing"
P13-4010,augustinus-etal-2012-example,0,0.0975963,"Missing"
P13-4010,C10-1011,0,\N,Missing
P13-4010,W08-2121,0,\N,Missing
P13-4010,C00-2143,0,\N,Missing
P13-4010,C12-3022,0,\N,Missing
P14-1005,P04-1015,0,0.183344,"as early updates or Learning as Search Optimization (LaSO) perform worse than a greedy baseline that only uses local features. By modifying LaSO to delay updates until the end of each instance we obtain significant improvements over the baseline. Our model obtains the best results to date on recent shared task data for Arabic, Chinese, and English. 1 Introduction This paper studies and extends previous work using the structured perceptron (Collins, 2002) for complex NLP tasks. We show that for the task of coreference resolution the straightforward combination of beam search and early update (Collins and Roark, 2004) falls short of more limited feature sets that allow for exact search. This contrasts with previous work on, e.g., syntactic parsing (Collins and Roark, 2004; Huang, 2008; Zhang and Clark, 2008) and linearization (Bohnet et al., 2011), and even simpler structured prediction problems, where early updates are not even necessary, such as part-of-speech tagging (Collins, 2002) and named entity recognition (Ratinov and Roth, 2009). The main reason why early updates underperform in our setting is that the task is too difficult and that the learning algorithm is not able to profit from all training d"
P14-1005,J08-1001,0,0.0511536,"ls (Luo et al., 2004; Rahman and Ng, 2009), however they did not improve performance in preliminary experiments. The one exception is the size of a cluster (Culotta et al., 2007). Additional features we use are Shape encodes the linear “shape” of a cluster in terms of mention type. For instance, the clusters representing Gary Wilber and Drug Emporium Inc. from the example in Figure 1, would be represented as RNPN and RNCCC, respectively. Where R , N , P , and C denote the root node, names, pronouns, and common noun phrases, respectively. Local syntactic context is inspired by the Entity Grid (Barzilay and Lapata, 2008), where the basic assumption is that references to an entity follow particular syntactic patterns. For instance, an entity may be introduced as an object in one sentence, whereas in subsequent sentences it is referred to in subject position. Grammatical functions are approximated by the path in the syntax tree from a mention to its closest S node. The partial paths of a mention and its linear predecessor, given the cluster of the current antecedent, informs the model about the local syntactic context. Cluster start distance denotes the distance in mentions from the beginning of the document wh"
P14-1005,D08-1031,0,0.448918,"1 49.71 43.52 50.96 Chinese 57.94 62.23 57.99 62.37 English 58.7 62.91 60.03 65.01 CEAFe CoNLL 46.49 47.18 45.44 46.67 57.05 57.12 60.27 60.5 56.03 56.8 61.56 62.52 Related Work Table 1: Comparison of local and non-local feature sets on the development sets. about one point. For Chinese the gains are generally not as pronounced, though the MUC metric goes up by more than half a point. Perceptrons for coreference. The perceptron has previously been used to train coreference resolvers either by casting the problem as a binary classification problem that considers pairs of mentions in isolation (Bengtson and Roth, 2008; Stoyanov et al., 2009; Chang et al., 2012, inter alia) or in the structured manner, where a clustering for an entire document is predicted in one go (Fernandes et al., 2012). However, none of these works use non-local features. Stoyanov and Eisner (2012) train an Easy-First coreference system with the perceptron to learn a sequence of join operations between arbitrary mentions in a document and accesses non-local features through previous merge operations in later stages. Culotta et al. (2007) also apply online learning in a first-order logic framework that enables non-local features, though"
P14-1005,W02-1001,0,0.918463,"perceptron models for coreference resolution when using non-local features and beam search. Our experimental results indicate that standard techniques such as early updates or Learning as Search Optimization (LaSO) perform worse than a greedy baseline that only uses local features. By modifying LaSO to delay updates until the end of each instance we obtain significant improvements over the baseline. Our model obtains the best results to date on recent shared task data for Arabic, Chinese, and English. 1 Introduction This paper studies and extends previous work using the structured perceptron (Collins, 2002) for complex NLP tasks. We show that for the task of coreference resolution the straightforward combination of beam search and early update (Collins and Roark, 2004) falls short of more limited feature sets that allow for exact search. This contrasts with previous work on, e.g., syntactic parsing (Collins and Roark, 2004; Huang, 2008; Zhang and Clark, 2008) and linearization (Bohnet et al., 2011), and even simpler structured prediction problems, where early updates are not even necessary, such as part-of-speech tagging (Collins, 2002) and named entity recognition (Ratinov and Roth, 2009). The"
P14-1005,P06-1005,0,0.169432,"h mentions in a pair include (among others): Mention type, which is either root, pro52 64 non-local features were selected with the same greedy forward strategy as the local features, starting from the optimized local feature sets. CoNLL avg. 7 62 Experimental Setup We apply our model to the CoNLL 2012 Shared Task data, which includes a training, development, and test set split for three languages: Arabic, Chinese and English. We follow the closed track setting where systems may only be trained on the provided training data, with the exception of the English gender and number data compiled by Bergsma and Lin (2006). We use automatically extracted mentions using the same mention extraction procedure as Bj¨orkelund and Farkas (2012). We evaluate our system using the CoNLL 2012 scorer, which computes several coreference metrics: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFe and CEAFm (Luo, 2005). We also report the CoNLL average (also known as MELA; Denis and Baldridge (2009)), i.e., the arithmetic mean of MUC, B3 , and CEAFe . It should be noted that for B3 and the CEAF metrics, multiple ways of handling twinless mentions7 have been proposed (Rahman and Ng, 2009; Stoyanov et al., 2009"
P14-1005,N07-1011,0,0.424004,": yˆ = E XTRACT B EST(AgendaP ) 14: ∆acc = ∆acc + Φ(ˆ y ) − Φ(˜ y) 15: lossacc = lossacc + L OSS(ˆ y) 16: AgendaP = AgendaG 17: yˆ = E XTRACT B EST(AgendaP ) 18: if ¬ C ORRECT(ˆ y ) then 19: y˜ = E XTRACT B EST(AgendaG ) 20: ∆acc = ∆acc + Φ(ˆ y ) − Φ(˜ y) 21: lossacc = lossacc + L OSS(ˆ y) − → 22: if ∆acc 6= 0 then 23: update w.r.t. ∆acc and lossacc 6.2 We experimented with non-local features drawn from previous work on entity-mention models (Luo et al., 2004; Rahman and Ng, 2009), however they did not improve performance in preliminary experiments. The one exception is the size of a cluster (Culotta et al., 2007). Additional features we use are Shape encodes the linear “shape” of a cluster in terms of mention type. For instance, the clusters representing Gary Wilber and Drug Emporium Inc. from the example in Figure 1, would be represented as RNPN and RNCCC, respectively. Where R , N , P , and C denote the root node, names, pronouns, and common noun phrases, respectively. Local syntactic context is inspired by the Entity Grid (Barzilay and Lapata, 2008), where the basic assumption is that references to an entity follow particular syntactic patterns. For instance, an entity may be introduced as an objec"
P14-1005,W12-4503,1,0.562918,"Missing"
P14-1005,W11-2835,0,0.0130941,"baseline. Our model obtains the best results to date on recent shared task data for Arabic, Chinese, and English. 1 Introduction This paper studies and extends previous work using the structured perceptron (Collins, 2002) for complex NLP tasks. We show that for the task of coreference resolution the straightforward combination of beam search and early update (Collins and Roark, 2004) falls short of more limited feature sets that allow for exact search. This contrasts with previous work on, e.g., syntactic parsing (Collins and Roark, 2004; Huang, 2008; Zhang and Clark, 2008) and linearization (Bohnet et al., 2011), and even simpler structured prediction problems, where early updates are not even necessary, such as part-of-speech tagging (Collins, 2002) and named entity recognition (Ratinov and Roth, 2009). The main reason why early updates underperform in our setting is that the task is too difficult and that the learning algorithm is not able to profit from all training data. Put another way, early updates happen too early, and the learning algorithm rarely reaches the end of the instances as it halts, updates, and moves on to the next instance. An alternative would be to continue decoding the same in"
P14-1005,H05-1013,0,0.194685,"Missing"
P14-1005,C10-1011,0,0.0131714,"} otherwise that is, if mention mj is non-referential or the first mention of its cluster, A˜j contains only the document root. Otherwise it is the set of all mentions to the left that belong to the same cluster as mj . Analogously to A, let A˜ denote the set of constrained antecedent sets. The latent tree y˜ needed 4 Incremental Search We now show that the search problem in (2) can equivalently be solved by the more intuitive bestfirst decoder (Ng and Cardie, 2002), rather than using the CLE decoder. The best-first decoder 4 We also implement the feature mapping function Φ as a hash kernel (Bohnet, 2010) and apply averaging (Collins, 2002), though for brevity we omit this from the pseudocode. 49 works incrementally by making a left-to-right pass over the mentions, selecting for each mention the highest scoring antecedent. The key aspect that makes the best-first decoder equivalent to the CLE decoder is that all arcs point from left to right, both in this paper and in the work of Fernandes et al. (2012). We sketch a proof that this decoder also returns the highest scoring tree. First, note that this algorithm indeed returns a tree. This can be shown by assuming the opposite, in which case the"
P14-1005,W12-4513,0,0.374185,".37 English 58.7 62.91 60.03 65.01 CEAFe CoNLL 46.49 47.18 45.44 46.67 57.05 57.12 60.27 60.5 56.03 56.8 61.56 62.52 Related Work Table 1: Comparison of local and non-local feature sets on the development sets. about one point. For Chinese the gains are generally not as pronounced, though the MUC metric goes up by more than half a point. Perceptrons for coreference. The perceptron has previously been used to train coreference resolvers either by casting the problem as a binary classification problem that considers pairs of mentions in isolation (Bengtson and Roth, 2008; Stoyanov et al., 2009; Chang et al., 2012, inter alia) or in the structured manner, where a clustering for an entire document is predicted in one go (Fernandes et al., 2012). However, none of these works use non-local features. Stoyanov and Eisner (2012) train an Easy-First coreference system with the perceptron to learn a sequence of join operations between arbitrary mentions in a document and accesses non-local features through previous merge operations in later stages. Culotta et al. (2007) also apply online learning in a first-order logic framework that enables non-local features, though using a greedy search algorithm. Final res"
P14-1005,D13-1057,0,0.463286,"d Jonas Kuhn Institute for Natural Language Processing University of Stuttgart {anders,jonas}@ims.uni-stuttgart.de Abstract which is equivalent to Learning as Search Optimization (LaSO; Daum´e III and Marcu (2005b)). The learning task we are tackling is however further complicated since the target structure is under-determined by the gold standard annotation. Coreferent mentions in a document are usually annotated as sets of mentions, where all mentions in a set are coreferent. We adopt the recently popularized approach of inducing a latent structure within these sets (Fernandes et al., 2012; Chang et al., 2013; Durrett and Klein, 2013). This approach provides a powerful boost to the performance of coreference resolvers, but we find that it does not combine well with the LaSO learning strategy. We therefore propose a modification to LaSO, which delays updates until after each instance. The combination of this modification with non-local features leads to further improvements in the clustering accuracy, as we show in evaluation results on all languages from the CoNLL 2012 Shared Task – Arabic, Chinese, and English. We obtain the best results to date on these data sets.1 We investigate different ways"
P14-1005,D13-1203,0,0.664673,"te for Natural Language Processing University of Stuttgart {anders,jonas}@ims.uni-stuttgart.de Abstract which is equivalent to Learning as Search Optimization (LaSO; Daum´e III and Marcu (2005b)). The learning task we are tackling is however further complicated since the target structure is under-determined by the gold standard annotation. Coreferent mentions in a document are usually annotated as sets of mentions, where all mentions in a set are coreferent. We adopt the recently popularized approach of inducing a latent structure within these sets (Fernandes et al., 2012; Chang et al., 2013; Durrett and Klein, 2013). This approach provides a powerful boost to the performance of coreference resolvers, but we find that it does not combine well with the LaSO learning strategy. We therefore propose a modification to LaSO, which delays updates until after each instance. The combination of this modification with non-local features leads to further improvements in the clustering accuracy, as we show in evaluation results on all languages from the CoNLL 2012 Shared Task – Arabic, Chinese, and English. We obtain the best results to date on these data sets.1 We investigate different ways of learning structured per"
P14-1005,W09-1119,0,0.03528,"ed perceptron (Collins, 2002) for complex NLP tasks. We show that for the task of coreference resolution the straightforward combination of beam search and early update (Collins and Roark, 2004) falls short of more limited feature sets that allow for exact search. This contrasts with previous work on, e.g., syntactic parsing (Collins and Roark, 2004; Huang, 2008; Zhang and Clark, 2008) and linearization (Bohnet et al., 2011), and even simpler structured prediction problems, where early updates are not even necessary, such as part-of-speech tagging (Collins, 2002) and named entity recognition (Ratinov and Roth, 2009). The main reason why early updates underperform in our setting is that the task is too difficult and that the learning algorithm is not able to profit from all training data. Put another way, early updates happen too early, and the learning algorithm rarely reaches the end of the instances as it halts, updates, and moves on to the next instance. An alternative would be to continue decoding the same instance after the early updates, 2 Background Coreference resolution is the task of grouping referring expressions (or mentions) in a text into disjoint clusters such that all mentions in a cluste"
P14-1005,W12-4502,0,0.196831,"Missing"
P14-1005,N12-1015,0,0.0372225,"Missing"
P14-1005,J01-4004,0,0.660476,"n which each coreference classification decision is limited to information about two mentions that make up a pair. This shortcoming has been addressed by entity-mention models, which relate a candidate mention to the full cluster of mentions predicted to be coreferent so far (for more discussion on the model types, see, e.g., (Ng, 2010)). Nevertheless, the two best systems in the latest CoNLL Shared Task on coreference resolution (Pradhan et al., 2012) were both variants of the mention-pair model. While the second best system (Bj¨orkelund and Farkas, 2012) followed the widely used baseline of Soon et al. (2001), the winning system (Fernandes et al., 2012) proposed the use of a tree representation. The tree-based model of Fernandes et al. (2012) construes the representation of coreference clusters as a rooted tree. Figure 2 displays an example tree over the clusters from Figure 1. Every mention corresponds to a node in the tree, and arcs between mentions indicate that they are coreferent. The tree additionally has a dummy root node. Every subtree under the root node corresponds to a cluster of coreferent mentions. Since coreference training data is typically not annotated with trees, Fernandes et al."
P14-1005,C12-1154,0,0.184632,"sets. about one point. For Chinese the gains are generally not as pronounced, though the MUC metric goes up by more than half a point. Perceptrons for coreference. The perceptron has previously been used to train coreference resolvers either by casting the problem as a binary classification problem that considers pairs of mentions in isolation (Bengtson and Roth, 2008; Stoyanov et al., 2009; Chang et al., 2012, inter alia) or in the structured manner, where a clustering for an entire document is predicted in one go (Fernandes et al., 2012). However, none of these works use non-local features. Stoyanov and Eisner (2012) train an Easy-First coreference system with the perceptron to learn a sequence of join operations between arbitrary mentions in a document and accesses non-local features through previous merge operations in later stages. Culotta et al. (2007) also apply online learning in a first-order logic framework that enables non-local features, though using a greedy search algorithm. Final results. In Table 2 we compare the results of the non-local system (This paper) to the best results from the CoNLL 2012 Shared Task.10 Specifically, this includes Fernandes et al.’s (2012) system for Arabic and Engli"
P14-1005,P08-1067,0,0.0357791,"h instance we obtain significant improvements over the baseline. Our model obtains the best results to date on recent shared task data for Arabic, Chinese, and English. 1 Introduction This paper studies and extends previous work using the structured perceptron (Collins, 2002) for complex NLP tasks. We show that for the task of coreference resolution the straightforward combination of beam search and early update (Collins and Roark, 2004) falls short of more limited feature sets that allow for exact search. This contrasts with previous work on, e.g., syntactic parsing (Collins and Roark, 2004; Huang, 2008; Zhang and Clark, 2008) and linearization (Bohnet et al., 2011), and even simpler structured prediction problems, where early updates are not even necessary, such as part-of-speech tagging (Collins, 2002) and named entity recognition (Ratinov and Roth, 2009). The main reason why early updates underperform in our setting is that the task is too difficult and that the learning algorithm is not able to profit from all training data. Put another way, early updates happen too early, and the learning algorithm rarely reaches the end of the instances as it halts, updates, and moves on to the next in"
P14-1005,P09-1074,0,0.0823464,"ergsma and Lin (2006). We use automatically extracted mentions using the same mention extraction procedure as Bj¨orkelund and Farkas (2012). We evaluate our system using the CoNLL 2012 scorer, which computes several coreference metrics: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFe and CEAFm (Luo, 2005). We also report the CoNLL average (also known as MELA; Denis and Baldridge (2009)), i.e., the arithmetic mean of MUC, B3 , and CEAFe . It should be noted that for B3 and the CEAF metrics, multiple ways of handling twinless mentions7 have been proposed (Rahman and Ng, 2009; Stoyanov et al., 2009). We use the most recent version of the CoNLL scorer (version 7), which implements the original definitions of these metrics.8 Our system is evaluated on the version of the data with automatic preprocessing information (e.g., predicted parse trees). Unless otherwise stated we use 25 iterations of perceptron training and a beam size of 20. We did not attempt to tune either of these parameters. We experiment with two feature sets for each language: the optimized local feature sets (denoted local), and the optimized local feature sets extended with non-local features (denoted non-local). 8 60 58"
P14-1005,P04-1018,0,0.164572,"PAND(AgendaG , A˜j , mj , k) 10: AgendaP = E XPAND(AgendaP , Aj , mj , k) 11: if ¬ C ONTAINS C ORRECT(AgendaP ) then 12: y˜ = E XTRACT B EST(AgendaG ) 13: yˆ = E XTRACT B EST(AgendaP ) 14: ∆acc = ∆acc + Φ(ˆ y ) − Φ(˜ y) 15: lossacc = lossacc + L OSS(ˆ y) 16: AgendaP = AgendaG 17: yˆ = E XTRACT B EST(AgendaP ) 18: if ¬ C ORRECT(ˆ y ) then 19: y˜ = E XTRACT B EST(AgendaG ) 20: ∆acc = ∆acc + Φ(ˆ y ) − Φ(˜ y) 21: lossacc = lossacc + L OSS(ˆ y) − → 22: if ∆acc 6= 0 then 23: update w.r.t. ∆acc and lossacc 6.2 We experimented with non-local features drawn from previous work on entity-mention models (Luo et al., 2004; Rahman and Ng, 2009), however they did not improve performance in preliminary experiments. The one exception is the size of a cluster (Culotta et al., 2007). Additional features we use are Shape encodes the linear “shape” of a cluster in terms of mention type. For instance, the clusters representing Gary Wilber and Drug Emporium Inc. from the example in Figure 1, would be represented as RNPN and RNCCC, respectively. Where R , N , P , and C denote the root node, names, pronouns, and common noun phrases, respectively. Local syntactic context is inspired by the Entity Grid (Barzilay and Lapata,"
P14-1005,H05-1004,0,0.115522,"data, which includes a training, development, and test set split for three languages: Arabic, Chinese and English. We follow the closed track setting where systems may only be trained on the provided training data, with the exception of the English gender and number data compiled by Bergsma and Lin (2006). We use automatically extracted mentions using the same mention extraction procedure as Bj¨orkelund and Farkas (2012). We evaluate our system using the CoNLL 2012 scorer, which computes several coreference metrics: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFe and CEAFm (Luo, 2005). We also report the CoNLL average (also known as MELA; Denis and Baldridge (2009)), i.e., the arithmetic mean of MUC, B3 , and CEAFe . It should be noted that for B3 and the CEAF metrics, multiple ways of handling twinless mentions7 have been proposed (Rahman and Ng, 2009; Stoyanov et al., 2009). We use the most recent version of the CoNLL scorer (version 7), which implements the original definitions of these metrics.8 Our system is evaluated on the version of the data with automatic preprocessing information (e.g., predicted parse trees). Unless otherwise stated we use 25 iterations of perce"
P14-1005,M95-1005,0,0.886338,"7 62 Experimental Setup We apply our model to the CoNLL 2012 Shared Task data, which includes a training, development, and test set split for three languages: Arabic, Chinese and English. We follow the closed track setting where systems may only be trained on the provided training data, with the exception of the English gender and number data compiled by Bergsma and Lin (2006). We use automatically extracted mentions using the same mention extraction procedure as Bj¨orkelund and Farkas (2012). We evaluate our system using the CoNLL 2012 scorer, which computes several coreference metrics: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFe and CEAFm (Luo, 2005). We also report the CoNLL average (also known as MELA; Denis and Baldridge (2009)), i.e., the arithmetic mean of MUC, B3 , and CEAFe . It should be noted that for B3 and the CEAF metrics, multiple ways of handling twinless mentions7 have been proposed (Rahman and Ng, 2009; Stoyanov et al., 2009). We use the most recent version of the CoNLL scorer (version 7), which implements the original definitions of these metrics.8 Our system is evaluated on the version of the data with automatic preprocessing information (e.g., predicted pars"
P14-1005,P08-1096,0,0.0678172,"processed. In the special case where only local features are used, this method coincides with standard structured perceptron learning that uses exact search. Moreover, it is also able to profit from nonlocal features resulting in improved performance. We evaluated our system on all three languages from the CoNLL 2012 Shared Task and present the best results to date on these data sets. Entity-mention models. Entity-mention models that compare a single mention to a (partial) cluster have been studied extensively and several works have evaluated non-local entity-level features (Luo et al., 2004; Yang et al., 2008; Rahman and Ng, 2009). Luo et al. (2004) also apply beam search at test time, but use a static assignment of antecedents and learns log-linear model using batch learning. Moreover, these works alter the basic feature definitions from their pairwise models when introducing entity-level features. This contrasts with our work, as our mention-pair model simply constitutes a special case of the non-local system. Acknowledgments We are grateful to the anonymous reviewers as well as Christian Scheible and Wolfgang Seeker for comments on earlier versions of this paper. This research has been funded b"
P14-1005,P02-1014,0,0.627581,"lly, let A˜j denote the set of correct antecedents for a mention mj , or A˜j = ( {m0 } if mj has no correct antecedent {ai |C OREF(ai , mj ), ai ∈ Aj } otherwise that is, if mention mj is non-referential or the first mention of its cluster, A˜j contains only the document root. Otherwise it is the set of all mentions to the left that belong to the same cluster as mj . Analogously to A, let A˜ denote the set of constrained antecedent sets. The latent tree y˜ needed 4 Incremental Search We now show that the search problem in (2) can equivalently be solved by the more intuitive bestfirst decoder (Ng and Cardie, 2002), rather than using the CLE decoder. The best-first decoder 4 We also implement the feature mapping function Φ as a hash kernel (Bohnet, 2010) and apply averaging (Collins, 2002), though for brevity we omit this from the pseudocode. 49 works incrementally by making a left-to-right pass over the mentions, selecting for each mention the highest scoring antecedent. The key aspect that makes the best-first decoder equivalent to the CLE decoder is that all arcs point from left to right, both in this paper and in the work of Fernandes et al. (2012). We sketch a proof that this decoder also returns t"
P14-1005,P10-1142,0,0.266475,"Computational Linguistics, pages 47–57, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics In recent years much work on coreference resolution has been devoted to increasing the expressivity of the classical mention-pair model, in which each coreference classification decision is limited to information about two mentions that make up a pair. This shortcoming has been addressed by entity-mention models, which relate a candidate mention to the full cluster of mentions predicted to be coreferent so far (for more discussion on the model types, see, e.g., (Ng, 2010)). Nevertheless, the two best systems in the latest CoNLL Shared Task on coreference resolution (Pradhan et al., 2012) were both variants of the mention-pair model. While the second best system (Bj¨orkelund and Farkas, 2012) followed the widely used baseline of Soon et al. (2001), the winning system (Fernandes et al., 2012) proposed the use of a tree representation. The tree-based model of Fernandes et al. (2012) construes the representation of coreference clusters as a rooted tree. Figure 2 displays an example tree over the clusters from Figure 1. Every mention corresponds to a node in the tr"
P14-1005,D08-1059,0,0.010665,"obtain significant improvements over the baseline. Our model obtains the best results to date on recent shared task data for Arabic, Chinese, and English. 1 Introduction This paper studies and extends previous work using the structured perceptron (Collins, 2002) for complex NLP tasks. We show that for the task of coreference resolution the straightforward combination of beam search and early update (Collins and Roark, 2004) falls short of more limited feature sets that allow for exact search. This contrasts with previous work on, e.g., syntactic parsing (Collins and Roark, 2004; Huang, 2008; Zhang and Clark, 2008) and linearization (Bohnet et al., 2011), and even simpler structured prediction problems, where early updates are not even necessary, such as part-of-speech tagging (Collins, 2002) and named entity recognition (Ratinov and Roth, 2009). The main reason why early updates underperform in our setting is that the task is too difficult and that the learning algorithm is not able to profit from all training data. Put another way, early updates happen too early, and the learning algorithm rarely reaches the end of the instances as it halts, updates, and moves on to the next instance. An alternative w"
P14-1005,W12-4501,0,0.558547,"Computational Linguistics In recent years much work on coreference resolution has been devoted to increasing the expressivity of the classical mention-pair model, in which each coreference classification decision is limited to information about two mentions that make up a pair. This shortcoming has been addressed by entity-mention models, which relate a candidate mention to the full cluster of mentions predicted to be coreferent so far (for more discussion on the model types, see, e.g., (Ng, 2010)). Nevertheless, the two best systems in the latest CoNLL Shared Task on coreference resolution (Pradhan et al., 2012) were both variants of the mention-pair model. While the second best system (Bj¨orkelund and Farkas, 2012) followed the widely used baseline of Soon et al. (2001), the winning system (Fernandes et al., 2012) proposed the use of a tree representation. The tree-based model of Fernandes et al. (2012) construes the representation of coreference clusters as a rooted tree. Figure 2 displays an example tree over the clusters from Figure 1. Every mention corresponds to a node in the tree, and arcs between mentions indicate that they are coreferent. The tree additionally has a dummy root node. Every su"
P14-1005,D08-1067,0,\N,Missing
P14-1005,W12-4504,0,\N,Missing
P14-5002,W03-2120,0,0.0619036,"Missing"
P14-5002,P09-4009,0,0.18117,"Missing"
P14-5002,W11-1901,0,0.146758,"odes. The tool can compare two different annotations on the same document, allowing system developers to evaluate errors in automatic system predictions. It features a flexible search engine, which enables the user to graphically construct search queries over sets of documents annotated with coreference. 1 Introduction Coreference resolution is the task of automatically grouping references to the same real-world entity in a document into a set. It is an active topic in current NLP research and has received considerable attention in recent years, including the 2011 and 2012 CoNLL shared tasks (Pradhan et al., 2011; Pradhan et al., 2012). Coreference relations are commonly represented by sets of mentions, where all mentions in one set (or coreference cluster) are considered coreferent. This type of representation does not support any internal structure within the clusters. However, many automatic coreference resolvers establish links between pairs of mentions which are subsequently transformed to a cluster by taking the transitive closure over all links, i.e., placing all mentions that are directly or transitively classified as coreferent in one cluster. This is particularly the case for several state-o"
P14-5002,W12-4501,0,0.389193,"pare two different annotations on the same document, allowing system developers to evaluate errors in automatic system predictions. It features a flexible search engine, which enables the user to graphically construct search queries over sets of documents annotated with coreference. 1 Introduction Coreference resolution is the task of automatically grouping references to the same real-world entity in a document into a set. It is an active topic in current NLP research and has received considerable attention in recent years, including the 2011 and 2012 CoNLL shared tasks (Pradhan et al., 2011; Pradhan et al., 2012). Coreference relations are commonly represented by sets of mentions, where all mentions in one set (or coreference cluster) are considered coreferent. This type of representation does not support any internal structure within the clusters. However, many automatic coreference resolvers establish links between pairs of mentions which are subsequently transformed to a cluster by taking the transitive closure over all links, i.e., placing all mentions that are directly or transitively classified as coreferent in one cluster. This is particularly the case for several state-of-the-art resolvers (Fe"
P14-5002,J08-1001,0,0.20091,"d Error Analysis for Coreference Annotations Markus G¨artner Anders Bj¨orkelund Gregor Thiele Wolfgang Seeker Jonas Kuhn Institute for Natural Language Processing University of Stuttgart {thielegr,seeker,gaertnms,anders,kuhn}@ims.uni-stuttgart.de Abstract ploited for detailed error analysis and more finegrained search queries on data automatically annotated for coreference. We present the ICARUS Coreference Explorer (ICE), an interactive tool to browse and search coreference-annotated data. In addition to standard text-based display modes, ICE features two other display modes: an entity-grid (Barzilay and Lapata, 2008) and a tree view, which makes use of the internal pairwise links within the clusters. ICE builds on ICARUS (G¨artner et al., 2013), a platform for search and exploration of dependency treebanks.1 ICE is geared towards two (typically) distinct users: The NLP developer who designs coreference resolution systems can inspect the predictions of his system using the three different display modes. Moreover, ICE can compare the predictions of a system to a gold standard annotation, enabling the developer to inspect system errors interactively. The second potential user is the corpus linguist, who migh"
P14-5002,W09-2411,0,0.0702445,"Missing"
P14-5002,P06-1005,0,0.0507793,"Missing"
P14-5002,P14-1005,1,0.896968,"Missing"
P14-5002,J01-4004,0,0.230686,"ion (see Section 2) are accessible via label patterns. For example, the allocations we use for Figure 1 include a number of properties on the mentions, most of which are internally computed by the coreference system: The T YPE of a mention, which can take any of the values In addition to user-specified allocations, ICE will always by default provide an internal structure for the clusters, in which the correct antecedent of every mention is the closest coreferent mention with respect to the linear order of the document (this is equivalent to the training instance creation heuristic proposed by Soon et al. (2001)). Therefore, the user is not required to define an allocation on their own. 3 Entity grid Display Modes In this section we describe the entity grid and tree display modes by means of screenshots. ICE additionally includes a standard text-based view, similar to other coreference visualization tools. The example document is taken from the CoNLL 2012 development set (Pradhan et al., 2012) and we use two allocations: (1) the predictions output by Bj¨orkelund and Kuhn (2014) system (predicted) and (2) a gold allocation that was obtained by running the same system in a restricted setting, where onl"
P14-5002,W08-1301,0,0.13158,"Missing"
P14-5002,D13-1203,0,0.0407813,"s are commonly represented by sets of mentions, where all mentions in one set (or coreference cluster) are considered coreferent. This type of representation does not support any internal structure within the clusters. However, many automatic coreference resolvers establish links between pairs of mentions which are subsequently transformed to a cluster by taking the transitive closure over all links, i.e., placing all mentions that are directly or transitively classified as coreferent in one cluster. This is particularly the case for several state-of-the-art resolvers (Fernandes et al., 2012; Durrett and Klein, 2013; Bj¨orkelund and Kuhn, 2014). These pairwise decisions, which give rise to a clustering, can be ex2 Data Representation ICE reads the formats used in the 2011 and 2012 CoNLL shared tasks as well as the SemEval 2010 format (Recasens et al., 2010).2 Since these formats cannot accommodate pairwise links, an auxiliary file with standoff annotation can be provided, which we call allocation. An allocation is a list of pairwise links between mentions. Multiple 1 ICE is written in Java and is therefore platform independent. It is open source (under GNU GPL) and we provide both sources and binaries fo"
P14-5002,W12-4502,0,0.057506,"Missing"
P14-5002,P13-4001,0,0.0331629,"Missing"
P14-5002,P13-4010,1,0.642783,"Missing"
P14-5002,S10-1001,0,\N,Missing
P14-5002,E12-2021,0,\N,Missing
P15-4005,P14-5002,1,0.702881,"Missing"
P15-4005,P09-4009,0,0.0332692,"Missing"
P15-4005,W11-1901,0,0.0516759,"scratch or by importing them from real examples in a corpus. Changes to individual parameters can be applied via sliders or input fields and are displayed in realtime. Additionally a persistent storage of PaIntE curves is provided where the user can save parameter sets that are of interest to him along with a description and identifier, the latter of which can be used when searching (see Section 5). Data Representation ICARUS for intonation ships with reader implementations for two very different formats. One is an extended version of the format used for the 2011 and 2012 CoNLL shared tasks (Pradhan et al., 2011; Pradhan et al., 2012) with a number of additional columns to accommodate features for the syllable level. This format stores all annotations corresponding to a word token in one line and packs syllable features into a list separated by pipe-characters (’|’). To address syllable centric data like the typical output of speech processing systems, a second flexible tabular format was specified where each line of text corresponds to a single syllable and a global header describes the content of all columns and how to read and map them to the internal data model of ICARUS. 4.2 Curve Preview For al"
P15-4005,W12-4501,0,0.0202415,"ng them from real examples in a corpus. Changes to individual parameters can be applied via sliders or input fields and are displayed in realtime. Additionally a persistent storage of PaIntE curves is provided where the user can save parameter sets that are of interest to him along with a description and identifier, the latter of which can be used when searching (see Section 5). Data Representation ICARUS for intonation ships with reader implementations for two very different formats. One is an extended version of the format used for the 2011 and 2012 CoNLL shared tasks (Pradhan et al., 2011; Pradhan et al., 2012) with a number of additional columns to accommodate features for the syllable level. This format stores all annotations corresponding to a word token in one line and packs syllable features into a list separated by pipe-characters (’|’). To address syllable centric data like the typical output of speech processing systems, a second flexible tabular format was specified where each line of text corresponds to a single syllable and a global header describes the content of all columns and how to read and map them to the internal data model of ICARUS. 4.2 Curve Preview For all visualizations dealin"
P15-4005,bjorkelund-etal-2014-extended,1,0.895416,"Missing"
P15-4005,wittenburg-etal-2006-elan,0,0.0556697,"ine in ICARUS can be used to analyze value distributions for an annotation. Using the then looks for PaIntE curves which do not differ from the prototype by more than 5 degrees. When using PaIntE curves as part of a search 28 (a) graphical query (b) result outline with highlighting Figure 4: Example search query combining syntax and intonation constraints and an excerpt of the corresponding result outline. (a) ˇ ep´anek, tations (Zeldes et al., 2009; Pajas and Stˇ 2009). However, they do not support a dedicated search and visualization for prosodic syllable level annotations. Tools like ELAN (Wittenburg et al., 2006) provide an interface for adding (flat) annotations to multi-modal corpora, but focus on audio and video data. More importantly, ICARUS for intonation is so far the first tool using the PaIntE model for F0 contour visualizations, a task previously worked around via general curve plotting tools like R3 and also is first to provide a collection of search constraints dedicated to PaIntE curves. Eckart et al. (2010) describe a database that serves as a generic query tool for multiple annotation layers. It allows to take annotations of tonal features into account and has also been tested with the D"
P15-4005,P13-4010,1,0.707635,"Missing"
P16-1181,P14-1005,1,0.90603,"Missing"
P16-1181,E12-1009,1,0.835499,"(F1 ) for baselines for sentence boundary detection on dev sets. Parser implementation. Our parser implements the labeled version of the transition system described in Section 2 with a default beam size of 20. We use the oracle by Nivre et al. (2009) to create transition sequences for each sentence of a document, and then concatenate them with S B transitions that occur as early as possible (cf. 5 http://opennlp.apache.org A true positive is defined as a token that was correctly predicted to begin a new sentence. 6 Section 2). The feature set is based on previous work (Zhang and Nivre, 2011; Bohnet and Kuhn, 2012; Bohnet et al., 2013) and was developed for a sentence-based parser for the WSJ. We made initial experiments trying to introduce new features aimed at capturing sentence boundaries such as trying to model verb subcategorization or sentence length, however none of these proved useful compared to the baseline feature set. Following the line of work by Bohnet et al., we use the passiveaggressive algorithm (Crammer et al., 2006) instead of the vanilla perceptron, parameter averaging (Collins, 2002), and a hash function to map features (Bohnet, 2010).7 5 Analysis Comparison of training methods. Fi"
P16-1181,D12-1133,0,0.0390108,"es (e.g., OpenNLP, a re-implementation of Reynar and Ratnaparkhi (1997)’s MxTerminator). The task is often simplified to the task of period disambiguation (Kiss and Strunk, 2006), which only works on text that uses punctuation consistently. The current state of the art uses sequence labelers, e.g., a CRF (Evang et al., 2013; Dridan and Oepen, 2013). For a broad survey of methodology and tools, we refer the reader to Read et al. (2012). Joint models. Solving several tasks jointly has lately been popular in transition-based parsing, e.g., combining parsing with POS tagging (Hatori et al., 2011; Bohnet and Nivre, 2012) and tokenization (Zhang et al., 2013b; Zhang et al., 2014). Joint approaches avoid error propagation between the subtasks and often lead to overall better models, especially for the lower level tasks that suddenly have access to syntactic information. Our transition system is inspired by the work of Zhang et al. (2013a). They present a projective transition-based parser that jointly predicts punctuation and syntax. Their ArcEager transition system (Nivre, 2003) includes an additional transition that introduces punctuation similar to our S B transition. They also use beam search and circumvent"
P16-1181,Q13-1034,0,0.0336713,"Missing"
P16-1181,C10-1011,0,0.0284067,"n previous work (Zhang and Nivre, 2011; Bohnet and Kuhn, 2012; Bohnet et al., 2013) and was developed for a sentence-based parser for the WSJ. We made initial experiments trying to introduce new features aimed at capturing sentence boundaries such as trying to model verb subcategorization or sentence length, however none of these proved useful compared to the baseline feature set. Following the line of work by Bohnet et al., we use the passiveaggressive algorithm (Crammer et al., 2006) instead of the vanilla perceptron, parameter averaging (Collins, 2002), and a hash function to map features (Bohnet, 2010).7 5 Analysis Comparison of training methods. Figure 4 shows learning curves of the different training algorithms where sentence boundary F1 and parsing accuracy LAS are plotted as a function of training iterations. The plots show performance for early update, max-violation, and DL A SO updates. In addition, a greedy version of the parser is also included. The greedy parser uses a plain averaged perceptron classifier that is trained on all the training data. The straight dashed line corresponds to the M AR M OT baseline. While the greedy parser, DL A SO, and the M AR M OT baseline all exploit"
P16-1181,N01-1016,0,0.0829751,"s available. 4 Experimental setup Data sets. We experiment with two parts of the English Penn Treebank (Marcus et al., 1993). We use the Wall Street Journal (WSJ) as an example of copy-edited newspaper-quality texts with proper punctuation and capitalized sentences. We also use the Switchboard portion which consists of (transcribed) telephone conversations between strangers. Following previous work on Switchboard we lowercase all text and remove punctuation and disfluency markups. We use sections 2-21 of the WSJ for training, 24 as development set and 23 as test set. For Switchboard we follow Charniak and Johnson (2001). We convert both data sets to Stanford dependencies with the Stanford dependency converter (de Marneffe et al., 2006). We predict part-of-speech tags with the CRF tagger M AR M OT (M¨uller et al., 2013) and annotate the training sets via 10-fold jackknifing. Depending on the experimental sce1928 nario we use M AR M OT in two different settings – standard sentence-level where we train and apply it on sentences, and document-level where a whole document is fed to the tagger, implicitly treating it as a single very long sentence. Sentence boundary detection. We work with two well-established sen"
P16-1181,P04-1015,0,0.622206,"i have four cats how old are they . . . Figure 1: The beginning of a sample document from the Switchboard corpus. Tokens that start a sentence are underlined. The task is to predict syntactic structure and sentence boundaries jointly. maintaining a beam of several candidate derivations throughout the parsing process. We will show in this paper that, besides efficient decoding, a second, equally significant challenge lies in the way such a parser is trained. Normally, beam-search transition-based parsers are trained with structured perceptrons using either early update (Zhang and Clark, 2008; Collins and Roark, 2004) or max-violation updates (Huang et al., 2012). Yet our analysis demonstrates that neither of these update strategies is appropriate for training on very long input sequences as they discard a large portion of the training data.2 A significant part of the training data is therefore never used to train the model. As a remedy to this problem, we instead use an adaptation of the update strategy in Bj¨orkelund and Kuhn (2014). They apply early update in a coreference resolution system and observe that the task is inherently so difficult that the correct item practically never stays in the beam. So"
P16-1181,W02-1001,0,0.398689,"ew sentence. 6 Section 2). The feature set is based on previous work (Zhang and Nivre, 2011; Bohnet and Kuhn, 2012; Bohnet et al., 2013) and was developed for a sentence-based parser for the WSJ. We made initial experiments trying to introduce new features aimed at capturing sentence boundaries such as trying to model verb subcategorization or sentence length, however none of these proved useful compared to the baseline feature set. Following the line of work by Bohnet et al., we use the passiveaggressive algorithm (Crammer et al., 2006) instead of the vanilla perceptron, parameter averaging (Collins, 2002), and a hash function to map features (Bohnet, 2010).7 5 Analysis Comparison of training methods. Figure 4 shows learning curves of the different training algorithms where sentence boundary F1 and parsing accuracy LAS are plotted as a function of training iterations. The plots show performance for early update, max-violation, and DL A SO updates. In addition, a greedy version of the parser is also included. The greedy parser uses a plain averaged perceptron classifier that is trained on all the training data. The straight dashed line corresponds to the M AR M OT baseline. While the greedy pars"
P16-1181,de-marneffe-etal-2006-generating,0,0.0484482,"Missing"
P16-1181,W13-5715,0,0.0926384,"ugh punctuation mostly provides reliable cues for segmenting longer texts into sentence units, human readers are able to exploit their understanding of the syntactic and semantic structure to (re-)segment input in the absence of such cues. When working with carefully copy-edited text documents, sentence boundary detection can be viewed as a minor preprocessing task in Natural Language Processing, solvable with very high accuracy. However, when dealing with the output of automatic speech recognition or “noisier” texts such as blogs and emails, non-trivial sentence segmentation issues do occur. Dridan and Oepen (2013), for example, show how much impact fully automatic preprocessing can have on parsing quality for well-edited and less-edited text. Two possible strategies to approach this problem are (i) to exploit other cues for sentence boundaries, such as prosodic phrasing and intonation in speech (e.g., Kol´aˇr et al. (2006)) or formatting cues in text documents (Read et al., 2012), and (ii) to emulate the human ability to exploit syntactic competence for segmentation. We focus here on the latter, which has received little attention, and propose to cast sentence boundary detection and syntactic (dependen"
P16-1181,D13-1146,0,0.0805111,"ntence Boundary Detection. Sentence boundary detection has attracted only modest attention by the research community even though it is a component in every real-world NLP application. Previous work is divided into rule-based, e.g., CoreNLP (Manning et al., 2014), and machine learning approaches (e.g., OpenNLP, a re-implementation of Reynar and Ratnaparkhi (1997)’s MxTerminator). The task is often simplified to the task of period disambiguation (Kiss and Strunk, 2006), which only works on text that uses punctuation consistently. The current state of the art uses sequence labelers, e.g., a CRF (Evang et al., 2013; Dridan and Oepen, 2013). For a broad survey of methodology and tools, we refer the reader to Read et al. (2012). Joint models. Solving several tasks jointly has lately been popular in transition-based parsing, e.g., combining parsing with POS tagging (Hatori et al., 2011; Bohnet and Nivre, 2012) and tokenization (Zhang et al., 2013b; Zhang et al., 2014). Joint approaches avoid error propagation between the subtasks and often lead to overall better models, especially for the lower level tasks that suddenly have access to syntactic information. Our transition system is inspired by the work of"
P16-1181,I11-1136,0,0.0199477,"ine learning approaches (e.g., OpenNLP, a re-implementation of Reynar and Ratnaparkhi (1997)’s MxTerminator). The task is often simplified to the task of period disambiguation (Kiss and Strunk, 2006), which only works on text that uses punctuation consistently. The current state of the art uses sequence labelers, e.g., a CRF (Evang et al., 2013; Dridan and Oepen, 2013). For a broad survey of methodology and tools, we refer the reader to Read et al. (2012). Joint models. Solving several tasks jointly has lately been popular in transition-based parsing, e.g., combining parsing with POS tagging (Hatori et al., 2011; Bohnet and Nivre, 2012) and tokenization (Zhang et al., 2013b; Zhang et al., 2014). Joint approaches avoid error propagation between the subtasks and often lead to overall better models, especially for the lower level tasks that suddenly have access to syntactic information. Our transition system is inspired by the work of Zhang et al. (2013a). They present a projective transition-based parser that jointly predicts punctuation and syntax. Their ArcEager transition system (Nivre, 2003) includes an additional transition that introduces punctuation similar to our S B transition. They also use b"
P16-1181,P10-1110,0,0.0616141,"rs are used. Here, both J OINT and J OINT-R EPARSED 1931 obtain significantly better parsing accuracies than the systems that do not have access to syntax during sentence boundary prediction. Although J OINT-R EPARSED performs a bit worse, the difference compared to J OINT is not significant. 7 Related work Zhang and Clark (2008) first showed how to train transition-based parsers with the structured perceptron (Collins, 2002) using beam search and early update (Collins and Roark, 2004). It has since become the de facto standard way of training search-based transition-based dependency parsers (Huang and Sagae, 2010; Zhang and Nivre, 2011; Bohnet et al., 2013). Huang et al. (2012) showed how max-violation leads to faster convergence for transition-based parsers and max-violation updates have subsequently been applied to other tasks such as machine translation (Yu et al., 2013) and semantic parsing (Zhao and Huang, 2015). Sentence Boundary Detection. Sentence boundary detection has attracted only modest attention by the research community even though it is a component in every real-world NLP application. Previous work is divided into rule-based, e.g., CoreNLP (Manning et al., 2014), and machine learning a"
P16-1181,N12-1015,0,0.102802,"Missing"
P16-1181,J06-4003,0,0.019207,"ion updates have subsequently been applied to other tasks such as machine translation (Yu et al., 2013) and semantic parsing (Zhao and Huang, 2015). Sentence Boundary Detection. Sentence boundary detection has attracted only modest attention by the research community even though it is a component in every real-world NLP application. Previous work is divided into rule-based, e.g., CoreNLP (Manning et al., 2014), and machine learning approaches (e.g., OpenNLP, a re-implementation of Reynar and Ratnaparkhi (1997)’s MxTerminator). The task is often simplified to the task of period disambiguation (Kiss and Strunk, 2006), which only works on text that uses punctuation consistently. The current state of the art uses sequence labelers, e.g., a CRF (Evang et al., 2013; Dridan and Oepen, 2013). For a broad survey of methodology and tools, we refer the reader to Read et al. (2012). Joint models. Solving several tasks jointly has lately been popular in transition-based parsing, e.g., combining parsing with POS tagging (Hatori et al., 2011; Bohnet and Nivre, 2012) and tokenization (Zhang et al., 2013b; Zhang et al., 2014). Joint approaches avoid error propagation between the subtasks and often lead to overall better"
P16-1181,P14-5010,0,0.0133731,"006). We predict part-of-speech tags with the CRF tagger M AR M OT (M¨uller et al., 2013) and annotate the training sets via 10-fold jackknifing. Depending on the experimental sce1928 nario we use M AR M OT in two different settings – standard sentence-level where we train and apply it on sentences, and document-level where a whole document is fed to the tagger, implicitly treating it as a single very long sentence. Sentence boundary detection. We work with two well-established sentence boundary detection baselines. Following (Read et al., 2012) we use the tokenizer from the Stanford CoreNLP (Manning et al., 2014) and the sentence boundary detector from OpenNLP5 which has been shown to achieve state-of-the-art results on WSJ. We evaluate the performance of sentence boundary detection on the token level using F-measure (F1 ).6 Typical sentence boundary detectors such as C ORE NLP or O PEN NLP focus on punctuation marks and are therefore inapplicable to data like Switchboard that does not originally include punctuation. In such cases CRF taggers are commonly selected as baselines, e.g. for punctuation prediction experiments (Zhang et al., 2013a). We therefore introduce a third baseline using M AR M OT. F"
P16-1181,J93-2004,0,0.0582492,"d to L A SO (Daum´e III and Marcu, 2005), but differs in that it delays the updates until the full instance has been decoded. Bj¨orkelund and Kuhn (2014) show that the difference is important, as it prevents the learning algorithm from getting feedback within instances. Without the delay the learner can bias the weights for rare (e.g., lexicalized) features that occur within a single instance which renders the learning setting quite different from test time inference where no such feedback is available. 4 Experimental setup Data sets. We experiment with two parts of the English Penn Treebank (Marcus et al., 1993). We use the Wall Street Journal (WSJ) as an example of copy-edited newspaper-quality texts with proper punctuation and capitalized sentences. We also use the Switchboard portion which consists of (transcribed) telephone conversations between strangers. Following previous work on Switchboard we lowercase all text and remove punctuation and disfluency markups. We use sections 2-21 of the WSJ for training, 24 as development set and 23 as test set. For Switchboard we follow Charniak and Johnson (2001). We convert both data sets to Stanford dependencies with the Stanford dependency converter (de M"
P16-1181,D13-1032,0,0.0885763,"Missing"
P16-1181,W09-3811,0,0.0297112,"ms on the development sets. For WSJ all three algorithms achieve similar results which shows that M AR M OT is a competitive baseline. As can be seen, predicting sentence boundaries for the Switchboard dataset is a more difficult task than for well-formatted text like the WSJ. O PEN NLP C ORE NLP M AR M OT WSJ Switchboard 98.09 98.60 98.21 – – 71.78 Table 2: Results (F1 ) for baselines for sentence boundary detection on dev sets. Parser implementation. Our parser implements the labeled version of the transition system described in Section 2 with a default beam size of 20. We use the oracle by Nivre et al. (2009) to create transition sequences for each sentence of a document, and then concatenate them with S B transitions that occur as early as possible (cf. 5 http://opennlp.apache.org A true positive is defined as a token that was correctly predicted to begin a new sentence. 6 Section 2). The feature set is based on previous work (Zhang and Nivre, 2011; Bohnet and Kuhn, 2012; Bohnet et al., 2013) and was developed for a sentence-based parser for the WSJ. We made initial experiments trying to introduce new features aimed at capturing sentence boundaries such as trying to model verb subcategorization o"
P16-1181,P13-1013,0,0.0974686,"al., 2012) we use the tokenizer from the Stanford CoreNLP (Manning et al., 2014) and the sentence boundary detector from OpenNLP5 which has been shown to achieve state-of-the-art results on WSJ. We evaluate the performance of sentence boundary detection on the token level using F-measure (F1 ).6 Typical sentence boundary detectors such as C ORE NLP or O PEN NLP focus on punctuation marks and are therefore inapplicable to data like Switchboard that does not originally include punctuation. In such cases CRF taggers are commonly selected as baselines, e.g. for punctuation prediction experiments (Zhang et al., 2013a). We therefore introduce a third baseline using M AR M OT. For this, we augment the POS tags with information to indicate if a token starts a new sentence or not. We prepare the training data accordingly and train the document-level sequence labeler on them. Table 2 shows the accuracies of all baseline systems on the development sets. For WSJ all three algorithms achieve similar results which shows that M AR M OT is a competitive baseline. As can be seen, predicting sentence boundaries for the Switchboard dataset is a more difficult task than for well-formatted text like the WSJ. O PEN NLP C"
P16-1181,W03-3017,0,0.076596,"ointly has lately been popular in transition-based parsing, e.g., combining parsing with POS tagging (Hatori et al., 2011; Bohnet and Nivre, 2012) and tokenization (Zhang et al., 2013b; Zhang et al., 2014). Joint approaches avoid error propagation between the subtasks and often lead to overall better models, especially for the lower level tasks that suddenly have access to syntactic information. Our transition system is inspired by the work of Zhang et al. (2013a). They present a projective transition-based parser that jointly predicts punctuation and syntax. Their ArcEager transition system (Nivre, 2003) includes an additional transition that introduces punctuation similar to our S B transition. They also use beam search and circumvent the problem of long training sequences by chopping up the training data into pseudo-documents of at most 10 sentences. As we have shown, this solution works because the training instances are not long enough to hurt the performance. However, while this is possible for parsing, other tasks may not be able to chop up their training data. 8 Conclusion We have demonstrated that training a structured perceptron for inexact search on very long input sequences ignores"
P16-1181,P14-1125,0,0.0124128,"i (1997)’s MxTerminator). The task is often simplified to the task of period disambiguation (Kiss and Strunk, 2006), which only works on text that uses punctuation consistently. The current state of the art uses sequence labelers, e.g., a CRF (Evang et al., 2013; Dridan and Oepen, 2013). For a broad survey of methodology and tools, we refer the reader to Read et al. (2012). Joint models. Solving several tasks jointly has lately been popular in transition-based parsing, e.g., combining parsing with POS tagging (Hatori et al., 2011; Bohnet and Nivre, 2012) and tokenization (Zhang et al., 2013b; Zhang et al., 2014). Joint approaches avoid error propagation between the subtasks and often lead to overall better models, especially for the lower level tasks that suddenly have access to syntactic information. Our transition system is inspired by the work of Zhang et al. (2013a). They present a projective transition-based parser that jointly predicts punctuation and syntax. Their ArcEager transition system (Nivre, 2003) includes an additional transition that introduces punctuation similar to our S B transition. They also use beam search and circumvent the problem of long training sequences by chopping up the"
P16-1181,P09-1040,0,0.604381,"ser must predict the syntactic structure of the three sentences as well as the start points of each sentence.1 The simple fact that documents are considerably longer than sentences, often by orders of magnitude, creates some interesting challenges for a joint system. First of all, the decoder needs to handle long inputs efficiently. This problem is easily solved by using transition-based decoders, which excel in this kind of setting due to their incremental approach and their low theoretical complexity. Specifically, we use a transition-based decoder that extends the Swap transition system of Nivre (2009) in order to introduce sentence boundaries during the parsing process. The parser performs inexact search for the optimal structure by 1 We chose this example for its brevity. For this particular example, the task of sentence boundary prediction could be solved easily with speaker information since the second sentence is from another speaker’s turn. The interesting cases involve sentence segmentation within syntactically complex turns of a single speaker. 1924 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1924–1934, c Berlin, Germany, August 7-1"
P16-1181,C12-2096,0,0.240249,"Language Processing, solvable with very high accuracy. However, when dealing with the output of automatic speech recognition or “noisier” texts such as blogs and emails, non-trivial sentence segmentation issues do occur. Dridan and Oepen (2013), for example, show how much impact fully automatic preprocessing can have on parsing quality for well-edited and less-edited text. Two possible strategies to approach this problem are (i) to exploit other cues for sentence boundaries, such as prosodic phrasing and intonation in speech (e.g., Kol´aˇr et al. (2006)) or formatting cues in text documents (Read et al., 2012), and (ii) to emulate the human ability to exploit syntactic competence for segmentation. We focus here on the latter, which has received little attention, and propose to cast sentence boundary detection and syntactic (dependency) parsing as a joint problem, such that segmentations that would give rise to suboptimal syntactic structures can be discarded early on. A joint model for parsing and sentence boundary detection by definition operates on documents rather than single sentences, as is the standard case for parsing. The task is illustrated in Figure 1, which shows the beginning of a docum"
P16-1181,N15-1162,0,0.0134016,"hang and Clark (2008) first showed how to train transition-based parsers with the structured perceptron (Collins, 2002) using beam search and early update (Collins and Roark, 2004). It has since become the de facto standard way of training search-based transition-based dependency parsers (Huang and Sagae, 2010; Zhang and Nivre, 2011; Bohnet et al., 2013). Huang et al. (2012) showed how max-violation leads to faster convergence for transition-based parsers and max-violation updates have subsequently been applied to other tasks such as machine translation (Yu et al., 2013) and semantic parsing (Zhao and Huang, 2015). Sentence Boundary Detection. Sentence boundary detection has attracted only modest attention by the research community even though it is a component in every real-world NLP application. Previous work is divided into rule-based, e.g., CoreNLP (Manning et al., 2014), and machine learning approaches (e.g., OpenNLP, a re-implementation of Reynar and Ratnaparkhi (1997)’s MxTerminator). The task is often simplified to the task of period disambiguation (Kiss and Strunk, 2006), which only works on text that uses punctuation consistently. The current state of the art uses sequence labelers, e.g., a C"
P16-1181,A97-1004,0,0.399467,"ang et al. (2012) showed how max-violation leads to faster convergence for transition-based parsers and max-violation updates have subsequently been applied to other tasks such as machine translation (Yu et al., 2013) and semantic parsing (Zhao and Huang, 2015). Sentence Boundary Detection. Sentence boundary detection has attracted only modest attention by the research community even though it is a component in every real-world NLP application. Previous work is divided into rule-based, e.g., CoreNLP (Manning et al., 2014), and machine learning approaches (e.g., OpenNLP, a re-implementation of Reynar and Ratnaparkhi (1997)’s MxTerminator). The task is often simplified to the task of period disambiguation (Kiss and Strunk, 2006), which only works on text that uses punctuation consistently. The current state of the art uses sequence labelers, e.g., a CRF (Evang et al., 2013; Dridan and Oepen, 2013). For a broad survey of methodology and tools, we refer the reader to Read et al. (2012). Joint models. Solving several tasks jointly has lately been popular in transition-based parsing, e.g., combining parsing with POS tagging (Hatori et al., 2011; Bohnet and Nivre, 2012) and tokenization (Zhang et al., 2013b; Zhang et"
P16-1181,D13-1112,0,0.0669319,"NT is not significant. 7 Related work Zhang and Clark (2008) first showed how to train transition-based parsers with the structured perceptron (Collins, 2002) using beam search and early update (Collins and Roark, 2004). It has since become the de facto standard way of training search-based transition-based dependency parsers (Huang and Sagae, 2010; Zhang and Nivre, 2011; Bohnet et al., 2013). Huang et al. (2012) showed how max-violation leads to faster convergence for transition-based parsers and max-violation updates have subsequently been applied to other tasks such as machine translation (Yu et al., 2013) and semantic parsing (Zhao and Huang, 2015). Sentence Boundary Detection. Sentence boundary detection has attracted only modest attention by the research community even though it is a component in every real-world NLP application. Previous work is divided into rule-based, e.g., CoreNLP (Manning et al., 2014), and machine learning approaches (e.g., OpenNLP, a re-implementation of Reynar and Ratnaparkhi (1997)’s MxTerminator). The task is often simplified to the task of period disambiguation (Kiss and Strunk, 2006), which only works on text that uses punctuation consistently. The current state"
P16-1181,D08-1059,0,0.193985,"said you have four cats i have four cats how old are they . . . Figure 1: The beginning of a sample document from the Switchboard corpus. Tokens that start a sentence are underlined. The task is to predict syntactic structure and sentence boundaries jointly. maintaining a beam of several candidate derivations throughout the parsing process. We will show in this paper that, besides efficient decoding, a second, equally significant challenge lies in the way such a parser is trained. Normally, beam-search transition-based parsers are trained with structured perceptrons using either early update (Zhang and Clark, 2008; Collins and Roark, 2004) or max-violation updates (Huang et al., 2012). Yet our analysis demonstrates that neither of these update strategies is appropriate for training on very long input sequences as they discard a large portion of the training data.2 A significant part of the training data is therefore never used to train the model. As a remedy to this problem, we instead use an adaptation of the update strategy in Bj¨orkelund and Kuhn (2014). They apply early update in a coreference resolution system and observe that the task is inherently so difficult that the correct item practically n"
P16-1181,P11-2033,0,0.203463,"71.78 Table 2: Results (F1 ) for baselines for sentence boundary detection on dev sets. Parser implementation. Our parser implements the labeled version of the transition system described in Section 2 with a default beam size of 20. We use the oracle by Nivre et al. (2009) to create transition sequences for each sentence of a document, and then concatenate them with S B transitions that occur as early as possible (cf. 5 http://opennlp.apache.org A true positive is defined as a token that was correctly predicted to begin a new sentence. 6 Section 2). The feature set is based on previous work (Zhang and Nivre, 2011; Bohnet and Kuhn, 2012; Bohnet et al., 2013) and was developed for a sentence-based parser for the WSJ. We made initial experiments trying to introduce new features aimed at capturing sentence boundaries such as trying to model verb subcategorization or sentence length, however none of these proved useful compared to the baseline feature set. Following the line of work by Bohnet et al., we use the passiveaggressive algorithm (Crammer et al., 2006) instead of the vanilla perceptron, parameter averaging (Collins, 2002), and a hash function to map features (Bohnet, 2010).7 5 Analysis Comparison"
P16-1181,P13-1074,0,0.0734703,"al., 2012) we use the tokenizer from the Stanford CoreNLP (Manning et al., 2014) and the sentence boundary detector from OpenNLP5 which has been shown to achieve state-of-the-art results on WSJ. We evaluate the performance of sentence boundary detection on the token level using F-measure (F1 ).6 Typical sentence boundary detectors such as C ORE NLP or O PEN NLP focus on punctuation marks and are therefore inapplicable to data like Switchboard that does not originally include punctuation. In such cases CRF taggers are commonly selected as baselines, e.g. for punctuation prediction experiments (Zhang et al., 2013a). We therefore introduce a third baseline using M AR M OT. For this, we augment the POS tags with information to indicate if a token starts a new sentence or not. We prepare the training data accordingly and train the document-level sequence labeler on them. Table 2 shows the accuracies of all baseline systems on the development sets. For WSJ all three algorithms achieve similar results which shows that M AR M OT is a competitive baseline. As can be seen, predicting sentence boundaries for the Switchboard dataset is a more difficult task than for well-formatted text like the WSJ. O PEN NLP C"
P16-1181,P13-2111,0,\N,Missing
P17-1148,D15-1198,0,0.01974,"roaches to semantic parsing formalize the mapping from language to logic using a variety of formalisms including CFGs (B¨orschinger et al., 2011), CCGs (Kwiatkowski et al., 2010), synchronous CFGs (Wong and Mooney, 2007b). Deciding to use one formalism over another is often motivated by the complexities of the target representations being learned. For example, recent interest in learning graph-based representations such as those in the AMR bank (Banarescu et al., 2013) 3.2 Language Modeling Baselines 1615 requires parsing models that can generate complex graph shaped derivations such as CCGs (Artzi et al., 2015) or HRGs (Peng et al., 2015). Given the simplicity of our API representations, we opt for a simple semantic parsing model that exploits the finiteness of our target representations. Following ((Deng and Chrupała, 2014); henceforth DC), we treat the problem of component translation as a language modeling problem (Song and Croft, 1999). For a given query sequence or text x = wi , .., wI and component sequence z = uj , .., uJ , the probability of the component given the query is defined as follows using Bayes’ theorem: p(z|x) ∝ p(x|z)p(z). By assuming a uniform prior over the probability of each"
P17-1148,W13-2322,0,0.0317972,"set of output components are known. In the API documentation sets, this is because each standard library contains a finite number of funcExisting approaches to semantic parsing formalize the mapping from language to logic using a variety of formalisms including CFGs (B¨orschinger et al., 2011), CCGs (Kwiatkowski et al., 2010), synchronous CFGs (Wong and Mooney, 2007b). Deciding to use one formalism over another is often motivated by the complexities of the target representations being learned. For example, recent interest in learning graph-based representations such as those in the AMR bank (Banarescu et al., 2013) 3.2 Language Modeling Baselines 1615 requires parsing models that can generate complex graph shaped derivations such as CCGs (Artzi et al., 2015) or HRGs (Peng et al., 2015). Given the simplicity of our API representations, we opt for a simple semantic parsing model that exploits the finiteness of our target representations. Following ((Deng and Chrupała, 2014); henceforth DC), we treat the problem of component translation as a language modeling problem (Song and Croft, 1999). For a given query sequence or text x = wi , .., wI and component sequence z = uj , .., uJ , the probability of the co"
P17-1148,D13-1160,0,0.639395,"lso show that modest improvements can be achieved by using a more conventional 1613 discriminative model (Zettlemoyer and Collins, 2009) that, in part, exploits document-level features from the technical documentation sets. 2 Related Work Our work is situated within research on semantic parsing, which focuses on the problem of generating formal meaning representations from text for natural language understanding applications. Recent interest in this topic has centered around learning meaning representation from example text-meaning pairs, for applications such as automated question-answering (Berant et al., 2013), robot control (Matuszek et al., 2012) and text generation (Wong and Mooney, 2007a). While generating representations for natural language understanding is a complex task, most studies focus on the translation or generation problem independently of other semantic or knowledge representation issues. Earlier work looks at supervised learning of logical representations using example text-meaning pairs using tools from statistical machine translation (Wong and Mooney, 2006) and parsing (Zettlemoyer and Collins, 2009). These methods are meant to be applicable to a wide range of translation problem"
P17-1148,P14-1133,0,0.0300018,"r Accuracy @1. While one might expect better results when moving from a word-based model to a model that exploits phrase and hierarchical phrase features, the sparsity of the component vocabulary is such that most phrase patterns in the training are not observed in the evaluation. In many benchmark semantic parsing datasets, such sparsity issues do not occur (Cimiano and Minock, 2009), suggesting that state-of-the-art methods will have similar problems when applied to our datasets. Recent approaches to open-domain semantic parsing have dealt with this problem by using paraphrasing techniques (Berant and Liang, 2014) or distant supervision (Reddy et al., 2014). We expect that these methods can be used to improve our models and results, especially given the wide availability of technical documentation, for example, distributed within the Opus project (Tiedemann, 2012). Model Errors We performed analysis on some of the incorrect predictions made by our models. For some documentation sets, such as those in the GNU documentation collection2 , information is organized into a small and concrete set of categories/chapters, each corresponding to various features or modules in the language and related functions. G"
P17-1148,D11-1131,0,0.0773203,"Missing"
P17-1148,J93-2003,0,0.0984798,": procedure R ANK C OMPONENTS(x, C, k, A) 2: S CORES ← [ ] ▷ Initialize score list 3: for each component c ∈ C do 4: p ← A LIGNA (x, c) ▷ Score using A 5: S CORES += (c, p) ▷ Add to list 6: return K-Best(S CORES,k) ▷ k best components both input strings. This function, and the definition of l(j), assumes different forms according to the particular alignment model being used. We consider three different types of alignment models each defined in the following way:  1 (1)  J+1 pd (l(j)|...) = a(j|i, I, J) (2)  a(t(j)|i, I, tlen(J)) (3) Models (1-2) are the classic IBM word-alignment models of Brown et al. (1993). IBM Model 1, for example, assumes a uniform distribution over all positions, and is the main model investigated in DC. For comparison, we also experiment with IBM Model 2, where each l(j) refers to the string position of j in the component input, and a(..) ∑J defines a multinomial distribution such that j=0 a(j|i, I, J) = 1.0. We also define a new tree based alignment model (3) that takes into account the syntax associated with the function representations. Each l(j) is the relative tree position of the alignment point, shown as t(j), and tlen(J) is the length of the tree associated with z."
P17-1148,deng-chrupala-2014-semantic,0,0.238474,"epresentations using such a wide variety of programming and natural languages. In total, we introduce fourteen new datasets in the source code domain that include seven natural languages, and report new results for an existing dataset. As well, we look at learning simple code templates using a small collection of English Unix manuals. The main goal of this paper is to establish strong baselines results on these resources, which we hope can be used for benchmarking and developing new semantic parsing methods. We achieved initial baselines using the language modeling and translation approach of Deng and Chrupała (2014). We also show that modest improvements can be achieved by using a more conventional 1613 discriminative model (Zettlemoyer and Collins, 2009) that, in part, exploits document-level features from the technical documentation sets. 2 Related Work Our work is situated within research on semantic parsing, which focuses on the problem of generating formal meaning representations from text for natural language understanding applications. Recent interest in this topic has centered around learning meaning representation from example text-meaning pairs, for applications such as automated question-answe"
P17-1148,P16-1004,0,0.0129711,"lences, which is an issue that should be investigated in future work. Future work will also look systematically at the effect that types (i.e., in statically typed versus dynamic languages) have on prediction. 7 Future Work We see two possible use cases for this data. First, for benchmarking semantic parsing models on the task of semantic translation. While there has been a trend towards learning executable semantic parsers (Berant et al., 2013; Liang, 2016), there has also been renewed interest in supervised learning of formal representations in the context of neural semantic parsing models (Dong and Lapata, 2016; Jia and Liang, 2016). We believe that good performance on our datasets should lead to better performance on more conventional semantic parsing tasks, and raise new challenges involving sparsity and multilingual learning. We also see these resources as useful for investigations into natural language programming. While our experiments look at learning rudimentary translational correspondences between text and code, a next step might be learning to synthesize executable programs via these translations, along the lines of (Desai et al., 2016; Raza et al., 2015). Other document-level features, su"
P17-1148,P08-1115,0,0.012138,"Chiang, 2007) extracted from the alignment and tree information. which is simplified by the finite nature of the target language. The complexity of the scoring procedure, lines 3-5, is linear over the number components m in C. In practice, we implement the K-B EST sorting function on line 6 as a binary insertion sort on line 5, resulting in an overall complexity of O(m log m). While iterating over m API components might not be feasible given more complicated formal languages with recursion, a more clever decoding algorithm could be applied, e.g., one based on the lattice decoding approach of (Dyer et al., 2008). Since we are interested in providing initial baseline results, we leave this for future work. 4 ’the arg of..’ c4 ={ cosh ,acosh,sinh.} bool ZipArchive::deleteName(string $name) Discriminative Approach In this section, we introduce a new model that aims to improve on the previous baseline methods. While the previous models are restricted to word-level information, we extend this approach by using a discriminative reranking model that captures phrase information to see if this leads to an improvement. This model can also capture document-level information from the APIs, such as the additional"
P17-1148,P16-1195,0,0.0210219,"Chrupała (2014). Robustly learning the translation from language to code representations can help to facilitate natural language querying of API collections (Lv et al., 2015). As part of this effort, recent work in machine learning has focused on the similar problem of learning code representations using resources such as StackOverflow and Github. These studies primarily focus on learning longer programs (Allamanis et al., 2015) as opposed to function representations, or focus narrowly on a single programming language such as Java (Gu et al., 2016) or on related tasks such as text generation (Iyer et al., 2016; Oda et al., 2015). To our knowledge, none of this work has been applied to languages other than English or such a wide variety of programming languages. 3 Mapping Text to Representations In this section, we formulate the basic problem of translating to representations in technical documentation. 3.1 Problem Description We use the term technical documentation to refer to two types of resources: textual descriptions inside of source code collections, and computer utility manuals. In this paper, the first type includes high-level descriptions of functions in standard library source code documen"
P17-1148,P16-1002,0,0.0146912,"ue that should be investigated in future work. Future work will also look systematically at the effect that types (i.e., in statically typed versus dynamic languages) have on prediction. 7 Future Work We see two possible use cases for this data. First, for benchmarking semantic parsing models on the task of semantic translation. While there has been a trend towards learning executable semantic parsers (Berant et al., 2013; Liang, 2016), there has also been renewed interest in supervised learning of formal representations in the context of neural semantic parsing models (Dong and Lapata, 2016; Jia and Liang, 2016). We believe that good performance on our datasets should lead to better performance on more conventional semantic parsing tasks, and raise new challenges involving sparsity and multilingual learning. We also see these resources as useful for investigations into natural language programming. While our experiments look at learning rudimentary translational correspondences between text and code, a next step might be learning to synthesize executable programs via these translations, along the lines of (Desai et al., 2016; Raza et al., 2015). Other document-level features, such as example input-ou"
P17-1148,N03-1017,0,0.0246834,"output z for each input x: O(θ) = ∑n log p (zi |xi ; θ). i=1 4.2 Features Our model uses word-level features, such as word match, word pairs, as well as information from the underlying aligner model such as Viterbi alignment information and model score. Two additional categories of non-word features are described below. An illustration of the feature extraction procedure is shown in Figure 5 1 . Phrases Features We extract phrase features (e.g., (hyper. cosine, cosh) in Figure 5) from example text component pairs by training symmetric word aligners and applying standard word-level heuristics (Koehn et al., 2003). Additional features, such as phrase match/overlap, tree positions of phrases, are defined over the extracted phrases. We also extract hierarchical phrases (Chiang, 2007) using a variant of the SAMT method of Zollmann and Venugopal (2006) and the component syntax trees. Example rules are shown in Figure 4, where gaps (i.e., symbols in square brackets) are filled with smaller phrase-tree alignments. Document Level Features Document features are of two categories. The first includes additional textual descriptions of parameters, return values, and modules. One class of features is whether certa"
P17-1148,P14-1026,0,0.018472,"as a return value and a namespace. Components in utility manuals are short executable code sequences intended to show an example use of a utility. We assume typed code sequences following Richardson and Kuhn (2014), where the constituent parts of the sequences are abstracted by type. tion representations, roughly corresponding to the number of pairs as shown in Figure 3. For a given input, therefore, the goal is to find the best candidate function translation within the space of the total API components C (Deng and Chrupała, 2014). Given these constraints, our setup closely resembles that of Kushman et al. (2014), who learn to parse algebra word problems using a small set of equation templates. Their approach is inspired by template-based information extraction, where templates are recognized and instantiated by slotfilling. Our function signatures and code templates have a similar slot-like structure, consisting of slots such as return value, arguments, function name and namespace. Given a set of example text-component pairs, D = {(xi , zi )}ni=1 , the goal is to learn how to generate correct, well-formed components z ∈ C for each input x. Viewed as a semantic parsing problem, this treats the target"
P17-1148,N13-1103,0,0.0181533,"ily English-based, while we focus on learning in a multilingual setting using several new moderately sized datasets. Within semantic parsing, there has also been work on situated or grounded learning, that involves learning in domains with weak supervision and indirect cues (Liang, 2016; Richardson and Kuhn, 2016). This has sometimes involved learning from automatically generated parallel data and representations (Chen and Mooney, 2008) of the type we consider in this paper. Here one can find work in technical domains, including learning to generate regular expressions (Manshadi et al., 2013; Kushman and Barzilay, 2013) and other types of source code (Quirk et al., 2015), which ultimately aim to solve the problem of natural language programming. We view our work as one small step in this general direction. Our work is also related to software components retrieval and builds on the approach of Deng and Chrupała (2014). Robustly learning the translation from language to code representations can help to facilitate natural language querying of API collections (Lv et al., 2015). As part of this effort, recent work in machine learning has focused on the similar problem of learning code representations using resour"
P17-1148,D10-1119,0,0.0277056,"zi )}ni=1 , the goal is to learn how to generate correct, well-formed components z ∈ C for each input x. Viewed as a semantic parsing problem, this treats the target components as a kind of formal meaning representation, analogous to a logical form. In our experiments, we assume that the complete set of output components are known. In the API documentation sets, this is because each standard library contains a finite number of funcExisting approaches to semantic parsing formalize the mapping from language to logic using a variety of formalisms including CFGs (B¨orschinger et al., 2011), CCGs (Kwiatkowski et al., 2010), synchronous CFGs (Wong and Mooney, 2007b). Deciding to use one formalism over another is often motivated by the complexities of the target representations being learned. For example, recent interest in learning graph-based representations such as those in the AMR bank (Banarescu et al., 2013) 3.2 Language Modeling Baselines 1615 requires parsing models that can generate complex graph shaped derivations such as CCGs (Artzi et al., 2015) or HRGs (Peng et al., 2015). Given the simplicity of our API representations, we opt for a simple semantic parsing model that exploits the finiteness of our t"
P17-1148,P11-1060,0,0.037061,"ch In this section, we introduce a new model that aims to improve on the previous baseline methods. While the previous models are restricted to word-level information, we extend this approach by using a discriminative reranking model that captures phrase information to see if this leads to an improvement. This model can also capture document-level information from the APIs, such as the additional textual descriptions of parameters, see also declarations or classes of related functions and syntax information. 4.1 Modeling Like in most semantic parsing approaches (Zettlemoyer and Collins, 2009; Liang et al., 2011), our model is defined as a conditional log-linear model over components z ∈ C with parameters θ ∈ Rb , and a set of feature functions ϕ(x, z): p( z |x; θ) ∝ eθ·ϕ(x,z) . Formally, our training objective is to maximize the conditional log-likehood of the correct component output z for each input x: O(θ) = ∑n log p (zi |xi ; θ). i=1 4.2 Features Our model uses word-level features, such as word match, word pairs, as well as information from the underlying aligner model such as Viterbi alignment information and model score. Two additional categories of non-word features are described below. An ill"
P17-1148,J03-1002,0,0.0261619,"ine is defined below, where f (w|z) is the frequency of matching terms in the target signature, f (w|C) is frequency of the term word in the overall documentation collection, and λ is a smoothing parameter (for Jelinek-Mercer smoothing): ∏ p(x|z) = (1 − λ)f (w|z) + λf (w|C) w∈x Translation Model In order to account for the co-occurrence between non-matching words and component terms, DC employ a word-based translation model, which models the relation between natural language words wj and individual component terms uj . In this paper, we limit ourselves to sequence-based word alignment models (Och and Ney, 2003), which factor in the following manner: p(x|z) = I ∑ J ∏ i=1 j=0 pt (wi |uj )pd (l(j)|i, I, J) Algorithm 1 Rank Decoder Input: Query x, Components C of size m, rank k, model A, sort function K-B EST Output: Top k components ranked by A model score p 1: procedure R ANK C OMPONENTS(x, C, k, A) 2: S CORES ← [ ] ▷ Initialize score list 3: for each component c ∈ C do 4: p ← A LIGNA (x, c) ▷ Score using A 5: S CORES += (c, p) ▷ Add to list 6: return K-Best(S CORES,k) ▷ k best components both input strings. This function, and the definition of l(j), assumes different forms according to the particular"
P17-1148,P15-1142,0,0.0202876,"requires considerable domain knowledge and knowledge of logic. Alternatively, we construct parallel datasets automatically from technical documentation, which obviates the need for annotation. While the formal representations are not actual logical forms, they still provide a good test case for testing how well semantic parsers learn translations to representations. To date, most benchmark datasets are limited to small controlled domains, such as geography and navigation. While attempts have been made to do open-domain semantic parsing using larger, more complex datasets (Berant et al., 2013; Pasupat and Liang, 2015), such resources are still scarce. In Figure 3, we compare the details of one widely used dataset, Geoquery (Zelle and Mooney, 1996), to our new datasets. Our new resources are on average much larger than geoquery in terms of the number of example pairs, and the size of the different language vocabularies. Most existing datasets are also primarily English-based, while we focus on learning in a multilingual setting using several new moderately sized datasets. Within semantic parsing, there has also been work on situated or grounded learning, that involves learning in domains with weak supervisi"
P17-1148,K15-1004,0,0.0139986,"ormalize the mapping from language to logic using a variety of formalisms including CFGs (B¨orschinger et al., 2011), CCGs (Kwiatkowski et al., 2010), synchronous CFGs (Wong and Mooney, 2007b). Deciding to use one formalism over another is often motivated by the complexities of the target representations being learned. For example, recent interest in learning graph-based representations such as those in the AMR bank (Banarescu et al., 2013) 3.2 Language Modeling Baselines 1615 requires parsing models that can generate complex graph shaped derivations such as CCGs (Artzi et al., 2015) or HRGs (Peng et al., 2015). Given the simplicity of our API representations, we opt for a simple semantic parsing model that exploits the finiteness of our target representations. Following ((Deng and Chrupała, 2014); henceforth DC), we treat the problem of component translation as a language modeling problem (Song and Croft, 1999). For a given query sequence or text x = wi , .., wI and component sequence z = uj , .., uJ , the probability of the component given the query is defined as follows using Bayes’ theorem: p(z|x) ∝ p(x|z)p(z). By assuming a uniform prior over the probability of each component p(z), the problem"
P17-1148,P07-1121,0,0.291034,"13 discriminative model (Zettlemoyer and Collins, 2009) that, in part, exploits document-level features from the technical documentation sets. 2 Related Work Our work is situated within research on semantic parsing, which focuses on the problem of generating formal meaning representations from text for natural language understanding applications. Recent interest in this topic has centered around learning meaning representation from example text-meaning pairs, for applications such as automated question-answering (Berant et al., 2013), robot control (Matuszek et al., 2012) and text generation (Wong and Mooney, 2007a). While generating representations for natural language understanding is a complex task, most studies focus on the translation or generation problem independently of other semantic or knowledge representation issues. Earlier work looks at supervised learning of logical representations using example text-meaning pairs using tools from statistical machine translation (Wong and Mooney, 2006) and parsing (Zettlemoyer and Collins, 2009). These methods are meant to be applicable to a wide range of translation problems and representation types, which make new parallel datasets or resources useful f"
P17-1148,P09-1110,0,0.473837,"Missing"
P17-1148,W06-3119,0,0.0456184,"rmation and model score. Two additional categories of non-word features are described below. An illustration of the feature extraction procedure is shown in Figure 5 1 . Phrases Features We extract phrase features (e.g., (hyper. cosine, cosh) in Figure 5) from example text component pairs by training symmetric word aligners and applying standard word-level heuristics (Koehn et al., 2003). Additional features, such as phrase match/overlap, tree positions of phrases, are defined over the extracted phrases. We also extract hierarchical phrases (Chiang, 2007) using a variant of the SAMT method of Zollmann and Venugopal (2006) and the component syntax trees. Example rules are shown in Figure 4, where gaps (i.e., symbols in square brackets) are filled with smaller phrase-tree alignments. Document Level Features Document features are of two categories. The first includes additional textual descriptions of parameters, return values, and modules. One class of features is whether certain words under consideration appear in the @param and @return descriptions of the target components. For example, the arg token in 1 A more complete description of features is included as supplementary material, along with all source code."
P17-1148,P15-1085,0,0.0180501,"ual setting using several new moderately sized datasets. Within semantic parsing, there has also been work on situated or grounded learning, that involves learning in domains with weak supervision and indirect cues (Liang, 2016; Richardson and Kuhn, 2016). This has sometimes involved learning from automatically generated parallel data and representations (Chen and Mooney, 2008) of the type we consider in this paper. Here one can find work in technical domains, including learning to generate regular expressions (Manshadi et al., 2013; Kushman and Barzilay, 2013) and other types of source code (Quirk et al., 2015), which ultimately aim to solve the problem of natural language programming. We view our work as one small step in this general direction. Our work is also related to software components retrieval and builds on the approach of Deng and Chrupała (2014). Robustly learning the translation from language to code representations can help to facilitate natural language querying of API collections (Lv et al., 2015). As part of this effort, recent work in machine learning has focused on the similar problem of learning code representations using resources such as StackOverflow and Github. These studies"
P17-1148,Q14-1030,0,0.0199489,"ults when moving from a word-based model to a model that exploits phrase and hierarchical phrase features, the sparsity of the component vocabulary is such that most phrase patterns in the training are not observed in the evaluation. In many benchmark semantic parsing datasets, such sparsity issues do not occur (Cimiano and Minock, 2009), suggesting that state-of-the-art methods will have similar problems when applied to our datasets. Recent approaches to open-domain semantic parsing have dealt with this problem by using paraphrasing techniques (Berant and Liang, 2014) or distant supervision (Reddy et al., 2014). We expect that these methods can be used to improve our models and results, especially given the wide availability of technical documentation, for example, distributed within the Opus project (Tiedemann, 2012). Model Errors We performed analysis on some of the incorrect predictions made by our models. For some documentation sets, such as those in the GNU documentation collection2 , information is organized into a small and concrete set of categories/chapters, each corresponding to various features or modules in the language and related functions. Given this information, Figure 1619 2 https:/"
P17-1148,richardson-kuhn-2014-unixman,1,0.830129,", or components. In source code, components are formal representations of functions, or function signatures (Deng and Chrupała, 2014). The form of a function signature varies depending on the resource, but in general gives a specification of how a function is named and structured. The example function signatures in Figure 3 all specify a function name, a list of arguments, and other optional information such as a return value and a namespace. Components in utility manuals are short executable code sequences intended to show an example use of a utility. We assume typed code sequences following Richardson and Kuhn (2014), where the constituent parts of the sequences are abstracted by type. tion representations, roughly corresponding to the number of pairs as shown in Figure 3. For a given input, therefore, the goal is to find the best candidate function translation within the space of the total API components C (Deng and Chrupała, 2014). Given these constraints, our setup closely resembles that of Kushman et al. (2014), who learn to parse algebra word problems using a small set of equation templates. Their approach is inspired by template-based information extraction, where templates are recognized and instan"
P17-1148,Q16-1012,1,0.849506,"In Figure 3, we compare the details of one widely used dataset, Geoquery (Zelle and Mooney, 1996), to our new datasets. Our new resources are on average much larger than geoquery in terms of the number of example pairs, and the size of the different language vocabularies. Most existing datasets are also primarily English-based, while we focus on learning in a multilingual setting using several new moderately sized datasets. Within semantic parsing, there has also been work on situated or grounded learning, that involves learning in domains with weak supervision and indirect cues (Liang, 2016; Richardson and Kuhn, 2016). This has sometimes involved learning from automatically generated parallel data and representations (Chen and Mooney, 2008) of the type we consider in this paper. Here one can find work in technical domains, including learning to generate regular expressions (Manshadi et al., 2013; Kushman and Barzilay, 2013) and other types of source code (Quirk et al., 2015), which ultimately aim to solve the problem of natural language programming. We view our work as one small step in this general direction. Our work is also related to software components retrieval and builds on the approach of Deng and"
P17-1148,tiedemann-2012-parallel,0,0.0200085,"in the evaluation. In many benchmark semantic parsing datasets, such sparsity issues do not occur (Cimiano and Minock, 2009), suggesting that state-of-the-art methods will have similar problems when applied to our datasets. Recent approaches to open-domain semantic parsing have dealt with this problem by using paraphrasing techniques (Berant and Liang, 2014) or distant supervision (Reddy et al., 2014). We expect that these methods can be used to improve our models and results, especially given the wide availability of technical documentation, for example, distributed within the Opus project (Tiedemann, 2012). Model Errors We performed analysis on some of the incorrect predictions made by our models. For some documentation sets, such as those in the GNU documentation collection2 , information is organized into a small and concrete set of categories/chapters, each corresponding to various features or modules in the language and related functions. Given this information, Figure 1619 2 https://www.gnu.org/doc/doc.en.html dash sel Files Backups Buffers Windows CommandLoop Keymaps Modes Documentation Frames Positions Strings Datatypes Characters Numbers Hash Tables Sequences Evaluation Symbols OS Garba"
P17-1148,N06-1056,0,0.0898699,"around learning meaning representation from example text-meaning pairs, for applications such as automated question-answering (Berant et al., 2013), robot control (Matuszek et al., 2012) and text generation (Wong and Mooney, 2007a). While generating representations for natural language understanding is a complex task, most studies focus on the translation or generation problem independently of other semantic or knowledge representation issues. Earlier work looks at supervised learning of logical representations using example text-meaning pairs using tools from statistical machine translation (Wong and Mooney, 2006) and parsing (Zettlemoyer and Collins, 2009). These methods are meant to be applicable to a wide range of translation problems and representation types, which make new parallel datasets or resources useful for furthering the research. In general, however, such datasets are hard to construct since building them requires considerable domain knowledge and knowledge of logic. Alternatively, we construct parallel datasets automatically from technical documentation, which obviates the need for annotation. While the formal representations are not actual logical forms, they still provide a good test c"
P17-1148,N07-1022,0,0.211154,"13 discriminative model (Zettlemoyer and Collins, 2009) that, in part, exploits document-level features from the technical documentation sets. 2 Related Work Our work is situated within research on semantic parsing, which focuses on the problem of generating formal meaning representations from text for natural language understanding applications. Recent interest in this topic has centered around learning meaning representation from example text-meaning pairs, for applications such as automated question-answering (Berant et al., 2013), robot control (Matuszek et al., 2012) and text generation (Wong and Mooney, 2007a). While generating representations for natural language understanding is a complex task, most studies focus on the translation or generation problem independently of other semantic or knowledge representation issues. Earlier work looks at supervised learning of logical representations using example text-meaning pairs using tools from statistical machine translation (Wong and Mooney, 2006) and parsing (Zettlemoyer and Collins, 2009). These methods are meant to be applicable to a wide range of translation problems and representation types, which make new parallel datasets or resources useful f"
P19-1012,P18-2003,0,0.0179253,"syntactic relations. Such analyses differ in terms of: (1) methodology they employ to probe what type of knowledge the representations learned and (2) tasks on which the representations are trained on. Shi et al. (2016) demonstrated that sequence-to-sequence machinetranslation systems capture source-language syntactic relations. Linzen et al. (2016) showed that when trained on the task of number agreement prediction the representations capture a nontrivial amount of grammatical structure (although recursive neural networks are better at this task than sequential LSTMs (Kuncoro et al., 2018)). Blevins et al. (2018) found that RNN representations trained on a variety of NLP tasks (including dependency parsing) are able to induce syntactic features (e.g., constituency labels of parent or grandparent) even without explicit supervision. Finally, Conneau et al. (2018) designed a set of tasks probing linguistic knowledge of sentence embedding methods. Our work contributes to this line of research in two ways: (1) from the angle of methodology, we show how to employ derivatives to pinpoint what syntactic relations the representations learn; (2) from the perspective of tasks, we demonstrate how BiLSTM-based dep"
P19-1012,P18-1248,0,0.150744,"Missing"
P19-1012,D14-1082,0,0.109794,"bSibl TbMin TbExt 80k 60k 0.7 40k 0.6 Bin size 4 20k 0.5 1 2 3 4 5 6 7 8 9 10 11 12 13 1415+ Dependency length 0 Figure 2: Dependency recall relative to arc length on development sets. The corresponding plot for precision shows similar trends (see Figure 7 in Appendix A). 4.2 Influence of BiLSTMs We now investigate whether BiLSTMs are the reason for models being able to compensate for lack of features drawn from partial subtrees. Transition-based parser. We train T B PARS in two settings: with and without BiLSTMs (when no BiLSTMs are used we pass vectors xi directly to the MLP layer following Chen and Manning (2014)) and with different feature sets. We start with a feature set {s0 } and consecutively add more until we reach the full feature model of T B E XT. Figure 3a displays the accuracy of all the trained models. First of all, we notice that the models without BiLSTMs (light bars) benefit from structural features. The biggest gains in the average performance are visible after adding vectors s0L (5.15 LAS) and s1R (1.12 LAS) . After adding s1R the average improvements become modest. Adding the BiLSTM representations changes the picture (dark bars). First of all, as in the case of arc-standard system ("
P19-1012,Q16-1023,0,0.632895,"l ablations we demonstrate that the structural context is not only present in the models, but it significantly influences their performance. 1 Introduction When designing a conventional non-neural parser substantial effort is required to design a powerful feature extraction function. Such a function (McDonald et al., 2005; Zhang and Nivre, 2011, among others) is constructed so that it captures as much structural context as possible. The context allows the parser to make well-informed decisions.1 It is encoded in features built from partial subtrees and explicitly used by the models. Recently, Kiperwasser and Goldberg (2016, K&G) showed that the conventional feature extraction functions can be replaced by modeling the left- and right-context of each word with BiLSTMs (Hochreiter and Schmidhuber, 1997; 2 See results from the recent CoNLL 2018 shared task on dependency parsing (Zeman et al., 2018) for a comparison of various high-performing dependency parsers. 3 To the best of our knowledge, this is the first BiLSTMbased second-order dependency parser. G´omez-Rodr´ıguez et al. (2018) incorporate BiLSTM-based representations into the third-order 1-Endpoint-Crossing parser of Pitler (2014). 1 See Figure 1 for the co"
P19-1012,P18-1198,0,0.0258429,"Missing"
P19-1012,P18-1132,0,0.0326713,"whether they can learn syntactic relations. Such analyses differ in terms of: (1) methodology they employ to probe what type of knowledge the representations learned and (2) tasks on which the representations are trained on. Shi et al. (2016) demonstrated that sequence-to-sequence machinetranslation systems capture source-language syntactic relations. Linzen et al. (2016) showed that when trained on the task of number agreement prediction the representations capture a nontrivial amount of grammatical structure (although recursive neural networks are better at this task than sequential LSTMs (Kuncoro et al., 2018)). Blevins et al. (2018) found that RNN representations trained on a variety of NLP tasks (including dependency parsing) are able to induce syntactic features (e.g., constituency labels of parent or grandparent) even without explicit supervision. Finally, Conneau et al. (2018) designed a set of tasks probing linguistic knowledge of sentence embedding methods. Our work contributes to this line of research in two ways: (1) from the angle of methodology, we show how to employ derivatives to pinpoint what syntactic relations the representations learn; (2) from the perspective of tasks, we demonstr"
P19-1012,P16-2006,0,0.0182853,"the  vector s0 is recalculated without knowledge of s1 the model loses information about the distance between them. Secondly, we can notice that other drops depend on both the impact and frequency of positions. The biggest declines are visible after removing s0L and s1R – precisely the positions which we found to have the highest impact on the parsing decisions. Interestingly, the positions which were not a part of the T B E XT feature set, such as s0L or s1R , although not frequent are important for the performance.   6 Related Work Feature extraction. Kiperwasser and Goldberg (2016) and Cross and Huang (2016) first applied BiLSTMs to extract features for transition-based dependency parsers. The authors demonstrated that an architecture using only a few positional features (four for the arc-hybrid system and three for arc-standard) is sufficient to achieve state-ofthe-art performance. Shi et al. (2017) showed that this number can be further reduced to two features for arc-hybrid and arc-eager systems. Decreasing the size of the feature set not only allows for construction of lighter and faster neural networks (Wang and Chang, 2016; Vilares and G´omez-Rodr´ıguez, 2018) but also enables the use of ex"
P19-1012,N19-1159,0,0.189833,"Missing"
P19-1012,P81-1022,0,0.562185,"Missing"
P19-1012,K17-3022,0,0.135207,"Missing"
P19-1012,P15-1033,0,0.0367233,"he difference between the two architectures. The one clear advantage of the graphbased parser is that it performs global inference (but exact search algorithms are already being applied to projective (Shi et al., 2017) and nonprojective (G´omez-Rodr´ıguez et al., 2018) transition systems). Therefore, an interesting question is if integrating those two architectures can still be beneficial for the parsing accuracy as in Nivre and McDonald (2008). We leave this question for future work. tive than ours – composition. They showed that composing the structural context with recursive networks as in Dyer et al. (2015) is redundant for the K&G transition-based architecture. The authors analyze components of the BiLSTMs to show which of them (forward v. backward LSTM) is responsible for capturing subtree information. RNNs and syntax. Recurrent neural networks, which BiLSTMs are a variant of, have been repeatedly analyzed to understand whether they can learn syntactic relations. Such analyses differ in terms of: (1) methodology they employ to probe what type of knowledge the representations learned and (2) tasks on which the representations are trained on. Shi et al. (2016) demonstrated that sequence-to-seque"
P19-1012,Q16-1037,0,0.170692,"ncode more information than provided by the manually designed feature sets. 5 Implicit Structural Context Now that we have established that structural features are indeed redundant for models which employ BiLSTMs we examine the ways in which the simple parsing models (T B M IN and G B M IN) implicitly encode information about partial subtrees. 5.1 Structure and BiLSTM Representations We start by looking at the BiLSTM representations. We know that the representations are capable of capturing syntactic relations when they are trained on a syntactically related task, e.g, number prediction task (Linzen et al., 2016). We evaluate how complicated those relations can be when the representations are trained together with a dependency parser. To do so, we follow Gaddy et al. (2018) and use derivatives to estimate how sensitive a particular part of the architecture is with respect to changes  in input. Specifically, for every vector x we measure how it is influenced by every word represen121 20 Other Head Grand Child Sibl 15 10 Average impact Average impact 20 5 0 0 5 10 Distance 15 Other Head Grand Child Sibl 15 10 5 0 20 0 (a) Transition-based parser (T B M IN) 5 10 Distance 15 20 (b) Graph-based parser (G"
P19-1012,C96-1058,0,0.409957,"learns a scoring function which scores the correct tree higher than all the other possible ones. While decoding it searches for the highest scoring tree for a given sentence. The parser employs an arcfactored approach (McDonald et al., 2005), i.e., it decomposes the score of a tree to the sum of the scores of its arcs. Figure 1b shows the K&G graph-based architecture. At parsing time, every pair of words   hxi , xj i yields a BiLSTM representation { xi , xj } (red arrows in the figure) which is passed to MLP to compute the score for an arc xi → xj . To find the highest scoring tree we apply Eisner (1996)’s algorithm. We denote this architecture G B M IN. We note in passing that, although this decoding algorithm is restricted to projective trees, it has the advantage that it can be extended to incorporate non-local features while still maintaining exact search in polynomial time.4 The above-mentioned simple architecture uses   a feature set of two vectors { h , d }. We extend it and add information about structural context. Specifically, we incorporate information  about siblings s (blue arrows in the figure). The model follows the second-order model from McDonald and Pereira (2006) and dec"
P19-1012,P05-1012,0,0.817933,"at features drawn from partial subtrees become redundant when the BiLSTMs are used. We provide a deep insight into information flow in transition- and graph-based neural architectures to demonstrate where the implicit information comes from when the parsers make their decisions. Finally, with model ablations we demonstrate that the structural context is not only present in the models, but it significantly influences their performance. 1 Introduction When designing a conventional non-neural parser substantial effort is required to design a powerful feature extraction function. Such a function (McDonald et al., 2005; Zhang and Nivre, 2011, among others) is constructed so that it captures as much structural context as possible. The context allows the parser to make well-informed decisions.1 It is encoded in features built from partial subtrees and explicitly used by the models. Recently, Kiperwasser and Goldberg (2016, K&G) showed that the conventional feature extraction functions can be replaced by modeling the left- and right-context of each word with BiLSTMs (Hochreiter and Schmidhuber, 1997; 2 See results from the recent CoNLL 2018 shared task on dependency parsing (Zeman et al., 2018) for a compariso"
P19-1012,N18-1091,0,0.0493119,"Missing"
P19-1012,E06-1011,0,0.201001,"scoring tree we apply Eisner (1996)’s algorithm. We denote this architecture G B M IN. We note in passing that, although this decoding algorithm is restricted to projective trees, it has the advantage that it can be extended to incorporate non-local features while still maintaining exact search in polynomial time.4 The above-mentioned simple architecture uses   a feature set of two vectors { h , d }. We extend it and add information about structural context. Specifically, we incorporate information  about siblings s (blue arrows in the figure). The model follows the second-order model from McDonald and Pereira (2006) and decomposes the score of the tree into the sum of adjacent edge pair scores. We use the implementation of the secondorder decoder from Zhang and Zhao (2015). We denote this architecture G B S IBL. We use automatically predicted universal POS tags in all the experiments. The tags are assigned using a CRF tagger (Mueller et al., 2013). We annotate the training sets via 5-fold jackknifing. Evaluation. We evaluate the experiments using Labeled Attachment Score (LAS).6 We train models for 30 epochs and select the best model based on development LAS. We follow recommendations from Reimers and Gu"
P19-1012,D13-1032,0,0.0755647,"Missing"
P19-1012,D16-1159,0,0.0252782,"text with recursive networks as in Dyer et al. (2015) is redundant for the K&G transition-based architecture. The authors analyze components of the BiLSTMs to show which of them (forward v. backward LSTM) is responsible for capturing subtree information. RNNs and syntax. Recurrent neural networks, which BiLSTMs are a variant of, have been repeatedly analyzed to understand whether they can learn syntactic relations. Such analyses differ in terms of: (1) methodology they employ to probe what type of knowledge the representations learned and (2) tasks on which the representations are trained on. Shi et al. (2016) demonstrated that sequence-to-sequence machinetranslation systems capture source-language syntactic relations. Linzen et al. (2016) showed that when trained on the task of number agreement prediction the representations capture a nontrivial amount of grammatical structure (although recursive neural networks are better at this task than sequential LSTMs (Kuncoro et al., 2018)). Blevins et al. (2018) found that RNN representations trained on a variety of NLP tasks (including dependency parsing) are able to induce syntactic features (e.g., constituency labels of parent or grandparent) even witho"
P19-1012,D18-1291,0,0.267079,"Missing"
P19-1012,W04-0308,0,0.166666,"ate and the parsing history. Figure 1a illustrates the architecture of the transition-based K&G parser. For every configuration c consisting of a stack, buffer, and a set of arcs introduced so far, the parser selects a few core items from the stack and buffer (red arrows in the figure) as features. Next, it concatenates their BiLSTM vectors and passes them to a multi-layer perceptron (MLP) which assigns scores to all possible transitions. The highest scoring transition is used to proceed to the next configuration. Our implementation (denoted T B PARS) uses the arc-standard decoding algorithm (Nivre, 2004) extended with a SWAP transition (AS WAP, Nivre (2009)) to handle non-projective trees. The system applies arc transitions between the two topmost items of the stack (denoted s0 and s1 ). We use the lazy SWAP oracle by Nivre et al. (2009) for training. Labels are predicted together with the transitions. We experiment with two models with different feature sets: Parsing Model Architecture Our graph- and transition-based parsers are based on the K&G architecture (see Figure 1). The architecture has subsequently been extended by, e.g., character-based embeddings (de Lhoneux et al., 2017) or atten"
P19-1012,W18-6019,0,0.0276326,"Missing"
P19-1012,P09-1040,0,0.10098,"architecture of the transition-based K&G parser. For every configuration c consisting of a stack, buffer, and a set of arcs introduced so far, the parser selects a few core items from the stack and buffer (red arrows in the figure) as features. Next, it concatenates their BiLSTM vectors and passes them to a multi-layer perceptron (MLP) which assigns scores to all possible transitions. The highest scoring transition is used to proceed to the next configuration. Our implementation (denoted T B PARS) uses the arc-standard decoding algorithm (Nivre, 2004) extended with a SWAP transition (AS WAP, Nivre (2009)) to handle non-projective trees. The system applies arc transitions between the two topmost items of the stack (denoted s0 and s1 ). We use the lazy SWAP oracle by Nivre et al. (2009) for training. Labels are predicted together with the transitions. We experiment with two models with different feature sets: Parsing Model Architecture Our graph- and transition-based parsers are based on the K&G architecture (see Figure 1). The architecture has subsequently been extended by, e.g., character-based embeddings (de Lhoneux et al., 2017) or attention (Dozat and Manning, 2016). To keep the experiment"
P19-1012,P16-1218,0,0.183508,"antage drops down to insignificant 0.07 LAS when the BiLSTMs are incorporated. advantage over the parsers which do not, regardless of the feature model used. Graph-based parser. We train two models: G B M IN and G B S IBL with and without BiLSTMs. To ensure a fairer comparison with the models without BiLSTMs we expand the basic feature      sets ({ h , d } and { h , d , s }) with additional surface features known from classic graph-based parsers, such as distance between head and dependent (dist), words at distance of 1 from heads and dependents (h±1 , d±1 ) and at distance ±2. We follow Wang and Chang (2016) and encode distance as randomly initialized embeddings. Figure 3b displays the accuracy of all the trained models with incremental extensions to their feature sets. First of all, we see that surface features (dist, h±1 , d±1 , h±2 , d±2 ) are beneficial for the models without BiLSTM representations (light bars). The improvements are visible for both parsers, with the smallest gains after adding h±2 , d±2 vectors: on average 0.35 LAS for G B S IBL and 0.83 LAS for G B M IN. As expected, adding BiLSTMs changes the picture. Since the representations capture surface context, they already contain"
P19-1012,W09-3811,0,0.0720824,"ms from the stack and buffer (red arrows in the figure) as features. Next, it concatenates their BiLSTM vectors and passes them to a multi-layer perceptron (MLP) which assigns scores to all possible transitions. The highest scoring transition is used to proceed to the next configuration. Our implementation (denoted T B PARS) uses the arc-standard decoding algorithm (Nivre, 2004) extended with a SWAP transition (AS WAP, Nivre (2009)) to handle non-projective trees. The system applies arc transitions between the two topmost items of the stack (denoted s0 and s1 ). We use the lazy SWAP oracle by Nivre et al. (2009) for training. Labels are predicted together with the transitions. We experiment with two models with different feature sets: Parsing Model Architecture Our graph- and transition-based parsers are based on the K&G architecture (see Figure 1). The architecture has subsequently been extended by, e.g., character-based embeddings (de Lhoneux et al., 2017) or attention (Dozat and Manning, 2016). To keep the experimental setup clean and simple while focusing on the information flow in the architecture, we abstain from these extensions. We use the basic K&G architecture as our starting point with a f"
P19-1012,K18-2001,0,0.069517,"Missing"
P19-1012,P11-2033,0,0.0508394,"partial subtrees become redundant when the BiLSTMs are used. We provide a deep insight into information flow in transition- and graph-based neural architectures to demonstrate where the implicit information comes from when the parsers make their decisions. Finally, with model ablations we demonstrate that the structural context is not only present in the models, but it significantly influences their performance. 1 Introduction When designing a conventional non-neural parser substantial effort is required to design a powerful feature extraction function. Such a function (McDonald et al., 2005; Zhang and Nivre, 2011, among others) is constructed so that it captures as much structural context as possible. The context allows the parser to make well-informed decisions.1 It is encoded in features built from partial subtrees and explicitly used by the models. Recently, Kiperwasser and Goldberg (2016, K&G) showed that the conventional feature extraction functions can be replaced by modeling the left- and right-context of each word with BiLSTMs (Hochreiter and Schmidhuber, 1997; 2 See results from the recent CoNLL 2018 shared task on dependency parsing (Zeman et al., 2018) for a comparison of various high-perfo"
P19-1012,P08-1108,0,0.086052,"feature set. And the graph-based parser makes use of far away surface tokens but also structurally related words. Evidently, the employment of BiLSTM feature extractors blurs the difference between the two architectures. The one clear advantage of the graphbased parser is that it performs global inference (but exact search algorithms are already being applied to projective (Shi et al., 2017) and nonprojective (G´omez-Rodr´ıguez et al., 2018) transition systems). Therefore, an interesting question is if integrating those two architectures can still be beneficial for the parsing accuracy as in Nivre and McDonald (2008). We leave this question for future work. tive than ours – composition. They showed that composing the structural context with recursive networks as in Dyer et al. (2015) is redundant for the K&G transition-based architecture. The authors analyze components of the BiLSTMs to show which of them (forward v. backward LSTM) is responsible for capturing subtree information. RNNs and syntax. Recurrent neural networks, which BiLSTMs are a variant of, have been repeatedly analyzed to understand whether they can learn syntactic relations. Such analyses differ in terms of: (1) methodology they employ to"
P19-1012,Y15-1014,0,0.019105,"ective trees, it has the advantage that it can be extended to incorporate non-local features while still maintaining exact search in polynomial time.4 The above-mentioned simple architecture uses   a feature set of two vectors { h , d }. We extend it and add information about structural context. Specifically, we incorporate information  about siblings s (blue arrows in the figure). The model follows the second-order model from McDonald and Pereira (2006) and decomposes the score of the tree into the sum of adjacent edge pair scores. We use the implementation of the secondorder decoder from Zhang and Zhao (2015). We denote this architecture G B S IBL. We use automatically predicted universal POS tags in all the experiments. The tags are assigned using a CRF tagger (Mueller et al., 2013). We annotate the training sets via 5-fold jackknifing. Evaluation. We evaluate the experiments using Labeled Attachment Score (LAS).6 We train models for 30 epochs and select the best model based on development LAS. We follow recommendations from Reimers and Gurevych (2018) and report averages and standard deviations from six models trained with different random seeds. We test for significance using the Wilcoxon rank-"
P19-1012,Q14-1004,0,0.0196662,"Recently, Kiperwasser and Goldberg (2016, K&G) showed that the conventional feature extraction functions can be replaced by modeling the left- and right-context of each word with BiLSTMs (Hochreiter and Schmidhuber, 1997; 2 See results from the recent CoNLL 2018 shared task on dependency parsing (Zeman et al., 2018) for a comparison of various high-performing dependency parsers. 3 To the best of our knowledge, this is the first BiLSTMbased second-order dependency parser. G´omez-Rodr´ıguez et al. (2018) incorporate BiLSTM-based representations into the third-order 1-Endpoint-Crossing parser of Pitler (2014). 1 See Figure 1 for the concept of structural context, details of the architectures will be described in Section 2. 117 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 117–128 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics arc score scores: LAlbl RAlbl SHIFT [3] MLP [2] BiLSTM  x1  x2 ...  xi ...  xj xn   x1  x2 ...  xi ...  xj xn [1] word repr. x1 x2 ... xi ... xj xn x1 x2 ... xi ... xj xn structural context s1 s0R b0 head (h) s0L s0 sibling (s) (a) Transition-based parser; scoring transition"
P19-1012,D07-1013,0,\N,Missing
P19-1273,D17-1181,0,0.0280569,"Missing"
P19-1273,E14-1005,0,0.0138038,"e. However new actors can appear and take on importance at any time. Discourse context. Tasks 3 and 4 regularly involves coreference resolution: in the example, the expression the amendment can only be mapped to the correct claim if its content can be inferred. Similarly, actors realized as pronouns have to be resolved. Coreference resolution is still a difficult problem (Martschat and Strube, 2014). Dependencies among tasks. The various tasks are clearly not independent of one another, and joint models have been developed for a subset of the tasks, such as coreference and relation detection (Almeida et al., 2014) or entity and relation classification (Miwa and Sasaki, 2014; Adel and Sch¨utze, 2017; Bekoulis et al., 2018). However, state-of-the-art models still struggle with sentence complexity, and there are no comprehensive models of the complete task including aggregation. 2842 C1: C2: C3: C4: C5: C6: C7: C8: Steuerung von Migration (Controlling Migration) Aufenthalt (Residency) Integration (Integration) Innere Sicherheit (Domestic Security) Aussenpolitik (Foreign Policy) ¨ Okonomie, Arbeitsmarkt (Economy, Labor Market) Gesellschaft (Society) Verfahren (Procedures) Table 1: Migration: Main categorie"
P19-1273,W12-4102,0,0.0296046,"n explored in depth. Also related is the growing field of argumentation analysis/mining (e.g. Peldszus and Stede 2013; Swanson et al. 2015; Stab and Gurevych 2017). However, a core interest there is analyzing the argument structure of longer pieces of argumentative text (i.e., claims and their (recursive) justifications), whereas we focus on the core claims that actors put forward in news coverage. The aspect of dynamics in interaction among actors is shared with work on the extraction of actor/character networks from texts, which has been applied mostly to literary texts (Elson et al., 2010; Hassan et al., 2012; Iyyer et al., 2016). 4 Computational Construction of Discourse Networks Seen as an end-to-end task, the computational construction of affiliation networks from newspaper articles as introduced in Section 2 represents a task that combines binary relation extraction (Doddington et al., 2004; Hendrickx et al., 2010) with ontologization (Pennacchiotti and Pantel, 2006; Hachey et al., 2013, i.a.). The task can be decomposed conceptually as shown in Figure 2. From bottom to top, the first task is to identify claims and actors in the text (Tasks 1 and 2). Then, they need to be mapped onto entities"
P19-1273,P19-3018,1,0.595542,"Missing"
P19-1273,Q17-1010,0,0.0163882,"Missing"
P19-1273,doddington-etal-2004-automatic,0,0.0207698,"d their (recursive) justifications), whereas we focus on the core claims that actors put forward in news coverage. The aspect of dynamics in interaction among actors is shared with work on the extraction of actor/character networks from texts, which has been applied mostly to literary texts (Elson et al., 2010; Hassan et al., 2012; Iyyer et al., 2016). 4 Computational Construction of Discourse Networks Seen as an end-to-end task, the computational construction of affiliation networks from newspaper articles as introduced in Section 2 represents a task that combines binary relation extraction (Doddington et al., 2004; Hendrickx et al., 2010) with ontologization (Pennacchiotti and Pantel, 2006; Hachey et al., 2013, i.a.). The task can be decomposed conceptually as shown in Figure 2. From bottom to top, the first task is to identify claims and actors in the text (Tasks 1 and 2). Then, they need to be mapped onto entities that are represented in the affiliation Task 3: actor mapping support Category A13: delay Brexit Task 4: claim mapping Labour has said it will support the amendment Task 2: actor detection Task 1: claim detection Figure 2: Construction of affiliation network construction (top) from text (bo"
P19-1273,P10-1015,0,0.0392302,"tion 4) have not been explored in depth. Also related is the growing field of argumentation analysis/mining (e.g. Peldszus and Stede 2013; Swanson et al. 2015; Stab and Gurevych 2017). However, a core interest there is analyzing the argument structure of longer pieces of argumentative text (i.e., claims and their (recursive) justifications), whereas we focus on the core claims that actors put forward in news coverage. The aspect of dynamics in interaction among actors is shared with work on the extraction of actor/character networks from texts, which has been applied mostly to literary texts (Elson et al., 2010; Hassan et al., 2012; Iyyer et al., 2016). 4 Computational Construction of Discourse Networks Seen as an end-to-end task, the computational construction of affiliation networks from newspaper articles as introduced in Section 2 represents a task that combines binary relation extraction (Doddington et al., 2004; Hendrickx et al., 2010) with ontologization (Pennacchiotti and Pantel, 2006; Hachey et al., 2013, i.a.). The task can be decomposed conceptually as shown in Figure 2. From bottom to top, the first task is to identify claims and actors in the text (Tasks 1 and 2). Then, they need to be"
P19-1273,D18-1393,0,0.0139055,"Social Media Analysis using NLP – in particular sentiment analysis (e.g. Ceron et al. 2014), but also going into fine-grained analysis of groups of users/actors (Cesare et al., 2017). Nevertheless, most analyses in social media concern typically relatively broad categories, such as party preferences (see Hong et al. 2016 for a comparison of social media and news texts). NLP techniques are also used for stance classification (e.g. Vilares and He 2017) and measuring ideology in speeches (Sim et al., 2013), and there is a fair amount of work on agenda-setting and framing (e.g. Tsur et al. 2015; Field et al. 2018). To our knowledge, finegrained distinctions both for actors and claims that are necessary for discourse network consideration (cf. Section 4) have not been explored in depth. Also related is the growing field of argumentation analysis/mining (e.g. Peldszus and Stede 2013; Swanson et al. 2015; Stab and Gurevych 2017). However, a core interest there is analyzing the argument structure of longer pieces of argumentative text (i.e., claims and their (recursive) justifications), whereas we focus on the core claims that actors put forward in news coverage. The aspect of dynamics in interaction among"
P19-1273,N16-1180,0,0.022164,"Missing"
P19-1273,D14-1221,0,0.0164094,"n topic (Koopmans and Statham, 2010). We thus build on an expert-defined ontology of claims (cf. Section 5). With regard to actors, the issue is less clear: knowledge bases such as Wikidata cover many persons in the public eye. However new actors can appear and take on importance at any time. Discourse context. Tasks 3 and 4 regularly involves coreference resolution: in the example, the expression the amendment can only be mapped to the correct claim if its content can be inferred. Similarly, actors realized as pronouns have to be resolved. Coreference resolution is still a difficult problem (Martschat and Strube, 2014). Dependencies among tasks. The various tasks are clearly not independent of one another, and joint models have been developed for a subset of the tasks, such as coreference and relation detection (Almeida et al., 2014) or entity and relation classification (Miwa and Sasaki, 2014; Adel and Sch¨utze, 2017; Bekoulis et al., 2018). However, state-of-the-art models still struggle with sentence complexity, and there are no comprehensive models of the complete task including aggregation. 2842 C1: C2: C3: C4: C5: C6: C7: C8: Steuerung von Migration (Controlling Migration) Aufenthalt (Residency) Integ"
P19-1273,mcnamee-etal-2010-evaluation,0,0.0155496,"k 3: actor mapping support Category A13: delay Brexit Task 4: claim mapping Labour has said it will support the amendment Task 2: actor detection Task 1: claim detection Figure 2: Construction of affiliation network construction (top) from text (bottom) as relation extraction graph, that is, discourse referents for actors (Task 3: entity linking) and categories for claims (Task 4). Next, claims need to be attributed to actors and classified as support or opposition (Task 5). Finally, relations need to be aggregated across documents (Task 6). This setup is related to Knowledge Base Population (McNamee et al., 2010) and presents itself as a series of rather challenging tasks: Actor and claim ontologies. The actors and claims can either be known a priori (then Tasks 3 and 4 amount to classification) or can emerge from the data (then they become clustering tasks). We assume that there is a limited set of claims that structures public debates on a given topic (Koopmans and Statham, 2010). We thus build on an expert-defined ontology of claims (cf. Section 5). With regard to actors, the issue is less clear: knowledge bases such as Wikidata cover many persons in the public eye. However new actors can appear an"
P19-1273,W15-4631,0,0.0969719,"eferences (see Hong et al. 2016 for a comparison of social media and news texts). NLP techniques are also used for stance classification (e.g. Vilares and He 2017) and measuring ideology in speeches (Sim et al., 2013), and there is a fair amount of work on agenda-setting and framing (e.g. Tsur et al. 2015; Field et al. 2018). To our knowledge, finegrained distinctions both for actors and claims that are necessary for discourse network consideration (cf. Section 4) have not been explored in depth. Also related is the growing field of argumentation analysis/mining (e.g. Peldszus and Stede 2013; Swanson et al. 2015; Stab and Gurevych 2017). However, a core interest there is analyzing the argument structure of longer pieces of argumentative text (i.e., claims and their (recursive) justifications), whereas we focus on the core claims that actors put forward in news coverage. The aspect of dynamics in interaction among actors is shared with work on the extraction of actor/character networks from texts, which has been applied mostly to literary texts (Elson et al., 2010; Hassan et al., 2012; Iyyer et al., 2016). 4 Computational Construction of Discourse Networks Seen as an end-to-end task, the computational"
P19-1273,P15-1157,0,0.0223801,"nsiderable work in Social Media Analysis using NLP – in particular sentiment analysis (e.g. Ceron et al. 2014), but also going into fine-grained analysis of groups of users/actors (Cesare et al., 2017). Nevertheless, most analyses in social media concern typically relatively broad categories, such as party preferences (see Hong et al. 2016 for a comparison of social media and news texts). NLP techniques are also used for stance classification (e.g. Vilares and He 2017) and measuring ideology in speeches (Sim et al., 2013), and there is a fair amount of work on agenda-setting and framing (e.g. Tsur et al. 2015; Field et al. 2018). To our knowledge, finegrained distinctions both for actors and claims that are necessary for discourse network consideration (cf. Section 4) have not been explored in depth. Also related is the growing field of argumentation analysis/mining (e.g. Peldszus and Stede 2013; Swanson et al. 2015; Stab and Gurevych 2017). However, a core interest there is analyzing the argument structure of longer pieces of argumentative text (i.e., claims and their (recursive) justifications), whereas we focus on the core claims that actors put forward in news coverage. The aspect of dynamics"
P19-1273,D17-1165,0,0.157902,"ask 5: claim attribution NLP and Political Science Our analytical goals have connecting points with a range of activities in NLP. There has been considerable work in Social Media Analysis using NLP – in particular sentiment analysis (e.g. Ceron et al. 2014), but also going into fine-grained analysis of groups of users/actors (Cesare et al., 2017). Nevertheless, most analyses in social media concern typically relatively broad categories, such as party preferences (see Hong et al. 2016 for a comparison of social media and news texts). NLP techniques are also used for stance classification (e.g. Vilares and He 2017) and measuring ideology in speeches (Sim et al., 2013), and there is a fair amount of work on agenda-setting and framing (e.g. Tsur et al. 2015; Field et al. 2018). To our knowledge, finegrained distinctions both for actors and claims that are necessary for discourse network consideration (cf. Section 4) have not been explored in depth. Also related is the growing field of argumentation analysis/mining (e.g. Peldszus and Stede 2013; Swanson et al. 2015; Stab and Gurevych 2017). However, a core interest there is analyzing the argument structure of longer pieces of argumentative text (i.e., clai"
P19-1273,D14-1200,0,0.0129076,"y time. Discourse context. Tasks 3 and 4 regularly involves coreference resolution: in the example, the expression the amendment can only be mapped to the correct claim if its content can be inferred. Similarly, actors realized as pronouns have to be resolved. Coreference resolution is still a difficult problem (Martschat and Strube, 2014). Dependencies among tasks. The various tasks are clearly not independent of one another, and joint models have been developed for a subset of the tasks, such as coreference and relation detection (Almeida et al., 2014) or entity and relation classification (Miwa and Sasaki, 2014; Adel and Sch¨utze, 2017; Bekoulis et al., 2018). However, state-of-the-art models still struggle with sentence complexity, and there are no comprehensive models of the complete task including aggregation. 2842 C1: C2: C3: C4: C5: C6: C7: C8: Steuerung von Migration (Controlling Migration) Aufenthalt (Residency) Integration (Integration) Innere Sicherheit (Domestic Security) Aussenpolitik (Foreign Policy) ¨ Okonomie, Arbeitsmarkt (Economy, Labor Market) Gesellschaft (Society) Verfahren (Procedures) Table 1: Migration: Main categories in claim ontology 5 Claim Ontology and Corpus Annotation We"
P19-1273,P06-1100,0,0.0116172,"ms that actors put forward in news coverage. The aspect of dynamics in interaction among actors is shared with work on the extraction of actor/character networks from texts, which has been applied mostly to literary texts (Elson et al., 2010; Hassan et al., 2012; Iyyer et al., 2016). 4 Computational Construction of Discourse Networks Seen as an end-to-end task, the computational construction of affiliation networks from newspaper articles as introduced in Section 2 represents a task that combines binary relation extraction (Doddington et al., 2004; Hendrickx et al., 2010) with ontologization (Pennacchiotti and Pantel, 2006; Hachey et al., 2013, i.a.). The task can be decomposed conceptually as shown in Figure 2. From bottom to top, the first task is to identify claims and actors in the text (Tasks 1 and 2). Then, they need to be mapped onto entities that are represented in the affiliation Task 3: actor mapping support Category A13: delay Brexit Task 4: claim mapping Labour has said it will support the amendment Task 2: actor detection Task 1: claim detection Figure 2: Construction of affiliation network construction (top) from text (bottom) as relation extraction graph, that is, discourse referents for actors ("
P19-1273,D13-1010,0,0.0272451,"alytical goals have connecting points with a range of activities in NLP. There has been considerable work in Social Media Analysis using NLP – in particular sentiment analysis (e.g. Ceron et al. 2014), but also going into fine-grained analysis of groups of users/actors (Cesare et al., 2017). Nevertheless, most analyses in social media concern typically relatively broad categories, such as party preferences (see Hong et al. 2016 for a comparison of social media and news texts). NLP techniques are also used for stance classification (e.g. Vilares and He 2017) and measuring ideology in speeches (Sim et al., 2013), and there is a fair amount of work on agenda-setting and framing (e.g. Tsur et al. 2015; Field et al. 2018). To our knowledge, finegrained distinctions both for actors and claims that are necessary for discourse network consideration (cf. Section 4) have not been explored in depth. Also related is the growing field of argumentation analysis/mining (e.g. Peldszus and Stede 2013; Swanson et al. 2015; Stab and Gurevych 2017). However, a core interest there is analyzing the argument structure of longer pieces of argumentative text (i.e., claims and their (recursive) justifications), whereas we f"
P19-1273,J17-3005,0,0.0562023,"t al. 2016 for a comparison of social media and news texts). NLP techniques are also used for stance classification (e.g. Vilares and He 2017) and measuring ideology in speeches (Sim et al., 2013), and there is a fair amount of work on agenda-setting and framing (e.g. Tsur et al. 2015; Field et al. 2018). To our knowledge, finegrained distinctions both for actors and claims that are necessary for discourse network consideration (cf. Section 4) have not been explored in depth. Also related is the growing field of argumentation analysis/mining (e.g. Peldszus and Stede 2013; Swanson et al. 2015; Stab and Gurevych 2017). However, a core interest there is analyzing the argument structure of longer pieces of argumentative text (i.e., claims and their (recursive) justifications), whereas we focus on the core claims that actors put forward in news coverage. The aspect of dynamics in interaction among actors is shared with work on the extraction of actor/character networks from texts, which has been applied mostly to literary texts (Elson et al., 2010; Hassan et al., 2012; Iyyer et al., 2016). 4 Computational Construction of Discourse Networks Seen as an end-to-end task, the computational construction of affiliat"
P19-3018,W17-2208,1,0.833045,"as well as to related annotation tasks (see Section 5). The four dashed boxes with labels in italics show the major tasks involved, each of which comes with a number of desiderata. Document Selection. We assume that annotation is performed on the basis of a potentially large overall corpus where full annotation of all documents is not feasible or desired. Thus, the first task is the selection of relevant documents for annotation. This is essentially an information retrieval task, where keyword-based approaches face the typical prob1 Specifically, e-Identity (Blessing et al., 2015) and CRETA (Blessing et al., 2017). lem of resulting in either high recall–low precision scenarios (too few keywords) or low recall–high precision scenarios (too many keywords). Annotation. Since projects typically involve several annotators, the environment should not just support annotation proper, but also user administration (user management, task assignment). Assuming that we annotate relations between actors and their claims, the annotation links markables to external knowledge bases, specifically the actors to a database of persons and other relevant entities (parties, companies, geopolitical entities etc.) and the clai"
P19-3018,P19-1273,1,0.595542,"Missing"
palmer-etal-2004-utilization,J97-1005,0,\N,Missing
palmer-etal-2004-utilization,C92-2117,0,\N,Missing
palmer-etal-2004-utilization,P97-1020,0,\N,Missing
palmer-etal-2004-utilization,P02-1035,0,\N,Missing
Q16-1012,P13-2009,0,0.0142836,"ce dataset is introduced. On this new dataset our method strongly outperforms several strong baselines. Separately, we obtain state-of-the-art results on the original Sportscaster semantic parsing task. 1 Introduction Semantic Parsing is the task of automatically translating natural language text to formal meaning representations (e.g., statements in a formal logic). Recent work has centered around learning such translations using parallel data, or raw collections of text-meaning pairs, often by employing methods from statistical machine translation (Wong and Mooney, 2006; Jones et al., 2012; Andreas et al., 2013) and parsing (Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010). Earlier attempts focused on learning to map natural language questions to simple database queries for database retrieval using collections of target questions and formal queries. A more recent focus has been on learning representations using weaker forms of supervision that require minimal amounts of manual annotation effort (Clarke et al., 2010; Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Artzi and Zettlemoyer, 2013; Berant et al., 2013; Kushman et al., 2014). For example, Liang et al. (2011) train a semantic pa"
Q16-1012,D14-1059,0,0.0240642,"tions. For example, knowing how the meaning, or denotation, of the symbol purple7 in general relates to the meaning of purple team, or how score relates to kick. In this section, we describe our general framework used for modeling textual entailment. Natural Logic Calculus We use a fragment of the natural logic calculus to model entailment (MacCartney and Manning, 2009; Icard III, 2012). Natural logic derives from work in linguistics on proof-theoretic approaches to semantics (van Benthem, 2008; Moss, 2010). More recently, it has been used in NLP for work on RTE (MacCartney and Manning, 2008; Angeli and Manning, 2014; Bowman et al., 2014). 160 Components of the calculus are shown in Figure 5. A small set of primitive set-theoretic relations are defined, which are used to relate the denotations of arbitrary lexical items (w.r.t to a domain of discourse D). We use a subset of the original seven relations to relate symbols (and by extension, word/phrases) in our domain. For example, purple3 (or “purple 3”) has a v (or subset) relation to purple team (or “purple team”), which is illustrated in Figure 5 using a Venn diagram. These primitive relations are then composed using two operations: atomic join rules, o"
Q16-1012,N12-1049,0,0.122344,"where the entailments relate to default properties of particular modifiers when they are added or dropped. Lexical-based inferences relate to general inferences and implications between primitive semantic symbols or concepts: kick w score, pass v kick, and pink1 v pink team. 2.3 Outline of Approach Experiments are done by first training a standard semantic parser on the Sportscaster dataset, then improving this parser using an extended corpus of sentences annotated with entailment judgements. Semantic parsing is done using a probabilistic grammar induction approach (B¨orschinger et al., 2011; Angeli et al., 2012), which we extend to accommodate entailment modeling. The natural logic calculus is used as the underlying logical inference engine (MacCartney and Manning, 2009). To evaluate the quality of our resulting semantic parser and the acquired knowledge, we run our 158 system on a held-out set of inference pairs. The results are compared to the na¨ıve inferences computed by the initial semantic parser. 3 Semantic Parsing In this section, we describe the technical details behind the semantic parsing. We also describe the underlying natural logic inference engine used for computing inferences, and how"
Q16-1012,Q13-1005,0,0.0243034,"n by employing methods from statistical machine translation (Wong and Mooney, 2006; Jones et al., 2012; Andreas et al., 2013) and parsing (Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010). Earlier attempts focused on learning to map natural language questions to simple database queries for database retrieval using collections of target questions and formal queries. A more recent focus has been on learning representations using weaker forms of supervision that require minimal amounts of manual annotation effort (Clarke et al., 2010; Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Artzi and Zettlemoyer, 2013; Berant et al., 2013; Kushman et al., 2014). For example, Liang et al. (2011) train a semantic parser in a question-answering domain using the denotation (or answer) of each question as the sole supervision. Particularly impressive is their system’s ability to learn complex linguistic structure not handled by earlier methods that use more direct supervision. Similarly, Artzi and Zettlemoyer (2013) train a parser that generates higherorder logical representations in a navigation domain using low-level navigation cues. What is missing in such approaches, however, is an explicit account of entai"
Q16-1012,W14-2402,0,0.0258157,"aspects of natural language meaning should support inferences about sentence-level entailments (i.e., determining whether the meaning of one sentence follows from another). In many cases, the target representations being learned remain inexpressive, making it difficult to learn the types of semantic generalizations and world-knowledge needed for modeling entailment (see discussion in Schubert (2015)). Attempts to integrate more general knowledge into semantic parsing pipelines have often involved additional hand-engineering or external lexical resources (Wang et al., 2014; Tian et al., 2014; Beltagy et al., 2014). We propose a different learning-based approach that uses textual inference judgements between sentences as additional supervision to learn semantic generaliza155 Transactions of the Association for Computational Linguistics, vol. 4, pp. 155–168, 2016. Action Editor: Mark Steedman. Submission batch: 10/2015; Revision batch: 2/2016; Published 5/2016. c 2016 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. x: pink3 y: (latent) quickly kicks to Entailment pink7 1. z: { pass( pink3 pink7 ),...} 99 JzK 1 10 4 2 8 7 3 3 6 4 5 8 Hypothesis h t!h h!t Pink 3 quickly ki"
Q16-1012,D13-1160,0,0.0722098,"tatistical machine translation (Wong and Mooney, 2006; Jones et al., 2012; Andreas et al., 2013) and parsing (Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010). Earlier attempts focused on learning to map natural language questions to simple database queries for database retrieval using collections of target questions and formal queries. A more recent focus has been on learning representations using weaker forms of supervision that require minimal amounts of manual annotation effort (Clarke et al., 2010; Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Artzi and Zettlemoyer, 2013; Berant et al., 2013; Kushman et al., 2014). For example, Liang et al. (2011) train a semantic parser in a question-answering domain using the denotation (or answer) of each question as the sole supervision. Particularly impressive is their system’s ability to learn complex linguistic structure not handled by earlier methods that use more direct supervision. Similarly, Artzi and Zettlemoyer (2013) train a parser that generates higherorder logical representations in a navigation domain using low-level navigation cues. What is missing in such approaches, however, is an explicit account of entailment (e.g., learning"
Q16-1012,D11-1131,0,0.160583,"Missing"
Q16-1012,W10-2903,0,0.028184,"lations using parallel data, or raw collections of text-meaning pairs, often by employing methods from statistical machine translation (Wong and Mooney, 2006; Jones et al., 2012; Andreas et al., 2013) and parsing (Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010). Earlier attempts focused on learning to map natural language questions to simple database queries for database retrieval using collections of target questions and formal queries. A more recent focus has been on learning representations using weaker forms of supervision that require minimal amounts of manual annotation effort (Clarke et al., 2010; Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Artzi and Zettlemoyer, 2013; Berant et al., 2013; Kushman et al., 2014). For example, Liang et al. (2011) train a semantic parser in a question-answering domain using the denotation (or answer) of each question as the sole supervision. Particularly impressive is their system’s ability to learn complex linguistic structure not handled by earlier methods that use more direct supervision. Similarly, Artzi and Zettlemoyer (2013) train a parser that generates higherorder logical representations in a navigation domain using low-level navigation"
Q16-1012,W05-1506,0,0.0255108,"tion instructions used in the RTE experiments of Snow et al. (2008) 163 Table 1: Results on the semantic parsing (top) and inference (bottom) cross validation experiments. ence grammars. Like in the semantic parsing task, we perform cross-validation on the games using both the original data and sentence pairs to jointly train our models, and evaluate on left-out sets of inference pairs. Each proof generated in the evaluation phrase is considered correct if the resulting inference label matches a gold inference. We implemented the learning algorithm in Section 3.3 using the k-best algorithm by Huang and Chiang (2005), with a beam size of 1,000. The base semantic grammars were each trained for 3 iterations and re-trained using the additional inference grammar rules for 10 iterations. Two Dirichlet priors were used, ↵1 = 0.05 (for lexical rules) and ↵2 = 0.3 (for non-lexical rules) throughout. Lexical rule probabilities were initialized using co-occurrence statistics estimated using an IBM Model1 word aligner (uniform initialization otherwise). 5 additional senses were added to the inference grammar for the most frequent events. 4.3 Results The results of both tasks are shown in Table 1. Scores are averaged"
Q16-1012,P12-1051,0,0.0161879,"RTE-inspired inference dataset is introduced. On this new dataset our method strongly outperforms several strong baselines. Separately, we obtain state-of-the-art results on the original Sportscaster semantic parsing task. 1 Introduction Semantic Parsing is the task of automatically translating natural language text to formal meaning representations (e.g., statements in a formal logic). Recent work has centered around learning such translations using parallel data, or raw collections of text-meaning pairs, often by employing methods from statistical machine translation (Wong and Mooney, 2006; Jones et al., 2012; Andreas et al., 2013) and parsing (Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010). Earlier attempts focused on learning to map natural language questions to simple database queries for database retrieval using collections of target questions and formal queries. A more recent focus has been on learning representations using weaker forms of supervision that require minimal amounts of manual annotation effort (Clarke et al., 2010; Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Artzi and Zettlemoyer, 2013; Berant et al., 2013; Kushman et al., 2014). For example, Liang et al. (20"
Q16-1012,D12-1069,0,0.0470647,"Missing"
Q16-1012,P14-1026,0,0.0701602,"anslation (Wong and Mooney, 2006; Jones et al., 2012; Andreas et al., 2013) and parsing (Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010). Earlier attempts focused on learning to map natural language questions to simple database queries for database retrieval using collections of target questions and formal queries. A more recent focus has been on learning representations using weaker forms of supervision that require minimal amounts of manual annotation effort (Clarke et al., 2010; Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Artzi and Zettlemoyer, 2013; Berant et al., 2013; Kushman et al., 2014). For example, Liang et al. (2011) train a semantic parser in a question-answering domain using the denotation (or answer) of each question as the sole supervision. Particularly impressive is their system’s ability to learn complex linguistic structure not handled by earlier methods that use more direct supervision. Similarly, Artzi and Zettlemoyer (2013) train a parser that generates higherorder logical representations in a navigation domain using low-level navigation cues. What is missing in such approaches, however, is an explicit account of entailment (e.g., learning entailment rules from"
Q16-1012,D10-1119,0,0.0992807,"outperforms several strong baselines. Separately, we obtain state-of-the-art results on the original Sportscaster semantic parsing task. 1 Introduction Semantic Parsing is the task of automatically translating natural language text to formal meaning representations (e.g., statements in a formal logic). Recent work has centered around learning such translations using parallel data, or raw collections of text-meaning pairs, often by employing methods from statistical machine translation (Wong and Mooney, 2006; Jones et al., 2012; Andreas et al., 2013) and parsing (Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010). Earlier attempts focused on learning to map natural language questions to simple database queries for database retrieval using collections of target questions and formal queries. A more recent focus has been on learning representations using weaker forms of supervision that require minimal amounts of manual annotation effort (Clarke et al., 2010; Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Artzi and Zettlemoyer, 2013; Berant et al., 2013; Kushman et al., 2014). For example, Liang et al. (2011) train a semantic parser in a question-answering domain using the denotation (or answer) o"
Q16-1012,P11-1060,0,0.224271,"Missing"
Q16-1012,P11-2057,0,0.01944,"c utterances describing the same situation. Our training setup tries to imitate these types of cases, where being able to reason and learn about entailment directly is essential. Since capturing inference is our main goal, we also propose using textual entailment as an evaluation metric for semantic parsing. As reflected in the results, our inference task (i.e., Task 2) is considerably harder than the original semantic parsing evaluation (i.e., Task 1). This is not surprising, given that entailment recognition is in general known to involve considerable amounts of lexical and world knowledge (LoBue and Yates, 2011). Since the difference in performance on the original task is minimal between our base grammars and the inference grammars, one might conclude that the original evaluation does not tell us very much about the quality of the semantic grammar being learned in the same way as our new inference evaluation. We hope that our work pushes others in the direction of using entailment not only as a tool for learning, but for evaluating and comparing the quality of semantic parsers. While our current model only handles simple types of inferences relating to inclusion/exclusion, we believe that our overall"
Q16-1012,C08-1066,0,0.0629136,"Missing"
Q16-1012,W09-3714,0,0.454872,"erences and implications between primitive semantic symbols or concepts: kick w score, pass v kick, and pink1 v pink team. 2.3 Outline of Approach Experiments are done by first training a standard semantic parser on the Sportscaster dataset, then improving this parser using an extended corpus of sentences annotated with entailment judgements. Semantic parsing is done using a probabilistic grammar induction approach (B¨orschinger et al., 2011; Angeli et al., 2012), which we extend to accommodate entailment modeling. The natural logic calculus is used as the underlying logical inference engine (MacCartney and Manning, 2009). To evaluate the quality of our resulting semantic parser and the acquired knowledge, we run our 158 system on a held-out set of inference pairs. The results are compared to the na¨ıve inferences computed by the initial semantic parser. 3 Semantic Parsing In this section, we describe the technical details behind the semantic parsing. We also describe the underlying natural logic inference engine used for computing inferences, and how to integrate this into a standard semantic parsing pipeline for modeling our extended corpus. 3.1 Base Semantic Grammars Semantic grammars (Allen, 1987) are used"
Q16-1012,D08-1027,0,0.0672298,"Missing"
Q16-1012,P14-1008,0,0.0216601,"t captures the core aspects of natural language meaning should support inferences about sentence-level entailments (i.e., determining whether the meaning of one sentence follows from another). In many cases, the target representations being learned remain inexpressive, making it difficult to learn the types of semantic generalizations and world-knowledge needed for modeling entailment (see discussion in Schubert (2015)). Attempts to integrate more general knowledge into semantic parsing pipelines have often involved additional hand-engineering or external lexical resources (Wang et al., 2014; Tian et al., 2014; Beltagy et al., 2014). We propose a different learning-based approach that uses textual inference judgements between sentences as additional supervision to learn semantic generaliza155 Transactions of the Association for Computational Linguistics, vol. 4, pp. 155–168, 2016. Action Editor: Mark Steedman. Submission batch: 10/2015; Revision batch: 2/2016; Published 5/2016. c 2016 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. x: pink3 y: (latent) quickly kicks to Entailment pink7 1. z: { pass( pink3 pink7 ),...} 99 JzK 1 10 4 2 8 7 3 3 6 4 5 8 Hypothesis h t!"
Q16-1012,D14-1135,0,0.0129196,"semantic parser that captures the core aspects of natural language meaning should support inferences about sentence-level entailments (i.e., determining whether the meaning of one sentence follows from another). In many cases, the target representations being learned remain inexpressive, making it difficult to learn the types of semantic generalizations and world-knowledge needed for modeling entailment (see discussion in Schubert (2015)). Attempts to integrate more general knowledge into semantic parsing pipelines have often involved additional hand-engineering or external lexical resources (Wang et al., 2014; Tian et al., 2014; Beltagy et al., 2014). We propose a different learning-based approach that uses textual inference judgements between sentences as additional supervision to learn semantic generaliza155 Transactions of the Association for Computational Linguistics, vol. 4, pp. 155–168, 2016. Action Editor: Mark Steedman. Submission batch: 10/2015; Revision batch: 2/2016; Published 5/2016. c 2016 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. x: pink3 y: (latent) quickly kicks to Entailment pink7 1. z: { pass( pink3 pink7 ),...} 99 JzK 1 10 4 2 8 7 3 3 6 4"
Q16-1012,N06-1056,0,0.0529367,"ey, 2008), and a novel RTE-inspired inference dataset is introduced. On this new dataset our method strongly outperforms several strong baselines. Separately, we obtain state-of-the-art results on the original Sportscaster semantic parsing task. 1 Introduction Semantic Parsing is the task of automatically translating natural language text to formal meaning representations (e.g., statements in a formal logic). Recent work has centered around learning such translations using parallel data, or raw collections of text-meaning pairs, often by employing methods from statistical machine translation (Wong and Mooney, 2006; Jones et al., 2012; Andreas et al., 2013) and parsing (Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010). Earlier attempts focused on learning to map natural language questions to simple database queries for database retrieval using collections of target questions and formal queries. A more recent focus has been on learning representations using weaker forms of supervision that require minimal amounts of manual annotation effort (Clarke et al., 2010; Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Artzi and Zettlemoyer, 2013; Berant et al., 2013; Kushman et al., 2014). For examp"
Q16-1012,P09-1110,0,0.401976,"Missing"
Q16-1012,W15-4002,0,\N,Missing
R15-1037,W04-3212,0,0.115897,"Missing"
R15-1037,P09-1004,0,0.0215537,"011) use Conditional Random Fields in relation extraction approach. We follow previous work (Hou and Li, 2008; Kessler and Kuhn, 2013) and tackle comparisons with a SRL approach, but move from a completely supervised setting to a semi-supervised one. Several unsupervised or weakly supervised approaches have been presented for SRL. Gildea and Jurafsky (2002) – the first work that tackles SRL as an independent task – use bootstrapping, where an initial system is trained on the available data, applied to a large unlabeled corpus, and the resulting annotations are then used to re-train the model. Abend et al. (2009) do unsupervised argument identification by using pointwise mutual information to determine which constituents are the most probable arguments. Other approaches use the extensive resources that exist for SRL as a basis, e.g., Swier and Stevenson (2005) leverage VerbNet which lists possible argument structures allowable for each predicate. For comparison detection we do not have extensive resources to tap into. We do however think that a small seed set of comparison sentences can be annotated in reasonable time for any new domain or language. This set may not be sufficiently large for bootstrap"
R15-1037,W09-1206,0,0.03667,"Missing"
R15-1037,E09-1026,0,0.0637933,"Missing"
R15-1037,J12-1005,0,0.047819,"Missing"
R15-1037,C08-1031,0,0.039695,"will behave similarly with respect to their arguments so that the labels from the seed sentences can be projected to the unlabeled sentences. These newly labeled sentences can then be used as additional training data. Related work Sentiment analysis has in recent years moved from the document-level prediction of polarity or star rating to a more fine-grained analysis. Jindal and Liu (2006a) are the first to specifically distinguish comparison sentences from other sentences in product reviews. In follow-up work, Jindal and Liu (2006b) detect comparison arguments with label sequential rules and Ganapathibhotla and Liu (2008) identify the preferred entity in a ranked comparison. Xu et al. (2011) use Conditional Random Fields in relation extraction approach. We follow previous work (Hou and Li, 2008; Kessler and Kuhn, 2013) and tackle comparisons with a SRL approach, but move from a completely supervised setting to a semi-supervised one. Several unsupervised or weakly supervised approaches have been presented for SRL. Gildea and Jurafsky (2002) – the first work that tackles SRL as an independent task – use bootstrapping, where an initial system is trained on the available data, applied to a large unlabeled corpus,"
R15-1037,J02-3001,0,0.00824828,"guish comparison sentences from other sentences in product reviews. In follow-up work, Jindal and Liu (2006b) detect comparison arguments with label sequential rules and Ganapathibhotla and Liu (2008) identify the preferred entity in a ranked comparison. Xu et al. (2011) use Conditional Random Fields in relation extraction approach. We follow previous work (Hou and Li, 2008; Kessler and Kuhn, 2013) and tackle comparisons with a SRL approach, but move from a completely supervised setting to a semi-supervised one. Several unsupervised or weakly supervised approaches have been presented for SRL. Gildea and Jurafsky (2002) – the first work that tackles SRL as an independent task – use bootstrapping, where an initial system is trained on the available data, applied to a large unlabeled corpus, and the resulting annotations are then used to re-train the model. Abend et al. (2009) do unsupervised argument identification by using pointwise mutual information to determine which constituents are the most probable arguments. Other approaches use the extensive resources that exist for SRL as a basis, e.g., Swier and Stevenson (2005) leverage VerbNet which lists possible argument structures allowable for each predicate."
R15-1037,D13-1194,1,0.929667,"roach is of course to invest in quality-controlled manual annotation of a relatively large amount of training data. However, since the higher-level semantic structure of comparisons as they appear in reviews is clear-cut, the problem setting could respond favorably to weakly supervised training strategies that start out from a seed set of manually annotated data. The experiments we present in this paper are exploring this very question. Comparisons can be mapped to a predicateargument structure, so we cast the task of detecting them as a semantic role-labeling (SRL) problem (Hou and Li, 2008; Kessler and Kuhn, 2013). Starting with a small set of labeled seed sentences, we use structural alignment (F¨urstenau and Lapata, 2009), which has been successfully applied to SRL, to automatically find and annotate sentences that are similar to these seed sentences as a way to get more training data. There are several challenges that make our task different from a typical SRL setting: Our data is not news, but user-generated data (product reviews), which is much more noisy. We have a There tends to be a substantial proportion of reviews that include explicit textual comparisons between the reviewed item and another"
R15-1037,kessler-kuhn-2014-corpus,1,0.838105,"y parser. This setup is equivalent to (Kessler and Kuhn, 2013). Experiments 4.1 System for comparison detection 4.3 Data Experimental setup To evaluate whether the found expansion sentences are useful, we add the k best expansion sentences per seed predicate to the seed data and train on this expanded corpus. We use the test data for evaluation and compare classification performance of training on the expanded seed data with the baseline trained on the seed data only. We test four versions of the expansion: As our core labeled data set we use comparison sentences from English camera reviews1 (Kessler and Kuhn, 2014). We divide the data into five folds and use one fold as seed data and the rest as test data. The full seed data contains 342 sentences with 415 predicates. The test data contains 1365 sentences with 1693 predicates. As the unlabeled expansion data, we use a set of 280.000 camera review sentences from epinions.com. Note that expansion sentences are never used in testing, we always only test on human-annotated data. To calculate vector space similarities we use co-occurrence vectors (symmetric window of 2 words, retain 2000 most frequent dimensions) extracted from a large set of reviews with a"
R15-1037,H05-1111,0,0.0318957,"one. Several unsupervised or weakly supervised approaches have been presented for SRL. Gildea and Jurafsky (2002) – the first work that tackles SRL as an independent task – use bootstrapping, where an initial system is trained on the available data, applied to a large unlabeled corpus, and the resulting annotations are then used to re-train the model. Abend et al. (2009) do unsupervised argument identification by using pointwise mutual information to determine which constituents are the most probable arguments. Other approaches use the extensive resources that exist for SRL as a basis, e.g., Swier and Stevenson (2005) leverage VerbNet which lists possible argument structures allowable for each predicate. For comparison detection we do not have extensive resources to tap into. We do however think that a small seed set of comparison sentences can be annotated in reasonable time for any new domain or language. This set may not be sufficiently large for bootstrapping, but it can be used as an initial seed set. In this work, we use structural alignment (F¨urstenau and 3.1 Outline of structural alignment Given a small set of labeled sentences (seed corpus) and a large set of unlabeled sentences (expansion corpus"
richardson-kuhn-2014-unixman,N03-5008,0,\N,Missing
richardson-kuhn-2014-unixman,N06-1056,0,\N,Missing
richardson-kuhn-2014-unixman,D07-1071,0,\N,Missing
richardson-kuhn-2014-unixman,P13-2009,0,\N,Missing
richardson-kuhn-2014-unixman,Q13-1005,0,\N,Missing
richardson-kuhn-2014-unixman,S13-1045,0,\N,Missing
richardson-kuhn-2014-unixman,N12-1049,0,\N,Missing
seeker-kuhn-2012-making,W04-2407,0,\N,Missing
seeker-kuhn-2012-making,daum-etal-2004-automatic,0,\N,Missing
seeker-kuhn-2012-making,C10-2129,1,\N,Missing
seeker-kuhn-2012-making,W03-3023,0,\N,Missing
seeker-kuhn-2012-making,burchardt-etal-2006-salsa,0,\N,Missing
seeker-kuhn-2012-making,W08-1008,0,\N,Missing
seeker-kuhn-2012-making,P06-1033,0,\N,Missing
seeker-kuhn-2012-making,H05-1066,0,\N,Missing
seeker-kuhn-2012-making,W09-1201,0,\N,Missing
seeker-kuhn-2012-making,W11-2912,0,\N,Missing
seeker-kuhn-2012-making,D11-1036,0,\N,Missing
seeker-kuhn-2012-making,W07-2416,0,\N,Missing
seeker-kuhn-2012-making,W10-1834,0,\N,Missing
seeker-kuhn-2012-making,vincze-etal-2010-hungarian,0,\N,Missing
seeker-kuhn-2012-making,I05-6010,0,\N,Missing
seeker-kuhn-2014-domain,daum-etal-2004-automatic,0,\N,Missing
seeker-kuhn-2014-domain,D10-1004,0,\N,Missing
seeker-kuhn-2014-domain,J93-2004,0,\N,Missing
seeker-kuhn-2014-domain,C10-1011,0,\N,Missing
seeker-kuhn-2014-domain,A00-2013,0,\N,Missing
seeker-kuhn-2014-domain,W01-0521,0,\N,Missing
seeker-kuhn-2014-domain,W08-1008,0,\N,Missing
seeker-kuhn-2014-domain,H05-1108,0,\N,Missing
seeker-kuhn-2014-domain,P06-1063,0,\N,Missing
seeker-kuhn-2014-domain,W09-1201,0,\N,Missing
seeker-kuhn-2014-domain,2005.mtsummit-papers.11,0,\N,Missing
seeker-kuhn-2014-domain,D13-1032,0,\N,Missing
seeker-kuhn-2014-domain,seeker-kuhn-2012-making,1,\N,Missing
seeker-kuhn-2014-domain,P07-1033,0,\N,Missing
seeker-kuhn-2014-domain,schmid-etal-2004-smor,0,\N,Missing
spreyer-etal-2008-identification,H01-1035,0,\N,Missing
spreyer-etal-2008-identification,W02-1503,0,\N,Missing
spreyer-etal-2008-identification,W06-2008,0,\N,Missing
spreyer-etal-2008-identification,P06-1043,0,\N,Missing
spreyer-etal-2008-identification,J03-1002,0,\N,Missing
spreyer-etal-2008-identification,P07-1092,0,\N,Missing
spreyer-etal-2008-identification,2005.mtsummit-papers.11,0,\N,Missing
spreyer-etal-2008-identification,P06-1146,0,\N,Missing
spreyer-etal-2010-training,N04-1013,0,\N,Missing
spreyer-etal-2010-training,J93-2004,0,\N,Missing
spreyer-etal-2010-training,W09-1104,1,\N,Missing
spreyer-etal-2010-training,H01-1035,0,\N,Missing
spreyer-etal-2010-training,W06-1614,0,\N,Missing
spreyer-etal-2010-training,W06-2920,0,\N,Missing
spreyer-etal-2010-training,P08-1108,0,\N,Missing
spreyer-etal-2010-training,P09-2010,1,\N,Missing
spreyer-etal-2010-training,J03-1002,0,\N,Missing
spreyer-etal-2010-training,I08-1064,1,\N,Missing
spreyer-etal-2010-training,W04-1905,0,\N,Missing
spreyer-etal-2010-training,P06-1146,0,\N,Missing
W05-0803,N04-1035,0,0.0143742,"lism goes along with the continuity assumption that every complete constituent is continuous in both languages. Various recent studies in the field of syntax-based Statistical MT have shown that such an assumption is problematic when based on typical treebank-style analyses. As (Melamed, 2003) discusses for instance, in the context of binary branching structures even simple examples like the English/French pair a gift for you from France ↔ un cadeau de France pour vouz [a gift from France for you] lead to discontinuity of a “synchronous phrase” in one of the two languages. (Gildea, 2003) and (Galley et al., 2004) discuss different ways of generalizing the tree-level crosslinguistic correspondence relation, so it is not confined to single tree nodes, thereby avoiding a continuity assumption. We believe that in order to obtain full coverage on real parallel corpora, some mechanism along these lines will be required. However, if the typical rich phrase structure analyses (with fairly detailed fine structure) are replaced by flat, multiply branching analyses, most of the highly frequent problematic cases are resolved. 7 In 6 This detail will be relevant for the parsing inference rule (5) below. 7 Compare"
W05-0803,P03-1011,0,0.0965593,"at the simple formalism goes along with the continuity assumption that every complete constituent is continuous in both languages. Various recent studies in the field of syntax-based Statistical MT have shown that such an assumption is problematic when based on typical treebank-style analyses. As (Melamed, 2003) discusses for instance, in the context of binary branching structures even simple examples like the English/French pair a gift for you from France ↔ un cadeau de France pour vouz [a gift from France for you] lead to discontinuity of a “synchronous phrase” in one of the two languages. (Gildea, 2003) and (Galley et al., 2004) discuss different ways of generalizing the tree-level crosslinguistic correspondence relation, so it is not confined to single tree nodes, thereby avoiding a continuity assumption. We believe that in order to obtain full coverage on real parallel corpora, some mechanism along these lines will be required. However, if the typical rich phrase structure analyses (with fairly detailed fine structure) are replaced by flat, multiply branching analyses, most of the highly frequent problematic cases are resolved. 7 In 6 This detail will be relevant for the parsing inference"
W05-0803,P96-1027,0,0.0360668,"e of a number of open 19 practical questions, e.g.: Will the fact that real parallel corpora often contain rather free translations undermine our idea of using the consensus structure for learning basic syntactic constraints? Statistical alignments are imperfect – can the constraints imposed by the word alignment be relaxed accordingly without sacrificing tractability and the effect of indirect supervision?8 3 Alignment-guided synchronous parsing Our dynamic programming algorithm can be described as a variant of standard Earley-style chart parsing (Earley, 1970) and generation (Shieber, 1988; Kay, 1996). The chart is a data structure which stores all sub-analyses that cover part of the input string (in parsing) or meaning representation (in generation). Memoizing such partial results has the standard advantage of dynamic programming techniques – it helps one to avoid unnecessary recomputation of partial results. The chart structure for context-free parsing is also exploited directly in dynamic programming algorithms for probabilistic context-free grammars (PCFGs): (i) the inside (or outside) algorithm for summing over the probabilities for every possible analysis of a given string, (ii) the"
W05-0803,P04-1060,1,0.786496,"ages. 2 Induction of grammars from parallel corpora is rarely viewed as a promising task in its own right; in work that has addressed the issue directly (Wu, 1997; Melamed, 2003; Melamed, 2004), the synchronous grammar is mainly viewed as instrumental in the process of improving the translation model in a noisy channel approach to statistical MT. 3 In the present paper, we provide an important prerequisite for parallel corpus-based grammar induction work: an efficient algorithm for synchronous parsing of sentence pairs, given a word alignment. This work represents a second pilot study (after (Kuhn, 2004)) for the longer-term P TOLEMAIOS project at Saarland University4 with the goal of learning linguistic grammars from parallel corpora (compare (Kuhn, 2005)). The grammars should be robust and assign a 2 In the present paper we use examples from English/German for illustration, but the approach is of course independent of the language pair under consideration. 3 Of course, there is related work (e.g., (Hwa et al., 2002; L¨u et al., 2002)) using aligned parallel corpora in order to “project” bracketings or dependency structures from English to another language and exploit them for training a par"
W05-0803,C02-1003,0,0.0480282,"Missing"
W05-0803,N03-1021,0,0.378209,"other natural languages serves as an approximation of a (much more costly) manual structural or semantic annotation – one might speak of automatic indirect supervision in learning. The technique will be most useful for low-resource languages and languages for which there is no funding for treebanking activities. The only requirement will be that a parallel corpus exist for the language under consideration and one or more other languages. 2 Induction of grammars from parallel corpora is rarely viewed as a promising task in its own right; in work that has addressed the issue directly (Wu, 1997; Melamed, 2003; Melamed, 2004), the synchronous grammar is mainly viewed as instrumental in the process of improving the translation model in a noisy channel approach to statistical MT. 3 In the present paper, we provide an important prerequisite for parallel corpus-based grammar induction work: an efficient algorithm for synchronous parsing of sentence pairs, given a word alignment. This work represents a second pilot study (after (Kuhn, 2004)) for the longer-term P TOLEMAIOS project at Saarland University4 with the goal of learning linguistic grammars from parallel corpora (compare (Kuhn, 2005)). The gram"
W05-0803,P04-1083,0,0.0215604,"anguages serves as an approximation of a (much more costly) manual structural or semantic annotation – one might speak of automatic indirect supervision in learning. The technique will be most useful for low-resource languages and languages for which there is no funding for treebanking activities. The only requirement will be that a parallel corpus exist for the language under consideration and one or more other languages. 2 Induction of grammars from parallel corpora is rarely viewed as a promising task in its own right; in work that has addressed the issue directly (Wu, 1997; Melamed, 2003; Melamed, 2004), the synchronous grammar is mainly viewed as instrumental in the process of improving the translation model in a noisy channel approach to statistical MT. 3 In the present paper, we provide an important prerequisite for parallel corpus-based grammar induction work: an efficient algorithm for synchronous parsing of sentence pairs, given a word alignment. This work represents a second pilot study (after (Kuhn, 2004)) for the longer-term P TOLEMAIOS project at Saarland University4 with the goal of learning linguistic grammars from parallel corpora (compare (Kuhn, 2005)). The grammars should be r"
W05-0803,C88-2128,0,0.428816,"ns. We are aware of a number of open 19 practical questions, e.g.: Will the fact that real parallel corpora often contain rather free translations undermine our idea of using the consensus structure for learning basic syntactic constraints? Statistical alignments are imperfect – can the constraints imposed by the word alignment be relaxed accordingly without sacrificing tractability and the effect of indirect supervision?8 3 Alignment-guided synchronous parsing Our dynamic programming algorithm can be described as a variant of standard Earley-style chart parsing (Earley, 1970) and generation (Shieber, 1988; Kay, 1996). The chart is a data structure which stores all sub-analyses that cover part of the input string (in parsing) or meaning representation (in generation). Memoizing such partial results has the standard advantage of dynamic programming techniques – it helps one to avoid unnecessary recomputation of partial results. The chart structure for context-free parsing is also exploited directly in dynamic programming algorithms for probabilistic context-free grammars (PCFGs): (i) the inside (or outside) algorithm for summing over the probabilities for every possible analysis of a given strin"
W05-0803,J97-3002,0,0.386799,"nces into other natural languages serves as an approximation of a (much more costly) manual structural or semantic annotation – one might speak of automatic indirect supervision in learning. The technique will be most useful for low-resource languages and languages for which there is no funding for treebanking activities. The only requirement will be that a parallel corpus exist for the language under consideration and one or more other languages. 2 Induction of grammars from parallel corpora is rarely viewed as a promising task in its own right; in work that has addressed the issue directly (Wu, 1997; Melamed, 2003; Melamed, 2004), the synchronous grammar is mainly viewed as instrumental in the process of improving the translation model in a noisy channel approach to statistical MT. 3 In the present paper, we provide an important prerequisite for parallel corpus-based grammar induction work: an efficient algorithm for synchronous parsing of sentence pairs, given a word alignment. This work represents a second pilot study (after (Kuhn, 2004)) for the longer-term P TOLEMAIOS project at Saarland University4 with the goal of learning linguistic grammars from parallel corpora (compare (Kuhn, 2"
W05-0803,W02-1039,0,\N,Missing
W06-2002,J04-4004,0,0.0642145,"Missing"
W06-2002,A00-2018,0,0.178751,"vely inﬂexible in the type of feature information we can include in its probabilistic model. In this work, we discuss preliminary work in developing a new probabilistic parsing model that allows us to easily incorporate many different types of features, including crosslingual information. We show how this model can be used to build a successful parser for a small handmade gold-standard corpus of 188 sentences (in 3 languages) from the Europarl corpus. 1 Introduction Much of the current research into probabilistic parsing is founded on probabilistic contextfree grammars (PCFGs) (Collins, 1999; Charniak, 2000; Charniak, 2001). For instance, consider the parse tree in Figure 1. One way to decompose this parse tree is to view it as a sequence of applications of CFG rules. For this particular tree, we could view it as the application of rule “NP → NP PP,” followed by rule “NP → DT NN,” followed by rule “DT → that,” and so forth. Hence instead of analyzing P (tree), we deal with the more modular: P(NP → NP PP, NP → DT NN, DT → that, NN → money, PP → IN NP, IN → in, NP → DT NN, DT → the, NN → market) Obviously this joint distribution is just as difﬁcult to assess and compute with as P (tree). However t"
W06-2002,P01-1017,0,0.0199675,"n the type of feature information we can include in its probabilistic model. In this work, we discuss preliminary work in developing a new probabilistic parsing model that allows us to easily incorporate many different types of features, including crosslingual information. We show how this model can be used to build a successful parser for a small handmade gold-standard corpus of 188 sentences (in 3 languages) from the Europarl corpus. 1 Introduction Much of the current research into probabilistic parsing is founded on probabilistic contextfree grammars (PCFGs) (Collins, 1999; Charniak, 2000; Charniak, 2001). For instance, consider the parse tree in Figure 1. One way to decompose this parse tree is to view it as a sequence of applications of CFG rules. For this particular tree, we could view it as the application of rule “NP → NP PP,” followed by rule “NP → DT NN,” followed by rule “DT → that,” and so forth. Hence instead of analyzing P (tree), we deal with the more modular: P(NP → NP PP, NP → DT NN, DT → that, NN → money, PP → IN NP, IN → in, NP → DT NN, DT → the, NN → market) Obviously this joint distribution is just as difﬁcult to assess and compute with as P (tree). However there exist cubic"
W06-2002,W97-0301,0,0.0363841,"probabilistic context by changing the data itself, we make our data increasingly sparse as we add features. Thus we are constrained from adding too 9 NP 1 2 3 4 5 PP NP DT NN IN that money in NP DT NN the market NP-TOP PP-NP DT-NP NN-NP IN-PP that money in 2 true true - 3 false false true - 4 false false false true - 5 true false true true true Figure 3: Span chart for example parse tree. Chart entry (i, j) = true iff span (i, j) is a constituent in the tree. Figure 1: Example parse tree. NP-NP 1 true - NP-PP DT-NP NN-NP the market of our framework is the maximum-entropy parser of Ratnaparkhi(Ratnaparkhi, 1997). Both frameworks are bottom-up, but while Ratnaparkhi’s views parse trees as the sequence of applications of four different types of tree construction rules, our framework strives to be somewhat simpler and more general. 2 The Probability Model Figure 2: Example parse tree with parent annotations. many features, because at some point we will not have enough data to sustain them. Hence in this approach, feature selection is not merely a matter of including good features. Rather, we must strike a delicate balance between how much context we want to include versus how much we dare to partition o"
W06-2002,J98-4004,0,\N,Missing
W06-2002,J03-4003,0,\N,Missing
W06-2002,kuhn-jellinghaus-2006-multilingual,1,\N,Missing
W07-0406,P05-1033,0,0.0275772,"language to a destination language. As our first step, we will assume that the system will be learning from a corpus consisting of triples hf, e, ai, where: (i) f is a sentence from our source language, which is parsed (the words of the sentence and the nodes of the parse tree may or may not be annotated with auxiliary information), (ii) e is a gold-standard translation of sentence f (the words of sentence e may or may not be annotated with auxiliary information), and (iii) a is an automaticallygenerated word alignment (e.g. via GIZA++) between source sentence f and destination sentence e. 1 (Chiang, 2005) also reports that with his hierarchical generalization of the phrase-based approach, the addition of parser information doesn’t lead to any improvements. 41 Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 41–48, c Rochester, New York, April 2007. 2007 Association for Computational Linguistics Figure 1: Example translation object. Let us refer to these triples as translation objects. The learning task is: using the training data, produce a scoring function P that assigns a score to every translation object hf, e, ai, such that this"
W07-0406,N03-1017,0,0.031353,"Missing"
W07-0406,P00-1056,0,0.0165372,"tiative comparison with the phrase-based approach, using the BLEU metric; then we discuss two concrete translation examples as a preliminary qualitative evaluation. Finally, we present a detailed manual error analysis. Our data was a subset of the Europarl corpus consisting of sentences of lengths ranging from 8 to 17 words. Our training corpus contained 50000 sentences and our test corpus contained 300 sentences. We also had a small number of reserved sentences for development. The English sentences were parsed using the Bikel parser (Bikel, 2004), and the sentences were aligned with GIZA++ (Och and Ney, 2000). We used the WEKA machine learning package (Witten and Frank, 2005) to train the distributions (specifically, we used model trees). For comparison, we also trained and evaluated Pharaoh (Koehn, 2005) on this limited corpus, using Pharaoh’s default parameters. Pharaoh achieved a BLEU score of 11.17 on the test set, whereas our 46 system achieved a BLEU score of 11.52. What is notable here is not the scores themselves (low due to the size of the training corpus). However our system managed to perform comparably with Pharaoh in a very early stage of its development, with rudimentary features and"
W07-0406,J03-1002,0,0.00273078,"ation system, based on reducing the machine translation task to a tree-labeling task. This tree labeling is further reduced to a sequence of decisions (of four varieties), which can be discriminatively trained. The optimal tree labeling (i.e. translation) is then found through a simple depth-first branch-andbound search. An early system founded on these ideas has been shown to be competitive with Pharaoh when both are trained on a small subsection of the Europarl corpus. 1 Motivation Statistical machine translation has, for a while now, been dominated by the phrase-based translation paradigm (Och and Ney, 2003). In this paradigm, sentences are translated from a source language to a target language through the repeated substitution of contiguous word sequences (“phrases”) from the source language for word sequences in the target language. Training of the phrase translation model builds on top of a standard statistical word alignment over the training corpus for identifying corresponding word blocks, assuming no further linguistic analysis of the source or target language. In decoding, these systems then typically rely on n-gram language models and simple statistical reordering models to shuffle the p"
W07-0406,J97-3002,0,0.0326673,"Missing"
W07-0406,P01-1067,0,0.0667517,"Missing"
W07-0406,koen-2004-pharaoh,0,\N,Missing
W07-0406,N04-1035,1,\N,Missing
W07-0406,J04-4004,0,\N,Missing
W07-1205,J93-2003,0,0.00592167,"guistics University of Potsdam, Germany hopkins@ling.uni-potsdam.de kuhn@ling.uni-potsdam.de Abstract The most successful statistical MT paradigm has been, for a while now, the so-call phrase-based MT approach (Och and Ney, 2003). In this paradigm, sentences are translated from a source language to a target language through the repeated substitution of contiguous word sequences (“phrases”) from the source language for word sequences in the target language. Training of the phrase translation model builds on top of a standard statistical word alignment over the training corpus of parallel text (Brown et al., 1993) for identifying corresponding word blocks, assuming no further linguistic analysis of the source or target language. In decoding, i.e. the application of the acquired translation model to unseen source sentences, these systems then typically rely on ngram language models and simple statistical reordering models to shuffle the phrases into an order that is coherent in the target language. In this paper, we propose a new syntaxbased machine translation (MT) approach based on reducing the MT task to a treelabeling task, which is further decomposed into a sequence of simple decisions for which di"
W07-1205,P05-1033,0,0.0266569,"translate sentences from a source language to a destination language. The general set-up is simple. Firstly, we have a training corpus of paired sentences f and e, where target sentence e is a gold standard translation of source sentence f . These sentence pairs are annotated with auxiliary information, which can include word alignments and syntactic information. We refer to these annotated sentence pairs as complete translation objects. Secondly, we have an evaluation corpus of source sentences. These sentences are annotated with a subset of the auxiliary information used to annotate the 1 (Chiang, 2005) also reports that with his hierarchical generalization of the phrase-based approach, the addition of parser information doesn’t lead to any improvements. 34 Figure 1: Example translation object. training corpus. We refer to these partially annotated source sentences as partial translation objects. The task at hand: use the training corpus to learn a procedure, through which we can successfully induce a complete translation object from a partial translation object. This is what we will define as translation. 3 Specific Task Addressed by this Paper Before going on to actually describe a transla"
W07-1205,N03-1017,0,0.0114334,"ge pairs and sublanguages is very costly for these systems). The other form of hybridization – a statistical MT model that is based on a deeper analysis of the syntactic 33 Proceedings of the ACL 2007 Workshop on Deep Linguistic Processing, pages 33–40, c Prague, Czech Republic, June, 2007. 2007 Association for Computational Linguistics structure of a sentence – has also long been identified as a desirable objective in principle (consider (Wu, 1997; Yamada and Knight, 2001)). However, attempts to retrofit syntactic information into the phrase-based paradigm have not met with enormous success (Koehn et al., 2003; Och et al., 2003)1 , and purely phrase-based MT systems continue to outperform these syntax/phrase-based hybrids. In this work, we try to make a fresh start with syntax-based statistical MT, discarding the phrasebased paradigm and designing a MT system from the ground up, using syntax as our central guiding star – besides the word alignment over a parallel corpus. Our approach is compatible with and can benefit substantially from rich linguistic representations obtained from deep grammars like the ParGram LFGs. Nevertheless, contrary to classical interlingual or deep transfer-based systems,"
W07-1205,P02-1035,0,0.0213626,"as output, but this is not required. 5.2 Exploiting deep linguistic information The use of discriminative classifiers makes our approach very flexible in terms of the information that can be exploited in the labeling (or translation) process. Any information that can be encoded as features relative to GHKM tree nodes can be used. For the experiments reported in this paper, we parsed the source language side of a parallel corpus (a small subsection of the English-German Europarl corpus; (Koehn, 2002)) with the XLE system, using the ParGram LFG grammar and applying probabilistic disambiguation (Riezler et al., 2002) to obtain a single analysis (i.e., a c-structure [phrase structure tree] and an f-structure [an associated attributevalue matrix with morphosyntactic feature information and a shallow semantic interpretation]) for each sentence. A fall-back mechanism integrated in the parser/grammar ensures that even for sentences that do not receive a full parse, substrings are deeply parsed and can often be treated successfully. We convert the c-structure/f-structure representation that is based on XLE’s sophisticated wordinternal analysis into a plain phrase structure tree representation based on the origi"
W07-1205,J97-3002,0,0.0316848,"ed data-based resource acquisition (although they may be among the best candidates for high-quality special-purpose translation – but adaption to new language pairs and sublanguages is very costly for these systems). The other form of hybridization – a statistical MT model that is based on a deeper analysis of the syntactic 33 Proceedings of the ACL 2007 Workshop on Deep Linguistic Processing, pages 33–40, c Prague, Czech Republic, June, 2007. 2007 Association for Computational Linguistics structure of a sentence – has also long been identified as a desirable objective in principle (consider (Wu, 1997; Yamada and Knight, 2001)). However, attempts to retrofit syntactic information into the phrase-based paradigm have not met with enormous success (Koehn et al., 2003; Och et al., 2003)1 , and purely phrase-based MT systems continue to outperform these syntax/phrase-based hybrids. In this work, we try to make a fresh start with syntax-based statistical MT, discarding the phrasebased paradigm and designing a MT system from the ground up, using syntax as our central guiding star – besides the word alignment over a parallel corpus. Our approach is compatible with and can benefit substantially fro"
W07-1205,P01-1067,0,0.0338322,"sed resource acquisition (although they may be among the best candidates for high-quality special-purpose translation – but adaption to new language pairs and sublanguages is very costly for these systems). The other form of hybridization – a statistical MT model that is based on a deeper analysis of the syntactic 33 Proceedings of the ACL 2007 Workshop on Deep Linguistic Processing, pages 33–40, c Prague, Czech Republic, June, 2007. 2007 Association for Computational Linguistics structure of a sentence – has also long been identified as a desirable objective in principle (consider (Wu, 1997; Yamada and Knight, 2001)). However, attempts to retrofit syntactic information into the phrase-based paradigm have not met with enormous success (Koehn et al., 2003; Och et al., 2003)1 , and purely phrase-based MT systems continue to outperform these syntax/phrase-based hybrids. In this work, we try to make a fresh start with syntax-based statistical MT, discarding the phrasebased paradigm and designing a MT system from the ground up, using syntax as our central guiding star – besides the word alignment over a parallel corpus. Our approach is compatible with and can benefit substantially from rich linguistic represen"
W07-1205,J03-1002,0,0.00578391,"Missing"
W07-1205,koen-2004-pharaoh,0,\N,Missing
W07-1205,N04-1035,1,\N,Missing
W07-1205,W02-1503,0,\N,Missing
W09-1104,W06-2920,0,0.0727066,"k in Fig. 1). To reflect this notion of loss during optimization, we also adjust the definition of the score of a tree: s(T ) = X s(i, j, l) (i,j,l)∈AT : l6= F RAG 6 Parsing with Fragmented Trees We refer to this modified model as f(iltering)MST. To make effective use of the fragmented trees produced by partial correspondence projection, both parsing approaches need to be adapted for training on sentences with unconnected substructures. Here we briefly discuss how we represent these structures, and then describe how we modified the parsers. We use the CoNLL-X data format for dependency trees (Buchholz and Marsi, 2006) to encode partial structures. Specifically, every fragment root specifies as its head an artificial root token w0 (distinguished from a true root dependency by a special relation F RAG). Thus, sentences with a fragmented parse are still represented as a single sentence, including all words; the difference from a fully parsed sentence is that unconnected substructures are attached directly under w0 . For instance, the partial parse in Fig. 1b would be represented as follows (details omitted): 6.2 (1) 6.1 1 2 3 4 U heeft volkomen gelijk pron verb adj noun 0 0 4 0 FRAG ROOT mod FRAG Transition-b"
W09-1104,N06-1019,0,0.0214896,"sed and weakly supervised parsing ranges from various forms of EM training (Pereira and Schabes, 1992; Klein and Manning, 2004; Smith and Eisner, 2004; Smith and Eisner, 2005) over bootstrapping approaches like selftraining (McClosky et al., 2006) to feature-based enhancements of discriminative reranking models (Koo et al., 2008) and the application of semisupervised SVMs (Wang et al., 2008). The partial correspondence method we present in this paper is compatible with such approaches and can be combined with other weakly supervised machine learning schemes. Our approach is similar to that of Clark and Curran (2006) who use partial training data (CCG lexical categories) for domain adaptation; however, they assume an existing CCG resource for the language in question to provide this data. 3 Projection of Dependency Trees Most state-of-the-art parsers for natural languages are data-driven and depend on the availability of sufficient amounts of labeled training data. However, manual creation of treebanks is time-consuming and labour-intensive. One way to avoid the expensive annotation process is to automatically label the training data using annotation projection (Yarowsky et al., 2001): Given a suitable re"
W09-1104,P04-1061,0,0.0124213,"accommodate for nonisomorphic sentences. Systematic non-parallelisms between source and target language are then addressed by hand-crafted rules in a post-projection step. These rules account for an enormous increase in the unlabeled f-score of the direct projections, from 33.9 to 65.7 for Spanish and from 26.3 to 52.4 for Chinese. But they need to be designed anew for every target language, which is time-consuming and requires knowledge of that language. 13 Research in the field of unsupervised and weakly supervised parsing ranges from various forms of EM training (Pereira and Schabes, 1992; Klein and Manning, 2004; Smith and Eisner, 2004; Smith and Eisner, 2005) over bootstrapping approaches like selftraining (McClosky et al., 2006) to feature-based enhancements of discriminative reranking models (Koo et al., 2008) and the application of semisupervised SVMs (Wang et al., 2008). The partial correspondence method we present in this paper is compatible with such approaches and can be combined with other weakly supervised machine learning schemes. Our approach is similar to that of Clark and Curran (2006) who use partial training data (CCG lexical categories) for domain adaptation; however, they assume an"
W09-1104,N03-1017,0,0.00114242,"and Frank, 2008), or semantic role labeling (Pad´o and Lapata, 2006). In these tasks, word labels can technically be introduced in isolation, without reference to the rest of the annotation. This means that an aggressive filter can be used to discard unreliable data points (words in a sentence) without necessarily affecting highprecision data points in the same sentence. By using only the bidirectional word alignment links, one can implement a very robust such filter, as the bidirectional links are generally reliable, even though they have low recall for overall translational correspondences (Koehn et al., 2003). The bidirectional alignment filter is common practice (Pad´o and Lapata, 2006); a similar strategy is to discard entire sentences with low aggregated alignment scores (Yarowsky et al., 2001). On the sentence level, Hwa et al. (2005) were the first to project dependency trees from English to Spanish and Chinese. They identify unreliable target parses (as a whole) on the basis of the number of unaligned or over-aligned words. In addition, they manipulate the trees to accommodate for nonisomorphic sentences. Systematic non-parallelisms between source and target language are then addressed by ha"
W09-1104,2005.mtsummit-papers.11,0,0.0126861,"eb and have) such that hE is the head of wE in the English tree. Annotation projection assumes direct correspondence (Hwa et al., 2005) between languages (or annotations), which—although it is valid in many cases—does not hold in general: non-parallelism between corresponding expressions in L1 and L2 causes errors in the target annotations. The word alignment constitutes a further source for errors if it is established automatically—which is typically the case in large parallel corpora. We have implemented a language-independent framework for dependency projection and use the Europarl corpus (Koehn, 2005) as the parallel text. Europarl consists of the proceedings of the European Parliament, professionally translated in 11 languages (approx. 30mln words per language). The data was aligned on the word level with G IZA ++ (Och and Ney, 2003).1 In the experiments reported here, we use the language pair English-Dutch, with English as the source for projection (L1 ) and Dutch as L2 . The English portion of the Europarl corpus was lemmatized and POS tagged with the TreeTagger (Schmid, 1994) and then parsed with MaltParser (which is described in Sec. 6), trained on a dependency-converted version of th"
W09-1104,P08-1068,0,0.0576706,"Missing"
W09-1104,H94-1020,0,0.0270846,"sists of the proceedings of the European Parliament, professionally translated in 11 languages (approx. 30mln words per language). The data was aligned on the word level with G IZA ++ (Och and Ney, 2003).1 In the experiments reported here, we use the language pair English-Dutch, with English as the source for projection (L1 ) and Dutch as L2 . The English portion of the Europarl corpus was lemmatized and POS tagged with the TreeTagger (Schmid, 1994) and then parsed with MaltParser (which is described in Sec. 6), trained on a dependency-converted version of the WSJ part from the Penn Treebank (Marcus et al., 1994), but with the automatic POS tags. The Dutch sentences were only POS tagged (with TreeTagger).2 3.1 Data Loss Through Filtering We quantitatively assess the impact of various filtering techniques on a random sample of 100,000 English-Dutch sentence pairs from Europarl (avg. 1 Following standard practice, we computed word alignments in both directions (L1 → L2 and L2 → L1 ); this gives rise to two unidirectional alignments. The bidirectional alignment is the intersection of the two unidirectional ones. 2 The Dutch POS tags are used to train the monolingual parsers from the projected dependency"
W09-1104,N06-1020,0,0.0329056,"by hand-crafted rules in a post-projection step. These rules account for an enormous increase in the unlabeled f-score of the direct projections, from 33.9 to 65.7 for Spanish and from 26.3 to 52.4 for Chinese. But they need to be designed anew for every target language, which is time-consuming and requires knowledge of that language. 13 Research in the field of unsupervised and weakly supervised parsing ranges from various forms of EM training (Pereira and Schabes, 1992; Klein and Manning, 2004; Smith and Eisner, 2004; Smith and Eisner, 2005) over bootstrapping approaches like selftraining (McClosky et al., 2006) to feature-based enhancements of discriminative reranking models (Koo et al., 2008) and the application of semisupervised SVMs (Wang et al., 2008). The partial correspondence method we present in this paper is compatible with such approaches and can be combined with other weakly supervised machine learning schemes. Our approach is similar to that of Clark and Curran (2006) who use partial training data (CCG lexical categories) for domain adaptation; however, they assume an existing CCG resource for the language in question to provide this data. 3 Projection of Dependency Trees Most state-of-t"
W09-1104,D07-1013,0,0.00757012,"idirectional projection.4 As can be seen in Table 1 (‘bi+frags≤3 ’), this combination boosts the amount of usable data to a range similar to that of the fallback technique for trees; but unlike the latter, partial correspondence continues to impose a highprecision filter (bidirectionality) while improving recall through relaxed structural requirements (partial correspondence). Table 2 shows how fragment size varies with sentence length. 5 Data-driven Dependency Parsing Models for data-driven dependency parsing can be roughly divided into two paradigms: Graph-based and transition-based models (McDonald and Nivre, 2007). 5.1 Graph-based Models In the graph-based approach, global optimization considers all possible arcs to find the tree Tˆ s.t. X Tˆ = arg max s(T ) = arg max s(i, j, l) T ∈D T ∈D (i,j,l)∈AT where D is the set of all well-formed dependency trees for the sentence, AT is the set of arcs in T , and s(i, j, l) is the score of an arc between words wi and wj with label l. The specific graph-based parser we use in this paper is the MSTParser of McDonald et al. (2005). The MSTParser learns the scoring function s using an online learning algorithm (Crammer and Singer, 2003) which maximizes the margin be"
W09-1104,H05-1066,0,0.160725,"Missing"
W09-1104,W06-2932,0,0.0606394,"Missing"
W09-1104,P08-1108,0,0.0660811,"Missing"
W09-1104,P05-1013,0,0.021943,"ssible to single out this 16 Setup We train instances of the graph- and the transitionbased parser on projected dependencies, and occasionally refer to these as “projected parsers”.5 All results were obtained on the held-out CoNLL-X test set of 386 sentences (avg. 12.9 5 The MaltParsers use the projective Nivre arc-standard parsing algorithm. For SVM training, data are split on the coarse POS tag, with a threshold of 5,000 instances. MSTParser instances use the projective Eisner parsing algorithm, and firstorder features. The input for both systems is projectivized using the head+path schema (Nivre and Nilsson, 2005). Alpino EP Alpino + EP baseline 1 (previous) baseline 2 (next) Malt MST 80.05 82.43 75.33 73.09 77.47 81.63 23.65 27.63 words 13,500 62,500 68,000 68,000 Malt 65.94 59.28 55.09 69.15 MST 67.76 65.08 57.14 70.02 b. trees (bidirectional) 100,000 trees (fallback) 100,000 bi+frags≤3 100,000 bi+frags≤3 (fMalt/fMST) 100,000 61.86 60.05 54.50 68.65 69.91 64.84 55.87 69.86 c. trees (bidirectional) 102,300 trees (fallback) 465,500 bi+frags≤3 523,000 bi+frags≤3 (fMalt/fMST) 523,000 63.32 53.45 51.48 69.52 69.85 64.88 57.20 70.33 a. trees (bidirectional) trees (fallback) bi+frags≤3 bi+frags≤3 (fMalt/fMS"
W09-1104,W06-2933,0,0.0298967,"Missing"
W09-1104,J03-1002,0,0.00201456,"neral: non-parallelism between corresponding expressions in L1 and L2 causes errors in the target annotations. The word alignment constitutes a further source for errors if it is established automatically—which is typically the case in large parallel corpora. We have implemented a language-independent framework for dependency projection and use the Europarl corpus (Koehn, 2005) as the parallel text. Europarl consists of the proceedings of the European Parliament, professionally translated in 11 languages (approx. 30mln words per language). The data was aligned on the word level with G IZA ++ (Och and Ney, 2003).1 In the experiments reported here, we use the language pair English-Dutch, with English as the source for projection (L1 ) and Dutch as L2 . The English portion of the Europarl corpus was lemmatized and POS tagged with the TreeTagger (Schmid, 1994) and then parsed with MaltParser (which is described in Sec. 6), trained on a dependency-converted version of the WSJ part from the Penn Treebank (Marcus et al., 1994), but with the automatic POS tags. The Dutch sentences were only POS tagged (with TreeTagger).2 3.1 Data Loss Through Filtering We quantitatively assess the impact of various filterin"
W09-1104,P06-1146,0,0.0566443,"Missing"
W09-1104,P92-1017,0,0.286588,"ey manipulate the trees to accommodate for nonisomorphic sentences. Systematic non-parallelisms between source and target language are then addressed by hand-crafted rules in a post-projection step. These rules account for an enormous increase in the unlabeled f-score of the direct projections, from 33.9 to 65.7 for Spanish and from 26.3 to 52.4 for Chinese. But they need to be designed anew for every target language, which is time-consuming and requires knowledge of that language. 13 Research in the field of unsupervised and weakly supervised parsing ranges from various forms of EM training (Pereira and Schabes, 1992; Klein and Manning, 2004; Smith and Eisner, 2004; Smith and Eisner, 2005) over bootstrapping approaches like selftraining (McClosky et al., 2006) to feature-based enhancements of discriminative reranking models (Koo et al., 2008) and the application of semisupervised SVMs (Wang et al., 2008). The partial correspondence method we present in this paper is compatible with such approaches and can be combined with other weakly supervised machine learning schemes. Our approach is similar to that of Clark and Curran (2006) who use partial training data (CCG lexical categories) for domain adaptation;"
W09-1104,P04-1062,0,0.0216379,"phic sentences. Systematic non-parallelisms between source and target language are then addressed by hand-crafted rules in a post-projection step. These rules account for an enormous increase in the unlabeled f-score of the direct projections, from 33.9 to 65.7 for Spanish and from 26.3 to 52.4 for Chinese. But they need to be designed anew for every target language, which is time-consuming and requires knowledge of that language. 13 Research in the field of unsupervised and weakly supervised parsing ranges from various forms of EM training (Pereira and Schabes, 1992; Klein and Manning, 2004; Smith and Eisner, 2004; Smith and Eisner, 2005) over bootstrapping approaches like selftraining (McClosky et al., 2006) to feature-based enhancements of discriminative reranking models (Koo et al., 2008) and the application of semisupervised SVMs (Wang et al., 2008). The partial correspondence method we present in this paper is compatible with such approaches and can be combined with other weakly supervised machine learning schemes. Our approach is similar to that of Clark and Curran (2006) who use partial training data (CCG lexical categories) for domain adaptation; however, they assume an existing CCG resource fo"
W09-1104,P05-1044,0,0.0155557,"ic non-parallelisms between source and target language are then addressed by hand-crafted rules in a post-projection step. These rules account for an enormous increase in the unlabeled f-score of the direct projections, from 33.9 to 65.7 for Spanish and from 26.3 to 52.4 for Chinese. But they need to be designed anew for every target language, which is time-consuming and requires knowledge of that language. 13 Research in the field of unsupervised and weakly supervised parsing ranges from various forms of EM training (Pereira and Schabes, 1992; Klein and Manning, 2004; Smith and Eisner, 2004; Smith and Eisner, 2005) over bootstrapping approaches like selftraining (McClosky et al., 2006) to feature-based enhancements of discriminative reranking models (Koo et al., 2008) and the application of semisupervised SVMs (Wang et al., 2008). The partial correspondence method we present in this paper is compatible with such approaches and can be combined with other weakly supervised machine learning schemes. Our approach is similar to that of Clark and Curran (2006) who use partial training data (CCG lexical categories) for domain adaptation; however, they assume an existing CCG resource for the language in questio"
W09-1104,I08-1064,1,0.860937,"English (L1 ): I have two questions You are absolutely right You are absolutely right Dutch (L2 ): Ik heb twee vragen U heeft volkomen gelijk U heeft volkomen gelijk 1 2 3 Figure 1: Dependency tree projection from English to Dutch. (a) Ideal scenario with bidirectional alignments. (b) Projection fails due to weak alignments. (c) Constrained fallback projection. 2 Related Work Annotation projection has been applied to many different NLP tasks. On the word or phrase level, these include morphological analysis, part-of-speech tagging and NP-bracketing (Yarowsky et al., 2001), temporal analysis (Spreyer and Frank, 2008), or semantic role labeling (Pad´o and Lapata, 2006). In these tasks, word labels can technically be introduced in isolation, without reference to the rest of the annotation. This means that an aggressive filter can be used to discard unreliable data points (words in a sentence) without necessarily affecting highprecision data points in the same sentence. By using only the bidirectional word alignment links, one can implement a very robust such filter, as the bidirectional links are generally reliable, even though they have low recall for overall translational correspondences (Koehn et al., 20"
W09-1104,P08-1061,0,0.0931715,"Missing"
W09-1104,H01-1035,0,0.178966,"tion for Computational Linguistics a. b. c. English (L1 ): I have two questions You are absolutely right You are absolutely right Dutch (L2 ): Ik heb twee vragen U heeft volkomen gelijk U heeft volkomen gelijk 1 2 3 Figure 1: Dependency tree projection from English to Dutch. (a) Ideal scenario with bidirectional alignments. (b) Projection fails due to weak alignments. (c) Constrained fallback projection. 2 Related Work Annotation projection has been applied to many different NLP tasks. On the word or phrase level, these include morphological analysis, part-of-speech tagging and NP-bracketing (Yarowsky et al., 2001), temporal analysis (Spreyer and Frank, 2008), or semantic role labeling (Pad´o and Lapata, 2006). In these tasks, word labels can technically be introduced in isolation, without reference to the rest of the annotation. This means that an aggressive filter can be used to discard unreliable data points (words in a sentence) without necessarily affecting highprecision data points in the same sentence. By using only the bidirectional word alignment links, one can implement a very robust such filter, as the bidirectional links are generally reliable, even though they have low recall for overall tr"
W09-2303,W07-1512,0,0.0206485,"gner aligned only sammen and add, but not lægger and add. This would mean that the alignments or translations of add would most likely be associated with the following probabilities: .66 .33 (add, sammen) (add, addere) which again means that our system is likely to arrive at the wrong alignment or translation in (4). Nevertheless these alignments are rewarded in AER. TUER, on the other hand, reflects the intuition that unless you get the entire translation unit it’s better to get nothing at all. The hand-aligned parallel corpora in our experiments come from the Copenhagen Dependency Treebank (Buch-Kromann, 2007), for five different language pairs, the German-English parallel corpus used in Pad´o and Lapata (2006), and the six parallel corpora of the first 100 sentences of Europarl (Koehn, 2005) for different language pairs documented in Graca et al. (2008). Consequently, our experiments include a total of 12 parallel corpora. The biggest parallel corpus consists of 4,729 sentence pairs; the smallest of 61 sentence pairs. The average size is 541 sentence pairs. The six parallel corpora documented in Graca et al. (2008) use sure and possible alignments; in our experiments, as already mentioned, the two"
W09-2303,E06-1019,0,0.0156313,"for syntax-based machine translation, but some prior knowledge is assumed. Sect. 5 brings the three sections together and presents lower bounds on the coverage of the systems discussed in Sect. 4, obtained by inspection of the results in Sect. 2 and 3. Sect. 6 compares our results to related work, in particular Zens and Ney (2003). 2 Inside-out alignments Wu (1997) identified so-called inside-out alignments, two alignment configurations that cannot be induced by binary synchronous context-free grammars; these alignment configurations, while infrequent in language pairs such as English–French (Cherry and Lin, 2006; Wellington et al., 2006), have been argued to be frequent in other language pairs, incl. English–Chinese (Wellington et al., 2006) and English–Spanish (Lepage and Denoual, 2005). While our main focus is on configurations that involve discontinuous translation units, the frequencies of inside-out alignments in our parallel corpora are also reported. Recall that inside-out alignments are of the form (or upside-down): a b c d a b c d or e f g h e f g h Our findings are summarized in Figure 1. Note that there is some variation across the corpora. The fact that there are no inside-out alignments"
W09-2303,W04-3302,0,0.109693,"Missing"
W09-2303,J07-2003,0,0.159621,"d by the German Research Foundation in the Emmy Noether project Ptolemaios on grammar learning from parallel corpora; and while he was a Postdoctoral Researcher at the ISV Computational Linguistics Group, Copenhagen Business School, supported by the Danish Research Foundation in the project Efficient syntax- and semantics-based machine translation. † The second author is supported by the German Research Foundation in the Emmy Noether project Ptolemaios on grammar learning from parallel corpora. production rules are typically learned from alignment structures (Wu, 1997; Zhang and Gildea, 2004; Chiang, 2007) or from alignment structures and derivation trees for the source string (Yamada and Knight, 2001; Zhang and Gildea, 2004). They are also used for inducing alignments (Wu, 1997; Zhang and Gildea, 2004). It is for all three reasons, i.e. translation, induction from alignment structures and induction of alignment structures, important that the synchronous grammars are expressive enough to induce all the alignment structures found in hand-aligned gold standard parallel corpora (Wellington et al., 2006). Such alignments are supposed to reflect the structure of translations, typically contain fewer"
W09-2303,P03-2041,0,0.107286,"hang et al. (2006) and Chiang (2007) are not expressive enough to do that. The synchronous grammars used in these systems are, formally, synchronous context-free grammars of rank two (2-SCFGs), or equivalently (normal form) inversion transduction grammars (ITGs).1 The notion of rank is defined as the maximum number of constituents aligned by a production rule, i.e. the maximum number of distinct indeces. Our results will be extended to slight extensions of 2SCFGs, incl. the extension of ITGs proposed by Zens and Ney (2003) (xITGs), synchronous tree substitution grammars of rank two (2-STSGs) (Eisner, 2003; Shieber, 2007), i.e. where tree pairs include at most two linked pairs of nonterminals, and synchronous tree-adjoining grammars of rank two 1 2-SCFGs allow distinct LHS nonterminals, while ITGs do not; but for any 2-SCFG an equivalent ITG can be constructed by creating a cross-product of nonterminals from two sides. 19 Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 19–27, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics (2-STAGs) (Shieber and Schabes, 1990; Harbusch and Poller, 1996; Nesson et al., 2008). The over"
W09-2303,J07-3002,0,0.0657338,"r is a set of terminals that are recognized or generated simultaneously. Consequently, synchronous grammars can only induce complete alignment structures (by transitivity of simultaneity).2 Syntax-based approaches to machine translations are commonly evaluated in terms of their alignment error rate (AER) on one or more parallel corpora (Och and Ney, 2000; Zhang and Gildea, 2004). The AER, in the case where all alignments are sure alignments, is AER = 1 − 2|SA ∩ GA | |SA |+ |GA | where GA are the gold standard alignments, and SA the alignments produced by the system. AER has been criticized by Fraser and Marcu (2007). They show that AER does not penalize unequal precision and recall when a distinction between sure and possible alignments is 2 One of the hand-aligned parallel corpora used in our experiments, the one also used in Pad´o and Lapata (2006), includes incomplete alignment structures. 20 made. Since no such distinction is assumed below, the classical definition is used. We introduce also the notion of translation unit error rate (TUER), which is defined as TUER = 1 − 2|SU ∩ GU | |SU |+ |GU | where GU are the translation units in the gold standard, and SU the translation units produced by the syst"
W09-2303,P04-1064,0,0.153862,"verage of the systems are derived from our results. Our notion of an alignment structure is standard. Words can be aligned to multiple words. Unaligned nodes are permitted. Maximally connected subgraphs are called translation units. There is one more choice to make in the context of many-to-many alignments, namely whether the alignment relation is such that if wi |wk′ and wi |wl′ , resp., are aligned, and wj |wk′ are aligned too, then wj |wl′ are also aligned. If so, the alignment structure is divided into complete translation units. Such alignment structures are therefore called complete; in Goutte et al. (2004), alignment structures with this property are said to be closed under transitivity. An alignment structure is simply written as a sequence of alignments, e.g. hwi |wk′ , wi |wl′ , wj |wk′ , wj |wk′ i, or, alternatively, as sequences of (possibly discontinuous) translation units, e.g. hwi wj |wk′ wl′ i. A translation unit induced by a synchronous grammar is a set of terminals that are recognized or generated simultaneously. Consequently, synchronous grammars can only induce complete alignment structures (by transitivity of simultaneity).2 Syntax-based approaches to machine translations are comm"
W09-2303,graca-etal-2008-building,0,0.131827,"tem is likely to arrive at the wrong alignment or translation in (4). Nevertheless these alignments are rewarded in AER. TUER, on the other hand, reflects the intuition that unless you get the entire translation unit it’s better to get nothing at all. The hand-aligned parallel corpora in our experiments come from the Copenhagen Dependency Treebank (Buch-Kromann, 2007), for five different language pairs, the German-English parallel corpus used in Pad´o and Lapata (2006), and the six parallel corpora of the first 100 sentences of Europarl (Koehn, 2005) for different language pairs documented in Graca et al. (2008). Consequently, our experiments include a total of 12 parallel corpora. The biggest parallel corpus consists of 4,729 sentence pairs; the smallest of 61 sentence pairs. The average size is 541 sentence pairs. The six parallel corpora documented in Graca et al. (2008) use sure and possible alignments; in our experiments, as already mentioned, the two types of alignments are treated alike.3 3 The annotations of the parallel corpora differ in format and consistency. In fact the empirical lower bounds obtained below are lower bounds in two senses: (i) they are lower bounds on TUERs because TUERs m"
W09-2303,2005.mtsummit-papers.11,0,0.00807339,"dd, sammen) (add, addere) which again means that our system is likely to arrive at the wrong alignment or translation in (4). Nevertheless these alignments are rewarded in AER. TUER, on the other hand, reflects the intuition that unless you get the entire translation unit it’s better to get nothing at all. The hand-aligned parallel corpora in our experiments come from the Copenhagen Dependency Treebank (Buch-Kromann, 2007), for five different language pairs, the German-English parallel corpus used in Pad´o and Lapata (2006), and the six parallel corpora of the first 100 sentences of Europarl (Koehn, 2005) for different language pairs documented in Graca et al. (2008). Consequently, our experiments include a total of 12 parallel corpora. The biggest parallel corpus consists of 4,729 sentence pairs; the smallest of 61 sentence pairs. The average size is 541 sentence pairs. The six parallel corpora documented in Graca et al. (2008) use sure and possible alignments; in our experiments, as already mentioned, the two types of alignments are treated alike.3 3 The annotations of the parallel corpora differ in format and consistency. In fact the empirical lower bounds obtained below are lower bounds in"
W09-2303,P08-1069,0,0.0849397,"nk two (2-STSGs) (Eisner, 2003; Shieber, 2007), i.e. where tree pairs include at most two linked pairs of nonterminals, and synchronous tree-adjoining grammars of rank two 1 2-SCFGs allow distinct LHS nonterminals, while ITGs do not; but for any 2-SCFG an equivalent ITG can be constructed by creating a cross-product of nonterminals from two sides. 19 Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 19–27, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics (2-STAGs) (Shieber and Schabes, 1990; Harbusch and Poller, 1996; Nesson et al., 2008). The overall frequency of alignment structures that cannot be induced by these approaches is examined across a wide collection of hand-aligned parallel corpora. Empirical lower bounds on the coverage of the systems are derived from our results. Our notion of an alignment structure is standard. Words can be aligned to multiple words. Unaligned nodes are permitted. Maximally connected subgraphs are called translation units. There is one more choice to make in the context of many-to-many alignments, namely whether the alignment relation is such that if wi |wk′ and wi |wl′ , resp., are aligned, a"
W09-2303,C00-2163,0,0.452993,"ank two (2-SCFGs) (Satta and Peserico, 2005), used in syntaxbased machine translation systems such as Wu (1997), Zhang et al. (2006) and Chiang (2007), in terms of what alignments they induce, has been discussed in Wu (1997) and Wellington et al. (2006), but with a one-sided focus on so-called “inside-out alignments”. Other alignment configurations that cannot be induced by 2-SCFGs are identified in this paper, and their frequencies across a wide collection of hand-aligned parallel corpora are examined. Empirical lower bounds on two measures of alignment error rate, i.e. the one introduced in Och and Ney (2000) and one where only complete translation units are considered, are derived for 2-SCFGs and related formalisms. 1 Introduction Syntax-based approaches to machine translation typically use synchronous grammars to recognize or produce translation equivalents. The synchronous ∗ This work was done while the first author was a Senior Researcher at the Dpt. of Linguistics, University of Potsdam, supported by the German Research Foundation in the Emmy Noether project Ptolemaios on grammar learning from parallel corpora; and while he was a Postdoctoral Researcher at the ISV Computational Linguistics Gr"
W09-2303,P06-1146,0,0.327535,"Missing"
W09-2303,C90-3045,0,0.424016,"(xITGs), synchronous tree substitution grammars of rank two (2-STSGs) (Eisner, 2003; Shieber, 2007), i.e. where tree pairs include at most two linked pairs of nonterminals, and synchronous tree-adjoining grammars of rank two 1 2-SCFGs allow distinct LHS nonterminals, while ITGs do not; but for any 2-SCFG an equivalent ITG can be constructed by creating a cross-product of nonterminals from two sides. 19 Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 19–27, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics (2-STAGs) (Shieber and Schabes, 1990; Harbusch and Poller, 1996; Nesson et al., 2008). The overall frequency of alignment structures that cannot be induced by these approaches is examined across a wide collection of hand-aligned parallel corpora. Empirical lower bounds on the coverage of the systems are derived from our results. Our notion of an alignment structure is standard. Words can be aligned to multiple words. Unaligned nodes are permitted. Maximally connected subgraphs are called translation units. There is one more choice to make in the context of many-to-many alignments, namely whether the alignment relation is such th"
W09-2303,2008.eamt-1.23,1,0.712883,"computer science, there is a trade-off between expressivity and complexity. The results presented here, namely that classes of alignment structures excluded by syntax-based translation systems, occur frequently in hand-aligned parallel corpora, could be taken to indicate that more expressive formalisms are needed. This at least seems to be the case to the extent alignment error rates are reasonable measures of the adequacy of syntax-based machine translation systems. On the other hand parsing complexities in 26 Two other challenges for this type of approach are: (i) The use of intersection in Søgaard (2008b) to induce insideout alignments and cross-serial DTUs seems to miss important generalizations; see Chiang (2004) for a similar point in the context of parsing. (ii) If the class of alignment structures is restricted in any natural way, i.e. to 1 : 1 alignments, the problem whether there exists a possible alignment given two sentences and a grammar becomes NP-hard (Søgaard, 2009). NB: The undecidability of computing tight estimators was pointed out to us by Mark-Jan Nederhof (p.c.), but Alexander Clark (p.c.) and others have suggested that pseudo-tight estimators can be used in practice. Refe"
W09-2303,C08-2026,1,0.857018,"computer science, there is a trade-off between expressivity and complexity. The results presented here, namely that classes of alignment structures excluded by syntax-based translation systems, occur frequently in hand-aligned parallel corpora, could be taken to indicate that more expressive formalisms are needed. This at least seems to be the case to the extent alignment error rates are reasonable measures of the adequacy of syntax-based machine translation systems. On the other hand parsing complexities in 26 Two other challenges for this type of approach are: (i) The use of intersection in Søgaard (2008b) to induce insideout alignments and cross-serial DTUs seems to miss important generalizations; see Chiang (2004) for a similar point in the context of parsing. (ii) If the class of alignment structures is restricted in any natural way, i.e. to 1 : 1 alignments, the problem whether there exists a possible alignment given two sentences and a grammar becomes NP-hard (Søgaard, 2009). NB: The undecidability of computing tight estimators was pointed out to us by Mark-Jan Nederhof (p.c.), but Alexander Clark (p.c.) and others have suggested that pseudo-tight estimators can be used in practice. Refe"
W09-2303,P06-1123,0,0.701618,"rpora. production rules are typically learned from alignment structures (Wu, 1997; Zhang and Gildea, 2004; Chiang, 2007) or from alignment structures and derivation trees for the source string (Yamada and Knight, 2001; Zhang and Gildea, 2004). They are also used for inducing alignments (Wu, 1997; Zhang and Gildea, 2004). It is for all three reasons, i.e. translation, induction from alignment structures and induction of alignment structures, important that the synchronous grammars are expressive enough to induce all the alignment structures found in hand-aligned gold standard parallel corpora (Wellington et al., 2006). Such alignments are supposed to reflect the structure of translations, typically contain fewer errors and are used to evaluate automatically induced alignments. In this paper it is shown that the synchronous grammars used in Wu (1997), Zhang et al. (2006) and Chiang (2007) are not expressive enough to do that. The synchronous grammars used in these systems are, formally, synchronous context-free grammars of rank two (2-SCFGs), or equivalently (normal form) inversion transduction grammars (ITGs).1 The notion of rank is defined as the maximum number of constituents aligned by a production rule"
W09-2303,J97-3002,0,0.921945,"s, University of Potsdam, supported by the German Research Foundation in the Emmy Noether project Ptolemaios on grammar learning from parallel corpora; and while he was a Postdoctoral Researcher at the ISV Computational Linguistics Group, Copenhagen Business School, supported by the Danish Research Foundation in the project Efficient syntax- and semantics-based machine translation. † The second author is supported by the German Research Foundation in the Emmy Noether project Ptolemaios on grammar learning from parallel corpora. production rules are typically learned from alignment structures (Wu, 1997; Zhang and Gildea, 2004; Chiang, 2007) or from alignment structures and derivation trees for the source string (Yamada and Knight, 2001; Zhang and Gildea, 2004). They are also used for inducing alignments (Wu, 1997; Zhang and Gildea, 2004). It is for all three reasons, i.e. translation, induction from alignment structures and induction of alignment structures, important that the synchronous grammars are expressive enough to induce all the alignment structures found in hand-aligned gold standard parallel corpora (Wellington et al., 2006). Such alignments are supposed to reflect the structure o"
W09-2303,P01-1067,0,0.0949335,"earning from parallel corpora; and while he was a Postdoctoral Researcher at the ISV Computational Linguistics Group, Copenhagen Business School, supported by the Danish Research Foundation in the project Efficient syntax- and semantics-based machine translation. † The second author is supported by the German Research Foundation in the Emmy Noether project Ptolemaios on grammar learning from parallel corpora. production rules are typically learned from alignment structures (Wu, 1997; Zhang and Gildea, 2004; Chiang, 2007) or from alignment structures and derivation trees for the source string (Yamada and Knight, 2001; Zhang and Gildea, 2004). They are also used for inducing alignments (Wu, 1997; Zhang and Gildea, 2004). It is for all three reasons, i.e. translation, induction from alignment structures and induction of alignment structures, important that the synchronous grammars are expressive enough to induce all the alignment structures found in hand-aligned gold standard parallel corpora (Wellington et al., 2006). Such alignments are supposed to reflect the structure of translations, typically contain fewer errors and are used to evaluate automatically induced alignments. In this paper it is shown that"
W09-2303,P03-1019,0,0.74412,"ed alignments. In this paper it is shown that the synchronous grammars used in Wu (1997), Zhang et al. (2006) and Chiang (2007) are not expressive enough to do that. The synchronous grammars used in these systems are, formally, synchronous context-free grammars of rank two (2-SCFGs), or equivalently (normal form) inversion transduction grammars (ITGs).1 The notion of rank is defined as the maximum number of constituents aligned by a production rule, i.e. the maximum number of distinct indeces. Our results will be extended to slight extensions of 2SCFGs, incl. the extension of ITGs proposed by Zens and Ney (2003) (xITGs), synchronous tree substitution grammars of rank two (2-STSGs) (Eisner, 2003; Shieber, 2007), i.e. where tree pairs include at most two linked pairs of nonterminals, and synchronous tree-adjoining grammars of rank two 1 2-SCFGs allow distinct LHS nonterminals, while ITGs do not; but for any 2-SCFG an equivalent ITG can be constructed by creating a cross-product of nonterminals from two sides. 19 Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 19–27, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics (2-STAGs) ("
W09-2303,C04-1060,0,0.15659,"ity of Potsdam, supported by the German Research Foundation in the Emmy Noether project Ptolemaios on grammar learning from parallel corpora; and while he was a Postdoctoral Researcher at the ISV Computational Linguistics Group, Copenhagen Business School, supported by the Danish Research Foundation in the project Efficient syntax- and semantics-based machine translation. † The second author is supported by the German Research Foundation in the Emmy Noether project Ptolemaios on grammar learning from parallel corpora. production rules are typically learned from alignment structures (Wu, 1997; Zhang and Gildea, 2004; Chiang, 2007) or from alignment structures and derivation trees for the source string (Yamada and Knight, 2001; Zhang and Gildea, 2004). They are also used for inducing alignments (Wu, 1997; Zhang and Gildea, 2004). It is for all three reasons, i.e. translation, induction from alignment structures and induction of alignment structures, important that the synchronous grammars are expressive enough to induce all the alignment structures found in hand-aligned gold standard parallel corpora (Wellington et al., 2006). Such alignments are supposed to reflect the structure of translations, typicall"
W09-2303,N06-1033,0,0.094295,"for inducing alignments (Wu, 1997; Zhang and Gildea, 2004). It is for all three reasons, i.e. translation, induction from alignment structures and induction of alignment structures, important that the synchronous grammars are expressive enough to induce all the alignment structures found in hand-aligned gold standard parallel corpora (Wellington et al., 2006). Such alignments are supposed to reflect the structure of translations, typically contain fewer errors and are used to evaluate automatically induced alignments. In this paper it is shown that the synchronous grammars used in Wu (1997), Zhang et al. (2006) and Chiang (2007) are not expressive enough to do that. The synchronous grammars used in these systems are, formally, synchronous context-free grammars of rank two (2-SCFGs), or equivalently (normal form) inversion transduction grammars (ITGs).1 The notion of rank is defined as the maximum number of constituents aligned by a production rule, i.e. the maximum number of distinct indeces. Our results will be extended to slight extensions of 2SCFGs, incl. the extension of ITGs proposed by Zens and Ney (2003) (xITGs), synchronous tree substitution grammars of rank two (2-STSGs) (Eisner, 2003; Shie"
W09-2303,H05-1101,0,\N,Missing
W09-2303,W07-0412,0,\N,Missing
W09-2303,W90-0102,0,\N,Missing
W09-2904,W02-2001,0,0.323471,"rs. We will show that these correspondences can be reliably detected on dependency-parsed, wordaligned sentences and are able to identify various MWE patterns. In a monolingual setting, the task of MWE extraction is usually conceived of as a lexical association problem where distributional measures model the syntactic and semantic idiosyncracy exhibited by MWEs, e.g. (Pecina, 2008). This approach generally involves two main steps: 1) the extraction of a candidate list of potential MWEs, often constrained by a particular target pattern of the detection method, like verb particle constructions (Baldwin and Villavicencio, 2002) or verb PP combinations (Villada Moir´on and Tiedemann, 2006), 2) the ranking of this candidate list by an appropriate assocation measure. The crosslingual MWE identification we present in this paper is, a priori, independent of any specific association measure or syntactic pattern. The translation scenario allows us to adopt a completely data-driven definition of what constitutes an MWE: Given a parallel corpus, we propose to consider those tokens in a target language as MWEs which correspond to a single lexical item in the source language. The intuition is that if a group of lexical items i"
W09-2904,P05-1074,0,0.0859961,"Missing"
W09-2904,cyrus-2006-building,0,0.0276178,"blematic and challenging type of translational correspondence in our gold standard. While the MWE literature typically discusses the distinction between collocations and MWEs, the boarderline between paraphrases and MWEs is not really clear. On the hand, paraphrases, as we classified them here, are transparent combinations of lexical items, like in the example below ensure that something increases. However, semantically, these transparent combinations can also be rendered by an atomic expression increase. A further problem raised by paraphrases is that they often involve translational shifts (Cyrus, 2006). These shifts are hard to identify automatically and present a general challenge for semantic processing of parallel corpora. An example is given below. (6) The committee set out to equip the institutions with a political instrument. Verb preposition combinations: While this class isn’t discussed very often in the MWE literature, it can nevertheless be considered as an idiosyncratic combination of lexical items. Sag et al (2002) propose an analysis within an MWE framework. (7) Sie werden den Treibhauseffekt verschlimmern. They will the green house effect aggravate. (8) They will add to the gr"
W09-2904,W09-0208,0,0.0192418,"rm of one-to-many translations. By contrast, previous approaches to paraphrase extraction made more explicit use of crosslingual semantic information. In (Bannard and CallisonBurch, 2005), the authors use the target language as a pivot providing contextual features for identifying semantically similar expressions. Paraphrasing is however only partially comparable to the crosslingual MWE detection we propose in this paper. Recently, the very pronounced context dependence of monolingual pairs of semantically similar expressions has been recognized as a major challenge in modelling word meaning (Erk and Pado, 2009). The idea that parallel corpora can be used as a linguistic resource that provides empirical evidence for monolingual idiosyncrasies has already Related Work The problem sketched in this paper has clear conncetions to statistical MT. So-called phrase-based translation models generally target whole sentence alignment and do not necessarily recur to linguistically motivated phrase correspondences (Koehn et al., 2003). Syntax-based translation that specifies formal relations between bilingual parses was 29 References been exploited in, e.g. morphology projection (Yarowsky et al., 2001) or word s"
W09-2904,N03-1017,0,0.0109718,"paper. Recently, the very pronounced context dependence of monolingual pairs of semantically similar expressions has been recognized as a major challenge in modelling word meaning (Erk and Pado, 2009). The idea that parallel corpora can be used as a linguistic resource that provides empirical evidence for monolingual idiosyncrasies has already Related Work The problem sketched in this paper has clear conncetions to statistical MT. So-called phrase-based translation models generally target whole sentence alignment and do not necessarily recur to linguistically motivated phrase correspondences (Koehn et al., 2003). Syntax-based translation that specifies formal relations between bilingual parses was 29 References been exploited in, e.g. morphology projection (Yarowsky et al., 2001) or word sense disambiguation (Dyvik, 2004). While in a monolingual setting, it is quite tricky to come up with theoretical or empirical definitions of sense discriminations, the crosslingual scenario offers a theory-neutral, data-driven solution: Since ambiguity is an idiosyncratic property of a lexical item in a given language, it is not likely to be mirrored in a target language. Similarly, our approach can also be seen as"
W09-2904,2005.mtsummit-papers.11,0,0.0234508,"machine translation, but also for crosslingual induction of morphological, syntactic and semantic analyses (Yarowsky et al., 2001; Dyvik, 2004). In this paper, we propose an approach to the identification of multiword expressions (MWEs) that exploits translational correspondences in a parallel corpus. We will consider in translations of the following type: ¨ (1) Der Rat sollte unsere Position berucksichtigen. The Council should our position consider. (2) The Council should take account of our position. This sentence pair has been taken from the German - English section of the Europarl corpus (Koehn, 2005). It exemplifies a translational correspondence between an English MWE take account of and a German simplex verb ber¨ucksichtigen. In the following, we refer to such correspondences as one-to-many translations. Based on a study of verb translations in Europarl, we will explore to what extent one-to-many translations provide evidence for MWE realization in the target language. It will turn out that crosslingual corre23 Proceedings of the 2009 Workshop on Multiword Expressions, ACL-IJCNLP 2009, pages 23–30, c Suntec, Singapore, 6 August 2009. 2009 ACL and AFNLP one-to-many translations correspon"
W09-2904,nivre-etal-2006-maltparser,0,0.0289514,"from our candidate set. Coordination can be discarded by imposing the condition on the configuration not to contain a coordination relation. This Generate-and-Filter strategy now extracts a set of sentences where we are likely to find a good one-to-one or one-to-many translation for the source verb. behindert X1 Y1 Y2 an X2 to obstacle create Figure 1: Example of a typical syntactic MWE configuration Data We word-aligned the German and English portion of the Europarl corpus by means of the GIZA++ tool. Both portions where assigned flat syntactic dependency analyses by means of the MaltParser (Nivre et al., 2006) such that we obtain a parallel resource of word-aligned dependency parses. Each sentence in our resource can be represented by the triple (DG , DE , AG,E ). DG is the set of dependency triples (s1 , rel, s2 ) such Alignment Post-editing In the final alignment step, one now needs to figure out which lexical material in the aligned syntactic configurations actually corresponds to the translation of the source item. The intuition discussed in 3.2 was that all 27 the items lying on a path between the root item and the terminals belong to the translation of the source item. However, these items ma"
W09-2904,J03-1002,0,0.0164145,"35.7 0.5 17 182 verschlimmern (v4 ) 30.2 21.5 28.6 44.5 275 Table 1: Proportions of types of translational correspondences (token-level) in our gold standard. terns exhibited by one-to-many translations. We constructed a gold standard covering all English translations of four German verb lemmas extracted from the Europarl Corpus. These verbs subcategorize for a nominative subject and an accusative object and are in the middle frequency layer (around 200 occurrences). We extracted all sentences in Europarl with occurences of these lemmas and their automatic word alignments produced by GIZA++ (Och and Ney, 2003). These alignments were manually corrected on the basis of the crosslingual word alignment guidelines developped by (Grac¸a et al., 2008). For each of the German source lemmas, our gold standard records four translation categories: one-to-one, one-to-many, many-to-one, many-tomany translations. Table 1 shows the distribution of these categories for each verb. Strikingly, the four verbs show very different proportions concerning the types of their translational correspondences. Thus, while the German verb anheben (en. increase) seems to have a frequent parallel realization, the verbs bezwecken"
W09-2904,H94-1027,0,0.32763,"source item. However, these items may have other syntactic dependents that may also be part of the one-to-many translation. As an example, consider the configuration in figure 1 where the article an which is part of the LVC create an obstacle to has to be aligned to the German source verb. Thus, for a set of items ti for which there is a dependency relation (tx , rel, ti ) ∈ DE such that tx is an element of our target configuration, we need to decide whether (s1 , ti ) ∈ AG,E . This translation problem now largely parallels collocation translation problems discussed in the literature, as in (Smadja and McKeown, 1994). But, crucially, our syntactic filtering strategy has substantially narrowed down the number of items that are possible parts of the one-to-many translation. Thus, a straightforward way to assemble the translational correspondence is to compute the correlation or association of the possibly missing items with the given translation pair as proposed in (Smadja and McKeown, 1994). Therefore, we propose the following alignment post-editing algorithm: Given the source item s1 and the set of target items T , where each ti ∈ T is an element of our target configuration, The output translation can the"
W09-2904,W06-2405,0,0.308057,"Missing"
W09-2904,J97-3002,0,0.190991,"Missing"
W09-2904,H01-1035,0,0.0371309,"tions that exhibit a correspondence between a single lexical item in the source language and a group of lexical items in the target language. We show that these correspondences can be reliably detected on dependency-parsed, word-aligned sentences. We propose an extraction method that combines word alignment with syntactic filters and is independent of the structural pattern of the translation. 1 Introduction Parallel corpora have proved to be a valuable resource not only for statistical machine translation, but also for crosslingual induction of morphological, syntactic and semantic analyses (Yarowsky et al., 2001; Dyvik, 2004). In this paper, we propose an approach to the identification of multiword expressions (MWEs) that exploits translational correspondences in a parallel corpus. We will consider in translations of the following type: ¨ (1) Der Rat sollte unsere Position berucksichtigen. The Council should our position consider. (2) The Council should take account of our position. This sentence pair has been taken from the German - English section of the Europarl corpus (Koehn, 2005). It exemplifies a translational correspondence between an English MWE take account of and a German simplex verb ber¨"
W09-3831,W06-2920,0,0.0414787,"ich considers both words and tags), scores are now comparable to more mature dependency parsers: ULA excl. punctuation for Arabic is 70.74 for Vine+ROOT+LEFT+RIGHT which is better than six of the systems who participated in the CONLL-X Shared Task and who had access to all data in the treebank, i.e. tokens, lemmas, POS 2 Data Our languages are chosen from different language families. Arabic is a Semitic language, Czech is Slavic, Dutch is Germanic, Italian is Romance, Japanese is Japonic-Ryukyuan, and Turkish is Uralic. All treebanks, except Italian, were also used in the CONLL-X Shared Task (Buchholz and Marsi, 2006). The Italian treebank is the law section of the TUT Treebank used in the Evalita 2007 Dependency Parsing Challenge (Bosco et al., 2000). 3 Experiments The Python/C++ implementation of the maximum entropy-based part-of-speech (POS) tagger first described in Ratnaparkhi (1998) that comes with 207 Arabic ROOT LEFT RIGHT Czech ROOT LEFT RIGHT Dutch ROOT LEFT RIGHT Italian ROOT LEFT RIGHT Japanese ROOT LEFT RIGHT Turkish ROOT LEFT RIGHT Gold 443 3035 313 Gold 737 1485 1288 Gold 522 1734 1300 Gold 100 1601 192 Gold 939 1398 2838 Gold 694 750 3433 Predicted 394 3180 196 Predicted 649 1384 1177 Predi"
W09-3831,2003.mtsummit-papers.6,0,0.125342,"Missing"
W09-3831,W06-2929,0,0.469474,"84.64 84.17 84.78 85.45 Turkish 68.94 68.53 68.45 68.37 69.87 69.79 69.74 Figure 2: Labeled attachment scores (LASs) for MaltParser limited to POS tags, our baseline vine parser (Vine) and our extensions of Vine. Best scores bold-faced. 208 which may be used to hardwire dependency relations into candidate weight matrices. POS taggers may also be used to identify other dependency relations or more fine-grained features that can improve the accuracy of dependency parsers. tags, features and dependency relations; not just the POS tags as in our case. In particular, our result is 2.28 better than Dreyer et al. (2006) who also use soft and hard constraints on dependency lengths. They extend the parsing algorithm in Eisner and Smith (2005) to labeled k-best parsing and use a reranker to find the best parse according to predefined global features. ULA excl. punctuation for Turkish is 67.06 which is better than six of the shared task participants, incl. Dreyer et al. (2006) (60.45). The improvements come at an extremely low cost. The POS tagger simply stores its decisions in a very small table, typically 5–10 cells per sentence, that is queried in no time in parsing. Parsing a standard small test suite takes"
W09-3831,W05-1504,0,0.315097,"Missing"
W09-3831,P09-1087,0,0.118938,"Missing"
W09-3831,P08-2060,0,0.023726,"w in terms of training (hours, sometimes days), and relatively big models are queried in parsing. MSTParser and MaltParser can be optimized for speed in various ways,1 but the many applications of dependency parsers today may turn model size into a serious problem. MSTParser typically takes about a minute to parse a small standard test suite, say 2–300 sentences; the stand-alone version of MaltParser may take 5–8 minutes. Such parsing times are problematic in, say, a machine translation system where for each sentence pair multiple 1 Recent work has optimized MaltParser considerably for speed. Goldberg and Elhadad (2008) speed up the MaltParser by a factor of 30 by simplifying the decision function for the classifiers. Parsing is still considerably slower than with our vine parser, i.e. a test suite is parsed in about 15–20 seconds, whereas our vine parser parses a test suite in less than two seconds. 206 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 206–209, c Paris, October 2009. 2009 Association for Computational Linguistics the maximum entropy library in Zhang (2004) was used to identify arcs to the root node and to tokens immediately left or right of the dependent"
W09-3831,H05-1066,0,0.203303,"Missing"
W09-3831,bosco-etal-2000-building,0,\N,Missing
W09-3831,D07-1096,0,\N,Missing
W10-2106,rohrer-forst-2006-improving,1,0.942795,"Missing"
W10-2106,H01-1035,0,0.15152,"3). parallel corpora and cross-lingual NLP induction techniques. Since adverbs are often overtly marked in other languages (i.e. the ly-suffix in English), adverbial participles can be straightforwadly detected on word-aligned parallel text. We describe the ingretation of the automatically induced resource of adverbial participles into the German LFG, and provide a detailed evaluation of its effect on the grammar, see Section 5. While the use of parallel resources is rather familiar in a wide range of NLP domains, such as statistical machine translation (Koehn, 2005) or annotation projection (Yarowsky et al., 2001), our work shows that they can be exploited for very specific problems that arise in deep linguistic analysis (see Section 4). In this way, highprecision, data-oriented induction techniques can clearly improve rule-based system development through combining the benefits of high empirical accuracy and little manual effort. 2 3 Participles in the German LFG 3.1 Analysis The morphosyntactic ambiguity of German participles presents a notorious difficulty for theoretical and computational analysis. The reason is that adjectives (i.e. adjectival participles) do not only occur as attributive modifier"
W10-2106,W02-1503,1,0.901049,"ller et al., 1995) assigns the tag “ADJD” to predicative adjectives as well as adverbs. A Broad-Coverage LFG for German Lexical Functional Grammar (LFG) (Bresnan, 2000) is a constraint-based theory of grammar. It posits two levels of representation, c(onstituent)structure and f(unctional)- structure. C-structure is represented by contextfree phrase-structure trees, and captures surface grammatical configurations. F-structures approximate basic predicateargument and adjunct structures. The experiments reported in this paper use the German LFG grammar constructed as part of the ParGram project (Butt et al., 2002). The grammar is implemented in the XLE, a grammar development environment which includes a very efficient LFG parser. Within the spectrum of appraoches to natural language parsing, XLE can be considered a hybrid system combining a hand-crafted grammar with a number of automatic ambiguity management techniques: (i) c-structure pruning where, based on information from statstically obtained parses, some trees are ruled out before fstructure unification (Cahill et al., 2007), (ii) an Optimaly Theory-style constraint mechanism for filtering and ranking competing analyses (Frank et al., 2001), and"
W10-2106,2005.mtsummit-papers.11,0,0.00413936,"ic morphological tags, see (Dipper, 2003). parallel corpora and cross-lingual NLP induction techniques. Since adverbs are often overtly marked in other languages (i.e. the ly-suffix in English), adverbial participles can be straightforwadly detected on word-aligned parallel text. We describe the ingretation of the automatically induced resource of adverbial participles into the German LFG, and provide a detailed evaluation of its effect on the grammar, see Section 5. While the use of parallel resources is rather familiar in a wide range of NLP domains, such as statistical machine translation (Koehn, 2005) or annotation projection (Yarowsky et al., 2001), our work shows that they can be exploited for very specific problems that arise in deep linguistic analysis (see Section 4). In this way, highprecision, data-oriented induction techniques can clearly improve rule-based system development through combining the benefits of high empirical accuracy and little manual effort. 2 3 Participles in the German LFG 3.1 Analysis The morphosyntactic ambiguity of German participles presents a notorious difficulty for theoretical and computational analysis. The reason is that adjectives (i.e. adjectival parti"
W10-2106,nivre-etal-2006-maltparser,0,0.107237,"Missing"
W10-2106,J03-1002,0,0.00381894,"erbial translation for other reasons. A typical configuration is exemplified in (6) where the German main verb vorlegen is translated as the verb-adverb combination put forward. c. Apr`es l’ e´ largissement a` l’ Est, la tendance sera davantage a` la lib´eralisation. In the following, we describe experiments on Europarl where we automatically extract and filter adverbially translated German participles. 4.1 Data We base our experiments on the German, English, French and Dutch part of the Europarl corpus. We automatically word-aligned the German part to each of the others with the GIZA++ tool (Och and Ney, 2003). Note that, due to divergences in sentence alignment and tokenisation, the three word-alignments are not completely synchronised. Moreover, each of the 4 languages has been automatically PoS tagged using the TreeTagger (Schmid, 1994). In addition, the German and English parts have been parsed with MaltParser (Nivre et al., 2006). Since we want to limit our investigation to those participles that are not already recorded as lexicalised adjective or adverb in the DMOR morphology, we first have to generate the set of participle candidates from the tagged Europarl data. We extract all distinct wo"
W10-2106,P02-1035,0,0.169128,"ry efficient LFG parser. Within the spectrum of appraoches to natural language parsing, XLE can be considered a hybrid system combining a hand-crafted grammar with a number of automatic ambiguity management techniques: (i) c-structure pruning where, based on information from statstically obtained parses, some trees are ruled out before fstructure unification (Cahill et al., 2007), (ii) an Optimaly Theory-style constraint mechanism for filtering and ranking competing analyses (Frank et al., 2001), and (iii) a stochastic disambiguation component which is based on a log-linear probability model (Riezler et al., 2002) and works on the packed representations. The German LFG grammar integrates a morphological component which is a variant of (2) a. Das Experiment hat ihn begeistert. ‘The experiment has enthused him.’ b. Er scheint von dem Experiment begeistert. ‘He seems enthusiastic about the experiment.’ c. Er hat begeistert experimentiert. ‘He has experimented in an enthusiastic way’ or: ‘He was enthusiastic when he experimented.’ For performance reasons, the German LFG does not cover free predicatives at the moment. In the context of our crosslingual induction approach, the distinction between predicative"
W10-2106,W08-1705,1,\N,Missing
W11-2908,W09-1201,0,0.0666967,"Missing"
W11-2908,W09-1210,0,0.0581009,"ases. However, all the differences in f-score between the three models turn out to be statistically significant,5 so there seems to be a small number of cases where morphology can help in disambiguation. Noun phrases like in Example 1 illustrate these cases where, in principle, arbitrarily many phrases can appear between the determiner and the head noun. Evaluation We evaluate the data-driven dependency parser described in Bohnet (2010), a state-of-the-art second-order maximum spanning tree parser performing second on German in the CoNLL 2009 Shared Task. The parser uses a rich feature model (Bohnet, 2009) and is fully lexicalized. We used statistical tools4 to automatically lemmatize, partof-speech tag, and morphologically annotate the training section (36k sentences) of the data by using ten-fold cross annotation. We parsed the whole corpus creating three different models: one using the gold morphology, one using predicted morphology, and one using no explicit morphology. Morphological information was represented as in the CoNLL 2009 Shared Task. In the following two experiments we test, whether the parser does need explicit morphological information to correctly recover noun phrases and thei"
W11-2908,W06-1614,0,0.0350015,"Missing"
W11-2908,W08-1006,0,0.0177835,"ompared to Czech or other morphologically rich languages, the morphological system is less elaborate and characterized by a large amount of form syncretism, which introduces a lot of ambiguity. A lot of work investigated the best way to utilize morphological information in statistical PCFG parsers for German, mostly by transforming the treebank making morphological information more accessible (Schiehlen, 2004; Dubey, 2005). Lexicalization of PCFGs has been a controversial subject of research in German, where some found no effect (Dubey and Keller, 2003) while others did (K¨ubler et al., 2006; Rafferty and Manning, 2008). However, this work concentrated on constituent parsing. While there are many parsing results of dependency parsers on German (Buchholz and Marsi, 2006; K¨ubler, 2008; Hajiˇc et al., 2009), the investigation of morphological representations and their interplay with dependency parsing algorithms has been started only recently (cf. Tsarfaty ¨ Ol ART+nom/acc.sg.fem NN+acc.sg.neut oil the verarbeitende Industrie ADJ+nom/acc.sg.fem NN+nom/acc.sg.fem processing industry ’the oil processing industry’ (1) die Example 1 shows a German noun phrase consisting of a determiner (die), an adjective (verarbe"
W11-2908,W06-2920,0,0.0575195,", which introduces a lot of ambiguity. A lot of work investigated the best way to utilize morphological information in statistical PCFG parsers for German, mostly by transforming the treebank making morphological information more accessible (Schiehlen, 2004; Dubey, 2005). Lexicalization of PCFGs has been a controversial subject of research in German, where some found no effect (Dubey and Keller, 2003) while others did (K¨ubler et al., 2006; Rafferty and Manning, 2008). However, this work concentrated on constituent parsing. While there are many parsing results of dependency parsers on German (Buchholz and Marsi, 2006; K¨ubler, 2008; Hajiˇc et al., 2009), the investigation of morphological representations and their interplay with dependency parsing algorithms has been started only recently (cf. Tsarfaty ¨ Ol ART+nom/acc.sg.fem NN+acc.sg.neut oil the verarbeitende Industrie ADJ+nom/acc.sg.fem NN+nom/acc.sg.fem processing industry ’the oil processing industry’ (1) die Example 1 shows a German noun phrase consisting of a determiner (die), an adjective (verarbeitende), and a noun (Industrie). Additionally, ¨ is an argument of the adjective. Withthe noun Ol out morphological information we might in principle be"
W11-2908,C04-1056,0,0.218082,"or languages like English. It does show phenomena that are typical for morphologically rich languages, e. g. a rich nominal and verbal inflection system and hence a relatively free word order. However, compared to Czech or other morphologically rich languages, the morphological system is less elaborate and characterized by a large amount of form syncretism, which introduces a lot of ambiguity. A lot of work investigated the best way to utilize morphological information in statistical PCFG parsers for German, mostly by transforming the treebank making morphological information more accessible (Schiehlen, 2004; Dubey, 2005). Lexicalization of PCFGs has been a controversial subject of research in German, where some found no effect (Dubey and Keller, 2003) while others did (K¨ubler et al., 2006; Rafferty and Manning, 2008). However, this work concentrated on constituent parsing. While there are many parsing results of dependency parsers on German (Buchholz and Marsi, 2006; K¨ubler, 2008; Hajiˇc et al., 2009), the investigation of morphological representations and their interplay with dependency parsing algorithms has been started only recently (cf. Tsarfaty ¨ Ol ART+nom/acc.sg.fem NN+acc.sg.neut oil"
W11-2908,P03-1013,0,0.0341484,"ection system and hence a relatively free word order. However, compared to Czech or other morphologically rich languages, the morphological system is less elaborate and characterized by a large amount of form syncretism, which introduces a lot of ambiguity. A lot of work investigated the best way to utilize morphological information in statistical PCFG parsers for German, mostly by transforming the treebank making morphological information more accessible (Schiehlen, 2004; Dubey, 2005). Lexicalization of PCFGs has been a controversial subject of research in German, where some found no effect (Dubey and Keller, 2003) while others did (K¨ubler et al., 2006; Rafferty and Manning, 2008). However, this work concentrated on constituent parsing. While there are many parsing results of dependency parsers on German (Buchholz and Marsi, 2006; K¨ubler, 2008; Hajiˇc et al., 2009), the investigation of morphological representations and their interplay with dependency parsing algorithms has been started only recently (cf. Tsarfaty ¨ Ol ART+nom/acc.sg.fem NN+acc.sg.neut oil the verarbeitende Industrie ADJ+nom/acc.sg.fem NN+nom/acc.sg.fem processing industry ’the oil processing industry’ (1) die Example 1 shows a German"
W11-2908,W10-1401,0,0.161694,"Missing"
W11-2908,P05-1039,0,0.0182198,"English. It does show phenomena that are typical for morphologically rich languages, e. g. a rich nominal and verbal inflection system and hence a relatively free word order. However, compared to Czech or other morphologically rich languages, the morphological system is less elaborate and characterized by a large amount of form syncretism, which introduces a lot of ambiguity. A lot of work investigated the best way to utilize morphological information in statistical PCFG parsers for German, mostly by transforming the treebank making morphological information more accessible (Schiehlen, 2004; Dubey, 2005). Lexicalization of PCFGs has been a controversial subject of research in German, where some found no effect (Dubey and Keller, 2003) while others did (K¨ubler et al., 2006; Rafferty and Manning, 2008). However, this work concentrated on constituent parsing. While there are many parsing results of dependency parsers on German (Buchholz and Marsi, 2006; K¨ubler, 2008; Hajiˇc et al., 2009), the investigation of morphological representations and their interplay with dependency parsing algorithms has been started only recently (cf. Tsarfaty ¨ Ol ART+nom/acc.sg.fem NN+acc.sg.neut oil the verarbeite"
W11-2908,W08-1008,0,\N,Missing
W13-2708,C10-1011,0,0.0287056,"d a system to identify direct and indirect speech and a sentiment system to determine the orientation of the statement. These systems in turn need various kinds of preprocessing starting from tokenization over syntactic parsing up to coreference resolution. The Complex Concept Builder is the collection of all these systems with the goal to assist the political scientists. So far, the Complex Concept Builder implements tokenization (Schmid, 2009), lemmatisation (Schmid, 1995), part-of-speech tagging (Schmid and Laws, 2008), named entity detection (Faruqui and Pad´o, 2010), syntactical parsing (Bohnet, 2010), coreference analysis for German (Lappin and Leass, 1994; Stuckardt, 2001), relation extraction (Blessing et al., 2012) and sentiment analysis for English (Taboada et al., 2011). It is important for a researcher of the humanities to be able to adapt existing classification systems according to his own needs. A common procedure in both, NLP and political sciences, is to annotate data. Therefore, one major goal of the project and the Complex Concept Builder is to provide machine learning systems with a wide range of possible features — including high level information like sentiment, text type,"
W13-2708,J96-2004,0,0.103544,"ists as the separation of metadata and 6 Conclusion and Outlook We developed and implemented a pipeline of various text processing tools which is designed to assist political scientists in finding specific, complex concepts within large amounts of text. Our case studies showed that our approach can provide beneficial assistance for the research of political scientists as well as researcher from other social sciences and the humanities. A future aspect will be to find metrics to evaluate our pipeline. In recently started annotation experiments on topic classification Cohen’s kappa coefficient (Carletta, 1996) is mediocre. It may very well be possible that the complex concepts, like multiple collective identities, are intrinsically hard to detect, and the annotations cannot be improved substantially. The extension of the NLP pipeline will be another major working area in the future. Examples are sentiment analysis for German, adding world knowledge about named entities (e.g. persons and events), identification of relations between entities. Finally, all these systems need to be evaluated not only in terms of f-score, precision and recall, but also in terms of usability for the political scientists."
W13-2708,C08-1098,0,0.0274623,"tician who tries to rally support for his political party. In order to detect such text, we need a system to identify direct and indirect speech and a sentiment system to determine the orientation of the statement. These systems in turn need various kinds of preprocessing starting from tokenization over syntactic parsing up to coreference resolution. The Complex Concept Builder is the collection of all these systems with the goal to assist the political scientists. So far, the Complex Concept Builder implements tokenization (Schmid, 2009), lemmatisation (Schmid, 1995), part-of-speech tagging (Schmid and Laws, 2008), named entity detection (Faruqui and Pad´o, 2010), syntactical parsing (Bohnet, 2010), coreference analysis for German (Lappin and Leass, 1994; Stuckardt, 2001), relation extraction (Blessing et al., 2012) and sentiment analysis for English (Taboada et al., 2011). It is important for a researcher of the humanities to be able to adapt existing classification systems according to his own needs. A common procedure in both, NLP and political sciences, is to annotate data. Therefore, one major goal of the project and the Complex Concept Builder is to provide machine learning systems with a wide ra"
W13-2708,N12-1066,0,0.0162412,"pic filter Concept detection Web-based Userinterface Data cleaning is important for the data-driven studies. Not only duplicate articles have a negative impact, also articles which are not of interest for the given topic have to be filtered out. There are different approaches to classify articles into a range of predefined topics. In the last years LDA (Blei et al., 2003; Niekler and J¨ahnichen, 2012) is one of the most successful methods to find topics in articles. But for social scientists the categories typically used in LDA are not sufficient. We follow the idea of Dualist (Settles, 2011; Settles and Zhu, 2012) which is an interactive method for classification. The architecture of Dualist is based on MALLET (McCallum, 2002) which is easily integrable into our architecture. Our goal is to design the correct feature to find relevant articles for a given topic. Word features are not sufficient since we have to model more complex features (cf. Section 2.1). Figure 2: Overview of the complete processing chain. We split the workflow for the user into two parts: The first part is only used if the user imports new data into the repository. For that he can use the exploration workbench (Section 3.1). Secondl"
W13-2708,P10-4005,0,0.0309161,"ation is gathered and visualised as well. Major differences between the EMM and our approach are the user group and the domain of the corpus. The complex concepts political scientists are interested in are much more nuanced than the concepts relevant for topic detection and the construction of social networks. Additionally, the EMM does not allow its users to look for their own concepts and issues, while this interactivity is a central contribution of our approach (cf. Sections 1, 2.1 and 3.2). The CLARIN-D project also provides a webbased platform to create NLP-chains. It is called WebLicht (Hinrichs et al., 2010), but in its current form, the tool is not immediately usable for social scientists as the separation of metadata and 6 Conclusion and Outlook We developed and implemented a pipeline of various text processing tools which is designed to assist political scientists in finding specific, complex concepts within large amounts of text. Our case studies showed that our approach can provide beneficial assistance for the research of political scientists as well as researcher from other social sciences and the humanities. A future aspect will be to find metrics to evaluate our pipeline. In recently sta"
W13-2708,D11-1136,0,0.0485983,"cept Builder Topic filter Concept detection Web-based Userinterface Data cleaning is important for the data-driven studies. Not only duplicate articles have a negative impact, also articles which are not of interest for the given topic have to be filtered out. There are different approaches to classify articles into a range of predefined topics. In the last years LDA (Blei et al., 2003; Niekler and J¨ahnichen, 2012) is one of the most successful methods to find topics in articles. But for social scientists the categories typically used in LDA are not sufficient. We follow the idea of Dualist (Settles, 2011; Settles and Zhu, 2012) which is an interactive method for classification. The architecture of Dualist is based on MALLET (McCallum, 2002) which is easily integrable into our architecture. Our goal is to design the correct feature to find relevant articles for a given topic. Word features are not sufficient since we have to model more complex features (cf. Section 2.1). Figure 2: Overview of the complete processing chain. We split the workflow for the user into two parts: The first part is only used if the user imports new data into the repository. For that he can use the exploration workbenc"
W13-2708,W04-0213,1,0.689214,"ally by the political scientists. The following are further applications of the identified indirect speeches a) using the frequency of speeches per text as a feature for classification; e.g. a classification system for news reports/commentaries as described in Section 4.4 b) a project-goal is to find texts in which collective A useful distinction for political scientists dealing with newspaper articles is the distinction between articles that report objectively on events or backgrounds and editorials or press commentaries. We first extracted opinionated and objective texts from DeReKo corpus (Stede, 2004; Kupietz et al., 2010). Some texts were removed in order to balance the corpus. The balanced corpus contains 2848 documents and has been split into a development and a training and test set. 570 documents were used for the manual creation of features. The remaining 2278 documents were used to train and evaluate classifiers using 10-fold cross-validation with the WEKA machine learning toolkit (Hall et al., 2009) and various classifiers (cf. Table 1). The challenge is that the newspaper articles from the training and evaluation corpus come from different newspapers and, of course, from differen"
W13-2708,kupietz-etal-2010-german,0,0.0200728,"olitical scientists. The following are further applications of the identified indirect speeches a) using the frequency of speeches per text as a feature for classification; e.g. a classification system for news reports/commentaries as described in Section 4.4 b) a project-goal is to find texts in which collective A useful distinction for political scientists dealing with newspaper articles is the distinction between articles that report objectively on events or backgrounds and editorials or press commentaries. We first extracted opinionated and objective texts from DeReKo corpus (Stede, 2004; Kupietz et al., 2010). Some texts were removed in order to balance the corpus. The balanced corpus contains 2848 documents and has been split into a development and a training and test set. 570 documents were used for the manual creation of features. The remaining 2278 documents were used to train and evaluate classifiers using 10-fold cross-validation with the WEKA machine learning toolkit (Hall et al., 2009) and various classifiers (cf. Table 1). The challenge is that the newspaper articles from the training and evaluation corpus come from different newspapers and, of course, from different authors. Commentaries"
W13-2708,J01-4002,0,0.0247788,"to determine the orientation of the statement. These systems in turn need various kinds of preprocessing starting from tokenization over syntactic parsing up to coreference resolution. The Complex Concept Builder is the collection of all these systems with the goal to assist the political scientists. So far, the Complex Concept Builder implements tokenization (Schmid, 2009), lemmatisation (Schmid, 1995), part-of-speech tagging (Schmid and Laws, 2008), named entity detection (Faruqui and Pad´o, 2010), syntactical parsing (Bohnet, 2010), coreference analysis for German (Lappin and Leass, 1994; Stuckardt, 2001), relation extraction (Blessing et al., 2012) and sentiment analysis for English (Taboada et al., 2011). It is important for a researcher of the humanities to be able to adapt existing classification systems according to his own needs. A common procedure in both, NLP and political sciences, is to annotate data. Therefore, one major goal of the project and the Complex Concept Builder is to provide machine learning systems with a wide range of possible features — including high level information like sentiment, text type, relations to other texts, etc. — that can be used by non-experts for semia"
W13-2708,J11-2001,1,0.14676,"cessing starting from tokenization over syntactic parsing up to coreference resolution. The Complex Concept Builder is the collection of all these systems with the goal to assist the political scientists. So far, the Complex Concept Builder implements tokenization (Schmid, 2009), lemmatisation (Schmid, 1995), part-of-speech tagging (Schmid and Laws, 2008), named entity detection (Faruqui and Pad´o, 2010), syntactical parsing (Bohnet, 2010), coreference analysis for German (Lappin and Leass, 1994; Stuckardt, 2001), relation extraction (Blessing et al., 2012) and sentiment analysis for English (Taboada et al., 2011). It is important for a researcher of the humanities to be able to adapt existing classification systems according to his own needs. A common procedure in both, NLP and political sciences, is to annotate data. Therefore, one major goal of the project and the Complex Concept Builder is to provide machine learning systems with a wide range of possible features — including high level information like sentiment, text type, relations to other texts, etc. — that can be used by non-experts for semiautomatic annotation and text selection. Active learning is used to provide immediate results that 3.3 I"
W13-2708,J94-4002,0,\N,Missing
W13-3704,W10-1403,0,0.0607378,"tion as features in parsing has been a commonly used method for MRLs (Tsarfaty et al., 2010). The effect is controversial: in some cases gold morphology clearly (1) Kahveleri bende içelim a.kahve+Noun+A3pl+Pnon+Acc b.kahve+Noun+A3pl+P3sg+Nom c.kahve+Noun+A3sg+P3pl+Nom d.kahve+Noun+A3pl+P3pl+Nom e.ben+PersP+A1sg+Pnon+Loc f.ben+Noun+A3sg+Pnon+Loc g.bent+Noun+A3sg+Pnon+Dat h.bende+Noun+A3sg+Pnon+Nom iç+Verb+Pos+Opt+A1pl helps, in others its impact is little. For some settings predicted information causes a drop, for some settings a partial set of morphological features improves parsing accuracy. Ambati et al. (2010) explore ways of integrating local morphosyntactic features into Hindi dependency parsing. They experiment with different sets of features both on a graph-based and a transition-based dependency parser. Both with gold and predicted settings using morphological features root, case, and suffix outperform using POS as the only feature. Bengoetxea and Gojenola (2010) utilise the CoNLL-X format and MaltParser’s feature configuration file to take advantage of morphological features in parsing Basque with gold data. Their experiments show that case and subordination type increase parsing accuracy. Ma"
W13-3704,W10-1404,0,0.0937912,"oun+A3sg+Pnon+Dat h.bende+Noun+A3sg+Pnon+Nom iç+Verb+Pos+Opt+A1pl helps, in others its impact is little. For some settings predicted information causes a drop, for some settings a partial set of morphological features improves parsing accuracy. Ambati et al. (2010) explore ways of integrating local morphosyntactic features into Hindi dependency parsing. They experiment with different sets of features both on a graph-based and a transition-based dependency parser. Both with gold and predicted settings using morphological features root, case, and suffix outperform using POS as the only feature. Bengoetxea and Gojenola (2010) utilise the CoNLL-X format and MaltParser’s feature configuration file to take advantage of morphological features in parsing Basque with gold data. Their experiments show that case and subordination type increase parsing accuracy. Marton et al. (2010) explore which morphological features could be useful in dependency parsing of Arabic. They observe the effect of features by adding them one at a time separately and comparing the outcomes. Experiments show that when gold morphology is provided, case markers help the most, whereas when the morphology is automatically predicted the outcome is th"
W13-3704,D12-1133,0,0.0446634,"ecause of the lack of a multiword extractor. Hence the experiments are not in a fully predicted setting. 24 ning, 2010) and for Hebrew (Goldberg and Tsarfaty, 2008). On the dependency parsing front, Lee et al. (2011) introduces a joint morphological disambiguation and dependency parsing architecture which proves to outperform their pipeline architecture for Latin, Ancient Greek, Czech, and Hungarian. However it is limited to unlabelled dependency parsing and initial scores are below the state-of-the-art. On the other hand, parsers that can jointly POS tag become more common in the last years (Bohnet and Nivre, 2012; Hatori et al., 2011; Li et al., 2011). Bohnet and Nivre (2012) propose a joint POS tagger and labelled dependency parser that outperforms the pipeline results and also improves the state-of-the-art accuracy for German, Czech, English, and Chinese. Joint POS tagger and dependency parsers are not originally designed for predicting morphological features, but they provide a flexible field (POS) where the parser is not dependent on the morphological disambiguator decisions. So the use of this field can actually be extended to accommodating morphological features instead of or in addition to POS"
W13-3704,C10-1011,0,0.0596389,"Missing"
W13-3704,W06-2920,0,0.31682,"alysis are available for analysing Turkish raw text input at the word level, and in work on the Turkish Dependency Treebank (Oflazer et al., 2003), a representation scheme has been developed that captures the peculiarities at the morphology-syntax interface in a dependency format that is formally compatible with the standard CoNLL dependency format. So, it might seem as if all Turkish-specific challenges have been resolved, and only languageindependent data-driven methods are required from now on (after all, the Turkish Dependency Treebank was included in the CoNLL 2006 and 2007 Shared Tasks (Buchholz and Marsi, 2006; Nivre et al., 2007), and several researchers working on language-independent methods have reported scores on the available data). 23 However, Turkish still causes a considerable architectural challenge for the standard pipeline architecture used in data-driven dependency parsing: the dependency treebank scheme for Turkish is based on segments that are not identical to the words from the raw text input, but are often sublexical units that form parts of morphological derivations. While it is straightforward to train data-driven parsers on the gold standard segmentation from the treebank (which"
W13-3704,W11-3802,0,0.0364662,"Missing"
W13-3704,eryigit-2012-impact,0,0.437707,"Missing"
W13-3704,J08-3003,0,0.411037,"Missing"
W13-3704,W09-3819,0,0.100244,"heir no-feature baseline, and also conduct a second set of experiments where they remove one feature at a time from the whole feature set. They also conclude that leaving out the predicted case improved the parsing most among the possible candidates to remove, this time for constituency parsing. In the single feature experiments, genitive clitics help the most. The optimal combination they achieve consists of the features determiner, proper noun, genitive clitics, and negation. 25 Another Semitic language that is studied within the MRLs is Hebrew. Initial results on Hebrew dependency parsing (Goldberg and Elhadad, 2009) show predicted morphological features help in a transition-based parser with a tailored feature configuration file, although scores drop in a graphbased parser. The same authors later prove both gold and predicted agreement features improve accuracy for an easy-first, non-directional dependency parser (Goldberg and Elhadad, 2010). Tsarfaty and Sima’an (2010) report agreement features are useful also for constituency parsing when they extend the Relational Realisational (Tsarfaty and Sima’an, 2008) models with this information. Seeker and Kuhn (2011) focus on the internal structures and gramma"
W13-3704,W10-1412,0,0.0530458,"genitive clitics help the most. The optimal combination they achieve consists of the features determiner, proper noun, genitive clitics, and negation. 25 Another Semitic language that is studied within the MRLs is Hebrew. Initial results on Hebrew dependency parsing (Goldberg and Elhadad, 2009) show predicted morphological features help in a transition-based parser with a tailored feature configuration file, although scores drop in a graphbased parser. The same authors later prove both gold and predicted agreement features improve accuracy for an easy-first, non-directional dependency parser (Goldberg and Elhadad, 2010). Tsarfaty and Sima’an (2010) report agreement features are useful also for constituency parsing when they extend the Relational Realisational (Tsarfaty and Sima’an, 2008) models with this information. Seeker and Kuhn (2011) focus on the internal structures and grammatical functions of German noun phrases. Their experiments show grammatical functions are predicted with higher accuracy when a graph-based dependency parser is provided with both gold and predicted case markers. They further explore the effects of using case in dependency parsing, this time for Czech and Hungarian as well as for G"
W13-3704,P08-1043,0,0.0609138,"s on morphological disambiguation and parsing. It has been shown that such an architecture improves constituency parsing accuracy both for Arabic (Green and Man1 A3pl: 3rd personal plural agreement, A3sg: 3rd personal singular agreement, Pnon: no possessives, P3sg: 3rd personal singular possessive, P3pl: 3rd personal plural possessive, Nom: Nominative, Acc: Accusative, Loc: Locative, Dat: Dative, Zero: No overt derivation, Pos: Positive, Opt: Optative mood 2 because of the lack of a multiword extractor. Hence the experiments are not in a fully predicted setting. 24 ning, 2010) and for Hebrew (Goldberg and Tsarfaty, 2008). On the dependency parsing front, Lee et al. (2011) introduces a joint morphological disambiguation and dependency parsing architecture which proves to outperform their pipeline architecture for Latin, Ancient Greek, Czech, and Hungarian. However it is limited to unlabelled dependency parsing and initial scores are below the state-of-the-art. On the other hand, parsers that can jointly POS tag become more common in the last years (Bohnet and Nivre, 2012; Hatori et al., 2011; Li et al., 2011). Bohnet and Nivre (2012) propose a joint POS tagger and labelled dependency parser that outperforms th"
W13-3704,C10-1045,0,0.110666,"Missing"
W13-3704,I11-1136,0,0.0287636,"multiword extractor. Hence the experiments are not in a fully predicted setting. 24 ning, 2010) and for Hebrew (Goldberg and Tsarfaty, 2008). On the dependency parsing front, Lee et al. (2011) introduces a joint morphological disambiguation and dependency parsing architecture which proves to outperform their pipeline architecture for Latin, Ancient Greek, Czech, and Hungarian. However it is limited to unlabelled dependency parsing and initial scores are below the state-of-the-art. On the other hand, parsers that can jointly POS tag become more common in the last years (Bohnet and Nivre, 2012; Hatori et al., 2011; Li et al., 2011). Bohnet and Nivre (2012) propose a joint POS tagger and labelled dependency parser that outperforms the pipeline results and also improves the state-of-the-art accuracy for German, Czech, English, and Chinese. Joint POS tagger and dependency parsers are not originally designed for predicting morphological features, but they provide a flexible field (POS) where the parser is not dependent on the morphological disambiguator decisions. So the use of this field can actually be extended to accommodating morphological features instead of or in addition to POS tags, which gives par"
W13-3704,P11-1089,0,0.0190755,"n that such an architecture improves constituency parsing accuracy both for Arabic (Green and Man1 A3pl: 3rd personal plural agreement, A3sg: 3rd personal singular agreement, Pnon: no possessives, P3sg: 3rd personal singular possessive, P3pl: 3rd personal plural possessive, Nom: Nominative, Acc: Accusative, Loc: Locative, Dat: Dative, Zero: No overt derivation, Pos: Positive, Opt: Optative mood 2 because of the lack of a multiword extractor. Hence the experiments are not in a fully predicted setting. 24 ning, 2010) and for Hebrew (Goldberg and Tsarfaty, 2008). On the dependency parsing front, Lee et al. (2011) introduces a joint morphological disambiguation and dependency parsing architecture which proves to outperform their pipeline architecture for Latin, Ancient Greek, Czech, and Hungarian. However it is limited to unlabelled dependency parsing and initial scores are below the state-of-the-art. On the other hand, parsers that can jointly POS tag become more common in the last years (Bohnet and Nivre, 2012; Hatori et al., 2011; Li et al., 2011). Bohnet and Nivre (2012) propose a joint POS tagger and labelled dependency parser that outperforms the pipeline results and also improves the state-of-th"
W13-3704,W11-2908,1,0.844908,"results on Hebrew dependency parsing (Goldberg and Elhadad, 2009) show predicted morphological features help in a transition-based parser with a tailored feature configuration file, although scores drop in a graphbased parser. The same authors later prove both gold and predicted agreement features improve accuracy for an easy-first, non-directional dependency parser (Goldberg and Elhadad, 2010). Tsarfaty and Sima’an (2010) report agreement features are useful also for constituency parsing when they extend the Relational Realisational (Tsarfaty and Sima’an, 2008) models with this information. Seeker and Kuhn (2011) focus on the internal structures and grammatical functions of German noun phrases. Their experiments show grammatical functions are predicted with higher accuracy when a graph-based dependency parser is provided with both gold and predicted case markers. They further explore the effects of using case in dependency parsing, this time for Czech and Hungarian as well as for German (Seeker and Kuhn, 2013). On a graph-based parser German does not benefit much from using predicted morphology but Czech and Hungarian clearly profit. They also use case as a constraint on integer linear programming (IL"
W13-3704,J13-1004,1,0.838502,"aty and Sima’an (2010) report agreement features are useful also for constituency parsing when they extend the Relational Realisational (Tsarfaty and Sima’an, 2008) models with this information. Seeker and Kuhn (2011) focus on the internal structures and grammatical functions of German noun phrases. Their experiments show grammatical functions are predicted with higher accuracy when a graph-based dependency parser is provided with both gold and predicted case markers. They further explore the effects of using case in dependency parsing, this time for Czech and Hungarian as well as for German (Seeker and Kuhn, 2013). On a graph-based parser German does not benefit much from using predicted morphology but Czech and Hungarian clearly profit. They also use case as a constraint on integer linear programming (ILP) parsing models to filter out ungrammatical case-function mappings. For all three languages, the constrained models outperform the unconstrained models and graph-based parser in predicting core grammatical functions. The research discussed in this section show case and agreement are among the most investigated features, and most of the time they are among the most beneficial ones. These are the featu"
W13-3704,D07-1099,0,0.0566232,"Missing"
W13-3704,C08-1112,0,0.0516512,"Missing"
W13-3704,W10-1405,0,0.0357771,"Missing"
W13-3704,D11-1109,0,0.0446181,"Hence the experiments are not in a fully predicted setting. 24 ning, 2010) and for Hebrew (Goldberg and Tsarfaty, 2008). On the dependency parsing front, Lee et al. (2011) introduces a joint morphological disambiguation and dependency parsing architecture which proves to outperform their pipeline architecture for Latin, Ancient Greek, Czech, and Hungarian. However it is limited to unlabelled dependency parsing and initial scores are below the state-of-the-art. On the other hand, parsers that can jointly POS tag become more common in the last years (Bohnet and Nivre, 2012; Hatori et al., 2011; Li et al., 2011). Bohnet and Nivre (2012) propose a joint POS tagger and labelled dependency parser that outperforms the pipeline results and also improves the state-of-the-art accuracy for German, Czech, English, and Chinese. Joint POS tagger and dependency parsers are not originally designed for predicting morphological features, but they provide a flexible field (POS) where the parser is not dependent on the morphological disambiguator decisions. So the use of this field can actually be extended to accommodating morphological features instead of or in addition to POS tags, which gives parsers an opportunit"
W13-3704,W10-1401,0,0.077084,"ve the parsing accuracy statistically significantly. The paper is structured as follows: Section 2 gives an overview on how morphological features are used in parsing MRLs. Section 3 explains the morphological analysis representation and its relation with segmentation. Section 4 describes the use of morphological features in joint parsing experiments. The setup for experiments are given in Section 5 and results are discussed in Section 6. We conclude with Section 7. 2 Use of Morphological Features Using morphological information as features in parsing has been a commonly used method for MRLs (Tsarfaty et al., 2010). The effect is controversial: in some cases gold morphology clearly (1) Kahveleri bende içelim a.kahve+Noun+A3pl+Pnon+Acc b.kahve+Noun+A3pl+P3sg+Nom c.kahve+Noun+A3sg+P3pl+Nom d.kahve+Noun+A3pl+P3pl+Nom e.ben+PersP+A1sg+Pnon+Loc f.ben+Noun+A3sg+Pnon+Loc g.bent+Noun+A3sg+Pnon+Dat h.bende+Noun+A3sg+Pnon+Nom iç+Verb+Pos+Opt+A1pl helps, in others its impact is little. For some settings predicted information causes a drop, for some settings a partial set of morphological features improves parsing accuracy. Ambati et al. (2010) explore ways of integrating local morphosyntactic features into Hindi d"
W13-3704,W10-1402,0,0.018246,"0) explore ways of integrating local morphosyntactic features into Hindi dependency parsing. They experiment with different sets of features both on a graph-based and a transition-based dependency parser. Both with gold and predicted settings using morphological features root, case, and suffix outperform using POS as the only feature. Bengoetxea and Gojenola (2010) utilise the CoNLL-X format and MaltParser’s feature configuration file to take advantage of morphological features in parsing Basque with gold data. Their experiments show that case and subordination type increase parsing accuracy. Marton et al. (2010) explore which morphological features could be useful in dependency parsing of Arabic. They observe the effect of features by adding them one at a time separately and comparing the outcomes. Experiments show that when gold morphology is provided, case markers help the most, whereas when the morphology is automatically predicted the outcome is the opposite: using case harms the results the most. When features are combined in a greedy heuristic, using definiteness, person, number, and gender information improves accuracy. To overcome the exhaustive feature space problem of Arabic, Dehdari et al."
W13-3704,P12-2002,0,0.114057,"Missing"
W13-3704,W11-3806,0,\N,Missing
W13-3704,J08-4010,0,\N,Missing
W13-3704,P13-1054,1,\N,Missing
W13-3704,D07-1096,0,\N,Missing
W15-0706,P14-1035,0,0.0230183,"its translation to English. As research in the field of statistical machine translation has shown, word alignments of surprisingly good quality can be induced automatically exploiting co-occurrence statistics, given a sufficient amount of parallel text (from a reasonably homogeneous corpus of translated texts). In this pilot study, we present a manually annotated reference word alignment and use this to assess the principled usefulness of word alignment as an additional information source for the (monolingually motivated) task of identifying mentions of the same literary character in texts ((Bamman et al., 2014)). This is a very important analytical sub-step for further analysis (e.g., network analysis, event recognition for narratological analysis, stylistic analysis of character speech etc.). Literary character identification is related to, but not identical to named entity recognition, as is pointed out in Jannidis et al. (2015). In addition, coreference resolution is required to map pronominal and other anaphoric references to full mentions of the character, using his or her name or other characteristic descriptions. In our present study we focus on the technically well-explored task of co-refere"
W15-0706,P14-1005,1,0.826812,"Missing"
W15-0706,A00-1020,0,0.117518,"elated Work As mentioned earlier, the “annotation projection” paradigm was first described by Yarowsky et al. (2001) in order to improve POS-tagging. However, it has proven useful for a number of other applications, e.g. for multilingual co-reference resolution. Most approaches aim at projecting coreferences which are available for one (usually wellresourced) language to another (less-resourced) language for which no tools or not even annotated training data are available. The degree of automatic processing ranges from using manually annotated co-references and hand-crafted translations (e.g. Harabagiu and Maiorano (2000)) to automatically obtained word alignments and combined with a manual post-editing of the obtained co-references (e.g. (Postolache et al., 2006)). Finally, some approaches make use of automatic word alignment, but instead of manual post-editing, they access the quality of the projection through training an own co-reference resolver for the under-resourced language based on the projected data (e.g. de Souza and Or˘asan (2011) and Rahman and Ng (2012)). Zhekova et al. (2014) were to our knowledge the first ones who projected co-reference annotations in 55 the literature domain, in contrast to g"
W15-0706,P10-1142,0,0.0214562,"a state-of-the-art co-reference resolver on both the German original and the English translation of the novel. For a subset of pronouns, we will then manually compare the outcome of the resolver with the translation to see in which of the cases highlighted in Table 1 (where access to a translation might help co-reference resolution), the access to the translation actually can improve co-reference resolution. (2) 4 Automatic Coreference Resolution Noun phrase coreference resolution is the task of determining which noun phrases (NPs) in a text or dialogue refer to the same real-world entities (Ng, 2010). Coreference resolution has been extensively addressed in NLP research, e.g. in the CoNLL shared task 2011 and 2012 (Pradhan et al., 2011; Pradhan et al., 2012)6 or in the SemEval shared task 2010 (Recasens et al., 2010)7 . State-of-the-art tools that take into account a variety of linguistic rules and features, most of the time in a machine learning setting, achieve promising results. However, when applying co-reference resolution tools on out-of-domain texts, i.e. texts that the system has not been trained on, performance typically decreases. Thus, co-reference resolution on literature text"
W15-0706,postolache-etal-2006-transferring,0,0.0317913,"However, it has proven useful for a number of other applications, e.g. for multilingual co-reference resolution. Most approaches aim at projecting coreferences which are available for one (usually wellresourced) language to another (less-resourced) language for which no tools or not even annotated training data are available. The degree of automatic processing ranges from using manually annotated co-references and hand-crafted translations (e.g. Harabagiu and Maiorano (2000)) to automatically obtained word alignments and combined with a manual post-editing of the obtained co-references (e.g. (Postolache et al., 2006)). Finally, some approaches make use of automatic word alignment, but instead of manual post-editing, they access the quality of the projection through training an own co-reference resolver for the under-resourced language based on the projected data (e.g. de Souza and Or˘asan (2011) and Rahman and Ng (2012)). Zhekova et al. (2014) were to our knowledge the first ones who projected co-reference annotations in 55 the literature domain, in contrast to general language texts. In lack of a reasonable amount of parallel training data to train an automatic word alignment of the language pair Russian"
W15-0706,W11-1901,0,0.0149718,"pronouns, we will then manually compare the outcome of the resolver with the translation to see in which of the cases highlighted in Table 1 (where access to a translation might help co-reference resolution), the access to the translation actually can improve co-reference resolution. (2) 4 Automatic Coreference Resolution Noun phrase coreference resolution is the task of determining which noun phrases (NPs) in a text or dialogue refer to the same real-world entities (Ng, 2010). Coreference resolution has been extensively addressed in NLP research, e.g. in the CoNLL shared task 2011 and 2012 (Pradhan et al., 2011; Pradhan et al., 2012)6 or in the SemEval shared task 2010 (Recasens et al., 2010)7 . State-of-the-art tools that take into account a variety of linguistic rules and features, most of the time in a machine learning setting, achieve promising results. However, when applying co-reference resolution tools on out-of-domain texts, i.e. texts that the system has not been trained on, performance typically decreases. Thus, co-reference resolution on literature text is even more challenging as most state-of-the-art co-reference resolver are trained on newspaper text. For a system that has been trained"
W15-0706,W12-4501,0,0.0177182,"n manually compare the outcome of the resolver with the translation to see in which of the cases highlighted in Table 1 (where access to a translation might help co-reference resolution), the access to the translation actually can improve co-reference resolution. (2) 4 Automatic Coreference Resolution Noun phrase coreference resolution is the task of determining which noun phrases (NPs) in a text or dialogue refer to the same real-world entities (Ng, 2010). Coreference resolution has been extensively addressed in NLP research, e.g. in the CoNLL shared task 2011 and 2012 (Pradhan et al., 2011; Pradhan et al., 2012)6 or in the SemEval shared task 2010 (Recasens et al., 2010)7 . State-of-the-art tools that take into account a variety of linguistic rules and features, most of the time in a machine learning setting, achieve promising results. However, when applying co-reference resolution tools on out-of-domain texts, i.e. texts that the system has not been trained on, performance typically decreases. Thus, co-reference resolution on literature text is even more challenging as most state-of-the-art co-reference resolver are trained on newspaper text. For a system that has been trained on newspaper articles,"
W15-0706,N12-1090,0,0.0135091,"are available. The degree of automatic processing ranges from using manually annotated co-references and hand-crafted translations (e.g. Harabagiu and Maiorano (2000)) to automatically obtained word alignments and combined with a manual post-editing of the obtained co-references (e.g. (Postolache et al., 2006)). Finally, some approaches make use of automatic word alignment, but instead of manual post-editing, they access the quality of the projection through training an own co-reference resolver for the under-resourced language based on the projected data (e.g. de Souza and Or˘asan (2011) and Rahman and Ng (2012)). Zhekova et al. (2014) were to our knowledge the first ones who projected co-reference annotations in 55 the literature domain, in contrast to general language texts. In lack of a reasonable amount of parallel training data to train an automatic word alignment of the language pair Russian to German, they developed an alignment tool which facilitates manual alignment. In contrast to previous works, they used not only one translation but different German translations of a Russian novel. Mikhaylova (2014) applied automatic word alignment to the same Russian novel and its German translations, tr"
W15-0706,W09-2411,0,0.0707765,"Missing"
W15-0706,H01-1035,0,0.0638741,"ting a text, can often be exploited for analytical questions that are not per se taking a cross-linguistic perspective: Tentative word-by-word correspondences in a parallel corpus can be induced automatically, taking advantage of co-occurrence statistics from a large collection, with surprising reliability. These “word alignment” links can then be used to map concepts that are sufficiently invariant across languages. The so-called paradigm of “annotation projection” (pio2 For a more extensive discussion of our methodological considerations, see also Kuhn and Reiter (2015 to appear). neered by Yarowsky et al. (2001)) has been enormously successful, even for concepts that one would not consider particularly invariant (such as grammatical categories of words): here, strong statistical tendencies can be exploited. Since the statistical induction of word alignments requires no knowledge-based preprocessing (the required sentence alignment can also be calculated automatically), it can in principle be applied to any collection of parallel texts. Hence, it is possible to test quite easily for which analytical categories that are of interest to literary scholars the translator’s interpretive guidance could be ex"
W15-0706,S10-1001,0,\N,Missing
W15-0706,P13-4010,1,\N,Missing
W15-2908,P10-2049,0,0.0159052,"oached as a text classification task in which snippets (like sentences, paragraphs, or phrases) are categorized into being objective or subjective and in the latter case positive, negative, or neutral (Liu, 2015; T¨ackstr¨om and McDonald, 2011; Sayeed et al., 2012; Pang and Lee, 2004). More differentiated results can be obtained by methods that additionally identify the target of the opinion, specific mentions of product characteristics usually called aspects (Choi et al., 2010; Johansson and Moschitti, 2011; Yang and Cardie, 2012; Hu and Liu, 2004; Li et al., 2010; Popescu and Etzioni, 2005; Jakob and Gurevych, 2010; Klinger and Cimiano, 2013). It has been proposed to use the extracted information for summarizing specific information about a product (Hu and Liu, 2004). The main advantage of such result is that a star rating is not only associated to the whole product but separated for specific aspects. This is helpful when a user aims at getting an overview of the content of reviews but it might still be leading to an overwhelming amount of information. In this work, we propose to aim at generating a ranked list of products and hypothesize that such a ranking would be more helpful for the typical task of"
W15-2908,P11-2018,0,0.0204027,"views for products, for instance from Amazon.com, are a typical resource for opinions. Often, opinion mining is approached as a text classification task in which snippets (like sentences, paragraphs, or phrases) are categorized into being objective or subjective and in the latter case positive, negative, or neutral (Liu, 2015; T¨ackstr¨om and McDonald, 2011; Sayeed et al., 2012; Pang and Lee, 2004). More differentiated results can be obtained by methods that additionally identify the target of the opinion, specific mentions of product characteristics usually called aspects (Choi et al., 2010; Johansson and Moschitti, 2011; Yang and Cardie, 2012; Hu and Liu, 2004; Li et al., 2010; Popescu and Etzioni, 2005; Jakob and Gurevych, 2010; Klinger and Cimiano, 2013). It has been proposed to use the extracted information for summarizing specific information about a product (Hu and Liu, 2004). The main advantage of such result is that a star rating is not only associated to the whole product but separated for specific aspects. This is helpful when a user aims at getting an overview of the content of reviews but it might still be leading to an overwhelming amount of information. In this work, we propose to aim at generat"
W15-2908,D13-1194,1,0.891587,"Missing"
W15-2908,kessler-kuhn-2014-corpus,1,0.852135,"lting list contains 920 products with a total of 71,409 reviews. Product names are extracted from the title of the page and shortened to the first six tokens to remove additional descriptions. As a second external gold ranking, we use the quality ranking provided by Snapsort. From the top 150 products in the Amazon sales ranking, 56 are found on Snapsort. We use the rank in the category “best overall” of “all digital cameras announced in the last 48 month” as retrieved on June 12th, 2015.3 J FSA is trained on the camera data set by Kessler et al. (2010). C SRL is trained on the camera data by Kessler and Kuhn (2014). For the methods D ICT and D ICT-N ORM, we try two different sources of opinion words, the general 2 The lists for aspect mention normalization are available as supplementary material. For instance, video contains “video”, “videos”, “film”, “films”, “movie”, “movies”, “record”, “records”, “recording”. 3 The full list of products with their names and the rankings are available in the supplementary material. 54 Aspect performance video size pictures battery price zoom shutter features autofocus screen lens flash # ρ σ 637 600 513 790 541 625 514 410 629 403 501 457 591 0.301 0.278 0.218 0.213 0"
W15-2908,P13-2147,1,0.745033,"ation task in which snippets (like sentences, paragraphs, or phrases) are categorized into being objective or subjective and in the latter case positive, negative, or neutral (Liu, 2015; T¨ackstr¨om and McDonald, 2011; Sayeed et al., 2012; Pang and Lee, 2004). More differentiated results can be obtained by methods that additionally identify the target of the opinion, specific mentions of product characteristics usually called aspects (Choi et al., 2010; Johansson and Moschitti, 2011; Yang and Cardie, 2012; Hu and Liu, 2004; Li et al., 2010; Popescu and Etzioni, 2005; Jakob and Gurevych, 2010; Klinger and Cimiano, 2013). It has been proposed to use the extracted information for summarizing specific information about a product (Hu and Liu, 2004). The main advantage of such result is that a star rating is not only associated to the whole product but separated for specific aspects. This is helpful when a user aims at getting an overview of the content of reviews but it might still be leading to an overwhelming amount of information. In this work, we propose to aim at generating a ranked list of products and hypothesize that such a ranking would be more helpful for the typical task of a user to select a product"
W15-2908,P04-1035,0,0.00548492,"iltrud Kessler, Roman Klinger, and Jonas Kuhn Institute for Natural Language Processing University of Stuttgart 70569 Stuttgart, Germany {wiltrud.kessler,roman.klinger,jonas.kuhn}@ims.uni-stuttgart.de Abstract text. Reviews for products, for instance from Amazon.com, are a typical resource for opinions. Often, opinion mining is approached as a text classification task in which snippets (like sentences, paragraphs, or phrases) are categorized into being objective or subjective and in the latter case positive, negative, or neutral (Liu, 2015; T¨ackstr¨om and McDonald, 2011; Sayeed et al., 2012; Pang and Lee, 2004). More differentiated results can be obtained by methods that additionally identify the target of the opinion, specific mentions of product characteristics usually called aspects (Choi et al., 2010; Johansson and Moschitti, 2011; Yang and Cardie, 2012; Hu and Liu, 2004; Li et al., 2010; Popescu and Etzioni, 2005; Jakob and Gurevych, 2010; Klinger and Cimiano, 2013). It has been proposed to use the extracted information for summarizing specific information about a product (Hu and Liu, 2004). The main advantage of such result is that a star rating is not only associated to the whole product but"
W15-2908,H05-1043,0,0.12577,"ten, opinion mining is approached as a text classification task in which snippets (like sentences, paragraphs, or phrases) are categorized into being objective or subjective and in the latter case positive, negative, or neutral (Liu, 2015; T¨ackstr¨om and McDonald, 2011; Sayeed et al., 2012; Pang and Lee, 2004). More differentiated results can be obtained by methods that additionally identify the target of the opinion, specific mentions of product characteristics usually called aspects (Choi et al., 2010; Johansson and Moschitti, 2011; Yang and Cardie, 2012; Hu and Liu, 2004; Li et al., 2010; Popescu and Etzioni, 2005; Jakob and Gurevych, 2010; Klinger and Cimiano, 2013). It has been proposed to use the extracted information for summarizing specific information about a product (Hu and Liu, 2004). The main advantage of such result is that a star rating is not only associated to the whole product but separated for specific aspects. This is helpful when a user aims at getting an overview of the content of reviews but it might still be leading to an overwhelming amount of information. In this work, we propose to aim at generating a ranked list of products and hypothesize that such a ranking would be more helpf"
W15-2908,N12-1085,0,0.0134701,"of Product Rankings Wiltrud Kessler, Roman Klinger, and Jonas Kuhn Institute for Natural Language Processing University of Stuttgart 70569 Stuttgart, Germany {wiltrud.kessler,roman.klinger,jonas.kuhn}@ims.uni-stuttgart.de Abstract text. Reviews for products, for instance from Amazon.com, are a typical resource for opinions. Often, opinion mining is approached as a text classification task in which snippets (like sentences, paragraphs, or phrases) are categorized into being objective or subjective and in the latter case positive, negative, or neutral (Liu, 2015; T¨ackstr¨om and McDonald, 2011; Sayeed et al., 2012; Pang and Lee, 2004). More differentiated results can be obtained by methods that additionally identify the target of the opinion, specific mentions of product characteristics usually called aspects (Choi et al., 2010; Johansson and Moschitti, 2011; Yang and Cardie, 2012; Hu and Liu, 2004; Li et al., 2010; Popescu and Etzioni, 2005; Jakob and Gurevych, 2010; Klinger and Cimiano, 2013). It has been proposed to use the extracted information for summarizing specific information about a product (Hu and Liu, 2004). The main advantage of such result is that a star rating is not only associated to t"
W15-2908,P11-2100,0,0.0579987,"Missing"
W15-2908,H05-1044,0,0.0123269,"SRL leads to the best result of all experiments with ρ = 0.51. In comparison to using all information extracted from reviews to generate a ranking, the aspectspecific results allow for an understanding of the Table 2: Results (Spearman’s ρ and standard deviation σ) of J FSA for predicting the Amazon sales ranking when only the subjective phrases are taken into account which refer to the specified target aspect. The number of products for which at least one evaluation of the target aspect is found is shown in column #. inquirer dictionary (Stone et al., 1996)4 and the MPQA subjectivity clues (Wilson et al., 2005)5 . To measure the correlation of the rankings generated by our different methods with the gold ranking, we calculate Spearman’s rank correlation coefficient ρ (Spearman, 1904). We test for significance with the Steiger test (Steiger, 1980). 4.2 Results As described in Section 2, we take into account two different rankings for evaluation: The Amazon.com sales ranking contains 920 products and is an example for a ranking that may be useful for sales managers or product designers. The second is the expert ranking by Snapsort.com which contains 56 products. These two rankings are conceptually dif"
W15-2908,D12-1122,0,0.0144485,"e from Amazon.com, are a typical resource for opinions. Often, opinion mining is approached as a text classification task in which snippets (like sentences, paragraphs, or phrases) are categorized into being objective or subjective and in the latter case positive, negative, or neutral (Liu, 2015; T¨ackstr¨om and McDonald, 2011; Sayeed et al., 2012; Pang and Lee, 2004). More differentiated results can be obtained by methods that additionally identify the target of the opinion, specific mentions of product characteristics usually called aspects (Choi et al., 2010; Johansson and Moschitti, 2011; Yang and Cardie, 2012; Hu and Liu, 2004; Li et al., 2010; Popescu and Etzioni, 2005; Jakob and Gurevych, 2010; Klinger and Cimiano, 2013). It has been proposed to use the extracted information for summarizing specific information about a product (Hu and Liu, 2004). The main advantage of such result is that a star rating is not only associated to the whole product but separated for specific aspects. This is helpful when a user aims at getting an overview of the content of reviews but it might still be leading to an overwhelming amount of information. In this work, we propose to aim at generating a ranked list of pr"
W15-2908,H05-2017,0,\N,Missing
W15-2908,D08-1083,0,\N,Missing
W17-3516,D10-1049,0,0.025787,"iptions from these representations, or gen : z → x. As discussed above, such descriptions should not only cover what the associated function does in general, but should also describe the function’s various parameters. As a secondary (optional) task, we will allow generation systems that accommodate processing in the other direction to compete on the task of semantic parsing, sp : x → z, or generating function representations from text input. 3.1 Main Research Questions Recent data-driven approaches in NLG have been successful in modeling end-to-end generation from unaligned input-output, cf. (Angeli et al., 2010; Mairesse and Young, 2014; Duˇsek and Jurcicek, 117 2015; Wen et al., 2016). However, these system have been mostly tested on datasets (e.g., in the restaurant domain) that require describing very similar entities, entities that are encoded in MRs that have considerable lexical overlap with the target text output. A central research question is whether these end-toend approaches scale to NLG settings that involve substantially harder lexicalization problems, such as with our datasets where the overlap is considerably less. Similarly, generating source code documentation also involves describi"
W17-3516,W13-2111,0,0.169779,"cerns the problem of generating well-formed, natural language descriptions from non-linguistic, formal meaning representations (Gatt and Krahmer, 2017). In our case, the input to a given generation system is a source code representation. In order to learn a natural language generation (NLG) system from data, a parallel corpus containing pairs of inputs and outputs must be constructed. In many studies on data-to-text generation, these parallel resources are relatively small, cf. work on sportscasting (Chen and Mooney, 2008), weather reporting (Belz, 2008; Liang et al., 2009), or biology facts (Banik et al., 2013). We follow similar efforts to build automatic parallel resources (Belz and Kow, 2010) by mining example software libraries for (raw) pairs of short text descriptions and function representations. A recent trend is the use of crowd-sourcing to obtain parallel NLG data (Wen et al., 2016; Novikova 116 et al., 2016; Gardent et al., 2017). Crowd-workers are presented with some meaning representation (MR, e.g., triples from a knowledge base) and asked to verbalize these representations in natural language. For example, the MR input in Figure 1.3 in the restaurant domain is verbalized as the NL outp"
W17-3516,W09-0603,0,0.706082,"n input to function representations within known software libraries allows for more controlled experimentation. On the resource side, our datasets are taken from (Richardson and Kuhn, 2017b; Richardson and Kuhn, 2017a). These resources have been used to study the problem of semantic parser induction, which is the inverse of the proposed data-to-text task. Given the close connection between the two tasks, there is often considerable overlap between the techniques used to solve either problem, techniques that are largely drawn from work on statistical machine translation (Wong and Mooney, 2006; Belz and Kow, 2009) and parsing (Zettlemoyer and Collins, 2012; Konstas and Lapata, 2012). While some approaches to generation explicitly use semantic parsing methods (Wong and Mooney, 2007; Zarrieß and Richardson, 2013), a more systematic investigation into the relation between these two tasks seems missing, which is a topic that we hope to address in this shared task. 3 Task Description Given a collection of datasets consisting of text x and function representation z pairs, or D = {(x, z)i }ni , the goal is to create a generation system that can produce well-formed, natural language descriptions from these rep"
W17-3516,W10-4217,0,0.0243533,"nguistic, formal meaning representations (Gatt and Krahmer, 2017). In our case, the input to a given generation system is a source code representation. In order to learn a natural language generation (NLG) system from data, a parallel corpus containing pairs of inputs and outputs must be constructed. In many studies on data-to-text generation, these parallel resources are relatively small, cf. work on sportscasting (Chen and Mooney, 2008), weather reporting (Belz, 2008; Liang et al., 2009), or biology facts (Banik et al., 2013). We follow similar efforts to build automatic parallel resources (Belz and Kow, 2010) by mining example software libraries for (raw) pairs of short text descriptions and function representations. A recent trend is the use of crowd-sourcing to obtain parallel NLG data (Wen et al., 2016; Novikova 116 et al., 2016; Gardent et al., 2017). Crowd-workers are presented with some meaning representation (MR, e.g., triples from a knowledge base) and asked to verbalize these representations in natural language. For example, the MR input in Figure 1.3 in the restaurant domain is verbalized as the NL output text. While this method allows for fast annotation, and thus solves the data scarci"
W17-3516,W16-6626,0,0.0538738,"pairs for each language. By having two separate sets according to language, we can see whether generation quality differs between different types of programming languages. Taking an idea from the recent CoNLL 2017 shared task on dependency parsing, the third evaluation track will include examples from a surprise programming language that has not been observed during the training phase. The idea is to see how generation systems generalize to unobserved languages where the inputs vary slightly. 5 Evaluation, Baselines and Scheduling Following other data-to-text shared tasks (Banik et al., 2013; Colin et al., 2016) and previous work on text generating from code (Iyer et al., 2016), we will use automatic evaluation metrics such as BLEU and METEOR to evaluate system output. We will also perform fluency-based human evaluation on a subset of each test set using student volunteers from the Institute for Natural Language Processing (IMS), at the University of Stuttgart, Germany. To establish baseline results, we have already started a pilot study that uses phrase-based SMT to do generation. Such models have previously been used to establish strong baseline generation results (Belz and Kow, 2009; Wong and Moon"
W17-3516,P15-1044,0,0.0451726,"Missing"
W17-3516,P17-1017,0,0.0319106,"ning pairs of inputs and outputs must be constructed. In many studies on data-to-text generation, these parallel resources are relatively small, cf. work on sportscasting (Chen and Mooney, 2008), weather reporting (Belz, 2008; Liang et al., 2009), or biology facts (Banik et al., 2013). We follow similar efforts to build automatic parallel resources (Belz and Kow, 2010) by mining example software libraries for (raw) pairs of short text descriptions and function representations. A recent trend is the use of crowd-sourcing to obtain parallel NLG data (Wen et al., 2016; Novikova 116 et al., 2016; Gardent et al., 2017). Crowd-workers are presented with some meaning representation (MR, e.g., triples from a knowledge base) and asked to verbalize these representations in natural language. For example, the MR input in Figure 1.3 in the restaurant domain is verbalized as the NL output text. While this method allows for fast annotation, and thus solves the data scarcity problem, it also raises some new issues. For instance, sentences or utterances are produced by crowd-workers without much context, which puts to question the naturalness of the resulting text. Novikova et al. (2016) compare collecting data from lo"
W17-3516,P16-1195,0,0.223221,"o randomly generate input representations) yet it still corresponds to a formal language. We expect that there is relatively little lexical correspondence between source code representations and verbal descriptions and that this is an interesting challenge for data-driven NLG, as simple “alignment” methods might fail to predict lexicalization. While natural language generation in technical domains has long been of interest to the NLG community (Reiter et al., 1995), there has been renewed interest in this and other closely related topics over the last few years in NLP (Allamanis et al., 2015; Iyer et al., 2016; Yin and Neubig, 2017), making a shared task on the topic rather timely. While preparing the final version of this paper, we learned about the work of (Miceli Barone and Sennrich, 2017), who similarly look at generating text from automatically mined Python projects, using a similar set of tools as ours. This interest seems largely related to the wider availability of new data resources in the technical domain, especially through technical websites such as Github and StackOverflow. Rather than focus on unconstrained source code representations, as done in some of these studies, we believe that"
W17-3516,N12-1093,0,0.0214323,"ies allows for more controlled experimentation. On the resource side, our datasets are taken from (Richardson and Kuhn, 2017b; Richardson and Kuhn, 2017a). These resources have been used to study the problem of semantic parser induction, which is the inverse of the proposed data-to-text task. Given the close connection between the two tasks, there is often considerable overlap between the techniques used to solve either problem, techniques that are largely drawn from work on statistical machine translation (Wong and Mooney, 2006; Belz and Kow, 2009) and parsing (Zettlemoyer and Collins, 2012; Konstas and Lapata, 2012). While some approaches to generation explicitly use semantic parsing methods (Wong and Mooney, 2007; Zarrieß and Richardson, 2013), a more systematic investigation into the relation between these two tasks seems missing, which is a topic that we hope to address in this shared task. 3 Task Description Given a collection of datasets consisting of text x and function representation z pairs, or D = {(x, z)i }ni , the goal is to create a generation system that can produce well-formed, natural language descriptions from these representations, or gen : z → x. As discussed above, such descriptions sh"
W17-3516,P09-1011,0,0.0881319,"elated Work Data-to-text generation concerns the problem of generating well-formed, natural language descriptions from non-linguistic, formal meaning representations (Gatt and Krahmer, 2017). In our case, the input to a given generation system is a source code representation. In order to learn a natural language generation (NLG) system from data, a parallel corpus containing pairs of inputs and outputs must be constructed. In many studies on data-to-text generation, these parallel resources are relatively small, cf. work on sportscasting (Chen and Mooney, 2008), weather reporting (Belz, 2008; Liang et al., 2009), or biology facts (Banik et al., 2013). We follow similar efforts to build automatic parallel resources (Belz and Kow, 2010) by mining example software libraries for (raw) pairs of short text descriptions and function representations. A recent trend is the use of crowd-sourcing to obtain parallel NLG data (Wen et al., 2016; Novikova 116 et al., 2016; Gardent et al., 2017). Crowd-workers are presented with some meaning representation (MR, e.g., triples from a knowledge base) and asked to verbalize these representations in natural language. For example, the MR input in Figure 1.3 in the restaur"
W17-3516,J14-4003,0,0.0196845,"presentations, or gen : z → x. As discussed above, such descriptions should not only cover what the associated function does in general, but should also describe the function’s various parameters. As a secondary (optional) task, we will allow generation systems that accommodate processing in the other direction to compete on the task of semantic parsing, sp : x → z, or generating function representations from text input. 3.1 Main Research Questions Recent data-driven approaches in NLG have been successful in modeling end-to-end generation from unaligned input-output, cf. (Angeli et al., 2010; Mairesse and Young, 2014; Duˇsek and Jurcicek, 117 2015; Wen et al., 2016). However, these system have been mostly tested on datasets (e.g., in the restaurant domain) that require describing very similar entities, entities that are encoded in MRs that have considerable lexical overlap with the target text output. A central research question is whether these end-toend approaches scale to NLG settings that involve substantially harder lexicalization problems, such as with our datasets where the overlap is considerably less. Similarly, generating source code documentation also involves describing a wide variety of funct"
W17-3516,I17-2053,0,0.208056,"Missing"
W17-3516,W16-6627,0,0.0160461,"from example software projects. Data is drawn from existing resources used for studying the related problem of semantic parser induction (Richardson and Kuhn, 2017b; Richardson and Kuhn, 2017a), and spans a wide variety of both natural languages and programming languages. In this paper, we describe these existing resources, which will serve as training and development data for the task, and discuss plans for building new independent test sets. 1 2. Python Documentation # from decimal.Context max(self, a, b): """"""Compares two values numerically and returns the maximum"""""" 3. aNALoGuE Challenge (Novikova and Rieser, 2016) MR input: name[Bibmbap House] food[French] priceRange[cheap], area[riverside] near[Clare Hall] NL output: Near Clare Hall, in the riverside area, Bibimbap serves French food in the price range cheap. Figure 1: Example source code documentation, or docstrings in 1-2, and an example MR/text pair. Introduction Source code libraries are collections of computer programs/instructions expressed in a target programming language that aim to solve some set of problems. Within these libraries, the designers of the code often use natural language to describe how various internal components work. For exam"
W17-3516,W16-6644,0,0.0128252,"2016; Novikova 116 et al., 2016; Gardent et al., 2017). Crowd-workers are presented with some meaning representation (MR, e.g., triples from a knowledge base) and asked to verbalize these representations in natural language. For example, the MR input in Figure 1.3 in the restaurant domain is verbalized as the NL output text. While this method allows for fast annotation, and thus solves the data scarcity problem, it also raises some new issues. For instance, sentences or utterances are produced by crowd-workers without much context, which puts to question the naturalness of the resulting text. Novikova et al. (2016) compare collecting data from logic-based MRs, of the type shown in Figure 1.3, and pictorial MRs, and find that the former approach leads to less natural and less informative descriptions. This seems to be related to the problem that the natural language sentence is a very close verbalization of the “logic” input, i.e., many terms in the MR can be simply taken up in the sentence. Our approach relies on naturally occurring verbal descriptions produced by human developers. Our input data (source code representations) seems more abstract than previously used representations e.g. in the restauran"
W17-3516,D17-2012,1,0.436876,"guage Processing, University of Stuttgart, Germany kyle@ims.uni-stuttgart.de ‡ Dialogue Systems Group // CITEC, Bielefeld University, Germany sina.zarriess@uni-bielefeld.de Abstract 1. Java Documentation *Returns the greater of two long values */ ... public static long max(long a, long b) We propose a new shared task for tactical datato-text generation in the domain of source code libraries. Specifically, we focus on text generation of function descriptions from example software projects. Data is drawn from existing resources used for studying the related problem of semantic parser induction (Richardson and Kuhn, 2017b; Richardson and Kuhn, 2017a), and spans a wide variety of both natural languages and programming languages. In this paper, we describe these existing resources, which will serve as training and development data for the task, and discuss plans for building new independent test sets. 1 2. Python Documentation # from decimal.Context max(self, a, b): """"""Compares two values numerically and returns the maximum"""""" 3. aNALoGuE Challenge (Novikova and Rieser, 2016) MR input: name[Bibmbap House] food[French] priceRange[cheap], area[riverside] near[Clare Hall] NL output: Near Clare Hall, in the riversi"
W17-3516,N16-1015,0,0.0601972,"Missing"
W17-3516,N06-1056,0,0.0768151,"sivity of the generation input to function representations within known software libraries allows for more controlled experimentation. On the resource side, our datasets are taken from (Richardson and Kuhn, 2017b; Richardson and Kuhn, 2017a). These resources have been used to study the problem of semantic parser induction, which is the inverse of the proposed data-to-text task. Given the close connection between the two tasks, there is often considerable overlap between the techniques used to solve either problem, techniques that are largely drawn from work on statistical machine translation (Wong and Mooney, 2006; Belz and Kow, 2009) and parsing (Zettlemoyer and Collins, 2012; Konstas and Lapata, 2012). While some approaches to generation explicitly use semantic parsing methods (Wong and Mooney, 2007; Zarrieß and Richardson, 2013), a more systematic investigation into the relation between these two tasks seems missing, which is a topic that we hope to address in this shared task. 3 Task Description Given a collection of datasets consisting of text x and function representation z pairs, or D = {(x, z)i }ni , the goal is to create a generation system that can produce well-formed, natural language descri"
W17-3516,N07-1022,0,0.150289,"rdson and Kuhn, 2017b; Richardson and Kuhn, 2017a). These resources have been used to study the problem of semantic parser induction, which is the inverse of the proposed data-to-text task. Given the close connection between the two tasks, there is often considerable overlap between the techniques used to solve either problem, techniques that are largely drawn from work on statistical machine translation (Wong and Mooney, 2006; Belz and Kow, 2009) and parsing (Zettlemoyer and Collins, 2012; Konstas and Lapata, 2012). While some approaches to generation explicitly use semantic parsing methods (Wong and Mooney, 2007; Zarrieß and Richardson, 2013), a more systematic investigation into the relation between these two tasks seems missing, which is a topic that we hope to address in this shared task. 3 Task Description Given a collection of datasets consisting of text x and function representation z pairs, or D = {(x, z)i }ni , the goal is to create a generation system that can produce well-formed, natural language descriptions from these representations, or gen : z → x. As discussed above, such descriptions should not only cover what the associated function does in general, but should also describe the funct"
W17-3516,P17-1041,0,0.0390958,"input representations) yet it still corresponds to a formal language. We expect that there is relatively little lexical correspondence between source code representations and verbal descriptions and that this is an interesting challenge for data-driven NLG, as simple “alignment” methods might fail to predict lexicalization. While natural language generation in technical domains has long been of interest to the NLG community (Reiter et al., 1995), there has been renewed interest in this and other closely related topics over the last few years in NLP (Allamanis et al., 2015; Iyer et al., 2016; Yin and Neubig, 2017), making a shared task on the topic rather timely. While preparing the final version of this paper, we learned about the work of (Miceli Barone and Sennrich, 2017), who similarly look at generating text from automatically mined Python projects, using a similar set of tools as ours. This interest seems largely related to the wider availability of new data resources in the technical domain, especially through technical websites such as Github and StackOverflow. Rather than focus on unconstrained source code representations, as done in some of these studies, we believe that limiting the expressiv"
W17-3516,W13-2130,1,0.798733,"Richardson and Kuhn, 2017a). These resources have been used to study the problem of semantic parser induction, which is the inverse of the proposed data-to-text task. Given the close connection between the two tasks, there is often considerable overlap between the techniques used to solve either problem, techniques that are largely drawn from work on statistical machine translation (Wong and Mooney, 2006; Belz and Kow, 2009) and parsing (Zettlemoyer and Collins, 2012; Konstas and Lapata, 2012). While some approaches to generation explicitly use semantic parsing methods (Wong and Mooney, 2007; Zarrieß and Richardson, 2013), a more systematic investigation into the relation between these two tasks seems missing, which is a topic that we hope to address in this shared task. 3 Task Description Given a collection of datasets consisting of text x and function representation z pairs, or D = {(x, z)i }ni , the goal is to create a generation system that can produce well-formed, natural language descriptions from these representations, or gen : z → x. As discussed above, such descriptions should not only cover what the associated function does in general, but should also describe the function’s various parameters. As a"
W18-4509,P16-1036,0,0.0144087,"results for two German corpora. When initialized orthographically, we achieve over .70 in accuracy. EM is initialized either uniformly (uni) or with orthographic overlap (orth) of potential rhyme words. train anti dta test anti anti uni .63 .37 orth .77 .71 Table 2: Accuracy of EM on German stanzas 3.2 Siamese Networks Siamese recurrent networks (SRN) have been successfully applied in NLP applications to measure the distance of texts, both on the level of characters and words. Neculoiu et al. (2016) use it for job title normalization, Mueller and Thyagarajan (2016) on sentence similarity and Das et al. (2016) for Quora question pair retrieval. A SRN consists of two identical recurrent sub-networks that learn a vector representation from input pairs. The sub-networks each receive a rhyme word as character embedding vector and encode it through several layers of bidirectional Long Short Term Memory (biLSTM) Networks. The activations at each timestep of the final biLSTM layer are averaged to produce a fixed-dimensional output. This output is projected through a single densely connected feedforward layer. The respective dense layers are then connected at their outputs through an energy function. The e"
W18-4509,W15-0702,0,0.0140126,"ven word pronunciations and a definition of rhyme, the problem is fairly easy. However, for domain specific or historical data, obtaining precise pronunciation information is a challenge (Katz, 2015). Also, a narrow definition of perfect rhyme1 disregards frequently used and accepted deviations, as in imperfect rhyme (Primus, 2002) (Berg, 1990) or the related sonic devices assonance, consonance and alliteration. Information on the phonological similarity of two rhyme words can be used e.g. for the reconstruction of historical pronunciation (List et al., 2017) or the analysis of sonic pattern (McCurdy et al., 2015). A rhyme detection on grapheme strings is a step in this direction. Previous research on the detection of rhyme is scarce. Reddy and Knight (2011) employed Expectation Maximization (EM) to predict (generate) the most probable scheme (e.g. ’abba’) of a stanza. We use a supervised approach to rhyme detection to model the properties of rhyme itself. An accurate similarity measure between words would benefit an EM. Siamese Recurrent Networks are adept for rhyme, as they learn a (non-linear) similarity metric on variable length character sequences. This metric can be used to gauge the degree of im"
W18-4509,W16-1617,0,0.157524,"provided by Sravana Reddy.4 For certain experiments on English, they report accuracy up to .88 F1. Table 2 lists the results for two German corpora. When initialized orthographically, we achieve over .70 in accuracy. EM is initialized either uniformly (uni) or with orthographic overlap (orth) of potential rhyme words. train anti dta test anti anti uni .63 .37 orth .77 .71 Table 2: Accuracy of EM on German stanzas 3.2 Siamese Networks Siamese recurrent networks (SRN) have been successfully applied in NLP applications to measure the distance of texts, both on the level of characters and words. Neculoiu et al. (2016) use it for job title normalization, Mueller and Thyagarajan (2016) on sentence similarity and Das et al. (2016) for Quora question pair retrieval. A SRN consists of two identical recurrent sub-networks that learn a vector representation from input pairs. The sub-networks each receive a rhyme word as character embedding vector and encode it through several layers of bidirectional Long Short Term Memory (biLSTM) Networks. The activations at each timestep of the final biLSTM layer are averaged to produce a fixed-dimensional output. This output is projected through a single densely connected feed"
W18-4509,P11-2014,0,0.386419,"pronunciation information is a challenge (Katz, 2015). Also, a narrow definition of perfect rhyme1 disregards frequently used and accepted deviations, as in imperfect rhyme (Primus, 2002) (Berg, 1990) or the related sonic devices assonance, consonance and alliteration. Information on the phonological similarity of two rhyme words can be used e.g. for the reconstruction of historical pronunciation (List et al., 2017) or the analysis of sonic pattern (McCurdy et al., 2015). A rhyme detection on grapheme strings is a step in this direction. Previous research on the detection of rhyme is scarce. Reddy and Knight (2011) employed Expectation Maximization (EM) to predict (generate) the most probable scheme (e.g. ’abba’) of a stanza. We use a supervised approach to rhyme detection to model the properties of rhyme itself. An accurate similarity measure between words would benefit an EM. Siamese Recurrent Networks are adept for rhyme, as they learn a (non-linear) similarity metric on variable length character sequences. This metric can be used to gauge the degree of imperfection in a rhyme, and by threshold, for a binary classification. We describe our architecture in section 3.2, followed by experiments and a qu"
W19-4815,2018.lilt-16.1,0,0.26119,"or balanced Dyck words. We compare four models with different architectures and different training objectives, but the main target is the same. All models have the same encoder architecture, a one-layer bidirectional LSTM with 50 hidden units, and only differ in the decoder. The first model tagger-last predicts the target with a simple linear transformation from the LSTM state of the last token. The second model tagger-all has exactly the same architecture, but it is trained to predict a closing bracket after every token in the sequence. The last predicted bracket is the main target for evaluaBernardy (2018) and Sennhauser and Berwick (2018) both frame the task as predicting the next valid closing bracket at any position in a Dyck word, which is arguably more difficult to bypass. Both works also put much emphasis on the generalization over the depth. Comparing to Sennhauser and Berwick (2018), our tagger models, while not perfectly generalizable, perform well above chance level, and the discrepancy between training and testing performance is much smaller. But the general conclusion holds, the RNN-based taggers do not generalize well for the task. Bernardy (2018) reports better results, but they u"
W19-4815,J76-4004,0,0.396313,"Missing"
W19-4815,W18-5414,0,0.483081,"has been a testbed for several research on the ability of Recurrent Neural Networks (RNNs), in particular the Long Short-term Memory model (LSTM) (Hochreiter and Schmidhuber, 1997), to learn contextfree grammars. It consists of strings with balanced pairs of brackets of different types, e.g., “[ < > ] [ ] < [ ] >”. Recognizing the generalized Dyck language is considered to be more difficult than an bn as tested in Gers and Schmidhuber (2001), since it cannot be simply solved by counting. Rather, the model has to remember the sequence of different (unclosed) brackets. Among the recent studies, Sennhauser and Berwick (2018) analyze the generalizability of LSTMs to learn the generalized Dyck language 2 Related Work Modeling context-free grammars with RNNs is of great interest for natural language processing, since recursion is considered an essential characteristic of natural languages if not universal (Hauser et al., 2002). Several recent studies focus on the RNNs’ ability to model deeper structural information against surface-level attractors (Linzen et al., 2016; Gulordava et al., 2018; Wilcox et al., 2018). 138 Proceedings of the Second BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NL"
W19-4815,W18-5425,0,0.508136,"nd Sch¨utzenberger, 1963). 3 Task and Models The Dyck language consists of strings (a Dyck word) of equal number of opening and closing brackets, and the number of closing brackets is never more than the opening brackets in any prefix of the string (a Dyck prefix). The generalized Dyck language has more than one type of bracket pairs, where all pairs have to be balanced and no crossing of different pairs is allowed. Formally, the generalized Dyck language is defined as DP with (oi , ci ) ∈ P, where (oi , ci ) are different bracket pairs. The language can be described by the following grammar: Skachkova et al. (2018) probes the recognition of the generalized Dyck language in two tasks. The first one is a language modeling task which predicts the next bracket in the actual generated data and measures the perplexity, which complicates the evaluation by introducing unnecessary non-determinism. The second task predicts the last closing bracket of a balanced Dyck word, which could be solved with a short-cut. One simply needs to keep a counter for the depth of the sequence, and record the most recent opening bracket each time the counter hits zero. The task is over-simplified by the fact that all the instances"
W19-4815,N18-1108,0,0.0312651,"by counting. Rather, the model has to remember the sequence of different (unclosed) brackets. Among the recent studies, Sennhauser and Berwick (2018) analyze the generalizability of LSTMs to learn the generalized Dyck language 2 Related Work Modeling context-free grammars with RNNs is of great interest for natural language processing, since recursion is considered an essential characteristic of natural languages if not universal (Hauser et al., 2002). Several recent studies focus on the RNNs’ ability to model deeper structural information against surface-level attractors (Linzen et al., 2016; Gulordava et al., 2018; Wilcox et al., 2018). 138 Proceedings of the Second BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 138–146 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics is much simpler both as a task and as a language. To solve this task, the model only needs to approximate a counter, and increment or decrement upon an opening or closing bracket. The LSTM’s ability to approximate a counter machine is discussed in Weiss et al. (2018b). While all these studies test the RNNs as acceptor (classifier) or transducer (tagger), we also test thei"
W19-4815,P18-2117,0,0.0856143,"’ ability to model deeper structural information against surface-level attractors (Linzen et al., 2016; Gulordava et al., 2018; Wilcox et al., 2018). 138 Proceedings of the Second BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 138–146 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics is much simpler both as a task and as a language. To solve this task, the model only needs to approximate a counter, and increment or decrement upon an opening or closing bracket. The LSTM’s ability to approximate a counter machine is discussed in Weiss et al. (2018b). While all these studies test the RNNs as acceptor (classifier) or transducer (tagger), we also test their ability to decode sequences, which is arguably a harder task. Instead of natural language data, where the correlation between structural and contextual/semantic information is difficult to avoid, many analyses with synthetic data have been conducted on RNNs since their invention, e.g., handling the XOR problem (Elman, 1990), the context-free language an bn and context-sensitive language an bn cn (Gers and Schmidhuber, 2001; Weiss et al., 2018b), and extracting finite-state automata (We"
W19-4815,W18-5423,0,0.0190741,"model has to remember the sequence of different (unclosed) brackets. Among the recent studies, Sennhauser and Berwick (2018) analyze the generalizability of LSTMs to learn the generalized Dyck language 2 Related Work Modeling context-free grammars with RNNs is of great interest for natural language processing, since recursion is considered an essential characteristic of natural languages if not universal (Hauser et al., 2002). Several recent studies focus on the RNNs’ ability to model deeper structural information against surface-level attractors (Linzen et al., 2016; Gulordava et al., 2018; Wilcox et al., 2018). 138 Proceedings of the Second BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 138–146 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics is much simpler both as a task and as a language. To solve this task, the model only needs to approximate a counter, and increment or decrement upon an opening or closing bracket. The LSTM’s ability to approximate a counter machine is discussed in Weiss et al. (2018b). While all these studies test the RNNs as acceptor (classifier) or transducer (tagger), we also test their ability to decode se"
W19-4815,Q16-1037,0,0.0260566,"not be simply solved by counting. Rather, the model has to remember the sequence of different (unclosed) brackets. Among the recent studies, Sennhauser and Berwick (2018) analyze the generalizability of LSTMs to learn the generalized Dyck language 2 Related Work Modeling context-free grammars with RNNs is of great interest for natural language processing, since recursion is considered an essential characteristic of natural languages if not universal (Hauser et al., 2002). Several recent studies focus on the RNNs’ ability to model deeper structural information against surface-level attractors (Linzen et al., 2016; Gulordava et al., 2018; Wilcox et al., 2018). 138 Proceedings of the Second BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 138–146 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics is much simpler both as a task and as a language. To solve this task, the model only needs to approximate a counter, and increment or decrement upon an opening or closing bracket. The LSTM’s ability to approximate a counter machine is discussed in Weiss et al. (2018b). While all these studies test the RNNs as acceptor (classifier) or transducer (ta"
W19-4815,D15-1166,0,0.049821,"which 139 tion and comparison, while the rest can be viewed as an auxiliary task. The third model generator-simple follows the common seq2seq architecture (Sutskever et al., 2014). It generates a sequence of closing brackets to eagerly complete the Dyck prefix into a balanced Dyck word, and the generation stops when the ‘$’ symbol is predicted. For this model, the first generated bracket is the main target and the rest is the auxiliary task. The fourth model generator-attention augments the decoder with the attention mechanism (Bahdanau et al., 2014). It uses the general variant of attention (Luong et al., 2015): [ < < [ depth = 2 > [ < < > [ ] distance = 8 [ ] < > [ ] [ embedded depth = 3 ] Figure 1: A visual representation of a Dyck prefix and the property values of the target token (the last ‘]’). probability 1 − p. If the current sequence is balanced (no unclosed brackets), then generating a random opening bracket is the only option. This generation procedure is similar to Skachkova et al. (2018), except that we do not stop generation once the sequence is balanced, instead we generate a new opening bracket and continue until the desired length is reached. Also, the generated sequence is not neces"
W19-7911,C10-1012,0,0.347049,"Missing"
W19-7911,W16-1202,0,0.0538133,"Missing"
W19-7911,P07-1024,0,0.109934,"Missing"
W19-7911,W15-2115,0,0.01911,". (2015) present a recent expansion of this type of treebank study to a set of 37 languages (which they argue to be the first comprehensive analysis that covers a broad range of typologically diverse languages), presenting comparisons of the real dependency trees from the treebank with random reorderings of the same dependency structures. The analysis shows that indeed across all 37 analyzed languages the real DL is significantly shorter than chance. This result corroborates findings from a broad range of empirical studies that are typologically less comprehensive (Gildea and Temperley, 2010; Gulordava and Merlo, 2015; Gulordava et al., 2015). This type of cross-treebank study has prompted a fair number of expansions and discussion regarding the typological implications (e.g. Jiang and Liu (2015), Chen and Gerdes (2018), Temperley and Gildea (2018)). In the present contribution, we go into some detail regarding a question that Futrell et al. (2015) have touched on, namely the relation between (the objective of minimizing) dependency length and language-specific word order constraints (which can also contribute to minimizing the cognitive load in parsing – but may conflict with the DLM objective). Futrell e"
W19-7911,P15-2078,0,0.167487,"xpansion of this type of treebank study to a set of 37 languages (which they argue to be the first comprehensive analysis that covers a broad range of typologically diverse languages), presenting comparisons of the real dependency trees from the treebank with random reorderings of the same dependency structures. The analysis shows that indeed across all 37 analyzed languages the real DL is significantly shorter than chance. This result corroborates findings from a broad range of empirical studies that are typologically less comprehensive (Gildea and Temperley, 2010; Gulordava and Merlo, 2015; Gulordava et al., 2015). This type of cross-treebank study has prompted a fair number of expansions and discussion regarding the typological implications (e.g. Jiang and Liu (2015), Chen and Gerdes (2018), Temperley and Gildea (2018)). In the present contribution, we go into some detail regarding a question that Futrell et al. (2015) have touched on, namely the relation between (the objective of minimizing) dependency length and language-specific word order constraints (which can also contribute to minimizing the cognitive load in parsing – but may conflict with the DLM objective). Futrell et al. (2015) are careful"
W19-7911,J93-2004,0,0.0656123,"Missing"
W19-7911,W17-0419,0,0.0468353,"Missing"
W19-8636,W18-3601,0,0.326729,"es to capture the structural information from the input and score the (partial) output sequence, and use the perceptron to learn the parameters. Both linearizers achieve state-of-the-art performance on the Surface Realization Shared Task 2011 data (Belz et al., 2011) as part of a pipeline or joint system for the full task including deep semantic generation and word inflection (Bohnet et al., 2011; Puduppully et al., 2017). However, to the best of our knowledge, the two linearizers alone have never been directly compared. Also, they have not been tested on the data from the recent shared task (Belz et al., 2018), where they could have served as very strong baselines to put recent developments into context. Song et al. (2018) are the first to use a neural model for syntactic linearization; they adapt the neural dependency parsing model by Chen and Manning (2014) to predict transitions for linearization, which essentially replaces the perceptron with an MLP for the transition system in Liu et al. (2015). However, their adoption of neural models only takes advantage of the token-level representation such as word embeddings, while the structural information is still not well modeled. Recently, many neura"
W19-8636,P14-1005,1,0.900164,"Missing"
W19-8636,C10-1012,0,0.71832,"the red dotted arrows indicate the bottom-up pass, and the blue dashed arrows indicate the top-down pass. We highlight how node 8 influences node 4. Its representation v8◦ is first propagated up to the lowest common ancestor v2↑ , then goes down to v4↓ . 2 3 5 4 6 7 8 Figure 2: An illustration of the information flow in the encoder, where the red dotted arrows represent the bottom-up pass and the blue dashed arrows represent the top-down pass. The solid arrows illustrate the information flow from node 8 to node 4. 2.1.2 Head-First Decoder We adopt the general divide-and-conquer strategy as in Bohnet et al. (2010), by first linearizing each subtree and then combining the ordered subtrees into a full sentence. Instead of generating the sequences from left to right as in Bohnet et al. (2010), we generate the sequence from inside out, i.e., we initialize the sequence with the head, and expand outwards by appending the dependents to the left or the right end of the sequence. This new generation order is motivated by the linguistic research on word order constraints, which largely focuses on the relative direction and distance of the dependent to the head (Gibson, 1998; Liu, 2010; Gulordava, 2018). Followin"
W19-8636,D14-1082,0,0.0423061,"Belz et al., 2011) as part of a pipeline or joint system for the full task including deep semantic generation and word inflection (Bohnet et al., 2011; Puduppully et al., 2017). However, to the best of our knowledge, the two linearizers alone have never been directly compared. Also, they have not been tested on the data from the recent shared task (Belz et al., 2018), where they could have served as very strong baselines to put recent developments into context. Song et al. (2018) are the first to use a neural model for syntactic linearization; they adapt the neural dependency parsing model by Chen and Manning (2014) to predict transitions for linearization, which essentially replaces the perceptron with an MLP for the transition system in Liu et al. (2015). However, their adoption of neural models only takes advantage of the token-level representation such as word embeddings, while the structural information is still not well modeled. Recently, many neural models are proposed to represent graph structures, cf. Zhou et al. (2018) for an overview. Among them, Tree-LSTM, in particular the Child-Sum variation (Tai et al., 2015), has been proposed to model (unordered) We present a dependency tree linearizatio"
W19-8636,P04-1015,0,0.10905,"in Bj¨orkelund and Kuhn (2014): at every step after pruning the beam, we check if there is still at least one gold partial sequence in the beam. If not, then we calculate the hinge loss between the highest scoring gold sequence and all incorrect sequences in the beam2 . We also follow the delayed LaSO strategy in Bj¨orkelund and Kuhn (2014): after all gold partial sequences fall out of the beam and a loss is incurred, we continue training by putting the gold sequence back into the beam, until reaching the full sequence. This is shown to be more sample efficient than the early-update strategy (Collins and Roark, 2004), since it allows the model to train on the full sequence, even if the gold path falls out of the beam early. The standard hinge loss updates the gold sequence against the incorrect ones by enforcing a margin (typically 1), which punishes all incorrect sequences equally. However, not all incorrect sequences are equally bad in terms of the BLEU score, therefore, maintaining a larger margin for worse sequences could improve the performance. We cannot directly use BLEU score as the margin, since it is calculated on the sentence level, while we are training on the subtree level, and the sequences"
W19-8636,P17-1183,0,0.0484982,"H2LR and Voting) with Bohnet et al. (2010) and Puduppully et al. (2016). 3.4 Inflection Table 2 shows the inflection performance with different models: the first model predicts edit script as a tag (EditTag); the second model predicts the character sequence of the inflected word (CharSeq); the third model predicts the edit scripts as sequences of actions (EditSeq); and the last one uses the same model as the third, but first applies the extracted rules if available (+rule). The results are compared to the reported inflection accuracy in Puzikov and Gurevych (2018) (P18), which is adapted from Aharoni and Goldberg (2017). Among our first three models, EditTag performs the lowest, mainly because of the very large tag sets in many languages (the sizes vary from around 300 for English to over 10000 for Finnish and Russian), which prevents effective learning and generalization. The CharSeq model performs much 6 https://www.ims.uni-stuttgart.de/ institut/mitarbeiter/xiangyu/ 7 They use beam size of 1000, which can cover all possible permutations of up to 6 tokens (6! = 720). 284 P18 EditTag CharSeq EditSeq +rule ar cs en es fi fr it nl pt ru 93.07 99.53 98.11 99.59 95.46 95.56 97.44 95.68 99.30 98.22 88.02 97.52 9"
W19-8636,W18-3606,0,0.0855203,"e we compare with the results from other participants in the shared task, as well as the linearizers of Bohnet et al. (2010) and Puduppully et al. (2016) combined with our inflection and detokenization models as additional baselines for the shared task. Table 4 shows the performance of the full pipeline on the test sets. B10 and P16 are the linearizers by Bohnet et al. (2010) and Puduppully et al. (2016) combined with our inflection and detokenization model, ST18 are the best results for each language in the shared task (King and White, 2018; Puzikov and Gurevych, 2018; Ferreira et al., 2018; Elder and Hokamp, 2018). The last column contains the results of our system. Detokenization As the final step, we evaluate the performance of the detokenization, which includes contracting words and attaching punctuation. We use gold linearization and inflection as the input. We separate the evaluation into two parts: for contraction, we evaluate the token-based BLEU score against the gold contraction on the UD development set; for the punctuation attachment, we 285 It is apparent that both B10 and P16 have higher performance than the other systems by a large margin. The advantage of our linearizer also carries over"
W19-8636,C00-1007,0,0.148962,"n, as in two previous surface realization shared tasks (Belz et al., 2011, 2018). As morphological inflection prediction is in itself a separate task (Cotterell et al., 2016), we mainly focus on the linearization in this paper. Syntactic linearization has been extensively studied in the literature. Earlier work mostly focuses on grammar-based approaches using different syntactic formalisms (Elhadad and Robin, 1992; Lavoie and Rainbow, 1997; Carroll et al., 1999). Recently, with the increasing availability of annotated treebanks, statistical methods gain popularity (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Filippova and Strube, 2009). 279 Proceedings of The 12th International Conference on Natural Language Generation, pages 279–289, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics cheio dependency trees. It differs from the sequential LSTM (Hochreiter and Schmidhuber, 1997) in that it aggregates the hidden states of multiple dependents by summation. It is in turn improved by adding the attention mechanism to the hidden states (Zhou et al., 2016), so that each dependent influences the head representation to different degrees. Miwa and Bansal (2016) propose a"
W19-8636,W11-2832,0,0.690464,"ategy and use beam search to incrementally find the best linearization for each subtree; Liu et al. (2015) propose a transition system akin to dependency parsing that produces a sentence that respects the given tree constraints, which is later improved by Puduppully et al. (2016) with look-ahead features. Both approaches rely on rich feature templates to capture the structural information from the input and score the (partial) output sequence, and use the perceptron to learn the parameters. Both linearizers achieve state-of-the-art performance on the Surface Realization Shared Task 2011 data (Belz et al., 2011) as part of a pipeline or joint system for the full task including deep semantic generation and word inflection (Bohnet et al., 2011; Puduppully et al., 2017). However, to the best of our knowledge, the two linearizers alone have never been directly compared. Also, they have not been tested on the data from the recent shared task (Belz et al., 2018), where they could have served as very strong baselines to put recent developments into context. Song et al. (2018) are the first to use a neural model for syntactic linearization; they adapt the neural dependency parsing model by Chen and Manning ("
W19-8636,W18-3604,0,0.0141388,"e full experiment, where we compare with the results from other participants in the shared task, as well as the linearizers of Bohnet et al. (2010) and Puduppully et al. (2016) combined with our inflection and detokenization models as additional baselines for the shared task. Table 4 shows the performance of the full pipeline on the test sets. B10 and P16 are the linearizers by Bohnet et al. (2010) and Puduppully et al. (2016) combined with our inflection and detokenization model, ST18 are the best results for each language in the shared task (King and White, 2018; Puzikov and Gurevych, 2018; Ferreira et al., 2018; Elder and Hokamp, 2018). The last column contains the results of our system. Detokenization As the final step, we evaluate the performance of the detokenization, which includes contracting words and attaching punctuation. We use gold linearization and inflection as the input. We separate the evaluation into two parts: for contraction, we evaluate the token-based BLEU score against the gold contraction on the UD development set; for the punctuation attachment, we 285 It is apparent that both B10 and P16 have higher performance than the other systems by a large margin. The advantage of our lin"
W19-8636,N09-2057,0,0.0421763,"e realization shared tasks (Belz et al., 2011, 2018). As morphological inflection prediction is in itself a separate task (Cotterell et al., 2016), we mainly focus on the linearization in this paper. Syntactic linearization has been extensively studied in the literature. Earlier work mostly focuses on grammar-based approaches using different syntactic formalisms (Elhadad and Robin, 1992; Lavoie and Rainbow, 1997; Carroll et al., 1999). Recently, with the increasing availability of annotated treebanks, statistical methods gain popularity (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Filippova and Strube, 2009). 279 Proceedings of The 12th International Conference on Natural Language Generation, pages 279–289, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics cheio dependency trees. It differs from the sequential LSTM (Hochreiter and Schmidhuber, 1997) in that it aggregates the hidden states of multiple dependents by summation. It is in turn improved by adding the attention mechanism to the hidden states (Zhou et al., 2016), so that each dependent influences the head representation to different degrees. Miwa and Bansal (2016) propose a bidirectional extension that"
W19-8636,A97-1039,0,0.115898,"is a natural language generation task that searches for the natural linear order of words given an unordered syntax tree. Often, the task is accompanied by predicting word inflection, as in two previous surface realization shared tasks (Belz et al., 2011, 2018). As morphological inflection prediction is in itself a separate task (Cotterell et al., 2016), we mainly focus on the linearization in this paper. Syntactic linearization has been extensively studied in the literature. Earlier work mostly focuses on grammar-based approaches using different syntactic formalisms (Elhadad and Robin, 1992; Lavoie and Rainbow, 1997; Carroll et al., 1999). Recently, with the increasing availability of annotated treebanks, statistical methods gain popularity (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Filippova and Strube, 2009). 279 Proceedings of The 12th International Conference on Natural Language Generation, pages 279–289, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics cheio dependency trees. It differs from the sequential LSTM (Hochreiter and Schmidhuber, 1997) in that it aggregates the hidden states of multiple dependents by summation. It is in turn improved by add"
W19-8636,W15-2112,0,0.0206996,". (2) inflection est´ a (3) detokenization est´ a Figure 1: Overview of the pipeline and an example of the process from an unordered dependency tree to the final sentence. hensive comparison with several strong baselines on the recent multilingual linearization shared task data, and achieve state-of-the-art performance. In most linearization models, the incremental generation algorithm follows the left-to-right sequential order. However, in the linguistic study, the head position often plays a central role in describing the constraints and optimization of word orders (Gibson, 1998; Liu, 2010; Futrell et al., 2015). In the linearization models that employ left-to-right generation, such word order properties are only implicitly reflected in the features, if at all. Inspired by the above-mentioned study on head-oriented word order constraints, we adopt an improved linearization algorithm, in which we generate the sequence starting from the head and expanding to both directions. The head-first generation order can easily capture the constraints, since it naturally separates the decision into two aspects: (1) which side of the head to append the dependent and (2) which dependent to attach closer to the head"
W19-8636,N15-1012,0,0.0184646,"1; Puduppully et al., 2017). However, to the best of our knowledge, the two linearizers alone have never been directly compared. Also, they have not been tested on the data from the recent shared task (Belz et al., 2018), where they could have served as very strong baselines to put recent developments into context. Song et al. (2018) are the first to use a neural model for syntactic linearization; they adapt the neural dependency parsing model by Chen and Manning (2014) to predict transitions for linearization, which essentially replaces the perceptron with an MLP for the transition system in Liu et al. (2015). However, their adoption of neural models only takes advantage of the token-level representation such as word embeddings, while the structural information is still not well modeled. Recently, many neural models are proposed to represent graph structures, cf. Zhou et al. (2018) for an overview. Among them, Tree-LSTM, in particular the Child-Sum variation (Tai et al., 2015), has been proposed to model (unordered) We present a dependency tree linearization model with two novel components: (1) a tree-structured encoder based on bidirectional Tree-LSTM that propagates information first bottom-up t"
W19-8636,N10-1115,0,0.107977,"ation, inflection, and detokenization and evaluate with the official evaluation script. We also apply our inflection and detokenization steps on the predicted linearization of B10 and P16, so that they can also be compared to other systems. 3.2 The H2LR order performs better than L2R and R2L, which could be explained in multiple aspects. One explanation is our motivation that generating from the head could better reflect word order constraints. The other explanation is that training with latent generation order allows the model to make easier decision first, similar to the easyfirst parser by Goldberg and Elhadad (2010). Finally, when combining the three decoders together by voting, it achieves 2 BLEU points higher than B10. There are two main reasons for this improvement: (1) multitask-style training helps regularize the parameters, and (2) different generation directions tend to prune the correct sequences at different locations, and the mistake in one direction might be saved by the other two. Implementation Details Our model is implemented with the DyNet Library (Neubig et al., 2017), and is available at the first author’s website6 . We use the embedding sizes of 64, 32 and 32 for lemma, UPOS and depende"
W19-8636,P16-1105,0,0.0843817,"1998; Bangalore and Rambow, 2000; Filippova and Strube, 2009). 279 Proceedings of The 12th International Conference on Natural Language Generation, pages 279–289, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics cheio dependency trees. It differs from the sequential LSTM (Hochreiter and Schmidhuber, 1997) in that it aggregates the hidden states of multiple dependents by summation. It is in turn improved by adding the attention mechanism to the hidden states (Zhou et al., 2016), so that each dependent influences the head representation to different degrees. Miwa and Bansal (2016) propose a bidirectional extension that traverses the tree both bottom-up and top-down to allow the tokens access information from their descendants as well as ancestors. We adopt and combine their proposed models to represent the tree structure in our task, while improving the bidirectional extension by using the output of the bottom-up pass as the input for the top-down pass, so that each token can access information from all other tokens. tesouro este estar . de (1) linearization estar cheio de este tesouro . cheia de estes tesouros . cheia destes tesouros. (2) inflection est´ a (3) detoken"
W19-8636,L16-1262,0,0.0749806,"Missing"
W19-8636,P09-1091,0,0.020622,"nted word order constraints, we adopt an improved linearization algorithm, in which we generate the sequence starting from the head and expanding to both directions. The head-first generation order can easily capture the constraints, since it naturally separates the decision into two aspects: (1) which side of the head to append the dependent and (2) which dependent to attach closer to the head, which exactly correspond to the two aspects of the word order constraints, namely (1) the direction of the dependent and (2) the distance of dependent to the head. The algorithm is somewhat similar to He et al. (2009), which also emphasizes the central role of the head by first predicting for each dependent which side of the head it is placed. However, they exhaustively score all permutations, which could be intractable for subtrees with too many dependents, while we use incremental beam-search to guarantee the efficiency. 2 Model We use a pipeline system for the surface realization task, consisting of three steps: linearization (§2.1), inflection (§2.2), and detokenization (§2.3). Figure 1 gives an overview of the pipeline along with an example from the input tree to output text. The input is an unordered"
W19-8636,N16-1058,0,0.278197,"earization with Tree-Structured Representation Xiang Yu, Agnieszka Falenska, Ngoc Thang Vu, Jonas Kuhn Institut f¨ur Maschinelle Sprachverarbeitung Universit¨at Stuttgart, Germany firstname.lastname@ims.uni-stuttgart.de Abstract Among the most successful statistical linearization systems, Bohnet et al. (2010) employ the divide-and-conquer strategy and use beam search to incrementally find the best linearization for each subtree; Liu et al. (2015) propose a transition system akin to dependency parsing that produces a sentence that respects the given tree constraints, which is later improved by Puduppully et al. (2016) with look-ahead features. Both approaches rely on rich feature templates to capture the structural information from the input and score the (partial) output sequence, and use the perceptron to learn the parameters. Both linearizers achieve state-of-the-art performance on the Surface Realization Shared Task 2011 data (Belz et al., 2011) as part of a pipeline or joint system for the full task including deep semantic generation and word inflection (Bohnet et al., 2011; Puduppully et al., 2017). However, to the best of our knowledge, the two linearizers alone have never been directly compared. Al"
W19-8636,W16-2010,0,0.0707323,"Missing"
W19-8636,E17-1061,0,0.0135066,"y parsing that produces a sentence that respects the given tree constraints, which is later improved by Puduppully et al. (2016) with look-ahead features. Both approaches rely on rich feature templates to capture the structural information from the input and score the (partial) output sequence, and use the perceptron to learn the parameters. Both linearizers achieve state-of-the-art performance on the Surface Realization Shared Task 2011 data (Belz et al., 2011) as part of a pipeline or joint system for the full task including deep semantic generation and word inflection (Bohnet et al., 2011; Puduppully et al., 2017). However, to the best of our knowledge, the two linearizers alone have never been directly compared. Also, they have not been tested on the data from the recent shared task (Belz et al., 2018), where they could have served as very strong baselines to put recent developments into context. Song et al. (2018) are the first to use a neural model for syntactic linearization; they adapt the neural dependency parsing model by Chen and Manning (2014) to predict transitions for linearization, which essentially replaces the perceptron with an MLP for the transition system in Liu et al. (2015). However,"
W19-8636,W18-3605,0,0.270518,"best variant for each step in the pipeline for the full experiment, where we compare with the results from other participants in the shared task, as well as the linearizers of Bohnet et al. (2010) and Puduppully et al. (2016) combined with our inflection and detokenization models as additional baselines for the shared task. Table 4 shows the performance of the full pipeline on the test sets. B10 and P16 are the linearizers by Bohnet et al. (2010) and Puduppully et al. (2016) combined with our inflection and detokenization model, ST18 are the best results for each language in the shared task (King and White, 2018; Puzikov and Gurevych, 2018; Ferreira et al., 2018; Elder and Hokamp, 2018). The last column contains the results of our system. Detokenization As the final step, we evaluate the performance of the detokenization, which includes contracting words and attaching punctuation. We use gold linearization and inflection as the input. We separate the evaluation into two parts: for contraction, we evaluate the token-based BLEU score against the gold contraction on the UD development set; for the punctuation attachment, we 285 It is apparent that both B10 and P16 have higher performance than the other"
W19-8636,W18-3602,0,0.154943,"where we compare different generation orders (L2R, R2L, H2LR and Voting) with Bohnet et al. (2010) and Puduppully et al. (2016). 3.4 Inflection Table 2 shows the inflection performance with different models: the first model predicts edit script as a tag (EditTag); the second model predicts the character sequence of the inflected word (CharSeq); the third model predicts the edit scripts as sequences of actions (EditSeq); and the last one uses the same model as the third, but first applies the extracted rules if available (+rule). The results are compared to the reported inflection accuracy in Puzikov and Gurevych (2018) (P18), which is adapted from Aharoni and Goldberg (2017). Among our first three models, EditTag performs the lowest, mainly because of the very large tag sets in many languages (the sizes vary from around 300 for English to over 10000 for Finnish and Russian), which prevents effective learning and generalization. The CharSeq model performs much 6 https://www.ims.uni-stuttgart.de/ institut/mitarbeiter/xiangyu/ 7 They use beam size of 1000, which can cover all possible permutations of up to 6 tokens (6! = 720). 284 P18 EditTag CharSeq EditSeq +rule ar cs en es fi fr it nl pt ru 93.07 99.53 98.1"
W19-8636,W18-6553,0,0.0157674,"tron to learn the parameters. Both linearizers achieve state-of-the-art performance on the Surface Realization Shared Task 2011 data (Belz et al., 2011) as part of a pipeline or joint system for the full task including deep semantic generation and word inflection (Bohnet et al., 2011; Puduppully et al., 2017). However, to the best of our knowledge, the two linearizers alone have never been directly compared. Also, they have not been tested on the data from the recent shared task (Belz et al., 2018), where they could have served as very strong baselines to put recent developments into context. Song et al. (2018) are the first to use a neural model for syntactic linearization; they adapt the neural dependency parsing model by Chen and Manning (2014) to predict transitions for linearization, which essentially replaces the perceptron with an MLP for the transition system in Liu et al. (2015). However, their adoption of neural models only takes advantage of the token-level representation such as word embeddings, while the structural information is still not well modeled. Recently, many neural models are proposed to represent graph structures, cf. Zhou et al. (2018) for an overview. Among them, Tree-LSTM,"
W19-8636,P15-1150,0,0.474288,"syntactic linearization; they adapt the neural dependency parsing model by Chen and Manning (2014) to predict transitions for linearization, which essentially replaces the perceptron with an MLP for the transition system in Liu et al. (2015). However, their adoption of neural models only takes advantage of the token-level representation such as word embeddings, while the structural information is still not well modeled. Recently, many neural models are proposed to represent graph structures, cf. Zhou et al. (2018) for an overview. Among them, Tree-LSTM, in particular the Child-Sum variation (Tai et al., 2015), has been proposed to model (unordered) We present a dependency tree linearization model with two novel components: (1) a tree-structured encoder based on bidirectional Tree-LSTM that propagates information first bottom-up then top-down, which allows each token to access information from the entire tree; and (2) a linguistically motivated headfirst decoder that emphasizes the central role of the head and linearizes the subtree by incrementally attaching the dependents on both sides of the head. With the new encoder and decoder, we reach state-of-the-art performance on the Surface Realization"
W19-8636,C16-1274,0,0.271415,"sing availability of annotated treebanks, statistical methods gain popularity (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Filippova and Strube, 2009). 279 Proceedings of The 12th International Conference on Natural Language Generation, pages 279–289, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics cheio dependency trees. It differs from the sequential LSTM (Hochreiter and Schmidhuber, 1997) in that it aggregates the hidden states of multiple dependents by summation. It is in turn improved by adding the attention mechanism to the hidden states (Zhou et al., 2016), so that each dependent influences the head representation to different degrees. Miwa and Bansal (2016) propose a bidirectional extension that traverses the tree both bottom-up and top-down to allow the tokens access information from their descendants as well as ancestors. We adopt and combine their proposed models to represent the tree structure in our task, while improving the bidirectional extension by using the output of the bottom-up pass as the input for the top-down pass, so that each token can access information from all other tokens. tesouro este estar . de (1) linearization estar ch"
ziering-etal-2012-corpus,W07-1203,0,\N,Missing
ziering-etal-2012-corpus,P11-1101,1,\N,Missing
ziering-etal-2012-corpus,rohrer-forst-2006-improving,0,\N,Missing
