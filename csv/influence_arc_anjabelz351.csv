2007.mtsummit-ucnlg.14,P89-1009,0,0.466714,"do peer attribute sets uniquely describe the target referent? Minimality: A minimal attribute set is defined as an attribute set which uniquely identifies the referent such that there is no smaller attribute set which uniquely identifies the referent. For example, a minimal description of e1 in the above example is {COLOUR:red}, since it is the only red object. There may be more than one minimal attribute set. As an aggregate measure, we computed the proportion of minimal distinguishing outputs produced by peer systems. Minimality has frequently been cited as a desideratum for GRE algorithms (Dale, 1989; Gardent, 2002). 2. Minimality: are peer attribute sets of minimal size? 3. Humanlikeness: are peer attribute sets similar to reference attribute sets? 4. Identification Accuracy: do peer attribute sets enable people to identify the target referent accurately? 5. Identification Speed: do peer attribute sets enable people to identify a referent quickly? 4.1 Automatic Evaluation Methods Humanlikeness: We measured the similarity between the peer attribute sets and (human-produced) reference attribute sets, because (Grice’s maxim of Clarity notwithstanding) humans choose to overspecify and unders"
2007.mtsummit-ucnlg.14,W07-2307,1,\N,Missing
belz-gatt-2012-repository,passonneau-2006-measuring,0,\N,Missing
belz-gatt-2012-repository,W09-2816,1,\N,Missing
belz-gatt-2012-repository,W07-2315,0,\N,Missing
belz-gatt-2012-repository,P02-1040,0,\N,Missing
belz-gatt-2012-repository,P89-1009,0,\N,Missing
bouayad-agha-etal-2002-pills,P98-2173,1,\N,Missing
bouayad-agha-etal-2002-pills,C98-2168,1,\N,Missing
C02-1068,A00-2018,0,\N,Missing
C02-1068,J98-4004,0,\N,Missing
C02-1068,C00-2105,0,\N,Missing
C02-1068,W00-0725,0,\N,Missing
C02-1068,W00-0726,0,\N,Missing
C02-1068,C00-2124,0,\N,Missing
C02-1068,W01-0712,1,\N,Missing
C02-1068,W00-0730,0,\N,Missing
D19-5526,W10-1915,0,0.109247,"Missing"
D19-5526,desmet-hoste-2014-recognising,0,0.0248542,"Missing"
D19-5526,W15-1211,0,0.0692196,"Missing"
D19-5526,W18-5908,0,0.0662541,"Missing"
D19-6301,W13-3520,0,0.0329649,"e, available parsers such as UUParser (Smith et al., 2018) could be run to create a silver standard versions of provided datasets and use them as additional or alternative training material, and publicly available off-the3 In the case of one team, we agreed to move the two week window between test data release and submission to one week earlier. 4 At SR’18, there were ten languages from five families. 5 https://www.aclweb.org/portal/ content/sigmorphon-shared-task-2019 6 universaldependencies.org shelf language models such as GPT-2 (Radford et al., 2019), ELMo (Peters et al., 2018), polyglot (Al-Rfou et al., 2013) or BERT (Devlin et al., 2018) could be fine-tuned with publicly available datasets such as WikiText (Merity et al., 2016) or the DeepMind Q&A Dataset (Hermann et al., 2015). Datasets were created for 11 languages in the Shallow Track, and for three of those languages, namely English, French and Spanish, in the Deep Track. As in 2018, Shallow Track inputs were generated with the aid of Python scripts from the original UD structures, this time using all available input sentences. Deep Track inputs were then generated by automatically processing the Shallow Track structures using a series of gra"
D19-6301,P11-2040,1,0.829216,"availability of evaluators: four Shallow Track in-domain datasets (Chinese-GSD, English-EWT, RussianSynTagRus, Spanish-AnCora), one Shallow Track dataset coming from parsed data (SpanishAnCoraHIT ) and one (in-domain) Deep Track dataset (English-EWT). As in SR’11 (Belz et al., 2011) and SR’18 (Mille et al., 2018), we assessed two quality criteria in the human evaluations, in separate evaluation experiments, Readability and Meaning Similarity, and used continuous sliders as rating tools, the evidence being that raters tend to prefer them 14 Thank you to Yevgeniy Puzikov for pointing this out. (Belz and Kow, 2011). Slider positions were mapped to values from 0 to 100 (best). Raters were first given brief instructions, including the direction to ignore formatting errors, superfluous whitespace, capitalisation issues, and poor hyphenation. The statement to be assessed in the Readability evaluation was: The text reads well and is free from grammatical errors and awkward constructions. The corresponding statement in the Meaning Similarity evaluation, in which system outputs (‘the black text’) were compared to reference sentences (‘the grey text’), was as follows: The meaning of the grey text is adequately"
D19-6301,W11-2832,1,0.65823,"ncies.org/ Bernd Bohnet Google Inc. bohnetbd@google.com Leo Wanner ICREA and UPF, Barcelona leo.wanner@upf.edu growing, as is their size (and thus the volume of available training material).2 The SR tasks require participating systems to generate sentences from structures at the level of abstraction of outputs produced by state-of-the-art parsing. In order to promote linkage with parsing and earlier stages of generation, participants are encouraged to explore the extent to which neural network parsing algorithms can be reversed for generation. As was the case with its predecessor tasks SR’11 (Belz et al., 2011) and SR’18 (Mille et al., 2018), SR’19 comprises two tracks distinguished by the level of specificity of the inputs: Shallow Track (T1): This track starts from UD structures in which most of the word order information has been removed and tokens have been lemmatised. In other words, it starts from unordered dependency trees with lemmatised nodes that hold PoS tags and morphological information as found in the original treebank annotations. The task in this track therefore amounts to determining the word order and inflecting words. Deep Track (T2): This track starts from UD structures from whic"
D19-6301,K17-3005,0,0.0303018,"bed by syntactic structure or agreement (such as verbal finiteness or verbal number) was removed, whereas semanticlevel information such as nominal number and verbal tense was retained. UD2.3 version of the dataset, whereas CoNLL’18 used UD2.2; we selected treebanks that had not undergone major updates from one version to the next according to their README files on the UD site, and for which the best available parse reached a Labeled Attachment Score of 85 and over.11 There were datasets meeting these criteria for English (2), Hindi, Korean, Portuguese and Spanish; the Harbin HIT-SCIR parser (Che et al., 2017) had best scores on four of these datasets; LATTICE (Lim et al., 2018) and Stanford (Qi et al., 2019) had the best scores for the remaining two;12 see Table 3 for an overview. As is the case for all test data, in the additional automatically parsed test data alignments with surface tokens and with Shallow Track tokens are not provided; however, in the cases described in 4 above, the relative order is provided. 10. Fine-grained PoS labels found in some treebanks (see e.g. column 5 in Figure 2) were removed, and only coarse-grained ones were retained (column 4 in Figures 2 and 3). 11. In the tra"
D19-6301,D19-6302,0,0.0736613,"tion is very similar to ADAPT’s SR’18 submission (Elder and Hokamp, 2018). The BME-UW system (Kov´acs et al., 2019) learns weighted rules of an Interpreted Regular Tree Grammar (IRTG) to encode the correspondence between word sequences and UDsubgraphs. For the inflection step, a standard sequence-to-sequence model with a biLSTM encoder and an LSTM decoder with attention is used. CLaC (Farahnak et al., 2019) is a pointer network trained to find the best order of the input. A slightly modified version of the transformer model was used as the encoder and decoder for the pointer network. The CMU (Du and Black, 2019) system uses a graph neural network for end-to-end ordering, and a character RNN for morphology. DepDist (Dyer, 2019) uses syntactic embeddings and a graph neural network with message passing to learn the tolerances for how far a dependent tends to be from its head. These directed 15 https://github.com/ygraham/ segment-mteval dependency distance tolerances form an edgeweighted directed acyclic graph (DAG) (equivalent to a partially ordered set, or poset) for each sentence, the topological sort of which generates a surface order. Inflection is addressed with regex patterns and substitutions app"
D19-6301,D19-6303,0,0.0332354,"ighted rules of an Interpreted Regular Tree Grammar (IRTG) to encode the correspondence between word sequences and UDsubgraphs. For the inflection step, a standard sequence-to-sequence model with a biLSTM encoder and an LSTM decoder with attention is used. CLaC (Farahnak et al., 2019) is a pointer network trained to find the best order of the input. A slightly modified version of the transformer model was used as the encoder and decoder for the pointer network. The CMU (Du and Black, 2019) system uses a graph neural network for end-to-end ordering, and a character RNN for morphology. DepDist (Dyer, 2019) uses syntactic embeddings and a graph neural network with message passing to learn the tolerances for how far a dependent tends to be from its head. These directed 15 https://github.com/ygraham/ segment-mteval dependency distance tolerances form an edgeweighted directed acyclic graph (DAG) (equivalent to a partially ordered set, or poset) for each sentence, the topological sort of which generates a surface order. Inflection is addressed with regex patterns and substitutions approximating productive inflectional paradigms. The DipInfoUnito realiser (Mazzei and Basile, 2019) is a supervised sta"
D19-6301,W18-3606,0,0.194017,"andard scores (or z-scores) computed on the set of all raw scores by the given evaluator using each evaluator’s mean and standard deviation. For both raw and standard scores, we compute the mean of sentence-level scores. Code: We were able to reuse, with minor adaptations, the code produced for the WMT’17 evaluations.15 4 Overview of Submitted Systems ADAPT is a sequence to sequence model with dependency features attached to word embeddings. A BERT sentence classifier was used as a reranker to choose between different hypotheses. The implementation is very similar to ADAPT’s SR’18 submission (Elder and Hokamp, 2018). The BME-UW system (Kov´acs et al., 2019) learns weighted rules of an Interpreted Regular Tree Grammar (IRTG) to encode the correspondence between word sequences and UDsubgraphs. For the inflection step, a standard sequence-to-sequence model with a biLSTM encoder and an LSTM decoder with attention is used. CLaC (Farahnak et al., 2019) is a pointer network trained to find the best order of the input. A slightly modified version of the transformer model was used as the encoder and decoder for the pointer network. The CMU (Du and Black, 2019) system uses a graph neural network for end-to-end ord"
D19-6301,W18-3604,0,0.0728895,"nto a tree of constituents that is transformed into an English sentence by an existing English realiser, JSrealB (Molins and Lapalme, 2015). This realiser was then slightly modified for the two tracks. Surfers (Hong et al., 2019) first performs delexicalisation to obtain a dictionary for proper names and numbers. A GCN is then used to encode the tree inputs, and an LSTM encoder-decoder with copy attention to generate delexicalised outputs. No part-of-speech tags, universal features or pretrained embeddings / language models are used. The Tilburg approach (Ferreira and Krahmer, 2019), based on Ferreira et al. (2018), realises multilingual texts by first preprocessing an input dependency tree into an ordered linearised string, which is then realised using a rule-based and a statistical machine translation (SMT) model. Baseline: In order to set a lower boundary for the automatic and human evaluations, a simple English baseline consisting of 7 lines of python code was implemented16 . It generates from a UD file with an in-order traversal of the tree read by pyconll and outputting the form of each node. 5 Evaluation results There were 14 submissions to the task, of which two were withdrawn; 9 teams participa"
D19-6301,D19-6310,0,0.021759,"tree-LSTM encoders) whose outputs also contain tokens marking the tree structure. N-best outputs are obtained for orderings and the highest confidence output sequence with a valid tree is chosen (i.e, one where the input and output trees are isomorphic up to sibling order, ensuring projectivity). The RALI system (Lapalme, 2019) uses a symbolic approach to transform the dependency tree into a tree of constituents that is transformed into an English sentence by an existing English realiser, JSrealB (Molins and Lapalme, 2015). This realiser was then slightly modified for the two tracks. Surfers (Hong et al., 2019) first performs delexicalisation to obtain a dictionary for proper names and numbers. A GCN is then used to encode the tree inputs, and an LSTM encoder-decoder with copy attention to generate delexicalised outputs. No part-of-speech tags, universal features or pretrained embeddings / language models are used. The Tilburg approach (Ferreira and Krahmer, 2019), based on Ferreira et al. (2018), realises multilingual texts by first preprocessing an input dependency tree into an ordered linearised string, which is then realised using a rule-based and a statistical machine translation (SMT) model. B"
D19-6301,D19-6304,0,0.339584,"Missing"
D19-6301,K18-2014,0,0.041076,"verbal number) was removed, whereas semanticlevel information such as nominal number and verbal tense was retained. UD2.3 version of the dataset, whereas CoNLL’18 used UD2.2; we selected treebanks that had not undergone major updates from one version to the next according to their README files on the UD site, and for which the best available parse reached a Labeled Attachment Score of 85 and over.11 There were datasets meeting these criteria for English (2), Hindi, Korean, Portuguese and Spanish; the Harbin HIT-SCIR parser (Che et al., 2017) had best scores on four of these datasets; LATTICE (Lim et al., 2018) and Stanford (Qi et al., 2019) had the best scores for the remaining two;12 see Table 3 for an overview. As is the case for all test data, in the additional automatically parsed test data alignments with surface tokens and with Shallow Track tokens are not provided; however, in the cases described in 4 above, the relative order is provided. 10. Fine-grained PoS labels found in some treebanks (see e.g. column 5 in Figure 2) were removed, and only coarse-grained ones were retained (column 4 in Figures 2 and 3). 11. In the training data, the alignments with the tokens of the Shallow Track struct"
D19-6301,D19-6311,0,0.0501754,"ter RNN for morphology. DepDist (Dyer, 2019) uses syntactic embeddings and a graph neural network with message passing to learn the tolerances for how far a dependent tends to be from its head. These directed 15 https://github.com/ygraham/ segment-mteval dependency distance tolerances form an edgeweighted directed acyclic graph (DAG) (equivalent to a partially ordered set, or poset) for each sentence, the topological sort of which generates a surface order. Inflection is addressed with regex patterns and substitutions approximating productive inflectional paradigms. The DipInfoUnito realiser (Mazzei and Basile, 2019) is a supervised statistical system for surface realisation, in which two neural network-based models run in parallel on the same input structure, namely a list-wise learning to rank network for linearisation and a seq2seq network for morphology inflection prediction. IMS (Yu et al., 2019) uses a pipeline approach for both tracks, consisting of linearisation, completion (for T2 only), inflection, and contraction. All models use the same bidirectional Tree-LSTM encoder architecture. The linearisation model orders each subtree separately with beam search and then combines them into a full projec"
D19-6301,W04-2705,0,0.268698,"Missing"
D19-6301,W18-3601,1,0.502146,"Inc. bohnetbd@google.com Leo Wanner ICREA and UPF, Barcelona leo.wanner@upf.edu growing, as is their size (and thus the volume of available training material).2 The SR tasks require participating systems to generate sentences from structures at the level of abstraction of outputs produced by state-of-the-art parsing. In order to promote linkage with parsing and earlier stages of generation, participants are encouraged to explore the extent to which neural network parsing algorithms can be reversed for generation. As was the case with its predecessor tasks SR’11 (Belz et al., 2011) and SR’18 (Mille et al., 2018), SR’19 comprises two tracks distinguished by the level of specificity of the inputs: Shallow Track (T1): This track starts from UD structures in which most of the word order information has been removed and tokens have been lemmatised. In other words, it starts from unordered dependency trees with lemmatised nodes that hold PoS tags and morphological information as found in the original treebank annotations. The task in this track therefore amounts to determining the word order and inflecting words. Deep Track (T2): This track starts from UD structures from which functional words (in particul"
D19-6301,W15-4719,0,0.125325,"The linearised constituent trees are fed to seq2seq models (including models with copy and with tree-LSTM encoders) whose outputs also contain tokens marking the tree structure. N-best outputs are obtained for orderings and the highest confidence output sequence with a valid tree is chosen (i.e, one where the input and output trees are isomorphic up to sibling order, ensuring projectivity). The RALI system (Lapalme, 2019) uses a symbolic approach to transform the dependency tree into a tree of constituents that is transformed into an English sentence by an existing English realiser, JSrealB (Molins and Lapalme, 2015). This realiser was then slightly modified for the two tracks. Surfers (Hong et al., 2019) first performs delexicalisation to obtain a dictionary for proper names and numbers. A GCN is then used to encode the tree inputs, and an LSTM encoder-decoder with copy attention to generate delexicalised outputs. No part-of-speech tags, universal features or pretrained embeddings / language models are used. The Tilburg approach (Ferreira and Krahmer, 2019), based on Ferreira et al. (2018), realises multilingual texts by first preprocessing an input dependency tree into an ordered linearised string, whic"
D19-6301,J05-1004,0,0.0811396,"n the English-gum dataset);8 3. The lines corresponding to combined lexical units (e.g. Spanish “del” &lt;de+el&gt; lit. ’of.the’) and the contents of columns [9] and [10] were removed; 4. Information about the relative order of components of named entities, multiple coordinations and punctuation signs was added in the FEATS column (dependency relations compound, compound:prt, compound:svc, flat, flat:foreign, flat:name, fixed, conj, punct); For the Deep Track, the following steps were additionally carried out: 5. Edge labels were generalised into predicate/argument labels, in the PropBank/NomBank (Palmer et al., 2005; Meyers et al., 2004) fashion. That is, the 8 Thank you to Guy Lapalme for spotting this. syntactic relations were mapped to core (A1, A2, etc.) and non-core (AM) labels, applying the following rules: (i) the first argument is always labeled A1 (i.e. there is no external argument A0); (ii) in order to maintain the tree structure and account for some cases of shared arguments, there can be inverted argument relations; (iii) all modifier edges are assigned the same generic label AM; (iv) there is a coordinating relation. See also the inventory of relations in Table 2. 6. Functional prepositions"
D19-6301,P02-1040,0,0.108135,"a que los nuevos miembros del CNE deben tener experiencia para “dirigir procesos complejos”. In the original UD files, the reference sentences are by default detokenised. In order to carry out the evaluations of the tokenised outputs, we built a tokenised version of the reference sentences by concatenating the words of the second column of the UD structures (see Figure 1) separated by a whitespace. 3 Evaluation Methods 3.1 Automatic methods We used BLEU, NIST, and inverse normalised character-based string-edit distance (referred to as DIST, for short, below) to assess submitted systems. BLEU (Papineni et al., 2002) is a precision metric that computes the geometric mean of the n-gram precisions between generated text and reference texts and adds a brevity penalty for shorter sentences. We use the smoothed version and report results for n = 4. NIST13 is a related n-gram similarity metric 13 http://www.itl.nist.gov/iad/mig/ tests/mt/doc/ngram-study.pdf; http:// www.itl.nist.gov/iad/mig/tests/mt/2009/ weighted in favor of less frequent n-grams which are taken to be more informative. DIST starts by computing the minimum number of character inserts, deletes and substitutions (all at cost 1) required to turn t"
D19-6301,N18-1202,0,0.019743,"however, permissible. For example, available parsers such as UUParser (Smith et al., 2018) could be run to create a silver standard versions of provided datasets and use them as additional or alternative training material, and publicly available off-the3 In the case of one team, we agreed to move the two week window between test data release and submission to one week earlier. 4 At SR’18, there were ten languages from five families. 5 https://www.aclweb.org/portal/ content/sigmorphon-shared-task-2019 6 universaldependencies.org shelf language models such as GPT-2 (Radford et al., 2019), ELMo (Peters et al., 2018), polyglot (Al-Rfou et al., 2013) or BERT (Devlin et al., 2018) could be fine-tuned with publicly available datasets such as WikiText (Merity et al., 2016) or the DeepMind Q&A Dataset (Hermann et al., 2015). Datasets were created for 11 languages in the Shallow Track, and for three of those languages, namely English, French and Spanish, in the Deep Track. As in 2018, Shallow Track inputs were generated with the aid of Python scripts from the original UD structures, this time using all available input sentences. Deep Track inputs were then generated by automatically processing the Shallow Track"
D19-6301,D19-6312,0,0.0504468,"els use the same bidirectional Tree-LSTM encoder architecture. The linearisation model orders each subtree separately with beam search and then combines them into a full projective tree; the completion model generates absent function words in a sequential way given the linearised tree of content words; the inflection model predicts a sequence of edit operations to convert the lemma to word form character by character; the contraction model predicts BIO tags to group the words to be contracted, and then generate the contracted word form of each group with a seq2seq model. The LORIA submission (Shimorina and Gardent, 2019) presents a modular approach to surface realisation with three subsequent steps: word ordering, morphological inflection, and contraction generation (for some languages). For word ordering, the data is delexicalised, the input tree is linearised, and the mapping between an input tree and output lemma sequence is learned using a factored sequence-to-sequence model. Morphological inflection makes use of a neural characterbased model, which produces word forms based on lemmas coupled with morphological features; finally, a rule-based contraction generation module is applied for some languages. Th"
D19-6301,K18-2011,1,0.820368,"Missing"
D19-6301,D19-6309,0,0.0269112,"rface realisation with three subsequent steps: word ordering, morphological inflection, and contraction generation (for some languages). For word ordering, the data is delexicalised, the input tree is linearised, and the mapping between an input tree and output lemma sequence is learned using a factored sequence-to-sequence model. Morphological inflection makes use of a neural characterbased model, which produces word forms based on lemmas coupled with morphological features; finally, a rule-based contraction generation module is applied for some languages. The OSU-FB pipeline for generation (Upasani et al., 2019) starts by generating inflected word forms in the tree using character seq2seq models. These inflected syntactic trees are then linearised as constituent trees by converting the relations to non-terminals. The linearised constituent trees are fed to seq2seq models (including models with copy and with tree-LSTM encoders) whose outputs also contain tokens marking the tree structure. N-best outputs are obtained for orderings and the highest confidence output sequence with a valid tree is chosen (i.e, one where the input and output trees are isomorphic up to sibling order, ensuring projectivity)."
D19-6301,D19-6306,0,0.0857639,"weighted directed acyclic graph (DAG) (equivalent to a partially ordered set, or poset) for each sentence, the topological sort of which generates a surface order. Inflection is addressed with regex patterns and substitutions approximating productive inflectional paradigms. The DipInfoUnito realiser (Mazzei and Basile, 2019) is a supervised statistical system for surface realisation, in which two neural network-based models run in parallel on the same input structure, namely a list-wise learning to rank network for linearisation and a seq2seq network for morphology inflection prediction. IMS (Yu et al., 2019) uses a pipeline approach for both tracks, consisting of linearisation, completion (for T2 only), inflection, and contraction. All models use the same bidirectional Tree-LSTM encoder architecture. The linearisation model orders each subtree separately with beam search and then combines them into a full projective tree; the completion model generates absent function words in a sequential way given the linearised tree of content words; the inflection model predicts a sequence of edit operations to convert the lemma to word form character by character; the contraction model predicts BIO tags to g"
dahab-belz-2010-game,poesio-etal-2008-anawiki,0,\N,Missing
E06-1040,W00-1401,0,0.709535,"se than the human evaluations that have traditionally been used to evaluate NLG systems. However, the use of such corpus-based evaluation metrics is only sensible if they are known to be correlated with the results of human-based evaluations. While studies have shown that ratings of MT systems by BLEU and similar metrics correlate well with human judgments (Papineni et al., 2002; Doddington, 2002), we are not aware of any studies that have shown that corpus-based evaluation metrics of NLG systems are correlated with human judgments; correlation studies have been made of individual components (Bangalore et al., 2000), but not of systems. In this paper we present an empirical study of how well various corpus-based metrics agree with human judgments, when evaluating several NLG systems that generate sentences which describe changes in the wind (for weather forecasts). These systems do not perform content determination (they are limited to microplanning and realisation), so our study does not address corpus-based evaluation of content determination. 2 Background 2.1 Evaluation of NLG systems NLG systems have traditionally been evaluated using human subjects (Mellish and Dale, 1998). NLG evaluations have tend"
E06-1040,W05-1601,1,0.923631,"he success of the BLEU evaluation metric (Papineni et al., 2002) in Machine Translation (MT), which has transformed the MT field by allowing researchers to quickly and cheaply evaluate the impact of new ideas, algorithms, and data sets. BLEU and related metrics work by comparing the output of an Ehud Reiter Dept of Computing Science University of Aberdeen UK ereiter@csd.abdn.ac.uk MT system to a set of reference (‘gold standard’) translations, and in principle this kind of evaluation could be done with NLG systems as well. Indeed NLG researchers are already starting to use BLEU (Habash, 2004; Belz, 2005) in their evaluations, as this is much cheaper and easier to organise than the human evaluations that have traditionally been used to evaluate NLG systems. However, the use of such corpus-based evaluation metrics is only sensible if they are known to be correlated with the results of human-based evaluations. While studies have shown that ratings of MT systems by BLEU and similar metrics correlate well with human judgments (Papineni et al., 2002; Doddington, 2002), we are not aware of any studies that have shown that corpus-based evaluation metrics of NLG systems are correlated with human judgm"
E06-1040,C96-1043,0,0.0658248,"NLG systems NLG systems have traditionally been evaluated using human subjects (Mellish and Dale, 1998). NLG evaluations have tended to be of the intrinsic type (Sparck Jones and Galliers, 1996), involving subjects reading and rating texts; usually subjects 313 are shown both NLG and human-written texts, and the NLG system is evaluated by comparing the ratings of its texts and human texts. In some cases, subjects are shown texts generated by several NLG systems, including a baseline system which serves as another point of comparison. This methodology was first used in NLG in the mid-1990s by Coch (1996) and Lester and Porter (1997), and continues to be popular today. Other, extrinsic, types of human evaluations of NLG systems include measuring the impact of different generated texts on task performance (Young, 1999), measuring how much experts postedit generated texts (Sripada et al., 2005), and measuring how quickly people read generated texts (Williams and Reiter, 2005). In recent years there has been growing interest in evaluating NLG texts by comparing them to a corpus of human-written texts. As in other areas of NLP, the advantages of automatic corpusbased evaluation are that it is pote"
E06-1040,E06-1045,0,0.0523936,"8 abilities, pCRU-roulette will tend to use different words and phrases in different texts, whereas the other statistical generators will stick to those with the highest frequency. This behaviour is penalised by the automatic evaluation metrics, but the human evaluators do not seem to mind it. One of the classic rules of writing is to vary lexical and syntactic choices, in order to keep text interesting. However, this behaviour (variation for variation’s sake) will always reduce a system’s score under corpus-similarity metrics, even if it enhances text quality from the perspective of readers. Foster and Oberlander (2006), in their study of facial gestures, have also noted that humans do not mind and indeed in some cases prefer variation, whereas corpus-based evaluations give higher ratings to systems which follow corpus frequency. Using more reference texts does counteract this tendency, but only up to a point: no matter how many reference texts are used, there will still be one, or a small number of, most frequent variants, and using anything else will still worsen corpussimilarity scores. Canvassing expert opinion of text quality and averaging the results is also in a sense frequencybased, as results reflec"
E06-1040,J97-1004,0,0.096838,"systems have traditionally been evaluated using human subjects (Mellish and Dale, 1998). NLG evaluations have tended to be of the intrinsic type (Sparck Jones and Galliers, 1996), involving subjects reading and rating texts; usually subjects 313 are shown both NLG and human-written texts, and the NLG system is evaluated by comparing the ratings of its texts and human texts. In some cases, subjects are shown texts generated by several NLG systems, including a baseline system which serves as another point of comparison. This methodology was first used in NLG in the mid-1990s by Coch (1996) and Lester and Porter (1997), and continues to be popular today. Other, extrinsic, types of human evaluations of NLG systems include measuring the impact of different generated texts on task performance (Young, 1999), measuring how much experts postedit generated texts (Sripada et al., 2005), and measuring how quickly people read generated texts (Williams and Reiter, 2005). In recent years there has been growing interest in evaluating NLG texts by comparing them to a corpus of human-written texts. As in other areas of NLP, the advantages of automatic corpusbased evaluation are that it is potentially much cheaper and quic"
E06-1040,N03-1020,0,0.0613763,"l reference translations (four appears to be standard in MT). Properly calculated BLEU scores have been shown to correlate reliably with human judgments (Papineni et al., 2002). The NIST MT evaluation metric (Doddington, 2002) is an adaptation of BLEU, but where BLEU gives equal weight to all n-grams, NIST gives more importance to less frequent (hence more informative) n-grams. BLEU’s ability to detect subtle but important differences in translation quality has been questioned, some research showing NIST to be more sensitive (Doddington, 2002; Riezler and Maxwell III, 2005). The ROUGE metric (Lin and Hovy, 2003) was conceived as document summarisation’s answer to BLEU , but it does not appear to have met with the same degree of enthusiasm. There are several different ROUGE metrics. The simplest is ROUGE-N, which computes the highest proportion in any reference summary of n-grams that are matched by the system-generated summary. A procedure is applied that averages the score across leave-oneout subsets of the set of reference texts. ROUGEN is an almost straightforward n-gram recall metric between two texts, and has several counterintuitive properties, including that even a text composed entirely of se"
E06-1040,P02-1040,0,0.107787,"Missing"
E06-1040,W02-2113,1,0.768323,"in other areas of NLP, the advantages of automatic corpusbased evaluation are that it is potentially much cheaper and quicker than human-based evaluation, and also that it is repeatable. Corpus-based evaluation was first used in NLG by Langkilde (1998), who parsed texts from a corpus, fed the output of her parser to her NLG system, and then compared the generated texts to the original corpus texts. Similar evaluations have been used e.g. by Bangalore et al. (2000) and Marciniak and Strube (2004). Such corpus-based evaluations have sometimes been criticised in the NLG community, for example by Reiter and Sripada (2002). Grounds for criticism include the fact that regenerating a parsed text is not a realistic NLG task; that texts can be very different from a corpus text but still effectively meet the system’s communicative goal; and that corpus texts are often not of high enough quality to form a realistic test. 2.2 Automatic evaluation of generated texts in MT and Summarisation The MT and document summarisation communities have developed evaluation metrics based on comparing output texts to a corpus of human texts, and have shown that some of these metrics are highly correlated with human judgments. The BLE"
E06-1040,W05-0908,0,0.0333862,"Missing"
E06-1040,W05-1616,1,0.746935,"gs of its texts and human texts. In some cases, subjects are shown texts generated by several NLG systems, including a baseline system which serves as another point of comparison. This methodology was first used in NLG in the mid-1990s by Coch (1996) and Lester and Porter (1997), and continues to be popular today. Other, extrinsic, types of human evaluations of NLG systems include measuring the impact of different generated texts on task performance (Young, 1999), measuring how much experts postedit generated texts (Sripada et al., 2005), and measuring how quickly people read generated texts (Williams and Reiter, 2005). In recent years there has been growing interest in evaluating NLG texts by comparing them to a corpus of human-written texts. As in other areas of NLP, the advantages of automatic corpusbased evaluation are that it is potentially much cheaper and quicker than human-based evaluation, and also that it is repeatable. Corpus-based evaluation was first used in NLG by Langkilde (1998), who parsed texts from a corpus, fed the output of her parser to her NLG system, and then compared the generated texts to the original corpus texts. Similar evaluations have been used e.g. by Bangalore et al. (2000)"
E06-1040,W02-2103,0,\N,Missing
hastie-belz-2014-comparative,N07-1021,1,\N,Missing
hastie-belz-2014-comparative,E09-1078,0,\N,Missing
hastie-belz-2014-comparative,W06-1410,0,\N,Missing
hastie-belz-2014-comparative,W09-2816,1,\N,Missing
hastie-belz-2014-comparative,W08-0128,0,\N,Missing
hastie-belz-2014-comparative,W07-2307,0,\N,Missing
hastie-belz-2014-comparative,P00-1019,0,\N,Missing
hastie-belz-2014-comparative,P13-1123,1,\N,Missing
hastie-belz-2014-comparative,W11-2832,1,\N,Missing
hastie-belz-2014-comparative,P01-1056,0,\N,Missing
hastie-belz-2014-comparative,W11-2002,1,\N,Missing
hastie-belz-2014-comparative,P04-1011,0,\N,Missing
hastie-belz-2014-comparative,E14-1074,1,\N,Missing
hastie-belz-2014-comparative,P08-1073,0,\N,Missing
hastie-belz-2014-comparative,P10-1008,0,\N,Missing
hastie-belz-2014-comparative,P89-1009,0,\N,Missing
hastie-belz-2014-comparative,2007.sigdial-1.23,0,\N,Missing
hastie-belz-2014-comparative,W11-2853,0,\N,Missing
hastie-belz-2014-comparative,D10-1049,0,\N,Missing
hastie-belz-2014-comparative,W11-2838,0,\N,Missing
hastie-belz-2014-comparative,W10-4233,0,\N,Missing
hastie-belz-2014-comparative,W11-2017,1,\N,Missing
hastie-belz-2014-comparative,P12-3009,0,\N,Missing
J09-1008,W08-1131,1,0.828564,"Missing"
J09-1008,P06-2053,0,0.0195712,"Missing"
J09-1008,C04-1072,0,0.027627,"s for ‘understudy’). Surrogate measures in science in general need to be tested in terms of their correlation with some reliable measure which is known to be a true indicator of the condition or property (e.g., karyotyping for chromosomal abnormalities) for which the surrogate measure (e.g., serum testing for speciﬁc protein types) is intended to be an approximate indicator. In HLT, we test (surrogate) automatic metrics in terms of their correlation with human ratings of quality, using Pearson’s product-moment correlation coefﬁcient, and sometimes Spearman’s rank-order correlation coefﬁcient (Lin and Och 2004). The stronger and more signiﬁcant the correlation, the better metrics are deemed to be. The human ratings are not tested. In this set-up, clearly, there is no way in which the results of human quality assessment can ever be shown to be wrong. If human judgment says a system is good, then if an automatic measure says the system is good, it simply conﬁrms human judgment; if the automatic measure says the system is bad, then the measure is a bad one, its results are disregarded, and the system is still a good system. This is a classic closedcircle set-up: It isn’t falsiﬁable, and it doesn’t incl"
J09-1008,S07-1009,0,0.0164182,"Missing"
J09-1008,P08-1006,0,0.0595321,"LDOCE, COMLEX , Roget, OALDCE and WordNet—would be relevant to a wide range of NLP applications. (Kilgarriff 1997, page 107) WSD may still be the most notorious “task in need of an application” (McCarthy and Navigli 2007), but the case of WSD points to a more general issue: in intrinsic evaluations, the output representation formalism (e.g., tag set, syntactic formalism) is ﬁxed in the form of gold-standard reference annotations, and alternative representations are not subject to evaluation. There is evidence that it may be worth looking at how different representations perform. For example, Miyao et al. (2008) found signiﬁcant differences between different parse representations given the same parser type when evaluating their effect on the performance of a biomedical IR tool. The intrinsic set-up makes it impossible to perform such evaluations of alternative representations, because this 3 Some examples are the NIST-run DUC document summarization evaluation campaign (now part of TAC), the NIST-run Open MT evaluation campaign (MT-Eval), and the academic-led SENSEVAL / SEMEVAL WSD evaluations, among many others. 4 All three techniques have been used in competitive evaluations: i and iii have been use"
J09-1008,W08-1203,0,0.0263617,"Missing"
J09-1008,W03-2409,0,0.0412904,"Missing"
J09-1008,H94-1018,0,0.0616736,"Missing"
J09-1008,W06-0707,0,0.158692,"Missing"
J09-1008,2003.mtsummit-papers.51,0,0.0434852,"Missing"
J09-1008,E06-1040,1,\N,Missing
J09-1008,J09-4008,1,\N,Missing
J09-1008,E03-1003,0,\N,Missing
J09-1008,J10-3003,0,\N,Missing
J09-1008,W09-0629,1,\N,Missing
J09-1008,P08-2050,1,\N,Missing
J09-1008,J10-3010,0,\N,Missing
J09-4008,W05-0909,0,0.0618254,"Missing"
J09-4008,W00-1401,0,0.0147265,"Missing"
J09-4008,2007.mtsummit-ucnlg.14,1,0.593518,"ologist” methodology described earlier (van der Meulen et al. 2009). SKILLSUM (Williams and Reiter 2008), which generates feedback reports from literacy assessments, was evaluated on the basis of educational effectiveness; we gave 200 assessment takers either SKILLSUM texts or control texts, and measured whether they increased the accuracy of self-assessments of their literacy skills. We also evaluated several referring-expression generation algorithms by conducting experiments in which participants were presented with generated referring expressions and asked to identify the target referent (Belz and Gatt 2007; Gatt, Belz, and Kow 2008, 2009); these were carried out in conjunction with shared-task events organized under the Generation Challenges initiative (Generation Challenges is further discussed in Section 2.1.4). Task-based evaluations have traditionally been regarded as the most meaningful kind of evaluation in NLG, especially in contexts where the evaluation needs to convince people in other communities (such as psychologists and doctors). However, they can be expensive and time-consuming. The STOP evaluation cost UK£75,000, and required 20 months to design, carry out, and analyze; the SKILL"
J09-4008,P08-2050,1,0.745331,"ally simulated system “outputs.” Probably the most similar study to our work is that by Stent, Marge, and Singhai (2005), who examined the correlation between human judgments and several automatic metrics when evaluating computer-generated paraphrases; this is further discussed in Section 3.3.3. Very recently some validation studies have been done in the context of the Generation Challenges initiative for shared tasks in NLG, by evaluating systems entered in the shared task using automatic metrics, human ratings, and task-based evaluation, and analyzing correlations between these. For example Belz and Gatt (2008) analyzed correlations between several automatic evaluation metrics and task performance in a referring-expression generation task; they found that there was no signiﬁcant correlation between any of the automatic metrics they looked at (which included specialized metrics for the reference task as well as BLEU and ROUGE) and their task-based measures of effectiveness, such as how long it took human subjects to identify objects from a referring expression, and how many mistakes the subjects made. However the different automatic metrics they examined did tend to correlate with each other, as did"
J09-4008,E06-1040,1,0.801973,"ference for the corpus texts is probably an artefact of the way the reference texts were produced. The forecasters were asked to rewrite the corpus texts, which resulted in considerable similarity between the reference texts and the corpus texts. In calculating correlation ﬁgures (shown in Table 6), we therefore produced two sets of ﬁgures, one for the NLG systems and the corpus texts (I in the table) and one for just the NLG systems (II); set II should be regarded as a post hoc analysis. For set I, none of the metrics signiﬁcantly correlate 5 When we ﬁrst reported results for Experiment 1 in Belz and Reiter (2006), we only had reference texts from two meteorologists, but we have since obtained reference texts from a third meteorologist. This is why the numbers in Tables 5 and 6 differ from the numbers given in Belz and Reiter (2006). 6 The important information in Table 5 is the differences in the scores assigned by the same metric to different systems. Differences in the scores assigned by different metrics to the same system are not meaningful; they are just mathematical artefacts of the formulas used to calculate the metrics. For example, the fact that BLEU-4 gives S UM T IME-Hybrid a higher score t"
J09-4008,P06-1130,0,0.0457868,"Missing"
J09-4008,W08-0309,0,0.0101201,"f NLP Of course, evaluation and experimentation are crucial to all ﬁelds of NLP; here we look at insights from two other NLP subﬁelds which need to evaluate the quality of texts: machine translation and document summarization. 534 Reiter and Belz Validity of Some Metrics for NLG Evaluation 2.2.1 Evaluation in Machine Translation. There is a rich literature in MT evaluation, including a number of specialist workshops on this topic; as in NLG, there is also considerable interest in using shared-task events to provide data about how well different evaluation techniques correlate with each other (Callison-Burch et al. 2008). From an NLG perspective, the most surprising aspect of current MT evaluation is the dominance of BLEU and other automatic corpus-based metrics (Callison-Burch, Osborne, and Koehn 2006). BLEU was ﬁrst proposed as a supplement (the U in BLEU stands for “understudy”) for human evaluation (Papineni et al. 2002), but it is now routinely used as the main technique for evaluating research contributions. It is accepted and indeed the norm for an article on MT in Computational Linguistics to report evaluations that are solely based on automatic corpus-based metrics; this is not the case in NLG, where"
J09-4008,E06-1032,0,0.0484019,"Missing"
J09-4008,W98-1435,0,0.013552,", and then analyzing the correlation between the techniques. One potential weakness of our experiments was that we did not look at correlations with task-effectiveness evaluations. This was because we did not have the resources (money and domain-expert goodwill) to conduct a task-based evaluation. This issue is further discussed in Section 4.2. 536 Reiter and Belz Validity of Some Metrics for NLG Evaluation 3.1 Domain and Systems Our work was done in the domain of computer-generated weather forecasts. This is one of the most popular applications of NLG (Goldberg, Driedger, and Kittredge 1994; Coch 1998; Reiter et al. 2005), and several NLG weather-forecast systems have been ﬁelded and used. Weather forecast generation is probably the closest that NLG comes to a “standard” application domain, and hence seems a good choice for validation studies from this perspective. On the other hand, though, one could also argue that weather-forecast generators are atypical in that the language they generate tends to be very simple, even by the standards of NLG systems: very limited syntax (which differs from conventional English), very small vocabulary, no real text structure above the sentence level, and"
J09-4008,2003.mtsummit-papers.9,0,0.0278439,"t have tried to correlate BLEU-like metrics with the results of task-effectiveness studies. Although a number of studies have analyzed the correlation between BLEU-type metrics and human judgments, most of these have used human judgments from NIST MT evaluations. Human judgments in most of these evaluations were solicited from monolingual subjects who were asked to compare the output of MT systems to a single reference translation, without any context; also in many of these studies the subjects were asked to assess individual sentences or even phrases, not complete texts (Doddington 2002). As Coughlin (2003) and others have pointed out, it is not clear that human judgments solicited in this way would match the judgments of bilingual subjects who were shown complete source and MT texts, and asked to evaluate the quality of the translation in a speciﬁc real-world context. Papineni et al. (2002) in fact found that BLEU scores were more highly correlated with human judgments from monolingual subjects than human judgments from bilingual subjects. In any case, regardless of the effectiveness of BLEU as an MT evaluation metric, another issue is whether an MT evaluation technique can in general be expect"
J09-4008,W06-0707,0,0.016538,"summarization track is the pyramid technique (Nenkova and Passonneau 2004), which is a structured human-based evaluation, based on asking human judges to identify ‘summarization content units’ (SCU) in model and system-generated summaries, and measuring how many SCUs from the model summaries occur in the system summary. This is an interesting technique for evaluating content, and might be worth investigating for evaluating content determination in NLG systems. In terms of validation, a number of studies have claimed that ROUGE correlates with human ratings, for example Lin and Hovy (2003) and Dang (2006). Dorr et al. (2005) checked if ROUGE scores correlated with task effectiveness; they did not ﬁnd a strong correlation. 2.3 Summary In summary, evaluation of NLG texts in the past has primarily been done using human subjects, either by measuring the impact of texts on task performance, or by asking subjects to rate texts. However, a growing number of NLG researchers are using automatic metrics to evaluate their systems, perhaps inspired by the popularity of automatic metrics in other areas of NLP which involve evaluating output texts, most notably machine translation and document summarization"
J09-4008,W02-2116,0,0.0868329,"Missing"
J09-4008,W05-0901,0,0.212341,"track is the pyramid technique (Nenkova and Passonneau 2004), which is a structured human-based evaluation, based on asking human judges to identify ‘summarization content units’ (SCU) in model and system-generated summaries, and measuring how many SCUs from the model summaries occur in the system summary. This is an interesting technique for evaluating content, and might be worth investigating for evaluating content determination in NLG systems. In terms of validation, a number of studies have claimed that ROUGE correlates with human ratings, for example Lin and Hovy (2003) and Dang (2006). Dorr et al. (2005) checked if ROUGE scores correlated with task effectiveness; they did not ﬁnd a strong correlation. 2.3 Summary In summary, evaluation of NLG texts in the past has primarily been done using human subjects, either by measuring the impact of texts on task performance, or by asking subjects to rate texts. However, a growing number of NLG researchers are using automatic metrics to evaluate their systems, perhaps inspired by the popularity of automatic metrics in other areas of NLP which involve evaluating output texts, most notably machine translation and document summarization. This use of metric"
J09-4008,W08-1131,1,0.851702,"Missing"
J09-4008,W08-1120,0,0.0338708,"keholders, in contrast, would be satisﬁed with a single evaluation at the end of the project. The software house would like to know if BabyTalk would be commercially proﬁtable. This partially depends on medical effectiveness (see previous point), which determines the demand for the system. But it also depends on how expensive it is to develop and support BabyTalk; from this perspective the company is especially interested in evaluations of the cost of adapting/porting BabyTalk to different hospitals in the NICU domain in the short term, and to different medical domains in the longer term. See Harris (2008) for a commercial perspective on medical NLG systems. All of these stakeholders are interested in evaluations which assess the quality and effectiveness of generated texts; such evaluations are the focus of our article. The software 530 Reiter and Belz Validity of Some Metrics for NLG Evaluation house and the computer scientists are also interested in engineering-cost evaluations; although this is a very important topic, we will not discuss it here: a separate article would be needed to do justice to this topic. 2.1 Evaluation in NLG The quality of texts generated by NLG systems has been evalu"
J09-4008,W02-2103,0,0.0398413,"has allowed MT researchers to quickly and cheaply evaluate the impact of new ideas, algorithms, and data sets. BLEU and related metrics work by comparing the output of an MT system to a set of reference translations (human translations of the source text), and in principle this kind of evaluation could be done with NLG systems as well. As in other areas of NLP, the advantages of automatic corpus-based evaluation are that it is potentially much cheaper and quicker than human-based evaluation, and that it is repeatable. Indeed, NLG researchers have used BLEU in their evaluations for some time (Langkilde 2002; Habash 2004). The use of such automatic evaluation metrics is, however, only sensible if they are known to be correlated with the results of reliable human-based evaluations. Although a number of previous studies have analyzed correlations between human judgments ∗ Department of Computing Science, University of Aberdeen, UK. E-mail: e.reiter@abdn.ac.uk. ∗∗ Natural Language Technology Group, University of Brighton, UK. E-mail: A.S.Belz@brighton.ac.uk. Submission received: 23 March 2007; revised submission received: 6 October 2008; accepted for publication: 29 December 2008. © 2009 Association"
J09-4008,P98-1116,0,0.0453349,"02; Lin and Hovy 2003), much less is known about how well automatic metrics correlate with human judgments in NLG. In this article we present two empirical studies of how well BLEU and various other corpus-based metrics agree with human judgments, when evaluating the outputs of several NLG systems that generate texts which describe changes in the wind (for weather forecasts). We also discuss several caveats that need to be kept in mind when interpreting our study and perhaps other validation studies of automatic metrics as well. 2. Background: Evaluation in NLG and Related Fields As Hirschman (1998), Mellish and Dale (1998), and others have pointed out, evaluations can be used for many purposes, and different evaluations are often needed for different stakeholders. For example, the BabyTalk project at Aberdeen (Portet et al. 2009), which is attempting to create a set of NLG systems which can generate textual summaries of clinical data about babies in a neonatal intensive care unit (NICU), is a collaboration between medical researchers, psychologists, computer scientists, and a commercial software house. Each of these groups has its own evaluation agenda: r r r r The medical researchers w"
J09-4008,J97-1004,0,0.018279,"d time costs, all of these evaluations also depended on goodwill from participants, in most cases busy domain experts who used their own standing in their community to arrange access to subjects and otherwise facilitate the evaluation. Such goodwill in itself is a scarce resource which must be used with care. 2.1.2 Evaluations Based on Human Ratings and Judgments. Another way of evaluating an NLG system is to ask human subjects to rate generated texts on an n-point rating scale; this is an intrinsic form of evaluation (Sp¨arck Jones and Galliers 1995). This methodology was ﬁrst used in NLG by Lester and Porter (1997), who asked eight domain experts to each rate 15 texts on a number of different dimensions: overall quality and coherence, content, organization, writing style, and correctness. Some of the texts were humanwritten and some were computer-generated, but the judges did not know the origin of speciﬁc texts they read. Many more such evaluations have been performed since, often with fewer dimensions. For example, Binsted, Pain, and Ritchie (1997) evaluated a jokegeneration system by asking children to rate the funniness of texts on a 5-point scale; and Walker, Rambow, and Rogati (2002) evaluated the"
J09-4008,N03-1020,0,0.841376,"evious studies have analyzed correlations between human judgments ∗ Department of Computing Science, University of Aberdeen, UK. E-mail: e.reiter@abdn.ac.uk. ∗∗ Natural Language Technology Group, University of Brighton, UK. E-mail: A.S.Belz@brighton.ac.uk. Submission received: 23 March 2007; revised submission received: 6 October 2008; accepted for publication: 29 December 2008. © 2009 Association for Computational Linguistics Computational Linguistics Volume 35, Number 4 and automatic evaluation metrics in machine translation and document summarization (Doddington 2002; Papineni et al. 2002; Lin and Hovy 2003), much less is known about how well automatic metrics correlate with human judgments in NLG. In this article we present two empirical studies of how well BLEU and various other corpus-based metrics agree with human judgments, when evaluating the outputs of several NLG systems that generate texts which describe changes in the wind (for weather forecasts). We also discuss several caveats that need to be kept in mind when interpreting our study and perhaps other validation studies of automatic metrics as well. 2. Background: Evaluation in NLG and Related Fields As Hirschman (1998), Mellish and Da"
J09-4008,N04-1019,0,0.0746361,"luation perspective, an important difference between MT and summarization is that summarization evaluations have placed much more emphasis on content determination. Perhaps in part because of this, the summarization community places more emphasis on human evaluations. Although there are automatic corpus-based metrics for summarization such as ROUGE (Lin and Hovy 2003), they do not seem to dominate summarization evaluation in the same way that BLEU-type metrics dominate MT evaluation. The main summarization evaluation technique in the NIST TAC 2008 summarization track is the pyramid technique (Nenkova and Passonneau 2004), which is a structured human-based evaluation, based on asking human judges to identify ‘summarization content units’ (SCU) in model and system-generated summaries, and measuring how many SCUs from the model summaries occur in the system summary. This is an interesting technique for evaluating content, and might be worth investigating for evaluating content determination in NLG systems. In terms of validation, a number of studies have claimed that ROUGE correlates with human ratings, for example Lin and Hovy (2003) and Dang (2006). Dorr et al. (2005) checked if ROUGE scores correlated with ta"
J09-4008,P03-1021,0,0.0118187,"oblems in a system and suggest improvements. From this 554 Reiter and Belz Validity of Some Metrics for NLG Evaluation perspective, an advantage of human evaluations is that human subjects can be asked to make free-text comments on the texts that they see, and these comments are often extremely useful from a diagnostic perspective. On the other hand, an advantage of automatic metrics is that they allow developers to rapidly evaluate changes to systems and algorithms; indeed, some machine translation researchers use automatic metrics to automatically tune parameters without human intervention (Och 2003). However, as Och points out, this is only sensible if automatic metrics are known to be very accurate predictors of text quality. Because our results suggest that current automatic metrics are not highly accurate predictors of the quality of texts produced by NLG systems, we recommend developers be cautious in using metrics for diagnostic evaluation, and do not use metrics for automatic parameter tuning. On the other hand, automatic metrics do have a potential advantage in small diagnostic evaluations, which is that they are not inﬂuenced by the individual preferences of a small number of hum"
J09-4008,P02-1040,0,0.145102,"sts. Our results suggest that, at least in this domain, metrics may provide a useful measure of language quality, although the evidence for this is not as strong as we would ideally like to see; however, they do not provide a useful measure of content quality. We also discuss a number of caveats which must be kept in mind when interpreting this and other validation studies. 1. Introduction Evaluation is becoming an increasingly important topic in Natural Language Generation (NLG), as in other ﬁelds of computational linguistics. Many NLG researchers are impressed by the BLEU evaluation metric (Papineni et al. 2002) in Machine Translation (MT), which has allowed MT researchers to quickly and cheaply evaluate the impact of new ideas, algorithms, and data sets. BLEU and related metrics work by comparing the output of an MT system to a set of reference translations (human translations of the source text), and in principle this kind of evaluation could be done with NLG systems as well. As in other areas of NLP, the advantages of automatic corpus-based evaluation are that it is potentially much cheaper and quicker than human-based evaluation, and that it is repeatable. Indeed, NLG researchers have used BLEU i"
J09-4008,W02-2113,1,0.816357,"and (in some cases) lexical choices do not vary; this means there is less concern about reference texts not adequately covering the solution space. Automatic corpus-based evaluations are appealing in NLG, as in other areas of NLP, because they are relatively cheap and quick to do if a corpus is available, do not require support from domain experts, and are repeatable. However, their use in NLG is controversial, at least when evaluating systems as a whole instead of just surface realizers, because many people are concerned that the results of such evaluations may not be meaningful. For example Reiter and Sripada (2002) point out that corpus texts are often not of high enough quality to form good reference texts; and Scott and Moore (2007) express concern that metrics will not be able to evaluate many important linguistic properties such as information structure. 533 Computational Linguistics Volume 35, Number 4 A more general concern is that automatic metrics based on comparison to reference texts measure how well a text matches what writers do, whereas most human evaluations (task or judgment-based) measure the impact of a text on readers. Because writers do not always produce optimal texts from a reader’s"
J09-4008,W03-0611,1,0.729602,"ed a corpus and data set, called S UM T IME -M ETEO (Sripada et al. 2003). This consists of a corpus of 1,045 weather forecasts written by professional forecasters, and the numerical predictions of wind, temperature, and so forth, that forecasters examined when they wrote the forecasts. For wind descriptions only, the corpus also contains simple content representations containing information about wind speed and direction, time of day, and position in forecast (we call these “content tuples”). The content tuples were created by parsing the corpus texts and extracting the relevant information (Reiter and Sripada 2003), and are similar to the representations produced by the S UM T IME content-determination system. Figures 1, 2, and 3 show an extract from a numerical data ﬁle, an extract from the corresponding human-written forecast, and the content tuples derived from the human text. 537 Computational Linguistics day/hour 05/06 05/09 05/12 05/15 05/18 05/21 06/00 wind direction SSW S S S SSE SSE VAR Volume 35, Number 4 avg wind speed 18 16 14 14 12 10 6 max (gust) wind speed 22 20 17 17 15 12 7 Figure 1 Extract from meteorological data ﬁle for 05-10-2000 (morning forecast). FORECAST 06-24 GMT, THURSDAY, WIN"
J09-4008,W09-0629,1,\N,Missing
J09-4008,J09-1008,1,\N,Missing
J09-4008,C98-1112,0,\N,Missing
J09-4008,J98-3006,0,\N,Missing
kow-belz-2012-lg,W11-2844,0,\N,Missing
kow-belz-2012-lg,W11-2819,0,\N,Missing
kow-belz-2012-lg,belz-gatt-2012-repository,1,\N,Missing
N07-1021,2006.amta-papers.8,0,0.0390342,"Missing"
N07-1021,P98-1116,0,0.194772,"stem, and (iii) a HALOGEN-style statistical generator. The most striking result is that despite acquiring all decision-making abilities automatically, the best pCRU generators receive higher scores from human judges than forecasts written by experts. 1 Introduction and background Over the last decade, there has been a lot of interest in statistical techniques among researchers in natural language generation (NLG), a field that was largely unaffected by the statistical revolution in NLP that started in the 1980s. Since Langkilde and Knight’s influential work on statistical surface realisation (Knight and Langkilde, 1998), a number of statistical and corpus-based methods have been reported. However, this interest does not appear to have translated into practice: of the 30 implemented systems and modules with development starting in or after 2000 that are listed on a key NLG website1 , only five have any statistical component at all (another six involve techniques that are in some way corpus-based). The likely reasons for this lack of take-up are that (i) many existing statistical NLG techniques are inherently expensive, requiring the set of alternatives to be generated in full before the statistical model is a"
N07-1021,P05-1008,0,0.0481131,"systems, http://www.fb10.uni-bremen.de/anglistik/ langpro/NLG-table/, 20/01/2006. 164 Proceedings of NAACL HLT 2007, pages 164–171, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics of time and expense involved in building new systems, and the almost complete lack in the field of reusable systems and modules. Both trends have the potential to improve on development time and reusability, but have drawbacks. Existing statistical NLG (i) uses corpus statistics to inform heuristic decisions in what is otherwise symbolic generation (Varges and Mellish, 2001; White, 2004; Paiva and Evans, 2005); (ii) applies n-gram models to select the overall most likely realisation after generation (HALOGEN family); or (iii) reuses an existing parsing grammar or treebank for surface realisation (Velldal et al., 2004; Cahill and van Genabith, 2006). N -gram models are not linguistically informed, (i) and (iii) come with a substantial manual overhead, and (ii) overgenerates vastly and has a high computational cost (see also Section 3). Existing comprehensive approaches tend to incur a manual overhead (finetuning in ICONOCLAST, corpus annotation in Langkilde and Marciniak & Strube). Handling violabil"
N07-1021,P02-1040,0,0.0729798,"NG VARIABLE 4-8 BY LATE EVENING SSW 16-20 GRADUALLY BACKING SSE THEN BECOMING VARIABLE 10 OR LESS BY MIDNIGHT SSW 16-20 BACKING SSE FOR A TIME THEN FALLING VARIABLE 4-8 BY LATE EVENING SSW 16-20 GRADUALLY BACKING SSE AND VARIABLE 4-8 SSW 16-20 BACKING SSE VARIABLE 4-8 LATER SSW 16-20 BACKING SSE VARIABLE 4-8 LATER SSW 16-20 AT FIRST FROM MIDDAY BECOMING SSE DURING THE AFTERNOON THEN VARIABLE 4-8 Table 1: Forecast texts (for 05-10-2000) generated by each of the pCRU generators, the S UM T IME-Hybrid system and three experts. The corresponding input to the generators is shown in the first row. (Papineni et al., 2002) is a precision metric that assesses the quality of a translation in terms of the proportion of its word n-grams (n ≤ 4 has become standard) that it shares with several reference translations. BLEU also incorporates a ‘brevity penalty’ to counteract scores increasing as length decreases. BLEU scores range from 0 to 1. The NIST metric (Doddington, 2002) is an adaptation of BLEU, but where BLEU gives equal weight to all n-grams, NIST gives more weight to less frequent (hence more informative) n-grams. There is evidence that NIST correlates better with human judgments than BLEU (Doddington, 2002;"
N07-1021,E06-1040,1,0.938676,". pCRU-1.0 was then run in all five modes to generate forecasts for the inputs in both training and test sets. This procedure was repeated five times for holdout cross-validation. The small amount of variation across the five repeats, and the small differences between results for training and test sets (Table 2) indicated that five repeats were sufficient. 3.4 Evaluation 3.4.1 Evaluation methods The two automatic metrics used in the evaluations, NIST and BLEU have been shown to correlate highly with expert judgments (Pearson correlation coefficients 0.82 and 0.79 respectively) in this domain (Belz and Reiter, 2006). Input Corpus Reference 1 Reference 2 S UM T IME-Hyb. pCRU-greedy pCRU-roulette pCRU-viterbi pCRU-2gram pCRU-random [[1,SSW,16,20,-,-,0600],[2,SSE,-,-,-,-,NOTIME],[3,VAR,04,08,-,-,2400]] SSW 16-20 GRADUALLY BACKING SSE THEN FALLING VARIABLE 4-8 BY LATE EVENING SSW’LY 16-20 GRADUALLY BACKING SSE’LY THEN DECREASING VARIABLE 4-8 BY LATE EVENING SSW 16-20 GRADUALLY BACKING SSE BY 1800 THEN FALLING VARIABLE 4-8 BY LATE EVENING SSW 16-20 GRADUALLY BACKING SSE THEN BECOMING VARIABLE 10 OR LESS BY MIDNIGHT SSW 16-20 BACKING SSE FOR A TIME THEN FALLING VARIABLE 4-8 BY LATE EVENING SSW 16-20 GRADUALLY"
N07-1021,W05-1601,1,0.898285,"ft constraints is problematic, and converting corpus-derived probabilities into costs associated with constraints (Langkilde, Marciniak & Strube) turns straightforward statistics into an ad hoc search heuristic. Older approaches are not globally optimisable (PAULINE) or involve exhaustive search (ICONOCLAST). The pCRU language generation framework combines a probabilistic generation methodology with a comprehensive model of the generation space, where probabilistic choice informs generation as it goes along, instead of after all alternatives have been generated. pCRU uses existing techniques (Belz, 2005), but extends these substantially. This paper describes the pCRU framework and reports experiments designed to rigorously test pCRU in practice and to determine whether improvements in development time and reusability can be achieved without sacrificing quality of outputs. 2 pCRU language generation pCRU (Belz, 2006) is a probabilistic language generation framework that was developed with the aim of providing the formal underpinnings for creating NLG systems that are driven by comprehensive probabilistic models of the entire generation space (including deep generation). NLG systems tend to be"
N07-1021,P06-1130,0,0.0790806,"Missing"
N07-1021,2003.mtsummit-papers.6,0,0.0634226,"Missing"
N07-1021,C00-2093,0,0.0282672,"the most likely; and (ii) statistical NLG techniques have not been shown to produce outputs of high enough quality. There has also been a rethinking of the traditional modular NLG architecture (Reiter, 1994). Some research has moved towards a more comprehensive view, e.g. construing the generation task as a single constraint satisfaction problem. Precursors to current approaches were Hovy’s PAULINE which kept track of the satisfaction status of global ‘rhetorical goals’ (Hovy, 1988), and Power et al.’s ICON OCLAST which allowed users to fine-tune different combinations of global constraints (Power, 2000). In recent comprehensive approaches, the focus is on automatic adaptability, e.g. automatically determining degrees of constraint violability on the basis of corpus frequencies. Examples include Langkilde’s (2005) general approach to generation and parsing based on constraint optimisation, and Marciniak and Strube’s (2005) integrated, globally optimisable network of classifiers and constraints. Both probabilistic and recent comprehensive trends have developed at least in part to address two interrelated issues in NLG: the considerable amount 1 Bateman and Zock’s list of NLG systems, http://ww"
N07-1021,W94-0319,0,0.0736543,"in or after 2000 that are listed on a key NLG website1 , only five have any statistical component at all (another six involve techniques that are in some way corpus-based). The likely reasons for this lack of take-up are that (i) many existing statistical NLG techniques are inherently expensive, requiring the set of alternatives to be generated in full before the statistical model is applied to select the most likely; and (ii) statistical NLG techniques have not been shown to produce outputs of high enough quality. There has also been a rethinking of the traditional modular NLG architecture (Reiter, 1994). Some research has moved towards a more comprehensive view, e.g. construing the generation task as a single constraint satisfaction problem. Precursors to current approaches were Hovy’s PAULINE which kept track of the satisfaction status of global ‘rhetorical goals’ (Hovy, 1988), and Power et al.’s ICON OCLAST which allowed users to fine-tune different combinations of global constraints (Power, 2000). In recent comprehensive approaches, the focus is on automatic adaptability, e.g. automatically determining degrees of constraint violability on the basis of corpus frequencies. Examples include"
N07-1021,C98-1112,0,\N,Missing
P08-2050,W00-1401,0,0.0648136,"the similarity between a peer attribute set A1 and a (human-produced) reference 1 ∩A2 | attribute set A2 as 2×|A |A1 |+|A2 |. MASI (Passonneau, 2006) is similar but biased in favour of similarity where one set is a subset of the other. 4. String-similarity measures: In order to apply string-similarity metrics, peer and reference outputs were converted to word-strings by the method described under 1 above. String-edit distance (SE) is straightforward Levenshtein distance with a substitution cost of 2 and insertion/deletion cost of 1. We also used the version of string-edit distance (‘SEB’) of Bangalore et al. (2000) which normalises for length. BLEU computes the proportion of word ngrams (n ≤ 4 is standard) that a peer output shares with several reference outputs. The NIST MT evaluation metric (Doddington, 2002) is an adaptation of BLEU which gives more importance to less frequent (hence more informative) n-grams. We also used two versions of the ROUGE metric (Lin and Hovy, 2003), ROUGE-2 and ROUGE-SU4 (based on non-contiguous, or ‘skip’, n-grams), which were official scores in the DUC 2005 summarization task. 4 Results Results for all evaluation measures and all systems are shown in Table 1. Uniqueness"
P08-2050,2007.mtsummit-ucnlg.14,1,0.910466,"a and Systems 1 Introduction In recent years, NLG evaluation has taken on a more comparative character. NLG now has evaluation results for comparable, but independently developed systems, including results for systems that regenerate the Penn Treebank (Langkilde, 2002) and systems that generate weather forecasts (Belz and Reiter, 2006). The growing interest in comparative evaluation has also resulted in a tentative interest in shared-task evaluation events, which led to the first such event for NLG (the Attribute Selection for Generation of Referring Expressions, or ASGRE, Challenge) in 2007 (Belz and Gatt, 2007), with a second event (the Referring Expression Generation, or REG, Challenge) currently underway. In HLT in general, comparative evaluations (and shared-task evaluation events in particular) are dominated by intrinsic evaluation methodologies, in contrast to the more extrinsic evaluation traditions of NLG . In this paper, we present research in which we applied both intrinsic and extrinsic evaluation methods to the same task, in order to shed light on how Referring expression generation (REG) is concerned with the generation of expressions that describe entities in a given piece of discourse."
P08-2050,E06-1040,1,0.684901,"task-performance evaluations more in keeping with NLG traditions, to 15 systems implementing a language generation task. We analyse the evaluation results and find that there are no significant correlations between intrinsic and extrinsic evaluation measures for this task. 2 Task, Data and Systems 1 Introduction In recent years, NLG evaluation has taken on a more comparative character. NLG now has evaluation results for comparable, but independently developed systems, including results for systems that regenerate the Penn Treebank (Langkilde, 2002) and systems that generate weather forecasts (Belz and Reiter, 2006). The growing interest in comparative evaluation has also resulted in a tentative interest in shared-task evaluation events, which led to the first such event for NLG (the Attribute Selection for Generation of Referring Expressions, or ASGRE, Challenge) in 2007 (Belz and Gatt, 2007), with a second event (the Referring Expression Generation, or REG, Challenge) currently underway. In HLT in general, comparative evaluations (and shared-task evaluation events in particular) are dominated by intrinsic evaluation methodologies, in contrast to the more extrinsic evaluation traditions of NLG . In this"
P08-2050,P89-1009,0,0.360234,"ents in particular) are dominated by intrinsic evaluation methodologies, in contrast to the more extrinsic evaluation traditions of NLG . In this paper, we present research in which we applied both intrinsic and extrinsic evaluation methods to the same task, in order to shed light on how Referring expression generation (REG) is concerned with the generation of expressions that describe entities in a given piece of discourse. REG research goes back at least to the 1980s (Appelt, Grosz, Joshi, McDonald and others), but the field as it is today was shaped in particular by Dale and Reiter’s work (Dale, 1989; Dale and Reiter, 1995). REG tends to be divided into the stages of attribute selection (selecting properties of entities) and realisation (converting selected properties into word strings). Attribute selection in its standard formulation was the shared task in the ASGRE Challenge: given an intended referent (‘target’) and the other domain entities (‘distractors’) each with possible attributes, select a set of attributes for the target referent. The ASGRE data (which is now publicly available) consists of all 780 singular items in the TUNA corpus (Gatt et al., 2007) in two subdomains, consist"
P08-2050,W07-2307,1,0.613693,"Missing"
P08-2050,W02-2103,0,0.0151408,"current comparative HLT evaluation, and (ii) extrinsic, human task-performance evaluations more in keeping with NLG traditions, to 15 systems implementing a language generation task. We analyse the evaluation results and find that there are no significant correlations between intrinsic and extrinsic evaluation measures for this task. 2 Task, Data and Systems 1 Introduction In recent years, NLG evaluation has taken on a more comparative character. NLG now has evaluation results for comparable, but independently developed systems, including results for systems that regenerate the Penn Treebank (Langkilde, 2002) and systems that generate weather forecasts (Belz and Reiter, 2006). The growing interest in comparative evaluation has also resulted in a tentative interest in shared-task evaluation events, which led to the first such event for NLG (the Attribute Selection for Generation of Referring Expressions, or ASGRE, Challenge) in 2007 (Belz and Gatt, 2007), with a second event (the Referring Expression Generation, or REG, Challenge) currently underway. In HLT in general, comparative evaluations (and shared-task evaluation events in particular) are dominated by intrinsic evaluation methodologies, in c"
P08-2050,N03-1020,0,0.107985,"Missing"
P08-2050,passonneau-2006-measuring,0,0.111661,"Missing"
P11-2040,D10-1049,0,0.0165366,"face and the label “statement couldn’t be more unclear/read worse”. The raters’ task was to move a pointer (initially in the middle of the line) to the place corresponding to their rating. 3 Data Weather forecast texts: In one half of our evaluation experiments we used human-written and automatically generated weather forecasts for the same weather data. The data in our evaluations was for 22 different forecast dates and included outputs from 10 generator systems and one set of human forecasts. This data has also been used for comparative system evaluation in previous research (Langner, 2010; Angeli et al., 2010; Belz and Kow, 2009). The following are examples of weather forecast texts from the data: 1: SSE 28-32 INCREASING 36-40 BY MID AFTERNOON 2: S’LY 26-32 BACKING SSE 30-35 BY AFTERNOON INCREASING 35-40 GUSTS 50 BY MID EVENING Image descriptions: In the other half of our evaluations, we used human-written and automatically generated image descriptions for the same images. The data in our evaluations was for 112 different image sets and included outputs from 6 generator systems and 2 sets of human-authored descriptions. This data was originally created in the TUNA Project (van Deemter et al., 2006"
P11-2040,W09-0603,1,0.810633,"tatement couldn’t be more unclear/read worse”. The raters’ task was to move a pointer (initially in the middle of the line) to the place corresponding to their rating. 3 Data Weather forecast texts: In one half of our evaluation experiments we used human-written and automatically generated weather forecasts for the same weather data. The data in our evaluations was for 22 different forecast dates and included outputs from 10 generator systems and one set of human forecasts. This data has also been used for comparative system evaluation in previous research (Langner, 2010; Angeli et al., 2010; Belz and Kow, 2009). The following are examples of weather forecast texts from the data: 1: SSE 28-32 INCREASING 36-40 BY MID AFTERNOON 2: S’LY 26-32 BACKING SSE 30-35 BY AFTERNOON INCREASING 35-40 GUSTS 50 BY MID EVENING Image descriptions: In the other half of our evaluations, we used human-written and automatically generated image descriptions for the same images. The data in our evaluations was for 112 different image sets and included outputs from 6 generator systems and 2 sets of human-authored descriptions. This data was originally created in the TUNA Project (van Deemter et al., 2006). The following is a"
P11-2040,W09-0629,1,0.831037,"y horizontal line (although vertical lines have also been used (Scott and Huskisson, 2003)), on which the two end points represent the extreme values of the variable to be measured. Such lines can be mono-polar or bi-polar, and the end points are labelled with an image (smiling/frowning face), or a brief verbal descriptor, to indicate which end of the line corresponds to which extreme of the variable. The labels are commonly chosen to represent a point beyond any response actually likely to be chosen by raters. There is only one examples of a VAS in NLP system evaluation that we are aware of (Gatt et al., 2009). Hybrid scales, known as a graphic rating scales, combine the features of VDSs and VASs, and are also used in psychology. Here, the verbal descriptors are aligned along the line of a VAS and the endpoints are typically unmarked (Svensson, 2000). We are aware of one example in NLP (Williams and Reiter, 2008); 231 Q1: Grammaticality The summary should have no datelines, system-internal formatting, capitalization errors or obviously ungrammatical sentences (e.g., fragments, missing components) that make the text difficult to read. extremely bad excellent Figure 2: Evaluation of Grammaticality wi"
P11-2040,W06-1420,0,0.061293,"Missing"
W01-0712,A00-2018,0,\N,Missing
W01-0712,J93-2004,0,\N,Missing
W01-0712,A00-2007,1,\N,Missing
W01-0712,W00-0729,1,\N,Missing
W01-0712,C00-1085,1,\N,Missing
W01-0712,C00-1034,1,\N,Missing
W01-0712,W00-0702,1,\N,Missing
W01-0712,A88-1019,0,\N,Missing
W01-0712,W00-0733,1,\N,Missing
W01-0712,J96-1002,0,\N,Missing
W01-0712,W00-0726,1,\N,Missing
W01-0712,W00-0731,1,\N,Missing
W01-0712,W01-0702,1,\N,Missing
W01-0712,P98-1081,0,\N,Missing
W01-0712,C98-1078,0,\N,Missing
W01-0712,W00-0727,1,\N,Missing
W01-0712,C00-2124,1,\N,Missing
W01-0712,W99-0708,1,\N,Missing
W01-0712,W00-0730,0,\N,Missing
W05-1601,P00-1059,0,0.0360201,"Missing"
W05-1601,C00-1007,0,0.280786,"Missing"
W05-1601,W00-1401,0,0.0875465,"Missing"
W05-1601,J93-1002,0,0.149792,"Missing"
W05-1601,W01-0812,0,0.0376316,"Missing"
W05-1601,P98-1116,0,0.50062,"Missing"
W05-1601,1994.amta-1.18,0,0.0370016,"Missing"
W05-1601,A00-2023,0,0.32905,"Missing"
W05-1601,E91-1004,0,0.0743718,"Missing"
W05-1601,P00-1012,0,0.0335302,"Missing"
W05-1601,W00-0306,0,0.0788892,"Missing"
W05-1601,2001.mtsummit-papers.68,0,0.0215555,"Missing"
W05-1601,A00-2026,0,0.44613,"Missing"
W05-1601,P99-1018,0,0.0336241,"Missing"
W05-1601,A00-2003,0,0.0346878,"Missing"
W05-1601,N01-1001,0,0.0253622,"Missing"
W05-1601,P02-1040,0,\N,Missing
W05-1601,C98-1112,0,\N,Missing
W05-1601,1991.iwpt-1.22,0,\N,Missing
W06-1421,E06-1040,1,0.880254,"on of results, and by isolation from the rest of NLP where STE is now standard. It is, moreover, a shrinking field (state-of-the-art MT and summarisation no longer perform generation as a subtask) which lacks the kind of funding and participation that natural language understanding (NLU) has attracted. Evidence from other NLP fields shows that STE campaigns (STECs) can lead to rapid technological progress and substantially increased participation. The past year has seen a groundswell of interest in comparative evaluation among NLG researchers, the first comparative results are being reported (Belz and Reiter, 2006), and the move towards some form of comparative evaluation seems inevitable. In this paper we look at how two decades of NLP STECs might help us decide how best to make this move. Evaluation: NLP STECs have tended to use automatic evaluations because of their speed and reproducibility, but some have used human evaluators, in particular in fields where language is generated (MT, summarisation, speech synthesis). Evaluation scores are not independent of the task and context for which they are calculated. This is clearly true of human-based evaluation, but even scores by a simple metric like word"
W06-1421,W03-2808,1,0.836059,"to consolidate and progress collectively. Conforming to the evaluation paradigm now common to the rest of NLP will also help re-integration, and open up the field to new researchers. Sharing: As PARSEVAL shows, measures and resources alone are not enough. Also required are (i) an event (or better, cycle of events) so people can attend and feel part of a community; (ii) a forum for reviewing task definitions and evaluation methods; (iii) a committee which ‘owns’ the STEC, and organises the next campaign. Funding is usually needed for gold-standard corpus creation but rarely for anything else (Kilgarriff, 2003). Participants can be expected to cover the cost of system development and workshop attendance. A funded project is best seen as supporting and enabling the STEC (especially during the early stages) rather than being it. In sum, STECs are good for community building. They produce energy (as we saw when the possibility was raised for NLG at UCNLG’05 and ENLG ’05) which can lead to rapid scientific and technological progress. They make the field look like a game and draw people in. Tasks: In defining sharable tasks with associated data resources for NLG, the core problem is deciding what inputs"
W06-1421,W06-1422,1,0.715035,"evaluation (as in Morpholympics). An alternative is to approach the issue through tasks with inputs and outputs that ‘occur naturally’, so that participants can use their own NLG-specific representations. Examples include data-to-text mappings where e.g. time-series data or a data repository are mapped to fault reports, forecasts, etc. Both data-independent task definitions and tasks with naturally occurring data have promise, but we propose the second as the simpler, easier to organise solution, at least initially. A specific proposal of a set of tasks can be found elsewhere in this volume (Reiter and Belz, 2006). An interesting idea (recommended by ELRA / ELDA) is to break down the input-output mapping into stages (as in the TC - STAR workshops, see table) and then, in a second round of evaluations, to make available intermediate representations from the most successful systems from the first round. In this way, standardised representations might develop almost as a side-effect of STECs. (Belz and Reiter, 2006), and there is a lot of scepticism among NLG researchers regarding automatic evaluation. We believe that NLG should develop its own automatic metrics (development of such metrics is part of the"
W06-1422,W00-1401,0,0.194158,"s, including machine translation and information extraction (see discussion of history of shared-task initiatives and their impact in Belz and Kilgarriff (2006)). One of the best-known comparative studies of evaluation techniques was by Papineni et al. (2002) who proposed the BLEU metric for machine translation and showed that BLEU correlated well with human judgements when comparing several machine translation systems. Several other studies of this type have been carried out in the MT and Summarisation communities. The first comparison of NLG evaluation techniques which we are aware of is by Bangalore et al. (2000). The authors manually created several variants of sentences from the Wall Street Journal, and evaluated these sentences using both human judgements and several corpus-based metrics. They used linear regression to suggest a combination of the corpus-based metrics which they be136 Proceedings of the Fourth International Natural Language Generation Conference, pages 136–138, c Sydney, July 2006. 2006 Association for Computational Linguistics We will create the semantic-level representations by parsing the corpus texts, probably using a LinGO parser1 . We will create the content representations u"
W06-1422,W06-1421,1,0.829864,"ems with similar functionalities. Correlating the results of the different evaluation techniques will give us empirical insight as 2 Comparative Evaluations in NLG There is a long history of shared task initiatives in NLP, of which the best known is perhaps MUC (Hirschman, 1998); others include TREC, PARSE VAL, SENSEVAL , and the range of shared tasks organised by CoNLL. Such exercises are now common in most areas of NLP, and have had a major impact on many areas, including machine translation and information extraction (see discussion of history of shared-task initiatives and their impact in Belz and Kilgarriff (2006)). One of the best-known comparative studies of evaluation techniques was by Papineni et al. (2002) who proposed the BLEU metric for machine translation and showed that BLEU correlated well with human judgements when comparing several machine translation systems. Several other studies of this type have been carried out in the MT and Summarisation communities. The first comparison of NLG evaluation techniques which we are aware of is by Bangalore et al. (2000). The authors manually created several variants of sentences from the Wall Street Journal, and evaluated these sentences using both human"
W06-1422,E06-1040,1,0.926429,"l summaries of statistical data. The actual data will come from opinion polls or national statistics offices. The corpus will also include data about the authors (e.g., age, sex, domain expertise). Nurses’ reports: As part of a new project at Aberdeen, Babytalk2 , we will be acquiring a corpus of texts written by nurses to summarise the status of a baby in a neonatal intensive care unit, along with the raw data this is based on (sensor readings, records of actions taken such as giving medication). lieve is a better predictor of human judgements than any of the individual metrics. In our work (Belz and Reiter, 2006), we used several different evaluation techniques (human and corpus-based) to evaluate the output of five NLG systems which generated wind descriptions for weather forecasts. We then analysed how well the corpus-based evaluations correlated with the human-based evaluations. Amongst other things, we concluded that BLEU-type metrics work reasonably well when comparing statistical NLG systems, but less well when comparing statistical NLG systems to knowledge-based NLG systems. We worked in this domain because of the availability of the SumTime corpus (Sripada et al., 2003), which contains both nu"
W06-1422,P02-1040,0,0.087069,"ive us empirical insight as 2 Comparative Evaluations in NLG There is a long history of shared task initiatives in NLP, of which the best known is perhaps MUC (Hirschman, 1998); others include TREC, PARSE VAL, SENSEVAL , and the range of shared tasks organised by CoNLL. Such exercises are now common in most areas of NLP, and have had a major impact on many areas, including machine translation and information extraction (see discussion of history of shared-task initiatives and their impact in Belz and Kilgarriff (2006)). One of the best-known comparative studies of evaluation techniques was by Papineni et al. (2002) who proposed the BLEU metric for machine translation and showed that BLEU correlated well with human judgements when comparing several machine translation systems. Several other studies of this type have been carried out in the MT and Summarisation communities. The first comparison of NLG evaluation techniques which we are aware of is by Bangalore et al. (2000). The authors manually created several variants of sentences from the Wall Street Journal, and evaluated these sentences using both human judgements and several corpus-based metrics. They used linear regression to suggest a combination"
W06-1422,W02-2113,1,\N,Missing
W07-2302,N01-1002,0,0.0232326,"onut corpus of dialogues and showed that some dialogue states and communicative goals make overspecific REs more likely. Among the few corpora of texts within which REs have been annotated in some way (as opposed to corpora of annotated REs such as those created by van Deemter et al. (2006)) are the GNOME, Coconut and Maptask corpora. In the GNOME Corpus (Poesio, 2000; Poesio, 2004) different types of discourse and semantic information are annotated, including reference and semantic attributes. The corpus annotation was e.g. used to train a decision tree learner 10 for NP modifier generation (Cheng et al., 2001). The RE annotations in the Coconut corpus represent information at the discourse level (reference and attributes used) and at the utterance level (information about dialogue state). The 400 REs and annotations in the corpus were used to train an RE generation module (Jordan and Walker, 2000). Gupta and Stent (2005) annotated both the Maptask and Coconut corpora for POS-tags, NPs, referent of NPs, and knowledge representations for each speaker which included values for different attributes for potential referents. While context has been taken into account to some extent in existing research on"
W07-2302,P89-1009,0,0.0741194,": encyclopaedic entries. In this paper, we describe a corpus of such texts we have compiled and annotated (Section 3), and report first insights from our analysis of the corpus data (Section 4). We further report the results of an experiment where subjects selected REs in context (Section 5), and establish baseline results for automatic methods of selection (Section 6). 2 Related Research The most classical form of GRE algorithm takes into account two main factors in selecting expressions: unique identification (of the intended referent from a set including possible distractors), and brevity (Dale, 1989; Reiter and Dale, 1992). Most GRE research focuses on definite, non-first mentions of the target referent. The most influential of these algorithms, the ‘incremental algorithm’ (IA) (Dale and Reiter, 1995), originally just selected attributive properties, but a range of extensions have been reported. Siddharthan and Copestake’s algorithm (2004) is able to identify attributes that are particularly discriminating given the entities in the contrast set, and van Deemter’s SET algorithm can generate REs to sets of entities (van Deemter, 2002). Krahmer and Theune (2002) moved away from unique ident"
W07-2302,C00-1045,0,0.239918,"ble to identify attributes that are particularly discriminating given the entities in the contrast set, and van Deemter’s SET algorithm can generate REs to sets of entities (van Deemter, 2002). Krahmer and Theune (2002) moved away from unique identification, also taking discourse context into account: they replaced the requirement that the intended referent be the only entity that matches the RE, to the requirement that it be the most salient in a given context. Several versions of centering theory have been used as a basis for pronominalisation algorithms (Dale, 1992; McCoy and Strube, 1999; Henschel et al., 2000). Jordan (2002) highlighted a factor other than salience that influences choice of RE: she found a large proportion of overspecified redescriptions in the Coconut corpus of dialogues and showed that some dialogue states and communicative goals make overspecific REs more likely. Among the few corpora of texts within which REs have been annotated in some way (as opposed to corpora of annotated REs such as those created by van Deemter et al. (2006)) are the GNOME, Coconut and Maptask corpora. In the GNOME Corpus (Poesio, 2000; Poesio, 2004) different types of discourse and semantic information ar"
W07-2302,P00-1024,0,0.0196467,"06)) are the GNOME, Coconut and Maptask corpora. In the GNOME Corpus (Poesio, 2000; Poesio, 2004) different types of discourse and semantic information are annotated, including reference and semantic attributes. The corpus annotation was e.g. used to train a decision tree learner 10 for NP modifier generation (Cheng et al., 2001). The RE annotations in the Coconut corpus represent information at the discourse level (reference and attributes used) and at the utterance level (information about dialogue state). The 400 REs and annotations in the corpus were used to train an RE generation module (Jordan and Walker, 2000). Gupta and Stent (2005) annotated both the Maptask and Coconut corpora for POS-tags, NPs, referent of NPs, and knowledge representations for each speaker which included values for different attributes for potential referents. While context has been taken into account to some extent in existing research on generation of REs, our goal is to model a range of contextual factors and the interactions between them. Our corpus creation work provides — for the first time, as far as we are aware — a resource that includes multiple human-selected REs for the same referent in the same place in a discours"
W07-2302,W99-0108,0,0.303112,"’s algorithm (2004) is able to identify attributes that are particularly discriminating given the entities in the contrast set, and van Deemter’s SET algorithm can generate REs to sets of entities (van Deemter, 2002). Krahmer and Theune (2002) moved away from unique identification, also taking discourse context into account: they replaced the requirement that the intended referent be the only entity that matches the RE, to the requirement that it be the most salient in a given context. Several versions of centering theory have been used as a basis for pronominalisation algorithms (Dale, 1992; McCoy and Strube, 1999; Henschel et al., 2000). Jordan (2002) highlighted a factor other than salience that influences choice of RE: she found a large proportion of overspecified redescriptions in the Coconut corpus of dialogues and showed that some dialogue states and communicative goals make overspecific REs more likely. Among the few corpora of texts within which REs have been annotated in some way (as opposed to corpora of annotated REs such as those created by van Deemter et al. (2006)) are the GNOME, Coconut and Maptask corpora. In the GNOME Corpus (Poesio, 2000; Poesio, 2004) different types of discourse and"
W07-2302,poesio-2000-annotating,0,0.0268156,"ominalisation algorithms (Dale, 1992; McCoy and Strube, 1999; Henschel et al., 2000). Jordan (2002) highlighted a factor other than salience that influences choice of RE: she found a large proportion of overspecified redescriptions in the Coconut corpus of dialogues and showed that some dialogue states and communicative goals make overspecific REs more likely. Among the few corpora of texts within which REs have been annotated in some way (as opposed to corpora of annotated REs such as those created by van Deemter et al. (2006)) are the GNOME, Coconut and Maptask corpora. In the GNOME Corpus (Poesio, 2000; Poesio, 2004) different types of discourse and semantic information are annotated, including reference and semantic attributes. The corpus annotation was e.g. used to train a decision tree learner 10 for NP modifier generation (Cheng et al., 2001). The RE annotations in the Coconut corpus represent information at the discourse level (reference and attributes used) and at the utterance level (information about dialogue state). The 400 REs and annotations in the corpus were used to train an RE generation module (Jordan and Walker, 2000). Gupta and Stent (2005) annotated both the Maptask and Co"
W07-2302,W04-0210,0,0.0131277,"algorithms (Dale, 1992; McCoy and Strube, 1999; Henschel et al., 2000). Jordan (2002) highlighted a factor other than salience that influences choice of RE: she found a large proportion of overspecified redescriptions in the Coconut corpus of dialogues and showed that some dialogue states and communicative goals make overspecific REs more likely. Among the few corpora of texts within which REs have been annotated in some way (as opposed to corpora of annotated REs such as those created by van Deemter et al. (2006)) are the GNOME, Coconut and Maptask corpora. In the GNOME Corpus (Poesio, 2000; Poesio, 2004) different types of discourse and semantic information are annotated, including reference and semantic attributes. The corpus annotation was e.g. used to train a decision tree learner 10 for NP modifier generation (Cheng et al., 2001). The RE annotations in the Coconut corpus represent information at the discourse level (reference and attributes used) and at the utterance level (information about dialogue state). The 400 REs and annotations in the corpus were used to train an RE generation module (Jordan and Walker, 2000). Gupta and Stent (2005) annotated both the Maptask and Coconut corpora f"
W07-2302,C92-1038,0,0.0624487,"dic entries. In this paper, we describe a corpus of such texts we have compiled and annotated (Section 3), and report first insights from our analysis of the corpus data (Section 4). We further report the results of an experiment where subjects selected REs in context (Section 5), and establish baseline results for automatic methods of selection (Section 6). 2 Related Research The most classical form of GRE algorithm takes into account two main factors in selecting expressions: unique identification (of the intended referent from a set including possible distractors), and brevity (Dale, 1989; Reiter and Dale, 1992). Most GRE research focuses on definite, non-first mentions of the target referent. The most influential of these algorithms, the ‘incremental algorithm’ (IA) (Dale and Reiter, 1995), originally just selected attributive properties, but a range of extensions have been reported. Siddharthan and Copestake’s algorithm (2004) is able to identify attributes that are particularly discriminating given the entities in the contrast set, and van Deemter’s SET algorithm can generate REs to sets of entities (van Deemter, 2002). Krahmer and Theune (2002) moved away from unique identification, also taking d"
W07-2302,P04-1052,0,0.570731,"Missing"
W07-2302,W06-1420,0,0.0225119,"Missing"
W07-2302,J02-1003,0,0.117104,"Missing"
W07-2302,U06-1017,0,0.0140579,"in the discourse? While existing GRE research has taken discourse context into account to some extent (see Section 2), the question why people choose different REs in different contexts has not really been addressed: Not only do different people use different referring expressions for the same object, but the same person may use different expressions for the same object on different occasions. Although this may seem like a rather unsurprising observation, it has never, as far as we are aware, been taken into account in the development of any algorithm for generation of referring expressions. (Viethen and Dale, 2006, p. 119) Selection of a particular RE in a particular context is likely to be affected by a range of factors in addition to discourse-familiarity and unique identification. In our research we ultimately aim to (i) investigate the factors that influence choice of RE in context, (ii) determine what information is needed for a GRE module to be able to generate appropriate REs in context, and (iii) develop reliable methods for automatically generating REs in context. Our basic approach is to annotate occurrences of MSR in naturally occurring texts, analyse the texts in various ways, and obtain mu"
W07-2304,N07-1021,1,0.83879,"strictly speaking, according to our model the training parameters are part of the traversal algorithm, not the tree. is considerably more expensive than the greedy modes. 2. Greedy generation: make the single most likely decision at each choice point (rule expansion) in a generation process. This is not guaranteed to result in the most likely generation process, but the computational cost is very low. 3 Examples of this control model 3. Greedy roulette-wheel generation: use a nonuniform random distribution proportional to the likelihoods of alternatives. 3.1 Example 1 – pCRU pCRU (Belz, 2006; Belz, 2007) is a probabilistic language generation framework for creating NLG systems that contain a probabilistic model of the entire generation space, represented by a context-free underspecification grammar. The basic idea is to view all generation rules as context-free rules and to estimate a single probabilistic model from a corpus of texts to guide the generation process. In nonprobabilistic mode, the generator operates by taking any sentential form of the grammar as an input and expanding it using the grammar to all possible fully specified forms, which are the outputs. Thus a pCRU grammar looks r"
W07-2304,W02-2119,1,0.852946,"output gives rise to one source of variation of control. We are not interested in what outputs are, but only how they are constructed, and so we think of them purely in terms of the content operations that give rise to them. Adopting this very abstract view, we can conceptualise a generator as having the following principal components: 2 The control model We start with a very general view of the generation process2 . Generation takes an input and produces an output, which is a ‘more linguistically instantiated’ representation of the input (but we will not say precisely what that means — cf. (Evans et al., 2002; McDonald, 1993). In the process of doing this, the generator makes various decisions about the content of its output — it reaches a choice-point at which several options are possible and selects one to follow, and then reaches another choice point, and so on. In fact, this is all any generation algorithm does: visit choice points one after another and make a decision relating to the content of output at each one. Each decision may be constrained by the input, constrained by other decisions already made, or determined by the generation algorithm itself. These constraints may not reduce the nu"
W07-2304,W02-2103,0,0.0357578,"tional grammar for syntax, except that it is used to model deep generation as well as surface realisation. The probabilistic version of pCRU introduces a probability distribution over the generator decisions. This is achieved by using treebank training, that is, estimating a distribution over the expansion rules that encode the generation space from a corpus using two steps3 : Belz (2006) describes an application of the system to weather forecast generation, and compares the different control techniques with human generation and a more traditional generate-and-test probabilistic architecture (Langkilde-Geary, 2002). pCRU can be interpreted using the model introduce here in the following way (cf. the first example given in section 2.1). The generation tree is defined by all the possible derivations according to the grammar. Each node corresponds to a sentential form and each child node is the result of rewriting a single non-terminal using a grammar rule. Thus the content operations are grammar rules. The content structures are sentences in the grammar. The input is itself a sentential form which identifies where in the complete tree to start generating from, and the input constraint algorithm does nothi"
W07-2304,J93-1009,0,0.0372096,"o one source of variation of control. We are not interested in what outputs are, but only how they are constructed, and so we think of them purely in terms of the content operations that give rise to them. Adopting this very abstract view, we can conceptualise a generator as having the following principal components: 2 The control model We start with a very general view of the generation process2 . Generation takes an input and produces an output, which is a ‘more linguistically instantiated’ representation of the input (but we will not say precisely what that means — cf. (Evans et al., 2002; McDonald, 1993). In the process of doing this, the generator makes various decisions about the content of its output — it reaches a choice-point at which several options are possible and selects one to follow, and then reaches another choice point, and so on. In fact, this is all any generation algorithm does: visit choice points one after another and make a decision relating to the content of output at each one. Each decision may be constrained by the input, constrained by other decisions already made, or determined by the generation algorithm itself. These constraints may not reduce the number of choices a"
W07-2304,P05-1008,1,0.908156,"ration process, and correspondingly of generation systems. The simple pipeline model of Reiter and Dale (2000) actually conceals a wide range of underlying approaches to generation (cf. (Mellish et al., 2006, section 2.1)), and a recent initiative to provide a ‘reference architecture’ for such systems (involving some of the present authors) abandoned any attempt to harmonise control aspects of the systems it studied (Mellish et al., 2006). In such a situation, it is difficult to see how any general statements, results or techniques relating to controlling generation can be developed, although Paiva and Evans (2005) report an approach that has the potential for wider applicability. In the present paper, we introduce a view of the generation process which abstracts away from specific generation systems or architectures, to a point at which it is possible to separate control from content decisions in the generation process. This allows us to explore systematically different control strategies for constructing the same content (mapping from an input to the same output), and examine different approaches (e.g. generative, empirical) to the problem of controlling generation. The approach we develop is quite ab"
W08-1108,W07-2307,1,0.897826,"Missing"
W08-1108,karasimos-isard-2004-multi,0,0.0292183,"w these relate to algorithmic properties (Section 4.1 and 4.2). Finally we look at how intrinsic and extrinsic evaluations correlate with each other (Section 4.3). ASGRE and Evaluation Though ASGRE evaluations have been carried out (Gupta and Stent, 2005; Viethen and Dale, 2006; Gatt et al., 2007), these have focused on ‘classic’ algorithms, and have been corpus-based. The absence of task-performance evaluations is surprising, considering the well-defined nature of the ASGRE task, and the predominance of task-performance studies elsewhere in the NLG evaluation literature (Reiter et al., 2003; Karasimos and Isard, 2004). Given the widespread agreement on task definition and input/output specifications, ASGRE was an ideal candidate for the first NLG shared task evaluation challenge. The challenge was first discussed 2 The ASGRE Challenge The ASGRE Challenge used the TUNA Corpus (Gatt et al., 2007), a set of human-produced referring expressions (REs) for entities in visual domains of pictures of furniture or people. The corpus was collected during an online elicitation experiment in which subjects typed descriptions of a target referent in a DOMAIN in which there were also 6 other entities (‘distractors’). Eac"
W08-1108,P06-1131,0,0.061127,"z Natural Language Technology Group University of Brighton Brighton BN2 4GJ, UK a.s.belz@brighton.ac.uk Abstract focus has increasingly been on definite descriptions and identification, where the set of attributes selected should uniquely distinguish the intended referent from other entities (its ‘distractors’). Unique Reference in this sense is a dominant criterion for selecting attribute sets in classic ASGRE algorithms. Following Dale (1989), and especially Dale and Reiter (1995), several contributions have extended the remit of ASGRE algorithms to handle relations (Dale and Haddock, 1991; Kelleher and Kruijff, 2006) and gradable attributes (van Deemter, 2006); and also to guarantee logical completeness of algorithms (van Deemter, 2002; Gardent, 2002; Horacek, 2004; Gatt and van Deemter, 2007). Much of this work has incorporated the principle of brevity. Based on the Gricean Quantity maxim (Grice, 1975), and originally discussed by Appelt (1985), and further by Dale (1989), Reiter (1990) and Gardent (2002), this principle holds that descriptions should contain no more information than is necessary to distinguish an intended referent. In ASGRE, this has been translated into a criterion which determines the"
W08-1108,passonneau-2006-measuring,0,0.0571199,"s to the ASGRE Challenge. Properties are indicated in the first column (abbreviations are explained in the table caption). The version of the IS - FBS system that was originally submitted to ASGRE contained a bug and did not actually output minimal attribute sets (but added an arbitrary attribute to each set). Unlike the ASGRE Challenge task-performance evaluation, the analysis presented in this paper uses the corrected version of this system. 3 Intrinsic measures 2 × |DS ∩ DH | |DS |+ |DH | (1) For this paper, we also computed MASI, a version of the Jaccard similarity coefficient proposed by Passonneau (2006) which multiplies the similarity value by a monotonicity coefficient, biasing the measure towards those cases where DS and DH have an empty set difference. Intuitively, this means that those system-produced descriptions are preferred which do not include attributes that are omitted by a human. Thus, two of our intrinsic measures assess Humanlikeness (Dice and MASI), while Minimality reflects the extent to which an algorithm conforms to brevity, one of the principles that has emerged from the ASGRE literature. Evaluation methods 3.2 Evaluation methods can be characterised as either intrinsic or"
W08-1108,P90-1013,0,0.082054,"sets in classic ASGRE algorithms. Following Dale (1989), and especially Dale and Reiter (1995), several contributions have extended the remit of ASGRE algorithms to handle relations (Dale and Haddock, 1991; Kelleher and Kruijff, 2006) and gradable attributes (van Deemter, 2006); and also to guarantee logical completeness of algorithms (van Deemter, 2002; Gardent, 2002; Horacek, 2004; Gatt and van Deemter, 2007). Much of this work has incorporated the principle of brevity. Based on the Gricean Quantity maxim (Grice, 1975), and originally discussed by Appelt (1985), and further by Dale (1989), Reiter (1990) and Gardent (2002), this principle holds that descriptions should contain no more information than is necessary to distinguish an intended referent. In ASGRE, this has been translated into a criterion which determines the adequacy of an attribute set, implemented in its most straightforward form in Full Brevity algorithms which select the smallest attribute set that uniquely refers to the intended referent (Dale, 1989). Another frequent property of ASGRE algorithms is Incrementality which involves selection of attributes one at a time (rather than exhaustive search for a distinguishing set),"
W08-1108,J02-1003,0,0.047089,"Missing"
W08-1108,J06-2002,0,0.0703273,"Missing"
W08-1108,W06-1410,0,0.0312618,"orithmic properties of the participating systems. We thus focus on two issues in ASGRE and its evaluation. We examine the similarities and differences among the systems submitted to the ASGRE Challenge and compare them to classic approaches (Section 2.1). We look at the results of intrinsic and extrinsic evaluations (Section 4) and examine how these relate to algorithmic properties (Section 4.1 and 4.2). Finally we look at how intrinsic and extrinsic evaluations correlate with each other (Section 4.3). ASGRE and Evaluation Though ASGRE evaluations have been carried out (Gupta and Stent, 2005; Viethen and Dale, 2006; Gatt et al., 2007), these have focused on ‘classic’ algorithms, and have been corpus-based. The absence of task-performance evaluations is surprising, considering the well-defined nature of the ASGRE task, and the predominance of task-performance studies elsewhere in the NLG evaluation literature (Reiter et al., 2003; Karasimos and Isard, 2004). Given the widespread agreement on task definition and input/output specifications, ASGRE was an ideal candidate for the first NLG shared task evaluation challenge. The challenge was first discussed 2 The ASGRE Challenge The ASGRE Challenge used the T"
W08-1108,2007.mtsummit-ucnlg.14,1,0.835716,", that is, it bases selection on the extent to which an attribute helps distinguish an entity from its distractors. In the remainder of this paper, we uniformly use the term ’algorithmic property’ for the selection criteria and other properties of ASGRE algorithms described above, and refer to them by the following short forms: Full Brevity, Uniqueness, Discriminatory Power, Hardwired Type Selection, Human Preference Modelling and Incrementality.1 1.2 during a workshop held at Arlington, Va. (Dale and White, 2007), and eventually organised as part of the UCNLG + MT Workshop in September 2007 (Belz and Gatt, 2007). The ASGRE Shared Task provided an opportunity to (a) assess the extent to which the field has diversified since its inception; (b) carry out a comparative evaluation involving both automatic methods and human task-performance methods. 1.3 Overview In the ASGRE Challenge report (Belz and Gatt, 2007) we presented the results of the ASGRE Challenge evaluations objectively and with little interpretation. In this paper, we present the results of a new task-performance evaluation and a new intrinsic measure involving the same 15 systems and compare the new results with the earlier ones. We also ex"
W08-1108,P08-2050,1,0.858454,"possibly arising from the use of attributes (such as SIZE) which impose a greater cognitive load on the reader. The very strong correlation (0.97) between Dice and MASI is to be expected, given the similarity in the way they are defined. Another unambiguous result emerges: none of the similarity-based metrics covary significantly with any of the task-performance measures. An extended analysis involving a larger range of intrinsic metrics confirmed this lack of significant covariation for string-based similarity metrics as well as setsimilarity metrics across two task-performance experiments (Belz and Gatt, 2008). This indicates that at least for some areas of HLT, task-performance evaluation is vital: without the external reality check provided by extrinsic evaluations, intrinsic evaluations may end up being too self-contained and disconnected from notions of usefulness to provide a meaningful assessment of systems’ quality. 5 Acknowledgements We gratefully acknowledge the contribution made to the evaluations by the faculty and staff at Brighton University who participated in the identification experiments. The biggest contribution was, of course, made by the participants in the ASGRE Challenge who c"
W08-1108,E91-1028,0,0.0873793,"gatt@abdn.ac.uk Anja Belz Natural Language Technology Group University of Brighton Brighton BN2 4GJ, UK a.s.belz@brighton.ac.uk Abstract focus has increasingly been on definite descriptions and identification, where the set of attributes selected should uniquely distinguish the intended referent from other entities (its ‘distractors’). Unique Reference in this sense is a dominant criterion for selecting attribute sets in classic ASGRE algorithms. Following Dale (1989), and especially Dale and Reiter (1995), several contributions have extended the remit of ASGRE algorithms to handle relations (Dale and Haddock, 1991; Kelleher and Kruijff, 2006) and gradable attributes (van Deemter, 2006); and also to guarantee logical completeness of algorithms (van Deemter, 2002; Gardent, 2002; Horacek, 2004; Gatt and van Deemter, 2007). Much of this work has incorporated the principle of brevity. Based on the Gricean Quantity maxim (Grice, 1975), and originally discussed by Appelt (1985), and further by Dale (1989), Reiter (1990) and Gardent (2002), this principle holds that descriptions should contain no more information than is necessary to distinguish an intended referent. In ASGRE, this has been translated into a c"
W08-1108,P89-1009,0,0.51902,"neration: New Algorithms and Evaluation Methods Albert Gatt Department of Computing Science University of Aberdeen Aberdeen AB24 3UE, UK a.gatt@abdn.ac.uk Anja Belz Natural Language Technology Group University of Brighton Brighton BN2 4GJ, UK a.s.belz@brighton.ac.uk Abstract focus has increasingly been on definite descriptions and identification, where the set of attributes selected should uniquely distinguish the intended referent from other entities (its ‘distractors’). Unique Reference in this sense is a dominant criterion for selecting attribute sets in classic ASGRE algorithms. Following Dale (1989), and especially Dale and Reiter (1995), several contributions have extended the remit of ASGRE algorithms to handle relations (Dale and Haddock, 1991; Kelleher and Kruijff, 2006) and gradable attributes (van Deemter, 2006); and also to guarantee logical completeness of algorithms (van Deemter, 2002; Gardent, 2002; Horacek, 2004; Gatt and van Deemter, 2007). Much of this work has incorporated the principle of brevity. Based on the Gricean Quantity maxim (Grice, 1975), and originally discussed by Appelt (1985), and further by Dale (1989), Reiter (1990) and Gardent (2002), this principle holds t"
W08-1108,P02-1013,0,0.136631,"te descriptions and identification, where the set of attributes selected should uniquely distinguish the intended referent from other entities (its ‘distractors’). Unique Reference in this sense is a dominant criterion for selecting attribute sets in classic ASGRE algorithms. Following Dale (1989), and especially Dale and Reiter (1995), several contributions have extended the remit of ASGRE algorithms to handle relations (Dale and Haddock, 1991; Kelleher and Kruijff, 2006) and gradable attributes (van Deemter, 2006); and also to guarantee logical completeness of algorithms (van Deemter, 2002; Gardent, 2002; Horacek, 2004; Gatt and van Deemter, 2007). Much of this work has incorporated the principle of brevity. Based on the Gricean Quantity maxim (Grice, 1975), and originally discussed by Appelt (1985), and further by Dale (1989), Reiter (1990) and Gardent (2002), this principle holds that descriptions should contain no more information than is necessary to distinguish an intended referent. In ASGRE, this has been translated into a criterion which determines the adequacy of an attribute set, implemented in its most straightforward form in Full Brevity algorithms which select the smallest attribu"
W08-1108,D07-1011,1,0.796767,"Missing"
W08-1127,W00-1401,0,0.0392065,"s) is that an RE is not good or bad in its own right, but depends on the other MSRs in the same text.4 String Accuracy: This is defined just like REG 08-Type Accuracy, except here what is determined is identity between REFEX word strings (the MSREs themselves), not between REG08-Types. String-edit distance metrics: String-edit distance (SE) is straightforward Levenshtein distance with a substitution cost of 2 and insertion/deletion 4 This definition is also slightly different from the one given in the Participants’ Pack. cost of 1. We also used the version of string-edit distance described by Bangalore et al. (2000) which normalises for length. This version is denoted ‘SEB’ below. For the single-RE test sets, the global score is simply the average of all RE-level scores. For Test Set C-2, we used an approach analogous to that described above for REG08-Type Accuracy. We first computed the best string-edit distance at the text level (here, just the sum of RE-level distances) and then obtained the global distance by dividing the sum of best text-level distances by the number of REFs in all the texts. Other metrics: BLEU is a precision metric from MT that assesses the quality of a peer translation in terms o"
W08-1127,W07-2302,1,0.820701,"The immediate motivating application context for the GREC Task is the improvement of referential clarity and coherence in extractive summarisation by regenerating referring expressions in summaries. There has recently been a small flurry of work in this area (Steinberger et al., 2007; Nenkova, 2008). In the longer term, the GREC Task is intended to be a step in the direction of the more general task of generating referential expressions in discourse context. The GREC Task Corpus is an extension of GREC 1.0 which had about 1,000 texts in the subdomains of cities, countries, rivers and people (Belz and Varges, 2007a). for the purpose of the REG’08 GREC Task, we obtained an additional 1,000 texts in the new subdomain of mountain texts and developed a new XML annotation scheme (Section 2.2). Five teams from four countries registered for the GREC Task, of which three teams eventually submitted 6 systems. We also used the corpus texts themselves as ‘system’ outputs, and created four baseline systems. We evaluated the resulting 10 systems using a range of intrinsic and extrinsic evaluation methods. This report presents the results of all evaluations (Section 6), along with descriptions of the GREC data and t"
W08-1127,H05-1004,0,0.0209919,"ctation is that the tool performs worse (are less able to identify coreference chains correctly) with worse MSR reference chains. To counteract the potential problem of results being a function of a specific coreference resolution algorithm or tool, we decided to use three different resolvers—those included in LingPipe,5 JavaRap (Qiu et al., 2004) and OpenNLP (Morton, 2005)— and to average results. There does not appear to be a single standard eval5 http://alias-i.com/lingpipe/ uation metric in the coreference resolution community, so we opted to use three: MUC -6 (Vilain et al., 1995), CEAF (Luo, 2005), and B - CUBED (Bagga and Baldwin, 1998), which seem to be the most widely accepted metrics. All three metrics compute Recall, Precision and F-Scores on aligned gold-standard and resolver-tool coreference chains. They differ in how the alignment is obtained and what components of coreference chains are counted for calculating scores. Results for the automatic extrinsic evaluations are reported below in terms of the F-Scores from these three metrics, as well as in terms of their average. 5 Systems Base-rand, Base-freq, Base-1st, Base-name: We created four baseline systems. Base-rand selects on"
W08-1127,I08-1016,0,0.0250676,"of possible referring expressions for selection. As this is a new referring expression generation (REG) task, the shared task definition was kept fairly simple and the aim for participating systems was to select the appropriate type of referring expression (more specifically, its REG08-TYPE, full details below). The immediate motivating application context for the GREC Task is the improvement of referential clarity and coherence in extractive summarisation by regenerating referring expressions in summaries. There has recently been a small flurry of work in this area (Steinberger et al., 2007; Nenkova, 2008). In the longer term, the GREC Task is intended to be a step in the direction of the more general task of generating referential expressions in discourse context. The GREC Task Corpus is an extension of GREC 1.0 which had about 1,000 texts in the subdomains of cities, countries, rivers and people (Belz and Varges, 2007a). for the purpose of the REG’08 GREC Task, we obtained an additional 1,000 texts in the new subdomain of mountain texts and developed a new XML annotation scheme (Section 2.2). Five teams from four countries registered for the GREC Task, of which three teams eventually submitte"
W08-1127,qiu-etal-2004-public,0,0.0300994,"similar to that in the humanbased experiments described above: badly chosen reference chains seem likely to affect the reader’s ability to resolve REs. In the automatic version, the role of the reader is played by an automatic coreference resolution tool and the expectation is that the tool performs worse (are less able to identify coreference chains correctly) with worse MSR reference chains. To counteract the potential problem of results being a function of a specific coreference resolution algorithm or tool, we decided to use three different resolvers—those included in LingPipe,5 JavaRap (Qiu et al., 2004) and OpenNLP (Morton, 2005)— and to average results. There does not appear to be a single standard eval5 http://alias-i.com/lingpipe/ uation metric in the coreference resolution community, so we opted to use three: MUC -6 (Vilain et al., 1995), CEAF (Luo, 2005), and B - CUBED (Bagga and Baldwin, 1998), which seem to be the most widely accepted metrics. All three metrics compute Recall, Precision and F-Scores on aligned gold-standard and resolver-tool coreference chains. They differ in how the alignment is obtained and what components of coreference chains are counted for calculating scores. Re"
W08-1127,M95-1005,0,0.0328313,"resolution tool and the expectation is that the tool performs worse (are less able to identify coreference chains correctly) with worse MSR reference chains. To counteract the potential problem of results being a function of a specific coreference resolution algorithm or tool, we decided to use three different resolvers—those included in LingPipe,5 JavaRap (Qiu et al., 2004) and OpenNLP (Morton, 2005)— and to average results. There does not appear to be a single standard eval5 http://alias-i.com/lingpipe/ uation metric in the coreference resolution community, so we opted to use three: MUC -6 (Vilain et al., 1995), CEAF (Luo, 2005), and B - CUBED (Bagga and Baldwin, 1998), which seem to be the most widely accepted metrics. All three metrics compute Recall, Precision and F-Scores on aligned gold-standard and resolver-tool coreference chains. They differ in how the alignment is obtained and what components of coreference chains are counted for calculating scores. Results for the automatic extrinsic evaluations are reported below in terms of the F-Scores from these three metrics, as well as in terms of their average. 5 Systems Base-rand, Base-freq, Base-1st, Base-name: We created four baseline systems. Ba"
W08-1131,W08-1108,1,0.841081,"point at which a participant called up the next screen via mouse click; (b) identification time (IT), measured from the point at which pictures (the visual domain) were presented on the screen to the point where a participant identified a referent by clicking on it; (c) error rate (ER), the proportion of times the wrong referent was identified. This design differs from that used in the 2007 ASGRE Challenge, in which descriptions and visual domains were presented in a single phase (on the same screen), so that RT and IT were conflated. The new experiment replicates the methodology reported in Gatt and Belz (2008), in a follow-up study on the ASGRE 2007 data. Another difference between the two experiments is that the current one is based on peer outputs which are themselves realisations, whereas the ASGRE experiment involved attribute sets which had to be realised before they could be used. Design: We used a Repeated Latin Squares design, in which each combination of SYSTEM3 and test set item is allocated one trial. Since there were 12 levels of SYSTEM, but 112 test set items, 8 randomly selected items (4 furniture and 4 people) were duplicated, yielding 120 items and 10 12 × 12 latin squares. The item"
W08-1131,W07-2307,1,0.766492,"Missing"
W08-1131,P02-1040,0,0.0984351,"MAIN. String-edit distance (TUNA - R, TUNA - REG): This is the classic Levenshtein distance measure, used to compare the difference between a peer output and a reference output in the corpus, as the minimal number of insertions, deletions and/or substitutions of words required to transform one string into another. The cost for insertions and deletions was set to 1, that for substitutions to 2. Edit distance is an integer bounded by the length of the longest description in the pair being compared. (TUNA - R, TUNA - REG): This is an n-gram based string comparison measure, originally proposed by Papineni et al. (2002) for evaluation of Machine Translation systems. It evaluates a system based on the proportion of word n-grams (considering all n-grams of length n ≤ 4 is standard) that it shares with several reference translations. Unlike Dice, MASI and String-edit, BLEU is by definition an aggregate measure (i.e. a single BLEU score is obtained for a system based on the entire set of items to be compared, and this is generally not equal to the average of BLEU scores for individual items). BLEU ranges between 0 and 1. BLEU NIST ( TUNA - R , TUNA - REG ): This is a version of BLEU , which gives more importance"
W08-1131,passonneau-2006-measuring,0,0.141439,"Missing"
W08-1131,2007.mtsummit-ucnlg.14,1,0.759345,"ction for referring expressions (TUNA - AS), realisation (TUNA - R) and end-toend referring expression generation (TUNA REG ). 8 teams submitted a total of 33 systems to the three tasks, with an additional submission to the Open Track. The evaluation used a range of automatically computed measures. In addition, an evaluation experiment was carried out using the peer outputs for the TUNA REG task. This report describes each task and the evaluation methods used, and presents the evaluation results. 1 Introduction The TUNA Challenge 2008 built on the foundations laid in the ASGRE 2007 Challenge (Belz and Gatt, 2007), which consisted of a single shared task, based on a subset of the TUNA Corpus (Gatt et al., 2007). The TUNA Corpus is a collection of human-authored descriptions of a referent, paired with a representation of the domain in which that description was elicited. The 2008 Challenge expanded the scope of the previous edition in a variety of ways. This year, there were three shared tasks. TUNA - AS is the Attribute Selection task piloted in the 2007 ASGRE Challenge, which involves the selection of a set of attributes which are true of a target referent, and help to distinguish it from its distract"
W09-0603,E06-1040,1,0.956437,"ferent grammars (resulting in two different generators). The ‘unstructured’ grammar encodes raw corpus input vectors augmented as described in Section 4.2, whereas the ‘semantic’ grammar encodes representations with recursive predicateargument structure that more resemble semantic forms. These were produced automatically from the raw input vectors. 5 Evaluation Methods 5.1 Automatic evaluation methods The two automatic metrics used in the evaluations, NIST2 and BLEU 3 , have been shown to correlate well with expert judgments (Pearson’s r = 0.82 and 0.79 respectively) in the S UM T IME domain (Belz and Reiter, 2006). Both the PSCFG-unstructured and the PSCFGsemantic generators were built in the same way, by feeding the CFG for wind data representations and the corpus of paired wind data representations and forecasts to WASP−1 which then created probabilistic SCFGs from it. 2 http://cio.nist.gov/esd/emaildir/ lists/mt_list/bin00000.bin 3 ftp://jaguar.ncsl.nist.gov/mt/ resources/mteval-v11b.pl 20 H System corpus PCFG -greedy PSCFG -sem PSCFG -unstr PCFG -roule PBSMT -unstr PCFG -viterbi PCFG -2gram S UM T IME PCFG -rand PBSMT -struc BLEU -x is an n-gram based string comparison measure, originally proposed"
W09-0603,J93-2003,0,0.00952061,"ion rules such that for 18 provides a way in which a probabilistic SCFG can be constructed for the most part automatically. The training process requires two resources as input: a CFG of MRs and a set of sentences paired with their MRs. As output, it produces a probabilistic SCFG. The training process works in two phases, producing a (non-probabilistic) SCFG in the ‘lexical acquisition phase’, and associating the rules with probabilities in the ‘parameter estimation phase’. The lexical acquisition phase uses the GIZA++ word-alignment tool, an implementation (Och and Ney, 2003) of IBM Model 5 (Brown et al., 1993) to construct an alignment of MRs with NL strings. An SCFG is then constructed by using the MR CFG as a skeleton and inferring the NL grammar from the alignment. For the parameter estimation phase, WASP−1 uses a log-linear model from Koehn et al. (2003) which defines a conditional probability distribution over derivations d given an input MR f as Pr(d|f ) ∝ Pr(e(d))λ1 λ Y the most likely linearisation of the translated substrings. The currently most popular phrase-based SMT (PBSMT) approach translates phrases (an arbitrary sequence of words, rather than the linguistic sense), whereas the origi"
W09-0603,P98-1116,0,0.0929385,"ments in this paper focussed on the part of the forecasts that predicts wind characteristics for the next 15 hours. Figure 1 shows an example data file and Figure 2 shows the corresponding wind forecast written by one of the meteorologists. In Figure 1, the Traditional Natural Language Generation (NLG) systems tend to be handcrafted knowledge-based systems. Such systems tend to be brittle, expensive to create and hard to adapt to new domains or applications. Over the last decade or so, in particular following Knight and Langkilde’s work on n-gram-based generate-and-select surface realisation (Knight and Langkilde, 1998; Langkilde, 2000), NLG researchers have become increasingly interested in systems that are automatically trainable from data. Systems that have a trainable component tend to be easier to adapt to new domains Proceedings of the 12th European Workshop on Natural Language Generation, pages 16–24, c Athens, Greece, 30 – 31 March 2009. 2009 Association for Computational Linguistics 16 Oil1/Oil2/Oil3_FIELDS 05-10-00 05/06 05/09 05/12 05/15 05/18 05/21 06/00 ... SSW S S S SSE SSE VAR 18 16 14 14 12 10 6 22 20 17 17 15 12 7 27 25 21 21 18 15 8 3.0 2.7 2.5 2.3 2.4 2.4 2.4 4.8 4.3 4.0 3.7 3.8 3.8 3.8 S"
W09-0603,W05-0908,0,0.0183704,"osed by Papineni et al. (2001) for evaluation of MT systems. It computes the proportion of word n-grams of length x and less that a system output shares with several reference outputs. Setting x = 4 (i.e. considering all ngrams of length ≤ 4) is standard. NIST (Doddington, 2002) is a version of BLEU, but where BLEU gives equal weight to all n-grams, NIST gives more importance to less frequent (hence more informative) n-grams, and the range of NIST scores depends on the size of the test set. Some research has shown NIST to correlate with human judgments more highly than BLEU (Doddington, 2002; Riezler and Maxwell, 2005; Belz and Reiter, 2006). 5.2 Homogeneous subsets NIST 4.062 3.361 3.303 3.191 3.033 2.924 2.854 2.854 2.707 2.540 2.331 A B B B C C D D D D E E E F F G Table 3: Mean forecast-level NIST scores and homogeneous subsets (Tukey HSD, alpha = .05) for S UM T IME test sets. Human evaluation In each case we report the main effect of System on the measure and (if it is significant) we also report significant differences between pairs of systems in the form of homogeneous subsets obtained with a post-hoc Tukey HSD analysis. Tables 2 and 3 display the results for the BLEU and NIST evaluations, where sco"
W09-0603,N03-1017,0,0.044475,"oduces a probabilistic SCFG. The training process works in two phases, producing a (non-probabilistic) SCFG in the ‘lexical acquisition phase’, and associating the rules with probabilities in the ‘parameter estimation phase’. The lexical acquisition phase uses the GIZA++ word-alignment tool, an implementation (Och and Ney, 2003) of IBM Model 5 (Brown et al., 1993) to construct an alignment of MRs with NL strings. An SCFG is then constructed by using the MR CFG as a skeleton and inferring the NL grammar from the alignment. For the parameter estimation phase, WASP−1 uses a log-linear model from Koehn et al. (2003) which defines a conditional probability distribution over derivations d given an input MR f as Pr(d|f ) ∝ Pr(e(d))λ1 λ Y the most likely linearisation of the translated substrings. The currently most popular phrase-based SMT (PBSMT) approach translates phrases (an arbitrary sequence of words, rather than the linguistic sense), whereas the original ‘IBM models’ translated words. Different PBSMT methods differ in how they construct the phrase translation table. We used the phrase-based translation model proposed by Koehn et al. (2003) and implemented in the MOSES toolkit (Koehn et al., 2007) wh"
W09-0603,P07-2045,0,0.0114816,"om Koehn et al. (2003) which defines a conditional probability distribution over derivations d given an input MR f as Pr(d|f ) ∝ Pr(e(d))λ1 λ Y the most likely linearisation of the translated substrings. The currently most popular phrase-based SMT (PBSMT) approach translates phrases (an arbitrary sequence of words, rather than the linguistic sense), whereas the original ‘IBM models’ translated words. Different PBSMT methods differ in how they construct the phrase translation table. We used the phrase-based translation model proposed by Koehn et al. (2003) and implemented in the MOSES toolkit (Koehn et al., 2007) which is based on the noisy channel model, where Bayes’s rule is used to reformulate the task of translating a source language string f into a target language string e as finding the sentence e∗ such that e∗ = argmaxe Pr(e) Pr(f |e). The translation model (which gives Pr(f |e)) is obtained from a parallel corpus of source and target language texts, where the first step is automatic alignment using the GIZA ++ word-level aligner. Word-level alignments are used to obtain phrase translation pairs using a set of heuristics. A 3-gram language model (which gives Pr(e)) for the target language is tr"
W09-0603,N06-1056,0,0.020223,"g the same derivation tree, but using the target language CFG to produce the output string. When using SCFGs for content-to-text generation one of the paired CFGs encodes the meaning representation language, and the other the (natural) language in which text is supposed to be generated. A generation process then consists in (i) ‘parsing’ the meaning representation (MR) into its constituent structure, and, in the opposite direction, (ii) assembling strings of words corresponding to constituent parts of the input MR into a sentence or text that realises the entire MR. We used the WASP−1 method (Wong and Mooney, 2006; Wong and Mooney, 2007) which cesses from inputs to outputs, and has no decisionmaking ability. A probability distribution over this base CFG is estimated from a corpus, and this is what enables decisions between alternative generation rules to be made. The pCRU package permits this distribution to be used in one of the following three modes to drive generation processes: (i) greedy – apply only the most likely rule at each choice point; (ii) Viterbi – apply all expansion rules to each nonterminal to create the generation forest for the input, then do a Viterbi search of the generation forest"
W09-0603,A00-2023,0,0.0222995,"on the part of the forecasts that predicts wind characteristics for the next 15 hours. Figure 1 shows an example data file and Figure 2 shows the corresponding wind forecast written by one of the meteorologists. In Figure 1, the Traditional Natural Language Generation (NLG) systems tend to be handcrafted knowledge-based systems. Such systems tend to be brittle, expensive to create and hard to adapt to new domains or applications. Over the last decade or so, in particular following Knight and Langkilde’s work on n-gram-based generate-and-select surface realisation (Knight and Langkilde, 1998; Langkilde, 2000), NLG researchers have become increasingly interested in systems that are automatically trainable from data. Systems that have a trainable component tend to be easier to adapt to new domains Proceedings of the 12th European Workshop on Natural Language Generation, pages 16–24, c Athens, Greece, 30 – 31 March 2009. 2009 Association for Computational Linguistics 16 Oil1/Oil2/Oil3_FIELDS 05-10-00 05/06 05/09 05/12 05/15 05/18 05/21 06/00 ... SSW S S S SSE SSE VAR 18 16 14 14 12 10 6 22 20 17 17 15 12 7 27 25 21 21 18 15 8 3.0 2.7 2.5 2.3 2.4 2.4 2.4 4.8 4.3 4.0 3.7 3.8 3.8 3.8 SSW SSW SSW SSW SSW"
W09-0603,N07-1022,0,0.462716,"ree, but using the target language CFG to produce the output string. When using SCFGs for content-to-text generation one of the paired CFGs encodes the meaning representation language, and the other the (natural) language in which text is supposed to be generated. A generation process then consists in (i) ‘parsing’ the meaning representation (MR) into its constituent structure, and, in the opposite direction, (ii) assembling strings of words corresponding to constituent parts of the input MR into a sentence or text that realises the entire MR. We used the WASP−1 method (Wong and Mooney, 2006; Wong and Mooney, 2007) which cesses from inputs to outputs, and has no decisionmaking ability. A probability distribution over this base CFG is estimated from a corpus, and this is what enables decisions between alternative generation rules to be made. The pCRU package permits this distribution to be used in one of the following three modes to drive generation processes: (i) greedy – apply only the most likely rule at each choice point; (ii) Viterbi – apply all expansion rules to each nonterminal to create the generation forest for the input, then do a Viterbi search of the generation forest; (iii) greedy roulette-"
W09-0603,J03-1002,0,0.0036595,"of CFGs G1 , G2 with paired production rules such that for 18 provides a way in which a probabilistic SCFG can be constructed for the most part automatically. The training process requires two resources as input: a CFG of MRs and a set of sentences paired with their MRs. As output, it produces a probabilistic SCFG. The training process works in two phases, producing a (non-probabilistic) SCFG in the ‘lexical acquisition phase’, and associating the rules with probabilities in the ‘parameter estimation phase’. The lexical acquisition phase uses the GIZA++ word-alignment tool, an implementation (Och and Ney, 2003) of IBM Model 5 (Brown et al., 1993) to construct an alignment of MRs with NL strings. An SCFG is then constructed by using the MR CFG as a skeleton and inferring the NL grammar from the alignment. For the parameter estimation phase, WASP−1 uses a log-linear model from Koehn et al. (2003) which defines a conditional probability distribution over derivations d given an input MR f as Pr(d|f ) ∝ Pr(e(d))λ1 λ Y the most likely linearisation of the translated substrings. The currently most popular phrase-based SMT (PBSMT) approach translates phrases (an arbitrary sequence of words, rather than the"
W09-0603,2001.mtsummit-papers.68,0,0.0269518,"oth the PSCFG-unstructured and the PSCFGsemantic generators were built in the same way, by feeding the CFG for wind data representations and the corpus of paired wind data representations and forecasts to WASP−1 which then created probabilistic SCFGs from it. 2 http://cio.nist.gov/esd/emaildir/ lists/mt_list/bin00000.bin 3 ftp://jaguar.ncsl.nist.gov/mt/ resources/mteval-v11b.pl 20 H System corpus PCFG -greedy PSCFG -sem PSCFG -unstr PCFG -roule PBSMT -unstr PCFG -viterbi PCFG -2gram S UM T IME PCFG -rand PBSMT -struc BLEU -x is an n-gram based string comparison measure, originally proposed by Papineni et al. (2001) for evaluation of MT systems. It computes the proportion of word n-grams of length x and less that a system output shares with several reference outputs. Setting x = 4 (i.e. considering all ngrams of length ≤ 4) is standard. NIST (Doddington, 2002) is a version of BLEU, but where BLEU gives equal weight to all n-grams, NIST gives more importance to less frequent (hence more informative) n-grams, and the range of NIST scores depends on the size of the test set. Some research has shown NIST to correlate with human judgments more highly than BLEU (Doddington, 2002; Riezler and Maxwell, 2005; Bel"
W09-0603,P02-1040,0,\N,Missing
W09-0603,C98-1112,0,\N,Missing
W09-0629,2007.mtsummit-ucnlg.14,1,0.776163,"−LOC condition, they were discouraged from doing so, though not prevented. The XML format we have been using in the TUNA - REG STEC s, shown in Figure 1, is a variant of the original format of the TUNA corpus. The root TRIAL node has a unique ID and an indication of the +/ − LOC experimental condi1 Introduction This year’s run of the TUNA - REG Shared-Task Evaluation Competition (STEC) is the third, and final, competition to involve the TUNA Corpus of referring expressions. The TUNA Corpus was first used in the Pilot Attribute Selection for Generating Referring Expressions (ASGRE) Challenge (Belz and Gatt, 2007) which took place between May and September 2007; and again for three of the shared tasks in Referring Expression Generation (REG) Challenges 2008, which ran between September 2007 and May 2008 (Gatt et al., 2008). This year’s TUNA Task replicates one of the three tasks from REG’08, the TUNA - REG Task. It uses the same test data, to enable direct comparison against the 2008 results. Four participating teams submitted 6 different systems this year; teams and their affiliations are shown in Table 1. 1 http://www.csd.abdn.ac.uk/research/tuna/ http://genpsylab-wexlist.unizh.ch 3 The elicitation e"
W09-0629,P08-2050,1,0.603169,"0.48 Extrinsic ID Acc. ID Speed 0.50 -0.89* 0.95** -0.65 1 -0.39 -0.39 1 0.68 -0.79 -0.01 0.68 0.49 -0.51 0.60 0.06 Auto-assessed, intrinsic Acc. SE BLEU NIST .85* -0.57 0.66 0.30 .83* -0.29 0.60 0.48 0.68 -0.01 0.49 0.60 -0.79 0.68 -0.51 0.06 1.00 -0.68 .859* 0.49 -0.68 1 -0.75 -0.07 .86* -0.75 1 0.71 0.49 -0.07 0.71 1 Table 10: Correlations (Pearson’s r) between all evaluation measures. (∗ significant at p ≤ .05; at p ≤ .01) ∗∗ significant Fluency and Identification Speed, implying that more fluent descriptions led to faster identification. While these results differ from previous findings (Belz and Gatt, 2008), in which no significant correlations were found between extrinsic measures and automatic intrinsic metrics, it is worth noting that significance in the results reported here was only observed between human-assessed intrinsic measures and the extrinsic ones. tions of the evaluation methods we have developed. Among such experiments will be direct comparisons between the results of the three variants of the identification experiment we have tried out, and a direct comparison between different designs for human-assessed intrinsic evaluations (e.g. comparing the slider design reported here to pre"
W09-0629,W08-1131,1,0.808649,"root TRIAL node has a unique ID and an indication of the +/ − LOC experimental condi1 Introduction This year’s run of the TUNA - REG Shared-Task Evaluation Competition (STEC) is the third, and final, competition to involve the TUNA Corpus of referring expressions. The TUNA Corpus was first used in the Pilot Attribute Selection for Generating Referring Expressions (ASGRE) Challenge (Belz and Gatt, 2007) which took place between May and September 2007; and again for three of the shared tasks in Referring Expression Generation (REG) Challenges 2008, which ran between September 2007 and May 2008 (Gatt et al., 2008). This year’s TUNA Task replicates one of the three tasks from REG’08, the TUNA - REG Task. It uses the same test data, to enable direct comparison against the 2008 results. Four participating teams submitted 6 different systems this year; teams and their affiliations are shown in Table 1. 1 http://www.csd.abdn.ac.uk/research/tuna/ http://genpsylab-wexlist.unizh.ch 3 The elicitation experiment had an additional independent variable, manipulating whether descriptions were elicited in a ‘fault-critical’ or ‘non-fault-critical’ condition. For the shared tasks this was ignored by collapsing all th"
W09-0629,2001.mtsummit-papers.68,0,0.030421,"erally equal to the average of scores for individual items). Because the test data has two human-authored reference descriptions per domain, the Accuracy and SE scores had to be computed slightly differently to obtain test data scores (whereas BLEU and NIST are designed for multiple reference texts). For the test data only, therefore, Accuracy expresses the percentage of a system’s outputs that match at least one of the reference outputs, and SE is the average of the two pairwise scores against the reference outputs. BLEU -x is an n-gram based string comparison measure, originally proposed by Papineni et al. (2001; 2002) for evaluation of Machine Translation systems. It computes the proportion of word n-grams of length x and less that a system output shares with several reference outputs. Setting x = 4 (i.e. considering all n-grams of length ≤ 4) is standard, but because many of the TUNA descriptions are shorter than 4 tokens, we compute BLEU -3 instead. BLEU ranges from 0 to 1. Results: Table 4 is an overview of the selfreported scores on the development set included in the participants’ reports (not all participants report Accuracy scores). The corresponding scores for the test data set as well as NI"
W09-0629,P02-1040,0,0.11671,"Missing"
W09-0629,passonneau-2006-measuring,0,\N,Missing
W09-0629,qiu-etal-2004-public,0,\N,Missing
W09-0629,M95-1005,0,\N,Missing
W09-0629,W07-2307,1,\N,Missing
W09-0629,W07-2302,1,\N,Missing
W09-0629,W08-1108,1,\N,Missing
W09-0629,H05-1004,0,\N,Missing
W09-0629,I08-1016,0,\N,Missing
W09-0629,W00-1401,0,\N,Missing
W09-2816,H05-1004,0,0.0246037,"that it seems likely that badly chosen reference chains affect the ability to resolve REs in automatic coreference resolution tools which will tend to perform worse with poorly selected MSR reference chains. To counteract the possibility of results being a function of a specific coreference resolution algorithm or tool, we used two different resolvers— those included in LingPipe7 and OpenNLP (Morton, 2005)—and averaged results. There does not appear to be a single standard evaluation metric in the coreference resolution community, so we opted to use three: MUC -6 (Vilain et al., 1995), CEAF (Luo, 2005), and B CUBED (Bagga and Baldwin, 1998), which seem to be the most widely accepted metrics. All three metrics compute Recall, Precision and F-Scores on aligned gold-standard and resolver-tool coreference chains. They differ in how the alignment is obtained and what components of coreference chains are counted for calculating scores. Results for the automatic extrinsic evaluations are reported below in terms of the F-Scores from these three metrics, as well as in terms of their mean. 4.3 Human intrinsic evaluation The intrinsic human evaluation involved 24 randomly selected items from Test Set"
W09-2816,I08-1016,0,0.0410324,"Missing"
W09-2816,qiu-etal-2004-public,0,0.0697036,"Missing"
W09-2816,M95-1005,0,0.142828,"formance.6 The basic idea is that it seems likely that badly chosen reference chains affect the ability to resolve REs in automatic coreference resolution tools which will tend to perform worse with poorly selected MSR reference chains. To counteract the possibility of results being a function of a specific coreference resolution algorithm or tool, we used two different resolvers— those included in LingPipe7 and OpenNLP (Morton, 2005)—and averaged results. There does not appear to be a single standard evaluation metric in the coreference resolution community, so we opted to use three: MUC -6 (Vilain et al., 1995), CEAF (Luo, 2005), and B CUBED (Bagga and Baldwin, 1998), which seem to be the most widely accepted metrics. All three metrics compute Recall, Precision and F-Scores on aligned gold-standard and resolver-tool coreference chains. They differ in how the alignment is obtained and what components of coreference chains are counted for calculating scores. Results for the automatic extrinsic evaluations are reported below in terms of the F-Scores from these three metrics, as well as in terms of their mean. 4.3 Human intrinsic evaluation The intrinsic human evaluation involved 24 randomly selected it"
W09-2816,W07-2302,1,\N,Missing
W09-2817,M95-1005,0,0.0607527,"two preceding REFs; POS tags of 4 words before and 3 words after; correlation between SYNFUNC and CASE values; size of the chain. WLV- BIAS is the same except that it is retrained on reweighted training instances. The reweighting scheme assigns a cost of 3 to false negatives and 1 to false positives. To counteract the possibility of results being a function of a specific coreference resolution algorithm or tool, we used two different resolvers— those included in LingPipe2 and OpenNLP (Morton, 2005)—and averaged results. For the same reason we used three different performance measures: MUC -6 (Vilain et al., 1995), CEAF (Luo, 2005), and B - CUBED (Bagga and Baldwin, 1998). 5 Systems Base-rand, Base-freq, Base-1st, Base-name: We created four baseline systems each with a different way of selecting a REFEX from those REFEXs in the ALT-REFEX list that have matching entity IDs. Base-rand selects a REFEX at random. Base-1st selects the first REFEX. Base-freq selects the first REFEX with a REG08-TYPE that is the overall most frequent (as determined from the training/development data) given the SYNCAT, SYNFUNC and SEMCAT of the reference. Basename selects the shortest REFEX with attribute REG08-TYPE=name. 6 Re"
W09-2817,H05-1004,0,\N,Missing
W10-3206,gargett-etal-2010-give,0,0.0165687,"o participants, who worked together on a simple 2-D design task, buying and arranging furniture for two rooms. The COCONUT corpus is limited in annotations which describe symbolic object information such as object intrinsic attributes and location in discrete co-ordinates. As an initial work of constructing a corpus for collaborative tasks, the COCONUT corpus can be characterised as having a rather simple domain as well As for a multilingual aspect, all the above corpora are English. There have been several recent attempts at collecting multilingual corpora in situated domains. For instance, (Gargett et al., 2010) collected German and English corpora in the same setting. Their domain is similar to the QUAKE corpus. Van der Sluis et al. (2009) aim at a comparative study of referring expressions between English and Japanese. Their domain is still static at the moment. Our corpora aim at dealing with the dynamic nature of situated dialogues between very different languages, English and Japanese. 6 We called such expressions as action-mentioning expressions (AME) in our previous work. 44 eration, and evaluation in different tasks (puzzles) as well. We are planning to distribute the REX-J corpus family thro"
W10-3206,W09-0618,1,0.828626,"notations which describe symbolic object information such as object intrinsic attributes and location in discrete co-ordinates. As an initial work of constructing a corpus for collaborative tasks, the COCONUT corpus can be characterised as having a rather simple domain as well As for a multilingual aspect, all the above corpora are English. There have been several recent attempts at collecting multilingual corpora in situated domains. For instance, (Gargett et al., 2010) collected German and English corpora in the same setting. Their domain is similar to the QUAKE corpus. Van der Sluis et al. (2009) aim at a comparative study of referring expressions between English and Japanese. Their domain is still static at the moment. Our corpora aim at dealing with the dynamic nature of situated dialogues between very different languages, English and Japanese. 6 We called such expressions as action-mentioning expressions (AME) in our previous work. 44 eration, and evaluation in different tasks (puzzles) as well. We are planning to distribute the REX-J corpus family through GSK (Language Resources Association in Japan)8 , and the REX-E corpus from both University of Brighton and GSK. Table 7: The RE"
W10-3206,J95-3003,0,0.0876286,"he QUAKE and SCARE corpora, we allowed a comparatively large flexibility in the actions necessary for achieving the goal shape (i.e. flipping, turning and moving of puzzle pieces at different degrees), relative to the complexity of the domain. Providing this relatively larger freedom of actions to the participants together with the recording of detailed information allows for research into new aspects of referring expressions. Related work Over the last decade, with a growing recognition that referring expressions frequently appear in collaborative task dialogues (Clark and WilkesGibbs, 1986; Heeman and Hirst, 1995), a number of corpora have been constructed to study the nature of their use. This tendency also reflects the recognition that this area yields both challenging research topics as well as promising applications such as human-robot interaction (Foster et al., 2008; Kruijff et al., 2010). The COCONUT corpus (Di Eugenio et al., 2000) was collected from keyboard-dialogs between two participants, who worked together on a simple 2-D design task, buying and arranging furniture for two rooms. The COCONUT corpus is limited in annotations which describe symbolic object information such as object intrins"
W10-3206,W10-4214,1,0.485114,"Missing"
W10-3206,P10-1128,1,0.781753,"Missing"
W10-3206,stoia-etal-2008-scare,0,0.0932985,"erest (e.g. used in describing where the object is located in space). REs have attracted a great deal of attention in both language analysis and language generation research. In language analysis research, 1 2 http://www.nlpir.nist.gov/related projects/muc/ http://www.itl.nist.gov/iad/tests/ace/ 38 Proceedings of the 8th Workshop on Asian Language Resources, pages 38–46, c Beijing, China, 21-22 August 2010. 2010 Asian Federation for Natural Language Processing have been developed (Di Eugenio et al., 2000; Byron, 2005; van Deemter et al., 2006; Foster and Oberlander, 2007; Foster et al., 2008; Stoia et al., 2008; Spanger et al., 2009a; Belz et al., 2010). Unlike the corpora of MUC and ACE, many are collected from situated dialogues, and therefore include multimodal information (e.g. gestures and eye-gaze) other than just transcribed text (Martin et al., 2007). Foster and Oberlander (2007) emphasised that any corpus for language generation should include all possible contextual information at the appropriate granularity. Since constructing a dialogue corpus generally requires experiments for data collection, this kind of corpus tends to be small-scale compared with corpora for reference resolution. Ag"
W10-3206,W06-1420,0,0.0651162,"Missing"
W10-3206,P93-1007,0,0.0262941,"y the sequence of possible referents as its referent, if any are present. • All expressions appearing in muttering to oneself are excluded. Table 2 shows a list of attributes of referring expressions used in annotating the corpus. The rest of the 20 Japanese dialogues were annotated by two of the authors and discrepancies were resolved by discussion. Four English dialogues have been annotated so far by one of the authors. • The minimum span of a noun phrase including necessary information to identify a referent is annotated. The span might include repairs with their reparandum and disfluency (Nakatani and Hirschberg, 1993) if needed. 4 Preliminary corpus analysis We have already completed the Japanese corpus, which is named REX-J (2008-08), but only 4 out of 24 dialogues have been annotated for the English counterpart (REX-E (2010-03)). Table 3 shows a summary of the trials. The horizontal • Demonstrative adjectives are included in expressions. 42 lines divide the trials by pairs, “o” in the “success” column denotes that the trial was successfully completed in the time limit (15 minutes), and the “OP-REX” and “SV-REX” columns show the number of referring expressions used by the operator and the solver respectiv"
W10-3206,byron-fosler-lussier-2006-osu,0,\N,Missing
W10-4201,W09-0603,1,0.860664,"d gmax are the minimum and maximum gust speeds, and t is a time stamp (indicating for what time of the day the data is valid). The wind forecast texts were taken from comprehensive maritime weather forecasts produced by the professional meteorologists employed by a commercial weather forecasting company for clients who run offshore oilrigs. There were two evaluation criteria; Clarity was explained as indicating how understandable a forecast was, and Readability as indicating how fluent and readable it was. The experiment involved 22 forecast dates and outputs from the 10 systems described in (Belz and Kow, 2009) (also included in the corpus release) for those dates (as well as the corresponding forecasts in the corpus) in the evaluation, i.e. a total of 242 forecast texts. 4.2 Rating scale experiment We used the results of a previous experiment (Belz and Kow, 2009) in which participants were asked to rate forecast texts for Clarity and Readability, each on a scale of 1–7. The 22 participants were all University of Brighton staff whose first language was English and who had no experience of NLP. While earlier experiments used master mariners as well as lay-people in a similar evaluation (Belz and Reit"
W10-4201,E06-1040,1,0.919574,"ave different tendencies in using rating scales, e.g. what is known as ‘end-aversion’ tendency where certain individuals tend to stay away from the extreme ends of scales; other examples are positive skew and acquiescence bias, where individuals make disproportionately many positive or agreeing judgements; see e.g. Choi and Pak, (2005). It is not surprising then that stable averages of quality judgements, let alone high levels of agreement, are hard to achieve, as has been observed for MT (Turian et al., 2003; Lin and Och, 2004), text summarisation (Trang Dang, 2006), and language generation (Belz and Reiter, 2006). It has even been demonstrated that increasing the number of evaluators and/or data can have no stabilising effect at all on means (DUC literature). The result of a rating scale experiment is ordinal data (sets of scores selected from the discrete rating scale). The means-based ranks and statistical significance tests that are commonly presented with the results of RSEs are not generally considered appropriate for ordinal data in the statistics literature (Siegel, 1957). At the least, “a test on the means imposes the requirement that the measures must be additive, i.e. numerical” (Siegel, 195"
W10-4201,W09-2817,1,0.834975,"rential expressions (REs) and annotating them with coreference information as well as syntactic and semantic features. The following is an example of an annotated RE from the corpus: <REF ENTITY=""0"" MENTION=""1"" SEMCAT=""person"" SYNCAT=""np"" SYNFUNC=""subj""&gt;<REFEX ENTITY=""0"" REG08-TYPE=""name"" 2 The GREC - NEG data and documentation is available for download from http://www.nltg.brighton.ac.uk/home/Anja.Belz CASE=""plain""&gt;Sir Alexander Fleming</REFEX&gt; </REF&gt; (6 August 1881 - 11 March 1955) was a Scottish biologist and pharmacologist. This data was used in the GREC - NEG ’09 shared-task competition (Belz et al., 2009), where the task was to create systems which automatically select suitable REs for all references to all person entities in a text. The evaluation experiments use Clarity and Fluency as quality criteria which were explained in the introduction as follows (the wording of the first is from DUC): 1. Referential Clarity: It should be easy to identify who the referring expressions are referring to. If a person is mentioned, it should be clear what their role in the story is. So, a reference would be unclear if a person is referenced, but their identity or relation to the story remains unclear. 2. F"
W10-4201,W09-0629,1,0.825288,"here were 484 trials in this experiment. 4.3 Preference judgement experiment Our new experiment used our standardised preference strength sliders (bottom of Figure 2). We recruited 22 different evaluators from among students currently completing or recently having Type RSE Measure F(10,473) N sig diffs K’s W ( Text F(21,462) ( Evaluator F(21,462) PJE F(10,1865) N sig diffs K’s W Clarity 23.507** 24/55 .497** Readability 24.351** 23/55 .533** 1.467 1.961** ) 4.832** 4.824** ) 45.081** 34/55 .626** 41.318** 32/55 .542** ( Text F(21,916) 1.436 1.573 ) ( Evaluator F(21,921) .794 1.057 ) petition (Gatt et al., 2009).5 The TUNA data is a collection of images of domain entities paired with descriptions of entities. Each pair consists of seven entity images where one is highlighted (by a red box surrounding it), paired with a description of the highlighted entity, e.g.: Table 3: METEO RSE/PJE: Results of analyses looking at effect of System. completed a linguistics-related degree at Oxford, KCL, UCL, Sussex and Brighton. We had at our disposal 11 METEO systems, so 11 there were 2 = 55 system combinations to evaluate on the 22 test data items. We decided on a design of ten 11 × 11 Latin Squares to accommoda"
W10-4201,C04-1072,0,0.0377057,"different meanings to scores and the distances between them. Individual evaluators have different tendencies in using rating scales, e.g. what is known as ‘end-aversion’ tendency where certain individuals tend to stay away from the extreme ends of scales; other examples are positive skew and acquiescence bias, where individuals make disproportionately many positive or agreeing judgements; see e.g. Choi and Pak, (2005). It is not surprising then that stable averages of quality judgements, let alone high levels of agreement, are hard to achieve, as has been observed for MT (Turian et al., 2003; Lin and Och, 2004), text summarisation (Trang Dang, 2006), and language generation (Belz and Reiter, 2006). It has even been demonstrated that increasing the number of evaluators and/or data can have no stabilising effect at all on means (DUC literature). The result of a rating scale experiment is ordinal data (sets of scores selected from the discrete rating scale). The means-based ranks and statistical significance tests that are commonly presented with the results of RSEs are not generally considered appropriate for ordinal data in the statistics literature (Siegel, 1957). At the least, “a test on the means"
W10-4201,W06-0707,0,0.0211305,"between them. Individual evaluators have different tendencies in using rating scales, e.g. what is known as ‘end-aversion’ tendency where certain individuals tend to stay away from the extreme ends of scales; other examples are positive skew and acquiescence bias, where individuals make disproportionately many positive or agreeing judgements; see e.g. Choi and Pak, (2005). It is not surprising then that stable averages of quality judgements, let alone high levels of agreement, are hard to achieve, as has been observed for MT (Turian et al., 2003; Lin and Och, 2004), text summarisation (Trang Dang, 2006), and language generation (Belz and Reiter, 2006). It has even been demonstrated that increasing the number of evaluators and/or data can have no stabilising effect at all on means (DUC literature). The result of a rating scale experiment is ordinal data (sets of scores selected from the discrete rating scale). The means-based ranks and statistical significance tests that are commonly presented with the results of RSEs are not generally considered appropriate for ordinal data in the statistics literature (Siegel, 1957). At the least, “a test on the means imposes the requirement that the measur"
W10-4201,2003.mtsummit-papers.51,0,0.0441827,"aluators may ascribe different meanings to scores and the distances between them. Individual evaluators have different tendencies in using rating scales, e.g. what is known as ‘end-aversion’ tendency where certain individuals tend to stay away from the extreme ends of scales; other examples are positive skew and acquiescence bias, where individuals make disproportionately many positive or agreeing judgements; see e.g. Choi and Pak, (2005). It is not surprising then that stable averages of quality judgements, let alone high levels of agreement, are hard to achieve, as has been observed for MT (Turian et al., 2003; Lin and Och, 2004), text summarisation (Trang Dang, 2006), and language generation (Belz and Reiter, 2006). It has even been demonstrated that increasing the number of evaluators and/or data can have no stabilising effect at all on means (DUC literature). The result of a rating scale experiment is ordinal data (sets of scores selected from the discrete rating scale). The means-based ranks and statistical significance tests that are commonly presented with the results of RSEs are not generally considered appropriate for ordinal data in the statistics literature (Siegel, 1957). At the least, “"
W10-4217,W09-0603,1,0.815813,"a subset of data having corresponding text fragments, and other text fragments having no obvious corresponding data items. Moreover, data transformations may be necessary before corresponding text fragments can be identified. In this report, we look at the possibility of automatically extracting parallel data-text fragments from comparable corpora in the case of D 2 T from static database records. Such a parallel data-text resource could then be used to train an existing D 2 T generation system, or even build a new statistical generator from scratch, e.g. using techniques from statistical MT (Belz and Kow, 2009). The steps involved in going from comparable data and text resources to generators that produce texts similar to those in the text resource are then as follows: (1) identify sources on the Web for comparable data and texts; (2) pair up data records and texts; (3) extract parallel fragments (sets of data fields paired with word strings); (4) train a D 2 T generator using the parallel fragments; and (5) feed data inputs to the generator which then Figure 1: Overview of processing steps. generates new texts describing them. Figure 1 illustrates steps 1–3 which this paper focuses on. In Section 3"
W10-4217,J93-1003,0,0.12671,"ll’s name and height is replaced by lexical class tokens NAME , HEIGHT METRES or HEIGHT FEET , the first step is to construct a kind of lexicon of pairs (d, w) of data fields d and words w, such that w is often seen in the realisation of d. For this purpose we adapt Munteanu & Marcu’s (2006) method for (language to language) lexicon construction. For this purpose we compute a measure of the strength of association between data fields and words; we use the G2 loglikelihood ratio which has been widely used for this sort of purpose (especially lexical association) since it was introduced to NLP (Dunning, 1993). Following Moore (2004a) rather than Munteanu & Marcu, our current notion of cooccurrence is that a data field and word cooccur if they are present in the same pair of data fields and sentence (as identified by the method described in Section 4.1 above). We then obtain counts for the number of times each word cooccurs with each data field, and the number of times it occurs without the data field being present (and conversely). This allows us to compute the G2 score, for which we use the formulation from Moore (2004b) shown in Figure 2. If the G2 score for a given (d, w) pair is greater than p"
W10-4217,W08-1131,1,0.748587,"orpus of data and texts relating to British hills and the techniques for extracting paired input/output fragments we have developed so far. 1 Introduction Starting with Knight, Langkilde and Hatzivassiloglou’s work on Nitrogen and its successor Halogen (Knight and Hatzivassiloglou, 1995; Knight and Langkilde, 2000), NLG has over the past 15 years moved towards using statistical techniques, in particular in surface realisation (Langkilde, 2002; White, 2004), referring expression generation (most of the sytems submitted to the TUNA and GREC shared task evaluation challenges are statistical, see Gatt et al. (2008), for example), and data-to-text generation (Belz, 2008). The impetus for introducing statistical techniques in NLG can be said to have originally come from machine translation (MT),1 but unlike MT, where parallel corpora of inputs (source language texts) and outputs (translated texts) occur naturally at least in some domains,2 NLG on the whole has to use manually created input/output pairs. Data-to-text generation (D 2 T) is the type of NLG that perhaps comes closest to having naturally occuring inputs and outputs at its disposal. Work in D 2 T has involved different domains including generat"
W10-4217,P95-1034,0,0.1114,"uk Abstract Building NLG systems, in particular statistical ones, requires parallel data (paired inputs and outputs) which do not generally occur naturally. In this paper, we investigate the idea of automatically extracting parallel resources for data-to-text generation from comparable corpora obtained from the Web. We describe our comparable corpus of data and texts relating to British hills and the techniques for extracting paired input/output fragments we have developed so far. 1 Introduction Starting with Knight, Langkilde and Hatzivassiloglou’s work on Nitrogen and its successor Halogen (Knight and Hatzivassiloglou, 1995; Knight and Langkilde, 2000), NLG has over the past 15 years moved towards using statistical techniques, in particular in surface realisation (Langkilde, 2002; White, 2004), referring expression generation (most of the sytems submitted to the TUNA and GREC shared task evaluation challenges are statistical, see Gatt et al. (2008), for example), and data-to-text generation (Belz, 2008). The impetus for introducing statistical techniques in NLG can be said to have originally come from machine translation (MT),1 but unlike MT, where parallel corpora of inputs (source language texts) and outputs ("
W10-4217,W02-2103,0,0.0350775,"vestigate the idea of automatically extracting parallel resources for data-to-text generation from comparable corpora obtained from the Web. We describe our comparable corpus of data and texts relating to British hills and the techniques for extracting paired input/output fragments we have developed so far. 1 Introduction Starting with Knight, Langkilde and Hatzivassiloglou’s work on Nitrogen and its successor Halogen (Knight and Hatzivassiloglou, 1995; Knight and Langkilde, 2000), NLG has over the past 15 years moved towards using statistical techniques, in particular in surface realisation (Langkilde, 2002; White, 2004), referring expression generation (most of the sytems submitted to the TUNA and GREC shared task evaluation challenges are statistical, see Gatt et al. (2008), for example), and data-to-text generation (Belz, 2008). The impetus for introducing statistical techniques in NLG can be said to have originally come from machine translation (MT),1 but unlike MT, where parallel corpora of inputs (source language texts) and outputs (translated texts) occur naturally at least in some domains,2 NLG on the whole has to use manually created input/output pairs. Data-to-text generation (D 2 T) i"
W10-4217,W04-3243,0,0.0961351,"placed by lexical class tokens NAME , HEIGHT METRES or HEIGHT FEET , the first step is to construct a kind of lexicon of pairs (d, w) of data fields d and words w, such that w is often seen in the realisation of d. For this purpose we adapt Munteanu & Marcu’s (2006) method for (language to language) lexicon construction. For this purpose we compute a measure of the strength of association between data fields and words; we use the G2 loglikelihood ratio which has been widely used for this sort of purpose (especially lexical association) since it was introduced to NLP (Dunning, 1993). Following Moore (2004a) rather than Munteanu & Marcu, our current notion of cooccurrence is that a data field and word cooccur if they are present in the same pair of data fields and sentence (as identified by the method described in Section 4.1 above). We then obtain counts for the number of times each word cooccurs with each data field, and the number of times it occurs without the data field being present (and conversely). This allows us to compute the G2 score, for which we use the formulation from Moore (2004b) shown in Figure 2. If the G2 score for a given (d, w) pair is greater than p(d)p(w), then the assoc"
W10-4217,J05-4003,0,0.0668074,"dentifying comparable corpora. Much of it has focused on identifying word translations in comparable corpora, e.g. Rapp’s approach was based on the simple and elegant assumption that if words Af and Bf have a higher than chance co-occurrence frequency in one language, then two appropriate translations Ae and Be in another language will also have a higher than chance co-occurrence frequency (Rapp, 1995; Rapp, 1999). At the other end of the spectrum, Resnik & Smith (2003) search the Web to detect web pages that are translations of each other. Other approaches aim to identify pairs of sentences (Munteanu and Marcu, 2005) or subsentential fragments (Munteanu and Marcu, 2006) that are parallel within comparable corpora. The latter approach is particularly relevant to our work. They start by translating each document in the source language (SL) word for word into the target language (TL). The result is given to an information retrieval (IR) system as a query, and the top 20 results are retained and paired with the given SL document. They then obtain all sentence pairs from each pair of SL and TL documents, and discard those sentence pairs with few words that are translations of each other. To the remaining sente"
W10-4217,P06-1011,0,0.0573115,"on identifying word translations in comparable corpora, e.g. Rapp’s approach was based on the simple and elegant assumption that if words Af and Bf have a higher than chance co-occurrence frequency in one language, then two appropriate translations Ae and Be in another language will also have a higher than chance co-occurrence frequency (Rapp, 1995; Rapp, 1999). At the other end of the spectrum, Resnik & Smith (2003) search the Web to detect web pages that are translations of each other. Other approaches aim to identify pairs of sentences (Munteanu and Marcu, 2005) or subsentential fragments (Munteanu and Marcu, 2006) that are parallel within comparable corpora. The latter approach is particularly relevant to our work. They start by translating each document in the source language (SL) word for word into the target language (TL). The result is given to an information retrieval (IR) system as a query, and the top 20 results are retained and paired with the given SL document. They then obtain all sentence pairs from each pair of SL and TL documents, and discard those sentence pairs with few words that are translations of each other. To the remaining sentences they then apply a fragment detection method which"
W10-4217,P95-1050,0,0.071099,"ra, and the need for very large amounts of parallel training data, has led to a sizeable research effort to develop methods for automatically constructing parallel resources. This work typically starts by identifying comparable corpora. Much of it has focused on identifying word translations in comparable corpora, e.g. Rapp’s approach was based on the simple and elegant assumption that if words Af and Bf have a higher than chance co-occurrence frequency in one language, then two appropriate translations Ae and Be in another language will also have a higher than chance co-occurrence frequency (Rapp, 1995; Rapp, 1999). At the other end of the spectrum, Resnik & Smith (2003) search the Web to detect web pages that are translations of each other. Other approaches aim to identify pairs of sentences (Munteanu and Marcu, 2005) or subsentential fragments (Munteanu and Marcu, 2006) that are parallel within comparable corpora. The latter approach is particularly relevant to our work. They start by translating each document in the source language (SL) word for word into the target language (TL). The result is given to an information retrieval (IR) system as a query, and the top 20 results are retained"
W10-4217,P99-1067,0,0.0407031,"need for very large amounts of parallel training data, has led to a sizeable research effort to develop methods for automatically constructing parallel resources. This work typically starts by identifying comparable corpora. Much of it has focused on identifying word translations in comparable corpora, e.g. Rapp’s approach was based on the simple and elegant assumption that if words Af and Bf have a higher than chance co-occurrence frequency in one language, then two appropriate translations Ae and Be in another language will also have a higher than chance co-occurrence frequency (Rapp, 1995; Rapp, 1999). At the other end of the spectrum, Resnik & Smith (2003) search the Web to detect web pages that are translations of each other. Other approaches aim to identify pairs of sentences (Munteanu and Marcu, 2005) or subsentential fragments (Munteanu and Marcu, 2006) that are parallel within comparable corpora. The latter approach is particularly relevant to our work. They start by translating each document in the source language (SL) word for word into the target language (TL). The result is given to an information retrieval (IR) system as a query, and the top 20 results are retained and paired wi"
W10-4217,J03-3002,0,0.0274472,"g data, has led to a sizeable research effort to develop methods for automatically constructing parallel resources. This work typically starts by identifying comparable corpora. Much of it has focused on identifying word translations in comparable corpora, e.g. Rapp’s approach was based on the simple and elegant assumption that if words Af and Bf have a higher than chance co-occurrence frequency in one language, then two appropriate translations Ae and Be in another language will also have a higher than chance co-occurrence frequency (Rapp, 1995; Rapp, 1999). At the other end of the spectrum, Resnik & Smith (2003) search the Web to detect web pages that are translations of each other. Other approaches aim to identify pairs of sentences (Munteanu and Marcu, 2005) or subsentential fragments (Munteanu and Marcu, 2006) that are parallel within comparable corpora. The latter approach is particularly relevant to our work. They start by translating each document in the source language (SL) word for word into the target language (TL). The result is given to an information retrieval (IR) system as a query, and the top 20 results are retained and paired with the given SL document. They then obtain all sentence p"
W10-4217,W09-0629,1,\N,Missing
W10-4217,P04-1066,0,\N,Missing
W10-4226,W07-2302,1,0.823235,"elements that are embedded in REFEX elements contained in an ALT-REFEX list have an unspecified MENTION id (the ‘?’ value). Furthermore, such REF elements have had their enclosed REFEX removed. The two test data sets exist in two versions: 1. Version a: each text has a single human-selected referring expression for each reference (i.e. the one found in the original Wikipedia article). 2. Version b: the same subset of texts as in (a); for this set we did not use the RE s in the corpus, but replaced each of them with human-selected alternatives obtained in an online experiment as described in (Belz and Varges, 2007); this version of the test set therefore contains three versions of each text where all the REFEXs in a given version were selected by one ‘author’. The training, development and test data for the GREC - NEG task is exactly as described above. The training and development data for the GREC NER / Full tasks comes in two versions. The first is identical to the standard XML-annotated version of the GREC-People corpus as described above (Section 2). The second is in the test data input format. In this format, texts have no REFEX and REF tags, and no ALT-REFEX element. A further difference is that"
W10-4226,H05-1004,0,0.0233533,"contained in the outermost REFEX i.e. embedded REFEX word strings were not considered separately. We also computed BLEU -3, NIST, string-edit distance and length-normalised string-edit distance, all on word strings defined as for String Accuracy. BLEU and NIST are designed for multiple output versions, and for the string-edit metrics we computed the mean of means over the three textlevel scores (computed against the three versions of a text). To measure accuracy in the NER task, we applied three commonly used performance measures for coreference resolution: MUC -6 (Vilain et al., 1995), CEAF (Luo, 2005), and B - CUBED (Bagga and Baldwin, 1998). 3.2 Human-assessed evaluations We designed the human-assessed intrinsic evaluation as a preference-judgement test where subjects expressed their preference, in terms of two criteria, for either the original Wikipedia text or the version of it with system-generated referring expressions in it. For the GREC - NEG systems, the intrinsic human evaluation involved system outputs for 30 randomly selected items from the test set. We used a Repeated Latin Squares design which ensures that each subject sees the same number of outputs from each system and for e"
W10-4226,M95-1005,0,0.0699942,"e, ‘flattened’ word strings contained in the outermost REFEX i.e. embedded REFEX word strings were not considered separately. We also computed BLEU -3, NIST, string-edit distance and length-normalised string-edit distance, all on word strings defined as for String Accuracy. BLEU and NIST are designed for multiple output versions, and for the string-edit metrics we computed the mean of means over the three textlevel scores (computed against the three versions of a text). To measure accuracy in the NER task, we applied three commonly used performance measures for coreference resolution: MUC -6 (Vilain et al., 1995), CEAF (Luo, 2005), and B - CUBED (Bagga and Baldwin, 1998). 3.2 Human-assessed evaluations We designed the human-assessed intrinsic evaluation as a preference-judgement test where subjects expressed their preference, in terms of two criteria, for either the original Wikipedia text or the version of it with system-generated referring expressions in it. For the GREC - NEG systems, the intrinsic human evaluation involved system outputs for 30 randomly selected items from the test set. We used a Repeated Latin Squares design which ensures that each subject sees the same number of outputs from eac"
W10-4237,P03-2030,0,0.0265808,"Arabic over five years. There are other resources which may be useful. Zettelmoyer and Collins (2009) have manually converted the original SQL meaning annotations of the ATIS corpus (et al., 1994)— some 4,637 sentences—into lambda-calculus expressions which were used for training and testing their semantic parser. This resource might make a good out-of-domain test set for generation systems trained on WSJ data. FrameNet, used for semantic parsing, see for example Gildea and Jurafsky (2002), identifies a sentence’s frame elements and assigns semantic roles to the frame elements. FrameNet data (Baker and Sato, 2003) was used for training and test sets in one of the SensEval-3 shared tasks in 2004 (Automatic Labeling of Semantic Roles). There has been some work combining FrameNet with other lexical resources. For example, Shi and Mihalcea (2005) integrated FrameNet with VerbNet and WordNet for the purpose of enabling more robust semantic parsing. The Semlink project (http://verbs.colorado. edu/semlink/) aims to integrate Propbank, FrameNet, WordNet and VerbNet. Other relevant work includes Moldovan and Rus (Moldovan and Rus, 2001; Rus, 2002) who developed a technique for parsing into logical forms and use"
W10-4237,P06-1130,1,0.915952,"Missing"
W10-4237,I05-1015,0,0.0150032,"underspecified, and any decision not made before the realiser was decided simply by highest likelihood according to a language model, automatically trainable from raw corpora. The Halogen/Nitrogen work sparked an interest in statistical NLG which led to a range of surface realisation methods that used corpus frequencies in one way or another (Varges and Mellish, 2001; White, 2004; Velldal et al., 2004; Paiva and Evans, 2005). Some surface realisation work looked at directly applying statistical models during a linguistically informed generation process to prune the search space (White, 2004; Carroll and Oepen, 2005). While statistical techniques have led to realisers that are more (re)usable, we currently still have no way of determining what the state of the art is. A significant subset of statistical realisation work (Langkilde, 2002; Callaway, 2003; Nakanishi et al., 2005; Zhong and Stent, 2005; Cahill and van Genabith, 2006; White and Rajkumar, 2009) has recently produced results for regenerating the Penn Treebank. The basic approach in all this work is to remove information from the Penn Treebank parses (the word strings themselves as well as some of the parse information), and then convert and use"
W10-4237,W96-0501,0,0.0418685,"cognisers, coreference resolvers, and many other tools. Not only is there a real choice between a range of different systems performing the same task, there are also evaluation methodologies to help determine what the state of the art is. Natural Language Generation (NLG) has not so far developed generic tools and methods for comparing them to the same extent as Natural Language Analysis (NLA) has. The subfield of NLG that has perhaps come closest to developing generic tools is surface realisation. Wide-coverage surface realisers such as PENMAN / NIGEL (Mann and Mathiesen, 1983), FUF / SURGE (Elhadad and Robin, 1996) and REALPRO (Lavoie and Rambow, 1997) were intended to be more or less offthe-shelf plug-and-play modules. But they tended to require a significant amount of work to adapt and integrate, and required highly specific inputs incorporating up to several hundred features that needed to be set. With the advent of statistical techniques in NLG surface realisers appeared for which it was far simpler to supply inputs, as information not provided in the inputs could be added on the basis of likelihood. An early example, the Japan-Gloss system (Knight et al., 1995) replaced PENMAN’s default settings wi"
W10-4237,H94-1010,0,0.0443218,"Missing"
W10-4237,J02-3001,0,0.0132479,"ogy, and coreference. The current goal is to annotate over a million words each of English and Chinese, and half a million words of Arabic over five years. There are other resources which may be useful. Zettelmoyer and Collins (2009) have manually converted the original SQL meaning annotations of the ATIS corpus (et al., 1994)— some 4,637 sentences—into lambda-calculus expressions which were used for training and testing their semantic parser. This resource might make a good out-of-domain test set for generation systems trained on WSJ data. FrameNet, used for semantic parsing, see for example Gildea and Jurafsky (2002), identifies a sentence’s frame elements and assigns semantic roles to the frame elements. FrameNet data (Baker and Sato, 2003) was used for training and test sets in one of the SensEval-3 shared tasks in 2004 (Automatic Labeling of Semantic Roles). There has been some work combining FrameNet with other lexical resources. For example, Shi and Mihalcea (2005) integrated FrameNet with VerbNet and WordNet for the purpose of enabling more robust semantic parsing. The Semlink project (http://verbs.colorado. edu/semlink/) aims to integrate Propbank, FrameNet, WordNet and VerbNet. Other relevant work"
W10-4237,P98-1116,0,0.0286606,"more or less offthe-shelf plug-and-play modules. But they tended to require a significant amount of work to adapt and integrate, and required highly specific inputs incorporating up to several hundred features that needed to be set. With the advent of statistical techniques in NLG surface realisers appeared for which it was far simpler to supply inputs, as information not provided in the inputs could be added on the basis of likelihood. An early example, the Japan-Gloss system (Knight et al., 1995) replaced PENMAN’s default settings with statistical decisions. The Halogen/Nitrogen developers (Langkilde and Knight, 1998a) allowed inputs to be arbitrarily underspecified, and any decision not made before the realiser was decided simply by highest likelihood according to a language model, automatically trainable from raw corpora. The Halogen/Nitrogen work sparked an interest in statistical NLG which led to a range of surface realisation methods that used corpus frequencies in one way or another (Varges and Mellish, 2001; White, 2004; Velldal et al., 2004; Paiva and Evans, 2005). Some surface realisation work looked at directly applying statistical models during a linguistically informed generation process to pr"
W10-4237,W02-2103,0,0.0463994,"ical NLG which led to a range of surface realisation methods that used corpus frequencies in one way or another (Varges and Mellish, 2001; White, 2004; Velldal et al., 2004; Paiva and Evans, 2005). Some surface realisation work looked at directly applying statistical models during a linguistically informed generation process to prune the search space (White, 2004; Carroll and Oepen, 2005). While statistical techniques have led to realisers that are more (re)usable, we currently still have no way of determining what the state of the art is. A significant subset of statistical realisation work (Langkilde, 2002; Callaway, 2003; Nakanishi et al., 2005; Zhong and Stent, 2005; Cahill and van Genabith, 2006; White and Rajkumar, 2009) has recently produced results for regenerating the Penn Treebank. The basic approach in all this work is to remove information from the Penn Treebank parses (the word strings themselves as well as some of the parse information), and then convert and use these underspecified representations as inputs to the surface realiser whose task it is to reproduce the original treebank sentence. Results are typically evaluated using BLEU, and, roughly speaking, BLEU scores go down as m"
W10-4237,A97-1039,0,0.0697101,"many other tools. Not only is there a real choice between a range of different systems performing the same task, there are also evaluation methodologies to help determine what the state of the art is. Natural Language Generation (NLG) has not so far developed generic tools and methods for comparing them to the same extent as Natural Language Analysis (NLA) has. The subfield of NLG that has perhaps come closest to developing generic tools is surface realisation. Wide-coverage surface realisers such as PENMAN / NIGEL (Mann and Mathiesen, 1983), FUF / SURGE (Elhadad and Robin, 1996) and REALPRO (Lavoie and Rambow, 1997) were intended to be more or less offthe-shelf plug-and-play modules. But they tended to require a significant amount of work to adapt and integrate, and required highly specific inputs incorporating up to several hundred features that needed to be set. With the advent of statistical techniques in NLG surface realisers appeared for which it was far simpler to supply inputs, as information not provided in the inputs could be added on the basis of likelihood. An early example, the Japan-Gloss system (Knight et al., 1995) replaced PENMAN’s default settings with statistical decisions. The Halogen/"
W10-4237,W04-0413,0,0.0130197,"ank bracketing style allows extraction of simple predicate/argument structure. In addition to Treebank-1 material, Treebank-3 contains documents from the Switchboard and Brown corpora. 2. Propbank (Palmer et al., 2005): This is a semantic annotation of the Wall Street Journal section of Penn Treebank-2. More specifically, each verb occurring in the Treebank has been treated as a semantic predicate and the surrounding text has been annotated for arguments and adjuncts of the predicate. The verbs have also been tagged with coarse grained senses and with inflectional information. 3. NomBank 1.0 (Meyers et al., 2004): NomBank is an annotation project at New York University that provides argument structure for common nouns in the Penn Treebank. NomBank marks the sets of arguments that occur with nouns in PropBank I, just as the latter records such information for verbs. 4. BBN Pronoun Coreference and Entity Type Corpus (Weischedel and Brunstein, 2005): supplements the Wall Street Journal corpus, adding annotation of pronoun coreference, and a variety of entity and numeric types. 5. FrameNet (Johnson et al., 2002): 150,000 sentences annotated for semantic roles and possible syntactic realisations. The annot"
W10-4237,P01-1052,0,0.0217028,"ame elements and assigns semantic roles to the frame elements. FrameNet data (Baker and Sato, 2003) was used for training and test sets in one of the SensEval-3 shared tasks in 2004 (Automatic Labeling of Semantic Roles). There has been some work combining FrameNet with other lexical resources. For example, Shi and Mihalcea (2005) integrated FrameNet with VerbNet and WordNet for the purpose of enabling more robust semantic parsing. The Semlink project (http://verbs.colorado. edu/semlink/) aims to integrate Propbank, FrameNet, WordNet and VerbNet. Other relevant work includes Moldovan and Rus (Moldovan and Rus, 2001; Rus, 2002) who developed a technique for parsing into logical forms and used this to transform WordNet concept definitions into logical forms. The same method (with additional manual correction) was used to produce the test set for another SensEval-3 shared task (Identification of Logic Forms in English). 4.1 CoNLL 2008 Shared Task Data Perhaps the most immediately promising resource is is the CoNLL shared task data from 2008 (Surdeanu et al., 2008) which has syntactic dependency annotations, named-entity boundaries and the semantic dependencies model roles of both verbal and nominal predica"
W10-4237,W05-1510,0,0.232262,"surface realisation methods that used corpus frequencies in one way or another (Varges and Mellish, 2001; White, 2004; Velldal et al., 2004; Paiva and Evans, 2005). Some surface realisation work looked at directly applying statistical models during a linguistically informed generation process to prune the search space (White, 2004; Carroll and Oepen, 2005). While statistical techniques have led to realisers that are more (re)usable, we currently still have no way of determining what the state of the art is. A significant subset of statistical realisation work (Langkilde, 2002; Callaway, 2003; Nakanishi et al., 2005; Zhong and Stent, 2005; Cahill and van Genabith, 2006; White and Rajkumar, 2009) has recently produced results for regenerating the Penn Treebank. The basic approach in all this work is to remove information from the Penn Treebank parses (the word strings themselves as well as some of the parse information), and then convert and use these underspecified representations as inputs to the surface realiser whose task it is to reproduce the original treebank sentence. Results are typically evaluated using BLEU, and, roughly speaking, BLEU scores go down as more information is removed. While public"
W10-4237,P05-1008,0,0.0183244,"-Gloss system (Knight et al., 1995) replaced PENMAN’s default settings with statistical decisions. The Halogen/Nitrogen developers (Langkilde and Knight, 1998a) allowed inputs to be arbitrarily underspecified, and any decision not made before the realiser was decided simply by highest likelihood according to a language model, automatically trainable from raw corpora. The Halogen/Nitrogen work sparked an interest in statistical NLG which led to a range of surface realisation methods that used corpus frequencies in one way or another (Varges and Mellish, 2001; White, 2004; Velldal et al., 2004; Paiva and Evans, 2005). Some surface realisation work looked at directly applying statistical models during a linguistically informed generation process to prune the search space (White, 2004; Carroll and Oepen, 2005). While statistical techniques have led to realisers that are more (re)usable, we currently still have no way of determining what the state of the art is. A significant subset of statistical realisation work (Langkilde, 2002; Callaway, 2003; Nakanishi et al., 2005; Zhong and Stent, 2005; Cahill and van Genabith, 2006; White and Rajkumar, 2009) has recently produced results for regenerating the Penn Tre"
W10-4237,J05-1004,0,0.0427927,"these include documents originally included in the Penn Treebank, and thus make it possible in principle to combine the various levels of annotation into a single commonground representation. The following is a (nonexhaustive) list of such resources: 1. Penn Treebank-3 (Marcus et al., 1999): one million words of hand-parsed 1989 Wall Street Journal material annotated in Treebank II style. The Treebank bracketing style allows extraction of simple predicate/argument structure. In addition to Treebank-1 material, Treebank-3 contains documents from the Switchboard and Brown corpora. 2. Propbank (Palmer et al., 2005): This is a semantic annotation of the Wall Street Journal section of Penn Treebank-2. More specifically, each verb occurring in the Treebank has been treated as a semantic predicate and the surrounding text has been annotated for arguments and adjuncts of the predicate. The verbs have also been tagged with coarse grained senses and with inflectional information. 3. NomBank 1.0 (Meyers et al., 2004): NomBank is an annotation project at New York University that provides argument structure for common nouns in the Penn Treebank. NomBank marks the sets of arguments that occur with nouns in PropBan"
W10-4237,W08-2121,0,0.0516008,"Missing"
W10-4237,N01-1001,0,0.0158715,"added on the basis of likelihood. An early example, the Japan-Gloss system (Knight et al., 1995) replaced PENMAN’s default settings with statistical decisions. The Halogen/Nitrogen developers (Langkilde and Knight, 1998a) allowed inputs to be arbitrarily underspecified, and any decision not made before the realiser was decided simply by highest likelihood according to a language model, automatically trainable from raw corpora. The Halogen/Nitrogen work sparked an interest in statistical NLG which led to a range of surface realisation methods that used corpus frequencies in one way or another (Varges and Mellish, 2001; White, 2004; Velldal et al., 2004; Paiva and Evans, 2005). Some surface realisation work looked at directly applying statistical models during a linguistically informed generation process to prune the search space (White, 2004; Carroll and Oepen, 2005). While statistical techniques have led to realisers that are more (re)usable, we currently still have no way of determining what the state of the art is. A significant subset of statistical realisation work (Langkilde, 2002; Callaway, 2003; Nakanishi et al., 2005; Zhong and Stent, 2005; Cahill and van Genabith, 2006; White and Rajkumar, 2009)"
W10-4237,P09-1110,0,0.022058,"Missing"
W10-4237,D09-1043,0,\N,Missing
W10-4237,C98-1112,0,\N,Missing
W11-1214,W05-1617,0,0.0178344,"s (Section 7). 2 Background and Related Research Work in data-to-text generation has involved a variety of different domains, including generating weather forecasts from meteorological data (Sripada et al., 2003), nursing reports from intensive care data (Portet et al., 2009), and museum exhibit descriptions from database records (Isard et al., 2003; Stock et al., 2007); types of data have included dynamic time-series data (such as meteorological or medical data) and static database entries (as in museum exhibits). The following is an example of an input/output pair from the M - PIRO project (Androutsopoulos et al., 2005), where the input is a database record for a museum artifact, and the output is a description of the artifact: creation-period=archaic-period, current-location=Un-museum-Pennsylvania, painting-techinique-used=red-figure-technique, painted-by=Eucharides, creation-time=between (500 year BC)(480 year BC) Classical kylix This exhibit is a kylix; it was created during the archaic period and was painted with the red figure technique by Eucharides. It dates from between 500 and 480 B.C. and currently it is in the University Museum of Pennsylvania. While data and texts in the three example domains cit"
W11-1214,W09-0603,1,0.841378,"responding text fragments, and other text fragments having no obvious corresponding data items. Moreover, data transformations may be necessary before corresponding text fragments can be identified. In this paper we look at the possibility of automatically identifying parallel data-text fragments from comparable corpora in the case of data-to-text generation from static database records. Such a parallel data-text resource could then be used to train an existing data-to-text generation system, or even to build a new statistical generator from scratch, e.g. using techniques from statistical MT (Belz and Kow, 2009). In statistical MT, the expense of manually creating new parallel MT corpora, and the need for very large amounts of parallel training data, has led to a sizeable research effort to develop methods for automatically constructing parallel resources. This work typically starts by identifying comparable corpora. Much of it has focused on identifying word translations in comparable corpora, e.g. Rapp’s approach was based on the simple and elegant assumption that if words Af and Bf have a higher than chance cooccurrence frequency in one language, then two appropriate translations Ae and Be in anot"
W11-1214,P06-4020,0,0.0199711,"Missing"
W11-1214,J93-1003,0,0.19683,"rst two sentences only. Our sentence selection method consists of (i) estimating the strength of association between data and text tokens (Section 5.1); and (ii) selecting those sentences for further consideration that have sufficiently strong and/or numerous associations with data tokens (Section 5.2). 5.1 Computing positive and negative associations between data and text We measure the strength of association between data tokens and text tokens using log-likelihood ratios which have been widely used for this sort of purpose (especially lexical association) since they were introduced to NLP (Dunning, 1993). They were e.g. used by Munteanu & Marcu (2006) to obtain a translation lexicon from word-aligned parallel texts. We start by obtaining counts for the number of times each text token w co-occurs with each data token d, the number of times w occurs without d being present, the number of times d occurs without w, and finally, the number of times neither occurs. Cooccurrence here is at the document/data record level, i.e. a data token and a text token co-occur if they are present in the same document/data record pair (pairs as produced by the method described in Section 3). This allows us to com"
W11-1214,W04-3243,0,0.301305,"allel texts. We start by obtaining counts for the number of times each text token w co-occurs with each data token d, the number of times w occurs without d being present, the number of times d occurs without w, and finally, the number of times neither occurs. Cooccurrence here is at the document/data record level, i.e. a data token and a text token co-occur if they are present in the same document/data record pair (pairs as produced by the method described in Section 3). This allows us to compute log likelihood ratios for all data-token/text-token pairs, using one of the G2 formulations from Moore (2004) which is shown in slightly different representation in Figure 3. The resulting G2 scores tell us whether the frequency with which a data token d and a text token w co-occur deviates from that expected by chance. If the G2 score for a given (d, w) pair is greater than their joint probability p(d)p(w), then the assoWikipedia text: Black Chew Head is the highest point ( or county top ) of Greater Manchester , and forms part of the Peak District , in northern England . Lying within the Saddleworth parish of the Metropolitan Borough of Oldham , close to Crowden , Derbyshire , it stands at a height"
W11-1214,J05-4003,0,0.0238144,"entifying comparable corpora. Much of it has focused on identifying word translations in comparable corpora, e.g. Rapp’s approach was based on the simple and elegant assumption that if words Af and Bf have a higher than chance cooccurrence frequency in one language, then two appropriate translations Ae and Be in another language will also have a higher than chance co-occurrence frequency (Rapp, 1995; Rapp, 1999). At the other end of the spectrum, Resnik and Smith (2003) search the Web to detect web pages that are translations of each other. Other approaches aim to identify pairs of sentences (Munteanu and Marcu, 2005) or subsentential fragments (Munteanu and Marcu, 2006) that are parallel within comparable corpora. The latter approach is particularly relevant to our work. Munteanu and Marcu start by translating each document in the source language (SL) word for word into the target language (TL). The result is given to an information retrieval (IR) system as a query, and the top 20 results are retained and paired with the given SL document. They then obtain all sentence pairs from each pair of SL and TL documents, and discard those sentence pairs that have only a small number of words that are translations"
W11-1214,P06-1011,0,0.0863137,"n identifying word translations in comparable corpora, e.g. Rapp’s approach was based on the simple and elegant assumption that if words Af and Bf have a higher than chance cooccurrence frequency in one language, then two appropriate translations Ae and Be in another language will also have a higher than chance co-occurrence frequency (Rapp, 1995; Rapp, 1999). At the other end of the spectrum, Resnik and Smith (2003) search the Web to detect web pages that are translations of each other. Other approaches aim to identify pairs of sentences (Munteanu and Marcu, 2005) or subsentential fragments (Munteanu and Marcu, 2006) that are parallel within comparable corpora. The latter approach is particularly relevant to our work. Munteanu and Marcu start by translating each document in the source language (SL) word for word into the target language (TL). The result is given to an information retrieval (IR) system as a query, and the top 20 results are retained and paired with the given SL document. They then obtain all sentence pairs from each pair of SL and TL documents, and discard those sentence pairs that have only a small number of words that are translations of each other. To the remaining sentences they then a"
W11-1214,P95-1050,0,0.077664,"ora, and the need for very large amounts of parallel training data, has led to a sizeable research effort to develop methods for automatically constructing parallel resources. This work typically starts by identifying comparable corpora. Much of it has focused on identifying word translations in comparable corpora, e.g. Rapp’s approach was based on the simple and elegant assumption that if words Af and Bf have a higher than chance cooccurrence frequency in one language, then two appropriate translations Ae and Be in another language will also have a higher than chance co-occurrence frequency (Rapp, 1995; Rapp, 1999). At the other end of the spectrum, Resnik and Smith (2003) search the Web to detect web pages that are translations of each other. Other approaches aim to identify pairs of sentences (Munteanu and Marcu, 2005) or subsentential fragments (Munteanu and Marcu, 2006) that are parallel within comparable corpora. The latter approach is particularly relevant to our work. Munteanu and Marcu start by translating each document in the source language (SL) word for word into the target language (TL). The result is given to an information retrieval (IR) system as a query, and the top 20 resul"
W11-1214,P99-1067,0,0.0688536,"need for very large amounts of parallel training data, has led to a sizeable research effort to develop methods for automatically constructing parallel resources. This work typically starts by identifying comparable corpora. Much of it has focused on identifying word translations in comparable corpora, e.g. Rapp’s approach was based on the simple and elegant assumption that if words Af and Bf have a higher than chance cooccurrence frequency in one language, then two appropriate translations Ae and Be in another language will also have a higher than chance co-occurrence frequency (Rapp, 1995; Rapp, 1999). At the other end of the spectrum, Resnik and Smith (2003) search the Web to detect web pages that are translations of each other. Other approaches aim to identify pairs of sentences (Munteanu and Marcu, 2005) or subsentential fragments (Munteanu and Marcu, 2006) that are parallel within comparable corpora. The latter approach is particularly relevant to our work. Munteanu and Marcu start by translating each document in the source language (SL) word for word into the target language (TL). The result is given to an information retrieval (IR) system as a query, and the top 20 results are retain"
W11-1214,J03-3002,0,0.00971313,"ng data, has led to a sizeable research effort to develop methods for automatically constructing parallel resources. This work typically starts by identifying comparable corpora. Much of it has focused on identifying word translations in comparable corpora, e.g. Rapp’s approach was based on the simple and elegant assumption that if words Af and Bf have a higher than chance cooccurrence frequency in one language, then two appropriate translations Ae and Be in another language will also have a higher than chance co-occurrence frequency (Rapp, 1995; Rapp, 1999). At the other end of the spectrum, Resnik and Smith (2003) search the Web to detect web pages that are translations of each other. Other approaches aim to identify pairs of sentences (Munteanu and Marcu, 2005) or subsentential fragments (Munteanu and Marcu, 2006) that are parallel within comparable corpora. The latter approach is particularly relevant to our work. Munteanu and Marcu start by translating each document in the source language (SL) word for word into the target language (TL). The result is given to an information retrieval (IR) system as a query, and the top 20 results are retained and paired with the given SL document. They then obtain"
W11-2832,W05-0909,0,0.0208607,"selected (and removed) from PTB Section 24. Note that a small number of sentences from the selected WSJ sections were not included in the CoNLL-08 data (and are thus not included in the SR Task data) due to difficulties in merging the various data sets (e.g. Section 23 has 17 fewer sentences). 3 Automatic Evaluations We computed scores using the following wellknown automatic evaluation metrics: implementations use smoothing to allow sentencelevel scores to be computed. 2. NIST:4,5 n-gram similarity weighted in favour of less frequent n-grams which are taken to be more informative. 3. METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2011):6 lexical similarity based on exact, stem, synonym, and paraphrase matches between words and phrases. 4. TER (Snover et al., 2006):7 a length-normalized edit distance metric where phrasal shifts are counted as one edit. For each metric, we calculated system-level scores, the mean of the sentence-level scores and weighted n-best scores (described below). Text normalisation: Output texts were normalised by lower-casing all tokens, removing any extraneous white space characters and ensuring consistent treatment of ampersands. 4 http://www.itl.nist.gov/iad/mig/tests/mt"
W11-2832,P11-2040,1,0.695361,"ccessful in choosing a single-best output that is more similar to the reference sentence than the others in the top 5. In the absence of multiple reference sentences or human evaluation results for the n-best list though, it is unclear to what extent the outputs in the n-best list might represent valid paraphrases versus clearly less acceptable outputs. 4 Human Evaluations 4.1 Experimental Set-up We assessed three criteria in the human evaluations: Clarity, Readability and Meaning Similarity. We used continuous sliders as rating tools (see Figures 1 and 2), because raters tend to prefer them (Belz and Kow, 2011). Slider positions were mapped to values from 0 to 100 (best). The instructions relating to Clarity and Readability read as follows:8 The first criterion you need to assess is Clarity. How clear (easy to understand) is the highlighted sentence within the context of the text extract? The second criterion to assess is Readability. This is sometimes called ’fluency’, and your task is to decide how well the highlighted sentence reads; is it good fluent English, or does it have grammatical errors, awkward constructions, etc. Note that you should assess Clarity separately from Readability: it is pos"
W11-2832,W10-4237,1,0.90252,"Missing"
W11-2832,C10-1012,0,0.117338,"ncies via the LTH Constituent-to-Dependency Conversion Tool for Penn-style Treebanks (Pennconverter) (Johansson and Nugues, 2007). The resulting dependency bank was then merged with the Nombank (Meyers et al., 2004) and Propbank (Palmer et al., 2005) corpora. Named entity information from the BBN Entity Type corpus was also integrated into the CoNLL-08 data. Our shallow representation is based on the Pennconverter dependencies. The deep representation is derived from the merged Nombank, Propbank and syntactic dependencies in a process similar to the graph completion 218 algorithm outlined in (Bohnet et al., 2010) (see Section 2.2 for differences). 2.1 Shallow representation The shallow data consists of unordered syntactic dependency trees. Each word and punctuation marker from the original sentence is represented as a node in a syntactic dependency tree. Nodes: The node information consists of a word’s lemma, a coarse-grained POS-tag, and, where appropriate, number, tense and participle features and a sense tag id (as a suffix to the lemma). In addition, two punctuation features encode the quotation and bracketing information for the sentence. The POS-tag set is slightly less fine-grained than the Pen"
W11-2832,P06-1130,0,0.108586,"Missing"
W11-2832,W11-2107,0,0.00555759,"om PTB Section 24. Note that a small number of sentences from the selected WSJ sections were not included in the CoNLL-08 data (and are thus not included in the SR Task data) due to difficulties in merging the various data sets (e.g. Section 23 has 17 fewer sentences). 3 Automatic Evaluations We computed scores using the following wellknown automatic evaluation metrics: implementations use smoothing to allow sentencelevel scores to be computed. 2. NIST:4,5 n-gram similarity weighted in favour of less frequent n-grams which are taken to be more informative. 3. METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2011):6 lexical similarity based on exact, stem, synonym, and paraphrase matches between words and phrases. 4. TER (Snover et al., 2006):7 a length-normalized edit distance metric where phrasal shifts are counted as one edit. For each metric, we calculated system-level scores, the mean of the sentence-level scores and weighted n-best scores (described below). Text normalisation: Output texts were normalised by lower-casing all tokens, removing any extraneous white space characters and ensuring consistent treatment of ampersands. 4 http://www.itl.nist.gov/iad/mig/tests/mt/doc /ngram-study.pdf 5 http"
W11-2832,W07-2416,0,0.0146291,"h are ordered). The shallow input representation is intended to be a more ‘surfacey’, syntactic represention of the sentence. The deep(er) input type is intended to be closer to a semantic, more abstract, representation of the meaning of the sentence. The input representations were created by postprocessing the CoNLL 2008 Shared Task data (Surdeanu et al., 2008). For the preparation of the CoNLL-08 Shared task data, selected sections of the Penn WSJ Treebank were converted to syntactic dependencies via the LTH Constituent-to-Dependency Conversion Tool for Penn-style Treebanks (Pennconverter) (Johansson and Nugues, 2007). The resulting dependency bank was then merged with the Nombank (Meyers et al., 2004) and Propbank (Palmer et al., 2005) corpora. Named entity information from the BBN Entity Type corpus was also integrated into the CoNLL-08 data. Our shallow representation is based on the Pennconverter dependencies. The deep representation is derived from the merged Nombank, Propbank and syntactic dependencies in a process similar to the graph completion 218 algorithm outlined in (Bohnet et al., 2010) (see Section 2.2 for differences). 2.1 Shallow representation The shallow data consists of unordered syntact"
W11-2832,W02-2103,0,0.249839,"aning Similarity. This report presents the evaluation results, along with descriptions of the SR Task Tracks and evaluation methods. For descriptions of the participating systems, see the separate system reports in this volume, immediately following this results report. 1 Introduction and Overview Many different surface realisers have been developed over the past three decades or so. While symbolic realisers dominated for much of this period, the past decade has seen the development of many different types of statistical surface realisers. A significant subset of statistical realisation work (Langkilde, 2002; Callaway, 2003; Nakanishi et al., 2005; Zhong and Stent, 2005; Cahill and van Genabith, 2006; White and Rajkumar, 2009) has produced results for regenerating the Penn Treebank (PTB) (Marcus et al., 1995). The basic approach in all this work was to remove information from the Penn Treebank parses (the word strings themselves as well as some of the parse information), and then 217 Dominic Espinosa2 Eric Kow1 Department of Linguistics Ohio State University Columbus, OH, 43210, US {espinosa,mwhite}@ling.osu.edu 2 Amanda Stent AT&T Labs Research Florham Park, NJ 07932, US stent@research.att.com c"
W11-2832,W04-2705,0,0.123598,"ic represention of the sentence. The deep(er) input type is intended to be closer to a semantic, more abstract, representation of the meaning of the sentence. The input representations were created by postprocessing the CoNLL 2008 Shared Task data (Surdeanu et al., 2008). For the preparation of the CoNLL-08 Shared task data, selected sections of the Penn WSJ Treebank were converted to syntactic dependencies via the LTH Constituent-to-Dependency Conversion Tool for Penn-style Treebanks (Pennconverter) (Johansson and Nugues, 2007). The resulting dependency bank was then merged with the Nombank (Meyers et al., 2004) and Propbank (Palmer et al., 2005) corpora. Named entity information from the BBN Entity Type corpus was also integrated into the CoNLL-08 data. Our shallow representation is based on the Pennconverter dependencies. The deep representation is derived from the merged Nombank, Propbank and syntactic dependencies in a process similar to the graph completion 218 algorithm outlined in (Bohnet et al., 2010) (see Section 2.2 for differences). 2.1 Shallow representation The shallow data consists of unordered syntactic dependency trees. Each word and punctuation marker from the original sentence is re"
W11-2832,W05-1510,0,0.0626047,"sents the evaluation results, along with descriptions of the SR Task Tracks and evaluation methods. For descriptions of the participating systems, see the separate system reports in this volume, immediately following this results report. 1 Introduction and Overview Many different surface realisers have been developed over the past three decades or so. While symbolic realisers dominated for much of this period, the past decade has seen the development of many different types of statistical surface realisers. A significant subset of statistical realisation work (Langkilde, 2002; Callaway, 2003; Nakanishi et al., 2005; Zhong and Stent, 2005; Cahill and van Genabith, 2006; White and Rajkumar, 2009) has produced results for regenerating the Penn Treebank (PTB) (Marcus et al., 1995). The basic approach in all this work was to remove information from the Penn Treebank parses (the word strings themselves as well as some of the parse information), and then 217 Dominic Espinosa2 Eric Kow1 Department of Linguistics Ohio State University Columbus, OH, 43210, US {espinosa,mwhite}@ling.osu.edu 2 Amanda Stent AT&T Labs Research Florham Park, NJ 07932, US stent@research.att.com convert and use these underspecified repr"
W11-2832,J05-1004,0,0.0968466,"e deep(er) input type is intended to be closer to a semantic, more abstract, representation of the meaning of the sentence. The input representations were created by postprocessing the CoNLL 2008 Shared Task data (Surdeanu et al., 2008). For the preparation of the CoNLL-08 Shared task data, selected sections of the Penn WSJ Treebank were converted to syntactic dependencies via the LTH Constituent-to-Dependency Conversion Tool for Penn-style Treebanks (Pennconverter) (Johansson and Nugues, 2007). The resulting dependency bank was then merged with the Nombank (Meyers et al., 2004) and Propbank (Palmer et al., 2005) corpora. Named entity information from the BBN Entity Type corpus was also integrated into the CoNLL-08 data. Our shallow representation is based on the Pennconverter dependencies. The deep representation is derived from the merged Nombank, Propbank and syntactic dependencies in a process similar to the graph completion 218 algorithm outlined in (Bohnet et al., 2010) (see Section 2.2 for differences). 2.1 Shallow representation The shallow data consists of unordered syntactic dependency trees. Each word and punctuation marker from the original sentence is represented as a node in a syntactic"
W11-2832,P02-1040,0,0.0951709,"sal shifts are counted as one edit. For each metric, we calculated system-level scores, the mean of the sentence-level scores and weighted n-best scores (described below). Text normalisation: Output texts were normalised by lower-casing all tokens, removing any extraneous white space characters and ensuring consistent treatment of ampersands. 4 http://www.itl.nist.gov/iad/mig/tests/mt/doc /ngram-study.pdf 5 http://www.itl.nist.gov/iad/mig/tests/mt/2009/ 6 http://www.cs.cmu.edu/ alavie/METEOR/ 3 7 http://www.itl.nist.gov/iad/mig/tests/mt/2009/ http://www.umiacs.umd.edu/ snover/terp/ 3 1. BLEU (Papineni et al., 2002): geometric mean of 1- to 4-gram precision with a brevity penalty; recent 220 N-best, ranked system outputs: Ranked 5-best outputs were scored using a weighted average of the sentence-level scores for each metric, with these sentence-level weighted sums averaged across all outputs. The weight wi assigned to the ith system output was in inverse proportion to its rank ri (K = 5): wi = PKK−ri +1 j=1 K−rj +1 Missing outputs: Missing outputs were scored as zero (one for TER); in the n-best evaluation, missing or duplicate outputs were scored as 0 (1 for TER). Since coverage was high for all systems"
W11-2832,2006.amta-papers.25,0,0.0358882,"hus not included in the SR Task data) due to difficulties in merging the various data sets (e.g. Section 23 has 17 fewer sentences). 3 Automatic Evaluations We computed scores using the following wellknown automatic evaluation metrics: implementations use smoothing to allow sentencelevel scores to be computed. 2. NIST:4,5 n-gram similarity weighted in favour of less frequent n-grams which are taken to be more informative. 3. METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2011):6 lexical similarity based on exact, stem, synonym, and paraphrase matches between words and phrases. 4. TER (Snover et al., 2006):7 a length-normalized edit distance metric where phrasal shifts are counted as one edit. For each metric, we calculated system-level scores, the mean of the sentence-level scores and weighted n-best scores (described below). Text normalisation: Output texts were normalised by lower-casing all tokens, removing any extraneous white space characters and ensuring consistent treatment of ampersands. 4 http://www.itl.nist.gov/iad/mig/tests/mt/doc /ngram-study.pdf 5 http://www.itl.nist.gov/iad/mig/tests/mt/2009/ 6 http://www.cs.cmu.edu/ alavie/METEOR/ 3 7 http://www.itl.nist.gov/iad/mig/tests/mt/200"
W11-2832,W08-2121,0,0.0614246,"Missing"
W11-2832,D09-1043,1,\N,Missing
W11-2832,H94-1020,0,\N,Missing
W12-1525,W04-2705,0,\N,Missing
W12-1525,C10-1012,1,\N,Missing
W12-1525,W08-2121,0,\N,Missing
W12-1525,W11-2832,1,\N,Missing
W12-1525,W07-2416,0,\N,Missing
W12-1525,J05-1004,0,\N,Missing
W12-1525,W12-1528,1,\N,Missing
W12-1525,W04-3250,0,\N,Missing
W12-1525,kow-belz-2012-lg,1,\N,Missing
W12-1525,W12-1527,1,\N,Missing
W15-2816,D13-1128,0,0.656294,"-created descriptions of the whole image (Rashtchian et al., 2010). We collected additional annotations for the images listing, for each object pair, a set of prepositions that have been selected by human annotators as correctly describing the spatial relationship between the given object pair (Section 2.3). We did this in separate experiments for both English and French. The overall aim is to create models for the mapping from image, bounding boxes and labels to spatial prepositions as indicated in Figure 1. We compare two approaches to modelling the mapping. One is taken from previous work (Elliott and Keller, 2013) and defines manually constructed rules to implement the mapping from image geThe context for the work we report here is the automatic description of spatial relationships between pairs of objects in images. We investigate the task of selecting prepositions for such spatial relationships. We describe the two datasets of object pairs and prepositions we have created for English and French, and report results for predicting prepositions for object pairs in both of these languages, using two methods: (a) an existing approach which manually fixes the mapping from geometrical features to prepositio"
W15-2816,D10-1040,0,0.03308,"n.fr Abstract an important subtask in image description, but it is rarely addressed as a subtask in its own right. If an image description method produces spatial prepositions it tends to be as a side-effect of the overall method (Mitchell et al., 2012; Kulkarni et al., 2013), or else relationships are not between objects, but e.g. between objects and the ‘scene’ (Yang et al., 2011). An example of preposition selection as a separate subtask is Elliott & Keller (2013) where the mapping is rule-based. Spatial relations also play a role in referring expression generation (Viethen and Dale, 2008; Golland et al., 2010) where the problem is, however, often framed as a content selection problem from known abstract representations of the objects and scene, and the aim is to enable unique identification of the object referred to. Our main data source is a corpus of images (Everingham et al., 2010) in which objects have been annotated with rectangular bounding boxes and object class labels. For a subset of 1,000 of the images we also have five human-created descriptions of the whole image (Rashtchian et al., 2010). We collected additional annotations for the images listing, for each object pair, a set of preposi"
W15-2816,E12-1076,0,0.111514,"ng, Engineering and Maths Communications & Computer Engineering University of Brighton University of Malta Lewes Road, Brighton BN2 4GJ, UK Msida MSD 2080, Malta a.s.belz@brighton.ac.uk adrian.muscat@um.edu.mt Maxime Aberton and Sami Benjelloun INSA Rouen Avenue de l’Universit´e ´ 76801 Saint-Etienne-du-Rouvray Cedex, France {maxime.aberton,sami.benjelloun}@insa-rouen.fr Abstract an important subtask in image description, but it is rarely addressed as a subtask in its own right. If an image description method produces spatial prepositions it tends to be as a side-effect of the overall method (Mitchell et al., 2012; Kulkarni et al., 2013), or else relationships are not between objects, but e.g. between objects and the ‘scene’ (Yang et al., 2011). An example of preposition selection as a separate subtask is Elliott & Keller (2013) where the mapping is rule-based. Spatial relations also play a role in referring expression generation (Viethen and Dale, 2008; Golland et al., 2010) where the problem is, however, often framed as a content selection problem from known abstract representations of the objects and scene, and the aim is to enable unique identification of the object referred to. Our main data sourc"
W15-2816,W10-0721,0,0.220488,"-based. Spatial relations also play a role in referring expression generation (Viethen and Dale, 2008; Golland et al., 2010) where the problem is, however, often framed as a content selection problem from known abstract representations of the objects and scene, and the aim is to enable unique identification of the object referred to. Our main data source is a corpus of images (Everingham et al., 2010) in which objects have been annotated with rectangular bounding boxes and object class labels. For a subset of 1,000 of the images we also have five human-created descriptions of the whole image (Rashtchian et al., 2010). We collected additional annotations for the images listing, for each object pair, a set of prepositions that have been selected by human annotators as correctly describing the spatial relationship between the given object pair (Section 2.3). We did this in separate experiments for both English and French. The overall aim is to create models for the mapping from image, bounding boxes and labels to spatial prepositions as indicated in Figure 1. We compare two approaches to modelling the mapping. One is taken from previous work (Elliott and Keller, 2013) and defines manually constructed rules t"
W15-2816,W08-1109,0,0.0330789,"mi.benjelloun}@insa-rouen.fr Abstract an important subtask in image description, but it is rarely addressed as a subtask in its own right. If an image description method produces spatial prepositions it tends to be as a side-effect of the overall method (Mitchell et al., 2012; Kulkarni et al., 2013), or else relationships are not between objects, but e.g. between objects and the ‘scene’ (Yang et al., 2011). An example of preposition selection as a separate subtask is Elliott & Keller (2013) where the mapping is rule-based. Spatial relations also play a role in referring expression generation (Viethen and Dale, 2008; Golland et al., 2010) where the problem is, however, often framed as a content selection problem from known abstract representations of the objects and scene, and the aim is to enable unique identification of the object referred to. Our main data source is a corpus of images (Everingham et al., 2010) in which objects have been annotated with rectangular bounding boxes and object class labels. For a subset of 1,000 of the images we also have five human-created descriptions of the whole image (Rashtchian et al., 2010). We collected additional annotations for the images listing, for each object"
W15-2816,D11-1041,0,\N,Missing
W15-2816,W10-0707,0,\N,Missing
W15-4717,E12-1076,0,0.0780813,"ions that can be used to describe the spatial relationships between pairs of objects in images. This is not the same as inferring the actual 3-D realworld spatial relationships between objects, but has some similarities with that task. This is an important subtask in automatic image description (which is important not just for assistive technology, but also for applications such as text-based querying of image databases), but it is rarely addressed as a subtask in its own right. If an image description method produces spatial prepositions it tends to be as a side-effect of the overall method (Mitchell et al., 2012; Kulkarni et al., 2013), or else relationships are not between objects, but e.g. between objects and the ‘scene’ (Yang et al., 2011). An example of preposition selection as a separate sub-task is Elliott & Keller (2013) where the mapping is hard-wired manually. Our main data source is a corpus of images (Everingham et al., 2010) in which objects have been VOC’08 The PASCAL VOC 2008 Shared Task Competition (VOC’08) data consists of 8,776 images and 20,739 objects in 20 object classes (Everingham et al., 2010). In each image, every object belonging to one of the 20 VOC’08 object classes is anno"
W15-4717,W10-0721,0,0.103343,"Competition (VOC’08) data consists of 8,776 images and 20,739 objects in 20 object classes (Everingham et al., 2010). In each image, every object belonging to one of the 20 VOC’08 object classes is annotated with its object class label and a bounding box (among other annotations): 1. class: one of: aeroplane, bird, bicycle, boat, bottle, bus, car, cat, chair, cow, dining table, dog, horse, motorbike, person, potted plant, sheep, sofa, train, tv/monitor. 2. bounding box: an axis-aligned bounding box surrounding the extent of the object visible in the image. 2.2 VOC’08 1K Using Mechanical Turk, Rashtchian et al. (2010) collected five descriptions each for 1,000 VOC’08 images selected randomly but ensuring there were Proceedings of the 15th European Workshop on Natural Language Generation (ENLG), pages 100–104, c Brighton, September 2015. 2015 Association for Computational Linguistics 100 −→ beside(person(Obj1 ), person(Obj2 )); beside(person(Obj2 ), dog(Obj3 )); in front of(dog(Obj3 ), person(Obj1 )) Figure 1: Image from PASCAL VOC 2008 with annotations, and prepositions representing spatial relationships (objects numbered in descending order of size of area of bounding box). dog car chair horse cat bird bi"
W15-4717,D11-1041,0,\N,Missing
W15-4717,W10-0707,0,\N,Missing
W15-4717,D13-1128,0,\N,Missing
W16-3209,D15-1022,0,0.0254336,"es models (F 0 and F 1 are the language features, and F 2..F 8 are the vision features). Some commonalities emerge, e.g. F 4, F 7 and F 8 are high-performing features that tend to be selected early, while F 6 tends to be selected late. In all three cases, greedy feature selection reveals a maximum (highlighted in bold) before the complete set of features is reached which outperforms results achieved with all features, by a margin of between 3 and just over 7 percentage points. The highest Accuracy achieved (53.25) is lower than accuracy rates reported in other preposition prediction research (Ramisa et al., 2015); however that work used different datasets and results varied widely between them. 5 Discussion Through investigating the set of prepositions, the type of learning method, and the set of features used, we were able to improve previous best Accuracy results from 46.2 to 50.2 by removing synonyms and very low frequency prepositions from the annotations. Two new learning methods, SVMs and decision-trees, did not in themselves result in improved scores. Finally, a simple approach to feature set optimisation, greedy LASSO feature selection, further improved the best Accuracy score from 50.2 to 53."
W16-3209,W10-0721,0,0.031642,"nnotations, alongside the kind of descriptions we aim to generate: each describes the spatial relationship between two of the objects in the image in simple terms focused around a preposition. Data Our starting point is the data set we adapted previously (Belz et al., 2015) from the VOC’08 data (Everingham et al., 2010) by additionally annotating images with prepositions which describe the spatial relationships between the annotated objects in the image. We previously used a set of 38 prepositions which were obtained in the following fashion: (a) the (complete) image descriptions collected by Rashtchian et al. (2010) for 1,000 VOC’08 (Everingham et al., 2010) images (five for each image) were parsed with the Stanford Parser version 3.5.21 with the PCFG model, (b) the nmod:prep prepositional modifier relations were extracted automatically, and (c) the non-spatial ones were removed manually. While this provided a nonarbitrary way of selecting a set of prepositions for the annotation task, it contained a large number of synonyms and near-synonyms (e.g. in, within, inside), which appeared to make the learning task harder (see also discussion in Section 4.2 below). Using as a basis the frequencies and synonym"
W16-3914,baccianella-etal-2010-sentiwordnet,0,0.0203569,"Some of the sentiment analysers, despite not being designed for this task, did reasonably well at it; the four best ones were the following: EmoticonsDS: Uses a large sentiment-scored word list based on the co-occurrence of each token with emoticons in a corpus of over 1.5 billion messages (Hannak et al., 2012). SenticNet: A semantic and affective resource for concept-level sentiment analysis, modelling the polarities of common-sense concepts and relations between them (Cambria et al., 2014). SentiWordNet: Lexical SA tool based on WordNet using polarity scores associated with WordNet synsets (Baccianella et al., 2010). AFINN: Twitter based sentiment lexicon providing emotion ratings for words (Nielsen, 2011). Table 4 shows the results for these four tools. Interestingly, there is no overlap with the three tools that did best at the standard SA task (Table 2), in fact those three methods were the three worst ones at this task. 9 The Unified Medical Language System (UMLS) “integrates and distributes key terminology, classification and coding standards, and associated resources” (https://www.nlm.nih.gov/research/umls/). 99 Classifier Multinomial NB Logistic Regression SVM Both classes R P F1 0.847 0.852 0.849"
W16-3914,P12-3005,0,0.0380886,"ge Identification 6 In order to be able to analyse tweets in a meaningful way, moreover using text analysis tools trained for English, it is important that we can reliably filter out non-English tweets. Twitter tags tweets for language, so part of our first set of experiments was to test the reliability of the Twitter language tags, as well as to see how it compared against existing language identification tools. For the latter we chose the three tools that were, combined as an ensemble method, identified by Lui & Baldwin (2014) as the best language identification tools for tweets: langid.py (Lui and Baldwin, 2012), LangDetect,6 and CLD2.7 We randomly selected a subset of 300 tweets (test set A) and manually annotated each for English vs. other. Table 5 shows results for each of the methods tested, as well as for combining Sentiment Analysis Following the language identification experiments, we filtered out the non-English tweets using langid.py, which left us with 93,697 tweets and an average of 46.87 tweets per drug name. In the experiments in this section, we aim to predict the overall sentiment of an (English) tweet, independently of whether a drug effect is being reported. For this purpose we selec"
W16-3914,W14-1303,0,0.10522,"for the English class). langid.py, CLD2 and LangDetect in majority voting. langid.py outperforms the others, including Twitter, slightly on the raw tweets (see end of Section 4). First cleaning/normalising tweets led to a slight improvement for CLD2, and a slight deterioration for languid.py; additionally parsing hashtags led to slight improvement for languid.py, and slight deteriorations for CLD2 and LangDetect. The final best result is an F1-Score of 0.94 for langid.py. This confirms (for this domain) previous results that good language identification tools outperform Twitter language tags (Lui and Baldwin, 2014). Language Identification 6 In order to be able to analyse tweets in a meaningful way, moreover using text analysis tools trained for English, it is important that we can reliably filter out non-English tweets. Twitter tags tweets for language, so part of our first set of experiments was to test the reliability of the Twitter language tags, as well as to see how it compared against existing language identification tools. For the latter we chose the three tools that were, combined as an ensemble method, identified by Lui & Baldwin (2014) as the best language identification tools for tweets: lan"
W16-6639,W15-2816,1,0.80975,"2015) is closely related to our work and also uses geometric and label features to predict prepositions. 2 Data The new data set we have created for the experiments in this paper is a set of photographs in which objects in 20 classes are annotated with bounding boxes and class labels, and each object pair with prepositions that describe the spatial relationship between the objects. The data was derived from the VOC’08 data (Everingham et al., 2010) by selecting images with 2 or 3 bounding boxes, and adding the preposition annotations. The data has twice as many images as in our previous work (Belz et al., 2015), and a smaller set of prepositions (see below). 2.1 Annotation For each object pair in each image, and for both orderings of the object labels, Ls , Lo and Lo , Ls , three French native speakers selected (i) the best preposition for the given pair (free text entry), and (ii) the possible prepositions for the given pair (from a given list) that accurately described the spatial relationship between the two objects in the pair. As a result, we have a total of 4,140 object pair annotations which fold out into 9,278 training instances. Proceedings of The 9th International Natural Language Generati"
W16-6639,D13-1128,0,0.0213614,"applications in automatic image captioning and assistive technologies. An important aspect, and long-standing research topic, is to identify the entities, or objects, in images. However, a good image description will also say something about how entities relate to each other, not just list them. Spatial relations, and prepositions to express them, are particularly important in this context, but until very recently there had been no research directly aimed at this subtask, although 237 some research came close (Mitchell et al., 2012; Kulkarni et al., 2013; Yang et al., 2011). Elliott & Keller (Elliott and Keller, 2013) did address the subtask, but with hardwired rules for just eight preposition. The work reported by Ramisa et al. (2015) is closely related to our work and also uses geometric and label features to predict prepositions. 2 Data The new data set we have created for the experiments in this paper is a set of photographs in which objects in 20 classes are annotated with bounding boxes and class labels, and each object pair with prepositions that describe the spatial relationship between the objects. The data was derived from the VOC’08 data (Everingham et al., 2010) by selecting images with 2 or 3"
W16-6639,E12-1076,0,0.0727398,"Missing"
W16-6639,D15-1022,0,0.0140543,"is to identify the entities, or objects, in images. However, a good image description will also say something about how entities relate to each other, not just list them. Spatial relations, and prepositions to express them, are particularly important in this context, but until very recently there had been no research directly aimed at this subtask, although 237 some research came close (Mitchell et al., 2012; Kulkarni et al., 2013; Yang et al., 2011). Elliott & Keller (Elliott and Keller, 2013) did address the subtask, but with hardwired rules for just eight preposition. The work reported by Ramisa et al. (2015) is closely related to our work and also uses geometric and label features to predict prepositions. 2 Data The new data set we have created for the experiments in this paper is a set of photographs in which objects in 20 classes are annotated with bounding boxes and class labels, and each object pair with prepositions that describe the spatial relationship between the objects. The data was derived from the VOC’08 data (Everingham et al., 2010) by selecting images with 2 or 3 bounding boxes, and adding the preposition annotations. The data has twice as many images as in our previous work (Belz"
W16-6639,D11-1041,0,0.0842772,"Missing"
W17-3517,W11-2832,1,0.948241,"Missing"
W17-3517,kow-belz-2012-lg,1,0.830242,"em, synonym and paraphrase matches. We will apply text normalization before scoring. For n-best ranked system outputs, we will compute a single score for all outputs by computing the weighted sum of their individual scores, with a weight assigned to an output in inverse proportion to its rank. For a subset of the test data we may obtain additional alternative realizations via Mechanical Turk for use in the automatic evaluations. 8 http://universaldependencies.org/ format.html For the human-assessed evaluation, we are planning to use a type of evaluation that is based on preference judgements (Kow and Belz, 2012, p.4035), using the existing evaluation interface described in Kow and Belz’s paper. As in SR’11, we plan to use students in the third year of an undergraduate degree, from Cambridge, Oxford and Edinburgh. Two candidate outputs9 will be presented to the evaluators, who will assess them for Clarity, Fluency and Meaning Similarity. For each criterion, they will be asked not only to state which system output they prefer, but also how strong is their preference. We plan to organize a workshop collocated with ACL ’18, COLING ’18, or EMNLP ’18 at which the results of the SR’18 will be presented. To"
W17-3517,S17-2090,0,0.0229781,"rained PoS and morphological information will be removed from the input trees. The first shared task on Surface Realization was carried out in 2011 with a similar setup, with a focus on English. We think that it is time for relaunching such a shared task effort in view of the arrival of Universal Dependencies annotated treebanks for a large number of languages on the one hand, and the increasing dominance of Deep Learning, which proved to be a game changer for NLP, on the other hand. 1 Introduction In 2017, three shared tasks on Natural Language Generation (NLG) take place: Task 9 of SemEval (May and Priyadarshi, 2017), WebNLG1 and E2E2 . The first starts from Abstract Meaning Representations (AMRs), the second from RDF triples, and the third from dialog act-based Meaning Representations (MRs) respectively. With these efforts, the focus is put on “real-life” generation, since the respective inputs come from existing analyzers (for AMRs) or existing databases (for RDF triples and 1 http://talc1.loria.fr/webnlg/stories/ challenge.html 2 http://www.macs.hw.ac.uk/ InteractionLab/E2E/ MRs). This shows that the research on NLG is on the right track and that there is an interest in large scale “deep” NLG. However,"
W17-3517,W04-2705,0,0.187846,"Missing"
W17-3517,L16-1262,0,0.0611377,"Missing"
W17-3517,J05-1004,0,0.0504225,"their lemmas or stems, depending on the availability of lemmatization and stemming tools, respectively. For the Deep Track, additionally: 3. functional prepositions and conjunctions that can be inferred from other lexical units or from the syntactic structure will be removed, as e.g., “by” and “of” in Figure 2; 4. determiners and auxiliaries will be replaced (when needed) by attribute/value pairs, as, e.g., “Definiteness” and “Aspect” in Figure 3; 5. edge labels will be generalized into predicate argument labels, following the PropBank/NomBank edge label nomenclature (Meyers and et al., 2004; Palmer et al., 2005), with three main differences: (i) there will be no special label for external arguments (i.e., no “A0”), which means that all first arguments of a predicate will be mapped to A1, and the rest of the arguments will be labeled starting from A2; (ii) all modifier edges “AM-...” will be generalized to “AM”; (iii) there will be a coordinative relation; and (iv) any relation that does not fall into the first three cases will be assigned an underspecified edge label. 6. morphological information coming from the syntactic structure or from agreements will be removed; in other words, only “semantic” i"
W18-3601,P11-2040,1,0.888439,"Missing"
W18-3601,W11-2832,1,0.629164,"e languages may also work for others. The SR’18 task is to generate sentences from structures at the level of abstraction of outputs in state-of-the-art parsing, encouraging participants to explore the extent to which neural network parsing algorithms can be reversed for generation. SR’18 also addresses questions about just how suitable and useful the notion of universal dependencies—which is in the process of becoming the dominant linguistic formalism across a wide range of NLP applications, parsing in particular—is for NLG. SR’18 follows the SR’11 pilot surface realisation task for English (Belz et al., 2011) which was part of Generation Challenges 2011 (GenChal’11), the fifth round of shared-task evaluation competitions (STECs) involving the language generation tasks. Outside of the SR tasks, just three ‘deep’ NLG shared tasks focusing on language generation from abstract semantic representations have been organised to date: WebNLG3 (Gardent et al., 2017), SemEval Task 94 (May and Priyadarshi, 2017), and E2E5 (Novikova et al., 2017). What is more, these 2 See the recent parsing shared task based on UDs (Nivre and de Marneffe et al., 2016): http:// universaldependencies.org/conll17/. 3 http://talc"
W18-3601,W17-4755,1,0.838761,"gy) of 100 outputs, of which 20 are used solely for quality assurance (QA) (i.e. do not count towards system scores): (i) some are repeated as are, (ii) some are repeated in a ‘damaged’ version and (iii) some are replaced by their corresponding reference texts. In each case, a minimum threshold has to be reached for the HIT to be accepted: for (i), scores must be similar enough, for (ii) the score for the damaged version must be worse, and for (iii) the score for the reference text must be high. For full details of how these additional texts are created and thresholds applied, please refer to Bojar et al. (2017a). Below we report QA figures for the MTurk evaluations (Section 3.2.1). Code: We were able to reuse, with minor adaptations, the code produced for the WMT’17 evaluations.10 3.2.2 Google Data Compute Evaluation In order to cover more languages, and to enable comparison between crowdsourced and expert evaluation, we also conducted human evaluations using Google’s internal ‘Data Compute’ system evaluation service, where experienced evaluators carefully assess each system output. We used an interface that matches the WMT’17 interface above, as closely as was possible within the constraints of th"
W18-3601,P17-1017,0,0.0630018,"he notion of universal dependencies—which is in the process of becoming the dominant linguistic formalism across a wide range of NLP applications, parsing in particular—is for NLG. SR’18 follows the SR’11 pilot surface realisation task for English (Belz et al., 2011) which was part of Generation Challenges 2011 (GenChal’11), the fifth round of shared-task evaluation competitions (STECs) involving the language generation tasks. Outside of the SR tasks, just three ‘deep’ NLG shared tasks focusing on language generation from abstract semantic representations have been organised to date: WebNLG3 (Gardent et al., 2017), SemEval Task 94 (May and Priyadarshi, 2017), and E2E5 (Novikova et al., 2017). What is more, these 2 See the recent parsing shared task based on UDs (Nivre and de Marneffe et al., 2016): http:// universaldependencies.org/conll17/. 3 http://talc1.loria.fr/webnlg/stories/ challenge.html 4 http://alt.qcri.org/semeval2017/ task9/ 5 http://www.macs.hw.ac.uk/ InteractionLab/E2E/ tasks have only been offered for English. As in SR’11, the Multilingual Surface Realisation shared task (SR’18) comprises two tracks with different levels of difficulty: Shallow Track: This track starts from genuine UD str"
W18-3601,D14-1020,1,0.831205,"Missing"
W18-3601,S17-2090,0,0.0582517,"h is in the process of becoming the dominant linguistic formalism across a wide range of NLP applications, parsing in particular—is for NLG. SR’18 follows the SR’11 pilot surface realisation task for English (Belz et al., 2011) which was part of Generation Challenges 2011 (GenChal’11), the fifth round of shared-task evaluation competitions (STECs) involving the language generation tasks. Outside of the SR tasks, just three ‘deep’ NLG shared tasks focusing on language generation from abstract semantic representations have been organised to date: WebNLG3 (Gardent et al., 2017), SemEval Task 94 (May and Priyadarshi, 2017), and E2E5 (Novikova et al., 2017). What is more, these 2 See the recent parsing shared task based on UDs (Nivre and de Marneffe et al., 2016): http:// universaldependencies.org/conll17/. 3 http://talc1.loria.fr/webnlg/stories/ challenge.html 4 http://alt.qcri.org/semeval2017/ task9/ 5 http://www.macs.hw.ac.uk/ InteractionLab/E2E/ tasks have only been offered for English. As in SR’11, the Multilingual Surface Realisation shared task (SR’18) comprises two tracks with different levels of difficulty: Shallow Track: This track starts from genuine UD structures in which word order information has b"
W18-3601,W04-2705,0,0.458005,"Missing"
W18-3601,L16-1262,0,0.0728681,"Missing"
W18-3601,W17-5525,0,0.0981398,"Missing"
W18-3601,J05-1004,0,0.102132,"in CoNLL-U format, with no meta-information.7 Figures 1, 2 and 3 show 6 universaldependencies.org 7 http://universaldependencies.org/ a sample original UD annotation for English, and the corresponding shallow and deep input structures derived from it. To create inputs to the Shallow Track, the UD structures were processed as follows: 1. Word order information was removed by randomised scrambling; 2. Words were replaced by their lemmas. For the Deep Track, the following steps were additionally carried out: 3. Edge labels were generalised into predicate/argument labels, in the PropBank/NomBank (Palmer et al., 2005; Meyers et al., 2004) fashion. That is, the syntactic relations were mapped to core (A1, A2, etc.) and non-core (AM) labels, applying the following rules: (i) the first argument is always labeled A1 (i.e. there is no external argument A0); (ii) in order to maintain the tree structure and account for some cases of shared arguments, there can be inverted argument relations; (iii) all modifier edges are assigned the same generic label AM; (iv) there is a coordinating relation; see the inventory of relations in Table 1. 4. Functional prepositions and conjunctions in argument position (i.e. prepos"
W18-3601,P02-1040,0,0.102984,"nt Example fall→ the ball the ball→ fall fall→ last night fall→ [and] bounce Tower→ Eiffel N/A Table 1: Deep labels. train dev test ar 6,016 897 676 cs 66,485 9,016 9,876 en 12,375 1,978 2,061 es 14,289 1,651 1,719 fi 12,030 1,336 1,525 fr 14,529 1,473 416 it 12,796 562 480 nl 12,318 720 685 pt 8,325 559 476 ru 48,119 6,441 6,366 Table 2: SR’18 dataset sizes for training, development and test sets. 3 3.1 Evaluation Methods Automatic methods We used BLEU, NIST, and inverse normalised character-based string-edit distance (referred to as DIST, for short, below) to assess submitted systems. BLEU (Papineni et al., 2002) is a precision metric that computes the geometric mean of the n-gram precisions between generated text and reference texts and adds a brevity penalty for shorter sentences. We use the smoothed version and report results for n = 4. NIST9 is a related n-gram similarity metric weighted in favour of less frequent n-grams which are taken to be more informative. Inverse, normalised, character-based string-edit distance (DIST in the tables below) starts by computing the minimum number of character inserts, deletes and substitutions (all at cost 1) required to turn the system output into the (single)"
W18-6516,D13-1128,0,0.1238,"Pixel-level depth values were computed via the method described in (Birmingham et al., 2018), which uses depth maps computed with monoDepth7 (Godard et al., 2017) . 5 Inter-AA/intra-AA for English and additional dataset statistics will be added to the project home on Github. 6 GloVe is a count-based method for creating distributed word representations. 7 https://github.com/mrharicot/monodepth 7 Related Datasets A number of datasets are available that incorporate annotations representing relations between objects 143 Name Authors Task Categories of relations Visual Phrases Sadeghi et al. 2011 Elliott and Keller, 2013 Johnson et al. 2015 Ramisa et al. 2015 Lu et al. 2016 Krishna et al. 2016 Phrase Classification Image Description action, verbal, spatial Annotated relations 1,796 action, verbal, spatial 5748 Image Retrieval action, verbal, spatial, preposition spatial, preposition 112,707 78,317 33,262 action, verbal, spatial, preposition, comparative action, verbal, spatial, preposition, comparative 37,993 5K Visual and Linguistic Treebank Scene Graphs ViSen VRD Visual Genome Preposition Prediction Relation, Phrase Prediction Image Understanding Images 2,769 341 / 2424 1.5M 5K 108K Table 2: Overview of rel"
W18-6516,W10-0721,0,0.099653,"Missing"
W18-6516,Q14-1006,0,0.0539286,"s see in text. in images. Types of relationships that have been annotated include actions (e.g. person kicks ball), other verbal relations (person wears shirt), spatial relations (person on horse), and comparative relations (one car bigger than another). In this section, we provide a brief overview of available datasets with relation annotations, in terms of their stated purpose (application task), the types of relations included, the range of spatial prepositions included, as well as size and other properties of the dataset. Table 2 has a summary of the datasets. et al., 2014) and Flickr30k (Young et al., 2014). Prepositions covered include all those extracted from the image descriptions including non-spatial ones. By far not all descriptions contain prepositions so not all images have spatial relation annotations; the task addressed is preposition prediction, not spatial relation prediction. Visual Relationships Dataset (VRD) (Lu et al., 2016) contains 5,000 images, 100 object categories, 6,672 unique relationships, and 24.25 relations per object category. Scant information is available about how the dataset was created other than that relations broadly fit into the categories action, verbal, spati"
W18-6516,W15-4717,1,0.876415,"Missing"
W18-6516,D14-1162,0,0.0811978,"utes the weighted mean of individual per-preposition precision scores. The individual per-preposition precision for a given system and a given preposition p is the proportion of times that p is among the corresponding human-selected prepositions out of all the times that p is returned as the top-ranked preposition by the system. Spatially Relevant Features Table 1 provides an overview of the 19 features included in SpatialVOC2K: F0, F1, F15 and F16 are language features. F0 is the class label of the first object, F1 of the second (e.g. person). F15 and F16 are GloVe word vectors of length 50 (Pennington et al., 2014) for the object labels.6 F2– F14 are visual features measuring various aspects of the geometries of the image and two bounding boxes (BBs). Most features express a property of just one of the objects, but F4–F9 express a property of both objects jointly, e.g. F6 is the normalized BB overlap. F17 and F18 are the average pixel-level depth value within the BB of Objs and Objo , respectively. Pixel-level depth values were computed via the method described in (Birmingham et al., 2018), which uses depth maps computed with monoDepth7 (Godard et al., 2017) . 5 Inter-AA/intra-AA for English and additio"
W18-6516,D15-1022,0,0.0171231,"he method described in (Birmingham et al., 2018), which uses depth maps computed with monoDepth7 (Godard et al., 2017) . 5 Inter-AA/intra-AA for English and additional dataset statistics will be added to the project home on Github. 6 GloVe is a count-based method for creating distributed word representations. 7 https://github.com/mrharicot/monodepth 7 Related Datasets A number of datasets are available that incorporate annotations representing relations between objects 143 Name Authors Task Categories of relations Visual Phrases Sadeghi et al. 2011 Elliott and Keller, 2013 Johnson et al. 2015 Ramisa et al. 2015 Lu et al. 2016 Krishna et al. 2016 Phrase Classification Image Description action, verbal, spatial Annotated relations 1,796 action, verbal, spatial 5748 Image Retrieval action, verbal, spatial, preposition spatial, preposition 112,707 78,317 33,262 action, verbal, spatial, preposition, comparative action, verbal, spatial, preposition, comparative 37,993 5K Visual and Linguistic Treebank Scene Graphs ViSen VRD Visual Genome Preposition Prediction Relation, Phrase Prediction Image Understanding Images 2,769 341 / 2424 1.5M 5K 108K Table 2: Overview of related datasets. For explanation of relat"
W18-6517,E12-1076,0,0.0925006,"Missing"
W18-6517,W14-5405,0,0.0307682,"iven image from scratch. Such methods can be said to involve three main steps: (1) identification of type and, optionally, location of objects and background/scene; (2) detection of attributes, relations and activities involving objects from Step 1; and (3) generation of 146 Proceedings of The 11th International Natural Language Generation Conference, pages 146–151, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Association for Computational Linguistics tion in person at a table, depends on the functional relationship between the trajector object, person, and the landmark object, table. Dobnik and Kelleher (2014) derive functional semantic knowledge from corpora and use it to explore the dependency of spatial prepositions on functional knowledge. Regier and Carlson (2001) show that projective spatial terms such as above are grounded in attention processes and vector-sum coding of overall direction, formalising these notions in their attentional vector-sum (AVS) model. The model is shown to predict linguistic acceptability judgments for spatial terms, for a variety of spatial configurations. Results indicate that spatial prepositions require more attention on the image compared to detecting an object,"
W18-6517,D13-1128,0,0.023026,"sum over an area rather than the centre of mass are better predictors. Kelleher et al. (2011) show that object occlusion degrades the performance of models that are based solely on geometric and functional features e.g. in the case of in front of, a projective preposition. Kelleher et al.’s occlusion-enabled regression-based model is shown to outperform Regier and Carlson’s AVS model. a word string from the outputs from Steps 1 and 2. In Step 2, the focus of this paper, systems determine object attributes (Yatskar et al., 2014; Kulkarni et al., 2011), spatial relationships (Yang et al., 2011; Elliott and Keller, 2013), activities (Yatskar et al., 2014; Elliott and Keller, 2013), etc. Identifying the spatial relationships between pairs of objects in images is an important part of Step 2, but overlaps into Step 3 if prepositions are selected directly. Methods that produce spatial prepositions sometimes do so as a side-effect of the overall method (Mitchell et al., 2012; Kulkarni et al., 2013); examples of preposition selection as a separate subtask include Elliott and Keller (2013) who base the mapping from features to spatial relations on manually composed rules, and Ramisa et al. (2015) and our own previou"
W18-6517,D15-1022,0,0.0915112,"et al., 2011; Elliott and Keller, 2013), activities (Yatskar et al., 2014; Elliott and Keller, 2013), etc. Identifying the spatial relationships between pairs of objects in images is an important part of Step 2, but overlaps into Step 3 if prepositions are selected directly. Methods that produce spatial prepositions sometimes do so as a side-effect of the overall method (Mitchell et al., 2012; Kulkarni et al., 2013); examples of preposition selection as a separate subtask include Elliott and Keller (2013) who base the mapping from features to spatial relations on manually composed rules, and Ramisa et al. (2015) and our own previous work (Muscat and Belz, 2017) where the mapping is learnt automatically. Elliott (2014) manually adds 3rd dimension annotations to images (e.g. whether objects are behind other objects). There is a sizable literature on spatial relations and spatial language from cognitive and psycholinguistic perspectives, and the remainder of this section briefly surveys a selection of relevant results. Indications are that whether speakers use spatial relations in scene descriptions and referring expressions depends at least in part on individual preference and the context. E.g. when ge"
W18-6517,W05-1607,0,0.0393077,"expressions depends at least in part on individual preference and the context. E.g. when generating referring expressions, some people prefer not to use spatial relations at all (Viethen and Dale, 2008). Furthermore, speakers tend to make more use of spatial relations in domains unknown to them, whereas they use them comparatively less when the domain is known (Viethen and Dale, 2008). Kelleher and Kruiff (2005) categorise spatial relations as combinations of topological vs. projective, and contrastive vs. relative, the latter being dependent on context. Both studies (Viethen and Dale, 2008; Kelleher and Kruijff, 2005) agree that people are generally less likely to use projective spatial relations like in front of than topological relations like on top of. The former depend on a landmark whereas the latter depend on intersection, overlap and contiguity, which require less cognitive effort to process. For similar reasons, contrastive relations are used more than relative relations (Kelleher and Kruijff, 2005). 3 Data and Features In the research reported here, we use a subset of the French part of the SpatialVOC2K dataset (Belz et al., 2018), referred to as ‘DS-F-Best’ below, for consistency with previous pu"
W18-6517,Q14-1017,0,0.0476732,"ird dimension (depth), except via manual annotations (Elliott, 2014). This is an issue for spatial relation detection, because many spatial relations Related Research Research on associating text with images goes back at least to the 1960s with early work focusing on object/region labelling (Rosenfeld, 1978). Image description proper starts where a summarising description of the whole image is aimed for. Some approaches measure the similarity of a new image with other images for which descriptions exist, and then use one or more of those descriptions to create a description for the new image (Socher et al., 2014; Karpathy and Fei-Fei, 2015; Ordonez et al., 2011). Our focus here is on methods that create a new description for a given image from scratch. Such methods can be said to involve three main steps: (1) identification of type and, optionally, location of objects and background/scene; (2) detection of attributes, relations and activities involving objects from Step 1; and (3) generation of 146 Proceedings of The 11th International Natural Language Generation Conference, pages 146–151, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Association for Computational Linguistics tion in person at"
W18-6517,W08-1109,0,0.0459656,"ally. Elliott (2014) manually adds 3rd dimension annotations to images (e.g. whether objects are behind other objects). There is a sizable literature on spatial relations and spatial language from cognitive and psycholinguistic perspectives, and the remainder of this section briefly surveys a selection of relevant results. Indications are that whether speakers use spatial relations in scene descriptions and referring expressions depends at least in part on individual preference and the context. E.g. when generating referring expressions, some people prefer not to use spatial relations at all (Viethen and Dale, 2008). Furthermore, speakers tend to make more use of spatial relations in domains unknown to them, whereas they use them comparatively less when the domain is known (Viethen and Dale, 2008). Kelleher and Kruiff (2005) categorise spatial relations as combinations of topological vs. projective, and contrastive vs. relative, the latter being dependent on context. Both studies (Viethen and Dale, 2008; Kelleher and Kruijff, 2005) agree that people are generally less likely to use projective spatial relations like in front of than topological relations like on top of. The former depend on a landmark whe"
W18-6517,D11-1041,0,0.105979,"Missing"
W18-6517,S14-1015,0,0.0183839,"e image compared to detecting an object, and geometric features based on the net vector sum over an area rather than the centre of mass are better predictors. Kelleher et al. (2011) show that object occlusion degrades the performance of models that are based solely on geometric and functional features e.g. in the case of in front of, a projective preposition. Kelleher et al.’s occlusion-enabled regression-based model is shown to outperform Regier and Carlson’s AVS model. a word string from the outputs from Steps 1 and 2. In Step 2, the focus of this paper, systems determine object attributes (Yatskar et al., 2014; Kulkarni et al., 2011), spatial relationships (Yang et al., 2011; Elliott and Keller, 2013), activities (Yatskar et al., 2014; Elliott and Keller, 2013), etc. Identifying the spatial relationships between pairs of objects in images is an important part of Step 2, but overlaps into Step 3 if prepositions are selected directly. Methods that produce spatial prepositions sometimes do so as a side-effect of the overall method (Mitchell et al., 2012; Kulkarni et al., 2013); examples of preposition selection as a separate subtask include Elliott and Keller (2013) who base the mapping from features"
W18-6527,W10-4226,1,0.705033,"atically parsed data mentioned above shows. It is conceivable that a future shared task in NLG will involve paired (structured) data and text, plus an automatically created intermediate level of representation comprising underspecified UD (UUD) structures enriched with additional information obtained from the structured data level. This would correspond to three linked tracks (data-to-text, data-to-UUD, and UUD-totext) where one track is the end-to-end task, and the other two tracks are subtasks that can be combined to solve the end-to-end task, similar to the GREC’10 shared task competition (Belz and Kow, 2010). Or it could be argued, perhaps controversially still, that the days of structured linguistic representations in NLG are numbered anyway. The rapid development and spread of highly successful neural approaches to diverse NLG tasks, and the limited success so far of attempts to inject linguistic knowledge directly into neural networks, certainly lends some strength to this point of view. In the meantime, the above tripartite shared-task structure has the potential to accommodate both systems that map directly from data to text without structured representations, and two-component systems with"
W18-6527,W11-2832,1,0.853605,"arious ways, as inputs to surface realisation. Annotation/sentence pairs constitute the training data, and similarity to the original sentences in the treebank is the main measure of success. A lot of this work used inputs derived from the Wall Street Journal corpus with varying amounts of information removed from the parse-tree annotations (Langkilde-Geary, 2002; Nakanishi et al., 2005; Zhong and Stent, 2005; Cahill and van Genabith, 2006; White and Rajkumar, 2009). Because of the variation among inputs, results were not entirely comparable. The first Surface Realisation Shared Task (SR’11) (Belz et al., 2011) was thus conceived with the aim of developing a commonground input representation that would make different systems, for the first time, directly comparable. SR’11 used shallow and deep inputs for its two respective tracks, both derived from CoNLL’08 shared task data, which was in turn derived from the WSJ Corpus by automatically converting the corresponding Penn TreeBank parse trees to dependency structures (Surdeanu et al., 2008). While dependency structures offer a more flexible input structure and statistical systems, in principle, offer more robustness, the uptake of such systems as comp"
W18-6527,W04-2705,0,0.339346,"Missing"
W18-6527,W18-3601,1,0.855834,"as Inputs for Multilingual Surface Realisation Simon Mille Universitat Pompeu Fabra Barcelona, Spain simon.mille@upf.edu Anja Belz University of Brighton Brighton, UK a.s.belz@brighton.ac.uk Bernd Bohnet Google Inc. London, UK bohnetbd@google.com Leo Wanner ICREA and Universitat Pompeu Fabra Barcelona, Spain leo.wanner@upf.edu Abstract resolved. The success of SimpleNLG (Gatt and Reiter, 2009) which had much reduced grammatical coverage, but accepted radically simpler inputs demonstrated the importance of this issue. The recently completed first Multilingual Surface Realisation Task (SR’18) (Mille et al., 2018) used for the first time inputs derived from the Universal Dependencies (UDs) (de Marneffe et al., 2014), a framework which was devised with the aim of facilitating cross-linguistically consistent grammatical annotation, and which has grown into a large-scale community effort involving more than 200 contributors, who have created over 100 treebanks in over 70 languages between them.1 UDs provide a more general and potentially flexible input representation for surface realisation (SR). However, their use for NLG has not so far been demonstrated. In this paper, we present the UD datasets used in"
W18-6527,bohnet-wanner-2010-open,1,0.85998,"Missing"
W18-6527,P06-1130,0,0.111814,"Missing"
W18-6527,W96-0501,0,0.474787,"In addition, we examine the motivation for, and likely usefulness of, deriving NLG inputs from annotations in resources originally developed for Natural Language Understanding (NLU), and assess whether the resulting inputs supply enough information of the right kind for the final stage in the NLG process. 1 Introduction There has long been an assumption in Natural Language Generation (NLG) that surface realisation can be treated as an independent subtask for which stand-alone, plug-and-play tools can, and should, be created. Early surface realisers such as KPML (Bateman, 1997) and FUF/Surge (Elhadad and Robin, 1996) were ambitious, independent surface realisation tools for English with wide grammatical coverage. However, the question of how the NLG components addressing the stage before surface realisation were supposed to put together inputs of the level of grammatical sophistication required by such tools was never quite 1 http://universaldependencies.org/ 199 Proceedings of The 11th International Natural Language Generation Conference, pages 199–209, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Association for Computational Linguistics while Section 6 discusses their suitability for SR and NLG"
W18-6527,P17-1017,0,0.0215344,"mmatical coverage. However, the question of how the NLG components addressing the stage before surface realisation were supposed to put together inputs of the level of grammatical sophistication required by such tools was never quite 1 http://universaldependencies.org/ 199 Proceedings of The 11th International Natural Language Generation Conference, pages 199–209, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Association for Computational Linguistics while Section 6 discusses their suitability for SR and NLG more generally. Some conclusions are presented in Section 7. • WebNLG dataset (Gardent et al., 2017): DBpedia triples covering properties of 15 DBpedia categories; 2 • E2E dataset (Novikova et al., 2017): attribute-value pairs covering 8 properties related to the restaurant domain. Background With the advent of large-scale treebanks and statistical NLG, surface realisation research turned to the use of treebank annotations, processed in various ways, as inputs to surface realisation. Annotation/sentence pairs constitute the training data, and similarity to the original sentences in the treebank is the main measure of success. A lot of this work used inputs derived from the Wall Street Journa"
W18-6527,W05-1510,0,0.0204745,"ibute-value pairs covering 8 properties related to the restaurant domain. Background With the advent of large-scale treebanks and statistical NLG, surface realisation research turned to the use of treebank annotations, processed in various ways, as inputs to surface realisation. Annotation/sentence pairs constitute the training data, and similarity to the original sentences in the treebank is the main measure of success. A lot of this work used inputs derived from the Wall Street Journal corpus with varying amounts of information removed from the parse-tree annotations (Langkilde-Geary, 2002; Nakanishi et al., 2005; Zhong and Stent, 2005; Cahill and van Genabith, 2006; White and Rajkumar, 2009). Because of the variation among inputs, results were not entirely comparable. The first Surface Realisation Shared Task (SR’11) (Belz et al., 2011) was thus conceived with the aim of developing a commonground input representation that would make different systems, for the first time, directly comparable. SR’11 used shallow and deep inputs for its two respective tracks, both derived from CoNLL’08 shared task data, which was in turn derived from the WSJ Corpus by automatically converting the corresponding Penn Tree"
W18-6527,W09-0613,0,0.204092,"Missing"
W18-6527,W17-5525,0,0.0700189,"Missing"
W18-6527,W02-2103,0,0.0384065,"ova et al., 2017): attribute-value pairs covering 8 properties related to the restaurant domain. Background With the advent of large-scale treebanks and statistical NLG, surface realisation research turned to the use of treebank annotations, processed in various ways, as inputs to surface realisation. Annotation/sentence pairs constitute the training data, and similarity to the original sentences in the treebank is the main measure of success. A lot of this work used inputs derived from the Wall Street Journal corpus with varying amounts of information removed from the parse-tree annotations (Langkilde-Geary, 2002; Nakanishi et al., 2005; Zhong and Stent, 2005; Cahill and van Genabith, 2006; White and Rajkumar, 2009). Because of the variation among inputs, results were not entirely comparable. The first Surface Realisation Shared Task (SR’11) (Belz et al., 2011) was thus conceived with the aim of developing a commonground input representation that would make different systems, for the first time, directly comparable. SR’11 used shallow and deep inputs for its two respective tracks, both derived from CoNLL’08 shared task data, which was in turn derived from the WSJ Corpus by automatically converting the"
W18-6527,J05-1004,0,0.700717,"0 2 3 4 A2 ROOT A1 A2 AM Figure 3: Deep input (Track 2) derived from UD structure in Figure 1. (left: CoNLL-U, righ: graphical) ures 1 and 2 show an original UD structure and a SR’18 Shallow input, respectively. • the in the head can be seen as a marker for nominal definiteness; 3.2 • the conjunction (complementiser) that in, e.g., I demand that you apologise, appears because it connects a finite verb apologise as an argument of another verb demand. Deep inputs The Deep Track input structures are trees that contain only content words linked by predicateargument edges, in the PropBank/NomBank (Palmer et al., 2005; Meyers et al., 2004) fashion. The Deep inputs can be seen as closer to a realistic application context for NLG systems, in which the component that generates the inputs presumably would not have access to syntactic or language-specific information. At the same time, we used only information found in the UD structures to create the Deep inputs, and tried to keep their structure simple. In Deep inputs, words are not disambiguated, full (semantically loaded) prepositions may be missing, and some argument relations may be underspecified or missing. The next two subsections provide more details a"
W18-6527,P09-1011,0,0.0151157,"ncies, the UD V2.0 treebank, as released in the context of the CoNLL 2017 shared task on multilingual dependency parsing (Zeman et al., 2017), was used. A subset of ten languages was selected that contains the necessary part-ofspeech and morphological tags for the Shallow Track: Arabic, Czech, Dutch, English, Finnish, French, Italian, Portuguese, Russian and Spanish. Three of these languages, namely English, French and Spanish were used also for the Deep Track. Starting from UD structures as they appear in the treebanks, Shallow and Deep inputs • Weather forecast generation (Weather) dataset (Liang et al., 2009): time series from weather-related measurements; • Abstract Meaning Representation (AMR) dataset (May and Priyadarshi, 2017): abstract predicate-argument graphs that cover several genres; 200 1 2 3 4 5 6 7 8 9 10 11 12 13 The third was being run by the head of an investment firm . the third be be run by the head of a investment firm . DET ADJ AUX AUX VERB ADP DET NOUN ADP DET NOUN NOUN PUNCT DT JJ VBD VBG VBN IN DT NN IN DT NN NN . Definite=Def|PronType=Art Degree=Pos|NumType=Ord Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin VerbForm=Ger Tense=Past|VerbForm=Part|Voice=Pass Definite=Def"
W18-6527,W08-2121,0,0.136007,"Missing"
W18-6527,de-marneffe-etal-2014-universal,0,0.0679075,"Missing"
W18-6527,D09-1043,0,0.027139,"ground With the advent of large-scale treebanks and statistical NLG, surface realisation research turned to the use of treebank annotations, processed in various ways, as inputs to surface realisation. Annotation/sentence pairs constitute the training data, and similarity to the original sentences in the treebank is the main measure of success. A lot of this work used inputs derived from the Wall Street Journal corpus with varying amounts of information removed from the parse-tree annotations (Langkilde-Geary, 2002; Nakanishi et al., 2005; Zhong and Stent, 2005; Cahill and van Genabith, 2006; White and Rajkumar, 2009). Because of the variation among inputs, results were not entirely comparable. The first Surface Realisation Shared Task (SR’11) (Belz et al., 2011) was thus conceived with the aim of developing a commonground input representation that would make different systems, for the first time, directly comparable. SR’11 used shallow and deep inputs for its two respective tracks, both derived from CoNLL’08 shared task data, which was in turn derived from the WSJ Corpus by automatically converting the corresponding Penn TreeBank parse trees to dependency structures (Surdeanu et al., 2008). While dependen"
W18-6527,K17-3001,0,0.0322515,"a Shallow Track, starting from syntactic structures in which word order information has been removed and tokens have been lemmatised, and a Deep Track, which starts from more abstract structures from which, additionally, functional words (in particular, auxiliaries, functional prepositions and conjunctions) and surface-oriented morphological information have been removed. Taking advantage of the growing availability of multilingual treebanks annotated with Universal Dependencies, the UD V2.0 treebank, as released in the context of the CoNLL 2017 shared task on multilingual dependency parsing (Zeman et al., 2017), was used. A subset of ten languages was selected that contains the necessary part-ofspeech and morphological tags for the Shallow Track: Arabic, Czech, Dutch, English, Finnish, French, Italian, Portuguese, Russian and Spanish. Three of these languages, namely English, French and Spanish were used also for the Deep Track. Starting from UD structures as they appear in the treebanks, Shallow and Deep inputs • Weather forecast generation (Weather) dataset (Liang et al., 2009): time series from weather-related measurements; • Abstract Meaning Representation (AMR) dataset (May and Priyadarshi, 201"
