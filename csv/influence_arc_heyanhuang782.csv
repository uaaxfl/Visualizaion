2020.aacl-main.2,D19-1572,0,0.0177822,"hods outperform vanilla multilingual fine-tuning. 1 Introduction Supervised text classification heavily relies on manually annotated training data, while the data are usually only available in rich-resource languages, such as English. It requires great effort to make the resources available in other languages. Various methods have been proposed to build cross-lingual classification models by exploiting machine translation systems (Xu and Yang, 2017; Chen et al., 2018; Conneau et al., 2018), and learning multilingual embeddings (Conneau et al., 2018; Yu et al., 2018; Artetxe and Schwenk, 2019; Eisenschlos et al., 2019). Recently, multilingual pretrained language models have shown surprising cross-lingual effectiveness on a wide range of downstream tasks (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020; Chi et al., 2020a,b). Even without using any parallel corpora, the pretrained models can still perform zero-shot cross-lingual classification (Pires et al., 2019; Wu and Dredze, 2019; Keung et al., 2019). That is, these models can be fine-tuned in a source language, and then directly evaluated in other target languages. Despite ∗ 2 Background: Multilingual Fine-Tuning We use multilingual B"
2020.aacl-main.2,Q19-1038,0,0.0286828,"enchmarks show that our methods outperform vanilla multilingual fine-tuning. 1 Introduction Supervised text classification heavily relies on manually annotated training data, while the data are usually only available in rich-resource languages, such as English. It requires great effort to make the resources available in other languages. Various methods have been proposed to build cross-lingual classification models by exploiting machine translation systems (Xu and Yang, 2017; Chen et al., 2018; Conneau et al., 2018), and learning multilingual embeddings (Conneau et al., 2018; Yu et al., 2018; Artetxe and Schwenk, 2019; Eisenschlos et al., 2019). Recently, multilingual pretrained language models have shown surprising cross-lingual effectiveness on a wide range of downstream tasks (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020; Chi et al., 2020a,b). Even without using any parallel corpora, the pretrained models can still perform zero-shot cross-lingual classification (Pires et al., 2019; Wu and Dredze, 2019; Keung et al., 2019). That is, these models can be fine-tuned in a source language, and then directly evaluated in other target languages. Despite ∗ 2 Background: Multilingual Fine-T"
2020.aacl-main.2,Q18-1039,0,0.0246351,"dge from monolingual pretrained models to multilingual ones. Experimental results on two cross-lingual classification benchmarks show that our methods outperform vanilla multilingual fine-tuning. 1 Introduction Supervised text classification heavily relies on manually annotated training data, while the data are usually only available in rich-resource languages, such as English. It requires great effort to make the resources available in other languages. Various methods have been proposed to build cross-lingual classification models by exploiting machine translation systems (Xu and Yang, 2017; Chen et al., 2018; Conneau et al., 2018), and learning multilingual embeddings (Conneau et al., 2018; Yu et al., 2018; Artetxe and Schwenk, 2019; Eisenschlos et al., 2019). Recently, multilingual pretrained language models have shown surprising cross-lingual effectiveness on a wide range of downstream tasks (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020; Chi et al., 2020a,b). Even without using any parallel corpora, the pretrained models can still perform zero-shot cross-lingual classification (Pires et al., 2019; Wu and Dredze, 2019; Keung et al., 2019). That is, these models can be fine"
2020.aacl-main.2,D19-1138,0,0.0253137,"nslation systems (Xu and Yang, 2017; Chen et al., 2018; Conneau et al., 2018), and learning multilingual embeddings (Conneau et al., 2018; Yu et al., 2018; Artetxe and Schwenk, 2019; Eisenschlos et al., 2019). Recently, multilingual pretrained language models have shown surprising cross-lingual effectiveness on a wide range of downstream tasks (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020; Chi et al., 2020a,b). Even without using any parallel corpora, the pretrained models can still perform zero-shot cross-lingual classification (Pires et al., 2019; Wu and Dredze, 2019; Keung et al., 2019). That is, these models can be fine-tuned in a source language, and then directly evaluated in other target languages. Despite ∗ 2 Background: Multilingual Fine-Tuning We use multilingual BERT (Devlin et al., 2019) for multilingual pretrained language models. The pretrained model uses the BERT-style Transformer (Vaswani et al., 2017) architecture, and follows the similar fine-tuning procedure as BERT for text classification, which is illustrated in Figure 1(a). To be specific, the first input token of the models is always a special classification token Contribution during internship at Microso"
2020.aacl-main.2,P19-1493,0,0.0185862,"fication models by exploiting machine translation systems (Xu and Yang, 2017; Chen et al., 2018; Conneau et al., 2018), and learning multilingual embeddings (Conneau et al., 2018; Yu et al., 2018; Artetxe and Schwenk, 2019; Eisenschlos et al., 2019). Recently, multilingual pretrained language models have shown surprising cross-lingual effectiveness on a wide range of downstream tasks (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020; Chi et al., 2020a,b). Even without using any parallel corpora, the pretrained models can still perform zero-shot cross-lingual classification (Pires et al., 2019; Wu and Dredze, 2019; Keung et al., 2019). That is, these models can be fine-tuned in a source language, and then directly evaluated in other target languages. Despite ∗ 2 Background: Multilingual Fine-Tuning We use multilingual BERT (Devlin et al., 2019) for multilingual pretrained language models. The pretrained model uses the BERT-style Transformer (Vaswani et al., 2017) architecture, and follows the similar fine-tuning procedure as BERT for text classification, which is illustrated in Figure 1(a). To be specific, the first input token of the models is always a special classification token"
2020.aacl-main.2,2020.acl-main.747,0,0.067487,"Missing"
2020.aacl-main.2,P10-1114,0,0.0857345,"Missing"
2020.aacl-main.2,P16-1162,0,0.0418231,"Missing"
2020.aacl-main.2,D19-1077,0,0.0206782,"xploiting machine translation systems (Xu and Yang, 2017; Chen et al., 2018; Conneau et al., 2018), and learning multilingual embeddings (Conneau et al., 2018; Yu et al., 2018; Artetxe and Schwenk, 2019; Eisenschlos et al., 2019). Recently, multilingual pretrained language models have shown surprising cross-lingual effectiveness on a wide range of downstream tasks (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020; Chi et al., 2020a,b). Even without using any parallel corpora, the pretrained models can still perform zero-shot cross-lingual classification (Pires et al., 2019; Wu and Dredze, 2019; Keung et al., 2019). That is, these models can be fine-tuned in a source language, and then directly evaluated in other target languages. Despite ∗ 2 Background: Multilingual Fine-Tuning We use multilingual BERT (Devlin et al., 2019) for multilingual pretrained language models. The pretrained model uses the BERT-style Transformer (Vaswani et al., 2017) architecture, and follows the similar fine-tuning procedure as BERT for text classification, which is illustrated in Figure 1(a). To be specific, the first input token of the models is always a special classification token Contribution during"
2020.aacl-main.2,P17-1130,0,0.0144372,"sferring the knowledge from monolingual pretrained models to multilingual ones. Experimental results on two cross-lingual classification benchmarks show that our methods outperform vanilla multilingual fine-tuning. 1 Introduction Supervised text classification heavily relies on manually annotated training data, while the data are usually only available in rich-resource languages, such as English. It requires great effort to make the resources available in other languages. Various methods have been proposed to build cross-lingual classification models by exploiting machine translation systems (Xu and Yang, 2017; Chen et al., 2018; Conneau et al., 2018), and learning multilingual embeddings (Conneau et al., 2018; Yu et al., 2018; Artetxe and Schwenk, 2019; Eisenschlos et al., 2019). Recently, multilingual pretrained language models have shown surprising cross-lingual effectiveness on a wide range of downstream tasks (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020; Chi et al., 2020a,b). Even without using any parallel corpora, the pretrained models can still perform zero-shot cross-lingual classification (Pires et al., 2019; Wu and Dredze, 2019; Keung et al., 2019). That is, these"
2020.aacl-main.2,W18-3023,0,0.0238199,"classification benchmarks show that our methods outperform vanilla multilingual fine-tuning. 1 Introduction Supervised text classification heavily relies on manually annotated training data, while the data are usually only available in rich-resource languages, such as English. It requires great effort to make the resources available in other languages. Various methods have been proposed to build cross-lingual classification models by exploiting machine translation systems (Xu and Yang, 2017; Chen et al., 2018; Conneau et al., 2018), and learning multilingual embeddings (Conneau et al., 2018; Yu et al., 2018; Artetxe and Schwenk, 2019; Eisenschlos et al., 2019). Recently, multilingual pretrained language models have shown surprising cross-lingual effectiveness on a wide range of downstream tasks (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020; Chi et al., 2020a,b). Even without using any parallel corpora, the pretrained models can still perform zero-shot cross-lingual classification (Pires et al., 2019; Wu and Dredze, 2019; Keung et al., 2019). That is, these models can be fine-tuned in a source language, and then directly evaluated in other target languages. Despite ∗ 2 Back"
2021.acl-long.193,D18-1547,0,0.0121764,"ber of unknown slot values are included in their test set, so the generalization ability of the model can be reflected more accurately. WOZ2.0 and DSTC2 datasets are both collected in the restaurant domain and have the same three slots f ood, area, and price range. These two datasets provide automatic speech recognition (ASR) hypotheses of user utterances and can therefore be used to verify the robustness of the model against ASR errors. As in previous works, we use manuscript user utterance for training and top ASR hypothesis for testing. MultiWOZ2.1 is the corrected version of the MultiWOZ (Budzianowski et al., 2018). Compared to the four datasets above, MultiWOZ2.1 is a more challenging and currently widely used benchmark for multi-turn multi-domain dialogue state tracking, consisting of 7 domains, over 30 slots, and over 4500 possible slot values. Following previous works (Wu et al., 2019; Kim et al., 2020; Heck et al., 2020; Zhu et al., 2020), we only use 5 domains 2483 (restaurant, train, hotel, taxi, attraction) that contain a total of 30 slots. 3.2 Baselines We use 5 different types of baselines whose characteristics are shown in Table 2. SpanPtr: This is the first model to extract slot values direc"
2021.acl-long.193,D14-1179,0,0.0100587,"Missing"
2021.acl-long.193,2020.sigdial-1.4,0,0.0303065,"Missing"
2021.acl-long.193,W14-4337,0,0.0251892,"t dialogues, so 0 is used in the paper to refer to the maximum granularity N , -1 to refer to granularity N − 1, and so on. 3 Experimental Settings In order to investigate how the context information of different granularity affects dialogue state tracking, we analyze the performance of several different types of dialogue state tracking models on different datasets. For a clearer illustration, the detailed settings are introduced in this section. 3.1 Datasets Our experiments were carried out on 5 datasets, Sim-M (Shah et al., 2018), Sim-R (Shah et al., 2018), WOZ2.0 (Wen et al., 2016), DSTC2 (Henderson et al., 2014) and MultiWOZ2.1 (Eric et al., 2019). The statistics for all datasets are shown in Table 1. Sim-M and Sim-R are multi-turn dialogue datasets in the movie and restaurant domains, respectively, which are specially designed to evaluate the scalability of dialogue state tracking model. A large number of unknown slot values are included in their test set, so the generalization ability of the model can be reflected more accurately. WOZ2.0 and DSTC2 datasets are both collected in the restaurant domain and have the same three slots f ood, area, and price range. These two datasets provide automatic spe"
2021.acl-long.193,2020.acl-main.53,0,0.429289,"h International Joint Conference on Natural Language Processing, pages 2481–2491 August 1–6, 2021. ©2021 Association for Computational Linguistics strategy obtains each slot value in dialogue state by inquiring all the dialogue history (Xu and Hu, 2018; Lei et al., 2018; Goel et al., 2019; Ren et al., 2019; Wu et al., 2019; Shan et al., 2020; Zhang et al., 2020), the advantage of this strategy is to ensure the integrity of the dialogue information. The previous-based strategy relies on the current turn dialogue to update the previous dialogue state (Mrkˇsi´c et al., 2017; Chao and Lane, 2019; Kim et al., 2020; Heck et al., 2020; Zhu et al., 2020), the main character of this strategy is to greatly improve the efficiency of dialogue state prediction and avoid the computational cost of encoding all dialogue history. However, both kinds of strategies above have great defects because of their own characters. For the scratch-based strategy, it is hard to correctly track short-dependency dialogue state because of the noise associated with encoding all dialogue history. For example, the dialogue history of turn 1 to 3 in Figure 1 (a) does not contribute to the prediction of slot values in the restaurant d"
2021.acl-long.193,P19-1546,0,0.0797806,"# Slots Avg. turns Sim-M Sim-R WOZ2.0 DSTC2 MultiWOZ2.1 1 1 1 1 5 5 9 3 3 30 5.14 5.53 4.23 7.24 6.53 # Dialogues train dev test 384 120 264 1,116 349 775 600 200 400 1,612 506 1,117 8,420 1,000 999 train 1,973 6,175 2,536 11,677 54,984 # Turns dev 627 1,489 830 3,934 7,371 test 1,364 3,436 1,646 9,890 7,368 Table 1: Data statistics of Sim-M, Sim-R, WOZ2.0, DSTC2 and MultiWOZ2.1. Avg. turns indicates the average number of turns involved in the dialogue in the training data. Models SpanPtr (Xu and Hu, 2018) TRADE (Wu et al., 2019) BERTDST (Chao and Lane, 2019) SOMDST (Kim et al., 2020) SUMBT (Lee et al., 2019) Open vocabulary X X X X × Encoder RNN RNN BERT BERT BERT Decoder Extractive Generative Extractive Generative Classification Tracking strategy scratch-based scratch-based previous-based previous-based previous-based Table 2: Statistics on the characteristics of the 5 baselines studied in the paper. In the decoder, the extractive mode refers to the extraction of slot values directly from the dialogue context, the generative mode refers to the vocabulary-dependent sequence decoding, and the classification mode is the slot value ontology-based classification. this case corresponds to the strategy"
2021.acl-long.193,P18-1133,0,0.0143352,"e 1, the dialogue state at turn 2 is {(attraction − type, cinema), (attraction − area, south)}. In general, there are two strategies to track a dialogue state: predicting it from scratch and updating it from previous state. The scratch-based 2481 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2481–2491 August 1–6, 2021. ©2021 Association for Computational Linguistics strategy obtains each slot value in dialogue state by inquiring all the dialogue history (Xu and Hu, 2018; Lei et al., 2018; Goel et al., 2019; Ren et al., 2019; Wu et al., 2019; Shan et al., 2020; Zhang et al., 2020), the advantage of this strategy is to ensure the integrity of the dialogue information. The previous-based strategy relies on the current turn dialogue to update the previous dialogue state (Mrkˇsi´c et al., 2017; Chao and Lane, 2019; Kim et al., 2020; Heck et al., 2020; Zhu et al., 2020), the main character of this strategy is to greatly improve the efficiency of dialogue state prediction and avoid the computational cost of encoding all dialogue history. However, both kinds of strategies above have"
2021.acl-long.193,P17-1163,0,0.0508783,"Missing"
2021.acl-long.193,W18-5045,0,0.0484125,"Missing"
2021.acl-long.193,D19-1196,0,0.0230096,"Missing"
2021.acl-long.193,2020.acl-main.563,0,0.0248207,"action − area, south)}. In general, there are two strategies to track a dialogue state: predicting it from scratch and updating it from previous state. The scratch-based 2481 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2481–2491 August 1–6, 2021. ©2021 Association for Computational Linguistics strategy obtains each slot value in dialogue state by inquiring all the dialogue history (Xu and Hu, 2018; Lei et al., 2018; Goel et al., 2019; Ren et al., 2019; Wu et al., 2019; Shan et al., 2020; Zhang et al., 2020), the advantage of this strategy is to ensure the integrity of the dialogue information. The previous-based strategy relies on the current turn dialogue to update the previous dialogue state (Mrkˇsi´c et al., 2017; Chao and Lane, 2019; Kim et al., 2020; Heck et al., 2020; Zhu et al., 2020), the main character of this strategy is to greatly improve the efficiency of dialogue state prediction and avoid the computational cost of encoding all dialogue history. However, both kinds of strategies above have great defects because of their own characters. For the scratch-based stra"
2021.acl-long.193,P19-1078,0,0.23121,"e, cinema), (attraction − area, south)}. In general, there are two strategies to track a dialogue state: predicting it from scratch and updating it from previous state. The scratch-based 2481 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2481–2491 August 1–6, 2021. ©2021 Association for Computational Linguistics strategy obtains each slot value in dialogue state by inquiring all the dialogue history (Xu and Hu, 2018; Lei et al., 2018; Goel et al., 2019; Ren et al., 2019; Wu et al., 2019; Shan et al., 2020; Zhang et al., 2020), the advantage of this strategy is to ensure the integrity of the dialogue information. The previous-based strategy relies on the current turn dialogue to update the previous dialogue state (Mrkˇsi´c et al., 2017; Chao and Lane, 2019; Kim et al., 2020; Heck et al., 2020; Zhu et al., 2020), the main character of this strategy is to greatly improve the efficiency of dialogue state prediction and avoid the computational cost of encoding all dialogue history. However, both kinds of strategies above have great defects because of their own characters. For the"
2021.acl-long.193,P18-1134,0,0.24921,"example, in Figure 1, the dialogue state at turn 2 is {(attraction − type, cinema), (attraction − area, south)}. In general, there are two strategies to track a dialogue state: predicting it from scratch and updating it from previous state. The scratch-based 2481 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2481–2491 August 1–6, 2021. ©2021 Association for Computational Linguistics strategy obtains each slot value in dialogue state by inquiring all the dialogue history (Xu and Hu, 2018; Lei et al., 2018; Goel et al., 2019; Ren et al., 2019; Wu et al., 2019; Shan et al., 2020; Zhang et al., 2020), the advantage of this strategy is to ensure the integrity of the dialogue information. The previous-based strategy relies on the current turn dialogue to update the previous dialogue state (Mrkˇsi´c et al., 2017; Chao and Lane, 2019; Kim et al., 2020; Heck et al., 2020; Zhu et al., 2020), the main character of this strategy is to greatly improve the efficiency of dialogue state prediction and avoid the computational cost of encoding all dialogue history. However, both kinds of stra"
2021.acl-long.193,2020.starsem-1.17,0,0.0314208,"h)}. In general, there are two strategies to track a dialogue state: predicting it from scratch and updating it from previous state. The scratch-based 2481 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2481–2491 August 1–6, 2021. ©2021 Association for Computational Linguistics strategy obtains each slot value in dialogue state by inquiring all the dialogue history (Xu and Hu, 2018; Lei et al., 2018; Goel et al., 2019; Ren et al., 2019; Wu et al., 2019; Shan et al., 2020; Zhang et al., 2020), the advantage of this strategy is to ensure the integrity of the dialogue information. The previous-based strategy relies on the current turn dialogue to update the previous dialogue state (Mrkˇsi´c et al., 2017; Chao and Lane, 2019; Kim et al., 2020; Heck et al., 2020; Zhu et al., 2020), the main character of this strategy is to greatly improve the efficiency of dialogue state prediction and avoid the computational cost of encoding all dialogue history. However, both kinds of strategies above have great defects because of their own characters. For the scratch-based strategy, it is hard to c"
2021.acl-long.193,2020.findings-emnlp.68,0,0.061518,"atural Language Processing, pages 2481–2491 August 1–6, 2021. ©2021 Association for Computational Linguistics strategy obtains each slot value in dialogue state by inquiring all the dialogue history (Xu and Hu, 2018; Lei et al., 2018; Goel et al., 2019; Ren et al., 2019; Wu et al., 2019; Shan et al., 2020; Zhang et al., 2020), the advantage of this strategy is to ensure the integrity of the dialogue information. The previous-based strategy relies on the current turn dialogue to update the previous dialogue state (Mrkˇsi´c et al., 2017; Chao and Lane, 2019; Kim et al., 2020; Heck et al., 2020; Zhu et al., 2020), the main character of this strategy is to greatly improve the efficiency of dialogue state prediction and avoid the computational cost of encoding all dialogue history. However, both kinds of strategies above have great defects because of their own characters. For the scratch-based strategy, it is hard to correctly track short-dependency dialogue state because of the noise associated with encoding all dialogue history. For example, the dialogue history of turn 1 to 3 in Figure 1 (a) does not contribute to the prediction of slot values in the restaurant domain. For the previous-based strategy"
2021.acl-long.265,2020.acl-main.421,0,0.382399,"Missing"
2021.acl-long.265,J93-2003,0,0.168381,"Missing"
2021.acl-long.265,2021.emnlp-main.125,1,0.795284,"Missing"
2021.acl-long.265,2021.naacl-main.280,1,0.918277,"most applications and resources are still English-centric, making non-English users hard to access. Therefore, it is essential to build cross-lingual transferable models that can learn from the training data in highresource languages and generalize on low-resource languages. Recently, pretrained cross-lingual language models have shown their effectiveness for cross-lingual transfer. By pre-training on monolingual text and parallel sentences, the models provide significant improvements on a wide range of crosslingual end tasks (Conneau and Lample, 2019; Conneau et al., 2020; Liu et al., 2020; Chi et al., 2021b). Cross-lingual language model pre-training is typically achieved by learning various pretext tasks on ∗ Contribution during internship at Microsoft Research. monolingual and parallel corpora. By simply learning masked language modeling (MLM; Devlin et al. 2019) on monolingual text of multiple languages, the models surprisingly achieve competitive results on cross-lingual tasks (Wu and Dredze, 2019; K et al., 2020). Besides, several pretext tasks are proposed to utilize parallel corpora to learn better sentence-level cross-lingual representations (Conneau and Lample, 2019; Chi et al., 2021b;"
2021.acl-long.265,2020.tacl-1.30,0,0.109618,"Missing"
2021.acl-long.265,2020.acl-main.747,0,0.181358,"Missing"
2021.acl-long.265,2020.acl-main.653,0,0.260231,"Missing"
2021.acl-long.265,P19-1124,0,0.0233816,"plicit alignment objective in large-scale pre-training can provide consistent improvements over baseline models. Word alignment The IBM models (Brown et al., 1993) are statistical models for modeling the translation process that can extract word alignments between sentence pairs. A large number of word alignment models are based on the IBM models (Och and Ney, 2003; Mermer and Sarac¸lar, ¨ 2011; Dyer et al., 2013; Ostling and Tiedemann, 2016). Recent studies have shown that word alignments can be extracted from neural machine translation models (Ghader and Monz, 2017; Koehn and Knowles, 2017; Li et al., 2019) or from pretrained cross-lingual LMs (Jalili Sabet et al., 2020; Nagata et al., 2020). 3 Method Figure 1 illustrates an overview of our method for pre-training our cross-lingual LM, which is called XLM-A LIGN. XLM-A LIGN is pretrained in an expectation-maximization manner with two alternating steps, which are word alignment selflabeling and denoising word alignment. We first formulate word alignment as an optimal transport problem, and self-label word alignments of the input translation pair on-the-fly. Then, we update the model parameters with the denoising word alignment task, where the mod"
2021.acl-long.265,P19-1015,0,0.133748,"Missing"
2021.acl-long.538,2020.acl-main.554,0,0.444376,"volving alignments and summary patterns across languages, which encourages attaining knowledge transfer. Experiments on two CLS datasets demonstrate that our model significantly outperforms three baseline models in both low-resource and full-dataset scenarios. Moreover, in-depth analysis on the generated summaries and attention heads verifies that interactions are learned well using MCLAS, which benefits the CLS task under limited parallel resources. 1 Figure 1: An example of the alignments across summaries in different languages. Each color represents phrases with one specific meaning. 2019; Cao et al., 2020; Zhu et al., 2020). However, two challenges arise with these approaches: 1) most languages are low-resource, thereby lacking document-summary paired data; 2) large parallel datasets across different languages for neuralbased CLS are rare and expensive, especially under the current trend of neural networks. Therefore, a low-resource setting is more realistic, and challenging, one for cross-lingual summarization. To our best knowledge, cross-lingual summarization under low-resource settings has not been well investigated and explored. Therefore, in this paper, we will develop a new model for cr"
2021.acl-long.538,N19-1423,0,0.0741635,") and the CLS task (for DA ), achieved by optimizing the second term in Equation (4). With the modification of the target, our model can easily capture interactions between cross-lingual summaries. The trained model shows effectiveness in aligning the summaries. Not only the output tokens, but also the attention distributions are aligned. The model we designed leverages this phenomenon to enable monolingual knowledge to be transferred under low-resource scenarios. Detailed investigation is presented in Section 6. We adopt Transformers as our base model. In addition, we use multilingual BERT (Devlin et al., 2019) to initialize the encoder, improving its ability to produce multilingual representations. Additionally, having tried many different position embedding and language segmentation embedding methods, we find that [LSEP] is enough for the model to distinguish whether it is generating S B . Hence keeping the original position embedding (Vaswani et al., 2017) and employing no segmentation embedding are best for performance and efficiency. 4.2 Learning Schemes for MCLAS under Limited Resources Since our proposed framework enforces interactions between cross multilingual summaries, it has further bene"
2021.acl-long.538,P19-1305,0,0.260745,"ces. Our implementation and data are available at https://github.com/WoodenWhite/MCLAS. 2 2.1 Related Work Cross-Lingual Summarization Recently, cross-lingual summarization has received attention in research due to the increasing demand to produce cross-lingual information. Traditional CLS systems are based on a pipeline paradigm (Wan et al., 2010; Wan, 2011; Zhang et al., 2016). These pipeline systems first translate the document and then summarize it or vice versa. Shen et al. (2018) propose the use of pseudo summaries to train the cross-lingual abstractive summarization model. In contrast, Duan et al. (2019a) and Ouyang et al. (2019) generate pseudo sources to construct the cross-lingual summarization dataset. The first large-scale cross-lingual summarization datasets are acquired by use of a round-trip translation strategy (Zhu et al., 2019). Additionly, Zhu et al. (2019) propose a multi-task framework to improve their cross-lingual summarization system. Following Zhu et al. (2019), more methods have been proposed to improve the CLS task. Zhu et al. (2020) use a pointer-generator network to exploit the translation patterns in cross-lingual summarization. Cao et al. (2020) utilize two encoders a"
2021.acl-long.538,D18-1398,0,0.0264964,"wing Zhu et al. (2019), more methods have been proposed to improve the CLS task. Zhu et al. (2020) use a pointer-generator network to exploit the translation patterns in cross-lingual summarization. Cao et al. (2020) utilize two encoders and two decoders to jointly learn to align and summarize. In contrast to previous methods, MCLAS generates the concatenation of monolingual and crosslingual summaries, thereby modeling relationships between them. 2.2 Low-Resource Natural Language Generation Natural language generation (NLG) for lowresource languages or domains has attracted lots of attention. Gu et al. (2018) leverage meta-learning 6911 to improve low-resource neural machine translation. Meanwhile, many pretrained NLG models have been proposed and adapted to low-resource scenarios (Song et al., 2019; Chi et al., 2020; Radford et al., 2019; Zhang et al., 2019a). However, these models require large-scale pretraining. Our work does not require any large pretrained generation models or translation models, enabling a vital decreases in training cost. 3 3.1 Background Neural Cross-lingual Summarization Given a source document DA = A A A {x1 , x2 , . . . , xm } in language A, a monolingual summarization"
2021.acl-long.538,P19-1580,0,0.0608443,"Missing"
2021.acl-long.538,P17-2074,0,0.0493656,"Missing"
2021.acl-long.538,D19-1387,0,0.0197263,"formance under low-resource scenarios. Detailed numbers are presented in Table 1. 5.2 Training and Inference We use multilingual BERT (mBERT) (Devlin et al., 2019) to initialize our Transformer encoder. The decoder is a Transformer decoder with 6 layers. Each attention module has 8 different attention heads. The hidden size of the decoder’s self-attention is 768 and that of the feed-forward network is 2048. The final model contains 296,046,231 parameters. Because the encoder is pretrained when the decoder is randomly initialized, we use two separate optimizers for the encoder and the decoder (Liu and Lapata, 2019). The encoder’s learning rate ηe is set as 0.005, while the decoder’s learning rate ηd is 0.2. Warmup-steps for the encoder are 10,000 and 5,000 for the decoder. We train the model on two TITAN RTX GPUs for one day with gradient accumulation every 5 steps. Dropout with a probability 0.1 is applied before all the linear layers. We find that the target vocabulary type doesn’t have much influence on the final result. Therefore, we directly use mBERT’s subwords vocabulary as our target vocabulary. Nevertheless, in case tokens would be produced in the wrong language, we constructe a target token vo"
2021.acl-long.538,N19-1204,0,0.0675654,"and data are available at https://github.com/WoodenWhite/MCLAS. 2 2.1 Related Work Cross-Lingual Summarization Recently, cross-lingual summarization has received attention in research due to the increasing demand to produce cross-lingual information. Traditional CLS systems are based on a pipeline paradigm (Wan et al., 2010; Wan, 2011; Zhang et al., 2016). These pipeline systems first translate the document and then summarize it or vice versa. Shen et al. (2018) propose the use of pseudo summaries to train the cross-lingual abstractive summarization model. In contrast, Duan et al. (2019a) and Ouyang et al. (2019) generate pseudo sources to construct the cross-lingual summarization dataset. The first large-scale cross-lingual summarization datasets are acquired by use of a round-trip translation strategy (Zhu et al., 2019). Additionly, Zhu et al. (2019) propose a multi-task framework to improve their cross-lingual summarization system. Following Zhu et al. (2019), more methods have been proposed to improve the CLS task. Zhu et al. (2020) use a pointer-generator network to exploit the translation patterns in cross-lingual summarization. Cao et al. (2020) utilize two encoders and two decoders to jointly"
2021.acl-long.538,P11-1155,0,0.0308472,"hat MCLAS learns the alignments and interactions between two languages, and this facilitates translation and summarization in the decoder stage. Our analysis provides a clear explanation of why MCLAS is capable of supporting CLS under limited resources. Our implementation and data are available at https://github.com/WoodenWhite/MCLAS. 2 2.1 Related Work Cross-Lingual Summarization Recently, cross-lingual summarization has received attention in research due to the increasing demand to produce cross-lingual information. Traditional CLS systems are based on a pipeline paradigm (Wan et al., 2010; Wan, 2011; Zhang et al., 2016). These pipeline systems first translate the document and then summarize it or vice versa. Shen et al. (2018) propose the use of pseudo summaries to train the cross-lingual abstractive summarization model. In contrast, Duan et al. (2019a) and Ouyang et al. (2019) generate pseudo sources to construct the cross-lingual summarization dataset. The first large-scale cross-lingual summarization datasets are acquired by use of a round-trip translation strategy (Zhu et al., 2019). Additionly, Zhu et al. (2019) propose a multi-task framework to improve their cross-lingual summariza"
2021.acl-long.538,P10-1094,0,0.227702,"decoder, proving that MCLAS learns the alignments and interactions between two languages, and this facilitates translation and summarization in the decoder stage. Our analysis provides a clear explanation of why MCLAS is capable of supporting CLS under limited resources. Our implementation and data are available at https://github.com/WoodenWhite/MCLAS. 2 2.1 Related Work Cross-Lingual Summarization Recently, cross-lingual summarization has received attention in research due to the increasing demand to produce cross-lingual information. Traditional CLS systems are based on a pipeline paradigm (Wan et al., 2010; Wan, 2011; Zhang et al., 2016). These pipeline systems first translate the document and then summarize it or vice versa. Shen et al. (2018) propose the use of pseudo summaries to train the cross-lingual abstractive summarization model. In contrast, Duan et al. (2019a) and Ouyang et al. (2019) generate pseudo sources to construct the cross-lingual summarization dataset. The first large-scale cross-lingual summarization datasets are acquired by use of a round-trip translation strategy (Zhu et al., 2019). Additionly, Zhu et al. (2019) propose a multi-task framework to improve their cross-lingua"
2021.acl-long.538,D15-1012,0,0.0604217,"Missing"
2021.acl-long.538,D18-1448,0,0.0283598,"how to produce a monolingual summary for a given document. Then, we jointly learn MS and CLS with few training samples, optimizing Equation (4). We adopt similar initialization to existing CLS methods, which is introduced in Section 5.3. 5 Experiments 5.1 Datasets we conduct experiments on the En2ZhSum, Zh2EnSum CLS datasets1 (Zhu et al., 2019) and a newly constructed En2DeSum dataset. En2ZhSum is an English-to-Chinese dataset containing 364,687 training samples, 3,000 validation, and 3,000 testing samples. The dataset is converted from the union set of CNN/DM (Hermann et al., 2015) and MSMO (Zhu et al., 2018) using a round-trip translation strategy. Converted from the LCSTS dataset, Zh2EnSum contains 1,693,713 Chinese-to-English training samples, 3,000 validation, and 3,000 testing samples. To better verify the CLS ability of MCLAS, we construct a new English-to-German dataset (En2DeSum), using the same methods proposed by Zhu et al. (2019). We use WMT’19 English-German winner2 as our translation model to process the English Gigaword dataset.3 We set the threshold T1 = 0.6 and T2 = 0.2. The final En2DeSum contains 429,393 training samples, 4,305 validation samples, and 4,099 testing samples. All t"
2021.acl-long.538,D19-1302,0,0.0317951,"Missing"
2021.acl-long.538,2020.acl-main.121,0,0.220156,"and summary patterns across languages, which encourages attaining knowledge transfer. Experiments on two CLS datasets demonstrate that our model significantly outperforms three baseline models in both low-resource and full-dataset scenarios. Moreover, in-depth analysis on the generated summaries and attention heads verifies that interactions are learned well using MCLAS, which benefits the CLS task under limited parallel resources. 1 Figure 1: An example of the alignments across summaries in different languages. Each color represents phrases with one specific meaning. 2019; Cao et al., 2020; Zhu et al., 2020). However, two challenges arise with these approaches: 1) most languages are low-resource, thereby lacking document-summary paired data; 2) large parallel datasets across different languages for neuralbased CLS are rare and expensive, especially under the current trend of neural networks. Therefore, a low-resource setting is more realistic, and challenging, one for cross-lingual summarization. To our best knowledge, cross-lingual summarization under low-resource settings has not been well investigated and explored. Therefore, in this paper, we will develop a new model for cross-lingual abstrac"
2021.emnlp-main.125,2020.acl-main.421,0,0.147399,"s on an Nvidia DGX-2 Station. Details of the pre-training hyperparameters are described in Appendix. 4.2 4.2.1 Results XTREME Cross-lingual Understanding To validate the performance of M T6, we evaluate the pretrained models on XTREME (Hu et al., 2020b), which is a widely used benchmark for cross-lingual understanding. Following M T5 (Xue et al., 2020), we consider six downstream tasks included by XTREME: the named entity recognition (NER) task on the WikiAnn (Pan et al., 2017; Rahimi et al., 2019) dataset in 40 languages, the question answering (QA) task on MLQA (Lewis et al., 2020b), XQuAD (Artetxe et al., 2020), and TyDiQA-GoldP (Clark et al., 2020), the cross-lingual natural language inference task on XNLI (Conneau et al., 2018), and crosslingual paraphrase adversaries on PAWS-X (Yang et al., 2019). The models are evaluated under the cross-lingual transfer setting (Conneau et al., 2020; Hu et al., 2020b). Under this setting, the models should be fine-tuned only on English training data but evaluated on all target languages. Moreover, for each pretrained model, only one model is used for all languages rather than selecting fine-tuned models separately. Details of the fine-tuning hyperparameters are"
2021.emnlp-main.125,Q19-1038,0,0.0291276,". We believe the better-aligned representations potentially improve the cross-lingual transferability. Furthermore, the results also indicate that our pre-training objective is more effective for training the encoder than M T5. 4.6 Word Alignment In addition to cross-lingual sentence retrieval that We analyze the cross-lingual representations pro- evaluates sentence-level representations, we also duced by our M T6 model. Following Chi et al. explore whether the representations produced by (2021a), we evaluate the representations on the M T6 are better-aligned at token-level. Thus, we Tatoeba (Artetxe and Schwenk, 2019) cross-lingual compare our M T6 with M T5 on the word alignsentence retrieval task. The test sets consist of 14 ment task, where the goal is to find corresponding English-centric language pairs covered by the par- word pairs in a translation pair. We use the hidden allel data in our experiments. Figure 4 illustrates vectors from the last encoder layer, and apply the the average accuracy@1 scores of cross-lingual SimAlign (Jalili Sabet et al., 2020) tool to obtain sentence retrieval. The scores are averaged over the resulting word alignments. Table 5 shows the 14 language pairs and both the dir"
2021.emnlp-main.125,2021.naacl-main.280,1,0.930284,"asks and the 1 Introduction training objective. We present three cross-lingual Multilingual pretrained language models, such as tasks for text-to-text Transformer pre-training, i.e., mBERT (Devlin et al., 2019), have attracted in- machine translation, translation pair span corrupcreasing attention. They not only improve the tion, and translation span corruption. In the transperformance on downstream multilingual NLP lation span corruption task, the model is trained to tasks (Conneau and Lample, 2019; Conneau et al., predict the text spans based on the input translation 2020; Liu et al., 2020; Chi et al., 2021c), but pair. The cross-lingual tasks encourage the model also show an impressive cross-lingual transferabil- to align representations of different languages. ity (Wu and Dredze, 2019; K et al., 2020; Hu et al., We also propose a new objective for text-to-text 2020b; Chi et al., 2021a). pre-training, called partially non-autoregressive Multilingual pretrained models are typically (PNAT) decoding. The PNAT objective divides the trained on multilingual unlabeled text with unsu- target sequence into several groups, and constrains pervised language modeling tasks, e.g., masked that the predictions"
2021.emnlp-main.125,2021.acl-long.265,1,0.835814,"asks and the 1 Introduction training objective. We present three cross-lingual Multilingual pretrained language models, such as tasks for text-to-text Transformer pre-training, i.e., mBERT (Devlin et al., 2019), have attracted in- machine translation, translation pair span corrupcreasing attention. They not only improve the tion, and translation span corruption. In the transperformance on downstream multilingual NLP lation span corruption task, the model is trained to tasks (Conneau and Lample, 2019; Conneau et al., predict the text spans based on the input translation 2020; Liu et al., 2020; Chi et al., 2021c), but pair. The cross-lingual tasks encourage the model also show an impressive cross-lingual transferabil- to align representations of different languages. ity (Wu and Dredze, 2019; K et al., 2020; Hu et al., We also propose a new objective for text-to-text 2020b; Chi et al., 2021a). pre-training, called partially non-autoregressive Multilingual pretrained models are typically (PNAT) decoding. The PNAT objective divides the trained on multilingual unlabeled text with unsu- target sequence into several groups, and constrains pervised language modeling tasks, e.g., masked that the predictions"
2021.emnlp-main.125,2020.tacl-1.30,0,0.0611303,"the pre-training hyperparameters are described in Appendix. 4.2 4.2.1 Results XTREME Cross-lingual Understanding To validate the performance of M T6, we evaluate the pretrained models on XTREME (Hu et al., 2020b), which is a widely used benchmark for cross-lingual understanding. Following M T5 (Xue et al., 2020), we consider six downstream tasks included by XTREME: the named entity recognition (NER) task on the WikiAnn (Pan et al., 2017; Rahimi et al., 2019) dataset in 40 languages, the question answering (QA) task on MLQA (Lewis et al., 2020b), XQuAD (Artetxe et al., 2020), and TyDiQA-GoldP (Clark et al., 2020), the cross-lingual natural language inference task on XNLI (Conneau et al., 2018), and crosslingual paraphrase adversaries on PAWS-X (Yang et al., 2019). The models are evaluated under the cross-lingual transfer setting (Conneau et al., 2020; Hu et al., 2020b). Under this setting, the models should be fine-tuned only on English training data but evaluated on all target languages. Moreover, for each pretrained model, only one model is used for all languages rather than selecting fine-tuned models separately. Details of the fine-tuning hyperparameters are described in Appendix. As shown in Tabl"
2021.emnlp-main.125,2020.acl-main.747,0,0.151214,"Missing"
2021.emnlp-main.125,2020.acl-main.703,0,0.382809,"put sentence or the hsepi token for the other situations. Classification The goal of the text classification task is to predict the label of a given text. Following T5 (Raffel et al., 2020), we directly use the label text as the output text sequence. We provide an example for the MNLI natural language infer- 4 Experiments ence task (Williams et al., 2018). Given an input sentence pair of “You have access to the facts .” 4.1 Setup and “The facts are accessible to you .”, the goal is Data Following previous work on cross-lingual to classify the input into the relationships of “en- pre-training (Conneau et al., 2020; Chi et al., tailment”, “contradiction”, or “neutral”. The input 2021a), we use the natural sentences from CCand target sequences are constructed as Net (Wenzek et al., 2019) in 94 languages Input: hbosi You have access to the facts. heosi for monolingual text-to-text tasks. For crossThe facts are accessible to you. heosi lingual text-to-text tasks, we use parallel corpora Output: hbosi entailment heosi of 14 English-centric language pairs, collected Since multi-task fine-tuning is not the focus of from MultiUN (Ziemski et al., 2016), IIT Bomthis work, we do not prepend a task prefix in the b"
2021.emnlp-main.125,D19-1252,0,0.0527437,"Missing"
2021.emnlp-main.125,2020.findings-emnlp.147,0,0.0582365,"Missing"
2021.emnlp-main.125,L18-1548,0,0.0349859,"i et al., tailment”, “contradiction”, or “neutral”. The input 2021a), we use the natural sentences from CCand target sequences are constructed as Net (Wenzek et al., 2019) in 94 languages Input: hbosi You have access to the facts. heosi for monolingual text-to-text tasks. For crossThe facts are accessible to you. heosi lingual text-to-text tasks, we use parallel corpora Output: hbosi entailment heosi of 14 English-centric language pairs, collected Since multi-task fine-tuning is not the focus of from MultiUN (Ziemski et al., 2016), IIT Bomthis work, we do not prepend a task prefix in the bay (Kunchukuttan et al., 2018), OPUS (Tiedeinput text. We also adopt a constrained decoding mann, 2012), and WikiMatrix (Schwenk et al., 1674 2019). Details of the pre-training data are described in Appendix. Training Details In the experiments, we consider the small-size Transformer model (Xue et al., 2020), with dmodel = 512, dff = 1, 024, 6 attention heads, and 8 layers for both the encoder and the decoder1 . We use the vocabulary provided by XLMR (Conneau et al., 2020), and extend it with 100 unique mask tokens for the span corruption tasks. We pretrain our M T6 for 0.5M steps with batches of 256 length-512 input seque"
2021.emnlp-main.125,2020.findings-emnlp.360,0,0.0333655,". Besides, under the setting with fewer training data, M T6 shows more improvements over M T5. Model es-en ru-en vi-en tr-en M T5 M T6 11.36 11.83 8.77 9.49 8.98 9.52 10.57 10.80 Table 3: ROUGE-2 scores on Wikilingua cross-lingual summarization. Results are averaged over three runs. Model XQuAD MLQA TyDiQA XNLI PAWS-X M T5 M T6 30.4 28.6 27.5 27.2 27.5 25.9 19.5 14.6 16.0 13.2 Table 4: The cross-lingual transfer gap scores on the XTREME tasks. A lower transfer gap score indicates better cross-lingual transferability. We use the EM scores to compute the gap scores for the QA tasks. Wikilingua (Ladhak et al., 2020) dataset containing passage-summary pairs in four language pairs. We fine-tune the models for 100K steps with a batch size of 32 and a learning rate of 0.0001. We use the greedy decoding for all evaluated models. The Cross-Lingual Summarization The cross- evaluation results are shown in Table 3, where lingual summarization task aims to generate M T6 outperforms M T5 on the test sets of four summaries in a different language. We use the language pairs. 1676 Averaged Accuracy 40 mT5 mT6 30 20 10 4 Layer 6 8 Figure 4: Evaluation results of different layers on Tatoeba cross-lingual sentence retrie"
2021.emnlp-main.125,2020.acl-main.653,0,0.538333,"procedure takes about 2.5 days on an Nvidia DGX-2 Station. Details of the pre-training hyperparameters are described in Appendix. 4.2 4.2.1 Results XTREME Cross-lingual Understanding To validate the performance of M T6, we evaluate the pretrained models on XTREME (Hu et al., 2020b), which is a widely used benchmark for cross-lingual understanding. Following M T5 (Xue et al., 2020), we consider six downstream tasks included by XTREME: the named entity recognition (NER) task on the WikiAnn (Pan et al., 2017; Rahimi et al., 2019) dataset in 40 languages, the question answering (QA) task on MLQA (Lewis et al., 2020b), XQuAD (Artetxe et al., 2020), and TyDiQA-GoldP (Clark et al., 2020), the cross-lingual natural language inference task on XNLI (Conneau et al., 2018), and crosslingual paraphrase adversaries on PAWS-X (Yang et al., 2019). The models are evaluated under the cross-lingual transfer setting (Conneau et al., 2020; Hu et al., 2020b). Under this setting, the models should be fine-tuned only on English training data but evaluated on all target languages. Moreover, for each pretrained model, only one model is used for all languages rather than selecting fine-tuned models separately. Details of the"
2021.emnlp-main.125,W04-1013,0,0.0271001,"ion. RG is short for ROUGE. Results of XLM and XNLG are taken from (Chi et al., 2020). Results of M T5 and M T6 are averaged over three runs. lines as the input documents and summaries, respectively. The dataset consists of examples in the languages of English, French, and Chinese. For each language, it contains 500K, 5K, and 5K examples for the training, validation, and test, respectively. We fine-tune the models for 20 epochs with a batch size of 32 and a learning rate of 0.00001. During decoding, we use the greedy decoding for all evaluated models. As shown in Table 2, we report the ROUGE (Lin, 2004) scores of the models on Gigaword multilingual abstractive summarization. We observe that M T6 consistently outperforms M T5 on all the three target languages. Comparing with the XLM (Conneau and Lample, 2019) and XNLG (Chi et al., 2020) models with 800M parameters, our M T6 model achieves a similar performance with only 300M parameters. Besides, under the setting with fewer training data, M T6 shows more improvements over M T5. Model es-en ru-en vi-en tr-en M T5 M T6 11.36 11.83 8.77 9.49 8.98 9.52 10.57 10.80 Table 3: ROUGE-2 scores on Wikilingua cross-lingual summarization. Results are aver"
2021.emnlp-main.125,P19-1015,0,0.0907957,"optimizer (Kingma and Ba, 2015) with a linear learning rate scheduler. The pre-training procedure takes about 2.5 days on an Nvidia DGX-2 Station. Details of the pre-training hyperparameters are described in Appendix. 4.2 4.2.1 Results XTREME Cross-lingual Understanding To validate the performance of M T6, we evaluate the pretrained models on XTREME (Hu et al., 2020b), which is a widely used benchmark for cross-lingual understanding. Following M T5 (Xue et al., 2020), we consider six downstream tasks included by XTREME: the named entity recognition (NER) task on the WikiAnn (Pan et al., 2017; Rahimi et al., 2019) dataset in 40 languages, the question answering (QA) task on MLQA (Lewis et al., 2020b), XQuAD (Artetxe et al., 2020), and TyDiQA-GoldP (Clark et al., 2020), the cross-lingual natural language inference task on XNLI (Conneau et al., 2018), and crosslingual paraphrase adversaries on PAWS-X (Yang et al., 2019). The models are evaluated under the cross-lingual transfer setting (Conneau et al., 2020; Hu et al., 2020b). Under this setting, the models should be fine-tuned only on English training data but evaluated on all target languages. Moreover, for each pretrained model, only one model is used"
2021.emnlp-main.125,tiedemann-2012-parallel,0,0.0711811,"Missing"
2021.emnlp-main.125,N18-1101,0,0.0159465,"e hsepi tag means the end of entity span. We use the following constrained decoding rules: (1) The model should decode entity tags or the end-of-sentence tag (heosi) after a hbosi token or a hsepi token; (2) Otherwise, the model should decode the tokens from the input sentence or the hsepi token for the other situations. Classification The goal of the text classification task is to predict the label of a given text. Following T5 (Raffel et al., 2020), we directly use the label text as the output text sequence. We provide an example for the MNLI natural language infer- 4 Experiments ence task (Williams et al., 2018). Given an input sentence pair of “You have access to the facts .” 4.1 Setup and “The facts are accessible to you .”, the goal is Data Following previous work on cross-lingual to classify the input into the relationships of “en- pre-training (Conneau et al., 2020; Chi et al., tailment”, “contradiction”, or “neutral”. The input 2021a), we use the natural sentences from CCand target sequences are constructed as Net (Wenzek et al., 2019) in 94 languages Input: hbosi You have access to the facts. heosi for monolingual text-to-text tasks. For crossThe facts are accessible to you. heosi lingual text"
2021.emnlp-main.260,D17-1209,0,0.0305201,"Missing"
2021.emnlp-main.260,W18-5431,0,0.0413782,"Missing"
2021.emnlp-main.260,P16-1162,0,0.0415434,"Missing"
2021.emnlp-main.260,N18-2074,0,0.0355485,"Missing"
2021.emnlp-main.260,2021.acl-long.310,0,0.0232495,"Missing"
2021.emnlp-main.260,P18-1249,0,0.0665983,"Missing"
2021.emnlp-main.260,P18-1117,0,0.0423773,"Missing"
2021.emnlp-main.260,P17-4012,0,0.0205864,"Missing"
2021.emnlp-main.260,D18-1475,0,0.067311,"Missing"
2021.findings-acl.59,2020.acl-main.499,0,0.0821763,"l. ∗ Corresponding author However, the most current datasets on qualitative reasoning are much smaller than standard Question Answering datasets, making the task challenging and receiving limited attention. At present, in the two mainstream qualitative relationship question datasets, QuaRel (Tafjord et al., 2019a) and QuaRTz (Tafjord et al., 2019b), the existing methods can be divided into two categories, symbolic reasoning based on a semantic parser (Krishnamurthy et al., 2017; Tafjord et al., 2019a) and a “black-box” model based on representations (Mitra et al., 2019; Tafjord et al., 2019b; Asai and Hajishirzi, 2020; Mitra et al., 2020). The two types of methods have their advantages and disadvantages. On one hand, symbolic reasoning provides solid explanations for the problemsolving process, but existing semantic parsers are 664 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 664–675 August 1–6, 2021. ©2021 Association for Computational Linguistics trained to translate natural-language sentences into a task-specific logical representation that naturally increases the demand for additional annotated data and has limited generalization capability. On the other, approaches"
2021.findings-acl.59,D18-1241,0,0.0135291,"g processes. Experiments on two qualitative reasoning question answering datasets, QuaRTz and QuaRel, show our methods’ effectiveness and generalization capability, and the intermediate outputs provided by the modules make the reasoning process interpretable. 1 Introduction Figure 1: Examples from QuaRTz dataset. Qualitative relationships abound in our world, especially in science, economics, and medicine. Since Question Answering (QA) has been significantly developed in recent years, various challenging datasets have been proposed (Lai et al., 2017; Rajpurkar et al., 2018; Yang et al., 2018; Choi et al., 2018; Dua et al., 2019), and one often encounters the context and questions about qualitative relationships. Figure 1 shows an example that requires reasoning about the qualitative relationship between the mass and the gravitational pull, where the knowledge sentence states that the mass is positively correlated with the gravitational pull, and the questions describe different scenarios that test the flexible application of the knowledge. Therefore, understanding the qualitative relationships behind the context and applying the qualitative textual knowledge for reasoning is essential. ∗ Correspond"
2021.findings-acl.59,D19-5808,0,0.0144937,"retability. Neural Network Modules: These modules were first adopted in the Visual Question Answering (VQA) domain (Johnson et al., 2017; Andreas et al., 2016; Hu et al., 2017). Due to the interpretable, modular, and inherently compositional nature of neural network modules, they were further extended to natural language (Gupta and Lewis, 2018; Jiang and Bansal, 2019; Jiang et al., 2019). Gupta et al. (2019) extend neural module networks to answer compositional questions. Ren et al. (2020) and Liu and Gardner (2020) further introduce neural network modules on one complex reasoning task ROPES (Lin et al., 2019). In this work, we explore the effectiveness of neural network modules on a qualitative reasoning task. Related Work Qualitative Reasoning: This type of reasoning is an indispensable part of artificial intelligence. Forbus (1984) and Weld and De Kleer 665 3 Method Our work solves the question through a three-step process illustrated in Figure 2a: 1. The Contextual Encoding component encodes the concatenation of knowledge, the question, and options into contextual representations. 2. The Reasoning component arranges different reasoning chains according to the type of problem, where Prediction a"
2021.findings-acl.59,2020.emnlp-main.245,0,0.0150775,"0). Our work enjoys the performance improvement that a neural network brings but also provides interpretability. Neural Network Modules: These modules were first adopted in the Visual Question Answering (VQA) domain (Johnson et al., 2017; Andreas et al., 2016; Hu et al., 2017). Due to the interpretable, modular, and inherently compositional nature of neural network modules, they were further extended to natural language (Gupta and Lewis, 2018; Jiang and Bansal, 2019; Jiang et al., 2019). Gupta et al. (2019) extend neural module networks to answer compositional questions. Ren et al. (2020) and Liu and Gardner (2020) further introduce neural network modules on one complex reasoning task ROPES (Lin et al., 2019). In this work, we explore the effectiveness of neural network modules on a qualitative reasoning task. Related Work Qualitative Reasoning: This type of reasoning is an indispensable part of artificial intelligence. Forbus (1984) and Weld and De Kleer 665 3 Method Our work solves the question through a three-step process illustrated in Figure 2a: 1. The Contextual Encoding component encodes the concatenation of knowledge, the question, and options into contextual representations. 2. The Reasoning co"
2021.findings-acl.59,2021.ccl-1.108,0,0.0235843,"Missing"
2021.findings-acl.59,2020.spnlp-1.12,0,0.16552,"owever, the most current datasets on qualitative reasoning are much smaller than standard Question Answering datasets, making the task challenging and receiving limited attention. At present, in the two mainstream qualitative relationship question datasets, QuaRel (Tafjord et al., 2019a) and QuaRTz (Tafjord et al., 2019b), the existing methods can be divided into two categories, symbolic reasoning based on a semantic parser (Krishnamurthy et al., 2017; Tafjord et al., 2019a) and a “black-box” model based on representations (Mitra et al., 2019; Tafjord et al., 2019b; Asai and Hajishirzi, 2020; Mitra et al., 2020). The two types of methods have their advantages and disadvantages. On one hand, symbolic reasoning provides solid explanations for the problemsolving process, but existing semantic parsers are 664 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 664–675 August 1–6, 2021. ©2021 Association for Computational Linguistics trained to translate natural-language sentences into a task-specific logical representation that naturally increases the demand for additional annotated data and has limited generalization capability. On the other, approaches based on pre-trained"
2021.findings-acl.59,D19-1608,0,0.225173,"e mass is positively correlated with the gravitational pull, and the questions describe different scenarios that test the flexible application of the knowledge. Therefore, understanding the qualitative relationships behind the context and applying the qualitative textual knowledge for reasoning is essential. ∗ Corresponding author However, the most current datasets on qualitative reasoning are much smaller than standard Question Answering datasets, making the task challenging and receiving limited attention. At present, in the two mainstream qualitative relationship question datasets, QuaRel (Tafjord et al., 2019a) and QuaRTz (Tafjord et al., 2019b), the existing methods can be divided into two categories, symbolic reasoning based on a semantic parser (Krishnamurthy et al., 2017; Tafjord et al., 2019a) and a “black-box” model based on representations (Mitra et al., 2019; Tafjord et al., 2019b; Asai and Hajishirzi, 2020; Mitra et al., 2020). The two types of methods have their advantages and disadvantages. On one hand, symbolic reasoning provides solid explanations for the problemsolving process, but existing semantic parsers are 664 Findings of the Association for Computational Linguistics: ACL-IJCNLP"
2021.findings-acl.59,D18-1259,0,0.0310722,"te the two reasoning processes. Experiments on two qualitative reasoning question answering datasets, QuaRTz and QuaRel, show our methods’ effectiveness and generalization capability, and the intermediate outputs provided by the modules make the reasoning process interpretable. 1 Introduction Figure 1: Examples from QuaRTz dataset. Qualitative relationships abound in our world, especially in science, economics, and medicine. Since Question Answering (QA) has been significantly developed in recent years, various challenging datasets have been proposed (Lai et al., 2017; Rajpurkar et al., 2018; Yang et al., 2018; Choi et al., 2018; Dua et al., 2019), and one often encounters the context and questions about qualitative relationships. Figure 1 shows an example that requires reasoning about the qualitative relationship between the mass and the gravitational pull, where the knowledge sentence states that the mass is positively correlated with the gravitational pull, and the questions describe different scenarios that test the flexible application of the knowledge. Therefore, understanding the qualitative relationships behind the context and applying the qualitative textual knowledge for reasoning is esse"
2021.findings-acl.59,2020.emnlp-main.548,1,0.73562,"020; Mitra et al., 2020). Our work enjoys the performance improvement that a neural network brings but also provides interpretability. Neural Network Modules: These modules were first adopted in the Visual Question Answering (VQA) domain (Johnson et al., 2017; Andreas et al., 2016; Hu et al., 2017). Due to the interpretable, modular, and inherently compositional nature of neural network modules, they were further extended to natural language (Gupta and Lewis, 2018; Jiang and Bansal, 2019; Jiang et al., 2019). Gupta et al. (2019) extend neural module networks to answer compositional questions. Ren et al. (2020) and Liu and Gardner (2020) further introduce neural network modules on one complex reasoning task ROPES (Lin et al., 2019). In this work, we explore the effectiveness of neural network modules on a qualitative reasoning task. Related Work Qualitative Reasoning: This type of reasoning is an indispensable part of artificial intelligence. Forbus (1984) and Weld and De Kleer 665 3 Method Our work solves the question through a three-step process illustrated in Figure 2a: 1. The Contextual Encoding component encodes the concatenation of knowledge, the question, and options into contextual represent"
2021.findings-acl.64,L16-1153,0,0.0200823,"le 1: Two examples of Chinese spelling errors and their candidate corrections. “Sent./Cand./Trans.” are short for sentence/candidates/translation respectively. The wrong/candidate/correct characters with their pronunciation and translation are in red/orange/blue color. Introduction The Chinese Spell Checking (CSC) task aims to identify erroneous characters and generate candidates for correction. It has attracted much research attention, due to its fundamental and wide applications such as search query correction (Martins and Silva, 2004; Gao et al., 2010), optical character recognition (OCR) (Afli et al., 2016), automatic essay scoring (Dong and Zhang, 2016). Recently, rapid progress (Zhang et al., 2020; Cheng et al., 2020) has been made on this task, because of the success of large pretrained language models (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019). In alphabetic languages such as English, spelling errors often occur owing to one or more wrong ∗ 晚饭后他递给我一平(p´ıng, flat)红酒。 Heng-Da Xu and Zhongli Li contributed equally. Work is done during internship at Tencent Cloud Xiaowei. Qingyu Zhou is the corresponding author. 1 Code and model are available at https://github. com/DaDaMrX/ReaLiS"
2021.findings-acl.64,W18-6402,0,0.026713,"tic and graphic information of the character itself. of Chinese characters. In this work, we discard the predefined confusion set and directly use the multimodal information to discover the subtle similarity relationship between all Chinese characters. 2.2 Multimodal Learning There has been much research to integrate information from different modalities to achieve better performance. Tasks such as Multimodal Sentiment Analysis (Zadeh et al., 2016; Zhang et al., 2019), Visual Question Answering (Antol et al., 2015; Chao et al., 2018) and Multimodal Machine Translation (Hitschler et al., 2016; Barrault et al., 2018) have made much progress. Recently, multimodal pretraining models have been proposed, such as VL-BERT (Su et al., 2020), Unicoder-VL (Li et al., 2020), and LXMERT (Tan and Bansal, 2019). In order to incorporate the visual information of Chinese characters into language models, Meng et al. (2019) design a Tianzige-CNN to facilitate some NLP tasks, such as named entity recognition and sentence classification. To the best of our knowledge, this paper is the first work to leverage multimodal information to tackle the CSC task. 3 The R EA L I S E Model In this section, we introduce the R EA L I S E"
2021.findings-acl.64,W15-3109,0,0.170275,"se to leverage phonetic and graphic information of Chinese characters besides textual semantics for the CSC task; (ii) we introduce the selective fusion mechanism to integrate multimodal information; (iii) we propose acoustic and visual pretraining tasks to further boost the model performance; (iv) to the best of our knowledge, the proposed R EA L I S E model achieves the best results on the SIGHAN CSC benchmarks. 2 2.1 Related Work Chinese Spell Checking The CSC task is to detect and correct spelling errors in Chinese sentences. Early works design various rules to deal with different errors (Chang et al., 2015; Chu and Lin, 2015). Next, traditional machine learning algorithms are brought to this field, such as Conditional Random Field and Hidden Markov Model (Wang and Liao, 2015; Zhang et al., 2015). Then, neural-based methods have made great progress in CSC. Wang et al. (2018) treat the CSC task as a sequence labeling problem, and use a bidirectional LSTM to predict the correct characters. With the great success of large pretrained language models (e.g., BERT (Devlin et al., 2019)), Hong et al. (2019) propose the FASpell model, which use a BERT-based denoising autoencoder to generate candidate cha"
2021.findings-acl.64,N18-1040,0,0.0256444,"g, light), we need not only the contextual text information, but also the phonetic and graphic information of the character itself. of Chinese characters. In this work, we discard the predefined confusion set and directly use the multimodal information to discover the subtle similarity relationship between all Chinese characters. 2.2 Multimodal Learning There has been much research to integrate information from different modalities to achieve better performance. Tasks such as Multimodal Sentiment Analysis (Zadeh et al., 2016; Zhang et al., 2019), Visual Question Answering (Antol et al., 2015; Chao et al., 2018) and Multimodal Machine Translation (Hitschler et al., 2016; Barrault et al., 2018) have made much progress. Recently, multimodal pretraining models have been proposed, such as VL-BERT (Su et al., 2020), Unicoder-VL (Li et al., 2020), and LXMERT (Tan and Bansal, 2019). In order to incorporate the visual information of Chinese characters into language models, Meng et al. (2019) design a Tianzige-CNN to facilitate some NLP tasks, such as named entity recognition and sentence classification. To the best of our knowledge, this paper is the first work to leverage multimodal information to tackle th"
2021.findings-acl.64,2020.acl-main.81,0,0.124992,"sentence/candidates/translation respectively. The wrong/candidate/correct characters with their pronunciation and translation are in red/orange/blue color. Introduction The Chinese Spell Checking (CSC) task aims to identify erroneous characters and generate candidates for correction. It has attracted much research attention, due to its fundamental and wide applications such as search query correction (Martins and Silva, 2004; Gao et al., 2010), optical character recognition (OCR) (Afli et al., 2016), automatic essay scoring (Dong and Zhang, 2016). Recently, rapid progress (Zhang et al., 2020; Cheng et al., 2020) has been made on this task, because of the success of large pretrained language models (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019). In alphabetic languages such as English, spelling errors often occur owing to one or more wrong ∗ 晚饭后他递给我一平(p´ıng, flat)红酒。 Heng-Da Xu and Zhongli Li contributed equally. Work is done during internship at Tencent Cloud Xiaowei. Qingyu Zhou is the corresponding author. 1 Code and model are available at https://github. com/DaDaMrX/ReaLiSe. characters, resulting in the written word not in the dictionary problem (Tachibana and Komachi, 2016). However,"
2021.findings-acl.64,D14-1179,0,0.0180473,"Missing"
2021.findings-acl.64,W15-3121,0,0.0547057,"tic and graphic information of Chinese characters besides textual semantics for the CSC task; (ii) we introduce the selective fusion mechanism to integrate multimodal information; (iii) we propose acoustic and visual pretraining tasks to further boost the model performance; (iv) to the best of our knowledge, the proposed R EA L I S E model achieves the best results on the SIGHAN CSC benchmarks. 2 2.1 Related Work Chinese Spell Checking The CSC task is to detect and correct spelling errors in Chinese sentences. Early works design various rules to deal with different errors (Chang et al., 2015; Chu and Lin, 2015). Next, traditional machine learning algorithms are brought to this field, such as Conditional Random Field and Hidden Markov Model (Wang and Liao, 2015; Zhang et al., 2015). Then, neural-based methods have made great progress in CSC. Wang et al. (2018) treat the CSC task as a sequence labeling problem, and use a bidirectional LSTM to predict the correct characters. With the great success of large pretrained language models (e.g., BERT (Devlin et al., 2019)), Hong et al. (2019) propose the FASpell model, which use a BERT-based denoising autoencoder to generate candidate characters and uses som"
2021.findings-acl.64,C10-2085,0,0.836206,"written word not in the dictionary problem (Tachibana and Komachi, 2016). However, Chinese characters are valid if they can be typed in computer systems, which causes that the spelling errors are de facto misused characters in the context of computer-based language processing. Considering the formation of Chinese characters, a few of them were originally pictograms or phono-semantic compound characters (Jerry, 1988). Thus, in Chinese, the spelling errors are not only the misused characters with confusing semantic meaning, but also the characters which are phonetically or graphically similar (Liu et al., 2010, 2011). Table 1 shows two examples of Chinese spelling error. In the first example, phonetic information of “平” (flat) is needed to get the correct character “瓶” (bottle) since they share the same pronunciation “p´ıng”. The second example needs not only phonetic, but also graphic information of the erroneous character “轻” (light). The 716 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 716–728 August 1–6, 2021. ©2021 Association for Computational Linguistics correct one, “经” (go), has the same right radical as “轻” and similar pronunciation (“q¯ıng” and “j¯ıng"
2021.findings-acl.64,L16-1060,0,0.0135743,"Zhang et al., 2020; Cheng et al., 2020) has been made on this task, because of the success of large pretrained language models (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019). In alphabetic languages such as English, spelling errors often occur owing to one or more wrong ∗ 晚饭后他递给我一平(p´ıng, flat)红酒。 Heng-Da Xu and Zhongli Li contributed equally. Work is done during internship at Tencent Cloud Xiaowei. Qingyu Zhou is the corresponding author. 1 Code and model are available at https://github. com/DaDaMrX/ReaLiSe. characters, resulting in the written word not in the dictionary problem (Tachibana and Komachi, 2016). However, Chinese characters are valid if they can be typed in computer systems, which causes that the spelling errors are de facto misused characters in the context of computer-based language processing. Considering the formation of Chinese characters, a few of them were originally pictograms or phono-semantic compound characters (Jerry, 1988). Thus, in Chinese, the spelling errors are not only the misused characters with confusing semantic meaning, but also the characters which are phonetically or graphically similar (Liu et al., 2010, 2011). Table 1 shows two examples of Chinese spelling e"
2021.findings-acl.64,D19-1514,0,0.0270437,"the subtle similarity relationship between all Chinese characters. 2.2 Multimodal Learning There has been much research to integrate information from different modalities to achieve better performance. Tasks such as Multimodal Sentiment Analysis (Zadeh et al., 2016; Zhang et al., 2019), Visual Question Answering (Antol et al., 2015; Chao et al., 2018) and Multimodal Machine Translation (Hitschler et al., 2016; Barrault et al., 2018) have made much progress. Recently, multimodal pretraining models have been proposed, such as VL-BERT (Su et al., 2020), Unicoder-VL (Li et al., 2020), and LXMERT (Tan and Bansal, 2019). In order to incorporate the visual information of Chinese characters into language models, Meng et al. (2019) design a Tianzige-CNN to facilitate some NLP tasks, such as named entity recognition and sentence classification. To the best of our knowledge, this paper is the first work to leverage multimodal information to tackle the CSC task. 3 The R EA L I S E Model In this section, we introduce the R EA L I S E model, which utilizes the semantic, phonetic, and graphic information to distinguish the similarities of Chinese characters and correct the spelling errors. As shown in Figure 1, multi"
2021.findings-acl.64,W15-3120,0,0.132381,"75.8 77.2 82.1 60.5 77.8 73.1 77.7 78.3 84.6 60.5 71.6 72.7 77.8 52.1 66.2 74.6 75.4 81.0 BERT† R EA L I S E† 77.0 82.7 85.0 88.6 77.0 82.5 80.8 85.4 75.2 81.4 83.0 87.2 75.2 81.2 78.9 84.1 Sequence Labeling (Wang et al., 2018) FASpell (Hong et al., 2019) SpellGCN (Cheng et al., 2020) 70.0 - 51.9 61.0 65.1 66.2 53.5 69.5 58.2 57.0 67.2 69.3 - 59.4 63.1 52.0 67.2 56.1 55.4 65.3 BERT R EA L I S E 75.7 78.4 64.5 67.8 68.6 71.5 66.5 69.6 74.6 77.7 62.4 66.3 66.3 70.0 64.3 68.1 KUAS (Chang et al., 2015) NTOU (Chu and Lin, 2015) NCTU-NTUT (Wang and Liao, 2015) HanSpeller++ (Zhang et al., 2015) LMC (Xie et al., 2015) 53.2 42.2 60.1 70.1 54.6 57.5 42.2 71.7 80.3 63.8 24.6 41.8 33.6 53.3 21.5 34.4 42.0 45.7 64.0 32.1 51.5 39.0 56.4 69.2 52.3 53.7 38.1 66.3 79.7 57.9 21.1 35.2 26.1 51.5 16.7 30.3 36.6 37.5 62.5 26.0 Sequence Labeling (Wang et al., 2018) FASpell (Hong et al., 2019) Soft-Masked BERT (Zhang et al., 2020) SpellGCN (Cheng et al., 2020) 74.2 80.9 - 56.6 67.6 73.7 74.8 69.4 60.0 73.2 80.7 62.3 63.5 73.5 77.7 73.7 77.4 - 66.6 66.7 72.1 59.1 66.2 77.7 57.1 62.6 66.4 75.9 BERT R EA L I S E 82.4 84.7 74.2 77.3 78.0 81.3 76.1 79.3 81.0 84.0 71.6 75.9 75.3 79.9 73.4 77.8 Table 3: The performance of our m"
2021.findings-acl.64,W14-6835,0,0.217193,"he CSC task as a sequence labeling problem, and use a bidirectional LSTM to predict the correct characters. With the great success of large pretrained language models (e.g., BERT (Devlin et al., 2019)), Hong et al. (2019) propose the FASpell model, which use a BERT-based denoising autoencoder to generate candidate characters and uses some empirical measures to select the most likely ones. Besides, the Soft-Masked BERT model (Zhang et al., 2020) leverages a cascading architecture where GRU is used to detect the erroneous positions and BERT is used to predict correct characters. Previous works (Yu and Li, 2014; Wang et al., 2019; Cheng et al., 2020) using handcrafted Chinese character confusion set (Lee et al., 2019) aim to correct the errors by discovering the similarity of the easily-misused characters. Wang et al. (2019) leverage the pointer network (Vinyals et al., 2015) by picking the correct character from the confusion set. Cheng et al. (2020) propose a SpellGCN model which models the character similarity through Graph Convolution Network (GCNs) (Kipf and Welling, 2016) on the confusion set. However, the character confusion set is predefined and fixed, which cannot cover all the similarity r"
2021.findings-acl.64,D18-1273,0,0.419571,"he model performance; (iv) to the best of our knowledge, the proposed R EA L I S E model achieves the best results on the SIGHAN CSC benchmarks. 2 2.1 Related Work Chinese Spell Checking The CSC task is to detect and correct spelling errors in Chinese sentences. Early works design various rules to deal with different errors (Chang et al., 2015; Chu and Lin, 2015). Next, traditional machine learning algorithms are brought to this field, such as Conditional Random Field and Hidden Markov Model (Wang and Liao, 2015; Zhang et al., 2015). Then, neural-based methods have made great progress in CSC. Wang et al. (2018) treat the CSC task as a sequence labeling problem, and use a bidirectional LSTM to predict the correct characters. With the great success of large pretrained language models (e.g., BERT (Devlin et al., 2019)), Hong et al. (2019) propose the FASpell model, which use a BERT-based denoising autoencoder to generate candidate characters and uses some empirical measures to select the most likely ones. Besides, the Soft-Masked BERT model (Zhang et al., 2020) leverages a cascading architecture where GRU is used to detect the erroneous positions and BERT is used to predict correct characters. Previous"
2021.findings-acl.64,P19-1578,0,0.691323,"in Chinese sentences. Early works design various rules to deal with different errors (Chang et al., 2015; Chu and Lin, 2015). Next, traditional machine learning algorithms are brought to this field, such as Conditional Random Field and Hidden Markov Model (Wang and Liao, 2015; Zhang et al., 2015). Then, neural-based methods have made great progress in CSC. Wang et al. (2018) treat the CSC task as a sequence labeling problem, and use a bidirectional LSTM to predict the correct characters. With the great success of large pretrained language models (e.g., BERT (Devlin et al., 2019)), Hong et al. (2019) propose the FASpell model, which use a BERT-based denoising autoencoder to generate candidate characters and uses some empirical measures to select the most likely ones. Besides, the Soft-Masked BERT model (Zhang et al., 2020) leverages a cascading architecture where GRU is used to detect the erroneous positions and BERT is used to predict correct characters. Previous works (Yu and Li, 2014; Wang et al., 2019; Cheng et al., 2020) using handcrafted Chinese character confusion set (Lee et al., 2019) aim to correct the errors by discovering the similarity of the easily-misused characters. Wang e"
2021.findings-acl.64,W15-3108,0,0.134341,"rate multimodal information; (iii) we propose acoustic and visual pretraining tasks to further boost the model performance; (iv) to the best of our knowledge, the proposed R EA L I S E model achieves the best results on the SIGHAN CSC benchmarks. 2 2.1 Related Work Chinese Spell Checking The CSC task is to detect and correct spelling errors in Chinese sentences. Early works design various rules to deal with different errors (Chang et al., 2015; Chu and Lin, 2015). Next, traditional machine learning algorithms are brought to this field, such as Conditional Random Field and Hidden Markov Model (Wang and Liao, 2015; Zhang et al., 2015). Then, neural-based methods have made great progress in CSC. Wang et al. (2018) treat the CSC task as a sequence labeling problem, and use a bidirectional LSTM to predict the correct characters. With the great success of large pretrained language models (e.g., BERT (Devlin et al., 2019)), Hong et al. (2019) propose the FASpell model, which use a BERT-based denoising autoencoder to generate candidate characters and uses some empirical measures to select the most likely ones. Besides, the Soft-Masked BERT model (Zhang et al., 2020) leverages a cascading architecture where G"
2021.findings-acl.64,2020.emnlp-demos.6,0,0.0307589,"Missing"
2021.findings-acl.64,2020.acl-main.82,0,0.489581,"ans.” are short for sentence/candidates/translation respectively. The wrong/candidate/correct characters with their pronunciation and translation are in red/orange/blue color. Introduction The Chinese Spell Checking (CSC) task aims to identify erroneous characters and generate candidates for correction. It has attracted much research attention, due to its fundamental and wide applications such as search query correction (Martins and Silva, 2004; Gao et al., 2010), optical character recognition (OCR) (Afli et al., 2016), automatic essay scoring (Dong and Zhang, 2016). Recently, rapid progress (Zhang et al., 2020; Cheng et al., 2020) has been made on this task, because of the success of large pretrained language models (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019). In alphabetic languages such as English, spelling errors often occur owing to one or more wrong ∗ 晚饭后他递给我一平(p´ıng, flat)红酒。 Heng-Da Xu and Zhongli Li contributed equally. Work is done during internship at Tencent Cloud Xiaowei. Qingyu Zhou is the corresponding author. 1 Code and model are available at https://github. com/DaDaMrX/ReaLiSe. characters, resulting in the written word not in the dictionary problem (Tachibana and Koma"
2021.findings-acl.64,W15-3107,0,0.0861862,"mation; (iii) we propose acoustic and visual pretraining tasks to further boost the model performance; (iv) to the best of our knowledge, the proposed R EA L I S E model achieves the best results on the SIGHAN CSC benchmarks. 2 2.1 Related Work Chinese Spell Checking The CSC task is to detect and correct spelling errors in Chinese sentences. Early works design various rules to deal with different errors (Chang et al., 2015; Chu and Lin, 2015). Next, traditional machine learning algorithms are brought to this field, such as Conditional Random Field and Hidden Markov Model (Wang and Liao, 2015; Zhang et al., 2015). Then, neural-based methods have made great progress in CSC. Wang et al. (2018) treat the CSC task as a sequence labeling problem, and use a bidirectional LSTM to predict the correct characters. With the great success of large pretrained language models (e.g., BERT (Devlin et al., 2019)), Hong et al. (2019) propose the FASpell model, which use a BERT-based denoising autoencoder to generate candidate characters and uses some empirical measures to select the most likely ones. Besides, the Soft-Masked BERT model (Zhang et al., 2020) leverages a cascading architecture where GRU is used to detect"
2021.findings-acl.66,D18-1015,0,0.0665065,"Missing"
2021.naacl-main.280,D19-1252,1,0.753364,"Missing"
2021.naacl-main.280,L18-1548,0,0.0642253,"oss-lingual understanding tasks. We also conduct ablation studies to understand the major components of I NFOXLM. 4.1 Setup Corpus We use the same pre-training corpora as previous models (Conneau et al., 2020a; Conneau and Lample, 2019). Specifically, we reconstruct CC-100 (Conneau et al., 2020a) for MMLM, which remains 94 languages by filtering the language code larger than 0.1GB. Following (Conneau and Lample, 2019), for the TLM and X L C O tasks, we employ 14 language pairs of parallel data that involves English. We collect translation pairs from MultiUN (Ziemski et al., 2016), IIT Bombay (Kunchukuttan et al., 2018), OPUS (Tiedemann, 2012), and WikiMatrix (Schwenk et al., 2019). The size of parallel corpora is about 42GB. More details about the pre-training data are described in the appendix. Model Size We follow the model configurations of XLM-R (Conneau et al., 2020a). For the Transformer (Vaswani et al., 2017) architecture, we use 12 layers and 768 hidden states for I NFOXLM (i.e., base size), and 24 layers and 1,024 hidden states for I NFOXLM LARGE (i.e., large size). 4.2 Evaluation Cross-Lingual Natural Language Inference The Cross-Lingual Natural Language Inference corpus (XNLI; Conneau et al. 2018"
2021.naacl-main.280,2020.acl-main.653,0,0.715277,"h monolingual and parallel corpora. Contribution during internship at Microsoft Research. Contact person: Li Dong and Furu Wei. We jointly train I NFOXLM with MMLM, TLM 3576 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3576–3588 June 6–11, 2021. ©2021 Association for Computational Linguistics and X L C O. We conduct extensive experiments on several cross-lingual understanding tasks, including cross-lingual natural language inference (Conneau et al., 2018), cross-lingual question answering (Lewis et al., 2020), and cross-lingual sentence retrieval (Artetxe and Schwenk, 2019). Experimental results show that I NFOXLM outperforms strong baselines on all the benchmarks. Moreover, the analysis indicates that I NFOXLM achieves better cross-lingual transferability. 2 2.1 Related Work between the sampled positive and negative pairs. In addition to the estimators, various view pairs are employed in these methods. The view pair can be the local and global features of an image (Hjelm et al., 2019; Bachman et al., 2019), the random data augmentations of the same image (Tian et al., 2019; He et al., 2020; Chen"
2021.naacl-main.280,L16-1561,0,0.0179259,"OXLM with previous work on three cross-lingual understanding tasks. We also conduct ablation studies to understand the major components of I NFOXLM. 4.1 Setup Corpus We use the same pre-training corpora as previous models (Conneau et al., 2020a; Conneau and Lample, 2019). Specifically, we reconstruct CC-100 (Conneau et al., 2020a) for MMLM, which remains 94 languages by filtering the language code larger than 0.1GB. Following (Conneau and Lample, 2019), for the TLM and X L C O tasks, we employ 14 language pairs of parallel data that involves English. We collect translation pairs from MultiUN (Ziemski et al., 2016), IIT Bombay (Kunchukuttan et al., 2018), OPUS (Tiedemann, 2012), and WikiMatrix (Schwenk et al., 2019). The size of parallel corpora is about 42GB. More details about the pre-training data are described in the appendix. Model Size We follow the model configurations of XLM-R (Conneau et al., 2020a). For the Transformer (Vaswani et al., 2017) architecture, we use 12 layers and 768 hidden states for I NFOXLM (i.e., base size), and 24 layers and 1,024 hidden states for I NFOXLM LARGE (i.e., large size). 4.2 Evaluation Cross-Lingual Natural Language Inference The Cross-Lingual Natural Language Inf"
2021.naacl-main.280,2020.findings-emnlp.147,0,0.110105,"Missing"
2021.naacl-main.280,tiedemann-2012-parallel,0,0.0390072,"e also conduct ablation studies to understand the major components of I NFOXLM. 4.1 Setup Corpus We use the same pre-training corpora as previous models (Conneau et al., 2020a; Conneau and Lample, 2019). Specifically, we reconstruct CC-100 (Conneau et al., 2020a) for MMLM, which remains 94 languages by filtering the language code larger than 0.1GB. Following (Conneau and Lample, 2019), for the TLM and X L C O tasks, we employ 14 language pairs of parallel data that involves English. We collect translation pairs from MultiUN (Ziemski et al., 2016), IIT Bombay (Kunchukuttan et al., 2018), OPUS (Tiedemann, 2012), and WikiMatrix (Schwenk et al., 2019). The size of parallel corpora is about 42GB. More details about the pre-training data are described in the appendix. Model Size We follow the model configurations of XLM-R (Conneau et al., 2020a). For the Transformer (Vaswani et al., 2017) architecture, we use 12 layers and 768 hidden states for I NFOXLM (i.e., base size), and 24 layers and 1,024 hidden states for I NFOXLM LARGE (i.e., large size). 4.2 Evaluation Cross-Lingual Natural Language Inference The Cross-Lingual Natural Language Inference corpus (XNLI; Conneau et al. 2018) is a widely used cross"
C12-2137,P03-1058,0,0.102511,"Missing"
C12-2137,P95-1026,0,0.665723,"rganized as follows: Section 2 briefly introduces the related work. The proposed method is described in detail in Section 3, and experimental results are presented in Section 4. Lastly we conclude this paper in Section 5. 2. Related Work Generally speaking, Word Sense Disambiguation methods are either knowledge-based or corpus-based. In addition, the latter can further be further divided into two kinds: unsupervised ones and supervised ones. In this paper we focus on supervised WSD method. 1402 More recently, WSD approaches based on pseudo word have gained much attention in the NLP community (Yarowsky, 1995; Leacock et al, 1998; Mihalcea and Moldovan, 1999; Agirre and Marinez, 2004; Brody and Lapata, 2008; Lu et al, 2006). The pseudo words can simulate the function of the real ambiguous words. In most cases, synonyms are used as pseudo words to acquire semantic knowledge as the real ambiguous word does. Specifically, These approaches exploits a sense inventory such as WordNet or corpus to collect pseudo words for ambiguous words, and use pseudo words to automatically create sense label data which can subsequently serve to train any supervised classifier. Such approaches are often regarded as wea"
C12-2137,J98-1006,0,0.137816,"ows: Section 2 briefly introduces the related work. The proposed method is described in detail in Section 3, and experimental results are presented in Section 4. Lastly we conclude this paper in Section 5. 2. Related Work Generally speaking, Word Sense Disambiguation methods are either knowledge-based or corpus-based. In addition, the latter can further be further divided into two kinds: unsupervised ones and supervised ones. In this paper we focus on supervised WSD method. 1402 More recently, WSD approaches based on pseudo word have gained much attention in the NLP community (Yarowsky, 1995; Leacock et al, 1998; Mihalcea and Moldovan, 1999; Agirre and Marinez, 2004; Brody and Lapata, 2008; Lu et al, 2006). The pseudo words can simulate the function of the real ambiguous words. In most cases, synonyms are used as pseudo words to acquire semantic knowledge as the real ambiguous word does. Specifically, These approaches exploits a sense inventory such as WordNet or corpus to collect pseudo words for ambiguous words, and use pseudo words to automatically create sense label data which can subsequently serve to train any supervised classifier. Such approaches are often regarded as weakly supervised learni"
C12-2137,P99-1020,0,0.0726921,"y introduces the related work. The proposed method is described in detail in Section 3, and experimental results are presented in Section 4. Lastly we conclude this paper in Section 5. 2. Related Work Generally speaking, Word Sense Disambiguation methods are either knowledge-based or corpus-based. In addition, the latter can further be further divided into two kinds: unsupervised ones and supervised ones. In this paper we focus on supervised WSD method. 1402 More recently, WSD approaches based on pseudo word have gained much attention in the NLP community (Yarowsky, 1995; Leacock et al, 1998; Mihalcea and Moldovan, 1999; Agirre and Marinez, 2004; Brody and Lapata, 2008; Lu et al, 2006). The pseudo words can simulate the function of the real ambiguous words. In most cases, synonyms are used as pseudo words to acquire semantic knowledge as the real ambiguous word does. Specifically, These approaches exploits a sense inventory such as WordNet or corpus to collect pseudo words for ambiguous words, and use pseudo words to automatically create sense label data which can subsequently serve to train any supervised classifier. Such approaches are often regarded as weakly supervised learning or semi-supervised learnin"
C12-2137,W04-3204,0,0.0679754,"Missing"
C12-2137,C08-1009,0,0.0190702,"escribed in detail in Section 3, and experimental results are presented in Section 4. Lastly we conclude this paper in Section 5. 2. Related Work Generally speaking, Word Sense Disambiguation methods are either knowledge-based or corpus-based. In addition, the latter can further be further divided into two kinds: unsupervised ones and supervised ones. In this paper we focus on supervised WSD method. 1402 More recently, WSD approaches based on pseudo word have gained much attention in the NLP community (Yarowsky, 1995; Leacock et al, 1998; Mihalcea and Moldovan, 1999; Agirre and Marinez, 2004; Brody and Lapata, 2008; Lu et al, 2006). The pseudo words can simulate the function of the real ambiguous words. In most cases, synonyms are used as pseudo words to acquire semantic knowledge as the real ambiguous word does. Specifically, These approaches exploits a sense inventory such as WordNet or corpus to collect pseudo words for ambiguous words, and use pseudo words to automatically create sense label data which can subsequently serve to train any supervised classifier. Such approaches are often regarded as weakly supervised learning or semi-supervised learning methods. Inspired by these approaches, we use sy"
C12-2137,P06-1058,0,0.0212352,"ction 3, and experimental results are presented in Section 4. Lastly we conclude this paper in Section 5. 2. Related Work Generally speaking, Word Sense Disambiguation methods are either knowledge-based or corpus-based. In addition, the latter can further be further divided into two kinds: unsupervised ones and supervised ones. In this paper we focus on supervised WSD method. 1402 More recently, WSD approaches based on pseudo word have gained much attention in the NLP community (Yarowsky, 1995; Leacock et al, 1998; Mihalcea and Moldovan, 1999; Agirre and Marinez, 2004; Brody and Lapata, 2008; Lu et al, 2006). The pseudo words can simulate the function of the real ambiguous words. In most cases, synonyms are used as pseudo words to acquire semantic knowledge as the real ambiguous word does. Specifically, These approaches exploits a sense inventory such as WordNet or corpus to collect pseudo words for ambiguous words, and use pseudo words to automatically create sense label data which can subsequently serve to train any supervised classifier. Such approaches are often regarded as weakly supervised learning or semi-supervised learning methods. Inspired by these approaches, we use synonymy of context"
C12-2137,S07-1004,0,0.0191285,"ifth level have similar sense and linguistic function, they can be substituted for each other without changing the meaning of the sentence. Collocation relationship: In the experiment, Sogou Chinese collocation relation2was used to filter out uncommonly used collocations. The collocation corpus involves more than 20 million collocation relations and more than 15000 high-frequency words, which was extracted from over 100 million internet pages on web in October 2006. Training and testing data: In SemEval-2007, the 4th international workshop on semantic evaluations under conference of ACL-2007 (Jin et al, 2007), we used task#5 multilingual Chinese English lexical sample to test our methods. Macro-average precision (Liu et al., 2007) was used to evaluate word sense disambiguation performance. Since we aim to evaluate discriminating power of synonymy feature, in the experiment, only some basic features such as topic words, collocations, and words assigned with their positions were used. We compare two baseline methods with our methods, the two baseline methods are as follows: (1) Original: WSD method based on traditional Bayesian Classifier. (2) SRCP_WSD (Xing Y, 2007): The system participated in seme"
C12-2137,S07-1065,0,\N,Missing
C16-1315,N13-1016,0,0.0138793,"ling and parent-child relations among topics. The structured data in DBpedia is used to label topics (Hulpus et al., 2013). Mehdad et al. (2013) introduced a topic labeling approach that assigns the most representative phrases for a given set of sentences covering the same topic. Cano et al. (2014) proposed a summarisation framework to label the topics learned from Twitter, which is independent of external sources and only relies on the identification of dominant terms in documents related to the latent topic. Word embedding is also used to help label topics (Jin et al., 2016). Interestingly, Aletras and Stevenson (2013) proposed to label topics with images rather than text. Candidate images for each topic are retrieved from the web by querying a search engine using the top-n terms. The most suitable image is selected by using a graph-based algorithm. Overall, these methods first extract the candidate labels, then rank these labels according to corresponding scoring function. The labeling framework is showed in Figure 1 (a). Despite the success of these works, they are either time-consuming or hard to assign labels for new emerging topics. Thus, in this paper, we will focus on the problems above, and proposed"
C16-1315,P14-2101,0,0.0968346,"to facilitate topic interpretation, has attracted increasing attention recently. Early research on topic labeling generally either select top words in the distribution as primitive labels (Blei et al., 2003; Ramage et al., 2009), or generate labels manually in a subjective manner (Mei et al., 2006; Mei and Zhai, 2005). However, it is highly desirable to automatically generate meaningful labels. Several automatical labeling methods have been proposed recently (Mei et al., 2007; Lau et al., 2010; Lau et al., 2011; Magatti et al., 2009; Mao et al., 2012; Hulpus et al., 2013; Mehdad et al., 2013; Cano et al., 2014). These existing approaches generally take following steps to generate meaningful labels for a given topic: (1) extract candidate labels; (2) rank candidate labels. First, most of existing methods extract candidate labels from a document collection by natural language processing techniques for a given topic, which is time-consuming; for example, the candidate labels in the method proposed by Mei et al. (2007) are extracted from a reference collection using chunking and statistically important bigrams, which is time-consuming. Second, most of existing approaches depend on external knowledge sou"
C16-1315,C10-2069,0,0.21022,"y are not familiar with the background knowledge. Topic labeling, which generates meaningful labels for a topic so as to facilitate topic interpretation, has attracted increasing attention recently. Early research on topic labeling generally either select top words in the distribution as primitive labels (Blei et al., 2003; Ramage et al., 2009), or generate labels manually in a subjective manner (Mei et al., 2006; Mei and Zhai, 2005). However, it is highly desirable to automatically generate meaningful labels. Several automatical labeling methods have been proposed recently (Mei et al., 2007; Lau et al., 2010; Lau et al., 2011; Magatti et al., 2009; Mao et al., 2012; Hulpus et al., 2013; Mehdad et al., 2013; Cano et al., 2014). These existing approaches generally take following steps to generate meaningful labels for a given topic: (1) extract candidate labels; (2) rank candidate labels. First, most of existing methods extract candidate labels from a document collection by natural language processing techniques for a given topic, which is time-consuming; for example, the candidate labels in the method proposed by Mei et al. (2007) are extracted from a reference collection using chunking and statis"
C16-1315,P11-1154,0,0.126173,"with the background knowledge. Topic labeling, which generates meaningful labels for a topic so as to facilitate topic interpretation, has attracted increasing attention recently. Early research on topic labeling generally either select top words in the distribution as primitive labels (Blei et al., 2003; Ramage et al., 2009), or generate labels manually in a subjective manner (Mei et al., 2006; Mei and Zhai, 2005). However, it is highly desirable to automatically generate meaningful labels. Several automatical labeling methods have been proposed recently (Mei et al., 2007; Lau et al., 2010; Lau et al., 2011; Magatti et al., 2009; Mao et al., 2012; Hulpus et al., 2013; Mehdad et al., 2013; Cano et al., 2014). These existing approaches generally take following steps to generate meaningful labels for a given topic: (1) extract candidate labels; (2) rank candidate labels. First, most of existing methods extract candidate labels from a document collection by natural language processing techniques for a given topic, which is time-consuming; for example, the candidate labels in the method proposed by Mei et al. (2007) are extracted from a reference collection using chunking and statistically important"
C16-1315,N13-1018,0,0.139368,"ls for a topic so as to facilitate topic interpretation, has attracted increasing attention recently. Early research on topic labeling generally either select top words in the distribution as primitive labels (Blei et al., 2003; Ramage et al., 2009), or generate labels manually in a subjective manner (Mei et al., 2006; Mei and Zhai, 2005). However, it is highly desirable to automatically generate meaningful labels. Several automatical labeling methods have been proposed recently (Mei et al., 2007; Lau et al., 2010; Lau et al., 2011; Magatti et al., 2009; Mao et al., 2012; Hulpus et al., 2013; Mehdad et al., 2013; Cano et al., 2014). These existing approaches generally take following steps to generate meaningful labels for a given topic: (1) extract candidate labels; (2) rank candidate labels. First, most of existing methods extract candidate labels from a document collection by natural language processing techniques for a given topic, which is time-consuming; for example, the candidate labels in the method proposed by Mei et al. (2007) are extracted from a reference collection using chunking and statistically important bigrams, which is time-consuming. Second, most of existing approaches depend on ex"
C18-1172,D17-1056,0,0.0778974,"ords to have a clear classification boundary. • We design a 5AbstractsGroup dataset and present a qualitative analysis, giving an intuitive understanding on how our method works from the classification perspective. Experimental results on five text classification datasets also show that the proposed method is more optimal for classification on account of revealing functional attributes of words. 2 Related Work Word embeddings that provide continuous low-dimensional vector representations of words have been widely studied by NLP communities (Yu et al., 2017; Liu et al., 2017; Li et al., 2017b; Chih et al., 2017). The last few years have seen the development of word embedding methods purely based on the co-occurrence information in a corpus (Bengio et al., 2003; Mnih and Hinton, 2008; Collobert et al., 2011; Mikolov et al., 2013; Mnih and Kavukcuoglu, 2013; Lebret and Collobert, 2014; Pennington et al., 2014; Cao and Lu, 2017; Bollegala et al., 2018). Some studies also pay attention to the semantic knowledge stored in the knowledge bases (Nie et al., 2015). For example, Faruqui et al. (2015) refine word representations using relational information from semantic lexicons, Liu et al. (2015b) represent s"
C18-1172,J81-4005,0,0.739722,"Missing"
C18-1172,P16-1035,0,0.021161,"Lebret and Collobert, 2014; Pennington et al., 2014; Cao and Lu, 2017; Bollegala et al., 2018). Some studies also pay attention to the semantic knowledge stored in the knowledge bases (Nie et al., 2015). For example, Faruqui et al. (2015) refine word representations using relational information from semantic lexicons, Liu et al. (2015b) represent semantic knowledge as a number of ordinal similarity inequalities of related word pairs to learn semantic word embeddings. Recent works have thrown light on the problems associated with directly applying word embeddings into real-world applications. Diaz et al. (2016) demonstrated that the globally trained word embedding underperform corpus and query-specific embeddings for retrieval tasks. They proposed locally training word embeddings in a query-specific manner for the query expansion task. Zamani and Croft (2017) indicated that the underlying assumption in typical word embedding methods is not equal to the need of IR tasks, and they proposed relevance-based models to learn word representations based on querydocument relevance information, which is the primary objective of most IR task. For the sentiment 2024 analysis task, Yu et al.(2017) refined word e"
C18-1172,N15-1184,0,0.109297,"Missing"
C18-1172,E14-1051,0,0.0260826,"sets also show that the proposed method is more optimal for classification on account of revealing functional attributes of words. 2 Related Work Word embeddings that provide continuous low-dimensional vector representations of words have been widely studied by NLP communities (Yu et al., 2017; Liu et al., 2017; Li et al., 2017b; Chih et al., 2017). The last few years have seen the development of word embedding methods purely based on the co-occurrence information in a corpus (Bengio et al., 2003; Mnih and Hinton, 2008; Collobert et al., 2011; Mikolov et al., 2013; Mnih and Kavukcuoglu, 2013; Lebret and Collobert, 2014; Pennington et al., 2014; Cao and Lu, 2017; Bollegala et al., 2018). Some studies also pay attention to the semantic knowledge stored in the knowledge bases (Nie et al., 2015). For example, Faruqui et al. (2015) refine word representations using relational information from semantic lexicons, Liu et al. (2015b) represent semantic knowledge as a number of ordinal similarity inequalities of related word pairs to learn semantic word embeddings. Recent works have thrown light on the problems associated with directly applying word embeddings into real-world applications. Diaz et al. (2016) demonstr"
C18-1172,P15-1145,0,0.493252,"al., 2017b; Chih et al., 2017). The last few years have seen the development of word embedding methods purely based on the co-occurrence information in a corpus (Bengio et al., 2003; Mnih and Hinton, 2008; Collobert et al., 2011; Mikolov et al., 2013; Mnih and Kavukcuoglu, 2013; Lebret and Collobert, 2014; Pennington et al., 2014; Cao and Lu, 2017; Bollegala et al., 2018). Some studies also pay attention to the semantic knowledge stored in the knowledge bases (Nie et al., 2015). For example, Faruqui et al. (2015) refine word representations using relational information from semantic lexicons, Liu et al. (2015b) represent semantic knowledge as a number of ordinal similarity inequalities of related word pairs to learn semantic word embeddings. Recent works have thrown light on the problems associated with directly applying word embeddings into real-world applications. Diaz et al. (2016) demonstrated that the globally trained word embedding underperform corpus and query-specific embeddings for retrieval tasks. They proposed locally training word embeddings in a query-specific manner for the query expansion task. Zamani and Croft (2017) indicated that the underlying assumption in typical word embeddin"
C18-1172,P11-1015,0,0.149879,"a test set of 7,532 documents. (2) The 5AbstractsGroup dataset is academic papers from five different domains collected from the Web of Science namely, business, artificial intelligence, sociology, transport and law. We extracted the abstract and title fields of each paper as a document. The dataset contains 6,256 documents, and we randomly selected 500 papers in each category as the training set, and the others as the test set. The dataset is published on the Github3 . (3) The IMDB4 contains movie reviews with binary classes (i.e., positive and negative). It consists of 50,000 movie reviews (Maas et al., 2011), and each movie review has several sentences. (4) The MR5 dataset consists of movie reviews from Rotten Tomato website with two classes labeled by (Pang and Lee, 2005). Each review contains only one sentence. (5) The SST6 dataset contains the movie reviews 1 https://github.com/qianliu0708/ToWE http://qwone.com/ jason/20Newsgroups/. 3 https://github.com/qianliu0708/5AbstractsGroup 4 https://www.cs.jhu.edu/mdredze/datasets/sentiment/unprocessed.tar.gz 5 https://www.cs.cornell.edu/people/pabo/movie-review-data/ 6 http://nlp.stanford.edu/sentiment 2 2027 Business AI Law Sociology Transport 87.5 e"
C18-1172,P05-1015,0,0.195469,"icial intelligence, sociology, transport and law. We extracted the abstract and title fields of each paper as a document. The dataset contains 6,256 documents, and we randomly selected 500 papers in each category as the training set, and the others as the test set. The dataset is published on the Github3 . (3) The IMDB4 contains movie reviews with binary classes (i.e., positive and negative). It consists of 50,000 movie reviews (Maas et al., 2011), and each movie review has several sentences. (4) The MR5 dataset consists of movie reviews from Rotten Tomato website with two classes labeled by (Pang and Lee, 2005). Each review contains only one sentence. (5) The SST6 dataset contains the movie reviews 1 https://github.com/qianliu0708/ToWE http://qwone.com/ jason/20Newsgroups/. 3 https://github.com/qianliu0708/5AbstractsGroup 4 https://www.cs.jhu.edu/mdredze/datasets/sentiment/unprocessed.tar.gz 5 https://www.cs.cornell.edu/people/pabo/movie-review-data/ 6 http://nlp.stanford.edu/sentiment 2 2027 Business AI Law Sociology Transport 87.5 employee entrepreneur stakeholder trait consumer marketplace asset bond manager markets distributional predefine inference recognition variant analytic learn aggregate o"
C18-1172,D14-1162,0,0.101445,"ar classification boundary. We evaluate our method using five text classification datasets. The experiment results show that our method significantly outperforms the state-of-the-art methods. 1 Introduction Learning word representation is a fundamental step in various natural language processing tasks. Tremendous advances have been made by distributed representations (also known as word embeddings) which learn a transformation of each word from raw text data to a dense, lower-dimensional vector space. Most existing methods leverage contextual information from the corpus (Mikolov et al., 2013; Pennington et al., 2014) and other complementary information, such as subword information (Cao and Lu, 2017), implicitly syntactic dependencies (Shen et al., 2018a; Shen et al., 2018b), and semantic relations (Bollegala et al., 2016; Liu et al., 2018). In traditional evaluations such as word similarity and word analogy, the aforementioned context-aware word embeddings work well since semantic information plays a vital role in these tasks, and this information is naturally addressed by word contexts. However, in real-world applications, such as text classification and information retrieval, word contexts alone are ins"
C18-1172,D13-1170,0,0.010643,"Missing"
D18-1125,P17-1110,0,0.0894867,"Missing"
D18-1125,I17-2072,0,0.171838,"3/Genre-SeparationNetwork-for-Relation-Extraction 1 Target Genre 2015; Gormley et al., 2015) tackle this problem by manually crafting genre-agnostic features such as word clusters and word embeddings, to train a genre-shared relation extractor. These methods suffer from information loss due to the limited human knowledge to capture all genre-agnostic features. As depicted in Figure 1, where red rectangles are features shared by two genres, and blue and green triangles are source and target genre features respectively, Feature Engineering only captures a portion of the genre-agnostic features. Fu et al. (2017), depicted as Feature Projection, applies a domain adversarial neural network to automatically project the source and target genre features into one unified feature space. However, it unnecessarily introduces genre-specific features which undermine the overall performance. To address these problems, we propose a genreseparation network, which consists of two separate Convolutional Neural Networks (CNNs) to automatically separates genre-specific and genreagnostic features for each genre, which is depicted as Genre Separation Network in Figure 1. To avoid information loss during feature encoding"
D18-1125,D15-1205,0,0.205408,"ities and their contexts, a supervised model trained in one genre suffers from dramatical performance decrease when applied to a new genre, due to the distinct contexts among various genres. Previous studies (Plank and Moschitti, 2013; Nguyen and Grishman, 2014, 2015; Yu et al., ∗ Feature Projection source feature Genre Separation Network target feature Figure 1: Comparison of Genre Separation Methods. Introduction *Corresponding author We make all cleaned codes and resources publicly available at https://github.com/Garym713/Genre-SeparationNetwork-for-Relation-Extraction 1 Target Genre 2015; Gormley et al., 2015) tackle this problem by manually crafting genre-agnostic features such as word clusters and word embeddings, to train a genre-shared relation extractor. These methods suffer from information loss due to the limited human knowledge to capture all genre-agnostic features. As depicted in Figure 1, where red rectangles are features shared by two genres, and blue and green triangles are source and target genre features respectively, Feature Engineering only captures a portion of the genre-agnostic features. Fu et al. (2017), depicted as Feature Projection, applies a domain adversarial neural networ"
D18-1125,D17-1274,1,0.848663,"e sentence (s, e1 , e2 , r) where s = [w1 , ..., wm ], for each word wik , we generate a multi-type embedding: v˜i = [vi , pi , p˜i , ti , t˜i , ci , ηi ] where vi denotes a pretrained word embedding. pi and p˜i are position embeddings (Al-Badrashiny et al., 2017) indicating the distance from wi to e1 and e2 respectively. ti and t˜i are entity type embedding (Ren et al., 2016; Huang et al., 2016) of e1 and e2 . ci is the chunking embedding, and ηi is a binary digit indicating whether the word is within the shortest dependency path between e1 and e2 (Bunescu and Mooney, 2005; Liu et al., 2015; Huang et al., 2017). All these embeddings except pre-trained word embedding are randomly initialized and optimized during training. Thus the input layer is a sequence of word representations V = {˜ v1 , v˜2 , ..., v˜n }. We then apply the convolution weights W to each sliding n-gram phrase gj 0 with a biased vector b, i.e., gj = tanh(W · V ) + b. 0 All n-gram representations gj are further used to get an overall vector representation f by maxpooling. Once we obtain fsp , fsc , fsp and fsc , we compute the difference loss: Ldif f = ||fsp> · fsc + ftp> · ftc ||F2 where ||.||F2 represents the squared Frobenius norm"
D18-1125,P17-1001,0,0.0830369,"Missing"
D18-1125,P15-2047,1,0.860361,"lly, given a source sentence (s, e1 , e2 , r) where s = [w1 , ..., wm ], for each word wik , we generate a multi-type embedding: v˜i = [vi , pi , p˜i , ti , t˜i , ci , ηi ] where vi denotes a pretrained word embedding. pi and p˜i are position embeddings (Al-Badrashiny et al., 2017) indicating the distance from wi to e1 and e2 respectively. ti and t˜i are entity type embedding (Ren et al., 2016; Huang et al., 2016) of e1 and e2 . ci is the chunking embedding, and ηi is a binary digit indicating whether the word is within the shortest dependency path between e1 and e2 (Bunescu and Mooney, 2005; Liu et al., 2015; Huang et al., 2017). All these embeddings except pre-trained word embedding are randomly initialized and optimized during training. Thus the input layer is a sequence of word representations V = {˜ v1 , v˜2 , ..., v˜n }. We then apply the convolution weights W to each sliding n-gram phrase gj 0 with a biased vector b, i.e., gj = tanh(W · V ) + b. 0 All n-gram representations gj are further used to get an overall vector representation f by maxpooling. Once we obtain fsp , fsc , fsp and fsc , we compute the difference loss: Ldif f = ||fsp> · fsc + ftp> · ftc ||F2 where ||.||F2 represents the s"
D18-1125,P14-2012,0,0.0508339,"rpose 1 . 1 Feature Input Feature Output Feature Engineering shared feature Relation extraction aims to identify and categorize the semantic relation between two entity mentions based on the contexts within the sentence. Supervised learning approaches have shown to be effective on this task. However, as relation extraction highly depends on information about entities and their contexts, a supervised model trained in one genre suffers from dramatical performance decrease when applied to a new genre, due to the distinct contexts among various genres. Previous studies (Plank and Moschitti, 2013; Nguyen and Grishman, 2014, 2015; Yu et al., ∗ Feature Projection source feature Genre Separation Network target feature Figure 1: Comparison of Genre Separation Methods. Introduction *Corresponding author We make all cleaned codes and resources publicly available at https://github.com/Garym713/Genre-SeparationNetwork-for-Relation-Extraction 1 Target Genre 2015; Gormley et al., 2015) tackle this problem by manually crafting genre-agnostic features such as word clusters and word embeddings, to train a genre-shared relation extractor. These methods suffer from information loss due to the limited human knowledge to captur"
D18-1125,P13-1147,0,0.0572814,"y available for research purpose 1 . 1 Feature Input Feature Output Feature Engineering shared feature Relation extraction aims to identify and categorize the semantic relation between two entity mentions based on the contexts within the sentence. Supervised learning approaches have shown to be effective on this task. However, as relation extraction highly depends on information about entities and their contexts, a supervised model trained in one genre suffers from dramatical performance decrease when applied to a new genre, due to the distinct contexts among various genres. Previous studies (Plank and Moschitti, 2013; Nguyen and Grishman, 2014, 2015; Yu et al., ∗ Feature Projection source feature Genre Separation Network target feature Figure 1: Comparison of Genre Separation Methods. Introduction *Corresponding author We make all cleaned codes and resources publicly available at https://github.com/Garym713/Genre-SeparationNetwork-for-Relation-Extraction 1 Target Genre 2015; Gormley et al., 2015) tackle this problem by manually crafting genre-agnostic features such as word clusters and word embeddings, to train a genre-shared relation extractor. These methods suffer from information loss due to the limite"
D18-1125,D16-1144,1,0.829068,"ncoder respectively. To separate fsp from fsc and separate ftp from ftc , we introduce a difference loss following previous studies (Bousmalis et al., 2016; Liu et al., 2017). More details will be elaborated below. Formally, given a source sentence (s, e1 , e2 , r) where s = [w1 , ..., wm ], for each word wik , we generate a multi-type embedding: v˜i = [vi , pi , p˜i , ti , t˜i , ci , ηi ] where vi denotes a pretrained word embedding. pi and p˜i are position embeddings (Al-Badrashiny et al., 2017) indicating the distance from wi to e1 and e2 respectively. ti and t˜i are entity type embedding (Ren et al., 2016; Huang et al., 2016) of e1 and e2 . ci is the chunking embedding, and ηi is a binary digit indicating whether the word is within the shortest dependency path between e1 and e2 (Bunescu and Mooney, 2005; Liu et al., 2015; Huang et al., 2017). All these embeddings except pre-trained word embedding are randomly initialized and optimized during training. Thus the input layer is a sequence of word representations V = {˜ v1 , v˜2 , ..., v˜n }. We then apply the convolution weights W to each sliding n-gram phrase gj 0 with a biased vector b, i.e., gj = tanh(W · V ) + b. 0 All n-gram representations"
D18-1125,N15-1155,0,0.0834558,"Missing"
D18-1156,P11-1113,0,0.802359,"Missing"
D18-1156,P08-1030,0,0.505837,"vector Ti and entity vector Ej by average pooling along the sequence length dimension. Then we concatenate them together and feed into a fully-connected network to predict the argument role as: Dataset, Resources and Evaluation Metric We evaluate our JMEE framework on the ACE 2005 dataset. The ACE 2005 dataset annotate 33 event subtypes and 36 role classes, along with the NONE class and BIO annotation schema, we will classify each token into 67 categories in event detection and 37 categories in argument extraction. To comply with previous work, we use the same data split as the previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Li et al., 2013; Chen et al., 2015; Liu et al., 2016b; Yang and Mitchell, 2016; Nguyen et al., 2016; Sha et al., 2018). This data split includes 40 newswire articles (881 sentences) for the test set, 30 other documents (1087 sentences) for the development set and 529 remaining documents (21,090 sentences) for the training set. We deploy the Stanford CoreNLP toolkit5 to preprocess the data, including tokenizing, sentence splitting, pos-tagging and generating dependency parsing trees. Also, we follow the criteria of the previous work (Ji and Grishman, 2008; Liao and Gr"
D18-1156,D17-1163,0,0.0835134,"Missing"
D18-1156,P03-1054,0,0.0168465,"u,v) are the weight matrix and the bias for the certain type label K(u, v), respectively; N (v) is the set of neighbors of v including v (because of the self-loop); f is the activation function. Moreover, we use the output of the word representation module xi to initialize the node representation h0vi of the first layer of GCNs. After applying the above two changes, the number of predefined directed arc type label (let us say, N ) will be doubled (to 2N + 1). It means we will (k) (k) have 2N + 1 sets of parameter pairs Wk and bk for a single layer of GCN. In this work, we use Stanford Parser (Klein and Manning, 2003) to generate the arcs in dependency parsing trees for sentences as the shortcut arcs. The current representa1250 tion contains approximately 50 different grammatical relations, which is too high for the parameter number of a single layer of GCN and not compatible with the existing training data scale. To reduce the parameter numbers, following Marcheggiani and Titov (2017), we modify the definition of type label K(wi , wj ) to:  (vi , vj ) ∈ E  along, rev, i! = j&(vj , vi ) ∈ E K(wi , wj ) =  loop, i == j (2) where the new K(wi , wj ) only have three type labels. As not all types of edges a"
D18-1156,P13-1008,0,0.232711,"g along the sequence length dimension. Then we concatenate them together and feed into a fully-connected network to predict the argument role as: Dataset, Resources and Evaluation Metric We evaluate our JMEE framework on the ACE 2005 dataset. The ACE 2005 dataset annotate 33 event subtypes and 36 role classes, along with the NONE class and BIO annotation schema, we will classify each token into 67 categories in event detection and 37 categories in argument extraction. To comply with previous work, we use the same data split as the previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Li et al., 2013; Chen et al., 2015; Liu et al., 2016b; Yang and Mitchell, 2016; Nguyen et al., 2016; Sha et al., 2018). This data split includes 40 newswire articles (881 sentences) for the test set, 30 other documents (1087 sentences) for the development set and 529 remaining documents (21,090 sentences) for the training set. We deploy the Stanford CoreNLP toolkit5 to preprocess the data, including tokenizing, sentence splitting, pos-tagging and generating dependency parsing trees. Also, we follow the criteria of the previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Li et al., 2013; Chen et al."
D18-1156,P15-1017,0,0.305584,"nce length dimension. Then we concatenate them together and feed into a fully-connected network to predict the argument role as: Dataset, Resources and Evaluation Metric We evaluate our JMEE framework on the ACE 2005 dataset. The ACE 2005 dataset annotate 33 event subtypes and 36 role classes, along with the NONE class and BIO annotation schema, we will classify each token into 67 categories in event detection and 37 categories in argument extraction. To comply with previous work, we use the same data split as the previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Li et al., 2013; Chen et al., 2015; Liu et al., 2016b; Yang and Mitchell, 2016; Nguyen et al., 2016; Sha et al., 2018). This data split includes 40 newswire articles (881 sentences) for the test set, 30 other documents (1087 sentences) for the development set and 529 remaining documents (21,090 sentences) for the training set. We deploy the Stanford CoreNLP toolkit5 to preprocess the data, including tokenizing, sentence splitting, pos-tagging and generating dependency parsing trees. Also, we follow the criteria of the previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Li et al., 2013; Chen et al., 2015; Liu et al.,"
D18-1156,W15-4502,0,0.0843706,"Missing"
D18-1156,P10-1081,0,0.698406,"ctor Ej by average pooling along the sequence length dimension. Then we concatenate them together and feed into a fully-connected network to predict the argument role as: Dataset, Resources and Evaluation Metric We evaluate our JMEE framework on the ACE 2005 dataset. The ACE 2005 dataset annotate 33 event subtypes and 36 role classes, along with the NONE class and BIO annotation schema, we will classify each token into 67 categories in event detection and 37 categories in argument extraction. To comply with previous work, we use the same data split as the previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Li et al., 2013; Chen et al., 2015; Liu et al., 2016b; Yang and Mitchell, 2016; Nguyen et al., 2016; Sha et al., 2018). This data split includes 40 newswire articles (881 sentences) for the test set, 30 other documents (1087 sentences) for the development set and 529 remaining documents (21,090 sentences) for the training set. We deploy the Stanford CoreNLP toolkit5 to preprocess the data, including tokenizing, sentence splitting, pos-tagging and generating dependency parsing trees. Also, we follow the criteria of the previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Li et al.,"
D18-1156,P16-2011,0,0.431,"Missing"
D18-1156,P16-1201,0,0.630439,"n. Then we concatenate them together and feed into a fully-connected network to predict the argument role as: Dataset, Resources and Evaluation Metric We evaluate our JMEE framework on the ACE 2005 dataset. The ACE 2005 dataset annotate 33 event subtypes and 36 role classes, along with the NONE class and BIO annotation schema, we will classify each token into 67 categories in event detection and 37 categories in argument extraction. To comply with previous work, we use the same data split as the previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Li et al., 2013; Chen et al., 2015; Liu et al., 2016b; Yang and Mitchell, 2016; Nguyen et al., 2016; Sha et al., 2018). This data split includes 40 newswire articles (881 sentences) for the test set, 30 other documents (1087 sentences) for the development set and 529 remaining documents (21,090 sentences) for the training set. We deploy the Stanford CoreNLP toolkit5 to preprocess the data, including tokenizing, sentence splitting, pos-tagging and generating dependency parsing trees. Also, we follow the criteria of the previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Li et al., 2013; Chen et al., 2015; Liu et al., 2016b; Yang and M"
D18-1156,P17-1164,0,0.346861,"Missing"
D18-1156,P12-1088,0,0.178353,"Missing"
D18-1156,D17-1159,0,0.508776,"o witnesses, along the acl-arc from witnesses to called, and along the xcomp-arc from called to barrage). These three arcs consist of a shortcut path2 , draining the dependency syntactic information flow from killed to barrage with fewer hops3 . In this paper, we propose a novel Jointly Multiple Events Extraction (JMEE) framework by introducing syntactic shortcut arcs to enhance information flow and attention-based graphic convolution networks to model the graph information. To implement modeling with the shortcut arcs, we adopt the graph convolutional networks (GCNs) (Kipf and Welling, 2016; Marcheggiani and Titov, 2017; Nguyen and Grishman, 2018) to learn syntactic contextual representations of each node by the representative vectors of its immediate neighbors in the graph. And then we utilize the syntactic contextual representations to extract triggers and arguments jointly by a self-attention mechanism to aggregate information especially keeping the associations between multiple events. 2 In a shortcut path which consists of existing arcs, some arcs may reverse their directions. 3 The length of the longest path in a tree is always no more than the sequential length consisting of the same number of nodes,"
D18-1156,N16-1033,0,0.28201,"ate them together and feed into a fully-connected network to predict the argument role as: Dataset, Resources and Evaluation Metric We evaluate our JMEE framework on the ACE 2005 dataset. The ACE 2005 dataset annotate 33 event subtypes and 36 role classes, along with the NONE class and BIO annotation schema, we will classify each token into 67 categories in event detection and 37 categories in argument extraction. To comply with previous work, we use the same data split as the previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Li et al., 2013; Chen et al., 2015; Liu et al., 2016b; Yang and Mitchell, 2016; Nguyen et al., 2016; Sha et al., 2018). This data split includes 40 newswire articles (881 sentences) for the test set, 30 other documents (1087 sentences) for the development set and 529 remaining documents (21,090 sentences) for the training set. We deploy the Stanford CoreNLP toolkit5 to preprocess the data, including tokenizing, sentence splitting, pos-tagging and generating dependency parsing trees. Also, we follow the criteria of the previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Li et al., 2013; Chen et al., 2015; Liu et al., 2016b; Yang and Mitchell, 2016; Nguyen et a"
D18-1156,P11-1163,0,0.260635,"Missing"
D18-1156,N16-1034,0,0.505738,"subj Current Word nmod nmod Police have arrested four people in connection with the killings Argument Role Labeling Graph Convolution Network self-attention Trigger Classification word POS- entity positional tagging type label Trigger label in BIO Embedding Layer BiLSTM Layer GCN Layer Joint Extraction Layer Figure 3: The architecture of our jointly multiple events extraction framework. • The positional embedding vector of wi : If wc is the current word, we encode the relative distance i − c from wi to wc as a realvalued vector by looking up the randomly initialized position embedding table (Nguyen et al., 2016; Liu et al., 2017; Nguyen and Grishman, 2018). • The entity type label embedding vector of wi : Similarly to the POS-tagging label embedding vector of wi , we annotate the entity mentions in a sentence using BIO annotation schema and transform the entity type labels to real-valued vectors by looking up the embedding table. It should be noticed that we use the whole entity extent in ACE 2005 dataset which contains overlapping entity mentions and we sum all the possible entity type label embedding vectors for each token. add reversed edge (vj , vi ) with the type label K ′ (wi , wj ). Following"
D18-1156,D16-1085,0,0.424082,"Missing"
D18-1156,D14-1162,0,0.089785,"Missing"
D18-1156,N12-1008,0,0.0536602,"Missing"
D19-1304,P15-1153,0,0.0260561,"ical analysis of quantitative results and human evaluations from comparative experiments with several state-of-the-art models that shows the proposed method provides promising performance. 2 Related Work Abstractive summarization supposedly digests and understands the source content and, consequently, the generated summaries are typically a reorganization of the wording that sometimes form new sentences. Historically, abstractive summarization has been performed through rule-based sentence selection (Dorr et al., 2003), key information extraction (Genest and Lapalme, 2011), syntactic parsing (Bing et al., 2015) and so on. However, more recently, seq2seq models with attention have played a more dominant role in generating abstractive summaries (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; Zhou et al., 2017). Extensions to the seq2seq approach include an intra-decoder attention (Paulus et al., 2017) and coverage vectors (See et al., 2017) to decrease repetition in phrasing. Copy mechanism (Gu et al., 2016) has been integrated into these models to tackle OOV problem. Zhou et al. (2018) went on to propose SeqCopyNet which copies complete sequences from an input sentence to further mai"
D19-1304,N18-1150,0,0.0196717,"2017) and coverage vectors (See et al., 2017) to decrease repetition in phrasing. Copy mechanism (Gu et al., 2016) has been integrated into these models to tackle OOV problem. Zhou et al. (2018) went on to propose SeqCopyNet which copies complete sequences from an input sentence to further maintain the readability of the generated summary. Pointer mechanism (Vinyals et al., 2015) has drawn much attention in text summarization (See et al., 2017), because this technique not only provides a potential solution for rare words and OOV but also extends abstractive summarization in a flexible way (C¸elikyilmaz et al., 2018). Further, pointer generator models can effectively adaptive to both extractor and abstractor networks (Chen and Bansal, 2018), and summaries can be generated by incorporating a pointer-generator and multiple relevant tasks (Guo et al., 2018), such as question or entailment generation, or multiple source texts (Sun et al., 2018). However, work particularly targets the problem of the abstraction is rare. Abstract Meaning Representation (AMR) is used to transform a sentence into a concept graph, then merge those similar concept nodes to form a new summary graph (Liu et al., 2018). Concepts are a"
D19-1304,P18-1063,0,0.173003,"2015; Chopra et al., 2016; Paulus et al., 2017; Guo et al., 2018; Gao et al., 2019). In tandem with seq2seq models, pointer generator was developed by See et al. (2017) as a solution to tackle the rare words and out-of-vocabulary (OOV) problem associated with generative-based models. The idea behind is to use attention as a pointer to determine the probability of generating a word from both a vocab∗ Corresponding author ulary distribution and the source text. Pointer generator networks have also been extensively accepted by the ABS community due to their efficacy with long document summaries (Chen and Bansal, 2018; Hsu et al., 2018), title summarization (Sun et al., 2018), etc. However, the current power of abstractive summarization falls short of their potential. As the example in Figure 1 shows, a seq2seq model with a pointer mechanism (marked as the direct pointer) is likely to merely copy parts of the original text to form a summary using keywords and phrases, such as “317 athletes”. Conversely, a more humanlike summary would be based on one’s own understanding of the detail in the words, expressed as higher-level concepts drawn from world knowledge—like using the word “group” to replace “athletes"
D19-1304,N16-1012,0,0.541282,"l, the proposed approach provides statistically significant improvements over several state-of-the-art models on both the DUC2004 and Gigaword datasets. A human evaluation of the model’s abstractive abilities also supports the quality of the summaries produced within this framework. 1 Figure 1: “summary1” only copies keyword from the source text, while “summary2” generates new concepts to convey the meaning. Introduction Abstractive summarization (ABS) has gained overwhelming success owing to a tremendous development of sequence-to-sequence (seq2seq) model and its variants (Rush et al., 2015; Chopra et al., 2016; Paulus et al., 2017; Guo et al., 2018; Gao et al., 2019). In tandem with seq2seq models, pointer generator was developed by See et al. (2017) as a solution to tackle the rare words and out-of-vocabulary (OOV) problem associated with generative-based models. The idea behind is to use attention as a pointer to determine the probability of generating a word from both a vocab∗ Corresponding author ulary distribution and the source text. Pointer generator networks have also been extensively accepted by the ABS community due to their efficacy with long document summaries (Chen and Bansal, 2018; Hs"
D19-1304,W03-0501,0,0.0549542,"forcement learning optimization on a test-only dataset in terms of ROUGE metrics; 3) a statistical analysis of quantitative results and human evaluations from comparative experiments with several state-of-the-art models that shows the proposed method provides promising performance. 2 Related Work Abstractive summarization supposedly digests and understands the source content and, consequently, the generated summaries are typically a reorganization of the wording that sometimes form new sentences. Historically, abstractive summarization has been performed through rule-based sentence selection (Dorr et al., 2003), key information extraction (Genest and Lapalme, 2011), syntactic parsing (Bing et al., 2015) and so on. However, more recently, seq2seq models with attention have played a more dominant role in generating abstractive summaries (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; Zhou et al., 2017). Extensions to the seq2seq approach include an intra-decoder attention (Paulus et al., 2017) and coverage vectors (See et al., 2017) to decrease repetition in phrasing. Copy mechanism (Gu et al., 2016) has been integrated into these models to tackle OOV problem. Zhou et al. (2018) went"
D19-1304,W11-1608,0,0.0337105,"ataset in terms of ROUGE metrics; 3) a statistical analysis of quantitative results and human evaluations from comparative experiments with several state-of-the-art models that shows the proposed method provides promising performance. 2 Related Work Abstractive summarization supposedly digests and understands the source content and, consequently, the generated summaries are typically a reorganization of the wording that sometimes form new sentences. Historically, abstractive summarization has been performed through rule-based sentence selection (Dorr et al., 2003), key information extraction (Genest and Lapalme, 2011), syntactic parsing (Bing et al., 2015) and so on. However, more recently, seq2seq models with attention have played a more dominant role in generating abstractive summaries (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; Zhou et al., 2017). Extensions to the seq2seq approach include an intra-decoder attention (Paulus et al., 2017) and coverage vectors (See et al., 2017) to decrease repetition in phrasing. Copy mechanism (Gu et al., 2016) has been integrated into these models to tackle OOV problem. Zhou et al. (2018) went on to propose SeqCopyNet which copies complete sequence"
D19-1304,P16-1154,0,0.029887,"abstractive summarization has been performed through rule-based sentence selection (Dorr et al., 2003), key information extraction (Genest and Lapalme, 2011), syntactic parsing (Bing et al., 2015) and so on. However, more recently, seq2seq models with attention have played a more dominant role in generating abstractive summaries (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; Zhou et al., 2017). Extensions to the seq2seq approach include an intra-decoder attention (Paulus et al., 2017) and coverage vectors (See et al., 2017) to decrease repetition in phrasing. Copy mechanism (Gu et al., 2016) has been integrated into these models to tackle OOV problem. Zhou et al. (2018) went on to propose SeqCopyNet which copies complete sequences from an input sentence to further maintain the readability of the generated summary. Pointer mechanism (Vinyals et al., 2015) has drawn much attention in text summarization (See et al., 2017), because this technique not only provides a potential solution for rare words and OOV but also extends abstractive summarization in a flexible way (C¸elikyilmaz et al., 2018). Further, pointer generator models can effectively adaptive to both extractor and abstract"
D19-1304,P18-1064,0,0.430602,"cally significant improvements over several state-of-the-art models on both the DUC2004 and Gigaword datasets. A human evaluation of the model’s abstractive abilities also supports the quality of the summaries produced within this framework. 1 Figure 1: “summary1” only copies keyword from the source text, while “summary2” generates new concepts to convey the meaning. Introduction Abstractive summarization (ABS) has gained overwhelming success owing to a tremendous development of sequence-to-sequence (seq2seq) model and its variants (Rush et al., 2015; Chopra et al., 2016; Paulus et al., 2017; Guo et al., 2018; Gao et al., 2019). In tandem with seq2seq models, pointer generator was developed by See et al. (2017) as a solution to tackle the rare words and out-of-vocabulary (OOV) problem associated with generative-based models. The idea behind is to use attention as a pointer to determine the probability of generating a word from both a vocab∗ Corresponding author ulary distribution and the source text. Pointer generator networks have also been extensively accepted by the ABS community due to their efficacy with long document summaries (Chen and Bansal, 2018; Hsu et al., 2018), title summarization (S"
D19-1304,P18-1013,0,0.0353144,"16; Paulus et al., 2017; Guo et al., 2018; Gao et al., 2019). In tandem with seq2seq models, pointer generator was developed by See et al. (2017) as a solution to tackle the rare words and out-of-vocabulary (OOV) problem associated with generative-based models. The idea behind is to use attention as a pointer to determine the probability of generating a word from both a vocab∗ Corresponding author ulary distribution and the source text. Pointer generator networks have also been extensively accepted by the ABS community due to their efficacy with long document summaries (Chen and Bansal, 2018; Hsu et al., 2018), title summarization (Sun et al., 2018), etc. However, the current power of abstractive summarization falls short of their potential. As the example in Figure 1 shows, a seq2seq model with a pointer mechanism (marked as the direct pointer) is likely to merely copy parts of the original text to form a summary using keywords and phrases, such as “317 athletes”. Conversely, a more humanlike summary would be based on one’s own understanding of the detail in the words, expressed as higher-level concepts drawn from world knowledge—like using the word “group” to replace “athletes and officials”. Thi"
D19-1304,D18-1207,0,0.0307172,"e to both extractor and abstractor networks (Chen and Bansal, 2018), and summaries can be generated by incorporating a pointer-generator and multiple relevant tasks (Guo et al., 2018), such as question or entailment generation, or multiple source texts (Sun et al., 2018). However, work particularly targets the problem of the abstraction is rare. Abstract Meaning Representation (AMR) is used to transform a sentence into a concept graph, then merge those similar concept nodes to form a new summary graph (Liu et al., 2018). Concepts are also incorporated as auxiliary features (Guo et al., 2017). Kryscinski et al. (2018) and Weber et al. (2018) define the number of new n-grams as the primary criteria of abstractiveness. This makes sense in most cases. But, we believe that abstraction means summarizing detailed content with higher-level semantically related concepts, which has motivated the development of the model proposed in this paper. 3077 3 The Proposed Model 3.2 Neural abstractive summarization can be described as a generation process where a sequential input is summarized into a shorter sequential output through a neural network. Suppose that the sequential input x = {x1 , . . . , xi , . . . , xn } is a"
D19-1304,C18-1121,0,0.135507,"nding papers. Underlined scores are the best without additional optimization. Bold scores are the best between the two optimization strategies. ? mark indicates the improvements from the baselines to the concept pointer are statistically significant using a two-tailed t-test (p < 0.01). Models ABS+† (Rush et al., 2015) Luong-NMT† (Luong et al., 2015) RAS-Elman† (Chopra et al., 2016) lvt5k-lsent† (Nallapati et al., 2016) SEASS† (Zhou et al., 2017) Seq2seq+att ? (our impl.) Pointer-generator ? (our impl.) (See et al., 2017) Pointer-Cov.-Entail.-Quest.† (Guo et al., 2018) Seq2seq-Sel.-MTL-ERAM† (Li et al., 2018) CGU† (Lin et al., 2018) Concept pointer Concept pointer+RL Concept pointer+DS ing on two popular datasets. The first was the English Gigaword Fifth Edition corpus (Parker et al., 2011). We replicated the pre-processing steps in (Rush et al., 2015) to obtain the same training/testing data. After pre-processing, the corpus contained about 3.8M sentence-summary pairs as training set and 189K pairs as the development set. Once pairs with empty titles were removed, the testing set numbered 1951 pairs. The second dataset, DUC2004, was only used for testing. This dataset consists of 500 document-hea"
D19-1304,P18-2027,0,0.0295797,"t al., 2003), key information extraction (Genest and Lapalme, 2011), syntactic parsing (Bing et al., 2015) and so on. However, more recently, seq2seq models with attention have played a more dominant role in generating abstractive summaries (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; Zhou et al., 2017). Extensions to the seq2seq approach include an intra-decoder attention (Paulus et al., 2017) and coverage vectors (See et al., 2017) to decrease repetition in phrasing. Copy mechanism (Gu et al., 2016) has been integrated into these models to tackle OOV problem. Zhou et al. (2018) went on to propose SeqCopyNet which copies complete sequences from an input sentence to further maintain the readability of the generated summary. Pointer mechanism (Vinyals et al., 2015) has drawn much attention in text summarization (See et al., 2017), because this technique not only provides a potential solution for rare words and OOV but also extends abstractive summarization in a flexible way (C¸elikyilmaz et al., 2018). Further, pointer generator models can effectively adaptive to both extractor and abstractor networks (Chen and Bansal, 2018), and summaries can be generated by incorpora"
D19-1304,D15-1166,0,0.0256174,"d training. 4 Experiments Datasets: To evaluate the effectiveness of our proposed model, we conducted training and test3080 Table 1: ROUGE F1 evaluation results on the Gigaword and ROUGE recall on DUC-2004 test set. The results with † mark are taken from the corresponding papers. Underlined scores are the best without additional optimization. Bold scores are the best between the two optimization strategies. ? mark indicates the improvements from the baselines to the concept pointer are statistically significant using a two-tailed t-test (p < 0.01). Models ABS+† (Rush et al., 2015) Luong-NMT† (Luong et al., 2015) RAS-Elman† (Chopra et al., 2016) lvt5k-lsent† (Nallapati et al., 2016) SEASS† (Zhou et al., 2017) Seq2seq+att ? (our impl.) Pointer-generator ? (our impl.) (See et al., 2017) Pointer-Cov.-Entail.-Quest.† (Guo et al., 2018) Seq2seq-Sel.-MTL-ERAM† (Li et al., 2018) CGU† (Lin et al., 2018) Concept pointer Concept pointer+RL Concept pointer+DS ing on two popular datasets. The first was the English Gigaword Fifth Edition corpus (Parker et al., 2011). We replicated the pre-processing steps in (Rush et al., 2015) to obtain the same training/testing data. After pre-processing, the corpus contained ab"
D19-1304,P17-1101,0,0.237096,"mmarization supposedly digests and understands the source content and, consequently, the generated summaries are typically a reorganization of the wording that sometimes form new sentences. Historically, abstractive summarization has been performed through rule-based sentence selection (Dorr et al., 2003), key information extraction (Genest and Lapalme, 2011), syntactic parsing (Bing et al., 2015) and so on. However, more recently, seq2seq models with attention have played a more dominant role in generating abstractive summaries (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; Zhou et al., 2017). Extensions to the seq2seq approach include an intra-decoder attention (Paulus et al., 2017) and coverage vectors (See et al., 2017) to decrease repetition in phrasing. Copy mechanism (Gu et al., 2016) has been integrated into these models to tackle OOV problem. Zhou et al. (2018) went on to propose SeqCopyNet which copies complete sequences from an input sentence to further maintain the readability of the generated summary. Pointer mechanism (Vinyals et al., 2015) has drawn much attention in text summarization (See et al., 2017), because this technique not only provides a potential solution"
D19-1304,D15-1044,0,0.855497,"testing set. Overall, the proposed approach provides statistically significant improvements over several state-of-the-art models on both the DUC2004 and Gigaword datasets. A human evaluation of the model’s abstractive abilities also supports the quality of the summaries produced within this framework. 1 Figure 1: “summary1” only copies keyword from the source text, while “summary2” generates new concepts to convey the meaning. Introduction Abstractive summarization (ABS) has gained overwhelming success owing to a tremendous development of sequence-to-sequence (seq2seq) model and its variants (Rush et al., 2015; Chopra et al., 2016; Paulus et al., 2017; Guo et al., 2018; Gao et al., 2019). In tandem with seq2seq models, pointer generator was developed by See et al. (2017) as a solution to tackle the rare words and out-of-vocabulary (OOV) problem associated with generative-based models. The idea behind is to use attention as a pointer to determine the probability of generating a word from both a vocab∗ Corresponding author ulary distribution and the source text. Pointer generator networks have also been extensively accepted by the ABS community due to their efficacy with long document summaries (Chen"
D19-1304,P17-1099,0,0.210418,"tasets. A human evaluation of the model’s abstractive abilities also supports the quality of the summaries produced within this framework. 1 Figure 1: “summary1” only copies keyword from the source text, while “summary2” generates new concepts to convey the meaning. Introduction Abstractive summarization (ABS) has gained overwhelming success owing to a tremendous development of sequence-to-sequence (seq2seq) model and its variants (Rush et al., 2015; Chopra et al., 2016; Paulus et al., 2017; Guo et al., 2018; Gao et al., 2019). In tandem with seq2seq models, pointer generator was developed by See et al. (2017) as a solution to tackle the rare words and out-of-vocabulary (OOV) problem associated with generative-based models. The idea behind is to use attention as a pointer to determine the probability of generating a word from both a vocab∗ Corresponding author ulary distribution and the source text. Pointer generator networks have also been extensively accepted by the ABS community due to their efficacy with long document summaries (Chen and Bansal, 2018; Hsu et al., 2018), title summarization (Sun et al., 2018), etc. However, the current power of abstractive summarization falls short of their pote"
D19-1304,P16-5007,0,0.0105459,"for extracting salient information, while remaining flexible enough to interface with a seq2seq model for generating an abstractive summarization (See et al., 2017). Our proposed model is essentially an upgrade to this configuration that integrates a new concept pointer network within a unified framework. 3.2.1 Context-aware Conceptualization “Understanding” the instances of a word requires a taxonomic knowledge base that relates those words to a concept space. In our model, we use an isA taxonomy, called the Microsoft Concept Graph1 (Wang et al., 2015), to serve this purpose for two reasons (Wang and Wang, 2016). First, this graph provides a huge concept space with multi-word terms that cover concepts of worldly facts as concepts, instances, relationships, and values2 . Second, the relationships between concepts and entities are probabilistic as a measure of how strongly they are related. Moreover, the probabilities are trustworthy given they have been derived from evidence found in billions of webpages, search log data, and other existing taxonomies. Our model is data-driven and, therefore, is more easily adaptable with probabilities. All these characteristics make the Microsoft Concept Graph a suit"
K19-1025,C10-3004,0,0.00934491,"6) to the output layer. The model dimension d is set to 512. Alignment Score and Discriminator Loss. With the source and target sentence encodings ex and ey , the alignment score s(X,Y) can be computed as: s(X,Y) = e> (5) x ey . ˆ LG = L1−pair ({X, Y+ , Y}) = log(1 + exp(s(X,Y+ ) − s(X,Y) ˆ )). 263 (7) Chinese-to-English. For Chinese-to-English translation, our training data are extracted from four LDC corpora3 . The training set contains totally 1.3M parallel sentence pairs. For preprocessing, the Chinese part for both training and testing sets is segmented by the LTP Chinese word segmentor (Che et al., 2010) before applying bpe (Sennrich et al., 2016) to the corpus. We get a Chinese vocabulary of about 39K tokens, and an English vocabulary of about 30K tokens. We use NIST2005 dataset for validation and NIST2002, NIST2003, and NIST2004 datasets for testing. In the following parts of the paper, the Chinese examples are presented by segmented italic romanized form, and different Chinese characters are delimited by single quotation marks. English-to-German. For English-to-German translation, we conduct experiments on the publicly available corpora WMT’14 En-De. The training set of En-De task totally"
K19-1025,P16-1185,0,0.0200693,"ansformed to the target side. Thus, it cannot handle translation adequacy problem (Tu et al., 2017). One way to alleviate these problems is to apply coverage and fertility to NMT model. Feng et al. (2016) aim at controlling the fertilities of source words by appending additional additive terms to train objectives. Tu et al. (2016) employ coverage vector or coverage ratio as a lexical-level indicator to represent whether a source word is translated or not. On the other hand, some recent efforts introduce additional source side constraints and explore duality properties of NMT (He et al., 2016; Cheng et al., 2016; Xia et al., 2017; Tu et al., 2017). Cheng et al. (2016) present a semi-supervised approach to train bidirectional NMT models and reconstruct the monolingual corpora using an autoencoder (Socher et al., 2011). Tu et al. (2017) add a re-constructor to traditional NMT model, which introduces an auxiliary score to measure the adequacy of translation. Dual learning (He et al., 2016) and dual supervised learning (Xia et al., 2017) are also proposed to exploit the probabilistic correlation between dual tasks to regularize the training process. These previous approaches apply a reconstruction reward"
K19-1025,W14-4012,0,0.101707,"Missing"
K19-1025,D14-1179,0,0.0597037,"Missing"
K19-1025,C16-1290,0,0.0781123,"f two independent NMT systems. Lines between Source and NMTs represent model generated alignments (each source word cannot be covered more than once). Words in boxes are key words and red dotted dashed boxes indicate incorrect translations. Based on the model generated attention weights, NMT2 covers more source words than NMT1, which is opposite to human judgments. ment for the information transformational completeness from the source side to the target side. Some existing work alleviates this problem by directly incorporating coverage or fertility mechanisms to an NMT model (Tu et al., 2016; Feng et al., 2016; Kong et al., 2019). However, the problem is that attention weights based coverage calculation for NMT is insensitive and sometimes even inaccurate to translation errors. Furthermore, it is unreasonable to consider the coverage of all kinds of source words equally, since various words contribute differently to sentences in semantics and syntax. For example, as illustrated in Fig. 1, translation errors are recorded as positive examples, and the alignments between function words also dilute the impact of key words alignments. In this paper, we address the problem of inadequate translation by in"
K19-1025,D17-1230,0,0.433278,"of source words by appending additional additive terms to train objectives. Tu et al. (2016) employ coverage vector or coverage ratio as a lexical-level indicator to represent whether a source word is translated or not. On the other hand, some recent efforts introduce additional source side constraints and explore duality properties of NMT (He et al., 2016; Cheng et al., 2016; Xia et al., 2017; Tu et al., 2017). Cheng et al. (2016) present a semi-supervised approach to train bidirectional NMT models and reconstruct the monolingual corpora using an autoencoder (Socher et al., 2011). Tu et al. (2017) add a re-constructor to traditional NMT model, which introduces an auxiliary score to measure the adequacy of translation. Dual learning (He et al., 2016) and dual supervised learning (Xia et al., 2017) are also proposed to exploit the probabilistic correlation between dual tasks to regularize the training process. These previous approaches apply a reconstruction reward by comparing the source input and the reconstructed sentence, while we use alignment score directly to model the discrepancy between the source and the translation. GAN (Goodfellow et al., 2014) is another promising framework"
K19-1025,ma-2006-champollion,0,0.459297,"(MLE) of each word in the ground truth translations during the training procedure. However, such an objective cannot guarantee the sufficiency of the generated translations in the NMT model, due to the lack of quantitative measure∗ a Corresponding author: Ping Jian 260 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 260–270 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics transferability. Further analyses show the specific alignment-oriented knowledge that the discriminator transfers to the NMT model. alignment pairs (Ma, 2006) at a sentence representation level; and ii) a standard NMT model G aims to produce an appropriate translation with the highest ranking score (assigned by D) in the candidate list. To better capture the semantic alignment evidence of the input data, we also propose a novel gated self-attention based encoder for bilingual sentences encoding in discriminator D. Then, an N -pair training loss (Sohn, 2016) is introduced to select appropriate translation results from the candidates. We also leverage Gumbel-Softmax (GS) (Jang et al., 2017; Kusner and Hern´andezLobato, 2016) approximation for G to so"
K19-1025,P02-1040,0,0.104263,"he golden standard translation in the comparable paragraph. If the context sentence number Nc is less than 99, we will randomly sample another 99 − Nc sentences from the whole rest target corpus. As for the data format, we follow most of Das et al. (2019). Evaluation. As for generative models, following Vaswani et al. (2017), we report the result of a single model obtained by averaging the 5 checkpoints around the best model selected on the development set. We apply beam search during decoding with the beam size of 6. The translation results in this paper are measured in caseinsensitive BLEU (Papineni et al., 2002) by the Intuitively, updating generator parameters to minimize LG can be interpreted as learning to produce a translation Yˆ that “fools” the discriminator into believing that this answer should score higher than the human response Y + under the D’s scoring function. 3.4 Gumbel-Softmax Sampler ˆ with G is The process of sampling a translation Y not differentiable, since it includes argmax(·) operator to perform one-hot encoding. We leverage the Gumbel-softmax (Jang et al., 2017) sampler to solve this problem. Formally, at the decoding step j, suppose that pj ∈ RVy contains the model output log"
K19-1025,D13-1176,0,0.0590992,"function words also dilute the impact of key words alignments. In this paper, we address the problem of inadequate translation by introducing a novel sentence alignment constrain under an adversarial training framework (Goodfellow et al., 2014; Lu et al., 2017; Yang et al., 2018). Specifically, our approach contains two sub-models: i) a sentence alignment oriented discriminator D learns to estimate the alignment score and sort the translation candidates by mainly considering the weighted Introduction Recently, with the renaissance of deep learning, end-to-end Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Cho et al., 2014a; Sutskever et al., 2014; Bahdanau et al., 2014) has gained remarkable performance (Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017). Early NMT solutions are typically optimized to maximize the likelihood estimation (MLE) of each word in the ground truth translations during the training procedure. However, such an objective cannot guarantee the sufficiency of the generated translations in the NMT model, due to the lack of quantitative measure∗ a Corresponding author: Ping Jian 260 Proceedings of the 23rd Conference on Computational Natural Language Learning, page"
K19-1025,P16-1162,0,0.331677,"sion d is set to 512. Alignment Score and Discriminator Loss. With the source and target sentence encodings ex and ey , the alignment score s(X,Y) can be computed as: s(X,Y) = e> (5) x ey . ˆ LG = L1−pair ({X, Y+ , Y}) = log(1 + exp(s(X,Y+ ) − s(X,Y) ˆ )). 263 (7) Chinese-to-English. For Chinese-to-English translation, our training data are extracted from four LDC corpora3 . The training set contains totally 1.3M parallel sentence pairs. For preprocessing, the Chinese part for both training and testing sets is segmented by the LTP Chinese word segmentor (Che et al., 2010) before applying bpe (Sennrich et al., 2016) to the corpus. We get a Chinese vocabulary of about 39K tokens, and an English vocabulary of about 30K tokens. We use NIST2005 dataset for validation and NIST2002, NIST2003, and NIST2004 datasets for testing. In the following parts of the paper, the Chinese examples are presented by segmented italic romanized form, and different Chinese characters are delimited by single quotation marks. English-to-German. For English-to-German translation, we conduct experiments on the publicly available corpora WMT’14 En-De. The training set of En-De task totally contains 4.5M sentence pairs, and we use a s"
K19-1025,W17-3204,0,0.0168744,"hen re-order the N -best translation candidates by D. Experimental results are shown in Table 4. We can observe that the larger beam search size leads to the worse performance, since the likelihood score for decoding tends to score short translations higher than long sentences. Larger searching space also brings more good translation candidates, and D re-orders them by alignment score and gains better BLEU scores than most baseline setups as shown in Table 4. The above observation indicates that D can successfully handle the unseen data generated by NMT models. Previous work (Wu et al., 2016; Koehn and Knowles, 2017) introduces length normalization to solve the above beam search decoding problem, whose results are also presented in Table 4 for a fair comparison. 5.3 6 Conclusion In this work, we propose a novel training framework which achieves sentence alignment oriented knowledge transfer to improve the NMT. We design a discriminator to measure sentence alignment by mainly considering lexical evidence via a gated self-attention mechanism. Then, a discriminative loss as well as a teacher-forcing objective is used to make NMT model generate sufficient and fluent translations during training procedure. Exp"
K19-1025,P16-1159,0,0.106525,"is a temperature parameter and is set to 0.5 in our experiments. 3.5 Teacher-forcing Step Since LG in Eq. (7) mainly considers the discrepancy of alignment and integrity between the model output and the ground-truth, it rarely inspects grammar correctness and language fluency. To alleviate this problem, following (Li et al., 2017; Lu et al., 2017), we adopt the similar teacher-forcing step to our training process. We perform two different teacher-forcing objectives for comparison: i) a likelihood objective OLM and ii) a BLEU score reward (RBLEU ), under the training strategies of MLE and MRT (Shen et al., 2016), respectively. 4 4.1 Experiments Datasets and Setups1 We evaluate the proposed approach on Chinese-toEnglish (Zh-En) and English-to-German (En-De) translation tasks. For both of the two translation tasks, we tokenize all corpora with the Moses tokenizer2 . Sentences longer than 100 words are discarded, and all the sentences are encoded with byte-pair encoding (bpe) (Sennrich et al., 2016). 1 The demo data and source codes will be released online at https://github.com/PolarLion/ Sentence-Alignment-Learning 2 https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/ tokenizer/tokenizer.pe"
K19-1025,D11-1014,0,0.0929943,"Missing"
K19-1025,N18-1122,0,0.0376234,"Missing"
K19-1025,P16-1008,0,0.0852766,"lation examples of two independent NMT systems. Lines between Source and NMTs represent model generated alignments (each source word cannot be covered more than once). Words in boxes are key words and red dotted dashed boxes indicate incorrect translations. Based on the model generated attention weights, NMT2 covers more source words than NMT1, which is opposite to human judgments. ment for the information transformational completeness from the source side to the target side. Some existing work alleviates this problem by directly incorporating coverage or fertility mechanisms to an NMT model (Tu et al., 2016; Feng et al., 2016; Kong et al., 2019). However, the problem is that attention weights based coverage calculation for NMT is insensitive and sometimes even inaccurate to translation errors. Furthermore, it is unreasonable to consider the coverage of all kinds of source words equally, since various words contribute differently to sentences in semantics and syntax. For example, as illustrated in Fig. 1, translation errors are recorded as positive examples, and the alignments between function words also dilute the impact of key words alignments. In this paper, we address the problem of inadequat"
P16-1048,P14-2131,0,0.03396,"t al., 2015). Shot-text conceptualization, is an interesting task to infer the most likely concepts for terms in the short-text, which could help better make sense of text data, and extend the texts with categorical or topical information (Song et al., 2011). Therefore, our models utilize shorttext conceptualization algorithm to discriminate concept-level sentence senses and provide a good performance on short-texts. Recently, attention model has been used to improve many neural natural language pro-cessing researches by selectively focusing on parts of the source data (Bahdanau et al., 2014; Bansal et al., 2014; Wang et al., 2015a). To the best of our knowledge, there has not been any other work exploring the use of attentional mechanism for sentence embeddings. popularity recently in neural natural language processing researches, which allowing models to learn alignments between different modalities (Bahdanau et al., 2014; Bansal et al., 2014; Rush et al., 2015). In this work, we further propose the extensions to CSE, which adds an attention model that considers contextual words differently depending on the word type and its relative position to the predicted word. The main intuition behind the ext"
P16-1048,D15-1044,0,0.17558,"ntence senses and provide a good performance on short-texts. Recently, attention model has been used to improve many neural natural language pro-cessing researches by selectively focusing on parts of the source data (Bahdanau et al., 2014; Bansal et al., 2014; Wang et al., 2015a). To the best of our knowledge, there has not been any other work exploring the use of attentional mechanism for sentence embeddings. popularity recently in neural natural language processing researches, which allowing models to learn alignments between different modalities (Bahdanau et al., 2014; Bansal et al., 2014; Rush et al., 2015). In this work, we further propose the extensions to CSE, which adds an attention model that considers contextual words differently depending on the word type and its relative position to the predicted word. The main intuition behind the extended model is that prediction of a word is mainly dependent on certain words surrounding it. In summary, the basic idea of CSE is that, we allow each word to have different embeddings under different concepts. Taking word apple into consideration, it may indicate a fruit under the concept food, and indicate an IT company under the concept information techn"
P16-1048,C02-1150,0,0.0120695,"y train topic model-based models. NewsTitle: The news articles are extracted from a large news corpus, which contains about one million articles searched from Web pages. We organize volunteers to classify these news articles manually into topics according its article content (Song et al., 2015), and we select six topics: company, health, entertainment, food, politician, and sports. We randomly select 3,000 news articles in each topic, and only keep its title and its first one line of article. The average word count of titles is 9.41. TREC: It is the corpus for question classification on TREC (Li and Roth, 2002), which is widely used as benchmark in text classification task. There are 5,952 sentences in the entire dataset, classified into the 6 categories as follows: person, abbreviation, entity, description, location and numeric. Tweet11: This is the official tweet collections used in TREC Microblog Task 2011 and 2012 (Ounis et al., 2011; Soboroff et al., 2012). Using the official API, we crawled a set of local copies of the corpus. Our local Tweets11 collection has a sample of about 16 million tweets, and a set of 49 (TMB2011) and 60 (TMB2012) timestamped topics. Twitter: This dataset is constructe"
P16-1048,D15-1166,0,0.00427655,"tioned only on the relative position i. Note that, attention models have been reported expensive for large tables in terms of storage and performance (Bahdanau et al., 2014; Wang et al., 2015a). Nevertheless the computation consumption here is simple, and compute the attention of all words in the input requires 2k operations, as it simply requires retrieving on value from the lookup-matrix D for each word and one value from the bias vector R for each word in the context. Although this strategy may not be the best approach and there exist more elaborate attention models (Bahdanau et al., 2014; Luong et al., 2015), the proposed attention model is a proper balance of computational efficiency and complexity. Thus, besides {W,C,S} in CSE models, D and R are added into parameter set which relates to CSE based on Attention Model As mentioned above, setting a good value for contextual window size k is difficult. Because a larger value of k may introduce a degenerative behavior in the model, and more effort is spent predict509 4 gradients of the loss function Eq.(1). All parameters are computed with backpropagation and updated after each training instance using a fixed learning rate. We denote the attention-b"
P16-1048,P15-2029,0,0.0224802,"er is as follows. Section 2 surveys related researches. Section 3 formally de-scribes the proposed model of conceptual sentence embedding. Corresponding experimental results are shown in Section 4. Finally, we conclude the paper. 2 Related Works Conventionally, one-hot sentence representation has been widely used as the basis of bag-of-words (BOW) text model. However, it can-not take the semantic information into consideration. Recently, in sentence representation and classification, deep neural network approaches have achieved state-of-the-art results (Le and Mikolov, 2014; Liu et al., 2015; Ma et al., 2015; Palangi et al., 2015; 3 Conceptual Sentence Embedding This paper proposes four conceptual sentence embedding models. The first one is based on continu506 ous bag-of-word model (denoted as CSE-1) which have not taken word order into consideration. To overcome this drawback, its extension model (denoted as CSE-2), which is based on Skip-Gram model, is proposed. Based on the basic conceptual sentence embedding models above, we obtain their variants (aCSE-1 and aCSE-2) by introducing attention model. 3.1 ct = As inspiration of the proposed conceptual sentence embedding models, we start by discus"
P16-1048,D15-1161,0,0.23176,"s in the document. However, their model depends only on word surface, ignoring semantic information such as topics or concepts. In this paper, we extent PV by introducing concept information. Aiming at enhancing discriminativeness for ubiquitous polysemy, (Liu et al., 2015) employed latent topic models to assign topics for each word in the text corpus, and learn topical word embeddings (TWE) and sentence embeddings based on both words and their topics. Besides, to combine deep learning with linguistic structures, many syntax-based embedding algorithms have been proposed (Severyn et al., 2014; Wang et al., 2015b) to utilize long-distance dependencies. However, short-texts usually do not observe the syntax of a written language, nor do they contain enough signals for statistical inference (e.g., topic model). Therefore, neither parsing nor topic modeling works well because there are simply not enough signals in the input, and we must derive more semantic signals from the input, e.g., concepts, which have been demonstrated effective in knowledge representation (Wang et al., 2015c; Song et al., 2015). Shot-text conceptualization, is an interesting task to infer the most likely concepts for terms in the"
P19-1276,W17-2710,0,0.0609551,"th various slot combinations. Event Schema Induction seminal work studies patterns (Shinyama and Sekine, 2006; Filatova et al., 2006; Qiu et al., 2008) and event chains (Chambers and Jurafsky, 2011) for template induction. For MUC 4, the current dominant methods include probabilistic generative methods (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015) that jointly model predicate and ar1 https://news.google.com/?hl=en-US&gl= US&ceid=US:en, crawled from Oct. 2018 to Jan. 2019. gument assignment, and ad-hoc clustering algorithms for inducing slots (Sha et al., 2016; Huang et al., 2016; Ahn, 2017; Yuan et al., 2018). These methods all rely on hand-crafted discrete features without fully model the textual redundancy. There are also works on modeling event schemas and scripts using neural language models (Modi and Titov, 2014; Rudinger et al., 2015; Pichotta and Mooney, 2016), but they do not explore neural latent variables and redundancy. Event Extraction work typically assumes that event schemas are given, recognizing event triggers and their corresponding arguments. This can be regarded as a subtask of ODEE. Existing work exploits sentence-level (McClosky et al., 2011; Li et al., 201"
P19-1276,P11-1040,0,0.0371388,"lso been work using RNNs (Nguyen et al., 2016b; Sha et al., 2018; Liu et al., 2018a; Chen et al., 2018), CNNs (Chen et al., 2015; Feng et al., 2016; Nguyen and Grishman, 2016) and GCNs (Liu et al., 2018b) to represent sentences of events. Event extraction has been treated as a supervised or semi-supervised (Liao and Grishman, 2010a; Huang and Riloff, 2012) task. In contrast, ODEE is a fully unsupervised setting. Event Discovery in Tweet Streams extracts news-worthy clusters of words, segments and frames. Both supervised and unsupervised methods have been used. The former (Sakaki et al., 2010; Benson et al., 2011) are typically designed to monitor certain event types, while the latter cluster features according to their burstiness (Becker et al., 2011; Cui et al., 2012; Li et al., 2012; Ritter et al., 2012; Qin et al., 2013; Ifrim et al., 2014; McMinn and Jose, 2015; Qin et al., 2017). This line of work is similar to our work in using information redundancy, but different because we focus on formal news texts and induce structural event schemas. First Story Detection (FSD) systems aim to identify news articles that discuss events not reported before. Most work on FSD detects first stories by finding th"
P19-1276,D13-1185,0,0.788263,"not been considered in traditional methods. First, more than one event can be extracted from a news cluster, where events can be flexible in having varying numbers of slots in the open domain, and slots can be flexible without identical distributions regardless of the event type, which has been assumed by previous work on schema induction. Second, mentions of the same entities from different reports in a news cluster should be taken into account for improved performance. We build an unsupervised generative model to address these challenges. While previous work on generative schema induction (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015) relies on hand-crafted indicator features, we introduce latent variables produced by neural networks for better representation power. A novel graph model 2860 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2860–2871 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics is designed, with a latent event type vector for each news cluster from a global parameterized normal distribution, and textual redundancy features for entities. Our model takes advantage of contextua"
P19-1276,P11-1098,0,0.523624,"r slots - Perpetrator, Instrument, Target and Victim - are defined. We compare the task settings of MUC 4 and ODEE in Figure 1. For MUC 4, the inputs are single news documents, and the output belongs to four types of events with schemas consisting of fixed slots. For ODEE, in contrast, the inputs are news clusters rather than the individual news, and the output is unconstrained types of open domain events and unique schemas with various slot combinations. Event Schema Induction seminal work studies patterns (Shinyama and Sekine, 2006; Filatova et al., 2006; Qiu et al., 2008) and event chains (Chambers and Jurafsky, 2011) for template induction. For MUC 4, the current dominant methods include probabilistic generative methods (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015) that jointly model predicate and ar1 https://news.google.com/?hl=en-US&gl= US&ceid=US:en, crawled from Oct. 2018 to Jan. 2019. gument assignment, and ad-hoc clustering algorithms for inducing slots (Sha et al., 2016; Huang et al., 2016; Ahn, 2017; Yuan et al., 2018). These methods all rely on hand-crafted discrete features without fully model the textual redundancy. There are also works on modeling event schemas and scripts using n"
P19-1276,P15-1017,0,0.173081,"not explore neural latent variables and redundancy. Event Extraction work typically assumes that event schemas are given, recognizing event triggers and their corresponding arguments. This can be regarded as a subtask of ODEE. Existing work exploits sentence-level (McClosky et al., 2011; Li et al., 2013; Liu et al., 2016; Yang and Mitchell, 2016) and document-level statistics (Liao and Grishman, 2010b; Ji and Grishman, 2008; Hong et al., 2011; Reichart and Barzilay, 2012). There has also been work using RNNs (Nguyen et al., 2016b; Sha et al., 2018; Liu et al., 2018a; Chen et al., 2018), CNNs (Chen et al., 2015; Feng et al., 2016; Nguyen and Grishman, 2016) and GCNs (Liu et al., 2018b) to represent sentences of events. Event extraction has been treated as a supervised or semi-supervised (Liao and Grishman, 2010a; Huang and Riloff, 2012) task. In contrast, ODEE is a fully unsupervised setting. Event Discovery in Tweet Streams extracts news-worthy clusters of words, segments and frames. Both supervised and unsupervised methods have been used. The former (Sakaki et al., 2010; Benson et al., 2011) are typically designed to monitor certain event types, while the latter cluster features according to their"
P19-1276,D18-1158,0,0.0573605,"ooney, 2016), but they do not explore neural latent variables and redundancy. Event Extraction work typically assumes that event schemas are given, recognizing event triggers and their corresponding arguments. This can be regarded as a subtask of ODEE. Existing work exploits sentence-level (McClosky et al., 2011; Li et al., 2013; Liu et al., 2016; Yang and Mitchell, 2016) and document-level statistics (Liao and Grishman, 2010b; Ji and Grishman, 2008; Hong et al., 2011; Reichart and Barzilay, 2012). There has also been work using RNNs (Nguyen et al., 2016b; Sha et al., 2018; Liu et al., 2018a; Chen et al., 2018), CNNs (Chen et al., 2015; Feng et al., 2016; Nguyen and Grishman, 2016) and GCNs (Liu et al., 2018b) to represent sentences of events. Event extraction has been treated as a supervised or semi-supervised (Liao and Grishman, 2010a; Huang and Riloff, 2012) task. In contrast, ODEE is a fully unsupervised setting. Event Discovery in Tweet Streams extracts news-worthy clusters of words, segments and frames. Both supervised and unsupervised methods have been used. The former (Sakaki et al., 2010; Benson et al., 2011) are typically designed to monitor certain event types, while the latter cluster fe"
P19-1276,N13-1104,0,0.475598,"ered in traditional methods. First, more than one event can be extracted from a news cluster, where events can be flexible in having varying numbers of slots in the open domain, and slots can be flexible without identical distributions regardless of the event type, which has been assumed by previous work on schema induction. Second, mentions of the same entities from different reports in a news cluster should be taken into account for improved performance. We build an unsupervised generative model to address these challenges. While previous work on generative schema induction (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015) relies on hand-crafted indicator features, we introduce latent variables produced by neural networks for better representation power. A novel graph model 2860 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2860–2871 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics is designed, with a latent event type vector for each news cluster from a global parameterized normal distribution, and textual redundancy features for entities. Our model takes advantage of contextualized pre-trained lan"
P19-1276,P98-1013,0,0.235638,"ferent sources. In each news cluster, there are no more than five news reports. For each news report, we obtain the title, publish timestamp, download timestamp, source URL and full text. In total, we obtain 55,618 business news reports with 13,047 news clusters in 288 batches from Oct. 17, 2018, to Jan. 22, 2019. The crawler is executed about three times per day. The full text corpus is released as GNBusinessFull-Text. For this paper, we trim the news reports in each news cluster by keeping the title and first paragraph, releasing as GNBusiness-All. Inspired by the general slots in FrameNet (Baker et al., 1998), we design reference event schemas for open domain event types, which include eight possible slots: Agent, Patient, Time, Place, Aim, Old Value, New Value and Variation. Agent and Patient are the semantic agent and patient of the trigger, respectively; Aim is the target or reason for the event. If the event involves value changes, Old Value serves the old value, New Value serves the new value and Variation is the variation between New Value and Old Value. Note that the roles that we define are more thematic and less specific to detailed events as some of the existing event extraction datasets"
P19-1276,P03-1054,0,0.00900482,"or qω and the prior pα . Due 2864 pβ 0 ,θ,λ (s|e, t) ∝ pβ 0 ,θ,λ (s, h, f 0 , t) = pθ (s|t) × pλ (h|s) × pβ 0 (f 0 |s) (5) Name Slots number S Feature Dimension n Fully connected layer size MLP layer number Activation function Learning rate Momentum Dropout rate Batch size Value 30 256 100 1 softplus 0.002 0.99 0.2 200 Table 3: Hyper-parameters setting. 4.5 Assembling Events for Output To assemble the events in a news cluster c for final output, we need to find the predicate for each entity, which now has a slot value. We use POStags and parse trees produced by the Stanford dependency parser (Klein and Manning, 2003) to extract the predicate for the head word of each entity mention. The following rules are applied: (1) if the governor of a head word is VB, or (2) if the governor of a head word is NN and belongs to the noun.ACT or noun.EVENT category of WordNet, then it is regarded as a predicate. We merge the predicates of entity mentions in the same coreference chain as a predicate set. For each predicate v in these sets, we find the entities whose predicate set contains v, treating the entities as arguments of the event triggered by v. Finally, by ranking the numbers of arguments, we obtain top-N open-d"
P19-1276,H05-1016,0,0.0761238,"tain event types, while the latter cluster features according to their burstiness (Becker et al., 2011; Cui et al., 2012; Li et al., 2012; Ritter et al., 2012; Qin et al., 2013; Ifrim et al., 2014; McMinn and Jose, 2015; Qin et al., 2017). This line of work is similar to our work in using information redundancy, but different because we focus on formal news texts and induce structural event schemas. First Story Detection (FSD) systems aim to identify news articles that discuss events not reported before. Most work on FSD detects first stories by finding the nearest neighbors of new documents (Kumaran and Allan, 2005; Moran et al., 2016; Panagiotou et al., 2016; Vuurens and de Vries, 2016). This line of work exploits textual redundancy in massive streams predicting whether or not a document contains a new event as a clas2861 Split #C #R #S #W Test 574 2,433 5,830 96,745 Dev 106 414 991 16,839 Unlabelled 12,305 52,464 127,416 2,101,558 All 12,985 55,311 134,237 2,215,142 Full-Text 12,985 55,311 1,450,336 31,103,698 sification task. In contrast, we study the event schemas and extract detailed events. 3 Task and Data Task Definition. In ODEE, the input consists of news clusters, each containing reports about"
P19-1276,P16-2011,0,0.132178,"News Cluster Document News News Report News Report Report Introduction Extracting events from news text has received much research attention. The task typically consists of two subtasks, namely schema induction, which is to extract event templates that specify argument slots for given event types (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015; Sha et al., 2016; Huang et al., 2016; Ahn, 2017; Yuan et al., 2018), and event extraction, which is to identify events with filled slots from a piece of news (Nguyen et al., 2016b; Sha et al., 2018; Liu et al., 2018a; Chen et al., 2018, 2015; Feng et al., 2016; Nguyen and Grishman, 2016; Liu et al., 2018b). Previous work focuses on extracting events from single news documents according to a set of pre-specified event types, such as arson, attack or earthquakes. While useful for tracking highly specific types of events from news, the above setting can be relatively less useful for decision making in security and financial markets, which can require comprehensive knowledge on broad-coverage, finegrained and dynamically-evolving event categories. In addition, given the fact that different news agencies can report the same events, redundancy can be lev"
P19-1276,P06-2027,0,0.0604128,"ent types - Arson, Attack, Bombing and Kidnapping - and four slots - Perpetrator, Instrument, Target and Victim - are defined. We compare the task settings of MUC 4 and ODEE in Figure 1. For MUC 4, the inputs are single news documents, and the output belongs to four types of events with schemas consisting of fixed slots. For ODEE, in contrast, the inputs are news clusters rather than the individual news, and the output is unconstrained types of open domain events and unique schemas with various slot combinations. Event Schema Induction seminal work studies patterns (Shinyama and Sekine, 2006; Filatova et al., 2006; Qiu et al., 2008) and event chains (Chambers and Jurafsky, 2011) for template induction. For MUC 4, the current dominant methods include probabilistic generative methods (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015) that jointly model predicate and ar1 https://news.google.com/?hl=en-US&gl= US&ceid=US:en, crawled from Oct. 2018 to Jan. 2019. gument assignment, and ad-hoc clustering algorithms for inducing slots (Sha et al., 2016; Huang et al., 2016; Ahn, 2017; Yuan et al., 2018). These methods all rely on hand-crafted discrete features without fully model the textual redundancy."
P19-1276,P11-1113,0,0.0308881,"re are also works on modeling event schemas and scripts using neural language models (Modi and Titov, 2014; Rudinger et al., 2015; Pichotta and Mooney, 2016), but they do not explore neural latent variables and redundancy. Event Extraction work typically assumes that event schemas are given, recognizing event triggers and their corresponding arguments. This can be regarded as a subtask of ODEE. Existing work exploits sentence-level (McClosky et al., 2011; Li et al., 2013; Liu et al., 2016; Yang and Mitchell, 2016) and document-level statistics (Liao and Grishman, 2010b; Ji and Grishman, 2008; Hong et al., 2011; Reichart and Barzilay, 2012). There has also been work using RNNs (Nguyen et al., 2016b; Sha et al., 2018; Liu et al., 2018a; Chen et al., 2018), CNNs (Chen et al., 2015; Feng et al., 2016; Nguyen and Grishman, 2016) and GCNs (Liu et al., 2018b) to represent sentences of events. Event extraction has been treated as a supervised or semi-supervised (Liao and Grishman, 2010a; Huang and Riloff, 2012) task. In contrast, ODEE is a fully unsupervised setting. Event Discovery in Tweet Streams extracts news-worthy clusters of words, segments and frames. Both supervised and unsupervised methods have b"
P19-1276,P16-1025,0,0.0905794,"nd unique schemas with various slot combinations. Event Schema Induction seminal work studies patterns (Shinyama and Sekine, 2006; Filatova et al., 2006; Qiu et al., 2008) and event chains (Chambers and Jurafsky, 2011) for template induction. For MUC 4, the current dominant methods include probabilistic generative methods (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015) that jointly model predicate and ar1 https://news.google.com/?hl=en-US&gl= US&ceid=US:en, crawled from Oct. 2018 to Jan. 2019. gument assignment, and ad-hoc clustering algorithms for inducing slots (Sha et al., 2016; Huang et al., 2016; Ahn, 2017; Yuan et al., 2018). These methods all rely on hand-crafted discrete features without fully model the textual redundancy. There are also works on modeling event schemas and scripts using neural language models (Modi and Titov, 2014; Rudinger et al., 2015; Pichotta and Mooney, 2016), but they do not explore neural latent variables and redundancy. Event Extraction work typically assumes that event schemas are given, recognizing event triggers and their corresponding arguments. This can be regarded as a subtask of ODEE. Existing work exploits sentence-level (McClosky et al., 2011; Li"
P19-1276,E12-1029,0,0.0252816,"E. Existing work exploits sentence-level (McClosky et al., 2011; Li et al., 2013; Liu et al., 2016; Yang and Mitchell, 2016) and document-level statistics (Liao and Grishman, 2010b; Ji and Grishman, 2008; Hong et al., 2011; Reichart and Barzilay, 2012). There has also been work using RNNs (Nguyen et al., 2016b; Sha et al., 2018; Liu et al., 2018a; Chen et al., 2018), CNNs (Chen et al., 2015; Feng et al., 2016; Nguyen and Grishman, 2016) and GCNs (Liu et al., 2018b) to represent sentences of events. Event extraction has been treated as a supervised or semi-supervised (Liao and Grishman, 2010a; Huang and Riloff, 2012) task. In contrast, ODEE is a fully unsupervised setting. Event Discovery in Tweet Streams extracts news-worthy clusters of words, segments and frames. Both supervised and unsupervised methods have been used. The former (Sakaki et al., 2010; Benson et al., 2011) are typically designed to monitor certain event types, while the latter cluster features according to their burstiness (Becker et al., 2011; Cui et al., 2012; Li et al., 2012; Ritter et al., 2012; Qin et al., 2013; Ifrim et al., 2014; McMinn and Jose, 2015; Qin et al., 2017). This line of work is similar to our work in using informatio"
P19-1276,E14-1056,0,0.219679,"guistics is designed, with a latent event type vector for each news cluster from a global parameterized normal distribution, and textual redundancy features for entities. Our model takes advantage of contextualized pre-trained language model (ELMo, Peters et al. (2018)) and scalable neural variational inference (Srivastava and Sutton, 2017). To evaluate model performance, we collect and annotate a large-scale dataset from Google Business News1 with diverse event types and explainable event schemas. In addition to the standard metrics for schema matching, we adapt slot coherence based on NPMI (Lau et al., 2014) for quantitatively measuring the intrinsic qualities of slots and schemas, which are inherently clusters. Results show that our neural latent variable model outperforms state-of-the-art event schema induction methods. In addition, redundancy is highly useful for improving open domain event extraction. Visualizations of learned parameters show that our model can give reasonable latent event types. To our knowledge, we are the first to use neural latent variable model for inducing event schemas and extracting events. We release our code and dataset at https://github.com/ lx865712528/ACL2019-ODE"
P19-1276,P13-1008,0,0.303462,"016; Ahn, 2017; Yuan et al., 2018). These methods all rely on hand-crafted discrete features without fully model the textual redundancy. There are also works on modeling event schemas and scripts using neural language models (Modi and Titov, 2014; Rudinger et al., 2015; Pichotta and Mooney, 2016), but they do not explore neural latent variables and redundancy. Event Extraction work typically assumes that event schemas are given, recognizing event triggers and their corresponding arguments. This can be regarded as a subtask of ODEE. Existing work exploits sentence-level (McClosky et al., 2011; Li et al., 2013; Liu et al., 2016; Yang and Mitchell, 2016) and document-level statistics (Liao and Grishman, 2010b; Ji and Grishman, 2008; Hong et al., 2011; Reichart and Barzilay, 2012). There has also been work using RNNs (Nguyen et al., 2016b; Sha et al., 2018; Liu et al., 2018a; Chen et al., 2018), CNNs (Chen et al., 2015; Feng et al., 2016; Nguyen and Grishman, 2016) and GCNs (Liu et al., 2018b) to represent sentences of events. Event extraction has been treated as a supervised or semi-supervised (Liao and Grishman, 2010a; Huang and Riloff, 2012) task. In contrast, ODEE is a fully unsupervised setting."
P19-1276,C10-1077,0,0.0369674,"s without fully model the textual redundancy. There are also works on modeling event schemas and scripts using neural language models (Modi and Titov, 2014; Rudinger et al., 2015; Pichotta and Mooney, 2016), but they do not explore neural latent variables and redundancy. Event Extraction work typically assumes that event schemas are given, recognizing event triggers and their corresponding arguments. This can be regarded as a subtask of ODEE. Existing work exploits sentence-level (McClosky et al., 2011; Li et al., 2013; Liu et al., 2016; Yang and Mitchell, 2016) and document-level statistics (Liao and Grishman, 2010b; Ji and Grishman, 2008; Hong et al., 2011; Reichart and Barzilay, 2012). There has also been work using RNNs (Nguyen et al., 2016b; Sha et al., 2018; Liu et al., 2018a; Chen et al., 2018), CNNs (Chen et al., 2015; Feng et al., 2016; Nguyen and Grishman, 2016) and GCNs (Liu et al., 2018b) to represent sentences of events. Event extraction has been treated as a supervised or semi-supervised (Liao and Grishman, 2010a; Huang and Riloff, 2012) task. In contrast, ODEE is a fully unsupervised setting. Event Discovery in Tweet Streams extracts news-worthy clusters of words, segments and frames. Both"
P19-1276,P10-1081,0,0.126776,"s without fully model the textual redundancy. There are also works on modeling event schemas and scripts using neural language models (Modi and Titov, 2014; Rudinger et al., 2015; Pichotta and Mooney, 2016), but they do not explore neural latent variables and redundancy. Event Extraction work typically assumes that event schemas are given, recognizing event triggers and their corresponding arguments. This can be regarded as a subtask of ODEE. Existing work exploits sentence-level (McClosky et al., 2011; Li et al., 2013; Liu et al., 2016; Yang and Mitchell, 2016) and document-level statistics (Liao and Grishman, 2010b; Ji and Grishman, 2008; Hong et al., 2011; Reichart and Barzilay, 2012). There has also been work using RNNs (Nguyen et al., 2016b; Sha et al., 2018; Liu et al., 2018a; Chen et al., 2018), CNNs (Chen et al., 2015; Feng et al., 2016; Nguyen and Grishman, 2016) and GCNs (Liu et al., 2018b) to represent sentences of events. Event extraction has been treated as a supervised or semi-supervised (Liao and Grishman, 2010a; Huang and Riloff, 2012) task. In contrast, ODEE is a fully unsupervised setting. Event Discovery in Tweet Streams extracts news-worthy clusters of words, segments and frames. Both"
P19-1276,P16-1201,0,0.0176097,"uan et al., 2018). These methods all rely on hand-crafted discrete features without fully model the textual redundancy. There are also works on modeling event schemas and scripts using neural language models (Modi and Titov, 2014; Rudinger et al., 2015; Pichotta and Mooney, 2016), but they do not explore neural latent variables and redundancy. Event Extraction work typically assumes that event schemas are given, recognizing event triggers and their corresponding arguments. This can be regarded as a subtask of ODEE. Existing work exploits sentence-level (McClosky et al., 2011; Li et al., 2013; Liu et al., 2016; Yang and Mitchell, 2016) and document-level statistics (Liao and Grishman, 2010b; Ji and Grishman, 2008; Hong et al., 2011; Reichart and Barzilay, 2012). There has also been work using RNNs (Nguyen et al., 2016b; Sha et al., 2018; Liu et al., 2018a; Chen et al., 2018), CNNs (Chen et al., 2015; Feng et al., 2016; Nguyen and Grishman, 2016) and GCNs (Liu et al., 2018b) to represent sentences of events. Event extraction has been treated as a supervised or semi-supervised (Liao and Grishman, 2010a; Huang and Riloff, 2012) task. In contrast, ODEE is a fully unsupervised setting. Event Discovery i"
P19-1276,P08-1030,0,0.432239,"textual redundancy. There are also works on modeling event schemas and scripts using neural language models (Modi and Titov, 2014; Rudinger et al., 2015; Pichotta and Mooney, 2016), but they do not explore neural latent variables and redundancy. Event Extraction work typically assumes that event schemas are given, recognizing event triggers and their corresponding arguments. This can be regarded as a subtask of ODEE. Existing work exploits sentence-level (McClosky et al., 2011; Li et al., 2013; Liu et al., 2016; Yang and Mitchell, 2016) and document-level statistics (Liao and Grishman, 2010b; Ji and Grishman, 2008; Hong et al., 2011; Reichart and Barzilay, 2012). There has also been work using RNNs (Nguyen et al., 2016b; Sha et al., 2018; Liu et al., 2018a; Chen et al., 2018), CNNs (Chen et al., 2015; Feng et al., 2016; Nguyen and Grishman, 2016) and GCNs (Liu et al., 2018b) to represent sentences of events. Event extraction has been treated as a supervised or semi-supervised (Liao and Grishman, 2010a; Huang and Riloff, 2012) task. In contrast, ODEE is a fully unsupervised setting. Event Discovery in Tweet Streams extracts news-worthy clusters of words, segments and frames. Both supervised and unsuperv"
P19-1276,D18-1156,1,0.90501,"Missing"
P19-1276,P11-1163,0,0.1642,"Missing"
P19-1276,W14-1606,0,0.0125304,". For MUC 4, the current dominant methods include probabilistic generative methods (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015) that jointly model predicate and ar1 https://news.google.com/?hl=en-US&gl= US&ceid=US:en, crawled from Oct. 2018 to Jan. 2019. gument assignment, and ad-hoc clustering algorithms for inducing slots (Sha et al., 2016; Huang et al., 2016; Ahn, 2017; Yuan et al., 2018). These methods all rely on hand-crafted discrete features without fully model the textual redundancy. There are also works on modeling event schemas and scripts using neural language models (Modi and Titov, 2014; Rudinger et al., 2015; Pichotta and Mooney, 2016), but they do not explore neural latent variables and redundancy. Event Extraction work typically assumes that event schemas are given, recognizing event triggers and their corresponding arguments. This can be regarded as a subtask of ODEE. Existing work exploits sentence-level (McClosky et al., 2011; Li et al., 2013; Liu et al., 2016; Yang and Mitchell, 2016) and document-level statistics (Liao and Grishman, 2010b; Ji and Grishman, 2008; Hong et al., 2011; Reichart and Barzilay, 2012). There has also been work using RNNs (Nguyen et al., 2016b"
P19-1276,N10-1012,0,0.117149,"Missing"
P19-1276,P15-1019,0,0.186122,"Missing"
P19-1276,L16-1307,0,0.0436377,"Missing"
P19-1276,N16-1034,0,0.6201,"Modi and Titov, 2014; Rudinger et al., 2015; Pichotta and Mooney, 2016), but they do not explore neural latent variables and redundancy. Event Extraction work typically assumes that event schemas are given, recognizing event triggers and their corresponding arguments. This can be regarded as a subtask of ODEE. Existing work exploits sentence-level (McClosky et al., 2011; Li et al., 2013; Liu et al., 2016; Yang and Mitchell, 2016) and document-level statistics (Liao and Grishman, 2010b; Ji and Grishman, 2008; Hong et al., 2011; Reichart and Barzilay, 2012). There has also been work using RNNs (Nguyen et al., 2016b; Sha et al., 2018; Liu et al., 2018a; Chen et al., 2018), CNNs (Chen et al., 2015; Feng et al., 2016; Nguyen and Grishman, 2016) and GCNs (Liu et al., 2018b) to represent sentences of events. Event extraction has been treated as a supervised or semi-supervised (Liao and Grishman, 2010a; Huang and Riloff, 2012) task. In contrast, ODEE is a fully unsupervised setting. Event Discovery in Tweet Streams extracts news-worthy clusters of words, segments and frames. Both supervised and unsupervised methods have been used. The former (Sakaki et al., 2010; Benson et al., 2011) are typically designed t"
P19-1276,D16-1085,0,0.0977141,"nt News News Report News Report Report Introduction Extracting events from news text has received much research attention. The task typically consists of two subtasks, namely schema induction, which is to extract event templates that specify argument slots for given event types (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015; Sha et al., 2016; Huang et al., 2016; Ahn, 2017; Yuan et al., 2018), and event extraction, which is to identify events with filled slots from a piece of news (Nguyen et al., 2016b; Sha et al., 2018; Liu et al., 2018a; Chen et al., 2018, 2015; Feng et al., 2016; Nguyen and Grishman, 2016; Liu et al., 2018b). Previous work focuses on extracting events from single news documents according to a set of pre-specified event types, such as arson, attack or earthquakes. While useful for tracking highly specific types of events from news, the above setting can be relatively less useful for decision making in security and financial markets, which can require comprehensive knowledge on broad-coverage, finegrained and dynamically-evolving event categories. In addition, given the fact that different news agencies can report the same events, redundancy can be leveraged for better event ext"
P19-1276,N18-1202,0,0.0745603,"15) relies on hand-crafted indicator features, we introduce latent variables produced by neural networks for better representation power. A novel graph model 2860 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2860–2871 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics is designed, with a latent event type vector for each news cluster from a global parameterized normal distribution, and textual redundancy features for entities. Our model takes advantage of contextualized pre-trained language model (ELMo, Peters et al. (2018)) and scalable neural variational inference (Srivastava and Sutton, 2017). To evaluate model performance, we collect and annotate a large-scale dataset from Google Business News1 with diverse event types and explainable event schemas. In addition to the standard metrics for schema matching, we adapt slot coherence based on NPMI (Lau et al., 2014) for quantitatively measuring the intrinsic qualities of slots and schemas, which are inherently clusters. Results show that our neural latent variable model outperforms state-of-the-art event schema induction methods. In addition, redundancy is highly"
P19-1276,P16-1027,0,0.0203117,"lude probabilistic generative methods (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015) that jointly model predicate and ar1 https://news.google.com/?hl=en-US&gl= US&ceid=US:en, crawled from Oct. 2018 to Jan. 2019. gument assignment, and ad-hoc clustering algorithms for inducing slots (Sha et al., 2016; Huang et al., 2016; Ahn, 2017; Yuan et al., 2018). These methods all rely on hand-crafted discrete features without fully model the textual redundancy. There are also works on modeling event schemas and scripts using neural language models (Modi and Titov, 2014; Rudinger et al., 2015; Pichotta and Mooney, 2016), but they do not explore neural latent variables and redundancy. Event Extraction work typically assumes that event schemas are given, recognizing event triggers and their corresponding arguments. This can be regarded as a subtask of ODEE. Existing work exploits sentence-level (McClosky et al., 2011; Li et al., 2013; Liu et al., 2016; Yang and Mitchell, 2016) and document-level statistics (Liao and Grishman, 2010b; Ji and Grishman, 2008; Hong et al., 2011; Reichart and Barzilay, 2012). There has also been work using RNNs (Nguyen et al., 2016b; Sha et al., 2018; Liu et al., 2018a; Chen et al.,"
P19-1276,I13-1035,1,0.837493,"sentences of events. Event extraction has been treated as a supervised or semi-supervised (Liao and Grishman, 2010a; Huang and Riloff, 2012) task. In contrast, ODEE is a fully unsupervised setting. Event Discovery in Tweet Streams extracts news-worthy clusters of words, segments and frames. Both supervised and unsupervised methods have been used. The former (Sakaki et al., 2010; Benson et al., 2011) are typically designed to monitor certain event types, while the latter cluster features according to their burstiness (Becker et al., 2011; Cui et al., 2012; Li et al., 2012; Ritter et al., 2012; Qin et al., 2013; Ifrim et al., 2014; McMinn and Jose, 2015; Qin et al., 2017). This line of work is similar to our work in using information redundancy, but different because we focus on formal news texts and induce structural event schemas. First Story Detection (FSD) systems aim to identify news articles that discuss events not reported before. Most work on FSD detects first stories by finding the nearest neighbors of new documents (Kumaran and Allan, 2005; Moran et al., 2016; Panagiotou et al., 2016; Vuurens and de Vries, 2016). This line of work exploits textual redundancy in massive streams predicting w"
P19-1276,I08-1021,0,0.0431016,"ck, Bombing and Kidnapping - and four slots - Perpetrator, Instrument, Target and Victim - are defined. We compare the task settings of MUC 4 and ODEE in Figure 1. For MUC 4, the inputs are single news documents, and the output belongs to four types of events with schemas consisting of fixed slots. For ODEE, in contrast, the inputs are news clusters rather than the individual news, and the output is unconstrained types of open domain events and unique schemas with various slot combinations. Event Schema Induction seminal work studies patterns (Shinyama and Sekine, 2006; Filatova et al., 2006; Qiu et al., 2008) and event chains (Chambers and Jurafsky, 2011) for template induction. For MUC 4, the current dominant methods include probabilistic generative methods (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015) that jointly model predicate and ar1 https://news.google.com/?hl=en-US&gl= US&ceid=US:en, crawled from Oct. 2018 to Jan. 2019. gument assignment, and ad-hoc clustering algorithms for inducing slots (Sha et al., 2016; Huang et al., 2016; Ahn, 2017; Yuan et al., 2018). These methods all rely on hand-crafted discrete features without fully model the textual redundancy. There are also work"
P19-1276,N12-1008,0,0.0313509,"n modeling event schemas and scripts using neural language models (Modi and Titov, 2014; Rudinger et al., 2015; Pichotta and Mooney, 2016), but they do not explore neural latent variables and redundancy. Event Extraction work typically assumes that event schemas are given, recognizing event triggers and their corresponding arguments. This can be regarded as a subtask of ODEE. Existing work exploits sentence-level (McClosky et al., 2011; Li et al., 2013; Liu et al., 2016; Yang and Mitchell, 2016) and document-level statistics (Liao and Grishman, 2010b; Ji and Grishman, 2008; Hong et al., 2011; Reichart and Barzilay, 2012). There has also been work using RNNs (Nguyen et al., 2016b; Sha et al., 2018; Liu et al., 2018a; Chen et al., 2018), CNNs (Chen et al., 2015; Feng et al., 2016; Nguyen and Grishman, 2016) and GCNs (Liu et al., 2018b) to represent sentences of events. Event extraction has been treated as a supervised or semi-supervised (Liao and Grishman, 2010a; Huang and Riloff, 2012) task. In contrast, ODEE is a fully unsupervised setting. Event Discovery in Tweet Streams extracts news-worthy clusters of words, segments and frames. Both supervised and unsupervised methods have been used. The former (Sakaki e"
P19-1276,D15-1195,0,0.0473595,"Missing"
P19-1276,N16-1049,0,0.272957,"en domain events and unique schemas with various slot combinations. Event Schema Induction seminal work studies patterns (Shinyama and Sekine, 2006; Filatova et al., 2006; Qiu et al., 2008) and event chains (Chambers and Jurafsky, 2011) for template induction. For MUC 4, the current dominant methods include probabilistic generative methods (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015) that jointly model predicate and ar1 https://news.google.com/?hl=en-US&gl= US&ceid=US:en, crawled from Oct. 2018 to Jan. 2019. gument assignment, and ad-hoc clustering algorithms for inducing slots (Sha et al., 2016; Huang et al., 2016; Ahn, 2017; Yuan et al., 2018). These methods all rely on hand-crafted discrete features without fully model the textual redundancy. There are also works on modeling event schemas and scripts using neural language models (Modi and Titov, 2014; Rudinger et al., 2015; Pichotta and Mooney, 2016), but they do not explore neural latent variables and redundancy. Event Extraction work typically assumes that event schemas are given, recognizing event triggers and their corresponding arguments. This can be regarded as a subtask of ODEE. Existing work exploits sentence-level (McClos"
P19-1276,N06-1039,0,0.0720711,"is MUC 4, in which four event types - Arson, Attack, Bombing and Kidnapping - and four slots - Perpetrator, Instrument, Target and Victim - are defined. We compare the task settings of MUC 4 and ODEE in Figure 1. For MUC 4, the inputs are single news documents, and the output belongs to four types of events with schemas consisting of fixed slots. For ODEE, in contrast, the inputs are news clusters rather than the individual news, and the output is unconstrained types of open domain events and unique schemas with various slot combinations. Event Schema Induction seminal work studies patterns (Shinyama and Sekine, 2006; Filatova et al., 2006; Qiu et al., 2008) and event chains (Chambers and Jurafsky, 2011) for template induction. For MUC 4, the current dominant methods include probabilistic generative methods (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015) that jointly model predicate and ar1 https://news.google.com/?hl=en-US&gl= US&ceid=US:en, crawled from Oct. 2018 to Jan. 2019. gument assignment, and ad-hoc clustering algorithms for inducing slots (Sha et al., 2016; Huang et al., 2016; Ahn, 2017; Yuan et al., 2018). These methods all rely on hand-crafted discrete features without fully model t"
P19-1276,M92-1001,0,0.505227,"hich can require comprehensive knowledge on broad-coverage, finegrained and dynamically-evolving event categories. In addition, given the fact that different news agencies can report the same events, redundancy can be leveraged for better event extraction. In this paper, we investigate open domain ∗ Corresponding author. Figure 1: Comparison between MUC 4 and ODEE. event extraction (ODEE), which is to extract unconstraint types of events and induce universal event schemas from clusters of news reports. As shown in Figure 1, compared with traditional event extraction task exemplified by MUC 4 (Sundheim, 1992), the task of ODEE poses additional challenges to modeling, which have not been considered in traditional methods. First, more than one event can be extracted from a news cluster, where events can be flexible in having varying numbers of slots in the open domain, and slots can be flexible without identical distributions regardless of the event type, which has been assumed by previous work on schema induction. Second, mentions of the same entities from different reports in a news cluster should be taken into account for improved performance. We build an unsupervised generative model to address"
P19-1276,N16-1033,0,0.112869,"These methods all rely on hand-crafted discrete features without fully model the textual redundancy. There are also works on modeling event schemas and scripts using neural language models (Modi and Titov, 2014; Rudinger et al., 2015; Pichotta and Mooney, 2016), but they do not explore neural latent variables and redundancy. Event Extraction work typically assumes that event schemas are given, recognizing event triggers and their corresponding arguments. This can be regarded as a subtask of ODEE. Existing work exploits sentence-level (McClosky et al., 2011; Li et al., 2013; Liu et al., 2016; Yang and Mitchell, 2016) and document-level statistics (Liao and Grishman, 2010b; Ji and Grishman, 2008; Hong et al., 2011; Reichart and Barzilay, 2012). There has also been work using RNNs (Nguyen et al., 2016b; Sha et al., 2018; Liu et al., 2018a; Chen et al., 2018), CNNs (Chen et al., 2015; Feng et al., 2016; Nguyen and Grishman, 2016) and GCNs (Liu et al., 2018b) to represent sentences of events. Event extraction has been treated as a supervised or semi-supervised (Liao and Grishman, 2010a; Huang and Riloff, 2012) task. In contrast, ODEE is a fully unsupervised setting. Event Discovery in Tweet Streams extracts n"
S16-1105,S12-1051,0,0.280188,"015 STS shared task. The results suggest that beyond the core similarity algorithm, other factors such as data preprocessing and use of domain-specific knowledge are also important to similarity prediction performance. 1 System Overview 2.1 Introduction Given two short texts or sentences, similarity systems or models should output a score that reflects how similar the two texts are in meaning. Semantic textual similarity (STS) formalizes an operation that is an important component of many natural language processing systems and has generated substantial interest within the research community (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). STS methods can be applied in example-based machine translation, machine translation evaluation, information retrieval, text summarization, question answering, and recommendation systems. ∗ Corresponding author Run 1: Simple Vector Method In this run, we use a sentence vector derived from word embeddings obtained from word2vec (Mikolov et al., 2013). Using these sentence level vector representations, the similarity between two texts can be computed using the cosine operation. We train word embeddings by running the word2vec tool"
S16-1105,S14-2010,0,0.062145,"hat beyond the core similarity algorithm, other factors such as data preprocessing and use of domain-specific knowledge are also important to similarity prediction performance. 1 System Overview 2.1 Introduction Given two short texts or sentences, similarity systems or models should output a score that reflects how similar the two texts are in meaning. Semantic textual similarity (STS) formalizes an operation that is an important component of many natural language processing systems and has generated substantial interest within the research community (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). STS methods can be applied in example-based machine translation, machine translation evaluation, information retrieval, text summarization, question answering, and recommendation systems. ∗ Corresponding author Run 1: Simple Vector Method In this run, we use a sentence vector derived from word embeddings obtained from word2vec (Mikolov et al., 2013). Using these sentence level vector representations, the similarity between two texts can be computed using the cosine operation. We train word embeddings by running the word2vec toolkit1 over the fifth edition of the Gigawor"
S16-1105,S15-2045,0,0.0300731,"Missing"
S16-1105,P06-4018,0,0.0441026,"cosine of their associated sentence level embedding vectors. 686 Proceedings of SemEval-2016, pages 686–690, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics 2.2 Run 2: Weighted Vector Method The above method weights all word embeddings equally. We submitted an alternative run that weights the word embeddings by the information content (IC) of the concepts referenced by their word sense tagged tokens (Resnik, 1995). Word sense disambiguation is performed using BabelNet (Navigli and Ponzetto, 2012) with the WordNet (Miller, 1995) sense inventory. NLTK (Bird, 2006) is used to obtain the frequencies of words belongs to the WordNet synset. The probability associated with each concept is estimated over the BNC1 using add one smoothing. Following Resnik (1995), we then compute the information content of each concept as follows: (2) IC (c) = − log P (c). Here P (c) refers to the statistical frequency of concept c. This method allows us to compute IC based weights only for the nouns and verbs covered by WordNet. We heuristically set the weight of adjectives and adverbs to 5 and other words to 2. 2.3 Run 3: Word Alignment Method Our final run differs from the"
S16-1105,P07-2045,0,0.00312305,"ormation retrieval, text summarization, question answering, and recommendation systems. ∗ Corresponding author Run 1: Simple Vector Method In this run, we use a sentence vector derived from word embeddings obtained from word2vec (Mikolov et al., 2013). Using these sentence level vector representations, the similarity between two texts can be computed using the cosine operation. We train word embeddings by running the word2vec toolkit1 over the fifth edition of the Gigaword corpus (LDC2011T07). We preprocess the Gigaword data with the following tools from the Moses machine translation toolkit (Koehn et al., 2007): the data is tokenized using tokenizer.perl; truecase.perl4 is used to standardize capitalizing. As illustrated in Equation (1), we construct the sentence vector ~s by simply summing together the word embeddings, t~i , associated with each token in a sentence. * s= |s| X ~ti . (1) i=1 Here |s |is the number of tokens that the sentence contains. The similarity between a pair of sentences is computed as the cosine of their associated sentence level embedding vectors. 686 Proceedings of SemEval-2016, pages 686–690, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Lin"
S16-1105,Q14-1018,0,0.344593,"add one smoothing. Following Resnik (1995), we then compute the information content of each concept as follows: (2) IC (c) = − log P (c). Here P (c) refers to the statistical frequency of concept c. This method allows us to compute IC based weights only for the nouns and verbs covered by WordNet. We heuristically set the weight of adjectives and adverbs to 5 and other words to 2. 2.3 Run 3: Word Alignment Method Our final run differs from the vector based methods described above and follows a popular alternative approach to assessing sentence similarity through word alignments. We make use of Sultan et al. (2014a)’s open-source monolingual word aligner with default parameters and the similarity formula proposed in Sultan et al. (2015). An unsupervised system based on Sultan et al. (2015)’s similarity formula above took fifth place at STS 2015. Its predecessor, based on a similar formula, took 1st place at STS 2014. As shown in Equation (3), similarity is computed as   sts S (1) , S (2) =      nac S (1) + nac S (2)  nc S (1) + nc S (2)     . (3) Here nac S (i) and nc S (i) are the number of content words and the number of aligned content words in sentence S (i) , respectively. 1 http://ota"
S16-1105,S14-2039,0,0.202879,"add one smoothing. Following Resnik (1995), we then compute the information content of each concept as follows: (2) IC (c) = − log P (c). Here P (c) refers to the statistical frequency of concept c. This method allows us to compute IC based weights only for the nouns and verbs covered by WordNet. We heuristically set the weight of adjectives and adverbs to 5 and other words to 2. 2.3 Run 3: Word Alignment Method Our final run differs from the vector based methods described above and follows a popular alternative approach to assessing sentence similarity through word alignments. We make use of Sultan et al. (2014a)’s open-source monolingual word aligner with default parameters and the similarity formula proposed in Sultan et al. (2015). An unsupervised system based on Sultan et al. (2015)’s similarity formula above took fifth place at STS 2015. Its predecessor, based on a similar formula, took 1st place at STS 2014. As shown in Equation (3), similarity is computed as   sts S (1) , S (2) =      nac S (1) + nac S (2)  nc S (1) + nc S (2)     . (3) Here nac S (i) and nc S (i) are the number of content words and the number of aligned content words in sentence S (i) , respectively. 1 http://ota"
S16-1105,S15-2027,0,0.191946,"− log P (c). Here P (c) refers to the statistical frequency of concept c. This method allows us to compute IC based weights only for the nouns and verbs covered by WordNet. We heuristically set the weight of adjectives and adverbs to 5 and other words to 2. 2.3 Run 3: Word Alignment Method Our final run differs from the vector based methods described above and follows a popular alternative approach to assessing sentence similarity through word alignments. We make use of Sultan et al. (2014a)’s open-source monolingual word aligner with default parameters and the similarity formula proposed in Sultan et al. (2015). An unsupervised system based on Sultan et al. (2015)’s similarity formula above took fifth place at STS 2015. Its predecessor, based on a similar formula, took 1st place at STS 2014. As shown in Equation (3), similarity is computed as   sts S (1) , S (2) =      nac S (1) + nac S (2)  nc S (1) + nc S (2)     . (3) Here nac S (i) and nc S (i) are the number of content words and the number of aligned content words in sentence S (i) , respectively. 1 http://ota.ox.ac.uk/desc/2554 687 No. 1 2 3 4 5 Dataset answer-answer headlines plagiarism postediting question-question Total Total Pa"
S17-2007,S17-2001,0,0.146023,"on 2 Given two snippets of text, semantic textual similarity (STS) measures the degree of equivalence in the underlying semantics. STS is a basic but important issue with multitude of application areas in natural language processing (NLP) such as example based machine translation (EBMT), machine translation evaluation, information retrieval (IR), question answering (QA), text summarization and so on. The SemEval STS task has become the most famous activity for STS evaluation in recent years and the STS shared task has been held annually since 2012 (Agirre et al., 2012, 2013, 2014, 2015, 2016; Cer et al., 2017), as part of the SemEval/*SEM family of workshops. The organizers have set up publicly available datasets of sentence pairs with similarity scores from human annotators, which are up to more than 16,000 Preliminaries Following the standard argumentation of information theory, Resnik (1995) proposed the definition of the information content (IC) of a concept as follows: IC (c) = − log P(c), (1) where P(c) refers to statistical frequency of concept c. Since information content (IC) for multiple words, which sums the non-overlapping concepts IC, is a computational difficulties for knowledge based"
S17-2007,S12-1051,0,0.0788555,"ance on Track 1 (AR-AR) dataset. 1 Introduction 2 Given two snippets of text, semantic textual similarity (STS) measures the degree of equivalence in the underlying semantics. STS is a basic but important issue with multitude of application areas in natural language processing (NLP) such as example based machine translation (EBMT), machine translation evaluation, information retrieval (IR), question answering (QA), text summarization and so on. The SemEval STS task has become the most famous activity for STS evaluation in recent years and the STS shared task has been held annually since 2012 (Agirre et al., 2012, 2013, 2014, 2015, 2016; Cer et al., 2017), as part of the SemEval/*SEM family of workshops. The organizers have set up publicly available datasets of sentence pairs with similarity scores from human annotators, which are up to more than 16,000 Preliminaries Following the standard argumentation of information theory, Resnik (1995) proposed the definition of the information content (IC) of a concept as follows: IC (c) = − log P(c), (1) where P(c) refers to statistical frequency of concept c. Since information content (IC) for multiple words, which sums the non-overlapping concepts IC, is a com"
S17-2007,S13-1005,0,0.0508968,"yan Huang∗, Ping Jian, Yuhang Guo, Chao Su Beijing Engineering Research Center of High Volume Language Information Processing and Cloud Computing Applications, School of Computer Science, Beijing Institute of Technology, Beijing, China {wuhao123, hhy63, pjian, guoyuhang, suchao}@bit.edu.cn Abstract sentence pairs for training and evaluation, and attracted a large number of teams with a variety of systems to participate the competitions. Generally, STS systems could be divided into two categories: One kind is unsupervised systems (Li et al., 2006; Mihalcea et al., 2006; Islam and Inkpen, 2008; Han et al., 2013; Sultan et al., 2014b; Wu and Huang, 2016), some of which are appeared for a long time when there wasn’t enough training data; The other kind is ˇ c et al., supervised systems (B¨ar et al., 2012; Sari´ 2012; Sultan et al., 2015; Rychalska et al., 2016; Brychc´ın and Svoboda, 2016) applying machine learning algorithms, including deep learning, after adequate training data has been constructed. Each kind of methods has its advantages and application areas. In this paper, we present three systems, one unsupervised system and two supervised systems which simply make use of the unsupervised one. T"
S17-2007,S13-1004,0,0.0669884,"Missing"
S17-2007,O97-1002,0,0.615457,"atasets of sentence pairs with similarity scores from human annotators, which are up to more than 16,000 Preliminaries Following the standard argumentation of information theory, Resnik (1995) proposed the definition of the information content (IC) of a concept as follows: IC (c) = − log P(c), (1) where P(c) refers to statistical frequency of concept c. Since information content (IC) for multiple words, which sums the non-overlapping concepts IC, is a computational difficulties for knowledge based methods. For a long time, IC related methods were usually used as word similarity (Resnik, 1995; Jiang and Conrath, 1997; Lin, 1997) or word weight (Li et al., 2006; Han et al., 2013) rather than the core evaluation modules of sentence similarity methods (Wu and Huang, 2016). ∗ Corresponding author 77 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 77–84, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics 2.1 STS evaluation using SIS Algorithm 1: getInExT otalIC(S ) Input: S : {ci |i = 1, 2, . . . , n; n = |S |} Output: tIC: Total IC of input S 1 if S = ∅ then 2 return 0 To apply non-overlapping IC of sentences in STS evaluation,"
S17-2007,S12-1059,0,0.0949057,"Missing"
S17-2007,P07-2045,0,0.00321038,"ential minimal optimization (Shevade et al., 2000) (SMO). There two features: One is the output of SIS, the other is that of unsupervised method of Sultan et al. (2015). Actually, we tested some other regression methods. We found that LR and SVM always outperform the others. The tool for regression methods are implemented in WEKA (Hall et al., 2009). First of all, we translated all the other languages into English by employing Google machine translation system3 and preprocessed the test datasets with tokenizer.perl and truecase.perl, which are the tools from Moses machine translation toolkit (Koehn et al., 2007), then utilized the preprocessed datasets to do POS obtaining and lemmatization by utilizing NLTK (Bird, 2006), and finally made use of lemma to do sentence alignment (Sultan et al., 2014a) and name entity recognition (Manning et al., 2014). We use the lemma instead of the original word in all the situations where need words to participate for the consideration of simplicity. We also developed a word spelling correction module based on Levenshtein distance which is special for the spelling mistakes in STS datasets. It proved important for the eventual performances in previous years, however, i"
S17-2007,P06-4018,0,0.141137,"asets, we simulate the IC of out-of-vocabulary NEs in SIS. Finally, sentence IC is augmented by word weights which could deem as the importance of words. The above contents of this subsection is mainly based on the work which is currently under review. 3 the probability of a concept c is: P n∈words(c) count(n) P (c) = N where words (c) is the set of all the words contained in concept c and its sub-concepts in WordNet, N is the sum of frequencies of words contained in all the concepts in the hierarchy of semantic net. The word statistics are from British National Corpus (BNC) obtained by NLTK (Bird, 2006). Sentence IC computation applies Equation (9). For the simplification, we choose the concept of a word with the minimal IC, which denotes the most common sense of a word, in all the circumstances of conversion of word-to-concept and the selection between two aligned words, instead of word sense disambiguation (WSD). System Overview We submitted three systems: One is the unsupervised system of exploiting non-overlapping IC in SIS, the other two are supervised systems of making use of the methods of sentence alignment and word embedding respectively. 3.1 3.3 Preprocessing 3.4 Run 3: Supervised"
S17-2007,P97-1009,0,0.0690985,"with similarity scores from human annotators, which are up to more than 16,000 Preliminaries Following the standard argumentation of information theory, Resnik (1995) proposed the definition of the information content (IC) of a concept as follows: IC (c) = − log P(c), (1) where P(c) refers to statistical frequency of concept c. Since information content (IC) for multiple words, which sums the non-overlapping concepts IC, is a computational difficulties for knowledge based methods. For a long time, IC related methods were usually used as word similarity (Resnik, 1995; Jiang and Conrath, 1997; Lin, 1997) or word weight (Li et al., 2006; Han et al., 2013) rather than the core evaluation modules of sentence similarity methods (Wu and Huang, 2016). ∗ Corresponding author 77 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 77–84, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics 2.1 STS evaluation using SIS Algorithm 1: getInExT otalIC(S ) Input: S : {ci |i = 1, 2, . . . , n; n = |S |} Output: tIC: Total IC of input S 1 if S = ∅ then 2 return 0 To apply non-overlapping IC of sentences in STS evaluation, we construct"
S17-2007,Q14-1018,0,0.0230283,"Missing"
S17-2007,S14-2039,0,0.18432,"ian, Yuhang Guo, Chao Su Beijing Engineering Research Center of High Volume Language Information Processing and Cloud Computing Applications, School of Computer Science, Beijing Institute of Technology, Beijing, China {wuhao123, hhy63, pjian, guoyuhang, suchao}@bit.edu.cn Abstract sentence pairs for training and evaluation, and attracted a large number of teams with a variety of systems to participate the competitions. Generally, STS systems could be divided into two categories: One kind is unsupervised systems (Li et al., 2006; Mihalcea et al., 2006; Islam and Inkpen, 2008; Han et al., 2013; Sultan et al., 2014b; Wu and Huang, 2016), some of which are appeared for a long time when there wasn’t enough training data; The other kind is ˇ c et al., supervised systems (B¨ar et al., 2012; Sari´ 2012; Sultan et al., 2015; Rychalska et al., 2016; Brychc´ın and Svoboda, 2016) applying machine learning algorithms, including deep learning, after adequate training data has been constructed. Each kind of methods has its advantages and application areas. In this paper, we present three systems, one unsupervised system and two supervised systems which simply make use of the unsupervised one. This paper presents th"
S17-2007,P14-5010,0,0.00596348,"ystem with lemmas and theorems has been established for supporting the correctness of Equation (8) and (9). For more details about this section, please see the paper (Wu and Huang, 2017) for reference. 2.3 Increasing Word Recall Rate for SIS We made three aspects improvements in our another previous work: First, we utilize WordNet to directly obtain the nominal forms of a content word which is not a noun mainly through derivational pointers in WordNet. The word formation helps enhance the recall rate of known content words in sentence-toSIS mappings. Second, name entity (NE) recognition tool (Manning et al., 2014) and the alignment ICG(Ci ) = IC(ci )−totalIC(Intersect(i|i−1)). (9) Algorithm 2 and 3 are according to Equation (8) and (9). Algorithm 3 is approximately equal to one time subsumer searching between concepts, thus 2 79 https://github.com/hao123wu/STS tool (Sultan et al., 2014a) are employed to obtain non-overlapping unknown NEs, which are used for simulating non-overlapping IC in SIS. The alignment tool is mainly used for finding actually same NEs with different string forms and inconsistent NE annotations by the NE recognition tool. Through the statistic values of known NEs of the same kinds"
S17-2007,S15-2027,0,0.14544,"ing, China {wuhao123, hhy63, pjian, guoyuhang, suchao}@bit.edu.cn Abstract sentence pairs for training and evaluation, and attracted a large number of teams with a variety of systems to participate the competitions. Generally, STS systems could be divided into two categories: One kind is unsupervised systems (Li et al., 2006; Mihalcea et al., 2006; Islam and Inkpen, 2008; Han et al., 2013; Sultan et al., 2014b; Wu and Huang, 2016), some of which are appeared for a long time when there wasn’t enough training data; The other kind is ˇ c et al., supervised systems (B¨ar et al., 2012; Sari´ 2012; Sultan et al., 2015; Rychalska et al., 2016; Brychc´ın and Svoboda, 2016) applying machine learning algorithms, including deep learning, after adequate training data has been constructed. Each kind of methods has its advantages and application areas. In this paper, we present three systems, one unsupervised system and two supervised systems which simply make use of the unsupervised one. This paper presents three systems for semantic textual similarity (STS) evaluation at SemEval-2017 STS task. One is an unsupervised system and the other two are supervised systems which simply employ the unsupervised one. All our"
S17-2007,S16-1091,0,0.0434571,"hhy63, pjian, guoyuhang, suchao}@bit.edu.cn Abstract sentence pairs for training and evaluation, and attracted a large number of teams with a variety of systems to participate the competitions. Generally, STS systems could be divided into two categories: One kind is unsupervised systems (Li et al., 2006; Mihalcea et al., 2006; Islam and Inkpen, 2008; Han et al., 2013; Sultan et al., 2014b; Wu and Huang, 2016), some of which are appeared for a long time when there wasn’t enough training data; The other kind is ˇ c et al., supervised systems (B¨ar et al., 2012; Sari´ 2012; Sultan et al., 2015; Rychalska et al., 2016; Brychc´ın and Svoboda, 2016) applying machine learning algorithms, including deep learning, after adequate training data has been constructed. Each kind of methods has its advantages and application areas. In this paper, we present three systems, one unsupervised system and two supervised systems which simply make use of the unsupervised one. This paper presents three systems for semantic textual similarity (STS) evaluation at SemEval-2017 STS task. One is an unsupervised system and the other two are supervised systems which simply employ the unsupervised one. All our systems mainly depend o"
S17-2007,S12-1060,0,0.148756,"Missing"
S17-2036,S17-2002,0,0.0494346,"Missing"
S18-1046,S17-2126,0,0.0335385,"Missing"
S18-1046,S17-2094,0,0.0281724,"Missing"
S18-1046,W17-5207,0,0.0367588,"Missing"
S18-1046,D15-1044,0,0.0223047,", we use a mix of deep learning methods to make our system obtain better predictive performance. Inspired by the boosting algorithms, we use a logistic regression to improve the accuracy of these four methods and the architecture is shown in Figure 5. In order to make the model simple, it only takes the output of the four methods as input rather than training data. A CNN-based Attention Model (CA) Since attention mechanism has achieved significant improvements in many NLP tasks, including machine translation (Bahdanau et al., 2014), caption generation (Xu et al., 2015) and text summarization (Rush et al., 2015), it becomes an integral part of compelling sequence modeling and transduction models in various tasks. Motivated by Du’s work on sentence classification (Du et al., 2017), the architecture of our CNNbased attention model resembles his model. We first use a CNN-based network to model the attention signal in sentences. The convolution operation here is same as that described in Section 2.1. The attention signal of original text is represented by the output of convolutional filter. In order to reduce the noise, multiple filters with same size of windows are applied. After that, we get the corres"
S18-1046,D14-1181,0,0.00830449,"Missing"
S18-1046,S18-1001,0,0.0670598,"Missing"
S18-1046,S16-1001,0,0.0276979,"e subtask EI-reg and an average Pearson correlation score of 0.784 in subtask V-reg, which ranks 19th among 48 systems in EI-reg and 17th among 38 systems in V-reg. 1 Before 2016, most systems use Support Vector Machine (SVM), Naive Bayes, maximum entropy and linear regression (Nakov et al., 2013; Rosenthal et al., 2014, 2015). In SemEval 2014, deep learning methods started to appear and a team using them won the second place. Since 2015, more and more teams who were rank at the top used deep learning methods and now deep learning methods including CNN and LSTM networks become really popular (Nakov et al., 2016; Rosenthal et al., 2017). The system described in this paper is an ensemble of four different DNN methods including CNN, Bidirectional LSTM (Bi-LSTM), LSTMCNN and a CNN-based Attention model (CA). In these methods, words in tweets are firstly mapped to word vectors. After intensity scores are calculated by these models, we use a logistic regression and finally give the scores. The rest of the paper is organized as follows. Section 2 describes the four various methods and the ensemble method used in our system. Section 3 and Section 4 give the implementation and training details of our system"
S18-1046,D14-1162,0,0.0819263,", hashtags and emoticons are converted into the special tokens like &lt;date&gt; , &lt;url&gt;, &lt;hashtag&gt; and &lt;joy&gt;, but these tokens are not in the dictionary of pre-trained word vectors, which means the information of these tokens is still wasted in the embedding process. There is much room for the improvement of our method: LSTM Nil 300 300 150 Table 2: Network hyper-parameters for the filters of CNN and hidden size of LSTM, and p is the dropout rate. For example, [2, 3, 4], 256 means the filter height is set to 2, 3 and 4, and the number of filters is set to 256 for different sizes of filters. GloVe (Pennington et al., 2014) trained by Common Crawl. Model Hyper-parameters: Table 1 and Table 2 show the hyper-parameters we use in our system. For fully connected layers, no more than two fully-connected layers are used in the four methods and all fully-connected layers are followed by ReLU. Before the outputs of pooling layers and LSTMs are fed to the fully connected layers, a dropout is applied and the details are described in Table 2. 4 Result and Discussion Training The dataset used in our system is provided by the AIT task and no external datasets are used in training period. For the subtask EI-reg and subtask Vr"
S18-1046,S17-2088,0,0.0165055,"an average Pearson correlation score of 0.784 in subtask V-reg, which ranks 19th among 48 systems in EI-reg and 17th among 38 systems in V-reg. 1 Before 2016, most systems use Support Vector Machine (SVM), Naive Bayes, maximum entropy and linear regression (Nakov et al., 2013; Rosenthal et al., 2014, 2015). In SemEval 2014, deep learning methods started to appear and a team using them won the second place. Since 2015, more and more teams who were rank at the top used deep learning methods and now deep learning methods including CNN and LSTM networks become really popular (Nakov et al., 2016; Rosenthal et al., 2017). The system described in this paper is an ensemble of four different DNN methods including CNN, Bidirectional LSTM (Bi-LSTM), LSTMCNN and a CNN-based Attention model (CA). In these methods, words in tweets are firstly mapped to word vectors. After intensity scores are calculated by these models, we use a logistic regression and finally give the scores. The rest of the paper is organized as follows. Section 2 describes the four various methods and the ensemble method used in our system. Section 3 and Section 4 give the implementation and training details of our system for subtask EI-reg and V-"
S18-1046,S15-2078,0,0.0743508,"Missing"
S18-1046,S14-2009,0,0.0233558,"emotions need to be quantified into a real valued score in [0, 1]. We propose an ensemble system including four different deep learning methods which are CNN, Bidirectional LSTM (BLSTM), LSTMCNN and a CNN-based Attention model (CA). Our system gets an average Pearson correlation score of 0.682 in the subtask EI-reg and an average Pearson correlation score of 0.784 in subtask V-reg, which ranks 19th among 48 systems in EI-reg and 17th among 38 systems in V-reg. 1 Before 2016, most systems use Support Vector Machine (SVM), Naive Bayes, maximum entropy and linear regression (Nakov et al., 2013; Rosenthal et al., 2014, 2015). In SemEval 2014, deep learning methods started to appear and a team using them won the second place. Since 2015, more and more teams who were rank at the top used deep learning methods and now deep learning methods including CNN and LSTM networks become really popular (Nakov et al., 2016; Rosenthal et al., 2017). The system described in this paper is an ensemble of four different DNN methods including CNN, Bidirectional LSTM (Bi-LSTM), LSTMCNN and a CNN-based Attention model (CA). In these methods, words in tweets are firstly mapped to word vectors. After intensity scores are calculat"
S18-1046,S17-2128,0,0.0676766,"Missing"
W10-4134,J04-1004,0,0.0701844,"Missing"
W10-4157,O03-5002,1,0.866134,"Missing"
W10-4157,E09-1013,0,\N,Missing
W10-4157,S07-1042,0,\N,Missing
W10-4157,S07-1030,0,\N,Missing
W10-4157,S07-1012,0,\N,Missing
W14-6828,C10-3004,0,0.0136064,"and abnorAfter all the non-words are substituted by in dicmal labeling comes up. Sometimes it happens on tionary words, several heuristic rules are utilized the wrong words themselves, such as “過失” to deal with some phenomena with strong regubeing labeled with an impossible class “VV” larity. These rules include: (verb); sometimes other words around are af Replace “門” by “們”: if there is any word in fected by the wrong word, such as “台灣” being a predefined set or its first-class similar tagged as a verb due to the wrongly used word words in HIT-CIR TongyiciCilin (Extend“再” before it. ed) 9 (Che et al., 2010) appearing before To locate and correct these wrong words, a “門”, it should be “們”. The set used in the dependency parsing is carried out following the task is: POS retagging and all the dependency pairs in{我, 你, 妳, 他, 她, 人, 同學, 兄弟, 親人, volving the abnormal word are extracted to be 客人, 對手, 成員, 公司, 工廠, 企業}. examined. The left side in Figure 2 shows the  Correction of interjections: if “阿”, “把”, dependency pairs related with “過失”. Distinct “巴”, “拉” and “麻” etc. locate before a dot with the first one, POS tagging at this stage is conducted on the sentence where the non-words mark (。？！ ，、 ； ：) an"
W14-6828,Y09-1025,1,0.871089,"Missing"
W14-6828,W13-4406,0,0.0171367,"e from Sinorama), bc (broadcast conversation from New Tang Dynasty TV etc.) and wb (weblogs) parts of CTB7.0, which form a dependency corpus including 30,861 sentences. The simplified characters in the corpus are also converted by OpenCC. We get about 42,000 items in the word-POS base and the format is as following: Figure 1: System architecture extended the principles of decomposing Chinese characters with the Cangjie codes to judge the visual similarity between Chinese characters. SIGHAN Bake-off 2013 for Chinese spelling check inspired a variety of spelling check and correction techniques (Wu et al., 2013). Typical statistical approaches such as maximum entropy model and machine translation model performed well assisted by rule based model and other language analysis techniques. Compared with the test data in SIGHAN Bake-off 2013, there are more wrong words and the text is more colloquial in the current Bake-off, which make the correction task more challenging. 2. 胛骨 jia gu 1 慚色 can se 1 慚愧 can kui System Architecture In terms of the error types of the task, our system is mainly composed by two stages: non-word correction and wrong word correction. In detail, stage one consists of several parts"
W14-6828,P00-1032,0,0.0583545,"For example, writers often confuse “在” and “再”, such as “高雄是 再台灣南部一個現代化城市”. Here, it is “在” but not “ 再 ” the right one. Different from non-words, we call these words “wrong words”. According to the statistics obtained from the training data of CLP 2014 Back-off, there are nearly 3,400 wrong words which are about twice more than non-words, 1,800 ones. Spelling check and correction is a traditional task in natural language processing. Pollock and Zamora (1984) built a misspelling dictionary for spelling check. Chang (1995) adopted a bi-gram language model to substitute the confusing character. Zhang et al. (2000) proposed an approximate word matching method to detect and correct spelling errors. Liu et al. (2011) Abstract This paper describes the Chinese spelling correction system submitted by BIT at CLP Bake-off 2014 task 2. The system mainly includes two parts: 1) N-gram model is adopted to retrieve the non-words which are wrongly separated by word segmentation. The non-words are then corrected in terms of word frequency, pronunciation similarity, shape similarity and POS (part of speech) tag. 2) For wrong words, abnormal POS tag is used to indicate their location and dependency relation matching is"
W15-3124,C00-1044,0,0.148287,"Missing"
W15-3124,P02-1053,0,0.0428322,"Missing"
W15-3124,W02-1011,0,0.0327679,"Missing"
W15-3124,P09-1079,0,0.0378563,"Missing"
W15-3124,D10-1101,0,0.0249861,"Missing"
W15-3124,C10-3004,0,0.0266188,"Missing"
W15-3124,W04-3252,0,\N,Missing
Y06-1071,P03-2025,0,0.0270803,"e traditional monolingual information retrieval technology with machine translation (MT) technology. It is becoming a worldwide subject of crucial importance in the Information Age, as proved in some way by the fact that CLIR is included from time to time as an important subtask in the annual event of Text Retrieval Conference (TREC). In CLIR systems, the query sentence is usually input as a combination of a series of keywords rather than a sentence in its exact sense. Due to the absence in the series of query keywords of necessary syntactic and semantic information, traditional MT technology [1, 2] can not be readily used for the precise translation of the query sentence. In this paper, a new translation and transform algorithm of query sentence is proposed. This approach is based on large-scale corpora, traditional monolingual IR technology and the theories of vector space model (VSM) and lexical mutual information [3, 4]. 2 Query and Query Transform In cross-language information retrieval, the input is often a combination of a series of keywords rather than a complete sentence. This sequence of query keywords lacks necessary contextual and syntacticosemantic information, so they can n"
Y06-1071,W03-1108,0,0.0288454,"e traditional monolingual information retrieval technology with machine translation (MT) technology. It is becoming a worldwide subject of crucial importance in the Information Age, as proved in some way by the fact that CLIR is included from time to time as an important subtask in the annual event of Text Retrieval Conference (TREC). In CLIR systems, the query sentence is usually input as a combination of a series of keywords rather than a sentence in its exact sense. Due to the absence in the series of query keywords of necessary syntactic and semantic information, traditional MT technology [1, 2] can not be readily used for the precise translation of the query sentence. In this paper, a new translation and transform algorithm of query sentence is proposed. This approach is based on large-scale corpora, traditional monolingual IR technology and the theories of vector space model (VSM) and lexical mutual information [3, 4]. 2 Query and Query Transform In cross-language information retrieval, the input is often a combination of a series of keywords rather than a complete sentence. This sequence of query keywords lacks necessary contextual and syntacticosemantic information, so they can n"
Y11-1035,C92-2070,0,0.241809,"imental results are presented in Section 4. Lastly we conclude this paper in Section 5. 2. Related Work Generally speaking, Word Sense Disambiguation methods are either knowledge-based or corpus-based. In addition, the latter can further be further divided into two kinds: unsupervised ones and supervised ones. In this paper we focus on unsupervised WSD method. Knowledge-based method disambiguates words by matching context with information from a prescribed knowledge source. These methods include Lesk’s algorithm (Lesk, 1986), Walker’s algorithm (Walker and Amsler, 1986), Yarowsky's algorithm (Yarowsky, 1992) and so on. Unsupervised methods cluster words into some sets which indicate the same meaning, but they cannot give an exact meaning of the target word, these methods can be categorized into methods based on word clustering (Lin, 1998) and co-occurrence graphs (Widdows and Dorow, 2002). 334 Supervised method learns form annotated sense examples, the learning algorithms including: SVM (Escudero et al, 2000a), naïve Bayesian learning (Escudero et al, 2000b) and maximum entropy (Tratz et al, 2007). Though corpus-based approach usually has better performance, the mount of words it can disambiguate"
Y11-1035,P98-2127,0,0.353798,"Missing"
Y11-1035,C02-1114,0,0.0779907,"Missing"
Y11-1035,S07-1057,0,0.0662956,"Missing"
Y11-1035,H05-1052,0,0.0379724,"Though corpus-based approach usually has better performance, the mount of words it can disambiguate essentially relies on the size of training corpus, while knowledge-based approach has the advantage of providing larger coverage. Knowledge based methods for word sense disambiguation are usually applicable to all words in the text, while corpus-based techniques usually target only few selected word for which large corpora are available. More recently, graph-based methods for WSD have gained much attention in the NLP community (Veronis, 2004, Sinha and Mihalcea, 2007, Navigli and Lapata, 2007, Mihalcea, 2005, Agirre E, 2006). The HyperLex (Veronis, 2004) algorithm is entirely corpus-based, which uses small-world properties of co-occurrence graphs. TexRank (Mihalcea, 2005) creates a complete weighted graph formed by the synsets of the words in the input context, and the weight of the edge linking two synsets is calculated by executing Lesk’s algorithm. These methods have been proposed to rank word sense based on the “vote” or “recommendations” between each other. When a word sense links to another one, it is basically casting a vote for that word sense. The higher the number of vote that is cast f"
Y11-1035,W06-1669,0,0.0562565,"Missing"
Y11-1035,S07-1035,0,0.0717057,"Missing"
Y11-1035,C98-2122,0,\N,Missing
Y11-1049,D09-1124,0,0.0333453,"improve the effectiveness of word semantic similarity measures. With the progress of the globalization, information emerged from a variety of languages to the Internet. Under this circumstance, crosslingual applications such as machine translation, cross-lingual information retrieval, cross-lingual text categorization and clustering etc., become more and more attractive. Consequently, crosslingual word semantic similarity measure becomes a meaningful research topic. Although there are some statistical and hybrid measures which have good overall performance (Li et al., 2003; Xia et al., 2011; Hassan and Mihalcea, 2009), the knowledge-based measures, to us, is still an important method. Even in a hybrid measure, the knowledge-based part is still a key part. Furthermore, if a large scale ontological knowledge base is available, the knowledge-based measures can be further improved if the knowledge base is comprehensively investigated and the similarity measures are elaborately designed. In this work, we “re-examine” merely knowledge based word similarity measure and a crosslingual semantic similarity measure is proposed. As we know, concepts are language-independent and it is natural that each concept can be d"
Y11-1049,N03-1032,0,0.0180793,"Missing"
Y12-1030,W11-0705,0,0.0130532,"negative seed words collection respectively. PMI(word1, word2) is described in formula (2), P(word1&word2), P(word1) and P(word2) are probabilities of word1 and word2 co-occurring, word1 appearing, and word2 appearing in a micro-blog post respectively. When SO(word) is greater than zero, sentiment orientation of word is positive. Otherwise it is negative. PMI ( word1 , word 2 ) = log( P( word1 & word 2 ) ) P( word1 ) P( word 2 ) (2) 3.1.2 The Dictionary of Internet Slang People usually use homophonic words, abbreviated words and network slang to express their sentiment in social network, and (Agarwal et al., 2011) has analysed the sentiment of twitter data. Sometimes new words, produced by important events or news reports, are used to express their opinions. So we construct the dictionary of internet slang to support emotional tendency identification algorithm on micro-blog topic, containing homophonic words, abbreviated words, network slang and many new words. National Language Resource Monitoring & Research Center (Network Media) 3 has some internet slang, two persons from our lab manually collect more network language through social network, and then integrate these resources together. Finally we ac"
Y12-1030,P11-1016,0,\N,Missing
Y12-1030,C10-2005,0,\N,Missing
Y12-1030,W02-1011,0,\N,Missing
Y12-1030,W11-0700,0,\N,Missing
Y17-1021,P01-1017,0,0.222346,"Missing"
Y17-1021,P98-1035,0,0.398654,"Missing"
Y17-1021,J92-4003,0,0.604595,"Missing"
Y17-1021,W06-1673,0,0.0412813,"and Ringger, 1999; Wang and Harper, 2002). Jelinek (1985) pointed out that we can replace the classes with POS tags in language model. Kneser and Ney (1993) incorporated POS tags into n-gram LM and got 37 percents improvement. But they got only 10 percents improvement with classes through clustering. A. Heeman (1998) redefined the objective of automatic speech recognition: to get both the word sequence and the POS sequence. His experiments showed 4.2 percent reduction on perplexity over classes. It is common to build probabilistic graphical models using many different linguistic annotations (Finkel et al., 2006). However, the problem to combine neural architectures with conventional linguistic annotations seems hard. This is because neural architectures lack flexibility to incorporate achievements from other NLP tasks (Ji et al., 2016). To address the problem, (Ji et al., 2016) used a latent variable recurrent neural network (LVRNN) to construct language models with discourse relations. LVRNN was proposed by Chung et al. (2015) to model variables observed in sequential data. 140 31st Pacific Asia Conference on Language, Information and Computation (PACLIC 31), pages 140–147 Cebu City, Philippines, No"
Y17-1021,N16-1037,0,0.309593,"they got only 10 percents improvement with classes through clustering. A. Heeman (1998) redefined the objective of automatic speech recognition: to get both the word sequence and the POS sequence. His experiments showed 4.2 percent reduction on perplexity over classes. It is common to build probabilistic graphical models using many different linguistic annotations (Finkel et al., 2006). However, the problem to combine neural architectures with conventional linguistic annotations seems hard. This is because neural architectures lack flexibility to incorporate achievements from other NLP tasks (Ji et al., 2016). To address the problem, (Ji et al., 2016) used a latent variable recurrent neural network (LVRNN) to construct language models with discourse relations. LVRNN was proposed by Chung et al. (2015) to model variables observed in sequential data. 140 31st Pacific Asia Conference on Language, Information and Computation (PACLIC 31), pages 140–147 Cebu City, Philippines, November 16-18, 2017 c Copyright 2017 Chao Su, Heyan Huang, Shumin Shi, Yuhang Guo and Hao Wu Inspired by the POS language models and the LVRNN models above, we use POS features to improve the performance of RNNLM. We assume that"
Y17-1021,P03-1021,0,0.0951754,"Missing"
Y17-1021,P07-2045,0,0.0139665,"Missing"
Y17-1021,J10-4005,0,0.0234435,"odels is evaluated both intrinsically by perplexity and extrinsically by quality of reranking machine translation outputs. The perplexity (PPL) of a word sequence w is defined as v uK uY 1 K PPL = t P (wi |w1...i−1 ) (5) i=1 1 = 2− K PK i=1 log2 P (wi |w1...i−1 ) Perplexity can be easily evaluated and the model which yields the lowest perplexity is in some sense the closest to the true model which generated the data. Language model is an essential part of statistical machine translation systems, for measuring how likely it is that a translation hypothesis would be uttered by a native speaker (Koehn, 2010). Under the same conditions, a better language model brings a better translation system. Thus, we also evaluate our language model by evaluating the translation system who uses it. We use the most popular automatic evaluation metric for translation system, BLEU (Bilingual Evaluation Understudy) (Papineni et al., 2002); higher is better. 3 Parallel RNN LM with POS Feature The traditional RNNLM models word sequences but ignores other linguistic knowledge. POS is such a kind of linguistic knowledge. It is easy to acquire with high annotation accuracy. We now present a parallel RNN structure over"
Y17-1021,P02-1040,0,0.0977472,"del which yields the lowest perplexity is in some sense the closest to the true model which generated the data. Language model is an essential part of statistical machine translation systems, for measuring how likely it is that a translation hypothesis would be uttered by a native speaker (Koehn, 2010). Under the same conditions, a better language model brings a better translation system. Thus, we also evaluate our language model by evaluating the translation system who uses it. We use the most popular automatic evaluation metric for translation system, BLEU (Bilingual Evaluation Understudy) (Papineni et al., 2002); higher is better. 3 Parallel RNN LM with POS Feature The traditional RNNLM models word sequences but ignores other linguistic knowledge. POS is such a kind of linguistic knowledge. It is easy to acquire with high annotation accuracy. We now present a parallel RNN structure over sequences of words and POS tag information. In this structure, we train two RNNs simultaneously, one for word sequence and another for POS sequence. We integrate the state of POS RNN with the word RNN. 3.1 Parallel RNN The structure of the parallel RNN is shown in Fig. 2. The parallel RNN consists of two RNNs, word 14"
Y17-1021,P16-1028,0,0.0383656,"Missing"
Y17-1021,P16-1125,0,0.0156713,"or vector to the former. This is why we tend to treat the former also as a latent variable affecting the word sequence. We train the POS RNN to maximize the loglikelihood function of the training data: T X The update of the matrices X and Z is similar to equation (11). The error vector propagated from the hidden layer to its previous is similar to equation (12). 143 Perplexity Setup We evaluated our model on three corpora, including Switchboard-1 Telephone Speech Corpus (SWB), Penn TreeBank (PTB)2 , and BBC3 . The former two corpora was used by Ji et al. (2016), while the last one was used by Wang and Cho (2016). We took all their work as comparisons. We splitted all the corpora into train, valid, and test sets, just like Ji et al. (2016) and Wang and Cho (2016) did. Statistics of the corpora are listed in Table 1. We tokenized all the corpora with tokenizer written by Pidong Wang, Josh Schroeder, and Philipp Koehn 4 , and POS tagged with the Stanford POS Tagger 5 . We implemented our model based on Mikolov’s RNNLM Tookit6 . We considered the value 100 for the hidden dimension, and 10K for the vocabulary size. The POS tagger’s tagset consists of 48 tags. We counted the times of each tag appeared in t"
Y17-1021,W02-1031,0,0.0665861,"ural language generation. Language modeling aims to predict the next word given context or to give the probability of a word sequence in textual data. In the past decades, n-gram based modeling techniques were most commonly used in such NLP applications. However, the recurrent neural network based language model (RNNLM) and ∗ Corresponding author Our code is available at https://github.com/ chao-su/prnnlm 1 Part-of-speech (POS) tags capture the syntactic role of each word, and has been proved to be useful for language modeling (Kneser and Ney, 1993; A. Heeman, 1998; Galescu and Ringger, 1999; Wang and Harper, 2002). Jelinek (1985) pointed out that we can replace the classes with POS tags in language model. Kneser and Ney (1993) incorporated POS tags into n-gram LM and got 37 percents improvement. But they got only 10 percents improvement with classes through clustering. A. Heeman (1998) redefined the objective of automatic speech recognition: to get both the word sequence and the POS sequence. His experiments showed 4.2 percent reduction on perplexity over classes. It is common to build probabilistic graphical models using many different linguistic annotations (Finkel et al., 2006). However, the problem"
Y17-1021,C98-1035,0,\N,Missing
