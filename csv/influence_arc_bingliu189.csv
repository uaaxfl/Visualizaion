2020.acl-main.512,D15-1075,0,0.0146944,"G) (Fey and Lenssen, 2019) and NVIDIA GPU GTX 1080 ti. 1 http://nlp.stanford.edu/data/glove.840B.300d.zip For all our BERT related experiments, we use the pretrained BERT model: https://storage.googleapis. com/bert_models/2018_10_18/uncased_L-12_ H-768_A-12.zip 2 Compared Models We compare models from the previous literature with several variations of our proposed model. Majority-Class assigns the majority label in the training set to each instance in the test set. SentEmbed given in (Panchenko et al., 2019) obtains sentence embeddings from a pretrained Sentence Encoder (Conneau et al., 2017; Bowman et al., 2015). The sentence embedding3 is then fed to XGBoost (Chen and Guestrin, 2016) for classification. For a fair comparison, we also feed the sentence embedding into a linear layer. They are represented as SentEmbedXGBoost and SentEmbedLinear . SVM-Tree4 given in (Tkachenko and Lauw, 2015) uses convolution kernel methods and dependency tree features to approach the CSI task. We use the one-vs-rest technique to adapt this model to our three-class CPC task. WordEmbed-Avg first constructs a sentence embedding by averaging the word embeddings of all words in a sentence, and then feeds it to a linear clas"
2020.acl-main.512,D14-1082,0,0.0601319,"head 0 attention outputs, hout ∈ RF is the output of node i k is the k-th attention bei at the current layer, αij F0 tween nodes i and j, W k ∈ R K ×F is linear trans2F 0 formation, ak ∈ R K is the weight vector, and f (·) is LeakyReLU non-linearity function. Overall, the input-output for a single GAT layer is summarized as H out = GAT (X, A; Θl ). The input is X ∈ Rn×F and the output is H out ∈ 0 Rn×F , where n is the number of nodes, F is the node feature size, F 0 is GAT hidden size, and A ∈ Rn×n is the adjacency matrix of the graph. 2.2 ED-GAT for CPC task We use the dependency parser in (Chen and Manning, 2014) to convert a sentence into a dependency parse graph. Each word corresponds to a node in the graph. The node features are the word embedding vectors, denoted as xi ∈ RF corresponding to node i. The input node feature matrix is X ∈ Rn×F . Note that an entity is either a single word or a multi-word phrase. To treat each entity as 5783 Output Next, we concatenate these two vectors as: v = [he1 k he2 ] and use a feed-forward layer with softmax function to project v into classes for prediction. Here using he1 and he2 makes the ED-GAT model entity-aware as they are the output of the nodes correspond"
2020.acl-main.512,D17-1070,0,0.0316484,"PyTorch Geometric (PyG) (Fey and Lenssen, 2019) and NVIDIA GPU GTX 1080 ti. 1 http://nlp.stanford.edu/data/glove.840B.300d.zip For all our BERT related experiments, we use the pretrained BERT model: https://storage.googleapis. com/bert_models/2018_10_18/uncased_L-12_ H-768_A-12.zip 2 Compared Models We compare models from the previous literature with several variations of our proposed model. Majority-Class assigns the majority label in the training set to each instance in the test set. SentEmbed given in (Panchenko et al., 2019) obtains sentence embeddings from a pretrained Sentence Encoder (Conneau et al., 2017; Bowman et al., 2015). The sentence embedding3 is then fed to XGBoost (Chen and Guestrin, 2016) for classification. For a fair comparison, we also feed the sentence embedding into a linear layer. They are represented as SentEmbedXGBoost and SentEmbedLinear . SVM-Tree4 given in (Tkachenko and Lauw, 2015) uses convolution kernel methods and dependency tree features to approach the CSI task. We use the one-vs-rest technique to adapt this model to our three-class CPC task. WordEmbed-Avg first constructs a sentence embedding by averaging the word embeddings of all words in a sentence, and then fee"
2020.acl-main.512,N19-1423,0,0.0843905,"set statistics are given in Table 2. The model is trained only on the newly split training set. We use the class-based F1 score as the evaluation measure. F1(B), F1(W) and F1(N) represent F1 score for classes BETTER, WORSE and NONE respectively. F1-Micro is the average F1 score as in (Panchenko et al., 2019). 4.2 Model Implementation Details The Stanford Neural Network Dependency Parser (Chen and Manning, 2014) is used to build the dependency parse graph for each sentence. In our experiment, we use two pretrained word embeddings: GloVe embeddings (Pennington et al., 2014)1 and BERT embedding (Devlin et al., 2019)2 . The input of BERT is formatted as the standard BERT input format, with “[CLS]” before and “[SEP]” after the sentence tokens. For this, we employ the BERT tokenizer to tokenize each word into word pieces (tokens). The output of the pretrainedBERT model is a sequence of embeddings, each of size 768, and corresponds to a word piece. We average the word piece embeddings of the original word to get the embedding for each word (node in the dependency graph). Note that, word embeddings are kept frozen and not fine-tuned by the subsequent model structure. For the ED-GAT model, we set the hidden si"
2020.acl-main.512,C08-1031,1,0.789102,"ormance in comparative preference classification. 1 s3 Sentences Golf is easier to pick up than baseball. I’m considering learning Python and more PHP if any of those would be better. The tools based on Perl and Python is much slower under Windows than K9. Table 1: Comparative sentence examples. Entities of interest are underlined in each sentence. Introduction Given a sentence that contains two entities of interest, the task of Comparative Preference Classification is to decide whether there is a comparison between the two entities and if so, which entity is preferred (Jindal and Liu, 2006a; Ganapathibhotla and Liu, 2008; Liu, 2012; Panchenko et al., 2019). For example, considering sentence s1 (shown in Table 1), there is a comparison between the two underlined entities, and “golf” is preferred over “baseball”. This sentence contains explicit comparative predicate “easier”. The task seems straightforward but is quite challenging due to many counterexamples. For example, s2 shows that “better” may not indicate a comparison. s3 , another counterexample, shows that “slower” indeed indicates a comparison, but not between “Perl” and “Python”, but between “tools” and “K9”. Problem statement. Given a sentence s = hw"
2020.acl-main.512,P19-1024,0,0.0560756,"Missing"
2020.acl-main.512,D13-1194,0,0.452539,"to exploring comparisons in text. For the CSI task, early works include those in (Jindal and Liu, 2006a; Ganapathibhotla and Liu, 2008). More recently, Park and Blake (2012) employed handcrafted syntactic rules to identify comparative sentences in scientific articles. For other languages such as Korean and Chinese, related works include (Huang et al., 2008), (Yang and Ko, 2009) and (Zhang and Jin, 2012). Other works are interested in identifying entities, aspects and comparative predicates in comparative sentences, e.g., (Jindal and Liu, 2006b), (Hou and Li, 2008), (Kessler and Kuhn, 2014), (Kessler and Kuhn, 2013), and (Feldman et al., 2007). Ganapathibhotla and Liu (2008) used lexicon properties to determine the preferred entities given the output of (Jindal and Liu, 2006b), which is quite different from our task. There are also works related to product ranking using comparisons, such as those in (Kurashima et al., 2008), (Zhang et al., 2013), (Tkachenko and Lauw, 2014) and (Li et al., 2011). All these related works solve very different problems in comparison analysis than our CPC task. Works in NLP that use Graph Neural Networks and dependency graph structures include (Huang and Carley, 2019), (Guo e"
2020.acl-main.512,W19-4516,0,0.446871,"cation. 1 s3 Sentences Golf is easier to pick up than baseball. I’m considering learning Python and more PHP if any of those would be better. The tools based on Perl and Python is much slower under Windows than K9. Table 1: Comparative sentence examples. Entities of interest are underlined in each sentence. Introduction Given a sentence that contains two entities of interest, the task of Comparative Preference Classification is to decide whether there is a comparison between the two entities and if so, which entity is preferred (Jindal and Liu, 2006a; Ganapathibhotla and Liu, 2008; Liu, 2012; Panchenko et al., 2019). For example, considering sentence s1 (shown in Table 1), there is a comparison between the two underlined entities, and “golf” is preferred over “baseball”. This sentence contains explicit comparative predicate “easier”. The task seems straightforward but is quite challenging due to many counterexamples. For example, s2 shows that “better” may not indicate a comparison. s3 , another counterexample, shows that “slower” indeed indicates a comparison, but not between “Perl” and “Python”, but between “tools” and “K9”. Problem statement. Given a sentence s = hw1 , w2 , ..., e1 , ..., e2 , ...wn i"
2020.acl-main.512,W12-4301,0,0.413611,"er l, Hout = GAT (H l , A; Θl ), is the input for layer (l + 1), denoted by H l+1 . H 0 is the initial 0 input. W0 ∈ RF ×F and b0 are the projection matrix and bias vector. For a L layer ED-GAT model, L ∈ Rn×F 0 . the output of the final layer is Hout We use a mask layer to fetch the two hidden vecL , which corresponds to the two entors from Hout L ). tities of interest: (he1 , he2 ) = Masklayer(Hout Related Works Many papers have been devoted to exploring comparisons in text. For the CSI task, early works include those in (Jindal and Liu, 2006a; Ganapathibhotla and Liu, 2008). More recently, Park and Blake (2012) employed handcrafted syntactic rules to identify comparative sentences in scientific articles. For other languages such as Korean and Chinese, related works include (Huang et al., 2008), (Yang and Ko, 2009) and (Zhang and Jin, 2012). Other works are interested in identifying entities, aspects and comparative predicates in comparative sentences, e.g., (Jindal and Liu, 2006b), (Hou and Li, 2008), (Kessler and Kuhn, 2014), (Kessler and Kuhn, 2013), and (Feldman et al., 2007). Ganapathibhotla and Liu (2008) used lexicon properties to determine the preferred entities given the output of (Jindal an"
2020.acl-main.512,D14-1162,0,0.0853131,"tention outputs, hout ∈ RF is the output of node i k is the k-th attention bei at the current layer, αij F0 tween nodes i and j, W k ∈ R K ×F is linear trans2F 0 formation, ak ∈ R K is the weight vector, and f (·) is LeakyReLU non-linearity function. Overall, the input-output for a single GAT layer is summarized as H out = GAT (X, A; Θl ). The input is X ∈ Rn×F and the output is H out ∈ 0 Rn×F , where n is the number of nodes, F is the node feature size, F 0 is GAT hidden size, and A ∈ Rn×n is the adjacency matrix of the graph. 2.2 ED-GAT for CPC task We use the dependency parser in (Chen and Manning, 2014) to convert a sentence into a dependency parse graph. Each word corresponds to a node in the graph. The node features are the word embedding vectors, denoted as xi ∈ RF corresponding to node i. The input node feature matrix is X ∈ Rn×F . Note that an entity is either a single word or a multi-word phrase. To treat each entity as 5783 Output Next, we concatenate these two vectors as: v = [he1 k he2 ] and use a feed-forward layer with softmax function to project v into classes for prediction. Here using he1 and he2 makes the ED-GAT model entity-aware as they are the output of the nodes correspond"
2020.acl-main.512,D19-1549,0,0.0386757,"Missing"
2020.acl-main.512,P15-1037,0,0.767849,"preference direction between these two entities into one of the three classes: {BETTER, WORSE, NONE}. BETTER (WORSE) means e1 is preferred (not preferred) over e2 . NONE means that there is no comparative relation between e1 and e2 . Although closely related, Comparative Preference Classification (CPC) is different from Comparative Sentence Identification (CSI), which is a 2-class classification problem that classifies a sentence as a comparative or a non-comparative sentence. In previous work, Jindal and Liu (2006a) did CSI without considering which two entities are involved in a comparison. Tkachenko and Lauw (2015) employed some dependency graph features to approach the CSI task given two entities of interest. In this entity-aware case, syntactic features are crucial. However, not using word embeddings in the model makes the model harder to generalize with a good performance given various ways of expressing comparisons. Panchenko et al. (2019) gave the state-of-the-art result on the CPC task by using a pretrained sentence encoder to produce sentence embeddings as a feature for classification. However, this model is not entity-aware and does not use the dependency graph information. 5782 Proceedings of t"
2020.acl-main.512,P09-2039,0,0.0296186,"0 . the output of the final layer is Hout We use a mask layer to fetch the two hidden vecL , which corresponds to the two entors from Hout L ). tities of interest: (he1 , he2 ) = Masklayer(Hout Related Works Many papers have been devoted to exploring comparisons in text. For the CSI task, early works include those in (Jindal and Liu, 2006a; Ganapathibhotla and Liu, 2008). More recently, Park and Blake (2012) employed handcrafted syntactic rules to identify comparative sentences in scientific articles. For other languages such as Korean and Chinese, related works include (Huang et al., 2008), (Yang and Ko, 2009) and (Zhang and Jin, 2012). Other works are interested in identifying entities, aspects and comparative predicates in comparative sentences, e.g., (Jindal and Liu, 2006b), (Hou and Li, 2008), (Kessler and Kuhn, 2014), (Kessler and Kuhn, 2013), and (Feldman et al., 2007). Ganapathibhotla and Liu (2008) used lexicon properties to determine the preferred entities given the output of (Jindal and Liu, 2006b), which is quite different from our task. There are also works related to product ranking using comparisons, such as those in (Kurashima et al., 2008), (Zhang et al., 2013), (Tkachenko and Lauw,"
2020.acl-main.726,W18-3012,0,0.0204583,"rks and others have not used feature projection to improve (or purify) representations for supervised learning, which we believe is a promising direction to explore. Unsupervised methods: These methods utilize a large unlabeled text corpus to learn word representations which are then composed into sentence and document representations. For example, Kiros et al. (2015) constructed sentence representations by trying to reconstruct neighbouring sentences. Hill et al. (2016) proposed a log-linear bag-of-words models for sentence representation. The unsupervised smooth inverse frequency method in (Ethayarajh, 2018) built on this but used a weighted average of word embeddings and principal component removal for sentence representations. Our work is again clearly different from these unsupervised methods as the proposed method works under supervised learning. Existing unsupervised methods also do not use feature projection. Some other works have also been done for semisupervised representation learning (Kevin Clark, 2018) and transfer learning (Tamaazousti et al., 2018). Jason Phang (2019) also proposed to use some data-rich intermediate supervised tasks for pre-training to help produce better representat"
2020.acl-main.726,P12-2034,0,0.0426884,"ethod projects existing features into the orthogonal space of the common features. The resulting projection is thus perpendicular to the common features and more discriminative for classification. We apply this new method to improve CNN, RNN, Transformer, and Bert based text classification and obtain markedly better results. 1 Introduction Text classification is an important task in natural language processing and text mining. It has a very wide range of applications, such as sentiment classification (Liu, 2012), question classification (Li and Roth, 2002), and deception detection (Liu, 2012; Feng et al., 2012). In recent years, deep learning models have been shown to outperform traditional classification methods (Kim, 2014; Iyyer et al., 2015; Tang et al., 2015; Dai and Le, 2015; Jin et al., 2016; Joulin et al., 2017; Shen et al., 2018). Given the input document, the system applies a mapping function (e.g., averaging or summation, a ∗ † Equal Contribution. Corresponding Author. convolution neural network (CNN), recurrent neural network (RNN), and so on) to learn a dense representation of the document and then uses this representation to perform the final classification. Representation learning is o"
2020.acl-main.726,S19-1028,0,0.0351878,"Missing"
2020.acl-main.726,N16-1162,0,0.0347552,"Missing"
2020.acl-main.726,D15-1075,0,0.0351363,"nchmark datasets: MR: This is a movie review dataset for sentiment classification. It has two classes: positive and negative (Pang and Lee, 2005).4 SST2: This is the Stanford Sentiment Treebank dataset.5 Each sample is marked as negative or positive. TREC: This is a question classification dataset, which is to classify a question into one of the six question types (Li and Roth, 2002).6 SNLI: This is a popular text entailment dataset. It contains 570k human annotated sentence pairs, in which the premises are drawn from the captions of the Flickr 30 corpus and hypotheses are manually annotated (Bowman et al., 2015). For this SNLI dataset, we created the following settings to suit our needs: (1) we concatenated the two sentences (in a pair) as a single sample; (2) when using 4 http://www.cs.cornell.edu/people/ pabo/movie-review-data/ 5 http://nlp.stanford.edu/sentiment/ 6 http://cogcomp.cs.illinois.edu/Data/ QA/QC/ Data MR SNLI SST2 TREC c 2 3 2 6 l 45 40 35 15 T rain 8,529 54,936 6,920 5,000 T est 1,066 9,824 1,821 952 |V | 17,884 33,944 16,789 8,834 Table 1: Dataset statistics. c: number of classes. l: average length of sentences, after padding and cutting. T rain, T est: number of training and testing"
2020.acl-main.726,P15-1162,0,0.0695258,"Missing"
2020.acl-main.726,E17-2068,0,0.0626786,"new method to improve CNN, RNN, Transformer, and Bert based text classification and obtain markedly better results. 1 Introduction Text classification is an important task in natural language processing and text mining. It has a very wide range of applications, such as sentiment classification (Liu, 2012), question classification (Li and Roth, 2002), and deception detection (Liu, 2012; Feng et al., 2012). In recent years, deep learning models have been shown to outperform traditional classification methods (Kim, 2014; Iyyer et al., 2015; Tang et al., 2015; Dai and Le, 2015; Jin et al., 2016; Joulin et al., 2017; Shen et al., 2018). Given the input document, the system applies a mapping function (e.g., averaging or summation, a ∗ † Equal Contribution. Corresponding Author. convolution neural network (CNN), recurrent neural network (RNN), and so on) to learn a dense representation of the document and then uses this representation to perform the final classification. Representation learning is one of the key strengthes of deep learning. In this paper, we propose to further improve the representation learning, i.e., to make the representation more discriminative for classification. Note that throughout"
2020.acl-main.726,D18-1217,0,0.0608243,"Missing"
2020.acl-main.726,D14-1181,0,0.0195608,"Missing"
2020.acl-main.726,C02-1150,0,0.29122,"epresentation learning, i.e., feature projection. This method projects existing features into the orthogonal space of the common features. The resulting projection is thus perpendicular to the common features and more discriminative for classification. We apply this new method to improve CNN, RNN, Transformer, and Bert based text classification and obtain markedly better results. 1 Introduction Text classification is an important task in natural language processing and text mining. It has a very wide range of applications, such as sentiment classification (Liu, 2012), question classification (Li and Roth, 2002), and deception detection (Liu, 2012; Feng et al., 2012). In recent years, deep learning models have been shown to outperform traditional classification methods (Kim, 2014; Iyyer et al., 2015; Tang et al., 2015; Dai and Le, 2015; Jin et al., 2016; Joulin et al., 2017; Shen et al., 2018). Given the input document, the system applies a mapping function (e.g., averaging or summation, a ∗ † Equal Contribution. Corresponding Author. convolution neural network (CNN), recurrent neural network (RNN), and so on) to learn a dense representation of the document and then uses this representation to perfor"
2020.acl-main.726,P05-1015,0,0.473305,"Eq. 4) 9: 10: TFs projection (Eq. 9) 11: Get the purified features (Eq. 11) 12: Perform classification (Eq. 12) 13: Update parameters: 14: C-net, P-net’s parameters are updated together (Eq. 8 & Eq. 13) 15: end for to verify whether the proposed feature purification is general and effective for different deep learning classification models (or more precisely, feature extractors) on diverse datasets. 4.1 Experimental Datasets We carried out experiments on four diverse benchmark datasets: MR: This is a movie review dataset for sentiment classification. It has two classes: positive and negative (Pang and Lee, 2005).4 SST2: This is the Stanford Sentiment Treebank dataset.5 Each sample is marked as negative or positive. TREC: This is a question classification dataset, which is to classify a question into one of the six question types (Li and Roth, 2002).6 SNLI: This is a popular text entailment dataset. It contains 570k human annotated sentence pairs, in which the premises are drawn from the captions of the Flickr 30 corpus and hypotheses are manually annotated (Bowman et al., 2015). For this SNLI dataset, we created the following settings to suit our needs: (1) we concatenated the two sentences (in a pai"
2020.acl-main.726,P18-1041,0,0.13676,"e CNN, RNN, Transformer, and Bert based text classification and obtain markedly better results. 1 Introduction Text classification is an important task in natural language processing and text mining. It has a very wide range of applications, such as sentiment classification (Liu, 2012), question classification (Li and Roth, 2002), and deception detection (Liu, 2012; Feng et al., 2012). In recent years, deep learning models have been shown to outperform traditional classification methods (Kim, 2014; Iyyer et al., 2015; Tang et al., 2015; Dai and Le, 2015; Jin et al., 2016; Joulin et al., 2017; Shen et al., 2018). Given the input document, the system applies a mapping function (e.g., averaging or summation, a ∗ † Equal Contribution. Corresponding Author. convolution neural network (CNN), recurrent neural network (RNN), and so on) to learn a dense representation of the document and then uses this representation to perform the final classification. Representation learning is one of the key strengthes of deep learning. In this paper, we propose to further improve the representation learning, i.e., to make the representation more discriminative for classification. Note that throughout the paper we will us"
2020.acl-main.726,P15-1150,0,0.0390774,"extracting interpretable sentence embeddings using self-attention. Ma et al. (2018) showed that attention mechanism is also effective for sentiment classification. Vaswani et al. (2017) further illustrated that they can get a stronger sentence-level representation by stacking multiple blocks of selfattention. Bert (Devlin et al., 2018) combines Transformer and a large corpus to produce an even more complete and better sentence-level representation. Some other studies improved the representation of sentences from the perspective of language structures (e.g., parse trees and dependency trees) (Tai et al., 2015; Mou et al., 2015). Subramanian et al. (2018) utilized a single multi-task framework to combine the benefits of diverse sentence representation learning objectives. However, to the best of our knowledge, these existing works and others have not used feature projection to improve (or purify) representations for supervised learning, which we believe is a promising direction to explore. Unsupervised methods: These methods utilize a large unlabeled text corpus to learn word representations which are then composed into sentence and document representations. For example, Kiros et al. (2015) constru"
2020.acl-main.726,D15-1167,0,0.187335,"d more discriminative for classification. We apply this new method to improve CNN, RNN, Transformer, and Bert based text classification and obtain markedly better results. 1 Introduction Text classification is an important task in natural language processing and text mining. It has a very wide range of applications, such as sentiment classification (Liu, 2012), question classification (Li and Roth, 2002), and deception detection (Liu, 2012; Feng et al., 2012). In recent years, deep learning models have been shown to outperform traditional classification methods (Kim, 2014; Iyyer et al., 2015; Tang et al., 2015; Dai and Le, 2015; Jin et al., 2016; Joulin et al., 2017; Shen et al., 2018). Given the input document, the system applies a mapping function (e.g., averaging or summation, a ∗ † Equal Contribution. Corresponding Author. convolution neural network (CNN), recurrent neural network (RNN), and so on) to learn a dense representation of the document and then uses this representation to perform the final classification. Representation learning is one of the key strengthes of deep learning. In this paper, we propose to further improve the representation learning, i.e., to make the representation more"
2020.acl-main.726,P18-1215,0,0.0114729,"and Schmidhuber, 1997) and gated recurrent unit (GRU) (Chung et al., 2014) networks are suitable for handling text because a sentence or document can be regarded as a sequence. Therefore, a large amount of work based on RNN and its variants for feature extraction and downstream tasks has been done (Tang et al., 2015; Wang and Tian, 2016; He et al., 2016). Unlike RNN’s sequence modeling approach, CNN (Convolutional Neural Network) uses different sized windows to capture local correlations and position-invariant information (Kim, 2014; Conneau et al., 2016; Lai et al., 2015; Xiao and Cho, 2016; Wang, 2018). A common approach In summary, the key contribution of this paper is the improvement to representation learning through feature vector projection. To the best of our knowledge, this is the first such technique. Specifically, 2 8162 Related Work of these methods is to create an instance-level representation by using the final hidden state of the RNN, the maximum (or average) pooling of the RNN hidden states, or convolutional n-grams. However, they may ignore the importance of special words that are highly discriminative for classification. After Bahdanau et al. (2014) introduced the attention"
2020.acl-main.726,D16-1093,0,0.0288177,"tion. Supervised methods: These methods improve data utilization efficiency and discriminative feature distillation as they can obtain better training signals from the labeled data. Sequence models such as recurrent neural networks (RNN), Long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) and gated recurrent unit (GRU) (Chung et al., 2014) networks are suitable for handling text because a sentence or document can be regarded as a sequence. Therefore, a large amount of work based on RNN and its variants for feature extraction and downstream tasks has been done (Tang et al., 2015; Wang and Tian, 2016; He et al., 2016). Unlike RNN’s sequence modeling approach, CNN (Convolutional Neural Network) uses different sized windows to capture local correlations and position-invariant information (Kim, 2014; Conneau et al., 2016; Lai et al., 2015; Xiao and Cho, 2016; Wang, 2018). A common approach In summary, the key contribution of this paper is the improvement to representation learning through feature vector projection. To the best of our knowledge, this is the first such technique. Specifically, 2 8162 Related Work of these methods is to create an instance-level representation by using the final"
2020.acl-main.726,N16-1174,0,0.047932,"the first such technique. Specifically, 2 8162 Related Work of these methods is to create an instance-level representation by using the final hidden state of the RNN, the maximum (or average) pooling of the RNN hidden states, or convolutional n-grams. However, they may ignore the importance of special words that are highly discriminative for classification. After Bahdanau et al. (2014) introduced the attention mechanism in machine translation, attention mechanism has been exploited in many natural language processing tasks including text classification to solve the above problem. For example, Yang et al. (2016) introduced attention as an integral part of the model for text classification. Lin et al. (2017) proposed a new model for extracting interpretable sentence embeddings using self-attention. Ma et al. (2018) showed that attention mechanism is also effective for sentiment classification. Vaswani et al. (2017) further illustrated that they can get a stronger sentence-level representation by stacking multiple blocks of selfattention. Bert (Devlin et al., 2018) combines Transformer and a large corpus to produce an even more complete and better sentence-level representation. Some other studies impro"
2020.acl-main.726,N16-1000,0,0.430218,"ch technique. Specifically, 2 8162 Related Work of these methods is to create an instance-level representation by using the final hidden state of the RNN, the maximum (or average) pooling of the RNN hidden states, or convolutional n-grams. However, they may ignore the importance of special words that are highly discriminative for classification. After Bahdanau et al. (2014) introduced the attention mechanism in machine translation, attention mechanism has been exploited in many natural language processing tasks including text classification to solve the above problem. For example, Yang et al. (2016) introduced attention as an integral part of the model for text classification. Lin et al. (2017) proposed a new model for extracting interpretable sentence embeddings using self-attention. Ma et al. (2018) showed that attention mechanism is also effective for sentiment classification. Vaswani et al. (2017) further illustrated that they can get a stronger sentence-level representation by stacking multiple blocks of selfattention. Bert (Devlin et al., 2018) combines Transformer and a large corpus to produce an even more complete and better sentence-level representation. Some other studies impro"
2020.coling-main.21,P17-1080,0,0.0299088,"LM in BERT (Devlin et al., 2019; Liu et al., 2019; Lan et al., 2019) yield significant performance gains when later fine-tuned on downstream NLP tasks. Recent studies also showed impressive results on tasks in aspect-based sentiment analysis (ABSA) (Xu et al., 2019; Sun et al., 2019a; Li et al., 2019b; Tian et al., 2020; Karimi et al., 2020), which aims to discover aspects and their associated opinions (Hu and Liu, 2004; Liu, 2012). Although there are existing studies of the hidden representations and attentions of LMs about tasks such as parsing and co-reference resolution (Adi et al., 2016; Belinkov et al., 2017; Clark et al., 2019), it is unclear how LMs capture aspects and sentiment/opinion from large-scale unlabeled texts. This paper attempts to investigate and understand the inner workings of the pretext task of the masked language model (MLM) in transformer and their connections with tasks in ABSA. This may benefit the following problems: (1) improving fine-tuning of ABSA if we have a better understanding about the gap between pretext tasks and fine-tuning tasks; (2) more importantly, self-supervised (or unsupervised) ABSA without fine-tuning to save the expensive efforts on annotating ABSA data"
2020.coling-main.21,D17-1047,0,0.0204519,"ks and fine-tuning tasks; (2) more importantly, self-supervised (or unsupervised) ABSA without fine-tuning to save the expensive efforts on annotating ABSA datasets (for a new domain). We are particularly interested in fine-grained token-level features that are typically required by ABSA and how MLM as a general task can cover them during pre-training. Typical tasks of ABSA are: aspect extraction (AE), aspect sentiment classification (ASC) (Hu and Liu, 2004; Dong et al., 2014; Nguyen and Shirai, 2015; Li et al., 2018; Tang et al., 2016; Wang et al., 2016a; Wang et al., 2016b; Ma et al., 2017; Chen et al., 2017; Ma et al., 2017; Tay et al., 2018; He et al., 2018; Liu et al., 2018) and end-to-end ABSA (E2E-ABSA) (Li et al., 2019a; Li et al., 2019b). AE aims to extract aspects (e.g., “battery” in the laptop domain), ASC identifies the polarity for a given aspect (e.g., positive about battery) and E2E-ABSA is a combination of AE and ASC that detects aspects and their associated polarities simultaneously. Existing studies show that tasks in ABSA require the understanding of the interactions of aspects (e.g., “screen” 1 The pre-trained model and code can be found at https://github.com/howardhsu/BERT-for-"
2020.coling-main.21,W19-4828,0,0.196454,"l., 2019; Liu et al., 2019; Lan et al., 2019) yield significant performance gains when later fine-tuned on downstream NLP tasks. Recent studies also showed impressive results on tasks in aspect-based sentiment analysis (ABSA) (Xu et al., 2019; Sun et al., 2019a; Li et al., 2019b; Tian et al., 2020; Karimi et al., 2020), which aims to discover aspects and their associated opinions (Hu and Liu, 2004; Liu, 2012). Although there are existing studies of the hidden representations and attentions of LMs about tasks such as parsing and co-reference resolution (Adi et al., 2016; Belinkov et al., 2017; Clark et al., 2019), it is unclear how LMs capture aspects and sentiment/opinion from large-scale unlabeled texts. This paper attempts to investigate and understand the inner workings of the pretext task of the masked language model (MLM) in transformer and their connections with tasks in ABSA. This may benefit the following problems: (1) improving fine-tuning of ABSA if we have a better understanding about the gap between pretext tasks and fine-tuning tasks; (2) more importantly, self-supervised (or unsupervised) ABSA without fine-tuning to save the expensive efforts on annotating ABSA datasets (for a new domai"
2020.coling-main.21,N19-1423,0,0.038961,"on heads to encode context words (such as prepositions or pronouns that indicating an aspect) and opinion words for an aspect. Most features in the representation of an aspect are dedicated to the finegrained semantics of the domain (or product category) and the aspect itself, instead of carrying summarized opinions from its context. We hope this investigation can help future research in improving self-supervised learning, unsupervised learning and fine-tuning for ABSA. 1 1 Introduction As a form of self-supervised learning in NLP, pre-trained language models (LMs) like the masked LM in BERT (Devlin et al., 2019; Liu et al., 2019; Lan et al., 2019) yield significant performance gains when later fine-tuned on downstream NLP tasks. Recent studies also showed impressive results on tasks in aspect-based sentiment analysis (ABSA) (Xu et al., 2019; Sun et al., 2019a; Li et al., 2019b; Tian et al., 2020; Karimi et al., 2020), which aims to discover aspects and their associated opinions (Hu and Liu, 2004; Liu, 2012). Although there are existing studies of the hidden representations and attentions of LMs about tasks such as parsing and co-reference resolution (Adi et al., 2016; Belinkov et al., 2017; Clark et"
2020.coling-main.21,P14-2009,0,0.0331805,"may benefit the following problems: (1) improving fine-tuning of ABSA if we have a better understanding about the gap between pretext tasks and fine-tuning tasks; (2) more importantly, self-supervised (or unsupervised) ABSA without fine-tuning to save the expensive efforts on annotating ABSA datasets (for a new domain). We are particularly interested in fine-grained token-level features that are typically required by ABSA and how MLM as a general task can cover them during pre-training. Typical tasks of ABSA are: aspect extraction (AE), aspect sentiment classification (ASC) (Hu and Liu, 2004; Dong et al., 2014; Nguyen and Shirai, 2015; Li et al., 2018; Tang et al., 2016; Wang et al., 2016a; Wang et al., 2016b; Ma et al., 2017; Chen et al., 2017; Ma et al., 2017; Tay et al., 2018; He et al., 2018; Liu et al., 2018) and end-to-end ABSA (E2E-ABSA) (Li et al., 2019a; Li et al., 2019b). AE aims to extract aspects (e.g., “battery” in the laptop domain), ASC identifies the polarity for a given aspect (e.g., positive about battery) and E2E-ABSA is a combination of AE and ASC that detects aspects and their associated polarities simultaneously. Existing studies show that tasks in ABSA require the understandi"
2020.coling-main.21,C18-1096,0,0.0165921,"upervised (or unsupervised) ABSA without fine-tuning to save the expensive efforts on annotating ABSA datasets (for a new domain). We are particularly interested in fine-grained token-level features that are typically required by ABSA and how MLM as a general task can cover them during pre-training. Typical tasks of ABSA are: aspect extraction (AE), aspect sentiment classification (ASC) (Hu and Liu, 2004; Dong et al., 2014; Nguyen and Shirai, 2015; Li et al., 2018; Tang et al., 2016; Wang et al., 2016a; Wang et al., 2016b; Ma et al., 2017; Chen et al., 2017; Ma et al., 2017; Tay et al., 2018; He et al., 2018; Liu et al., 2018) and end-to-end ABSA (E2E-ABSA) (Li et al., 2019a; Li et al., 2019b). AE aims to extract aspects (e.g., “battery” in the laptop domain), ASC identifies the polarity for a given aspect (e.g., positive about battery) and E2E-ABSA is a combination of AE and ASC that detects aspects and their associated polarities simultaneously. Existing studies show that tasks in ABSA require the understanding of the interactions of aspects (e.g., “screen” 1 The pre-trained model and code can be found at https://github.com/howardhsu/BERT-for-RRC-ABSA. This work is licensed under a Creative Com"
2020.coling-main.21,P18-1087,0,0.0145072,"oving fine-tuning of ABSA if we have a better understanding about the gap between pretext tasks and fine-tuning tasks; (2) more importantly, self-supervised (or unsupervised) ABSA without fine-tuning to save the expensive efforts on annotating ABSA datasets (for a new domain). We are particularly interested in fine-grained token-level features that are typically required by ABSA and how MLM as a general task can cover them during pre-training. Typical tasks of ABSA are: aspect extraction (AE), aspect sentiment classification (ASC) (Hu and Liu, 2004; Dong et al., 2014; Nguyen and Shirai, 2015; Li et al., 2018; Tang et al., 2016; Wang et al., 2016a; Wang et al., 2016b; Ma et al., 2017; Chen et al., 2017; Ma et al., 2017; Tay et al., 2018; He et al., 2018; Liu et al., 2018) and end-to-end ABSA (E2E-ABSA) (Li et al., 2019a; Li et al., 2019b). AE aims to extract aspects (e.g., “battery” in the laptop domain), ASC identifies the polarity for a given aspect (e.g., positive about battery) and E2E-ABSA is a combination of AE and ASC that detects aspects and their associated polarities simultaneously. Existing studies show that tasks in ABSA require the understanding of the interactions of aspects (e.g., “"
2020.coling-main.21,D19-5505,0,0.07291,"lf, instead of carrying summarized opinions from its context. We hope this investigation can help future research in improving self-supervised learning, unsupervised learning and fine-tuning for ABSA. 1 1 Introduction As a form of self-supervised learning in NLP, pre-trained language models (LMs) like the masked LM in BERT (Devlin et al., 2019; Liu et al., 2019; Lan et al., 2019) yield significant performance gains when later fine-tuned on downstream NLP tasks. Recent studies also showed impressive results on tasks in aspect-based sentiment analysis (ABSA) (Xu et al., 2019; Sun et al., 2019a; Li et al., 2019b; Tian et al., 2020; Karimi et al., 2020), which aims to discover aspects and their associated opinions (Hu and Liu, 2004; Liu, 2012). Although there are existing studies of the hidden representations and attentions of LMs about tasks such as parsing and co-reference resolution (Adi et al., 2016; Belinkov et al., 2017; Clark et al., 2019), it is unclear how LMs capture aspects and sentiment/opinion from large-scale unlabeled texts. This paper attempts to investigate and understand the inner workings of the pretext task of the masked language model (MLM) in transformer and their connections wi"
2020.coling-main.21,D15-1298,0,0.0233088,"lowing problems: (1) improving fine-tuning of ABSA if we have a better understanding about the gap between pretext tasks and fine-tuning tasks; (2) more importantly, self-supervised (or unsupervised) ABSA without fine-tuning to save the expensive efforts on annotating ABSA datasets (for a new domain). We are particularly interested in fine-grained token-level features that are typically required by ABSA and how MLM as a general task can cover them during pre-training. Typical tasks of ABSA are: aspect extraction (AE), aspect sentiment classification (ASC) (Hu and Liu, 2004; Dong et al., 2014; Nguyen and Shirai, 2015; Li et al., 2018; Tang et al., 2016; Wang et al., 2016a; Wang et al., 2016b; Ma et al., 2017; Chen et al., 2017; Ma et al., 2017; Tay et al., 2018; He et al., 2018; Liu et al., 2018) and end-to-end ABSA (E2E-ABSA) (Li et al., 2019a; Li et al., 2019b). AE aims to extract aspects (e.g., “battery” in the laptop domain), ASC identifies the polarity for a given aspect (e.g., positive about battery) and E2E-ABSA is a combination of AE and ASC that detects aspects and their associated polarities simultaneously. Existing studies show that tasks in ABSA require the understanding of the interactions of"
2020.coling-main.21,J11-1002,1,0.507104,"es simultaneously. Existing studies show that tasks in ABSA require the understanding of the interactions of aspects (e.g., “screen” 1 The pre-trained model and code can be found at https://github.com/howardhsu/BERT-for-RRC-ABSA. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 244 Proceedings of the 28th International Conference on Computational Linguistics, pages 244–250 Barcelona, Spain (Online), December 8-13, 2020 in the laptop domain) and its contexts, including sentiment (e.g., “clear”) (Qiu et al., 2011; Wang et al., 2017; He et al., ). As such, we believe how the hidden representation of an aspect encodes features about being an aspect and summarizing opinions of that aspect are crucial for ABSA. This paper represents a new addition to the existing analysis of BERT in (Clark et al., 2019), which focuses on studying the behavior of BERT’s hidden representation and self-attentions for general purposes. We focus on how the self-supervised training of BERT is prepared for fine-grained features that are important for ABSA. We leverage the annotated data of ABSA in our analysis to draw the releva"
2020.coling-main.21,N19-1035,0,0.335238,"and the aspect itself, instead of carrying summarized opinions from its context. We hope this investigation can help future research in improving self-supervised learning, unsupervised learning and fine-tuning for ABSA. 1 1 Introduction As a form of self-supervised learning in NLP, pre-trained language models (LMs) like the masked LM in BERT (Devlin et al., 2019; Liu et al., 2019; Lan et al., 2019) yield significant performance gains when later fine-tuned on downstream NLP tasks. Recent studies also showed impressive results on tasks in aspect-based sentiment analysis (ABSA) (Xu et al., 2019; Sun et al., 2019a; Li et al., 2019b; Tian et al., 2020; Karimi et al., 2020), which aims to discover aspects and their associated opinions (Hu and Liu, 2004; Liu, 2012). Although there are existing studies of the hidden representations and attentions of LMs about tasks such as parsing and co-reference resolution (Adi et al., 2016; Belinkov et al., 2017; Clark et al., 2019), it is unclear how LMs capture aspects and sentiment/opinion from large-scale unlabeled texts. This paper attempts to investigate and understand the inner workings of the pretext task of the masked language model (MLM) in transformer and th"
2020.coling-main.21,D16-1021,0,0.0201466,"of ABSA if we have a better understanding about the gap between pretext tasks and fine-tuning tasks; (2) more importantly, self-supervised (or unsupervised) ABSA without fine-tuning to save the expensive efforts on annotating ABSA datasets (for a new domain). We are particularly interested in fine-grained token-level features that are typically required by ABSA and how MLM as a general task can cover them during pre-training. Typical tasks of ABSA are: aspect extraction (AE), aspect sentiment classification (ASC) (Hu and Liu, 2004; Dong et al., 2014; Nguyen and Shirai, 2015; Li et al., 2018; Tang et al., 2016; Wang et al., 2016a; Wang et al., 2016b; Ma et al., 2017; Chen et al., 2017; Ma et al., 2017; Tay et al., 2018; He et al., 2018; Liu et al., 2018) and end-to-end ABSA (E2E-ABSA) (Li et al., 2019a; Li et al., 2019b). AE aims to extract aspects (e.g., “battery” in the laptop domain), ASC identifies the polarity for a given aspect (e.g., positive about battery) and E2E-ABSA is a combination of AE and ASC that detects aspects and their associated polarities simultaneously. Existing studies show that tasks in ABSA require the understanding of the interactions of aspects (e.g., “screen” 1 The pre-t"
2020.coling-main.21,2020.acl-main.374,0,0.115281,"rying summarized opinions from its context. We hope this investigation can help future research in improving self-supervised learning, unsupervised learning and fine-tuning for ABSA. 1 1 Introduction As a form of self-supervised learning in NLP, pre-trained language models (LMs) like the masked LM in BERT (Devlin et al., 2019; Liu et al., 2019; Lan et al., 2019) yield significant performance gains when later fine-tuned on downstream NLP tasks. Recent studies also showed impressive results on tasks in aspect-based sentiment analysis (ABSA) (Xu et al., 2019; Sun et al., 2019a; Li et al., 2019b; Tian et al., 2020; Karimi et al., 2020), which aims to discover aspects and their associated opinions (Hu and Liu, 2004; Liu, 2012). Although there are existing studies of the hidden representations and attentions of LMs about tasks such as parsing and co-reference resolution (Adi et al., 2016; Belinkov et al., 2017; Clark et al., 2019), it is unclear how LMs capture aspects and sentiment/opinion from large-scale unlabeled texts. This paper attempts to investigate and understand the inner workings of the pretext task of the masked language model (MLM) in transformer and their connections with tasks in ABSA. Th"
2020.coling-main.21,D16-1059,0,0.0207715,"a better understanding about the gap between pretext tasks and fine-tuning tasks; (2) more importantly, self-supervised (or unsupervised) ABSA without fine-tuning to save the expensive efforts on annotating ABSA datasets (for a new domain). We are particularly interested in fine-grained token-level features that are typically required by ABSA and how MLM as a general task can cover them during pre-training. Typical tasks of ABSA are: aspect extraction (AE), aspect sentiment classification (ASC) (Hu and Liu, 2004; Dong et al., 2014; Nguyen and Shirai, 2015; Li et al., 2018; Tang et al., 2016; Wang et al., 2016a; Wang et al., 2016b; Ma et al., 2017; Chen et al., 2017; Ma et al., 2017; Tay et al., 2018; He et al., 2018; Liu et al., 2018) and end-to-end ABSA (E2E-ABSA) (Li et al., 2019a; Li et al., 2019b). AE aims to extract aspects (e.g., “battery” in the laptop domain), ASC identifies the polarity for a given aspect (e.g., positive about battery) and E2E-ABSA is a combination of AE and ASC that detects aspects and their associated polarities simultaneously. Existing studies show that tasks in ABSA require the understanding of the interactions of aspects (e.g., “screen” 1 The pre-trained model and co"
2020.coling-main.21,D16-1058,0,0.0205395,"a better understanding about the gap between pretext tasks and fine-tuning tasks; (2) more importantly, self-supervised (or unsupervised) ABSA without fine-tuning to save the expensive efforts on annotating ABSA datasets (for a new domain). We are particularly interested in fine-grained token-level features that are typically required by ABSA and how MLM as a general task can cover them during pre-training. Typical tasks of ABSA are: aspect extraction (AE), aspect sentiment classification (ASC) (Hu and Liu, 2004; Dong et al., 2014; Nguyen and Shirai, 2015; Li et al., 2018; Tang et al., 2016; Wang et al., 2016a; Wang et al., 2016b; Ma et al., 2017; Chen et al., 2017; Ma et al., 2017; Tay et al., 2018; He et al., 2018; Liu et al., 2018) and end-to-end ABSA (E2E-ABSA) (Li et al., 2019a; Li et al., 2019b). AE aims to extract aspects (e.g., “battery” in the laptop domain), ASC identifies the polarity for a given aspect (e.g., positive about battery) and E2E-ABSA is a combination of AE and ASC that detects aspects and their associated polarities simultaneously. Existing studies show that tasks in ABSA require the understanding of the interactions of aspects (e.g., “screen” 1 The pre-trained model and co"
2020.coling-main.21,N19-1242,1,0.855247,"roduct category) and the aspect itself, instead of carrying summarized opinions from its context. We hope this investigation can help future research in improving self-supervised learning, unsupervised learning and fine-tuning for ABSA. 1 1 Introduction As a form of self-supervised learning in NLP, pre-trained language models (LMs) like the masked LM in BERT (Devlin et al., 2019; Liu et al., 2019; Lan et al., 2019) yield significant performance gains when later fine-tuned on downstream NLP tasks. Recent studies also showed impressive results on tasks in aspect-based sentiment analysis (ABSA) (Xu et al., 2019; Sun et al., 2019a; Li et al., 2019b; Tian et al., 2020; Karimi et al., 2020), which aims to discover aspects and their associated opinions (Hu and Liu, 2004; Liu, 2012). Although there are existing studies of the hidden representations and attentions of LMs about tasks such as parsing and co-reference resolution (Adi et al., 2016; Belinkov et al., 2017; Clark et al., 2019), it is unclear how LMs capture aspects and sentiment/opinion from large-scale unlabeled texts. This paper attempts to investigate and understand the inner workings of the pretext task of the masked language model (MLM) in"
2020.coling-main.290,D15-1075,0,0.0166434,"71.04 61.07 70.38 65.51 71.16 68.32 76.38 69.17 77.15 SST1 36.80 34.80 36.44 36.96 29.97 35.79 33.97 38.69 35.60 38.24 35.42 41.78 SST2 80.60 78.30 77.02 75.11 68.04 75.11 65.25 76.03 71.33 78.33 71.33 80.70 TREC 90.20 81.56 82.31 87.60 63.40 80.00 73.40 87.06 79.60 86.20 81.60 87.80 Experiments We evaluate the proposed method using one natural language inference dataset and four text classification datasets. The tasks act as good quality checks for the learned representations. The five datasets are SNLI, MR, SST1, SST2 and TREC, detailed training/dev/test splits are shown on Table 2: • SNLI (Bowman et al., 2015): a collection of human-written English sentence pairs manually labeled for balanced classificaTable 2: Summary statistics for the tion with labels: entailment, contradiction, and neutral. This is datasets after tokenization. c dethe natural language inference dataset, which is also solved via notes the number of target classes. classification. Data c Train Dev Test • MR v1.08 : Movie reviews with one sentence per review laSNLI 3 549367 9842 9842 MR 2 8529 1067 1066 beled positive or negative for sentiment classification. SST1 5 8544 1101 2210 • SST19 : an extension of MR but with fine-grained"
2020.coling-main.290,D17-1219,0,0.011919,"14; Chang et al., 2018). Several tasks have benefited from sparse representations, e.g., part-of-speech tagging (Ganchev et al., 2009), dependency parsing (Martins et al., 2011), and supervised classification (Yogatama and Smith, 2014). However, much of the research advances so far for NLP tasks are based on dense representations, e.g., text classification (Kim, 2014; Tang et al., 2015; Wu et al., 2017; Wang et al., 2018), natural language inference (Liu et al., 2019; Kim et al., 2019), machine translation (Cheng, 2019; He et al., 2016) and generation (Serban et al., 2017; Zhang et al., 2019; Du and Cardie, 2017). The study of sparse representations is still limited. There are two key limitations in the study of sparse representations. First, little work has been done to well connect dense and sparse spaces. The two types of representation are rather independent and cannot help each other to achieve synergy. Second, limited work has been done to generate representations of sentences or phrases in the sparse space using sparse word embeddings. Inspired by Fourier Transformation, this paper proposes a novel method called Semantic Transformation (ST) to address the problems. With the help of ST, dense an"
2020.coling-main.290,P15-2076,0,0.0422137,"Missing"
2020.coling-main.290,P15-1144,0,0.0197996,"th positive and negative classes. This demonstrates that the neutral class is more difficult to identify. 4 Related Work Sparse embeddings have been used in image (Ji et al., 2019; Zhou et al., 2016; Zhang and Patel, 2016), signal (Caiafa and Cichocki, 2013; Huang and Aviyente, 2007), and NLP (Subramanian et al., 2018; Kober et al., 2016) applications. Several sparse models have been proposed to produce sparse embeddings. For example, some previous works trained word embeddings with sparse or non-negative constraints (Murphy et al., 2012; Luo et al., 2015). Linguistically inspired dimensions (Faruqui et al., 2015) is another way to increase sparsity and interpretability. SPINE (SParse Interpretable Neural Embeddings) (Subramanian et al., 2018), a variant of denoising k-sparse autoencoder, can generate efficient and interpretable distributed word representations. Our method is different from these approaches. We not only construct sparse representations but also transform between dense and sparse spaces. We also combine word sparse representations to produce sentence representations. Some recent studies tried to achieve sparsity in novel ways (Park et al., 2017). We also proposed a novel method in this"
2020.coling-main.290,P14-1046,0,0.025271,"everaged. Inspired by Fourier Transformation, in this paper, we propose a novel Semantic Transformation method to bridge the dense and sparse spaces, which can facilitate the NLP research to shift from dense spaces to sparse spaces or to jointly use both spaces. Experiments using classification tasks and natural language inference task show that the proposed Semantic Transformation is effective. 1 Introduction A sparse vector is a vector that has a large number of zeros or near zeros. Many studies have shown that sparsity is a desirable property of representations, especially for explanation (Fyshe et al., 2014; Faruqui and Dyer, 2015). In this sense, sparse representation may hold the key to solving the explainability problem of deep neural networks. Apart from the interpretability property, sparse representations can also improve the usability of word vectors as features. The embeddings with good sparsity, interpretability or special meanings can also benefit downstream tasks (Guo et al., 2014; Chang et al., 2018). Several tasks have benefited from sparse representations, e.g., part-of-speech tagging (Ganchev et al., 2009), dependency parsing (Martins et al., 2011), and supervised classification ("
2020.coling-main.290,D14-1012,0,0.0327083,"ive. 1 Introduction A sparse vector is a vector that has a large number of zeros or near zeros. Many studies have shown that sparsity is a desirable property of representations, especially for explanation (Fyshe et al., 2014; Faruqui and Dyer, 2015). In this sense, sparse representation may hold the key to solving the explainability problem of deep neural networks. Apart from the interpretability property, sparse representations can also improve the usability of word vectors as features. The embeddings with good sparsity, interpretability or special meanings can also benefit downstream tasks (Guo et al., 2014; Chang et al., 2018). Several tasks have benefited from sparse representations, e.g., part-of-speech tagging (Ganchev et al., 2009), dependency parsing (Martins et al., 2011), and supervised classification (Yogatama and Smith, 2014). However, much of the research advances so far for NLP tasks are based on dense representations, e.g., text classification (Kim, 2014; Tang et al., 2015; Wu et al., 2017; Wang et al., 2018), natural language inference (Liu et al., 2019; Kim et al., 2019), machine translation (Cheng, 2019; He et al., 2016) and generation (Serban et al., 2017; Zhang et al., 2019; Du"
2020.coling-main.290,D14-1181,0,0.112956,"from the interpretability property, sparse representations can also improve the usability of word vectors as features. The embeddings with good sparsity, interpretability or special meanings can also benefit downstream tasks (Guo et al., 2014; Chang et al., 2018). Several tasks have benefited from sparse representations, e.g., part-of-speech tagging (Ganchev et al., 2009), dependency parsing (Martins et al., 2011), and supervised classification (Yogatama and Smith, 2014). However, much of the research advances so far for NLP tasks are based on dense representations, e.g., text classification (Kim, 2014; Tang et al., 2015; Wu et al., 2017; Wang et al., 2018), natural language inference (Liu et al., 2019; Kim et al., 2019), machine translation (Cheng, 2019; He et al., 2016) and generation (Serban et al., 2017; Zhang et al., 2019; Du and Cardie, 2017). The study of sparse representations is still limited. There are two key limitations in the study of sparse representations. First, little work has been done to well connect dense and sparse spaces. The two types of representation are rather independent and cannot help each other to achieve synergy. Second, limited work has been done to generate"
2020.coling-main.290,D16-1175,0,0.0272618,"t negative responses to negative classes are the positive sentiment bases, which directly indicate the sentiment polarities. Comparing with positive and negative classes, neutral class shows relative mixed responses. That means neutral class has similar semantemes to those of both positive and negative classes. This demonstrates that the neutral class is more difficult to identify. 4 Related Work Sparse embeddings have been used in image (Ji et al., 2019; Zhou et al., 2016; Zhang and Patel, 2016), signal (Caiafa and Cichocki, 2013; Huang and Aviyente, 2007), and NLP (Subramanian et al., 2018; Kober et al., 2016) applications. Several sparse models have been proposed to produce sparse embeddings. For example, some previous works trained word embeddings with sparse or non-negative constraints (Murphy et al., 2012; Luo et al., 2015). Linguistically inspired dimensions (Faruqui et al., 2015) is another way to increase sparsity and interpretability. SPINE (SParse Interpretable Neural Embeddings) (Subramanian et al., 2018), a variant of denoising k-sparse autoencoder, can generate efficient and interpretable distributed word representations. Our method is different from these approaches. We not only constr"
2020.coling-main.290,P19-1441,0,0.0223038,"rd vectors as features. The embeddings with good sparsity, interpretability or special meanings can also benefit downstream tasks (Guo et al., 2014; Chang et al., 2018). Several tasks have benefited from sparse representations, e.g., part-of-speech tagging (Ganchev et al., 2009), dependency parsing (Martins et al., 2011), and supervised classification (Yogatama and Smith, 2014). However, much of the research advances so far for NLP tasks are based on dense representations, e.g., text classification (Kim, 2014; Tang et al., 2015; Wu et al., 2017; Wang et al., 2018), natural language inference (Liu et al., 2019; Kim et al., 2019), machine translation (Cheng, 2019; He et al., 2016) and generation (Serban et al., 2017; Zhang et al., 2019; Du and Cardie, 2017). The study of sparse representations is still limited. There are two key limitations in the study of sparse representations. First, little work has been done to well connect dense and sparse spaces. The two types of representation are rather independent and cannot help each other to achieve synergy. Second, limited work has been done to generate representations of sentences or phrases in the sparse space using sparse word embeddings. Inspired by"
2020.coling-main.290,D15-1196,0,0.022459,"ans neutral class has similar semantemes to those of both positive and negative classes. This demonstrates that the neutral class is more difficult to identify. 4 Related Work Sparse embeddings have been used in image (Ji et al., 2019; Zhou et al., 2016; Zhang and Patel, 2016), signal (Caiafa and Cichocki, 2013; Huang and Aviyente, 2007), and NLP (Subramanian et al., 2018; Kober et al., 2016) applications. Several sparse models have been proposed to produce sparse embeddings. For example, some previous works trained word embeddings with sparse or non-negative constraints (Murphy et al., 2012; Luo et al., 2015). Linguistically inspired dimensions (Faruqui et al., 2015) is another way to increase sparsity and interpretability. SPINE (SParse Interpretable Neural Embeddings) (Subramanian et al., 2018), a variant of denoising k-sparse autoencoder, can generate efficient and interpretable distributed word representations. Our method is different from these approaches. We not only construct sparse representations but also transform between dense and sparse spaces. We also combine word sparse representations to produce sentence representations. Some recent studies tried to achieve sparsity in novel ways (P"
2020.coling-main.290,D11-1139,0,0.0390268,"ations, especially for explanation (Fyshe et al., 2014; Faruqui and Dyer, 2015). In this sense, sparse representation may hold the key to solving the explainability problem of deep neural networks. Apart from the interpretability property, sparse representations can also improve the usability of word vectors as features. The embeddings with good sparsity, interpretability or special meanings can also benefit downstream tasks (Guo et al., 2014; Chang et al., 2018). Several tasks have benefited from sparse representations, e.g., part-of-speech tagging (Ganchev et al., 2009), dependency parsing (Martins et al., 2011), and supervised classification (Yogatama and Smith, 2014). However, much of the research advances so far for NLP tasks are based on dense representations, e.g., text classification (Kim, 2014; Tang et al., 2015; Wu et al., 2017; Wang et al., 2018), natural language inference (Liu et al., 2019; Kim et al., 2019), machine translation (Cheng, 2019; He et al., 2016) and generation (Serban et al., 2017; Zhang et al., 2019; Du and Cardie, 2017). The study of sparse representations is still limited. There are two key limitations in the study of sparse representations. First, little work has been don"
2020.coling-main.290,C12-1118,0,0.034484,"ed responses. That means neutral class has similar semantemes to those of both positive and negative classes. This demonstrates that the neutral class is more difficult to identify. 4 Related Work Sparse embeddings have been used in image (Ji et al., 2019; Zhou et al., 2016; Zhang and Patel, 2016), signal (Caiafa and Cichocki, 2013; Huang and Aviyente, 2007), and NLP (Subramanian et al., 2018; Kober et al., 2016) applications. Several sparse models have been proposed to produce sparse embeddings. For example, some previous works trained word embeddings with sparse or non-negative constraints (Murphy et al., 2012; Luo et al., 2015). Linguistically inspired dimensions (Faruqui et al., 2015) is another way to increase sparsity and interpretability. SPINE (SParse Interpretable Neural Embeddings) (Subramanian et al., 2018), a variant of denoising k-sparse autoencoder, can generate efficient and interpretable distributed word representations. Our method is different from these approaches. We not only construct sparse representations but also transform between dense and sparse spaces. We also combine word sparse representations to produce sentence representations. Some recent studies tried to achieve sparsi"
2020.coling-main.290,D17-1041,0,0.0200617,"). Linguistically inspired dimensions (Faruqui et al., 2015) is another way to increase sparsity and interpretability. SPINE (SParse Interpretable Neural Embeddings) (Subramanian et al., 2018), a variant of denoising k-sparse autoencoder, can generate efficient and interpretable distributed word representations. Our method is different from these approaches. We not only construct sparse representations but also transform between dense and sparse spaces. We also combine word sparse representations to produce sentence representations. Some recent studies tried to achieve sparsity in novel ways (Park et al., 2017). We also proposed a novel method in this paper and experimentally verified its effectiveness. 5 Conclusion and Future Works This paper proposed a novel method to transform representations between dense and sparse spaces, and a technique to combine semantics in the sparse space. It also proposed and experimentally verified a new activation function that can be used to achieve sparseness. Natural language inference and text classification tasks were used to evaluate the proposed transformations with promising results. Based on this study, many other interesting directions can be pursued in the"
2020.coling-main.290,P14-1074,0,0.0235891,"; Faruqui and Dyer, 2015). In this sense, sparse representation may hold the key to solving the explainability problem of deep neural networks. Apart from the interpretability property, sparse representations can also improve the usability of word vectors as features. The embeddings with good sparsity, interpretability or special meanings can also benefit downstream tasks (Guo et al., 2014; Chang et al., 2018). Several tasks have benefited from sparse representations, e.g., part-of-speech tagging (Ganchev et al., 2009), dependency parsing (Martins et al., 2011), and supervised classification (Yogatama and Smith, 2014). However, much of the research advances so far for NLP tasks are based on dense representations, e.g., text classification (Kim, 2014; Tang et al., 2015; Wu et al., 2017; Wang et al., 2018), natural language inference (Liu et al., 2019; Kim et al., 2019), machine translation (Cheng, 2019; He et al., 2016) and generation (Serban et al., 2017; Zhang et al., 2019; Du and Cardie, 2017). The study of sparse representations is still limited. There are two key limitations in the study of sparse representations. First, little work has been done to well connect dense and sparse spaces. The two types o"
2020.coling-main.290,D18-1350,0,0.130254,"maintaining the semantic information consistency. The last term helps learn similar representations with LSTM. Table 1: Average accuracy over all tasks. Y and X’ are representations for making predictions (X’ is the back transformation of Y; Y is the sparse representation). Traditional penalty means the partial sparsity loss in (Subramanian et al., 2018). Helper loss refers to ML or BL. Note that only the experiments using X’ as the representations for prediction has RLo . RLo is not used when using Y as the prediction feature. Model CNN (Kim, 2014) Transformer (Vaswani et al., 2017) Capsule (Zhao et al., 2018) LSTM (Hochreiter and Schmidhuber, 1997) ST¶ [X’] (without sparse activation or helper loss) ST‡ [X’] (with sparse activation, without helper loss) † ST [X’] (using the traditional penalty, without sparse activation or helper loss) ST[X’] (full model) ST¶ [Y] (without sparse activation or helper loss) ST‡ [Y] (with sparse activation, without helper loss) † ST [Y] (using the traditional penalty, without sparse activation or helper loss) ST[Y] (full model) 3 SNLI 59.71 55.32 54.53 66.66 32.90 63.34 59.89 66.58 62.46 63.53 62.62 66.85 MR 76.10 75.23 72.57 71.04 61.07 70.38 65.51 71.16 68.32 76.38"
2020.coling-main.463,D18-1547,0,0.0595733,"Missing"
2020.coling-main.50,P07-1056,0,0.157582,"et al., 2015). Wang et al. (2019) and Lv et al. (2019) also used this dataset. The dataset contains Amazon reviews of 20 types of product domains, denoted by Amazon(20) 3 . Each domain consists of 1,000 reviews. Each review has been assigned a sentiment label, i.e., positive (+) or negative (-). We did not use the dataset in (Xia et al., 2017) as their data for each task/domain are from the same domain. In addition, we sampled a large Amazon review dataset from the SNAP corpus 4 , which contains product reviews from 24 types of product domains. Following the existing works (Pang et al., 2002; Blitzer et al., 2007; Chen et al., 2015), we treat reviews with rating &gt; 3 as positive and reviews with rating < 3 as negative. We randomly sampled 10,000 reviews for each domain from the SNAP corpus. The sampled dataset is denoted by SNAP(24). 3 4 http://anthology.aclweb.org/attachments/P/P15/P15-2123.Datasets.zip http://jmcauley.ucsd.edu/data/amazon/ 586 The Dataset Amazon(20) Alarm Clock Baby Bag Cable Modem Dumbbell Flashlight Jewelry Gloves Graphics Card Headphone The Dataset SNAP(24) Amazon Instant Video Apps for Android Automotive Baby Beauty Books CDs and Vinyl Cell Phones and Accessories Clothing Shoes a"
2020.coling-main.50,P15-2123,1,0.88008,"f sentiment classification tasks. This learning setting is called lifelong learning. Lifelong learning is a continual learning process where the learner learns a sequence of tasks; after each task is learned, its knowledge is retained and later used to help new/future task learning (Thrun, 1998; Silver et al., 2013; Ruvolo and Eaton, 2013; Mitchell et al., 2018; Parisi et al., 2019). Lifelong learning has been introduced extensively in a book, see (Chen and Liu, 2018). The first lifelong learning method for sentiment classification (called lifelong sentiment classification) was introduced in (Chen et al., 2015). Following Chen et al. (2015), several other lifelong sentiment classification approaches were proposed respectively in (Xia et al., 2017; Wang et al., 2019; Lv et al., 2019; Xu et al., 2020). We will discuss these and other related work in the next section. Among these, (Lv et al., 2019) is a pioneering lifelong sentiment classification method based on deep learning. Following these existing works, we treat the classification in each domain (e.g., a type of product) as a learning task. Thus, we use the terms domain and task interchangeably throughout the paper. Formally, we study the followi"
2020.coling-main.50,W19-4828,0,0.0220001,"nt classification method. Most of them learn in isolation as the classic deep learning paradigm learned, although some methods work for multi-domain sentiment classification which focus on leveraging data in source domain to help target domain. So, they are far from sufficient for our goal as we aim at building a model to learn a sequence of sentiment classification tasks and learn them successively. In addition, whether attention is explainable (or interpretable) was recently discussed in (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019). Letarte et al. (2018) and Clark et al. (2019) show the importance of attention for neural networks. In this paper, we introduce a knowledge-enhanced attention model which we call BLAN (as shorthand for Bayes-enhanced Lifelong Attention Network) for learning a sequence of sentiment classification tasks. The idea is that the model parameters of na¨ıve Bayes are first generated from each task. These parameters are then transformed to knowledge, retained in the knowledge base and later used to build lifelong attentions. Frequent Pattern Mining (Agrawal et al., 1994) is used in extracting lifelong attentions. The extracted lifelong attentions"
2020.coling-main.50,C18-1066,0,0.0220563,"a large number of (possibly never ending) domains/tasks. Thus, we motivate and propose to use attention from previous tasks as knowledge to help future task learning. Attention has become enormously popular as an essential component of neural networks especially for a wide range of natural language processing tasks (Chaudhari et al., 2019). Attention neural networks was first introduced for machine translation (Bahdanau et al., 2014). Since then, a large body of sentiment classification models using attention mechanisms have been arrived — to name a few (Zhou et al., 2016; Yang et al., 2016; Gu et al., 2018; He et al., 2018; Peng et al., 2019; Zhang et al., 2019; Yu et al., 2019). However, none of them is a lifelong sentiment classification method. Most of them learn in isolation as the classic deep learning paradigm learned, although some methods work for multi-domain sentiment classification which focus on leveraging data in source domain to help target domain. So, they are far from sufficient for our goal as we aim at building a model to learn a sequence of sentiment classification tasks and learn them successively. In addition, whether attention is explainable (or interpretable) was recently"
2020.coling-main.50,C18-1096,0,0.0211999,"f (possibly never ending) domains/tasks. Thus, we motivate and propose to use attention from previous tasks as knowledge to help future task learning. Attention has become enormously popular as an essential component of neural networks especially for a wide range of natural language processing tasks (Chaudhari et al., 2019). Attention neural networks was first introduced for machine translation (Bahdanau et al., 2014). Since then, a large body of sentiment classification models using attention mechanisms have been arrived — to name a few (Zhou et al., 2016; Yang et al., 2016; Gu et al., 2018; He et al., 2018; Peng et al., 2019; Zhang et al., 2019; Yu et al., 2019). However, none of them is a lifelong sentiment classification method. Most of them learn in isolation as the classic deep learning paradigm learned, although some methods work for multi-domain sentiment classification which focus on leveraging data in source domain to help target domain. So, they are far from sufficient for our goal as we aim at building a model to learn a sequence of sentiment classification tasks and learn them successively. In addition, whether attention is explainable (or interpretable) was recently discussed in (Ja"
2020.coling-main.50,N19-1357,0,0.0288402,"18; Peng et al., 2019; Zhang et al., 2019; Yu et al., 2019). However, none of them is a lifelong sentiment classification method. Most of them learn in isolation as the classic deep learning paradigm learned, although some methods work for multi-domain sentiment classification which focus on leveraging data in source domain to help target domain. So, they are far from sufficient for our goal as we aim at building a model to learn a sequence of sentiment classification tasks and learn them successively. In addition, whether attention is explainable (or interpretable) was recently discussed in (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019). Letarte et al. (2018) and Clark et al. (2019) show the importance of attention for neural networks. In this paper, we introduce a knowledge-enhanced attention model which we call BLAN (as shorthand for Bayes-enhanced Lifelong Attention Network) for learning a sequence of sentiment classification tasks. The idea is that the model parameters of na¨ıve Bayes are first generated from each task. These parameters are then transformed to knowledge, retained in the knowledge base and later used to build lifelong attentions. Frequent Pattern Minin"
2020.coling-main.50,W18-5429,0,0.0350481,"Missing"
2020.coling-main.50,P08-2065,0,0.0513627,"ive. One reason why we can understand these clearly is that we give different focuses/attentions to each word and each sentence. We give strong attentions to the words (good, heavy, and recommend) and the last two sentences. Another crucial reason is that we have accumulated a large amount of knowledge in the past, and use it now to help understanding. For example, we know that the word “good” is positive for all products, and the word “heavy” is negative for camera but may be positive for some other products. Such a problem is called domain generality and domain specificity as introduced in (Li and Zong, 2008). The problem is particularly acute in lifelong learning as lifelong learning needs to tackle a large number of (possibly never ending) domains/tasks. Thus, we motivate and propose to use attention from previous tasks as knowledge to help future task learning. Attention has become enormously popular as an essential component of neural networks especially for a wide range of natural language processing tasks (Chaudhari et al., 2019). Attention neural networks was first introduced for machine translation (Bahdanau et al., 2014). Since then, a large body of sentiment classification models using a"
2020.coling-main.50,W02-1011,0,0.0334742,"ataset as in (Chen et al., 2015). Wang et al. (2019) and Lv et al. (2019) also used this dataset. The dataset contains Amazon reviews of 20 types of product domains, denoted by Amazon(20) 3 . Each domain consists of 1,000 reviews. Each review has been assigned a sentiment label, i.e., positive (+) or negative (-). We did not use the dataset in (Xia et al., 2017) as their data for each task/domain are from the same domain. In addition, we sampled a large Amazon review dataset from the SNAP corpus 4 , which contains product reviews from 24 types of product domains. Following the existing works (Pang et al., 2002; Blitzer et al., 2007; Chen et al., 2015), we treat reviews with rating &gt; 3 as positive and reviews with rating < 3 as negative. We randomly sampled 10,000 reviews for each domain from the SNAP corpus. The sampled dataset is denoted by SNAP(24). 3 4 http://anthology.aclweb.org/attachments/P/P15/P15-2123.Datasets.zip http://jmcauley.ucsd.edu/data/amazon/ 586 The Dataset Amazon(20) Alarm Clock Baby Bag Cable Modem Dumbbell Flashlight Jewelry Gloves Graphics Card Headphone The Dataset SNAP(24) Amazon Instant Video Apps for Android Automotive Baby Beauty Books CDs and Vinyl Cell Phones and Access"
2020.coling-main.50,D14-1162,0,0.0871549,"te, the F1 of negative class should be more reliable than the ACC of both classes as negative reviews are minorities in each dataset. Model parameters. For the NB, the smoothing parameter is set to 1 (Laplacian smoothing). For the other non-neural networks models (i.e., NBSVM-uni, NBSVM-bi, LLV, LSC and LNB), we use their default parameter settings. In addition, we followed Pang et al. (2002) to deal with negation words as performed in (Chen et al., 2015) for NB, LSC and LNB. For the neural networks models (i.e., ConvGRNN, LSTM-GRNN, HAN, SRK, nBLAN and BLAN), we use pre-trained GloVe.840B 6 (Pennington et al., 2014) to initialize the word embeddings and the embedding dimension is 300. The number of convolutional filters is 3 with kernel sizes 1, 2 and 3 respectively. The size of LSTM and GRU hidden states is 300. The network parameters are updated using the Adam optimizer (Kingma and Ba, 2015) with a learning rate of 0.001. The learning rate is clipped gradually using a norm of 0.9 in performing the Adam optimization. The dropout rate is set to 0.5 in the input layer. We use Accuracy (ACC) of both classes in evaluation. We also use F1 of the negative class because the number of reviews in the negative cl"
2020.coling-main.50,D19-1005,0,0.0263917,"transfer from a labeled source domain to an unlabeled target domain. Domain adaptation with multiple sources (or called Multi-source Domain Adaption) aims to use the labeled data collected from multiple sources to help target domain learning (Sun et al., 2015; Wu and Huang, 2016). However, these methods are only one-directional, i.e., sources helps target because the target has no or little labeled data. Our work is also related to knowledge-enhanced attention neural networks. Knowledge-enhanced attention models have been introduced in (Chen et al., 2017; Zhou et al., 2018; Yang et al., 2019; Peters et al., 2019; Chen et al., 2019; Huang et al., 2020). However, they do not concern sentiment classification task. Although there are a large body of attention models for sentiment classification as we mentioned in the previous section, none of them uses enhanced attention mechanisms. They mainly use self-attention learned from the networks. Our proposed attention mechanism is a combination of self-attention and enhanced-attention. We detail our attention mechanism and the proposed model next. 3 Proposed Model The proposed BLAN model is a hierarchical architecture as shown in Figure 1. The key characterist"
2020.coling-main.50,P19-1282,0,0.0206329,"hang et al., 2019; Yu et al., 2019). However, none of them is a lifelong sentiment classification method. Most of them learn in isolation as the classic deep learning paradigm learned, although some methods work for multi-domain sentiment classification which focus on leveraging data in source domain to help target domain. So, they are far from sufficient for our goal as we aim at building a model to learn a sequence of sentiment classification tasks and learn them successively. In addition, whether attention is explainable (or interpretable) was recently discussed in (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019). Letarte et al. (2018) and Clark et al. (2019) show the importance of attention for neural networks. In this paper, we introduce a knowledge-enhanced attention model which we call BLAN (as shorthand for Bayes-enhanced Lifelong Attention Network) for learning a sequence of sentiment classification tasks. The idea is that the model parameters of na¨ıve Bayes are first generated from each task. These parameters are then transformed to knowledge, retained in the knowledge base and later used to build lifelong attentions. Frequent Pattern Mining (Agrawal et al., 1994)"
2020.coling-main.50,P17-2023,1,0.791682,"training data. Xu et al. (2020) proposed a continuous na¨ıve Bayes learning framework for e-commerce product review sentiment classification by extending the parameter estimation mechanism in na¨ıve Bayes. Lv et al. (2019) introduced a deep learning method for lifelong sentiment classification by jointly training two networks, a feature learning network and a knowledge retention network. Such a method is the first lifelong sentiment classification method based on deep learning. We have compared this deep learning method in our experiments and show that it is inferior to our proposed method. (Shu et al., 2017; Wang et al., 2018a; Wang et al., 2018b) also proposed lifelong learning methods for sentiment classification, but they focus on aspect-based analysis, which is different from document-level sentiment classification (our focus in this paper). There are currently several research topics that are closely related to lifelong learning — most notably, multi-task learning and transfer learning. Multi-task learning vs. lifelong learning: Multi-task learning (Zhang and Yang, 2017) jointly optimizes the learning of multiple tasks. Although it is possible to make it continual, multi-task learning does"
2020.coling-main.50,D15-1167,0,0.0168803,"y hard to classify. 4.2 Baselines In this work, we use na¨ıve Bayes (NB) parameters to enhance attention neural networks and propose a lifelong sentiment classification model. So, we compare with three types of methods, namely na¨ıve Bayes, neural networks, and lifelong sentiment classification. • Na¨ıve Bayes: we compare our model with the classic NB method. We also compare with a strong variant of NB, called NBSVM (Wang and Manning, 2012). There are two models of NBSVM with uni-grams and bi-grams, denoted by NBSVM-uni and NBSVM-bi, respectively. • Neural Networks: we compare with Conv-GRNN (Tang et al., 2015), LSTM-GRNN (Tang et al., 2015) and HAN (an attention based model) (Yang et al., 2016). • Lifelong Sentiment Classification: we compare with the the existing works LLV (Xia et al., 2017), LSC (Chen et al., 2015), LNB (Wang et al., 2019) and SRK (Lv et al., 2019). For our BLAN, we also created a variant without the Bayes-enhanced attentions, denoted by nBLAN. nBLAN is to evaluate our Bayes-enhanced attentions are helpful. 4.3 Settings Our experiment settings and model parameters are set as follows. Training set, validation set and test set. For each dataset, the review documents in each domain"
2020.coling-main.50,P12-2018,0,0.148871,"Missing"
2020.coling-main.50,P18-1088,1,0.822469,"et al. (2020) proposed a continuous na¨ıve Bayes learning framework for e-commerce product review sentiment classification by extending the parameter estimation mechanism in na¨ıve Bayes. Lv et al. (2019) introduced a deep learning method for lifelong sentiment classification by jointly training two networks, a feature learning network and a knowledge retention network. Such a method is the first lifelong sentiment classification method based on deep learning. We have compared this deep learning method in our experiments and show that it is inferior to our proposed method. (Shu et al., 2017; Wang et al., 2018a; Wang et al., 2018b) also proposed lifelong learning methods for sentiment classification, but they focus on aspect-based analysis, which is different from document-level sentiment classification (our focus in this paper). There are currently several research topics that are closely related to lifelong learning — most notably, multi-task learning and transfer learning. Multi-task learning vs. lifelong learning: Multi-task learning (Zhang and Yang, 2017) jointly optimizes the learning of multiple tasks. Although it is possible to make it continual, multi-task learning does not retain any know"
2020.coling-main.50,D19-1002,0,0.0154398,"al., 2019). However, none of them is a lifelong sentiment classification method. Most of them learn in isolation as the classic deep learning paradigm learned, although some methods work for multi-domain sentiment classification which focus on leveraging data in source domain to help target domain. So, they are far from sufficient for our goal as we aim at building a model to learn a sequence of sentiment classification tasks and learn them successively. In addition, whether attention is explainable (or interpretable) was recently discussed in (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019). Letarte et al. (2018) and Clark et al. (2019) show the importance of attention for neural networks. In this paper, we introduce a knowledge-enhanced attention model which we call BLAN (as shorthand for Bayes-enhanced Lifelong Attention Network) for learning a sequence of sentiment classification tasks. The idea is that the model parameters of na¨ıve Bayes are first generated from each task. These parameters are then transformed to knowledge, retained in the knowledge base and later used to build lifelong attentions. Frequent Pattern Mining (Agrawal et al., 1994) is used in extracting lifelon"
2020.coling-main.50,P16-1029,0,0.022393,"ain labeled data to help target domain learning which has no or little labeled data. Unlike lifelong learning, transfer learning is not continual and has no knowledge retention. The source should be similar to the target (which are normally selected by the user). Domain adaptation is a type of transfer learning by establishing knowledge transfer from a labeled source domain to an unlabeled target domain. Domain adaptation with multiple sources (or called Multi-source Domain Adaption) aims to use the labeled data collected from multiple sources to help target domain learning (Sun et al., 2015; Wu and Huang, 2016). However, these methods are only one-directional, i.e., sources helps target because the target has no or little labeled data. Our work is also related to knowledge-enhanced attention neural networks. Knowledge-enhanced attention models have been introduced in (Chen et al., 2017; Zhou et al., 2018; Yang et al., 2019; Peters et al., 2019; Chen et al., 2019; Huang et al., 2020). However, they do not concern sentiment classification task. Although there are a large body of attention models for sentiment classification as we mentioned in the previous section, none of them uses enhanced attention"
2020.coling-main.50,N16-1174,0,0.0603858,"ing needs to tackle a large number of (possibly never ending) domains/tasks. Thus, we motivate and propose to use attention from previous tasks as knowledge to help future task learning. Attention has become enormously popular as an essential component of neural networks especially for a wide range of natural language processing tasks (Chaudhari et al., 2019). Attention neural networks was first introduced for machine translation (Bahdanau et al., 2014). Since then, a large body of sentiment classification models using attention mechanisms have been arrived — to name a few (Zhou et al., 2016; Yang et al., 2016; Gu et al., 2018; He et al., 2018; Peng et al., 2019; Zhang et al., 2019; Yu et al., 2019). However, none of them is a lifelong sentiment classification method. Most of them learn in isolation as the classic deep learning paradigm learned, although some methods work for multi-domain sentiment classification which focus on leveraging data in source domain to help target domain. So, they are far from sufficient for our goal as we aim at building a model to learn a sequence of sentiment classification tasks and learn them successively. In addition, whether attention is explainable (or interpreta"
2020.coling-main.50,D16-1024,0,0.0180358,"g as lifelong learning needs to tackle a large number of (possibly never ending) domains/tasks. Thus, we motivate and propose to use attention from previous tasks as knowledge to help future task learning. Attention has become enormously popular as an essential component of neural networks especially for a wide range of natural language processing tasks (Chaudhari et al., 2019). Attention neural networks was first introduced for machine translation (Bahdanau et al., 2014). Since then, a large body of sentiment classification models using attention mechanisms have been arrived — to name a few (Zhou et al., 2016; Yang et al., 2016; Gu et al., 2018; He et al., 2018; Peng et al., 2019; Zhang et al., 2019; Yu et al., 2019). However, none of them is a lifelong sentiment classification method. Most of them learn in isolation as the classic deep learning paradigm learned, although some methods work for multi-domain sentiment classification which focus on leveraging data in source domain to help target domain. So, they are far from sufficient for our goal as we aim at building a model to learn a sequence of sentiment classification tasks and learn them successively. In addition, whether attention is explain"
2020.findings-emnlp.101,P17-1001,0,0.0632672,"Missing"
2020.findings-emnlp.101,P15-2123,1,0.659084,"gy, Peking University 3 Department of Information Science, Peking University {qinqi, wenpeng.hu}@pku.edu.cn, dcsliub@gmail.com Abstract prior knowledge to the new task to help it learn better. The answer should be yes because words and phrases used to express opinions or sentiments in different domains are similar and thus can mostly be shared or transferred across domains, although different domains do have domain specific sentiment expressions. This is a lifelong learning setting (Thrun, 1998; Silver et al., 2013; Chen and Liu, 2016). This paper focuses on lifelong sentiment classification (Chen et al., 2015). This paper studies sentiment classification in the lifelong learning setting that incrementally learns a sequence of sentiment classification tasks. It proposes a new lifelong learning model (called L2PG) that can retain and selectively transfer the knowledge learned in the past to help learn the new task. A key innovation of this proposed model is a novel parameter-gate (p-gate) mechanism that regulates the flow or transfer of the previously learned knowledge to the new task. Specifically, it can selectively use the network parameters (which represent the retained knowledge gained from the"
2020.findings-emnlp.101,P84-1044,0,0.361213,"Missing"
2020.findings-emnlp.101,D14-1181,0,0.0251657,"ssification corpus from reviews of 10 diverse product categories for lifelong learning evaluation. Such evaluations need many tasks. To our knowledge, no existing sentence sentiment classification corpus fits this need. Experimental results show that L2PG outperforms state-of-the-art baselines including multitask learning, which optimizes all the tasks at the same time. 2 Related Work Our work is related to sentiment classification (Liu, 2012), lifelong learning and continual learning. For sentiment classification, recent deep learning models have been shown to outperform traditional methods (Kim, 2014; Devlin et al., 2018; Shen et al., 2018; Zhang et al., 2019; Qin et al., 2020). However, these models don’t retain or transfer the knowledge to new tasks. Lifelong learning: Most relevant to our work is lifelong learning (Thrun, 1998; Silver et al., 2013; Ruvolo and Eaton, 2013; Chen and Liu, 2014, 2016). For lifelong sentiment classification, Chen et al. (2015) used naive Bayes to leverage word probabilities under different classes in old tasks/domains as priors to help optimize the new task learning. Wang et al. (2019) worked similarly but their method can improve the model of a previous ta"
2020.findings-emnlp.101,2020.acl-main.726,1,0.766184,"felong learning evaluation. Such evaluations need many tasks. To our knowledge, no existing sentence sentiment classification corpus fits this need. Experimental results show that L2PG outperforms state-of-the-art baselines including multitask learning, which optimizes all the tasks at the same time. 2 Related Work Our work is related to sentiment classification (Liu, 2012), lifelong learning and continual learning. For sentiment classification, recent deep learning models have been shown to outperform traditional methods (Kim, 2014; Devlin et al., 2018; Shen et al., 2018; Zhang et al., 2019; Qin et al., 2020). However, these models don’t retain or transfer the knowledge to new tasks. Lifelong learning: Most relevant to our work is lifelong learning (Thrun, 1998; Silver et al., 2013; Ruvolo and Eaton, 2013; Chen and Liu, 2014, 2016). For lifelong sentiment classification, Chen et al. (2015) used naive Bayes to leverage word probabilities under different classes in old tasks/domains as priors to help optimize the new task learning. Wang et al. (2019) worked similarly but their method can improve the model of a previous task without retraining. Xia et al. (2017) proposed a voting method but their met"
2020.findings-emnlp.101,P18-1041,0,0.0117502,"of 10 diverse product categories for lifelong learning evaluation. Such evaluations need many tasks. To our knowledge, no existing sentence sentiment classification corpus fits this need. Experimental results show that L2PG outperforms state-of-the-art baselines including multitask learning, which optimizes all the tasks at the same time. 2 Related Work Our work is related to sentiment classification (Liu, 2012), lifelong learning and continual learning. For sentiment classification, recent deep learning models have been shown to outperform traditional methods (Kim, 2014; Devlin et al., 2018; Shen et al., 2018; Zhang et al., 2019; Qin et al., 2020). However, these models don’t retain or transfer the knowledge to new tasks. Lifelong learning: Most relevant to our work is lifelong learning (Thrun, 1998; Silver et al., 2013; Ruvolo and Eaton, 2013; Chen and Liu, 2014, 2016). For lifelong sentiment classification, Chen et al. (2015) used naive Bayes to leverage word probabilities under different classes in old tasks/domains as priors to help optimize the new task learning. Wang et al. (2019) worked similarly but their method can improve the model of a previous task without retraining. Xia et al. (2017)"
2020.findings-emnlp.101,P18-1088,1,0.89002,"ive Bayes to leverage word probabilities under different classes in old tasks/domains as priors to help optimize the new task learning. Wang et al. (2019) worked similarly but their method can improve the model of a previous task without retraining. Xia et al. (2017) proposed a voting method but their method works on the same data from different time periods. Lv et al. (2019) proposed a model using two networks, one 1125 for knowledge retention and one for feature learning. But it was shown to be weaker than (Wang et al., 2019). L2PG has a very different approach and performs markedly better. Wang et al. (2018) studied aspect level sentiment classification, which is not the goal of L2PG. However, to the best of our knowledge, none of these methods used gated mechanisms to regulate the transfer of knowledge in the lifelong learning process. Continual learning: It is similar to lifelong learning, but its main goal is to overcome catastrophic forgetting to ensure learning of a new task will not forget the models learned for previous tasks (McCloskey and Cohen, 1989; Goodfellow et al., 2013). For example, LWF (Li and Hoiem, 2017) uses knowledge distillation loss to ensure that after learning a new task,"
2020.findings-emnlp.146,D19-1118,0,0.0252172,"Missing"
2020.findings-emnlp.146,C16-1251,0,0.0314247,"used for training, development and test for each of the five domains. Knowledge Resources. We used three types of knowledge resources as listed below. The first two are general KBs, while the third one is our mined domain-specific KB. 1. Commonsense knowledge graph (OMCS). We use the open mind common sense (OMCS) KB as general knowledge (Speer and Havasi, 2013). OMCS contains 600K crowd-sourced commonsense triplets such as (clock, UsedFor, keeping time). We follow (Zhang et al., 2019b) to select highly-confident triplets and build the OMCS KG consisting of total 62,730 triplets. 2. Senticnet (Cambria et al., 2016). Senticnet is another commonsense knowledge base that contains 50k concepts associated with affective properties including sentiment information. To make the knowledge base fit for deep neural models, we concatenate SenticNet embeddings with BERT embeddings to extend the embedding information. 3. Domain-specific KB. This is mined from the unlabeled review dataset as discussed in Sec 3.1. Hyper-parameter Settings. Following the previous work of (Joshi et al., 2019; Lee et al., 2018), we use (Base) BERT8 embeddings of context and knowledge representation (as discussed in Section 3). The number"
2020.findings-emnlp.146,D19-1606,0,0.0308444,"Missing"
2020.findings-emnlp.146,N19-1423,0,0.010914,"as Sm and Sp respectively. We represent mention, anaphor, the syntaxrelated phrases, and also the phrases of knowledge from domain-specific and general KBs as spans (a continuous sequence of words), and learn a vector representation for each span (we call it a span vector) based on the embeddings of words that compose the span. The span vectors are then used by our knowledge-driven OAC2 model (discussed in Section 3.3) for solving the OAC2 task. Below, we discuss the span vector representation learning for a given span (corresponding to a syntax-related phrase or a phrase in KB). We use BERT (Devlin et al., 2019) to learn the vector representation for each span. To encode 1619 the words in a span, we use BERT’s WordPiece 1 tokenizer. Given a span x, let {xi }N i=1 be the output token embeddings of x from BERT, where N1 is the total number of word-piece tokens for span x. BERT is a neural model consisting of stacked attention layers. To incorporate the syntax-based information, we want the head of a span and words that have a modifier relation to the head to have higher attention weights. To achieve the goal, we adopt syntax-based attention (He et al., 2018). The weight of a word in a span depends on t"
2020.findings-emnlp.146,C10-1031,1,0.294481,"d attribute coreferences. The approach extracts domain-specific knowledge from unlabeled review data and trains a knowledgeaware neural coreference classification model to leverage (useful) domain knowledge together with general commonsense knowledge for the task. Experimental evaluation on realworld datasets involving five domains (product types) shows the effectiveness of the approach. 1 Introduction Coreference resolution (CR) aims to determine whether two mentions (linguistic referring expressions) corefer or not, i.e., they refer to the same entity in the discourse model (Jurafsky, 2000; Ding and Liu, 2010; Atkinson et al., 2015; Lee et al., 2017, 2018; Joshi et al., 2019; Zhang et al., 2019b). The set of coreferring expressions forms a coreference chain or a cluster. Let’s have an example: [S1] I bought a green Moonbeam for myself. [S2] I like its voice because it is loud and long. Here all colored and/or underlined phrases are mentions. Considering S1 (sentence-1) and S2 (sentence-2), the three mentions “I”, “myself ” in S1 and “I” in S2 all refer to the same person and form a cluster. Similarly, “its” in S2 refers to the object “a green Moonbeam” in S1 and the cluster is {“its” (S2), “a gree"
2020.findings-emnlp.146,doddington-etal-2004-automatic,0,0.00988959,"ized. However, these works did not deal with opinionated reviews and also did not mine or use domain-driven knowledge. In regard to CR in opinion mining, Ding and Liu (2010) formally introduced the OAC2 task for opinionated reviews, which is perhaps the only prior study on this problem. However, it only focused on classifying coreferences in comparative sentences (not on all review sentences). We compare our approach with (Ding and Liu, 2010) in Section 4. Many existing general-purpose CR datasets are not suitable for our task, which include MUC6 and MUC-7 (Hirschman and Chinchor, 1998), ACE (Doddington et al., 2004), OntoNotes (Pradhan et al., 2012), and WikiCoref (Ghaddar and Langlais, 2016). Bailey et al. (2015) proposed an alternative Turing test, comprising a binary choice CR task that requires significant commonsense knowledge. Yu et al. (2019) proposed visual pronoun coreference resolution in dialogues that require the model to incorporate image information. These datasets are also not suitable for us as they are not opinionated reviews. We do not focus on solving pronoun resolution here because, for opinion text such as reviews, discussions and blogs, personal pronouns mostly refer to one person ("
2020.findings-emnlp.146,N18-4004,0,0.0282175,"as well as the coverage of mention words. With the domain-specific knowledge mined, the meaning of a mention in a certain domain can be better understood (by a model) with the support of its relevant mentions (extracted from the self-mined KB). 4 https://github.com/jeffchen2018/ review_coref 1617 et al., 2017, 2018; Joshi et al., 2019) have dominated the coreference resolution research. But they did not use external knowledge. Conisdering CR approaches that use external knowledge, Aralikatte et al. (2019) solved CR task by incorporate knowledge or information in reinforcement learning models. Emami et al. (2018) solved the binary choice coreference-resolution task by leveraging information retrieval results from search engines. Zhang et al. (2019a,b) solved pronoun coreference resolutions by leveraging contextual, linguistic features, and external knowledge where knowledge attention was utilized. However, these works did not deal with opinionated reviews and also did not mine or use domain-driven knowledge. In regard to CR in opinion mining, Ding and Liu (2010) formally introduced the OAC2 task for opinionated reviews, which is perhaps the only prior study on this problem. However, it only focused on"
2020.findings-emnlp.146,L16-1021,0,0.0154695,"id not mine or use domain-driven knowledge. In regard to CR in opinion mining, Ding and Liu (2010) formally introduced the OAC2 task for opinionated reviews, which is perhaps the only prior study on this problem. However, it only focused on classifying coreferences in comparative sentences (not on all review sentences). We compare our approach with (Ding and Liu, 2010) in Section 4. Many existing general-purpose CR datasets are not suitable for our task, which include MUC6 and MUC-7 (Hirschman and Chinchor, 1998), ACE (Doddington et al., 2004), OntoNotes (Pradhan et al., 2012), and WikiCoref (Ghaddar and Langlais, 2016). Bailey et al. (2015) proposed an alternative Turing test, comprising a binary choice CR task that requires significant commonsense knowledge. Yu et al. (2019) proposed visual pronoun coreference resolution in dialogues that require the model to incorporate image information. These datasets are also not suitable for us as they are not opinionated reviews. We do not focus on solving pronoun resolution here because, for opinion text such as reviews, discussions and blogs, personal pronouns mostly refer to one person (Ding and Liu, 2010). Also, we aim to leverage domainspecific knowledge on (unl"
2020.findings-emnlp.146,C18-1096,0,0.0198559,"phrase or a phrase in KB). We use BERT (Devlin et al., 2019) to learn the vector representation for each span. To encode 1619 the words in a span, we use BERT’s WordPiece 1 tokenizer. Given a span x, let {xi }N i=1 be the output token embeddings of x from BERT, where N1 is the total number of word-piece tokens for span x. BERT is a neural model consisting of stacked attention layers. To incorporate the syntax-based information, we want the head of a span and words that have a modifier relation to the head to have higher attention weights. To achieve the goal, we adopt syntax-based attention (He et al., 2018). The weight of a word in a span depends on the dependency parsing result of the span. Note, the dependency parsing of a span is different from what is described in Section 3.1. The dependency parsing in Section 3.1 extracts the relation between chunks of words while here we extract relations between single words. An example has been shown in top left corner of Figure 1. The head of “a green Moonbeam” is “Moonbeam” that we want to have the highest attention weight when computing the embedding of the span. The distance of (“a”, “Moonbeam”) and (“green”, “Moonbeam”) considering the dependency pa"
2020.findings-emnlp.146,M98-1029,0,0.079684,"e where knowledge attention was utilized. However, these works did not deal with opinionated reviews and also did not mine or use domain-driven knowledge. In regard to CR in opinion mining, Ding and Liu (2010) formally introduced the OAC2 task for opinionated reviews, which is perhaps the only prior study on this problem. However, it only focused on classifying coreferences in comparative sentences (not on all review sentences). We compare our approach with (Ding and Liu, 2010) in Section 4. Many existing general-purpose CR datasets are not suitable for our task, which include MUC6 and MUC-7 (Hirschman and Chinchor, 1998), ACE (Doddington et al., 2004), OntoNotes (Pradhan et al., 2012), and WikiCoref (Ghaddar and Langlais, 2016). Bailey et al. (2015) proposed an alternative Turing test, comprising a binary choice CR task that requires significant commonsense knowledge. Yu et al. (2019) proposed visual pronoun coreference resolution in dialogues that require the model to incorporate image information. These datasets are also not suitable for us as they are not opinionated reviews. We do not focus on solving pronoun resolution here because, for opinion text such as reviews, discussions and blogs, personal pronou"
2020.findings-emnlp.146,D19-1588,0,0.149182,"wledge from unlabeled review data and trains a knowledgeaware neural coreference classification model to leverage (useful) domain knowledge together with general commonsense knowledge for the task. Experimental evaluation on realworld datasets involving five domains (product types) shows the effectiveness of the approach. 1 Introduction Coreference resolution (CR) aims to determine whether two mentions (linguistic referring expressions) corefer or not, i.e., they refer to the same entity in the discourse model (Jurafsky, 2000; Ding and Liu, 2010; Atkinson et al., 2015; Lee et al., 2017, 2018; Joshi et al., 2019; Zhang et al., 2019b). The set of coreferring expressions forms a coreference chain or a cluster. Let’s have an example: [S1] I bought a green Moonbeam for myself. [S2] I like its voice because it is loud and long. Here all colored and/or underlined phrases are mentions. Considering S1 (sentence-1) and S2 (sentence-2), the three mentions “I”, “myself ” in S1 and “I” in S2 all refer to the same person and form a cluster. Similarly, “its” in S2 refers to the object “a green Moonbeam” in S1 and the cluster is {“its” (S2), “a green Moonbeam” (S1) }. The mentions “its voice” and “it” in S2 refer t"
2020.findings-emnlp.146,D17-1018,0,0.154638,"acts domain-specific knowledge from unlabeled review data and trains a knowledgeaware neural coreference classification model to leverage (useful) domain knowledge together with general commonsense knowledge for the task. Experimental evaluation on realworld datasets involving five domains (product types) shows the effectiveness of the approach. 1 Introduction Coreference resolution (CR) aims to determine whether two mentions (linguistic referring expressions) corefer or not, i.e., they refer to the same entity in the discourse model (Jurafsky, 2000; Ding and Liu, 2010; Atkinson et al., 2015; Lee et al., 2017, 2018; Joshi et al., 2019; Zhang et al., 2019b). The set of coreferring expressions forms a coreference chain or a cluster. Let’s have an example: [S1] I bought a green Moonbeam for myself. [S2] I like its voice because it is loud and long. Here all colored and/or underlined phrases are mentions. Considering S1 (sentence-1) and S2 (sentence-2), the three mentions “I”, “myself ” in S1 and “I” in S2 all refer to the same person and form a cluster. Similarly, “its” in S2 refers to the object “a green Moonbeam” in S1 and the cluster is {“its” (S2), “a green Moonbeam” (S1) }. The mentions “its voi"
2020.findings-emnlp.146,N18-2108,0,0.0407029,"Missing"
2020.findings-emnlp.146,P19-1056,1,0.828895,"y classification decision: coreferring or not (referred to as coreference classification), and (3) grouping coreferring mentions (referring to the same discourse entity) to form a coreference chain (known as clustering). In reviews, mention detection is equiv1616 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1616–1626 c November 16 - 20, 2020. 2020 Association for Computational Linguistics alent to extracting entities and aspects in reviews which has been widely studied in opinion mining or sentiment analysis (Hu and Liu, 2004; Qiu et al., 2011; Xu et al., 2019; Luo et al., 2019; Wang et al., 2018; Dragoni et al., 2019; Asghar et al., 2019). Also, once the coreferring mentions are detected via classification, clustering them could be straightforward1 . Thus, following (Ding and Liu, 2010), we only focus on solving the coreference classification task in this work, which we refer to as the object and attribute coreference classification (OAC2) task onwards. We formulate the OAC2 problem as follows. Problem Statement. Given a review text u (context), an anaphor2 p and a mention m which refers to either an object or an attribute (including their position information), ou"
2020.findings-emnlp.146,2020.acl-main.512,1,0.824549,"Missing"
2020.findings-emnlp.146,W12-4501,0,0.0418326,"al with opinionated reviews and also did not mine or use domain-driven knowledge. In regard to CR in opinion mining, Ding and Liu (2010) formally introduced the OAC2 task for opinionated reviews, which is perhaps the only prior study on this problem. However, it only focused on classifying coreferences in comparative sentences (not on all review sentences). We compare our approach with (Ding and Liu, 2010) in Section 4. Many existing general-purpose CR datasets are not suitable for our task, which include MUC6 and MUC-7 (Hirschman and Chinchor, 1998), ACE (Doddington et al., 2004), OntoNotes (Pradhan et al., 2012), and WikiCoref (Ghaddar and Langlais, 2016). Bailey et al. (2015) proposed an alternative Turing test, comprising a binary choice CR task that requires significant commonsense knowledge. Yu et al. (2019) proposed visual pronoun coreference resolution in dialogues that require the model to incorporate image information. These datasets are also not suitable for us as they are not opinionated reviews. We do not focus on solving pronoun resolution here because, for opinion text such as reviews, discussions and blogs, personal pronouns mostly refer to one person (Ding and Liu, 2010). Also, we aim"
2020.findings-emnlp.146,J11-1002,1,0.448866,"te mentions in text, making a binary classification decision: coreferring or not (referred to as coreference classification), and (3) grouping coreferring mentions (referring to the same discourse entity) to form a coreference chain (known as clustering). In reviews, mention detection is equiv1616 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1616–1626 c November 16 - 20, 2020. 2020 Association for Computational Linguistics alent to extracting entities and aspects in reviews which has been widely studied in opinion mining or sentiment analysis (Hu and Liu, 2004; Qiu et al., 2011; Xu et al., 2019; Luo et al., 2019; Wang et al., 2018; Dragoni et al., 2019; Asghar et al., 2019). Also, once the coreferring mentions are detected via classification, clustering them could be straightforward1 . Thus, following (Ding and Liu, 2010), we only focus on solving the coreference classification task in this work, which we refer to as the object and attribute coreference classification (OAC2) task onwards. We formulate the OAC2 problem as follows. Problem Statement. Given a review text u (context), an anaphor2 p and a mention m which refers to either an object or an attribute (includ"
2020.findings-emnlp.146,D19-1462,0,0.0325584,"Missing"
2020.findings-emnlp.146,D16-1058,0,0.0987684,"Missing"
2020.findings-emnlp.146,N19-1242,1,0.881783,"Missing"
2020.findings-emnlp.146,D19-1516,0,0.0151247,"aps the only prior study on this problem. However, it only focused on classifying coreferences in comparative sentences (not on all review sentences). We compare our approach with (Ding and Liu, 2010) in Section 4. Many existing general-purpose CR datasets are not suitable for our task, which include MUC6 and MUC-7 (Hirschman and Chinchor, 1998), ACE (Doddington et al., 2004), OntoNotes (Pradhan et al., 2012), and WikiCoref (Ghaddar and Langlais, 2016). Bailey et al. (2015) proposed an alternative Turing test, comprising a binary choice CR task that requires significant commonsense knowledge. Yu et al. (2019) proposed visual pronoun coreference resolution in dialogues that require the model to incorporate image information. These datasets are also not suitable for us as they are not opinionated reviews. We do not focus on solving pronoun resolution here because, for opinion text such as reviews, discussions and blogs, personal pronouns mostly refer to one person (Ding and Liu, 2010). Also, we aim to leverage domainspecific knowledge on (unlabeled) domain-specific reviews to help the CR task which has not been studied by any of these existing CR works. 3 Proposed Approach Model Overview. Our approa"
2020.findings-emnlp.146,N19-1093,0,0.191255,"d review data and trains a knowledgeaware neural coreference classification model to leverage (useful) domain knowledge together with general commonsense knowledge for the task. Experimental evaluation on realworld datasets involving five domains (product types) shows the effectiveness of the approach. 1 Introduction Coreference resolution (CR) aims to determine whether two mentions (linguistic referring expressions) corefer or not, i.e., they refer to the same entity in the discourse model (Jurafsky, 2000; Ding and Liu, 2010; Atkinson et al., 2015; Lee et al., 2017, 2018; Joshi et al., 2019; Zhang et al., 2019b). The set of coreferring expressions forms a coreference chain or a cluster. Let’s have an example: [S1] I bought a green Moonbeam for myself. [S2] I like its voice because it is loud and long. Here all colored and/or underlined phrases are mentions. Considering S1 (sentence-1) and S2 (sentence-2), the three mentions “I”, “myself ” in S1 and “I” in S2 all refer to the same person and form a cluster. Similarly, “its” in S2 refers to the object “a green Moonbeam” in S1 and the cluster is {“its” (S2), “a green Moonbeam” (S1) }. The mentions “its voice” and “it” in S2 refer to the same attribute"
2020.findings-emnlp.146,P19-1083,0,0.332247,"d review data and trains a knowledgeaware neural coreference classification model to leverage (useful) domain knowledge together with general commonsense knowledge for the task. Experimental evaluation on realworld datasets involving five domains (product types) shows the effectiveness of the approach. 1 Introduction Coreference resolution (CR) aims to determine whether two mentions (linguistic referring expressions) corefer or not, i.e., they refer to the same entity in the discourse model (Jurafsky, 2000; Ding and Liu, 2010; Atkinson et al., 2015; Lee et al., 2017, 2018; Joshi et al., 2019; Zhang et al., 2019b). The set of coreferring expressions forms a coreference chain or a cluster. Let’s have an example: [S1] I bought a green Moonbeam for myself. [S2] I like its voice because it is loud and long. Here all colored and/or underlined phrases are mentions. Considering S1 (sentence-1) and S2 (sentence-2), the three mentions “I”, “myself ” in S1 and “I” in S2 all refer to the same person and form a cluster. Similarly, “its” in S2 refers to the object “a green Moonbeam” in S1 and the cluster is {“its” (S2), “a green Moonbeam” (S1) }. The mentions “its voice” and “it” in S2 refer to the same attribute"
2020.findings-emnlp.156,N19-1423,0,0.598592,"orpora. This helps in learning domain language models with low-resources. Experiments are conducted on an assortment of tasks in aspectbased sentiment analysis (ABSA), demonstrating promising results. 1 1 Domain Astronomy [Irrelevant Domain] Liquids [Irrelevant Domain] Concepts [Irrelevant Domain] Desktop [Relevant Domain] Laptop [Target Domain] Table 1: Multiple choices to recover a masked token (an aspect in ABSA) for different domains: a target domain needs more examples from a relevant domain. Introduction Pre-trained language models (LMs) (Peters et al., 2018; Radford et al., 2018, 2019; Devlin et al., 2019) aim to learn general (or mixed-domain) knowledge for end tasks. Recent studies (Xu et al., 2019; Gururangan et al., 2020) show that learning domain-specific LMs are equally important. This is because the training corpus of general LMs is out-of-domain for end tasks in a particular domain and, more importantly, because general LMs may not capture the long-tailed and underrepresented domain details (Xu et al., 2018). An intuitive example related to corpus of aspect-based sentiment analysis (ABSA) can be found in Table 1, where all masked words sky, water, idea, screen and picture can appear in"
2020.findings-emnlp.156,P14-2009,0,0.0169988,"ased sentiment analysis (ABSA), which typically requires a lot of domain-specific knowledge. Reviews serve as a rich resource for sentiment analysis (Pang et al., 2002; Hu and Liu, 2004; Liu, 2012, 2015). ABSA aims to turn unstructured reviews into structured fine-grained aspects (such as the “battery” or aspect category of a laptop) and their associated opinions (e.g., “good battery” is positive about the aspect battery). This paper focuses on three (3) popular tasks in ABSA: aspect extraction (AE) (Hu and Liu, 2004; Li and Lam, 2017), aspect sentiment classification (ASC) (Hu and Liu, 2004; Dong et al., 2014; Nguyen and Shirai, 2015; Li et al., 2018; Tang et al., 2016; Wang et al., 2016a,b; Ma et al., 2017; Chen et al., 2017; Ma et al., 2017; Tay et al., 2018; He et al., 2018; Liu et al., 2018) and end-to-end ABSA (E2E-ABSA) (Li et al., 2019a,b). AE aims to extract aspects (e.g., “battery”), ASC identifies the polarity for a given aspect (e.g., positive for battery) and E2E-ABSA is a combination of AE and ASC that detects the aspects and their associated polarities simultaneously. This paper focuses on self-supervised methods2 to improve ABSA. 2 DomBERT This section presents DomBERT, which is an"
2020.findings-emnlp.156,2020.acl-main.740,0,0.0591494,"Missing"
2020.findings-emnlp.156,N16-1030,0,0.154067,"Missing"
2020.findings-emnlp.156,P18-1087,0,0.177244,"Missing"
2020.findings-emnlp.156,D19-5505,0,0.035644,"Missing"
2020.findings-emnlp.156,D17-1310,0,0.0223171,"training process of DomBERT. The experiment of this paper focuses on aspectbased sentiment analysis (ABSA), which typically requires a lot of domain-specific knowledge. Reviews serve as a rich resource for sentiment analysis (Pang et al., 2002; Hu and Liu, 2004; Liu, 2012, 2015). ABSA aims to turn unstructured reviews into structured fine-grained aspects (such as the “battery” or aspect category of a laptop) and their associated opinions (e.g., “good battery” is positive about the aspect battery). This paper focuses on three (3) popular tasks in ABSA: aspect extraction (AE) (Hu and Liu, 2004; Li and Lam, 2017), aspect sentiment classification (ASC) (Hu and Liu, 2004; Dong et al., 2014; Nguyen and Shirai, 2015; Li et al., 2018; Tang et al., 2016; Wang et al., 2016a,b; Ma et al., 2017; Chen et al., 2017; Ma et al., 2017; Tay et al., 2018; He et al., 2018; Liu et al., 2018) and end-to-end ABSA (E2E-ABSA) (Li et al., 2019a,b). AE aims to extract aspects (e.g., “battery”), ASC identifies the polarity for a given aspect (e.g., positive for battery) and E2E-ABSA is a combination of AE and ASC that detects the aspects and their associated polarities simultaneously. This paper focuses on self-supervised met"
2020.findings-emnlp.156,2021.ccl-1.108,0,0.0900883,"Missing"
2020.findings-emnlp.156,P19-1056,1,0.851303,"Missing"
2020.findings-emnlp.156,P16-1101,0,0.0608519,"Missing"
2020.findings-emnlp.156,D15-1298,0,0.0593524,"Missing"
2020.findings-emnlp.156,W02-1011,0,0.024752,"an underfitting task that requires more parameters instead of avoiding overfitting. The proposed domain-oriented learning task can be viewed as one type of transfer learning(Pan and Yang, 2009), which learns a transfer strategy implicitly that transfer training examples from relevant (source) domains to the target domain. This transfer process is conducted throughout the training process of DomBERT. The experiment of this paper focuses on aspectbased sentiment analysis (ABSA), which typically requires a lot of domain-specific knowledge. Reviews serve as a rich resource for sentiment analysis (Pang et al., 2002; Hu and Liu, 2004; Liu, 2012, 2015). ABSA aims to turn unstructured reviews into structured fine-grained aspects (such as the “battery” or aspect category of a laptop) and their associated opinions (e.g., “good battery” is positive about the aspect battery). This paper focuses on three (3) popular tasks in ABSA: aspect extraction (AE) (Hu and Liu, 2004; Li and Lam, 2017), aspect sentiment classification (ASC) (Hu and Liu, 2004; Dong et al., 2014; Nguyen and Shirai, 2015; Li et al., 2018; Tang et al., 2016; Wang et al., 2016a,b; Ma et al., 2017; Chen et al., 2017; Ma et al., 2017; Tay et al.,"
2020.findings-emnlp.156,N18-1202,0,0.187438,"n from both indomain corpus and relevant domain corpora. This helps in learning domain language models with low-resources. Experiments are conducted on an assortment of tasks in aspectbased sentiment analysis (ABSA), demonstrating promising results. 1 1 Domain Astronomy [Irrelevant Domain] Liquids [Irrelevant Domain] Concepts [Irrelevant Domain] Desktop [Relevant Domain] Laptop [Target Domain] Table 1: Multiple choices to recover a masked token (an aspect in ABSA) for different domains: a target domain needs more examples from a relevant domain. Introduction Pre-trained language models (LMs) (Peters et al., 2018; Radford et al., 2018, 2019; Devlin et al., 2019) aim to learn general (or mixed-domain) knowledge for end tasks. Recent studies (Xu et al., 2019; Gururangan et al., 2020) show that learning domain-specific LMs are equally important. This is because the training corpus of general LMs is out-of-domain for end tasks in a particular domain and, more importantly, because general LMs may not capture the long-tailed and underrepresented domain details (Xu et al., 2018). An intuitive example related to corpus of aspect-based sentiment analysis (ABSA) can be found in Table 1, where all masked words s"
2020.findings-emnlp.156,D16-1021,0,0.130044,"t of domain-specific knowledge. Reviews serve as a rich resource for sentiment analysis (Pang et al., 2002; Hu and Liu, 2004; Liu, 2012, 2015). ABSA aims to turn unstructured reviews into structured fine-grained aspects (such as the “battery” or aspect category of a laptop) and their associated opinions (e.g., “good battery” is positive about the aspect battery). This paper focuses on three (3) popular tasks in ABSA: aspect extraction (AE) (Hu and Liu, 2004; Li and Lam, 2017), aspect sentiment classification (ASC) (Hu and Liu, 2004; Dong et al., 2014; Nguyen and Shirai, 2015; Li et al., 2018; Tang et al., 2016; Wang et al., 2016a,b; Ma et al., 2017; Chen et al., 2017; Ma et al., 2017; Tay et al., 2018; He et al., 2018; Liu et al., 2018) and end-to-end ABSA (E2E-ABSA) (Li et al., 2019a,b). AE aims to extract aspects (e.g., “battery”), ASC identifies the polarity for a given aspect (e.g., positive for battery) and E2E-ABSA is a combination of AE and ASC that detects the aspects and their associated polarities simultaneously. This paper focuses on self-supervised methods2 to improve ABSA. 2 DomBERT This section presents DomBERT, which is an extension of BERT for domain knowledge learning. The goal of"
2020.findings-emnlp.156,D16-1059,0,0.0168816,"c knowledge. Reviews serve as a rich resource for sentiment analysis (Pang et al., 2002; Hu and Liu, 2004; Liu, 2012, 2015). ABSA aims to turn unstructured reviews into structured fine-grained aspects (such as the “battery” or aspect category of a laptop) and their associated opinions (e.g., “good battery” is positive about the aspect battery). This paper focuses on three (3) popular tasks in ABSA: aspect extraction (AE) (Hu and Liu, 2004; Li and Lam, 2017), aspect sentiment classification (ASC) (Hu and Liu, 2004; Dong et al., 2014; Nguyen and Shirai, 2015; Li et al., 2018; Tang et al., 2016; Wang et al., 2016a,b; Ma et al., 2017; Chen et al., 2017; Ma et al., 2017; Tay et al., 2018; He et al., 2018; Liu et al., 2018) and end-to-end ABSA (E2E-ABSA) (Li et al., 2019a,b). AE aims to extract aspects (e.g., “battery”), ASC identifies the polarity for a given aspect (e.g., positive for battery) and E2E-ABSA is a combination of AE and ASC that detects the aspects and their associated polarities simultaneously. This paper focuses on self-supervised methods2 to improve ABSA. 2 DomBERT This section presents DomBERT, which is an extension of BERT for domain knowledge learning. The goal of DomBERT is to disco"
2020.findings-emnlp.156,D16-1058,0,0.0846325,"Missing"
2020.findings-emnlp.156,P18-2094,1,0.940255,"r different domains: a target domain needs more examples from a relevant domain. Introduction Pre-trained language models (LMs) (Peters et al., 2018; Radford et al., 2018, 2019; Devlin et al., 2019) aim to learn general (or mixed-domain) knowledge for end tasks. Recent studies (Xu et al., 2019; Gururangan et al., 2020) show that learning domain-specific LMs are equally important. This is because the training corpus of general LMs is out-of-domain for end tasks in a particular domain and, more importantly, because general LMs may not capture the long-tailed and underrepresented domain details (Xu et al., 2018). An intuitive example related to corpus of aspect-based sentiment analysis (ABSA) can be found in Table 1, where all masked words sky, water, idea, screen and picture can appear in a mixed-domain corpus. A generalpurpose LM may favor frequent examples and ignore long-tailed choices in certain domains. In contrast, although domain-specific LMs can capture fine-grained domain details, they may suffer from insufficient training corpus (Gururangan et al., 2020) to strengthen general knowledge within a domain. To this end, we propose a domain1 The code will be released on https://github.com/ howar"
2020.findings-emnlp.156,N19-1242,1,0.931212,"n an assortment of tasks in aspectbased sentiment analysis (ABSA), demonstrating promising results. 1 1 Domain Astronomy [Irrelevant Domain] Liquids [Irrelevant Domain] Concepts [Irrelevant Domain] Desktop [Relevant Domain] Laptop [Target Domain] Table 1: Multiple choices to recover a masked token (an aspect in ABSA) for different domains: a target domain needs more examples from a relevant domain. Introduction Pre-trained language models (LMs) (Peters et al., 2018; Radford et al., 2018, 2019; Devlin et al., 2019) aim to learn general (or mixed-domain) knowledge for end tasks. Recent studies (Xu et al., 2019; Gururangan et al., 2020) show that learning domain-specific LMs are equally important. This is because the training corpus of general LMs is out-of-domain for end tasks in a particular domain and, more importantly, because general LMs may not capture the long-tailed and underrepresented domain details (Xu et al., 2018). An intuitive example related to corpus of aspect-based sentiment analysis (ABSA) can be found in Table 1, where all masked words sky, water, idea, screen and picture can appear in a mixed-domain corpus. A generalpurpose LM may favor frequent examples and ignore long-tailed ch"
2020.findings-emnlp.156,D14-1181,0,\N,Missing
2020.findings-emnlp.156,D17-1047,0,\N,Missing
2020.findings-emnlp.156,P18-1088,1,\N,Missing
2020.findings-emnlp.156,C18-1096,0,\N,Missing
2020.findings-emnlp.156,P11-1013,0,\N,Missing
2020.findings-emnlp.339,K16-1002,0,0.0737122,"ling of content and style increases the conveyance of the generated text, while at the same time generating more natural and fluent text. We tested FVN on two datasets, PersonageNLG (Oraby et al., 2018) and E2E (Duˇsek et al., 2020) that consist of content-utterance pairs with personality labels in the first case, and the experimental results show that it outperforms previous state-ofthe-art methods. A human evaluation further confirms that the naturalness and conveyance of FVN generated text is comparable to ground truth data. 2 Related Work Our work is related to CVAE based text generation (Bowman et al., 2016; Shen et al., 2018; Zhang et al., 2019), where the goal is to control a given attribute of the output text (for example, style) by providing it as additional input to a regular VAE. For instance, the controlled text generation method proposed by Hu et al. (2017) extends VAE and focuses on controlling attributes of the generated text like sentiment and style. Differently from ours, this method does not focus on generating text from content meaning representation (CMR) or on diversity of the generated text. (Song et al., 2019) use a memory augmented CVAE to control for persona, but with no cont"
2020.findings-emnlp.339,P19-1601,0,0.0133118,"neration models range from the difficulty of generating text according to the given attributes, to the lack of diversity of the generated texts. FVN addresses these issues by learning disjoint discrete latent spaces for each attribute inside codebooks, which allows for both controllability and diversity, while at the same time generating fluent text. We evaluate FVN on two text generation datasets with annotated content and style, and show state-of-the-art performance as assessed by automatic and human evaluations. 1 Introduction Recent developments in language modeling (Radford et al., 2019; Dai et al., 2019; Radford et al., 2018; Holtzman et al., 2020; Khandelwal et al., 2020) make it possible to generate fluent and mostly coherent text. Despite the quality of the samples, regular language models cannot be conditioned to generate language depending on attributes. Conditional language models have been developed to solve this problem, with methods that either train models given predetermined attributes (Shirish Keskar et al., 2019), use conditional generative models (Kikuchi et al., 2014; Ficler and Goldberg, 2017), fine-tune models using reinforcement learning (Ziegler et al., 2019), or modify th"
2020.findings-emnlp.339,W16-3622,0,0.0449956,"Missing"
2020.findings-emnlp.339,P16-2008,0,0.0231134,"Missing"
2020.findings-emnlp.339,N18-1014,0,0.0405293,"(from (Oraby et al., 2018)) are variants of the TGEN (Novikova et al., 2017) architecture, while token-m* and context-m* are from (Harrison et al., 2019) (which adopt OpenNMT-py (Klein et al., 2017) as the basic encoder-decoder architecture). token-* baselines use a special style token to provide style information while context-* baselines use 36 human defined pragmatic and aggregation-based features to provide style information ‘-m*’ indicates variants of how the style information is injected into the encoder and the decoder. For the E2E challenge dataset, TGEN (Novikova et al., 2017), SLUG (Juraska et al., 2018), and Thomson Reuters NLG (Davoodi et al., 2018; Smiley et al., 2018) are the best performing models. They have different architectures, re-rankers, beam search and data augmentation strategies. More details are provided in Appendix B. The results of the baselines (Oraby et al., 2018; Harrison et al., 2019) are taken from their original papers, but it’s unclear if they were evaluated using a single or multiple references (for this reason they are marked with †), but since these models are not dependent on sampling from a latent space, we would not expect that to change performance. We also com"
2020.findings-emnlp.339,W18-6554,0,0.0119934,"trolled text generation method proposed by Hu et al. (2017) extends VAE and focuses on controlling attributes of the generated text like sentiment and style. Differently from ours, this method does not focus on generating text from content meaning representation (CMR) or on diversity of the generated text. (Song et al., 2019) use a memory augmented CVAE to control for persona, but with no control over the content. The works of (Oraby et al., 2018; Harrison et al., 2019; Oraby et al., 2019) on style-variation generators adopt sequence-to-sequence based models and use human-engineered features (Juraska and Walker, 2018) (e.g. personality parameters or syntax features) as extra inputs alongside the content and style to control the generation and enhance text variation. However, using human-engineered features is labor-intensive and, as it is not possible to consider all possible feature combinations, performance can be sub-optimal. In our work we instead rely on codebooks to memorize textual variations. There is a variety of works that address the problem of incorporating knowledge or structured data into the generated text (for example, entities retrieved from a knowledge base) (Ye et al., 2020), or that try"
2020.findings-emnlp.339,P14-2052,0,0.0126324,"by automatic and human evaluations. 1 Introduction Recent developments in language modeling (Radford et al., 2019; Dai et al., 2019; Radford et al., 2018; Holtzman et al., 2020; Khandelwal et al., 2020) make it possible to generate fluent and mostly coherent text. Despite the quality of the samples, regular language models cannot be conditioned to generate language depending on attributes. Conditional language models have been developed to solve this problem, with methods that either train models given predetermined attributes (Shirish Keskar et al., 2019), use conditional generative models (Kikuchi et al., 2014; Ficler and Goldberg, 2017), fine-tune models using reinforcement learning (Ziegler et al., 2019), or modify the text on the fly during generation (Dathathri et al., 2020). As many researchers noted, injecting style into natural language generation can increase the naturalness and human-likeness of text by including pragmatic markers, characteristic of oral language (Biber, 1991; Paiva and Evans, 2004; Mairesse and Walker, 2007). Text generation with ∗ Work done while at Uber AI Labs. style-variation has been explored as a special case of conditional language generation that aims to map attri"
2020.findings-emnlp.339,W18-6539,0,0.0275393,"Missing"
2020.findings-emnlp.339,P17-4012,0,0.0108317,". The content codebook is uniformed initialized in the range of [−1/K, 1/K]. We use the Adam optimizer (Kingma and Ba, 2015) with a learning rate of 0.001 for minimizing the total loss. More dataset details are shown in Appendix A Table 13 and Table 14. We compare our proposed model against the best performing models in both datasets. All of them are sequence-to-sequence based models. For PersonageNLG, TOKEN, CONTEXT (from (Oraby et al., 2018)) are variants of the TGEN (Novikova et al., 2017) architecture, while token-m* and context-m* are from (Harrison et al., 2019) (which adopt OpenNMT-py (Klein et al., 2017) as the basic encoder-decoder architecture). token-* baselines use a special style token to provide style information while context-* baselines use 36 human defined pragmatic and aggregation-based features to provide style information ‘-m*’ indicates variants of how the style information is injected into the encoder and the decoder. For the E2E challenge dataset, TGEN (Novikova et al., 2017), SLUG (Juraska et al., 2018), and Thomson Reuters NLG (Davoodi et al., 2018; Smiley et al., 2018) are the best performing models. They have different architectures, re-rankers, beam search and data augment"
2020.findings-emnlp.339,W04-1013,0,0.0242393,"ll and F1 score for “* SLOT” tokens. Model CVAE Controlled CVAE FVN-ED FVN-VQ FVN-EVQ FVN 2-gram 0.949 0.931 0.927 0.924 0.943 0.943 0.935 Table 8: Diversity Evaluation on E2E. Distinct n-grams between generated texts and ground truth. Table 3: Quality Evaluation for PersonageNLG. Model CVAE Controlled CVAE FVN-ED FVN-VQ FVN-EVQ FVN 1-gram 0.878 0.841 0.834 0.839 0.855 0.855 0.841 p *** ** *** *** *** Table 9: The analysis result of Question A - grammaticality / naturalness. cision) (Doddington, 2002), METEOR (n-grams with synonym recall) (Banerjee and Lavie, 2005), and ROUGE (n-gram recall) (Lin, 2004) scores using up to 9-grams. To evaluate content correctness, we report micro precision, recall, and F1 score of slot special tokens in the generated text, with respect to the slots in the given CMR c. To evaluate diversity, we report the distinct n-grams of groundtruth and baselines’ examples. For style evaluation, we separately train a personality classifier (with GloVe embeddings, 3 bi-directional LSTM layers, 2 feed-forward linear layers) on the PersonageNLG training data. The macro precision, recall, and F1 score of the personality classifier on the test set is 0.996. We use this classifi"
2020.findings-emnlp.339,P07-1063,0,0.0664274,"been developed to solve this problem, with methods that either train models given predetermined attributes (Shirish Keskar et al., 2019), use conditional generative models (Kikuchi et al., 2014; Ficler and Goldberg, 2017), fine-tune models using reinforcement learning (Ziegler et al., 2019), or modify the text on the fly during generation (Dathathri et al., 2020). As many researchers noted, injecting style into natural language generation can increase the naturalness and human-likeness of text by including pragmatic markers, characteristic of oral language (Biber, 1991; Paiva and Evans, 2004; Mairesse and Walker, 2007). Text generation with ∗ Work done while at Uber AI Labs. style-variation has been explored as a special case of conditional language generation that aims to map attributes such as the informational content (usually structured data representing meaning like frames with keys and values) and the style (such as personality and politeness) into one of many natural language realisations that conveys them (Novikova et al., 2016, 2017; Wang et al., 2018). As the examples in Table 1 show, for one given content frame there can be multiple realisations.When a style (a personality trait in this case) is"
2020.findings-emnlp.339,W17-5525,0,0.035044,"Missing"
2020.findings-emnlp.339,P18-2031,0,0.0140732,"ontent and style decoders to perform backward prediction tasks that better control the generator. The decoders contain two components: we first reuse the text-to-content and text-to-style encoders to encode the embedded predicted text oL and obtain latent representations z 0C and z 0S , and then we classify them to predict content c0 and style s0 , as shown in the right side of Figure 1: z 0C = EncTC (oL ) and z 0S = EncTS (oL ). EncTC (·) and EncTS (·) denote the same text-to-content and text-to-style encoders we defined previously. This design is inspired by work on text style transfer (dos Santos et al., 2018). Both z 0 vectors and e vectors are used by two classification heads F C (multi-label) and F S (multi-class) for predicting content and style respectively in order to force those vectors to encode attribute information. We use g to denote the g-th element in the set of possible key-value pairs in the CMR and m(·) to represent an indicator function that returns whether the g-th element is in the ground-truth CMR.   P yzC (g) = m(g) = F C (z 0C ), P (yzS S P (yeS S 0S = s) = F (z ),   P yeC (g) = m(g) = F C (eC k ), = s) = F (eS n ). (7) (8) (9) (10) The loss for training the two prediction"
2020.findings-emnlp.339,W16-6644,0,0.0126341,"e generation can increase the naturalness and human-likeness of text by including pragmatic markers, characteristic of oral language (Biber, 1991; Paiva and Evans, 2004; Mairesse and Walker, 2007). Text generation with ∗ Work done while at Uber AI Labs. style-variation has been explored as a special case of conditional language generation that aims to map attributes such as the informational content (usually structured data representing meaning like frames with keys and values) and the style (such as personality and politeness) into one of many natural language realisations that conveys them (Novikova et al., 2016, 2017; Wang et al., 2018). As the examples in Table 1 show, for one given content frame there can be multiple realisations.When a style (a personality trait in this case) is injected, the text is adapted to that style (words in red) while conveying the correct informational content (words in blue). A key challenge is to generate text that respects the specified attributes while at the same time generating diverse outputs, as most existing methods fail to correctly generate text according to given attributes or exhibit a lack of diversity among different samples, leading to dull and repetitive"
2020.findings-emnlp.339,P19-1596,0,0.0177786,"attribute of the output text (for example, style) by providing it as additional input to a regular VAE. For instance, the controlled text generation method proposed by Hu et al. (2017) extends VAE and focuses on controlling attributes of the generated text like sentiment and style. Differently from ours, this method does not focus on generating text from content meaning representation (CMR) or on diversity of the generated text. (Song et al., 2019) use a memory augmented CVAE to control for persona, but with no control over the content. The works of (Oraby et al., 2018; Harrison et al., 2019; Oraby et al., 2019) on style-variation generators adopt sequence-to-sequence based models and use human-engineered features (Juraska and Walker, 2018) (e.g. personality parameters or syntax features) as extra inputs alongside the content and style to control the generation and enhance text variation. However, using human-engineered features is labor-intensive and, as it is not possible to consider all possible feature combinations, performance can be sub-optimal. In our work we instead rely on codebooks to memorize textual variations. There is a variety of works that address the problem of incorporating knowledg"
2020.findings-emnlp.339,W18-5019,0,0.0830527,"es disjoint latent space distributions that are conditional on the content and style respectively, which allows to sample latent representations in a focused way at prediction time. This choice ultimately helps both attribute conveyance and variability. As a result, FVN can preserve the diversity found in training examples as opposed to previous methods that tend to cancel out diverse examples. FVN’s disjoint modeling of content and style increases the conveyance of the generated text, while at the same time generating more natural and fluent text. We tested FVN on two datasets, PersonageNLG (Oraby et al., 2018) and E2E (Duˇsek et al., 2020) that consist of content-utterance pairs with personality labels in the first case, and the experimental results show that it outperforms previous state-ofthe-art methods. A human evaluation further confirms that the naturalness and conveyance of FVN generated text is comparable to ground truth data. 2 Related Work Our work is related to CVAE based text generation (Bowman et al., 2016; Shen et al., 2018; Zhang et al., 2019), where the goal is to control a given attribute of the output text (for example, style) by providing it as additional input to a regular VAE."
2020.findings-emnlp.339,P02-1040,0,0.106391,"All VAEs and FVN variants are evaluated using multiple references because the sampling from latent space may lead to generate a valid and fluent text that n-gram overlap metrics would not score high when evaluated against a single reference. 4.2 Automatic Evaluation We evaluate the quality and diversity of the generated text on both dataset. PersonageNLG is styleannotated and delexicalized, so we also report style and content correctness for it. To evaluate quality in the generated text, we use the automatic evaluation from the E2E generation challenge, which reports BLEU (n-gram precision) (Papineni et al., 2002), NIST (weighted n-gram pre3810 Model TOKEN† CONTEXT† token-m1† token-m2† token-m3† context-m1† context-m2† context-m3† CVAE Controlled CVAE FVN-ED FVN-VQ FVN-EVQ FVN BLEU 0.3464 0.3766 0.4904 0.4810 0.4906 0.5530 0.5229 0.5598 0.9 0.928 0.802 0.887 0.94 0.965 NIST 4.9285 5.3437 9.766 9.957 7.872 8.985 10.129 9.946 METEOR 0.3648 0.3964 0.449 0.463 0.378 0.423 0.476 0.486 ROUGE-L 0.5016 0.5255 0.702 0.721 0.696 0.715 0.748 0.768 Model ground truth CVAE Controlled CVAE FVN-ED FVN-VQ FVN-EVQ FVN Precision 0.961 0.961 0.997 0.87 0.963 0.987 Recall 0.942 0.969 0.748 0.799 0.989 1.0 Precision 0.973"
2020.findings-emnlp.339,D14-1162,0,0.0820616,"Missing"
2020.sigdial-1.30,W17-5526,0,0.0479654,"Missing"
2020.sigdial-1.30,D18-1547,0,0.0324937,"ty data collected via Fitbit cannot be shared, since consent did not include permission for such data; dataset 1 cannot be shared, because of lack of consent. tracking, dialogue act prediction, and response generation; labeled datasets for each of these tasks were provided (Williams et al., 2013). However, most of these datasets focused on traveling and restaurant booking domains (Henderson et al., 2014). Moreover, for data collection, predefined scenarios are given to the users and thus, the users’ responses are not as spontaneous as they would be in a real-life situation (Asri et al., 2017; Budzianowski et al., 2018). Unfortunately, there are no such publicly available datasets for dialogue systems in the health domain. review tasks, assess, counseling, assign task, preclosing, and closing. Conversely, our stages-phases schema looks at the fine-grained decomposition of review-tasks, counseling, and assign task, which Bickmore et al. (2011) did not do. As far as we know, no other work models HC dialogues collected in a SMART goal setting, focusing on slotvalues and higher-level conversation flow. 3 Based on the domain, practitioners modify the definition of SMART components to fit the task at hand. For phy"
2020.sigdial-1.30,D08-1100,0,0.0268695,"o this collaborative negotiation setting over multiple days in our corpus, goal information is spread throughout the dialogue. Motivated by these complexities, we decided to annotate our data for two types of information: (1) the SMART goal attributes in the dialogues to track patients’ goals, and (2) different stages and phases that model the conversation flow in HC dialogues. For our domain, SMART goal attributes are the slotvalues pertaining to a patient’s goal. Stages and phases are more abstract, but otherwise analogous to tasks and sub-tasks as defined in task-oriented dialogue systems (Chotimongkol and Rudnicky, 2008). We believe the SMART annotation schema that we designed can be applied to any task where SMART goal setting is being used and not just physical activity. Similarly, the stages-phases annotation schema can be used to model the flow of any collaborative decision making counseling dialogue. In this paper, we will discuss the two rounds of data collection process, the subsequent analysis of the dialogues, which includes developing schemas and annotating the data, and application of models trained on these annotations. Our contributions can be summarized as follows: • We describe the data collect"
2020.sigdial-1.30,L18-1631,0,0.166677,"viewing (MI) based counseling interviews from public sources such as YouTube and built models to predict the overall counseling quality using linguistic features. Before the YouTube data, the authors also worked on data collected in clinical settings, graduate student training and such, but didn’t release it due to privacy reasons (P´erez-Rosas et al., 2016). The authors used the well established Motivational Interviewing Treatment Integrity (MITI) coding system to annotate the data and score how well or poorly a clinician used MI (Moyers et al., 2016). The MITI coding system was also used by Guntakandla and Nielsen (2018) to annotate reflections in the health behavior change therapy conversations. Since MI based interventions focus on understanding patient’s attitudes towards the problem and persuading them to change, the MITI coding system supports assessing clinicians based on how well they bring forth patient’s experiences, cultivate change talk, provide education, persuade them through logical arguments, and such. However, specific goal setting is not the main focus of these interviews and is rarely discussed. SMART Goal Setting • Specific (S): Create a clear goal that is as specific as possible and focuse"
2020.sigdial-1.30,W14-4337,0,0.026391,"s such as the Dialogue State Tracking Challenge (DSTC) started in 2013 to provide a common testbed for different tasks related to domainspecific dialogue systems such as dialogue state 247 2 Unfortunately, the activity data collected via Fitbit cannot be shared, since consent did not include permission for such data; dataset 1 cannot be shared, because of lack of consent. tracking, dialogue act prediction, and response generation; labeled datasets for each of these tasks were provided (Williams et al., 2013). However, most of these datasets focused on traveling and restaurant booking domains (Henderson et al., 2014). Moreover, for data collection, predefined scenarios are given to the users and thus, the users’ responses are not as spontaneous as they would be in a real-life situation (Asri et al., 2017; Budzianowski et al., 2018). Unfortunately, there are no such publicly available datasets for dialogue systems in the health domain. review tasks, assess, counseling, assign task, preclosing, and closing. Conversely, our stages-phases schema looks at the fine-grained decomposition of review-tasks, counseling, and assign task, which Bickmore et al. (2011) did not do. As far as we know, no other work models"
2020.sigdial-1.30,W16-0305,0,0.135042,"Missing"
2020.sigdial-1.30,L18-1591,0,0.0626665,"Missing"
2020.sigdial-1.30,N18-1202,0,0.0296177,"Missing"
2020.sigdial-1.30,W13-4065,0,0.0183773,"range of actions that are found in human-human or humanmachine conversations in the given domain. Initiatives such as the Dialogue State Tracking Challenge (DSTC) started in 2013 to provide a common testbed for different tasks related to domainspecific dialogue systems such as dialogue state 247 2 Unfortunately, the activity data collected via Fitbit cannot be shared, since consent did not include permission for such data; dataset 1 cannot be shared, because of lack of consent. tracking, dialogue act prediction, and response generation; labeled datasets for each of these tasks were provided (Williams et al., 2013). However, most of these datasets focused on traveling and restaurant booking domains (Henderson et al., 2014). Moreover, for data collection, predefined scenarios are given to the users and thus, the users’ responses are not as spontaneous as they would be in a real-life situation (Asri et al., 2017; Budzianowski et al., 2018). Unfortunately, there are no such publicly available datasets for dialogue systems in the health domain. review tasks, assess, counseling, assign task, preclosing, and closing. Conversely, our stages-phases schema looks at the fine-grained decomposition of review-tasks,"
2021.acl-long.388,P19-2045,0,0.123096,"P) task. In the real world, the text classification is usually cast as a hierarchical text classification (HTC) problem, such as patent collection (Tikk et al., 2005), web content collection (Dumais and Chen, 2000) and medical record coding (Cao et al., 2020). In these scenarios, the HTC task aims to categorize a textual description within a set of labels that are organized in a structured class hierarchy (Silla and Freitas, 2011). Lots of researchers devote their effort to investigate this challenging problem. They have proposed various HTC solutions, which are usually categorized into flat (Aly et al., 2019), local (Xu and Geng, 2019), global (Qiu et al., 2011) and combined approaches (Wehrmann et al., 2018). In most of the previous HTC work, researchers mainly focus on modeling the text, the labels are simply represented as one-hot vectors (Zhu and Bain, 2017; Wehrmann et al., 2018). Actually, the one-hot vectors act as IDs without any semantic information. How to describe a class is also worthy of discussion. There is some work that embeds labels into a vector space which contains more semantic information. Compared with one-hot representations, label embeddings have advantages in capturing dom"
2021.acl-long.388,P19-1633,0,0.019785,"abel embeddings from the HTC task and present it in the above paragraph. Besides, existing work is usually categorized into flat, local and global approaches (Silla and Freitas, 2011). The flat classification approach completely ignores the class hierarchy and only predicts classes at the leaf nodes (Aly et al., 2019). The local classification approaches could be grouped as a local classifier per node (LCN), a local classifier per parent node (LCPN) and a local classifier per level (LCL). The LCN approach train one binary classifier for each node of the hierarchy (Fagni and Sebastiani, 2007). Banerjee et al. (2019) apply transfer learning in LCN by fine-tuning the parent classifier for the child class. For the LCPN, a multiclass classifier for each parent node is trained to distinguish between its child nodes (Wu et al., 2005; Dumais and Chen, 2000). Xu and Geng (2019) investigate the correlation among labels by the label distribution as an LCPN approach. The LCL approach consists of training one multi-class classifier for each class level (Kowsari et al., 2017; Shimura et al., 2018). Zhu and Bain (2017) introduce a BCNN model which outputs predictions corresponding to the hierarchical structure. Chen e"
2021.acl-long.388,2020.acl-main.282,0,0.519058,"widely used datasets prove that the proposed model outperforms several state-of-theart methods. We release our complementary resources (concepts and definitions of classes) for these two datasets to benefit the research on HTC. 1 Figure 1: Concepts shared among classes in WOS. Introduction Text classification is a classical Natural Language Processing (NLP) task. In the real world, the text classification is usually cast as a hierarchical text classification (HTC) problem, such as patent collection (Tikk et al., 2005), web content collection (Dumais and Chen, 2000) and medical record coding (Cao et al., 2020). In these scenarios, the HTC task aims to categorize a textual description within a set of labels that are organized in a structured class hierarchy (Silla and Freitas, 2011). Lots of researchers devote their effort to investigate this challenging problem. They have proposed various HTC solutions, which are usually categorized into flat (Aly et al., 2019), local (Xu and Geng, 2019), global (Qiu et al., 2011) and combined approaches (Wehrmann et al., 2018). In most of the previous HTC work, researchers mainly focus on modeling the text, the labels are simply represented as one-hot vectors (Zhu"
2021.acl-long.388,2020.acl-main.749,0,0.0571166,"Missing"
2021.acl-long.388,E17-2119,0,0.0581949,"Missing"
2021.acl-long.388,D19-1042,0,0.0203166,"concepts “optimization” and “design”. “Network security” is surrounded by “cloud”, “machine” and “security”. The class is described by several concepts in different views. The visualizations in Figure 3 and 4 indicate that we successfully model the concept sharing mechanism in a semantic and explicit way. 4 Related Work Hierarchical text classification with label embeddings Recently, researchers try to adopt the label embeddings in the hierarchical text classification task. Huang et al. (2019) propose hierarchical attention-based recurrent neural network (HARNN) by adopting label embeddings. Mao et al. (2019) propose to learn a label assignment policy via deep reinforcement learning with label embeddings. Peng et al. (2019) propose hierarchical taxonomy-aware and attentional graph RCNNs with label embeddings. Rivas Rojas et al. (2020) 5016 Figure 4: t-SNE plot of the concept embeddings of the class “Computer Science” and the concept-based label embeddings of its child classes. Figure 3: Dynamic routing scores between the concepts of class “Computer Science” (Y-axis) and its child classes (X-axis). define the HTC task as a sequence-to-sequence problem. Their label embedding is defined by external k"
2021.acl-long.388,D14-1162,0,0.0873061,"Missing"
2021.acl-long.388,P11-2105,0,0.104058,"usually cast as a hierarchical text classification (HTC) problem, such as patent collection (Tikk et al., 2005), web content collection (Dumais and Chen, 2000) and medical record coding (Cao et al., 2020). In these scenarios, the HTC task aims to categorize a textual description within a set of labels that are organized in a structured class hierarchy (Silla and Freitas, 2011). Lots of researchers devote their effort to investigate this challenging problem. They have proposed various HTC solutions, which are usually categorized into flat (Aly et al., 2019), local (Xu and Geng, 2019), global (Qiu et al., 2011) and combined approaches (Wehrmann et al., 2018). In most of the previous HTC work, researchers mainly focus on modeling the text, the labels are simply represented as one-hot vectors (Zhu and Bain, 2017; Wehrmann et al., 2018). Actually, the one-hot vectors act as IDs without any semantic information. How to describe a class is also worthy of discussion. There is some work that embeds labels into a vector space which contains more semantic information. Compared with one-hot representations, label embeddings have advantages in capturing domain-specific information and importing external knowle"
2021.acl-long.388,2020.acl-main.205,0,0.0694232,"vector space which contains more semantic information. Compared with one-hot representations, label embeddings have advantages in capturing domain-specific information and importing external knowledge. In the field of text classification (includes the HTC task), researchers propose several forms of label embeddings to encode different kinds of information, such as 1) anchor points (Du et al., 2019), 2) compatibility between labels and words (Wang et al., 2018; Huang et al., 2019; Tang et al., 2015), 3) taxonomic hierarchy (Cao et al., 2020; Zhou et al., 2020) and 4) external knowledge (Rivas Rojas et al., 2020). Although the external knowledge has been proven effective for HTC, it comes from a dictionary or knowledge base that humans constructed for entity definition, and it doesn’t focus on the class explanations of a certain HTC task. In this sense, external knowledge is a type of domainindependent information. The taxonomic hierarchy encoding can capture the structural information of classes, which is a sort of domain-specific information for HTC. However, actually it only models the hypernym-hyponym relations in the class hierarchy. The process is implicit and difficult to 5010 Proceedings of th"
2021.acl-long.388,D18-1093,0,0.0512478,"Missing"
2021.acl-long.388,D18-1094,0,0.208932,"ork. We summarize several kinds of existing label embeddings and propose a novel label representation: concept-based label embedding. • We propose a hierarchical network to extract the concepts and model the sharing process via a modified dynamic routing algorithm. To our best knowledge, this is the first work that explores the concepts of the HTC problem in an explicit and interpretable way. • The experimental results on two widely used datasets empirically demonstrate the effective performance of the proposed model. • We complement the public datasets WOS (Kowsari et al., 2017) and DBpedia (Sinha et al., 2018) by exacting the hierarchy concept and annotating the classes with the definitions from Wikipedia. We release these complementary resources and the code of the proposed model for further use by the community1 . 2 Model In this section, we detailedly introduce our model CLED (Figure 2). It is designed for hierarchical text classification with Concept-based Label 1 https://github.com/wxpkanon/ CLEDforHTC.git 5011 Figure 2: Illustration of our Concept-based Label Embedding via Dynamic routing (CLED) for HTC. Embeddings via a modified Dynamic routing mechanism. Firstly, we construct a hierarchical"
2021.acl-long.388,P18-1216,0,0.108959,"t vectors act as IDs without any semantic information. How to describe a class is also worthy of discussion. There is some work that embeds labels into a vector space which contains more semantic information. Compared with one-hot representations, label embeddings have advantages in capturing domain-specific information and importing external knowledge. In the field of text classification (includes the HTC task), researchers propose several forms of label embeddings to encode different kinds of information, such as 1) anchor points (Du et al., 2019), 2) compatibility between labels and words (Wang et al., 2018; Huang et al., 2019; Tang et al., 2015), 3) taxonomic hierarchy (Cao et al., 2020; Zhou et al., 2020) and 4) external knowledge (Rivas Rojas et al., 2020). Although the external knowledge has been proven effective for HTC, it comes from a dictionary or knowledge base that humans constructed for entity definition, and it doesn’t focus on the class explanations of a certain HTC task. In this sense, external knowledge is a type of domainindependent information. The taxonomic hierarchy encoding can capture the structural information of classes, which is a sort of domain-specific information for H"
2021.acl-long.388,D14-1039,0,0.0324083,"n, 2000). Xu and Geng (2019) investigate the correlation among labels by the label distribution as an LCPN approach. The LCL approach consists of training one multi-class classifier for each class level (Kowsari et al., 2017; Shimura et al., 2018). Zhu and Bain (2017) introduce a BCNN model which outputs predictions corresponding to the hierarchical structure. Chen et al. (2020b) propose a multi-level learning to rank model with multi-level hinge loss margins. The global approach learns a global classification model about the whole class hierarchy (Cai and Hofmann, 2004; Gopal and Yang, 2013; Wing and Baldridge, 2014; Karn et al., 2017). Qiu et al. (2011) exploit the latent nodes in the taxonomic hierarchy with a global approach. For the need for a large amount of training data, a weakly-supervised global HTC method is proposed by Meng et al. (2019). Meta-learning is adopted by Wu et al. (2019) for HTC in a global way. In addition, there is some work combined with both local and global approach (Wehrmann et al., 2018). A local flat tree classifier is introduced by Peng et al. (2018) which utilizes the graph-CNN. 5 Conclusion In this paper, we investigate the concept which is a kind of domain-specific and"
2021.acl-long.388,D19-1444,0,0.027996,"which outputs predictions corresponding to the hierarchical structure. Chen et al. (2020b) propose a multi-level learning to rank model with multi-level hinge loss margins. The global approach learns a global classification model about the whole class hierarchy (Cai and Hofmann, 2004; Gopal and Yang, 2013; Wing and Baldridge, 2014; Karn et al., 2017). Qiu et al. (2011) exploit the latent nodes in the taxonomic hierarchy with a global approach. For the need for a large amount of training data, a weakly-supervised global HTC method is proposed by Meng et al. (2019). Meta-learning is adopted by Wu et al. (2019) for HTC in a global way. In addition, there is some work combined with both local and global approach (Wehrmann et al., 2018). A local flat tree classifier is introduced by Peng et al. (2018) which utilizes the graph-CNN. 5 Conclusion In this paper, we investigate the concept which is a kind of domain-specific and fine-grained information for the hierarchical text classification. We propose a novel concept-based label embedding model. Compared with several competitive stateof-the-art methods, the experimental results on two widely used datasets prove the effectiveness of our proposed model. T"
2021.eacl-main.95,N16-1014,0,0.284479,"neural language generation (NLG). In particular, we focus on the opendomain dialogue response task, for the following reasons: (1) There is high similarity between the target dialogue response task (conditional NLG) and the pretraining language modeling (LM) objective, so we expect that language generation skills learnt during pretraining can be well transferred to the down-stream target task. (2) The sequence-tosequence (seq2seq) nature of the model allows us to characterize the model’s generation behavior in various ways (e.g., context sensitivity). End-to-end dialogue response generation (Li et al., 2016) can be formulated as a sequence-tosequence (seq2seq) task: Given a dialogue context (previous utterances), the model is asked to generate a high-quality response. In this work we adopt the encoder-decoder model architecture (Sutskever et al., 2014; Cho et al., 2014), which is widely used in NLG applications like dialogue response generation (Li et al., 2016), machine translation (Luong et al., 2015), etc. In particular, we use the transformer model (Vaswani et al., 2017), which has currently become the most popular encoderdecoder model architecture (Young et al., 2017). We use the same config"
2021.eacl-main.95,I17-1099,0,0.0177841,"large-scale CCNEWS data (Bakhtin et al., 2019) which is a de-duplicated subset of the English portion of the CommonCrawl news dataset1 . The dataset contains news articles published worldwide between September 2016 and February 2019. It has in total around 1 billion sentences or 27 billion words. To be able to complete experiments in a reasonable amount of time, we use the first 10 percent of the CCNEWS data for pretraining, which contains 100 million sentences and 2.7 billion words. For finetuning, three open-domain conversational dialogue datasets are used: Dailydialog (1.3 million words) (Li et al., 2017), Switchboard (1.2 million words), and Cornell Movie (Danescu-Niculescu-Mizil and Lee, 2011) (4.5 million words). To save space, we defer the details of the data-sets to Appendix B. To construct the vocabulary, we learn codes of Byte Pair Encoding (BPE) (Sennrich et al., 2016) from the CCNEWS-100m data with 50k merges. This results in a vocabulary of size 62k. We then apply the same BPE codes to all target dialogue datasets. 4.2 Implementation Our code is based on the Fairseq toolkit (Ott et al., 2019). The Adam optimizer (Kingma and Ba, 2014) is used for all experiments. For pretraining of bo"
2021.eacl-main.95,P18-1138,0,0.0213915,"ledge-grounded datasets such as Topical-chat (Gopalakrishnan et al., 2019). 7 Related Works Behavior of pretrained NLG Models Recently, multiple works (Radford et al., 2019; Jiang et al., 2020; Roberts et al., 2020; Talmor et al., 2019; Trinh and Le, 2019) have reported that pre-trained language models (LM) have implicitly stored large amounts of “world knowledge” in its parameters, and are able to answer common-sense questions. However, whether the world knowledge is well preserved after finetuning on target task dataset is not discussed. 1128 On the other hand, knowledge-grounded NLG model (Liu et al., 2018; Guu et al., 2020; Zhou et al., 2018) has been an important and exciting research topic. These studies usually involve additional retrieval modules or external knowledge bases to provide the model with relevant information. In contrast to these works, we study whether the model can conduct knowledgeable dialogues by itself. Forgetting As discussed in Section 3.2, in contrast to the “catastrophic forgetting” problem in sequential learning (Atkinson et al., 2018; Robins, 1995), the performance drop on pretraining data is not necessarily bad for the NLP pretrain-finetune framework, and its impli"
2021.eacl-main.95,2021.ccl-1.108,0,0.063944,"Missing"
2021.eacl-main.95,D15-1166,0,0.0459636,") The sequence-tosequence (seq2seq) nature of the model allows us to characterize the model’s generation behavior in various ways (e.g., context sensitivity). End-to-end dialogue response generation (Li et al., 2016) can be formulated as a sequence-tosequence (seq2seq) task: Given a dialogue context (previous utterances), the model is asked to generate a high-quality response. In this work we adopt the encoder-decoder model architecture (Sutskever et al., 2014; Cho et al., 2014), which is widely used in NLG applications like dialogue response generation (Li et al., 2016), machine translation (Luong et al., 2015), etc. In particular, we use the transformer model (Vaswani et al., 2017), which has currently become the most popular encoderdecoder model architecture (Young et al., 2017). We use the same configuration as (Vaswani et al., 2017), which has 6 encoder/decoder layers, 16 attention heads, with an embedding dimension of 1024 and a feed-forward dimension of 4096. During standard finetuning, the Adam optimizer (Kingma and Ba, 2014) is used to minimize the negative log-likelihood (NLL) of the reference target sentence y given the input context x in the data distribution (denoted as Pdata ): Lfinetun"
2021.eacl-main.95,N19-4009,1,0.840787,"three open-domain conversational dialogue datasets are used: Dailydialog (1.3 million words) (Li et al., 2017), Switchboard (1.2 million words), and Cornell Movie (Danescu-Niculescu-Mizil and Lee, 2011) (4.5 million words). To save space, we defer the details of the data-sets to Appendix B. To construct the vocabulary, we learn codes of Byte Pair Encoding (BPE) (Sennrich et al., 2016) from the CCNEWS-100m data with 50k merges. This results in a vocabulary of size 62k. We then apply the same BPE codes to all target dialogue datasets. 4.2 Implementation Our code is based on the Fairseq toolkit (Ott et al., 2019). The Adam optimizer (Kingma and Ba, 2014) is used for all experiments. For pretraining of both MASS and NS, we use a mini-batch size of 2048, with the learning rate (LR) set to 0.0001. Following (Vaswani et al., 2017), the “inverse square root” LR scheduler with a warm-up stage is used. Pretraining is conducted on 32 GPUs and half-precision (float16) speed-up is used. For both MASS and NS, we stop the pretraining after the CCNEWS data is swept 20 times. For all our experiments, a dropout rate of 0.1 is applied to the transformer model. We follow Song et al. (2019) for the recommended hyper-pa"
2021.eacl-main.95,N18-1202,0,0.214203,"from the perspectives of knowledge transfer, context sensitivity, and function space projection. As a preliminary attempt to alleviate the forgetting problem, we propose an intuitive finetuning strategy named “mix-review”. We find that mix-review effectively regularizes the finetuning process, and the forgetting problem is alleviated to some extent. Finally, we discuss interesting behavior of the resulting dialogue model and its implications. 1 Figure 1: During finetuning, the model’s performance on the pretraining data drastically degrades. Introduction Large-scale unsupervised pretraining (Peters et al., 2018; Devlin et al., 2018; Song et al., 2019; Yang et al., 2019; Liu et al., 2019) has recently been shown to greatly boost the performance of natural language processing (NLP) models. On a high level, the pretrain-finetune framework can be viewed as a simple two-stage procedure: (1) Use large-scale unsupervised text data to pretrain the model; (2) Use target task data to finetune the model. Recently, multiple works (Radford et al., 2019; Jiang et al., 2020; Roberts et al., 2020; Talmor et al., 2019) have reported that pretrained language models (LM) have implicitly stored large amounts of “world"
2021.eacl-main.95,P19-1004,0,0.0212955,"e dialogue responses more informative and engaging (e.g., the model can learn about the “Avengers” movie, and use it as a topic). To quantify how knowledgeable the finetuned model is, we prepare a set of knowledge terms such as iphone, pokemon, etc., and the corresponding reference description. We then query the model about these knowledge terms, and compare its output against the reference. We also conduct multi-turn human evaluation in the setting of knowledgeable conversations. More details will be given in Section 5.1. The other ability is the utilization of contextual input: as shown by (Sankar et al., 2019), the current open-domain dialogue models (without pretraining) are insensitive to contextual input, which gives rise to the generic response problem (Li et al., 2016). In our preliminary experiments with NS pretraining, we find that similarly to the GPT model (Radford et al., 2019) the pretrained model has the ability to generate closely related responses given the previous sentences as input. Ideally during finetuning, the model can transfer this skill to the target dialogue task. To quantify the model’s sensitivity to context, following (Sankar et al., 2019), we add noise to the input, and"
2021.eacl-main.95,P16-1162,0,0.0261507,"tences or 27 billion words. To be able to complete experiments in a reasonable amount of time, we use the first 10 percent of the CCNEWS data for pretraining, which contains 100 million sentences and 2.7 billion words. For finetuning, three open-domain conversational dialogue datasets are used: Dailydialog (1.3 million words) (Li et al., 2017), Switchboard (1.2 million words), and Cornell Movie (Danescu-Niculescu-Mizil and Lee, 2011) (4.5 million words). To save space, we defer the details of the data-sets to Appendix B. To construct the vocabulary, we learn codes of Byte Pair Encoding (BPE) (Sennrich et al., 2016) from the CCNEWS-100m data with 50k merges. This results in a vocabulary of size 62k. We then apply the same BPE codes to all target dialogue datasets. 4.2 Implementation Our code is based on the Fairseq toolkit (Ott et al., 2019). The Adam optimizer (Kingma and Ba, 2014) is used for all experiments. For pretraining of both MASS and NS, we use a mini-batch size of 2048, with the learning rate (LR) set to 0.0001. Following (Vaswani et al., 2017), the “inverse square root” LR scheduler with a warm-up stage is used. Pretraining is conducted on 32 GPUs and half-precision (float16) speed-up is used"
2021.emnlp-main.270,P19-1140,0,0.0182733,"dge America The Trump Organization PresidentOf D.J. Trump OwnerOf Trump Tower OwnerOf Wealth 2.5 Billion USD Business knowledge Figure 1: An example of Entity Alignment. step for KG fusion: it identifies equivalent entities across different KGs, supporting the unification of their complementary knowledge. For example, in Fig. 1 Donald Trump and US in the first KG correspond to D.J. Trump and America respectively in the second KG. By aligning them, the political and business knowledge about Donald Trump can be integrated within one KG. Neural models (Chen et al., 2017, 2018; Wang et al., 2018; Cao et al., 2019) are the current stateof-the-art in EA and are capable of matching en(2) How to recognise entities that appear in tities in an end-to-end manner. Typically, these one KG but not in the other KG (i.e., bachelneural EA models rely on a seed alignment as trainors). Identifying bachelors would likely save annotation budget. To address this challenge, ing data which is very labour-intensive to annotate. we devise a bachelor recognizer paying attenHowever, previous EA research has assumed the tion to alleviate the effect of sampling bias. availability of such seed alignment and ignored the Empirical"
2021.emnlp-main.270,2020.emnlp-main.515,0,0.0599234,"Missing"
2021.emnlp-main.270,D18-1032,0,0.0275545,"ty Political knowledge America The Trump Organization PresidentOf D.J. Trump OwnerOf Trump Tower OwnerOf Wealth 2.5 Billion USD Business knowledge Figure 1: An example of Entity Alignment. step for KG fusion: it identifies equivalent entities across different KGs, supporting the unification of their complementary knowledge. For example, in Fig. 1 Donald Trump and US in the first KG correspond to D.J. Trump and America respectively in the second KG. By aligning them, the political and business knowledge about Donald Trump can be integrated within one KG. Neural models (Chen et al., 2017, 2018; Wang et al., 2018; Cao et al., 2019) are the current stateof-the-art in EA and are capable of matching en(2) How to recognise entities that appear in tities in an end-to-end manner. Typically, these one KG but not in the other KG (i.e., bachelneural EA models rely on a seed alignment as trainors). Identifying bachelors would likely save annotation budget. To address this challenge, ing data which is very labour-intensive to annotate. we devise a bachelor recognizer paying attenHowever, previous EA research has assumed the tion to alleviate the effect of sampling bias. availability of such seed alignment and ig"
2021.emnlp-main.550,2021.naacl-main.378,1,0.880874,"ntinual learning (CL) learns a sequence of tasks objective is to transfer knowledge across tasks to incrementally. After learning a task, its training improve classification compared to learning each data is often discarded (Chen and Liu, 2018). The task separately. An important goal of any CL is to CL setting is useful when the data privacy is a conovercome catastrophic forgetting (CF) (McCloskey cern, i.e., the data owners do not want their data and Cohen, 1989), which means that in learning a used by others (Ke et al., 2020b; Qin et al., 2020; new task, the system may change the parameters Ke et al., 2021). In such cases, if we want to leverlearned for previous tasks and cause their perforage the knowledge learned in the past to improve mance to degrade. We solve the CF problem as the new task learning, CL is appropriate as it shares well; otherwise we cannot achieve improved accuonly the learned model, but not the data. In our racy. However, sharing the classification head for case, a task is a separate aspect sentiment classifiall tasks in DIL makes cross-task interfere/update cation (ASC) problem of a product or domain (e.g., inevitable. Without task information provided in camera or phone)"
2021.emnlp-main.550,D14-1181,0,0.00574848,"3e-5. and SRK are for document sentiment classificaFor the SemEval datasets, 10 epochs are used and tion. We use the concatenation of the aspect and for all other datasets, 30 epochs are used based the sentence as input. HAT, UCL, EWC, OWM on results from validation data. All runs use the and DER++ were originally designed for image classification. We replace their original image clas- batch size 32. For CL baselines, we train all models with the learning rate of 0.05, early-stop training sification networks with CNN for text classification when there is no improvement in the validation loss (Kim, 2014). HAT is one of the best TIL methods for 5 epochs and set the batch size to 64. We use with almost no forgetting. UCL is a recent TIL the code provided by their authors and adopt their method. EWC is a popular CIL method, which was original parameters (for EWC, we adopt the variant adapted for TIL in (Serrà et al., 2018). They are converted to DIL versions by merging their classi- implemented by (Serrà et al., 2018)). fication heads. OWM (Zeng et al., 2019) is a CIL 4.4 Results and Analysis method, which we also adapt to a DIL method like EWC. DER++ and SRK can work in the DIL set- As the orde"
2021.emnlp-main.550,P17-2023,1,0.754265,"ce information from the previous task models to create views for explicit knowledge transfer and distillation. Existing contrastive learning cannot do that. Several researchers have studied lifelong or continual learning for sentiment analysis. Early works are done under Lifelong Learning (LL) (Silver et al., 2013; Ruvolo and Eaton, 2013; Chen and Liu, 2014). Two Naive Bayes (NB) approaches were proposed to improve the new task learning (Chen et al., 2015; Wang et al., 2019). Xia et al. (2017) proposed a voting based approach. All these systems work on document sentiment classification (DSC). Shu et al. (2017) used LL for aspect extraction. These works do not use neural networks, and have no CF problem. L2PG (Qin et al., 2020) uses a neural network but improves only the new task learning for DSC. Wang et al. (2018) worked on ASC, but since they 3 Proposed CLASSIC Method improve only the new task learning, they did not deal with CF. Each task uses a separate network. State-of-the-art ASC systems all use BERT (DeExisting CL systems SRK (Lv et al., 2019) and vlin et al., 2019) or other language models as the KAN (Ke et al., 2020b) are for DSC in the TIL base. The proposed technique CLASSIC adopts sett"
2021.emnlp-main.550,N19-1035,0,0.0453706,"Missing"
2021.emnlp-main.550,D16-1021,0,0.0572294,"Missing"
2021.emnlp-main.590,2020.acl-main.573,0,0.0378034,"observe that by storing only a few samples per task (10-50) the model still greatly suffers from catastrophic forgetting, where with around 500 samples, which is equivalent to a total of 18,500 samples in our setting, the performance is closer to that of the multitask baseline (i.e., a possible upper bound). Similar observations are shown for the other two tasks in Figure 8, 9, and 10 in the Appendix. a fine-tuning step) during inference. Finally, continual learning has been used for sentence encoding (Liu et al., 2019), composition language learning (Li et al., 2019c) and relation learning (Han et al., 2020). However, these methods are specific to particular applications not generalizable to ToDs. 6 Related Work Continual learning methods are usually developed and benchmarked on computer visions tasks. Interested readers may refer to Mundt et al. (2020); Parisi et al. (2019); De Lange et al. (2019) for an overview of the existing approaches, and to Section 2.2 for more details on the three main CL approaches studied in this paper. Continual learning has also been studied in the Long Life Learning (LLL) scenario, where a learner continuously accumulates knowledge and makes use of it in the future"
2021.emnlp-main.622,D18-1547,0,0.416429,"of a list of slot-value pairs. Training a tion model for non-categorical slots and a classifiDST model often requires extensive annotated di- cation model for categorical slots, which hinders alogue data. These data are often collected via a the knowledge sharing from the different types of Wizard-of-Oz (Woz) (Kelley, 1984) setting, where QA datasets. Furthermore, unanswerable questions two workers converse with each other and anno- are not considered during their QA training phase. tate the dialogue states of each utterance (Wen Therefore, in a zero-shot DST setting, the model et al., 2017; Budzianowski et al., 2018; Moon et al., proposed by Gao et al. (2020) is not able to han2020), or with a Machines Talking To Machines dle “none” value slots (e.g., unmentioned slots) ∗ Work done during internship at Facebook that present in the dialogue state. 7890 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7890–7900 c November 7–11, 2021. 2021 Association for Computational Linguistics Figure 1: A high-level representation of the cross-task transfer for zero-shot DST (best viewed in color). During the QA training phase (top figure), the unified generative model (T5) i"
2021.emnlp-main.622,2020.acl-main.12,0,0.0330909,"Missing"
2021.emnlp-main.622,N19-1423,0,0.062187,"Missing"
2021.emnlp-main.622,Q19-1026,0,0.012715,"uncating the context passage from the first sentence that contains the answer span. As illustrated in Figure 3, given a question and a passage from a QA training set, we first truncate the passage according to the answer span annotation, then we pair the question and the truncated passage as an unanswerable sample. 3 Experiments 3.1 Datasets QA datasets. For the QA training, we use six extractive QA datasets such as SQuAD2.0 (Rajpurkar et al., 2018) 1 , NewsQA (Trischler et al., 2017), TriviaQA (Joshi et al., 2017), SearchQA (Dunn et al., 2017), HotpotQA (Yang et al., 2018), Natural Question (Kwiatkowski et al., 2019) from MRQA-2019 (Fisch et al., 2019), and two multichoice datasets such as RACE (Lai et al., 2017) and DREAM (Sun et al., 2019). The main train/dev statistics are reported in Table 1. DST datasets. The evaluation is conducted on two multi-domain task-oriented dialogue benchmark, MultiWoz (Budzianowski et al., 2018; Eric et al., 2020) and Schema-Guided-Dialogue (SGD) (Rastogi et al., 2020). Both datasets provide turn-level annotations of dialogue states. In MultiWoz, we follow the pre-processing and evaluation setup from Wu et al. (2019), where restaurant, train, attraction, hotel, and taxi dom"
2021.emnlp-main.622,D17-1082,0,0.0876989,"re 3, given a question and a passage from a QA training set, we first truncate the passage according to the answer span annotation, then we pair the question and the truncated passage as an unanswerable sample. 3 Experiments 3.1 Datasets QA datasets. For the QA training, we use six extractive QA datasets such as SQuAD2.0 (Rajpurkar et al., 2018) 1 , NewsQA (Trischler et al., 2017), TriviaQA (Joshi et al., 2017), SearchQA (Dunn et al., 2017), HotpotQA (Yang et al., 2018), Natural Question (Kwiatkowski et al., 2019) from MRQA-2019 (Fisch et al., 2019), and two multichoice datasets such as RACE (Lai et al., 2017) and DREAM (Sun et al., 2019). The main train/dev statistics are reported in Table 1. DST datasets. The evaluation is conducted on two multi-domain task-oriented dialogue benchmark, MultiWoz (Budzianowski et al., 2018; Eric et al., 2020) and Schema-Guided-Dialogue (SGD) (Rastogi et al., 2020). Both datasets provide turn-level annotations of dialogue states. In MultiWoz, we follow the pre-processing and evaluation setup from Wu et al. (2019), where restaurant, train, attraction, hotel, and taxi domains are used for training and testing. In SGD, the test set has 18 domains, and 5 of the domains"
2021.emnlp-main.622,P19-1546,0,0.0155263,"no “none” values are considered as active slots. Then the model gener2 https://github.com/jasonwu0731/ trade-dst 3 https://github.com/google-research/ google-research/tree/master/schema_ guided_dst 4 Source code is available in https://github.com/ facebookresearch/Zero-Shot-DST 7893 Model Joint Goal Accuracy Hotel Restaurant Taxi Train Average 20.06 22.46 22.60 31.25 14.20 16.28 19.80 22.72 12.59 13.56 16.50 26.28 59.21 59.27 59.50 61.87 22.39 22.76 22.50 36.72 25.69 26.87 28.18 35.77 56.81 53.90 56.81 63.22 49.57 56.06 Attraction TRADE† (Wu et al., 2019) MA-DST† (Kumar et al., 2020) SUMBT‡ (Lee et al., 2019) TransferQA (Ours) w/ Oracle Slot Gate SGD-baseline JGA AGA TransferQA JGA AGA Unseen Buses* Messaging* Payment* Trains* Alarm* 9.7 10.2 11.5 13.6 57.7 50.9 20.0 34.8 63.5 1.8 15.9 13.3 24.7 17.4 58.3 63.6 37.9 60.7 64.9 81.7 Seen Table 2: Zero-shot results on MultiWoz 2.1 (Eric et al., 2020). Results marked with † and ‡ are from Kumar et al. (2020) and Campagna et al. (2020). We also report the averaged zero shot joint goal accuracy among five domains. Note that this averaged per-domain accuracy is not comparable to the JGA in full shot setting. RentalCars Music RideSharing Media Homes Restau"
2021.emnlp-main.622,P18-1133,0,0.024592,"Predicted Values hotel-area hotel-pricerange what is the area of the hotel that the user wants? what is the price range of the hotel or guesthouse that the user wants? east cheap none none Table 6: Two typical errors of TransferQA zeroshot in MultiWoz 2.1. The first (top example) is predicting the values that not confirmed by the user yet, and the second (bottom example) is missing the values of implicit mentioned domain. Dialogue State Tracking is an essential yet challenging task in conversational AI research (Williams and Young, 2007; Williams et al., 2014). Recent state-of-the-art models (Lei et al., 2018; Zhang et al., 2020; Wu et al., 2020; Peng et al., 2020; Zhang et al., 2019; Kim et al., 2019; Lin et al., 2020; Chen et al., 2020; Heck et al., 2020; Mehri et al., 2020; Hosseini-Asl et al., 2020; Yu et al., 2020; Li et al., 2020; Madotto et al., 2020) trained with extensive annotated dialogue data have shown promising performance in complex multi-domain conversations (Budzianowski et al., 2018; Eric et al., 2020). However, collecting large amounts of data for every dialogue domain is often costly and inefficient. To reduce the expense of data acquisition, zero-shot (few-shot) transfer learn"
2021.emnlp-main.622,2021.acl-long.353,0,0.0363165,"Missing"
2021.emnlp-main.622,2021.naacl-main.448,1,0.866589,"Missing"
2021.emnlp-main.622,2020.emnlp-main.273,1,0.761706,"e range of the hotel or guesthouse that the user wants? east cheap none none Table 6: Two typical errors of TransferQA zeroshot in MultiWoz 2.1. The first (top example) is predicting the values that not confirmed by the user yet, and the second (bottom example) is missing the values of implicit mentioned domain. Dialogue State Tracking is an essential yet challenging task in conversational AI research (Williams and Young, 2007; Williams et al., 2014). Recent state-of-the-art models (Lei et al., 2018; Zhang et al., 2020; Wu et al., 2020; Peng et al., 2020; Zhang et al., 2019; Kim et al., 2019; Lin et al., 2020; Chen et al., 2020; Heck et al., 2020; Mehri et al., 2020; Hosseini-Asl et al., 2020; Yu et al., 2020; Li et al., 2020; Madotto et al., 2020) trained with extensive annotated dialogue data have shown promising performance in complex multi-domain conversations (Budzianowski et al., 2018; Eric et al., 2020). However, collecting large amounts of data for every dialogue domain is often costly and inefficient. To reduce the expense of data acquisition, zero-shot (few-shot) transfer learning has been proposed as an effective solution. Wu et al. (2019) adapt a copy mechanism for transferring prior k"
2021.emnlp-main.622,2021.ccl-1.108,0,0.0535518,"Missing"
2021.emnlp-main.622,P18-2124,0,0.020354,"omain unmentioned slots often appear in the middle of conversations, where some of the in-domain slots have not yet mentioned by the user. We simulate such scenario by truncating the context passage from the first sentence that contains the answer span. As illustrated in Figure 3, given a question and a passage from a QA training set, we first truncate the passage according to the answer span annotation, then we pair the question and the truncated passage as an unanswerable sample. 3 Experiments 3.1 Datasets QA datasets. For the QA training, we use six extractive QA datasets such as SQuAD2.0 (Rajpurkar et al., 2018) 1 , NewsQA (Trischler et al., 2017), TriviaQA (Joshi et al., 2017), SearchQA (Dunn et al., 2017), HotpotQA (Yang et al., 2018), Natural Question (Kwiatkowski et al., 2019) from MRQA-2019 (Fisch et al., 2019), and two multichoice datasets such as RACE (Lai et al., 2017) and DREAM (Sun et al., 2019). The main train/dev statistics are reported in Table 1. DST datasets. The evaluation is conducted on two multi-domain task-oriented dialogue benchmark, MultiWoz (Budzianowski et al., 2018; Eric et al., 2020) and Schema-Guided-Dialogue (SGD) (Rastogi et al., 2020). Both datasets provide turn-level an"
2021.emnlp-main.622,D16-1264,0,0.0515644,"datasets. The evaluation is conducted on two multi-domain task-oriented dialogue benchmark, MultiWoz (Budzianowski et al., 2018; Eric et al., 2020) and Schema-Guided-Dialogue (SGD) (Rastogi et al., 2020). Both datasets provide turn-level annotations of dialogue states. In MultiWoz, we follow the pre-processing and evaluation setup from Wu et al. (2019), where restaurant, train, attraction, hotel, and taxi domains are used for training and testing. In SGD, the test set has 18 domains, and 5 of the domains are not presented in the training set. 1 Note that original MRQA-2019 dataset use SQuAD (Rajpurkar et al., 2016), here we also add the unanswerable questions from SQuAD2.0. Dataset Type Train Dev SQuAD2.0 NewsQA TriviaQA SearchQA HotpotQA NaturalQA extractive extractive extractive extractive extractive extractive 130,319 74,160 61,688 117,384 72,928 104,071 11,873 4,212 7,785 16,980 5,904 12,836 multiple-choice multiple-choice 87,866 6,116 4,887 2,040 RACE DREAM Table 1: Datasets used in the QA pre-training. Statistics of extractive datasets (except SQuAD2.0) are taken from MRQA-2019 (Fisch et al., 2019), and that of multiple-choice datasets are from RACE (Lai et al., 2017) and DREAM (Sun et al., 2019)."
2021.emnlp-main.622,N18-2074,0,0.0549602,"Missing"
2021.emnlp-main.622,Q19-1014,0,0.141767,"passage from a QA training set, we first truncate the passage according to the answer span annotation, then we pair the question and the truncated passage as an unanswerable sample. 3 Experiments 3.1 Datasets QA datasets. For the QA training, we use six extractive QA datasets such as SQuAD2.0 (Rajpurkar et al., 2018) 1 , NewsQA (Trischler et al., 2017), TriviaQA (Joshi et al., 2017), SearchQA (Dunn et al., 2017), HotpotQA (Yang et al., 2018), Natural Question (Kwiatkowski et al., 2019) from MRQA-2019 (Fisch et al., 2019), and two multichoice datasets such as RACE (Lai et al., 2017) and DREAM (Sun et al., 2019). The main train/dev statistics are reported in Table 1. DST datasets. The evaluation is conducted on two multi-domain task-oriented dialogue benchmark, MultiWoz (Budzianowski et al., 2018; Eric et al., 2020) and Schema-Guided-Dialogue (SGD) (Rastogi et al., 2020). Both datasets provide turn-level annotations of dialogue states. In MultiWoz, we follow the pre-processing and evaluation setup from Wu et al. (2019), where restaurant, train, attraction, hotel, and taxi domains are used for training and testing. In SGD, the test set has 18 domains, and 5 of the domains are not presented in the trai"
2021.emnlp-main.622,W17-2623,0,0.0269985,"in the middle of conversations, where some of the in-domain slots have not yet mentioned by the user. We simulate such scenario by truncating the context passage from the first sentence that contains the answer span. As illustrated in Figure 3, given a question and a passage from a QA training set, we first truncate the passage according to the answer span annotation, then we pair the question and the truncated passage as an unanswerable sample. 3 Experiments 3.1 Datasets QA datasets. For the QA training, we use six extractive QA datasets such as SQuAD2.0 (Rajpurkar et al., 2018) 1 , NewsQA (Trischler et al., 2017), TriviaQA (Joshi et al., 2017), SearchQA (Dunn et al., 2017), HotpotQA (Yang et al., 2018), Natural Question (Kwiatkowski et al., 2019) from MRQA-2019 (Fisch et al., 2019), and two multichoice datasets such as RACE (Lai et al., 2017) and DREAM (Sun et al., 2019). The main train/dev statistics are reported in Table 1. DST datasets. The evaluation is conducted on two multi-domain task-oriented dialogue benchmark, MultiWoz (Budzianowski et al., 2018; Eric et al., 2020) and Schema-Guided-Dialogue (SGD) (Rastogi et al., 2020). Both datasets provide turn-level annotations of dialogue states. In Mul"
2021.emnlp-main.622,D19-1221,0,0.0383259,"Missing"
2021.emnlp-main.622,E17-1042,0,0.0744009,"Missing"
2021.emnlp-main.622,2020.emnlp-main.66,0,0.0179673,"erange what is the area of the hotel that the user wants? what is the price range of the hotel or guesthouse that the user wants? east cheap none none Table 6: Two typical errors of TransferQA zeroshot in MultiWoz 2.1. The first (top example) is predicting the values that not confirmed by the user yet, and the second (bottom example) is missing the values of implicit mentioned domain. Dialogue State Tracking is an essential yet challenging task in conversational AI research (Williams and Young, 2007; Williams et al., 2014). Recent state-of-the-art models (Lei et al., 2018; Zhang et al., 2020; Wu et al., 2020; Peng et al., 2020; Zhang et al., 2019; Kim et al., 2019; Lin et al., 2020; Chen et al., 2020; Heck et al., 2020; Mehri et al., 2020; Hosseini-Asl et al., 2020; Yu et al., 2020; Li et al., 2020; Madotto et al., 2020) trained with extensive annotated dialogue data have shown promising performance in complex multi-domain conversations (Budzianowski et al., 2018; Eric et al., 2020). However, collecting large amounts of data for every dialogue domain is often costly and inefficient. To reduce the expense of data acquisition, zero-shot (few-shot) transfer learning has been proposed as an effective"
2021.emnlp-main.622,P19-1078,1,0.83341,"nsuming manual annotations, while M2M requires exhaustive hand-crafted rules for covering various dialogue scenarios. In industrial applications, virtual assistants are required to add new services (domains) frequently based on user’s needs, but collecting extensive data for every new domain is costly and inefficient. Therefore, performing zero-shot prediction of dialogue states is becoming increasingly important since it does not require the expense of data acquisition. There are mainly two lines of work in the zero-shot transfer learning problem. The first is cross-domain transfer learning (Wu et al., 2019; Kumar et al., 2020; Rastogi et al., 2020; Lin et al., 2021a), where the models are first trained on several domains, then zero-shot to new domains. However, these methods rely on a considerable amount 1 Introduction of DST data to cover a broad range of slot types, Virtual assistants are designed to help users per- and it is still challenging for the models to handle form daily activities, such as travel planning, on- new slot types in the unseen domain. The second line shopping and restaurant booking. Dialogue line of work leverages machine reading question state tracking (DST), as an essen"
2021.emnlp-main.622,D18-1259,0,0.0255177,"user. We simulate such scenario by truncating the context passage from the first sentence that contains the answer span. As illustrated in Figure 3, given a question and a passage from a QA training set, we first truncate the passage according to the answer span annotation, then we pair the question and the truncated passage as an unanswerable sample. 3 Experiments 3.1 Datasets QA datasets. For the QA training, we use six extractive QA datasets such as SQuAD2.0 (Rajpurkar et al., 2018) 1 , NewsQA (Trischler et al., 2017), TriviaQA (Joshi et al., 2017), SearchQA (Dunn et al., 2017), HotpotQA (Yang et al., 2018), Natural Question (Kwiatkowski et al., 2019) from MRQA-2019 (Fisch et al., 2019), and two multichoice datasets such as RACE (Lai et al., 2017) and DREAM (Sun et al., 2019). The main train/dev statistics are reported in Table 1. DST datasets. The evaluation is conducted on two multi-domain task-oriented dialogue benchmark, MultiWoz (Budzianowski et al., 2018; Eric et al., 2020) and Schema-Guided-Dialogue (SGD) (Rastogi et al., 2020). Both datasets provide turn-level annotations of dialogue states. In MultiWoz, we follow the pre-processing and evaluation setup from Wu et al. (2019), where resta"
2021.emnlp-main.622,2020.nlp4convai-1.13,0,0.0601404,"Missing"
2021.emnlp-main.66,D08-1007,0,0.0455809,"Missing"
2021.emnlp-main.66,P19-1470,0,0.0225601,"ifferent: (1) Conceptually, SPLA and SPRE are related but different from novelty, (2) they are mostly based on structured Subject-Verb-Object triples, rather than natural language sentences, and (3) they use fully labeled data (Dasigi and Hovy, 2014) while we do novelty detection with only normal data in training. The work of commonsense reasoning is remotely related to our work. Existing works build multi-choice commonsense reasoners (Zellers et al., 2018, 2019), study the commonsense knowledge contained in language models (Davison et al., 2019; Trinh and Le, 2019, 2018) and knowledge graph (Bosselut et al., 2019), and build new datasets for better evaluation (Wang et al., 2020a). Several researchers also investigated physical commonsense reasoning (Bagherinezhad et al., 2016; Forbes and Choi, 2017; Wang et al., 2017; Bisk et al., 2020) and affordance of entities (Forbes et al., 2019). They do not perform novelty detection. Our work is also related to trivia fact mining (Merzbacher, 2002; Ganguly et al., 2014; Gamon et al., 2014; Prakash et al., 2015; Fatma et al., 2017; Mahesh and Karanth; Tsurel et al., 2017; Niina and Shimada, 2018; Korn et al., 2019; Kwon et al., 2020). However, trivia is more rela"
2021.emnlp-main.66,D15-1075,0,0.0441007,"elines and our proposed model (based on AUC score) OCSVM 68.07 iForest 50.55 General One-class classifier VAE DSVDD ICS OCGAN 51.43 54.89 56.15 50.80 ization. We could not compare with another latest baseline CSI (Tack et al., 2020) as it is based on various image transformations. We do not compare with out-of-distribution (OOD) detection methods as they require multiple classes to learn. Experiments settings. In general, we conduct experiments using various word and sentence embeddings, such as GloVe6 (Pennington et al., 2014), BERT7 (Devlin et al., 2019) and InferSent (Conneau et al., 2017; Bowman et al., 2015). We only show the best results in Table 2. The detailed hyperparameter settings for GAT-MA and baseline models are included in the Appendix Sec. E and F. HRN 56.83 Proposed GAT-MA 89.22 0.884 AUC Language model based model Ngram LSTM BERT GPT-2 76.76 77.95 82.13 77.87 0.882 0.880 2 4 6 Number of GAT-MAvanilla Stacked Layers Figure 2: Effects of the number of layers in GATMAvanilla on image data because images of a given class (e.g., in the MNIST dataset) contains images with very similar latent representations. Thus, auto-encoder and GAN-based models can learn latent representations for all i"
2021.emnlp-main.66,C14-1134,0,0.0297085,"mentation. Erk and Padó, 2010; Bergsma et al., 2008; Rit3. We create a new dataset called NSD2 for the ter et al., 2010; Ó Séaghdha, 2010; Van de Cruys, proposed task. The dataset can be used as a 2009). The performance is improved by neural benchmark dataset by the NLP community. networks (Van de Cruys, 2014; Dasigi and Hovy, 867 2014; Tilk et al., 2016). Our work is different: (1) Conceptually, SPLA and SPRE are related but different from novelty, (2) they are mostly based on structured Subject-Verb-Object triples, rather than natural language sentences, and (3) they use fully labeled data (Dasigi and Hovy, 2014) while we do novelty detection with only normal data in training. The work of commonsense reasoning is remotely related to our work. Existing works build multi-choice commonsense reasoners (Zellers et al., 2018, 2019), study the commonsense knowledge contained in language models (Davison et al., 2019; Trinh and Le, 2019, 2018) and knowledge graph (Bosselut et al., 2019), and build new datasets for better evaluation (Wang et al., 2020a). Several researchers also investigated physical commonsense reasoning (Bagherinezhad et al., 2016; Forbes and Choi, 2017; Wang et al., 2017; Bisk et al., 2020)"
2021.emnlp-main.66,D19-1109,0,0.0182365,"s, 2014; Dasigi and Hovy, 867 2014; Tilk et al., 2016). Our work is different: (1) Conceptually, SPLA and SPRE are related but different from novelty, (2) they are mostly based on structured Subject-Verb-Object triples, rather than natural language sentences, and (3) they use fully labeled data (Dasigi and Hovy, 2014) while we do novelty detection with only normal data in training. The work of commonsense reasoning is remotely related to our work. Existing works build multi-choice commonsense reasoners (Zellers et al., 2018, 2019), study the commonsense knowledge contained in language models (Davison et al., 2019; Trinh and Le, 2019, 2018) and knowledge graph (Bosselut et al., 2019), and build new datasets for better evaluation (Wang et al., 2020a). Several researchers also investigated physical commonsense reasoning (Bagherinezhad et al., 2016; Forbes and Choi, 2017; Wang et al., 2017; Bisk et al., 2020) and affordance of entities (Forbes et al., 2019). They do not perform novelty detection. Our work is also related to trivia fact mining (Merzbacher, 2002; Ganguly et al., 2014; Gamon et al., 2014; Prakash et al., 2015; Fatma et al., 2017; Mahesh and Karanth; Tsurel et al., 2017; Niina and Shimada, 20"
2021.emnlp-main.66,N19-1423,0,0.140799,"and sentences. (2) Each LM trained on normal descriptions can output the probability of each word in a description appearing in its context. Thus a sentence probability can be calculated from the list of word probabilities. We have tried various ways of calculating the sentence score from the word probability list, such as arithmetic mean, geometric mean, harmonic mean, and multiplication of all word probabilities and found harmonic mean to be the best choice. We use N-gram, the bag of words LM, N ∈ {1, 2, 3, 4, 5} (N = 1 gives the best result), LSTM (Hochreiter and Schmidhuber, 1997), BERT (Devlin et al., 2019), GPT-2 (Radford et al., 2019) as our LM baselines. The results are listed in Table 2. For general one-class classification models, most of them only work on images. We modified the related components of the models to make them X X 0 suitable for text data. More details regarding model L= max{S(d ) − S(d) + 1, 0} (1) modification and parameter setting are provided in 0 0 tr d∈D d ∈D Appendix Sec. F. The following 7 baselines are where, Dtr is the set of the normal descriptions, compared: (1) DSVDD (Deep SVDD) (Ruff et al., d0 ∈ D0 is the pseudo-novel description corre- 2018): a recent one-clas"
2021.emnlp-main.66,C14-1086,0,0.0311993,"ulti-choice commonsense reasoners (Zellers et al., 2018, 2019), study the commonsense knowledge contained in language models (Davison et al., 2019; Trinh and Le, 2019, 2018) and knowledge graph (Bosselut et al., 2019), and build new datasets for better evaluation (Wang et al., 2020a). Several researchers also investigated physical commonsense reasoning (Bagherinezhad et al., 2016; Forbes and Choi, 2017; Wang et al., 2017; Bisk et al., 2020) and affordance of entities (Forbes et al., 2019). They do not perform novelty detection. Our work is also related to trivia fact mining (Merzbacher, 2002; Ganguly et al., 2014; Gamon et al., 2014; Prakash et al., 2015; Fatma et al., 2017; Mahesh and Karanth; Tsurel et al., 2017; Niina and Shimada, 2018; Korn et al., 2019; Kwon et al., 2020). However, trivia is more related to interestingness. Some trivia facts are interesting because they are rare, but not necessarily novel. Existing works use labeled training data for learning, or rely on Wikipedia structure to retrieve interesting facts using information retrieval methods (Tsurel et al., 2017; Kwon et al., 2020). We have only normal data but not novel data. Our proposed model learns text representation using a Gr"
2021.emnlp-main.66,P19-1024,0,0.0905751,"ivia is more related to interestingness. Some trivia facts are interesting because they are rare, but not necessarily novel. Existing works use labeled training data for learning, or rely on Wikipedia structure to retrieve interesting facts using information retrieval methods (Tsurel et al., 2017; Kwon et al., 2020). We have only normal data but not novel data. Our proposed model learns text representation using a Graph Neural Network and leveraging dependency parsing. Other works in NLP that use Graph Neural Networks and dependency structures include (Huang and Carley, 2019; Ma et al., 2020; Guo et al., 2019; Wang et al., 2020b; Pouran Ben Veyseh et al., 2020; Xiao and Zhou, 2020), etc. But they solve different problems, such as sentiment analysis and argument mining. Their approaches are also different from ours and do not do novelty detection. are related to each other by verb “cook""); the actions (verbs) that an entity can support (e.g., only a person can perform action “cook""); actions an entity can be applied on (e.g. “cook"" can be applied on entity “vegetables""), etc., we aim to build a corpus rich in such knowledge. Text data like news articles, social media posts, reviews, etc., generally"
2021.emnlp-main.66,D19-1549,0,0.0540139,"l., 2019; Kwon et al., 2020). However, trivia is more related to interestingness. Some trivia facts are interesting because they are rare, but not necessarily novel. Existing works use labeled training data for learning, or rely on Wikipedia structure to retrieve interesting facts using information retrieval methods (Tsurel et al., 2017; Kwon et al., 2020). We have only normal data but not novel data. Our proposed model learns text representation using a Graph Neural Network and leveraging dependency parsing. Other works in NLP that use Graph Neural Networks and dependency structures include (Huang and Carley, 2019; Ma et al., 2020; Guo et al., 2019; Wang et al., 2020b; Pouran Ben Veyseh et al., 2020; Xiao and Zhou, 2020), etc. But they solve different problems, such as sentiment analysis and argument mining. Their approaches are also different from ours and do not do novelty detection. are related to each other by verb “cook""); the actions (verbs) that an entity can support (e.g., only a person can perform action “cook""); actions an entity can be applied on (e.g. “cook"" can be applied on entity “vegetables""), etc., we aim to build a corpus rich in such knowledge. Text data like news articles, social me"
2021.emnlp-main.66,C14-1140,0,0.0136479,"e reasoners (Zellers et al., 2018, 2019), study the commonsense knowledge contained in language models (Davison et al., 2019; Trinh and Le, 2019, 2018) and knowledge graph (Bosselut et al., 2019), and build new datasets for better evaluation (Wang et al., 2020a). Several researchers also investigated physical commonsense reasoning (Bagherinezhad et al., 2016; Forbes and Choi, 2017; Wang et al., 2017; Bisk et al., 2020) and affordance of entities (Forbes et al., 2019). They do not perform novelty detection. Our work is also related to trivia fact mining (Merzbacher, 2002; Ganguly et al., 2014; Gamon et al., 2014; Prakash et al., 2015; Fatma et al., 2017; Mahesh and Karanth; Tsurel et al., 2017; Niina and Shimada, 2018; Korn et al., 2019; Kwon et al., 2020). However, trivia is more related to interestingness. Some trivia facts are interesting because they are rare, but not necessarily novel. Existing works use labeled training data for learning, or rely on Wikipedia structure to retrieve interesting facts using information retrieval methods (Tsurel et al., 2017; Kwon et al., 2020). We have only normal data but not novel data. Our proposed model learns text representation using a Graph Neural Network a"
2021.emnlp-main.66,2020.coling-main.424,0,0.0367711,"018) and knowledge graph (Bosselut et al., 2019), and build new datasets for better evaluation (Wang et al., 2020a). Several researchers also investigated physical commonsense reasoning (Bagherinezhad et al., 2016; Forbes and Choi, 2017; Wang et al., 2017; Bisk et al., 2020) and affordance of entities (Forbes et al., 2019). They do not perform novelty detection. Our work is also related to trivia fact mining (Merzbacher, 2002; Ganguly et al., 2014; Gamon et al., 2014; Prakash et al., 2015; Fatma et al., 2017; Mahesh and Karanth; Tsurel et al., 2017; Niina and Shimada, 2018; Korn et al., 2019; Kwon et al., 2020). However, trivia is more related to interestingness. Some trivia facts are interesting because they are rare, but not necessarily novel. Existing works use labeled training data for learning, or rely on Wikipedia structure to retrieve interesting facts using information retrieval methods (Tsurel et al., 2017; Kwon et al., 2020). We have only normal data but not novel data. Our proposed model learns text representation using a Graph Neural Network and leveraging dependency parsing. Other works in NLP that use Graph Neural Networks and dependency structures include (Huang and Carley, 2019; Ma e"
2021.emnlp-main.66,P19-1548,0,0.020335,"test NL description d0 with respect to D, i.e., classifying d0 into one of the two classes {NORMAL, NOVEL}. “NORMAL"" means that d0 is a description of a common scene and ”NOVEL” means d0 is a description of a seNovelty or anomaly detection has been an important research topic since 1970s (Barnett and Lewis, 1994) due to numerous applications (Chalapathy et al., 2018; Pang et al., 2021). Recently, it has also become important for natural language processing (NLP). Many researchers have studied the problem in the text classification setting (Fei and Liu, 2016; Shu et al., 2017; Xu et al., 2019; Lin and Xu, 2019; Zheng et al., 2020). However, these text novelty classifiers are mainly coarse-grained, working at the document or topic level. Given a text document, their goal is to detect whether the text belongs to a known class or unknown class. This paper introduces a new text novelty detection problem - fine-grained semantic novelty detection. Specifically, given a text description d, we detect whether d represents a semantically novel fact or not. This work considers text data that describe scenes of real-world phenomena in natural language (NL). In our daily lives, we observe different real-world p"
2021.emnlp-main.66,P10-1045,0,0.0205747,"yntactic analysis. ting. Other related work includes creating datasets 2. We propose a highly effective technique called with plausibility ratings (Keller and Lapata, 2003) GAT-MA to solve the proposed semantic nov- and dealing with multi-event inference (Zhang elty detection problem, which is based on et al., 2017; Sap et al., 2019). For SPRE, the early GAT with dependency parsing and knowledge- works include (Resnik, 1996; Clark and Weir, 2001; based contrastive data augmentation. Erk and Padó, 2010; Bergsma et al., 2008; Rit3. We create a new dataset called NSD2 for the ter et al., 2010; Ó Séaghdha, 2010; Van de Cruys, proposed task. The dataset can be used as a 2009). The performance is improved by neural benchmark dataset by the NLP community. networks (Van de Cruys, 2014; Dasigi and Hovy, 867 2014; Tilk et al., 2016). Our work is different: (1) Conceptually, SPLA and SPRE are related but different from novelty, (2) they are mostly based on structured Subject-Verb-Object triples, rather than natural language sentences, and (3) they use fully labeled data (Dasigi and Hovy, 2014) while we do novelty detection with only normal data in training. The work of commonsense reasoning is remotely rel"
2021.emnlp-main.66,D14-1162,0,0.0892869,"criptions 2020): the latest model based on a holistic regular872 Table 2: Comparison of baselines and our proposed model (based on AUC score) OCSVM 68.07 iForest 50.55 General One-class classifier VAE DSVDD ICS OCGAN 51.43 54.89 56.15 50.80 ization. We could not compare with another latest baseline CSI (Tack et al., 2020) as it is based on various image transformations. We do not compare with out-of-distribution (OOD) detection methods as they require multiple classes to learn. Experiments settings. In general, we conduct experiments using various word and sentence embeddings, such as GloVe6 (Pennington et al., 2014), BERT7 (Devlin et al., 2019) and InferSent (Conneau et al., 2017; Bowman et al., 2015). We only show the best results in Table 2. The detailed hyperparameter settings for GAT-MA and baseline models are included in the Appendix Sec. E and F. HRN 56.83 Proposed GAT-MA 89.22 0.884 AUC Language model based model Ngram LSTM BERT GPT-2 76.76 77.95 82.13 77.87 0.882 0.880 2 4 6 Number of GAT-MAvanilla Stacked Layers Figure 2: Effects of the number of layers in GATMAvanilla on image data because images of a given class (e.g., in the MNIST dataset) contains images with very similar latent representati"
2021.emnlp-main.66,P10-1044,0,0.0521918,"Missing"
2021.emnlp-main.66,D16-1017,0,0.0197818,"and dealing with multi-event inference (Zhang elty detection problem, which is based on et al., 2017; Sap et al., 2019). For SPRE, the early GAT with dependency parsing and knowledge- works include (Resnik, 1996; Clark and Weir, 2001; based contrastive data augmentation. Erk and Padó, 2010; Bergsma et al., 2008; Rit3. We create a new dataset called NSD2 for the ter et al., 2010; Ó Séaghdha, 2010; Van de Cruys, proposed task. The dataset can be used as a 2009). The performance is improved by neural benchmark dataset by the NLP community. networks (Van de Cruys, 2014; Dasigi and Hovy, 867 2014; Tilk et al., 2016). Our work is different: (1) Conceptually, SPLA and SPRE are related but different from novelty, (2) they are mostly based on structured Subject-Verb-Object triples, rather than natural language sentences, and (3) they use fully labeled data (Dasigi and Hovy, 2014) while we do novelty detection with only normal data in training. The work of commonsense reasoning is remotely related to our work. Existing works build multi-choice commonsense reasoners (Zellers et al., 2018, 2019), study the commonsense knowledge contained in language models (Davison et al., 2019; Trinh and Le, 2019, 2018) and kn"
2021.emnlp-main.66,W09-0211,0,0.222006,"Missing"
2021.emnlp-main.66,D14-1004,0,0.0444285,"Missing"
2021.emnlp-main.66,2020.semeval-1.39,0,0.0811107,"Missing"
2021.emnlp-main.66,2020.acl-main.295,0,0.0125398,"om novelty, (2) they are mostly based on structured Subject-Verb-Object triples, rather than natural language sentences, and (3) they use fully labeled data (Dasigi and Hovy, 2014) while we do novelty detection with only normal data in training. The work of commonsense reasoning is remotely related to our work. Existing works build multi-choice commonsense reasoners (Zellers et al., 2018, 2019), study the commonsense knowledge contained in language models (Davison et al., 2019; Trinh and Le, 2019, 2018) and knowledge graph (Bosselut et al., 2019), and build new datasets for better evaluation (Wang et al., 2020a). Several researchers also investigated physical commonsense reasoning (Bagherinezhad et al., 2016; Forbes and Choi, 2017; Wang et al., 2017; Bisk et al., 2020) and affordance of entities (Forbes et al., 2019). They do not perform novelty detection. Our work is also related to trivia fact mining (Merzbacher, 2002; Ganguly et al., 2014; Gamon et al., 2014; Prakash et al., 2015; Fatma et al., 2017; Mahesh and Karanth; Tsurel et al., 2017; Niina and Shimada, 2018; Korn et al., 2019; Kwon et al., 2020). However, trivia is more related to interestingness. Some trivia facts are interesting because"
2021.emnlp-main.66,N18-2049,0,0.0168065,"and Croft, 2005; Zhang and Tsai, 2009). These tasks differ from our problem setting as we focus on fine-grained semantic novelty detection. Our work is also related to semantic plausibility (SPLA) and selectional preference (SPRE). SPLA is concerned with whether an event is plausible, and SPRE is about the “typicality"" of an event. For 1. We propose a new task of semantic novelty SPLA, existing models employ pretrained language detection in text. Whereas the existing work models (Porada et al., 2019) and manually elicited focuses on coarse-grained document- or topicentity property knowledge (Wang et al., 2018) to level novelty, our task requires fine-grained model physical plausibility in the supervised setsentence-level semantic & syntactic analysis. ting. Other related work includes creating datasets 2. We propose a highly effective technique called with plausibility ratings (Keller and Lapata, 2003) GAT-MA to solve the proposed semantic nov- and dealing with multi-event inference (Zhang elty detection problem, which is based on et al., 2017; Sap et al., 2019). For SPRE, the early GAT with dependency parsing and knowledge- works include (Resnik, 1996; Clark and Weir, 2001; based contrastive data"
2021.emnlp-main.66,I17-1021,0,0.0122141,"y labeled data (Dasigi and Hovy, 2014) while we do novelty detection with only normal data in training. The work of commonsense reasoning is remotely related to our work. Existing works build multi-choice commonsense reasoners (Zellers et al., 2018, 2019), study the commonsense knowledge contained in language models (Davison et al., 2019; Trinh and Le, 2019, 2018) and knowledge graph (Bosselut et al., 2019), and build new datasets for better evaluation (Wang et al., 2020a). Several researchers also investigated physical commonsense reasoning (Bagherinezhad et al., 2016; Forbes and Choi, 2017; Wang et al., 2017; Bisk et al., 2020) and affordance of entities (Forbes et al., 2019). They do not perform novelty detection. Our work is also related to trivia fact mining (Merzbacher, 2002; Ganguly et al., 2014; Gamon et al., 2014; Prakash et al., 2015; Fatma et al., 2017; Mahesh and Karanth; Tsurel et al., 2017; Niina and Shimada, 2018; Korn et al., 2019; Kwon et al., 2020). However, trivia is more related to interestingness. Some trivia facts are interesting because they are rare, but not necessarily novel. Existing works use labeled training data for learning, or rely on Wikipedia structure to retrieve i"
2021.emnlp-main.66,D18-1009,0,0.0221152,"ormance is improved by neural benchmark dataset by the NLP community. networks (Van de Cruys, 2014; Dasigi and Hovy, 867 2014; Tilk et al., 2016). Our work is different: (1) Conceptually, SPLA and SPRE are related but different from novelty, (2) they are mostly based on structured Subject-Verb-Object triples, rather than natural language sentences, and (3) they use fully labeled data (Dasigi and Hovy, 2014) while we do novelty detection with only normal data in training. The work of commonsense reasoning is remotely related to our work. Existing works build multi-choice commonsense reasoners (Zellers et al., 2018, 2019), study the commonsense knowledge contained in language models (Davison et al., 2019; Trinh and Le, 2019, 2018) and knowledge graph (Bosselut et al., 2019), and build new datasets for better evaluation (Wang et al., 2020a). Several researchers also investigated physical commonsense reasoning (Bagherinezhad et al., 2016; Forbes and Choi, 2017; Wang et al., 2017; Bisk et al., 2020) and affordance of entities (Forbes et al., 2019). They do not perform novelty detection. Our work is also related to trivia fact mining (Merzbacher, 2002; Ganguly et al., 2014; Gamon et al., 2014; Prakash et al"
2021.emnlp-main.66,P19-1472,0,0.0399499,"Missing"
2021.emnlp-main.66,P94-1019,0,0.317403,"ate contrastive instances to the normal scene descriptions in Dtr . These contrastive instances serve as pseudo-novel data and enable supervised learning of the Text Semantic Novelty Scorer (SNS). WordNet contains rich taxonomy of words and thus, is beneficial to our semantic novelty detection task. In our generator, a knowledge-based misfit sampler Smisf it (.) is the key component. Given a normal scene description d ∈ Dtr , Smisf it (e) [here, e is an entity, either a noun or a noun phrase] samples an entity e0 that is semantically distant from e in the WordNet. We use Wu-Palmer Similarity (Wu and Palmer, 1994) to measure the semantic distance between e and e0 . We randomly sample e0 from WordNet such that the similarity score between e and e0 is less than 0.9 (an empirically set threshold). Next, since e0 is semantically distant from e, e0 is a misfit in original description d. e is replaced with e0 in description d to generate a pseudo-novel description. For example, “a man is driving a car"" describes a normal scene. It is commonsense that the subject for verb “drive"" should be a person. Any thing outside of the category 4 The Proposed GAT-MA Model introduces novelty, e.g., “a dog is driving a car"
2021.findings-acl.320,K15-1006,0,0.0287755,"existing research about domain and context dependent sentiment. First, despite the fact that several researchers have studied context dependent sentiment words, which are based on sentences and topic/aspect context (Wilson et al., 2005; Ding et al., 2008; Choi and Cardie, 2008; Wu and Wen, 2010; Jijkoun et al., 2010; Lu et al., 2011; Zhao et al., 2012; Kessler and Sch¨utze, 2012; Teng et al., 2016; Wang et al., 2016, 2018a,b; Li et al., 2018a), our work is based on domains. Second, while the studies on transfer learning or domain adaptation for sentiment analysis deal with domain information (Bhatt et al., 2015; Yu and Jiang, 2016; Li et al., 2018b), our work does not lie in this direction. We do not have any source domain and our goal is not to transfer domain knowledge to another domain. Third, most importantly, the above works are either irrelevant to lexicons or not for detecting the sentiment discrepancy between a lexicon and the application domain. Our work is most related to the following studies that involve both sentiment lexicons and domain sentiment problems. Choi and Cardie (2009) adapted the word-level polarities of a generalpurpose sentiment lexicon to a particular domain by utilizing"
2021.findings-acl.320,D16-1057,0,0.0822535,"ular domain, and hence, cannot solve our problem. Du et al. (2010) studied the problem of adapting the sentiment lexicon from one domain to another domain. It further assumes that the source domain has a set of sentiment-labeled reviews. Their technique is therefore more about transfer learning and their learning settings differ from ours intrinsically. (Ortiz, 2017) designed a sentiment analysis application that allows plugin lexicons (if users can provide them) to help predict domain sentiment. It neither detects nor corrects domain polarity-changed words. Perhaps, the most related work is (Hamilton et al., 2016), which uses seed lexicon words, word embeddings, and random walk to generate a domain-specific lexicon. However, their model is for lexicon construction in essence, by (its capability of) functioning on a domain-oriented corpus. It does not aim to detect/change the sentiment polarity from a given lexicon. It is thus not directly applicable to our task. To make it workable for our task, we design a two-step approach, which will be detailed in the experiment section (Section 5). To the best of our knowledge, this is the first study to detect domain polarity-changes of words in a sentiment lexic"
2021.findings-acl.320,P10-1041,0,0.193531,"nt with lexicons. Extensive studies have been done for sentiment lexicons and the majority of them focus on lexicon construction. These approaches can be generally categorized as dictionary-based and corpus-based. Dictionary-based approaches first used some sentiment seed words to bootstrap based on the synonym and antonym structure of a dictionary (Hu and Liu, 2004; Valitutti et al., 2004). Later on, more sophisticated methods were proposed (Kim and Hovy, 2004; Esuli and Sebastiani, 2005; Takamura et al., 2007; Blair-Goldensohn et al., 2008; Rao and Ravichandran, 2009; Mohammad et al., 2009; Hassan and Radev, 2010; Dragut et al., 2010; Xu et al., 2010; Peng and Park, 2011; Gatti and Guerini, 2012; San Vicente et al., 2014). Corpus-based approaches build lexicons by discovering sentiment words in a large corpus. The first idea is to exploit some coordinating conjunctions (Hatzivassiloglou and McKeown, 1997; Hassan and Radev, 2010). Kanayama and Nasukawa (2006) extended this approach by introducing inter-sentential sentiment consistency. Other related work includes (Kamps et al., 2004; Kaji and Kitsuregawa, 2006; Wang et al., 2017). The second idea is to use syntactic relations between opinion and aspect"
2021.findings-acl.320,P97-1023,0,0.75228,"tstrap based on the synonym and antonym structure of a dictionary (Hu and Liu, 2004; Valitutti et al., 2004). Later on, more sophisticated methods were proposed (Kim and Hovy, 2004; Esuli and Sebastiani, 2005; Takamura et al., 2007; Blair-Goldensohn et al., 2008; Rao and Ravichandran, 2009; Mohammad et al., 2009; Hassan and Radev, 2010; Dragut et al., 2010; Xu et al., 2010; Peng and Park, 2011; Gatti and Guerini, 2012; San Vicente et al., 2014). Corpus-based approaches build lexicons by discovering sentiment words in a large corpus. The first idea is to exploit some coordinating conjunctions (Hatzivassiloglou and McKeown, 1997; Hassan and Radev, 2010). Kanayama and Nasukawa (2006) extended this approach by introducing inter-sentential sentiment consistency. Other related work includes (Kamps et al., 2004; Kaji and Kitsuregawa, 2006; Wang et al., 2017). The second idea is to use syntactic relations between opinion and aspect words (Zhuang et al., 2006; Wang and Wang, 2008; Qiu et al., 2011; Volkova et al., 2013). The third idea is to use word co-occurrences for lexicon induction (Turney and Littman, 2003; Igo and Riloff, 2009; Velikovich et al., 2010; Yang et al., 2014; Rothe et al., 2016). However, our work is very"
2021.findings-acl.320,D08-1083,0,0.0601696,"). The third idea is to use word co-occurrences for lexicon induction (Turney and Littman, 2003; Igo and Riloff, 2009; Velikovich et al., 2010; Yang et al., 2014; Rothe et al., 2016). However, our work is very different as we focus on detecting domain dependent sentiment words in a given general-purpose sentiment lexicon. Also related is the existing research about domain and context dependent sentiment. First, despite the fact that several researchers have studied context dependent sentiment words, which are based on sentences and topic/aspect context (Wilson et al., 2005; Ding et al., 2008; Choi and Cardie, 2008; Wu and Wen, 2010; Jijkoun et al., 2010; Lu et al., 2011; Zhao et al., 2012; Kessler and Sch¨utze, 2012; Teng et al., 2016; Wang et al., 2016, 2018a,b; Li et al., 2018a), our work is based on domains. Second, while the studies on transfer learning or domain adaptation for sentiment analysis deal with domain information (Bhatt et al., 2015; Yu and Jiang, 2016; Li et al., 2018b), our work does not lie in this direction. We do not have any source domain and our goal is not to transfer domain knowledge to another domain. Third, most importantly, the above works are either irrelevant to lexicons o"
2021.findings-acl.320,D09-1062,0,0.024391,"while the studies on transfer learning or domain adaptation for sentiment analysis deal with domain information (Bhatt et al., 2015; Yu and Jiang, 2016; Li et al., 2018b), our work does not lie in this direction. We do not have any source domain and our goal is not to transfer domain knowledge to another domain. Third, most importantly, the above works are either irrelevant to lexicons or not for detecting the sentiment discrepancy between a lexicon and the application domain. Our work is most related to the following studies that involve both sentiment lexicons and domain sentiment problems. Choi and Cardie (2009) adapted the word-level polarities of a generalpurpose sentiment lexicon to a particular domain by utilizing the expression-level polarities in that domain. However, their work targeted at reasoning the sentiment polarities of multi-word expressions. It does not detect or revise the sentiment polarities of individual words in the lexicon for a particular domain, and hence, cannot solve our problem. Du et al. (2010) studied the problem of adapting the sentiment lexicon from one domain to another domain. It further assumes that the source domain has a set of sentiment-labeled reviews. Their tech"
2021.findings-acl.320,W09-1703,0,0.055535,"a large corpus. The first idea is to exploit some coordinating conjunctions (Hatzivassiloglou and McKeown, 1997; Hassan and Radev, 2010). Kanayama and Nasukawa (2006) extended this approach by introducing inter-sentential sentiment consistency. Other related work includes (Kamps et al., 2004; Kaji and Kitsuregawa, 2006; Wang et al., 2017). The second idea is to use syntactic relations between opinion and aspect words (Zhuang et al., 2006; Wang and Wang, 2008; Qiu et al., 2011; Volkova et al., 2013). The third idea is to use word co-occurrences for lexicon induction (Turney and Littman, 2003; Igo and Riloff, 2009; Velikovich et al., 2010; Yang et al., 2014; Rothe et al., 2016). However, our work is very different as we focus on detecting domain dependent sentiment words in a given general-purpose sentiment lexicon. Also related is the existing research about domain and context dependent sentiment. First, despite the fact that several researchers have studied context dependent sentiment words, which are based on sentences and topic/aspect context (Wilson et al., 2005; Ding et al., 2008; Choi and Cardie, 2008; Wu and Wen, 2010; Jijkoun et al., 2010; Lu et al., 2011; Zhao et al., 2012; Kessler and Sch¨ut"
2021.findings-acl.320,P10-1060,0,0.0770597,"Missing"
2021.findings-acl.320,C12-2036,0,0.028361,"ajority of them focus on lexicon construction. These approaches can be generally categorized as dictionary-based and corpus-based. Dictionary-based approaches first used some sentiment seed words to bootstrap based on the synonym and antonym structure of a dictionary (Hu and Liu, 2004; Valitutti et al., 2004). Later on, more sophisticated methods were proposed (Kim and Hovy, 2004; Esuli and Sebastiani, 2005; Takamura et al., 2007; Blair-Goldensohn et al., 2008; Rao and Ravichandran, 2009; Mohammad et al., 2009; Hassan and Radev, 2010; Dragut et al., 2010; Xu et al., 2010; Peng and Park, 2011; Gatti and Guerini, 2012; San Vicente et al., 2014). Corpus-based approaches build lexicons by discovering sentiment words in a large corpus. The first idea is to exploit some coordinating conjunctions (Hatzivassiloglou and McKeown, 1997; Hassan and Radev, 2010). Kanayama and Nasukawa (2006) extended this approach by introducing inter-sentential sentiment consistency. Other related work includes (Kamps et al., 2004; Kaji and Kitsuregawa, 2006; Wang et al., 2017). The second idea is to use syntactic relations between opinion and aspect words (Zhuang et al., 2006; Wang and Wang, 2008; Qiu et al., 2011; Volkova et al.,"
2021.findings-acl.320,P06-2059,0,0.100392,"a et al., 2007; Blair-Goldensohn et al., 2008; Rao and Ravichandran, 2009; Mohammad et al., 2009; Hassan and Radev, 2010; Dragut et al., 2010; Xu et al., 2010; Peng and Park, 2011; Gatti and Guerini, 2012; San Vicente et al., 2014). Corpus-based approaches build lexicons by discovering sentiment words in a large corpus. The first idea is to exploit some coordinating conjunctions (Hatzivassiloglou and McKeown, 1997; Hassan and Radev, 2010). Kanayama and Nasukawa (2006) extended this approach by introducing inter-sentential sentiment consistency. Other related work includes (Kamps et al., 2004; Kaji and Kitsuregawa, 2006; Wang et al., 2017). The second idea is to use syntactic relations between opinion and aspect words (Zhuang et al., 2006; Wang and Wang, 2008; Qiu et al., 2011; Volkova et al., 2013). The third idea is to use word co-occurrences for lexicon induction (Turney and Littman, 2003; Igo and Riloff, 2009; Velikovich et al., 2010; Yang et al., 2014; Rothe et al., 2016). However, our work is very different as we focus on detecting domain dependent sentiment words in a given general-purpose sentiment lexicon. Also related is the existing research about domain and context dependent sentiment. First, des"
2021.findings-acl.320,kamps-etal-2004-using,0,0.241245,"Missing"
2021.findings-acl.320,W06-1642,0,0.173242,"nary (Hu and Liu, 2004; Valitutti et al., 2004). Later on, more sophisticated methods were proposed (Kim and Hovy, 2004; Esuli and Sebastiani, 2005; Takamura et al., 2007; Blair-Goldensohn et al., 2008; Rao and Ravichandran, 2009; Mohammad et al., 2009; Hassan and Radev, 2010; Dragut et al., 2010; Xu et al., 2010; Peng and Park, 2011; Gatti and Guerini, 2012; San Vicente et al., 2014). Corpus-based approaches build lexicons by discovering sentiment words in a large corpus. The first idea is to exploit some coordinating conjunctions (Hatzivassiloglou and McKeown, 1997; Hassan and Radev, 2010). Kanayama and Nasukawa (2006) extended this approach by introducing inter-sentential sentiment consistency. Other related work includes (Kamps et al., 2004; Kaji and Kitsuregawa, 2006; Wang et al., 2017). The second idea is to use syntactic relations between opinion and aspect words (Zhuang et al., 2006; Wang and Wang, 2008; Qiu et al., 2011; Volkova et al., 2013). The third idea is to use word co-occurrences for lexicon induction (Turney and Littman, 2003; Igo and Riloff, 2009; Velikovich et al., 2010; Yang et al., 2014; Rothe et al., 2016). However, our work is very different as we focus on detecting domain dependent se"
2021.findings-acl.320,C12-2056,0,0.0653689,"Missing"
2021.findings-acl.320,C04-1200,0,0.386251,"polarity changes of words in lexicons. So we first discuss the works related to sentiment lexicons, and then domain sentiment, and finally domain sentiment with lexicons. Extensive studies have been done for sentiment lexicons and the majority of them focus on lexicon construction. These approaches can be generally categorized as dictionary-based and corpus-based. Dictionary-based approaches first used some sentiment seed words to bootstrap based on the synonym and antonym structure of a dictionary (Hu and Liu, 2004; Valitutti et al., 2004). Later on, more sophisticated methods were proposed (Kim and Hovy, 2004; Esuli and Sebastiani, 2005; Takamura et al., 2007; Blair-Goldensohn et al., 2008; Rao and Ravichandran, 2009; Mohammad et al., 2009; Hassan and Radev, 2010; Dragut et al., 2010; Xu et al., 2010; Peng and Park, 2011; Gatti and Guerini, 2012; San Vicente et al., 2014). Corpus-based approaches build lexicons by discovering sentiment words in a large corpus. The first idea is to exploit some coordinating conjunctions (Hatzivassiloglou and McKeown, 1997; Hassan and Radev, 2010). Kanayama and Nasukawa (2006) extended this approach by introducing inter-sentential sentiment consistency. Other relate"
2021.findings-acl.320,P18-1087,0,0.0285848,"., 2016). However, our work is very different as we focus on detecting domain dependent sentiment words in a given general-purpose sentiment lexicon. Also related is the existing research about domain and context dependent sentiment. First, despite the fact that several researchers have studied context dependent sentiment words, which are based on sentences and topic/aspect context (Wilson et al., 2005; Ding et al., 2008; Choi and Cardie, 2008; Wu and Wen, 2010; Jijkoun et al., 2010; Lu et al., 2011; Zhao et al., 2012; Kessler and Sch¨utze, 2012; Teng et al., 2016; Wang et al., 2016, 2018a,b; Li et al., 2018a), our work is based on domains. Second, while the studies on transfer learning or domain adaptation for sentiment analysis deal with domain information (Bhatt et al., 2015; Yu and Jiang, 2016; Li et al., 2018b), our work does not lie in this direction. We do not have any source domain and our goal is not to transfer domain knowledge to another domain. Third, most importantly, the above works are either irrelevant to lexicons or not for detecting the sentiment discrepancy between a lexicon and the application domain. Our work is most related to the following studies that involve both sentimen"
2021.findings-acl.320,D09-1063,0,0.128917,"Missing"
2021.findings-acl.320,E17-3019,0,0.0192653,"the expression-level polarities in that domain. However, their work targeted at reasoning the sentiment polarities of multi-word expressions. It does not detect or revise the sentiment polarities of individual words in the lexicon for a particular domain, and hence, cannot solve our problem. Du et al. (2010) studied the problem of adapting the sentiment lexicon from one domain to another domain. It further assumes that the source domain has a set of sentiment-labeled reviews. Their technique is therefore more about transfer learning and their learning settings differ from ours intrinsically. (Ortiz, 2017) designed a sentiment analysis application that allows plugin lexicons (if users can provide them) to help predict domain sentiment. It neither detects nor corrects domain polarity-changed words. Perhaps, the most related work is (Hamilton et al., 2016), which uses seed lexicon words, word embeddings, and random walk to generate a domain-specific lexicon. However, their model is for lexicon construction in essence, by (its capability of) functioning on a domain-oriented corpus. It does not aim to detect/change the sentiment polarity from a given lexicon. It is thus not directly applicable to o"
2021.findings-acl.320,C08-2019,0,0.186675,"sentiment words (from the lexicon) are domain dependent. That is, they may be positive in some domains but negative in some others. We refer to this problem as domain polarity-changes of words from a sentiment lexicon. Detecting such words and correcting their sentiment for an application domain is very important. In this paper, we propose a graph-based technique to tackle this problem. Experimental results show its effectiveness on multiple datasets from different domains. 1 Introduction Sentiment words, also called opinion/polar words, are words that convey positive or negative sentiments (Pang and Lee, 2008). Such sentimentbearing words are usually pre-compiled as word lists in a sentiment lexicon, which is instrumental as well as an important linguistic resource to sentiment analysis (Liu, 2012). So far, numerous studies about how to construct lexicons have been reported, which will be discussed in Section 2. Despite the fact that there is extensive research on lexicon construction, limited work has been done to solve the problem of identifying and handling sentiment words in a given/constructed lexicon that have domain-dependent polarities. In real-life applications, there are almost always som"
2021.findings-acl.320,J11-1002,1,0.707166,"ark, 2011; Gatti and Guerini, 2012; San Vicente et al., 2014). Corpus-based approaches build lexicons by discovering sentiment words in a large corpus. The first idea is to exploit some coordinating conjunctions (Hatzivassiloglou and McKeown, 1997; Hassan and Radev, 2010). Kanayama and Nasukawa (2006) extended this approach by introducing inter-sentential sentiment consistency. Other related work includes (Kamps et al., 2004; Kaji and Kitsuregawa, 2006; Wang et al., 2017). The second idea is to use syntactic relations between opinion and aspect words (Zhuang et al., 2006; Wang and Wang, 2008; Qiu et al., 2011; Volkova et al., 2013). The third idea is to use word co-occurrences for lexicon induction (Turney and Littman, 2003; Igo and Riloff, 2009; Velikovich et al., 2010; Yang et al., 2014; Rothe et al., 2016). However, our work is very different as we focus on detecting domain dependent sentiment words in a given general-purpose sentiment lexicon. Also related is the existing research about domain and context dependent sentiment. First, despite the fact that several researchers have studied context dependent sentiment words, which are based on sentences and topic/aspect context (Wilson et al., 200"
2021.findings-acl.320,E09-1077,0,0.0662069,"d then domain sentiment, and finally domain sentiment with lexicons. Extensive studies have been done for sentiment lexicons and the majority of them focus on lexicon construction. These approaches can be generally categorized as dictionary-based and corpus-based. Dictionary-based approaches first used some sentiment seed words to bootstrap based on the synonym and antonym structure of a dictionary (Hu and Liu, 2004; Valitutti et al., 2004). Later on, more sophisticated methods were proposed (Kim and Hovy, 2004; Esuli and Sebastiani, 2005; Takamura et al., 2007; Blair-Goldensohn et al., 2008; Rao and Ravichandran, 2009; Mohammad et al., 2009; Hassan and Radev, 2010; Dragut et al., 2010; Xu et al., 2010; Peng and Park, 2011; Gatti and Guerini, 2012; San Vicente et al., 2014). Corpus-based approaches build lexicons by discovering sentiment words in a large corpus. The first idea is to exploit some coordinating conjunctions (Hatzivassiloglou and McKeown, 1997; Hassan and Radev, 2010). Kanayama and Nasukawa (2006) extended this approach by introducing inter-sentential sentiment consistency. Other related work includes (Kamps et al., 2004; Kaji and Kitsuregawa, 2006; Wang et al., 2017). The second idea is to use"
2021.findings-acl.320,N16-1091,0,0.0426219,"Missing"
2021.findings-acl.320,E14-1010,0,0.0204241,"con construction. These approaches can be generally categorized as dictionary-based and corpus-based. Dictionary-based approaches first used some sentiment seed words to bootstrap based on the synonym and antonym structure of a dictionary (Hu and Liu, 2004; Valitutti et al., 2004). Later on, more sophisticated methods were proposed (Kim and Hovy, 2004; Esuli and Sebastiani, 2005; Takamura et al., 2007; Blair-Goldensohn et al., 2008; Rao and Ravichandran, 2009; Mohammad et al., 2009; Hassan and Radev, 2010; Dragut et al., 2010; Xu et al., 2010; Peng and Park, 2011; Gatti and Guerini, 2012; San Vicente et al., 2014). Corpus-based approaches build lexicons by discovering sentiment words in a large corpus. The first idea is to exploit some coordinating conjunctions (Hatzivassiloglou and McKeown, 1997; Hassan and Radev, 2010). Kanayama and Nasukawa (2006) extended this approach by introducing inter-sentential sentiment consistency. Other related work includes (Kamps et al., 2004; Kaji and Kitsuregawa, 2006; Wang et al., 2017). The second idea is to use syntactic relations between opinion and aspect words (Zhuang et al., 2006; Wang and Wang, 2008; Qiu et al., 2011; Volkova et al., 2013). The third idea is to"
2021.findings-acl.320,J11-2001,0,0.670119,"ange of words plays a crucial role in sentiment classification. As we will see in the experiment section, without identifying and correcting such domain dependent sentiment words, the performance of sentiment classification could be much poorer. Although some researchers have studied the domain-specific sentiment problem with lexicons, their focuses are quite different and their approaches are not suitable for our task. We will discuss them further in the following sections. Regarding sentiment classification, it is important to note that our work mainly aims to help lexicon-based approaches (Taboada et al., 2011). It does not directly help machine-learning (ML) or supervised learning approaches (Zhang et al., 2018) because the domain-dependent polarities of words are already reflected in the manually labeled training data. Notice that for those ML approaches, the manual annotation for each application domain is required, which is a time-consuming and laborintensive task, and is thus hard to scale up. In many real-world scenarios, lexicon-based approaches are useful and could be a better alternative (Liu, 2012). However, to effectively apply a sentiment lexicon to an application domain, the domain-pola"
2021.findings-acl.320,N07-1037,0,0.036353,"st discuss the works related to sentiment lexicons, and then domain sentiment, and finally domain sentiment with lexicons. Extensive studies have been done for sentiment lexicons and the majority of them focus on lexicon construction. These approaches can be generally categorized as dictionary-based and corpus-based. Dictionary-based approaches first used some sentiment seed words to bootstrap based on the synonym and antonym structure of a dictionary (Hu and Liu, 2004; Valitutti et al., 2004). Later on, more sophisticated methods were proposed (Kim and Hovy, 2004; Esuli and Sebastiani, 2005; Takamura et al., 2007; Blair-Goldensohn et al., 2008; Rao and Ravichandran, 2009; Mohammad et al., 2009; Hassan and Radev, 2010; Dragut et al., 2010; Xu et al., 2010; Peng and Park, 2011; Gatti and Guerini, 2012; San Vicente et al., 2014). Corpus-based approaches build lexicons by discovering sentiment words in a large corpus. The first idea is to exploit some coordinating conjunctions (Hatzivassiloglou and McKeown, 1997; Hassan and Radev, 2010). Kanayama and Nasukawa (2006) extended this approach by introducing inter-sentential sentiment consistency. Other related work includes (Kamps et al., 2004; Kaji and Kitsu"
2021.findings-acl.320,D16-1169,0,0.0518903,"Missing"
2021.findings-acl.320,N10-1119,0,0.0348538,"irst idea is to exploit some coordinating conjunctions (Hatzivassiloglou and McKeown, 1997; Hassan and Radev, 2010). Kanayama and Nasukawa (2006) extended this approach by introducing inter-sentential sentiment consistency. Other related work includes (Kamps et al., 2004; Kaji and Kitsuregawa, 2006; Wang et al., 2017). The second idea is to use syntactic relations between opinion and aspect words (Zhuang et al., 2006; Wang and Wang, 2008; Qiu et al., 2011; Volkova et al., 2013). The third idea is to use word co-occurrences for lexicon induction (Turney and Littman, 2003; Igo and Riloff, 2009; Velikovich et al., 2010; Yang et al., 2014; Rothe et al., 2016). However, our work is very different as we focus on detecting domain dependent sentiment words in a given general-purpose sentiment lexicon. Also related is the existing research about domain and context dependent sentiment. First, despite the fact that several researchers have studied context dependent sentiment words, which are based on sentences and topic/aspect context (Wilson et al., 2005; Ding et al., 2008; Choi and Cardie, 2008; Wu and Wen, 2010; Jijkoun et al., 2010; Lu et al., 2011; Zhao et al., 2012; Kessler and Sch¨utze, 2012; Teng et al., 20"
2021.findings-acl.320,P13-2090,0,0.0232734,"nd Guerini, 2012; San Vicente et al., 2014). Corpus-based approaches build lexicons by discovering sentiment words in a large corpus. The first idea is to exploit some coordinating conjunctions (Hatzivassiloglou and McKeown, 1997; Hassan and Radev, 2010). Kanayama and Nasukawa (2006) extended this approach by introducing inter-sentential sentiment consistency. Other related work includes (Kamps et al., 2004; Kaji and Kitsuregawa, 2006; Wang et al., 2017). The second idea is to use syntactic relations between opinion and aspect words (Zhuang et al., 2006; Wang and Wang, 2008; Qiu et al., 2011; Volkova et al., 2013). The third idea is to use word co-occurrences for lexicon induction (Turney and Littman, 2003; Igo and Riloff, 2009; Velikovich et al., 2010; Yang et al., 2014; Rothe et al., 2016). However, our work is very different as we focus on detecting domain dependent sentiment words in a given general-purpose sentiment lexicon. Also related is the existing research about domain and context dependent sentiment. First, despite the fact that several researchers have studied context dependent sentiment words, which are based on sentences and topic/aspect context (Wilson et al., 2005; Ding et al., 2008; C"
2021.findings-acl.320,I08-1038,0,0.0550656,"al., 2010; Peng and Park, 2011; Gatti and Guerini, 2012; San Vicente et al., 2014). Corpus-based approaches build lexicons by discovering sentiment words in a large corpus. The first idea is to exploit some coordinating conjunctions (Hatzivassiloglou and McKeown, 1997; Hassan and Radev, 2010). Kanayama and Nasukawa (2006) extended this approach by introducing inter-sentential sentiment consistency. Other related work includes (Kamps et al., 2004; Kaji and Kitsuregawa, 2006; Wang et al., 2017). The second idea is to use syntactic relations between opinion and aspect words (Zhuang et al., 2006; Wang and Wang, 2008; Qiu et al., 2011; Volkova et al., 2013). The third idea is to use word co-occurrences for lexicon induction (Turney and Littman, 2003; Igo and Riloff, 2009; Velikovich et al., 2010; Yang et al., 2014; Rothe et al., 2016). However, our work is very different as we focus on detecting domain dependent sentiment words in a given general-purpose sentiment lexicon. Also related is the existing research about domain and context dependent sentiment. First, despite the fact that several researchers have studied context dependent sentiment words, which are based on sentences and topic/aspect context ("
2021.findings-acl.320,P18-1088,1,0.891986,"Missing"
2021.findings-acl.320,D12-1015,0,0.0258114,"d Littman, 2003; Igo and Riloff, 2009; Velikovich et al., 2010; Yang et al., 2014; Rothe et al., 2016). However, our work is very different as we focus on detecting domain dependent sentiment words in a given general-purpose sentiment lexicon. Also related is the existing research about domain and context dependent sentiment. First, despite the fact that several researchers have studied context dependent sentiment words, which are based on sentences and topic/aspect context (Wilson et al., 2005; Ding et al., 2008; Choi and Cardie, 2008; Wu and Wen, 2010; Jijkoun et al., 2010; Lu et al., 2011; Zhao et al., 2012; Kessler and Sch¨utze, 2012; Teng et al., 2016; Wang et al., 2016, 2018a,b; Li et al., 2018a), our work is based on domains. Second, while the studies on transfer learning or domain adaptation for sentiment analysis deal with domain information (Bhatt et al., 2015; Yu and Jiang, 2016; Li et al., 2018b), our work does not lie in this direction. We do not have any source domain and our goal is not to transfer domain knowledge to another domain. Third, most importantly, the above works are either irrelevant to lexicons or not for detecting the sentiment discrepancy between a lexicon and the appl"
2021.findings-acl.320,D17-1059,1,0.757609,"sohn et al., 2008; Rao and Ravichandran, 2009; Mohammad et al., 2009; Hassan and Radev, 2010; Dragut et al., 2010; Xu et al., 2010; Peng and Park, 2011; Gatti and Guerini, 2012; San Vicente et al., 2014). Corpus-based approaches build lexicons by discovering sentiment words in a large corpus. The first idea is to exploit some coordinating conjunctions (Hatzivassiloglou and McKeown, 1997; Hassan and Radev, 2010). Kanayama and Nasukawa (2006) extended this approach by introducing inter-sentential sentiment consistency. Other related work includes (Kamps et al., 2004; Kaji and Kitsuregawa, 2006; Wang et al., 2017). The second idea is to use syntactic relations between opinion and aspect words (Zhuang et al., 2006; Wang and Wang, 2008; Qiu et al., 2011; Volkova et al., 2013). The third idea is to use word co-occurrences for lexicon induction (Turney and Littman, 2003; Igo and Riloff, 2009; Velikovich et al., 2010; Yang et al., 2014; Rothe et al., 2016). However, our work is very different as we focus on detecting domain dependent sentiment words in a given general-purpose sentiment lexicon. Also related is the existing research about domain and context dependent sentiment. First, despite the fact that s"
2021.findings-acl.320,H05-1044,0,0.291054,"; Qiu et al., 2011; Volkova et al., 2013). The third idea is to use word co-occurrences for lexicon induction (Turney and Littman, 2003; Igo and Riloff, 2009; Velikovich et al., 2010; Yang et al., 2014; Rothe et al., 2016). However, our work is very different as we focus on detecting domain dependent sentiment words in a given general-purpose sentiment lexicon. Also related is the existing research about domain and context dependent sentiment. First, despite the fact that several researchers have studied context dependent sentiment words, which are based on sentences and topic/aspect context (Wilson et al., 2005; Ding et al., 2008; Choi and Cardie, 2008; Wu and Wen, 2010; Jijkoun et al., 2010; Lu et al., 2011; Zhao et al., 2012; Kessler and Sch¨utze, 2012; Teng et al., 2016; Wang et al., 2016, 2018a,b; Li et al., 2018a), our work is based on domains. Second, while the studies on transfer learning or domain adaptation for sentiment analysis deal with domain information (Bhatt et al., 2015; Yu and Jiang, 2016; Li et al., 2018b), our work does not lie in this direction. We do not have any source domain and our goal is not to transfer domain knowledge to another domain. Third, most importantly, the above"
2021.findings-acl.320,C10-1134,0,0.0387179,"use word co-occurrences for lexicon induction (Turney and Littman, 2003; Igo and Riloff, 2009; Velikovich et al., 2010; Yang et al., 2014; Rothe et al., 2016). However, our work is very different as we focus on detecting domain dependent sentiment words in a given general-purpose sentiment lexicon. Also related is the existing research about domain and context dependent sentiment. First, despite the fact that several researchers have studied context dependent sentiment words, which are based on sentences and topic/aspect context (Wilson et al., 2005; Ding et al., 2008; Choi and Cardie, 2008; Wu and Wen, 2010; Jijkoun et al., 2010; Lu et al., 2011; Zhao et al., 2012; Kessler and Sch¨utze, 2012; Teng et al., 2016; Wang et al., 2016, 2018a,b; Li et al., 2018a), our work is based on domains. Second, while the studies on transfer learning or domain adaptation for sentiment analysis deal with domain information (Bhatt et al., 2015; Yu and Jiang, 2016; Li et al., 2018b), our work does not lie in this direction. We do not have any source domain and our goal is not to transfer domain knowledge to another domain. Third, most importantly, the above works are either irrelevant to lexicons or not for detectin"
2021.findings-acl.320,C10-1136,0,0.0120585,"done for sentiment lexicons and the majority of them focus on lexicon construction. These approaches can be generally categorized as dictionary-based and corpus-based. Dictionary-based approaches first used some sentiment seed words to bootstrap based on the synonym and antonym structure of a dictionary (Hu and Liu, 2004; Valitutti et al., 2004). Later on, more sophisticated methods were proposed (Kim and Hovy, 2004; Esuli and Sebastiani, 2005; Takamura et al., 2007; Blair-Goldensohn et al., 2008; Rao and Ravichandran, 2009; Mohammad et al., 2009; Hassan and Radev, 2010; Dragut et al., 2010; Xu et al., 2010; Peng and Park, 2011; Gatti and Guerini, 2012; San Vicente et al., 2014). Corpus-based approaches build lexicons by discovering sentiment words in a large corpus. The first idea is to exploit some coordinating conjunctions (Hatzivassiloglou and McKeown, 1997; Hassan and Radev, 2010). Kanayama and Nasukawa (2006) extended this approach by introducing inter-sentential sentiment consistency. Other related work includes (Kamps et al., 2004; Kaji and Kitsuregawa, 2006; Wang et al., 2017). The second idea is to use syntactic relations between opinion and aspect words (Zhuang et al., 2006; Wang and"
2021.findings-acl.320,P14-2069,0,0.0209416,"ome coordinating conjunctions (Hatzivassiloglou and McKeown, 1997; Hassan and Radev, 2010). Kanayama and Nasukawa (2006) extended this approach by introducing inter-sentential sentiment consistency. Other related work includes (Kamps et al., 2004; Kaji and Kitsuregawa, 2006; Wang et al., 2017). The second idea is to use syntactic relations between opinion and aspect words (Zhuang et al., 2006; Wang and Wang, 2008; Qiu et al., 2011; Volkova et al., 2013). The third idea is to use word co-occurrences for lexicon induction (Turney and Littman, 2003; Igo and Riloff, 2009; Velikovich et al., 2010; Yang et al., 2014; Rothe et al., 2016). However, our work is very different as we focus on detecting domain dependent sentiment words in a given general-purpose sentiment lexicon. Also related is the existing research about domain and context dependent sentiment. First, despite the fact that several researchers have studied context dependent sentiment words, which are based on sentences and topic/aspect context (Wilson et al., 2005; Ding et al., 2008; Choi and Cardie, 2008; Wu and Wen, 2010; Jijkoun et al., 2010; Lu et al., 2011; Zhao et al., 2012; Kessler and Sch¨utze, 2012; Teng et al., 2016; Wang et al., 20"
2021.findings-acl.320,D16-1023,0,0.0174922,"out domain and context dependent sentiment. First, despite the fact that several researchers have studied context dependent sentiment words, which are based on sentences and topic/aspect context (Wilson et al., 2005; Ding et al., 2008; Choi and Cardie, 2008; Wu and Wen, 2010; Jijkoun et al., 2010; Lu et al., 2011; Zhao et al., 2012; Kessler and Sch¨utze, 2012; Teng et al., 2016; Wang et al., 2016, 2018a,b; Li et al., 2018a), our work is based on domains. Second, while the studies on transfer learning or domain adaptation for sentiment analysis deal with domain information (Bhatt et al., 2015; Yu and Jiang, 2016; Li et al., 2018b), our work does not lie in this direction. We do not have any source domain and our goal is not to transfer domain knowledge to another domain. Third, most importantly, the above works are either irrelevant to lexicons or not for detecting the sentiment discrepancy between a lexicon and the application domain. Our work is most related to the following studies that involve both sentiment lexicons and domain sentiment problems. Choi and Cardie (2009) adapted the word-level polarities of a generalpurpose sentiment lexicon to a particular domain by utilizing the expression-level"
2021.findings-emnlp.337,D18-1547,0,0.149469,"To this end, we build a new dataset named NUANCED that focuses on such realistic settings, with 5.1k dialogues, 26k turns of high-quality user responses. We conduct experiments, showing both the usefulness and challenges of our problem setting. We believe NUANCED can serve as a valuable resource to push existing research from the agent-centric system to the user-centric system. The dataset is publicly available1 . 1 Introduction Conversational artificial intelligence is one of the long-standing research problems in natural language processing, such as task-oriented dialogue (Wen et al., 2017; Budzianowski et al., 2018; Hosseini-Asl et al., 2020), conversational recommendation (Sun and Zhang, 2018; Zhang et al., 2018) and chi-chat (Adiwardana et al., 2020; Roller et al., 2020) etc. However, most existing systems are agent-centric. Such systems require the users to unnaturally adapt to and even have a learning curve on the system ontology, which is largely unknown System ontology: category: Japanese, Korean, Chinese, New American, etc. alcohol: full bar, beer and wine, don’t serve attire: casual, dressy, formal wifi: free, paid, no Traditional Dataset Hello, can you help me find some good Chinese restaurants"
2021.findings-emnlp.337,P19-1360,0,0.0168325,"rom large-scale data collected from multiple workers as “commonsense” distributions. We leave modeling user-specific distributions to future work. Task-oriented dialogue systems are typically divided into several sub modules, including user intent detection (Liu and Lane, 2016; Gangadharaiah and Narayanaswamy, 2019), dialogue state tracking (Rastogi et al., 2017; Heck et al., 2020), dialogue policy learning (Peng et al., 2017; Su et al., 2016), and response generation (Dusek et al., 2018; Wen et al., 2015). More recent approaches begin to build unified models that bring the pipeline together (Chen et al., 2019; Hosseini-Asl et al., 2020). Conversational recommendation focus on combining the recommendation system with online conver- 3.2 Dataset Construction sation to capture user preference (Fu et al., 2020; We first simulate the dialogue flow with the prefSun and Zhang, 2018; Zhang et al., 2018). Previous erence distributions, then we ask the annotators to works mostly focus on learning the agent side pol- compose utterances that imply the distribution. icy to ask the right questions and make accurate recommendations, such as (Xu et al., 2020; Lei et al., 3.2.1 Dialogue Simulator 2020; Li et al., 2"
2021.findings-emnlp.337,N19-1423,0,0.0205213,"Missing"
2021.findings-emnlp.337,W18-6539,0,0.0153413,"distributions may differ among individuals which causes variances, In this work, we aim to aggregate estimated distributions from large-scale data collected from multiple workers as “commonsense” distributions. We leave modeling user-specific distributions to future work. Task-oriented dialogue systems are typically divided into several sub modules, including user intent detection (Liu and Lane, 2016; Gangadharaiah and Narayanaswamy, 2019), dialogue state tracking (Rastogi et al., 2017; Heck et al., 2020), dialogue policy learning (Peng et al., 2017; Su et al., 2016), and response generation (Dusek et al., 2018; Wen et al., 2015). More recent approaches begin to build unified models that bring the pipeline together (Chen et al., 2019; Hosseini-Asl et al., 2020). Conversational recommendation focus on combining the recommendation system with online conver- 3.2 Dataset Construction sation to capture user preference (Fu et al., 2020; We first simulate the dialogue flow with the prefSun and Zhang, 2018; Zhang et al., 2018). Previous erence distributions, then we ask the annotators to works mostly focus on learning the agent side pol- compose utterances that imply the distribution. icy to ask the right q"
2021.findings-emnlp.337,D17-1237,0,0.0161506,"the dialogue up to the current turn. Note that the preference distributions may differ among individuals which causes variances, In this work, we aim to aggregate estimated distributions from large-scale data collected from multiple workers as “commonsense” distributions. We leave modeling user-specific distributions to future work. Task-oriented dialogue systems are typically divided into several sub modules, including user intent detection (Liu and Lane, 2016; Gangadharaiah and Narayanaswamy, 2019), dialogue state tracking (Rastogi et al., 2017; Heck et al., 2020), dialogue policy learning (Peng et al., 2017; Su et al., 2016), and response generation (Dusek et al., 2018; Wen et al., 2015). More recent approaches begin to build unified models that bring the pipeline together (Chen et al., 2019; Hosseini-Asl et al., 2020). Conversational recommendation focus on combining the recommendation system with online conver- 3.2 Dataset Construction sation to capture user preference (Fu et al., 2020; We first simulate the dialogue flow with the prefSun and Zhang, 2018; Zhang et al., 2018). Previous erence distributions, then we ask the annotators to works mostly focus on learning the agent side pol- compose"
2021.findings-emnlp.337,D15-1199,0,0.0492943,"Missing"
2021.findings-emnlp.337,2020.coling-main.463,1,0.904397,"ploy professional linguists to annotate the dataset, and end up with 5.1k dialogues and 26k turns of high-quality user utterances. Our dataset captures a wide range of phenomena naturally occurring in realistic user utterances, including specified factoid knowledge, commonsense knowledge and users’ own situations. We conduct comprehensive experiments and analyses to demonstrate the challenges. We hope NUANCED can serve as a valuable resource to bridge the gap between current researches and real-world applications. 2 Related Work et al., 2014), Multi-WOZ (Budzianowski et al., 2018), MGConvRex (Xu et al., 2020), etc, the utterances from the users mostly closely follow the system ontology. While in task-oriented dialogue systems, parsing the user utterances into dialogue states is more on hard matching, in conversational recommendation systems soft matching is more encouraged since the user preferences are more salient and diverse in this type of conversations. 3 3.1 The NUANCED Dataset User Preference Modeling Given a system ontology, denote the set of all slots as {Si }, with the option values for each slot as {Vij }. Denote the current user utterance as T and dialogue context (of past turns) as C."
2021.naacl-main.124,W17-5526,0,0.0400521,"Missing"
2021.naacl-main.124,2020.sigdial-1.4,0,0.0483049,"Missing"
2021.naacl-main.124,W14-4337,0,0.0922989,"Missing"
2021.naacl-main.124,2021.ccl-1.108,0,0.0897798,"Missing"
2021.naacl-main.124,D17-2014,0,0.065602,"Missing"
2021.naacl-main.124,P19-1081,1,0.900852,"Missing"
2021.naacl-main.124,P17-1163,0,0.0464355,"Missing"
2021.naacl-main.124,2020.findings-emnlp.17,0,0.268819,"gather With modeling innovations, increasing computing supervisory data for follow-up research, we propower, and a growing number of datasets, recent pose a Human↔AI collaborative data construction years have witnessed significant improvements in approach that can effectively add suitable chit-chat the performance of both task-oriented dialogue systems and chit-chat systems (Adiwardana et al., to the beginning or end of system responses in 2020; Roller et al., 2020; Hosseini-Asl et al., 2020; existing task-oriented dialogue datasets. Specifically, we first generate chit-chat candidates for augPeng et al., 2020a). Most research on dialogue mentation using off-the-shelf pre-trained language ∗ Work done as a research intern at Facebook. The models and open-domain chatbots (Section 2.1). code and data are available at https://github.com/ facebookresearch/accentor. Next, we automatically filter out candidates that 1570 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1570–1583 June 6–11, 2021. ©2021 Association for Computational Linguistics 2 Data Construction (Pilot Labels) User ... ... System ... ... ."
2021.naacl-main.124,P19-1534,0,0.0985687,"ns facilitate open-domain chatbot development with large amounts of human-created text data generated Approach. Our proposed strategy to augment task-oriented dialogue system responses with chit- in a social context (Baumgartner et al., 2020) and supervision for a variety of desirable general qualchat is simple, compared with how it emerges in human conversations, where both functional- ities such as being engaging, personable, knowledgeable, and empathetic (Zhang et al., 2018; Diity and engagingness structurally intertwine with each other in a more complex fashion. Our pro- nan et al., 2019; Rashkin et al., 2019; Moon et al., posed Rewriter model does have a modeling ca- 2019; Wang et al., 2019; Smith et al., 2020). Our pability to compose both functions organically work bridges the two lines. We compare ACCEN but is limited due to the dataset’s target arrange- TOR-SGD and ACCENTOR-MultiWOZ with relement (i.e., concatenation of two separate compo- vant and representative dialogue datasets in Table 5. nents). Despite the limitation, our chosen design Note that very few dialogue corpora contain exof “code-separation” has practical merits: we can plicit annotations for both task-oriented and chiteasily"
2021.naacl-main.124,2020.acl-main.222,1,0.769513,"is a bad candidate, which enables the model to potentially generate a suitable chit-chat augmented response even if the output of the offthe-shelf chit-chat model is not good. 3.3 Implementation Details Unless specified otherwise, for causal language models, we use the 12-layer GPT-2 (117M parameters) as the pre-trained language model (Radford et al., 2019) and fine-tune for ten epochs. We set the batch size to 36 and the learning rate to 1 × 10−3 . We employ the SimpleTOD baseline as the off-the-shelf task-oriented dialogue model for Arranger and Rewriter. We fine-tune a 90M parameter model (Shuster et al., 2020) on each of the good chit-chat candidates with the associated dialogue history as the context from the training set of ACCENTOR-SGD following hyperparameters employed by Roller et al. (2020) and employ the resulting model as the off-the-shelf chit-chat model in Arranger and Rewriter. We use RoBERTaBASE (Liu et al., 2019) as the pre-trained language model for Arranger and fine-tune for three epochs with a learning rate of 2 × 10−5 and a batch size of 24. Arranger. This model arranges the output of an off-the-shelf task-oriented dialogue model and an off-the-shelf chit-chat model without interve"
2021.naacl-main.124,2020.acl-main.183,0,0.0688776,"oach. Our proposed strategy to augment task-oriented dialogue system responses with chit- in a social context (Baumgartner et al., 2020) and supervision for a variety of desirable general qualchat is simple, compared with how it emerges in human conversations, where both functional- ities such as being engaging, personable, knowledgeable, and empathetic (Zhang et al., 2018; Diity and engagingness structurally intertwine with each other in a more complex fashion. Our pro- nan et al., 2019; Rashkin et al., 2019; Moon et al., posed Rewriter model does have a modeling ca- 2019; Wang et al., 2019; Smith et al., 2020). Our pability to compose both functions organically work bridges the two lines. We compare ACCEN but is limited due to the dataset’s target arrange- TOR-SGD and ACCENTOR-MultiWOZ with relement (i.e., concatenation of two separate compo- vant and representative dialogue datasets in Table 5. nents). Despite the limitation, our chosen design Note that very few dialogue corpora contain exof “code-separation” has practical merits: we can plicit annotations for both task-oriented and chiteasily extend the proposed approach to an exist- chat utterances. For example, task-oriented diaing production-l"
2021.naacl-main.124,W14-4343,1,0.820832,"ls for ACCENTOR. Evaluation results show that compared with the baseline trained on the original unaugmented data, our proposed models trained on the chit-chat augmented counterpart achieve a similar task performance level and higher human evaluation scores. Task-Oriented Dialogue Systems 6 Conclusion Over the past few years, neural models have achieved remarkable success in the development of the main components of task-oriented dialogue systems, including understanding user intent, tracking dialogue states, determining system actions, and generating system responses (Henderson et al., 2013; Sun et al., 2014; Wen et al., 2015; Liu and Lane, 2016; Mrkši´c et al., 2017; Wen et al., 2017; Nouri and Hosseini-Asl, 2018; Heck et al., 2020; Chen et al., 2020). Recently, connecting separate components and building end-to-end taskoriented neural dialogue systems have attracted increasing interest (Bordes et al., 2017; Peng et al., 2020b). The most recent thread is to unify all com- Acknowledgements ponents in a single end-to-end neural model by fine-tuning a pre-trained deep language model on We thank Gerald Demeunynck for helping with multiple tasks, which leads to state-of-the-art per- the data annotati"
2021.naacl-main.124,P19-1566,0,0.0136967,"data generated Approach. Our proposed strategy to augment task-oriented dialogue system responses with chit- in a social context (Baumgartner et al., 2020) and supervision for a variety of desirable general qualchat is simple, compared with how it emerges in human conversations, where both functional- ities such as being engaging, personable, knowledgeable, and empathetic (Zhang et al., 2018; Diity and engagingness structurally intertwine with each other in a more complex fashion. Our pro- nan et al., 2019; Rashkin et al., 2019; Moon et al., posed Rewriter model does have a modeling ca- 2019; Wang et al., 2019; Smith et al., 2020). Our pability to compose both functions organically work bridges the two lines. We compare ACCEN but is limited due to the dataset’s target arrange- TOR-SGD and ACCENTOR-MultiWOZ with relement (i.e., concatenation of two separate compo- vant and representative dialogue datasets in Table 5. nents). Despite the limitation, our chosen design Note that very few dialogue corpora contain exof “code-separation” has practical merits: we can plicit annotations for both task-oriented and chiteasily extend the proposed approach to an exist- chat utterances. For example, task-oriente"
2021.naacl-main.124,D15-1199,0,0.0356499,"Missing"
2021.naacl-main.124,W13-4065,0,0.0479335,"Missing"
2021.naacl-main.124,W16-3649,0,0.0262186,"r neither of the following justifications for a good candidate: We ask annotators (crowd workers) to label each • Social: The candidate keeps the conversation of the remaining candidates from Section 2.2 as flowing smoothly by appropriately switching good or bad. Additionally, to guide the annotation to relevant topics, asking casual follow up process, improve the potential quality, and facilquestions, or engaging in social pleasantries. 1572 The design of this subcategory is inspired by the line of research that studies different social and discourse strategies in chit-chat dialogue systems (Yu et al., 2016). Task-Oriented Response Candidate ?? , ?? [?? ; ?ሚ? ?? ] [?? ; ?? ?ሚ? ] Task Bot Dialogue Context RoBERTa [?? ; ?? ] Arranger ?? actions; system response • Useful: The candidate enhances the conversation by appropriately offering opinions, commentaries, or pertinent and truthful information. Truthfulness should be established by conversational context or real world knowledge. To reduce annotation workload, if annotators have to use external resources (e.g., Wikipedia, search engines, maps) to verify information, they are instructed to label the candidate as misleading instead. The design of t"
2021.naacl-main.124,P18-1205,0,0.160005,"Shah et al., 2018; Budzianowski et al., 2018; Rastogi et al., 2020). Another line of work seeks to 4.3 Limitations and Further Discussions facilitate open-domain chatbot development with large amounts of human-created text data generated Approach. Our proposed strategy to augment task-oriented dialogue system responses with chit- in a social context (Baumgartner et al., 2020) and supervision for a variety of desirable general qualchat is simple, compared with how it emerges in human conversations, where both functional- ities such as being engaging, personable, knowledgeable, and empathetic (Zhang et al., 2018; Diity and engagingness structurally intertwine with each other in a more complex fashion. Our pro- nan et al., 2019; Rashkin et al., 2019; Moon et al., posed Rewriter model does have a modeling ca- 2019; Wang et al., 2019; Smith et al., 2020). Our pability to compose both functions organically work bridges the two lines. We compare ACCEN but is limited due to the dataset’s target arrange- TOR-SGD and ACCENTOR-MultiWOZ with relement (i.e., concatenation of two separate compo- vant and representative dialogue datasets in Table 5. nents). Despite the limitation, our chosen design Note that very"
2021.naacl-main.124,W17-5505,0,0.0180856,"es. For example, task-oriented diaing production-level virtual assistant system as a logue corpora constructed by Rastogi et al. (2020) modularized solution, and it has minimal interfer- and Moon et al. (2020) contain annotations for a ence to the user-perceived task success rate, a core few chit-chat dialogue acts, but they are limited to metric widely adapted in virtual assistant systems. light social greetings (e.g., “Thank you!”, “Good Another limitation of our work is that we only aug- Bye.”) typically at the end of each dialogue sesment responses on the system side in our dataset, sion. Zhao et al. (2017) propose to artificially augand the augmentations are independent of each ment task-oriented dialogues with randomly samother, whereas in real-life situations, users are also pled utterances from a chit-chat corpus, mainly to likely to make chit-chat, and the chit-chat between improve the out-of-domain recovery performance. the user and the system should ideally be related to Akasaki and Kaji (2017) annotate user utterances each other. We leave for future research addressing with chat/non-chat binary labels. Still, they do these limitations. not study the contextual combination of these two 15"
2021.naacl-main.378,P15-2123,1,0.796559,"se methods are mainly for avoiding CF, after learning a sequence of tasks, their final models are typically worse than learning each task separately. The proposed B-CL not only deals with CF, but also performs knowledge transfer to improve the performance of both the new and the old tasks. Lifelong Learning (LL). LL is now regarded the same as CL, but early LL mainly aimed at improving the new task learning through forward transfer without tackling CF (Silver et al., 2013; Ruvolo and Eaton, 2013; Chen and Liu, 2018). Several researchers have used LL for documentlevel sentiment classification. Chen et al. (2015) and Wang et al. (2019) proposed two Naive Bayes (NB) approaches to help improve the new task learning. A heuristic NB method was also used in (Wang et al., 2019). Xia et al. (2017) presented a LL approach based on voting of individual task classifiers. All these works do not use neural net2 Related Work works, and are not concerned with the CF problem. Continual learning (CL) has been studied extenShu et al. (2017) used LL for aspect extraction, sively (Chen and Liu, 2018; Parisi et al., 2019). To which is a different problem. Wang et al. (2018) our knowledge, no existing work has been done o"
2021.naacl-main.378,P19-1052,0,0.0164747,"ved only the new task CL for a sequence of ASC tasks, although CL of and did not deal with CF. Existing CL systems a sequence of document sentiment classification SRK (Lv et al., 2019), KAN (Ke et al., 2020b) and tasks has been done. L2PG (Qin et al., 2020) are for document sentiment Continual Learning. Existing work has mainly fo- classification, but not ASC. Ke et al. (2020a) also cused on dealing with catastrophic forgetting (CF). performed transfer in the image domain. 4747 Recently, capsule networks (Hinton et al., 2011) have been used in sentiment classification and text classification (Chen and Qian, 2019; Zhao et al., 2019). But they have not been used in CL. 3 Preliminary This section introduces BERT, Adapter-BERT and Capsule Network as they are used in our model. BERT for ASC. Due to its superior perfor- Figure 1: (A). Adapter-BERT (Houlsby et al., 2019) mance, this work uses BERT (Devlin et al., 2019) and its adapters in a transformer (Vaswani et al., 2017) and its transformer (Vaswani et al., 2017) architec- layer. An adapter is a 2-layer fully connected network ture as the base. We also adopt the ASC formula- with a skip-connection. It is added twice to each Transformer layer. Only the a"
2021.naacl-main.378,N19-1423,0,0.170901,"for Computational Linguistics work parameters learned for previous tasks, which degrades the model performance for the previous tasks (McCloskey and Cohen, 1989). In our case, (1) is also important as ASC tasks are similar, i.e., words and phrases used to express sentiments for different products/tasks are similar. To achieve the objectives, the system needs to identify the shared knowledge that can be transferred to the new task to help it learn better and the task specific knowledge that needs to be protected to avoid forgetting of previous models. Table 1 gives an example. Fine-tuned BERT (Devlin et al., 2019) is one of the most effective methods for ASC (Xu et al., 2019; Sun et al., 2019). However, our experiments show that it works very poorly for TIL. The main reason is that the fine-tuned BERT on a task/domain captures highly task specific information which is difficult to transfer to a new task. In this paper, we propose a novel model called B-CL (BERT-based Continual Learning) for ASC continual learning. The key novelty is a building block, called Continual Learning Adapter (CLA) inspired by the Adapter-BERT in (Houlsby et al., 2019). CLA leverages capsules and dynamic routing (Sabour et al.,"
2021.naacl-main.378,L18-1550,0,0.0169269,"e dynamic routing W2V (word2vec embeddings). For BERT, we is repeated for 3 iterations. For the task-specific use trainable BERT to perform ASC (see Sec. 3); module, We employ the embedding with 2000 diAdapter-BERT adapts the BERT as in (Houlsby mensions as the final and hidden layer of the TSM. et al., 2019), where only the adapter blocks are The task ID embeddings have 2000 dimensions. A trainable; W2V uses embeddings trained on the fully connected layer with softmax output is used Amazon review data in (Xu et al., 2018) using Fast- as the classification heads in the last layer of the Text (Grave et al., 2018). We adopt the ASC classi- BERT, together with the categorical cross-entropy fication network in (Xue and Li, 2018), which takes loss. We use 140 for smax in Eq. 11, dropout of both aspect term and review sentence as input. 0.5 between fully connected layers. The training of Continual Learning (CL) Baselines. CL set- BERT, Adapter-BERT and B-CL follow that of (Xu ting includes 3 baselines without dealing with for- et al., 2019). We adopt BERTBASE (uncased). The getting (WDF) and 12 baselines from 6 state-of-the maximum length of the sum of sentence and aspect art task incremental learning (TIL"
2021.naacl-main.378,D14-1181,0,0.0168915,"Table 2: Number of examples in each task or dataset. More detailed data statistics are given in the Appendix. (Tang et al., 2016), examples belonging to the conflict polarity (both positive and negative sentiments are expressed about an aspect term) are not used. Statistics of the 19 datasets are given in Table 2. 5.2 Compared Baselines 2020b) and SRK (Lv et al., 2019) are TIL methods for document sentiment classification. HAT, UCL, EWC and OWM were originally designed for image classification. We replace their original MLP or CNN image classification network with CNN for text classification (Kim, 2014). HAT (Serrà et al., 2018) is one of the best TIL methods with almost no forgetting. UCL (Ahn et al., 2019) is a latest TIL method. EWC (Kirkpatrick et al., 2016) is a popular regularization-based class incremental learning (CIL) method, which was adapted for TIL by only training on the corresponding head of the specific task ID during training and only considering the corresponding head’s prediction during testing. OWM (Zeng et al., 2019) is a state-of-theart CIL method, which we also adapt to TIL. From the 6 systems, we created 6 baselines using W2V embeddings with the aspect term added befo"
2021.naacl-main.378,D16-1021,0,0.0795779,"4751 0 (t) gl (t) (tac ) = gl ⊗ (1 − ml ). (10) Data source Liu3domain HL5domain Ding9domain SemEval14 Task/domain Speaker Router Computer Nokia6610 Nikon4300 Creative CanonG3 ApexAD CanonD500 Canon100 Diaper Hitachi Ipod Linksys MicroMP3 Nokia6600 Norton Rest. Laptop Train 352 245 283 271 162 677 228 343 118 175 191 212 153 176 484 362 194 3452 2163 Validation 44 31 35 34 20 85 29 43 15 22 24 26 19 22 61 45 24 150 150 Test 44 31 36 34 21 85 29 43 15 22 24 27 20 23 61 46 25 1120 638 Table 2: Number of examples in each task or dataset. More detailed data statistics are given in the Appendix. (Tang et al., 2016), examples belonging to the conflict polarity (both positive and negative sentiments are expressed about an aspect term) are not used. Statistics of the 19 datasets are given in Table 2. 5.2 Compared Baselines 2020b) and SRK (Lv et al., 2019) are TIL methods for document sentiment classification. HAT, UCL, EWC and OWM were originally designed for image classification. We replace their original MLP or CNN image classification network with CNN for text classification (Kim, 2014). HAT (Serrà et al., 2018) is one of the best TIL methods with almost no forgetting. UCL (Ahn et al., 2019) is a latest"
2021.naacl-main.378,P18-1088,1,0.879328,"LL for documentlevel sentiment classification. Chen et al. (2015) and Wang et al. (2019) proposed two Naive Bayes (NB) approaches to help improve the new task learning. A heuristic NB method was also used in (Wang et al., 2019). Xia et al. (2017) presented a LL approach based on voting of individual task classifiers. All these works do not use neural net2 Related Work works, and are not concerned with the CF problem. Continual learning (CL) has been studied extenShu et al. (2017) used LL for aspect extraction, sively (Chen and Liu, 2018; Parisi et al., 2019). To which is a different problem. Wang et al. (2018) our knowledge, no existing work has been done on used LL for ASC, but improved only the new task CL for a sequence of ASC tasks, although CL of and did not deal with CF. Existing CL systems a sequence of document sentiment classification SRK (Lv et al., 2019), KAN (Ke et al., 2020b) and tasks has been done. L2PG (Qin et al., 2020) are for document sentiment Continual Learning. Existing work has mainly fo- classification, but not ASC. Ke et al. (2020a) also cused on dealing with catastrophic forgetting (CF). performed transfer in the image domain. 4747 Recently, capsule networks (Hinton et al."
2021.naacl-main.378,2020.findings-emnlp.101,1,0.817775,"s do not use neural net2 Related Work works, and are not concerned with the CF problem. Continual learning (CL) has been studied extenShu et al. (2017) used LL for aspect extraction, sively (Chen and Liu, 2018; Parisi et al., 2019). To which is a different problem. Wang et al. (2018) our knowledge, no existing work has been done on used LL for ASC, but improved only the new task CL for a sequence of ASC tasks, although CL of and did not deal with CF. Existing CL systems a sequence of document sentiment classification SRK (Lv et al., 2019), KAN (Ke et al., 2020b) and tasks has been done. L2PG (Qin et al., 2020) are for document sentiment Continual Learning. Existing work has mainly fo- classification, but not ASC. Ke et al. (2020a) also cused on dealing with catastrophic forgetting (CF). performed transfer in the image domain. 4747 Recently, capsule networks (Hinton et al., 2011) have been used in sentiment classification and text classification (Chen and Qian, 2019; Zhao et al., 2019). But they have not been used in CL. 3 Preliminary This section introduces BERT, Adapter-BERT and Capsule Network as they are used in our model. BERT for ASC. Due to its superior perfor- Figure 1: (A). Adapter-BERT (Ho"
2021.naacl-main.378,N19-1242,1,0.919902,"tasks, which degrades the model performance for the previous tasks (McCloskey and Cohen, 1989). In our case, (1) is also important as ASC tasks are similar, i.e., words and phrases used to express sentiments for different products/tasks are similar. To achieve the objectives, the system needs to identify the shared knowledge that can be transferred to the new task to help it learn better and the task specific knowledge that needs to be protected to avoid forgetting of previous models. Table 1 gives an example. Fine-tuned BERT (Devlin et al., 2019) is one of the most effective methods for ASC (Xu et al., 2019; Sun et al., 2019). However, our experiments show that it works very poorly for TIL. The main reason is that the fine-tuned BERT on a task/domain captures highly task specific information which is difficult to transfer to a new task. In this paper, we propose a novel model called B-CL (BERT-based Continual Learning) for ASC continual learning. The key novelty is a building block, called Continual Learning Adapter (CLA) inspired by the Adapter-BERT in (Houlsby et al., 2019). CLA leverages capsules and dynamic routing (Sabour et al., 2017) to identify previous tasks that are similar to the new"
2021.naacl-main.378,P18-1234,0,0.0197275,"inable BERT to perform ASC (see Sec. 3); module, We employ the embedding with 2000 diAdapter-BERT adapts the BERT as in (Houlsby mensions as the final and hidden layer of the TSM. et al., 2019), where only the adapter blocks are The task ID embeddings have 2000 dimensions. A trainable; W2V uses embeddings trained on the fully connected layer with softmax output is used Amazon review data in (Xu et al., 2018) using Fast- as the classification heads in the last layer of the Text (Grave et al., 2018). We adopt the ASC classi- BERT, together with the categorical cross-entropy fication network in (Xue and Li, 2018), which takes loss. We use 140 for smax in Eq. 11, dropout of both aspect term and review sentence as input. 0.5 between fully connected layers. The training of Continual Learning (CL) Baselines. CL set- BERT, Adapter-BERT and B-CL follow that of (Xu ting includes 3 baselines without dealing with for- et al., 2019). We adopt BERTBASE (uncased). The getting (WDF) and 12 baselines from 6 state-of-the maximum length of the sum of sentence and aspect art task incremental learning (TIL) methods deal- is set to 128. We use Adam optimizer and set the ing with forgetting. WDF baselines greedily learn"
2021.naacl-main.378,P19-1150,0,0.0180691,"CL for a sequence of ASC tasks, although CL of and did not deal with CF. Existing CL systems a sequence of document sentiment classification SRK (Lv et al., 2019), KAN (Ke et al., 2020b) and tasks has been done. L2PG (Qin et al., 2020) are for document sentiment Continual Learning. Existing work has mainly fo- classification, but not ASC. Ke et al. (2020a) also cused on dealing with catastrophic forgetting (CF). performed transfer in the image domain. 4747 Recently, capsule networks (Hinton et al., 2011) have been used in sentiment classification and text classification (Chen and Qian, 2019; Zhao et al., 2019). But they have not been used in CL. 3 Preliminary This section introduces BERT, Adapter-BERT and Capsule Network as they are used in our model. BERT for ASC. Due to its superior perfor- Figure 1: (A). Adapter-BERT (Houlsby et al., 2019) mance, this work uses BERT (Devlin et al., 2019) and its adapters in a transformer (Vaswani et al., 2017) and its transformer (Vaswani et al., 2017) architec- layer. An adapter is a 2-layer fully connected network ture as the base. We also adopt the ASC formula- with a skip-connection. It is added twice to each Transformer layer. Only the adapters (yellow boxe"
2021.naacl-main.378,P17-2023,1,0.780636,"orward transfer without tackling CF (Silver et al., 2013; Ruvolo and Eaton, 2013; Chen and Liu, 2018). Several researchers have used LL for documentlevel sentiment classification. Chen et al. (2015) and Wang et al. (2019) proposed two Naive Bayes (NB) approaches to help improve the new task learning. A heuristic NB method was also used in (Wang et al., 2019). Xia et al. (2017) presented a LL approach based on voting of individual task classifiers. All these works do not use neural net2 Related Work works, and are not concerned with the CF problem. Continual learning (CL) has been studied extenShu et al. (2017) used LL for aspect extraction, sively (Chen and Liu, 2018; Parisi et al., 2019). To which is a different problem. Wang et al. (2018) our knowledge, no existing work has been done on used LL for ASC, but improved only the new task CL for a sequence of ASC tasks, although CL of and did not deal with CF. Existing CL systems a sequence of document sentiment classification SRK (Lv et al., 2019), KAN (Ke et al., 2020b) and tasks has been done. L2PG (Qin et al., 2020) are for document sentiment Continual Learning. Existing work has mainly fo- classification, but not ASC. Ke et al. (2020a) also cused"
2021.naacl-main.378,N19-1035,0,0.0413586,"ades the model performance for the previous tasks (McCloskey and Cohen, 1989). In our case, (1) is also important as ASC tasks are similar, i.e., words and phrases used to express sentiments for different products/tasks are similar. To achieve the objectives, the system needs to identify the shared knowledge that can be transferred to the new task to help it learn better and the task specific knowledge that needs to be protected to avoid forgetting of previous models. Table 1 gives an example. Fine-tuned BERT (Devlin et al., 2019) is one of the most effective methods for ASC (Xu et al., 2019; Sun et al., 2019). However, our experiments show that it works very poorly for TIL. The main reason is that the fine-tuned BERT on a task/domain captures highly task specific information which is difficult to transfer to a new task. In this paper, we propose a novel model called B-CL (BERT-based Continual Learning) for ASC continual learning. The key novelty is a building block, called Continual Learning Adapter (CLA) inspired by the Adapter-BERT in (Houlsby et al., 2019). CLA leverages capsules and dynamic routing (Sabour et al., 2017) to identify previous tasks that are similar to the new task and exploit th"
2021.naacl-main.448,D18-1547,0,0.438485,"y incorporate a pre-trained 5640 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5640–5648 June 6–11, 2021. ©2021 Association for Computational Linguistics Slot Type Figure 2: Slot description examples. seq2seq model (e.g., T5 (Raffel et al., 2020)) without any task-specific modification. To further enhance the model’s cross-domain transferability, we propose Slot Type Informed Descriptions that capture the shared information of different slots. Experimental results on the MultiWOZ benchmark (Budzianowski et al., 2018) suggest that 1) our model achieves significantly higher joint goal accuracy compared to existing results in zero-shot cross domain DST; 2) models using the proposed slot description formulation substantially outperform those using other slot description variants. Our contributions are summarized as the following: • We propose a simple yet novel generative DST model based on T5 that significantly improves existing zero-shot cross-domain DST results; • We investigate the effectiveness of different slot description formulations. To the best of our knowledge, this is the first work that comprehen"
2021.naacl-main.448,N18-2074,0,0.036191,"wledge transfer among different slots. We construct a template for each slot type that follows &quot;[slot type] of [slot] of the [domain]&quot;. We denote such a slot description as Slot Type. More details are available in Appendix A.1. 4 4.1 vi = Seq2seq(Ct , si ). (1) The learning objective of this generation process is minimizing the negative log-likelihood of vi given Ct and si , that is, L=− n X log p(vi |Ct , si ), (2) i where n is the number of slots to be tracked. We initialize the model parameters with T5 (Raffel et al., 2020), an encoder-decoder Transformer with relative position embeddings (Shaw et al., 2018) pre-trained on a massive amount of English text. We denote our model as T5DST. To incorporate slot descriptions into T5DST, we replace the slot name with its corresponding slot description as the model input. 3.2 Slot Type Informed Descriptions Experiments Dataset and Evaluation We evaluate the proposed method on the MultiWOZ 2.0 dataset (Budzianowski et al., 2018), which has 7 domains. We use the pre-processing and evaluation setup from Wu et al. (2019), where restaurant, train, attraction, hotel, and taxi domains are used for training, as the test set only contains these 5 domains. In the z"
2021.naacl-main.448,2020.emnlp-main.66,0,0.0402704,"et al. (2019, 2020) formulated DST as a question answering problem by casting a slot name into questions. However, these works did not show the effectiveness of slot descriptions, by comparing the performance of models with and without them. There is no study on how to construct slot descriptions. In this paper, we aim to fill this research gap by providing an empirical study on the different slot description formulations. Dialogue State Tracking has been of broad interest to the dialogue research community (Williams and Young, 2007; Williams et al., 2014; Heck et al., 2020; Liu et al., 2020; Wu et al., 2020; Madotto et al., 2020). Current 3 Methodology state-of-the-art models (Chen et al., 2020; Lin et al., 2020; Heck et al., 2020; Hosseini-Asl et al., 3.1 T5DST 2020; Ye et al., 2021; Li et al., 2020) trained with extensive annotated data have been shown The design of our model follows the basis of genpromising performance in complex multi-domain erative question answering models. As illustrated conversations (Budzianowski et al., 2018). How- in Figure 1, given a dialogue history which conever, collecting large amounts of data for every sists of an alternating set of utterances from two 5641 Mod"
2021.naacl-main.448,P19-1078,1,0.899837,"tate tracking (DST) is an essential component of task-oriented dialogue systems that tracks users’ requirements over multi-turn conversations. A popular formulation of the dialogue state is in the form of a list of slot-value pairs. In DST, tracking unseen slots in a new domain, a.k.a. zeroshot domain adaptation, is a significant challenge, ∗ Work done during internship at Facebook since the model has never seen in-domain training samples. There are two main lines of work to tackle this problem. The first proposes domain transferable models using copy mechanisms or ontology graph information (Wu et al., 2019; Zhou and Small, 2019). A limitation of such models is that they may not fully leverage pre-trained language models due to the specialized model architecture. The second line of work uses slot-descriptions as input to the model to facilitate the slot understanding (Rastogi et al., 2020). However, the provided slot descriptions are collected by crowd sourced human annotators and might be inconsistent among different domains. In general, the optimal approach for constructing slot descriptions in zero-shot settings remains unexplored. In this work, we tackle the challenge of zeroshot cross-domai"
2021.naacl-main.448,2020.nlp4convai-1.13,0,0.263341,"Missing"
2021.sigdial-1.31,2020.lrec-1.74,1,0.842962,"Missing"
2021.sigdial-1.31,C18-1300,0,0.0561156,"-2, the international ISO standard for DA annotations (Bunt et al., 2010). It provides a domain- and task-independent DA 277 1 Many studies show Fitbit can help increase physical activity (Ringeval et al., 2020), but here we are interested in approaches with dialogue capabilities. schema with 56 DAs organized into nine dimensions. Paul et al. (2019) proposed a universal DA schema by aligning tags from different datasets such as the Dialogue State Tracking Challenge 2 (Henderson et al., 2014), Google Simulated Dialogue (Shah et al., 2018), and MultiWOZ 2.0 (Budzianowski et al., 2018) together. Mezza et al. (2018) reduced the ISO schema to 10 DAs and showed their applicability to datasets like Switchboard (Leech and Weisser, 2003), MapTask (Anderson et al., 1991), and VerbMobil (Alexandersson et al., 1998). On account of not reinventing the wheel, we used the ISO schema for our dialogues (Bunt et al., 2017a). Since many of the DAs didn’t apply to our dataset such as turn take/grab, stalling, and pausing, we reduced the schema to only 12 DAs, mostly following Mezza et al. (2018). Early work for DA modeling involved treating the task as a structured prediction or text classification problem. Stolcke et a"
2021.sigdial-1.31,P02-1040,0,0.109493,"ve dip in performance for the repetition attribute. Therefore, we adopt the goal extraction pipeline that uses both dialogue acts (BERT) and phases as shown in Figure 2. Given the small performance difference on time and location between phases and DAs, to process messages in real-time, we will use only the SMART+DA (BERT) model, as it only requires the dialogue history. Additionally, to generate messages in real-time, the current Goal-c could be used. E.g., if location is null in Goal-c, the coach can ask for location next. We previously showed in Gupta et al. (2020b) that metrics like BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) are not appropriate for our extraction-based goal summaries as they are sensitive to exact word match (Reiter, 2018). That is, if a given word, say ‘two’, is classified as days number instead of distance, they will still output a high score as ‘two’ is in the reference summary. BLEU also favors shorter sentences, so missing attributes lead to a higher score. Human Evaluation Evaluating models with automatic metrics is important, but it is equally important to evaluate the usefulness and usability of these models with their users. We performed a pilot evaluation with the"
2021.sigdial-1.31,W16-0305,0,0.0692564,"Missing"
2021.sigdial-1.31,N18-1202,0,0.0244406,"st: 22/5 patients). We experimented with both sequential and nonsequential classifiers such as CRF, Structured Perceptron (SP) (Collins, 2002), Logistic Regression (LR) (Grimm and Yarnold, 1995), Support Vector Machines (SVM) (Cortes and Vapnik, 1995), and Decision Trees (DT) (Quinlan, 1986). For features, we tried different combinations of - the current word, left and right context words, part-ofspeech (POS) tags, left and right context words’ POS tags, SpaCy named entity recognizer (NER), current word’s phase, and ELMo word embeddings Figure 3: Dialogue acts distribution (15 weeks of data) (Peters et al., 2018). The CRF, SP, and LR models performed the best without a significant difference between them using the current and context words, ELMo embeddings, and SpaCy NER. We decided to use the CRF model with an F1 macro score of 0.81. In our previous work (Gupta et al., 2020b), we used models with word2vec embeddings (Mikolov et al., 2013) but found ELMo embeddings to perform better. 4 The phase and SMART attribute models are described in Gupta et al. (2020b) and briefly summarized here and in the appendix. 280 4.2 Modeling Dialogue Acts For DA prediction, we annotated 15 weeks (377 messages, 655 utte"
2021.sigdial-1.31,J18-3002,0,0.0135433,"as shown in Figure 2. Given the small performance difference on time and location between phases and DAs, to process messages in real-time, we will use only the SMART+DA (BERT) model, as it only requires the dialogue history. Additionally, to generate messages in real-time, the current Goal-c could be used. E.g., if location is null in Goal-c, the coach can ask for location next. We previously showed in Gupta et al. (2020b) that metrics like BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) are not appropriate for our extraction-based goal summaries as they are sensitive to exact word match (Reiter, 2018). That is, if a given word, say ‘two’, is classified as days number instead of distance, they will still output a high score as ‘two’ is in the reference summary. BLEU also favors shorter sentences, so missing attributes lead to a higher score. Human Evaluation Evaluating models with automatic metrics is important, but it is equally important to evaluate the usefulness and usability of these models with their users. We performed a pilot evaluation with the help of three health coaches to answer two main questions: (1) What is the health coaches’ understanding of a correct goal summary? and (2)"
2021.sigdial-1.31,N18-3006,1,0.809605,"in different domains. One such effort led to the formation of the ISO 24617-2, the international ISO standard for DA annotations (Bunt et al., 2010). It provides a domain- and task-independent DA 277 1 Many studies show Fitbit can help increase physical activity (Ringeval et al., 2020), but here we are interested in approaches with dialogue capabilities. schema with 56 DAs organized into nine dimensions. Paul et al. (2019) proposed a universal DA schema by aligning tags from different datasets such as the Dialogue State Tracking Challenge 2 (Henderson et al., 2014), Google Simulated Dialogue (Shah et al., 2018), and MultiWOZ 2.0 (Budzianowski et al., 2018) together. Mezza et al. (2018) reduced the ISO schema to 10 DAs and showed their applicability to datasets like Switchboard (Leech and Weisser, 2003), MapTask (Anderson et al., 1991), and VerbMobil (Alexandersson et al., 1998). On account of not reinventing the wheel, we used the ISO schema for our dialogues (Bunt et al., 2017a). Since many of the DAs didn’t apply to our dataset such as turn take/grab, stalling, and pausing, we reduced the schema to only 12 DAs, mostly following Mezza et al. (2018). Early work for DA modeling involved treating the"
2021.sigdial-1.31,2020.emnlp-main.425,0,0.0307855,"rs from applying state-of-the-art deep learning techniques and end-to-end approaches for building dialogue agents that require large datasets. Researchers like Althoff et al. (2016) and Zhang and Danescu-Niculescu-Mizil (2020) were able to access a large counseling conversations dataset from the Crisis Text Line (CTL), a free 24/7 crisis counseling platform for a mental health crisis, for computational analysis through a fellowship program with CTL. Online sources such as Reddit have also been used for analyzing empathy in conversations, but consist of question-answer pairs and not dialogues (Sharma et al., 2020). Lastly, Shen et al. (2020) used the MI dataset collected by P´erezRosas et al. (2016) to build a model that can generate sample responses of type reflection to assist counselors. As far as we know, no existing work has focused on building a dialogue agent involving coaching components such as negotiation and feedback for promoting PA using SMART goal setting. Dialogue act (DA) modeling. This task involves finding the intent behind the speaker’s utterance in a dialogue such as request, clarification, and acknowledgment. The DA tags may differ depending on the dialogue’s domain. E.g., negotiat"
2021.sigdial-1.31,2020.sigdial-1.2,0,0.0792036,"Missing"
2021.sigdial-1.31,J00-3003,0,0.433466,"Missing"
2021.sigdial-1.31,2020.emnlp-main.66,0,0.0815742,"ied to the task (Kumar et al., 2020; Anikina and Kruijff-Korbayova, 2019). Convolutional Neural Networks (CNN) were also used for intent classification of a query (Hashemi et al., 2016). However, queries can be treated as individual sentences without any context. Given context is important in a dialogue, we experiment with approaches that can take dialogue history into account such as Conditional Random Fields (CRF) (Lafferty et al., 2001) and recent transformer-based BERT (Bidirectional Encoder Representations from Transformers) models (Devlin et al., 2019). In particular, we use the work by Wu et al. (2020) and Cohan et al. (2019) as the guide for our BERT-based DA prediction models. 3 Datasets and Annotations No health coaching dialogue dataset is publicly available. Therefore, to understand the feasibility of using SMS for health coaching, the challenges with patient recruitment and retention, and conversation flow between the coaches and the patients, we collected two health coaching datasets (Dataset 1 and Dataset 2; Dataset 2 is available upon request2 , while Dataset 1 cannot be shared due to lack of subject consent). To collect Dataset 1, we hired one health coach who coached 28 patients,"
2021.sigdial-1.31,2020.acl-main.470,0,0.027333,"to reflect on their PA performance with a series of follow-up questions, however, no goal-setting is involved.1 Interactions in these dialogue agents are still mostly scripted. Dynamic interactions require large datasets that are unfortunately scarce in the health domain due to privacy reasons. Moreover, collecting and labeling data particularly in real scenarios is resource intensive. This limits the researchers from applying state-of-the-art deep learning techniques and end-to-end approaches for building dialogue agents that require large datasets. Researchers like Althoff et al. (2016) and Zhang and Danescu-Niculescu-Mizil (2020) were able to access a large counseling conversations dataset from the Crisis Text Line (CTL), a free 24/7 crisis counseling platform for a mental health crisis, for computational analysis through a fellowship program with CTL. Online sources such as Reddit have also been used for analyzing empathy in conversations, but consist of question-answer pairs and not dialogues (Sharma et al., 2020). Lastly, Shen et al. (2020) used the MI dataset collected by P´erezRosas et al. (2016) to build a model that can generate sample responses of type reflection to assist counselors. As far as we know, no exi"
C08-1031,W06-1602,0,0.0228684,"ns depending on what features it is applied to. For example, in the camera domain, “long” is positive in “the battery life is very long” but negative in “it takes a long time to focus”. Thus, we should consider both the feature and the opinion word rather than only the opinion word. Second, we focus on studying opinionated comparative words. Third, our technique is quite different as we utilize readily available external opinion sources. As discussed in the introduction, a closely related work to ours is (Jindal and Liu 2006). However, it does not find which entities are preferred by authors. Bos and Nissim (2006) proposes a method to extract some useful items from superlative sentences. Fiszman et al (2007) studied the problem of identifying which entity has more of certain features in comparative sentences. It does not find which entity is preferred. 3 Problem Statement Definition (entity and feature): An entity is the name of a person, a product, a company, a location, etc, under comparison in a comparative sentence. A feature is a part or attribute of the entity that is being compared. For example, in the sentence, “Camera X’s battery life is longer than that of Camera Y”, “Camera X” and “Camera Y”"
C08-1031,E06-1025,0,0.0226356,"Missing"
C08-1031,W07-1018,0,0.0883074,"tive in “the battery life is very long” but negative in “it takes a long time to focus”. Thus, we should consider both the feature and the opinion word rather than only the opinion word. Second, we focus on studying opinionated comparative words. Third, our technique is quite different as we utilize readily available external opinion sources. As discussed in the introduction, a closely related work to ours is (Jindal and Liu 2006). However, it does not find which entities are preferred by authors. Bos and Nissim (2006) proposes a method to extract some useful items from superlative sentences. Fiszman et al (2007) studied the problem of identifying which entity has more of certain features in comparative sentences. It does not find which entity is preferred. 3 Problem Statement Definition (entity and feature): An entity is the name of a person, a product, a company, a location, etc, under comparison in a comparative sentence. A feature is a part or attribute of the entity that is being compared. For example, in the sentence, “Camera X’s battery life is longer than that of Camera Y”, “Camera X” and “Camera Y” are entities and “battery life” is the camera feature. Types of Comparatives 1) Non-equal grada"
C08-1031,C04-1200,0,0.571299,"ons from comparative sentences, i.e., identifying preferred entities of the author. This paper studies this problem, and proposes a technique to solve the problem. Our experiments using comparative sentences from product reviews and forum posts show that the approach is effective. 1 Introduction In the past few years, there was a growing interest in mining opinions in the user-generated content (UGC) on the Web, e.g., customer reviews, forum posts, and blogs. One major focus is sentiment classification and opinion mining (e.g., Pang et al 2002; Turney 2002; Hu and Liu 2004; Wilson et al 2004; Kim and Hovy 2004; Popescu and Etzioni 2005) © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0/). Some rights reserved. However, these studies mainly center on direct opinions or sentiments expressed on entities. Little study has been done on comparisons, which represent another type of opinion-bearing text. Comparisons are related to but are also quite different from direct opinions. For example, a typical direct opinion sentence is “the picture quality of Camera X is great”, while a typical comparative sente"
C08-1031,W02-1011,0,0.0195466,"arative sentence. However, there is still no study of mining opinions from comparative sentences, i.e., identifying preferred entities of the author. This paper studies this problem, and proposes a technique to solve the problem. Our experiments using comparative sentences from product reviews and forum posts show that the approach is effective. 1 Introduction In the past few years, there was a growing interest in mining opinions in the user-generated content (UGC) on the Web, e.g., customer reviews, forum posts, and blogs. One major focus is sentiment classification and opinion mining (e.g., Pang et al 2002; Turney 2002; Hu and Liu 2004; Wilson et al 2004; Kim and Hovy 2004; Popescu and Etzioni 2005) © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0/). Some rights reserved. However, these studies mainly center on direct opinions or sentiments expressed on entities. Little study has been done on comparisons, which represent another type of opinion-bearing text. Comparisons are related to but are also quite different from direct opinions. For example, a typical direct opinion sentence is “the pict"
C08-1031,W03-1014,0,0.0835571,"e. A new association measure is also proposed to suit our purpose. Our experiment results show that it can achieve high precision and recall. 2 Related Work Sentiment analysis has been studied by many researchers recently. Two main directions are sentiment classification at the document and sentence levels, and feature-based opinion mining. Sentiment classification at the document level investigates ways to classify each evaluative document (e.g., product review) as positive or negative (Pang et al 2002; Turney 2002). Sentiment classification at the sentence-level has also been studied (e.g., Riloff and Wiebe 2003; Kim and Hovy 2004; Wilson et al 2004; Gamon et al 242 2005; Stoyanov and Cardie 2006). These works are different from ours as we study comparatives. The works in (Hu and Liu 2004; Liu et al 2005; Popescu and Etzioni 2005; Mei et al 2007) perform opinion mining at the feature level. The task involves (1) extracting entity features (e.g., “picture quality” and “battery life” in a camera review) and (2) finding orientations (positive, negative or neutral) of opinions expressed on the features by reviewers. Again, our work is different because we deal with comparisons. Discovering orientations o"
C08-1031,W06-0302,0,0.0354858,"esults show that it can achieve high precision and recall. 2 Related Work Sentiment analysis has been studied by many researchers recently. Two main directions are sentiment classification at the document and sentence levels, and feature-based opinion mining. Sentiment classification at the document level investigates ways to classify each evaluative document (e.g., product review) as positive or negative (Pang et al 2002; Turney 2002). Sentiment classification at the sentence-level has also been studied (e.g., Riloff and Wiebe 2003; Kim and Hovy 2004; Wilson et al 2004; Gamon et al 242 2005; Stoyanov and Cardie 2006). These works are different from ours as we study comparatives. The works in (Hu and Liu 2004; Liu et al 2005; Popescu and Etzioni 2005; Mei et al 2007) perform opinion mining at the feature level. The task involves (1) extracting entity features (e.g., “picture quality” and “battery life” in a camera review) and (2) finding orientations (positive, negative or neutral) of opinions expressed on the features by reviewers. Again, our work is different because we deal with comparisons. Discovering orientations of context dependent opinion comparative words is related to identifying domain opinion"
C08-1031,P02-1053,0,0.00274321,"Missing"
C08-1031,H05-2017,0,\N,Missing
C08-1031,H05-1043,0,\N,Missing
C08-1031,W06-1642,0,\N,Missing
C08-1031,P06-1134,0,\N,Missing
C10-1031,E06-1027,0,0.107901,"Missing"
C10-1031,C88-1021,0,0.245064,"this feature work, we need to identify what opinion words are usually associated with what objects or attributes, which means that the system needs to discover such relationships from the corpus. These two features give significant boost to the coreference resolution accuracy. Experimental results based on three corpora demonstrate the effectiveness of the proposed features. 2 Related Work Coreference resolution is an extensively studied NLP problem (e.g., Morton, 2000; Ng and Cardie, 2002; Gasperin and Briscoe, 2008). Early knowledge-based approaches were domain and 269 linguistic dependent (Carbonell and Brown 1988), where researchers focused on diverse lexical and grammatical properties of referring expressions (Soon et al., 2001; Ng and Cardie, 2002; Zhou et al., 2004). Recent research relied more on exploiting semantic information. For example, Yang et al. (2005) used the semantic compatibility information, and Yang and Su (2007) used automatically discovered patterns integrated with semantic relatedness information, while Ng (2007) employed semantic class knowledge acquired from the Penn Treebank. Versley et al. (2008) used several kernel functions in learning. Perhaps, the most popular approach is b"
C10-1031,E06-1025,0,0.0589538,"Missing"
C10-1031,C08-1033,0,0.0142456,"arly, we know that “It” refers to “Canon camera” because “picture quality” cannot be expensive. To make this feature work, we need to identify what opinion words are usually associated with what objects or attributes, which means that the system needs to discover such relationships from the corpus. These two features give significant boost to the coreference resolution accuracy. Experimental results based on three corpora demonstrate the effectiveness of the proposed features. 2 Related Work Coreference resolution is an extensively studied NLP problem (e.g., Morton, 2000; Ng and Cardie, 2002; Gasperin and Briscoe, 2008). Early knowledge-based approaches were domain and 269 linguistic dependent (Carbonell and Brown 1988), where researchers focused on diverse lexical and grammatical properties of referring expressions (Soon et al., 2001; Ng and Cardie, 2002; Zhou et al., 2004). Recent research relied more on exploiting semantic information. For example, Yang et al. (2005) used the semantic compatibility information, and Yang and Su (2007) used automatically discovered patterns integrated with semantic relatedness information, while Ng (2007) employed semantic class knowledge acquired from the Penn Treebank. Ve"
C10-1031,P97-1023,0,0.0917109,"Missing"
C10-1031,W06-1642,0,0.0209765,"Missing"
C10-1031,C04-1200,0,0.10504,"Missing"
C10-1031,P07-1055,0,0.0355925,"Missing"
C10-1031,P02-1014,0,0.527308,"picture quality”. Clearly, we know that “It” refers to “Canon camera” because “picture quality” cannot be expensive. To make this feature work, we need to identify what opinion words are usually associated with what objects or attributes, which means that the system needs to discover such relationships from the corpus. These two features give significant boost to the coreference resolution accuracy. Experimental results based on three corpora demonstrate the effectiveness of the proposed features. 2 Related Work Coreference resolution is an extensively studied NLP problem (e.g., Morton, 2000; Ng and Cardie, 2002; Gasperin and Briscoe, 2008). Early knowledge-based approaches were domain and 269 linguistic dependent (Carbonell and Brown 1988), where researchers focused on diverse lexical and grammatical properties of referring expressions (Soon et al., 2001; Ng and Cardie, 2002; Zhou et al., 2004). Recent research relied more on exploiting semantic information. For example, Yang et al. (2005) used the semantic compatibility information, and Yang and Su (2007) used automatically discovered patterns integrated with semantic relatedness information, while Ng (2007) employed semantic class knowledge acquir"
C10-1031,D08-1067,0,0.0144845,"-looking centers, which consist of those discourse entities that can be interpreted by linguistic expressions in the sentences. Fang et al. (2009) employed the centering theory to replace the grammatical role features with semantic role information and showed superior accuracy performances. Ding et al. (2009) studied the entity assignment problem. They tried to discover the product names discussed in forum posts and assign the product entities to each sentence. The work did not deal with product attributes. Unsupervised approaches were also applied due to the cost of annotating large corpora. Ng (2008) used an Expectation-Maximization (EM) algorithm, and Poon and Domingos (2008) applied Markov Logic Network (MLN). Another related work is the indirect anaphora, known as bridging reference. It arises when an entity is part of an earlier mention. Resolving indirect anaphora requires background knowledge (e.g. Fan et al., 2005), and it is thus not in the scope of this paper. Our work differs from these existing studies as we work in the context of opinion mining, which gives us extra features to enable us to perform the task more effectively. 3 Problem of Object and Attribute Coreference Resolu"
C10-1031,W02-1011,0,0.0190579,"Missing"
C10-1031,H05-1043,0,0.145502,"For simplicity, attribute is used to denote both component and attribute in this paper. Thus, we have the two concepts, object and attribute. 3.1 Objective Task objective: To carry out coreference resolution on objects and attributes in opinion text. As we discussed in the introduction section, coreference resolution on objects and attributes is important because they are the core entities on which people express opinions. Due to our objective, we do not evaluate other types of coreferences. We assume that objects and entities have been discovered by an existing system (e.g., Hu and Liu 2004, Popescu and Etzioni 2005). Recall that a coreference relation holds between two noun phrases if they refer to the same entity. For example, we have the following three consecutive sentences: s1: I love the nokia n95 but not sure how good the flash would be? s2: and also it is quite expensive so anyone got any ideas? s3: I will be going on contract so as long as i can get a good deal of it. “it” in s2 refers to the entity “the nokia n95” in s1. In this case, we call “the nokia n95” the antecedent and pronoun “it” in s2 the anaphor. The referent of “it” in s3 is also “the nokia n95”, so the “it” in s3 is coreferent with"
C10-1031,J01-4004,0,0.328716,"that the system needs to discover such relationships from the corpus. These two features give significant boost to the coreference resolution accuracy. Experimental results based on three corpora demonstrate the effectiveness of the proposed features. 2 Related Work Coreference resolution is an extensively studied NLP problem (e.g., Morton, 2000; Ng and Cardie, 2002; Gasperin and Briscoe, 2008). Early knowledge-based approaches were domain and 269 linguistic dependent (Carbonell and Brown 1988), where researchers focused on diverse lexical and grammatical properties of referring expressions (Soon et al., 2001; Ng and Cardie, 2002; Zhou et al., 2004). Recent research relied more on exploiting semantic information. For example, Yang et al. (2005) used the semantic compatibility information, and Yang and Su (2007) used automatically discovered patterns integrated with semantic relatedness information, while Ng (2007) employed semantic class knowledge acquired from the Penn Treebank. Versley et al. (2008) used several kernel functions in learning. Perhaps, the most popular approach is based on supervised learning. In this approach, the system learns a pairwise function to predict whether a pair of nou"
C10-1031,W06-1640,0,0.0270972,"true if the noun phrase starts with the word “this”, “that”, “these”, or “those”; false otherwise. Number agreement feature: If the candidate antecedent and anaphor are both singular or both plural, the value is true; otherwise false. Both-proper-name feature: If both the candidates are proper nouns, which are determined by capitalization, return true; otherwise false. Alias feature: It is true if one candidate is an alias of the other or vice versa; false otherwise. Ng and Cardie (2002) expanded the feature set of Soon et al. (2001) from 12 to 53 features. The system was further improved by Stoyanov and Cardie (2006) who gave a partially supervised clustering algorithm and tackled the problem of opinion source coreference resolution. Centering theory is a linguistic approach tried to model the variation or shift of the main subject of the discourse in focus. In (Grosz et al., 1995; Tetreault, 2001), centering theory was applied to sort the antecedent candidates based on the ranking of the forward-looking centers, which consist of those discourse entities that can be interpreted by linguistic expressions in the sentences. Fang et al. (2009) employed the centering theory to replace the grammatical role feat"
C10-1031,P08-1036,0,0.00872233,"Missing"
C10-1031,J01-4003,0,0.0124709,"ouns, which are determined by capitalization, return true; otherwise false. Alias feature: It is true if one candidate is an alias of the other or vice versa; false otherwise. Ng and Cardie (2002) expanded the feature set of Soon et al. (2001) from 12 to 53 features. The system was further improved by Stoyanov and Cardie (2006) who gave a partially supervised clustering algorithm and tackled the problem of opinion source coreference resolution. Centering theory is a linguistic approach tried to model the variation or shift of the main subject of the discourse in focus. In (Grosz et al., 1995; Tetreault, 2001), centering theory was applied to sort the antecedent candidates based on the ranking of the forward-looking centers, which consist of those discourse entities that can be interpreted by linguistic expressions in the sentences. Fang et al. (2009) employed the centering theory to replace the grammatical role features with semantic role information and showed superior accuracy performances. Ding et al. (2009) studied the entity assignment problem. They tried to discover the product names discussed in forum posts and assign the product entities to each sentence. The work did not deal with product"
C10-1031,P02-1053,0,0.00561162,"Missing"
C10-1031,C08-1121,0,0.0134803,"8). Early knowledge-based approaches were domain and 269 linguistic dependent (Carbonell and Brown 1988), where researchers focused on diverse lexical and grammatical properties of referring expressions (Soon et al., 2001; Ng and Cardie, 2002; Zhou et al., 2004). Recent research relied more on exploiting semantic information. For example, Yang et al. (2005) used the semantic compatibility information, and Yang and Su (2007) used automatically discovered patterns integrated with semantic relatedness information, while Ng (2007) employed semantic class knowledge acquired from the Penn Treebank. Versley et al. (2008) used several kernel functions in learning. Perhaps, the most popular approach is based on supervised learning. In this approach, the system learns a pairwise function to predict whether a pair of noun phrases is coreferent. Subsequently, when making coreference resolution decisions on unseen documents, the learnt pairwise noun phrase coreference classifier is run, followed by a clustering step to produce the final clusters (coreference chains) of coreferent noun phrases. For both training and testing, coreference resolution algorithms rely on feature vectors for pairs of noun phrases that enc"
C10-1031,P05-1021,0,0.0143355,"Missing"
C10-1031,P07-1067,0,0.0104337,"ate the effectiveness of the proposed features. 2 Related Work Coreference resolution is an extensively studied NLP problem (e.g., Morton, 2000; Ng and Cardie, 2002; Gasperin and Briscoe, 2008). Early knowledge-based approaches were domain and 269 linguistic dependent (Carbonell and Brown 1988), where researchers focused on diverse lexical and grammatical properties of referring expressions (Soon et al., 2001; Ng and Cardie, 2002; Zhou et al., 2004). Recent research relied more on exploiting semantic information. For example, Yang et al. (2005) used the semantic compatibility information, and Yang and Su (2007) used automatically discovered patterns integrated with semantic relatedness information, while Ng (2007) employed semantic class knowledge acquired from the Penn Treebank. Versley et al. (2008) used several kernel functions in learning. Perhaps, the most popular approach is based on supervised learning. In this approach, the system learns a pairwise function to predict whether a pair of noun phrases is coreferent. Subsequently, when making coreference resolution decisions on unseen documents, the learnt pairwise noun phrase coreference classifier is run, followed by a clustering step to produ"
C10-1031,C04-1075,0,0.0495128,"Missing"
C10-1031,D07-1114,0,\N,Missing
C10-1031,D09-1103,0,\N,Missing
C10-1031,P00-1023,0,\N,Missing
C10-1031,H05-2017,0,\N,Missing
C10-1031,J95-2003,0,\N,Missing
C10-1031,P07-1068,0,\N,Missing
C10-1031,D08-1068,0,\N,Missing
C10-1143,N09-1003,0,0.00881178,"lve the problem. We augment EM with two soft constraints. These constraints help guide EM to 1273 produce better solutions. We note that these constraints can be relaxed in the process to correct the imperfection of the constraints. 3. It is shown experimentally the new method outperforms the main existing state-of-the-art methods that can be applied to the task. 2 Related Work This work is mainly related to existing research on synonyms grouping, which clusters words and phrases based on some form of similarity. The methods for measuring word similarity can be classified into two main types (Agirre et al., 2009): those relying on pre-existing knowledge resources (e.g., thesauri, or taxonomies) (Yang and Powers, 2005; Alvarez and Lim, 2007; Hughes and Ramage, 2007), and those based on distributional properties (Pereira et al., 1993; Lin, 1998; Chen et al., 2006; Sahami and Heilman, 2006; Pantel et al., 2009). In the category that relies on existing knowledge sources, the work of Carenini et al. (2005) is most related to ours. The authors proposed a method to map feature expressions to a given domain feature taxonomy, using several similarity metrics on WordNet. This work does not use the word distribu"
C10-1143,P06-1127,0,0.0642266,"soft (rather than hard) as they can be relaxed in the learning process. This relaxation is important because the above two constraints can result in wrong groupings. The EM algorithm is allowed to re-assign them to other groups in the learning process. We call the proposed framework constrained semi-supervised learning. Since we use EM and soft constraints, we call the proposed method SCEM. Clearly, the problem can also be attempted using some other techniques, e.g., topic modeling (e.g, LDA (Blei et al., 2003)), or clustering using distributional similarity (Pereira et al., 1993; Lin, 1998; Chen et al., 2006; Sahami and Heilman, 2006). However, our results show that these methods do not perform as well. The input to the proposed algorithm consists of: a set of reviews R, and a set of discovered feature expressions F from R (using an existing algorithm). The user labels a small set of feature expressions, i.e., assigning them to the userspecified feature groups. The system then assigns the rest of the discovered features to the feature groups. EM is run using the distributional (or surrounding words) contexts of feature expressions in review set R to build a naïve Bayesian classifier in each itera"
C10-1143,P99-1004,0,0.0169619,"my, using several similarity metrics on WordNet. This work does not use the word distribution information, which is its main weakness because many expressions of the same feature are not synonyms in WordNet as they are domain/application dependent. Dictionaries do not contain domain specific knowledge, for which a domain corpus is needed. Another related work is distributional similarity, i.e., words with similar meaning tend to appear in similar contexts (Harris, 1968). As such, it fetches the surrounding words as context for each term. Similarity measures such as Cosine, Jaccard, Dice, etc (Lee, 1999), can be employed to compute the similarities between the seeds and other feature expressions. To suit our need, we tested the k-means clustering with distributional similarity. However, it does not perform as well as the proposed method. Recent work also applied topic modeling (e.g., LDA) to solve the problem. Guo et al. (2009) proposed a multilevel latent semantic association technique (called mLSA) to group product feature expressions, which runs LDA twice. However, mLSA is an unsupervised approach. For our evaluation, we still implemented the method and compared it with our SC-EM method. O"
C10-1143,P98-2127,0,0.327175,"traints are soft (rather than hard) as they can be relaxed in the learning process. This relaxation is important because the above two constraints can result in wrong groupings. The EM algorithm is allowed to re-assign them to other groups in the learning process. We call the proposed framework constrained semi-supervised learning. Since we use EM and soft constraints, we call the proposed method SCEM. Clearly, the problem can also be attempted using some other techniques, e.g., topic modeling (e.g, LDA (Blei et al., 2003)), or clustering using distributional similarity (Pereira et al., 1993; Lin, 1998; Chen et al., 2006; Sahami and Heilman, 2006). However, our results show that these methods do not perform as well. The input to the proposed algorithm consists of: a set of reviews R, and a set of discovered feature expressions F from R (using an existing algorithm). The user labels a small set of feature expressions, i.e., assigning them to the userspecified feature groups. The system then assigns the rest of the discovered features to the feature groups. EM is run using the distributional (or surrounding words) contexts of feature expressions in review set R to build a naïve Bayesian class"
C10-1143,D09-1098,0,0.0203197,"isting state-of-the-art methods that can be applied to the task. 2 Related Work This work is mainly related to existing research on synonyms grouping, which clusters words and phrases based on some form of similarity. The methods for measuring word similarity can be classified into two main types (Agirre et al., 2009): those relying on pre-existing knowledge resources (e.g., thesauri, or taxonomies) (Yang and Powers, 2005; Alvarez and Lim, 2007; Hughes and Ramage, 2007), and those based on distributional properties (Pereira et al., 1993; Lin, 1998; Chen et al., 2006; Sahami and Heilman, 2006; Pantel et al., 2009). In the category that relies on existing knowledge sources, the work of Carenini et al. (2005) is most related to ours. The authors proposed a method to map feature expressions to a given domain feature taxonomy, using several similarity metrics on WordNet. This work does not use the word distribution information, which is its main weakness because many expressions of the same feature are not synonyms in WordNet as they are domain/application dependent. Dictionaries do not contain domain specific knowledge, for which a domain corpus is needed. Another related work is distributional similarity"
C10-1143,P93-1024,0,0.15104,"eature group. The constraints are soft (rather than hard) as they can be relaxed in the learning process. This relaxation is important because the above two constraints can result in wrong groupings. The EM algorithm is allowed to re-assign them to other groups in the learning process. We call the proposed framework constrained semi-supervised learning. Since we use EM and soft constraints, we call the proposed method SCEM. Clearly, the problem can also be attempted using some other techniques, e.g., topic modeling (e.g, LDA (Blei et al., 2003)), or clustering using distributional similarity (Pereira et al., 1993; Lin, 1998; Chen et al., 2006; Sahami and Heilman, 2006). However, our results show that these methods do not perform as well. The input to the proposed algorithm consists of: a set of reviews R, and a set of discovered feature expressions F from R (using an existing algorithm). The user labels a small set of feature expressions, i.e., assigning them to the userspecified feature groups. The system then assigns the rest of the discovered features to the feature groups. EM is run using the distributional (or surrounding words) contexts of feature expressions in review set R to build a naïve Bay"
C10-1143,D07-1061,0,0.0174291,"nts can be relaxed in the process to correct the imperfection of the constraints. 3. It is shown experimentally the new method outperforms the main existing state-of-the-art methods that can be applied to the task. 2 Related Work This work is mainly related to existing research on synonyms grouping, which clusters words and phrases based on some form of similarity. The methods for measuring word similarity can be classified into two main types (Agirre et al., 2009): those relying on pre-existing knowledge resources (e.g., thesauri, or taxonomies) (Yang and Powers, 2005; Alvarez and Lim, 2007; Hughes and Ramage, 2007), and those based on distributional properties (Pereira et al., 1993; Lin, 1998; Chen et al., 2006; Sahami and Heilman, 2006; Pantel et al., 2009). In the category that relies on existing knowledge sources, the work of Carenini et al. (2005) is most related to ours. The authors proposed a method to map feature expressions to a given domain feature taxonomy, using several similarity metrics on WordNet. This work does not use the word distribution information, which is its main weakness because many expressions of the same feature are not synonyms in WordNet as they are domain/application depend"
C10-1143,H05-1043,0,0.307533,"and even the “picture quality” itself. All the feature expressions in a feature group signify the same feature. Grouping feature expressions manually into suitable groups is time consuming as there are 1272 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1272–1280, Beijing, August 2010 often hundreds of feature expressions. This paper helps the user to perform the task more efficiently. To focus our research, we assume that feature expressions have been discovered from a review corpus by an existing system such as those in (Hu and Liu, 2004b; Popescu and Etzioni, 2005; Kim and Hovy, 2006; Kobayashi et al., 2007; Mei et al., 2007; Stoyanov and Cardie, 2008; Jin et al., 2009; Ku et al., 2009). To reflect the user needs, he/she can manually label a small number of seeds for each feature group. The feature groups are also provided by the user based on his/her application needs. The system then assigns the rest of the feature expressions to suitable groups. To the best of our knowledge, this problem has not been studied in opinion mining (Pang and Lee, 2008). The problem can be formulated as semisupervised learning. The small set of seeds labeled by the user is"
C10-1143,C08-1103,0,0.012072,"nify the same feature. Grouping feature expressions manually into suitable groups is time consuming as there are 1272 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1272–1280, Beijing, August 2010 often hundreds of feature expressions. This paper helps the user to perform the task more efficiently. To focus our research, we assume that feature expressions have been discovered from a review corpus by an existing system such as those in (Hu and Liu, 2004b; Popescu and Etzioni, 2005; Kim and Hovy, 2006; Kobayashi et al., 2007; Mei et al., 2007; Stoyanov and Cardie, 2008; Jin et al., 2009; Ku et al., 2009). To reflect the user needs, he/she can manually label a small number of seeds for each feature group. The feature groups are also provided by the user based on his/her application needs. The system then assigns the rest of the feature expressions to suitable groups. To the best of our knowledge, this problem has not been studied in opinion mining (Pang and Lee, 2008). The problem can be formulated as semisupervised learning. The small set of seeds labeled by the user is the labeled data, and the rest of the discovered feature expressions are the unlabeled d"
C10-1143,W06-0301,0,0.044312,"ty” itself. All the feature expressions in a feature group signify the same feature. Grouping feature expressions manually into suitable groups is time consuming as there are 1272 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1272–1280, Beijing, August 2010 often hundreds of feature expressions. This paper helps the user to perform the task more efficiently. To focus our research, we assume that feature expressions have been discovered from a review corpus by an existing system such as those in (Hu and Liu, 2004b; Popescu and Etzioni, 2005; Kim and Hovy, 2006; Kobayashi et al., 2007; Mei et al., 2007; Stoyanov and Cardie, 2008; Jin et al., 2009; Ku et al., 2009). To reflect the user needs, he/she can manually label a small number of seeds for each feature group. The feature groups are also provided by the user based on his/her application needs. The system then assigns the rest of the feature expressions to suitable groups. To the best of our knowledge, this problem has not been studied in opinion mining (Pang and Lee, 2008). The problem can be formulated as semisupervised learning. The small set of seeds labeled by the user is the labeled data, a"
C10-1143,D07-1114,0,0.448755,"feature expressions in a feature group signify the same feature. Grouping feature expressions manually into suitable groups is time consuming as there are 1272 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1272–1280, Beijing, August 2010 often hundreds of feature expressions. This paper helps the user to perform the task more efficiently. To focus our research, we assume that feature expressions have been discovered from a review corpus by an existing system such as those in (Hu and Liu, 2004b; Popescu and Etzioni, 2005; Kim and Hovy, 2006; Kobayashi et al., 2007; Mei et al., 2007; Stoyanov and Cardie, 2008; Jin et al., 2009; Ku et al., 2009). To reflect the user needs, he/she can manually label a small number of seeds for each feature group. The feature groups are also provided by the user based on his/her application needs. The system then assigns the rest of the feature expressions to suitable groups. To the best of our knowledge, this problem has not been studied in opinion mining (Pang and Lee, 2008). The problem can be formulated as semisupervised learning. The small set of seeds labeled by the user is the labeled data, and the rest of the disco"
C10-1143,H05-2017,0,\N,Missing
C10-1143,C98-2122,0,\N,Missing
C10-2087,P04-1054,0,0.0798595,"p between two proteins in a syntactic representation. With the wide application of kernel-based methods to many NLP tasks, various kernels such as subsequence kernels (Bunescu and Mooney, 2005) and tree kernels (Li et al., 2008), are also applied to PPI detection.. Particularly, dependency-based kernels such as edit distance kernels (Erkan et al., 2007) and graph kernels (Airola et al., 2008; Kim et al., 2010) show some promising results for PPI extraction. This suggests that dependency information play a critical role in PPI extraction as well as in relation extraction from newswire stories (Culotta and Sorensen, 2004). In order to appreciate the advantages of both feature-based methods and kernel-based methods, composite kernels (Miyao et al., 2008; Miwa et al., 2009a; Miwa et al., 2009b) are further employed to combine structural syntactic information with flat word features and significantly improve the performance of PPI extraction. However, one critical challenge for kernel-based methods is their computation complexity, which prevents them from being widely deployed in real-world applications regarding the large amount of biomedical literature being archived everyday. Considering the potential of depen"
C10-2087,P08-1006,0,0.176675,"h as subsequence kernels (Bunescu and Mooney, 2005) and tree kernels (Li et al., 2008), are also applied to PPI detection.. Particularly, dependency-based kernels such as edit distance kernels (Erkan et al., 2007) and graph kernels (Airola et al., 2008; Kim et al., 2010) show some promising results for PPI extraction. This suggests that dependency information play a critical role in PPI extraction as well as in relation extraction from newswire stories (Culotta and Sorensen, 2004). In order to appreciate the advantages of both feature-based methods and kernel-based methods, composite kernels (Miyao et al., 2008; Miwa et al., 2009a; Miwa et al., 2009b) are further employed to combine structural syntactic information with flat word features and significantly improve the performance of PPI extraction. However, one critical challenge for kernel-based methods is their computation complexity, which prevents them from being widely deployed in real-world applications regarding the large amount of biomedical literature being archived everyday. Considering the potential of dependency information for PPI extraction and the challenge of computation complexity of kernel-based methods, one may naturally ask the q"
C10-2087,W06-1634,0,0.0320787,"Missing"
C10-2087,D07-1024,0,0.197868,"n of the entire biological process. However, manual collection of relevant Protein-Protein Interaction (PPI) information from thousands of research papers published every day is so time-consuming that automatic extraction approaches with the help of Natural Language Processing (NLP) techniques become necessary. Various machine learning approaches for relation extraction have been applied to the biomedical domain, which can be classified into two categories: feature-based methods (Mitsumori et al., 2006; Giuliano et al., 2006; Sætre et al., 2007) and kernel-based methods (Bunescu et al., 2005; Erkan et al., 2007; Airola et al., 2008; Kim et al., 2010). Provided a large-scale manually annotated corpus, the task of PPI extraction can be formulated as a classification problem. Typically, for featured-based learning each protein pair is represented as a vector whose features are extracted from the sentence involving two protein names. Early studies identify the existence of protein interactions by using “bag-of-words” features (usually uni-gram or bi-gram) around the protein names as well as various kinds of shallow linguistic information, such as POS tag, lemma and orthographical features. However, thes"
C10-2087,P05-1052,0,0.0608613,"learning each protein pair is represented as a vector whose features are extracted from the sentence involving two protein names. Early studies identify the existence of protein interactions by using “bag-of-words” features (usually uni-gram or bi-gram) around the protein names as well as various kinds of shallow linguistic information, such as POS tag, lemma and orthographical features. However, these systems do not achieve promising results since they disregard any syntactic or semantic information altogether, which are very useful for the task of relation extraction in the newswire domain (Zhao and Grishman, 2005; Zhou et al., 2005). Furthermore, feature-based methods fail to effectively capture the structural information, which is essential to Abstract Recent kernel-based PPI extraction systems achieve promising performance because of their capability to capture structural syntactic information, but at the expense of computational complexity. This paper incorporates dependency information as well as other lexical and syntactic knowledge in a feature-based framework. Our motivation is that, considering the large amount of biomedical literature being archived daily, feature-based methods with comparabl"
C10-2087,E06-1051,0,0.590702,"ersity Email: liubingnlp@gmail.com {qianlonghua,redleaf,gdzhou}@suda.edu.cn the organization of the entire biological process. However, manual collection of relevant Protein-Protein Interaction (PPI) information from thousands of research papers published every day is so time-consuming that automatic extraction approaches with the help of Natural Language Processing (NLP) techniques become necessary. Various machine learning approaches for relation extraction have been applied to the biomedical domain, which can be classified into two categories: feature-based methods (Mitsumori et al., 2006; Giuliano et al., 2006; Sætre et al., 2007) and kernel-based methods (Bunescu et al., 2005; Erkan et al., 2007; Airola et al., 2008; Kim et al., 2010). Provided a large-scale manually annotated corpus, the task of PPI extraction can be formulated as a classification problem. Typically, for featured-based learning each protein pair is represented as a vector whose features are extracted from the sentence involving two protein names. Early studies identify the existence of protein interactions by using “bag-of-words” features (usually uni-gram or bi-gram) around the protein names as well as various kinds of shallow l"
C10-2087,P05-1053,1,0.950324,"r is represented as a vector whose features are extracted from the sentence involving two protein names. Early studies identify the existence of protein interactions by using “bag-of-words” features (usually uni-gram or bi-gram) around the protein names as well as various kinds of shallow linguistic information, such as POS tag, lemma and orthographical features. However, these systems do not achieve promising results since they disregard any syntactic or semantic information altogether, which are very useful for the task of relation extraction in the newswire domain (Zhao and Grishman, 2005; Zhou et al., 2005). Furthermore, feature-based methods fail to effectively capture the structural information, which is essential to Abstract Recent kernel-based PPI extraction systems achieve promising performance because of their capability to capture structural syntactic information, but at the expense of computational complexity. This paper incorporates dependency information as well as other lexical and syntactic knowledge in a feature-based framework. Our motivation is that, considering the large amount of biomedical literature being archived daily, feature-based methods with comparable performance are mo"
C10-2087,D09-1013,0,0.395439,"nels (Bunescu and Mooney, 2005) and tree kernels (Li et al., 2008), are also applied to PPI detection.. Particularly, dependency-based kernels such as edit distance kernels (Erkan et al., 2007) and graph kernels (Airola et al., 2008; Kim et al., 2010) show some promising results for PPI extraction. This suggests that dependency information play a critical role in PPI extraction as well as in relation extraction from newswire stories (Culotta and Sorensen, 2004). In order to appreciate the advantages of both feature-based methods and kernel-based methods, composite kernels (Miyao et al., 2008; Miwa et al., 2009a; Miwa et al., 2009b) are further employed to combine structural syntactic information with flat word features and significantly improve the performance of PPI extraction. However, one critical challenge for kernel-based methods is their computation complexity, which prevents them from being widely deployed in real-world applications regarding the large amount of biomedical literature being archived everyday. Considering the potential of dependency information for PPI extraction and the challenge of computation complexity of kernel-based methods, one may naturally ask the question: “Can the e"
C10-2167,C02-1025,0,0.071868,"Missing"
C10-2167,J06-1005,0,0.0366506,"preposition “on”. Note that “valley” is not actually a part of mattress, but an effect on the mattress. It is called a pseudo part-whole relation. For simplicity, we will not distinguish it from an actual part-whole relation because for our feature mining task, they have little difference. In this case, “noun1 on noun2” is a good indicative pattern which implies noun1 is part of noun2. So if we know “mattress” is a class concept, we can infer that “valley” is a feature for “mattress”. There are many phrase or sentence patterns representing this type of semantic relation which was studied in (Girju et al, 2006). Beside part-whole patterns, “no” pattern is another important and specific feature indicator in opinion documents. We introduce these patterns in detail in Sections 3.2 and 3.3. Now let us deal with the first problem: noise. With opinion words, part-whole and “no” patterns, we have three feature indicators at hands, but all of them are ambiguous, which means that they are not hard rules. We will inevitably extract wrong features (also called noises) by using them. Pruning noises from feature candidates is a hard task. Instead, we propose a new angle for solving this problem: feature ranking."
C10-2167,D07-1114,0,0.570072,"Missing"
C10-2167,I08-1038,0,0.437249,"ers have further explored the idea of using opinion words in product feature mining. A dependency based method was proposed in (Zhuang et al., 2006) for a movie review analysis application. Qiu et al. (2009) proposed a double propagation method, which exploits certain syntactic relations of opinion words and features, and propagates through both opinion words and features iteratively. The extraction rules are designed based on different relations between opinion words and features, and among opinion words and features themselves. Dependency grammar was adopted to describe these relations. In (Wang and Wang, 2008), another bootstrapping method was proposed. In (Kobayashi et al. 2007), a pattern mining method was used. The patterns are relations between feature and opinion pairs (they call aspect-evaluation pairs). The patterns are mined from a large corpus using pattern mining. Statistics from the corpus are used to determine the confidence scores of the extraction. In general information extraction, there are two approaches: rule-based and statistical. Early extraction systems are mainly based on rules (e.g., Riloff, 1993). In statistical methods, the most popular models are Hidden Markov Models (HMM)"
C10-2167,H05-1044,0,\N,Missing
C10-2167,C08-1103,0,\N,Missing
C10-2167,H05-2017,0,\N,Missing
C10-2167,H05-1043,0,\N,Missing
C10-2167,J95-4004,0,\N,Missing
C10-2167,D09-1098,0,\N,Missing
C12-1112,P11-1151,0,0.23608,"Missing"
C12-1112,P04-1085,0,0.736741,"Missing"
C12-1112,N03-2012,0,0.764556,"Missing"
C12-1112,C10-2100,0,0.216122,"Missing"
C12-1112,D10-1021,1,0.890246,"Missing"
C12-1112,P12-1034,1,0.836854,"Missing"
C12-1112,P11-1032,0,0.0945123,"Missing"
C12-1112,D09-1026,0,0.0785188,"Missing"
C12-1112,P09-1026,0,0.155594,"Missing"
C12-1112,D10-1006,0,0.282048,"Missing"
C12-1112,C08-2004,0,\N,Missing
C14-1063,N10-1122,0,0.165857,"they tend to produce topics that correspond to global properties of products (e.g., product brand name), but cannot separate different product aspects or features well. The reason is that all reviews of the same product type basically evaluate the same aspects of the product type. Only the brand names and product names are different. Thus, using individual reviews for modelling is ineffective for finding product aspects or features, which are our topics. Although there are approaches which model sentences (Jo and Oh, 2011; Zhao et al., 2010; Titov and McDonald, 2008), we take the approach in (Brody and Elhadad, 2010; Chen et al., 2013), dividing each review into sentences and treating each sentence as an independent document. Noun Phrase Detection: Although there are different types of phrases, in this first work we focus only on noun phrases as they are more representative of topics in online reviews. We will deal with other types of phrases in the future. Our first step is thus to obtain all noun phrases from each domain. Due to the efficiency issue of full natural language parser with a huge number of reviews, instead of applying the Stanford Parser to recognize noun phrases, we design a rule-based ap"
C14-1063,D10-1036,0,0.0367569,"and then sampling the word from a topic-specific unigram or bigram distribution. Although the “bag-of-words” assumption does not always hold in real-life applications, it offers a great computational advantage over more complex models taking word order into account for discovering significant ngrams. Our approach is different from these works in two ways. First, we still follow the “bag-of-words” or rather “bag-of-terms” assumption. Second, we find actual phrases rather than just n-grams. Most ngrams are still hard to understand because they are not natural phrases. Blei and Lafferty (2009), Liu et al. (2010) and Zhao et al. (2011) also try to extract keyphrases from texts. Their methods, however, are very different because they identify multi-word phrases using relevance and likelihood scores in the post-processing step based on the discovered topical unigrams. Mukherjee and Liu (2013) and Mukherjee et al. (2013) all try to include n-grams to enhance the expressiveness of their models while preserving the advantages of “bag-of-words” assumption, which has a similar idea as our paper. However, as we point out in the introduction, this way of including phrases/n-grams suffers from several shortcomi"
C14-1063,D10-1006,0,0.782123,"topical unigrams. Mukherjee and Liu (2013) and Mukherjee et al. (2013) all try to include n-grams to enhance the expressiveness of their models while preserving the advantages of “bag-of-words” assumption, which has a similar idea as our paper. However, as we point out in the introduction, this way of including phrases/n-grams suffers from several shortcomings. Solving these problems is the goal of our paper. Finally, since we use product reviews as our datasets, our work is also related to opinion mining using topic models, e.g. (Mei et al., 2007; Lu and Zhai, 2008; Titov and McDonald, 2008; Zhao et al., 2010; Li et al., 2010; Sauper and Barzilay, 2013; Lin and He, 2009; Jo and Oh, 2011). However, none of these models uses phrases. 3 Proposed Model We start by briefly reviewing the Latent Dirichlet Allocation (LDA) model (Blei et al., 2003). Then we describe the simple Pólya urn (SPU) model, which is embedded in LDA. After that, we present the generalized Pólya urn (GPU) model and discuss how it can be applied to our context. The proposed model uses GPU for its inference. It shares the same graphical model as LDA. However, the GPU inference mechanism is very different from that of LDA, which canno"
C14-1063,P13-1165,1,\N,Missing
C14-1063,P11-1039,0,\N,Missing
C14-1063,D11-1024,0,\N,Missing
C14-1063,D13-1172,1,\N,Missing
C14-1063,P13-1066,1,\N,Missing
C18-1160,P17-2061,0,0.0240633,"istance between source and target distributions (Baktashmotlagh et al., 2013; Tzeng et al., 2014; Long et al., 2015), maximizing of the loss of domain classifier (Ganin and Lempitsky, 2015; Tzeng et al., 2015; Tzeng et al., 2017; Gopalan et al., 2011), and applying semi-supervised or selective learning techniques (Ao et al., 2017; Tan et al., 2017). Recent years also witnessed the applications of domain adaption technique in many other areas, especially in natural language processing tasks. To name a few, typical applications include question answering (Min et al., 2017), machine translation (Chu et al., 2017; Johnson et al., 2016; Wang et al., 2017a), sentiment analysis (Li et al., 2017), recommendation system (Man et al., 2017) and image-text retrieval (Huang et al., 2017). While domain adaption technique was introduced in the above research fields, it was rarely adopted in spam review detection. We aim to exploit this technique to help alleviate the problem of data scarcity in the cold-start scenario of spam detection. 3 Methodology In this section, we present the details of our attribute-enhanced domain adaptive embedding model (AEDA). The key idea is to leverage the relations between entities"
C18-1160,P12-2034,0,0.129169,"follows. In Section 2, we present the related work. In Section 3, we introduce our methodology. In Section 4, we show the experimental evaluation. We conclude the paper in Section 5. 2 Related Work 2.1 Review Spam Detection The problem of spam review detection has aroused great research interests in recent years. This problem was first introduced in (Jindal and Liu, 2008), and the focus of this work was to find effective features to represent the fake and real reviews. Later, a variety of linguistic features were introduced in the literature (Ott et al., 2011; Xu and Zhao, 2013; Harris, 2012; Feng et al., 2012a; Kim et al., 2015; Li et al., 2013; Li et al., 2014b; Fornaciari and Poesio, 2014; Li et al., 2014a; Hovy, 2016). However, an in-depth study (Mukherjee et al., 2013b) found that the linguistic features were insufficient for detecting fake reviews in real business website. Therefore, the features besides review contents were exploited, and the behavioral features of reviewers were intensively studied in (Lim et al., 2010; Jindal et al., 2010; Feng et al., 2012b; Mukherjee et al., 2012; Xie et al., 2012; Fei et al., 2013; Li et al., 2015; KC and Mukherjee, 2016; Wang et al., 2011; Akoglu et al"
C18-1160,E14-1030,0,0.0133512,"oduce our methodology. In Section 4, we show the experimental evaluation. We conclude the paper in Section 5. 2 Related Work 2.1 Review Spam Detection The problem of spam review detection has aroused great research interests in recent years. This problem was first introduced in (Jindal and Liu, 2008), and the focus of this work was to find effective features to represent the fake and real reviews. Later, a variety of linguistic features were introduced in the literature (Ott et al., 2011; Xu and Zhao, 2013; Harris, 2012; Feng et al., 2012a; Kim et al., 2015; Li et al., 2013; Li et al., 2014b; Fornaciari and Poesio, 2014; Li et al., 2014a; Hovy, 2016). However, an in-depth study (Mukherjee et al., 2013b) found that the linguistic features were insufficient for detecting fake reviews in real business website. Therefore, the features besides review contents were exploited, and the behavioral features of reviewers were intensively studied in (Lim et al., 2010; Jindal et al., 2010; Feng et al., 2012b; Mukherjee et al., 2012; Xie et al., 2012; Fei et al., 2013; Li et al., 2015; KC and Mukherjee, 2016; Wang et al., 2011; Akoglu et al., 2013; Mukherjee et al., 2013a; Mukherjee et al., 2013b). The intuition is that t"
C18-1160,D16-1187,0,0.153772,"et al., 2013b). The intuition is that the reviewers with spammer-like behaviors are more likely to post fake reviews. Several methods (Li 1885 et al., 2011; Rayana and Akoglu, 2015) were proposed to utilize multiple information mentioned above. Overall, the traditional features are manually constructed and depend heavily on the experts’ knowledge. More recently, deep learning techniques are applied to fake review detection. Ren and Zhang (Ren and Zhang, 2016) compared several neural networks and found that CNN was more effective than RNN on review text encoding in spam review detection task. Hai et al. (2016) implemented a semi-supervised multi-task learning method, which introduced a covariance matrix to capture the relation between tasks and a Laplacian regularizer to leverage the unlabeled data. Wang et al. (2016) employed a tensor decomposition based on the global behavioral information to learn the representation of the reviewer and item. The cold-start problem in spam review detection was first introduced in (Wang et al., 2017b), where the authors proposed an embedding learning model to jointly utilize the behavioral information of reviewers and the textual information. While we aim to solve"
C18-1160,P16-2057,0,0.017504,"e experimental evaluation. We conclude the paper in Section 5. 2 Related Work 2.1 Review Spam Detection The problem of spam review detection has aroused great research interests in recent years. This problem was first introduced in (Jindal and Liu, 2008), and the focus of this work was to find effective features to represent the fake and real reviews. Later, a variety of linguistic features were introduced in the literature (Ott et al., 2011; Xu and Zhao, 2013; Harris, 2012; Feng et al., 2012a; Kim et al., 2015; Li et al., 2013; Li et al., 2014b; Fornaciari and Poesio, 2014; Li et al., 2014a; Hovy, 2016). However, an in-depth study (Mukherjee et al., 2013b) found that the linguistic features were insufficient for detecting fake reviews in real business website. Therefore, the features besides review contents were exploited, and the behavioral features of reviewers were intensively studied in (Lim et al., 2010; Jindal et al., 2010; Feng et al., 2012b; Mukherjee et al., 2012; Xie et al., 2012; Fei et al., 2013; Li et al., 2015; KC and Mukherjee, 2016; Wang et al., 2011; Akoglu et al., 2013; Mukherjee et al., 2013a; Mukherjee et al., 2013b). The intuition is that the reviewers with spammer-like"
C18-1160,P13-2039,0,0.0206409,"related work. In Section 3, we introduce our methodology. In Section 4, we show the experimental evaluation. We conclude the paper in Section 5. 2 Related Work 2.1 Review Spam Detection The problem of spam review detection has aroused great research interests in recent years. This problem was first introduced in (Jindal and Liu, 2008), and the focus of this work was to find effective features to represent the fake and real reviews. Later, a variety of linguistic features were introduced in the literature (Ott et al., 2011; Xu and Zhao, 2013; Harris, 2012; Feng et al., 2012a; Kim et al., 2015; Li et al., 2013; Li et al., 2014b; Fornaciari and Poesio, 2014; Li et al., 2014a; Hovy, 2016). However, an in-depth study (Mukherjee et al., 2013b) found that the linguistic features were insufficient for detecting fake reviews in real business website. Therefore, the features besides review contents were exploited, and the behavioral features of reviewers were intensively studied in (Lim et al., 2010; Jindal et al., 2010; Feng et al., 2012b; Mukherjee et al., 2012; Xie et al., 2012; Fei et al., 2013; Li et al., 2015; KC and Mukherjee, 2016; Wang et al., 2011; Akoglu et al., 2013; Mukherjee et al., 2013a; Mu"
C18-1160,P14-1147,0,0.0149114,"Section 3, we introduce our methodology. In Section 4, we show the experimental evaluation. We conclude the paper in Section 5. 2 Related Work 2.1 Review Spam Detection The problem of spam review detection has aroused great research interests in recent years. This problem was first introduced in (Jindal and Liu, 2008), and the focus of this work was to find effective features to represent the fake and real reviews. Later, a variety of linguistic features were introduced in the literature (Ott et al., 2011; Xu and Zhao, 2013; Harris, 2012; Feng et al., 2012a; Kim et al., 2015; Li et al., 2013; Li et al., 2014b; Fornaciari and Poesio, 2014; Li et al., 2014a; Hovy, 2016). However, an in-depth study (Mukherjee et al., 2013b) found that the linguistic features were insufficient for detecting fake reviews in real business website. Therefore, the features besides review contents were exploited, and the behavioral features of reviewers were intensively studied in (Lim et al., 2010; Jindal et al., 2010; Feng et al., 2012b; Mukherjee et al., 2012; Xie et al., 2012; Fei et al., 2013; Li et al., 2015; KC and Mukherjee, 2016; Wang et al., 2011; Akoglu et al., 2013; Mukherjee et al., 2013a; Mukherjee et al., 2"
C18-1160,P17-2081,0,0.0192627,"014; Liu et al., 2017), minimizing the distance between source and target distributions (Baktashmotlagh et al., 2013; Tzeng et al., 2014; Long et al., 2015), maximizing of the loss of domain classifier (Ganin and Lempitsky, 2015; Tzeng et al., 2015; Tzeng et al., 2017; Gopalan et al., 2011), and applying semi-supervised or selective learning techniques (Ao et al., 2017; Tan et al., 2017). Recent years also witnessed the applications of domain adaption technique in many other areas, especially in natural language processing tasks. To name a few, typical applications include question answering (Min et al., 2017), machine translation (Chu et al., 2017; Johnson et al., 2016; Wang et al., 2017a), sentiment analysis (Li et al., 2017), recommendation system (Man et al., 2017) and image-text retrieval (Huang et al., 2017). While domain adaption technique was introduced in the above research fields, it was rarely adopted in spam review detection. We aim to exploit this technique to help alleviate the problem of data scarcity in the cold-start scenario of spam detection. 3 Methodology In this section, we present the details of our attribute-enhanced domain adaptive embedding model (AEDA). The key idea is to"
C18-1160,P11-1032,0,0.322896,"ne methods. The rest of the paper is structured as follows. In Section 2, we present the related work. In Section 3, we introduce our methodology. In Section 4, we show the experimental evaluation. We conclude the paper in Section 5. 2 Related Work 2.1 Review Spam Detection The problem of spam review detection has aroused great research interests in recent years. This problem was first introduced in (Jindal and Liu, 2008), and the focus of this work was to find effective features to represent the fake and real reviews. Later, a variety of linguistic features were introduced in the literature (Ott et al., 2011; Xu and Zhao, 2013; Harris, 2012; Feng et al., 2012a; Kim et al., 2015; Li et al., 2013; Li et al., 2014b; Fornaciari and Poesio, 2014; Li et al., 2014a; Hovy, 2016). However, an in-depth study (Mukherjee et al., 2013b) found that the linguistic features were insufficient for detecting fake reviews in real business website. Therefore, the features besides review contents were exploited, and the behavioral features of reviewers were intensively studied in (Lim et al., 2010; Jindal et al., 2010; Feng et al., 2012b; Mukherjee et al., 2012; Xie et al., 2012; Fei et al., 2013; Li et al., 2015; KC"
C18-1160,C16-1014,0,0.0156123,"012; Xie et al., 2012; Fei et al., 2013; Li et al., 2015; KC and Mukherjee, 2016; Wang et al., 2011; Akoglu et al., 2013; Mukherjee et al., 2013a; Mukherjee et al., 2013b). The intuition is that the reviewers with spammer-like behaviors are more likely to post fake reviews. Several methods (Li 1885 et al., 2011; Rayana and Akoglu, 2015) were proposed to utilize multiple information mentioned above. Overall, the traditional features are manually constructed and depend heavily on the experts’ knowledge. More recently, deep learning techniques are applied to fake review detection. Ren and Zhang (Ren and Zhang, 2016) compared several neural networks and found that CNN was more effective than RNN on review text encoding in spam review detection task. Hai et al. (2016) implemented a semi-supervised multi-task learning method, which introduced a covariance matrix to capture the relation between tasks and a Laplacian regularizer to leverage the unlabeled data. Wang et al. (2016) employed a tensor decomposition based on the global behavioral information to learn the representation of the reviewer and item. The cold-start problem in spam review detection was first introduced in (Wang et al., 2017b), where the a"
C18-1160,D16-1083,0,0.0171875,"tiple information mentioned above. Overall, the traditional features are manually constructed and depend heavily on the experts’ knowledge. More recently, deep learning techniques are applied to fake review detection. Ren and Zhang (Ren and Zhang, 2016) compared several neural networks and found that CNN was more effective than RNN on review text encoding in spam review detection task. Hai et al. (2016) implemented a semi-supervised multi-task learning method, which introduced a covariance matrix to capture the relation between tasks and a Laplacian regularizer to leverage the unlabeled data. Wang et al. (2016) employed a tensor decomposition based on the global behavioral information to learn the representation of the reviewer and item. The cold-start problem in spam review detection was first introduced in (Wang et al., 2017b), where the authors proposed an embedding learning model to jointly utilize the behavioral information of reviewers and the textual information. While we aim to solve the same cold start problem as that in (Wang et al., 2017b), we propose a totally different framework which leverage both the new attribute and domain knowledge information. 2.2 Domain Adaption Domain adaption h"
C18-1160,P17-2089,0,0.0629628,"Missing"
C18-1160,P17-1034,0,0.410201,"and Zhang (Ren and Zhang, 2016) compared several neural networks and found that CNN was more effective than RNN on review text encoding in spam review detection task. Hai et al. (2016) implemented a semi-supervised multi-task learning method, which introduced a covariance matrix to capture the relation between tasks and a Laplacian regularizer to leverage the unlabeled data. Wang et al. (2016) employed a tensor decomposition based on the global behavioral information to learn the representation of the reviewer and item. The cold-start problem in spam review detection was first introduced in (Wang et al., 2017b), where the authors proposed an embedding learning model to jointly utilize the behavioral information of reviewers and the textual information. While we aim to solve the same cold start problem as that in (Wang et al., 2017b), we propose a totally different framework which leverage both the new attribute and domain knowledge information. 2.2 Domain Adaption Domain adaption has been applied to addressing the scarcity of labeled data in a variety of real-world applications. It has been well studied in many tasks in the area of computer vision, such as the image classification and object recog"
D09-1019,W06-1602,0,0.0196685,"Missing"
D09-1019,E06-1025,0,0.0134655,"Missing"
D09-1019,P97-1023,0,0.0683272,"Missing"
D09-1019,P06-2059,0,0.0232354,"Missing"
D09-1019,W06-1642,0,0.0152251,"ons. We obtained a list of over 6500 sentiment words gathered from various sources. The bulk of it is from http://www.cs.pitt.edu/mpqa. We also added some of our own. Our list is mainly from the work in (Hu and Liu, 2004; Ding, Liu and Yu, 2008). In addition to words, there are phrases that describe opinions. We have identified a set of such phrases. Although obtaining these phrases was time-consuming, it was only a one-time effort. We will make this list available as a community resource. It is possible that there is a better automated method for finding such phrases, such as the methods in (Kanayama and Nasukawa, 2006; Breck, Choi and Cardie, 2007). However, automatically generating sentiment phrases has not been the focus of this work as our objective is to study how the two clauses interact to determine opinions given the sentiment words and phrases are known. Our list of phrases is by no means complete and we will continue to expand it in the future. For each sentence, we also identify whether it contains sentiment words/phrases in its condition or consequent clause. It was observed that the presence of a sentiment word/phrase in the consequent clause has more effect on the sentiment of a sentence. II."
D09-1019,C04-1200,0,0.135315,"n products and services, which are called objects or entities. Each object is described by its parts and attributes, which are collectively called features in (Hu and Liu, 2004; Liu, 2006). For example, in the sentence, If this camera has great picture quality, I will buy it, “picture quality” is a feature of the camera. For formal definitions of objects and features, please refer to (Liu, 2006; Liu, 2009). In this paper, we use the term topic to mean feature as the feature here can confuse with the feature used in machine learning. The term topic has also been used by some researchers (e.g., Kim and Hovy, 2004; Stoyanov and Cardie, 2008). Our objective is to predict the sentiment orientation (positive, negative or neutral) on each topic that has been commented on in a sentence. The problem of automatically identifying features or topics being spoken about in a sentence has been studied in (Hu and Liu, 2004; Popescu and Etzioni, 2005; Stoyanov and Cardie, 2008). In this work, we do not attempt to identify such topics automatically. Instead, we assume that they are given because our objective is to study how the interaction of the condition and consequent clauses affects sentiments. For this purpose,"
D09-1019,D07-1114,0,0.112619,"Missing"
D09-1019,H05-1043,0,0.329832,".g., sentiment classification (classifying an opinion document as positive or negative) (e.g., Pang, Lee and Vaithyanathan, 2002; Turney, 2002), subjectivity classification (determining whether a sentence is subjective or objective, and its associated opinion) (Wiebe and Wilson, 2002; Yu and Hatzivassiloglou, 2003; Wilson et al, 2004; Kim and * This work was done when Bing Liu was on sabbatical leave at Northwestern University. Hovy, 2004; Riloff and Wiebe, 2005), feature/topic-based sentiment analysis (assigning positive or negative sentiments to topics or product features) (Hu and Liu 2004; Popescu and Etzioni, 2005; Carenini et al., 2005; Ku et al., 2006; Kobayashi, Inui and Matsumoto, 2007; Titov and McDonald. 2008). Formal definitions of different aspects of the sentiment analysis problem and discussions of major research directions and algorithms can be found in (Liu, 2006; Liu, 2009). A comprehensive survey of the field can be found in (Pang and Lee, 2008). Our work is in the area of topic/feature-based sentiment analysis or opinion mining (Hu and Liu, 2004). The existing research focuses on solving the general problem. However, we argue that it is unlikely to have a one-technique-fit-all solution b"
D09-1019,W03-1014,0,0.182873,"Missing"
D09-1019,C08-1103,0,0.00960765,"ces, which are called objects or entities. Each object is described by its parts and attributes, which are collectively called features in (Hu and Liu, 2004; Liu, 2006). For example, in the sentence, If this camera has great picture quality, I will buy it, “picture quality” is a feature of the camera. For formal definitions of objects and features, please refer to (Liu, 2006; Liu, 2009). In this paper, we use the term topic to mean feature as the feature here can confuse with the feature used in machine learning. The term topic has also been used by some researchers (e.g., Kim and Hovy, 2004; Stoyanov and Cardie, 2008). Our objective is to predict the sentiment orientation (positive, negative or neutral) on each topic that has been commented on in a sentence. The problem of automatically identifying features or topics being spoken about in a sentence has been studied in (Hu and Liu, 2004; Popescu and Etzioni, 2005; Stoyanov and Cardie, 2008). In this work, we do not attempt to identify such topics automatically. Instead, we assume that they are given because our objective is to study how the interaction of the condition and consequent clauses affects sentiments. For this purpose, we manually identify all th"
D09-1019,P08-1036,0,0.0472518,"Missing"
D09-1019,P02-1053,0,0.00575727,"such sentences, and then builds some supervised learning models to determine if sentiments expressed on different topics in a conditional sentence are positive, negative or neutral. Experimental results on conditional sentences from 5 diverse domains are given to demonstrate the effectiveness of the proposed approach. 1 Introduction Sentiment analysis (also called opinion mining) has been an active research area in recent years. There are many research directions, e.g., sentiment classification (classifying an opinion document as positive or negative) (e.g., Pang, Lee and Vaithyanathan, 2002; Turney, 2002), subjectivity classification (determining whether a sentence is subjective or objective, and its associated opinion) (Wiebe and Wilson, 2002; Yu and Hatzivassiloglou, 2003; Wilson et al, 2004; Kim and * This work was done when Bing Liu was on sabbatical leave at Northwestern University. Hovy, 2004; Riloff and Wiebe, 2005), feature/topic-based sentiment analysis (assigning positive or negative sentiments to topics or product features) (Hu and Liu 2004; Popescu and Etzioni, 2005; Carenini et al., 2005; Ku et al., 2006; Kobayashi, Inui and Matsumoto, 2007; Titov and McDonald. 2008). Formal defin"
D09-1019,P99-1032,0,0.0601397,"Missing"
D09-1019,W02-2034,0,0.00661546,"ional sentence are positive, negative or neutral. Experimental results on conditional sentences from 5 diverse domains are given to demonstrate the effectiveness of the proposed approach. 1 Introduction Sentiment analysis (also called opinion mining) has been an active research area in recent years. There are many research directions, e.g., sentiment classification (classifying an opinion document as positive or negative) (e.g., Pang, Lee and Vaithyanathan, 2002; Turney, 2002), subjectivity classification (determining whether a sentence is subjective or objective, and its associated opinion) (Wiebe and Wilson, 2002; Yu and Hatzivassiloglou, 2003; Wilson et al, 2004; Kim and * This work was done when Bing Liu was on sabbatical leave at Northwestern University. Hovy, 2004; Riloff and Wiebe, 2005), feature/topic-based sentiment analysis (assigning positive or negative sentiments to topics or product features) (Hu and Liu 2004; Popescu and Etzioni, 2005; Carenini et al., 2005; Ku et al., 2006; Kobayashi, Inui and Matsumoto, 2007; Titov and McDonald. 2008). Formal definitions of different aspects of the sentiment analysis problem and discussions of major research directions and algorithms can be found in (Li"
D09-1019,W03-1017,0,0.040213,"ive, negative or neutral. Experimental results on conditional sentences from 5 diverse domains are given to demonstrate the effectiveness of the proposed approach. 1 Introduction Sentiment analysis (also called opinion mining) has been an active research area in recent years. There are many research directions, e.g., sentiment classification (classifying an opinion document as positive or negative) (e.g., Pang, Lee and Vaithyanathan, 2002; Turney, 2002), subjectivity classification (determining whether a sentence is subjective or objective, and its associated opinion) (Wiebe and Wilson, 2002; Yu and Hatzivassiloglou, 2003; Wilson et al, 2004; Kim and * This work was done when Bing Liu was on sabbatical leave at Northwestern University. Hovy, 2004; Riloff and Wiebe, 2005), feature/topic-based sentiment analysis (assigning positive or negative sentiments to topics or product features) (Hu and Liu 2004; Popescu and Etzioni, 2005; Carenini et al., 2005; Ku et al., 2006; Kobayashi, Inui and Matsumoto, 2007; Titov and McDonald. 2008). Formal definitions of different aspects of the sentiment analysis problem and discussions of major research directions and algorithms can be found in (Liu, 2006; Liu, 2009). A comprehe"
D09-1019,P06-2079,0,0.0419302,"Missing"
D09-1019,W07-1018,0,\N,Missing
D09-1019,H05-2017,0,\N,Missing
D09-1019,C08-1031,1,\N,Missing
D09-1019,W02-1011,0,\N,Missing
D09-1019,P07-1055,0,\N,Missing
D10-1021,W06-1652,0,0.0223793,"Missing"
D10-1021,H05-1059,0,0.0126348,"hurt, hysterical, innocent, interested, jealous, lonely, mischievous, miserable, optimistic, paranoid, peaceful, proud, puzzled, regretful, relieved, sad, satisfied, shocked, shy, sorry, surprised, suspicious, thoughtful, undecided, withdrawn Table 3: Words implying positive, negative and emotional connotations 210 3.5 Proposed POS Sequence Pattern Features We now present the proposed POS sequence pattern features and the mining algorithm. This results in a new feature class. A POS sequence pattern is a sequence of consecutive POS tags that satisfy some constraints (discussed below). We used (Tsuruoka and Tsujii, 2005) as our POS tagger. As shown in (Baayen et. al., 1996), POS ngrams are good at capturing the heavy stylistic and syntactic information. Instead of using all such n-grams, we want to discover all those patterns that represent true regularities, and we also want to have flexible lengths (not fixed lengths as in n-grams). POS sequence patterns serve these purposes. Its mining algorithm mines all such patterns that satisfy the user-specified minimum support (minsup) and minimum adherence (minadherence) thresholds or constraints. These thresholds ensure that the mined patterns represent significant"
D10-1022,E09-1006,0,0.0135836,"et al. 2010). We will discuss this learning model further in Section 3. Another related work to ours is transfer learning or domain adaptation. Unlike our problem setting, transfer learning addresses the scenario where one has little or no training data for the target domain, but has ample training data in a related domain where the data could be in a different feature space and follow a different distribution. A survey of transfer learning can be found in (Pan and Yang 2009). Several NLP researchers have studied transfer learning for different applications (Wu et al. 2009a; Yang et al. 2009; Agirre & Lacalle 2009; Wu et al. 2009b; Sagae & Tsujii 2008; Goldwasser & Roth 2008; Li and Zong 2008; Andrew et al. 220 2008; Chan and Ng 2007; Jiang and Zhai 2007; Zhou et al. 2006), but none of them addresses the problem studied here. 3 PU Learning Techniques In traditional supervised learning, ideally, there is a large number of labeled positive and negative examples for learning. In practice, the negative examples can often be limited or unavailable. This has motivated the development of the model of learning from positive and unlabeled examples, or PU learning, where P denotes a set of positive examples, and"
D10-1022,P08-1029,0,0.0182737,"Missing"
D10-1022,P07-1007,0,0.010769,"main adaptation. Unlike our problem setting, transfer learning addresses the scenario where one has little or no training data for the target domain, but has ample training data in a related domain where the data could be in a different feature space and follow a different distribution. A survey of transfer learning can be found in (Pan and Yang 2009). Several NLP researchers have studied transfer learning for different applications (Wu et al. 2009a; Yang et al. 2009; Agirre & Lacalle 2009; Wu et al. 2009b; Sagae & Tsujii 2008; Goldwasser & Roth 2008; Li and Zong 2008; Andrew et al. 220 2008; Chan and Ng 2007; Jiang and Zhai 2007; Zhou et al. 2006), but none of them addresses the problem studied here. 3 PU Learning Techniques In traditional supervised learning, ideally, there is a large number of labeled positive and negative examples for learning. In practice, the negative examples can often be limited or unavailable. This has motivated the development of the model of learning from positive and unlabeled examples, or PU learning, where P denotes a set of positive examples, and U a set of unlabeled examples (which contains both hidden positive and hidden negative instances). The PU learning proble"
D10-1022,P08-2014,0,0.0270582,"Section 3. Another related work to ours is transfer learning or domain adaptation. Unlike our problem setting, transfer learning addresses the scenario where one has little or no training data for the target domain, but has ample training data in a related domain where the data could be in a different feature space and follow a different distribution. A survey of transfer learning can be found in (Pan and Yang 2009). Several NLP researchers have studied transfer learning for different applications (Wu et al. 2009a; Yang et al. 2009; Agirre & Lacalle 2009; Wu et al. 2009b; Sagae & Tsujii 2008; Goldwasser & Roth 2008; Li and Zong 2008; Andrew et al. 220 2008; Chan and Ng 2007; Jiang and Zhai 2007; Zhou et al. 2006), but none of them addresses the problem studied here. 3 PU Learning Techniques In traditional supervised learning, ideally, there is a large number of labeled positive and negative examples for learning. In practice, the negative examples can often be limited or unavailable. This has motivated the development of the model of learning from positive and unlabeled examples, or PU learning, where P denotes a set of positive examples, and U a set of unlabeled examples (which contains both hidden pos"
D10-1022,P07-1034,0,0.0390435,"nlike our problem setting, transfer learning addresses the scenario where one has little or no training data for the target domain, but has ample training data in a related domain where the data could be in a different feature space and follow a different distribution. A survey of transfer learning can be found in (Pan and Yang 2009). Several NLP researchers have studied transfer learning for different applications (Wu et al. 2009a; Yang et al. 2009; Agirre & Lacalle 2009; Wu et al. 2009b; Sagae & Tsujii 2008; Goldwasser & Roth 2008; Li and Zong 2008; Andrew et al. 220 2008; Chan and Ng 2007; Jiang and Zhai 2007; Zhou et al. 2006), but none of them addresses the problem studied here. 3 PU Learning Techniques In traditional supervised learning, ideally, there is a large number of labeled positive and negative examples for learning. In practice, the negative examples can often be limited or unavailable. This has motivated the development of the model of learning from positive and unlabeled examples, or PU learning, where P denotes a set of positive examples, and U a set of unlabeled examples (which contains both hidden positive and hidden negative instances). The PU learning problem is to build a class"
D10-1022,P10-2066,1,0.691436,"nd Tsuboi et al. (2008) estimated the weights for the training instances by minimizing the Kullback-Leibler divergence between the test and the weighted training distributions. Bickel et al. (2009) proposed an integrated model. In this paper, we adopt an entirely different approach by dropping the negative training data altogether in learning. Without the negative training data, we use PU learning to solve the problem (Liu et al. 2002; Yu et al. 2002; Denis et al. 2002; Li et al. 2003; Lee and Liu, 2003; Liu et al. 2003; Denis et al. 2003; Li et al. 2007; Elkan and Noto, 2008; Li et al. 2009; Li et al. 2010). We will discuss this learning model further in Section 3. Another related work to ours is transfer learning or domain adaptation. Unlike our problem setting, transfer learning addresses the scenario where one has little or no training data for the target domain, but has ample training data in a related domain where the data could be in a different feature space and follow a different distribution. A survey of transfer learning can be found in (Pan and Yang 2009). Several NLP researchers have studied transfer learning for different applications (Wu et al. 2009a; Yang et al. 2009; Agirre & Lac"
D10-1022,D09-1158,0,0.0129941,"and Noto, 2008; Li et al. 2009; Li et al. 2010). We will discuss this learning model further in Section 3. Another related work to ours is transfer learning or domain adaptation. Unlike our problem setting, transfer learning addresses the scenario where one has little or no training data for the target domain, but has ample training data in a related domain where the data could be in a different feature space and follow a different distribution. A survey of transfer learning can be found in (Pan and Yang 2009). Several NLP researchers have studied transfer learning for different applications (Wu et al. 2009a; Yang et al. 2009; Agirre & Lacalle 2009; Wu et al. 2009b; Sagae & Tsujii 2008; Goldwasser & Roth 2008; Li and Zong 2008; Andrew et al. 220 2008; Chan and Ng 2007; Jiang and Zhai 2007; Zhou et al. 2006), but none of them addresses the problem studied here. 3 PU Learning Techniques In traditional supervised learning, ideally, there is a large number of labeled positive and negative examples for learning. In practice, the negative examples can often be limited or unavailable. This has motivated the development of the model of learning from positive and unlabeled examples, or PU learning, where"
D10-1022,P09-2080,0,0.0161085,"and Noto, 2008; Li et al. 2009; Li et al. 2010). We will discuss this learning model further in Section 3. Another related work to ours is transfer learning or domain adaptation. Unlike our problem setting, transfer learning addresses the scenario where one has little or no training data for the target domain, but has ample training data in a related domain where the data could be in a different feature space and follow a different distribution. A survey of transfer learning can be found in (Pan and Yang 2009). Several NLP researchers have studied transfer learning for different applications (Wu et al. 2009a; Yang et al. 2009; Agirre & Lacalle 2009; Wu et al. 2009b; Sagae & Tsujii 2008; Goldwasser & Roth 2008; Li and Zong 2008; Andrew et al. 220 2008; Chan and Ng 2007; Jiang and Zhai 2007; Zhou et al. 2006), but none of them addresses the problem studied here. 3 PU Learning Techniques In traditional supervised learning, ideally, there is a large number of labeled positive and negative examples for learning. In practice, the negative examples can often be limited or unavailable. This has motivated the development of the model of learning from positive and unlabeled examples, or PU learning, where"
D10-1022,P09-1001,0,0.0114895,"i et al. 2009; Li et al. 2010). We will discuss this learning model further in Section 3. Another related work to ours is transfer learning or domain adaptation. Unlike our problem setting, transfer learning addresses the scenario where one has little or no training data for the target domain, but has ample training data in a related domain where the data could be in a different feature space and follow a different distribution. A survey of transfer learning can be found in (Pan and Yang 2009). Several NLP researchers have studied transfer learning for different applications (Wu et al. 2009a; Yang et al. 2009; Agirre & Lacalle 2009; Wu et al. 2009b; Sagae & Tsujii 2008; Goldwasser & Roth 2008; Li and Zong 2008; Andrew et al. 220 2008; Chan and Ng 2007; Jiang and Zhai 2007; Zhou et al. 2006), but none of them addresses the problem studied here. 3 PU Learning Techniques In traditional supervised learning, ideally, there is a large number of labeled positive and negative examples for learning. In practice, the negative examples can often be limited or unavailable. This has motivated the development of the model of learning from positive and unlabeled examples, or PU learning, where P denotes a set of"
D10-1022,D08-1072,0,\N,Missing
D13-1113,P11-1030,0,0.0550789,"Missing"
D13-1113,P03-1054,0,0.00902838,"s-feature. 7 Experimental Evaluation We now evaluate the proposed approach and compare it with baselines. All our experiments use the SVMperf classifier (Joachims, 2006). 1130 7.1 Experiment Setup Experiment Data: We use a set of reviews and their authors/reviewers from Amazon.com as our experiment data. We select the authors who have posted more than 30 reviews in the book category. After cleaning, we have 831 authors, 731 authors for training and 100 authors for testing. The numbers of reviews in the training and test author set are 59256 and 14308, respectively. We use the Stanford parser (Klein and Manning, 2003) to generate the grammar structure of review sentences for extracting syntactic d-features. Note that the authors here are in fact userids. However, since they are randomly selected from a large number of userids, the probability that two sampled userids belong to the same person is very small. Thus, it should be safe to assume that each userid here represents a unique author. Training data: We randomly choose 1 (one) review for each author as the query and all of his/her other reviews as q-positive reviews. The qnegative reviews consist of reviews randomly selected from the other 730 authors,"
D13-1113,C04-1088,0,0.155811,"n. This is unrealistic in practice as there is no way to know which author has and does not have multiple ids. Our work is also related to authorship attribution (AA). However, to our knowledge, our problem has not been attempted in AA. Existing works focused on two main themes: finding good writing style features, and developing effective classification methods. On finding good features (d-features in our case), it was found that the most promising features are function words (Mosteller, 1964; Argamon and Levitan, 2004; Argamon et al., 2007) and rewrite rules (Halteren et al., 1996). Length (Gamon 2004; Graham et al., 2005), richness (Halteren et al., 1996; Koppel and Schler, 2004), punctuations (Graham et al., 2005), character n-grams (Grieve, 2007; Hedegaard and Simonsen, 2011), word n-grams (Burrows, 1992; Sanderson and Guenter 2006), POS n-grams (Gamon, 2004; Hirst and Feiguina, 2007), syntactic category pairs (Narayanan et al., 2012) are also useful. On classification, numerous methods have been tried, e.g., Bayesian analysis (Mosteller, 1964), discriminant analysis (Stamatatos et al., 2000), PCA (Hoover, 2001), neural networks (Graham et al., 2005; Zheng et al., 2006; Graham et al., 2"
D13-1113,P11-2012,0,0.0169418,"(AA). However, to our knowledge, our problem has not been attempted in AA. Existing works focused on two main themes: finding good writing style features, and developing effective classification methods. On finding good features (d-features in our case), it was found that the most promising features are function words (Mosteller, 1964; Argamon and Levitan, 2004; Argamon et al., 2007) and rewrite rules (Halteren et al., 1996). Length (Gamon 2004; Graham et al., 2005), richness (Halteren et al., 1996; Koppel and Schler, 2004), punctuations (Graham et al., 2005), character n-grams (Grieve, 2007; Hedegaard and Simonsen, 2011), word n-grams (Burrows, 1992; Sanderson and Guenter 2006), POS n-grams (Gamon, 2004; Hirst and Feiguina, 2007), syntactic category pairs (Narayanan et al., 2012) are also useful. On classification, numerous methods have been tried, e.g., Bayesian analysis (Mosteller, 1964), discriminant analysis (Stamatatos et al., 2000), PCA (Hoover, 2001), neural networks (Graham et al., 2005; Zheng et al., 2006; Graham et al., 2005), clustering (Sanderson and Guenter, 2006), decision trees (Uzuner and Katz, 2005; Zhao and Zobel, 2005), regularized least squares classification (Narayanan et al., 2012), and"
D13-1113,J00-4001,0,0.0759671,"er, 1964; Argamon and Levitan, 2004; Argamon et al., 2007) and rewrite rules (Halteren et al., 1996). Length (Gamon 2004; Graham et al., 2005), richness (Halteren et al., 1996; Koppel and Schler, 2004), punctuations (Graham et al., 2005), character n-grams (Grieve, 2007; Hedegaard and Simonsen, 2011), word n-grams (Burrows, 1992; Sanderson and Guenter 2006), POS n-grams (Gamon, 2004; Hirst and Feiguina, 2007), syntactic category pairs (Narayanan et al., 2012) are also useful. On classification, numerous methods have been tried, e.g., Bayesian analysis (Mosteller, 1964), discriminant analysis (Stamatatos et al., 2000), PCA (Hoover, 2001), neural networks (Graham et al., 2005; Zheng et al., 2006; Graham et al., 2005), clustering (Sanderson and Guenter, 2006), decision trees (Uzuner and Katz, 2005; Zhao and Zobel, 2005), regularized least squares classification (Narayanan et al., 2012), and SVM (Diederich et al., 2000; Gamon 2004; Koppel and Schler, 2004; 1126 Hedegaard and Simonsen, 2011). Among them, SVM was found to be most accurate (Li et al., 2006; Kim et al., 2011). Although we also use supervised learning, we do not learn in the original document space as these existing methods do. The transformation"
D13-1113,I05-1084,0,0.0304806,"and Schler, 2004), punctuations (Graham et al., 2005), character n-grams (Grieve, 2007; Hedegaard and Simonsen, 2011), word n-grams (Burrows, 1992; Sanderson and Guenter 2006), POS n-grams (Gamon, 2004; Hirst and Feiguina, 2007), syntactic category pairs (Narayanan et al., 2012) are also useful. On classification, numerous methods have been tried, e.g., Bayesian analysis (Mosteller, 1964), discriminant analysis (Stamatatos et al., 2000), PCA (Hoover, 2001), neural networks (Graham et al., 2005; Zheng et al., 2006; Graham et al., 2005), clustering (Sanderson and Guenter, 2006), decision trees (Uzuner and Katz, 2005; Zhao and Zobel, 2005), regularized least squares classification (Narayanan et al., 2012), and SVM (Diederich et al., 2000; Gamon 2004; Koppel and Schler, 2004; 1126 Hedegaard and Simonsen, 2011). Among them, SVM was found to be most accurate (Li et al., 2006; Kim et al., 2011). Although we also use supervised learning, we do not learn in the original document space as these existing methods do. The transformation is important because it enables us to use documents from other authors in training. The traditional supervised learning (TSL) cannot do that. In our case, the only documents that TS"
D13-1113,P11-1032,0,0.1478,"Missing"
D13-1113,W06-1657,0,0.012848,"tempted in AA. Existing works focused on two main themes: finding good writing style features, and developing effective classification methods. On finding good features (d-features in our case), it was found that the most promising features are function words (Mosteller, 1964; Argamon and Levitan, 2004; Argamon et al., 2007) and rewrite rules (Halteren et al., 1996). Length (Gamon 2004; Graham et al., 2005), richness (Halteren et al., 1996; Koppel and Schler, 2004), punctuations (Graham et al., 2005), character n-grams (Grieve, 2007; Hedegaard and Simonsen, 2011), word n-grams (Burrows, 1992; Sanderson and Guenter 2006), POS n-grams (Gamon, 2004; Hirst and Feiguina, 2007), syntactic category pairs (Narayanan et al., 2012) are also useful. On classification, numerous methods have been tried, e.g., Bayesian analysis (Mosteller, 1964), discriminant analysis (Stamatatos et al., 2000), PCA (Hoover, 2001), neural networks (Graham et al., 2005; Zheng et al., 2006; Graham et al., 2005), clustering (Sanderson and Guenter, 2006), decision trees (Uzuner and Katz, 2005; Zhao and Zobel, 2005), regularized least squares classification (Narayanan et al., 2012), and SVM (Diederich et al., 2000; Gamon 2004; Koppel and Schler"
D13-1113,P12-2052,0,\N,Missing
D13-1113,C08-1065,0,\N,Missing
D13-1113,I11-1018,0,\N,Missing
D13-1172,P13-2144,0,0.00687436,"ardie, 2010; Jakob and Gurevych, 2010; Kobayashi et al., 2007; Li et al., 2010); 3) Topic models (Branavan et al., 2008; Brody and Elhadad, 2010; Fang and Huang, 2012; Jo and Oh, 2011; Kim et al., 2013; Lazaridou et al., 2013; Li et al., 2011; Lin and He, 2009; Lu et al., 2009, 2012, 2011; Lu and Zhai, 2008; Mei et al., 2007; Moghaddam and Ester, 2011; Mukherjee and Liu, 2012; Sauper et al., 2011; Titov and McDonald, 2008; Wang et al., 2010, 2011; Zhao et al., 2010). Other approaches include shallow semantic parsing (Li et al., 2012b), bootstrapping (Xia et al., 2009), Non-English techniques (Abu-Jbara et al., 2013; Zhou et al., 2012), graph-based representation (Wu et al., 2011), convolution kernels (Wiegand and Klakow, 2010) and domain adaption (Li et al., 2012). Stoyanov and Cardie (2011), Wang and Liu (2011), and Meng et al. (2012) studied opinion summarization outside the reviews. Some other works related with sentiment analysis include (Agarwal and Sabharwal, 2012; Kennedy and Inkpen, 2006; Kim et al., 2009; Mohammad et al., 2009). In this work, we focus on topic models owing to their advantage of performing both aspect extraction and clustering simultaneously. All other approaches only perform ex"
D13-1172,W12-5504,0,0.01952,"kherjee and Liu, 2012; Sauper et al., 2011; Titov and McDonald, 2008; Wang et al., 2010, 2011; Zhao et al., 2010). Other approaches include shallow semantic parsing (Li et al., 2012b), bootstrapping (Xia et al., 2009), Non-English techniques (Abu-Jbara et al., 2013; Zhou et al., 2012), graph-based representation (Wu et al., 2011), convolution kernels (Wiegand and Klakow, 2010) and domain adaption (Li et al., 2012). Stoyanov and Cardie (2011), Wang and Liu (2011), and Meng et al. (2012) studied opinion summarization outside the reviews. Some other works related with sentiment analysis include (Agarwal and Sabharwal, 2012; Kennedy and Inkpen, 2006; Kim et al., 2009; Mohammad et al., 2009). In this work, we focus on topic models owing to their advantage of performing both aspect extraction and clustering simultaneously. All other approaches only perform extraction. Although there are several related works on clustering aspect terms (e.g., Carenini et al., 2005; Guo et al., 2009; Zhai et al., 2011), they all assume that the aspect terms have been extracted beforehand. We also notice that some aspect extraction models in sentiment analysis separately discover aspect words and aspect specific sentiment words (e.g."
D13-1172,P08-1031,0,0.243219,"loff, 2005; Wiebe et al., 2004). According to (Liu, 2012), there are three main approaches to aspect extraction: 1) Using word frequency and syntactic dependency of aspects and sentiment words for extraction (e.g., Blairgoldensohn et al., 2008; Hu and Liu, 2004; Ku et al., 2006; Popescu and Etzioni, 2005; Qiu et al., 2011; Somasundaran and Wiebe, 2009; Wu et al., 2009; Yu et al., 2011; Zhang and Liu, 2011; Zhuang et al., 2006); 2) Using supervised sequence labeling/classification (e.g., Choi and Cardie, 2010; Jakob and Gurevych, 2010; Kobayashi et al., 2007; Li et al., 2010); 3) Topic models (Branavan et al., 2008; Brody and Elhadad, 2010; Fang and Huang, 2012; Jo and Oh, 2011; Kim et al., 2013; Lazaridou et al., 2013; Li et al., 2011; Lin and He, 2009; Lu et al., 2009, 2012, 2011; Lu and Zhai, 2008; Mei et al., 2007; Moghaddam and Ester, 2011; Mukherjee and Liu, 2012; Sauper et al., 2011; Titov and McDonald, 2008; Wang et al., 2010, 2011; Zhao et al., 2010). Other approaches include shallow semantic parsing (Li et al., 2012b), bootstrapping (Xia et al., 2009), Non-English techniques (Abu-Jbara et al., 2013; Zhou et al., 2012), graph-based representation (Wu et al., 2011), convolution kernels (Wiegand"
D13-1172,N10-1122,0,0.627544,"., 2004). According to (Liu, 2012), there are three main approaches to aspect extraction: 1) Using word frequency and syntactic dependency of aspects and sentiment words for extraction (e.g., Blairgoldensohn et al., 2008; Hu and Liu, 2004; Ku et al., 2006; Popescu and Etzioni, 2005; Qiu et al., 2011; Somasundaran and Wiebe, 2009; Wu et al., 2009; Yu et al., 2011; Zhang and Liu, 2011; Zhuang et al., 2006); 2) Using supervised sequence labeling/classification (e.g., Choi and Cardie, 2010; Jakob and Gurevych, 2010; Kobayashi et al., 2007; Li et al., 2010); 3) Topic models (Branavan et al., 2008; Brody and Elhadad, 2010; Fang and Huang, 2012; Jo and Oh, 2011; Kim et al., 2013; Lazaridou et al., 2013; Li et al., 2011; Lin and He, 2009; Lu et al., 2009, 2012, 2011; Lu and Zhai, 2008; Mei et al., 2007; Moghaddam and Ester, 2011; Mukherjee and Liu, 2012; Sauper et al., 2011; Titov and McDonald, 2008; Wang et al., 2010, 2011; Zhao et al., 2010). Other approaches include shallow semantic parsing (Li et al., 2012b), bootstrapping (Xia et al., 2009), Non-English techniques (Abu-Jbara et al., 2013; Zhou et al., 2012), graph-based representation (Wu et al., 2011), convolution kernels (Wiegand and Klakow, 2010) and dom"
D13-1172,P10-2050,0,0.0268811,"nt analysis has been studied extensively in recent years (Hu and Liu, 2004; Pang and Lee, 2008; Wiebe and Riloff, 2005; Wiebe et al., 2004). According to (Liu, 2012), there are three main approaches to aspect extraction: 1) Using word frequency and syntactic dependency of aspects and sentiment words for extraction (e.g., Blairgoldensohn et al., 2008; Hu and Liu, 2004; Ku et al., 2006; Popescu and Etzioni, 2005; Qiu et al., 2011; Somasundaran and Wiebe, 2009; Wu et al., 2009; Yu et al., 2011; Zhang and Liu, 2011; Zhuang et al., 2006); 2) Using supervised sequence labeling/classification (e.g., Choi and Cardie, 2010; Jakob and Gurevych, 2010; Kobayashi et al., 2007; Li et al., 2010); 3) Topic models (Branavan et al., 2008; Brody and Elhadad, 2010; Fang and Huang, 2012; Jo and Oh, 2011; Kim et al., 2013; Lazaridou et al., 2013; Li et al., 2011; Lin and He, 2009; Lu et al., 2009, 2012, 2011; Lu and Zhai, 2008; Mei et al., 2007; Moghaddam and Ester, 2011; Mukherjee and Liu, 2012; Sauper et al., 2011; Titov and McDonald, 2008; Wang et al., 2010, 2011; Zhao et al., 2010). Other approaches include shallow semantic parsing (Li et al., 2012b), bootstrapping (Xia et al., 2009), Non-English techniques (Abu-Jbara e"
D13-1172,P12-2065,0,0.0416553,"iu, 2012), there are three main approaches to aspect extraction: 1) Using word frequency and syntactic dependency of aspects and sentiment words for extraction (e.g., Blairgoldensohn et al., 2008; Hu and Liu, 2004; Ku et al., 2006; Popescu and Etzioni, 2005; Qiu et al., 2011; Somasundaran and Wiebe, 2009; Wu et al., 2009; Yu et al., 2011; Zhang and Liu, 2011; Zhuang et al., 2006); 2) Using supervised sequence labeling/classification (e.g., Choi and Cardie, 2010; Jakob and Gurevych, 2010; Kobayashi et al., 2007; Li et al., 2010); 3) Topic models (Branavan et al., 2008; Brody and Elhadad, 2010; Fang and Huang, 2012; Jo and Oh, 2011; Kim et al., 2013; Lazaridou et al., 2013; Li et al., 2011; Lin and He, 2009; Lu et al., 2009, 2012, 2011; Lu and Zhai, 2008; Mei et al., 2007; Moghaddam and Ester, 2011; Mukherjee and Liu, 2012; Sauper et al., 2011; Titov and McDonald, 2008; Wang et al., 2010, 2011; Zhao et al., 2010). Other approaches include shallow semantic parsing (Li et al., 2012b), bootstrapping (Xia et al., 2009), Non-English techniques (Abu-Jbara et al., 2013; Zhou et al., 2012), graph-based representation (Wu et al., 2011), convolution kernels (Wiegand and Klakow, 2010) and domain adaption (Li et al"
D13-1172,P11-1026,0,0.0207197,"), they all assume that the aspect terms have been extracted beforehand. We also notice that some aspect extraction models in sentiment analysis separately discover aspect words and aspect specific sentiment words (e.g., Sauper and Barzilay, 2013; Zhao et al., 2010). Our proposed model does not separate them as most sen1657 timent words also imply aspects and most adjectives modify specific attributes of objects. For example, sentiment words expensive and beautiful imply aspects price and appearance respectively. Regarding the knowledge-based models, besides those discussed in § 1, the model (Hu et al., 2011) enables the user to provide guidance interactively. Blei and McAuliffe (2007) and Ramage et al. (2009) used document labels in supervised setting. In (Chen et al., 2013a), we proposed MDKLDA to leverage multi-domain knowledge, which serves as the basic mechanism to exploit m-sets in MC-LDA. In (Chen et al., 2013b), we proposed a framework (called GK-LDA) to explicitly deal with the wrong knowledge when exploring the lexical semantic relations as the general (domain independent) knowledge in topic models. But these models above did not consider the knowledge in the form of c-sets (or cannot-li"
D13-1172,E12-1021,0,0.304875,"models do not always correlate well with human judgments and needs (Chang et al., 2009). To address the issue, several knowledge-based topic models have been proposed. The DF-LDA model (Andrzejewski et al., 2009) incorporates two forms of prior knowledge, also called two types of constraints: must-links and cannot-links. A must-link states that two words (or terms) should belong to the same topic whereas a cannotlink indicates that two words should not be in the same topic. In (Andrzejewski et al., 2011), more general knowledge can be specified using firstorder logic. In (Burns et al., 2012; Jagarlamudi et al., 2012; Lu et al., 2011; Mukherjee and Liu, 2012), seeded models were proposed. They enable the user to specify prior knowledge as seed words/terms for some topics. Petterson et al. (2010) also used word similarity as priors for guidance. However, none of the existing models is capable of incorporating the cannot-link type of knowledge except DF-LDA (Andrzejewski et al., 2009). Furthermore, none of the existing models, including DF-LDA, is able to automatically adjust the number of topics based on domain knowledge. The domain knowledge, such as cannot-links, may change the number of topics. There ar"
D13-1172,D10-1101,0,0.00996538,"udied extensively in recent years (Hu and Liu, 2004; Pang and Lee, 2008; Wiebe and Riloff, 2005; Wiebe et al., 2004). According to (Liu, 2012), there are three main approaches to aspect extraction: 1) Using word frequency and syntactic dependency of aspects and sentiment words for extraction (e.g., Blairgoldensohn et al., 2008; Hu and Liu, 2004; Ku et al., 2006; Popescu and Etzioni, 2005; Qiu et al., 2011; Somasundaran and Wiebe, 2009; Wu et al., 2009; Yu et al., 2011; Zhang and Liu, 2011; Zhuang et al., 2006); 2) Using supervised sequence labeling/classification (e.g., Choi and Cardie, 2010; Jakob and Gurevych, 2010; Kobayashi et al., 2007; Li et al., 2010); 3) Topic models (Branavan et al., 2008; Brody and Elhadad, 2010; Fang and Huang, 2012; Jo and Oh, 2011; Kim et al., 2013; Lazaridou et al., 2013; Li et al., 2011; Lin and He, 2009; Lu et al., 2009, 2012, 2011; Lu and Zhai, 2008; Mei et al., 2007; Moghaddam and Ester, 2011; Mukherjee and Liu, 2012; Sauper et al., 2011; Titov and McDonald, 2008; Wang et al., 2010, 2011; Zhao et al., 2010). Other approaches include shallow semantic parsing (Li et al., 2012b), bootstrapping (Xia et al., 2009), Non-English techniques (Abu-Jbara et al., 2013; Zhou et al.,"
D13-1172,P09-1029,0,0.0222673,"cDonald, 2008; Wang et al., 2010, 2011; Zhao et al., 2010). Other approaches include shallow semantic parsing (Li et al., 2012b), bootstrapping (Xia et al., 2009), Non-English techniques (Abu-Jbara et al., 2013; Zhou et al., 2012), graph-based representation (Wu et al., 2011), convolution kernels (Wiegand and Klakow, 2010) and domain adaption (Li et al., 2012). Stoyanov and Cardie (2011), Wang and Liu (2011), and Meng et al. (2012) studied opinion summarization outside the reviews. Some other works related with sentiment analysis include (Agarwal and Sabharwal, 2012; Kennedy and Inkpen, 2006; Kim et al., 2009; Mohammad et al., 2009). In this work, we focus on topic models owing to their advantage of performing both aspect extraction and clustering simultaneously. All other approaches only perform extraction. Although there are several related works on clustering aspect terms (e.g., Carenini et al., 2005; Guo et al., 2009; Zhai et al., 2011), they all assume that the aspect terms have been extracted beforehand. We also notice that some aspect extraction models in sentiment analysis separately discover aspect words and aspect specific sentiment words (e.g., Sauper and Barzilay, 2013; Zhao et al., 20"
D13-1172,D07-1114,0,0.220082,"t years (Hu and Liu, 2004; Pang and Lee, 2008; Wiebe and Riloff, 2005; Wiebe et al., 2004). According to (Liu, 2012), there are three main approaches to aspect extraction: 1) Using word frequency and syntactic dependency of aspects and sentiment words for extraction (e.g., Blairgoldensohn et al., 2008; Hu and Liu, 2004; Ku et al., 2006; Popescu and Etzioni, 2005; Qiu et al., 2011; Somasundaran and Wiebe, 2009; Wu et al., 2009; Yu et al., 2011; Zhang and Liu, 2011; Zhuang et al., 2006); 2) Using supervised sequence labeling/classification (e.g., Choi and Cardie, 2010; Jakob and Gurevych, 2010; Kobayashi et al., 2007; Li et al., 2010); 3) Topic models (Branavan et al., 2008; Brody and Elhadad, 2010; Fang and Huang, 2012; Jo and Oh, 2011; Kim et al., 2013; Lazaridou et al., 2013; Li et al., 2011; Lin and He, 2009; Lu et al., 2009, 2012, 2011; Lu and Zhai, 2008; Mei et al., 2007; Moghaddam and Ester, 2011; Mukherjee and Liu, 2012; Sauper et al., 2011; Titov and McDonald, 2008; Wang et al., 2010, 2011; Zhao et al., 2010). Other approaches include shallow semantic parsing (Li et al., 2012b), bootstrapping (Xia et al., 2009), Non-English techniques (Abu-Jbara et al., 2013; Zhou et al., 2012), graph-based repre"
D13-1172,P13-1160,0,0.0332015,"action: 1) Using word frequency and syntactic dependency of aspects and sentiment words for extraction (e.g., Blairgoldensohn et al., 2008; Hu and Liu, 2004; Ku et al., 2006; Popescu and Etzioni, 2005; Qiu et al., 2011; Somasundaran and Wiebe, 2009; Wu et al., 2009; Yu et al., 2011; Zhang and Liu, 2011; Zhuang et al., 2006); 2) Using supervised sequence labeling/classification (e.g., Choi and Cardie, 2010; Jakob and Gurevych, 2010; Kobayashi et al., 2007; Li et al., 2010); 3) Topic models (Branavan et al., 2008; Brody and Elhadad, 2010; Fang and Huang, 2012; Jo and Oh, 2011; Kim et al., 2013; Lazaridou et al., 2013; Li et al., 2011; Lin and He, 2009; Lu et al., 2009, 2012, 2011; Lu and Zhai, 2008; Mei et al., 2007; Moghaddam and Ester, 2011; Mukherjee and Liu, 2012; Sauper et al., 2011; Titov and McDonald, 2008; Wang et al., 2010, 2011; Zhao et al., 2010). Other approaches include shallow semantic parsing (Li et al., 2012b), bootstrapping (Xia et al., 2009), Non-English techniques (Abu-Jbara et al., 2013; Zhou et al., 2012), graph-based representation (Wu et al., 2011), convolution kernels (Wiegand and Klakow, 2010) and domain adaption (Li et al., 2012). Stoyanov and Cardie (2011), Wang and Liu (2011),"
D13-1172,P12-1043,0,0.006823,"2006); 2) Using supervised sequence labeling/classification (e.g., Choi and Cardie, 2010; Jakob and Gurevych, 2010; Kobayashi et al., 2007; Li et al., 2010); 3) Topic models (Branavan et al., 2008; Brody and Elhadad, 2010; Fang and Huang, 2012; Jo and Oh, 2011; Kim et al., 2013; Lazaridou et al., 2013; Li et al., 2011; Lin and He, 2009; Lu et al., 2009, 2012, 2011; Lu and Zhai, 2008; Mei et al., 2007; Moghaddam and Ester, 2011; Mukherjee and Liu, 2012; Sauper et al., 2011; Titov and McDonald, 2008; Wang et al., 2010, 2011; Zhao et al., 2010). Other approaches include shallow semantic parsing (Li et al., 2012b), bootstrapping (Xia et al., 2009), Non-English techniques (Abu-Jbara et al., 2013; Zhou et al., 2012), graph-based representation (Wu et al., 2011), convolution kernels (Wiegand and Klakow, 2010) and domain adaption (Li et al., 2012). Stoyanov and Cardie (2011), Wang and Liu (2011), and Meng et al. (2012) studied opinion summarization outside the reviews. Some other works related with sentiment analysis include (Agarwal and Sabharwal, 2012; Kennedy and Inkpen, 2006; Kim et al., 2009; Mohammad et al., 2009). In this work, we focus on topic models owing to their advantage of performing both a"
D13-1172,D11-1105,0,0.11627,"Missing"
D13-1172,D11-1024,0,0.743291,"t al. (2009) used document labels in supervised setting. In (Chen et al., 2013a), we proposed MDKLDA to leverage multi-domain knowledge, which serves as the basic mechanism to exploit m-sets in MC-LDA. In (Chen et al., 2013b), we proposed a framework (called GK-LDA) to explicitly deal with the wrong knowledge when exploring the lexical semantic relations as the general (domain independent) knowledge in topic models. But these models above did not consider the knowledge in the form of c-sets (or cannot-links). The generalized Pólya urn (GPU) model (Mahmoud, 2008) was first introduced in LDA by Mimno et al. (2011). However, Mimno et al. (2011) did not use domain knowledge. Our results in § 7 show that using domain knowledge can significantly improve aspect extraction. The GPU model was also employed in topic models in our work of (Chen et al., 2013a, 2013b). In this paper, we propose the Extended GPU (E-GPU) model. The EGPU model is more powerful in handling complex situations in dealing with c-sets. 3 Dealing with M-sets and Multiple Senses Since the proposed MC-LDA model is a major extension to our earlier work in (Chen et al., 2013a), which can deal with m-sets, we include this earlier work here as"
D13-1172,D09-1063,0,0.0659356,"g et al., 2010, 2011; Zhao et al., 2010). Other approaches include shallow semantic parsing (Li et al., 2012b), bootstrapping (Xia et al., 2009), Non-English techniques (Abu-Jbara et al., 2013; Zhou et al., 2012), graph-based representation (Wu et al., 2011), convolution kernels (Wiegand and Klakow, 2010) and domain adaption (Li et al., 2012). Stoyanov and Cardie (2011), Wang and Liu (2011), and Meng et al. (2012) studied opinion summarization outside the reviews. Some other works related with sentiment analysis include (Agarwal and Sabharwal, 2012; Kennedy and Inkpen, 2006; Kim et al., 2009; Mohammad et al., 2009). In this work, we focus on topic models owing to their advantage of performing both aspect extraction and clustering simultaneously. All other approaches only perform extraction. Although there are several related works on clustering aspect terms (e.g., Carenini et al., 2005; Guo et al., 2009; Zhai et al., 2011), they all assume that the aspect terms have been extracted beforehand. We also notice that some aspect extraction models in sentiment analysis separately discover aspect words and aspect specific sentiment words (e.g., Sauper and Barzilay, 2013; Zhao et al., 2010). Our proposed model"
D13-1172,P12-1036,1,0.675801,"uman judgments and needs (Chang et al., 2009). To address the issue, several knowledge-based topic models have been proposed. The DF-LDA model (Andrzejewski et al., 2009) incorporates two forms of prior knowledge, also called two types of constraints: must-links and cannot-links. A must-link states that two words (or terms) should belong to the same topic whereas a cannotlink indicates that two words should not be in the same topic. In (Andrzejewski et al., 2011), more general knowledge can be specified using firstorder logic. In (Burns et al., 2012; Jagarlamudi et al., 2012; Lu et al., 2011; Mukherjee and Liu, 2012), seeded models were proposed. They enable the user to specify prior knowledge as seed words/terms for some topics. Petterson et al. (2010) also used word similarity as priors for guidance. However, none of the existing models is capable of incorporating the cannot-link type of knowledge except DF-LDA (Andrzejewski et al., 2009). Furthermore, none of the existing models, including DF-LDA, is able to automatically adjust the number of topics based on domain knowledge. The domain knowledge, such as cannot-links, may change the number of topics. There are two types of cannot-links: consistent and"
D13-1172,H05-1043,0,0.177233,"ion has been conducted to compare MC-LDA with several state-ofthe-art models. Experimental results based on both qualitative and quantitative measures demonstrate the superiority of MC-LDA. 2 Related Work Sentiment analysis has been studied extensively in recent years (Hu and Liu, 2004; Pang and Lee, 2008; Wiebe and Riloff, 2005; Wiebe et al., 2004). According to (Liu, 2012), there are three main approaches to aspect extraction: 1) Using word frequency and syntactic dependency of aspects and sentiment words for extraction (e.g., Blairgoldensohn et al., 2008; Hu and Liu, 2004; Ku et al., 2006; Popescu and Etzioni, 2005; Qiu et al., 2011; Somasundaran and Wiebe, 2009; Wu et al., 2009; Yu et al., 2011; Zhang and Liu, 2011; Zhuang et al., 2006); 2) Using supervised sequence labeling/classification (e.g., Choi and Cardie, 2010; Jakob and Gurevych, 2010; Kobayashi et al., 2007; Li et al., 2010); 3) Topic models (Branavan et al., 2008; Brody and Elhadad, 2010; Fang and Huang, 2012; Jo and Oh, 2011; Kim et al., 2013; Lazaridou et al., 2013; Li et al., 2011; Lin and He, 2009; Lu et al., 2009, 2012, 2011; Lu and Zhai, 2008; Mei et al., 2007; Moghaddam and Ester, 2011; Mukherjee and Liu, 2012; Sauper et al., 2011; Ti"
D13-1172,J11-1002,1,0.076455,"ompare MC-LDA with several state-ofthe-art models. Experimental results based on both qualitative and quantitative measures demonstrate the superiority of MC-LDA. 2 Related Work Sentiment analysis has been studied extensively in recent years (Hu and Liu, 2004; Pang and Lee, 2008; Wiebe and Riloff, 2005; Wiebe et al., 2004). According to (Liu, 2012), there are three main approaches to aspect extraction: 1) Using word frequency and syntactic dependency of aspects and sentiment words for extraction (e.g., Blairgoldensohn et al., 2008; Hu and Liu, 2004; Ku et al., 2006; Popescu and Etzioni, 2005; Qiu et al., 2011; Somasundaran and Wiebe, 2009; Wu et al., 2009; Yu et al., 2011; Zhang and Liu, 2011; Zhuang et al., 2006); 2) Using supervised sequence labeling/classification (e.g., Choi and Cardie, 2010; Jakob and Gurevych, 2010; Kobayashi et al., 2007; Li et al., 2010); 3) Topic models (Branavan et al., 2008; Brody and Elhadad, 2010; Fang and Huang, 2012; Jo and Oh, 2011; Kim et al., 2013; Lazaridou et al., 2013; Li et al., 2011; Lin and He, 2009; Lu et al., 2009, 2012, 2011; Lu and Zhai, 2008; Mei et al., 2007; Moghaddam and Ester, 2011; Mukherjee and Liu, 2012; Sauper et al., 2011; Titov and McDonald,"
D13-1172,D09-1026,0,0.0577308,"spect extraction models in sentiment analysis separately discover aspect words and aspect specific sentiment words (e.g., Sauper and Barzilay, 2013; Zhao et al., 2010). Our proposed model does not separate them as most sen1657 timent words also imply aspects and most adjectives modify specific attributes of objects. For example, sentiment words expensive and beautiful imply aspects price and appearance respectively. Regarding the knowledge-based models, besides those discussed in § 1, the model (Hu et al., 2011) enables the user to provide guidance interactively. Blei and McAuliffe (2007) and Ramage et al. (2009) used document labels in supervised setting. In (Chen et al., 2013a), we proposed MDKLDA to leverage multi-domain knowledge, which serves as the basic mechanism to exploit m-sets in MC-LDA. In (Chen et al., 2013b), we proposed a framework (called GK-LDA) to explicitly deal with the wrong knowledge when exploring the lexical semantic relations as the general (domain independent) knowledge in topic models. But these models above did not consider the knowledge in the form of c-sets (or cannot-links). The generalized Pólya urn (GPU) model (Mahmoud, 2008) was first introduced in LDA by Mimno et al."
D13-1172,P11-1036,0,0.0753179,"pescu and Etzioni, 2005; Qiu et al., 2011; Somasundaran and Wiebe, 2009; Wu et al., 2009; Yu et al., 2011; Zhang and Liu, 2011; Zhuang et al., 2006); 2) Using supervised sequence labeling/classification (e.g., Choi and Cardie, 2010; Jakob and Gurevych, 2010; Kobayashi et al., 2007; Li et al., 2010); 3) Topic models (Branavan et al., 2008; Brody and Elhadad, 2010; Fang and Huang, 2012; Jo and Oh, 2011; Kim et al., 2013; Lazaridou et al., 2013; Li et al., 2011; Lin and He, 2009; Lu et al., 2009, 2012, 2011; Lu and Zhai, 2008; Mei et al., 2007; Moghaddam and Ester, 2011; Mukherjee and Liu, 2012; Sauper et al., 2011; Titov and McDonald, 2008; Wang et al., 2010, 2011; Zhao et al., 2010). Other approaches include shallow semantic parsing (Li et al., 2012b), bootstrapping (Xia et al., 2009), Non-English techniques (Abu-Jbara et al., 2013; Zhou et al., 2012), graph-based representation (Wu et al., 2011), convolution kernels (Wiegand and Klakow, 2010) and domain adaption (Li et al., 2012). Stoyanov and Cardie (2011), Wang and Liu (2011), and Meng et al. (2012) studied opinion summarization outside the reviews. Some other works related with sentiment analysis include (Agarwal and Sabharwal, 2012; Kennedy and I"
D13-1172,P09-1026,0,0.0069728,"several state-ofthe-art models. Experimental results based on both qualitative and quantitative measures demonstrate the superiority of MC-LDA. 2 Related Work Sentiment analysis has been studied extensively in recent years (Hu and Liu, 2004; Pang and Lee, 2008; Wiebe and Riloff, 2005; Wiebe et al., 2004). According to (Liu, 2012), there are three main approaches to aspect extraction: 1) Using word frequency and syntactic dependency of aspects and sentiment words for extraction (e.g., Blairgoldensohn et al., 2008; Hu and Liu, 2004; Ku et al., 2006; Popescu and Etzioni, 2005; Qiu et al., 2011; Somasundaran and Wiebe, 2009; Wu et al., 2009; Yu et al., 2011; Zhang and Liu, 2011; Zhuang et al., 2006); 2) Using supervised sequence labeling/classification (e.g., Choi and Cardie, 2010; Jakob and Gurevych, 2010; Kobayashi et al., 2007; Li et al., 2010); 3) Topic models (Branavan et al., 2008; Brody and Elhadad, 2010; Fang and Huang, 2012; Jo and Oh, 2011; Kim et al., 2013; Lazaridou et al., 2013; Li et al., 2011; Lin and He, 2009; Lu et al., 2009, 2012, 2011; Lu and Zhai, 2008; Mei et al., 2007; Moghaddam and Ester, 2011; Mukherjee and Liu, 2012; Sauper et al., 2011; Titov and McDonald, 2008; Wang et al., 2010, 2011;"
D13-1172,D12-1087,0,0.0211167,"plexity on held-out test data. However, the perplexity metric does not reflect the semantic coherence of individual topics learned by a topic model (Newman et al., 2010). Recent research has shown potential issues with perplexity as a measure: (Chang et al., 2009) suggested that the perplexity can sometimes be contrary to human judgments. Also, perplexity does not really reflect our goal of finding coherent aspects with accurate semantic clustering. It only provides a measure of how well the model fits the data. The Topic Coherence metric (Mimno et al., 2011) (also called the “UMass” measure (Stevens and Buttler, 2012)) was proposed as a better alternative for assessing topic quality. This metric relies upon word co-occurrence statistics within the documents, and does not depend on external resources or human labeling. It was shown that topic coherence is highly consistent with human expert labeling by Mimno et al. (2011). Higher topic coherence score indicates higher quality of topics, i.e., better topic interpretability. Effects of Number of Topics Since our proposed models and the baseline models are all parametric models, we first compare each model given different numbers of topics. Figure 4 shows the"
D13-1172,R11-1028,0,0.0183588,"h, 2011; Kim et al., 2013; Lazaridou et al., 2013; Li et al., 2011; Lin and He, 2009; Lu et al., 2009, 2012, 2011; Lu and Zhai, 2008; Mei et al., 2007; Moghaddam and Ester, 2011; Mukherjee and Liu, 2012; Sauper et al., 2011; Titov and McDonald, 2008; Wang et al., 2010, 2011; Zhao et al., 2010). Other approaches include shallow semantic parsing (Li et al., 2012b), bootstrapping (Xia et al., 2009), Non-English techniques (Abu-Jbara et al., 2013; Zhou et al., 2012), graph-based representation (Wu et al., 2011), convolution kernels (Wiegand and Klakow, 2010) and domain adaption (Li et al., 2012). Stoyanov and Cardie (2011), Wang and Liu (2011), and Meng et al. (2012) studied opinion summarization outside the reviews. Some other works related with sentiment analysis include (Agarwal and Sabharwal, 2012; Kennedy and Inkpen, 2006; Kim et al., 2009; Mohammad et al., 2009). In this work, we focus on topic models owing to their advantage of performing both aspect extraction and clustering simultaneously. All other approaches only perform extraction. Although there are several related works on clustering aspect terms (e.g., Carenini et al., 2005; Guo et al., 2009; Zhai et al., 2011), they all assume that the aspect te"
D13-1172,P11-1034,0,0.010915,"azaridou et al., 2013; Li et al., 2011; Lin and He, 2009; Lu et al., 2009, 2012, 2011; Lu and Zhai, 2008; Mei et al., 2007; Moghaddam and Ester, 2011; Mukherjee and Liu, 2012; Sauper et al., 2011; Titov and McDonald, 2008; Wang et al., 2010, 2011; Zhao et al., 2010). Other approaches include shallow semantic parsing (Li et al., 2012b), bootstrapping (Xia et al., 2009), Non-English techniques (Abu-Jbara et al., 2013; Zhou et al., 2012), graph-based representation (Wu et al., 2011), convolution kernels (Wiegand and Klakow, 2010) and domain adaption (Li et al., 2012). Stoyanov and Cardie (2011), Wang and Liu (2011), and Meng et al. (2012) studied opinion summarization outside the reviews. Some other works related with sentiment analysis include (Agarwal and Sabharwal, 2012; Kennedy and Inkpen, 2006; Kim et al., 2009; Mohammad et al., 2009). In this work, we focus on topic models owing to their advantage of performing both aspect extraction and clustering simultaneously. All other approaches only perform extraction. Although there are several related works on clustering aspect terms (e.g., Carenini et al., 2005; Guo et al., 2009; Zhai et al., 2011), they all assume that the aspect terms have been extract"
D13-1172,J04-3002,0,0.0168216,"s MC-LDA in terms of capabilities. 2. It proposed the E-GPU model to enable multiurn interactions, which enables c-sets to be naturally integrated into a topic model. To the best of our knowledge, E-GPU has not been proposed and used before. 3. A comprehensive evaluation has been conducted to compare MC-LDA with several state-ofthe-art models. Experimental results based on both qualitative and quantitative measures demonstrate the superiority of MC-LDA. 2 Related Work Sentiment analysis has been studied extensively in recent years (Hu and Liu, 2004; Pang and Lee, 2008; Wiebe and Riloff, 2005; Wiebe et al., 2004). According to (Liu, 2012), there are three main approaches to aspect extraction: 1) Using word frequency and syntactic dependency of aspects and sentiment words for extraction (e.g., Blairgoldensohn et al., 2008; Hu and Liu, 2004; Ku et al., 2006; Popescu and Etzioni, 2005; Qiu et al., 2011; Somasundaran and Wiebe, 2009; Wu et al., 2009; Yu et al., 2011; Zhang and Liu, 2011; Zhuang et al., 2006); 2) Using supervised sequence labeling/classification (e.g., Choi and Cardie, 2010; Jakob and Gurevych, 2010; Kobayashi et al., 2007; Li et al., 2010); 3) Topic models (Branavan et al., 2008; Brody an"
D13-1172,N10-1121,0,0.0101967,"l., 2008; Brody and Elhadad, 2010; Fang and Huang, 2012; Jo and Oh, 2011; Kim et al., 2013; Lazaridou et al., 2013; Li et al., 2011; Lin and He, 2009; Lu et al., 2009, 2012, 2011; Lu and Zhai, 2008; Mei et al., 2007; Moghaddam and Ester, 2011; Mukherjee and Liu, 2012; Sauper et al., 2011; Titov and McDonald, 2008; Wang et al., 2010, 2011; Zhao et al., 2010). Other approaches include shallow semantic parsing (Li et al., 2012b), bootstrapping (Xia et al., 2009), Non-English techniques (Abu-Jbara et al., 2013; Zhou et al., 2012), graph-based representation (Wu et al., 2011), convolution kernels (Wiegand and Klakow, 2010) and domain adaption (Li et al., 2012). Stoyanov and Cardie (2011), Wang and Liu (2011), and Meng et al. (2012) studied opinion summarization outside the reviews. Some other works related with sentiment analysis include (Agarwal and Sabharwal, 2012; Kennedy and Inkpen, 2006; Kim et al., 2009; Mohammad et al., 2009). In this work, we focus on topic models owing to their advantage of performing both aspect extraction and clustering simultaneously. All other approaches only perform extraction. Although there are several related works on clustering aspect terms (e.g., Carenini et al., 2005; Guo et"
D13-1172,D09-1159,0,0.0319781,"s. Experimental results based on both qualitative and quantitative measures demonstrate the superiority of MC-LDA. 2 Related Work Sentiment analysis has been studied extensively in recent years (Hu and Liu, 2004; Pang and Lee, 2008; Wiebe and Riloff, 2005; Wiebe et al., 2004). According to (Liu, 2012), there are three main approaches to aspect extraction: 1) Using word frequency and syntactic dependency of aspects and sentiment words for extraction (e.g., Blairgoldensohn et al., 2008; Hu and Liu, 2004; Ku et al., 2006; Popescu and Etzioni, 2005; Qiu et al., 2011; Somasundaran and Wiebe, 2009; Wu et al., 2009; Yu et al., 2011; Zhang and Liu, 2011; Zhuang et al., 2006); 2) Using supervised sequence labeling/classification (e.g., Choi and Cardie, 2010; Jakob and Gurevych, 2010; Kobayashi et al., 2007; Li et al., 2010); 3) Topic models (Branavan et al., 2008; Brody and Elhadad, 2010; Fang and Huang, 2012; Jo and Oh, 2011; Kim et al., 2013; Lazaridou et al., 2013; Li et al., 2011; Lin and He, 2009; Lu et al., 2009, 2012, 2011; Lu and Zhai, 2008; Mei et al., 2007; Moghaddam and Ester, 2011; Mukherjee and Liu, 2012; Sauper et al., 2011; Titov and McDonald, 2008; Wang et al., 2010, 2011; Zhao et al., 201"
D13-1172,D11-1123,0,0.015938,", 2010); 3) Topic models (Branavan et al., 2008; Brody and Elhadad, 2010; Fang and Huang, 2012; Jo and Oh, 2011; Kim et al., 2013; Lazaridou et al., 2013; Li et al., 2011; Lin and He, 2009; Lu et al., 2009, 2012, 2011; Lu and Zhai, 2008; Mei et al., 2007; Moghaddam and Ester, 2011; Mukherjee and Liu, 2012; Sauper et al., 2011; Titov and McDonald, 2008; Wang et al., 2010, 2011; Zhao et al., 2010). Other approaches include shallow semantic parsing (Li et al., 2012b), bootstrapping (Xia et al., 2009), Non-English techniques (Abu-Jbara et al., 2013; Zhou et al., 2012), graph-based representation (Wu et al., 2011), convolution kernels (Wiegand and Klakow, 2010) and domain adaption (Li et al., 2012). Stoyanov and Cardie (2011), Wang and Liu (2011), and Meng et al. (2012) studied opinion summarization outside the reviews. Some other works related with sentiment analysis include (Agarwal and Sabharwal, 2012; Kennedy and Inkpen, 2006; Kim et al., 2009; Mohammad et al., 2009). In this work, we focus on topic models owing to their advantage of performing both aspect extraction and clustering simultaneously. All other approaches only perform extraction. Although there are several related works on clustering a"
D13-1172,P11-1150,0,0.167766,"esults based on both qualitative and quantitative measures demonstrate the superiority of MC-LDA. 2 Related Work Sentiment analysis has been studied extensively in recent years (Hu and Liu, 2004; Pang and Lee, 2008; Wiebe and Riloff, 2005; Wiebe et al., 2004). According to (Liu, 2012), there are three main approaches to aspect extraction: 1) Using word frequency and syntactic dependency of aspects and sentiment words for extraction (e.g., Blairgoldensohn et al., 2008; Hu and Liu, 2004; Ku et al., 2006; Popescu and Etzioni, 2005; Qiu et al., 2011; Somasundaran and Wiebe, 2009; Wu et al., 2009; Yu et al., 2011; Zhang and Liu, 2011; Zhuang et al., 2006); 2) Using supervised sequence labeling/classification (e.g., Choi and Cardie, 2010; Jakob and Gurevych, 2010; Kobayashi et al., 2007; Li et al., 2010); 3) Topic models (Branavan et al., 2008; Brody and Elhadad, 2010; Fang and Huang, 2012; Jo and Oh, 2011; Kim et al., 2013; Lazaridou et al., 2013; Li et al., 2011; Lin and He, 2009; Lu et al., 2009, 2012, 2011; Lu and Zhai, 2008; Mei et al., 2007; Moghaddam and Ester, 2011; Mukherjee and Liu, 2012; Sauper et al., 2011; Titov and McDonald, 2008; Wang et al., 2010, 2011; Zhao et al., 2010). Other approac"
D13-1172,P11-2101,1,0.54029,"oth qualitative and quantitative measures demonstrate the superiority of MC-LDA. 2 Related Work Sentiment analysis has been studied extensively in recent years (Hu and Liu, 2004; Pang and Lee, 2008; Wiebe and Riloff, 2005; Wiebe et al., 2004). According to (Liu, 2012), there are three main approaches to aspect extraction: 1) Using word frequency and syntactic dependency of aspects and sentiment words for extraction (e.g., Blairgoldensohn et al., 2008; Hu and Liu, 2004; Ku et al., 2006; Popescu and Etzioni, 2005; Qiu et al., 2011; Somasundaran and Wiebe, 2009; Wu et al., 2009; Yu et al., 2011; Zhang and Liu, 2011; Zhuang et al., 2006); 2) Using supervised sequence labeling/classification (e.g., Choi and Cardie, 2010; Jakob and Gurevych, 2010; Kobayashi et al., 2007; Li et al., 2010); 3) Topic models (Branavan et al., 2008; Brody and Elhadad, 2010; Fang and Huang, 2012; Jo and Oh, 2011; Kim et al., 2013; Lazaridou et al., 2013; Li et al., 2011; Lin and He, 2009; Lu et al., 2009, 2012, 2011; Lu and Zhai, 2008; Mei et al., 2007; Moghaddam and Ester, 2011; Mukherjee and Liu, 2012; Sauper et al., 2011; Titov and McDonald, 2008; Wang et al., 2010, 2011; Zhao et al., 2010). Other approaches include shallow s"
D13-1172,D10-1006,0,0.684045,"Wu et al., 2009; Yu et al., 2011; Zhang and Liu, 2011; Zhuang et al., 2006); 2) Using supervised sequence labeling/classification (e.g., Choi and Cardie, 2010; Jakob and Gurevych, 2010; Kobayashi et al., 2007; Li et al., 2010); 3) Topic models (Branavan et al., 2008; Brody and Elhadad, 2010; Fang and Huang, 2012; Jo and Oh, 2011; Kim et al., 2013; Lazaridou et al., 2013; Li et al., 2011; Lin and He, 2009; Lu et al., 2009, 2012, 2011; Lu and Zhai, 2008; Mei et al., 2007; Moghaddam and Ester, 2011; Mukherjee and Liu, 2012; Sauper et al., 2011; Titov and McDonald, 2008; Wang et al., 2010, 2011; Zhao et al., 2010). Other approaches include shallow semantic parsing (Li et al., 2012b), bootstrapping (Xia et al., 2009), Non-English techniques (Abu-Jbara et al., 2013; Zhou et al., 2012), graph-based representation (Wu et al., 2011), convolution kernels (Wiegand and Klakow, 2010) and domain adaption (Li et al., 2012). Stoyanov and Cardie (2011), Wang and Liu (2011), and Meng et al. (2012) studied opinion summarization outside the reviews. Some other works related with sentiment analysis include (Agarwal and Sabharwal, 2012; Kennedy and Inkpen, 2006; Kim et al., 2009; Mohammad et al., 2009). In this work, we"
D13-1172,C10-1074,0,\N,Missing
D13-1172,H05-2017,0,\N,Missing
D14-1120,E12-2014,0,0.0268596,"es over the SSN and the price time series in a vector autoregression (VAR) framework. We will show that the neighbor relationships in SSN give very useful insights into the dynamics of the stock market. Our experimental results demonstrate that topic sentiments from close neighbors of a stock can help improve the prediction of the stock market markedly. 2 2.1 Related work Social Media & Economic Indices Many algorithms have been proposed to produce meaningful insights from massive social media data. Related works include detecting and summarizing events (Weng and Lee, 2011; Weng et al., 2011; Baldwin et al., 2012; Gao et al., 2012) and analyzing sentiments about them (Pang and Lee, 2008; Liu, 2012), etc. Some recent literature also used Twitter as a sentiment source for stock market prediction (Bollen et al., 2011; Si et al., 2013). This paper extends beyond the correlation between social media and stock market, but fur1139 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1139–1145, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics ther exploits the social relations between stocks from the social media context. Topic"
D14-1120,P12-1056,0,0.0133516,"tends beyond the correlation between social media and stock market, but fur1139 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1139–1145, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics ther exploits the social relations between stocks from the social media context. Topic modeling has been widely used in social media. Various extensions of the traditional LDA model (Blei et al., 2003) has been proposed for modeling social media data (Wang et al., 2011, Jo and Oh, 2011; Liu et al., 2007; Mei et al., 2007; Diao et al., 2012). Ramage et al. (2009; 2011) presented a partially supervised learning model called Labeled LDA to utilize supervision signal in topic modeling. Ma et al. (2013) predicted the topic popularity based on hash-tags on Twitter in a classification framework. Figure 1. An example stock network. 2.2 3.1 Table 1. co-occurrence statistics with $aapl. Figure 2. Tweet label design. our task. Finally, we obtained 629,977 tweets in total. Table 1 shows the top five most frequent stocks jointly mentioned with Apple Inc. in our dataset. Formally, we define the stock network as an undirected graph ? = {? , ?}"
D14-1120,D09-1026,0,0.24202,"relation between social media and stock market, but fur1139 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1139–1145, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics ther exploits the social relations between stocks from the social media context. Topic modeling has been widely used in social media. Various extensions of the traditional LDA model (Blei et al., 2003) has been proposed for modeling social media data (Wang et al., 2011, Jo and Oh, 2011; Liu et al., 2007; Mei et al., 2007; Diao et al., 2012). Ramage et al. (2009; 2011) presented a partially supervised learning model called Labeled LDA to utilize supervision signal in topic modeling. Ma et al. (2013) predicted the topic popularity based on hash-tags on Twitter in a classification framework. Figure 1. An example stock network. 2.2 3.1 Table 1. co-occurrence statistics with $aapl. Figure 2. Tweet label design. our task. Finally, we obtained 629,977 tweets in total. Table 1 shows the top five most frequent stocks jointly mentioned with Apple Inc. in our dataset. Formally, we define the stock network as an undirected graph ? = {? , ?}. The node set ? comp"
D14-1120,P13-2005,1,0.217492,"t about stock (node) topics and stock relationship (edge) topics are predictive of each stock’s market. For prediction, we propose to regress the topic-sentiment time-series and the stock’s price time series. Experimental results demonstrate that topic sentiments from close neighbors are able to help improve the prediction of a stock markedly. 1 Introduction Existing research has shown the usefulness of public sentiment in social media across a wide range of applications. Several works showed social media as a promising tool for stock market prediction (Bollen et al., 2011; Ruiz et al., 2012; Si et al., 2013). However, the semantic relationships between stocks have not yet been explored. In this paper, we show that the latent semantic relations among stocks and the associated social sentiment can yield a better prediction model. On Twitter, cash-tags (e.g., $aapl for Apple Inc.) are used in a tweet to indicate that the tweet talks about the stocks or some other related information about the companies. For example, one tweet containing cash-tags: $aapl and $goog (Google Inc.), is “$AAPL is loosing customers. everybody is buying android phones! $GOOG”. Such joint mentions directly reflect some kind"
D14-1120,P11-4023,0,0.0161171,"sentiment time-series over the SSN and the price time series in a vector autoregression (VAR) framework. We will show that the neighbor relationships in SSN give very useful insights into the dynamics of the stock market. Our experimental results demonstrate that topic sentiments from close neighbors of a stock can help improve the prediction of the stock market markedly. 2 2.1 Related work Social Media & Economic Indices Many algorithms have been proposed to produce meaningful insights from massive social media data. Related works include detecting and summarizing events (Weng and Lee, 2011; Weng et al., 2011; Baldwin et al., 2012; Gao et al., 2012) and analyzing sentiments about them (Pang and Lee, 2008; Liu, 2012), etc. Some recent literature also used Twitter as a sentiment source for stock market prediction (Bollen et al., 2011; Si et al., 2013). This paper extends beyond the correlation between social media and stock market, but fur1139 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1139–1145, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics ther exploits the social relations between stocks from the socia"
D15-1282,W00-0403,0,0.140926,"as outliers. Both (Yang and Madden, 2007) and (Tian and Gu, 2010) tried to refine Schölkopf’s models by searching optimal parameters. Luo et al., (2007) proposed a cost-sensitive one-class SVM algorithm for intrusion detection. We will see in the experiment section that one-class classification is far inferior to our proposed CBS-L method. In this work, we propose to represent documents in the similarity space and thus it is related to works on document representation. Alternative document representations have been proposed in the past and have been shown to perform well in many applications (Radev et al., 2000; He et al., 2004; Lebanon 2006; Ranzato and Szummer, 2008, Wang and Domeniconi, 2008). In (Radev et al., 2000), although the centroid sentence/document vector was computed, it was not transformed to a similarity space vector representation. Wang and Domeniconi (2008) proposed to use external knowledge to build semantic kernels for documents in order to improve text classification. In our problem, the main difficulty is that testing negative documents cannot be well covered in training. It is not clear how the enriched document representations could help solve our problem. Our work is also rel"
D15-1282,D10-1022,1,0.677041,"Missing"
D15-1282,D13-1113,1,0.837674,"unlabeled data, but there is no labeled negative training data. Clearly, their setting is different from ours too. There is also no guarantee that the unlabeled data has the same distribution as the future test data. Our problem is also very different from domain adaption as we work in the same domain. Due to the use of document similarity, our method has some resemblance to learning to rank (Li, 2011; Liu, 2011). However, CBS-L is very different because we perform supervised classification. Our similarity is also center-based rather than pair-wise document similarity, which is also used in (Qian and Liu 2013) for spam detection. 3 The Proposed CBS Learning We now formulate the proposed supervised learning in the CBS space, called CSB-L. The key difference between CBS learning and the classic document space (DS) learning is in the document representation, which applies to both training and testing documents or posts. In the next subsection, we first give the intuitive idea 2349 and a simple example. The detailed algorithm follows. In Section 3.5, we explain why CBS-L is better than DS-based learning when unexpected negative data appear in the test set. ing positive document d1 and negative document"
D16-1022,N10-1122,0,0.0760359,"a in isolation without considering any past learned knowledge (Silver et al., 2013). LML aims to mimic human learning, which always retains the learned knowledge from the past and uses it to help future learning. Our experimental results show that the proposed Lifelong-RL system is highly promising. The paradigm of LML helps improve the classification results greatly. 2 Related Work Although many target extraction methods exist (Hu and Liu, 2004; Zhuang et al., 2006; Ku et al., 2006; Wang and Wang, 2008; Wu et al., 2009; Lin and He, 2009; Zhang et al., 2010; Mei et al., 2007; Li et al., 2010; Brody and Elhadad, 2010; Wang et al., 2010; Mukherjee and Liu, 2012; Fang and Huang, 2012; Zhou et al., 2013; Liu et al., 2013; Poria et al., 2014), we are not aware of any attempt to solve the proposed problem. As mentioned in the introduction, although in supervised target extraction, one can annotate entities and aspects with different labels, supervised methods need manually labeled training data, which is time-consuming and laborintensive to produce (Jakob and Gurevych, 2010; Choi and Cardie, 2010; Mitchell et al., 2013). Note that relaxation labeling was used for sentiment classification in (Popescu and Etzion"
D16-1022,P10-2050,0,0.0699716,"6; Wang and Wang, 2008; Wu et al., 2009; Lin and He, 2009; Zhang et al., 2010; Mei et al., 2007; Li et al., 2010; Brody and Elhadad, 2010; Wang et al., 2010; Mukherjee and Liu, 2012; Fang and Huang, 2012; Zhou et al., 2013; Liu et al., 2013; Poria et al., 2014), we are not aware of any attempt to solve the proposed problem. As mentioned in the introduction, although in supervised target extraction, one can annotate entities and aspects with different labels, supervised methods need manually labeled training data, which is time-consuming and laborintensive to produce (Jakob and Gurevych, 2010; Choi and Cardie, 2010; Mitchell et al., 2013). Note that relaxation labeling was used for sentiment classification in (Popescu and Etzioni, 2007), but not for target classification. More details of opinion mining can be found in (Liu, 2012; Pang and Lee, 2008). Our work is related to transfer learning (Pan and Yang, 2010), which uses the source domain labeled data to help target domain learning, which has little or no labeled data. Our work is not just using a source domain to help a target domain. It is a continuous and cumulative learning process. Each new task can make use of the knowledge learned from all past"
D16-1022,P12-2065,0,0.165954,"et al., 2013). LML aims to mimic human learning, which always retains the learned knowledge from the past and uses it to help future learning. Our experimental results show that the proposed Lifelong-RL system is highly promising. The paradigm of LML helps improve the classification results greatly. 2 Related Work Although many target extraction methods exist (Hu and Liu, 2004; Zhuang et al., 2006; Ku et al., 2006; Wang and Wang, 2008; Wu et al., 2009; Lin and He, 2009; Zhang et al., 2010; Mei et al., 2007; Li et al., 2010; Brody and Elhadad, 2010; Wang et al., 2010; Mukherjee and Liu, 2012; Fang and Huang, 2012; Zhou et al., 2013; Liu et al., 2013; Poria et al., 2014), we are not aware of any attempt to solve the proposed problem. As mentioned in the introduction, although in supervised target extraction, one can annotate entities and aspects with different labels, supervised methods need manually labeled training data, which is time-consuming and laborintensive to produce (Jakob and Gurevych, 2010; Choi and Cardie, 2010; Mitchell et al., 2013). Note that relaxation labeling was used for sentiment classification in (Popescu and Etzioni, 2007), but not for target classification. More details of opini"
D16-1022,D10-1101,0,0.182246,"al., 2006; Ku et al., 2006; Wang and Wang, 2008; Wu et al., 2009; Lin and He, 2009; Zhang et al., 2010; Mei et al., 2007; Li et al., 2010; Brody and Elhadad, 2010; Wang et al., 2010; Mukherjee and Liu, 2012; Fang and Huang, 2012; Zhou et al., 2013; Liu et al., 2013; Poria et al., 2014), we are not aware of any attempt to solve the proposed problem. As mentioned in the introduction, although in supervised target extraction, one can annotate entities and aspects with different labels, supervised methods need manually labeled training data, which is time-consuming and laborintensive to produce (Jakob and Gurevych, 2010; Choi and Cardie, 2010; Mitchell et al., 2013). Note that relaxation labeling was used for sentiment classification in (Popescu and Etzioni, 2007), but not for target classification. More details of opinion mining can be found in (Liu, 2012; Pang and Lee, 2008). Our work is related to transfer learning (Pan and Yang, 2010), which uses the source domain labeled data to help target domain learning, which has little or no labeled data. Our work is not just using a source domain to help a target domain. It is a continuous and cumulative learning process. Each new task can make use of the knowledg"
D16-1022,P03-1054,0,0.00777994,", “camera” indicates an entity-aspect modifier for “battery”. Aspect-entity modifier mE|A : Same as above except that “battery” indicates an aspect-entity modifier for “camera”. Modifier Extraction: These modifiers are identified from the corpus d using three syntactic rules. “This” and “these” are used to extract type modifier M (t) = mE . CmE (t) is the occurrence count of that modifier on target t, which is used in determining the initial label distribution in Section 4.2. Relation modifiers are identified by dependency relations conj(ti , tj ) and poss(ti , tj ) using the Stanford Parser (Klein and Manning, 2003). Each occurrence of a relation rule contributes one count of Mtj (ti ) for ti and one count of Mti (tj ) for tj . We use Cmc ,tj (ti ), CmA|E ,tj (ti ) and CmE|A ,tj (ti ) to denote the count of tj modifying ti with conjunction, entity-aspect and aspect-entity modifiers respectively. For example, “price and service” will contribute one count to Cmc ,price (service) and one count to Cmc ,service (price). Similarly, “camera’s battery” will contribute one count to CmA|E ,camera (battery) and one count to CmE|A ,battery (camera). 4.2 Computing Initial Probabilities The initial label probability d"
D16-1022,P13-1172,0,0.0139896,"earning, which always retains the learned knowledge from the past and uses it to help future learning. Our experimental results show that the proposed Lifelong-RL system is highly promising. The paradigm of LML helps improve the classification results greatly. 2 Related Work Although many target extraction methods exist (Hu and Liu, 2004; Zhuang et al., 2006; Ku et al., 2006; Wang and Wang, 2008; Wu et al., 2009; Lin and He, 2009; Zhang et al., 2010; Mei et al., 2007; Li et al., 2010; Brody and Elhadad, 2010; Wang et al., 2010; Mukherjee and Liu, 2012; Fang and Huang, 2012; Zhou et al., 2013; Liu et al., 2013; Poria et al., 2014), we are not aware of any attempt to solve the proposed problem. As mentioned in the introduction, although in supervised target extraction, one can annotate entities and aspects with different labels, supervised methods need manually labeled training data, which is time-consuming and laborintensive to produce (Jakob and Gurevych, 2010; Choi and Cardie, 2010; Mitchell et al., 2013). Note that relaxation labeling was used for sentiment classification in (Popescu and Etzioni, 2007), but not for target classification. More details of opinion mining can be found in (Liu, 2012;"
D16-1022,P14-5010,0,0.0030751,"from a NER system as entities and the rest of the targets as aspects. However, a NER system cannot identify entities such as “this car” from “this car is great.” Its result is rather poor. But our type modifier (TM) does that, i.e., if an opinion target appears after “this” or “these” in at least two sentences, TM labels the target as an entity; otherwise an aspect. However, TM cannot extract named entities. Its result is also rather poor. We thus combine the two methods to give NER+TM as they complement each other very well. To make NER more powerful, we use two NER systems: Stanford-NER 1 (Manning et al., 2014) and UIUC-NER2 (Ratinov and Roth, 2009). NER+TM treats the extracted entities by the three systems as entities and the rest of the targets as aspects. NER+TM+DICT: We run NER+TM on the 100 datasets for LML to get a list of entities, which we call the dictionary (DICT). For a new task, if any target word is in the list, it is treated as an entity; otherwise an aspect. RL: This is the base method described in Section 3. It performs relaxation labeling (RL) without the help of LML. Lifelong-RL-1: This performs LML with RL but the current task only uses the relations in the KB from previous tasks"
D16-1022,D13-1171,0,0.150995,"Missing"
D16-1022,P12-1036,1,0.872747,"learned knowledge (Silver et al., 2013). LML aims to mimic human learning, which always retains the learned knowledge from the past and uses it to help future learning. Our experimental results show that the proposed Lifelong-RL system is highly promising. The paradigm of LML helps improve the classification results greatly. 2 Related Work Although many target extraction methods exist (Hu and Liu, 2004; Zhuang et al., 2006; Ku et al., 2006; Wang and Wang, 2008; Wu et al., 2009; Lin and He, 2009; Zhang et al., 2010; Mei et al., 2007; Li et al., 2010; Brody and Elhadad, 2010; Wang et al., 2010; Mukherjee and Liu, 2012; Fang and Huang, 2012; Zhou et al., 2013; Liu et al., 2013; Poria et al., 2014), we are not aware of any attempt to solve the proposed problem. As mentioned in the introduction, although in supervised target extraction, one can annotate entities and aspects with different labels, supervised methods need manually labeled training data, which is time-consuming and laborintensive to produce (Jakob and Gurevych, 2010; Choi and Cardie, 2010; Mitchell et al., 2013). Note that relaxation labeling was used for sentiment classification in (Popescu and Etzioni, 2007), but not for target classification."
D16-1022,W14-5905,0,0.0945459,"ays retains the learned knowledge from the past and uses it to help future learning. Our experimental results show that the proposed Lifelong-RL system is highly promising. The paradigm of LML helps improve the classification results greatly. 2 Related Work Although many target extraction methods exist (Hu and Liu, 2004; Zhuang et al., 2006; Ku et al., 2006; Wang and Wang, 2008; Wu et al., 2009; Lin and He, 2009; Zhang et al., 2010; Mei et al., 2007; Li et al., 2010; Brody and Elhadad, 2010; Wang et al., 2010; Mukherjee and Liu, 2012; Fang and Huang, 2012; Zhou et al., 2013; Liu et al., 2013; Poria et al., 2014), we are not aware of any attempt to solve the proposed problem. As mentioned in the introduction, although in supervised target extraction, one can annotate entities and aspects with different labels, supervised methods need manually labeled training data, which is time-consuming and laborintensive to produce (Jakob and Gurevych, 2010; Choi and Cardie, 2010; Mitchell et al., 2013). Note that relaxation labeling was used for sentiment classification in (Popescu and Etzioni, 2007), but not for target classification. More details of opinion mining can be found in (Liu, 2012; Pang and Lee, 2008)."
D16-1022,J11-1002,1,0.933742,"Past research has proposed many techniques to extract opinion targets (we will just call them targets Problem Statement: Given a set of opinion targets T = {t1 , . . . , tn } extracted from an opinion corpus d, we want to classify each target ti ∈ T into one of the three classes, entity, aspect, or NIL, which are called class labels. NIL means that the target is neither an entity nor an aspect and is used because target extraction algorithms can make mistakes. This paper does not propose a new target extraction algorithm. We use an existing unsupervised method, called Double Propagation (DP) (Qiu et al., 2011), for extraction. We only focus on target classification after the targets have been extracted. Note that an entity here can be a named entity, a prod225 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 225–235, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics uct category, or an abstract product (e.g., “this machine” and “this product”). An named entity can be the name of a brand, a model, or a manufacturer. An aspect is a part or attribute of an entity, e.g., “battery” and “price” of the entity “camera”. Since ou"
D16-1022,W09-1119,0,0.0657291,"e rest of the targets as aspects. However, a NER system cannot identify entities such as “this car” from “this car is great.” Its result is rather poor. But our type modifier (TM) does that, i.e., if an opinion target appears after “this” or “these” in at least two sentences, TM labels the target as an entity; otherwise an aspect. However, TM cannot extract named entities. Its result is also rather poor. We thus combine the two methods to give NER+TM as they complement each other very well. To make NER more powerful, we use two NER systems: Stanford-NER 1 (Manning et al., 2014) and UIUC-NER2 (Ratinov and Roth, 2009). NER+TM treats the extracted entities by the three systems as entities and the rest of the targets as aspects. NER+TM+DICT: We run NER+TM on the 100 datasets for LML to get a list of entities, which we call the dictionary (DICT). For a new task, if any target word is in the list, it is treated as an entity; otherwise an aspect. RL: This is the base method described in Section 3. It performs relaxation labeling (RL) without the help of LML. Lifelong-RL-1: This performs LML with RL but the current task only uses the relations in the KB from previous tasks (Section 5.1). Lifelong-RL: This is our"
D16-1022,I08-1038,0,0.214552,"or unsupervised) because classic learning has no memory. It basically runs a learning algorithm on a given data in isolation without considering any past learned knowledge (Silver et al., 2013). LML aims to mimic human learning, which always retains the learned knowledge from the past and uses it to help future learning. Our experimental results show that the proposed Lifelong-RL system is highly promising. The paradigm of LML helps improve the classification results greatly. 2 Related Work Although many target extraction methods exist (Hu and Liu, 2004; Zhuang et al., 2006; Ku et al., 2006; Wang and Wang, 2008; Wu et al., 2009; Lin and He, 2009; Zhang et al., 2010; Mei et al., 2007; Li et al., 2010; Brody and Elhadad, 2010; Wang et al., 2010; Mukherjee and Liu, 2012; Fang and Huang, 2012; Zhou et al., 2013; Liu et al., 2013; Poria et al., 2014), we are not aware of any attempt to solve the proposed problem. As mentioned in the introduction, although in supervised target extraction, one can annotate entities and aspects with different labels, supervised methods need manually labeled training data, which is time-consuming and laborintensive to produce (Jakob and Gurevych, 2010; Choi and Cardie, 2010;"
D16-1022,D09-1159,0,0.0498696,"ause classic learning has no memory. It basically runs a learning algorithm on a given data in isolation without considering any past learned knowledge (Silver et al., 2013). LML aims to mimic human learning, which always retains the learned knowledge from the past and uses it to help future learning. Our experimental results show that the proposed Lifelong-RL system is highly promising. The paradigm of LML helps improve the classification results greatly. 2 Related Work Although many target extraction methods exist (Hu and Liu, 2004; Zhuang et al., 2006; Ku et al., 2006; Wang and Wang, 2008; Wu et al., 2009; Lin and He, 2009; Zhang et al., 2010; Mei et al., 2007; Li et al., 2010; Brody and Elhadad, 2010; Wang et al., 2010; Mukherjee and Liu, 2012; Fang and Huang, 2012; Zhou et al., 2013; Liu et al., 2013; Poria et al., 2014), we are not aware of any attempt to solve the proposed problem. As mentioned in the introduction, although in supervised target extraction, one can annotate entities and aspects with different labels, supervised methods need manually labeled training data, which is time-consuming and laborintensive to produce (Jakob and Gurevych, 2010; Choi and Cardie, 2010; Mitchell et al.,"
D16-1022,C10-2167,1,0.843108,"Missing"
D16-1022,D13-1189,0,0.333799,"Missing"
D17-1059,P14-1050,0,0.0191165,"xploit some linguistic conventions on connectives such as AND and OR Hatzivassiloglou and McKeown (1997). For example, in the sentence “This car is beautiful and spacious,” if “beautiful” is known to be positive, it can be inferred that “spacious” is also positive. Kanayama and Nasukawa (2006) extended the idea to the sentence level by exploiting adversative expressions such as “but” and “however.” Qiu et al. (2011) proposed a double propagation (DP) method that uses both sentiment and target relation and various connectives to extract sentiment words. (Wang and Wang, 2008) did similar works. Huang et al. (2014) detected new sentiment words using lexical patterns. Wilson et al. (2005), Ding et al. (2008), Choi and Cardie (2008) and Zhang and Liu (2011) studied contextual sentiments at the phrase or expression 1. It formulates Step 1 of sentiment lexicon expansion as a PU learning problem. To the best of our knowledge, this is the first such formulation. 2. It proposes a new neural learning method AMP and shows that AMP outperforms the traditional SVM based PU learning approach. 3. It further proposes a new and general PU learning strategy that works in the opposite direction to the popular existing a"
D17-1059,P10-1060,0,0.0298756,"Missing"
D17-1059,D07-1115,0,0.0139336,"ividual (Chinese) characters in each word. In summary, this paper has several innovations: Related Work There are two main approaches for sentiment lexicon generation (Liu, 2012): the dictionary-based approach and the corpus-based approach. Under the dictionary-based approach, one method is to use synonym and antonym relations and WordNet graph in the dictionary to bootstrap a set of given seed sentiment words. There are numerous variations of and enhancements to this approach (Hu and Liu, 2004; Valitutti et al., 2004; Kim and Hovy, 2004; Takamura et al., 2007; Andreevskaia and Bergler, 2006; Kaji and Kitsuregawa, 2007; Blair-Goldensohn et al., 2008; Cambria et al., 2016; Rao and Ravichandran, 2009; Perez-Rosas et al., 2012). For example, (Valitutti et al., 2004; Kim and Hovy, 2004) tried to remove error words and assign a sentiment strength to each word. Mohammad et al. (2009) exploited many antonym-generating affix patterns, Kamps et al. (2004) used a WordNet distance, and Hassan and Radev (2010) used a Markov random walk model over a word relatedness graph. Dragut et al. (2010) used a set of inference rules to determine word sentiment polarity through a deductive process, and Schneider and Dragut (2015)"
D17-1059,kamps-etal-2004-using,0,0.0531098,"Missing"
D17-1059,W06-1642,0,0.445598,"for many sentiment analysis applications. So far many algorithms have been proposed to generate such lexicons (Liu, 2012). These algorithms are either dictionary-based or corpus-based. In the dictionary-based approach, one exploits synonym and antonym relations in the dictionary to bootstrap a given seed set of sentiment words (Hu and Liu, 2004; Kim and Hovy, 2004; Kamps et al., 2004), or learns a classifier to classify the gloss of each word in the dictionary (Esuli and Sebastiani, 2005). The corpus-based approach uses various linguistic rules or patterns (Hatzivassiloglou and McKeown, 1997; Kanayama and Nasukawa, 2006; Qiu et al., 2011; Tang et al., 2014). We will 553 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 553–563 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics 2 eratively to add more and more data to the RN set to finally build a classifier (Liu, 2011). In this work, we first adapt a popular such approach to an augmented multilayer perceptron (AMP) method and use it to replace SVM, and show that using SVM as the learning method is inferior to using AMP. However, we can do much better. We then propose a new P"
D17-1059,esuli-sebastiani-2006-sentiwordnet,0,0.0579113,"Missing"
D17-1059,C04-1200,0,0.179752,"hm. Experimental results show that the proposed approach greatly outperforms baseline methods. 1 Introduction Sentiment lexicons contain words (such as good, beautiful, bad, and awful) that convey positive or negative sentiments. They are instrumental for many sentiment analysis applications. So far many algorithms have been proposed to generate such lexicons (Liu, 2012). These algorithms are either dictionary-based or corpus-based. In the dictionary-based approach, one exploits synonym and antonym relations in the dictionary to bootstrap a given seed set of sentiment words (Hu and Liu, 2004; Kim and Hovy, 2004; Kamps et al., 2004), or learns a classifier to classify the gloss of each word in the dictionary (Esuli and Sebastiani, 2005). The corpus-based approach uses various linguistic rules or patterns (Hatzivassiloglou and McKeown, 1997; Kanayama and Nasukawa, 2006; Qiu et al., 2011; Tang et al., 2014). We will 553 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 553–563 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics 2 eratively to add more and more data to the RN set to finally build a classifier (Liu, 2011)"
D17-1059,D11-1101,0,0.0401694,"Missing"
D17-1059,D16-1057,0,0.0669092,"Missing"
D17-1059,P10-1041,0,0.0502453,"en seed sentiment words. There are numerous variations of and enhancements to this approach (Hu and Liu, 2004; Valitutti et al., 2004; Kim and Hovy, 2004; Takamura et al., 2007; Andreevskaia and Bergler, 2006; Kaji and Kitsuregawa, 2007; Blair-Goldensohn et al., 2008; Cambria et al., 2016; Rao and Ravichandran, 2009; Perez-Rosas et al., 2012). For example, (Valitutti et al., 2004; Kim and Hovy, 2004) tried to remove error words and assign a sentiment strength to each word. Mohammad et al. (2009) exploited many antonym-generating affix patterns, Kamps et al. (2004) used a WordNet distance, and Hassan and Radev (2010) used a Markov random walk model over a word relatedness graph. Dragut et al. (2010) used a set of inference rules to determine word sentiment polarity through a deductive process, and Schneider and Dragut (2015) employed a linear programming approach. Another method is to build a supervised sentiment classifier to classify the gloss text of each word in the dictionary (Esuli and Sebastiani, 2005, 2006). Xu et al. (2010a) integrated both dictionaries and corpora to find emotion words based on label-propagation. PerezRosas et al. (2012) also worked on cross lingual lexicon construction. In the"
D17-1059,P97-1023,0,0.160089,"e sentiments. They are instrumental for many sentiment analysis applications. So far many algorithms have been proposed to generate such lexicons (Liu, 2012). These algorithms are either dictionary-based or corpus-based. In the dictionary-based approach, one exploits synonym and antonym relations in the dictionary to bootstrap a given seed set of sentiment words (Hu and Liu, 2004; Kim and Hovy, 2004; Kamps et al., 2004), or learns a classifier to classify the gloss of each word in the dictionary (Esuli and Sebastiani, 2005). The corpus-based approach uses various linguistic rules or patterns (Hatzivassiloglou and McKeown, 1997; Kanayama and Nasukawa, 2006; Qiu et al., 2011; Tang et al., 2014). We will 553 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 553–563 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics 2 eratively to add more and more data to the RN set to finally build a classifier (Liu, 2011). In this work, we first adapt a popular such approach to an augmented multilayer perceptron (AMP) method and use it to replace SVM, and show that using SVM as the learning method is inferior to using AMP. However, we can do much be"
D17-1059,H05-1044,0,0.0932413,"ivassiloglou and McKeown (1997). For example, in the sentence “This car is beautiful and spacious,” if “beautiful” is known to be positive, it can be inferred that “spacious” is also positive. Kanayama and Nasukawa (2006) extended the idea to the sentence level by exploiting adversative expressions such as “but” and “however.” Qiu et al. (2011) proposed a double propagation (DP) method that uses both sentiment and target relation and various connectives to extract sentiment words. (Wang and Wang, 2008) did similar works. Huang et al. (2014) detected new sentiment words using lexical patterns. Wilson et al. (2005), Ding et al. (2008), Choi and Cardie (2008) and Zhang and Liu (2011) studied contextual sentiments at the phrase or expression 1. It formulates Step 1 of sentiment lexicon expansion as a PU learning problem. To the best of our knowledge, this is the first such formulation. 2. It proposes a new neural learning method AMP and shows that AMP outperforms the traditional SVM based PU learning approach. 3. It further proposes a new and general PU learning strategy that works in the opposite direction to the popular existing approach to suit our task. 4. It also proposes a double dictionary lookup t"
D17-1059,S13-2053,0,0.0201725,"Missing"
D17-1059,perez-rosas-etal-2012-learning,0,0.0110011,"are two main approaches for sentiment lexicon generation (Liu, 2012): the dictionary-based approach and the corpus-based approach. Under the dictionary-based approach, one method is to use synonym and antonym relations and WordNet graph in the dictionary to bootstrap a set of given seed sentiment words. There are numerous variations of and enhancements to this approach (Hu and Liu, 2004; Valitutti et al., 2004; Kim and Hovy, 2004; Takamura et al., 2007; Andreevskaia and Bergler, 2006; Kaji and Kitsuregawa, 2007; Blair-Goldensohn et al., 2008; Cambria et al., 2016; Rao and Ravichandran, 2009; Perez-Rosas et al., 2012). For example, (Valitutti et al., 2004; Kim and Hovy, 2004) tried to remove error words and assign a sentiment strength to each word. Mohammad et al. (2009) exploited many antonym-generating affix patterns, Kamps et al. (2004) used a WordNet distance, and Hassan and Radev (2010) used a Markov random walk model over a word relatedness graph. Dragut et al. (2010) used a set of inference rules to determine word sentiment polarity through a deductive process, and Schneider and Dragut (2015) employed a linear programming approach. Another method is to build a supervised sentiment classifier to clas"
D17-1059,C10-1136,0,0.0515449,"Missing"
D17-1059,J11-1002,1,0.93825,"pplications. So far many algorithms have been proposed to generate such lexicons (Liu, 2012). These algorithms are either dictionary-based or corpus-based. In the dictionary-based approach, one exploits synonym and antonym relations in the dictionary to bootstrap a given seed set of sentiment words (Hu and Liu, 2004; Kim and Hovy, 2004; Kamps et al., 2004), or learns a classifier to classify the gloss of each word in the dictionary (Esuli and Sebastiani, 2005). The corpus-based approach uses various linguistic rules or patterns (Hatzivassiloglou and McKeown, 1997; Kanayama and Nasukawa, 2006; Qiu et al., 2011; Tang et al., 2014). We will 553 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 553–563 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics 2 eratively to add more and more data to the RN set to finally build a classifier (Liu, 2011). In this work, we first adapt a popular such approach to an augmented multilayer perceptron (AMP) method and use it to replace SVM, and show that using SVM as the learning method is inferior to using AMP. However, we can do much better. We then propose a new PU learning method,"
D17-1059,Y10-1034,0,0.027443,"and assign a sentiment strength to each word. Mohammad et al. (2009) exploited many antonym-generating affix patterns, Kamps et al. (2004) used a WordNet distance, and Hassan and Radev (2010) used a Markov random walk model over a word relatedness graph. Dragut et al. (2010) used a set of inference rules to determine word sentiment polarity through a deductive process, and Schneider and Dragut (2015) employed a linear programming approach. Another method is to build a supervised sentiment classifier to classify the gloss text of each word in the dictionary (Esuli and Sebastiani, 2005, 2006). Xu et al. (2010a) integrated both dictionaries and corpora to find emotion words based on label-propagation. PerezRosas et al. (2012) also worked on cross lingual lexicon construction. In the corpus-based approach, one key idea is to exploit some linguistic conventions on connectives such as AND and OR Hatzivassiloglou and McKeown (1997). For example, in the sentence “This car is beautiful and spacious,” if “beautiful” is known to be positive, it can be inferred that “spacious” is also positive. Kanayama and Nasukawa (2006) extended the idea to the sentence level by exploiting adversative expressions such as"
D17-1059,E09-1077,0,0.153016,"ovations: Related Work There are two main approaches for sentiment lexicon generation (Liu, 2012): the dictionary-based approach and the corpus-based approach. Under the dictionary-based approach, one method is to use synonym and antonym relations and WordNet graph in the dictionary to bootstrap a set of given seed sentiment words. There are numerous variations of and enhancements to this approach (Hu and Liu, 2004; Valitutti et al., 2004; Kim and Hovy, 2004; Takamura et al., 2007; Andreevskaia and Bergler, 2006; Kaji and Kitsuregawa, 2007; Blair-Goldensohn et al., 2008; Cambria et al., 2016; Rao and Ravichandran, 2009; Perez-Rosas et al., 2012). For example, (Valitutti et al., 2004; Kim and Hovy, 2004) tried to remove error words and assign a sentiment strength to each word. Mohammad et al. (2009) exploited many antonym-generating affix patterns, Kamps et al. (2004) used a WordNet distance, and Hassan and Radev (2010) used a Markov random walk model over a word relatedness graph. Dragut et al. (2010) used a set of inference rules to determine word sentiment polarity through a deductive process, and Schneider and Dragut (2015) employed a linear programming approach. Another method is to build a supervised s"
D17-1059,P14-2069,0,0.0607067,"Missing"
D17-1059,P15-1099,0,0.0128564,"; Kaji and Kitsuregawa, 2007; Blair-Goldensohn et al., 2008; Cambria et al., 2016; Rao and Ravichandran, 2009; Perez-Rosas et al., 2012). For example, (Valitutti et al., 2004; Kim and Hovy, 2004) tried to remove error words and assign a sentiment strength to each word. Mohammad et al. (2009) exploited many antonym-generating affix patterns, Kamps et al. (2004) used a WordNet distance, and Hassan and Radev (2010) used a Markov random walk model over a word relatedness graph. Dragut et al. (2010) used a set of inference rules to determine word sentiment polarity through a deductive process, and Schneider and Dragut (2015) employed a linear programming approach. Another method is to build a supervised sentiment classifier to classify the gloss text of each word in the dictionary (Esuli and Sebastiani, 2005, 2006). Xu et al. (2010a) integrated both dictionaries and corpora to find emotion words based on label-propagation. PerezRosas et al. (2012) also worked on cross lingual lexicon construction. In the corpus-based approach, one key idea is to exploit some linguistic conventions on connectives such as AND and OR Hatzivassiloglou and McKeown (1997). For example, in the sentence “This car is beautiful and spaciou"
D17-1059,N07-1037,0,0.212277,"vel method that is based on polarity association of individual (Chinese) characters in each word. In summary, this paper has several innovations: Related Work There are two main approaches for sentiment lexicon generation (Liu, 2012): the dictionary-based approach and the corpus-based approach. Under the dictionary-based approach, one method is to use synonym and antonym relations and WordNet graph in the dictionary to bootstrap a set of given seed sentiment words. There are numerous variations of and enhancements to this approach (Hu and Liu, 2004; Valitutti et al., 2004; Kim and Hovy, 2004; Takamura et al., 2007; Andreevskaia and Bergler, 2006; Kaji and Kitsuregawa, 2007; Blair-Goldensohn et al., 2008; Cambria et al., 2016; Rao and Ravichandran, 2009; Perez-Rosas et al., 2012). For example, (Valitutti et al., 2004; Kim and Hovy, 2004) tried to remove error words and assign a sentiment strength to each word. Mohammad et al. (2009) exploited many antonym-generating affix patterns, Kamps et al. (2004) used a WordNet distance, and Hassan and Radev (2010) used a Markov random walk model over a word relatedness graph. Dragut et al. (2010) used a set of inference rules to determine word sentiment polarity t"
D17-1059,C14-1018,0,0.0227695,"r many algorithms have been proposed to generate such lexicons (Liu, 2012). These algorithms are either dictionary-based or corpus-based. In the dictionary-based approach, one exploits synonym and antonym relations in the dictionary to bootstrap a given seed set of sentiment words (Hu and Liu, 2004; Kim and Hovy, 2004; Kamps et al., 2004), or learns a classifier to classify the gloss of each word in the dictionary (Esuli and Sebastiani, 2005). The corpus-based approach uses various linguistic rules or patterns (Hatzivassiloglou and McKeown, 1997; Kanayama and Nasukawa, 2006; Qiu et al., 2011; Tang et al., 2014). We will 553 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 553–563 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics 2 eratively to add more and more data to the RN set to finally build a classifier (Liu, 2011). In this work, we first adapt a popular such approach to an augmented multilayer perceptron (AMP) method and use it to replace SVM, and show that using SVM as the learning method is inferior to using AMP. However, we can do much better. We then propose a new PU learning method, called SE-AMP (Spy-"
D17-1059,P11-2101,1,0.414291,"ar is beautiful and spacious,” if “beautiful” is known to be positive, it can be inferred that “spacious” is also positive. Kanayama and Nasukawa (2006) extended the idea to the sentence level by exploiting adversative expressions such as “but” and “however.” Qiu et al. (2011) proposed a double propagation (DP) method that uses both sentiment and target relation and various connectives to extract sentiment words. (Wang and Wang, 2008) did similar works. Huang et al. (2014) detected new sentiment words using lexical patterns. Wilson et al. (2005), Ding et al. (2008), Choi and Cardie (2008) and Zhang and Liu (2011) studied contextual sentiments at the phrase or expression 1. It formulates Step 1 of sentiment lexicon expansion as a PU learning problem. To the best of our knowledge, this is the first such formulation. 2. It proposes a new neural learning method AMP and shows that AMP outperforms the traditional SVM based PU learning approach. 3. It further proposes a new and general PU learning strategy that works in the opposite direction to the popular existing approach to suit our task. 4. It also proposes a double dictionary lookup technique to improve the result further. 5. It proposes a novel polari"
D17-1059,P02-1053,0,0.0679758,"POS tags are divided into 5 classes (noun, verb, adjective, adverb, others) and form a 5 dimension binary vector (e.g. [1, 0, 0, 0, 0] for noun). In the SVM approach, POS tags are concatenated to the word embedding features to form a 205 dimension feature vector (5 POS tags and 200 word embedding features). We used the RBF kernel, which gives the best result as compared to other kernels. Compared Systems We compare the following seven (7) techniques: DP: The Double Propagation (DP) Method in (Qiu et al., 2011). This method uses dependency patterns for extraction. PMI: The classic PMI method (Turney, 2002) using the full Weibo corpus and 100 positive and 100 negative words in the DUTIR sentiment lexicon as the reference words. These words appear most frequently in the Weibo corpus. In (Turney, 2002), only 1 positive and 1 negative reference words are used. We also tried to use 1, 50, 150, 200, and all words in the positive and negative classes as reference words, respectively. However, they give poorer results. Since the PMI method can only determine the polarity, but cannot decide whether a test word is a sentiment word or not. We make that decision by using the mean score the PMI method of al"
D17-1059,P16-2036,0,0.0117781,"Missing"
D17-1059,I08-1038,0,0.0245246,"pus-based approach, one key idea is to exploit some linguistic conventions on connectives such as AND and OR Hatzivassiloglou and McKeown (1997). For example, in the sentence “This car is beautiful and spacious,” if “beautiful” is known to be positive, it can be inferred that “spacious” is also positive. Kanayama and Nasukawa (2006) extended the idea to the sentence level by exploiting adversative expressions such as “but” and “however.” Qiu et al. (2011) proposed a double propagation (DP) method that uses both sentiment and target relation and various connectives to extract sentiment words. (Wang and Wang, 2008) did similar works. Huang et al. (2014) detected new sentiment words using lexical patterns. Wilson et al. (2005), Ding et al. (2008), Choi and Cardie (2008) and Zhang and Liu (2011) studied contextual sentiments at the phrase or expression 1. It formulates Step 1 of sentiment lexicon expansion as a PU learning problem. To the best of our knowledge, this is the first such formulation. 2. It proposes a new neural learning method AMP and shows that AMP outperforms the traditional SVM based PU learning approach. 3. It further proposes a new and general PU learning strategy that works in the oppos"
D17-1059,D13-1096,0,0.00676691,"th the strongest polarity to form a 4dimensional vector. Classifier Building: Using all positive and negative words in the existing lexicon P as the training sample, each word represented as a vector of four features, we apply a Naive Bayesian Classifier to build a polarity classifier. For testing, a word is represented in the same way. If a test word contains charters that don’t exist in the lexicon, we give each character (0.5, 0.5) as the polarity vector. 4 4.1 Sentiment Words Discovery 4.1.1 Data and Parameter Settings We use a large Chinese Weibo corpus (Chinese version of Twitter) from (Wang et al., 2013) for our lexicon expansion, which has about 4.4 million pairs of post and response messages. Although it was originally used to study natural language conversations, it is quite suitable for our purpose as online conversations are sentiment rich. Word embedding: We first used the Stanford Chinese word segmenter to split sentences into sequences of words (the POS-tag of each word is also obtained in the process). For word embedding, we used word2vec (Mikolov et al., 2013). Each word vector has 200 dimensions. P set, U set, validation set, and test set: We randomly sampled 200K messages, and use"
D17-1059,E06-1027,0,\N,Missing
D17-1059,D08-1083,0,\N,Missing
D17-1059,D09-1063,0,\N,Missing
D17-1059,C16-1251,0,\N,Missing
D17-1314,D15-1282,1,0.715414,"ch test instance x to one of the m training or seen classes in Y or reject it to indicate that it does not belong to any of the m training or seen classes, i.e., unseen. In other words, we want to build a (m + 1)-class classifier f (x) with the classes C = {l1 , l2 , . . . , lm , rejection}. There are some prior approaches for open classification. One-class SVM (Sch¨olkopf et al., 2001; Tax and Duin, 2004) is the earliest approach. However, as no negative training data is used, oneclass classifiers work poorly. Fei and Liu (2016) proposed a Center-Based Similarity (CBS) space learning method (Fei and Liu, 2015). This method first computes a center for each class and transforms each document to a vector of similarities to the center. A binary classifier is then built using the transformed data for each class. The decision surface is like a “ball” encircling each class. Everything outside the ball is considered not belonging to the class. Our proposed method outperforms this method greatly. Fei et al. (2016) further added the capability of incrementally or cumulatively learning new classes, which connects this work to lifelong learning (Chen and Liu, 2016) because without the ability to identify novel"
D17-1314,N16-1061,1,0.435575,"asingly in dynamic open environments where some new/test documents may not belong to any of the training classes, identifying these novel documents during classification presents an important problem. This problem is called openworld classification or open classification. This paper proposes a novel deep learning based approach. It outperforms existing state-of-the-art techniques dramatically. 1 Introduction A key assumption made by classic supervised text classification (or learning) is that classes appeared in the test data must have appeared in training, called the closed-world assumption (Fei and Liu, 2016; Chen and Liu, 2016). Although this assumption holds in many applications, it is violated in many others, especially in dynamic or open environments. For example, in social media, a classifier built with past topics or classes may not be effective in classifying future data because new topics appear constantly in social media (Fei et al., 2016). This is clearly true in other domains too, e.g., self-driving cars, where new objects may appear in the scene all the time. Ideally, in the text domain, the classifier should classify incoming documents to the right existing classes used in training a"
D17-1314,D14-1181,0,0.0148574,"Missing"
D17-1314,P17-2023,1,0.867445,"Missing"
D19-1130,P16-1154,0,0.0316879,"restaurantname=; starttime =) multiple choice(restaurantname) [] ‘inform’ ‘(’ ‘restaurantname’ ‘=’ ‘)’ ‘multiple choice’ ‘=’ ‘restaurantname’ ‘)’ ‘inform’ ‘(’ ‘restaurantname’ ‘=’ ‘;’ ‘;’, ‘;’, ‘=’, ‘;’ ‘starttime’ ‘=’ ‘)’ inform {restaurantname} inform {restaurantname} multiple choice{restaurantname} Table 7: Examples of predicted dialogue acts in the restaurant domain. loss is the sum of the binary cross-entropy of all of them. Seq2Seq (Sutskever et al., 2014) encodes the dialogue state as a sequence, and decodes agent acts as a sequence with attention (Bahdanau et al., 2015). Copy Seq2Seq (Gu et al., 2016) adds a copy mechanism to Seq2Seq, which allows copying words from the encoder input. CAS adopts a single GRU (Cho et al., 2014) for decoding and uses three different fully connected layers for mapping the output of the GRU to continue, act and slots. For each step in the sequence of CAS tuples, given the output of the GRU, continue, act and slot predictions are obtained by separate heads, each with one fully connected layer. The hidden state of the GRU and the predictions at the previous step are passed to the cell at the next step connecting them sequentially. gCAS uses our proposed recurren"
D19-1130,J08-4002,0,0.119401,"Missing"
D19-1130,P18-1133,0,0.152703,"Missing"
D19-1130,N18-4010,1,0.839815,"ly in terms of dialogue acts and domain specific slots. It is a crucial component that influences the efficiency (e.g., the conciseness and smoothness) of the communication between the user and the agent. Both supervised learning (SL) (Stent, 2002; Williams et al., 2017a; Williams and Zweig, 2016; Henderson et al., 2005, 2008) and reinforcement learning (RL) approaches (Walker, 2000; Young et al., 2007; Gasic and Young, 2014; Williams et al., 2017b; Su et al., 2017) have been adopted to learn policies. SL learns a policy to predict acts given the dialogue state. Recent work (Wen et al., 2017; Liu and Lane, 2018) also used SL as pre-training for RL to mitigate the sample inefficiency of RL approaches and to reduce the number of interactions. Sequence2Sequence 1 The code is available at https://leishu02. github.io/ (Seq2Seq) (Sutskever et al., 2014) approaches have also been adopted in user simulators to produce user acts (Gur et al., 2018). These approaches typically assume that the agent can only produce one act per turn through classification. Generating only one act per turn significantly limits what an agent can do in a turn and leads to lengthy dialogues, making tracking of state and context thro"
D19-1130,W17-5518,0,0.0149249,"e(moviename) Table 1: Dialogue example. Introduction In a task-oriented dialogue system, the dialogue manager policy module predicts actions usually in terms of dialogue acts and domain specific slots. It is a crucial component that influences the efficiency (e.g., the conciseness and smoothness) of the communication between the user and the agent. Both supervised learning (SL) (Stent, 2002; Williams et al., 2017a; Williams and Zweig, 2016; Henderson et al., 2005, 2008) and reinforcement learning (RL) approaches (Walker, 2000; Young et al., 2007; Gasic and Young, 2014; Williams et al., 2017b; Su et al., 2017) have been adopted to learn policies. SL learns a policy to predict acts given the dialogue state. Recent work (Wen et al., 2017; Liu and Lane, 2018) also used SL as pre-training for RL to mitigate the sample inefficiency of RL approaches and to reduce the number of interactions. Sequence2Sequence 1 The code is available at https://leishu02. github.io/ (Seq2Seq) (Sutskever et al., 2014) approaches have also been adopted in user simulators to produce user acts (Gur et al., 2018). These approaches typically assume that the agent can only produce one act per turn through classification. Generatin"
D19-1130,P17-1062,0,0.183784,"ide of the Door, and The Boy are all thrillers. Would you like to find tickets for a showing for any of them? inform(moviename=The Witch, The Other Side of the Door, The Boy; genre=thriller) multiple choice(moviename) Table 1: Dialogue example. Introduction In a task-oriented dialogue system, the dialogue manager policy module predicts actions usually in terms of dialogue acts and domain specific slots. It is a crucial component that influences the efficiency (e.g., the conciseness and smoothness) of the communication between the user and the agent. Both supervised learning (SL) (Stent, 2002; Williams et al., 2017a; Williams and Zweig, 2016; Henderson et al., 2005, 2008) and reinforcement learning (RL) approaches (Walker, 2000; Young et al., 2007; Gasic and Young, 2014; Williams et al., 2017b; Su et al., 2017) have been adopted to learn policies. SL learns a policy to predict acts given the dialogue state. Recent work (Wen et al., 2017; Liu and Lane, 2018) also used SL as pre-training for RL to mitigate the sample inefficiency of RL approaches and to reduce the number of interactions. Sequence2Sequence 1 The code is available at https://leishu02. github.io/ (Seq2Seq) (Sutskever et al., 2014) approaches"
D19-1130,D15-1216,0,0.0713642,"Missing"
D19-1655,C10-2005,0,0.0715818,"l sentiment analysis. Our experimental results show that the proposed model outperforms the state-of-the-art methods. 2 Related Work Our work is related to sentence sentiment classification (SSC). SSC has been studied extensively (Hu and Liu, 2004; Pang and Lee, 2005; Zhao et al., 2008; Narayanan et al., 2009; T¨ackstr¨om and McDonald, 2011; Wang and Manning, 2012; Yang and Cardie, 2014; Kim, 2014; Tang et al., 2015; Wu et al., 2017; Wang et al., 2018). None of them can handle noisy labels. Since many social media datasets are noisy, researchers have tried to build robust models (Gamon, 2004; Barbosa and Feng, 2010; Liu et al., 2012). However, they treat noisy data as additional information and don’t specifically handle noisy labels. A noiseaware classification model in (Zhan et al., 2019) trains using data annotated with multiple labels. Wang et al. (2016) exploited the connection of users and noisy labels of sentiments in social networks. Since the two works use multiple-labeled data or users’ information (we only use singlelabeled data, and we do not use any additional information), they have different settings than ours. Our work is closely related to DNNs based approaches to learning with noisy lab"
D19-1655,C04-1121,0,0.094286,"sentence-level sentiment analysis. Our experimental results show that the proposed model outperforms the state-of-the-art methods. 2 Related Work Our work is related to sentence sentiment classification (SSC). SSC has been studied extensively (Hu and Liu, 2004; Pang and Lee, 2005; Zhao et al., 2008; Narayanan et al., 2009; T¨ackstr¨om and McDonald, 2011; Wang and Manning, 2012; Yang and Cardie, 2014; Kim, 2014; Tang et al., 2015; Wu et al., 2017; Wang et al., 2018). None of them can handle noisy labels. Since many social media datasets are noisy, researchers have tried to build robust models (Gamon, 2004; Barbosa and Feng, 2010; Liu et al., 2012). However, they treat noisy data as additional information and don’t specifically handle noisy labels. A noiseaware classification model in (Zhan et al., 2019) trains using data annotated with multiple labels. Wang et al. (2016) exploited the connection of users and noisy labels of sentiments in social networks. Since the two works use multiple-labeled data or users’ information (we only use singlelabeled data, and we do not use any additional information), they have different settings than ours. Our work is closely related to DNNs based approaches to"
D19-1655,D14-1181,0,0.337951,"in early epochs and then train A B-network and Anetwork with their own loss functions in an alternating manner. To our knowledge, this is the first work that addresses the noisy label problem in sentence-level sentiment analysis. Our experimental results show that the proposed model outperforms the state-of-the-art methods. 2 Related Work Our work is related to sentence sentiment classification (SSC). SSC has been studied extensively (Hu and Liu, 2004; Pang and Lee, 2005; Zhao et al., 2008; Narayanan et al., 2009; T¨ackstr¨om and McDonald, 2011; Wang and Manning, 2012; Yang and Cardie, 2014; Kim, 2014; Tang et al., 2015; Wu et al., 2017; Wang et al., 2018). None of them can handle noisy labels. Since many social media datasets are noisy, researchers have tried to build robust models (Gamon, 2004; Barbosa and Feng, 2010; Liu et al., 2012). However, they treat noisy data as additional information and don’t specifically handle noisy labels. A noiseaware classification model in (Zhan et al., 2019) trains using data annotated with multiple labels. Wang et al. (2016) exploited the connection of users and noisy labels of sentiments in social networks. Since the two works use multiple-labeled data"
D19-1655,D09-1019,1,0.716741,"predict ‘clean’ labels for the input training (and test) sentences. In training, we pre-train A-network in early epochs and then train A B-network and Anetwork with their own loss functions in an alternating manner. To our knowledge, this is the first work that addresses the noisy label problem in sentence-level sentiment analysis. Our experimental results show that the proposed model outperforms the state-of-the-art methods. 2 Related Work Our work is related to sentence sentiment classification (SSC). SSC has been studied extensively (Hu and Liu, 2004; Pang and Lee, 2005; Zhao et al., 2008; Narayanan et al., 2009; T¨ackstr¨om and McDonald, 2011; Wang and Manning, 2012; Yang and Cardie, 2014; Kim, 2014; Tang et al., 2015; Wu et al., 2017; Wang et al., 2018). None of them can handle noisy labels. Since many social media datasets are noisy, researchers have tried to build robust models (Gamon, 2004; Barbosa and Feng, 2010; Liu et al., 2012). However, they treat noisy data as additional information and don’t specifically handle noisy labels. A noiseaware classification model in (Zhan et al., 2019) trains using data annotated with multiple labels. Wang et al. (2016) exploited the connection of users and no"
D19-1655,P05-1015,0,0.692075,"abels, while exploiting another CNN to predict ‘clean’ labels for the input training (and test) sentences. In training, we pre-train A-network in early epochs and then train A B-network and Anetwork with their own loss functions in an alternating manner. To our knowledge, this is the first work that addresses the noisy label problem in sentence-level sentiment analysis. Our experimental results show that the proposed model outperforms the state-of-the-art methods. 2 Related Work Our work is related to sentence sentiment classification (SSC). SSC has been studied extensively (Hu and Liu, 2004; Pang and Lee, 2005; Zhao et al., 2008; Narayanan et al., 2009; T¨ackstr¨om and McDonald, 2011; Wang and Manning, 2012; Yang and Cardie, 2014; Kim, 2014; Tang et al., 2015; Wu et al., 2017; Wang et al., 2018). None of them can handle noisy labels. Since many social media datasets are noisy, researchers have tried to build robust models (Gamon, 2004; Barbosa and Feng, 2010; Liu et al., 2012). However, they treat noisy data as additional information and don’t specifically handle noisy labels. A noiseaware classification model in (Zhan et al., 2019) trains using data annotated with multiple labels. Wang et al. (201"
D19-1655,D14-1162,0,0.0816281,") where I is the indicator function (if y == i, I = 1; ¨ is the number of senotherwise, I = 0), and |S| tences to train A B-network in each batch. Similarly, we use cross-entropy with the predicted labels yb and the input labels y to compute the clean loss, formulated as Lclean = − 1 XX I(y = i|x) log P (b y = i|x) b |S| b x∈S i (3) b where |S |is the number of sentences to train Anetwork in each batch. Next we introduce how our model learns the parameters (ϑ, θ and Q). An embedding matrix v is produced for each sentence x by looking up a pre-trained word embedding database (e.g., GloVe.840B (Pennington et al., 2014)). Then an encoding vector h = CN N (v) (and u = CN N (v)) is produced for each embedding matrix v in Anetwork (and A B-network). A sofmax classifier gives us P (ˆ y = i|x, ϑ) (i.e., ‘clean’ sentiment scores) on the learned encoding vector h. As the noise transition matrix Q indicates the transition values from clean labels to noisy labels, we com6288 Movie Laptop Restaurant #Noisy Training Data 13539P, 13350N 9702P, 7876N 8094P, 10299N #Clean Training Data 4265P, 4265N 1064P, 490N 1087P, 823N #Validation Data 105P, 106N 33P, 20N 39P, 14N #Test Data 960P, 957N 298P, 175N 339P, 116N Table 1: Su"
D19-1655,P11-2100,0,0.0785001,"Missing"
D19-1655,P14-1031,0,0.0263417,"we pre-train A-network in early epochs and then train A B-network and Anetwork with their own loss functions in an alternating manner. To our knowledge, this is the first work that addresses the noisy label problem in sentence-level sentiment analysis. Our experimental results show that the proposed model outperforms the state-of-the-art methods. 2 Related Work Our work is related to sentence sentiment classification (SSC). SSC has been studied extensively (Hu and Liu, 2004; Pang and Lee, 2005; Zhao et al., 2008; Narayanan et al., 2009; T¨ackstr¨om and McDonald, 2011; Wang and Manning, 2012; Yang and Cardie, 2014; Kim, 2014; Tang et al., 2015; Wu et al., 2017; Wang et al., 2018). None of them can handle noisy labels. Since many social media datasets are noisy, researchers have tried to build robust models (Gamon, 2004; Barbosa and Feng, 2010; Liu et al., 2012). However, they treat noisy data as additional information and don’t specifically handle noisy labels. A noiseaware classification model in (Zhan et al., 2019) trains using data annotated with multiple labels. Wang et al. (2016) exploited the connection of users and noisy labels of sentiments in social networks. Since the two works use multiple-l"
D19-1655,D08-1013,0,0.039409,"ing another CNN to predict ‘clean’ labels for the input training (and test) sentences. In training, we pre-train A-network in early epochs and then train A B-network and Anetwork with their own loss functions in an alternating manner. To our knowledge, this is the first work that addresses the noisy label problem in sentence-level sentiment analysis. Our experimental results show that the proposed model outperforms the state-of-the-art methods. 2 Related Work Our work is related to sentence sentiment classification (SSC). SSC has been studied extensively (Hu and Liu, 2004; Pang and Lee, 2005; Zhao et al., 2008; Narayanan et al., 2009; T¨ackstr¨om and McDonald, 2011; Wang and Manning, 2012; Yang and Cardie, 2014; Kim, 2014; Tang et al., 2015; Wu et al., 2017; Wang et al., 2018). None of them can handle noisy labels. Since many social media datasets are noisy, researchers have tried to build robust models (Gamon, 2004; Barbosa and Feng, 2010; Liu et al., 2012). However, they treat noisy data as additional information and don’t specifically handle noisy labels. A noiseaware classification model in (Zhan et al., 2019) trains using data annotated with multiple labels. Wang et al. (2016) exploited the co"
D19-1655,P12-2018,0,0.374599,"sentences. In training, we pre-train A-network in early epochs and then train A B-network and Anetwork with their own loss functions in an alternating manner. To our knowledge, this is the first work that addresses the noisy label problem in sentence-level sentiment analysis. Our experimental results show that the proposed model outperforms the state-of-the-art methods. 2 Related Work Our work is related to sentence sentiment classification (SSC). SSC has been studied extensively (Hu and Liu, 2004; Pang and Lee, 2005; Zhao et al., 2008; Narayanan et al., 2009; T¨ackstr¨om and McDonald, 2011; Wang and Manning, 2012; Yang and Cardie, 2014; Kim, 2014; Tang et al., 2015; Wu et al., 2017; Wang et al., 2018). None of them can handle noisy labels. Since many social media datasets are noisy, researchers have tried to build robust models (Gamon, 2004; Barbosa and Feng, 2010; Liu et al., 2012). However, they treat noisy data as additional information and don’t specifically handle noisy labels. A noiseaware classification model in (Zhan et al., 2019) trains using data annotated with multiple labels. Wang et al. (2016) exploited the connection of users and noisy labels of sentiments in social networks. Since the t"
I11-1131,H05-1043,0,0.208729,"plication domain. When the algorithm converges, we obtain a ranked list of candidate resource terms. Our experimental results based on 7 real-life data sets show the effectiveness of the proposed method. It outperforms 5 strong baselines. 2 Related work As we discussed in the introduction, this work is mainly related to product aspect extraction. Hu and Liu (2004) proposed a technique based on association rule mining to extract frequent nouns and noun phrases as product aspects. They also introduced the idea of using sentiment words to find additional (infrequent) aspects. Popescu and Etzioni (2005) improved the precision of this method by determining whether a noun/noun phrase is indeed a product aspect by computing the pointwise mutual information (PMI) score between the phrase and class discriminators, e.g., “xx has”, “xx comes with”, etc., where xx is a product class word, and using Web search. A dependency based method is proposed in (Zhuang et al., 2006) to extract aspects for a movie review application. Dependency relations are also used in (Qiu et al. 2011) to extract both aspects and sentiment words. Zhang et al. (2010) augmented this method by introducing aspect ranking. Wang a"
I11-1131,J95-4004,0,0.105261,"1 0.74 Table 4. Experimental results: Precision@10 Data sets Car Washer TF TFR HITS MRE-NI MRE-NS MRE 0.40 0.30 0.55 0.30 0.60 0.75 0.30 0.50 0.65 0.70 0.65 0.70 Paint Printer Haircare Mobile TV Ave. 0.20 0.30 0.50 0.45 0.50 0.65 0.35 0.20 0.50 0.50 0.55 0.60 0.35 0.40 0.35 0.45 0.45 0.55 0.32 0.34 0.51 0.48 0.55 0.65 Table 5. Experimental results: Precision@20 data set (“Sent.” means the sentence). Each data set contains a mixture of reviews, blogs, and forum discussions about one type of product. We split each posting into sentences and the sentences are POS-tagged using the Brill’s tagger (Brill, 1995). The tagged sentences are the input to our system MRE. The global resource terms (resource seeds) used in the first stage of our method are: “gas”, “water”, “electricity”, “money”, “ink”, “shampoo”, “detergent”, “room” “fabric softener”, and “soap”. In stage 1 of our algorithm, we used the combined data set of those in Table 2 to compute the hub scores for global resources usage verbs found to be associated with the resource seeds through some quantifiers. 4.2 Evaluation Metrics We adopt the rank precision, also called precision@N metric for the experimental evaluation. It gives the percentag"
I11-1131,esuli-sebastiani-2006-sentiwordnet,0,0.0397294,"Missing"
I11-1131,C04-1200,0,0.0799958,"initialize the iterative computation, some global seed resources are employed to find and to score some strong resource usage verbs. These scores are applied as initialization for the iterative computation in the bipartite graph for any application domain. When the algorithm converges, we obtain a ranked list of candidate resource terms. Our experimental results based on 7 real-life data sets show the effectiveness of the proposed method. It outperforms 5 strong baselines. 2 Related work As we discussed in the introduction, this work is mainly related to product aspect extraction. Hu and Liu (2004) proposed a technique based on association rule mining to extract frequent nouns and noun phrases as product aspects. They also introduced the idea of using sentiment words to find additional (infrequent) aspects. Popescu and Etzioni (2005) improved the precision of this method by determining whether a noun/noun phrase is indeed a product aspect by computing the pointwise mutual information (PMI) score between the phrase and class discriminators, e.g., “xx has”, “xx comes with”, etc., where xx is a product class word, and using Web search. A dependency based method is proposed in (Zhuang et al"
I11-1131,W06-1652,0,0.0184065,"ity” is negative but “this washer uses little water” is positive. Extracting such resource words and phrases are important for sentiment analysis. This paper formulates the problem based on a bipartite graph and proposes a novel iterative algorithm to solve the problem. Experimental results using diverse real-life sentiment corpora show good results. 1 Introduction Sentiment analysis or opinion mining has been an active research area in recent years (e.g., Pang and Lee 2008; Turney, 2002; Wiebe et al. 2004; Hu and Liu, 2004; Kim and Eduard, 2004; Wilson et al. 2005; Popescu and Etzioni, 2005; Riloff et al. 2006; Esuli and Fabrizio, 2006; Mei et al, 2007; Stoyanov and Cardie; 2008). Researchers have studied the problem at the document level, sentence level and aspect level to determine the sentiment polarity expressed in a document, in a sentence and on an aspect of an entity (see the surveys (Pang and Lee, 2008) and (Liu, 2010)). One type of key information used in almost all existing sentiment analysis techniques is a list of sentiment words (or opinion words). Positive sentiment words are words expressing desired states or qualities, e.g., good, amazing, and excellent, and negative sentiment words"
I11-1131,D07-1114,0,0.099426,"nsume no or little resource consume less resource consume a large quantity of resource consume more resource Figure 1: Sentiment polarity of statements involving resources. old GE washer”. To the best of our knowledge, there is no reported algorithm that extracts resource terms. In this paper, we propose an iterative algorithm to extract them from a domain corpus, e.g., a set of product reviews. In the above example sentence, we want to extract “water” as a resource term. The most related work to ours is the product aspect/feature extraction (e.g., Hu and Liu, 2004, Popescu and Etzioni, 2005, Kobayashi et al. 2007, Scaffidi et al. 2007, Titov and McDonald, 2008, Stoyanov and Cardie. 2008, Wong et al., 2008, Zhao et al., 2010). A resource in a domain is often an aspect or implies an aspect. For example, in “this camera uses a lot of battery power”, “battery power” clearly indicates battery life, which is an aspect of the camera entity. However, there are some important differences between resources and other types of aspects. The key difference is that resource terms often contribute directly to sentiments (e.g., based on the quantity that is consumed), while other aspects may not. e.g., “picture qualit"
I11-1131,P02-1053,0,0.00342574,"s is those that express resources such as water, electricity, gas, etc. For example, “this washer uses a lot of electricity” is negative but “this washer uses little water” is positive. Extracting such resource words and phrases are important for sentiment analysis. This paper formulates the problem based on a bipartite graph and proposes a novel iterative algorithm to solve the problem. Experimental results using diverse real-life sentiment corpora show good results. 1 Introduction Sentiment analysis or opinion mining has been an active research area in recent years (e.g., Pang and Lee 2008; Turney, 2002; Wiebe et al. 2004; Hu and Liu, 2004; Kim and Eduard, 2004; Wilson et al. 2005; Popescu and Etzioni, 2005; Riloff et al. 2006; Esuli and Fabrizio, 2006; Mei et al, 2007; Stoyanov and Cardie; 2008). Researchers have studied the problem at the document level, sentence level and aspect level to determine the sentiment polarity expressed in a document, in a sentence and on an aspect of an entity (see the surveys (Pang and Lee, 2008) and (Liu, 2010)). One type of key information used in almost all existing sentiment analysis techniques is a list of sentiment words (or opinion words). Positive sent"
I11-1131,I08-1038,0,0.0145126,"(2005) improved the precision of this method by determining whether a noun/noun phrase is indeed a product aspect by computing the pointwise mutual information (PMI) score between the phrase and class discriminators, e.g., “xx has”, “xx comes with”, etc., where xx is a product class word, and using Web search. A dependency based method is proposed in (Zhuang et al., 2006) to extract aspects for a movie review application. Dependency relations are also used in (Qiu et al. 2011) to extract both aspects and sentiment words. Zhang et al. (2010) augmented this method by introducing aspect ranking. Wang and Wang (2008) proposed a similar bootstrapping method but not based on dependencies. In (Kobayashi et al. 2007), a pattern mining method was proposed to find extraction patterns. Statistics from the corpus are employed to determine the extraction confidence. Other works on aspect extraction use topic modeling and probabilistic modeling to capture and group aspects at the same time (e.g., Mei et al., 2007; Titov and McDonald, 2008; Lu et al. 2009; Zhao et al., 2010; Wang et al. 2010; Jo and Oh, 2011). In (Su et al., 2008), a clustering method was also proposed with mutual reinforcement to identify aspects."
I11-1131,J04-3002,0,0.0448698,"t express resources such as water, electricity, gas, etc. For example, “this washer uses a lot of electricity” is negative but “this washer uses little water” is positive. Extracting such resource words and phrases are important for sentiment analysis. This paper formulates the problem based on a bipartite graph and proposes a novel iterative algorithm to solve the problem. Experimental results using diverse real-life sentiment corpora show good results. 1 Introduction Sentiment analysis or opinion mining has been an active research area in recent years (e.g., Pang and Lee 2008; Turney, 2002; Wiebe et al. 2004; Hu and Liu, 2004; Kim and Eduard, 2004; Wilson et al. 2005; Popescu and Etzioni, 2005; Riloff et al. 2006; Esuli and Fabrizio, 2006; Mei et al, 2007; Stoyanov and Cardie; 2008). Researchers have studied the problem at the document level, sentence level and aspect level to determine the sentiment polarity expressed in a document, in a sentence and on an aspect of an entity (see the surveys (Pang and Lee, 2008) and (Liu, 2010)). One type of key information used in almost all existing sentiment analysis techniques is a list of sentiment words (or opinion words). Positive sentiment words are wor"
I11-1131,H05-1044,0,0.0977165,"or example, “this washer uses a lot of electricity” is negative but “this washer uses little water” is positive. Extracting such resource words and phrases are important for sentiment analysis. This paper formulates the problem based on a bipartite graph and proposes a novel iterative algorithm to solve the problem. Experimental results using diverse real-life sentiment corpora show good results. 1 Introduction Sentiment analysis or opinion mining has been an active research area in recent years (e.g., Pang and Lee 2008; Turney, 2002; Wiebe et al. 2004; Hu and Liu, 2004; Kim and Eduard, 2004; Wilson et al. 2005; Popescu and Etzioni, 2005; Riloff et al. 2006; Esuli and Fabrizio, 2006; Mei et al, 2007; Stoyanov and Cardie; 2008). Researchers have studied the problem at the document level, sentence level and aspect level to determine the sentiment polarity expressed in a document, in a sentence and on an aspect of an entity (see the surveys (Pang and Lee, 2008) and (Liu, 2010)). One type of key information used in almost all existing sentiment analysis techniques is a list of sentiment words (or opinion words). Positive sentiment words are words expressing desired states or qualities, e.g., good, amazi"
I11-1131,D10-1101,0,\N,Missing
I11-1131,C08-1103,0,\N,Missing
I11-1131,H05-2017,0,\N,Missing
I11-1131,C10-2167,1,\N,Missing
I11-1131,D10-1006,0,\N,Missing
I11-1131,J11-1002,1,\N,Missing
J11-1002,P05-1045,0,0.0511258,"Missing"
J11-1002,P97-1023,0,0.0311078,"traction and target (or topic) extraction in opinion mining. 10 Qiu et al. Opinion Word Expansion and Target Extraction through Double Propagation 2.1 Opinion Word Extraction Extensive work has been done on sentiment analysis at word, expression (Breck, Choi, and Cardie 2007; Takamura, Inui, and Okumura 2007), sentence (Yu and Hatzivassiloglou 2003; Kim and Hovy 2004) and document (Pang, Lee, and Vaithyanathan 2002; Turney 2002) levels. We only describe work at word level as it is most relevant to our work. In general, the existing work can be categorized as corporabased (Hatzivassiloglou and McKeown 1997; Wiebe 2000; Wiebe et al. 2004; Turney and Littman 2003; Kanayama and Nasukawa 2006; Kaji and Kitsuregawa 2007) and dictionary-based (Hu and Liu 2004; Kim and Hovy 2004; Kamps et al. 2004; Esuli and Sebastiani 2005; Takamura, Inui, and Okumura 2005) approaches. Our work falls into the corpora-based category. Hatzivassiloglou and McKeown (1997) proposed the ﬁrst method for determining adjective polarities or orientations (positive, negative, and neutral). The method predicts orientations of adjectives by detecting pairs of such words conjoined by conjunctions such as and and or in a large docu"
J11-1002,D07-1115,0,0.0393331,"and Target Extraction through Double Propagation 2.1 Opinion Word Extraction Extensive work has been done on sentiment analysis at word, expression (Breck, Choi, and Cardie 2007; Takamura, Inui, and Okumura 2007), sentence (Yu and Hatzivassiloglou 2003; Kim and Hovy 2004) and document (Pang, Lee, and Vaithyanathan 2002; Turney 2002) levels. We only describe work at word level as it is most relevant to our work. In general, the existing work can be categorized as corporabased (Hatzivassiloglou and McKeown 1997; Wiebe 2000; Wiebe et al. 2004; Turney and Littman 2003; Kanayama and Nasukawa 2006; Kaji and Kitsuregawa 2007) and dictionary-based (Hu and Liu 2004; Kim and Hovy 2004; Kamps et al. 2004; Esuli and Sebastiani 2005; Takamura, Inui, and Okumura 2005) approaches. Our work falls into the corpora-based category. Hatzivassiloglou and McKeown (1997) proposed the ﬁrst method for determining adjective polarities or orientations (positive, negative, and neutral). The method predicts orientations of adjectives by detecting pairs of such words conjoined by conjunctions such as and and or in a large document set. The underlying intuition is that the orientations of conjoined adjectives are subject to some linguist"
J11-1002,kamps-etal-2004-using,0,0.026019,"Missing"
J11-1002,W06-1642,0,0.113762,"al. Opinion Word Expansion and Target Extraction through Double Propagation 2.1 Opinion Word Extraction Extensive work has been done on sentiment analysis at word, expression (Breck, Choi, and Cardie 2007; Takamura, Inui, and Okumura 2007), sentence (Yu and Hatzivassiloglou 2003; Kim and Hovy 2004) and document (Pang, Lee, and Vaithyanathan 2002; Turney 2002) levels. We only describe work at word level as it is most relevant to our work. In general, the existing work can be categorized as corporabased (Hatzivassiloglou and McKeown 1997; Wiebe 2000; Wiebe et al. 2004; Turney and Littman 2003; Kanayama and Nasukawa 2006; Kaji and Kitsuregawa 2007) and dictionary-based (Hu and Liu 2004; Kim and Hovy 2004; Kamps et al. 2004; Esuli and Sebastiani 2005; Takamura, Inui, and Okumura 2005) approaches. Our work falls into the corpora-based category. Hatzivassiloglou and McKeown (1997) proposed the ﬁrst method for determining adjective polarities or orientations (positive, negative, and neutral). The method predicts orientations of adjectives by detecting pairs of such words conjoined by conjunctions such as and and or in a large document set. The underlying intuition is that the orientations of conjoined adjectives"
J11-1002,C04-1200,0,0.390338,"anuary 2010; accepted for publication: 20 July 2010. © 2011 Association for Computational Linguistics Computational Linguistics Volume 37, Number 1 to many challenging research problems and practical applications. Two fundamental problems in opinion mining are opinion lexicon expansion and opinion target extraction (Liu 2006; Pang and Lee 2008). An opinion lexicon is a list of opinion words such as good, excellent, poor, and bad which are used to indicate positive or negative sentiments. It forms the foundation of many opinion mining tasks, for example, sentence (Yu and Hatzivassiloglou 2003; Kim and Hovy 2004) and document (Pang, Lee, and Vaithyanathan 2002; Turney 2002) sentiment classiﬁcation, and feature-based opinion summarization (Hu and Liu 2004). Although there are several opinion lexicons publicly available, it is hard, if not impossible, to maintain a universal opinion lexicon to cover all domains as opinion expressions vary signiﬁcantly from domain to domain. A word can be positive in one domain but has no opinion or even negative opinion in another domain. Therefore, it is necessary to expand a known opinion lexicon for applications in different domains using text corpora from the corres"
J11-1002,D07-1114,0,0.0301874,"Missing"
J11-1002,W02-1011,0,0.0294507,"Missing"
J11-1002,H05-1043,0,0.776964,"ies are domain-independent. For example, unpredictable is often a positive opinion word in movie reviews, as in unpredictable plot, but in car reviews unpredictable is likely to be negative, as in unpredictable steering. Our approach 11 Computational Linguistics Volume 37, Number 1 extracts opinion words using domain dependent corpora; thus we are able to ﬁnd domain-dependent opinion words. 2.2 Opinion Target Extraction Opinion target (or topic) extraction is a difﬁcult task in opinion mining. Several methods have been proposed, mainly in the context of product review mining (Hu and Liu 2004; Popescu and Etzioni 2005; Kobayashi, Inui, and Matsumoto 2007; Mei et al. 2007; Scafﬁdi et al. 2007; Wong, Lam, and Wong 2008; Stoyanov and Cardie 2008). In this mining task, opinion targets usually refer to product features, which are deﬁned as product components or attributes, as in Liu (2006). In the work of Hu and Liu (2004), frequent nouns and noun phrases are treated as product feature candidates. In our work, we also extract only noun targets. Different pruning methods are proposed to remove the noise. To cover infrequent features that are missed, they regard the nearest nouns/noun phrases of the opinion words"
J11-1002,P05-1017,0,0.0155506,"Missing"
J11-1002,P02-1053,0,0.125419,"ion for Computational Linguistics Computational Linguistics Volume 37, Number 1 to many challenging research problems and practical applications. Two fundamental problems in opinion mining are opinion lexicon expansion and opinion target extraction (Liu 2006; Pang and Lee 2008). An opinion lexicon is a list of opinion words such as good, excellent, poor, and bad which are used to indicate positive or negative sentiments. It forms the foundation of many opinion mining tasks, for example, sentence (Yu and Hatzivassiloglou 2003; Kim and Hovy 2004) and document (Pang, Lee, and Vaithyanathan 2002; Turney 2002) sentiment classiﬁcation, and feature-based opinion summarization (Hu and Liu 2004). Although there are several opinion lexicons publicly available, it is hard, if not impossible, to maintain a universal opinion lexicon to cover all domains as opinion expressions vary signiﬁcantly from domain to domain. A word can be positive in one domain but has no opinion or even negative opinion in another domain. Therefore, it is necessary to expand a known opinion lexicon for applications in different domains using text corpora from the corresponding domains. Opinion targets are topics on which opinions"
J11-1002,J04-3002,0,0.00751714,"pic) extraction in opinion mining. 10 Qiu et al. Opinion Word Expansion and Target Extraction through Double Propagation 2.1 Opinion Word Extraction Extensive work has been done on sentiment analysis at word, expression (Breck, Choi, and Cardie 2007; Takamura, Inui, and Okumura 2007), sentence (Yu and Hatzivassiloglou 2003; Kim and Hovy 2004) and document (Pang, Lee, and Vaithyanathan 2002; Turney 2002) levels. We only describe work at word level as it is most relevant to our work. In general, the existing work can be categorized as corporabased (Hatzivassiloglou and McKeown 1997; Wiebe 2000; Wiebe et al. 2004; Turney and Littman 2003; Kanayama and Nasukawa 2006; Kaji and Kitsuregawa 2007) and dictionary-based (Hu and Liu 2004; Kim and Hovy 2004; Kamps et al. 2004; Esuli and Sebastiani 2005; Takamura, Inui, and Okumura 2005) approaches. Our work falls into the corpora-based category. Hatzivassiloglou and McKeown (1997) proposed the ﬁrst method for determining adjective polarities or orientations (positive, negative, and neutral). The method predicts orientations of adjectives by detecting pairs of such words conjoined by conjunctions such as and and or in a large document set. The underlying intuit"
J11-1002,W03-1017,0,0.061013,"ised submission received: 20 January 2010; accepted for publication: 20 July 2010. © 2011 Association for Computational Linguistics Computational Linguistics Volume 37, Number 1 to many challenging research problems and practical applications. Two fundamental problems in opinion mining are opinion lexicon expansion and opinion target extraction (Liu 2006; Pang and Lee 2008). An opinion lexicon is a list of opinion words such as good, excellent, poor, and bad which are used to indicate positive or negative sentiments. It forms the foundation of many opinion mining tasks, for example, sentence (Yu and Hatzivassiloglou 2003; Kim and Hovy 2004) and document (Pang, Lee, and Vaithyanathan 2002; Turney 2002) sentiment classiﬁcation, and feature-based opinion summarization (Hu and Liu 2004). Although there are several opinion lexicons publicly available, it is hard, if not impossible, to maintain a universal opinion lexicon to cover all domains as opinion expressions vary signiﬁcantly from domain to domain. A word can be positive in one domain but has no opinion or even negative opinion in another domain. Therefore, it is necessary to expand a known opinion lexicon for applications in different domains using text cor"
J11-1002,N07-1037,0,\N,Missing
J11-1002,C08-1103,0,\N,Missing
J11-1002,H05-2017,0,\N,Missing
N13-1124,P07-1056,0,0.0650549,"0). Aue & Gamon (2005) tried training on a mixture of labeled reviews from other domains where such data are available and test on the target domain. This is basically one of our baseline methods 3TR-1TE in Section 4. Their work does not do multiple iterations and does not build two separate classifiers as we do. Some related methods were also proposed in (W. Dai, Xue, Yang & Yu, 2007; Tan et al., 2007; Yang, Si & Callan, 2006). More sophisticated transfer learning methods try to find common features in both the source and target domains and then try to map the differences of the two domains (Blitzer, Dredze, & Pereira, 2007; Pan, et al, 2010; Bollegala, Weir & Carroll, 2011; Tan et al., 2009). Some researchers also used topic modeling of both domains to transfer knowledge (Gao & Li, 2011; He, Lin & Alani, 2011). However, none of these methods deals with the two problems/difficulties of our task. Co-Class tackles them explicitly and effectively (Section 4). The proposed Co-Class method is also related to Co-Training method in (Blum & Mitchell, 1998). We will compare them in detail in Section 3.3. the difficulty of finding common features across different domains), Co-Class avoids it by using an EM-based method to"
N13-1124,P11-1014,0,0.0458671,"Missing"
N13-1124,P11-1013,0,0.0173959,"Missing"
N13-1124,C08-1052,0,0.0267386,"gh data (which represent the user’s behavior). Such intents are typically implicit because people usually do not issue a search query like “I want to buy a digital camera.” Instead, they may just type the keywords “digital camera”. Our interest is to identify explicit intents expressed in full text documents (forum posts). Another related problem is online commercial intention (OCI) identification (Dai et al., 2006; Hu et al., 2009), which focuses on capturing commercial intention based on a user query and web browsing history. In this sense, OCI is still a user query intent problem. In NLP, (Kanayama & Nasukawa, 2008) studied users’ needs and wants from opinions. For example, they aimed to identify the user needs from sentences such as “I’d be happy if it is equipped with a crisp LCD.” This is clearly different from our explicit intention to buy or to use a product/service, e.g., “I plan to buy a new TV.” Our proposed Co-Class technique is related to transfer learning or domain adaptation. The proposed method belongs to “feature representation transfer&quot; from source domain to target domain (Pan & Yang, 2010). Aue & Gamon (2005) tried training on a mixture of labeled reviews from other domains where such dat"
N13-1124,P10-1136,0,0.038904,"s. By explicit we mean that the intention is explicitly stated in the text, no need to deduce (hidden or implicit intention). For example, in the above sentence, the author clearly expressed that he/she wanted to buy a car. On the other hand, an example of an implicit sentence is “Anyone knows the battery life of iPhone?” The person may or may not be thinking about buying an iPhone. To our knowledge, there is no reported study of this problem in the context of text documents. The main related work is in Web search, where user (or query) intent classification is a major issue (Hu et al., 2009; Li, 2010; Li, Wang, & Acero, 2008). Its task is to determine what the user is searching for based on his/her keyword queries (2 to 3 words) and his/her click data. We will discuss this and other related work in Section 2. We formulate the proposed problem as a twoclass classification problem since an application may only be interested in a particular intention. We define intention posts (positive class) as the posts that explicitly express a particular intention of interest, e.g., the intention to buy a product. The other posts are non-intention posts (negative class). Note that we do not exploit inte"
N18-1187,P17-1045,0,0.577602,"query-regression networks (Seo et al., 2016), gated memory networks (Liu and Perez, 2017), and copy-augmented networks (Eric and Manning, 2017) to learn the dialogue state. These systems directly select a final response from a list of response candidates conditioning on the dialogue history without doing slot filling or user goal tracking. Our model, on the other hand, explicitly tracks user’s goal for effective integration with knowledge bases (KBs). Robust dialogue state tracking has been shown (Jurˇc´ıcˇ ek et al., 2012) to 2061 be critical in improving dialogue success in task completion. Dhingra et al. (2017) proposed an end-to-end RL dialogue agent for information access. Their model focuses on bringing differentiability to the KB query operation by introducing a “soft” retrieval process in selecting the KB entries. Such soft-KB lookup is prone to entity updates and additions in the KB, which is common in real world information systems. In our model, we use symbolic queries and leave the selection of KB entities to external services (e.g. a recommender system), as entity ranking in real world systems can be made with much richer features (e.g. user profiles, location and time context, etc.). Qual"
N18-1187,E17-2075,0,0.1117,"nknown how well the model performance generalizes to unseen dialogue state during user interactions. Our system is trained by a combination of supervised and deep RL methods, as it is shown that RL may effectively improve dialogue success rate by exploring a large dialogue action space (Henderson et al., 2008; Li et al., 2017). Bordes and Weston (2017) proposed a taskoriented dialogue model using end-to-end memory networks. In the same line of research, people explored using query-regression networks (Seo et al., 2016), gated memory networks (Liu and Perez, 2017), and copy-augmented networks (Eric and Manning, 2017) to learn the dialogue state. These systems directly select a final response from a list of response candidates conditioning on the dialogue history without doing slot filling or user goal tracking. Our model, on the other hand, explicitly tracks user’s goal for effective integration with knowledge bases (KBs). Robust dialogue state tracking has been shown (Jurˇc´ıcˇ ek et al., 2012) to 2061 be critical in improving dialogue success in task completion. Dhingra et al. (2017) proposed an end-to-end RL dialogue agent for information access. Their model focuses on bringing differentiability to the"
N18-1187,J08-4002,0,0.737487,"This makes it especially important for a system to be able to learn from users in an interactive manner. Comparing to SL models, systems trained with RL by receiving feedback during users interactions showed improved model robustness against diverse dialogue scenarios (Williams and Zweig, 2016; Liu and Lane, 2017b). A critical step in learning RL based taskoriented dialogue models is dialogue policy learning. Training dialogue policy online from scratch typically requires a large number of interactive learning sessions before an agent can reach a satisfactory performance level. Recent works (Henderson et al., 2008; Williams et al., 2017; Liu et al., 2017) explored pre-training the dialogue model using human-human or human-machine dialogue 2060 Proceedings of NAACL-HLT 2018, pages 2060–2069 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics corpora before performing interactive learning with RL to address this concern. A potential drawback with such pre-training approach is that the model may suffer from the mismatch of dialogue state distributions between supervised training and interactive learning stages. While interacting with users, the agent’s response at ea"
N18-1187,W14-4337,0,0.647552,"y in successfully completing a task. 1 Introduction Task-oriented dialogue systems assist users to complete tasks in specific domains by understanding user’s request and aggregate useful information from external resources within several dialogue turns. Conventional task-oriented dialogue systems have a complex pipeline (Rudnicky et al., 1999; Raux et al., 2005; Young et al., 2013) consisting of independently developed and modularly connected components for natural language understanding (NLU) (Mesnil et al., 2015; Liu and Lane, 2016; Hakkani-T¨ur et al., 2016), dialogue state tracking (DST) (Henderson et al., 2014c; ∗ † Work done while the author was an intern at Google. Work done while at Google Research. Mrkˇsi´c et al., 2016), and dialogue policy learning (Gasic and Young, 2014; Shah et al., 2016; Su et al., 2016, 2017). These system components are usually trained independently, and their optimization targets may not fully align with the overall system evaluation criteria (e.g. task success rate and user satisfaction). Moreover, errors made in the upper stream modules of the pipeline propagate to downstream components and get amplified, making it hard to track the source of errors. To address these"
N18-1187,W14-4340,0,0.697106,"y in successfully completing a task. 1 Introduction Task-oriented dialogue systems assist users to complete tasks in specific domains by understanding user’s request and aggregate useful information from external resources within several dialogue turns. Conventional task-oriented dialogue systems have a complex pipeline (Rudnicky et al., 1999; Raux et al., 2005; Young et al., 2013) consisting of independently developed and modularly connected components for natural language understanding (NLU) (Mesnil et al., 2015; Liu and Lane, 2016; Hakkani-T¨ur et al., 2016), dialogue state tracking (DST) (Henderson et al., 2014c; ∗ † Work done while the author was an intern at Google. Work done while at Google Research. Mrkˇsi´c et al., 2016), and dialogue policy learning (Gasic and Young, 2014; Shah et al., 2016; Su et al., 2016, 2017). These system components are usually trained independently, and their optimization targets may not fully align with the overall system evaluation criteria (e.g. task success rate and user satisfaction). Moreover, errors made in the upper stream modules of the pipeline propagate to downstream components and get amplified, making it hard to track the source of errors. To address these"
N18-1187,P16-1094,0,0.0254807,"kov Decision Process (POMDP) (Young et al., 2013). RL can be applied in the POMDP framework to learn dialogue policy online by interacting with users (Gaˇsi´c et al., 2013). The dialogue state and system action space have to be carefully designed in order to make the policy learning tractable (Young et al., 2013), which limits the model’s usage to restricted domains. Recent efforts have been made in designing end-to-end solutions for task-oriented dialogues, inspired by the success of encoder-decoder based neural network models in non-task-oriented conversational systems (Serban et al., 2015; Li et al., 2016). Wen et al. (Wen et al., 2017) designed an end-to-end trainable neural dialogue model with modularly connected system components. This system is a supervised learning model which is evaluated on fixed dialogue corpora. It is unknown how well the model performance generalizes to unseen dialogue state during user interactions. Our system is trained by a combination of supervised and deep RL methods, as it is shown that RL may effectively improve dialogue success rate by exploring a large dialogue action space (Henderson et al., 2008; Li et al., 2017). Bordes and Weston (2017) proposed a taskori"
N18-1187,I17-1074,0,0.134283,"aluation criteria (e.g. task success rate and user satisfaction). Moreover, errors made in the upper stream modules of the pipeline propagate to downstream components and get amplified, making it hard to track the source of errors. To address these limitations with the conventional task-oriented dialogue systems, recent efforts have been made in designing endto-end learning solutions with neural network based methods. Both supervised learning (SL) based (Wen et al., 2017; Bordes and Weston, 2017; Liu and Lane, 2017a) and deep reinforcement learning (RL) based systems (Zhao and Eskenazi, 2016; Li et al., 2017; Peng et al., 2017) have been studied in the literature. Comparing to chit-chat dialogue models that are usually trained offline using single-turn context-response pairs, task-oriented dialogue model involves reasoning and planning over multiple dialogue turns. This makes it especially important for a system to be able to learn from users in an interactive manner. Comparing to SL models, systems trained with RL by receiving feedback during users interactions showed improved model robustness against diverse dialogue scenarios (Williams and Zweig, 2016; Liu and Lane, 2017b). A critical step in"
N18-1187,W16-3603,1,0.805663,"dback after the imitation learning stage further improves the agent’s capability in successfully completing a task. 1 Introduction Task-oriented dialogue systems assist users to complete tasks in specific domains by understanding user’s request and aggregate useful information from external resources within several dialogue turns. Conventional task-oriented dialogue systems have a complex pipeline (Rudnicky et al., 1999; Raux et al., 2005; Young et al., 2013) consisting of independently developed and modularly connected components for natural language understanding (NLU) (Mesnil et al., 2015; Liu and Lane, 2016; Hakkani-T¨ur et al., 2016), dialogue state tracking (DST) (Henderson et al., 2014c; ∗ † Work done while the author was an intern at Google. Work done while at Google Research. Mrkˇsi´c et al., 2016), and dialogue policy learning (Gasic and Young, 2014; Shah et al., 2016; Su et al., 2016, 2017). These system components are usually trained independently, and their optimization targets may not fully align with the overall system evaluation criteria (e.g. task success rate and user satisfaction). Moreover, errors made in the upper stream modules of the pipeline propagate to downstream components"
N18-1187,P17-1163,0,0.0701032,"Missing"
N18-1187,D17-1237,0,0.0724012,"(e.g. task success rate and user satisfaction). Moreover, errors made in the upper stream modules of the pipeline propagate to downstream components and get amplified, making it hard to track the source of errors. To address these limitations with the conventional task-oriented dialogue systems, recent efforts have been made in designing endto-end learning solutions with neural network based methods. Both supervised learning (SL) based (Wen et al., 2017; Bordes and Weston, 2017; Liu and Lane, 2017a) and deep reinforcement learning (RL) based systems (Zhao and Eskenazi, 2016; Li et al., 2017; Peng et al., 2017) have been studied in the literature. Comparing to chit-chat dialogue models that are usually trained offline using single-turn context-response pairs, task-oriented dialogue model involves reasoning and planning over multiple dialogue turns. This makes it especially important for a system to be able to learn from users in an interactive manner. Comparing to SL models, systems trained with RL by receiving feedback during users interactions showed improved model robustness against diverse dialogue scenarios (Williams and Zweig, 2016; Liu and Lane, 2017b). A critical step in learning RL based ta"
N18-1187,N07-2038,0,0.0944419,"is last expression above gives us an unbiased gradient estimator. 4 Experiments 4.1 Datasets We evaluate the proposed method on DSTC2 (Henderson et al., 2014a) dataset in restaurant search domain and an internally collected dialogue corpus1 in movie booking domain. The movie booking dialogue corpus has an average number of 8.4 turns per dialogue. Its training set has 100K dialogues, and the development set and test set each has 10K dialogues. The movie booking dialogue corpus is generated (Shah et al., 2018) using a finite state machine based dialogue agent and an agenda based user simulator (Schatzmann et al., 2007) with natural language utterances rewritten by real users. The user simulator can be configured with different personalities, showing various levels of randomness and cooperativeness. This user simulator is also used to interact with our end-to-end training agent during imitation and reinforcement learning stages. We randomly select a user profile 1 The dataset can be accessed via https: //github.com/google-research-datasets/ simulated-dialogue when conducting each dialogue simulation. During model evaluation, we use an extended set of natural language surface forms over the ones used during t"
N18-1187,N18-3006,1,0.888483,"Missing"
N18-1187,W17-5518,0,0.10187,"Missing"
N18-1187,P16-1230,0,0.0434902,"resources within several dialogue turns. Conventional task-oriented dialogue systems have a complex pipeline (Rudnicky et al., 1999; Raux et al., 2005; Young et al., 2013) consisting of independently developed and modularly connected components for natural language understanding (NLU) (Mesnil et al., 2015; Liu and Lane, 2016; Hakkani-T¨ur et al., 2016), dialogue state tracking (DST) (Henderson et al., 2014c; ∗ † Work done while the author was an intern at Google. Work done while at Google Research. Mrkˇsi´c et al., 2016), and dialogue policy learning (Gasic and Young, 2014; Shah et al., 2016; Su et al., 2016, 2017). These system components are usually trained independently, and their optimization targets may not fully align with the overall system evaluation criteria (e.g. task success rate and user satisfaction). Moreover, errors made in the upper stream modules of the pipeline propagate to downstream components and get amplified, making it hard to track the source of errors. To address these limitations with the conventional task-oriented dialogue systems, recent efforts have been made in designing endto-end learning solutions with neural network based methods. Both supervised learning (SL) bas"
N18-1187,E17-1042,0,0.0800566,"Missing"
N18-1187,P17-1062,0,0.508865,"y important for a system to be able to learn from users in an interactive manner. Comparing to SL models, systems trained with RL by receiving feedback during users interactions showed improved model robustness against diverse dialogue scenarios (Williams and Zweig, 2016; Liu and Lane, 2017b). A critical step in learning RL based taskoriented dialogue models is dialogue policy learning. Training dialogue policy online from scratch typically requires a large number of interactive learning sessions before an agent can reach a satisfactory performance level. Recent works (Henderson et al., 2008; Williams et al., 2017; Liu et al., 2017) explored pre-training the dialogue model using human-human or human-machine dialogue 2060 Proceedings of NAACL-HLT 2018, pages 2060–2069 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics corpora before performing interactive learning with RL to address this concern. A potential drawback with such pre-training approach is that the model may suffer from the mismatch of dialogue state distributions between supervised training and interactive learning stages. While interacting with users, the agent’s response at each turn has a direct in"
N18-1187,W16-3601,0,0.133842,"ith the overall system evaluation criteria (e.g. task success rate and user satisfaction). Moreover, errors made in the upper stream modules of the pipeline propagate to downstream components and get amplified, making it hard to track the source of errors. To address these limitations with the conventional task-oriented dialogue systems, recent efforts have been made in designing endto-end learning solutions with neural network based methods. Both supervised learning (SL) based (Wen et al., 2017; Bordes and Weston, 2017; Liu and Lane, 2017a) and deep reinforcement learning (RL) based systems (Zhao and Eskenazi, 2016; Li et al., 2017; Peng et al., 2017) have been studied in the literature. Comparing to chit-chat dialogue models that are usually trained offline using single-turn context-response pairs, task-oriented dialogue model involves reasoning and planning over multiple dialogue turns. This makes it especially important for a system to be able to learn from users in an interactive manner. Comparing to SL models, systems trained with RL by receiving feedback during users interactions showed improved model robustness against diverse dialogue scenarios (Williams and Zweig, 2016; Liu and Lane, 2017b). A"
N18-3006,D16-1127,0,0.06104,"that (i) the developer must anticipate all ways in which users might interact with the agent, and (ii) since the programmed dialogue flows are not “differentiable”, the agent’s dialogue policy cannot be improved automatically with experience and each improvement requires human intervention to add logic to support a new dialogue flow or revise an existing flow. Recently proposed neural conversational models (Vinyals and Le (2015)) are trained with supervision over a large corpus of dialogues (Serban et al. (2016, 2017); Lowe et al. (2017)) or with reinforcement to optimize a long term reward (Li et al. (2016a,b)). End-to-end neural conversational models for task-oriented dialogues (Wen et al. (2016); Liu and Lane (2017a)) leverage annotated dialogues collected with an expert to embed the expert’s dialogue policy for a given task in End-to-end neural models show great promise towards building conversational agents that are trained from data and on-line experience using supervised and reinforcement learning. However, these models require a large corpus of dialogues to learn effectively. For goal-oriented dialogues, such datasets are expensive to collect and annotate, since each task involves a sepa"
N18-3006,W17-5526,0,0.184781,"Missing"
N18-3006,W16-3613,0,0.138296,"Missing"
N18-3006,D17-2014,0,0.0369977,"be replaced by machine learned generative models if available. Task Completion Platform (TCP) (Crook et al. (2016)) introduced a task configuration language for building goal-oriented dialogue interactions. The state update and policy modules of TCP could be used to implement agents that generate outlines for more complex tasks. The crowd-sourcing step uses human intelligence to gather diverse natural language utterances. Comparisons with the DSTC2 dataset show that this approach can create high-quality fully annotated datasets for training conversational agents in arbitrary domains. ParlAI (Miller et al. (2017)), a dialogue research software platform, provides easy integration with crowd sourcing for data collection and evaluation. However, the crowd sourcing tasks are open-ended and may result in lower quality dialogues as described in Section 4. In M2M, crowd workers are asked to paraphrase given utterances instead of writing new ones, which is at a suitable difficulty level for crowd workers. Finally, training a neural conversational model over the M2M generated dataset encodes the programmed policy in a differentiable neural model which can be deployed to interact with users. This model is amena"
N18-3006,N07-2038,0,0.152328,"ions consist of dialogue frames that encode the semantics of the turn through a dialogue act and a slot-value map (Table 1). For example “inform(date=tomorrow, time=evening)” is a dialogue frame that informs the system of the user’s constraints for the date and time slots. We use the Cambridge dialogue act schema (Henderson et al. (2013)) as the list of possible dialogue 43 acts. The process continues until either the user’s goals are achieved and the user exits the dialogue with a “bye()” act, or a maximum number of turns are reached. In our experiments we use an agenda-based user simulator (Schatzmann et al. (2007)) parameterized by a user goal and a user profile. The programmed system agent is modeled as a handcrafted finite state machine (Hopcroft et al. (2006)) which encodes a set of taskindependent rules for constructing system turns, with each turn consisting of a response frame which responds to the user’s previous turn, and an initiate frame which drives the dialogue forward through a predetermined sequence of subdialogues. For database querying applications, these sub-dialogues are: gather user preferences, query a database via an API, offer matching entities to the user, allow user to modify pr"
N18-3006,W17-5518,0,0.0624267,"Missing"
N18-3006,P16-1230,0,0.0449031,"Missing"
N18-3006,P17-4013,0,0.0196211,"ents from a programmed system agent, we trained an end-to-end conversa4 https://github.com/google-research-datasets/simulateddialogue 46 6 Related work and discussion We presented an approach for rapidly bootstrapping goal-oriented conversational agents for arbitrary database querying tasks, by combining dialogue self-play, crowd-sourcing and on-line reinforcement learning. The dialogue self-play step uses a taskindependent user simulator and programmed system agent seeded with a task-specific schema, which provides the developer with full control over the generated dialogue outlines. PyDial (Ultes et al. (2017)) is an extensible open-source toolkit which provides domain-independent implementations of dialogue system modules, which could be extended by adding dialogue self-play functionality. We described an FSM system agent for handling any transactional or form-filling task. For more complex tasks, the developer can extend the user simulator and system agents by adding their own rules. These components could also be replaced by machine learned generative models if available. Task Completion Platform (TCP) (Crook et al. (2016)) introduced a task configuration language for building goal-oriented dial"
N18-3006,P15-1129,0,0.018062,"API state for each turn. These annotated dialogues are sufficient for training end-toend models using supervision (Wen et al. (2016)). Dialogue self-play ensures sufficient coverage of flows encoded in the programmed system agent in the crowd sourced dataset. Consequently, the trained agent reads natural language user utterances and emits system turns by encoding the FSM policy of system agent in a differentiable neural model. Template utterances. Once a full dialogue has been sampled, a template utterance generator maps each annotation to a template utterance using a domain-general grammar (Wang et al. (2015)) parameterized with the task schema. For example, “inform(date=tomorrow, time=evening)” would map to a template “($slot is $value) (and ($slot is $value))*”, which is grounded as “Date is tomorrow and time is evening.” The developer can also provide a list of templates to use for some or all of the dialogue frames if they want more control over the language used in the utterances. Template utterances are an important bridge between the annotation and the corresponding natural language utterance, as they present the semantic information of a turn annotation in a format understandable by crowd"
N18-4010,N18-1187,1,0.541493,"ning method and let the dialog agent to learn interactively from user teaching. After obtaining a supervised training model, we deploy the agent to let it interact with users using its learned dialog policy. The agent may make errors during user interactions. We then ask expert users to correct the agent’s mistakes and demonstrate the right actions for the agent to take (Ross et al., 2011). In this manner, we collect additional dialog samples that are guided by the agent’s own policy. Learning on these samples directly addresses the limitation of the currently learned model. With experiments (Liu et al., 2018) in a movie booking domain, we show that the agent can efficiently learn from the expert demonstrations and improve dialog task success rate with the proposed imitation dialog learning method. 5.2 6 Learning from Human Feedback In this section, we describe our proposed methods in learning task-oriented dialog model interactively from human feedback with reinforcement learning (RL). 6.1 End-to-End Dialog Learning with RL After the supervised and imitation training stage, we propose to further optimize the dialog model with RL by letting the agent to interact with users and collecting simple for"
N18-4010,P17-1045,0,0.0208201,"ies retrieved from external resources (such as a database or a knowledge base). The dialog act emitted by the dialog policy module serves as the input to the NLG, through which a natural language format system response is generated. In this thesis work, we propose end-to-end solutions that focus on three core components of task-oriented dialog system: SLU, DST, and dialog policy. 2.2 Williams et al. (Williams et al., 2017) proposed a hybrid code network for task-oriented dialog that can be trained with supervised and reinforcement learning (RL). Li et al. (Li et al., 2017) and Dhingra et al. (Dhingra et al., 2017) also proposed end-to-end task-oriented dialog models that can be trained with hybrid supervised learning and RL. These systems apply RL directly on supervised pre-training models, without discussing the potential issue with dialog state distribution mismatch between supervised training and interactive learning. Moreover, current end-to-end dialog models are mostly trained and evaluated against user simulators. Ideally, RL based dialog learning should be performed with human users by collecting real user feedback. In interactive learning with human users, online learning efficiency becomes a c"
N18-4010,E17-2075,0,0.0239871,"ialog model training. Moreover, the system is trained with supervised learning on fixed dialog corpora, and thus may not generalize well to unseen dialog states when interacting with users. Related Work Bordes and Weston (Bordes and Weston, 2017) proposed a task-oriented dialog model from a machine reading and reasoning approach. They used an RNN to encode the dialog state and applied end-to-end memory networks to learn it. In the same line of research, people explored using query-regression networks (Seo et al., 2016), gated memory networks (Liu and Perez, 2017), and copy-augmented networks (Eric and Manning, 2017) to learn the dialog state RNN. Similar to (Wen et al., 2017), these systems are trained on fixed sets of simulated and human-machine dialog corpora, and thus are not capable to learn interactively from users. The knowledge base information is pulled offline based on existing dialog corpus. It is unknown whether the reasoning capability achieved in offline model training can generalize well to online user interactions. Task-Oriented Dialog Systems Domain / Intents / Slot Values ASR Hypothesis Input Speech Automatic Speech Recognition (ASR) Spoken Language Understanding (SLU) Dialog State Track"
N18-4010,P17-1163,0,0.0308732,"Missing"
N18-4010,W14-4340,0,0.0365154,"Task-Oriented Dialogs Bing Liu, Ian Lane Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA liubing@cmu.edu, lane@cmu.edu Abstract formation from external resources and reasoning over multiple dialog turns. This thesis work focuses on task-oriented dialog systems. Conventional task-oriented dialog systems have a complex pipeline (Raux et al., 2005; Young et al., 2013) consisting of independently developed and modularly connected components for spoken language understanding (SLU) (Sarikaya et al., 2014; Mesnil et al., 2015; Chen et al., 2016), dialog state tracking (DST) (Henderson et al., 2014; Mrkˇsi´c et al., 2016; Lee and Stent, 2016), and dialog policy learning (Gasic and Young, 2014; Su et al., 2016). Such pipeline system design has a number of limitations. Firstly, credit assignment in such pipeline systems can be challenging, as errors made in upper stream modules may propagate and be amplified in downstream components. Moreover, each component in the pipeline is ideally re-trained as preceding components are updated, so that we have inputs similar to the training examples at run-time. This domino effect causes several issues in practice. We address the limitations of pipeli"
N18-4010,W16-3602,0,0.0742672,"ie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA liubing@cmu.edu, lane@cmu.edu Abstract formation from external resources and reasoning over multiple dialog turns. This thesis work focuses on task-oriented dialog systems. Conventional task-oriented dialog systems have a complex pipeline (Raux et al., 2005; Young et al., 2013) consisting of independently developed and modularly connected components for spoken language understanding (SLU) (Sarikaya et al., 2014; Mesnil et al., 2015; Chen et al., 2016), dialog state tracking (DST) (Henderson et al., 2014; Mrkˇsi´c et al., 2016; Lee and Stent, 2016), and dialog policy learning (Gasic and Young, 2014; Su et al., 2016). Such pipeline system design has a number of limitations. Firstly, credit assignment in such pipeline systems can be challenging, as errors made in upper stream modules may propagate and be amplified in downstream components. Moreover, each component in the pipeline is ideally re-trained as preceding components are updated, so that we have inputs similar to the training examples at run-time. This domino effect causes several issues in practice. We address the limitations of pipeline dialog systems and propose end-to-end lear"
N18-4010,I17-1074,0,0.0270414,"ialog act based on the facts or entities retrieved from external resources (such as a database or a knowledge base). The dialog act emitted by the dialog policy module serves as the input to the NLG, through which a natural language format system response is generated. In this thesis work, we propose end-to-end solutions that focus on three core components of task-oriented dialog system: SLU, DST, and dialog policy. 2.2 Williams et al. (Williams et al., 2017) proposed a hybrid code network for task-oriented dialog that can be trained with supervised and reinforcement learning (RL). Li et al. (Li et al., 2017) and Dhingra et al. (Dhingra et al., 2017) also proposed end-to-end task-oriented dialog models that can be trained with hybrid supervised learning and RL. These systems apply RL directly on supervised pre-training models, without discussing the potential issue with dialog state distribution mismatch between supervised training and interactive learning. Moreover, current end-to-end dialog models are mostly trained and evaluated against user simulators. Ideally, RL based dialog learning should be performed with human users by collecting real user feedback. In interactive learning with human use"
N18-4010,W16-3603,1,0.764899,"terance-level LSTM, 69 Evaluator Agent State Dialog Agent Predicted Reward Agent Predicted Observation Inferred User State Action Proposal Modeled User (in agent’s mind) Figure 3: End-to-end task-oriented dialog system architecture state output at each time step is used for slot label prediction for each word in the utterance. A weighted average of these LSTM state outputs is further used as the representation of the utterance for user intent prediction. The model objective function is a linear interpolation of the crossentropy losses for intent and slot label predictions. Experiment results (Liu and Lane, 2016a,b) on ATIS SLU corpus show that the proposed joint training model achieves state-of-the-art intent detection accuracy and slot filling F1 scores. The joint training model also outperforms the independent training models on both tasks. a delexicalised system response based on the current dialog state. This can be seen as learning a supervised dialog policy by following the expert actions via behavior cloning. In supervised model training, we optimize the parameter set θ to minimize the cross-entropy losses for dialog state tracking and system action prediction: 4.2 where λs are the linear int"
N18-4010,P16-1230,0,0.0195725,"g@cmu.edu, lane@cmu.edu Abstract formation from external resources and reasoning over multiple dialog turns. This thesis work focuses on task-oriented dialog systems. Conventional task-oriented dialog systems have a complex pipeline (Raux et al., 2005; Young et al., 2013) consisting of independently developed and modularly connected components for spoken language understanding (SLU) (Sarikaya et al., 2014; Mesnil et al., 2015; Chen et al., 2016), dialog state tracking (DST) (Henderson et al., 2014; Mrkˇsi´c et al., 2016; Lee and Stent, 2016), and dialog policy learning (Gasic and Young, 2014; Su et al., 2016). Such pipeline system design has a number of limitations. Firstly, credit assignment in such pipeline systems can be challenging, as errors made in upper stream modules may propagate and be amplified in downstream components. Moreover, each component in the pipeline is ideally re-trained as preceding components are updated, so that we have inputs similar to the training examples at run-time. This domino effect causes several issues in practice. We address the limitations of pipeline dialog systems and propose end-to-end learning solutions. The proposed model is capable of robustly tracking di"
N18-4010,E17-1042,0,0.0774392,"Missing"
N18-4010,P17-1062,0,0.0744557,"then formatted as the input to DST, which maintains the current state of the dialog. Outputs of DST are passed to the dialog policy module, which produces a dialog act based on the facts or entities retrieved from external resources (such as a database or a knowledge base). The dialog act emitted by the dialog policy module serves as the input to the NLG, through which a natural language format system response is generated. In this thesis work, we propose end-to-end solutions that focus on three core components of task-oriented dialog system: SLU, DST, and dialog policy. 2.2 Williams et al. (Williams et al., 2017) proposed a hybrid code network for task-oriented dialog that can be trained with supervised and reinforcement learning (RL). Li et al. (Li et al., 2017) and Dhingra et al. (Dhingra et al., 2017) also proposed end-to-end task-oriented dialog models that can be trained with hybrid supervised learning and RL. These systems apply RL directly on supervised pre-training models, without discussing the potential issue with dialog state distribution mismatch between supervised training and interactive learning. Moreover, current end-to-end dialog models are mostly trained and evaluated against user si"
N19-1242,P17-1171,0,0.105743,"sk4 2164 P./805 N./633 Ne. 728 P./196 N./196 Ne. Table 3: Summary of datasets on aspect extraction and aspect sentiment classification. S: number of sentences; A: number of aspects; P., N., and Ne.: number of positive, negative and neutral polarities. domain, which roughly have one pass over the preprocessed data on the respective domain. 5.4 Compared Methods As BERT outperforms existing open source MRC baselines by a large margin, we do not intend to exhaust existing implementations but focus on variants of BERT introduced in this paper. DrQA is a baseline from the document reader12 of DrQA (Chen et al., 2017). We adopt this baseline because of its simple implementation for reproducibility. We run the document reader with random initialization and train it directly on ReviewRC. We use all default hyper-parameter settings for this baseline except the number of epochs, which is set as 60 for better convergence. DrQA+MRC is derived from the above baseline with official pre-trained weights on SQuAD. We fine-tune document reader with ReviewRC. We expand the vocabulary of the embedding layer from the pre-trained model on ReviewRC since reviews may have words that are rare in Wikipedia and keep other hype"
N19-1242,P18-1031,0,0.175129,"al., 2016; He et al., 2018) and many of these models use handcrafted features, lexicons, and complicated neural network architectures to remedy the insufficient training examples from both tasks. Although these approaches may achieve better performances by manually injecting human knowledge into the model, human baby-sat models may not be intelligent enough6 and automated representation learning from review corpora is always preferred (Xu et al., 2018a; He et al., 2018). We push forward this trend with the recent advance in pre-trained language models from deep learning (Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2018; Radford et al., 2018a,b). Although it is practical to train domain word embeddings from scratch on large-scale review corpora (Xu et al., 2018a), it is impractical to train language models from scratch with limited computational resources. As such, we show that it is practical to adapt language models pre-trained from formal texts to domain reviews. 3 BERT and Review-based Tasks In this section, we briefly review BERT and derive its fine-tuning formulation on three (3) reviewbased end tasks. 3.1 BERT BERT is one of the key innovations in the recent progress of contextual"
N19-1242,P17-1147,0,0.0222935,"ories (MCTest (Richardson et al., 2013), 3 The end tasks from the original BERT paper typically use tens of thousands of examples to ensure that the system is task-aware. 4 Due to limited computation resources, it is impractical for us to pre-train BERT directly on reviews from scratch (Devlin et al., 2018). 5 To simplify the writing, we refer MRC as a generalpurpose RC task on formal text (non-review) and RRC as an end-task specifically focused on reviews. 2325 CBT (Hill et al., 2015), NarrativeQA (Koˇcisk`y et al., 2018)), and general Web documents (MS MARCO (Nguyen et al., 2016), TriviaQA (Joshi et al., 2017), SearchQA (Dunn et al., 2017) ). Also, CoQA (Reddy et al., 2018) is built from multiple sources, such as Wikipedia, Reddit, News, Mid/High School Exams, Literature, etc. To the best of our knowledge, MRC has not been used on reviews, which are primarily subjective. As such, we created a review-based MRC dataset called ReviewRC. Answers from ReviewRC are extractive (similar to SQuAD (Rajpurkar et al., 2016, 2018)) rather than abstractive (or generative) (such as in MS MARCO (Nguyen et al., 2016) and CoQA (Reddy et al., 2018)). This is crucial because online businesses are typically cost-sensit"
N19-1242,P15-1026,0,0.025855,"ommunity QA (CQA) is widely adopted by online businesses (McAuley and Yang, 2016) to help users. However, since it solely relies on humans to give answers, it often takes a long time to get a question answered or even not answered at all as we discussed in the introduction. Although there exist researches that align reviews to questions as an information retrieval task (McAuley and Yang, 2016; Yu and Lam, 2018), giving a whole review to the user to read is time-consuming and not suitable for customer service settings that require interactive responses. Knowledge bases (KBs) (such as Freebase (Dong et al., 2015; Xu et al., 2016; Yao and Van Durme, 2014) or DBpedia (Lopez et al., 2010; Unger et al., 2012)) have been used for question answering (Yu and Lam, 2018). However, the ever-changing nature of online businesses, where new products and services appear constantly, makes it prohibitive to build a highquality KB to cover all new products and services. Reviews also serve as a rich resource for sentiment analysis (Pang et al., 2002; Hu and Liu, 2004; Liu, 2012, 2015). Although documentlevel (review) sentiment classification may be considered as a solved problem (given ratings are largely available),"
N19-1242,Q18-1023,0,0.0411398,"Missing"
N19-1242,P18-2092,0,0.0552151,"BSA aims to turn unstructured reviews into structured fine-grained aspects (such as the “battery” of a laptop) and their associated opinions (e.g., “good battery” is positive about the aspect battery). Two important tasks in ABSA are aspect extraction (AE) and aspect sentiment classification (ASC) (Hu and Liu, 2004), where the former aims to extract aspects (e.g., “battery”) and the latter targets to identify the polarity for a given aspect (e.g., positive for battery). Recently, supervised deep learning models dominate both tasks (Wang et al., 2016, 2017; Xu et al., 2018a; Tang et al., 2016; He et al., 2018) and many of these models use handcrafted features, lexicons, and complicated neural network architectures to remedy the insufficient training examples from both tasks. Although these approaches may achieve better performances by manually injecting human knowledge into the model, human baby-sat models may not be intelligent enough6 and automated representation learning from review corpora is always preferred (Xu et al., 2018a; He et al., 2018). We push forward this trend with the recent advance in pre-trained language models from deep learning (Peters et al., 2018; Howard and Ruder, 2018; Devl"
N19-1242,P16-1145,0,0.019091,"oposed method can also benefit ABSA tasks such as aspect extraction (AE) and aspect sentiment classification (ASC). The main contributions of this paper are as follows. (1) It proposes the new problem of review reading comprehension (RRC). (2) To solve this new problem, an annotated dataset for RRC is created. (3) It proposes a general-purpose posttraining approach to improve RRC, AE, and ASC. Experimental results demonstrate that the proposed approach is effective. 2 Related Works Many datasets have been created for MRC from formally written and objective texts, e.g., Wikipedia (WikiReading (Hewlett et al., 2016), SQuAD (Rajpurkar et al., 2016, 2018), WikiHop (Welbl et al., 2018), DRCD (Shao et al., 2018), QuAC (Choi et al., 2018), HotpotQA (Yang et al., 2018)) news and other articles (CNN/Daily Mail (Hermann et al., 2015), NewsQA (Trischler et al., 2016), RACE (Lai et al., 2017)), fictional stories (MCTest (Richardson et al., 2013), 3 The end tasks from the original BERT paper typically use tens of thousands of examples to ensure that the system is task-aware. 4 Due to limited computation resources, it is impractical for us to pre-train BERT directly on reviews from scratch (Devlin et al., 2018). 5 T"
N19-1242,W02-1011,0,0.0328298,"le review to the user to read is time-consuming and not suitable for customer service settings that require interactive responses. Knowledge bases (KBs) (such as Freebase (Dong et al., 2015; Xu et al., 2016; Yao and Van Durme, 2014) or DBpedia (Lopez et al., 2010; Unger et al., 2012)) have been used for question answering (Yu and Lam, 2018). However, the ever-changing nature of online businesses, where new products and services appear constantly, makes it prohibitive to build a highquality KB to cover all new products and services. Reviews also serve as a rich resource for sentiment analysis (Pang et al., 2002; Hu and Liu, 2004; Liu, 2012, 2015). Although documentlevel (review) sentiment classification may be considered as a solved problem (given ratings are largely available), aspect-based sentiment analysis (ABSA) is still an open challenge, where alleviating the cost of human annotation is also a major issue. ABSA aims to turn unstructured reviews into structured fine-grained aspects (such as the “battery” of a laptop) and their associated opinions (e.g., “good battery” is positive about the aspect battery). Two important tasks in ABSA are aspect extraction (AE) and aspect sentiment classificati"
N19-1242,D14-1162,0,0.0903181,"from scratch with limited computational resources. As such, we show that it is practical to adapt language models pre-trained from formal texts to domain reviews. 3 BERT and Review-based Tasks In this section, we briefly review BERT and derive its fine-tuning formulation on three (3) reviewbased end tasks. 3.1 BERT BERT is one of the key innovations in the recent progress of contextualized representation learning (Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018a; Devlin et al., 2018). The idea behind the progress is that even though the word embedding (Mikolov et al., 2013; Pennington et al., 2014) layer (in a typical neural network for NLP) is trained from large-scale corpora, training a wide variety of neural architectures that encode contextual representations only from the limited supervised data on end tasks is insufficient. Unlike ELMo (Peters et al., 2018) and ULMFiT 6 http://www.incompleteideas.net/ IncIdeas/BitterLesson.html 2326 3.2 Figure 1: Overview of BERT settings for review reading comprehension (RRC), aspect extraction (AE) and aspect sentiment classification (ASC). (Howard and Ruder, 2018) that are intended to provide additional features for a particular architecture th"
N19-1242,N18-1202,0,0.0556464,"t al., 2018a; Tang et al., 2016; He et al., 2018) and many of these models use handcrafted features, lexicons, and complicated neural network architectures to remedy the insufficient training examples from both tasks. Although these approaches may achieve better performances by manually injecting human knowledge into the model, human baby-sat models may not be intelligent enough6 and automated representation learning from review corpora is always preferred (Xu et al., 2018a; He et al., 2018). We push forward this trend with the recent advance in pre-trained language models from deep learning (Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2018; Radford et al., 2018a,b). Although it is practical to train domain word embeddings from scratch on large-scale review corpora (Xu et al., 2018a), it is impractical to train language models from scratch with limited computational resources. As such, we show that it is practical to adapt language models pre-trained from formal texts to domain reviews. 3 BERT and Review-based Tasks In this section, we briefly review BERT and derive its fine-tuning formulation on three (3) reviewbased end tasks. 3.1 BERT BERT is one of the key innovations in the recen"
N19-1242,P18-2124,0,0.0472455,"Missing"
N19-1242,D16-1264,0,0.130577,"nswered, the answers are delayed, which is not suitable for interactive QA. In this paper, we explore the potential of using product reviews as a large source of user experiences that can be exploited to obtain answers to user questions. Although there are existing studies that have used information retrieval (IR) techniques (McAuley and Yang, 2016; Yu and Lam, 2018) to find a whole review as the response to a user question, giving the whole review to the user is undesirable as it is quite time-consuming for the user to read it. Inspired by the success of Machine Reading Comphrenesions (MRC) (Rajpurkar et al., 2016, 2018), we propose a novel task called Review Reading Comprehension (RRC) as following. Problem Definition: Given a question q = (q1 , . . . , qm ) from a customer (or user) about a product and a review d = (d1 , . . . , dn ) for that product containing the information to answer q, find a sequence of tokens (a text span) a = (ds , . . . , de ) in d that answers q correctly, where 1 ≤ s ≤ n, 1 ≤ e ≤ n, and s ≤ e. A sample laptop review is shown in Table 1. We can see that customers may not only ask factoid 2324 Proceedings of NAACL-HLT 2019, pages 2324–2335 c Minneapolis, Minnesota, June 2 - J"
N19-1242,D13-1020,0,0.0226505,"oses a general-purpose posttraining approach to improve RRC, AE, and ASC. Experimental results demonstrate that the proposed approach is effective. 2 Related Works Many datasets have been created for MRC from formally written and objective texts, e.g., Wikipedia (WikiReading (Hewlett et al., 2016), SQuAD (Rajpurkar et al., 2016, 2018), WikiHop (Welbl et al., 2018), DRCD (Shao et al., 2018), QuAC (Choi et al., 2018), HotpotQA (Yang et al., 2018)) news and other articles (CNN/Daily Mail (Hermann et al., 2015), NewsQA (Trischler et al., 2016), RACE (Lai et al., 2017)), fictional stories (MCTest (Richardson et al., 2013), 3 The end tasks from the original BERT paper typically use tens of thousands of examples to ensure that the system is task-aware. 4 Due to limited computation resources, it is impractical for us to pre-train BERT directly on reviews from scratch (Devlin et al., 2018). 5 To simplify the writing, we refer MRC as a generalpurpose RC task on formal text (non-review) and RRC as an end-task specifically focused on reviews. 2325 CBT (Hill et al., 2015), NarrativeQA (Koˇcisk`y et al., 2018)), and general Web documents (MS MARCO (Nguyen et al., 2016), TriviaQA (Joshi et al., 2017), SearchQA (Dunn et"
N19-1242,L18-1431,0,0.0243037,"ssification (ASC). The main contributions of this paper are as follows. (1) It proposes the new problem of review reading comprehension (RRC). (2) To solve this new problem, an annotated dataset for RRC is created. (3) It proposes a general-purpose posttraining approach to improve RRC, AE, and ASC. Experimental results demonstrate that the proposed approach is effective. 2 Related Works Many datasets have been created for MRC from formally written and objective texts, e.g., Wikipedia (WikiReading (Hewlett et al., 2016), SQuAD (Rajpurkar et al., 2016, 2018), WikiHop (Welbl et al., 2018), DRCD (Shao et al., 2018), QuAC (Choi et al., 2018), HotpotQA (Yang et al., 2018)) news and other articles (CNN/Daily Mail (Hermann et al., 2015), NewsQA (Trischler et al., 2016), RACE (Lai et al., 2017)), fictional stories (MCTest (Richardson et al., 2013), 3 The end tasks from the original BERT paper typically use tens of thousands of examples to ensure that the system is task-aware. 4 Due to limited computation resources, it is impractical for us to pre-train BERT directly on reviews from scratch (Devlin et al., 2018). 5 To simplify the writing, we refer MRC as a generalpurpose RC task on formal text (non-review) a"
N19-1242,D16-1021,0,0.447079,"so a major issue. ABSA aims to turn unstructured reviews into structured fine-grained aspects (such as the “battery” of a laptop) and their associated opinions (e.g., “good battery” is positive about the aspect battery). Two important tasks in ABSA are aspect extraction (AE) and aspect sentiment classification (ASC) (Hu and Liu, 2004), where the former aims to extract aspects (e.g., “battery”) and the latter targets to identify the polarity for a given aspect (e.g., positive for battery). Recently, supervised deep learning models dominate both tasks (Wang et al., 2016, 2017; Xu et al., 2018a; Tang et al., 2016; He et al., 2018) and many of these models use handcrafted features, lexicons, and complicated neural network architectures to remedy the insufficient training examples from both tasks. Although these approaches may achieve better performances by manually injecting human knowledge into the model, human baby-sat models may not be intelligent enough6 and automated representation learning from review corpora is always preferred (Xu et al., 2018a; He et al., 2018). We push forward this trend with the recent advance in pre-trained language models from deep learning (Peters et al., 2018; Howard and"
N19-1242,D16-1059,0,0.140067,"eviating the cost of human annotation is also a major issue. ABSA aims to turn unstructured reviews into structured fine-grained aspects (such as the “battery” of a laptop) and their associated opinions (e.g., “good battery” is positive about the aspect battery). Two important tasks in ABSA are aspect extraction (AE) and aspect sentiment classification (ASC) (Hu and Liu, 2004), where the former aims to extract aspects (e.g., “battery”) and the latter targets to identify the polarity for a given aspect (e.g., positive for battery). Recently, supervised deep learning models dominate both tasks (Wang et al., 2016, 2017; Xu et al., 2018a; Tang et al., 2016; He et al., 2018) and many of these models use handcrafted features, lexicons, and complicated neural network architectures to remedy the insufficient training examples from both tasks. Although these approaches may achieve better performances by manually injecting human knowledge into the model, human baby-sat models may not be intelligent enough6 and automated representation learning from review corpora is always preferred (Xu et al., 2018a; He et al., 2018). We push forward this trend with the recent advance in pre-trained language models from dee"
N19-1242,Q18-1021,0,0.0189881,"E) and aspect sentiment classification (ASC). The main contributions of this paper are as follows. (1) It proposes the new problem of review reading comprehension (RRC). (2) To solve this new problem, an annotated dataset for RRC is created. (3) It proposes a general-purpose posttraining approach to improve RRC, AE, and ASC. Experimental results demonstrate that the proposed approach is effective. 2 Related Works Many datasets have been created for MRC from formally written and objective texts, e.g., Wikipedia (WikiReading (Hewlett et al., 2016), SQuAD (Rajpurkar et al., 2016, 2018), WikiHop (Welbl et al., 2018), DRCD (Shao et al., 2018), QuAC (Choi et al., 2018), HotpotQA (Yang et al., 2018)) news and other articles (CNN/Daily Mail (Hermann et al., 2015), NewsQA (Trischler et al., 2016), RACE (Lai et al., 2017)), fictional stories (MCTest (Richardson et al., 2013), 3 The end tasks from the original BERT paper typically use tens of thousands of examples to ensure that the system is task-aware. 4 Due to limited computation resources, it is impractical for us to pre-train BERT directly on reviews from scratch (Devlin et al., 2018). 5 To simplify the writing, we refer MRC as a generalpurpose RC task on"
N19-1242,P18-2094,1,0.553453,"answer spans from a review. questions such as the specs about some aspects of the laptop as in the first and second questions but also subjective or opinion questions about some aspects (capacity of the hard drive), as in the third question. RRC poses some domain challenges compared to the traditional MRC on Wikipedia, such as the need for rich product knowledge, informal text, and fine-grained opinions (there is almost no subjective content in Wikipedia articles). Research also shows that yes/no questions are very frequent for products with complicated specifications (McAuley and Yang, 2016; Xu et al., 2018b). To the best of our knowledge, no existing work has been done in RRC. This work first builds an RRC dataset called ReviewRC, using reviews from SemEval 2016 Task 52 , which is a popular dataset for aspect-based sentiment analysis (ABSA) (Hu and Liu, 2004) in the domains of laptop and restaurant. We detail ReviewRC in Sec. 5. Given the wide spectrum of domains (types of products or services) in online businesses and the prohibitive cost of annotation, ReviewRC can only be considered to have a limited number of annotated examples for supervised training, which still leaves the domain challeng"
N19-1242,P16-1220,0,0.0891856,"at such systems have limited capability to answer user questions about products (or 1 The datasets and code are available at https://www. cs.uic.edu/˜hxu/. services), which are vital for customer decision making. As such, an intelligent agent that can automatically answer customers’ questions is very important for the success of online businesses. Given the ever-changing environment of products and services, it is very hard, if not impossible, to pre-compile an up-to-date and reliable knowledge base to cover a wide assortment of questions that customers may ask, such as in factoidbased KB-QA (Xu et al., 2016; Fader et al., 2014; Kwok et al., 2001; Yin et al., 2015). As a compromise, many online businesses leverage community question-answering (CQA) (McAuley and Yang, 2016) to crowdsource answers from existing customers. However, the problem with this approach is that many questions are not answered, and if they are answered, the answers are delayed, which is not suitable for interactive QA. In this paper, we explore the potential of using product reviews as a large source of user experiences that can be exploited to obtain answers to user questions. Although there are existing studies that have u"
N19-1242,D18-1259,0,0.0163878,"are as follows. (1) It proposes the new problem of review reading comprehension (RRC). (2) To solve this new problem, an annotated dataset for RRC is created. (3) It proposes a general-purpose posttraining approach to improve RRC, AE, and ASC. Experimental results demonstrate that the proposed approach is effective. 2 Related Works Many datasets have been created for MRC from formally written and objective texts, e.g., Wikipedia (WikiReading (Hewlett et al., 2016), SQuAD (Rajpurkar et al., 2016, 2018), WikiHop (Welbl et al., 2018), DRCD (Shao et al., 2018), QuAC (Choi et al., 2018), HotpotQA (Yang et al., 2018)) news and other articles (CNN/Daily Mail (Hermann et al., 2015), NewsQA (Trischler et al., 2016), RACE (Lai et al., 2017)), fictional stories (MCTest (Richardson et al., 2013), 3 The end tasks from the original BERT paper typically use tens of thousands of examples to ensure that the system is task-aware. 4 Due to limited computation resources, it is impractical for us to pre-train BERT directly on reviews from scratch (Devlin et al., 2018). 5 To simplify the writing, we refer MRC as a generalpurpose RC task on formal text (non-review) and RRC as an end-task specifically focused on reviews. 2"
N19-1242,P14-1090,0,0.0433038,"Missing"
P10-2066,N09-1003,0,0.032981,"res, it is not applicable to general free texts. 2 Three Different Techniques 2.1 Distributional Similarity Distributional similarity is a classic technique for the entity set expansion problem. It is based on the hypothesis that words with similar meanings tend to appear in similar contexts (Harris, 1985). As such, a method based on distributional similarity typically fetches the surrounding contexts for each term (i.e. both seeds and candidates) and represents them as vectors by using TF-IDF or PMI (Pointwise Mutual Information) values (Lin, 1998; Gorman and Curran, 2006; Paşca et al. 2006; Agirre et al. 2009; Pantel et al. 2009). Similarity measures such as Cosine, Jaccard, Dice, etc, can then be employed to compute the similarities between each candidate vector and the seeds centroid vector (one centroid vector for all seeds). Lee (1998) surveyed and discussed various distribution similarity measures. The positive examples in SP are called “spies”. Then, a NB classifier is built using the set P– SP as positive and the set U∪SP as negative (line 3, 4, and 5). The NB classifier is applied to classify each u ∈ U∪SP, i.e., to assign a probabilistic class label p(+|u) (+ means positive). The probabil"
P10-2066,J95-4004,0,0.0426369,"s feature values of vectors, and Cosine and Jaccard as similarity measures. Due to space limitations, we only show the results of the PMI and Cosine combination as it performed the best. This combination was also used in (Pantel et al., 2009). 5.1 Corpora and Evaluation Metrics We used 10 diverse corpora to evaluate the techniques. They were obtained from a commercial company. The data were crawled and extracted from multiple online message boards and blogs discussing different products and services. We split each message into sentences, and the sentences were POS-tagged using Brill’s tagger (Brill, 1995). The tagged sentences were used to extract candidate entities and their contexts. Table 1 shows the domains and the number of sentences in each corpus, as well as the three seed entities used in our experiments for each corpus. The three seeds for each corpus were randomly selected from a set of common entities in the application domain. Table 1. Descriptions of the 10 corpora Domains # Sentences Bank Blu-ray Car Drug Insurance LCD Mattress Phone Stove Vacuum 17394 7093 2095 1504 12419 1733 13191 14884 25060 13491 Seed Entities Citi, Chase, Wesabe S300, Sony, Samsung Honda, A3, Toyota Enbrel,"
P10-2066,P04-1056,0,0.0378177,"Missing"
P10-2066,C02-1025,0,0.0575424,"Missing"
P10-2066,N06-1010,0,0.0612643,"Missing"
P10-2066,P99-1004,0,0.161281,"Missing"
P10-2066,P98-2127,0,0.0715985,"Cohen, 2008). However, as it relies on Web page structures, it is not applicable to general free texts. 2 Three Different Techniques 2.1 Distributional Similarity Distributional similarity is a classic technique for the entity set expansion problem. It is based on the hypothesis that words with similar meanings tend to appear in similar contexts (Harris, 1985). As such, a method based on distributional similarity typically fetches the surrounding contexts for each term (i.e. both seeds and candidates) and represents them as vectors by using TF-IDF or PMI (Pointwise Mutual Information) values (Lin, 1998; Gorman and Curran, 2006; Paşca et al. 2006; Agirre et al. 2009; Pantel et al. 2009). Similarity measures such as Cosine, Jaccard, Dice, etc, can then be employed to compute the similarities between each candidate vector and the seeds centroid vector (one centroid vector for all seeds). Lee (1998) surveyed and discussed various distribution similarity measures. The positive examples in SP are called “spies”. Then, a NB classifier is built using the set P– SP as positive and the set U∪SP as negative (line 3, 4, and 5). The NB classifier is applied to classify each u ∈ U∪SP, i.e., to assign a p"
P10-2066,P06-1102,0,0.012254,"on Web page structures, it is not applicable to general free texts. 2 Three Different Techniques 2.1 Distributional Similarity Distributional similarity is a classic technique for the entity set expansion problem. It is based on the hypothesis that words with similar meanings tend to appear in similar contexts (Harris, 1985). As such, a method based on distributional similarity typically fetches the surrounding contexts for each term (i.e. both seeds and candidates) and represents them as vectors by using TF-IDF or PMI (Pointwise Mutual Information) values (Lin, 1998; Gorman and Curran, 2006; Paşca et al. 2006; Agirre et al. 2009; Pantel et al. 2009). Similarity measures such as Cosine, Jaccard, Dice, etc, can then be employed to compute the similarities between each candidate vector and the seeds centroid vector (one centroid vector for all seeds). Lee (1998) surveyed and discussed various distribution similarity measures. The positive examples in SP are called “spies”. Then, a NB classifier is built using the set P– SP as positive and the set U∪SP as negative (line 3, 4, and 5). The NB classifier is applied to classify each u ∈ U∪SP, i.e., to assign a probabilistic class label p(+|u) (+ means pos"
P10-2066,P06-1046,0,0.0292911,"). However, as it relies on Web page structures, it is not applicable to general free texts. 2 Three Different Techniques 2.1 Distributional Similarity Distributional similarity is a classic technique for the entity set expansion problem. It is based on the hypothesis that words with similar meanings tend to appear in similar contexts (Harris, 1985). As such, a method based on distributional similarity typically fetches the surrounding contexts for each term (i.e. both seeds and candidates) and represents them as vectors by using TF-IDF or PMI (Pointwise Mutual Information) values (Lin, 1998; Gorman and Curran, 2006; Paşca et al. 2006; Agirre et al. 2009; Pantel et al. 2009). Similarity measures such as Cosine, Jaccard, Dice, etc, can then be employed to compute the similarities between each candidate vector and the seeds centroid vector (one centroid vector for all seeds). Lee (1998) surveyed and discussed various distribution similarity measures. The positive examples in SP are called “spies”. Then, a NB classifier is built using the set P– SP as positive and the set U∪SP as negative (line 3, 4, and 5). The NB classifier is applied to classify each u ∈ U∪SP, i.e., to assign a probabilistic class label"
P10-2066,C02-1054,0,\N,Missing
P10-2066,C98-2122,0,\N,Missing
P10-2066,D09-1098,0,\N,Missing
P11-2101,E06-1027,0,0.0540667,"yashi et al., 2007; Ding et al., 2008; Titov and McDonald, 2008; Pang and Lee, 2008; Lu et al., 2009). The key difficulty in finding such words is that opinions expressed by many of them are domain or context dependent. Several researchers have studied the problem of finding opinion words (Liu, 2010). The approaches can be grouped into corpus-based approaches (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Kanayama and Nasukawa, 2006; Qiu et al., 2009) and dictionary-based approaches (Hu and Liu 2004; Kim and Hovy, 2004; Kamps et al., 2004; Esuli and Sebastiani, 2005; Takamura et al., 2005; Andreevskaia and Bergler, 2006; Dragut et al., 2010). Dictionary-based approaches are generally not suitable for finding domain specific opinion words as dictionaries contain little domain specific information. Hatzivassiloglou and McKeown (1997) did the first work to tackle the problem for adjectives using a corpus. The approach exploits some conjunctive patterns, involving and, or, but, eitheror, or neither-nor, with the intuition that the conjoining adjectives subject to linguistic constraints on the orientation or polarity of the adjectives involved. Using these constraints, one can infer opinion polarities of unknown"
P11-2101,esuli-sebastiani-2006-sentiwordnet,0,0.0290251,"nerally not suitable for finding domain specific opinion words as dictionaries contain little domain specific information. Hatzivassiloglou and McKeown (1997) did the first work to tackle the problem for adjectives using a corpus. The approach exploits some conjunctive patterns, involving and, or, but, eitheror, or neither-nor, with the intuition that the conjoining adjectives subject to linguistic constraints on the orientation or polarity of the adjectives involved. Using these constraints, one can infer opinion polarities of unknown adjectives based on the known ones. Kanayama and Nasukawa (2006) improved this work by using the idea of coherency. They deal with both adjectives and verbs. Ding et al. (2008) introduced the concept of feature context because the polarities of many opinion bearing words are sentence context dependent rather than just domain dependent. Qiu et al. (2009) proposed a method called double propagation that uses dependency relations to extract both opinion words and product features. 575 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 575–580, c Portland, Oregon, June 19-24, 2011. 2011 Association for Co"
P11-2101,C04-1121,0,0.189312,"Missing"
P11-2101,C08-1031,1,0.784941,"Missing"
P11-2101,P97-1023,0,0.56604,"Missing"
P11-2101,W06-1642,0,0.0764782,"Missing"
P11-2101,D07-1114,0,0.0143495,"n phrases and their polarities is very challenging but critical for effective opinion mining in these domains. To the best of our knowledge, this problem has not been studied in the literature. This paper proposes a method to deal with the problem. Experimental results based on real-life datasets show promising results. 1 Introduction Opinion words are words that convey positive or negative polarities. They are critical for opinion mining (Pang et al., 2002; Turney, 2002; Hu and Liu, 2004; Wilson et al., 2004; Popescu and Etzioni, 2005; Gamon et al., 2005; Ku et al., 2006; Breck et al., 2007; Kobayashi et al., 2007; Ding et al., 2008; Titov and McDonald, 2008; Pang and Lee, 2008; Lu et al., 2009). The key difficulty in finding such words is that opinions expressed by many of them are domain or context dependent. Several researchers have studied the problem of finding opinion words (Liu, 2010). The approaches can be grouped into corpus-based approaches (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Kanayama and Nasukawa, 2006; Qiu et al., 2009) and dictionary-based approaches (Hu and Liu 2004; Kim and Hovy, 2004; Kamps et al., 2004; Esuli and Sebastiani, 2005; Takamura et al., 2005; Andreevskaia and B"
P11-2101,W02-1011,0,0.0370642,"not subjective but objective. Their involved sentences are also objective sentences and imply positive or negative opinions. Identifying such nouns and noun phrases and their polarities is very challenging but critical for effective opinion mining in these domains. To the best of our knowledge, this problem has not been studied in the literature. This paper proposes a method to deal with the problem. Experimental results based on real-life datasets show promising results. 1 Introduction Opinion words are words that convey positive or negative polarities. They are critical for opinion mining (Pang et al., 2002; Turney, 2002; Hu and Liu, 2004; Wilson et al., 2004; Popescu and Etzioni, 2005; Gamon et al., 2005; Ku et al., 2006; Breck et al., 2007; Kobayashi et al., 2007; Ding et al., 2008; Titov and McDonald, 2008; Pang and Lee, 2008; Lu et al., 2009). The key difficulty in finding such words is that opinions expressed by many of them are domain or context dependent. Several researchers have studied the problem of finding opinion words (Liu, 2010). The approaches can be grouped into corpus-based approaches (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Kanayama and Nasukawa, 2006; Qiu et al., 2009"
P11-2101,H05-1043,0,0.260021,"ve sentences and imply positive or negative opinions. Identifying such nouns and noun phrases and their polarities is very challenging but critical for effective opinion mining in these domains. To the best of our knowledge, this problem has not been studied in the literature. This paper proposes a method to deal with the problem. Experimental results based on real-life datasets show promising results. 1 Introduction Opinion words are words that convey positive or negative polarities. They are critical for opinion mining (Pang et al., 2002; Turney, 2002; Hu and Liu, 2004; Wilson et al., 2004; Popescu and Etzioni, 2005; Gamon et al., 2005; Ku et al., 2006; Breck et al., 2007; Kobayashi et al., 2007; Ding et al., 2008; Titov and McDonald, 2008; Pang and Lee, 2008; Lu et al., 2009). The key difficulty in finding such words is that opinions expressed by many of them are domain or context dependent. Several researchers have studied the problem of finding opinion words (Liu, 2010). The approaches can be grouped into corpus-based approaches (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Kanayama and Nasukawa, 2006; Qiu et al., 2009) and dictionary-based approaches (Hu and Liu 2004; Kim and Hovy, 2004; Kamps et"
P11-2101,W03-0404,0,0.0594721,"and Liu, 2004) to mine opinions in product reviews. We found that in some application domains product features which are indicated by nouns have implied opinions although they are not subjective words. This paper aims to identify such opinionated noun features. To make this concrete, let us see an example from a mattress review: “Within a month, a valley formed in the middle of the mattress.” Here “valley” indicates the quality of the mattress (a product feature) and also implies a negative opinion. The opinion implied by “valley” cannot be found by current techniques. Although Riloff et al. (2003) proposed a method to extract subjective nouns, our work is very different because many nouns implying opinions are not subjective nouns, but objective nouns, e.g., “valley” and “hole” on a mattress. Those sentences involving such nouns are usually also objective sentences. As much of the existing opinion mining research focuses on subjective sentences, we believe it is high time to study objective words and sentences that imply opinions as well. This paper represents a positive step towards this direction. Objective words (or sentences) that imply opinions are very difficult to recognize beca"
P11-2101,P08-1036,0,0.0522194,"lenging but critical for effective opinion mining in these domains. To the best of our knowledge, this problem has not been studied in the literature. This paper proposes a method to deal with the problem. Experimental results based on real-life datasets show promising results. 1 Introduction Opinion words are words that convey positive or negative polarities. They are critical for opinion mining (Pang et al., 2002; Turney, 2002; Hu and Liu, 2004; Wilson et al., 2004; Popescu and Etzioni, 2005; Gamon et al., 2005; Ku et al., 2006; Breck et al., 2007; Kobayashi et al., 2007; Ding et al., 2008; Titov and McDonald, 2008; Pang and Lee, 2008; Lu et al., 2009). The key difficulty in finding such words is that opinions expressed by many of them are domain or context dependent. Several researchers have studied the problem of finding opinion words (Liu, 2010). The approaches can be grouped into corpus-based approaches (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Kanayama and Nasukawa, 2006; Qiu et al., 2009) and dictionary-based approaches (Hu and Liu 2004; Kim and Hovy, 2004; Kamps et al., 2004; Esuli and Sebastiani, 2005; Takamura et al., 2005; Andreevskaia and Bergler, 2006; Dragut et al., 2010). Dictionar"
P11-2101,P02-1053,0,0.0292666,"objective. Their involved sentences are also objective sentences and imply positive or negative opinions. Identifying such nouns and noun phrases and their polarities is very challenging but critical for effective opinion mining in these domains. To the best of our knowledge, this problem has not been studied in the literature. This paper proposes a method to deal with the problem. Experimental results based on real-life datasets show promising results. 1 Introduction Opinion words are words that convey positive or negative polarities. They are critical for opinion mining (Pang et al., 2002; Turney, 2002; Hu and Liu, 2004; Wilson et al., 2004; Popescu and Etzioni, 2005; Gamon et al., 2005; Ku et al., 2006; Breck et al., 2007; Kobayashi et al., 2007; Ding et al., 2008; Titov and McDonald, 2008; Pang and Lee, 2008; Lu et al., 2009). The key difficulty in finding such words is that opinions expressed by many of them are domain or context dependent. Several researchers have studied the problem of finding opinion words (Liu, 2010). The approaches can be grouped into corpus-based approaches (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Kanayama and Nasukawa, 2006; Qiu et al., 2009) and dictiona"
P11-2101,I08-1040,0,0.0162275,"g et al. (2008) introduced the concept of feature context because the polarities of many opinion bearing words are sentence context dependent rather than just domain dependent. Qiu et al. (2009) proposed a method called double propagation that uses dependency relations to extract both opinion words and product features. 575 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 575–580, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics However, none of these approaches handle nouns or noun phrases. Although Zagibalov and Carroll (2008) noticed the issue, they did not study it. Esuli and Sebastiani (2006) used WordNet to determine polarities of words, which can include nouns. However, dictionaries do not contain domain specific information. Our work uses the feature-based opinion mining model in (Hu and Liu, 2004) to mine opinions in product reviews. We found that in some application domains product features which are indicated by nouns have implied opinions although they are not subjective words. This paper aims to identify such opinionated noun features. To make this concrete, let us see an example from a mattress review:"
P11-2101,N07-1037,0,\N,Missing
P11-2101,H05-2017,0,\N,Missing
P11-2101,C04-1200,0,\N,Missing
P12-1034,N10-1122,0,0.0127375,"2011), constrained (Andrzejewski et al., 2009) models, etc. These models produce only topics but not multiple types of expressions together with topics. Note that in labeled models, each document is labeled with one or multiple labels. For our work, there is no label for each comment. Our labeling is on topical terms and C-expressions with the purpose of obtaining some priors to separate topics and C-expressions. In sentiment analysis, researchers have jointly modeled topics and sentiment words (Lin and He, 2009; Mei et al., 2007; Lu and Zhai, 2008; Titov and McDonald, 2008b; Lu et al., 2009; Brody and Elhadad, 2010; Wang et al., 2010; Jo and Oh, 2011; Maghaddam and Ester, 2011; Sauper et al., 2011; Mukherjee and Liu, 2012a). Our model is more related to the ME-LDA model in (Zhao et al., 2010), which used a switch variable trained with Maximum-Entropy to separate topic and sentiment words. We also use such a variable. However, unlike sentiments and topics in reviews, which are emitted in the same sentence, C-expressions often interleave with topics across sentences and the same comment post may also have multiple types of C-expressions. Additionally, C-expressions are mostly phrases rather than individua"
P12-1034,P11-1151,0,0.0623782,"C-expressions often interleave with topics across sentences and the same comment post may also have multiple types of C-expressions. Additionally, C-expressions are mostly phrases rather than individual words. Thus, a different model is required to model them. There have also been works aimed at putting authors in debate into support/oppose camps, e.g., (Galley et al., 2004; Agarwal et al., 2003; Murakami and Raymond, 2010), modeling debate discussions considering reply relations (Mukherjee and Liu, 2012b), and identifying stances in debates (Somasundaran and Wiebe, 2009; Thomas et al., 2006; Burfoot et al., 2011). (Yano and Smith, γ u 2010) also modeled the relationship of a blog post r ψ λ and the number of comments it receives. These ψ r works are different as they do not mine Cx z z αE αE αT θT expressions or discover the points of contention αT θT θE θE and questions in comments. w w In (Kim et al., 2006; Zhang and Varadarajan, Nd Nd D D 2006; Ghose and Ipeirotis, 2007; Liu et al., 2007; Liu et al., 2008; O’Mahony and Smyth, 2009; Tsur β βE βT βE φE φT φT φE and Rappoport 2009), various classification and T T E T E regression approaches were taken to assess the (a) TME Model (b) ME-TME Model quali"
P12-1034,P04-1085,0,0.728602,"el in (Zhao et al., 2010), which used a switch variable trained with Maximum-Entropy to separate topic and sentiment words. We also use such a variable. However, unlike sentiments and topics in reviews, which are emitted in the same sentence, C-expressions often interleave with topics across sentences and the same comment post may also have multiple types of C-expressions. Additionally, C-expressions are mostly phrases rather than individual words. Thus, a different model is required to model them. There have also been works aimed at putting authors in debate into support/oppose camps, e.g., (Galley et al., 2004; Agarwal et al., 2003; Murakami and Raymond, 2010), modeling debate discussions considering reply relations (Mukherjee and Liu, 2012b), and identifying stances in debates (Somasundaran and Wiebe, 2009; Thomas et al., 2006; Burfoot et al., 2011). (Yano and Smith, γ u 2010) also modeled the relationship of a blog post r ψ λ and the number of comments it receives. These ψ r works are different as they do not mine Cx z z αE αE αT θT expressions or discover the points of contention αT θT θE θE and questions in comments. w w In (Kim et al., 2006; Zhang and Varadarajan, Nd Nd D D 2006; Ghose and Ipe"
P12-1034,W06-1650,0,0.25713,"Missing"
P12-1034,D07-1035,0,0.0157517,"l., 2003; Murakami and Raymond, 2010), modeling debate discussions considering reply relations (Mukherjee and Liu, 2012b), and identifying stances in debates (Somasundaran and Wiebe, 2009; Thomas et al., 2006; Burfoot et al., 2011). (Yano and Smith, γ u 2010) also modeled the relationship of a blog post r ψ λ and the number of comments it receives. These ψ r works are different as they do not mine Cx z z αE αE αT θT expressions or discover the points of contention αT θT θE θE and questions in comments. w w In (Kim et al., 2006; Zhang and Varadarajan, Nd Nd D D 2006; Ghose and Ipeirotis, 2007; Liu et al., 2007; Liu et al., 2008; O’Mahony and Smyth, 2009; Tsur β βE βT βE φE φT φT φE and Rappoport 2009), various classification and T T E T E regression approaches were taken to assess the (a) TME Model (b) ME-TME Model quality of reviews. (Jindal and Liu, 2008; Lim et Figure 1: Graphical Models in plate notations. al., 2010; Li et al. 2011; Ott et al., 2011; we have … topics and … expression types in Mukherjee et al., 2012) detect fake reviews and our corpus. Note that in our case of Amazon reviewers. However, all these works are not review comments, based on reading various posts, concerned with revie"
P12-1034,P12-1036,1,0.85632,"types of expressions together with topics. Note that in labeled models, each document is labeled with one or multiple labels. For our work, there is no label for each comment. Our labeling is on topical terms and C-expressions with the purpose of obtaining some priors to separate topics and C-expressions. In sentiment analysis, researchers have jointly modeled topics and sentiment words (Lin and He, 2009; Mei et al., 2007; Lu and Zhai, 2008; Titov and McDonald, 2008b; Lu et al., 2009; Brody and Elhadad, 2010; Wang et al., 2010; Jo and Oh, 2011; Maghaddam and Ester, 2011; Sauper et al., 2011; Mukherjee and Liu, 2012a). Our model is more related to the ME-LDA model in (Zhao et al., 2010), which used a switch variable trained with Maximum-Entropy to separate topic and sentiment words. We also use such a variable. However, unlike sentiments and topics in reviews, which are emitted in the same sentence, C-expressions often interleave with topics across sentences and the same comment post may also have multiple types of C-expressions. Additionally, C-expressions are mostly phrases rather than individual words. Thus, a different model is required to model them. There have also been works aimed at putting autho"
P12-1034,C10-2100,0,0.129191,"tch variable trained with Maximum-Entropy to separate topic and sentiment words. We also use such a variable. However, unlike sentiments and topics in reviews, which are emitted in the same sentence, C-expressions often interleave with topics across sentences and the same comment post may also have multiple types of C-expressions. Additionally, C-expressions are mostly phrases rather than individual words. Thus, a different model is required to model them. There have also been works aimed at putting authors in debate into support/oppose camps, e.g., (Galley et al., 2004; Agarwal et al., 2003; Murakami and Raymond, 2010), modeling debate discussions considering reply relations (Mukherjee and Liu, 2012b), and identifying stances in debates (Somasundaran and Wiebe, 2009; Thomas et al., 2006; Burfoot et al., 2011). (Yano and Smith, γ u 2010) also modeled the relationship of a blog post r ψ λ and the number of comments it receives. These ψ r works are different as they do not mine Cx z z αE αE αT θT expressions or discover the points of contention αT θT θE θE and questions in comments. w w In (Kim et al., 2006; Zhang and Varadarajan, Nd Nd D D 2006; Ghose and Ipeirotis, 2007; Liu et al., 2007; Liu et al., 2008; O"
P12-1034,P11-1032,0,0.0384644,"Missing"
P12-1034,D09-1026,0,0.0108258,"a large number of review comments from Amazon.com. Experimental results show that both TME and METME are effective in performing their tasks. METME also outperforms TME significantly. 2. Related Work We believe that this work is the first attempt to model review comments for fine-grained analysis. There are, however, several general research areas that are related to our work. Topic models such as LDA (Latent Dirichlet Allocation) (Blei et al., 2003) have been used to mine topics in large text collections. There have been various extensions to multi-grain (Titov and McDonald, 2008a), labeled (Ramage et al., 2009), partially-labeled (Ramage et al., 2011), constrained (Andrzejewski et al., 2009) models, etc. These models produce only topics but not multiple types of expressions together with topics. Note that in labeled models, each document is labeled with one or multiple labels. For our work, there is no label for each comment. Our labeling is on topical terms and C-expressions with the purpose of obtaining some priors to separate topics and C-expressions. In sentiment analysis, researchers have jointly modeled topics and sentiment words (Lin and He, 2009; Mei et al., 2007; Lu and Zhai, 2008; Titov an"
P12-1034,P11-1036,0,0.0169777,"Missing"
P12-1034,P09-1026,0,0.0978984,"n reviews, which are emitted in the same sentence, C-expressions often interleave with topics across sentences and the same comment post may also have multiple types of C-expressions. Additionally, C-expressions are mostly phrases rather than individual words. Thus, a different model is required to model them. There have also been works aimed at putting authors in debate into support/oppose camps, e.g., (Galley et al., 2004; Agarwal et al., 2003; Murakami and Raymond, 2010), modeling debate discussions considering reply relations (Mukherjee and Liu, 2012b), and identifying stances in debates (Somasundaran and Wiebe, 2009; Thomas et al., 2006; Burfoot et al., 2011). (Yano and Smith, γ u 2010) also modeled the relationship of a blog post r ψ λ and the number of comments it receives. These ψ r works are different as they do not mine Cx z z αE αE αT θT expressions or discover the points of contention αT θT θE θE and questions in comments. w w In (Kim et al., 2006; Zhang and Varadarajan, Nd Nd D D 2006; Ghose and Ipeirotis, 2007; Liu et al., 2007; Liu et al., 2008; O’Mahony and Smyth, 2009; Tsur β βE βT βE φE φT φT φE and Rappoport 2009), various classification and T T E T E regression approaches were taken to ass"
P12-1034,W06-1639,0,0.1359,"n the same sentence, C-expressions often interleave with topics across sentences and the same comment post may also have multiple types of C-expressions. Additionally, C-expressions are mostly phrases rather than individual words. Thus, a different model is required to model them. There have also been works aimed at putting authors in debate into support/oppose camps, e.g., (Galley et al., 2004; Agarwal et al., 2003; Murakami and Raymond, 2010), modeling debate discussions considering reply relations (Mukherjee and Liu, 2012b), and identifying stances in debates (Somasundaran and Wiebe, 2009; Thomas et al., 2006; Burfoot et al., 2011). (Yano and Smith, γ u 2010) also modeled the relationship of a blog post r ψ λ and the number of comments it receives. These ψ r works are different as they do not mine Cx z z αE αE αT θT expressions or discover the points of contention αT θT θE θE and questions in comments. w w In (Kim et al., 2006; Zhang and Varadarajan, Nd Nd D D 2006; Ghose and Ipeirotis, 2007; Liu et al., 2007; Liu et al., 2008; O’Mahony and Smyth, 2009; Tsur β βE βT βE φE φT φT φE and Rappoport 2009), various classification and T T E T E regression approaches were taken to assess the (a) TME Model"
P12-1034,P08-1036,0,0.0247788,"alitatively and quantitatively using a large number of review comments from Amazon.com. Experimental results show that both TME and METME are effective in performing their tasks. METME also outperforms TME significantly. 2. Related Work We believe that this work is the first attempt to model review comments for fine-grained analysis. There are, however, several general research areas that are related to our work. Topic models such as LDA (Latent Dirichlet Allocation) (Blei et al., 2003) have been used to mine topics in large text collections. There have been various extensions to multi-grain (Titov and McDonald, 2008a), labeled (Ramage et al., 2009), partially-labeled (Ramage et al., 2011), constrained (Andrzejewski et al., 2009) models, etc. These models produce only topics but not multiple types of expressions together with topics. Note that in labeled models, each document is labeled with one or multiple labels. For our work, there is no label for each comment. Our labeling is on topical terms and C-expressions with the purpose of obtaining some priors to separate topics and C-expressions. In sentiment analysis, researchers have jointly modeled topics and sentiment words (Lin and He, 2009; Mei et al.,"
P12-1034,D10-1006,0,0.108111,"document is labeled with one or multiple labels. For our work, there is no label for each comment. Our labeling is on topical terms and C-expressions with the purpose of obtaining some priors to separate topics and C-expressions. In sentiment analysis, researchers have jointly modeled topics and sentiment words (Lin and He, 2009; Mei et al., 2007; Lu and Zhai, 2008; Titov and McDonald, 2008b; Lu et al., 2009; Brody and Elhadad, 2010; Wang et al., 2010; Jo and Oh, 2011; Maghaddam and Ester, 2011; Sauper et al., 2011; Mukherjee and Liu, 2012a). Our model is more related to the ME-LDA model in (Zhao et al., 2010), which used a switch variable trained with Maximum-Entropy to separate topic and sentiment words. We also use such a variable. However, unlike sentiments and topics in reviews, which are emitted in the same sentence, C-expressions often interleave with topics across sentences and the same comment post may also have multiple types of C-expressions. Additionally, C-expressions are mostly phrases rather than individual words. Thus, a different model is required to model them. There have also been works aimed at putting authors in debate into support/oppose camps, e.g., (Galley et al., 2004; Agar"
P12-1036,P08-1031,0,0.124628,"models. However, all these methods do not group extracted aspect terms into categories. Although there are works on grouping aspect terms (Carenini et al., 2005; Zhai et al., 2010; Zhai et al., 340 2011; Guo et al., 2010), they all assume that aspect terms have been extracted beforehand. In recent years, topic models have been used to perform extraction and grouping at the same time. Existing works are based on two basic models, pLSA (Hofmann, 1999) and LDA (Blei et al., 2003). Some existing works include discovering global and local aspects (Titov and McDonald, 2008), extracting key phrases (Branavan et al., 2008), rating multi-aspects (Wang et al., 2010; Moghaddam and Ester, 2011), summarizing aspects and sentiments (Lu et al., 2009), and modeling attitudes (Sauper et al., 2011). In (Lu and Zhai, 2008), a semi-supervised model was proposed. However, their method is entirely different from ours as they use expert reviews to guide the analysis of user reviews. Aspect and sentiment extraction using topic modeling come in two flavors: discovering aspect words sentiment wise (i.e., discovering positive and negative aspect words and/or sentiments for each aspect without separating aspect and sentiment terms"
P12-1036,N10-1122,0,0.793209,"s (Wang et al., 2010; Moghaddam and Ester, 2011), summarizing aspects and sentiments (Lu et al., 2009), and modeling attitudes (Sauper et al., 2011). In (Lu and Zhai, 2008), a semi-supervised model was proposed. However, their method is entirely different from ours as they use expert reviews to guide the analysis of user reviews. Aspect and sentiment extraction using topic modeling come in two flavors: discovering aspect words sentiment wise (i.e., discovering positive and negative aspect words and/or sentiments for each aspect without separating aspect and sentiment terms) (Lin and He, 2009; Brody and Elhadad, 2010, Jo and Oh, 2011) and separately discovering both aspects and sentiments (e.g., Mei et al., 2007; Zhao et al., 2010). Zhao et al. (2010) used Maximum-Entropy to train a switch variable to separate aspect and sentiment words. We adopt this method as well but with no use of manually labeled data in training. One problem with these existing models is that many discovered aspects are not understandable/meaningful to users. Chang et al. (2009) stated that one reason is that the objective function of topic models does not always correlate well with human judgments. Our seeded models are designed to"
P12-1036,P10-2050,0,0.222971,"e-art baselines. Experimental results show that the proposed models outperform the two baselines by large margins. 2 Related Work There are many existing works on aspect extraction. One approach is to find frequent noun terms and possibly with the help of dependency relations (Hu and Liu, 2004; Popescu and Etzioni, 2005; Zhuang et al., 2006; Blair-Goldensohn et al., 2008; Ku et al., 2006; Wu et al., 2009; Somasundaran and Wiebe, 2009; Qiu et al., 2011). Another approach is to use supervised sequence labeling (Liu, Hu and Cheng 2005; Jin and Ho, 2009; Jakob and Gurevych, 2010; Li et al., 2010; Choi and Cardie, 2010; Kobayashi et al., 2007; Yu et al., 2011). Ma and Wan (2010) also exploited centering theory, and (Yi et al., 2003) used language models. However, all these methods do not group extracted aspect terms into categories. Although there are works on grouping aspect terms (Carenini et al., 2005; Zhai et al., 2010; Zhai et al., 340 2011; Guo et al., 2010), they all assume that aspect terms have been extracted beforehand. In recent years, topic models have been used to perform extraction and grouping at the same time. Existing works are based on two basic models, pLSA (Hofmann, 1999) and LDA (Blei e"
P12-1036,P11-1026,0,0.062303,"rds. We adopt this method as well but with no use of manually labeled data in training. One problem with these existing models is that many discovered aspects are not understandable/meaningful to users. Chang et al. (2009) stated that one reason is that the objective function of topic models does not always correlate well with human judgments. Our seeded models are designed to overcome this problem. Researchers have tried to generate “meaningful” and “specific” topics/aspects. Blei and McAuliffe (2007) and Ramage et al. (2009) used document label information in a supervised setting. Hu et al. (2011) relied on user feedback during Gibbs sampling iterations. Andrzejewski et al. (2011) incorporated first-order logic with Markov Logic Networks. However, it has a practical limitation for reasonably large corpora since the number of non-trivial groundings can grow to O(N2) where N is the number of unique tokens in the corpus. Andrzejewski et al. (2009) used another approach (DF-LDA) by introducing must-link and cannotlink constraints as Dirichlet Forest priors. Zhai et al. (2011) reported that the model does not scale up φA αa ψ za θa δa a room stain bed staff linens service shower pillows wal"
P12-1036,D10-1101,0,0.205869,"They are also compared with two state-of-the-art baselines. Experimental results show that the proposed models outperform the two baselines by large margins. 2 Related Work There are many existing works on aspect extraction. One approach is to find frequent noun terms and possibly with the help of dependency relations (Hu and Liu, 2004; Popescu and Etzioni, 2005; Zhuang et al., 2006; Blair-Goldensohn et al., 2008; Ku et al., 2006; Wu et al., 2009; Somasundaran and Wiebe, 2009; Qiu et al., 2011). Another approach is to use supervised sequence labeling (Liu, Hu and Cheng 2005; Jin and Ho, 2009; Jakob and Gurevych, 2010; Li et al., 2010; Choi and Cardie, 2010; Kobayashi et al., 2007; Yu et al., 2011). Ma and Wan (2010) also exploited centering theory, and (Yi et al., 2003) used language models. However, all these methods do not group extracted aspect terms into categories. Although there are works on grouping aspect terms (Carenini et al., 2005; Zhai et al., 2010; Zhai et al., 340 2011; Guo et al., 2010), they all assume that aspect terms have been extracted beforehand. In recent years, topic models have been used to perform extraction and grouping at the same time. Existing works are based on two basic mode"
P12-1036,D07-1114,0,0.0977319,"Missing"
P12-1036,C10-1074,0,0.07996,"h two state-of-the-art baselines. Experimental results show that the proposed models outperform the two baselines by large margins. 2 Related Work There are many existing works on aspect extraction. One approach is to find frequent noun terms and possibly with the help of dependency relations (Hu and Liu, 2004; Popescu and Etzioni, 2005; Zhuang et al., 2006; Blair-Goldensohn et al., 2008; Ku et al., 2006; Wu et al., 2009; Somasundaran and Wiebe, 2009; Qiu et al., 2011). Another approach is to use supervised sequence labeling (Liu, Hu and Cheng 2005; Jin and Ho, 2009; Jakob and Gurevych, 2010; Li et al., 2010; Choi and Cardie, 2010; Kobayashi et al., 2007; Yu et al., 2011). Ma and Wan (2010) also exploited centering theory, and (Yi et al., 2003) used language models. However, all these methods do not group extracted aspect terms into categories. Although there are works on grouping aspect terms (Carenini et al., 2005; Zhai et al., 2010; Zhai et al., 340 2011; Guo et al., 2010), they all assume that aspect terms have been extracted beforehand. In recent years, topic models have been used to perform extraction and grouping at the same time. Existing works are based on two basic models, pLSA (Hofmann"
P12-1036,C10-2090,0,0.0218341,"ls outperform the two baselines by large margins. 2 Related Work There are many existing works on aspect extraction. One approach is to find frequent noun terms and possibly with the help of dependency relations (Hu and Liu, 2004; Popescu and Etzioni, 2005; Zhuang et al., 2006; Blair-Goldensohn et al., 2008; Ku et al., 2006; Wu et al., 2009; Somasundaran and Wiebe, 2009; Qiu et al., 2011). Another approach is to use supervised sequence labeling (Liu, Hu and Cheng 2005; Jin and Ho, 2009; Jakob and Gurevych, 2010; Li et al., 2010; Choi and Cardie, 2010; Kobayashi et al., 2007; Yu et al., 2011). Ma and Wan (2010) also exploited centering theory, and (Yi et al., 2003) used language models. However, all these methods do not group extracted aspect terms into categories. Although there are works on grouping aspect terms (Carenini et al., 2005; Zhai et al., 2010; Zhai et al., 340 2011; Guo et al., 2010), they all assume that aspect terms have been extracted beforehand. In recent years, topic models have been used to perform extraction and grouping at the same time. Existing works are based on two basic models, pLSA (Hofmann, 1999) and LDA (Blei et al., 2003). Some existing works include discovering global"
P12-1036,H05-1043,0,0.505821,"lear separation of aspects from sentiments producing better results. Second, our way of treating seeds is also different from DF-LDA. We discuss these and other related work in Section 2. The proposed models are evaluated using a large number of hotel reviews. They are also compared with two state-of-the-art baselines. Experimental results show that the proposed models outperform the two baselines by large margins. 2 Related Work There are many existing works on aspect extraction. One approach is to find frequent noun terms and possibly with the help of dependency relations (Hu and Liu, 2004; Popescu and Etzioni, 2005; Zhuang et al., 2006; Blair-Goldensohn et al., 2008; Ku et al., 2006; Wu et al., 2009; Somasundaran and Wiebe, 2009; Qiu et al., 2011). Another approach is to use supervised sequence labeling (Liu, Hu and Cheng 2005; Jin and Ho, 2009; Jakob and Gurevych, 2010; Li et al., 2010; Choi and Cardie, 2010; Kobayashi et al., 2007; Yu et al., 2011). Ma and Wan (2010) also exploited centering theory, and (Yi et al., 2003) used language models. However, all these methods do not group extracted aspect terms into categories. Although there are works on grouping aspect terms (Carenini et al., 2005; Zhai et"
P12-1036,J11-1002,1,0.135367,"s these and other related work in Section 2. The proposed models are evaluated using a large number of hotel reviews. They are also compared with two state-of-the-art baselines. Experimental results show that the proposed models outperform the two baselines by large margins. 2 Related Work There are many existing works on aspect extraction. One approach is to find frequent noun terms and possibly with the help of dependency relations (Hu and Liu, 2004; Popescu and Etzioni, 2005; Zhuang et al., 2006; Blair-Goldensohn et al., 2008; Ku et al., 2006; Wu et al., 2009; Somasundaran and Wiebe, 2009; Qiu et al., 2011). Another approach is to use supervised sequence labeling (Liu, Hu and Cheng 2005; Jin and Ho, 2009; Jakob and Gurevych, 2010; Li et al., 2010; Choi and Cardie, 2010; Kobayashi et al., 2007; Yu et al., 2011). Ma and Wan (2010) also exploited centering theory, and (Yi et al., 2003) used language models. However, all these methods do not group extracted aspect terms into categories. Although there are works on grouping aspect terms (Carenini et al., 2005; Zhai et al., 2010; Zhai et al., 340 2011; Guo et al., 2010), they all assume that aspect terms have been extracted beforehand. In recent years"
P12-1036,D09-1026,0,0.350419,"010) used Maximum-Entropy to train a switch variable to separate aspect and sentiment words. We adopt this method as well but with no use of manually labeled data in training. One problem with these existing models is that many discovered aspects are not understandable/meaningful to users. Chang et al. (2009) stated that one reason is that the objective function of topic models does not always correlate well with human judgments. Our seeded models are designed to overcome this problem. Researchers have tried to generate “meaningful” and “specific” topics/aspects. Blei and McAuliffe (2007) and Ramage et al. (2009) used document label information in a supervised setting. Hu et al. (2011) relied on user feedback during Gibbs sampling iterations. Andrzejewski et al. (2011) incorporated first-order logic with Markov Logic Networks. However, it has a practical limitation for reasonably large corpora since the number of non-trivial groundings can grow to O(N2) where N is the number of unique tokens in the corpus. Andrzejewski et al. (2009) used another approach (DF-LDA) by introducing must-link and cannotlink constraints as Dirichlet Forest priors. Zhai et al. (2011) reported that the model does not scale up"
P12-1036,P11-1036,0,0.0545908,"l., 2010; Zhai et al., 340 2011; Guo et al., 2010), they all assume that aspect terms have been extracted beforehand. In recent years, topic models have been used to perform extraction and grouping at the same time. Existing works are based on two basic models, pLSA (Hofmann, 1999) and LDA (Blei et al., 2003). Some existing works include discovering global and local aspects (Titov and McDonald, 2008), extracting key phrases (Branavan et al., 2008), rating multi-aspects (Wang et al., 2010; Moghaddam and Ester, 2011), summarizing aspects and sentiments (Lu et al., 2009), and modeling attitudes (Sauper et al., 2011). In (Lu and Zhai, 2008), a semi-supervised model was proposed. However, their method is entirely different from ours as they use expert reviews to guide the analysis of user reviews. Aspect and sentiment extraction using topic modeling come in two flavors: discovering aspect words sentiment wise (i.e., discovering positive and negative aspect words and/or sentiments for each aspect without separating aspect and sentiment terms) (Lin and He, 2009; Brody and Elhadad, 2010, Jo and Oh, 2011) and separately discovering both aspects and sentiments (e.g., Mei et al., 2007; Zhao et al., 2010). Zhao e"
P12-1036,P09-1026,0,0.0318301,"fferent from DF-LDA. We discuss these and other related work in Section 2. The proposed models are evaluated using a large number of hotel reviews. They are also compared with two state-of-the-art baselines. Experimental results show that the proposed models outperform the two baselines by large margins. 2 Related Work There are many existing works on aspect extraction. One approach is to find frequent noun terms and possibly with the help of dependency relations (Hu and Liu, 2004; Popescu and Etzioni, 2005; Zhuang et al., 2006; Blair-Goldensohn et al., 2008; Ku et al., 2006; Wu et al., 2009; Somasundaran and Wiebe, 2009; Qiu et al., 2011). Another approach is to use supervised sequence labeling (Liu, Hu and Cheng 2005; Jin and Ho, 2009; Jakob and Gurevych, 2010; Li et al., 2010; Choi and Cardie, 2010; Kobayashi et al., 2007; Yu et al., 2011). Ma and Wan (2010) also exploited centering theory, and (Yi et al., 2003) used language models. However, all these methods do not group extracted aspect terms into categories. Although there are works on grouping aspect terms (Carenini et al., 2005; Zhai et al., 2010; Zhai et al., 340 2011; Guo et al., 2010), they all assume that aspect terms have been extracted beforeha"
P12-1036,D09-1159,0,0.31005,"seeds is also different from DF-LDA. We discuss these and other related work in Section 2. The proposed models are evaluated using a large number of hotel reviews. They are also compared with two state-of-the-art baselines. Experimental results show that the proposed models outperform the two baselines by large margins. 2 Related Work There are many existing works on aspect extraction. One approach is to find frequent noun terms and possibly with the help of dependency relations (Hu and Liu, 2004; Popescu and Etzioni, 2005; Zhuang et al., 2006; Blair-Goldensohn et al., 2008; Ku et al., 2006; Wu et al., 2009; Somasundaran and Wiebe, 2009; Qiu et al., 2011). Another approach is to use supervised sequence labeling (Liu, Hu and Cheng 2005; Jin and Ho, 2009; Jakob and Gurevych, 2010; Li et al., 2010; Choi and Cardie, 2010; Kobayashi et al., 2007; Yu et al., 2011). Ma and Wan (2010) also exploited centering theory, and (Yi et al., 2003) used language models. However, all these methods do not group extracted aspect terms into categories. Although there are works on grouping aspect terms (Carenini et al., 2005; Zhai et al., 2010; Zhai et al., 340 2011; Guo et al., 2010), they all assume that aspect term"
P12-1036,P11-1150,0,0.318529,"the proposed models outperform the two baselines by large margins. 2 Related Work There are many existing works on aspect extraction. One approach is to find frequent noun terms and possibly with the help of dependency relations (Hu and Liu, 2004; Popescu and Etzioni, 2005; Zhuang et al., 2006; Blair-Goldensohn et al., 2008; Ku et al., 2006; Wu et al., 2009; Somasundaran and Wiebe, 2009; Qiu et al., 2011). Another approach is to use supervised sequence labeling (Liu, Hu and Cheng 2005; Jin and Ho, 2009; Jakob and Gurevych, 2010; Li et al., 2010; Choi and Cardie, 2010; Kobayashi et al., 2007; Yu et al., 2011). Ma and Wan (2010) also exploited centering theory, and (Yi et al., 2003) used language models. However, all these methods do not group extracted aspect terms into categories. Although there are works on grouping aspect terms (Carenini et al., 2005; Zhai et al., 2010; Zhai et al., 340 2011; Guo et al., 2010), they all assume that aspect terms have been extracted beforehand. In recent years, topic models have been used to perform extraction and grouping at the same time. Existing works are based on two basic models, pLSA (Hofmann, 1999) and LDA (Blei et al., 2003). Some existing works include"
P12-1036,C10-1143,1,0.8843,"r aspect extraction. They largely fall into two main types. The first type only extracts aspect terms without grouping them into categories (although a subsequent step may be used for the grouping, see Section 2). The second type uses statistical topic models to extract aspects and group them at the same time in an unsupervised manner. Both approaches are useful. However, in practice, one also encounters another setting, where grouping is not straightforward because for different applications the user may need different groupings to reflect the application needs. This problem was reported in (Zhai et al., 2010), which gave the following example. In car reviews, internal design and external design can be regarded as two separate aspects, but can also be regarded as one aspect, called “design”, based on the level of details that the user wants to study. It is also possible that the same word may be put in different categories based on different needs. However, (Zhai et al., 2010) did not extract aspect terms. It only categorizes a set of given aspect terms. In this work, we propose two novel statistical models to extract and categorize aspect terms automatically given some seeds in the user interested"
P12-1036,D10-1006,0,0.911444,"x-Ent, we do not need manually labeled training data (see Section 4). 339 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 339–348, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics In practical applications, asking users to provide some seeds is easy as they are normally experts in their trades and have a good knowledge what are important in their domains. Our models are related to topic models in general (Blei et al., 2003) and joint models of aspects and sentiments in sentiment analysis in specific (e.g., Zhao et al., 2010). However, these current models are typically unsupervised. None of them can use seeds. With seeds, our models are thus semi-supervised and need a different formulation. Our models are also related to the DFLDA model in (Andrzejewski et al., 2009), which allows the user to set must-link and cannot-link constraints. A must-link means that two terms must be in the same topic (aspect category), and a cannot-link means that two terms cannot be in the same topic. Seeds may be expressed with mustlinks and cannot-links constraints. However, our models are very different from DF-LDA. First of all, we"
P12-1036,H05-2017,0,\N,Missing
P13-1066,P12-1042,0,0.0948877,"ssification: Several works have attempted to put debate authors into support/oppose camps. Agrawal et al. (2003) used a graph based method. Murakami and Raymond (2010) used a rule-based method. In (Galley et al., 2004; Hillard et al., 2003), speaker 672 utterances were classified into agreement, disagreement and backchannel classes. Stances in online debates: Somasundaran and Wiebe (2009), Thomas et al. (2006), Bansal et al. (2008), Burfoot et al. (2011), and Anand et al. (2011) proposed methods to recognize stances in online debates. Some other research directions include subgroup detection (Abu-Jbara et al., 2012), tolerance analysis (Mukherjee et al., 2013), mining opposing perspectives (Lin and Hauptmann, 2006), linguistic accommodation (Mukherjee and Liu, 2012c), and contention point mining (Mukherjee and Liu, 2012a). For this work, we adopt the JTE-P model in (Mukherjee and Liu, 2012a), and make two major advances. We propose a new method to improve the AD-expression mining and a new task of classifying pair interaction nature to determine whether each pair of users who have interacted based on replying relations mostly agree or disagree with each other. 3 Model We now introduce the JTE-P model wit"
P13-1066,D10-1111,0,0.0802746,"rgue with one another. Such ideological discussions on a myriad of social and political issues have practical implications in the fields of communication and political science as they give social scientists an opportunity to study real-life discussions/debates of almost any issue and analyze participant behaviors in a large scale. In this paper, we present such an application, which aims to perform fine-grained analysis of user-interactions in online discussions. There have been some related works that focus on discovering the general topics and ideological perspectives in online discussions (Ahmed and Xing, 2010), placing users in support/oppose camps (Agarwal et al., 2003), and classifying user stances (Somasundaran and Wiebe, 2009). However, these works are at a rather coarser level and have not considered more fine-grained characteristics of debates/discussions where users interact with each other by quoting/replying each other to express agreement or disagreement and argue with one another. In this work, we want to mine the following information: 1. The nature of interaction of each pair of users or participants who have engaged in the discussion of certain issues, i.e., whether the two persons mo"
P13-1066,W11-1701,0,0.0390704,"erjee and Liu (2012d) mined comment expressions. These works, however, don’t model pair interactions in debates. Support/oppose camp classification: Several works have attempted to put debate authors into support/oppose camps. Agrawal et al. (2003) used a graph based method. Murakami and Raymond (2010) used a rule-based method. In (Galley et al., 2004; Hillard et al., 2003), speaker 672 utterances were classified into agreement, disagreement and backchannel classes. Stances in online debates: Somasundaran and Wiebe (2009), Thomas et al. (2006), Bansal et al. (2008), Burfoot et al. (2011), and Anand et al. (2011) proposed methods to recognize stances in online debates. Some other research directions include subgroup detection (Abu-Jbara et al., 2012), tolerance analysis (Mukherjee et al., 2013), mining opposing perspectives (Lin and Hauptmann, 2006), linguistic accommodation (Mukherjee and Liu, 2012c), and contention point mining (Mukherjee and Liu, 2012a). For this work, we adopt the JTE-P model in (Mukherjee and Liu, 2012a), and make two major advances. We propose a new method to improve the AD-expression mining and a new task of classifying pair interaction nature to determine whether each pair of"
P13-1066,C08-2004,0,0.366415,"the work of LinkLDA (Erosheva et al., 2004). Mukherjee and Liu (2012d) mined comment expressions. These works, however, don’t model pair interactions in debates. Support/oppose camp classification: Several works have attempted to put debate authors into support/oppose camps. Agrawal et al. (2003) used a graph based method. Murakami and Raymond (2010) used a rule-based method. In (Galley et al., 2004; Hillard et al., 2003), speaker 672 utterances were classified into agreement, disagreement and backchannel classes. Stances in online debates: Somasundaran and Wiebe (2009), Thomas et al. (2006), Bansal et al. (2008), Burfoot et al. (2011), and Anand et al. (2011) proposed methods to recognize stances in online debates. Some other research directions include subgroup detection (Abu-Jbara et al., 2012), tolerance analysis (Mukherjee et al., 2013), mining opposing perspectives (Lin and Hauptmann, 2006), linguistic accommodation (Mukherjee and Liu, 2012c), and contention point mining (Mukherjee and Liu, 2012a). For this work, we adopt the JTE-P model in (Mukherjee and Liu, 2012a), and make two major advances. We propose a new method to improve the AD-expression mining and a new task of classifying pair inter"
P13-1066,N10-1122,0,0.0609779,"vi et al., 2004), author persona (Mimno and McCallum, 2007), social roles (McCallum et al., 2007), etc. However, these models do not model debates and hence are unable to discover AD-expressions and interaction natures of author pairs. Also related are topic models in sentiment analysis which are often referred to as Aspect and Sentiment models (ASMs). ASMs come in two main flavors: Type-1 ASMs discover aspect (or topic) words sentiment-wise (i.e., discovering positive and negative topic words and sentiments for each topic without separating topic and sentiment terms) (e.g., Lin and He, 2009; Brody and Elhadad, 2010, Jo and Oh, 2011). Type-2 ASMs separately discover both aspects and sentiments (e.g., Mei et al., 2007; Zhao et al., 2010). Recently, domain knowledge induced ASMs have also been proposed (Mukherjee and Liu, 2012b; Chen et al., 2013). The generative process of ASMs is, however, different from our model. Specifically, Type-1 ASMs use asymmetric hyper-parameters for aspects while Type-2 assumes that sentiments and aspects are emitted in the same sentence. However, ADexpressions are emitted differently. They are mostly interleaved with users’ topical viewpoints and span different sentences. Furt"
P13-1066,P11-1151,0,0.218147,"rosheva et al., 2004). Mukherjee and Liu (2012d) mined comment expressions. These works, however, don’t model pair interactions in debates. Support/oppose camp classification: Several works have attempted to put debate authors into support/oppose camps. Agrawal et al. (2003) used a graph based method. Murakami and Raymond (2010) used a rule-based method. In (Galley et al., 2004; Hillard et al., 2003), speaker 672 utterances were classified into agreement, disagreement and backchannel classes. Stances in online debates: Somasundaran and Wiebe (2009), Thomas et al. (2006), Bansal et al. (2008), Burfoot et al. (2011), and Anand et al. (2011) proposed methods to recognize stances in online debates. Some other research directions include subgroup detection (Abu-Jbara et al., 2012), tolerance analysis (Mukherjee et al., 2013), mining opposing perspectives (Lin and Hauptmann, 2006), linguistic accommodation (Mukherjee and Liu, 2012c), and contention point mining (Mukherjee and Liu, 2012a). For this work, we adopt the JTE-P model in (Mukherjee and Liu, 2012a), and make two major advances. We propose a new method to improve the AD-expression mining and a new task of classifying pair interaction nature to determ"
P13-1066,P10-2050,0,0.0401911,"and quantitatively using a large number of real-life discussion/debate posts from four domains. Experimental results show that the proposed model is highly effective in performing its tasks and outperforms several baselines. 2 Related Work There are several research areas that are related to our work. We compare with them below. Sentiment analysis: Sentiment analysis determines positive and negative opinions expressed on entities and aspects (Hu and Liu, 2004). Main tasks include aspect extraction (Hu and Liu, 2004; Popescu and Etzioni, 2005), polarity identification (Hassan and Radev, 2010; Choi and Cardie, 2010) and subjectivity analysis (Wiebe, 2000). As discussed earlier, agreement and disagreement are a special form of sentiments and are different from the sentiment studied in the mainstream research. Traditional sentiment is mainly expressed with sentiment terms (e.g., great and bad), while agreement and disagreement are inferred by AD-expressions (e.g., I agree and I disagree), which we also call AD-sentiment expressions. Thus, this work expands the sentiment analysis research. Topic models: Our work is also related to topic modeling and joint modeling of topics and other information as we joint"
P13-1066,N03-2012,0,0.807162,"Missing"
P13-1066,P10-1041,0,0.0598965,"uated both qualitatively and quantitatively using a large number of real-life discussion/debate posts from four domains. Experimental results show that the proposed model is highly effective in performing its tasks and outperforms several baselines. 2 Related Work There are several research areas that are related to our work. We compare with them below. Sentiment analysis: Sentiment analysis determines positive and negative opinions expressed on entities and aspects (Hu and Liu, 2004). Main tasks include aspect extraction (Hu and Liu, 2004; Popescu and Etzioni, 2005), polarity identification (Hassan and Radev, 2010; Choi and Cardie, 2010) and subjectivity analysis (Wiebe, 2000). As discussed earlier, agreement and disagreement are a special form of sentiments and are different from the sentiment studied in the mainstream research. Traditional sentiment is mainly expressed with sentiment terms (e.g., great and bad), while agreement and disagreement are inferred by AD-expressions (e.g., I agree and I disagree), which we also call AD-sentiment expressions. Thus, this work expands the sentiment analysis research. Topic models: Our work is also related to topic modeling and joint modeling of topics and other"
P13-1066,P06-1133,0,0.232011,"t al. (2003) used a graph based method. Murakami and Raymond (2010) used a rule-based method. In (Galley et al., 2004; Hillard et al., 2003), speaker 672 utterances were classified into agreement, disagreement and backchannel classes. Stances in online debates: Somasundaran and Wiebe (2009), Thomas et al. (2006), Bansal et al. (2008), Burfoot et al. (2011), and Anand et al. (2011) proposed methods to recognize stances in online debates. Some other research directions include subgroup detection (Abu-Jbara et al., 2012), tolerance analysis (Mukherjee et al., 2013), mining opposing perspectives (Lin and Hauptmann, 2006), linguistic accommodation (Mukherjee and Liu, 2012c), and contention point mining (Mukherjee and Liu, 2012a). For this work, we adopt the JTE-P model in (Mukherjee and Liu, 2012a), and make two major advances. We propose a new method to improve the AD-expression mining and a new task of classifying pair interaction nature to determine whether each pair of users who have interacted based on replying relations mostly agree or disagree with each other. 3 Model We now introduce the JTE-P model with additional details. JTE-P is a semi-supervised generative model motivated by the joint occurrence o"
P13-1066,P13-1165,1,0.690756,"put debate authors into support/oppose camps. Agrawal et al. (2003) used a graph based method. Murakami and Raymond (2010) used a rule-based method. In (Galley et al., 2004; Hillard et al., 2003), speaker 672 utterances were classified into agreement, disagreement and backchannel classes. Stances in online debates: Somasundaran and Wiebe (2009), Thomas et al. (2006), Bansal et al. (2008), Burfoot et al. (2011), and Anand et al. (2011) proposed methods to recognize stances in online debates. Some other research directions include subgroup detection (Abu-Jbara et al., 2012), tolerance analysis (Mukherjee et al., 2013), mining opposing perspectives (Lin and Hauptmann, 2006), linguistic accommodation (Mukherjee and Liu, 2012c), and contention point mining (Mukherjee and Liu, 2012a). For this work, we adopt the JTE-P model in (Mukherjee and Liu, 2012a), and make two major advances. We propose a new method to improve the AD-expression mining and a new task of classifying pair interaction nature to determine whether each pair of users who have interacted based on replying relations mostly agree or disagree with each other. 3 Model We now introduce the JTE-P model with additional details. JTE-P is a semi-supervi"
P13-1066,P12-1036,1,0.94653,"negative. We refer agreement and disagreement expressions as ADsentiment expressions, or AD-expressions for short. AD-expressions are crucial for the analysis of interactive discussions and debates just as sentiment expressions are instrumental in sentiment analysis (Liu, 2012). We thus regard this work as an extension to traditional sentiment 671 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 671–681, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics analysis (Pang and Lee, 2008; Liu, 2012). In our earlier work (Mukherjee and Liu, 2012a), we proposed three topic models to mine contention points, which also extract ADexpressions. In this paper, we further improve the work by coupling an information retrieval method to rank good candidate phrases with topic modeling in order to discover more accurate ADexpressions. Furthermore, we apply the resulting AD-expressions to the new task of classifying the arguing or interaction nature of each pair of users. Using discovered AD-expressions for classification has an important advantage over traditional classification because they are domain independent. We employ a semi-supervised ge"
P13-1066,C12-1112,1,0.937437,"negative. We refer agreement and disagreement expressions as ADsentiment expressions, or AD-expressions for short. AD-expressions are crucial for the analysis of interactive discussions and debates just as sentiment expressions are instrumental in sentiment analysis (Liu, 2012). We thus regard this work as an extension to traditional sentiment 671 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 671–681, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics analysis (Pang and Lee, 2008; Liu, 2012). In our earlier work (Mukherjee and Liu, 2012a), we proposed three topic models to mine contention points, which also extract ADexpressions. In this paper, we further improve the work by coupling an information retrieval method to rank good candidate phrases with topic modeling in order to discover more accurate ADexpressions. Furthermore, we apply the resulting AD-expressions to the new task of classifying the arguing or interaction nature of each pair of users. Using discovered AD-expressions for classification has an important advantage over traditional classification because they are domain independent. We employ a semi-supervised ge"
P13-1066,P12-1034,1,0.935371,"negative. We refer agreement and disagreement expressions as ADsentiment expressions, or AD-expressions for short. AD-expressions are crucial for the analysis of interactive discussions and debates just as sentiment expressions are instrumental in sentiment analysis (Liu, 2012). We thus regard this work as an extension to traditional sentiment 671 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 671–681, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics analysis (Pang and Lee, 2008; Liu, 2012). In our earlier work (Mukherjee and Liu, 2012a), we proposed three topic models to mine contention points, which also extract ADexpressions. In this paper, we further improve the work by coupling an information retrieval method to rank good candidate phrases with topic modeling in order to discover more accurate ADexpressions. Furthermore, we apply the resulting AD-expressions to the new task of classifying the arguing or interaction nature of each pair of users. Using discovered AD-expressions for classification has an important advantage over traditional classification because they are domain independent. We employ a semi-supervised ge"
P13-1066,C10-2100,0,0.1095,"an different sentences. Further, we capture the key characteristic of discussions by encoding pair-wise user interactions. Existing models do not model pair interactions. In terms of discussions and comments, Yano et al., (2009) proposed the CommentLDA model which builds on the work of LinkLDA (Erosheva et al., 2004). Mukherjee and Liu (2012d) mined comment expressions. These works, however, don’t model pair interactions in debates. Support/oppose camp classification: Several works have attempted to put debate authors into support/oppose camps. Agrawal et al. (2003) used a graph based method. Murakami and Raymond (2010) used a rule-based method. In (Galley et al., 2004; Hillard et al., 2003), speaker 672 utterances were classified into agreement, disagreement and backchannel classes. Stances in online debates: Somasundaran and Wiebe (2009), Thomas et al. (2006), Bansal et al. (2008), Burfoot et al. (2011), and Anand et al. (2011) proposed methods to recognize stances in online debates. Some other research directions include subgroup detection (Abu-Jbara et al., 2012), tolerance analysis (Mukherjee et al., 2013), mining opposing perspectives (Lin and Hauptmann, 2006), linguistic accommodation (Mukherjee and L"
P13-1066,H05-1043,0,0.142657,"g arguing nature. The proposed methods have been evaluated both qualitatively and quantitatively using a large number of real-life discussion/debate posts from four domains. Experimental results show that the proposed model is highly effective in performing its tasks and outperforms several baselines. 2 Related Work There are several research areas that are related to our work. We compare with them below. Sentiment analysis: Sentiment analysis determines positive and negative opinions expressed on entities and aspects (Hu and Liu, 2004). Main tasks include aspect extraction (Hu and Liu, 2004; Popescu and Etzioni, 2005), polarity identification (Hassan and Radev, 2010; Choi and Cardie, 2010) and subjectivity analysis (Wiebe, 2000). As discussed earlier, agreement and disagreement are a special form of sentiments and are different from the sentiment studied in the mainstream research. Traditional sentiment is mainly expressed with sentiment terms (e.g., great and bad), while agreement and disagreement are inferred by AD-expressions (e.g., I agree and I disagree), which we also call AD-sentiment expressions. Thus, this work expands the sentiment analysis research. Topic models: Our work is also related to topi"
P13-1066,D09-1026,0,0.0609738,"Missing"
P13-1066,P09-1026,0,0.275351,"cations in the fields of communication and political science as they give social scientists an opportunity to study real-life discussions/debates of almost any issue and analyze participant behaviors in a large scale. In this paper, we present such an application, which aims to perform fine-grained analysis of user-interactions in online discussions. There have been some related works that focus on discovering the general topics and ideological perspectives in online discussions (Ahmed and Xing, 2010), placing users in support/oppose camps (Agarwal et al., 2003), and classifying user stances (Somasundaran and Wiebe, 2009). However, these works are at a rather coarser level and have not considered more fine-grained characteristics of debates/discussions where users interact with each other by quoting/replying each other to express agreement or disagreement and argue with one another. In this work, we want to mine the following information: 1. The nature of interaction of each pair of users or participants who have engaged in the discussion of certain issues, i.e., whether the two persons mostly agree or disagree with each other in their interactions. 2. What language expressions are often used to express agreem"
P13-1066,W06-1639,0,0.489328,"model which builds on the work of LinkLDA (Erosheva et al., 2004). Mukherjee and Liu (2012d) mined comment expressions. These works, however, don’t model pair interactions in debates. Support/oppose camp classification: Several works have attempted to put debate authors into support/oppose camps. Agrawal et al. (2003) used a graph based method. Murakami and Raymond (2010) used a rule-based method. In (Galley et al., 2004; Hillard et al., 2003), speaker 672 utterances were classified into agreement, disagreement and backchannel classes. Stances in online debates: Somasundaran and Wiebe (2009), Thomas et al. (2006), Bansal et al. (2008), Burfoot et al. (2011), and Anand et al. (2011) proposed methods to recognize stances in online debates. Some other research directions include subgroup detection (Abu-Jbara et al., 2012), tolerance analysis (Mukherjee et al., 2013), mining opposing perspectives (Lin and Hauptmann, 2006), linguistic accommodation (Mukherjee and Liu, 2012c), and contention point mining (Mukherjee and Liu, 2012a). For this work, we adopt the JTE-P model in (Mukherjee and Liu, 2012a), and make two major advances. We propose a new method to improve the AD-expression mining and a new task of"
P13-1066,W03-1805,0,0.0236886,"xisting approaches. Topics in most topic models like LDA are usually unigram distributions. This offers a great computational advantage compared to more complex models which consider word ordering (Wallach, 2006; Wang et al., 2007). This thread of research models bigrams by encoding them into the generative process. For each word, a topic is sampled ﬁrst, then its status as a unigram or bigram is sampled, and finally the word is sampled from a topic-speciﬁc unigram or bigram distribution. This method, however, is expensive computationally and has a limitation for arbitrary length n-grams. In (Tomokiyo and Hurst, 2003), a language model approach is used for bigram phrase extraction. Yet another thread of research post-processes the discovered topical unigrams to form multiword phrases using likelihood scores (Blei and Lafferty, 2009). This approach considers adjacent word pairs and identifies n-grams which occur much more often than one would expect by chance alone by computing likelihood ratios. While this is reasonable, a significant n-gram with high likelihood score may not necessarily be relevant to the problem domain. For instance, in our case of discovering AD-expressions, the likelihood score 2 of ?1"
P13-1066,N09-1054,0,0.0325952,"roposed (Mukherjee and Liu, 2012b; Chen et al., 2013). The generative process of ASMs is, however, different from our model. Specifically, Type-1 ASMs use asymmetric hyper-parameters for aspects while Type-2 assumes that sentiments and aspects are emitted in the same sentence. However, ADexpressions are emitted differently. They are mostly interleaved with users’ topical viewpoints and span different sentences. Further, we capture the key characteristic of discussions by encoding pair-wise user interactions. Existing models do not model pair interactions. In terms of discussions and comments, Yano et al., (2009) proposed the CommentLDA model which builds on the work of LinkLDA (Erosheva et al., 2004). Mukherjee and Liu (2012d) mined comment expressions. These works, however, don’t model pair interactions in debates. Support/oppose camp classification: Several works have attempted to put debate authors into support/oppose camps. Agrawal et al. (2003) used a graph based method. Murakami and Raymond (2010) used a rule-based method. In (Galley et al., 2004; Hillard et al., 2003), speaker 672 utterances were classified into agreement, disagreement and backchannel classes. Stances in online debates: Somasu"
P13-1066,P11-1039,0,0.060905,"elated to (or appear with high probabilities in) the given AD-expression type, ? : Agreement ( ?? ) or disagreement (?????). Continuing from the previous example, ? , ?2 is relevant given the expression type ??=????? while ?1 is not as “government” and “disagree” are highly unlikely and likely respectively to be ? . Thus, we want to rank clustered in ??=????? phrases based on ?(??? = 1|?, ?) where ? denotes the expression type (Agreement/Disagreement), ? denotes a candidate phrase. Following the probabilistic relevance model in (Lafferty and Zhai, 2003), we use a similar technique to that in (Zhao et al., 2011) for deriving our relevance ranking function as follows: ?(??? = 1|?, ?) = 1 ?(???=0|?,?) ?(???=1|?,?) 1+ 1 = ?(???=1|?,?) ?(???=0|?,?)+?(???=1|?,?) 1 ?(???=0,? |?) ?(???=1,?|?) 1+ [?(?|???=0,?)×?(???=0|?)] 1+[?(?|???=1,?)×?(???=1|?)] = = (3) ?(???=0|?) We further define ? = ?(???=1|?). Without loss of generality, one can say that ?(??? = 0|?) ≫ ?(??? = 1|?) , because there are many more irrelevant phrases than relevant ones, i.e., ? ≫ 1. Thus, taking log, from equation (3), we get, log ?(??? = 1|?, ?) = log � log � ?(?|???=1,?) ?(?|???=0,?) 1 1+?× × � = log � ? 1 ?(?|???=0,?) ?(?|???=1,?) ?(?"
P13-1066,D10-1006,0,0.732652,"not model debates and hence are unable to discover AD-expressions and interaction natures of author pairs. Also related are topic models in sentiment analysis which are often referred to as Aspect and Sentiment models (ASMs). ASMs come in two main flavors: Type-1 ASMs discover aspect (or topic) words sentiment-wise (i.e., discovering positive and negative topic words and sentiments for each topic without separating topic and sentiment terms) (e.g., Lin and He, 2009; Brody and Elhadad, 2010, Jo and Oh, 2011). Type-2 ASMs separately discover both aspects and sentiments (e.g., Mei et al., 2007; Zhao et al., 2010). Recently, domain knowledge induced ASMs have also been proposed (Mukherjee and Liu, 2012b; Chen et al., 2013). The generative process of ASMs is, however, different from our model. Specifically, Type-1 ASMs use asymmetric hyper-parameters for aspects while Type-2 assumes that sentiments and aspects are emitted in the same sentence. However, ADexpressions are emitted differently. They are mostly interleaved with users’ topical viewpoints and span different sentences. Further, we capture the key characteristic of discussions by encoding pair-wise user interactions. Existing models do not model"
P13-1066,H05-2017,0,\N,Missing
P13-1066,P04-1085,0,\N,Missing
P13-1165,P12-1042,0,0.142276,"best of our knowledge, this is the first such study. Our experiments using real-life discussions demonstrate the effectiveness of the proposed technique and also provide some key insights into the psycholinguistic phenomenon of tolerance in online discussions. 1 Introduction Social media platforms have enabled people from anywhere in the world to express their views and discuss any issue of interest in online discussions/debates. Existing works in this context include recognition of support and oppose camps (Agrawal et al., 2003), mining of authorities and subgroups (Mayfield and Rosè, 2011; Abu-Jbara et al. (2012), dialogue act segmentation and classification (Morbini and Sagae, 2011; Boyer et al., 2011), etc. This paper probes further to study a different and important angle, i.e., the psycholinguistic phenomenon of tolerance in online discussions. Tolerance is an important concept in the field of communications. It is a subfacet of deliberation which refers to critical thinking and exchange of rational arguments on an issue among participants that seek to achieve consensus/solution (Habermas, 1984). Perhaps the most widely accepted definition of tolerance is that of Gastil (2005; 2007), who defines t"
P13-1165,C08-2004,0,0.0685868,"in (Galley et al., 2004; Hillard et al., 2003), speaker utterances were classified into agreement, disagreement, and backchannel classes. Also related are studies on linguistic style accommodation (Mukherjee and Liu, 2012d) and user pair interactions (Mukherjee and Liu, 2013) in online debates. However, these works do not consider tolerance analysis in debate discussions, which is the focus of this work. 1681 In a similar vein, several classification methods have been proposed to recognize opinion stances and speaker sides in online debates (Somasundaran and Wiebe, 2009; Thomas et al., 2006; Bansal et al., 2008; Burfoot et al., 2011; Yessenalina et al., 2010). Lin and Hauptmann (2006) also proposed a method to identify opposing perspectives. Abu-Jbara et al. (2012) identified subgroups. Kim and Hovy (2007) studied election prediction by analyzing online discussions. Other related works studying dialogue and discourse in discussions include authority recognition (Mayfield and Rosè, 2011), dialogue act segmentation and classification (Morbini and Sagae, 2011; Boyer et al., 2011), discourse structure prediction (Wang et al., 2011). All these prior works are valuable. But they are not designed to identi"
P13-1165,P11-1119,0,0.103651,"s demonstrate the effectiveness of the proposed technique and also provide some key insights into the psycholinguistic phenomenon of tolerance in online discussions. 1 Introduction Social media platforms have enabled people from anywhere in the world to express their views and discuss any issue of interest in online discussions/debates. Existing works in this context include recognition of support and oppose camps (Agrawal et al., 2003), mining of authorities and subgroups (Mayfield and Rosè, 2011; Abu-Jbara et al. (2012), dialogue act segmentation and classification (Morbini and Sagae, 2011; Boyer et al., 2011), etc. This paper probes further to study a different and important angle, i.e., the psycholinguistic phenomenon of tolerance in online discussions. Tolerance is an important concept in the field of communications. It is a subfacet of deliberation which refers to critical thinking and exchange of rational arguments on an issue among participants that seek to achieve consensus/solution (Habermas, 1984). Perhaps the most widely accepted definition of tolerance is that of Gastil (2005; 2007), who defines tolerance as a means to engage (in written or spoken communication) in critical thinking, jud"
P13-1165,P11-1151,0,0.0396342,"004; Hillard et al., 2003), speaker utterances were classified into agreement, disagreement, and backchannel classes. Also related are studies on linguistic style accommodation (Mukherjee and Liu, 2012d) and user pair interactions (Mukherjee and Liu, 2013) in online debates. However, these works do not consider tolerance analysis in debate discussions, which is the focus of this work. 1681 In a similar vein, several classification methods have been proposed to recognize opinion stances and speaker sides in online debates (Somasundaran and Wiebe, 2009; Thomas et al., 2006; Bansal et al., 2008; Burfoot et al., 2011; Yessenalina et al., 2010). Lin and Hauptmann (2006) also proposed a method to identify opposing perspectives. Abu-Jbara et al. (2012) identified subgroups. Kim and Hovy (2007) studied election prediction by analyzing online discussions. Other related works studying dialogue and discourse in discussions include authority recognition (Mayfield and Rosè, 2011), dialogue act segmentation and classification (Morbini and Sagae, 2011; Boyer et al., 2011), discourse structure prediction (Wang et al., 2011). All these prior works are valuable. But they are not designed to identify tolerance or to ana"
P13-1165,P10-2050,0,0.0451422,"he domain of political communications with an emphasis on political sophistication (Gastil and Dillard, 1999), civic culture (Dahlgren, 2002), and democracy (Fishkin, 1991). These existing works study tolerance from the qualitative perspective. Our focus is quantitative analysis. Sentiment analysis: Sentiment analysis determines positive or negative opinions expressed on topics (Liu, 2012; Pang and Lee, 2008). Main tasks include aspect extraction (Hu and Liu, 2004; Popescu and Etzioni, 2005; Mukherjee and Liu, 2012c; Chen et al., 2013), opinion polarity identification (Hassan and Radev, 2010; Choi and Cardie, 2010) and subjectivity analysis (Wiebe, 2000). Although related, tolerance is different from sentiment. Sentiments are mainly indicated by sentiment terms (e.g., great, good, bad, and poor). Tolerance in discussions refers to the reception of certain views and often indicated by agreement and disagreement expressions and other features (§5). Online discussions or debates: Several works put authors in debate into support and oppose camps. Agrawal et al. (2003) used a graph based method, and Murakami and Raymond (2010) used a rule-based method. In (Mukherjee and Liu, 2012a), contention points were id"
P13-1165,N03-2012,0,0.15611,"y sentiment terms (e.g., great, good, bad, and poor). Tolerance in discussions refers to the reception of certain views and often indicated by agreement and disagreement expressions and other features (§5). Online discussions or debates: Several works put authors in debate into support and oppose camps. Agrawal et al. (2003) used a graph based method, and Murakami and Raymond (2010) used a rule-based method. In (Mukherjee and Liu, 2012a), contention points were identified, in (Mukherjee and Liu, 2012b), various expressions in review comment discussions were mined, and in (Galley et al., 2004; Hillard et al., 2003), speaker utterances were classified into agreement, disagreement, and backchannel classes. Also related are studies on linguistic style accommodation (Mukherjee and Liu, 2012d) and user pair interactions (Mukherjee and Liu, 2013) in online debates. However, these works do not consider tolerance analysis in debate discussions, which is the focus of this work. 1681 In a similar vein, several classification methods have been proposed to recognize opinion stances and speaker sides in online debates (Somasundaran and Wiebe, 2009; Thomas et al., 2006; Bansal et al., 2008; Burfoot et al., 2011; Yess"
P13-1165,P10-1041,0,0.134037,"o been investigated in the domain of political communications with an emphasis on political sophistication (Gastil and Dillard, 1999), civic culture (Dahlgren, 2002), and democracy (Fishkin, 1991). These existing works study tolerance from the qualitative perspective. Our focus is quantitative analysis. Sentiment analysis: Sentiment analysis determines positive or negative opinions expressed on topics (Liu, 2012; Pang and Lee, 2008). Main tasks include aspect extraction (Hu and Liu, 2004; Popescu and Etzioni, 2005; Mukherjee and Liu, 2012c; Chen et al., 2013), opinion polarity identification (Hassan and Radev, 2010; Choi and Cardie, 2010) and subjectivity analysis (Wiebe, 2000). Although related, tolerance is different from sentiment. Sentiments are mainly indicated by sentiment terms (e.g., great, good, bad, and poor). Tolerance in discussions refers to the reception of certain views and often indicated by agreement and disagreement expressions and other features (§5). Online discussions or debates: Several works put authors in debate into support and oppose camps. Agrawal et al. (2003) used a graph based method, and Murakami and Raymond (2010) used a rule-based method. In (Mukherjee and Liu, 2012a), c"
P13-1165,D07-1113,0,0.0209733,"Mukherjee and Liu, 2012d) and user pair interactions (Mukherjee and Liu, 2013) in online debates. However, these works do not consider tolerance analysis in debate discussions, which is the focus of this work. 1681 In a similar vein, several classification methods have been proposed to recognize opinion stances and speaker sides in online debates (Somasundaran and Wiebe, 2009; Thomas et al., 2006; Bansal et al., 2008; Burfoot et al., 2011; Yessenalina et al., 2010). Lin and Hauptmann (2006) also proposed a method to identify opposing perspectives. Abu-Jbara et al. (2012) identified subgroups. Kim and Hovy (2007) studied election prediction by analyzing online discussions. Other related works studying dialogue and discourse in discussions include authority recognition (Mayfield and Rosè, 2011), dialogue act segmentation and classification (Morbini and Sagae, 2011; Boyer et al., 2011), discourse structure prediction (Wang et al., 2011). All these prior works are valuable. But they are not designed to identify tolerance or to analyze tipping points of disagreements for intolerance in discussions which are the focus of this work. 3 Discussion/Debate Data For this research, we used discussion posts from V"
P13-1165,P06-1133,0,0.119847,"re classified into agreement, disagreement, and backchannel classes. Also related are studies on linguistic style accommodation (Mukherjee and Liu, 2012d) and user pair interactions (Mukherjee and Liu, 2013) in online debates. However, these works do not consider tolerance analysis in debate discussions, which is the focus of this work. 1681 In a similar vein, several classification methods have been proposed to recognize opinion stances and speaker sides in online debates (Somasundaran and Wiebe, 2009; Thomas et al., 2006; Bansal et al., 2008; Burfoot et al., 2011; Yessenalina et al., 2010). Lin and Hauptmann (2006) also proposed a method to identify opposing perspectives. Abu-Jbara et al. (2012) identified subgroups. Kim and Hovy (2007) studied election prediction by analyzing online discussions. Other related works studying dialogue and discourse in discussions include authority recognition (Mayfield and Rosè, 2011), dialogue act segmentation and classification (Morbini and Sagae, 2011; Boyer et al., 2011), discourse structure prediction (Wang et al., 2011). All these prior works are valuable. But they are not designed to identify tolerance or to analyze tipping points of disagreements for intolerance"
P13-1165,P11-1102,0,0.0687205,"Missing"
P13-1165,P11-2017,0,0.138019,"sing real-life discussions demonstrate the effectiveness of the proposed technique and also provide some key insights into the psycholinguistic phenomenon of tolerance in online discussions. 1 Introduction Social media platforms have enabled people from anywhere in the world to express their views and discuss any issue of interest in online discussions/debates. Existing works in this context include recognition of support and oppose camps (Agrawal et al., 2003), mining of authorities and subgroups (Mayfield and Rosè, 2011; Abu-Jbara et al. (2012), dialogue act segmentation and classification (Morbini and Sagae, 2011; Boyer et al., 2011), etc. This paper probes further to study a different and important angle, i.e., the psycholinguistic phenomenon of tolerance in online discussions. Tolerance is an important concept in the field of communications. It is a subfacet of deliberation which refers to critical thinking and exchange of rational arguments on an issue among participants that seek to achieve consensus/solution (Habermas, 1984). Perhaps the most widely accepted definition of tolerance is that of Gastil (2005; 2007), who defines tolerance as a means to engage (in written or spoken communication) in c"
P13-1165,C10-2100,0,0.0439447,"iu, 2012c; Chen et al., 2013), opinion polarity identification (Hassan and Radev, 2010; Choi and Cardie, 2010) and subjectivity analysis (Wiebe, 2000). Although related, tolerance is different from sentiment. Sentiments are mainly indicated by sentiment terms (e.g., great, good, bad, and poor). Tolerance in discussions refers to the reception of certain views and often indicated by agreement and disagreement expressions and other features (§5). Online discussions or debates: Several works put authors in debate into support and oppose camps. Agrawal et al. (2003) used a graph based method, and Murakami and Raymond (2010) used a rule-based method. In (Mukherjee and Liu, 2012a), contention points were identified, in (Mukherjee and Liu, 2012b), various expressions in review comment discussions were mined, and in (Galley et al., 2004; Hillard et al., 2003), speaker utterances were classified into agreement, disagreement, and backchannel classes. Also related are studies on linguistic style accommodation (Mukherjee and Liu, 2012d) and user pair interactions (Mukherjee and Liu, 2013) in online debates. However, these works do not consider tolerance analysis in debate discussions, which is the focus of this work. 16"
P13-1165,P13-1066,1,0.690096,"r debates: Several works put authors in debate into support and oppose camps. Agrawal et al. (2003) used a graph based method, and Murakami and Raymond (2010) used a rule-based method. In (Mukherjee and Liu, 2012a), contention points were identified, in (Mukherjee and Liu, 2012b), various expressions in review comment discussions were mined, and in (Galley et al., 2004; Hillard et al., 2003), speaker utterances were classified into agreement, disagreement, and backchannel classes. Also related are studies on linguistic style accommodation (Mukherjee and Liu, 2012d) and user pair interactions (Mukherjee and Liu, 2013) in online debates. However, these works do not consider tolerance analysis in debate discussions, which is the focus of this work. 1681 In a similar vein, several classification methods have been proposed to recognize opinion stances and speaker sides in online debates (Somasundaran and Wiebe, 2009; Thomas et al., 2006; Bansal et al., 2008; Burfoot et al., 2011; Yessenalina et al., 2010). Lin and Hauptmann (2006) also proposed a method to identify opposing perspectives. Abu-Jbara et al. (2012) identified subgroups. Kim and Hovy (2007) studied election prediction by analyzing online discussion"
P13-1165,P12-1034,1,0.928487,"man, 1992), argument repertoire (Cappella et al., 2002), etc. Tolerance has also been investigated in the domain of political communications with an emphasis on political sophistication (Gastil and Dillard, 1999), civic culture (Dahlgren, 2002), and democracy (Fishkin, 1991). These existing works study tolerance from the qualitative perspective. Our focus is quantitative analysis. Sentiment analysis: Sentiment analysis determines positive or negative opinions expressed on topics (Liu, 2012; Pang and Lee, 2008). Main tasks include aspect extraction (Hu and Liu, 2004; Popescu and Etzioni, 2005; Mukherjee and Liu, 2012c; Chen et al., 2013), opinion polarity identification (Hassan and Radev, 2010; Choi and Cardie, 2010) and subjectivity analysis (Wiebe, 2000). Although related, tolerance is different from sentiment. Sentiments are mainly indicated by sentiment terms (e.g., great, good, bad, and poor). Tolerance in discussions refers to the reception of certain views and often indicated by agreement and disagreement expressions and other features (§5). Online discussions or debates: Several works put authors in debate into support and oppose camps. Agrawal et al. (2003) used a graph based method, and Murakami"
P13-1165,P12-1036,1,0.898593,"man, 1992), argument repertoire (Cappella et al., 2002), etc. Tolerance has also been investigated in the domain of political communications with an emphasis on political sophistication (Gastil and Dillard, 1999), civic culture (Dahlgren, 2002), and democracy (Fishkin, 1991). These existing works study tolerance from the qualitative perspective. Our focus is quantitative analysis. Sentiment analysis: Sentiment analysis determines positive or negative opinions expressed on topics (Liu, 2012; Pang and Lee, 2008). Main tasks include aspect extraction (Hu and Liu, 2004; Popescu and Etzioni, 2005; Mukherjee and Liu, 2012c; Chen et al., 2013), opinion polarity identification (Hassan and Radev, 2010; Choi and Cardie, 2010) and subjectivity analysis (Wiebe, 2000). Although related, tolerance is different from sentiment. Sentiments are mainly indicated by sentiment terms (e.g., great, good, bad, and poor). Tolerance in discussions refers to the reception of certain views and often indicated by agreement and disagreement expressions and other features (§5). Online discussions or debates: Several works put authors in debate into support and oppose camps. Agrawal et al. (2003) used a graph based method, and Murakami"
P13-1165,C12-1112,1,0.930715,"man, 1992), argument repertoire (Cappella et al., 2002), etc. Tolerance has also been investigated in the domain of political communications with an emphasis on political sophistication (Gastil and Dillard, 1999), civic culture (Dahlgren, 2002), and democracy (Fishkin, 1991). These existing works study tolerance from the qualitative perspective. Our focus is quantitative analysis. Sentiment analysis: Sentiment analysis determines positive or negative opinions expressed on topics (Liu, 2012; Pang and Lee, 2008). Main tasks include aspect extraction (Hu and Liu, 2004; Popescu and Etzioni, 2005; Mukherjee and Liu, 2012c; Chen et al., 2013), opinion polarity identification (Hassan and Radev, 2010; Choi and Cardie, 2010) and subjectivity analysis (Wiebe, 2000). Although related, tolerance is different from sentiment. Sentiments are mainly indicated by sentiment terms (e.g., great, good, bad, and poor). Tolerance in discussions refers to the reception of certain views and often indicated by agreement and disagreement expressions and other features (§5). Online discussions or debates: Several works put authors in debate into support and oppose camps. Agrawal et al. (2003) used a graph based method, and Murakami"
P13-1165,H05-1043,0,0.129618,"oanalysis (Slavin and Kriegman, 1992), argument repertoire (Cappella et al., 2002), etc. Tolerance has also been investigated in the domain of political communications with an emphasis on political sophistication (Gastil and Dillard, 1999), civic culture (Dahlgren, 2002), and democracy (Fishkin, 1991). These existing works study tolerance from the qualitative perspective. Our focus is quantitative analysis. Sentiment analysis: Sentiment analysis determines positive or negative opinions expressed on topics (Liu, 2012; Pang and Lee, 2008). Main tasks include aspect extraction (Hu and Liu, 2004; Popescu and Etzioni, 2005; Mukherjee and Liu, 2012c; Chen et al., 2013), opinion polarity identification (Hassan and Radev, 2010; Choi and Cardie, 2010) and subjectivity analysis (Wiebe, 2000). Although related, tolerance is different from sentiment. Sentiments are mainly indicated by sentiment terms (e.g., great, good, bad, and poor). Tolerance in discussions refers to the reception of certain views and often indicated by agreement and disagreement expressions and other features (§5). Online discussions or debates: Several works put authors in debate into support and oppose camps. Agrawal et al. (2003) used a graph b"
P13-1165,P09-1026,0,0.0721727,"sions in review comment discussions were mined, and in (Galley et al., 2004; Hillard et al., 2003), speaker utterances were classified into agreement, disagreement, and backchannel classes. Also related are studies on linguistic style accommodation (Mukherjee and Liu, 2012d) and user pair interactions (Mukherjee and Liu, 2013) in online debates. However, these works do not consider tolerance analysis in debate discussions, which is the focus of this work. 1681 In a similar vein, several classification methods have been proposed to recognize opinion stances and speaker sides in online debates (Somasundaran and Wiebe, 2009; Thomas et al., 2006; Bansal et al., 2008; Burfoot et al., 2011; Yessenalina et al., 2010). Lin and Hauptmann (2006) also proposed a method to identify opposing perspectives. Abu-Jbara et al. (2012) identified subgroups. Kim and Hovy (2007) studied election prediction by analyzing online discussions. Other related works studying dialogue and discourse in discussions include authority recognition (Mayfield and Rosè, 2011), dialogue act segmentation and classification (Morbini and Sagae, 2011; Boyer et al., 2011), discourse structure prediction (Wang et al., 2011). All these prior works are val"
P13-1165,W06-1639,0,0.133104,"sions were mined, and in (Galley et al., 2004; Hillard et al., 2003), speaker utterances were classified into agreement, disagreement, and backchannel classes. Also related are studies on linguistic style accommodation (Mukherjee and Liu, 2012d) and user pair interactions (Mukherjee and Liu, 2013) in online debates. However, these works do not consider tolerance analysis in debate discussions, which is the focus of this work. 1681 In a similar vein, several classification methods have been proposed to recognize opinion stances and speaker sides in online debates (Somasundaran and Wiebe, 2009; Thomas et al., 2006; Bansal et al., 2008; Burfoot et al., 2011; Yessenalina et al., 2010). Lin and Hauptmann (2006) also proposed a method to identify opposing perspectives. Abu-Jbara et al. (2012) identified subgroups. Kim and Hovy (2007) studied election prediction by analyzing online discussions. Other related works studying dialogue and discourse in discussions include authority recognition (Mayfield and Rosè, 2011), dialogue act segmentation and classification (Morbini and Sagae, 2011; Boyer et al., 2011), discourse structure prediction (Wang et al., 2011). All these prior works are valuable. But they are n"
P13-1165,D11-1002,0,0.0294352,"s in online debates (Somasundaran and Wiebe, 2009; Thomas et al., 2006; Bansal et al., 2008; Burfoot et al., 2011; Yessenalina et al., 2010). Lin and Hauptmann (2006) also proposed a method to identify opposing perspectives. Abu-Jbara et al. (2012) identified subgroups. Kim and Hovy (2007) studied election prediction by analyzing online discussions. Other related works studying dialogue and discourse in discussions include authority recognition (Mayfield and Rosè, 2011), dialogue act segmentation and classification (Morbini and Sagae, 2011; Boyer et al., 2011), discourse structure prediction (Wang et al., 2011). All these prior works are valuable. But they are not designed to identify tolerance or to analyze tipping points of disagreements for intolerance in discussions which are the focus of this work. 3 Discussion/Debate Data For this research, we used discussion posts from Volconvo.com. This forum is divided into various domains: Politics, Religion, Science, etc. Each domain consists of multiple discussion threads. Each thread consists of a list of posts. Our experimental data is from two domains, Politics and Religion. The data is summarized in Table 1(a). In this work, the terms users, authors"
P13-1165,D10-1102,0,0.0121816,"003), speaker utterances were classified into agreement, disagreement, and backchannel classes. Also related are studies on linguistic style accommodation (Mukherjee and Liu, 2012d) and user pair interactions (Mukherjee and Liu, 2013) in online debates. However, these works do not consider tolerance analysis in debate discussions, which is the focus of this work. 1681 In a similar vein, several classification methods have been proposed to recognize opinion stances and speaker sides in online debates (Somasundaran and Wiebe, 2009; Thomas et al., 2006; Bansal et al., 2008; Burfoot et al., 2011; Yessenalina et al., 2010). Lin and Hauptmann (2006) also proposed a method to identify opposing perspectives. Abu-Jbara et al. (2012) identified subgroups. Kim and Hovy (2007) studied election prediction by analyzing online discussions. Other related works studying dialogue and discourse in discussions include authority recognition (Mayfield and Rosè, 2011), dialogue act segmentation and classification (Morbini and Sagae, 2011; Boyer et al., 2011), discourse structure prediction (Wang et al., 2011). All these prior works are valuable. But they are not designed to identify tolerance or to analyze tipping points of disa"
P13-1165,D10-1006,0,0.304026,"Sunstein, 2002), and the hostile media effect, a scenario where partisans rigidly hold on to their stances (Hansen and Hyunjung, 2011). 2 Agreement levels are as follows. ? ∈ [0, 0.2]: Poor, ? ∈ (0.2, 0.4]:Fair, ? ∈ (0.4, 0.6]: Moderate, ? ∈ (0.6, 0.8]: Substantial, and ? ∈ (0.8, 1.0]: Almost perfect agreement. 1682 r z θT αT w βT φT T ψ x αE θE Na, d φE In this work, a document/post is viewed as a bag of n-grams and we use terms to denote both words (unigrams) and phrases (n-grams) 3. DTM is a switching graphical model performing a switch between topics and AD-expressions similar to that in (Zhao et al., 2010). The switch is done using a learned maximum entropy (MaxEnt) model. The rationale here is that topical and AD-expression terms usually play different syntactic roles in a sentence. Topical terms (e.g., “U.S. elections,” “government,” “income tax”) tend to be noun and noun phrases while expression terms (“I refute,” “how can you say,” “I’d agree”) usually contain pronouns, verbs, whdeterminers, and modals. In order to utilize the part-of-speech (POS) tag information, we place the topic/AD-expression distribution, ??,?,? (the prior over the indicator variable ??,?,? ) in the term plate (Figure"
P13-1165,H05-2017,0,\N,Missing
P13-1165,P04-1085,0,\N,Missing
P13-2005,P12-1036,1,0.367162,"eed to Model We now present our stock prediction framework. 3.1 (1) ( ) Topic modeling as a task of corpus exploration has attracted significant attention in recent years. One of the basic and most widely used models is Latent Dirichlet Allocation (LDA) (Blei et al., 2003). LDA can learn a predefined number of topics and has been widely applied in its extended forms in sentiment analysis and many other tasks (Mei et al., 2007; Branavan et al., 2008; Lin and He, 2009; Zhao et al., 2010; Wang et al., 2010; Brody and Elhadad, 2010; Jo and Oh, 2011; Moghaddam and Ester, 2011; Sauper et al., 2011; Mukherjee and Liu, 2012; He et al., 2012). The Dirichlet Processes Mixture (DPM) model is a non-parametric extension of LDA (Teh et al., 2006), which can estimate the number of topics inherent in the data itself. In this work, we employ topic based sentiment analysis using DPM on Twitter posts (or tweets). First, we employ a DPM to estimate the number of topics in the streaming snapshot of tweets in each day. Next, we build a sentiment time series based on the estimated topics of daily tweets. Lastly, we regress the stock index and the sentiment time series in an autoregressive framework. 3 ) where is a data point,"
P13-2005,P08-1031,0,0.0176645,"10; Teh et al., 2006) which have been shown to work well. Because a tweet has at most 140 characters, we assume that each tweet contains only one topic. Hence, we only need to Model We now present our stock prediction framework. 3.1 (1) ( ) Topic modeling as a task of corpus exploration has attracted significant attention in recent years. One of the basic and most widely used models is Latent Dirichlet Allocation (LDA) (Blei et al., 2003). LDA can learn a predefined number of topics and has been widely applied in its extended forms in sentiment analysis and many other tasks (Mei et al., 2007; Branavan et al., 2008; Lin and He, 2009; Zhao et al., 2010; Wang et al., 2010; Brody and Elhadad, 2010; Jo and Oh, 2011; Moghaddam and Ester, 2011; Sauper et al., 2011; Mukherjee and Liu, 2012; He et al., 2012). The Dirichlet Processes Mixture (DPM) model is a non-parametric extension of LDA (Teh et al., 2006), which can estimate the number of topics inherent in the data itself. In this work, we employ topic based sentiment analysis using DPM on Twitter posts (or tweets). First, we employ a DPM to estimate the number of topics in the streaming snapshot of tweets in each day. Next, we build a sentiment time series"
P13-2005,N10-1122,0,0.0207905,"t most 140 characters, we assume that each tweet contains only one topic. Hence, we only need to Model We now present our stock prediction framework. 3.1 (1) ( ) Topic modeling as a task of corpus exploration has attracted significant attention in recent years. One of the basic and most widely used models is Latent Dirichlet Allocation (LDA) (Blei et al., 2003). LDA can learn a predefined number of topics and has been widely applied in its extended forms in sentiment analysis and many other tasks (Mei et al., 2007; Branavan et al., 2008; Lin and He, 2009; Zhao et al., 2010; Wang et al., 2010; Brody and Elhadad, 2010; Jo and Oh, 2011; Moghaddam and Ester, 2011; Sauper et al., 2011; Mukherjee and Liu, 2012; He et al., 2012). The Dirichlet Processes Mixture (DPM) model is a non-parametric extension of LDA (Teh et al., 2006), which can estimate the number of topics inherent in the data itself. In this work, we employ topic based sentiment analysis using DPM on Twitter posts (or tweets). First, we employ a DPM to estimate the number of topics in the streaming snapshot of tweets in each day. Next, we build a sentiment time series based on the estimated topics of daily tweets. Lastly, we regress the stock index"
P13-2005,P11-1036,0,0.00826354,"pic. Hence, we only need to Model We now present our stock prediction framework. 3.1 (1) ( ) Topic modeling as a task of corpus exploration has attracted significant attention in recent years. One of the basic and most widely used models is Latent Dirichlet Allocation (LDA) (Blei et al., 2003). LDA can learn a predefined number of topics and has been widely applied in its extended forms in sentiment analysis and many other tasks (Mei et al., 2007; Branavan et al., 2008; Lin and He, 2009; Zhao et al., 2010; Wang et al., 2010; Brody and Elhadad, 2010; Jo and Oh, 2011; Moghaddam and Ester, 2011; Sauper et al., 2011; Mukherjee and Liu, 2012; He et al., 2012). The Dirichlet Processes Mixture (DPM) model is a non-parametric extension of LDA (Teh et al., 2006), which can estimate the number of topics inherent in the data itself. In this work, we employ topic based sentiment analysis using DPM on Twitter posts (or tweets). First, we employ a DPM to estimate the number of topics in the streaming snapshot of tweets in each day. Next, we build a sentiment time series based on the estimated topics of daily tweets. Lastly, we regress the stock index and the sentiment time series in an autoregressive framework. 3"
P13-2005,D10-1006,0,0.0337199,"wn to work well. Because a tweet has at most 140 characters, we assume that each tweet contains only one topic. Hence, we only need to Model We now present our stock prediction framework. 3.1 (1) ( ) Topic modeling as a task of corpus exploration has attracted significant attention in recent years. One of the basic and most widely used models is Latent Dirichlet Allocation (LDA) (Blei et al., 2003). LDA can learn a predefined number of topics and has been widely applied in its extended forms in sentiment analysis and many other tasks (Mei et al., 2007; Branavan et al., 2008; Lin and He, 2009; Zhao et al., 2010; Wang et al., 2010; Brody and Elhadad, 2010; Jo and Oh, 2011; Moghaddam and Ester, 2011; Sauper et al., 2011; Mukherjee and Liu, 2012; He et al., 2012). The Dirichlet Processes Mixture (DPM) model is a non-parametric extension of LDA (Teh et al., 2006), which can estimate the number of topics inherent in the data itself. In this work, we employ topic based sentiment analysis using DPM on Twitter posts (or tweets). First, we employ a DPM to estimate the number of topics in the streaming snapshot of tweets in each day. Next, we build a sentiment time series based on the estimated topics of dail"
P14-1033,D10-1124,0,0.0606798,"Missing"
P14-1033,P12-2065,0,0.0292422,"u and Etzioni, 2005, Qiu et al., 2011, Somasundaran and Wiebe, 2009, Wu et al., 2009, Xu et al., 2013, Yu et al., 2011, Zhao et al., 2012, Zhou et al., 2013, Zhuang et al., 2006). However, these works only perform extraction but not aspect term grouping or resolution. Separate aspect term grouping has been done in (Carenini et al., 2005, Guo et al., 2009, Zhai et al., 2011). They assume that aspect terms have been extracted beforehand. To extract and group aspects simultaneously, topic models have been applied by researchers (Branavan et al., 2008, Brody and Elhadad, 2010, Chen et al., 2013b, Fang and Huang, 2012, He et al., 2011, Jo and Oh, 2011, Kim et al., 2013, Lazaridou et al., 2013, Li et al., 2011, Lin and He, 2009, Lu et al., 2009, Lu et al., 2012, Lu and Zhai, 2008, Mei et al., 2007, Moghaddam and Ester, 2013, Mukherjee and Liu, 2012, Sauper and Barzilay, 2013, Titov and McDonald, 2008, Wang et al., 2010, Zhao et al., 2010). Our proposed AKL model belongs to the class of knowledge-based topic models. Besides the knowledge-based topic models discussed in Section 1, document labels are incorporated as implicit knowledge in (Blei and McAuliffe, 2007, Ramage et al., 2009). Geographical region kno"
P14-1033,P11-1013,0,0.0492429,"iu et al., 2011, Somasundaran and Wiebe, 2009, Wu et al., 2009, Xu et al., 2013, Yu et al., 2011, Zhao et al., 2012, Zhou et al., 2013, Zhuang et al., 2006). However, these works only perform extraction but not aspect term grouping or resolution. Separate aspect term grouping has been done in (Carenini et al., 2005, Guo et al., 2009, Zhai et al., 2011). They assume that aspect terms have been extracted beforehand. To extract and group aspects simultaneously, topic models have been applied by researchers (Branavan et al., 2008, Brody and Elhadad, 2010, Chen et al., 2013b, Fang and Huang, 2012, He et al., 2011, Jo and Oh, 2011, Kim et al., 2013, Lazaridou et al., 2013, Li et al., 2011, Lin and He, 2009, Lu et al., 2009, Lu et al., 2012, Lu and Zhai, 2008, Mei et al., 2007, Moghaddam and Ester, 2013, Mukherjee and Liu, 2012, Sauper and Barzilay, 2013, Titov and McDonald, 2008, Wang et al., 2010, Zhao et al., 2010). Our proposed AKL model belongs to the class of knowledge-based topic models. Besides the knowledge-based topic models discussed in Section 1, document labels are incorporated as implicit knowledge in (Blei and McAuliffe, 2007, Ramage et al., 2009). Geographical region knowledge has also b"
P14-1033,P08-1031,0,0.0406513,"atterns (Hu and Liu, 2004, Ku et al., 2006, Liu et al., 2013, Popescu and Etzioni, 2005, Qiu et al., 2011, Somasundaran and Wiebe, 2009, Wu et al., 2009, Xu et al., 2013, Yu et al., 2011, Zhao et al., 2012, Zhou et al., 2013, Zhuang et al., 2006). However, these works only perform extraction but not aspect term grouping or resolution. Separate aspect term grouping has been done in (Carenini et al., 2005, Guo et al., 2009, Zhai et al., 2011). They assume that aspect terms have been extracted beforehand. To extract and group aspects simultaneously, topic models have been applied by researchers (Branavan et al., 2008, Brody and Elhadad, 2010, Chen et al., 2013b, Fang and Huang, 2012, He et al., 2011, Jo and Oh, 2011, Kim et al., 2013, Lazaridou et al., 2013, Li et al., 2011, Lin and He, 2009, Lu et al., 2009, Lu et al., 2012, Lu and Zhai, 2008, Mei et al., 2007, Moghaddam and Ester, 2013, Mukherjee and Liu, 2012, Sauper and Barzilay, 2013, Titov and McDonald, 2008, Wang et al., 2010, Zhao et al., 2010). Our proposed AKL model belongs to the class of knowledge-based topic models. Besides the knowledge-based topic models discussed in Section 1, document labels are incorporated as implicit knowledge in (Blei"
P14-1033,P11-1026,0,0.0219019,"l semi-supervised topic models, also called knowledge-based topic models, have been proposed. DF-LDA (Andrzejewski et al., 2009) can incorporate two forms of prior knowledge from the user: must-links and cannot-links. A must-link implies that two terms (or words) should belong to the same topic whereas a cannot-link indicates that two terms should not be in the same topic. In a similar but more generic vein, must-sets and cannot-sets are used in MC-LDA (Chen et al., 2013b). Other related works include (Andrzejewski et al., 2011, Chen et al., 2013a, Chen et al., 2013c, Mukherjee and Liu, 2012, Hu et al., 2011, Jagarlamudi et al., 2012, Lu et al., 2011, Petterson et al., 2010). They all allow prior knowledge to be specified by the user to guide the modeling process. In this paper, we take a major step further. We mine the prior knowledge directly from a large amount of relevant data without any user intervention, and thus make this approach fully automatic. We hypothesize that it is possible to learn quality prior knowledge from the big data (of reviews) available on the Web. The intuition is that although every domain is different, there is a decent amount of aspect overlapping across domains. For"
P14-1033,N10-1122,0,0.0631702,"04, Ku et al., 2006, Liu et al., 2013, Popescu and Etzioni, 2005, Qiu et al., 2011, Somasundaran and Wiebe, 2009, Wu et al., 2009, Xu et al., 2013, Yu et al., 2011, Zhao et al., 2012, Zhou et al., 2013, Zhuang et al., 2006). However, these works only perform extraction but not aspect term grouping or resolution. Separate aspect term grouping has been done in (Carenini et al., 2005, Guo et al., 2009, Zhai et al., 2011). They assume that aspect terms have been extracted beforehand. To extract and group aspects simultaneously, topic models have been applied by researchers (Branavan et al., 2008, Brody and Elhadad, 2010, Chen et al., 2013b, Fang and Huang, 2012, He et al., 2011, Jo and Oh, 2011, Kim et al., 2013, Lazaridou et al., 2013, Li et al., 2011, Lin and He, 2009, Lu et al., 2009, Lu et al., 2012, Lu and Zhai, 2008, Mei et al., 2007, Moghaddam and Ester, 2013, Mukherjee and Liu, 2012, Sauper and Barzilay, 2013, Titov and McDonald, 2008, Wang et al., 2010, Zhao et al., 2010). Our proposed AKL model belongs to the class of knowledge-based topic models. Besides the knowledge-based topic models discussed in Section 1, document labels are incorporated as implicit knowledge in (Blei and McAuliffe, 2007, Ram"
P14-1033,E12-1021,0,0.0803209,"Missing"
P14-1033,D10-1101,0,0.0142552,"n each of these domains, as well as in new domains. Our proposed method aims to achieve this objective. There are two major challenges: (1) learning quality knowledge from a large number of domains, and (2) making the extraction model fault-tolerant, i.e., capable of handling possibly incorrect learned knowledge. We briefly introduce the proposed method below, which consists of two steps. 2 Related Work Aspect extraction has been studied by many researchers in sentiment analysis (Liu, 2012, Pang and Lee, 2008), e.g., using supervised sequence labeling or classification (Choi and Cardie, 2010, Jakob and Gurevych, 2010, Kobayashi et al., 2007, Li et al., 2010, Yang and Cardie, 2013) and using word frequency and syntactic patterns (Hu and Liu, 2004, Ku et al., 2006, Liu et al., 2013, Popescu and Etzioni, 2005, Qiu et al., 2011, Somasundaran and Wiebe, 2009, Wu et al., 2009, Xu et al., 2013, Yu et al., 2011, Zhao et al., 2012, Zhou et al., 2013, Zhuang et al., 2006). However, these works only perform extraction but not aspect term grouping or resolution. Separate aspect term grouping has been done in (Carenini et al., 2005, Guo et al., 2009, Zhai et al., 2011). They assume that aspect terms have been extracte"
P14-1033,D13-1172,1,0.182853,"the objective functions of topic models do not always correlate well with human judgments (Chang et al., 2009). To tackle the problem, several semi-supervised topic models, also called knowledge-based topic models, have been proposed. DF-LDA (Andrzejewski et al., 2009) can incorporate two forms of prior knowledge from the user: must-links and cannot-links. A must-link implies that two terms (or words) should belong to the same topic whereas a cannot-link indicates that two terms should not be in the same topic. In a similar but more generic vein, must-sets and cannot-sets are used in MC-LDA (Chen et al., 2013b). Other related works include (Andrzejewski et al., 2011, Chen et al., 2013a, Chen et al., 2013c, Mukherjee and Liu, 2012, Hu et al., 2011, Jagarlamudi et al., 2012, Lu et al., 2011, Petterson et al., 2010). They all allow prior knowledge to be specified by the user to guide the modeling process. In this paper, we take a major step further. We mine the prior knowledge directly from a large amount of relevant data without any user intervention, and thus make this approach fully automatic. We hypothesize that it is possible to learn quality prior knowledge from the big data (of reviews) availa"
P14-1033,D07-1114,0,0.00870534,"s well as in new domains. Our proposed method aims to achieve this objective. There are two major challenges: (1) learning quality knowledge from a large number of domains, and (2) making the extraction model fault-tolerant, i.e., capable of handling possibly incorrect learned knowledge. We briefly introduce the proposed method below, which consists of two steps. 2 Related Work Aspect extraction has been studied by many researchers in sentiment analysis (Liu, 2012, Pang and Lee, 2008), e.g., using supervised sequence labeling or classification (Choi and Cardie, 2010, Jakob and Gurevych, 2010, Kobayashi et al., 2007, Li et al., 2010, Yang and Cardie, 2013) and using word frequency and syntactic patterns (Hu and Liu, 2004, Ku et al., 2006, Liu et al., 2013, Popescu and Etzioni, 2005, Qiu et al., 2011, Somasundaran and Wiebe, 2009, Wu et al., 2009, Xu et al., 2013, Yu et al., 2011, Zhao et al., 2012, Zhou et al., 2013, Zhuang et al., 2006). However, these works only perform extraction but not aspect term grouping or resolution. Separate aspect term grouping has been done in (Carenini et al., 2005, Guo et al., 2009, Zhai et al., 2011). They assume that aspect terms have been extracted beforehand. To extract"
P14-1033,P10-2050,0,0.0151496,"ove aspect extraction in each of these domains, as well as in new domains. Our proposed method aims to achieve this objective. There are two major challenges: (1) learning quality knowledge from a large number of domains, and (2) making the extraction model fault-tolerant, i.e., capable of handling possibly incorrect learned knowledge. We briefly introduce the proposed method below, which consists of two steps. 2 Related Work Aspect extraction has been studied by many researchers in sentiment analysis (Liu, 2012, Pang and Lee, 2008), e.g., using supervised sequence labeling or classification (Choi and Cardie, 2010, Jakob and Gurevych, 2010, Kobayashi et al., 2007, Li et al., 2010, Yang and Cardie, 2013) and using word frequency and syntactic patterns (Hu and Liu, 2004, Ku et al., 2006, Liu et al., 2013, Popescu and Etzioni, 2005, Qiu et al., 2011, Somasundaran and Wiebe, 2009, Wu et al., 2009, Xu et al., 2013, Yu et al., 2011, Zhao et al., 2012, Zhou et al., 2013, Zhuang et al., 2006). However, these works only perform extraction but not aspect term grouping or resolution. Separate aspect term grouping has been done in (Carenini et al., 2005, Guo et al., 2009, Zhai et al., 2011). They assume that aspec"
P14-1033,P12-1036,1,0.899734,"ackle the problem, several semi-supervised topic models, also called knowledge-based topic models, have been proposed. DF-LDA (Andrzejewski et al., 2009) can incorporate two forms of prior knowledge from the user: must-links and cannot-links. A must-link implies that two terms (or words) should belong to the same topic whereas a cannot-link indicates that two terms should not be in the same topic. In a similar but more generic vein, must-sets and cannot-sets are used in MC-LDA (Chen et al., 2013b). Other related works include (Andrzejewski et al., 2011, Chen et al., 2013a, Chen et al., 2013c, Mukherjee and Liu, 2012, Hu et al., 2011, Jagarlamudi et al., 2012, Lu et al., 2011, Petterson et al., 2010). They all allow prior knowledge to be specified by the user to guide the modeling process. In this paper, we take a major step further. We mine the prior knowledge directly from a large amount of relevant data without any user intervention, and thus make this approach fully automatic. We hypothesize that it is possible to learn quality prior knowledge from the big data (of reviews) available on the Web. The intuition is that although every domain is different, there is a decent amount of aspect overlapping ac"
P14-1033,P13-1160,0,0.0135138,"al., 2009, Xu et al., 2013, Yu et al., 2011, Zhao et al., 2012, Zhou et al., 2013, Zhuang et al., 2006). However, these works only perform extraction but not aspect term grouping or resolution. Separate aspect term grouping has been done in (Carenini et al., 2005, Guo et al., 2009, Zhai et al., 2011). They assume that aspect terms have been extracted beforehand. To extract and group aspects simultaneously, topic models have been applied by researchers (Branavan et al., 2008, Brody and Elhadad, 2010, Chen et al., 2013b, Fang and Huang, 2012, He et al., 2011, Jo and Oh, 2011, Kim et al., 2013, Lazaridou et al., 2013, Li et al., 2011, Lin and He, 2009, Lu et al., 2009, Lu et al., 2012, Lu and Zhai, 2008, Mei et al., 2007, Moghaddam and Ester, 2013, Mukherjee and Liu, 2012, Sauper and Barzilay, 2013, Titov and McDonald, 2008, Wang et al., 2010, Zhao et al., 2010). Our proposed AKL model belongs to the class of knowledge-based topic models. Besides the knowledge-based topic models discussed in Section 1, document labels are incorporated as implicit knowledge in (Blei and McAuliffe, 2007, Ramage et al., 2009). Geographical region knowledge has also been considered in topic models (Eisenstein et al., 2010). A"
P14-1033,C10-1074,0,0.236842,". Our proposed method aims to achieve this objective. There are two major challenges: (1) learning quality knowledge from a large number of domains, and (2) making the extraction model fault-tolerant, i.e., capable of handling possibly incorrect learned knowledge. We briefly introduce the proposed method below, which consists of two steps. 2 Related Work Aspect extraction has been studied by many researchers in sentiment analysis (Liu, 2012, Pang and Lee, 2008), e.g., using supervised sequence labeling or classification (Choi and Cardie, 2010, Jakob and Gurevych, 2010, Kobayashi et al., 2007, Li et al., 2010, Yang and Cardie, 2013) and using word frequency and syntactic patterns (Hu and Liu, 2004, Ku et al., 2006, Liu et al., 2013, Popescu and Etzioni, 2005, Qiu et al., 2011, Somasundaran and Wiebe, 2009, Wu et al., 2009, Xu et al., 2013, Yu et al., 2011, Zhao et al., 2012, Zhou et al., 2013, Zhuang et al., 2006). However, these works only perform extraction but not aspect term grouping or resolution. Separate aspect term grouping has been done in (Carenini et al., 2005, Guo et al., 2009, Zhai et al., 2011). They assume that aspect terms have been extracted beforehand. To extract and group aspect"
P14-1033,D11-1105,0,0.0129355,"Missing"
P14-1033,H05-1043,0,0.237808,"mains, and (2) making the extraction model fault-tolerant, i.e., capable of handling possibly incorrect learned knowledge. We briefly introduce the proposed method below, which consists of two steps. 2 Related Work Aspect extraction has been studied by many researchers in sentiment analysis (Liu, 2012, Pang and Lee, 2008), e.g., using supervised sequence labeling or classification (Choi and Cardie, 2010, Jakob and Gurevych, 2010, Kobayashi et al., 2007, Li et al., 2010, Yang and Cardie, 2013) and using word frequency and syntactic patterns (Hu and Liu, 2004, Ku et al., 2006, Liu et al., 2013, Popescu and Etzioni, 2005, Qiu et al., 2011, Somasundaran and Wiebe, 2009, Wu et al., 2009, Xu et al., 2013, Yu et al., 2011, Zhao et al., 2012, Zhou et al., 2013, Zhuang et al., 2006). However, these works only perform extraction but not aspect term grouping or resolution. Separate aspect term grouping has been done in (Carenini et al., 2005, Guo et al., 2009, Zhai et al., 2011). They assume that aspect terms have been extracted beforehand. To extract and group aspects simultaneously, topic models have been applied by researchers (Branavan et al., 2008, Brody and Elhadad, 2010, Chen et al., 2013b, Fang and Huang, 201"
P14-1033,J11-1002,1,0.395475,"xtraction model fault-tolerant, i.e., capable of handling possibly incorrect learned knowledge. We briefly introduce the proposed method below, which consists of two steps. 2 Related Work Aspect extraction has been studied by many researchers in sentiment analysis (Liu, 2012, Pang and Lee, 2008), e.g., using supervised sequence labeling or classification (Choi and Cardie, 2010, Jakob and Gurevych, 2010, Kobayashi et al., 2007, Li et al., 2010, Yang and Cardie, 2013) and using word frequency and syntactic patterns (Hu and Liu, 2004, Ku et al., 2006, Liu et al., 2013, Popescu and Etzioni, 2005, Qiu et al., 2011, Somasundaran and Wiebe, 2009, Wu et al., 2009, Xu et al., 2013, Yu et al., 2011, Zhao et al., 2012, Zhou et al., 2013, Zhuang et al., 2006). However, these works only perform extraction but not aspect term grouping or resolution. Separate aspect term grouping has been done in (Carenini et al., 2005, Guo et al., 2009, Zhai et al., 2011). They assume that aspect terms have been extracted beforehand. To extract and group aspects simultaneously, topic models have been applied by researchers (Branavan et al., 2008, Brody and Elhadad, 2010, Chen et al., 2013b, Fang and Huang, 2012, He et al., 2011"
P14-1033,P13-1172,0,0.00972808,"large number of domains, and (2) making the extraction model fault-tolerant, i.e., capable of handling possibly incorrect learned knowledge. We briefly introduce the proposed method below, which consists of two steps. 2 Related Work Aspect extraction has been studied by many researchers in sentiment analysis (Liu, 2012, Pang and Lee, 2008), e.g., using supervised sequence labeling or classification (Choi and Cardie, 2010, Jakob and Gurevych, 2010, Kobayashi et al., 2007, Li et al., 2010, Yang and Cardie, 2013) and using word frequency and syntactic patterns (Hu and Liu, 2004, Ku et al., 2006, Liu et al., 2013, Popescu and Etzioni, 2005, Qiu et al., 2011, Somasundaran and Wiebe, 2009, Wu et al., 2009, Xu et al., 2013, Yu et al., 2011, Zhao et al., 2012, Zhou et al., 2013, Zhuang et al., 2006). However, these works only perform extraction but not aspect term grouping or resolution. Separate aspect term grouping has been done in (Carenini et al., 2005, Guo et al., 2009, Zhai et al., 2011). They assume that aspect terms have been extracted beforehand. To extract and group aspects simultaneously, topic models have been applied by researchers (Branavan et al., 2008, Brody and Elhadad, 2010, Chen et al.,"
P14-1033,D09-1026,0,0.0580087,"010, Chen et al., 2013b, Fang and Huang, 2012, He et al., 2011, Jo and Oh, 2011, Kim et al., 2013, Lazaridou et al., 2013, Li et al., 2011, Lin and He, 2009, Lu et al., 2009, Lu et al., 2012, Lu and Zhai, 2008, Mei et al., 2007, Moghaddam and Ester, 2013, Mukherjee and Liu, 2012, Sauper and Barzilay, 2013, Titov and McDonald, 2008, Wang et al., 2010, Zhao et al., 2010). Our proposed AKL model belongs to the class of knowledge-based topic models. Besides the knowledge-based topic models discussed in Section 1, document labels are incorporated as implicit knowledge in (Blei and McAuliffe, 2007, Ramage et al., 2009). Geographical region knowledge has also been considered in topic models (Eisenstein et al., 2010). All of these models assume that the prior knowledge is correct. GK-LDA (Chen et al., 2013a) is the only knowledge-based topic model that deals with wrong lexical knowledge to some extent. As we will see in Section 6, AKL outperformed GKLDA significantly due to AKL’s more effective error handling mechanism. Furthermore, GK-LDA does not learn any prior knowledge. Our work is also related to transfer learning to some extent. Topic models have been used to help Learning quality knowledge: Clearly, l"
P14-1033,P09-1026,0,0.027916,"ult-tolerant, i.e., capable of handling possibly incorrect learned knowledge. We briefly introduce the proposed method below, which consists of two steps. 2 Related Work Aspect extraction has been studied by many researchers in sentiment analysis (Liu, 2012, Pang and Lee, 2008), e.g., using supervised sequence labeling or classification (Choi and Cardie, 2010, Jakob and Gurevych, 2010, Kobayashi et al., 2007, Li et al., 2010, Yang and Cardie, 2013) and using word frequency and syntactic patterns (Hu and Liu, 2004, Ku et al., 2006, Liu et al., 2013, Popescu and Etzioni, 2005, Qiu et al., 2011, Somasundaran and Wiebe, 2009, Wu et al., 2009, Xu et al., 2013, Yu et al., 2011, Zhao et al., 2012, Zhou et al., 2013, Zhuang et al., 2006). However, these works only perform extraction but not aspect term grouping or resolution. Separate aspect term grouping has been done in (Carenini et al., 2005, Guo et al., 2009, Zhai et al., 2011). They assume that aspect terms have been extracted beforehand. To extract and group aspects simultaneously, topic models have been applied by researchers (Branavan et al., 2008, Brody and Elhadad, 2010, Chen et al., 2013b, Fang and Huang, 2012, He et al., 2011, Jo and Oh, 2011, Kim et al.,"
P14-1033,P08-1036,0,0.23456,"t term grouping has been done in (Carenini et al., 2005, Guo et al., 2009, Zhai et al., 2011). They assume that aspect terms have been extracted beforehand. To extract and group aspects simultaneously, topic models have been applied by researchers (Branavan et al., 2008, Brody and Elhadad, 2010, Chen et al., 2013b, Fang and Huang, 2012, He et al., 2011, Jo and Oh, 2011, Kim et al., 2013, Lazaridou et al., 2013, Li et al., 2011, Lin and He, 2009, Lu et al., 2009, Lu et al., 2012, Lu and Zhai, 2008, Mei et al., 2007, Moghaddam and Ester, 2013, Mukherjee and Liu, 2012, Sauper and Barzilay, 2013, Titov and McDonald, 2008, Wang et al., 2010, Zhao et al., 2010). Our proposed AKL model belongs to the class of knowledge-based topic models. Besides the knowledge-based topic models discussed in Section 1, document labels are incorporated as implicit knowledge in (Blei and McAuliffe, 2007, Ramage et al., 2009). Geographical region knowledge has also been considered in topic models (Eisenstein et al., 2010). All of these models assume that the prior knowledge is correct. GK-LDA (Chen et al., 2013a) is the only knowledge-based topic model that deals with wrong lexical knowledge to some extent. As we will see in Sectio"
P14-1033,D09-1159,0,0.188077,"handling possibly incorrect learned knowledge. We briefly introduce the proposed method below, which consists of two steps. 2 Related Work Aspect extraction has been studied by many researchers in sentiment analysis (Liu, 2012, Pang and Lee, 2008), e.g., using supervised sequence labeling or classification (Choi and Cardie, 2010, Jakob and Gurevych, 2010, Kobayashi et al., 2007, Li et al., 2010, Yang and Cardie, 2013) and using word frequency and syntactic patterns (Hu and Liu, 2004, Ku et al., 2006, Liu et al., 2013, Popescu and Etzioni, 2005, Qiu et al., 2011, Somasundaran and Wiebe, 2009, Wu et al., 2009, Xu et al., 2013, Yu et al., 2011, Zhao et al., 2012, Zhou et al., 2013, Zhuang et al., 2006). However, these works only perform extraction but not aspect term grouping or resolution. Separate aspect term grouping has been done in (Carenini et al., 2005, Guo et al., 2009, Zhai et al., 2011). They assume that aspect terms have been extracted beforehand. To extract and group aspects simultaneously, topic models have been applied by researchers (Branavan et al., 2008, Brody and Elhadad, 2010, Chen et al., 2013b, Fang and Huang, 2012, He et al., 2011, Jo and Oh, 2011, Kim et al., 2013, Lazaridou"
P14-1033,D11-1024,0,0.396219,"ation 1 below). If ci corroborates wi well, ci is likely to be useful, and thus should also provide guidance in determining zi . Otherwise, ci may not be a suitable piece of knowledge for wi in the domain. 2. Agreement between ci and zi . By agreement we mean the degree that the terms (union of all frequent 2-patterns of ci ) in cluster ci are reflected in topic zi . Unlike the first factor, this is a global factor as it concerns all the terms in a knowledge cluster. For the first factor, we measure how well ci corroborates wi given the corpus based on codocument frequency ratio. As shown in (Mimno et al., 2011), co-document frequency is a good indicator of term correlation in a domain. Following (Mimno et al., 2011), we define a symmetric co-document frequency ratio as follows: w Nm M β ψ T φ TXC γ Figure 2: Plate notation for AKL. a term w given topic z and cluster c. This plate notation of AKL and its associated generative process are similar to those of MC-LDA (Chen et al., 2013b). However, there are three key differences. 1. Our knowledge is automatically mined which may have errors (or noises), while the prior knowledge for MC-LDA is manually provided and assumed to be correct. As we will see i"
P14-1033,P13-1173,0,0.0319492,"y incorrect learned knowledge. We briefly introduce the proposed method below, which consists of two steps. 2 Related Work Aspect extraction has been studied by many researchers in sentiment analysis (Liu, 2012, Pang and Lee, 2008), e.g., using supervised sequence labeling or classification (Choi and Cardie, 2010, Jakob and Gurevych, 2010, Kobayashi et al., 2007, Li et al., 2010, Yang and Cardie, 2013) and using word frequency and syntactic patterns (Hu and Liu, 2004, Ku et al., 2006, Liu et al., 2013, Popescu and Etzioni, 2005, Qiu et al., 2011, Somasundaran and Wiebe, 2009, Wu et al., 2009, Xu et al., 2013, Yu et al., 2011, Zhao et al., 2012, Zhou et al., 2013, Zhuang et al., 2006). However, these works only perform extraction but not aspect term grouping or resolution. Separate aspect term grouping has been done in (Carenini et al., 2005, Guo et al., 2009, Zhai et al., 2011). They assume that aspect terms have been extracted beforehand. To extract and group aspects simultaneously, topic models have been applied by researchers (Branavan et al., 2008, Brody and Elhadad, 2010, Chen et al., 2013b, Fang and Huang, 2012, He et al., 2011, Jo and Oh, 2011, Kim et al., 2013, Lazaridou et al., 2013, Li"
P14-1033,P13-1161,0,0.0101688,"thod aims to achieve this objective. There are two major challenges: (1) learning quality knowledge from a large number of domains, and (2) making the extraction model fault-tolerant, i.e., capable of handling possibly incorrect learned knowledge. We briefly introduce the proposed method below, which consists of two steps. 2 Related Work Aspect extraction has been studied by many researchers in sentiment analysis (Liu, 2012, Pang and Lee, 2008), e.g., using supervised sequence labeling or classification (Choi and Cardie, 2010, Jakob and Gurevych, 2010, Kobayashi et al., 2007, Li et al., 2010, Yang and Cardie, 2013) and using word frequency and syntactic patterns (Hu and Liu, 2004, Ku et al., 2006, Liu et al., 2013, Popescu and Etzioni, 2005, Qiu et al., 2011, Somasundaran and Wiebe, 2009, Wu et al., 2009, Xu et al., 2013, Yu et al., 2011, Zhao et al., 2012, Zhou et al., 2013, Zhuang et al., 2006). However, these works only perform extraction but not aspect term grouping or resolution. Separate aspect term grouping has been done in (Carenini et al., 2005, Guo et al., 2009, Zhai et al., 2011). They assume that aspect terms have been extracted beforehand. To extract and group aspects simultaneously, topic"
P14-1033,P11-1150,0,0.032786,"ed knowledge. We briefly introduce the proposed method below, which consists of two steps. 2 Related Work Aspect extraction has been studied by many researchers in sentiment analysis (Liu, 2012, Pang and Lee, 2008), e.g., using supervised sequence labeling or classification (Choi and Cardie, 2010, Jakob and Gurevych, 2010, Kobayashi et al., 2007, Li et al., 2010, Yang and Cardie, 2013) and using word frequency and syntactic patterns (Hu and Liu, 2004, Ku et al., 2006, Liu et al., 2013, Popescu and Etzioni, 2005, Qiu et al., 2011, Somasundaran and Wiebe, 2009, Wu et al., 2009, Xu et al., 2013, Yu et al., 2011, Zhao et al., 2012, Zhou et al., 2013, Zhuang et al., 2006). However, these works only perform extraction but not aspect term grouping or resolution. Separate aspect term grouping has been done in (Carenini et al., 2005, Guo et al., 2009, Zhai et al., 2011). They assume that aspect terms have been extracted beforehand. To extract and group aspects simultaneously, topic models have been applied by researchers (Branavan et al., 2008, Brody and Elhadad, 2010, Chen et al., 2013b, Fang and Huang, 2012, He et al., 2011, Jo and Oh, 2011, Kim et al., 2013, Lazaridou et al., 2013, Li et al., 2011, Lin"
P14-1033,D10-1006,0,0.458319,"al., 2005, Guo et al., 2009, Zhai et al., 2011). They assume that aspect terms have been extracted beforehand. To extract and group aspects simultaneously, topic models have been applied by researchers (Branavan et al., 2008, Brody and Elhadad, 2010, Chen et al., 2013b, Fang and Huang, 2012, He et al., 2011, Jo and Oh, 2011, Kim et al., 2013, Lazaridou et al., 2013, Li et al., 2011, Lin and He, 2009, Lu et al., 2009, Lu et al., 2012, Lu and Zhai, 2008, Mei et al., 2007, Moghaddam and Ester, 2013, Mukherjee and Liu, 2012, Sauper and Barzilay, 2013, Titov and McDonald, 2008, Wang et al., 2010, Zhao et al., 2010). Our proposed AKL model belongs to the class of knowledge-based topic models. Besides the knowledge-based topic models discussed in Section 1, document labels are incorporated as implicit knowledge in (Blei and McAuliffe, 2007, Ramage et al., 2009). Geographical region knowledge has also been considered in topic models (Eisenstein et al., 2010). All of these models assume that the prior knowledge is correct. GK-LDA (Chen et al., 2013a) is the only knowledge-based topic model that deals with wrong lexical knowledge to some extent. As we will see in Section 6, AKL outperformed GKLDA significant"
P14-1033,D12-1015,0,0.0120338,"briefly introduce the proposed method below, which consists of two steps. 2 Related Work Aspect extraction has been studied by many researchers in sentiment analysis (Liu, 2012, Pang and Lee, 2008), e.g., using supervised sequence labeling or classification (Choi and Cardie, 2010, Jakob and Gurevych, 2010, Kobayashi et al., 2007, Li et al., 2010, Yang and Cardie, 2013) and using word frequency and syntactic patterns (Hu and Liu, 2004, Ku et al., 2006, Liu et al., 2013, Popescu and Etzioni, 2005, Qiu et al., 2011, Somasundaran and Wiebe, 2009, Wu et al., 2009, Xu et al., 2013, Yu et al., 2011, Zhao et al., 2012, Zhou et al., 2013, Zhuang et al., 2006). However, these works only perform extraction but not aspect term grouping or resolution. Separate aspect term grouping has been done in (Carenini et al., 2005, Guo et al., 2009, Zhai et al., 2011). They assume that aspect terms have been extracted beforehand. To extract and group aspects simultaneously, topic models have been applied by researchers (Branavan et al., 2008, Brody and Elhadad, 2010, Chen et al., 2013b, Fang and Huang, 2012, He et al., 2011, Jo and Oh, 2011, Kim et al., 2013, Lazaridou et al., 2013, Li et al., 2011, Lin and He, 2009, Lu e"
P14-1033,D13-1189,0,0.0770854,"Missing"
P14-1033,H05-2017,0,\N,Missing
P14-2057,D13-1113,1,0.832467,"dataset show that the proposed method dramatically improves the CNG+SVM method. It also outperforms the co-training method (Blum and Mitchell, 1998) based on our proposed views. 2 Related Work Existing AA methods either focused on finding suitable features or on developing effective techniques. Example features include function words (Argamon et al., 2007), richness features (Gamon 2004), punctuation frequencies (Graham et al., 2005), character (Grieve, 2007), word (Burrows, 1992) and POS n-grams (Gamon, 2004; Hirst and Feiguina, 2007), rewrite rules (Halteren et al., 1996), and similarities (Qian and Liu, 2013). On developing effective learning techniques, supervised classification has been the dominant approach, e.g., neural networks (Graham et al., 2005; Zheng et al., 2006), decision tree (Uzuner and Katz, 2005; Zhao and Zobel, 2005), logistic regression (Madigan et al., 2005), SVM (Diederich et al., 2000; Gamon 2004; Li et al., 2006; Kim et al., 2011), etc. The main problem in the traditional research is the unrealistic size of the training set. A size of about 10,000 words per author is regarded as a reasonable training set size (Argamon et al., 2007, Burrows, 2003). When no long documents are a"
P14-2057,D13-1193,0,0.109884,"approach, e.g., neural networks (Graham et al., 2005; Zheng et al., 2006), decision tree (Uzuner and Katz, 2005; Zhao and Zobel, 2005), logistic regression (Madigan et al., 2005), SVM (Diederich et al., 2000; Gamon 2004; Li et al., 2006; Kim et al., 2011), etc. The main problem in the traditional research is the unrealistic size of the training set. A size of about 10,000 words per author is regarded as a reasonable training set size (Argamon et al., 2007, Burrows, 2003). When no long documents are available, tens or hundreds of short texts are used (Halteren, 2007; Hirst and Feiguina, 2007; Schwartz et al., 2013). Apart from the existing works dealing with limited data discussed in the introduction, our preliminary study in (Qian et al., 2014) used one learning method on two views, but it is inferior to the proposed method in this paper. Input: A small set of labeled documents L = {l1,…, lr}, a large set of unlabeled documents U = {u1,…, us}, and a set of test documents T = {t1,…, tt}, Parameters: the number of iterations k, the size of selected unlabeled documents u Output: tk’s class assignment 1 Extract views Lc, Ll, Ls, Uc, Ul, Us, Tc, Tl, Ts from L, U, T 2 Loop for k iterations: 3 Randomly select"
P14-2057,C04-1088,0,\N,Missing
P14-2057,P03-1054,0,\N,Missing
P14-2057,P12-2052,0,\N,Missing
P14-2057,C08-1065,0,\N,Missing
P15-2123,P12-1043,0,0.0143597,"(2005) trained sentiment classifiers for the target domain using various mixes of labeled and unlabeled reviews. Blitzer et al. (2007) proposed to first find some common or pivot features from the source and the target, and then identify correlated features with the pivot features. The final classifier is built using the combined features. Li and Zong (2008) built a meta-classifier (called CLF) using the outputs of each base classifier constructed in each domain. Other works along similar lines include (Andreevskaia and Bergler, 2008, Bollegala et al., 2011, He et al., 2011, Ku et al., 2009, Li et al., 2012, Li et al., 2013, Pan and Yang, 2010, Tan et al., 2007, Wu et al., 2009, Xia and Zong, 2011, Yoshida et al., 2011). Additional details about these and other related works can be found in (Liu, 2012). However, as we discussed in the introduction, these methods do not focus on the ability to accumulate learned knowledge and leverage it in new learning in a lifelong manner. One question is why the past learning tasks can contribute to the target domain classification given that the target domain already has labeled training data. The key reason is that the training data may not be fully represen"
P15-2123,P08-1034,0,0.019038,"in adaptation (Pan and Yang, 2010). In the sentiment classification context, Aue and Gamon (2005) trained sentiment classifiers for the target domain using various mixes of labeled and unlabeled reviews. Blitzer et al. (2007) proposed to first find some common or pivot features from the source and the target, and then identify correlated features with the pivot features. The final classifier is built using the combined features. Li and Zong (2008) built a meta-classifier (called CLF) using the outputs of each base classifier constructed in each domain. Other works along similar lines include (Andreevskaia and Bergler, 2008, Bollegala et al., 2011, He et al., 2011, Ku et al., 2009, Li et al., 2012, Li et al., 2013, Pan and Yang, 2010, Tan et al., 2007, Wu et al., 2009, Xia and Zong, 2011, Yoshida et al., 2011). Additional details about these and other related works can be found in (Liu, 2012). However, as we discussed in the introduction, these methods do not focus on the ability to accumulate learned knowledge and leverage it in new learning in a lifelong manner. One question is why the past learning tasks can contribute to the target domain classification given that the target domain already has labeled traini"
P15-2123,P07-1056,0,0.69269,"multiple tasks. Multi-task learning optimizes the learning of multiple related tasks at the same time (Caruana, 1997, Chen et al., 2011, Saha et al., 2011, Zhang et al., 2008). However, these methods are not for sentiment analysis. Also, our na¨ıve Bayesian optimization based LL method is quite different from all these existing techniques. Our work is also related to transfer learning or domain adaptation (Pan and Yang, 2010). In the sentiment classification context, Aue and Gamon (2005) trained sentiment classifiers for the target domain using various mixes of labeled and unlabeled reviews. Blitzer et al. (2007) proposed to first find some common or pivot features from the source and the target, and then identify correlated features with the pivot features. The final classifier is built using the combined features. Li and Zong (2008) built a meta-classifier (called CLF) using the outputs of each base classifier constructed in each domain. Other works along similar lines include (Andreevskaia and Bergler, 2008, Bollegala et al., 2011, He et al., 2011, Ku et al., 2009, Li et al., 2012, Li et al., 2013, Pan and Yang, 2010, Tan et al., 2007, Wu et al., 2009, Xia and Zong, 2011, Yoshida et al., 2011). Add"
P15-2123,P11-1014,0,0.0473775,"s of a set of training documents with positive and negative polarity labels. Given the N th task, it uses the knowledge gained in the past N − 1 tasks to learn a better classifier for the N th task. It is useful to note that although many researchers have used transfer learning for supervised sentiment classification, LL is different from the classic transfer learning or domain adaptation (Pan and Yang, 2010). Transfer learning typically uses labeled training data from one (or more) source domain(s) to help learning in the target domain that has little or no labeled data (Aue and Gamon, 2005, Bollegala et al., 2011). It does not use the results of the past learning or knowledge mined from the results of the past learning. Further, transfer learning is usually inferior to traditional supervised learning when the target domain already has good training data. In contrast, our target (or future) domain/task has good training data and we aim to further improve the learning using both the target domain training data and the knowledge gained in past learning. To be consistent with prior research, we treat the classification of one domain as one learning task. 2 Related Work Our work is mainly related to lifelon"
P15-2123,W02-1011,0,0.0550789,"n coefficient): w∈VS KB /(M KB + where the ratio Rw is defined as M+,w +,w KB ). X 0 0 M−,w and X are the starting points for +,w −,w SGD (Section 3.3). Finally, we revise the partial derivatives in Eqs. 4-6 by adding the corresponding partial derivatives of Eqs. 7 and 8 to them. 4 Experiments Datasets. We created a large corpus containing reviews from 20 types of diverse products or domains crawled from Amazon.com (i.e., 20 datasets). The names of product domains are listed in Table 1. Each domain contains 1,000 reviews. Following the existing work of other researchers (Blitzer et al., 2007, Pang et al., 2002), we treat reviews with rating &gt; 3 as positive and reviews with rating < 3 as negative. The datasets are publically available at the authors websites. Natural class distribution: We keep the natural (or skewed) distribution of the positive and negative reviews to experiment with the real-life situation. F1-score is used due to the imbalance. 753 NB-T 56.21 NB-S 57.04 NB-ST 60.61 SVM-T 57.82 SVM-S 57.64 SVM-ST 61.05 CLF 12.87 LSC 67.00 Table 2: Natural class distribution: Average F1-score of the negative class over 20 domains. Negative class is the minority class and thus harder to classify. NB"
P15-2123,P11-1013,0,0.0219868,"ssification context, Aue and Gamon (2005) trained sentiment classifiers for the target domain using various mixes of labeled and unlabeled reviews. Blitzer et al. (2007) proposed to first find some common or pivot features from the source and the target, and then identify correlated features with the pivot features. The final classifier is built using the combined features. Li and Zong (2008) built a meta-classifier (called CLF) using the outputs of each base classifier constructed in each domain. Other works along similar lines include (Andreevskaia and Bergler, 2008, Bollegala et al., 2011, He et al., 2011, Ku et al., 2009, Li et al., 2012, Li et al., 2013, Pan and Yang, 2010, Tan et al., 2007, Wu et al., 2009, Xia and Zong, 2011, Yoshida et al., 2011). Additional details about these and other related works can be found in (Liu, 2012). However, as we discussed in the introduction, these methods do not focus on the ability to accumulate learned knowledge and leverage it in new learning in a lifelong manner. One question is why the past learning tasks can contribute to the target domain classification given that the target domain already has labeled training data. The key reason is that the train"
P15-2123,D09-1131,0,0.0582161,"xt, Aue and Gamon (2005) trained sentiment classifiers for the target domain using various mixes of labeled and unlabeled reviews. Blitzer et al. (2007) proposed to first find some common or pivot features from the source and the target, and then identify correlated features with the pivot features. The final classifier is built using the combined features. Li and Zong (2008) built a meta-classifier (called CLF) using the outputs of each base classifier constructed in each domain. Other works along similar lines include (Andreevskaia and Bergler, 2008, Bollegala et al., 2011, He et al., 2011, Ku et al., 2009, Li et al., 2012, Li et al., 2013, Pan and Yang, 2010, Tan et al., 2007, Wu et al., 2009, Xia and Zong, 2011, Yoshida et al., 2011). Additional details about these and other related works can be found in (Liu, 2012). However, as we discussed in the introduction, these methods do not focus on the ability to accumulate learned knowledge and leverage it in new learning in a lifelong manner. One question is why the past learning tasks can contribute to the target domain classification given that the target domain already has labeled training data. The key reason is that the training data may not"
P15-2123,P08-2065,0,0.478076,"is. Also, our na¨ıve Bayesian optimization based LL method is quite different from all these existing techniques. Our work is also related to transfer learning or domain adaptation (Pan and Yang, 2010). In the sentiment classification context, Aue and Gamon (2005) trained sentiment classifiers for the target domain using various mixes of labeled and unlabeled reviews. Blitzer et al. (2007) proposed to first find some common or pivot features from the source and the target, and then identify correlated features with the pivot features. The final classifier is built using the combined features. Li and Zong (2008) built a meta-classifier (called CLF) using the outputs of each base classifier constructed in each domain. Other works along similar lines include (Andreevskaia and Bergler, 2008, Bollegala et al., 2011, He et al., 2011, Ku et al., 2009, Li et al., 2012, Li et al., 2013, Pan and Yang, 2010, Tan et al., 2007, Wu et al., 2009, Xia and Zong, 2011, Yoshida et al., 2011). Additional details about these and other related works can be found in (Liu, 2012). However, as we discussed in the introduction, these methods do not focus on the ability to accumulate learned knowledge and leverage it in new le"
P15-2123,P09-2080,0,0.0211883,"s mixes of labeled and unlabeled reviews. Blitzer et al. (2007) proposed to first find some common or pivot features from the source and the target, and then identify correlated features with the pivot features. The final classifier is built using the combined features. Li and Zong (2008) built a meta-classifier (called CLF) using the outputs of each base classifier constructed in each domain. Other works along similar lines include (Andreevskaia and Bergler, 2008, Bollegala et al., 2011, He et al., 2011, Ku et al., 2009, Li et al., 2012, Li et al., 2013, Pan and Yang, 2010, Tan et al., 2007, Wu et al., 2009, Xia and Zong, 2011, Yoshida et al., 2011). Additional details about these and other related works can be found in (Liu, 2012). However, as we discussed in the introduction, these methods do not focus on the ability to accumulate learned knowledge and leverage it in new learning in a lifelong manner. One question is why the past learning tasks can contribute to the target domain classification given that the target domain already has labeled training data. The key reason is that the training data may not be fully representative of the test data due to the sample selection bias (Heckman, 1979,"
P15-2123,I11-1069,0,0.0210392,"d and unlabeled reviews. Blitzer et al. (2007) proposed to first find some common or pivot features from the source and the target, and then identify correlated features with the pivot features. The final classifier is built using the combined features. Li and Zong (2008) built a meta-classifier (called CLF) using the outputs of each base classifier constructed in each domain. Other works along similar lines include (Andreevskaia and Bergler, 2008, Bollegala et al., 2011, He et al., 2011, Ku et al., 2009, Li et al., 2012, Li et al., 2013, Pan and Yang, 2010, Tan et al., 2007, Wu et al., 2009, Xia and Zong, 2011, Yoshida et al., 2011). Additional details about these and other related works can be found in (Liu, 2012). However, as we discussed in the introduction, these methods do not focus on the ability to accumulate learned knowledge and leverage it in new learning in a lifelong manner. One question is why the past learning tasks can contribute to the target domain classification given that the target domain already has labeled training data. The key reason is that the training data may not be fully representative of the test data due to the sample selection bias (Heckman, 1979, Shimodaira, 2000, Z"
P17-2023,N10-1122,0,0.0321654,"opinion text. For example, from the sentence “The screen is great”, it aims to extract “screen”, which is a product feature, also called an aspect. Aspect extraction is commonly done using a supervised or an unsupervised approach. The unsupervised approach includes methods such as frequent pattern mining (Hu and Liu, 2004; Popescu and Etzioni, 2005; Zhu et al., 2009), syntactic rules-based extraction (Zhuang et al., 2006; Wang and Wang, 2008; Wu et al., 2009; Zhang et al., 2010; Qiu et al., 2011; Poria et al., 2014), topic modeling (Mei et al., 2007; Titov and McDonald, 2008; Li et al., 2010; Brody and Elhadad, 2010; Wang et al., 2010; Moghaddam and Ester, 2011; Mukherjee and Liu, 2012; Lin and He, 2009; Zhao et al., 2010; Jo and Oh, 2011; Fang and Huang, 2012; Wang et al., 2016), word alignment (Liu et al., 2013), label propagation (Zhou et al., 2013; Shu et al., 2016), and others (Zhao et al., 2015). This paper focuses on the supervised approach (Jakob and Gurevych, 2010; Choi and Cardie, 148 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 148–154 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics h"
P17-2023,D13-1171,0,0.118185,"Missing"
P17-2023,P14-1033,1,0.84342,"sociation for Computational Linguistics https://doi.org/10.18653/v1/P17-2023 where V d is the set of observed values in feature d ∈ {W, −1W, +1W, P, −1P, +1P } and we call V d feature d’s feature values. Eq. (3) is a FF that returns 1 when xl ’s feature d equals to the feature value v d and the variable yl (lth label) equals to the label value i; otherwise 0. We describe G and its feature function next, which also holds the key to the proposed L-CRF. (unlabeled) domain data used in extraction are not used or not available during model training. There are prior LML works for aspect extraction (Chen et al., 2014; Liu et al., 2016), but they were all unsupervised methods. Supervised LML methods exist (Chen et al., 2015; Ruvolo and Eaton, 2013), but they are for classification rather than for sequence learning or labeling like CRF. A semi-supervised LML method is used in NELL (Mitchell et al., 2015), but it is heuristic patternbased. It doesn’t use sequence learning and is not for aspect extraction. LML is related to transfer learning and multi-task learning (Pan and Yang, 2010), but they are also quite different (see (Chen and Liu, 2016) for details). To the best of our knowledge, this is the first pa"
P17-2023,P10-2050,0,0.0653544,"Missing"
P17-2023,P12-2065,0,0.0157196,"pect extraction is commonly done using a supervised or an unsupervised approach. The unsupervised approach includes methods such as frequent pattern mining (Hu and Liu, 2004; Popescu and Etzioni, 2005; Zhu et al., 2009), syntactic rules-based extraction (Zhuang et al., 2006; Wang and Wang, 2008; Wu et al., 2009; Zhang et al., 2010; Qiu et al., 2011; Poria et al., 2014), topic modeling (Mei et al., 2007; Titov and McDonald, 2008; Li et al., 2010; Brody and Elhadad, 2010; Wang et al., 2010; Moghaddam and Ester, 2011; Mukherjee and Liu, 2012; Lin and He, 2009; Zhao et al., 2010; Jo and Oh, 2011; Fang and Huang, 2012; Wang et al., 2016), word alignment (Liu et al., 2013), label propagation (Zhou et al., 2013; Shu et al., 2016), and others (Zhao et al., 2015). This paper focuses on the supervised approach (Jakob and Gurevych, 2010; Choi and Cardie, 148 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 148–154 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2023 where V d is the set of observed values in feature d ∈ {W, −1W, +1W, P, −1P, +1P } and we call V d feature d’s"
P17-2023,P12-1036,1,0.790443,"aims to extract “screen”, which is a product feature, also called an aspect. Aspect extraction is commonly done using a supervised or an unsupervised approach. The unsupervised approach includes methods such as frequent pattern mining (Hu and Liu, 2004; Popescu and Etzioni, 2005; Zhu et al., 2009), syntactic rules-based extraction (Zhuang et al., 2006; Wang and Wang, 2008; Wu et al., 2009; Zhang et al., 2010; Qiu et al., 2011; Poria et al., 2014), topic modeling (Mei et al., 2007; Titov and McDonald, 2008; Li et al., 2010; Brody and Elhadad, 2010; Wang et al., 2010; Moghaddam and Ester, 2011; Mukherjee and Liu, 2012; Lin and He, 2009; Zhao et al., 2010; Jo and Oh, 2011; Fang and Huang, 2012; Wang et al., 2016), word alignment (Liu et al., 2013), label propagation (Zhou et al., 2013; Shu et al., 2016), and others (Zhao et al., 2015). This paper focuses on the supervised approach (Jakob and Gurevych, 2010; Choi and Cardie, 148 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 148–154 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2023 where V d is the set of observed va"
P17-2023,D10-1101,0,0.634743,"2009), syntactic rules-based extraction (Zhuang et al., 2006; Wang and Wang, 2008; Wu et al., 2009; Zhang et al., 2010; Qiu et al., 2011; Poria et al., 2014), topic modeling (Mei et al., 2007; Titov and McDonald, 2008; Li et al., 2010; Brody and Elhadad, 2010; Wang et al., 2010; Moghaddam and Ester, 2011; Mukherjee and Liu, 2012; Lin and He, 2009; Zhao et al., 2010; Jo and Oh, 2011; Fang and Huang, 2012; Wang et al., 2016), word alignment (Liu et al., 2013), label propagation (Zhou et al., 2013; Shu et al., 2016), and others (Zhao et al., 2015). This paper focuses on the supervised approach (Jakob and Gurevych, 2010; Choi and Cardie, 148 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 148–154 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2023 where V d is the set of observed values in feature d ∈ {W, −1W, +1W, P, −1P, +1P } and we call V d feature d’s feature values. Eq. (3) is a FF that returns 1 when xl ’s feature d equals to the feature value v d and the variable yl (lth label) equals to the label value i; otherwise 0. We describe G and its feature function next"
P17-2023,H05-1043,0,0.230529,"ut using this prior knowledge. The key innovation is that even after CRF training, the model can still improve its extraction with experiences in its applications. 1 Introduction Aspect extraction is a key task of opinion mining (Liu, 2012). It extracts opinion targets from opinion text. For example, from the sentence “The screen is great”, it aims to extract “screen”, which is a product feature, also called an aspect. Aspect extraction is commonly done using a supervised or an unsupervised approach. The unsupervised approach includes methods such as frequent pattern mining (Hu and Liu, 2004; Popescu and Etzioni, 2005; Zhu et al., 2009), syntactic rules-based extraction (Zhuang et al., 2006; Wang and Wang, 2008; Wu et al., 2009; Zhang et al., 2010; Qiu et al., 2011; Poria et al., 2014), topic modeling (Mei et al., 2007; Titov and McDonald, 2008; Li et al., 2010; Brody and Elhadad, 2010; Wang et al., 2010; Moghaddam and Ester, 2011; Mukherjee and Liu, 2012; Lin and He, 2009; Zhao et al., 2010; Jo and Oh, 2011; Fang and Huang, 2012; Wang et al., 2016), word alignment (Liu et al., 2013), label propagation (Zhou et al., 2013; Shu et al., 2016), and others (Zhao et al., 2015). This paper focuses on the supervis"
P17-2023,W10-2910,0,0.0210215,"=1 and their corresponding weights θ = {θh }H h=1 . Feature Functions: We use two types of feature functions (FF). One is Label-Label (LL) FF: fijLL (yl , yl−1 ) = 1{yl = i}1{yl−1 = j}, ∀i, j ∈ Y, (1) LW (y , x ) = 1{y = i}1{x = v}, ∀i ∈ Y, ∀v ∈ V, fiv l l l l (2) LG (y , x ) = 1{y = i}1{xG = v G }, ∀i ∈ Y, ∀v G ∈ V G . fiv l l l G l (4) Such a FF returns 1 when the dependency feature of the variable xl equals to a dependency pattern v G and the variable yl equals to the label value i. 3.1 Dependency Relation Dependency relations have been shown useful in many sentiment analysis applications (Johansson and Moschitti, 2010; Jakob and Gurevych, 2010). A dependency relation 1 is a quintuple-tuple: (type, gov, govpos, dep, deppos), where type is the type of the dependency relation, gov is the governor word, govpos is the POS tag of the governor word, dep is the dependent word, and deppos is the POS tag of the dependent word. The l-th word can either be the governor word or the dependent word in a dependency relation. where Y is the set of labels, and 1{·} an indicator function. The other is Label-Word (LW) FF: where V is the vocabulary. This FF returns 1 when the l-th word is v and the l-th label is v’s specific l"
P17-2023,W14-5905,0,0.0169981,"n Aspect extraction is a key task of opinion mining (Liu, 2012). It extracts opinion targets from opinion text. For example, from the sentence “The screen is great”, it aims to extract “screen”, which is a product feature, also called an aspect. Aspect extraction is commonly done using a supervised or an unsupervised approach. The unsupervised approach includes methods such as frequent pattern mining (Hu and Liu, 2004; Popescu and Etzioni, 2005; Zhu et al., 2009), syntactic rules-based extraction (Zhuang et al., 2006; Wang and Wang, 2008; Wu et al., 2009; Zhang et al., 2010; Qiu et al., 2011; Poria et al., 2014), topic modeling (Mei et al., 2007; Titov and McDonald, 2008; Li et al., 2010; Brody and Elhadad, 2010; Wang et al., 2010; Moghaddam and Ester, 2011; Mukherjee and Liu, 2012; Lin and He, 2009; Zhao et al., 2010; Jo and Oh, 2011; Fang and Huang, 2012; Wang et al., 2016), word alignment (Liu et al., 2013), label propagation (Zhou et al., 2013; Shu et al., 2016), and others (Zhao et al., 2015). This paper focuses on the supervised approach (Jakob and Gurevych, 2010; Choi and Cardie, 148 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 1"
P17-2023,J11-1002,1,0.837609,"ons. 1 Introduction Aspect extraction is a key task of opinion mining (Liu, 2012). It extracts opinion targets from opinion text. For example, from the sentence “The screen is great”, it aims to extract “screen”, which is a product feature, also called an aspect. Aspect extraction is commonly done using a supervised or an unsupervised approach. The unsupervised approach includes methods such as frequent pattern mining (Hu and Liu, 2004; Popescu and Etzioni, 2005; Zhu et al., 2009), syntactic rules-based extraction (Zhuang et al., 2006; Wang and Wang, 2008; Wu et al., 2009; Zhang et al., 2010; Qiu et al., 2011; Poria et al., 2014), topic modeling (Mei et al., 2007; Titov and McDonald, 2008; Li et al., 2010; Brody and Elhadad, 2010; Wang et al., 2010; Moghaddam and Ester, 2011; Mukherjee and Liu, 2012; Lin and He, 2009; Zhao et al., 2010; Jo and Oh, 2011; Fang and Huang, 2012; Wang et al., 2016), word alignment (Liu et al., 2013), label propagation (Zhou et al., 2013; Shu et al., 2016), and others (Zhao et al., 2015). This paper focuses on the supervised approach (Jakob and Gurevych, 2010; Choi and Cardie, 148 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (S"
P17-2023,D16-1022,1,0.699479,"methods such as frequent pattern mining (Hu and Liu, 2004; Popescu and Etzioni, 2005; Zhu et al., 2009), syntactic rules-based extraction (Zhuang et al., 2006; Wang and Wang, 2008; Wu et al., 2009; Zhang et al., 2010; Qiu et al., 2011; Poria et al., 2014), topic modeling (Mei et al., 2007; Titov and McDonald, 2008; Li et al., 2010; Brody and Elhadad, 2010; Wang et al., 2010; Moghaddam and Ester, 2011; Mukherjee and Liu, 2012; Lin and He, 2009; Zhao et al., 2010; Jo and Oh, 2011; Fang and Huang, 2012; Wang et al., 2016), word alignment (Liu et al., 2013), label propagation (Zhou et al., 2013; Shu et al., 2016), and others (Zhao et al., 2015). This paper focuses on the supervised approach (Jakob and Gurevych, 2010; Choi and Cardie, 148 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 148–154 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2023 where V d is the set of observed values in feature d ∈ {W, −1W, +1W, P, −1P, +1P } and we call V d feature d’s feature values. Eq. (3) is a FF that returns 1 when xl ’s feature d equals to the feature value v d and the vari"
P17-2023,P08-1036,0,0.163862,"u, 2012). It extracts opinion targets from opinion text. For example, from the sentence “The screen is great”, it aims to extract “screen”, which is a product feature, also called an aspect. Aspect extraction is commonly done using a supervised or an unsupervised approach. The unsupervised approach includes methods such as frequent pattern mining (Hu and Liu, 2004; Popescu and Etzioni, 2005; Zhu et al., 2009), syntactic rules-based extraction (Zhuang et al., 2006; Wang and Wang, 2008; Wu et al., 2009; Zhang et al., 2010; Qiu et al., 2011; Poria et al., 2014), topic modeling (Mei et al., 2007; Titov and McDonald, 2008; Li et al., 2010; Brody and Elhadad, 2010; Wang et al., 2010; Moghaddam and Ester, 2011; Mukherjee and Liu, 2012; Lin and He, 2009; Zhao et al., 2010; Jo and Oh, 2011; Fang and Huang, 2012; Wang et al., 2016), word alignment (Liu et al., 2013), label propagation (Zhou et al., 2013; Shu et al., 2016), and others (Zhao et al., 2015). This paper focuses on the supervised approach (Jakob and Gurevych, 2010; Choi and Cardie, 148 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 148–154 c Vancouver, Canada, July 30 - August 4, 2017. 2017 A"
P17-2023,I08-1038,0,0.31469,"l improve its extraction with experiences in its applications. 1 Introduction Aspect extraction is a key task of opinion mining (Liu, 2012). It extracts opinion targets from opinion text. For example, from the sentence “The screen is great”, it aims to extract “screen”, which is a product feature, also called an aspect. Aspect extraction is commonly done using a supervised or an unsupervised approach. The unsupervised approach includes methods such as frequent pattern mining (Hu and Liu, 2004; Popescu and Etzioni, 2005; Zhu et al., 2009), syntactic rules-based extraction (Zhuang et al., 2006; Wang and Wang, 2008; Wu et al., 2009; Zhang et al., 2010; Qiu et al., 2011; Poria et al., 2014), topic modeling (Mei et al., 2007; Titov and McDonald, 2008; Li et al., 2010; Brody and Elhadad, 2010; Wang et al., 2010; Moghaddam and Ester, 2011; Mukherjee and Liu, 2012; Lin and He, 2009; Zhao et al., 2010; Jo and Oh, 2011; Fang and Huang, 2012; Wang et al., 2016), word alignment (Liu et al., 2013), label propagation (Zhou et al., 2013; Shu et al., 2016), and others (Zhao et al., 2015). This paper focuses on the supervised approach (Jakob and Gurevych, 2010; Choi and Cardie, 148 Proceedings of the 55th Annual Meet"
P17-2023,D09-1159,0,0.0630887,"ion with experiences in its applications. 1 Introduction Aspect extraction is a key task of opinion mining (Liu, 2012). It extracts opinion targets from opinion text. For example, from the sentence “The screen is great”, it aims to extract “screen”, which is a product feature, also called an aspect. Aspect extraction is commonly done using a supervised or an unsupervised approach. The unsupervised approach includes methods such as frequent pattern mining (Hu and Liu, 2004; Popescu and Etzioni, 2005; Zhu et al., 2009), syntactic rules-based extraction (Zhuang et al., 2006; Wang and Wang, 2008; Wu et al., 2009; Zhang et al., 2010; Qiu et al., 2011; Poria et al., 2014), topic modeling (Mei et al., 2007; Titov and McDonald, 2008; Li et al., 2010; Brody and Elhadad, 2010; Wang et al., 2010; Moghaddam and Ester, 2011; Mukherjee and Liu, 2012; Lin and He, 2009; Zhao et al., 2010; Jo and Oh, 2011; Fang and Huang, 2012; Wang et al., 2016), word alignment (Liu et al., 2013), label propagation (Zhou et al., 2013; Shu et al., 2016), and others (Zhao et al., 2015). This paper focuses on the supervised approach (Jakob and Gurevych, 2010; Choi and Cardie, 148 Proceedings of the 55th Annual Meeting of the Associ"
P17-2023,C10-2167,1,0.87525,"Missing"
P17-2023,D10-1006,0,0.0326294,"t feature, also called an aspect. Aspect extraction is commonly done using a supervised or an unsupervised approach. The unsupervised approach includes methods such as frequent pattern mining (Hu and Liu, 2004; Popescu and Etzioni, 2005; Zhu et al., 2009), syntactic rules-based extraction (Zhuang et al., 2006; Wang and Wang, 2008; Wu et al., 2009; Zhang et al., 2010; Qiu et al., 2011; Poria et al., 2014), topic modeling (Mei et al., 2007; Titov and McDonald, 2008; Li et al., 2010; Brody and Elhadad, 2010; Wang et al., 2010; Moghaddam and Ester, 2011; Mukherjee and Liu, 2012; Lin and He, 2009; Zhao et al., 2010; Jo and Oh, 2011; Fang and Huang, 2012; Wang et al., 2016), word alignment (Liu et al., 2013), label propagation (Zhou et al., 2013; Shu et al., 2016), and others (Zhao et al., 2015). This paper focuses on the supervised approach (Jakob and Gurevych, 2010; Choi and Cardie, 148 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 148–154 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2023 where V d is the set of observed values in feature d ∈ {W, −1W, +1W, P,"
P17-2023,D13-1189,0,0.177785,"Missing"
P17-2023,H05-2017,0,\N,Missing
P18-1088,E17-2091,0,0.154294,") are a type of neural models that involve such attention mechanisms (Bahdanau et al., 2015), and they can be applied to ASC. Tang et al. (2016) proposed an MN variant to ASC and achieved the state-of-the-art performance. Another common neural model using attention mechanism is the RNN/LSTM (Wang et al., 2016). As discussed in Section 1, the attention mechanism is suitable for ASC because it effectively addresses the targeted-context detection problem. Along this direction, researchers have studied more sophisticated attentions to further help the ASC task (Chen et al., 2017; Ma et al., 2017; Liu and Zhang, 2017). Chen et al. (2017) proposed to use a recurrent attention mechanism. Ma et al. (2017) used multiple sets of attentions, one for modeling the attention of aspect words and one for modeling the attention of context words. Liu and Zhang (2017) also used multiple sets of attentions, one obtained from the left context and one obtained from the right context of a given target. Notice that our work does not lie in this direction. Our goal is to solve the target-sensitive sen6 Experiments We perform experiments on the datasets of SemEval Task 2014 (Pontiki et al., 2014), which contain online reviews"
P18-1088,D17-1047,0,0.596998,"t al., 2015; Sukhbaatar et al., 2015) are a type of neural models that involve such attention mechanisms (Bahdanau et al., 2015), and they can be applied to ASC. Tang et al. (2016) proposed an MN variant to ASC and achieved the state-of-the-art performance. Another common neural model using attention mechanism is the RNN/LSTM (Wang et al., 2016). As discussed in Section 1, the attention mechanism is suitable for ASC because it effectively addresses the targeted-context detection problem. Along this direction, researchers have studied more sophisticated attentions to further help the ASC task (Chen et al., 2017; Ma et al., 2017; Liu and Zhang, 2017). Chen et al. (2017) proposed to use a recurrent attention mechanism. Ma et al. (2017) used multiple sets of attentions, one for modeling the attention of aspect words and one for modeling the attention of context words. Liu and Zhang (2017) also used multiple sets of attentions, one obtained from the left context and one obtained from the right context of a given target. Notice that our work does not lie in this direction. Our goal is to solve the target-sensitive sen6 Experiments We perform experiments on the datasets of SemEval Task 2014 (Pontiki et al"
P18-1088,D15-1166,0,0.351939,"sitive sentiment (or target-dependent sentiment) problem, which means that the sentiment polarity of a detected/attended context word is conditioned on the target and cannot be directly inferred from the context word alone, unlike “excellent” and “ridiculous”. To address this problem, we propose target-sensitive memory networks (TMNs), which can capture the sentiment interaction between targets and contexts. We present several approaches to implementing TMNs and experimentally evaluate their effectiveness. αi = sof tmax(vtT M mi ) (1) where M ∈ Rd×d is the general learning matrix suggested by Luong et al. (2015). In this manner, attention α = {α1 , α2 , ..αn } is represented as a vector of probabilities, indicating the weight/importance of context wordsPtowards a given target. Note that αi ∈ (0, 1) and αi = 1. i 958 Output Representation: Another embedding matrix C is used for generating the individual (output) continuous vector ci (ci = Cxi ) for each context word xi . A final response/output vector o is produced by summing over these P vectors weighted with the attention α, i.e., o = αi ci . Let us say we have two target words price and resolution (denoted as p and r). We also have two possible con"
P18-1088,D08-1083,0,0.0762526,"ure the TCS interaction, which is a different problem. This direction is also finergrained, and none of the above works addresses this problem. Certainly, both directions can improve the ASC task. We will also show in our experiments that our work can be integrated with an improved attention mechanism. To the best of our knowledge, none of the existing studies addresses the target-sensitive sentiment problem in ASC under the purely data-driven and supervised learning setting. Other concepts like sentiment shifter (Polanyi and Zaenen, 2006) and sentiment composition (Moilanen and Pulman, 2007; Choi and Cardie, 2008; Socher et al., 2013) are also related, but they are not learned automatically and require rule/patterns or external resources (Liu, 2012). Note that our approaches do not rely on handcrafted patterns (Ding et al., 2008; Wu and Wen, 2010), manually compiled sentiment constraints and review ratings (Lu et al., 2011), or parse trees (Socher et al., 2013). (10) (x,t)∈H k∈K 5 Related Work Aspect sentiment classification (ASC) (Hu and Liu, 2004), which is different from document or sentence level sentiment classification (Pang et al., 2002; Kim, 2014; Yang et al., 2016), has recently been tackled"
P18-1088,D15-1298,0,0.172375,"ned automatically and require rule/patterns or external resources (Liu, 2012). Note that our approaches do not rely on handcrafted patterns (Ding et al., 2008; Wu and Wen, 2010), manually compiled sentiment constraints and review ratings (Lu et al., 2011), or parse trees (Socher et al., 2013). (10) (x,t)∈H k∈K 5 Related Work Aspect sentiment classification (ASC) (Hu and Liu, 2004), which is different from document or sentence level sentiment classification (Pang et al., 2002; Kim, 2014; Yang et al., 2016), has recently been tackled by neural networks with promising results (Dong et al., 2014; Nguyen and Shirai, 2015) (also see the survey (Zhang et al., 2018)). Later on, the seminal work of using attention mechanism for neural machine translation (Bahdanau et al., 2015) popularized the application of the attention mechanism in many NLP tasks (Hermann et al., 2015; Cho et al., 2015; Luong et al., 2015), including ASC. Memory networks (MNs) (Weston et al., 2015; Sukhbaatar et al., 2015) are a type of neural models that involve such attention mechanisms (Bahdanau et al., 2015), and they can be applied to ASC. Tang et al. (2016) proposed an MN variant to ASC and achieved the state-of-the-art performance. Anoth"
P18-1088,W02-1011,0,0.0213833,"and sentiment composition (Moilanen and Pulman, 2007; Choi and Cardie, 2008; Socher et al., 2013) are also related, but they are not learned automatically and require rule/patterns or external resources (Liu, 2012). Note that our approaches do not rely on handcrafted patterns (Ding et al., 2008; Wu and Wen, 2010), manually compiled sentiment constraints and review ratings (Lu et al., 2011), or parse trees (Socher et al., 2013). (10) (x,t)∈H k∈K 5 Related Work Aspect sentiment classification (ASC) (Hu and Liu, 2004), which is different from document or sentence level sentiment classification (Pang et al., 2002; Kim, 2014; Yang et al., 2016), has recently been tackled by neural networks with promising results (Dong et al., 2014; Nguyen and Shirai, 2015) (also see the survey (Zhang et al., 2018)). Later on, the seminal work of using attention mechanism for neural machine translation (Bahdanau et al., 2015) popularized the application of the attention mechanism in many NLP tasks (Hermann et al., 2015; Cho et al., 2015; Luong et al., 2015), including ASC. Memory networks (MNs) (Weston et al., 2015; Sukhbaatar et al., 2015) are a type of neural models that involve such attention mechanisms (Bahdanau et"
P18-1088,P14-2009,0,0.0900684,"t they are not learned automatically and require rule/patterns or external resources (Liu, 2012). Note that our approaches do not rely on handcrafted patterns (Ding et al., 2008; Wu and Wen, 2010), manually compiled sentiment constraints and review ratings (Lu et al., 2011), or parse trees (Socher et al., 2013). (10) (x,t)∈H k∈K 5 Related Work Aspect sentiment classification (ASC) (Hu and Liu, 2004), which is different from document or sentence level sentiment classification (Pang et al., 2002; Kim, 2014; Yang et al., 2016), has recently been tackled by neural networks with promising results (Dong et al., 2014; Nguyen and Shirai, 2015) (also see the survey (Zhang et al., 2018)). Later on, the seminal work of using attention mechanism for neural machine translation (Bahdanau et al., 2015) popularized the application of the attention mechanism in many NLP tasks (Hermann et al., 2015; Cho et al., 2015; Luong et al., 2015), including ASC. Memory networks (MNs) (Weston et al., 2015; Sukhbaatar et al., 2015) are a type of neural models that involve such attention mechanisms (Bahdanau et al., 2015), and they can be applied to ASC. Tang et al. (2016) proposed an MN variant to ASC and achieved the state-of-"
P18-1088,S14-2004,0,0.378883,"et al., 2017; Ma et al., 2017; Liu and Zhang, 2017). Chen et al. (2017) proposed to use a recurrent attention mechanism. Ma et al. (2017) used multiple sets of attentions, one for modeling the attention of aspect words and one for modeling the attention of context words. Liu and Zhang (2017) also used multiple sets of attentions, one obtained from the left context and one obtained from the right context of a given target. Notice that our work does not lie in this direction. Our goal is to solve the target-sensitive sen6 Experiments We perform experiments on the datasets of SemEval Task 2014 (Pontiki et al., 2014), which contain online reviews from domain Laptop and Restaurant. In these datasets, aspect sentiment polarities are labeled. The training and test sets have also been provided. Full statistics of the datasets are given in Table 2. Dataset Restaurant Laptop Positive Train Test 2164 728 994 341 Neutral Train Test 637 196 464 169 Negative Train Test 807 196 870 128 Table 2: Statistics of Datasets 6.1 Candidate Models for Comparison MN: The classic end-to-end memory network (Sukhbaatar et al., 2015). AMN: A state-of-the-art memory network used for ASC (Tang et al., 2016). The main difference from"
P18-1088,D13-1170,0,0.00419619,", which is a different problem. This direction is also finergrained, and none of the above works addresses this problem. Certainly, both directions can improve the ASC task. We will also show in our experiments that our work can be integrated with an improved attention mechanism. To the best of our knowledge, none of the existing studies addresses the target-sensitive sentiment problem in ASC under the purely data-driven and supervised learning setting. Other concepts like sentiment shifter (Polanyi and Zaenen, 2006) and sentiment composition (Moilanen and Pulman, 2007; Choi and Cardie, 2008; Socher et al., 2013) are also related, but they are not learned automatically and require rule/patterns or external resources (Liu, 2012). Note that our approaches do not rely on handcrafted patterns (Ding et al., 2008; Wu and Wen, 2010), manually compiled sentiment constraints and review ratings (Lu et al., 2011), or parse trees (Socher et al., 2013). (10) (x,t)∈H k∈K 5 Related Work Aspect sentiment classification (ASC) (Hu and Liu, 2004), which is different from document or sentence level sentiment classification (Pang et al., 2002; Kim, 2014; Yang et al., 2016), has recently been tackled by neural networks wit"
P18-1088,D14-1181,0,0.00902262,"osition (Moilanen and Pulman, 2007; Choi and Cardie, 2008; Socher et al., 2013) are also related, but they are not learned automatically and require rule/patterns or external resources (Liu, 2012). Note that our approaches do not rely on handcrafted patterns (Ding et al., 2008; Wu and Wen, 2010), manually compiled sentiment constraints and review ratings (Lu et al., 2011), or parse trees (Socher et al., 2013). (10) (x,t)∈H k∈K 5 Related Work Aspect sentiment classification (ASC) (Hu and Liu, 2004), which is different from document or sentence level sentiment classification (Pang et al., 2002; Kim, 2014; Yang et al., 2016), has recently been tackled by neural networks with promising results (Dong et al., 2014; Nguyen and Shirai, 2015) (also see the survey (Zhang et al., 2018)). Later on, the seminal work of using attention mechanism for neural machine translation (Bahdanau et al., 2015) popularized the application of the attention mechanism in many NLP tasks (Hermann et al., 2015; Cho et al., 2015; Luong et al., 2015), including ASC. Memory networks (MNs) (Weston et al., 2015; Sukhbaatar et al., 2015) are a type of neural models that involve such attention mechanisms (Bahdanau et al., 2015),"
P18-1088,J81-4005,0,0.746932,"Missing"
P18-1088,D16-1021,0,0.334703,"rtment of Computer Science, University of Illinois at Chicago, USA ‡ Plus.AI, USA § Artificial Intelligence School, Jilin University, China shuaiwanghk@gmail.com, sahisnumazumder@gmail.com liub@uic.edu, mianwei.zhou@gmail.com, yichang@acm.org Abstract Due to their impressive results in many NLP tasks (Deng et al., 2014), neural networks have been applied to ASC (see the survey (Zhang et al., 2018)). Memory networks (MNs), a type of neural networks which were first proposed for question answering (Weston et al., 2015; Sukhbaatar et al., 2015), have achieved the state-of-the-art results in ASC (Tang et al., 2016). A key factor for their success is the attention mechanism. However, we found that using existing MNs to deal with ASC has an important problem and simply relying on attention modeling cannot solve it. That is, their performance degrades when the sentiment of a context word is sensitive to the given target. Let us consider the following sentences: Aspect sentiment classification (ASC) is a fundamental task in sentiment analysis. Given an aspect/target and a sentence, the task classifies the sentiment polarity expressed on the target in the sentence. Memory networks (MNs) have been used for th"
P18-1088,D16-1058,0,0.577182,"work of using attention mechanism for neural machine translation (Bahdanau et al., 2015) popularized the application of the attention mechanism in many NLP tasks (Hermann et al., 2015; Cho et al., 2015; Luong et al., 2015), including ASC. Memory networks (MNs) (Weston et al., 2015; Sukhbaatar et al., 2015) are a type of neural models that involve such attention mechanisms (Bahdanau et al., 2015), and they can be applied to ASC. Tang et al. (2016) proposed an MN variant to ASC and achieved the state-of-the-art performance. Another common neural model using attention mechanism is the RNN/LSTM (Wang et al., 2016). As discussed in Section 1, the attention mechanism is suitable for ASC because it effectively addresses the targeted-context detection problem. Along this direction, researchers have studied more sophisticated attentions to further help the ASC task (Chen et al., 2017; Ma et al., 2017; Liu and Zhang, 2017). Chen et al. (2017) proposed to use a recurrent attention mechanism. Ma et al. (2017) used multiple sets of attentions, one for modeling the attention of aspect words and one for modeling the attention of context words. Liu and Zhang (2017) also used multiple sets of attentions, one obtain"
P18-1088,C10-1134,0,0.0843262,"work can be integrated with an improved attention mechanism. To the best of our knowledge, none of the existing studies addresses the target-sensitive sentiment problem in ASC under the purely data-driven and supervised learning setting. Other concepts like sentiment shifter (Polanyi and Zaenen, 2006) and sentiment composition (Moilanen and Pulman, 2007; Choi and Cardie, 2008; Socher et al., 2013) are also related, but they are not learned automatically and require rule/patterns or external resources (Liu, 2012). Note that our approaches do not rely on handcrafted patterns (Ding et al., 2008; Wu and Wen, 2010), manually compiled sentiment constraints and review ratings (Lu et al., 2011), or parse trees (Socher et al., 2013). (10) (x,t)∈H k∈K 5 Related Work Aspect sentiment classification (ASC) (Hu and Liu, 2004), which is different from document or sentence level sentiment classification (Pang et al., 2002; Kim, 2014; Yang et al., 2016), has recently been tackled by neural networks with promising results (Dong et al., 2014; Nguyen and Shirai, 2015) (also see the survey (Zhang et al., 2018)). Later on, the seminal work of using attention mechanism for neural machine translation (Bahdanau et al., 201"
P18-1088,N16-1174,0,0.0244239,"ilanen and Pulman, 2007; Choi and Cardie, 2008; Socher et al., 2013) are also related, but they are not learned automatically and require rule/patterns or external resources (Liu, 2012). Note that our approaches do not rely on handcrafted patterns (Ding et al., 2008; Wu and Wen, 2010), manually compiled sentiment constraints and review ratings (Lu et al., 2011), or parse trees (Socher et al., 2013). (10) (x,t)∈H k∈K 5 Related Work Aspect sentiment classification (ASC) (Hu and Liu, 2004), which is different from document or sentence level sentiment classification (Pang et al., 2002; Kim, 2014; Yang et al., 2016), has recently been tackled by neural networks with promising results (Dong et al., 2014; Nguyen and Shirai, 2015) (also see the survey (Zhang et al., 2018)). Later on, the seminal work of using attention mechanism for neural machine translation (Bahdanau et al., 2015) popularized the application of the attention mechanism in many NLP tasks (Hermann et al., 2015; Cho et al., 2015; Luong et al., 2015), including ASC. Memory networks (MNs) (Weston et al., 2015; Sukhbaatar et al., 2015) are a type of neural models that involve such attention mechanisms (Bahdanau et al., 2015), and they can be app"
P18-2094,P15-1071,0,0.0478304,"ual features (Poria et al., 2016; Wang et al., 2016). Further, (Wang et al., 2016, 2017; Li and Lam, 2017) also proposed aspect and opinion terms co-extraction via a deep network. They took advantage of the goldstandard opinion terms or sentiment lexicon for aspect extraction. The proposed approach is close to (Liu et al., 2015), where only the annotated data for aspect extraction is used. However, we will show that our approach is more effective even compared with baselines using additional supervisions and/or resources. The proposed embedding mechanism is related to cross domain embeddings (Bollegala et al., 2015, 2017) and domain-specific embeddings (Xu et al., 2018a,b). However, we require the domain of the domain embeddings must exactly match the domain of the aspect extraction task. CNN (LeCun et al., 1995; Kim, 2014) is recently adopted for named entity recognition (Strubell et al., 2017). CNN classifiers are also used in sentiment analysis (Poria et al., 2016; Chen et al., 2017). We adopt CNN for sequence labeling for aspect extraction because CNN is simple and parallelized. Related Work Sentiment analysis has been studied at document, sentence and aspect levels (Liu, 2012; Pang and Lee, 2008; C"
P18-2094,P14-2050,0,0.0789028,"Missing"
P18-2094,D17-1310,0,0.426428,"has many applications (Liu, 2012). It aims to extract opinion targets (or aspects) from opinion text. In product reviews, aspects are product attributes or features. For example, from “Its speed is incredible” in a laptop review, it aims to extract “speed”. Aspect extraction has been performed using supervised (Jakob and Gurevych, 2010; Chernyshevich, 2014; Shu et al., 2017) and unsupervised approaches (Hu and Liu, 2004; Zhuang et al., 2006; Mei et al., 2007; Qiu et al., 2011; Yin et al., 2016; He et al., 2017). Recently, supervised deep learning models achieved state-of-the-art performances (Li and Lam, 2017). Many of these models use 1 The code of this paper can be found at https://www. cs.uic.edu/˜hxu/. 592 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 592–598 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics whereas “speed” in general embeddings or general review embeddings may mean how many miles per second. So using in-domain embeddings is important even when the in-domain embedding corpus is not large. Thus, we leverage both general embeddings and domain embeddings and let the rest of the"
P18-2094,S14-2051,0,0.379487,"existing methods. To our knowledge, this paper is the first to report such double embeddings based CNN model for aspect extraction and achieve very good results. 1 Introduction Aspect extraction is an important task in sentiment analysis (Hu and Liu, 2004) and has many applications (Liu, 2012). It aims to extract opinion targets (or aspects) from opinion text. In product reviews, aspects are product attributes or features. For example, from “Its speed is incredible” in a laptop review, it aims to extract “speed”. Aspect extraction has been performed using supervised (Jakob and Gurevych, 2010; Chernyshevich, 2014; Shu et al., 2017) and unsupervised approaches (Hu and Liu, 2004; Zhuang et al., 2006; Mei et al., 2007; Qiu et al., 2011; Yin et al., 2016; He et al., 2017). Recently, supervised deep learning models achieved state-of-the-art performances (Li and Lam, 2017). Many of these models use 1 The code of this paper can be found at https://www. cs.uic.edu/˜hxu/. 592 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 592–598 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics whereas “speed” in general em"
P18-2094,P17-1036,0,0.27022,"1 Introduction Aspect extraction is an important task in sentiment analysis (Hu and Liu, 2004) and has many applications (Liu, 2012). It aims to extract opinion targets (or aspects) from opinion text. In product reviews, aspects are product attributes or features. For example, from “Its speed is incredible” in a laptop review, it aims to extract “speed”. Aspect extraction has been performed using supervised (Jakob and Gurevych, 2010; Chernyshevich, 2014; Shu et al., 2017) and unsupervised approaches (Hu and Liu, 2004; Zhuang et al., 2006; Mei et al., 2007; Qiu et al., 2011; Yin et al., 2016; He et al., 2017). Recently, supervised deep learning models achieved state-of-the-art performances (Li and Lam, 2017). Many of these models use 1 The code of this paper can be found at https://www. cs.uic.edu/˜hxu/. 592 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 592–598 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics whereas “speed” in general embeddings or general review embeddings may mean how many miles per second. So using in-domain embeddings is important even when the in-domain embedding corpus"
P18-2094,D15-1168,0,0.216577,"eed” in general embeddings or general review embeddings may mean how many miles per second. So using in-domain embeddings is important even when the in-domain embedding corpus is not large. Thus, we leverage both general embeddings and domain embeddings and let the rest of the network to decide which embeddings have more useful information. To address the second consideration, we use a pure Convolutional Neural Network (CNN) (LeCun et al., 1995) model for sequence labeling. Although most existing models use LSTM (Hochreiter and Schmidhuber, 1997) as the core building block to model sequences (Liu et al., 2015; Li and Lam, 2017), we noticed that CNN is also successful in many NLP tasks (Kim, 2014; Zhang et al., 2015; Gehring et al., 2017). One major drawback of LSTM is that LSTM cells are sequentially dependent. The forward pass and backpropagation must serially go through the whole sequence, which slows down the training/testing process 2 . One challenge of applying CNN on sequence labeling is that convolution and max-pooling operations are usually used for summarizing sequential inputs and the outputs are not well-aligned with the inputs. We discuss the solutions in Section 3. We call the propose"
P18-2094,D13-1171,0,0.540154,"Missing"
P18-2094,D10-1101,0,0.862597,"-of-the-art sophisticated existing methods. To our knowledge, this paper is the first to report such double embeddings based CNN model for aspect extraction and achieve very good results. 1 Introduction Aspect extraction is an important task in sentiment analysis (Hu and Liu, 2004) and has many applications (Liu, 2012). It aims to extract opinion targets (or aspects) from opinion text. In product reviews, aspects are product attributes or features. For example, from “Its speed is incredible” in a laptop review, it aims to extract “speed”. Aspect extraction has been performed using supervised (Jakob and Gurevych, 2010; Chernyshevich, 2014; Shu et al., 2017) and unsupervised approaches (Hu and Liu, 2004; Zhuang et al., 2006; Mei et al., 2007; Qiu et al., 2011; Yin et al., 2016; He et al., 2017). Recently, supervised deep learning models achieved state-of-the-art performances (Li and Lam, 2017). Many of these models use 1 The code of this paper can be found at https://www. cs.uic.edu/˜hxu/. 592 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 592–598 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics whereas"
P18-2094,D14-1162,0,0.095317,"us, to achieve competitive performance whereas keeping the model as simple as possible is important. This paper proposes such a model. To address the first consideration, we propose a double embeddings mechanism that is shown crucial for aspect extraction. The embedding layer is the very first layer, where all the information about each word is encoded. The quality of the embeddings determines how easily later layers (e.g., LSTM, CNN or attention) can decode useful information. Existing deep learning models for aspect extraction use either a pre-trained general-purpose embedding, e.g., GloVe (Pennington et al., 2014), or a general review embedding (Poria et al., 2016). However, aspect extraction is a complex task that also requires fine-grained domain embeddings. For example, in the previous example, detecting “speed” may require embeddings of both “Its” and “speed”. However, the criteria for good embeddings for “Its” and “speed” can be totally different. “Its” is a general word and the general embedding (trained from a large corpus) is likely to have a better representation for “Its”. But, “speed” has a very fine-grained meaning (e.g., how many instructions per second) in the laptop domain, One key task"
P18-2094,S16-1045,0,0.269823,"Missing"
P18-2094,S16-1002,0,0.361347,"Missing"
P18-2094,I08-1038,0,0.0611946,". We adopt CNN for sequence labeling for aspect extraction because CNN is simple and parallelized. Related Work Sentiment analysis has been studied at document, sentence and aspect levels (Liu, 2012; Pang and Lee, 2008; Cambria and Hussain, 2012). This work focuses on the aspect level (Hu and Liu, 2004). Aspect extraction is one of its key tasks, and has been performed using both unsupervised and supervised approaches. The unsupervised approach includes methods such as frequent pattern mining (Hu and Liu, 2004; Popescu and Etzioni, 2005), syntactic rules-based extraction (Zhuang et al., 2006; Wang and Wang, 2008; Qiu et al., 2011), topic modeling (Mei et al., 2007; Titov and McDonald, 2008; Lin and He, 2009; Moghaddam and Ester, 2011), word alignment (Liu et al., 3 Model The proposed model is depicted in Figure 1. It has 2 embedding layers, 4 CNN layers, a fullyconnected layer shared across all positions of words, and a softmax layer over the labeling space Y = {B, I, O} for each position of inputs. Note that an aspect can be a phrase and B, I indicate the beginning word and non-beginning word of an aspect phrase and O indicates non-aspect words. Assume the input is a sequence of word indexes x = (x1"
P18-2094,D16-1059,0,0.428076,"sed sequence labeling model for aspect extraction. 2 2013) and label propagation (Zhou et al., 2013; Shu et al., 2016). Traditionally, the supervised approach (Jakob and Gurevych, 2010; Mitchell et al., 2013; Shu et al., 2017) uses Conditional Random Fields (CRF) (Lafferty et al., 2001). Recently, deep neural networks are applied to learn better features for supervised aspect extraction, e.g., using LSTM (Williams and Zipser, 1989; Hochreiter and Schmidhuber, 1997; Liu et al., 2015) and attention mechanism (Wang et al., 2017; He et al., 2017) together with manual features (Poria et al., 2016; Wang et al., 2016). Further, (Wang et al., 2016, 2017; Li and Lam, 2017) also proposed aspect and opinion terms co-extraction via a deep network. They took advantage of the goldstandard opinion terms or sentiment lexicon for aspect extraction. The proposed approach is close to (Liu et al., 2015), where only the annotated data for aspect extraction is used. However, we will show that our approach is more effective even compared with baselines using additional supervisions and/or resources. The proposed embedding mechanism is related to cross domain embeddings (Bollegala et al., 2015, 2017) and domain-specific em"
P18-2094,S14-2004,0,0.887921,"ny unseen words in test data. If embeddings are tunable, the features for seen words’ embeddings will be adjusted (e.g., forgetting useless features and infusing new features that are related to the labels of the training examples). And the CNN filters will adjust to the new features accordingly. But the embeddings of unseen words from test data still have the old features that may be mistakenly extracted by CNN. 4 4.1 Experiments Datasets Following the experiments of a recent aspect extraction paper (Li and Lam, 2017), we conduct experiments on two benchmark datasets from SemEval challenges (Pontiki et al., 2014, 2016) as shown in Table 4.1. The first dataset is from the laptop domain on subtask 1 of SemEval-2014 Task 4. The second dataset is from the restaurant domain on subtask 1 (slot 2) of SemEval-2016 Task 5. These two datasets consist of review sentences with aspect terms labeled as spans of characters. Then we concatenate two embeddings x(1) = ⊕ xd and feed the result into a stack of 4 CNN layers. A CNN layer has many 1D-convolution filters and each (the r-th) filter has a fixed kernel size k = 2c+1 and performs the following convolution xg 594 We use NLTK3 to tokenize each sentence into a seq"
P18-2094,H05-1043,0,0.623038,"sifiers are also used in sentiment analysis (Poria et al., 2016; Chen et al., 2017). We adopt CNN for sequence labeling for aspect extraction because CNN is simple and parallelized. Related Work Sentiment analysis has been studied at document, sentence and aspect levels (Liu, 2012; Pang and Lee, 2008; Cambria and Hussain, 2012). This work focuses on the aspect level (Hu and Liu, 2004). Aspect extraction is one of its key tasks, and has been performed using both unsupervised and supervised approaches. The unsupervised approach includes methods such as frequent pattern mining (Hu and Liu, 2004; Popescu and Etzioni, 2005), syntactic rules-based extraction (Zhuang et al., 2006; Wang and Wang, 2008; Qiu et al., 2011), topic modeling (Mei et al., 2007; Titov and McDonald, 2008; Lin and He, 2009; Moghaddam and Ester, 2011), word alignment (Liu et al., 3 Model The proposed model is depicted in Figure 1. It has 2 embedding layers, 4 CNN layers, a fullyconnected layer shared across all positions of words, and a softmax layer over the labeling space Y = {B, I, O} for each position of inputs. Note that an aspect can be a phrase and B, I indicate the beginning word and non-beginning word of an aspect phrase and O indica"
P18-2094,J11-1002,1,0.874858,"ction and achieve very good results. 1 Introduction Aspect extraction is an important task in sentiment analysis (Hu and Liu, 2004) and has many applications (Liu, 2012). It aims to extract opinion targets (or aspects) from opinion text. In product reviews, aspects are product attributes or features. For example, from “Its speed is incredible” in a laptop review, it aims to extract “speed”. Aspect extraction has been performed using supervised (Jakob and Gurevych, 2010; Chernyshevich, 2014; Shu et al., 2017) and unsupervised approaches (Hu and Liu, 2004; Zhuang et al., 2006; Mei et al., 2007; Qiu et al., 2011; Yin et al., 2016; He et al., 2017). Recently, supervised deep learning models achieved state-of-the-art performances (Li and Lam, 2017). Many of these models use 1 The code of this paper can be found at https://www. cs.uic.edu/˜hxu/. 592 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 592–598 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics whereas “speed” in general embeddings or general review embeddings may mean how many miles per second. So using in-domain embeddings is important even"
P18-2094,D17-1035,0,0.0401619,"Missing"
P18-2094,D16-1022,1,0.838878,"ially go through the whole sequence, which slows down the training/testing process 2 . One challenge of applying CNN on sequence labeling is that convolution and max-pooling operations are usually used for summarizing sequential inputs and the outputs are not well-aligned with the inputs. We discuss the solutions in Section 3. We call the proposed model Dual Embeddings CNN (DE-CNN). To the best of our knowledge, this is the first paper that reports a double embedding mechanism and a pure CNN-based sequence labeling model for aspect extraction. 2 2013) and label propagation (Zhou et al., 2013; Shu et al., 2016). Traditionally, the supervised approach (Jakob and Gurevych, 2010; Mitchell et al., 2013; Shu et al., 2017) uses Conditional Random Fields (CRF) (Lafferty et al., 2001). Recently, deep neural networks are applied to learn better features for supervised aspect extraction, e.g., using LSTM (Williams and Zipser, 1989; Hochreiter and Schmidhuber, 1997; Liu et al., 2015) and attention mechanism (Wang et al., 2017; He et al., 2017) together with manual features (Poria et al., 2016; Wang et al., 2016). Further, (Wang et al., 2016, 2017; Li and Lam, 2017) also proposed aspect and opinion terms co-ext"
P18-2094,D13-1189,0,0.044238,"Missing"
P18-2094,P17-2023,1,0.931749,"our knowledge, this paper is the first to report such double embeddings based CNN model for aspect extraction and achieve very good results. 1 Introduction Aspect extraction is an important task in sentiment analysis (Hu and Liu, 2004) and has many applications (Liu, 2012). It aims to extract opinion targets (or aspects) from opinion text. In product reviews, aspects are product attributes or features. For example, from “Its speed is incredible” in a laptop review, it aims to extract “speed”. Aspect extraction has been performed using supervised (Jakob and Gurevych, 2010; Chernyshevich, 2014; Shu et al., 2017) and unsupervised approaches (Hu and Liu, 2004; Zhuang et al., 2006; Mei et al., 2007; Qiu et al., 2011; Yin et al., 2016; He et al., 2017). Recently, supervised deep learning models achieved state-of-the-art performances (Li and Lam, 2017). Many of these models use 1 The code of this paper can be found at https://www. cs.uic.edu/˜hxu/. 592 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 592–598 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics whereas “speed” in general embeddings or general"
P18-2094,D17-1283,0,0.0177423,"osed approach is close to (Liu et al., 2015), where only the annotated data for aspect extraction is used. However, we will show that our approach is more effective even compared with baselines using additional supervisions and/or resources. The proposed embedding mechanism is related to cross domain embeddings (Bollegala et al., 2015, 2017) and domain-specific embeddings (Xu et al., 2018a,b). However, we require the domain of the domain embeddings must exactly match the domain of the aspect extraction task. CNN (LeCun et al., 1995; Kim, 2014) is recently adopted for named entity recognition (Strubell et al., 2017). CNN classifiers are also used in sentiment analysis (Poria et al., 2016; Chen et al., 2017). We adopt CNN for sequence labeling for aspect extraction because CNN is simple and parallelized. Related Work Sentiment analysis has been studied at document, sentence and aspect levels (Liu, 2012; Pang and Lee, 2008; Cambria and Hussain, 2012). This work focuses on the aspect level (Hu and Liu, 2004). Aspect extraction is one of its key tasks, and has been performed using both unsupervised and supervised approaches. The unsupervised approach includes methods such as frequent pattern mining (Hu and L"
P18-2094,P08-1036,0,0.0880507,"simple and parallelized. Related Work Sentiment analysis has been studied at document, sentence and aspect levels (Liu, 2012; Pang and Lee, 2008; Cambria and Hussain, 2012). This work focuses on the aspect level (Hu and Liu, 2004). Aspect extraction is one of its key tasks, and has been performed using both unsupervised and supervised approaches. The unsupervised approach includes methods such as frequent pattern mining (Hu and Liu, 2004; Popescu and Etzioni, 2005), syntactic rules-based extraction (Zhuang et al., 2006; Wang and Wang, 2008; Qiu et al., 2011), topic modeling (Mei et al., 2007; Titov and McDonald, 2008; Lin and He, 2009; Moghaddam and Ester, 2011), word alignment (Liu et al., 3 Model The proposed model is depicted in Figure 1. It has 2 embedding layers, 4 CNN layers, a fullyconnected layer shared across all positions of words, and a softmax layer over the labeling space Y = {B, I, O} for each position of inputs. Note that an aspect can be a phrase and B, I indicate the beginning word and non-beginning word of an aspect phrase and O indicates non-aspect words. Assume the input is a sequence of word indexes x = (x1 , . . . , xn ). This sequence gets its two corresponding continuous representa"
P18-2094,H05-2017,0,\N,Missing
P19-1056,P18-1234,0,0.0551063,"nships. Mei et al. (2007), He et al. (2011) and Chen et al. (2014) used topic modeling based on Latent Dirichlet Allocation (Blei et al., 2003). All of the above methods are unsupervised. For supervised methods, the ATE task is usually treated as a sequence labeling problem solved by CRF. For the ASC task, a large body of literature has tried to utilize the relation or position between the aspect terms and the surrounding context words as the relevant information or context for prediction (Tang et al., 2016a; Laddha and Mukherjee, 2016). Convolution neural networks (CNNs) (Poria et al., 2016; Li and Xue, 2018), attention network (Wang et al., 2016b; Ma et al., 2017; He et al., 2017), and memory network (Wang et al., 2018) are also active approaches. However, the above methods are proposed for either the ATE or the ASC task. Lakkaraju et al. (2014) proposed to use hierarchical deep learning to solve these two subtasks. Wu et al. (2016) utilized cascaded CNN and multi-task CNN to address aspect extraction and sentiment classification. Their main idea is to directly map each review sentence into pre-defined aspect terms by using classification and then classifying the corresponding polarities. We beli"
P19-1056,P18-1087,0,0.0667094,"hat concatenates target word embedding and context four-word embeddings besides using linguistically informed features plus CRF to finish the sequence labeling task (Zhang et al., 2015). Instead of using the officially released code8 due to the outdated library, we reproduce the results with the original settings. • Sentiment-Scope: A collapsed CRF model9 (Li and Lu, 2017), which expands the node types of CRF to capture sentiment scopes. The discrete features used in this model are exactly the same as the above two groups of models. Settings • DE-CNN+TNet: DE-CNN10 (Xu et al., 2018) and TNet (Li et al., 2018) are the current state-of-the-art models for ATE and ASC, respectively. DE-CNN+TNet combines them in a pipelined manner. We use the official TNet-AS variant11 as our TNet implementation. In our experiments, the regularization parameter λ is empirically set as 0.001, and dG and dD as 300 and 100, respectively. The hidden state size of d of ReGU is 300. The hyperparameter K is set to 5. We use Adam (Kingma et al., 2014) as the optimizer with the learning rate of 0.001 and the batch size of 16. We also employ dropout (Srivastava et al., 2014) on the outputs of the embedding layer and two BiReGU l"
P19-1056,P14-1033,1,0.858148,"same objective as us. The main difference is that their approach belongs to a collapsed approach but ours is a joint approach. The model proposed by (Li and Lu, 2017) is also a collapsed approach based on CRF. Its performance is heavily dependent on manually crafted features. been studied by many researchers. Hu and Liu (2004) extracted aspect terms using frequent pattern mining. Qiu et al. (2011) and Liu et al. (2015) proposed to use rule-based approach exploiting either hand-crafted or automatically generated rules about some syntactic relationships. Mei et al. (2007), He et al. (2011) and Chen et al. (2014) used topic modeling based on Latent Dirichlet Allocation (Blei et al., 2003). All of the above methods are unsupervised. For supervised methods, the ATE task is usually treated as a sequence labeling problem solved by CRF. For the ASC task, a large body of literature has tried to utilize the relation or position between the aspect terms and the surrounding context words as the relevant information or context for prediction (Tang et al., 2016a; Laddha and Mukherjee, 2016). Convolution neural networks (CNNs) (Poria et al., 2016; Li and Xue, 2018), attention network (Wang et al., 2016b; Ma et al"
P19-1056,P17-1036,0,0.126706,"ic modeling based on Latent Dirichlet Allocation (Blei et al., 2003). All of the above methods are unsupervised. For supervised methods, the ATE task is usually treated as a sequence labeling problem solved by CRF. For the ASC task, a large body of literature has tried to utilize the relation or position between the aspect terms and the surrounding context words as the relevant information or context for prediction (Tang et al., 2016a; Laddha and Mukherjee, 2016). Convolution neural networks (CNNs) (Poria et al., 2016; Li and Xue, 2018), attention network (Wang et al., 2016b; Ma et al., 2017; He et al., 2017), and memory network (Wang et al., 2018) are also active approaches. However, the above methods are proposed for either the ATE or the ASC task. Lakkaraju et al. (2014) proposed to use hierarchical deep learning to solve these two subtasks. Wu et al. (2016) utilized cascaded CNN and multi-task CNN to address aspect extraction and sentiment classification. Their main idea is to directly map each review sentence into pre-defined aspect terms by using classification and then classifying the corresponding polarities. We believe the pre-defined aspect terms are in general insufficient for most anal"
P19-1056,P11-1013,0,0.111686,"t al. (2019) have the same objective as us. The main difference is that their approach belongs to a collapsed approach but ours is a joint approach. The model proposed by (Li and Lu, 2017) is also a collapsed approach based on CRF. Its performance is heavily dependent on manually crafted features. been studied by many researchers. Hu and Liu (2004) extracted aspect terms using frequent pattern mining. Qiu et al. (2011) and Liu et al. (2015) proposed to use rule-based approach exploiting either hand-crafted or automatically generated rules about some syntactic relationships. Mei et al. (2007), He et al. (2011) and Chen et al. (2014) used topic modeling based on Latent Dirichlet Allocation (Blei et al., 2003). All of the above methods are unsupervised. For supervised methods, the ATE task is usually treated as a sequence labeling problem solved by CRF. For the ASC task, a large body of literature has tried to utilize the relation or position between the aspect terms and the surrounding context words as the relevant information or context for prediction (Tang et al., 2016a; Laddha and Mukherjee, 2016). Convolution neural networks (CNNs) (Poria et al., 2016; Li and Xue, 2018), attention network (Wang"
P19-1056,P16-1101,0,0.0418952,"Missing"
P19-1056,D10-1101,0,0.0403874,"ets. 1 Figure 1: Aspect terms extraction and aspect sentiment classification. four classes, e.g., positive (PO), conflict (CF), neutral (NT)1 , and negative (NG). To facilitate practical applications, our goal is to solve ATE and ASC simultaneously. For easy description and discussion, these two subtasks are referred to as aspect term-polarity co-extraction. Both ATE and ASC have attracted a great of attention among researchers, but they are rarely solved together at the same time due to some challenges: 1) ATE and ASC are quite different tasks. ATE is an extraction or sequence labeling task (Jakob and Gurevych, 2010; Wang et al., 2016a), while ASC is a classification task (Jiang et al., 2011; Wagner et al., 2014; Tang et al., 2016a,b; Tay et al., 2018). Thus, they are naturally treated as two separate tasks, and solved one by one in a pipeline manner. However, this two-stage framework is complicated and difficult to use in applications because it needs to train two models separately. There is also the latent error propagation when an aspect term is used to classify its corresponding polarity. Thus, due to the different natures of the two tasks, most current works focus either on extracting aspect terms ("
P19-1056,P11-1016,0,0.1492,"classes, e.g., positive (PO), conflict (CF), neutral (NT)1 , and negative (NG). To facilitate practical applications, our goal is to solve ATE and ASC simultaneously. For easy description and discussion, these two subtasks are referred to as aspect term-polarity co-extraction. Both ATE and ASC have attracted a great of attention among researchers, but they are rarely solved together at the same time due to some challenges: 1) ATE and ASC are quite different tasks. ATE is an extraction or sequence labeling task (Jakob and Gurevych, 2010; Wang et al., 2016a), while ASC is a classification task (Jiang et al., 2011; Wagner et al., 2014; Tang et al., 2016a,b; Tay et al., 2018). Thus, they are naturally treated as two separate tasks, and solved one by one in a pipeline manner. However, this two-stage framework is complicated and difficult to use in applications because it needs to train two models separately. There is also the latent error propagation when an aspect term is used to classify its corresponding polarity. Thus, due to the different natures of the two tasks, most current works focus either on extracting aspect terms (Yin et al., 2016; Luo et al., 2018; Xu et al., 2018) or on classifying aspect"
P19-1056,D13-1171,0,0.625433,"Missing"
P19-1056,D16-1060,0,0.0927016,"ing either hand-crafted or automatically generated rules about some syntactic relationships. Mei et al. (2007), He et al. (2011) and Chen et al. (2014) used topic modeling based on Latent Dirichlet Allocation (Blei et al., 2003). All of the above methods are unsupervised. For supervised methods, the ATE task is usually treated as a sequence labeling problem solved by CRF. For the ASC task, a large body of literature has tried to utilize the relation or position between the aspect terms and the surrounding context words as the relevant information or context for prediction (Tang et al., 2016a; Laddha and Mukherjee, 2016). Convolution neural networks (CNNs) (Poria et al., 2016; Li and Xue, 2018), attention network (Wang et al., 2016b; Ma et al., 2017; He et al., 2017), and memory network (Wang et al., 2018) are also active approaches. However, the above methods are proposed for either the ATE or the ASC task. Lakkaraju et al. (2014) proposed to use hierarchical deep learning to solve these two subtasks. Wu et al. (2016) utilized cascaded CNN and multi-task CNN to address aspect extraction and sentiment classification. Their main idea is to directly map each review sentence into pre-defined aspect terms by usin"
P19-1056,D14-1162,0,0.0824684,"urant respectively. Thus, for SL , we use Amazon embedding, and for SR , we use Yelp embedding. The Amazon review dataset contains 142.8M reviews, and the Yelp review dataset contains 2.2M restaurant reviews. The embeddings from all these datasets are trained by Gensim5 which contains the implementation of CBOW. The parameter min count is set to 10 and iter is set to 200. We use Amazon embedding as the domain-specific word embeddings of ST as Amazon corpora is large and comprehensive although not in the same domain. The general-purpose embeddings are initialized by Glove.840B.300d embeddings (Pennington et al., 2014). Its corpus is crawled from the Web. 3.3 Baseline Methods • NN+CRF-{pipelined, joint, collapsed}: An improvement of (Mitchell et al., 2013) that concatenates target word embedding and context four-word embeddings besides using linguistically informed features plus CRF to finish the sequence labeling task (Zhang et al., 2015). Instead of using the officially released code8 due to the outdated library, we reproduce the results with the original settings. • Sentiment-Scope: A collapsed CRF model9 (Li and Lu, 2017), which expands the node types of CRF to capture sentiment scopes. The discrete fea"
P19-1056,S15-2082,0,0.527365,"Missing"
P19-1056,N16-1030,0,0.184943,"Missing"
P19-1056,S16-1002,0,0.254607,"Missing"
P19-1056,S14-2004,0,0.672293,"Missing"
P19-1056,D16-1058,0,0.700068,"rms extraction and aspect sentiment classification. four classes, e.g., positive (PO), conflict (CF), neutral (NT)1 , and negative (NG). To facilitate practical applications, our goal is to solve ATE and ASC simultaneously. For easy description and discussion, these two subtasks are referred to as aspect term-polarity co-extraction. Both ATE and ASC have attracted a great of attention among researchers, but they are rarely solved together at the same time due to some challenges: 1) ATE and ASC are quite different tasks. ATE is an extraction or sequence labeling task (Jakob and Gurevych, 2010; Wang et al., 2016a), while ASC is a classification task (Jiang et al., 2011; Wagner et al., 2014; Tang et al., 2016a,b; Tay et al., 2018). Thus, they are naturally treated as two separate tasks, and solved one by one in a pipeline manner. However, this two-stage framework is complicated and difficult to use in applications because it needs to train two models separately. There is also the latent error propagation when an aspect term is used to classify its corresponding polarity. Thus, due to the different natures of the two tasks, most current works focus either on extracting aspect terms (Yin et al., 2016; L"
P19-1056,J11-1002,1,0.857544,"2015) are also about performing two sequence labeling tasks, but they extract named entities and their sentiment classes jointly. We have a different objective and utilize a different model. Li et al. (2019) have the same objective as us. The main difference is that their approach belongs to a collapsed approach but ours is a joint approach. The model proposed by (Li and Lu, 2017) is also a collapsed approach based on CRF. Its performance is heavily dependent on manually crafted features. been studied by many researchers. Hu and Liu (2004) extracted aspect terms using frequent pattern mining. Qiu et al. (2011) and Liu et al. (2015) proposed to use rule-based approach exploiting either hand-crafted or automatically generated rules about some syntactic relationships. Mei et al. (2007), He et al. (2011) and Chen et al. (2014) used topic modeling based on Latent Dirichlet Allocation (Blei et al., 2003). All of the above methods are unsupervised. For supervised methods, the ATE task is usually treated as a sequence labeling problem solved by CRF. For the ASC task, a large body of literature has tried to utilize the relation or position between the aspect terms and the surrounding context words as the re"
P19-1056,P18-2094,1,0.923427,"a classification task (Jiang et al., 2011; Wagner et al., 2014; Tang et al., 2016a,b; Tay et al., 2018). Thus, they are naturally treated as two separate tasks, and solved one by one in a pipeline manner. However, this two-stage framework is complicated and difficult to use in applications because it needs to train two models separately. There is also the latent error propagation when an aspect term is used to classify its corresponding polarity. Thus, due to the different natures of the two tasks, most current works focus either on extracting aspect terms (Yin et al., 2016; Luo et al., 2018; Xu et al., 2018) or on classifying aspect sentiment (Ma et al., 2017; Wang and Lu, 2018). A possible idea to bridge the difference between the two tasks is to change ASC to a sequence labeling task. Then, ATE and ASC Introduction Aspect terms extraction (ATE) and aspect sentiment classification (ASC) are two fundamental, fine-grained subtasks of aspect-based sentiment analysis. Aspect term extraction is the task of extracting the attributes (or aspects) of an entity upon which opinions have been expressed, and aspect sentiment classification is the task of identifying the polarities expressed on these extract"
P19-1056,D17-1035,0,0.01359,"of ht is 2d, and the size of dI will also become 2d when stacking a new BiReGU layer. We refer the outputs of dual BiReGU as hA and hP separately to differentiate ATE and ASC.  hM = hM + softmaxr SM hM , where softmaxr is a row-based softmax function, M ∈ {A, P}, M = P if M = A, and M = A if M = P. Such an operation can make ATE and ASC get enhanced information from each other. The process is shown in Figure 3b. Interface. To generate the final ATE tags and ASC tags, either a dense layer plus a softmax function or a Conditional Random Fields (CRF) can be used. According to the comparison in (Reimers and Gurevych, 2017), using a CRF instead of a softmax classifier as the last layer can obtain a performance increase for tasks with a high dependency between tags. Thus, we use the linear-chain CRF as our inference layer. Its log-likelihood is computed as follows: Cross-Shared Unit. When generating the representation after BiReGU, the information of ATE and ASC is separated from each other. However, the fact is that the labels of ATE and the labels of ASC have strong relations. For instance, if the label of ATE is O, the label for ASC should be O as well, and if the label of ASC is PO, the label for ATE should b"
P19-1056,D13-1170,0,0.00637693,"re equal) as the final polarity. For example, the final polarity of “PO NT” is “PO”, the final polarity of “PO PO” is also “PO”, and the final polarity of “PO NT NT” is “NT”. This method is simple and effective in our experiments. m where M ∈ {A, P}, m ∈ {a, p}, hm i ∈ hM , and G ∈ K×2d×2d R are 3-dimensional tensors. K is a hyperparameter. A, a and P, p are indexes of ATE and ASC, respectively, m = p, M = A if m = a, and m = a, M = P if m = p. Such tensor operators can be seen as multiple bilinear terms, which have the capability of modeling more complicated compositions between two vectors (Socher et al., 2013; Wang et al., 2017). After obtaining the composition vectors, the attention score SiMj is calculated as: M SiMj = v&gt; m αi j , (5) (4) where vm ∈ RK is a weight vector used to weight each value of the composition vector, M ∈ {A, P}, 594 Datasets #PO #NT SL #NG #CF #PO #NT SR #NG #CF #PO ST #NT #NG Auxiliary Aspect Term Length Enhancement. Although CRF is capable of considering the correlation of two adjacent labels, there are generated discontinuous labels, especially for a long target aspect term. To alleviate the influence resulted from the length of the aspect term, we designed an auxiliary"
P19-1056,D15-1073,0,0.588357,"–601 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics have the same formulation. 2) The number of aspect term-polarity pairs in a sentence is arbitrary. Considering the examples depicted in Figure 1, we can observe that some sentences contain two term-polarity pairs and some sentences contain one pair. Moreover, each aspect term can consist of any number of words, which makes the co-extraction task difficult to solve. Some existing research has treated ATE and ASC as two sequence labeling tasks and dealt with them together. Mitchell et al. (2013) and Zhang et al. (2015) compared pipelined, joint, and collapsed approaches to extracting named entities and their sentiments. They found that the joint and collapsed approaches are superior to the pipelined approach. Li and Lu (2017) proposed a collapsed CRF model. The difference with the standard CRF is that they expanded the node type at each word to capture sentiment scopes. Another interesting work comes from Li et al. (2019), where the authors proposed a unified model with the collapsed approach to do aspect term-polarity co-extraction. We can intuitively explain the pipelined, joint, and collapsed approaches"
P19-1056,C16-1311,0,0.122934,"Missing"
P19-1056,D16-1021,0,0.248164,"F), neutral (NT)1 , and negative (NG). To facilitate practical applications, our goal is to solve ATE and ASC simultaneously. For easy description and discussion, these two subtasks are referred to as aspect term-polarity co-extraction. Both ATE and ASC have attracted a great of attention among researchers, but they are rarely solved together at the same time due to some challenges: 1) ATE and ASC are quite different tasks. ATE is an extraction or sequence labeling task (Jakob and Gurevych, 2010; Wang et al., 2016a), while ASC is a classification task (Jiang et al., 2011; Wagner et al., 2014; Tang et al., 2016a,b; Tay et al., 2018). Thus, they are naturally treated as two separate tasks, and solved one by one in a pipeline manner. However, this two-stage framework is complicated and difficult to use in applications because it needs to train two models separately. There is also the latent error propagation when an aspect term is used to classify its corresponding polarity. Thus, due to the different natures of the two tasks, most current works focus either on extracting aspect terms (Yin et al., 2016; Luo et al., 2018; Xu et al., 2018) or on classifying aspect sentiment (Ma et al., 2017; Wang and Lu"
P19-1056,S14-2036,0,0.0230578,"ive (PO), conflict (CF), neutral (NT)1 , and negative (NG). To facilitate practical applications, our goal is to solve ATE and ASC simultaneously. For easy description and discussion, these two subtasks are referred to as aspect term-polarity co-extraction. Both ATE and ASC have attracted a great of attention among researchers, but they are rarely solved together at the same time due to some challenges: 1) ATE and ASC are quite different tasks. ATE is an extraction or sequence labeling task (Jakob and Gurevych, 2010; Wang et al., 2016a), while ASC is a classification task (Jiang et al., 2011; Wagner et al., 2014; Tang et al., 2016a,b; Tay et al., 2018). Thus, they are naturally treated as two separate tasks, and solved one by one in a pipeline manner. However, this two-stage framework is complicated and difficult to use in applications because it needs to train two models separately. There is also the latent error propagation when an aspect term is used to classify its corresponding polarity. Thus, due to the different natures of the two tasks, most current works focus either on extracting aspect terms (Yin et al., 2016; Luo et al., 2018; Xu et al., 2018) or on classifying aspect sentiment (Ma et al."
P19-1056,P18-1088,1,0.859026,"Allocation (Blei et al., 2003). All of the above methods are unsupervised. For supervised methods, the ATE task is usually treated as a sequence labeling problem solved by CRF. For the ASC task, a large body of literature has tried to utilize the relation or position between the aspect terms and the surrounding context words as the relevant information or context for prediction (Tang et al., 2016a; Laddha and Mukherjee, 2016). Convolution neural networks (CNNs) (Poria et al., 2016; Li and Xue, 2018), attention network (Wang et al., 2016b; Ma et al., 2017; He et al., 2017), and memory network (Wang et al., 2018) are also active approaches. However, the above methods are proposed for either the ATE or the ASC task. Lakkaraju et al. (2014) proposed to use hierarchical deep learning to solve these two subtasks. Wu et al. (2016) utilized cascaded CNN and multi-task CNN to address aspect extraction and sentiment classification. Their main idea is to directly map each review sentence into pre-defined aspect terms by using classification and then classifying the corresponding polarities. We believe the pre-defined aspect terms are in general insufficient for most analysis applications because they will almo"
P19-1056,D16-1059,0,0.252075,"rms extraction and aspect sentiment classification. four classes, e.g., positive (PO), conflict (CF), neutral (NT)1 , and negative (NG). To facilitate practical applications, our goal is to solve ATE and ASC simultaneously. For easy description and discussion, these two subtasks are referred to as aspect term-polarity co-extraction. Both ATE and ASC have attracted a great of attention among researchers, but they are rarely solved together at the same time due to some challenges: 1) ATE and ASC are quite different tasks. ATE is an extraction or sequence labeling task (Jakob and Gurevych, 2010; Wang et al., 2016a), while ASC is a classification task (Jiang et al., 2011; Wagner et al., 2014; Tang et al., 2016a,b; Tay et al., 2018). Thus, they are naturally treated as two separate tasks, and solved one by one in a pipeline manner. However, this two-stage framework is complicated and difficult to use in applications because it needs to train two models separately. There is also the latent error propagation when an aspect term is used to classify its corresponding polarity. Thus, due to the different natures of the two tasks, most current works focus either on extracting aspect terms (Yin et al., 2016; L"
W10-4171,E09-1013,0,0.0434854,"Missing"
W10-4171,S07-1002,0,0.0738724,"Missing"
W10-4171,P04-3026,0,0.0772313,"Missing"
W10-4171,E06-1018,0,\N,Missing
W16-3603,J80-3005,0,0.721055,"Missing"
W16-3603,H90-1021,0,0.596086,"ed joint online SLU-LM model and its variations. Section 4 discusses the experiment setup and results on ATIS benchmarking task, using both text and noisy speech inputs. Section 5 gives the conclusion. 2 2.1 A major task in spoken language understanding (SLU) is to extract semantic constituents by searching input text to fill in values for predefined slots in a semantic frame (Mesnil et al., 2015), which is often referred to as slot filling. The slot filling task can also be viewed as assigning an appropriate semantic label to each word in the given input text. In the below example from ATIS (Hemphill et al., 1990) corpus following the popular in/out/begin (IOB) annotation method, Seattle and San Diego are the from and to locations respectively according to the slot labels, and tomorrow is the departure date. Other words in the example utterance that carry no semantic meaning are assigned “O” label. Figure 1: ATIS corpus sample with intent and slot annotation (IOB format). Background Given an utterance consisting of a sequence of words w = (w1 , w2 , ..., wT ), the goal of slot filling is to find a sequence of semantic labels s = (s1 , s2 , ..., sT ), one for each word in the utterance, such that: Inten"
W16-3603,D14-1181,0,0.00189304,"Missing"
W18-5041,N18-1187,1,0.671152,"using adversarial learning in neural machine translation. The use of adversarial learning in task-oriented dialogs has not been well Task-Oriented Dialog Learning Popular approaches in learning task-oriented dialog systems include modeling the task as a partially observable Markov Decision Process (POMDP) (Young et al., 2013). Reinforcement learning can be applied in the POMDP framework to learn dialog policy online by interacting with users (Gaˇsi´c et al., 2013). Recent efforts have been made in designing endto-end solutions (Williams and Zweig, 2016; Liu and Lane, 2017a; Li et al., 2017b; Liu et al., 2018) for task-oriented dialogs. Wen et al. (2017) designed a supervised training end-to-end neural dialog model with modularly connected components. Bordes and Weston (2017) proposed a neural dialog model using end-to-end memory networks. These models are trained offline using fixed dialog corpora, and thus it is unknown how well 351 studied. Peng et al. (2018) recently explored using adversarial loss as an extra critic in addition to the main reward function based on task completion. This method still requires prior knowledge of a user’s goal, which can be hard to collect in practice, in defining"
W18-5041,P17-1045,0,0.0825591,"pairs (Li et al., 2016; Serban et al., 2016), taskoriented dialog systems (Young et al., 2013; Williams et al., 2017) involve retrieving information from external resources and reasoning over multiple dialog turns. This makes it especially im350 Proceedings of the SIGDIAL 2018 Conference, pages 350–359, c Melbourne, Australia, 12-14 July 2018. 2018 Association for Computational Linguistics the model performance generalizes to online user interactions. Williams et al. (2017) proposed a hybrid code network for task-oriented dialog that can be trained with supervised and reinforcement learning. Dhingra et al. (2017) proposed a reinforcement learning dialog agent for information access. Such models are trained against rule-based user simulators. A dialog reward from the user simulator is expected at the end of each turn or each dialog. sion (Denton et al., 2015) and natural language generation (Li et al., 2017a), we propose an adversarial learning method for task-oriented dialog systems. We jointly train two models, a generator that interacts with the environment to produce task-oriented dialogs, and a discriminator that marks a dialog sample as being successful or not. The generator is a neural network b"
W18-5041,D17-1237,0,0.165941,"ch domain for our model training and evaluation. We add entity information to each dialog sample in the original DSTC2 dataset. This makes entity information a part of the model training process, enabling the agent to handle entities during interactive evaluation with users. Different from the agent action definition used in DSTC2, actions in our system are produced by concatenating the act Training Settings We use a user simulator for our interactive training and evaluation with adversarial learning. Instead of using a rule-based user simulator as in many prior work (Zhao and Eskenazi, 2016; Peng et al., 2017), in our study we use a model-based simulator trained on DSTC2 dataset. We follow the design and training procedures of (Liu and Lane, 2017b) in building the model-based simulator. The stochastic policy used in the simulator introduces additional diversity in user behavior 354 4.3 during dialog simulation. Results and Analysis In this section, we show and discuss our empirical evaluation results. We first compare dialog agent trained using the proposed adversarial reward to those using human designed reward and using oracle reward. We then discuss the impact of discriminator model design and m"
W18-5041,W14-4337,0,0.0547844,"output (e.g. “conf irm(f ood = italian)” maps to “conf irm f ood”). The slot values are captured in the belief tracking outputs. Table 1 shows the statistics of the dataset used in our experiments. # of train/dev/test dialogs # of dialog turns in average # of slot value options Area Food Price range We continue to update both the dialog agent and the reward function via dialog simulation or real user interaction until convergence. 4 4.1 Experiments 1612/506/ 1117 7.88 5 91 3 Table 1: Statistics of DSTC2 dataset. Dataset 4.2 We use data from the second Dialog State Tracking Challenge (DSTC2) (Henderson et al., 2014) in the restaurant search domain for our model training and evaluation. We add entity information to each dialog sample in the original DSTC2 dataset. This makes entity information a part of the model training process, enabling the agent to handle entities during interactive evaluation with users. Different from the agent action definition used in DSTC2, actions in our system are produced by concatenating the act Training Settings We use a user simulator for our interactive training and evaluation with adversarial learning. Instead of using a rule-based user simulator as in many prior work (Zh"
W18-5041,P16-1094,0,0.0322141,". In the evaluation in a restaurant search domain, we show that the proposed adversarial dialog learning method achieves advanced dialog success rate comparing to strong baseline methods. We further discuss the covariate shift problem in online adversarial dialog learning and show how we can address that with partial access to user feedback. 1 Introduction Task-oriented dialog systems are designed to assist user in completing daily tasks, such as making reservations and providing customer support. Comparing to chit-chat systems that are usually modeled with single-turn context-response pairs (Li et al., 2016; Serban et al., 2016), taskoriented dialog systems (Young et al., 2013; Williams et al., 2017) involve retrieving information from external resources and reasoning over multiple dialog turns. This makes it especially im350 Proceedings of the SIGDIAL 2018 Conference, pages 350–359, c Melbourne, Australia, 12-14 July 2018. 2018 Association for Computational Linguistics the model performance generalizes to online user interactions. Williams et al. (2017) proposed a hybrid code network for task-oriented dialog that can be trained with supervised and reinforcement learning. Dhingra et al. (2017) p"
W18-5041,D17-1230,0,0.114787,"c Melbourne, Australia, 12-14 July 2018. 2018 Association for Computational Linguistics the model performance generalizes to online user interactions. Williams et al. (2017) proposed a hybrid code network for task-oriented dialog that can be trained with supervised and reinforcement learning. Dhingra et al. (2017) proposed a reinforcement learning dialog agent for information access. Such models are trained against rule-based user simulators. A dialog reward from the user simulator is expected at the end of each turn or each dialog. sion (Denton et al., 2015) and natural language generation (Li et al., 2017a), we propose an adversarial learning method for task-oriented dialog systems. We jointly train two models, a generator that interacts with the environment to produce task-oriented dialogs, and a discriminator that marks a dialog sample as being successful or not. The generator is a neural network based task-oriented dialog agent. The environment that the dialog agent interacts with is the user. Quality of a dialog produced by the agent and the user is measured by the likelihood that it fools the discriminator to believe that the dialog is a successful one conducted by a human agent. We treat"
W18-5041,W17-5518,0,0.113479,"7) designed a hybrid supervised and reinforcement learning end-to-end dialog agent. Dhingra et al. (2017) proposed an RL based model for information access that can learn online via user interactions. Such systems assume the model has access to a reward signal at the end of a dialog, either in the form of a binary user feedback or a continuous user score. A challenge with such learning systems is that user feedback may be inconsistent (Su et al., 2016) and may not always be available in practice. Further more, online dialog policy learning with RL usually suffers from sample efficiency issue (Su et al., 2017), which requires an agent to make a large number of feedback queries to users. To reduce the high demand for user feedback in online policy learning, solutions have been proposed to design or to learn a reward function that can be used to generate a reward in approximation to a user feedback. Designing a good reward function is not easy (Walker et al., 1997) as it typically requires strong domain knowledge. El Asri et al. (2014) proposed a learning based reward function that is trained with task completion transfer learning. Su et al. (2016) proposed an online active learning method for reward"
W18-5041,I17-1074,0,0.0886693,"c Melbourne, Australia, 12-14 July 2018. 2018 Association for Computational Linguistics the model performance generalizes to online user interactions. Williams et al. (2017) proposed a hybrid code network for task-oriented dialog that can be trained with supervised and reinforcement learning. Dhingra et al. (2017) proposed a reinforcement learning dialog agent for information access. Such models are trained against rule-based user simulators. A dialog reward from the user simulator is expected at the end of each turn or each dialog. sion (Denton et al., 2015) and natural language generation (Li et al., 2017a), we propose an adversarial learning method for task-oriented dialog systems. We jointly train two models, a generator that interacts with the environment to produce task-oriented dialogs, and a discriminator that marks a dialog sample as being successful or not. The generator is a neural network based task-oriented dialog agent. The environment that the dialog agent interacts with is the user. Quality of a dialog produced by the agent and the user is measured by the likelihood that it fools the discriminator to believe that the dialog is a successful one conducted by a human agent. We treat"
W18-5041,W16-3601,0,0.0449355,"4) in the restaurant search domain for our model training and evaluation. We add entity information to each dialog sample in the original DSTC2 dataset. This makes entity information a part of the model training process, enabling the agent to handle entities during interactive evaluation with users. Different from the agent action definition used in DSTC2, actions in our system are produced by concatenating the act Training Settings We use a user simulator for our interactive training and evaluation with adversarial learning. Instead of using a rule-based user simulator as in many prior work (Zhao and Eskenazi, 2016; Peng et al., 2017), in our study we use a model-based simulator trained on DSTC2 dataset. We follow the design and training procedures of (Liu and Lane, 2017b) in building the model-based simulator. The stochastic policy used in the simulator introduces additional diversity in user behavior 354 4.3 during dialog simulation. Results and Analysis In this section, we show and discuss our empirical evaluation results. We first compare dialog agent trained using the proposed adversarial reward to those using human designed reward and using oracle reward. We then discuss the impact of discriminato"
W18-5041,P16-1230,0,0.115413,"Missing"
W18-5041,P97-1035,0,0.914056,". A challenge with such learning systems is that user feedback may be inconsistent (Su et al., 2016) and may not always be available in practice. Further more, online dialog policy learning with RL usually suffers from sample efficiency issue (Su et al., 2017), which requires an agent to make a large number of feedback queries to users. To reduce the high demand for user feedback in online policy learning, solutions have been proposed to design or to learn a reward function that can be used to generate a reward in approximation to a user feedback. Designing a good reward function is not easy (Walker et al., 1997) as it typically requires strong domain knowledge. El Asri et al. (2014) proposed a learning based reward function that is trained with task completion transfer learning. Su et al. (2016) proposed an online active learning method for reward estimation using Gaussian process classification. These methods still require annotations of dialog ratings by users, and thus may also suffer from the rating consistency and learning efficiency issues. To address the above discussed challenges, we investigate the effectiveness of learning dialog rewards directly from dialog samples. Inspired by the success"
W18-5041,E17-1042,0,0.0846823,"Missing"
W18-5041,P17-1062,0,0.122718,"dialog learning method achieves advanced dialog success rate comparing to strong baseline methods. We further discuss the covariate shift problem in online adversarial dialog learning and show how we can address that with partial access to user feedback. 1 Introduction Task-oriented dialog systems are designed to assist user in completing daily tasks, such as making reservations and providing customer support. Comparing to chit-chat systems that are usually modeled with single-turn context-response pairs (Li et al., 2016; Serban et al., 2016), taskoriented dialog systems (Young et al., 2013; Williams et al., 2017) involve retrieving information from external resources and reasoning over multiple dialog turns. This makes it especially im350 Proceedings of the SIGDIAL 2018 Conference, pages 350–359, c Melbourne, Australia, 12-14 July 2018. 2018 Association for Computational Linguistics the model performance generalizes to online user interactions. Williams et al. (2017) proposed a hybrid code network for task-oriented dialog that can be trained with supervised and reinforcement learning. Dhingra et al. (2017) proposed a reinforcement learning dialog agent for information access. Such models are trained a"
W18-5041,N18-1122,0,0.0488912,"Missing"
W18-5041,N16-1174,0,0.00794326,"M state sk . At each dialog turn k, user input Uk and previous system output Ak−1 are firstly encoded to continuous representations. The user input can either in the form of a dialog act or a natural language utterance. We use dialog act form user input in our experiment. The dialog act representation is obtained by concatenating the embeddings of the act and the slotvalue pairs. If natural language form of input is used, we can encode the sequence of words using a bidirectional RNN and take the concatenation of the last forward and backward states as the utterance representation, similar to (Yang et al., 2016) and (Liu and Lane, 2017a). With the user input Uk and agent input Ak−1 , the dialog state sk is updated from the previous state sk−1 by: sk = LSTMG (sk−1 , [Uk , Ak−1 ]) (1) Belief Tracking Belief tracking maintains the state of a conversation, such as a user’s goals, by accumulating evidence along the sequence of dialog turns. A user’s goal is represented by a list of slot-value pairs. The belief tracker updates its estimation of the user’s goal by maintaining a probability distribution P (lkm ) over candidate values for each of the tracked goal slot type m ∈ M . With the current dialog stat"
W19-5903,W17-2609,0,0.0287178,"ut entity h, t and relation r correspond to high-dimensional vectors (either “one-hot” index vector or “n-hot” feature vector) xh , xt and xr respectively, which are then projected into low dimensional vectors vh , vt and vr using an entity embedding matrix WE and relation embedding matrix WR as given byvh = WE xh , vr = WR xr and vt = WE xt . The scoring function S(.) is then used to compute a validity score S(h, r, t) of the triple. Any KBE model can be used for learning M. For evaluation, we adopt DistMult (Yang et al., 2014) for its state-of-the art performance over many other KBE models (Kadlec et al., 2017). The scoring function of DistMult is defined as follows: S(h, r, t) = vhT diag(vr )vt = N X vh [i]vr [i]vt [i] Rejection in KB Inference. For a query with no answer entity existing in K, CILK attempts to reject the query from being answered. To decide whether to reject the query or not, CILK maintains a threshold buffer T that stores entity and relation specific prediction thresholds and updates it continuously over time, as described below. Besides the dataset for training M, CILK also creates a validation dataset Dvd , consisting of a set of validation query tuples of the form (q, E + , E −"
W19-5903,C02-1084,0,0.269518,"Missing"
W19-5903,P15-1034,0,0.0210722,"ike response generation, semantic parsing, fact extraction from user utterances, entity linking, etc., which have been studied extensively before and are assumed to be available for use. Thus, this paper works only with structured queries (h, r, ?), e.g., (Boston, LocatedInCountry, ?) meaning “In what Country is Boston located ?,” or (?, r, t), e.g., (?, PresidentOf, USA) meaning “Who is the President of USA?” It assumes that a semantic parser is available that can convert natural language queries from users into query triples. Similarly, it assumes an information extraction tool like OpenIE (Angeli et al., 2015) is employed to extract facts as triples (h, r, t) from Dialogue systems are increasingly using knowledge bases (KBs) storing real-world facts to help generate quality responses. However, as the KBs are inherently incomplete and remain fixed during conversation, it limits dialogue systems’ ability to answer questions and to handle questions involving entities or relations that are not in the KB. In this paper, we make an attempt to propose an engine for Continuous and Interactive Learning of Knowledge (CILK) for dialogue systems to give them the ability to continuously and interactively learn"
W19-5903,P12-3007,0,0.0253955,"s paper, we make an attempt to propose an engine for Continuous and Interactive Learning of Knowledge (CILK) for dialogue systems to give them the ability to continuously and interactively learn and infer new knowledge during conversations. With more knowledge accumulated over time, they will be able to learn better and answer more questions. Our empirical evaluation shows that CILK is promising. 1 Introduction Dialogue systems, including question-answering (QA) systems are now commonly used in practice. Early such systems were built mainly based on rules and information retrieval techniques (Banchs and Li, 2012; Ameixa et al., 2014; Lowe et al., 2015; Serban et al., 2015). Recent deep learning models (Vinyals and Le, 2015; Xing et al., 2017; Li et al., 2017c) learn from large corpora. However, since they do not use explicit knowledge bases (KBs), they often suffer from generic and dull responses (Xing et al., 2017; Young et al., 2018). KBs have been used to deal with the problem (Ghazvininejad et al., 2018; Le et al., 2016; Young et al., 2018; Long et al., 2017; Zhou et al., 2018). Many task-oriented dialogue systems (Eric and Manning, 2017; Madotto et al., 2018) also use KBs to support information-"
W19-5903,D11-1049,0,0.516347,"fact. which is then scored. The entity e with the highest score is predicted as the answer of the query. Scoring each candidate is modeled as a knowledge base completion (KBC) problem (Lao and Cohen, 2010; Bordes et al., 2011). KBC aims to infer new facts (knowledge) from existing facts in a KB and is defined as a link prediction problem: Given a query triple, (e, r, ?) [or (?, r, e)], it predicts a tail entity ttrue [head entity htrue ] which makes the query triple true and thus should be added to the KB. KBC makes the closed-world assumption that h, r and t are all known to exist in the KB (Lao et al., 2011; Bordes et al., 2011, 2013; Nickel et al., 2015). This is not suitable for knowledge learning in conversations because in a conversation, the user can ask or say anything, which may contain entities and relations that are not in the KB. CILK removes the closed-world assumption and allows all h (or t) and/or r to be unknown (not in the KB). Step 1 above basically asks the user questions to make h (or t) and/or r known to the KB. Then, an existing KBC model as a query inference model can be applied to retrieve an answer entity from KB. Figure 1 shows an example. CILK acquires supporting facts S"
W19-5903,D14-1067,0,0.0649801,"Missing"
W19-5903,W16-1611,0,0.0240843,"systems, including question-answering (QA) systems are now commonly used in practice. Early such systems were built mainly based on rules and information retrieval techniques (Banchs and Li, 2012; Ameixa et al., 2014; Lowe et al., 2015; Serban et al., 2015). Recent deep learning models (Vinyals and Le, 2015; Xing et al., 2017; Li et al., 2017c) learn from large corpora. However, since they do not use explicit knowledge bases (KBs), they often suffer from generic and dull responses (Xing et al., 2017; Young et al., 2018). KBs have been used to deal with the problem (Ghazvininejad et al., 2018; Le et al., 2016; Young et al., 2018; Long et al., 2017; Zhou et al., 2018). Many task-oriented dialogue systems (Eric and Manning, 2017; Madotto et al., 2018) also use KBs to support information-seeking conversations. One major shortcoming of existing systems that use KBs is that the KBs are fixed once the dialogue systems are deployed. However, it is almost impossible for the initial KBs to contain all possible 21 Proceedings of the SIGDial 2019 Conference, pages 21–31 c Stockholm, Sweden, 11-13 September 2019. 2019 Association for Computational Linguistics USER: (Boston, LocatedInCountry, ?) “In what Count"
W19-5903,D17-1230,0,0.231925,"y to continuously and interactively learn and infer new knowledge during conversations. With more knowledge accumulated over time, they will be able to learn better and answer more questions. Our empirical evaluation shows that CILK is promising. 1 Introduction Dialogue systems, including question-answering (QA) systems are now commonly used in practice. Early such systems were built mainly based on rules and information retrieval techniques (Banchs and Li, 2012; Ameixa et al., 2014; Lowe et al., 2015; Serban et al., 2015). Recent deep learning models (Vinyals and Le, 2015; Xing et al., 2017; Li et al., 2017c) learn from large corpora. However, since they do not use explicit knowledge bases (KBs), they often suffer from generic and dull responses (Xing et al., 2017; Young et al., 2018). KBs have been used to deal with the problem (Ghazvininejad et al., 2018; Le et al., 2016; Young et al., 2018; Long et al., 2017; Zhou et al., 2018). Many task-oriented dialogue systems (Eric and Manning, 2017; Madotto et al., 2018) also use KBs to support information-seeking conversations. One major shortcoming of existing systems that use KBs is that the KBs are fixed once the dialogue systems are deployed. Howev"
W19-5903,W17-5506,0,0.0295478,"lt mainly based on rules and information retrieval techniques (Banchs and Li, 2012; Ameixa et al., 2014; Lowe et al., 2015; Serban et al., 2015). Recent deep learning models (Vinyals and Le, 2015; Xing et al., 2017; Li et al., 2017c) learn from large corpora. However, since they do not use explicit knowledge bases (KBs), they often suffer from generic and dull responses (Xing et al., 2017; Young et al., 2018). KBs have been used to deal with the problem (Ghazvininejad et al., 2018; Le et al., 2016; Young et al., 2018; Long et al., 2017; Zhou et al., 2018). Many task-oriented dialogue systems (Eric and Manning, 2017; Madotto et al., 2018) also use KBs to support information-seeking conversations. One major shortcoming of existing systems that use KBs is that the KBs are fixed once the dialogue systems are deployed. However, it is almost impossible for the initial KBs to contain all possible 21 Proceedings of the SIGDial 2019 Conference, pages 21–31 c Stockholm, Sweden, 11-13 September 2019. 2019 Association for Computational Linguistics USER: (Boston, LocatedInCountry, ?) “In what Country is Boston located?” [Query] CILK: I do not know what “located in Country” means? Can you provide me an example? [Ask"
W19-5903,D14-1044,0,0.0783366,"Missing"
W19-5903,W15-4640,0,0.0312056,"engine for Continuous and Interactive Learning of Knowledge (CILK) for dialogue systems to give them the ability to continuously and interactively learn and infer new knowledge during conversations. With more knowledge accumulated over time, they will be able to learn better and answer more questions. Our empirical evaluation shows that CILK is promising. 1 Introduction Dialogue systems, including question-answering (QA) systems are now commonly used in practice. Early such systems were built mainly based on rules and information retrieval techniques (Banchs and Li, 2012; Ameixa et al., 2014; Lowe et al., 2015; Serban et al., 2015). Recent deep learning models (Vinyals and Le, 2015; Xing et al., 2017; Li et al., 2017c) learn from large corpora. However, since they do not use explicit knowledge bases (KBs), they often suffer from generic and dull responses (Xing et al., 2017; Young et al., 2018). KBs have been used to deal with the problem (Ghazvininejad et al., 2018; Le et al., 2016; Young et al., 2018; Long et al., 2017; Zhou et al., 2018). Many task-oriented dialogue systems (Eric and Manning, 2017; Madotto et al., 2018) also use KBs to support information-seeking conversations. One major shortco"
W19-5903,P18-1136,0,0.0321266,"and information retrieval techniques (Banchs and Li, 2012; Ameixa et al., 2014; Lowe et al., 2015; Serban et al., 2015). Recent deep learning models (Vinyals and Le, 2015; Xing et al., 2017; Li et al., 2017c) learn from large corpora. However, since they do not use explicit knowledge bases (KBs), they often suffer from generic and dull responses (Xing et al., 2017; Young et al., 2018). KBs have been used to deal with the problem (Ghazvininejad et al., 2018; Le et al., 2016; Young et al., 2018; Long et al., 2017; Zhou et al., 2018). Many task-oriented dialogue systems (Eric and Manning, 2017; Madotto et al., 2018) also use KBs to support information-seeking conversations. One major shortcoming of existing systems that use KBs is that the KBs are fixed once the dialogue systems are deployed. However, it is almost impossible for the initial KBs to contain all possible 21 Proceedings of the SIGDial 2019 Conference, pages 21–31 c Stockholm, Sweden, 11-13 September 2019. 2019 Association for Computational Linguistics USER: (Boston, LocatedInCountry, ?) “In what Country is Boston located?” [Query] CILK: I do not know what “located in Country” means? Can you provide me an example? [Ask for Clue] USER: (London"
W19-5903,P16-1224,0,0.030956,"erred query answer is not added to the KB as it may be incorrect. But it can be added in a multi-user environment through cross-verification (see footnote 1 and Sec. 4). 22 2 Related Work In the following subsections, we formalize the interactive knowledge learning problem (Sec. 3.1), describe the Inference Model M (Sec. 3.2) and discuss how CILK interacts and processes a query from the user (Sec. 3.3). To the best of our knowledge, no existing system can perform the proposed task. We reported a priliminary research in (Mazumder et al., 2018). CILK is related to interactive language learning (Wang et al., 2016, 2017), which is mainly about language grounding, not about knowledge learning. Li et al. (2017a,b) and Zhang et al. (2017) train chatbots using human teachers who can ask and answer the chatbot questions. Ono et al. (2017), Otsuka et al. (2013), Ono et al. (2016) and Komatani et al. (2016) allow a system to ask the user whether its prediction of category of a term is correct or not. Compared to these works, CILK performs interactive knowledge learning and inference (over existing and acquired knowledge) while conversing with users after the dialogue system has been deployed (i.e., learning o"
W19-5903,P17-1086,0,0.0330841,"Missing"
W19-5903,P84-1044,0,0.736188,"Missing"
W19-5903,D18-1223,0,0.0152358,"ersing with users after the dialogue system has been deployed (i.e., learning on the job (Chen and Liu, 2018)) without any teacher supervision or help. NELL (Mitchell et al., 2015) updates its KB using facts extracted from the Web (complementary to our work). We do not do Web fact extraction. KB completion (KBC) has been studied in recent years (Lao et al., 2011; Bordes et al., 2011, 2015; Mazumder and Liu, 2017). But they mainly handle facts with known entities and relations. Neelakantan et al. (2015) work on fixed unknown relations with known embeddings, but does not allow unknown entities. Xiong et al. (2018) also deal with queries involving unknown relations, but known entities in the KB. Shi and Weninger (2018) handles unknown entities by exploiting an external text corpus. None of the KBC methods perform conversational knowledge learning like CILK. 3 3.1 CILK’s KB K is a triple store {(h, r, t)} ⊆ E ×R× E, where E is the entity set and R is the relation set. Let q be a query of the form (e, r, ?) [or (?, r, e)] issued to CILK, where e is termed as query entity and r as the query relation. If e ∈ / E and/or r ∈ /R (we also say e, r ∈ / K), we call q an open-world query. Otherwise, q is referred"
W19-5903,P15-1016,0,0.0177122,"se works, CILK performs interactive knowledge learning and inference (over existing and acquired knowledge) while conversing with users after the dialogue system has been deployed (i.e., learning on the job (Chen and Liu, 2018)) without any teacher supervision or help. NELL (Mitchell et al., 2015) updates its KB using facts extracted from the Web (complementary to our work). We do not do Web fact extraction. KB completion (KBC) has been studied in recent years (Lao et al., 2011; Bordes et al., 2011, 2015; Mazumder and Liu, 2017). But they mainly handle facts with known entities and relations. Neelakantan et al. (2015) work on fixed unknown relations with known embeddings, but does not allow unknown entities. Xiong et al. (2018) also deal with queries involving unknown relations, but known entities in the KB. Shi and Weninger (2018) handles unknown entities by exploiting an external text corpus. None of the KBC methods perform conversational knowledge learning like CILK. 3 3.1 CILK’s KB K is a triple store {(h, r, t)} ⊆ E ×R× E, where E is the entity set and R is the relation set. Let q be a query of the form (e, r, ?) [or (?, r, e)] issued to CILK, where e is termed as query entity and r as the query relat"
W19-5903,W17-5507,0,0.146146,"malize the interactive knowledge learning problem (Sec. 3.1), describe the Inference Model M (Sec. 3.2) and discuss how CILK interacts and processes a query from the user (Sec. 3.3). To the best of our knowledge, no existing system can perform the proposed task. We reported a priliminary research in (Mazumder et al., 2018). CILK is related to interactive language learning (Wang et al., 2016, 2017), which is mainly about language grounding, not about knowledge learning. Li et al. (2017a,b) and Zhang et al. (2017) train chatbots using human teachers who can ask and answer the chatbot questions. Ono et al. (2017), Otsuka et al. (2013), Ono et al. (2016) and Komatani et al. (2016) allow a system to ask the user whether its prediction of category of a term is correct or not. Compared to these works, CILK performs interactive knowledge learning and inference (over existing and acquired knowledge) while conversing with users after the dialogue system has been deployed (i.e., learning on the job (Chen and Liu, 2018)) without any teacher supervision or help. NELL (Mitchell et al., 2015) updates its KB using facts extracted from the Web (complementary to our work). We do not do Web fact extraction. KB comple"
W19-5903,W13-4009,0,0.382147,"ive knowledge learning problem (Sec. 3.1), describe the Inference Model M (Sec. 3.2) and discuss how CILK interacts and processes a query from the user (Sec. 3.3). To the best of our knowledge, no existing system can perform the proposed task. We reported a priliminary research in (Mazumder et al., 2018). CILK is related to interactive language learning (Wang et al., 2016, 2017), which is mainly about language grounding, not about knowledge learning. Li et al. (2017a,b) and Zhang et al. (2017) train chatbots using human teachers who can ask and answer the chatbot questions. Ono et al. (2017), Otsuka et al. (2013), Ono et al. (2016) and Komatani et al. (2016) allow a system to ask the user whether its prediction of category of a term is correct or not. Compared to these works, CILK performs interactive knowledge learning and inference (over existing and acquired knowledge) while conversing with users after the dialogue system has been deployed (i.e., learning on the job (Chen and Liu, 2018)) without any teacher supervision or help. NELL (Mitchell et al., 2015) updates its KB using facts extracted from the Web (complementary to our work). We do not do Web fact extraction. KB completion (KBC) has been st"
W19-5922,P16-1154,0,0.241581,"opy score φc (yjk ) based on the current step’s hidI den state hkj , (3) calculate the probability using the copy mechanism: I I ckj = Attn(hkj−1 , {hE i }),   I I kI I hkj = GRUI (ckj ◦ eyj ), hkj−1 , I I I φg (yjk ) = WgK · hkj , I kI I (1) I φc (yjk ) = tanh(WcK · hyj ) · hkj , I yjk ∈ At−1 ∪ Bt−1 ∪ Ut ,   I I I I kI P (yjk |yj−1 , hkj−1 ) = Copy φc (yjk ), φg (yjk ) , I where for each informable slot k I , y0k = k I and kI I yj hk0 = hE is the embedding of the curl , e rent input word (the one generated at the previous I I step), and WgK and WcK are learned weight matrices. We follow (Gu et al., 2016) and (Bahdanau et al., 2015) for the copy Copy(·, ·) and attention Attn(·, ·) mechanisms implementation respectively. The loss for the informable slot values decoder is calculated as follows: LI = − 1 1 XX I |{k } ||Y kI |I j k I log P (yjk = (2) I I kI zjk |yj−1 , hkj−1 ), I where Y K is the sequence of informable slot value decoder predictions and z is the ground truth label. 3.3 Requestable Slot Binary Classifier As the other part of a belief state, requestable slots are the attributes of KB entries that are explicitly requested by the user. We introduce a separate 181 multi-label requestab"
W19-5922,P18-1133,0,0.134438,"l., 2017; Lowe et al., 2017; Li et al., 2018; Liu et al., 2018; Budzianowski et al., 2018; Bordes et al., 2017; Wen et al., 2017b; Serban et al., 2016, among others). These systems train one whole model to read the current user’s utterance, the past state (that may contain all previous interactions) and generate the current state and response. There are two main approaches for modeling the belief state in end-to-end task-oriented dialogue systems in the literature: the fully structured approach based on classification (Wen et al., 2017b,a), and the free-form approach based on text generation (Lei et al., 2018). The fully structured approaches (Ramadan et al., 2018; Ren et al., 2018) use the full structure of the KB, both its schema and the values available in it, and assumes that the sets of informable slot values and requestable slots are fixed. In real-world scenarios, this assumption is too restrictive as the content of the KB may change and users’ utterances may contain information outside the pre-defined sets. An ideal end-toend architecture for state tracking should be able to identify the values of the informable slots and the requestable slots, easily adapt to new domains, to the changes in"
W19-5922,W17-5506,0,0.207856,"to generate the slots that will be present in the response more precisely before generating the final textual agent response (see Section 3 for details). 1. FSDM, a task-oriented dialogue system with a new belief state tracking architecture that overcomes the limits of existing approaches and scales to real-world settings; 2. a new module, namely the response slot binary classifier, that helps to improve the performance of agent response generation; 3. FSDM achieves state-of-the-art results on both the Cambridge Restaurant dataset (Wen et al., 2017b) and the Stanford in-car assistant dataset (Eric et al., 2017) without the need for fine-tuning through reinforcement learning 2 Related Work Our work is related to end-to-end task-oriented dialogue systems in general (Liu and Lane, 2018; Williams et al., 2017; Lowe et al., 2017; Li et al., 2018; Liu et al., 2018; Budzianowski et al., 2018; Bordes et al., 2017; Hori et al., 2016; Wen et al., 2017b; Serban et al., 2016, among others) and those that extend the Seq2Seq (Sutskever et al., 2014) architecture in particular (Eric et al., 2017; Fung et al., 2018; Wen et al., 2018). Belief tracking, which is necessary to form KB queries, is not explicitly perform"
W19-5922,P18-1136,0,0.0144445,"both the Cambridge Restaurant dataset (Wen et al., 2017b) and the Stanford in-car assistant dataset (Eric et al., 2017) without the need for fine-tuning through reinforcement learning 2 Related Work Our work is related to end-to-end task-oriented dialogue systems in general (Liu and Lane, 2018; Williams et al., 2017; Lowe et al., 2017; Li et al., 2018; Liu et al., 2018; Budzianowski et al., 2018; Bordes et al., 2017; Hori et al., 2016; Wen et al., 2017b; Serban et al., 2016, among others) and those that extend the Seq2Seq (Sutskever et al., 2014) architecture in particular (Eric et al., 2017; Fung et al., 2018; Wen et al., 2018). Belief tracking, which is necessary to form KB queries, is not explicitly performed in the latter works. To compensate, Eric et al. (2017); Xu and Hu (2018a); Wen et al. (2018) adopt a copy mechanism that allows copying information retrieved from the KB to the generated response. Fung et al. (2018) adopt Memory Networks (Sukhbaatar et al., 2015) to memorize the retrieved KB entities and words appearing in the dialogue history. These models scale linearly with the size of the KB and need to be retrained at each update of the KB. Both issues make these approaches less practi"
W19-5922,N18-4010,1,0.924478,"r wants a cheap Italian restaurant at this stage. Requestable slots capture the information requested by the user, e.g., {address, phone} means the user wants to know the address and phone number of a restaurant. Dialogue policy model decides on the system action which is then realized by a language generation component. To mitigate the problems with such a classic modularized dialogue system, such as the error propagation between modules, the cascade effect that the updates of the modules have and the expensiveness of annotation, end-to-end training of dialogue systems was recently proposed (Liu and Lane, 2018; Williams et al., 2017; Lowe et al., 2017; Li et al., 2018; Liu et al., 2018; Budzianowski et al., 2018; Bordes et al., 2017; Wen et al., 2017b; Serban et al., 2016, among others). These systems train one whole model to read the current user’s utterance, the past state (that may contain all previous interactions) and generate the current state and response. There are two main approaches for modeling the belief state in end-to-end task-oriented dialogue systems in the literature: the fully structured approach based on classification (Wen et al., 2017b,a), and the free-form approach based on te"
W19-5922,N18-1187,1,0.930229,"information requested by the user, e.g., {address, phone} means the user wants to know the address and phone number of a restaurant. Dialogue policy model decides on the system action which is then realized by a language generation component. To mitigate the problems with such a classic modularized dialogue system, such as the error propagation between modules, the cascade effect that the updates of the modules have and the expensiveness of annotation, end-to-end training of dialogue systems was recently proposed (Liu and Lane, 2018; Williams et al., 2017; Lowe et al., 2017; Li et al., 2018; Liu et al., 2018; Budzianowski et al., 2018; Bordes et al., 2017; Wen et al., 2017b; Serban et al., 2016, among others). These systems train one whole model to read the current user’s utterance, the past state (that may contain all previous interactions) and generate the current state and response. There are two main approaches for modeling the belief state in end-to-end task-oriented dialogue systems in the literature: the fully structured approach based on classification (Wen et al., 2017b,a), and the free-form approach based on text generation (Lei et al., 2018). The fully structured approaches (Ramadan et"
W19-5922,D16-1230,0,0.0251687,"a and Ba, 2015) with a learning rate of 0.00025 for minimizing the loss in Equation 12 for both datasets. We apply dropout with a rate of 0.5 after 3 https://scikit-optimize.github.io/ Evaluation Metrics We evaluate the performance concerning belief state tracking, response language quality, and task completion. For belief state tracking, we report precision, recall, and F1 score of informable slot values and requestable slots. BLEU (Papineni et al., 2002) is applied to the generated agent responses for evaluating language quality. Although it is a poor choice for evaluating dialogue systems (Liu et al., 2016), we still report it in order to compare with previous work that has adopted it. For task completion evaluation, the Entity Match Rate (EMR) (Wen et al., 2017b) and Success F1 score (SuccF1 ) (Lei et al., 2018) are reported. EMR evaluates whether a system can correctly retrieve the user’s indicated entity (record) from the KB based on the generated constraints so it can have only a score of 0 or 1 for each dialogue. The SuccF1 score evaluates how a system responds to the user’s requests at dialogue level: it is F1 score of the response slots in the agent responses. 4.3 Benchmarks We compare FS"
W19-5922,P02-1040,0,0.103579,"Missing"
W19-5922,P18-1134,0,0.219438,"rning 2 Related Work Our work is related to end-to-end task-oriented dialogue systems in general (Liu and Lane, 2018; Williams et al., 2017; Lowe et al., 2017; Li et al., 2018; Liu et al., 2018; Budzianowski et al., 2018; Bordes et al., 2017; Hori et al., 2016; Wen et al., 2017b; Serban et al., 2016, among others) and those that extend the Seq2Seq (Sutskever et al., 2014) architecture in particular (Eric et al., 2017; Fung et al., 2018; Wen et al., 2018). Belief tracking, which is necessary to form KB queries, is not explicitly performed in the latter works. To compensate, Eric et al. (2017); Xu and Hu (2018a); Wen et al. (2018) adopt a copy mechanism that allows copying information retrieved from the KB to the generated response. Fung et al. (2018) adopt Memory Networks (Sukhbaatar et al., 2015) to memorize the retrieved KB entities and words appearing in the dialogue history. These models scale linearly with the size of the KB and need to be retrained at each update of the KB. Both issues make these approaches less practical in real-world applications. Our work is also akin to modularly connected end-to-end trainable networks (Wen et al., 2017b,a; Liu and Lane, 2018; Liu et al., 2018; Li et al."
W19-5922,D14-1162,0,0.0813247,"Missing"
W19-5922,P18-2069,0,0.0222331,"al., 2018; Budzianowski et al., 2018; Bordes et al., 2017; Wen et al., 2017b; Serban et al., 2016, among others). These systems train one whole model to read the current user’s utterance, the past state (that may contain all previous interactions) and generate the current state and response. There are two main approaches for modeling the belief state in end-to-end task-oriented dialogue systems in the literature: the fully structured approach based on classification (Wen et al., 2017b,a), and the free-form approach based on text generation (Lei et al., 2018). The fully structured approaches (Ramadan et al., 2018; Ren et al., 2018) use the full structure of the KB, both its schema and the values available in it, and assumes that the sets of informable slot values and requestable slots are fixed. In real-world scenarios, this assumption is too restrictive as the content of the KB may change and users’ utterances may contain information outside the pre-defined sets. An ideal end-toend architecture for state tracking should be able to identify the values of the informable slots and the requestable slots, easily adapt to new domains, to the changes in the content of the KB, and to the 178 Proceedings of t"
W19-5922,P18-1135,0,0.0222013,"et al. (2018) adopt a copy mechanism that allows copying information retrieved from the KB to the generated response. Fung et al. (2018) adopt Memory Networks (Sukhbaatar et al., 2015) to memorize the retrieved KB entities and words appearing in the dialogue history. These models scale linearly with the size of the KB and need to be retrained at each update of the KB. Both issues make these approaches less practical in real-world applications. Our work is also akin to modularly connected end-to-end trainable networks (Wen et al., 2017b,a; Liu and Lane, 2018; Liu et al., 2018; Li et al., 2018; Zhong et al., 2018). Wen et al. (2017b) includes belief state tracking and has two phases in training: the first phase uses belief state supervision, and then the second phase uses response generation supervision. Wen et al. (2017a) improves Wen et al. (2017b) by adding a policy network using latent representations so that the dialogue system can be continuously improved through reinforcement learning. These methods utilize classification as a way to decode the belief state. 179 Input Encoder Agent Response Decoder A t-1 B t-1 Ut At What cusine do you prefer? END_A cheap END_price north END_area END_food END_B I"
W19-5922,D18-1299,0,0.0271274,"Missing"
W19-5922,C18-1320,0,0.0159278,"Restaurant dataset (Wen et al., 2017b) and the Stanford in-car assistant dataset (Eric et al., 2017) without the need for fine-tuning through reinforcement learning 2 Related Work Our work is related to end-to-end task-oriented dialogue systems in general (Liu and Lane, 2018; Williams et al., 2017; Lowe et al., 2017; Li et al., 2018; Liu et al., 2018; Budzianowski et al., 2018; Bordes et al., 2017; Hori et al., 2016; Wen et al., 2017b; Serban et al., 2016, among others) and those that extend the Seq2Seq (Sutskever et al., 2014) architecture in particular (Eric et al., 2017; Fung et al., 2018; Wen et al., 2018). Belief tracking, which is necessary to form KB queries, is not explicitly performed in the latter works. To compensate, Eric et al. (2017); Xu and Hu (2018a); Wen et al. (2018) adopt a copy mechanism that allows copying information retrieved from the KB to the generated response. Fung et al. (2018) adopt Memory Networks (Sukhbaatar et al., 2015) to memorize the retrieved KB entities and words appearing in the dialogue history. These models scale linearly with the size of the KB and need to be retrained at each update of the KB. Both issues make these approaches less practical in real-world a"
W19-5922,E17-1042,0,0.0684828,"Missing"
W19-5922,P17-1062,0,0.194208,"ian restaurant at this stage. Requestable slots capture the information requested by the user, e.g., {address, phone} means the user wants to know the address and phone number of a restaurant. Dialogue policy model decides on the system action which is then realized by a language generation component. To mitigate the problems with such a classic modularized dialogue system, such as the error propagation between modules, the cascade effect that the updates of the modules have and the expensiveness of annotation, end-to-end training of dialogue systems was recently proposed (Liu and Lane, 2018; Williams et al., 2017; Lowe et al., 2017; Li et al., 2018; Liu et al., 2018; Budzianowski et al., 2018; Bordes et al., 2017; Wen et al., 2017b; Serban et al., 2016, among others). These systems train one whole model to read the current user’s utterance, the past state (that may contain all previous interactions) and generate the current state and response. There are two main approaches for modeling the belief state in end-to-end task-oriented dialogue systems in the literature: the fully structured approach based on classification (Wen et al., 2017b,a), and the free-form approach based on text generation (Lei et a"
W19-5922,D14-1179,0,\N,Missing
