W19-3410,Detecting Everyday Scenarios in Narrative Texts,2019,42,0,3,1,24460,lilian wanzare,Proceedings of the Second Workshop on Storytelling,0,"Script knowledge consists of detailed information on everyday activities. Such information is often taken for granted in text and needs to be inferred by readers. Therefore, script knowledge is a central component to language comprehension. Previous work on representing scripts is mostly based on extensive manual work or limited to scenarios that can be found with sufficient redundancy in large corpora. We introduce the task of scenario detection, in which we identify references to scripts. In this task, we address a wide range of different scripts (200 scenarios) and we attempt to identify all references to them in a collection of narrative texts. We present a first benchmark data set and a baseline model that tackles scenario detection using techniques from topic segmentation and text classification."
S19-1012,{MCS}cript2.0: A Machine Comprehension Corpus Focused on Script Events and Participants,2019,32,1,3,1,25223,simon ostermann,Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*{SEM} 2019),0,"We introduce MCScript2.0, a machine comprehension corpus for the end-to-end evaluation of script knowledge. MCScript2.0 contains approx. 20,000 questions on approx. 3,500 texts, crowdsourced based on a new collection process that results in challenging questions. Half of the questions cannot be answered from the reading texts, but require the use of commonsense and, in particular, script knowledge. We give a thorough analysis of our corpus and show that while the task is not challenging to humans, existing machine comprehension models fail to perform well on the data, even if they make use of a commonsense knowledge base. The dataset is available at http://www.sfb1102. uni-saarland.de/?page{\_}id=2582"
S18-1119,{S}em{E}val-2018 Task 11: Machine Comprehension Using Commonsense Knowledge,2018,0,45,5,1,25223,simon ostermann,Proceedings of The 12th International Workshop on Semantic Evaluation,0,"This report summarizes the results of the SemEval 2018 task on machine comprehension using commonsense knowledge. For this machine comprehension task, we created a new corpus, MCScript. It contains a high number of questions that require commonsense knowledge for finding the correct answer. 11 teams from 4 different countries participated in this shared task, most of them used neural approaches. The best performing system achieves an accuracy of 83.95{\%}, outperforming the baselines by a large margin, but still far from the human upper bound, which was found to be at 98{\%}."
L18-1011,Multi-layer Annotation of the Rigveda,2018,0,0,4,0,14905,oliver hellwig,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1512,Mapping Texts to Scripts: An Entailment Study,2018,0,1,4,1,25223,simon ostermann,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1564,{MCS}cript: A Novel Dataset for Assessing Machine Comprehension Using Script Knowledge,2018,21,11,5,1,25223,simon ostermann,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"We introduce a large dataset of narrative texts and questions about these texts, intended to be used in a machine comprehension task that requires reasoning using commonsense knowledge. Our dataset complements similar datasets in that we focus on stories about everyday activities, such as going to the movies or working in the garden, and that the questions require commonsense knowledge, or more specifically, script knowledge, to be answered. We show that our mode of data collection via crowdsourcing results in a substantial amount of such inference questions. The dataset forms the basis of a shared task on commonsense and script knowledge organized at SemEval 2018 and provides challenging test cases for the broader natural language understanding community."
L18-1641,Semi-Supervised Clustering for Short Answer Scoring,2018,0,0,2,0.516073,657,andrea horbach,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
D18-1282,Grounding Semantic Roles in Images,2018,0,2,2,0,18053,carina silberer,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"We address the task of visual semantic role labeling (vSRL), the identification of the participants of a situation or event in a visual scene, and their labeling with their semantic relations to the event or situation. We render candidate participants as image regions of objects, and train a model which learns to ground roles in the regions which depict the corresponding participant. Experimental results demonstrate that we can train a vSRL model without reliance on prohibitive image-based role annotations, by utilizing noisy data which we extract automatically from image captions using a linguistic SRL system. Furthermore, our model induces frame{---}semantic visual representations, and their comparison to previous work on supervised visual verb sense disambiguation yields overall better results."
W17-0901,Inducing Script Structure from Crowdsourced Event Descriptions via Semi-Supervised Clustering,2017,22,4,4,1,24460,lilian wanzare,"Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics",0,"We present a semi-supervised clustering approach to induce script structure from crowdsourced descriptions of event sequences by grouping event descriptions into paraphrase sets (representing event types) and inducing their temporal order. Our approach exploits semantic and positional similarity and allows for flexible event order, thus overcoming the rigidity of previous approaches. We incorporate crowdsourced alignments as prior knowledge and show that exploiting a small number of alignments results in a substantial improvement in cluster quality over state-of-the-art models and provides an appropriate basis for the induction of temporal order. We also show a coverage study to demonstrate the scalability of our approach."
S17-1015,A Mixture Model for Learning Multi-Sense Word Embeddings,2017,40,10,5,0,22809,dai nguyen,Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*{SEM} 2017),0,"Word embeddings are now a standard technique for inducing meaning representations for words. For getting good representations, it is important to take into account different senses of a word. In this paper, we propose a mixture model for learning multi-sense word embeddings. Our model generalizes the previous works in that it allows to induce different weights of different senses of a word. The experimental results show that our model outperforms previous models on standard evaluation tasks."
S17-1016,Aligning Script Events with Narrative Texts,2017,24,0,4,1,25223,simon ostermann,Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*{SEM} 2017),0,"Script knowledge plays a central role in text understanding and is relevant for a variety of downstream tasks. In this paper, we consider two recent datasets which provide a rich and general representation of script events in terms of paraphrase sets. We introduce the task of mapping event mentions in narrative texts to such script event types, and present a model for this task that exploits rich linguistic representations as well as information on temporal ordering. The results of our experiments demonstrate that this complex task is indeed feasible."
Q17-1003,Modeling Semantic Expectation: Using Script Knowledge for Referent Prediction,2017,8,6,5,1,422,ashutosh modi,Transactions of the Association for Computational Linguistics,0,"Recent research in psycholinguistics has provided increasing evidence that humans predict upcoming content. Prediction also affects perception and might be a key to robustness in human language processing. In this paper, we investigate the factors that affect human prediction by building a computational model that can predict upcoming discourse referents based on linguistic knowledge alone vs. linguistic knowledge jointly with common-sense knowledge in the form of scripts. We find that script knowledge significantly improves model estimates of human predictions. In a second study, we test the highly controversial hypothesis that predictability influences referring expression type but do not find evidence for such an effect."
I17-2007,Sequence to Sequence Learning for Event Prediction,2017,17,1,5,0,22809,dai nguyen,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"This paper presents an approach to the task of predicting an event description from a preceding sentence in a text. Our approach explores sequence-to-sequence learning using a bidirectional multi-layer recurrent neural network. Our approach substantially outperforms previous work in terms of the BLEU score on two datasets derived from WikiHow and DeScript respectively. Since the BLEU score is not easy to interpret as a measure of event prediction, we complement our study with a second evaluation that exploits the rich linguistic annotation of gold paraphrase sets of events."
P16-1166,Situation entity types: automatic classification of clause-level aspect,2016,21,6,3,1,785,annemarie friedrich,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,None
L16-1135,A Corpus of Literal and Idiomatic Uses of {G}erman Infinitive-Verb Compounds,2016,0,1,9,0.666667,657,andrea horbach,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We present an annotation study on a representative dataset of literal and idiomatic uses of German infinitive-verb compounds in newspaper and journal texts. Infinitive-verb compounds form a challenge for writers of German, because spelling regulations are different for literal and idiomatic uses. Through the participation of expert lexicographers we were able to obtain a high-quality corpus resource which offers itself as a testbed for automatic idiomaticity detection and coarse-grained word-sense disambiguation. We trained a classifier on the corpus which was able to distinguish literal and idiomatic uses with an accuracy of 85 {\%}."
L16-1555,{I}n{S}cript: Narrative texts annotated with script information,2016,0,11,4,1,422,ashutosh modi,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"This paper presents the InScript corpus (Narrative Texts Instantiating Script structure). InScript is a corpus of 1,000 stories centered around 10 different scenarios. Verbs and noun phrases are annotated with event and participant types, respectively. Additionally, the text is annotated with coreference information. The corpus shows rich lexical variation and will serve as a unique resource for the study of the role of script knowledge in natural language processing."
L16-1556,A Crowdsourced Database of Event Sequence Descriptions for the Acquisition of High-quality Script Knowledge,2016,0,10,4,1,24460,lilian wanzare,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Scripts are standardized event sequences describing typical everyday activities, which play an important role in the computational modeling of cognitive abilities (in particular for natural language processing). We present a large-scale crowdsourced collection of explicit linguistic descriptions of script-specific event sequences (40 scenarios with 100 sequences each). The corpus is enriched with crowdsourced alignment annotation on a subset of the event descriptions, to be used in future work as seed data for automatic alignment of event descriptions (for example via clustering). The event descriptions to be aligned were chosen among those expected to have the strongest corrective effect on the clustering algorithm. The alignment annotation was evaluated against a gold standard of expert annotators. The resulting database of partially-aligned script-event descriptions provides a sound empirical basis for inducing high-quality script knowledge, as well as for any task involving alignment and paraphrase detection of events."
W15-4408,Annotating Entailment Relations for Shortanswer Questions,2015,27,1,3,1,25223,simon ostermann,Proceedings of the 2nd Workshop on Natural Language Processing Techniques for Educational Applications,0,"This paper presents an annotation project that explores the relationship between textual entailment and short answer scoring (SAS). We annotate entailment relations between learner and target answers in the Corpus of Reading Comprehension Exercises for German (CREG) with a finegrained label inventory and compare them in various ways to correctness scores assigned by teachers. Our main finding is that although both tasks are clearly related, not all of our entailment tags can be directly mapped to SAS scores and that especially the area of partial entailment covers instances that are problematic for automatic scoring and need further investigation."
W15-2702,Linking discourse modes and situation entity types in a cross-linguistic corpus study,2015,18,2,5,0,36905,kleioisidora mavridou,"Proceedings of the First Workshop on Linking Computational Models of Lexical, Sentential and Discourse-level Semantics",0,"The main contribution of this paper is a cross-linguistic empirical analysis of two interacting levels of linguistic analysis of written text: situation entity (SE) types, the semantic types of situations evoked by clauses of text, and discourse modes (DMs), a characterization of passages at the sub-document level. We adapt an existing annotation scheme for SEs in English to be used for German data, with a detailed discussion of the most important differences. We create the first parallel corpus annotated for SEs, and the first DM-annotated corpus. We find that: (a) the adapted scheme is supported by evidence from a large-scale experimental study; (b) SEs mainly correspond to each other in parallel text, and a large part of the mismatches are systematic; (c) the DM annotation task can be performed intuitively with reasonable agreement; and (d) the annotated DMs show the predicted differences in the distributions of SE types."
W15-1603,"Annotating genericity: a survey, a scheme, and a corpus",2015,27,10,4,1,785,annemarie friedrich,Proceedings of The 9th Linguistic Annotation Workshop,0,"Generics are linguistic expressions that make statements about or refer to kinds, or that report regularities of events. Non-generic expressions make statements about particular individuals or specific episodes. Generics are treated extensively in semantic theory (Krifka et al., 1995). In practice, it is often hard to decide whether a referring expression is generic or non-generic, and to date there is no data set which is both large and satisfactorily annotated. Such a data set would be valuable for creating automatic systems for identifying generic expressions, in turn facilitating knowledge extraction from natural language text. In this paper we provide the next steps for such an annotation endeavor. Our contributions are: (1) we survey the most important previous projects annotating genericity, focusing on resources for English; (2) with a new agreement study we identify problems in the annotation scheme of the largest currentlyavailable resource (ACE-2005); and (3) we introduce a linguistically-motivated annotation scheme for marking both clauses and their subjects with regard to their genericity. (4) We present a corpus of MASC (Ide et al., 2010) and Wikipedia texts annotated according to our scheme, achieving substantial agreement."
S15-1024,Learning to predict script events from domain-specific text,2015,14,1,5,0,8348,rachel rudinger,Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics,0,"The automatic induction of scripts (Schank and Abelson, 1977) has been the focus of many recent works. In this paper, we employ a variety of these methods to learn Schank and Abelsonxe2x80x99s canonical restaurant script, using a novel dataset of restaurant narratives we have compiled from a website called xe2x80x9cDinners from Hell.xe2x80x9d Our models learn narrative chains, script-like structures that we evaluate with the xe2x80x9cnarrative clozexe2x80x9d task (Chambers and Jurafsky, 2008)."
P15-1123,Discourse-sensitive Automatic Identification of Generic Expressions,2015,23,4,2,1,785,annemarie friedrich,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"This paper describes a novel sequence labeling method for identifying generic expressions, which refer to kinds or arbitrary members of a class, in discourse context. The automatic recognition of such expressions is important for any natural language processing task that requires text understanding. Prior work has focused on identifying generic noun phrases; we present a new corpus in which not only subjects but also clauses are annotated for genericity according to an annotation scheme motivated by semantic theory. Our contextaware approach for automatically identifying generic expressions uses conditional random fields and outperforms previous work based on local decisions when evaluated on this corpus and on related data sets (ACE-2 and ACE-2005)."
D15-1294,Automatic recognition of habituals: a three-way classification of clausal aspect,2015,27,3,2,1,785,annemarie friedrich,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"This paper provides the first fully automatic approach for classifying clauses with respect to their aspectual properties as habitual, episodic or static. We bring together two strands of previous work, which address only the related tasks of the episodic-habitual and stative-dynamic distinctions, respectively. Our method combines different sources of information found to be useful for these tasks. We are the first to exhaustively classify all clauses of a text, achieving up to 80% accuracy (baseline 58%) for the three-way classification task, and up to 85% accuracy for related subtasks (baselines 50% and 60%), outperforming previous work. In addition, we provide a new large corpus of Wikipedia texts labeled according to our linguistically motivated guidelines."
W14-3505,Paraphrase Detection for Short Answer Scoring,2014,24,1,5,0,37466,nikolina koleva,Proceedings of the third workshop on {NLP} for computer-assisted language learning,0,"We describe a system that grades learner answers in reading comprehension tests in the context of foreign language learning. This task, also known as short answer scoring, essentially requires determining whether a semantic entailment relationship holds between an individual learner answer and a target answer; thus semantic information is a necessary part of any automatic short answer scoring system. At the same time the method must be robust to the particularities of learner language. We propose using paraphrase detection, a method that meets both requirements. The basis for our specific paraphrasing method is word alignment learned from parallel corpora which we create from the available data in the CREG corpus (Corpus for Reading Comprehension Exercises for German). We show the usefulness of this kind of information for the task of short answer scoring. Combining our results with existing approaches we obtain an improvement tendency."
regneri-etal-2014-aligning,Aligning Predicate-Argument Structures for Paraphrase Fragment Extraction,2014,42,3,3,1,21823,michaela regneri,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Paraphrases and paraphrasing algorithms have been found of great importance in various natural language processing tasks. While most paraphrase extraction approaches extract equivalent sentences, sentences are an inconvenient unit for further processing, because they are too specific, and often not exact paraphrases. Paraphrase fragment extraction is a technique that post-processes sentential paraphrases and prunes them to more convenient phrase-level units. We present a new approach that uses semantic roles to extract paraphrase fragments from sentence pairs that share semantic content to varying degrees, including full paraphrases. In contrast to previous systems, the use of semantic parses allows for extracting paraphrases with high wording variance and different syntactic categories. The approach is tested on four different input corpora and compared to two previous systems for extracting paraphrase fragments. Our system finds three times as many good paraphrase fragments per sentence pair as the baselines, and at the same time outputs 30{\%} fewer unrelated fragment pairs."
E14-1006,A Hierarchical {B}ayesian Model for Unsupervised Induction of Script Knowledge,2014,24,37,3,0,1785,lea frermann,Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Scripts representing common sense knowledge about stereotyped sequences of events have been shown to be a valuable resource for NLP applications. We present a hierarchical Bayesian model for unsupervised learning of script knowledge from crowdsourced descriptions of human activities. Events and constraints on event ordering are induced jointly in one unified framework. We use a statistical model over permutations which captures event ordering constraints in a more flexible way than previous approaches. In order to alleviate the sparsity problem caused by using relatively small datasets, we incorporate in our hierarchical model an informed prior on word distributions. The resulting model substantially outperforms a state-of-the-art method on the event ordering task."
S13-1041,Using the text to evaluate short answers for reading comprehension exercises,2013,19,13,3,1,657,andrea horbach,"Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity",0,"Short answer questions for reading comprehension are a common task in foreign language learning. Automatic short answer scoring is the task of automatically assessing the semantic content of a studentxe2x80x99s answer, marking it e.g. as correct or incorrect. While previous approaches mainly focused on comparing a learner answer to some reference answer provided by the teacher, we explore the use of the underlying reading texts as additional evidence for the classification. First, we conduct a corpus study targeting the links between sentences in reading texts for learners of German and answers to reading comprehension questions based on those texts. Second, we use the reading text directly for classification, considering three different models: an answer-based classifier extended with textual features, a simple text-based classifier, and a model that combines the two according to confidence of the text-based classification. The most promising approach is the first one, results for which show that textual features improve classification accuracy. While the other two models do not improve classification accuracy, they do investigate the role of the text and suggest possibilities for developing automatic answer scoring systems with less supervision needed from instructors."
Q13-1003,Grounding Action Descriptions in Videos,2013,29,137,6,1,21823,michaela regneri,Transactions of the Association for Computational Linguistics,0,"Recent work has shown that the integration of visual information into text-based models can substantially improve model predictions, but so far only visual information extracted from static images has been used. In this paper, we consider the problem of grounding sentences describing actions in visual information extracted from videos. We present a general purpose corpus that aligns high quality videos with multiple natural language descriptions of the actions portrayed in the videos, together with an annotation of how similar the action descriptions are to each other. Experimental results demonstrate that a text-based model of similarity between actions improves substantially when combined with visual information from videos depicting the described actions."
C12-2033,A Comparison of Knowledge-based Algorithms for Graded Word Sense Assignment,2012,20,0,4,1,785,annemarie friedrich,Proceedings of {COLING} 2012: Posters,0,"Standard word sense disambiguation (WSD) data sets annotate each word instance in context with exactly one sense of a predefined inventory, and WSD systems are traditionally evaluated with regard to how good they are at picking this sense. Recently, the notion of graded word sense assignment (GWSA) has gained attention as a more natural view of the contextual specification of word meaning; multiple senses may apply simultaneously to one instance of a word, and they may be applicable to different degrees. In this paper, we apply three different WSD algorithms to the task of GWSA. The three models belong to the class of knowledge-based models in the WSD terminology; they are unsupervised in the sense that they do not depend on annotated training material. We evaluate the models on two recently published GWSA data sets. We find positive correlations with the human judgments for all models, and develop a metric based on the notion of accuracy that highlights differences in the behaviors of the models."
R11-1064,Learning Script Participants from Unlabeled Data,2011,26,11,4,1,21823,michaela regneri,Proceedings of the International Conference Recent Advances in Natural Language Processing 2011,0,"We introduce a system that learns the participants of arbitrary given scripts. This system processes data from web experiments, in which each participant can be realized with different expressions. It computes participants by encoding semantic similarity and global structural information into an Integer Linear Program. An evaluation against a gold standard shows that we significantly outperform two informed baselines."
I11-1127,Word Meaning in Context: A Simple and Effective Vector Model,2011,28,56,3,0.727053,28852,stefan thater,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"We present a model that represents word meaning in context by vectors which are modified according to the words in the targetxe2x80x99s syntactic context. Contextualization of a vector is realized by reweighting its components, based on distributional information about the context words. Evaluation on a paraphrase ranking task derived from the SemEval 2007 Lexical Substitution Task shows that our model outperforms all previous models on this task. We show that our model supports a wider range of applications by evaluating it on a word sense disambiguation task. Results show that our model achieves state-of-the-art performance."
D11-1072,Robust Disambiguation of Named Entities in Text,2011,27,571,5,0,7615,johannes hoffart,Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,0,"Disambiguating named entities in natural-language text maps mentions of ambiguous names onto canonical entities like people or places, registered in a knowledge base such as DBpedia or YAGO. This paper presents a robust method for collective disambiguation, by harnessing context from knowledge bases and using a new form of coherence graph. It unifies prior approaches into a comprehensive framework that combines three measures: the prior probability of an entity being mentioned, the similarity between the contexts of a mention and a candidate entity, as well as the coherence among candidate entities for all mentions together. The method builds a weighted graph of mentions and candidate entities, and computes a dense subgraph that approximates the best joint mention-entity mapping. Experiments show that the new method significantly outperforms prior methods in terms of accuracy, with robust behavior across a variety of inputs."
P10-1097,Contextualizing Semantic Representations Using Syntactically Enriched Vector Models,2010,22,83,3,0.753307,28852,stefan thater,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"We present a syntactically enriched vector model that supports the computation of contextualized semantic representations in a quasi compositional fashion. It employs a systematic combination of first- and second-order context vectors. We apply our model to two different tasks and show that (i) it substantially outperforms previous work on a paraphrase ranking task, and (ii) achieves promising results on a wordsense similarity task; to our knowledge, it is the first time that an unsupervised method has been applied to this task."
P10-1100,Learning Script Knowledge with Web Experiments,2010,31,78,3,1,21823,michaela regneri,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"We describe a novel approach to unsupervised learning of the events that make up a script, along with constraints on their temporal ordering. We collect natural-language descriptions of script-specific event sequences from volunteers over the Internet. Then we compute a graph representation of the script's temporal structure using a multiple sequence alignment algorithm. The evaluation of our system shows that we outperform two informed baselines."
ruppenhofer-etal-2010-generating,Generating {F}rame{N}ets of Various Granularities: The {F}rame{N}et Transformer,2010,12,5,3,0.380952,3382,josef ruppenhofer,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"We present a method and a software tool, the FrameNet Transformer, for deriving customized versions of the FrameNet database based on frame and frame element relations. The FrameNet Transformer allows users to iteratively coarsen the FrameNet sense inventory in two ways. First, the tool can merge entire frames that are related by user-specified relations. Second, it can merge word senses that belong to frames related by specified relations. Both methods can be interleaved. The Transformer automatically outputs format-compliant FrameNet versions, including modified corpus annotation files that can be used for automatic processing. The customized FrameNet versions can be used to determine which granularity is suitable for particular applications. In our evaluation of the tool, we show that our method increases accuracy of statistical semantic parsers by reducing the number of word-senses (frames) per lemma, and increasing the number of annotated sentences per lexical unit and frame. We further show in an experiment on the FATE corpus that by coarsening FrameNet we do not incur a significant loss of information that is relevant to the Recognizing Textual Entailment task."
W09-2506,Ranking Paraphrases in Context,2009,14,24,3,0.753307,28852,stefan thater,Proceedings of the 2009 Workshop on Applied Textual Inference ({T}ext{I}nfer),0,"We present a vector space model that supports the computation of appropriate vector representations for words in context, and apply it to a paraphrase ranking task. An evaluation on the SemEval 2007 lexical substitution task data shows promising results: the model significantly outperforms a current state of the art model, and our treatment of context is effective."
perera-etal-2008-clios,{CLI}o{S}: Cross-lingual Induction of Speech Recognition Grammars,2008,13,0,3,0,48371,nadine perera,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"We present an approach for the cross-lingual induction of speech recognition grammars that separates the task of translation from the task of grammar generation. The source speech recognition grammar is used to generate phrases, which are translated by a common translation service. The target recognition grammar is induced by using the production rules of the source language, manually translated sentences and a statistical word alignment tool. We induce grammars for the target languages Spanish and Japanese. The coverage of the resulting grammars is evaluated on two corpora and compared quantitatively and qualitatively to a grammar induced with unsupervised monolingual grammar induction."
W06-0203,Automatic Extraction of Definitions from {G}erman Court Decisions,2006,11,33,2,0,48495,stephan walter,Proceedings of the Workshop on Information Extraction Beyond The Document,0,This paper deals with the use of computational linguistic analysis techniques for information access and ontology learning within the legal domain. We present a rule-based approach for extracting and analysing definitions from parsed text and evaluate it on a corpus of about 6000 German court decisions. The results are applied to improve the quality of a text based ontology learning method on this corpus.
burchardt-etal-2006-salsa,The {SALSA} Corpus: a {G}erman Corpus Resource for Lexical Semantics,2006,23,144,6,0,13863,aljoscha burchardt,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"This paper describes the SALSA corpus, a large German corpus manually annotated with manual role-semantic annotation, based on the syntactically annotated TIGER newspaper corpus. The first release, comprising about 20,000 annotated predicate instances (about half the TIGER corpus), is scheduled for mid-2006. In this paper we discuss the annotation framework (frame semantics) and its cross-lingual applicability, problems arising from exhaustive annotation, strategies for quality control, and possible applications."
P03-1068,Towards a Resource for Lexical Semantics: A Large {G}erman Corpus with Extensive Semantic Annotation,2003,10,68,4,0.3125,2274,katrin erk,Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,1,"We describe the ongoing construction of a large, semantically annotated corpus resource as reliable basis for the large-scale acquisition of word-semantic information, e.g. the construction of domain-independent lexica. The backbone of the annotation are semantic roles in the frame semantics paradigm. We report experiences and evaluate the annotated data from the first project stage. On this basis, we discuss the problems of vagueness and ambiguity in semantic annotation."
P00-1066,Feature Logic for Dotted Types: A Formalism for Complex Word Meanings,2000,7,5,1,1,24461,manfred pinkal,Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,1,"In this paper we revisit Pustejovsky's proposal to treat ontologically complex word meaning by so-called dotted pairs. We use a higher-order feature logic based on Ohori's record xcexbb-calculus to model the semantics of words like book and library, in particular their behavior in the context of quantification and cardinality statements."
P97-1053,A Uniform Approach to Underspecification and Parallelism,1997,21,30,2,0,51760,joachim niehren,35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,1,"We propose a unified framework in which to treat semantic underspecification and parallelism phenomena in discourse. The framework employs a constraint language that can express equality and subtree relations between finite trees. In addition, our constraint language can express the equality up-to relation over trees which captures parallelism between them. The constraints are solved by context unification. We demonstrate the use of our framework at the examples of quantifier scope, ellipsis, and their interaction."
C96-2197,An Education and Research Tool for Computational Semantics,1996,9,3,4,0,54709,karsten konrad,{COLING} 1996 Volume 2: The 16th International Conference on Computational Linguistics,0,"This paper describes an interactive graphical environment for computational semantics. The system provides a teaching tool, a stand alone extendible grapher, and a library of algorithms together with test suites. The teaching tool allows users to work step by step through derivations of semantic representations, and to compare the properties of various semantic formalisms such as Intensional Logic, DRT, and Situation Semantics. The system is freely available on the Internet."
C96-1024,Compositional Semantics in Verbmobil,1996,6,40,5,0,6245,johan bos,{COLING} 1996 Volume 1: The 16th International Conference on Computational Linguistics,0,"The paper discusses how compositional semantics is implemented in the Verbmobil speech-to-speech translation system using LUD, a description language for underspecified discourse representation structures. The description language and its formal interpretation in DRT are described as well as its implementation together with the architecture of the system's entire syntactic-semantic processing module. We show that a linguistically sound theory and formalism can be properly implemented in a system with (near) real-time requirements."
E91-1009,On the Syntactic-Semantic Analysis of Bound Anaphora,1991,10,3,1,1,24461,manfred pinkal,Fifth Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Two well-known phenomena in the area of pronoun binding are considered: Indirect binding of pronouns by indefinite NPs (donkey sentences) and surface-syntactic constraints on binding (weak cross-over). A common treatment is proposed, and general consequences for the relation between syntactic and semantic processing are discussed. It is argued that syntactic and semantic analysis must interact in a complex way, rather than in a simple sequential or strict rule-to-rule fashion."
C86-1088,Definite Noun Phrases and the Semantics of Discourse,1986,8,15,1,1,24461,manfred pinkal,Coling 1986 Volume 1: The 11th International Conference on Computational Linguistics,0,None
