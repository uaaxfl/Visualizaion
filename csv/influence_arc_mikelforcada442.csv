2001.mtsummit-teach.7,2000.bcs-1.7,1,0.528421,"theses about the observed differences. The five sentences are chosen so as they are correctly translated by at least one of the programs. Students’ work may be guided by the instructor by formulating some questions, such as, 3 http://www.lhsl.com 4 http://www.freetranslation.com 5 http://www.reverso.net 6 After obtaining the word-for-word translation, these extra lines can be removed to obtain the translation of sentences as a whole. 7 In our course, this assignment is followed by another one in which students infer the reordering rules applied by MT systems to growingcomplexity noun phrases (Forcada 2000). What follows is an analysis of the results produced by each of the programs to help the instructor guide the students during the assignment.8 Power Translator 5.0 (PT). As an example, we analyse the results of PT in detail. 1. My tailor is rich. Word-for-word: Mi / adapte / es / rico. Whole sentence: Mi sastre es rico. The word tailor is a homograph in English. PT considers it as a verb (imperative of to tailor) when seen in isolation, giving adapte in Spanish; on the other hand, the second translation gives sastre (a noun). PT may have considered the preceding word (my) and discarded the un"
2002.eamt-1.16,2000.bcs-1.7,1,0.739602,"procedures (after a careful reflection) in the form of rules is required. In addition, only those rules that can be programmed in reasonable time and with reasonable effort into a MT system and only those which execute taking reasonable time and memory will be useful. These requirements are another source of approximations, sacrifices and compromises. 3.2 Obvious errors and intuitive refinements Model zero produces a number of consistent errors, the most obvious being: 5 For details on how to learn details of the particular rules of this kind of transformer architectures in the classroom, see Forcada (2000) or Mira i Gimènez and Forcada (1998). 1. incorrect choice of equivalents for lexically ambiguous words (e.g., for homographs); 141 2. incorrect TL word order when SL word order and TL word order should be different; 3. agreement errors (the gender or number of a SL noun and its TL counterpart may be different and should change also for their modifiers); 4. wrong translations for idioms and other multi-word units. real MT departs from model zero by studying the differences between the translation produced by a commercial MT system, first for each word in isolation and then inside a sentence, a"
2002.eamt-1.16,2001.mtsummit-teach.7,1,0.614841,"Missing"
2002.tmi-papers.7,2001.mtsummit-papers.14,0,0.834881,"Missing"
2002.tmi-papers.7,J02-2004,1,0.836603,"rface form is the (possibly inflected) form of the word as it appears in the text; the lexical form gives the base form of the word, its part of speech, and grammatical information about its inflection. and removal of transductions may be crucial: for example, when debugging the lexical modules of the MT system or when a user wants to add new words for immediate use. In this paper, we show a simple and efficient method to modify a minimal FST so that a single transduction is added to or removed from the language accepted by it. The algorithms presented here are derived from those presented by Carrasco & Forcada (2002) for finite-state acceptors (FSAs), which in turn extend the range of those by Daciuk et al. (2000) to cyclic FSAs and to the removal of entries. The algorithms in this paper are applicable to a particular class of FSTs called deterministic augmented letter transducers (deterministic ALTs or DALTs). Any FST may always be turned into an equivalent letter transducer (Roche & Schabes 1997); as will be shown, augmented letter transducers are a convenient way of implementing morphological analysers: (1) they analyse text simultaneously to segmenting (tokenizing) it into contextual, morphologically-"
2002.tmi-papers.7,J00-1002,0,0.422048,"s the base form of the word, its part of speech, and grammatical information about its inflection. and removal of transductions may be crucial: for example, when debugging the lexical modules of the MT system or when a user wants to add new words for immediate use. In this paper, we show a simple and efficient method to modify a minimal FST so that a single transduction is added to or removed from the language accepted by it. The algorithms presented here are derived from those presented by Carrasco & Forcada (2002) for finite-state acceptors (FSAs), which in turn extend the range of those by Daciuk et al. (2000) to cyclic FSAs and to the removal of entries. The algorithms in this paper are applicable to a particular class of FSTs called deterministic augmented letter transducers (deterministic ALTs or DALTs). Any FST may always be turned into an equivalent letter transducer (Roche & Schabes 1997); as will be shown, augmented letter transducers are a convenient way of implementing morphological analysers: (1) they analyse text simultaneously to segmenting (tokenizing) it into contextual, morphologically-motivated units (which may be composed of more than one word) by using one-character lookahead; (2)"
2002.tmi-tmiw.3,P91-1022,0,0.0156729,"ts that may be considered to be translations of each other; Chen & Nie (2000) describe in detail a complete architecture, partially built upon Resnik’s proposal, to perform this task using statistical alignment techniques. As a side effect of bitext validation, each text is (or may easily be) divided into segments which are found to correspond to segments in the counterpart text (these bitext segments are sometimes called translation units or TUs for short). But there are a large number of bitext-alignment methods which might be used to validate or refine these alignments (Gale & Church 1991; Brown et al. 1991; Melamed 1996). An obvious application of these bilingual segments could be the translation of new documents in either direction — this is indeed the basis of a well-known technology, translation memory (TM). In view of the number of bitexts available on the web, the coverage would be very large for some language pairs, subjects, and registers. In particular, it would be specially interesting to explore those language pairs for which no translation software or other costly linguistic resources are available and for which there are, however, a large number of bitexts (for example Spanish–Basqu"
2002.tmi-tmiw.3,J02-2004,1,0.783792,"o”) and (“forgetting”,“olvidando”) with the algorithm mentioned (Wagner & Fischer 1974), one finds that they share the prefix alignment (“ forg”,“olvid”). w:o o:s t:d _:_ :d c:c a:a s:s e:o s:s n:z t:i e:e Figure 2: A finite-state letter transducer storing the TUs (“two cases”,“dos casos”) and (“ten cases”,“diez casos”). An underscore stands for any amount of valid inter-word whitespace. As with most string search algorithms, the speed of finite-state machines degrades only very gracefully (logaritmically) with the number of entries. Also recently, a number of researchers (Daciuk et al. 2000; Carrasco & Forcada 2002; Garrido-Alenda et al. 2002) have found ways to incrementally maintain finite-state automata and LTs so that entries may be added to them or deleted from them while keeping them minimal (i.e, very efficient spatially), allowing not only for the easy addition of new TUs, but also for the deletion of TUs which are found to be in error. 5 Updating the finite-state translation memory 5.1 Updating during bitext alignment Intuitively, a consistent way to process a new bitext to obtain new translation units to be used in an FST-based machine translation memory would be to perform this processing usi"
2002.tmi-tmiw.3,J00-1002,0,0.0160321,"“forgotten”,“olvidado”) and (“forgetting”,“olvidando”) with the algorithm mentioned (Wagner & Fischer 1974), one finds that they share the prefix alignment (“ forg”,“olvid”). w:o o:s t:d _:_ :d c:c a:a s:s e:o s:s n:z t:i e:e Figure 2: A finite-state letter transducer storing the TUs (“two cases”,“dos casos”) and (“ten cases”,“diez casos”). An underscore stands for any amount of valid inter-word whitespace. As with most string search algorithms, the speed of finite-state machines degrades only very gracefully (logaritmically) with the number of entries. Also recently, a number of researchers (Daciuk et al. 2000; Carrasco & Forcada 2002; Garrido-Alenda et al. 2002) have found ways to incrementally maintain finite-state automata and LTs so that entries may be added to them or deleted from them while keeping them minimal (i.e, very efficient spatially), allowing not only for the easy addition of new TUs, but also for the deletion of TUs which are found to be in error. 5 Updating the finite-state translation memory 5.1 Updating during bitext alignment Intuitively, a consistent way to process a new bitext to obtain new translation units to be used in an FST-based machine translation memory would be to pe"
2002.tmi-tmiw.3,P91-1023,0,0.0224555,"hat is, pairs of texts that may be considered to be translations of each other; Chen & Nie (2000) describe in detail a complete architecture, partially built upon Resnik’s proposal, to perform this task using statistical alignment techniques. As a side effect of bitext validation, each text is (or may easily be) divided into segments which are found to correspond to segments in the counterpart text (these bitext segments are sometimes called translation units or TUs for short). But there are a large number of bitext-alignment methods which might be used to validate or refine these alignments (Gale & Church 1991; Brown et al. 1991; Melamed 1996). An obvious application of these bilingual segments could be the translation of new documents in either direction — this is indeed the basis of a well-known technology, translation memory (TM). In view of the number of bitexts available on the web, the coverage would be very large for some language pairs, subjects, and registers. In particular, it would be specially interesting to explore those language pairs for which no translation software or other costly linguistic resources are available and for which there are, however, a large number of bitexts (for ex"
2002.tmi-tmiw.3,2002.tmi-papers.7,1,0.709462,"vidando”) with the algorithm mentioned (Wagner & Fischer 1974), one finds that they share the prefix alignment (“ forg”,“olvid”). w:o o:s t:d _:_ :d c:c a:a s:s e:o s:s n:z t:i e:e Figure 2: A finite-state letter transducer storing the TUs (“two cases”,“dos casos”) and (“ten cases”,“diez casos”). An underscore stands for any amount of valid inter-word whitespace. As with most string search algorithms, the speed of finite-state machines degrades only very gracefully (logaritmically) with the number of entries. Also recently, a number of researchers (Daciuk et al. 2000; Carrasco & Forcada 2002; Garrido-Alenda et al. 2002) have found ways to incrementally maintain finite-state automata and LTs so that entries may be added to them or deleted from them while keeping them minimal (i.e, very efficient spatially), allowing not only for the easy addition of new TUs, but also for the deletion of TUs which are found to be in error. 5 Updating the finite-state translation memory 5.1 Updating during bitext alignment Intuitively, a consistent way to process a new bitext to obtain new translation units to be used in an FST-based machine translation memory would be to perform this processing using also a LRLM scheme, to bui"
2002.tmi-tmiw.3,P01-1050,0,0.0686869,"alidation of segments is sometimes required; most TM packages offer interactive alignment tools to perform this task. It is the case that sentencelong segments are usually too large to give good coverage for new texts, but may be decomposed in smaller segments which are still acceptable translation units (some commercial programs indeed try to find smaller segments, partial matches, etc.). The lack of coverage may be partly alleviated by using more advanced, sub-sentence or even subword alignment strategies (Simard & Langlais 2001), or statistical machine translation techniques. For instance, Marcu (2001) uses statistically-obtained source– target word alignments to obtain TU candidates: pairs of segments such that all words included in the source segment are aligned only with words included in the target segment and vice-versa (“contiguous alignments”). Subsentential strategies are expected to work very well for related languages (Tom´as & Casacuberta 2001; Ribeiro et al. 2001). It is true that, even if subsentential TUs will clearly improve translation coverage, they are more likely to be ambiguous than complete-sentence TUs. Therefore, any application designed to use subsentential TUs has t"
2002.tmi-tmiw.3,W96-0201,0,0.0137661,"idered to be translations of each other; Chen & Nie (2000) describe in detail a complete architecture, partially built upon Resnik’s proposal, to perform this task using statistical alignment techniques. As a side effect of bitext validation, each text is (or may easily be) divided into segments which are found to correspond to segments in the counterpart text (these bitext segments are sometimes called translation units or TUs for short). But there are a large number of bitext-alignment methods which might be used to validate or refine these alignments (Gale & Church 1991; Brown et al. 1991; Melamed 1996). An obvious application of these bilingual segments could be the translation of new documents in either direction — this is indeed the basis of a well-known technology, translation memory (TM). In view of the number of bitexts available on the web, the coverage would be very large for some language pairs, subjects, and registers. In particular, it would be specially interesting to explore those language pairs for which no translation software or other costly linguistic resources are available and for which there are, however, a large number of bitexts (for example Spanish–Basque): resources a"
2002.tmi-tmiw.3,P99-1068,0,0.180933,"rform translation-unit-driven segmentation and translation of the source text. Section 5 sketches possible strategies for updating the kind of subsentential finite-state translation memories proposed with information from new bitexts. In section 6, I give some features of the kind of web-based machine translation application aimed at. Finally, closing comments may be found in section 7. 2 The web as a source of translation units The efficient use of publicly-available multilingual content in the world wide web for linguistic purposes has been the subject of a number of studies. In particular, Resnik (1999) describes a system which automatically discovers bitexts (bilingual texts) on the web, that is, pairs of texts that may be considered to be translations of each other; Chen & Nie (2000) describe in detail a complete architecture, partially built upon Resnik’s proposal, to perform this task using statistical alignment techniques. As a side effect of bitext validation, each text is (or may easily be) divided into segments which are found to correspond to segments in the counterpart text (these bitext segments are sometimes called translation units or TUs for short). But there are a large number"
2002.tmi-tmiw.3,2001.mtsummit-papers.60,0,0.0171259,"ion of segments is sometimes required; most TM packages offer interactive alignment tools to perform this task. It is the case that sentencelong segments are usually too large to give good coverage for new texts, but may be decomposed in smaller segments which are still acceptable translation units (some commercial programs indeed try to find smaller segments, partial matches, etc.). The lack of coverage may be partly alleviated by using more advanced, sub-sentence or even subword alignment strategies (Simard & Langlais 2001), or statistical machine translation techniques. For instance, Marcu (2001) uses statistically-obtained source– target word alignments to obtain TU candidates: pairs of segments such that all words included in the source segment are aligned only with words included in the target segment and vice-versa (“contiguous alignments”). Subsentential strategies are expected to work very well for related languages (Tom´as & Casacuberta 2001; Ribeiro et al. 2001). It is true that, even if subsentential TUs will clearly improve translation coverage, they are more likely to be ambiguous than complete-sentence TUs. Therefore, any application designed to use subsentential TUs has t"
2002.tmi-tmiw.3,2001.mtsummit-papers.64,0,0.0253773,"Missing"
2003.mtsummit-tttt.2,2000.bcs-1.7,1,0.883623,"or example, in unit 7 (lab session L6 ) students are asked to analyse what does a given commercial MT system do besides simply substituting words, by first forcing the system to translate in isolation (e.g., each one in a paragraph) the words of a set of sentences and then translating the whole sentences (P´erez-Ortiz and Forcada 2001). In another assignment (lab session L7 ), students feed a set of increasingly complex noun phrases designed by the instructors into an MT system to incrementally infer the word-reordering rules used by explaining the resulting correct or incorrect translations (Forcada 2000). The following section gives brief descriptions of the remaining laboratory assignments. 4 Syllabus The current design of the syllabus started in 1995, before proposals like LETRAC (Badia et al. 1999) or surveys about the teaching of these matters (Balkan et al. 1997) were available. I basically interpreted the official description of the subject and made a quick survey of what other universities in Spain were doing (according to their webpages). After eight years of redesign, the course was eventually structured in 10 units or blocks (B1 . . . B10 ), which will be briefly described and comme"
2003.mtsummit-tttt.2,2001.mtsummit-teach.7,1,0.698331,"Missing"
2004.tmi-1.15,2001.mtsummit-papers.14,0,0.0444576,"Missing"
2004.tmi-1.15,A92-1018,0,0.581067,"languages are refined by using the statistical data supplied by the current HMM for the other language. Both models bootstrap by learning cooperatively in an unsupervised manner and require only monolingual texts; no aligned texts are needed. Preliminary results are promising and surpass those of traditional unsupervised approaches. 1 Introduction One of the main sources of errors in machine translation (MT) systems, specially for related languages, is the incorrect resolution of part-of-speech (PoS) ambiguities. Hidden Markov models (HMMs, Rabiner 1989) are the standard statistical approach (Cutting et al. 1992) to automatic PoS tagging. Typically, unsupervised training of this kind of taggers has been carried out from source-language (SL) untagged corpora (see below in this introduction) using the Baum-Welch algorithm (Rabiner 1989). But target-language (TL) information may also be taken into account in order to improve the performance of these PoS taggers, specially as to the resulting translation quality, an aspect not addressed by training algorithms which use information from the SL only. Statistics from the TL might be a source of useful information as well, an idea which will be explored in th"
2004.tmi-1.15,N01-1026,0,0.0370061,"w to train the two corresponding PoS taggers simultaneously in a single iterative process using cooperative learning. A brief overview of our proposal follows: consider a system translating between languages A and B. At every iteration, the existing HMM for A is refined by using B as TL and estimating the likelihood of the translations of each of the segments in A with the current HMM for B. Then, the roles are interchanged and the existing HMM for B is updated correspondingly. The resulting algorithm is based on a model of TL tags —instead of words— and still works in an unsupervised manner. Yarowsky & Ngai (2001) proposed a method which also uses information from TL in order to train PoS taggers. They consider information from aligned parallel corpora and from (at least) one manually tagged corpus for the TL. Our method, however, needs neither aligned parallel corpora nor manually tagged texts. Most current MT systems follow the indirect or transfer approach (Hutchins & Somers 1992, ch. 4): SL text is analysed and converted into an intermediate representation which becomes the basis for generating the corresponding TL text. Analysis modules usually include a PoS tagger for the SL. Our method for train"
2005.eamt-1.12,2001.mtsummit-papers.14,0,0.89117,"hat has to be done for each pattern EAMT 2005 Conference Proceedings An open-source shallow-transfer machine translation engine for the Romance languages of Spain (much like in languages such as perl or lex). Using a declarative notation such as XML is rather straightforward for the pattern part of rules but using it for the action (procedural) part means stretching it a bit; we have, however, found a reasonable way to translate the ad-hoc C-style action language used in the corresponding module of interNOSTRUM and Traductor Universia, which was defined in detail in Garrido-Alenda and Forcada (2001), into a simple XML notation having the same expressiveness. In this way, we follow as close as possible the declarative approach used in the XML files defining the linguistic data used for the tagger and for the lexical processing modules. 3.4. De-formatter and re-formatter The current de-formatter and re-formatter used in Traductor Universia and interNOSTRUM are different for each of the three formats supported (plain ISO-8859-1 text, HTML and RTF). Their behaviour is specified following a pattern–action scheme, with patterns specified as regular expressions and actions written in C code, us"
2005.eamt-1.12,carreras-etal-2004-freeling,0,0.106391,"Missing"
2005.eamt-1.12,2002.tmi-papers.7,1,0.882427,"Missing"
2005.mtsummit-osmtw.2,W04-0813,0,0.0554133,"Missing"
2005.mtsummit-osmtw.2,2001.mtsummit-papers.14,0,0.286088,"our group (Díaz de Ilarraza et al., 2000) but is now integrated in the OpenTrad initiative, a larger government-funded project shared 7 1. An open source shallow-transfer machine translation engine for the Romance languages of Spain (the main ones being Spanish, Catalan and Galician). The MT architecture proposed uses finite-state transducers for lexical processing, hidden Markov models for part-of-speech tagging, and finite-state based chunking for structural transfer, and is largely based upon that of systems already developed by the Transducens group such as InterNOSTRUM (Spanish-Catalan, Canals-Marote et al., 2001) and Traductor Universia (SpanishPortuguese, Garrido-Alenda, 2003). 2. A deeper-transfer engine for the Spanish— Basque pair, which will be described in this paper. Some of the components (modules, data formats and compilers) from the first architecture will also be useful for the second. Indeed, an important additional goal of this work is testing which modules from the first architecture can be integrated in deeper-transfer architectures for more difficult language pairs. We expect that the introduction of an open source MT architecture will help finding solutions for well known problems in"
2005.mtsummit-osmtw.2,carreras-etal-2004-freeling,1,0.827581,"m Spanish to Basque and generation of the Basque output. It is based on the previous work of our group (Díaz de Ilarraza et al., 2000) but with new features and a new aim: interoperability with other linguistic resources and convergence with the other engines in the OpenTrad project through the use of XML . The previous objectoriented architecture is being rewritten into a open source one which will use modules which are shared with other engines in the OpenTrad project and will comply with its format specifications. The main modules are five: de-formatter, Spanish analysis based on FreeLing (Carreras et al., 2004), Spanish-Basque transfer, Basque generation and re-formatter. De-Formatter --&gt; Spanish | Analyzer | | | Transfer | | De-Formatter <-- Generation The following sections describe each module. The transfer and generation phases work in three levels: lexical form (tagged as node), chunk and sentence. No semantic disambiguation is applied, but a large number of multi-word units representing collocations, named-entities and complex terms are being included in the bilingual dictionary in order to minimize this limitation. 2.1. The de-formatter The de-formatter separates the text to be translated fro"
2005.mtsummit-osmtw.2,2005.eamt-1.12,1,0.803665,"Missing"
2005.mtsummit-osmtw.4,2001.mtsummit-papers.14,0,0.411613,"Missing"
2005.mtsummit-osmtw.4,2005.eamt-1.12,1,0.86182,"Missing"
2005.mtsummit-osmtw.4,2005.eamt-1.13,0,0.0322622,"Missing"
2005.mtsummit-osmtw.4,2002.tmi-papers.7,1,0.825583,"Missing"
2005.mtsummit-osmtw.4,sheremetyeva-nirenburg-2000-towards,0,0.0203702,"Missing"
2007.tmi-papers.22,2004.eamt-1.14,0,0.0254192,"Missing"
2007.tmi-papers.22,1999.tmi-1.3,0,0.0354785,"o languages. The parallel corpus is obtained by translating a controlled corpus from a “major” language (English or Spanish) to a “minor” language by means of an elicitation tool. This tool is also used to graphically annotate the word alignments between the two sentences. Finally, hierarchical syntactic rules, that can be seen as constituting a context-free transfer grammar, are inferred from the aligned parallel corpus. On the other hand, in the EBMT framework, some researchers deal with the problem of inferring the kinds of translation rules called translation templates (Kaji et al., 1992; Brown, 1999; Cicekli and G¨ uvenir, 2001). A translation template can be deﬁned as a bilingual pair of sentences in which corresponding units (words or phrases) are coupled and replaced by variables. Liu and Zong (2004) provide an interesting review of the diﬀerent research works dealing with translation templates. Brown (1999) uses a parallel corpus and some linguistic knowledge in the form of equivalence classes (both syntactic and semantic) to perform a generalization over the bilingual examples collected. The method works by replacing each word by its corresponding equivalence class and then using a"
2007.tmi-papers.22,A92-1018,0,0.0268649,"Missing"
2007.tmi-papers.22,P96-1043,0,0.0391791,"part-of-speech and inﬂection information provided by the TL word class; • the bilingual phrase the AT comes from cannot be reproduced by the MT system in which the transfer rules will be used. This happens when the translation equivalent (in the bilingual dictionary) diﬀers from that in the bilingual phrase extracted from the corpus. • if the word class corresponds to a lexicalized word, it is introduced as is; remember that word classes belonging to lexicalized words store complete lexical forms consisting of lemma, part-of-speech and inﬂection information. 10 A similar approach was used by Mikheev (1996) in his work on learning part-of-speech guessing rules to prioritize longer suﬃxes over shorter ones. Note that the information about SL lexicalized words is not taken into account when generating the code for a given AT. 187 Lang. es ca # sent. 100 834 100 834 # words 1 952 317 2 032 925 Trans. dir. es-ca ca-es Table 1: Number of sentences and words in the Spanish–Catalan parallel corpus used for training. Example of AT application. The following example illustrates how the AT shown in ﬁgure 3 would be applied to translate from Spanish to Catalan the input text vivieron en Francia.11 This tex"
2007.tmi-papers.22,J03-1002,0,0.0049987,"used in SMT, perform a generalization over bilingual phrase pairs using word classes instead of words. An AT z = (Sm , Tn , A) consists of a sequence Sm of m SL word classes, a sequence Tn of n TL word classes, and a set of pairs A = {(i, j) : i ∈ [1, n] ∧ j ∈ [1, m]} with the alignment information between TL and SL word classes. Learning a set of ATs from a parallel corpus consists of: 1. the computation of the word alignments, 2. the extraction of bilingual phrase pairs, and 3. the substitution of each word by its corresponding word class. Word alignments. A variety of methods, statistical (Och and Ney, 2003) or heuristic (Caseli et al., 2005), may be used to compute word alignments from a (sentence aligned) parallel corpus. For our experiments (section 6) we have used the open-source GIZA++ toolkit5 in the following way. First, standard GIZA++ training runs to estimate translation models to translate from language L1 to language L2 , and vice versa. Then, from the training corpus, Viterbi alignments6 A1 and A2 are obtained (one for each translation 5 http://www.fjoch.com/GIZA++.html The Viterbi alignment between source and target sentences is deﬁned as the alignment whose probability is maximal u"
2007.tmi-papers.22,J04-4002,0,0.0410588,"ces (Arnold, 2003) such as structural transfer rules. In this paper we focus on the automatic inference of structural transfer rules from parallel corpora, which are small compared to the size of corpora commonly used to build SMT or (some) EBMT systems. The approach we present is tested on the shallow transfer MT platform Apertium for which structural transfer rules are generated. Overview. In rule-based MT, transfer rules are needed to perform syntactic and lexical changes. The approach we present in this paper to infer shallow-transfer MT rules is based on the alignment templates approach (Och and Ney, 2004) already used in SMT (see section 3). An alignment template (AT) can be deﬁned as a generalization performed over aligned phrase1 pairs (or translation units) by using word classes. The method we present is entirely unsupervised and needs, in addition to the linguistic data used by the MT system in which the inferred rules are used, only a (comparatively) small parallel corpus and a ﬁle deﬁning a reduced set of lexical categories usually involved in lexical changes. S´anchez-Mart´ınez and Ney (2006) use ATs to infer shallow-transfer rules to be used in 1 For the purpose of this paper, with phr"
2007.tmi-papers.22,W99-0604,0,0.0567839,". 33):7 • ﬁrst the intersection A = A1 ∩ A2 of both alignments is computed, then • the alignment A is iteratively extended with alignments (i, j) ∈ A1 or (i, j) ∈ A2 if neither SL word wSj nor TL word wTi has an alignment in A, or the following two conditions hold: 1. One of the following (neighboring) alignments (i − 1, j), (i + 1, j), (i, j − 1), (i, j + 1) is already in A. 2. The new alignment A ∪ {(i, j)} does not contain any alignment with both horizontal ((i − 1, j), (i + 1, j)) and vertical ((i, j − 1), (i, j + 1)) neighbors. Bilingual phrase pairs. The extraction of bilingual phrases (Och et al., 1999) is performed by considering all possible pairs within a certain length and ensuring that (see ﬁgure 2): 1. all words are consecutive, and 2. words within the bilingual phrase are not aligned with words from outside. 7 For easier understanding, think about the alignment information as a binary matrix (see ﬁgure 2). 184 The set of bilingual phrases that are extracted from the word-aligned sentence pair (wS1 , . . . , wSJ ), (wT1 , . . . , wTI ) can be formally expressed as follows: BP (wS J1 , wT I1 , A) = {(wS j+m , wT i+n ): j i ∀(i0 , j 0 ) ∈ A : j ≤ j 0 ≤ j + m ⇔ i ≤ i0 ≤ i + n}. Generaliza"
2007.tmi-papers.22,C92-2101,0,\N,Missing
2009.freeopmt-1.3,2005.mtsummit-osmtw.2,1,0.834247,"Missing"
2009.freeopmt-1.3,J82-2005,0,0.74795,"Missing"
2009.freeopmt-1.3,2001.mtsummit-papers.14,0,0.379014,"Missing"
2009.freeopmt-1.3,2005.mtsummit-osmtw.4,1,0.894947,"Missing"
2009.freeopmt-1.3,carreras-etal-2004-freeling,0,0.0349938,"lan (ca) and es and Galician (gl). The MT engine and tools in Apertium were not built from scratch, but are rather the result of a complete FOS rewriting and extension of two existing MT systems developed by the Transducens group at the Universitat d’Alacant, namely interNOSTRUM4 (Canals-Marote et al., 2001) (es– ca) and Traductor Universia5 (Garrido-Alenda et al., 2004) (es–Portuguese (pt)),to provide a platform to build MT systems for related languages. Linguistic data for the initial language pairs were built combining in-house resources with FOS data such as the ones present in Freeling6 (Carreras et al., 2004). cian, Catalan (also called Valencian), and Occitan (Aranese). Other languages such as Asturian or Aragonese have a more limited legal status. 2 This consortium, which involved 4 universities (Universitat d’Alacant, Universitat Polit`ecnica de Catalunya, Universidade de Vigo and Euskal Herriko Unibertsitatea) 2 companies (Eleka Ingeniaritza Linguistikoa, imaxin|software) and 1 Foundation (Elhuyar) adopted the name Opentrad during the project. Currently this name is used as a trademark by some of the companies in the original consortium to commercialize machine translation services based on Ap"
2009.freeopmt-1.3,2005.eamt-1.12,1,0.880475,"Missing"
2009.freeopmt-1.3,2007.tmi-papers.22,1,0.893387,"Missing"
2009.freeopmt-1.3,2009.eamt-1.29,1,0.815961,"Missing"
2009.freeopmt-1.3,2009.eamt-1.17,1,0.865984,"Missing"
2009.freeopmt-1.3,J08-3010,0,\N,Missing
2011.eamt-1.13,J96-1002,0,0.0362507,"words in s0 and it should be changed (see Figure 1). with one or more words in si that are matched with Note that wij may not be aligned with any word words in s0 , and, at the same time, it is aligned with 82 one or more unmatched words in si . In the experiments we have tried two ways of dealing with this, one that requires all SL words in si to be matched, and another one that only requires the majority of words aligned with wij to be matched. These strategies have been chosen because of their simplicity, although it could also be possible to use, for example, a maximum entropy classifier (Berger et al., 1996), in order to determine which words should be changed or kept unedited. In that case, fK would be one of the features used by the maximum entropy classifier. To illustrate these ideas, Figure 2 shows an example of a word-aligned pair of segments (si and ti ) and a segment s0 to be translated. As can be seen, the word he in ti is aligned with the word e´ l in si , which does not match with any word in s0 . Therefore, he should be marked to be changed. Conversely, the words his and brother are aligned with su and hermano, respectively, which are matched in s0 and, therefore should be kept unedit"
2011.eamt-1.13,J93-2003,0,0.0322612,"ems, we have chosen a fuzzy-match score function based on the Levenshtein distance (Levenshtein, 1966): score(s0 , si ) = 1 − a su hermano D(s0 , si ) max(|s0 |, |si |) where |x |stands for the length (in words) of string x and D(x, y) refers to the word-based Levenshtein distance (edit distance) between x and y. a su hermano Figure 2: Example of alignment and matching. 3.2 Corpora For the experiments in this paper we have used word alignments obtained by means of the free/open-source GIZA++1 tool (Och and Ney, 2003) which implements standard word-based statistical machine translation models (Brown et al., 1993) as well as a hidden-Markov-model-based alignment model (Vogel et al., 1996). GIZA++ produces alignments in which a source word can be aligned with many target words, whereas a target word is aligned with, at most, one source word. Following common practice in statistical machine translation (Koehn, 2010, Ch. 4) we have obtained The TMs we have used were extracted from the JRC-Acquis corpus version 3 (Steinberger et al., 2006),2 which contains the total body of European Union (EU) law. Before extracting the TMs used, this corpus was tokenized and lowercased, and then segment pairs in which eit"
2011.eamt-1.13,N03-1017,0,0.0214063,"ord would be marked neither as “keep” nor as “change”. Otherwise, if the criterion of majority is applied, the word would be marked to be changed. ti : si : [edit] he [?] missed J  J J  e´ l ech´o de menos s0 : ella ech´o de casa [keep] [keep] his brother a set of symmetric word alignments by running GIZA++ in both translation directions, and then symmetrizing both sets of alignments. In the experiments we have tried the following symmetrization methods: • the union of both sets of alignments, • the intersection of the two alignment sets, and • the use of the grow-diag-final-and heuristic (Koehn et al., 2003) as implemented in Moses (Koehn et al., 2007). 3 Experimental settings We have tested our approach in the translation of Spanish texts into English by using two TMs: TMtrans and TMtest . Evaluation was carried out by simulating the translation of the SL segments in TMtrans by using the TUs in TMtest . We firstly obtained the word alignments between the parallel segments of TMtest by training and running GIZA++ on the TM itself. Then, for each source segment in TMtrans , we obtained the TUs in TMtest having a fuzzy-match score above threshold Θ, and tagged the words in their target segments as"
2011.eamt-1.13,P07-2045,0,0.00745878,"“change”. Otherwise, if the criterion of majority is applied, the word would be marked to be changed. ti : si : [edit] he [?] missed J  J J  e´ l ech´o de menos s0 : ella ech´o de casa [keep] [keep] his brother a set of symmetric word alignments by running GIZA++ in both translation directions, and then symmetrizing both sets of alignments. In the experiments we have tried the following symmetrization methods: • the union of both sets of alignments, • the intersection of the two alignment sets, and • the use of the grow-diag-final-and heuristic (Koehn et al., 2003) as implemented in Moses (Koehn et al., 2007). 3 Experimental settings We have tested our approach in the translation of Spanish texts into English by using two TMs: TMtrans and TMtest . Evaluation was carried out by simulating the translation of the SL segments in TMtrans by using the TUs in TMtest . We firstly obtained the word alignments between the parallel segments of TMtest by training and running GIZA++ on the TM itself. Then, for each source segment in TMtrans , we obtained the TUs in TMtest having a fuzzy-match score above threshold Θ, and tagged the words in their target segments as “keep” or “change”. 3.1 Fuzzy-match score fun"
2011.eamt-1.13,J10-4005,0,0.0221968,". a su hermano Figure 2: Example of alignment and matching. 3.2 Corpora For the experiments in this paper we have used word alignments obtained by means of the free/open-source GIZA++1 tool (Och and Ney, 2003) which implements standard word-based statistical machine translation models (Brown et al., 1993) as well as a hidden-Markov-model-based alignment model (Vogel et al., 1996). GIZA++ produces alignments in which a source word can be aligned with many target words, whereas a target word is aligned with, at most, one source word. Following common practice in statistical machine translation (Koehn, 2010, Ch. 4) we have obtained The TMs we have used were extracted from the JRC-Acquis corpus version 3 (Steinberger et al., 2006),2 which contains the total body of European Union (EU) law. Before extracting the TMs used, this corpus was tokenized and lowercased, and then segment pairs in which either of the segments was empty or had more than 9 times words than its counterpart were removed. Finally, segments longer than 40 words (and their corresponding counterparts) were removed because of the inability of GIZA++ to align longer segments. 1 2 http://code.google.com/p/giza-pp/ 83 http://wt.jrc.it"
2011.eamt-1.13,kranias-samiotou-2004-automatic,0,0.652111,"anslation spotting consists of identifying, for a pair of parallel sentences, the words or phrases in a TL segment that correspond to the words in a SL segment. The work by Bourdaillet et al. (2009) follows a similar approach, although it does not focus on traditional TM-based CAT systems, but Mikel L. Forcada, Heidi Depraetere, Vincent Vandeghinste (eds.) Proceedings of the 15th Conference of the European Association for Machine Translation, p. 8188 Leuven, Belgium, May 2011 on the use of a bilingual concordancer to assist professional translators. More similar to our approach is the one by Kranias and Samiotou (2004) which is implemented on the ESTeam CAT system. Kranias and Samiotou (2004) align the source and target segments in each TU at different sub-segment levels by using a bilingual dictionary (Meyers et al., 1998), and then use these alignments to (i) identify the sub-segments in a translation proposal ti that need to be changed, and (ii) propose a machine translation for them. In this paper we propose a different way of using word alignments in a TM-based CAT system to alleviate the task of professional translators. The main difference between our approach and those previously described is that i"
2011.eamt-1.13,meyers-etal-1998-multilingual,0,0.560273,"approach, although it does not focus on traditional TM-based CAT systems, but Mikel L. Forcada, Heidi Depraetere, Vincent Vandeghinste (eds.) Proceedings of the 15th Conference of the European Association for Machine Translation, p. 8188 Leuven, Belgium, May 2011 on the use of a bilingual concordancer to assist professional translators. More similar to our approach is the one by Kranias and Samiotou (2004) which is implemented on the ESTeam CAT system. Kranias and Samiotou (2004) align the source and target segments in each TU at different sub-segment levels by using a bilingual dictionary (Meyers et al., 1998), and then use these alignments to (i) identify the sub-segments in a translation proposal ti that need to be changed, and (ii) propose a machine translation for them. In this paper we propose a different way of using word alignments in a TM-based CAT system to alleviate the task of professional translators. The main difference between our approach and those previously described is that in our approach word alignments are used only to recommend the words to be changed or kept unedited, without proposing a translation for them, so that the user can focus on choosing a translation where words ha"
2011.eamt-1.13,J03-1002,0,0.0566822,"e changed or kept unedited. Related work. In the literature one can find different approaches that use word or phrase alignments to improve existing TM-based CAT systems; although, to our knowledge, none of them use word alignments for the purpose we study in this paper. Simard (2003) focuses on the creation of TMbased CAT systems able to work at the sub-segment level by proposing as translation sub-segments extracted from longer segments in the matching TUs. To do this, he implements the translation spotting (V´eronis and Langlais, 2000) technique by using statistical word-alignment methods (Och and Ney, 2003); translation spotting consists of identifying, for a pair of parallel sentences, the words or phrases in a TL segment that correspond to the words in a SL segment. The work by Bourdaillet et al. (2009) follows a similar approach, although it does not focus on traditional TM-based CAT systems, but Mikel L. Forcada, Heidi Depraetere, Vincent Vandeghinste (eds.) Proceedings of the 15th Conference of the European Association for Machine Translation, p. 8188 Leuven, Belgium, May 2011 on the use of a bilingual concordancer to assist professional translators. More similar to our approach is the one"
2011.eamt-1.13,W03-0313,0,0.0294151,"do so, we pre-process the user’s TM to compute the word alignments between the source and target segments in each TU. Then, when a new segment s0 is to be translated, the TUs with a fuzzy-match score above the threshold Θ are obtained and the alignment between the words in si and ti are used to mark which words in ti should be changed or kept unedited. Related work. In the literature one can find different approaches that use word or phrase alignments to improve existing TM-based CAT systems; although, to our knowledge, none of them use word alignments for the purpose we study in this paper. Simard (2003) focuses on the creation of TMbased CAT systems able to work at the sub-segment level by proposing as translation sub-segments extracted from longer segments in the matching TUs. To do this, he implements the translation spotting (V´eronis and Langlais, 2000) technique by using statistical word-alignment methods (Och and Ney, 2003); translation spotting consists of identifying, for a pair of parallel sentences, the words or phrases in a TL segment that correspond to the words in a SL segment. The work by Bourdaillet et al. (2009) follows a similar approach, although it does not focus on tradit"
2011.eamt-1.13,steinberger-etal-2006-jrc,0,0.102721,"Missing"
2011.eamt-1.13,C96-2141,0,0.238788,"stance (Levenshtein, 1966): score(s0 , si ) = 1 − a su hermano D(s0 , si ) max(|s0 |, |si |) where |x |stands for the length (in words) of string x and D(x, y) refers to the word-based Levenshtein distance (edit distance) between x and y. a su hermano Figure 2: Example of alignment and matching. 3.2 Corpora For the experiments in this paper we have used word alignments obtained by means of the free/open-source GIZA++1 tool (Och and Ney, 2003) which implements standard word-based statistical machine translation models (Brown et al., 1993) as well as a hidden-Markov-model-based alignment model (Vogel et al., 1996). GIZA++ produces alignments in which a source word can be aligned with many target words, whereas a target word is aligned with, at most, one source word. Following common practice in statistical machine translation (Koehn, 2010, Ch. 4) we have obtained The TMs we have used were extracted from the JRC-Acquis corpus version 3 (Steinberger et al., 2006),2 which contains the total body of European Union (EU) law. Before extracting the TMs used, this corpus was tokenized and lowercased, and then segment pairs in which either of the segments was empty or had more than 9 times words than its counte"
2011.eamt-1.28,J10-4005,0,0.0535847,"ammatical target output. On the other hand, CAT systems segment the input text to be translated and compare each segment against the TUs in the TM (Bowker, 2002). CAT systems produce one or more target equivalences for the source segment and professional translators select and recombine them (perhaps with modification) to produce the desired translation. Both EBMT and CAT systems are developed based on a similar premise but in an EBMT approach, selection and recombination are done automatically to produce the translation without the help of a professional translator. Phrase-based SMT systems (Koehn, 2010), produce a source–target aligned subsentential phrase table which can be adapted as an additional TM to a CAT environment (Simard, 2003; Bourdaillet et al., 2009). SMT phrases have also been used to populate the knowledge database of an EBMT system (Groves and Way, 2006). However, to the best of our knowledge, the use of SMT phrase tables within an EBMT system as an additional sub-sentential TM, has not been attempted so far. Some work has been carried out to integrate MT in a CAT environment to translate the whole segment using the MT system when no matching TU is found in the TM. The TransT"
2011.eamt-1.28,P01-1050,0,0.0370713,"slation is required to tackle the issue of scarce resources, but it can still suffer from very low accuracy within the SMT framework, even for homogeneous domains (Dandapat et al., 2010). Although SMT and EBMT are both data-driven approaches to MT, both of them have their own advantages and limitations. Typically, an SMT system works well with significant amounts of training data. In contrast, an EBMT approach can be developed with a limited example-base (Somers, 2003); also, as with any other data-driven system, an EBMT system works well when training and test sets are quite close in nature (Marcu, 2001). This is because EBMT systems reuse the segments of test sentences that can be found in the source side of the example-base at runtime. Keeping these points in mind, it is important to develop an MT system of reasonably good quality based on limited amounts of data. In this direction, we are inspired to examine different EBMT approaches which can handle the problem of data sparseness. It is often the case that EBMT systems produce a good translation where SMT fails and vice versa. In order to harness the advantages of both approaches, we use a careful combination of both EBMT and SMT to impro"
2011.eamt-1.28,P02-1040,0,0.0802406,"Missing"
2011.eamt-1.28,2007.mtsummit-papers.49,0,0.0905695,"same applies to the differing parts between two parallel sentences. Generalization in this approach consists of replacing the similar or differing sequences with variables and producing a set of translation templates (including atomic translation templates containing no variables). These translation templates are later used to translate new input sentences. Prior to the above approach, other research was carried out to learn translation templates based on syntactic generalization, e.g. (Kaji et al., 1992). A recent work has also focused on morphological generalization to learn EBMT templates (Phillips et al., 2007). EBMT is often linked with a related technique, namely TM. A TM essentially stores source- and target-language translation pairs (called translation units, TUs) for effective reuse of the previous translations. TM is often used to store examples for EBMT systems. It is also widely used in computer-aided translation (CAT) systems to assist professional translators. EBMT systems first find the example (or a set of ex3 Related Issues 3.1 Type of Corpora Both EBMT and SMT are data-driven approaches to MT which need machine-readable corpora as a prerequisite. The size and type of corpus is also im"
2011.eamt-1.28,W03-0313,0,0.0751211,"in the TM (Bowker, 2002). CAT systems produce one or more target equivalences for the source segment and professional translators select and recombine them (perhaps with modification) to produce the desired translation. Both EBMT and CAT systems are developed based on a similar premise but in an EBMT approach, selection and recombination are done automatically to produce the translation without the help of a professional translator. Phrase-based SMT systems (Koehn, 2010), produce a source–target aligned subsentential phrase table which can be adapted as an additional TM to a CAT environment (Simard, 2003; Bourdaillet et al., 2009). SMT phrases have also been used to populate the knowledge database of an EBMT system (Groves and Way, 2006). However, to the best of our knowledge, the use of SMT phrase tables within an EBMT system as an additional sub-sentential TM, has not been attempted so far. Some work has been carried out to integrate MT in a CAT environment to translate the whole segment using the MT system when no matching TU is found in the TM. The TransType system (Langlais et al., 2002) integrates an SMT system within a text editor to suggest possible continuations of the translations b"
2011.eamt-1.28,2006.eamt-1.15,1,0.90968,"ors select and recombine them (perhaps with modification) to produce the desired translation. Both EBMT and CAT systems are developed based on a similar premise but in an EBMT approach, selection and recombination are done automatically to produce the translation without the help of a professional translator. Phrase-based SMT systems (Koehn, 2010), produce a source–target aligned subsentential phrase table which can be adapted as an additional TM to a CAT environment (Simard, 2003; Bourdaillet et al., 2009). SMT phrases have also been used to populate the knowledge database of an EBMT system (Groves and Way, 2006). However, to the best of our knowledge, the use of SMT phrase tables within an EBMT system as an additional sub-sentential TM, has not been attempted so far. Some work has been carried out to integrate MT in a CAT environment to translate the whole segment using the MT system when no matching TU is found in the TM. The TransType system (Langlais et al., 2002) integrates an SMT system within a text editor to suggest possible continuations of the translations being typed by the translator. Our approach attempts to integrate the subsentential TM obtained using SMT techniques within an EBMT syste"
2011.eamt-1.28,C92-2101,0,\N,Missing
2011.mtsummit-papers.18,2011.eamt-1.13,1,0.857511,"Missing"
2011.mtsummit-papers.18,2010.jec-1.4,0,0.0703541,"towards ti . This is done by augmenting the PBSMT translation table with bilingual sub-segments coming from the fuzzy match (si , ti ). Simard and Isabelle (2009) propose a similar approach in which a new feature function is introduced in the log-linear model combination of a PBSMT system to promote the use of the bilingual sub-segments from the fuzzy match (si , ti ). Neither of these two approaches guarantees that the PBSMT system will produce a translated segment containing the translation of the sub-segments that are common to si and s . In contrast, Zhechev and van Genabith (2010) and Koehn and Senellart (2010), who also use a PBSMT system, guarantee that the sub-segments of ti that have been detected to be aligned with the sub-segments in si matched by s appear in the translated segment. Our approach differs from those described above in two ways. First, while they use the TM to improve the results of MT, or MT to translate sub-segments of the TUs, our MT-based approach uses MT to improve the experience of using a TM-based CAT system without actually translating any new material. Second, the approaches above focus on a speciﬁc MT system or family of MT systems (namely, SMT), whereas our MT-based a"
2011.mtsummit-papers.18,kranias-samiotou-2004-automatic,0,0.203955,"ther source of bilingual information such as dictionaries, glossaries, or terminology data bases. As regards commercial TM-based CAT tools, D´ej`aVu1 integrates example-based MT (EBMT) to suggest candidate translations in those cases in which 1 http://www.atril.com 173 an exact match is not found, but partial matches are available (Lagoudaki, 2008). The EBMT-inspired system is used to propose a translation by putting together sub-segments of the partial matchings available. Unfortunately, we have been unable to ﬁnd further details on how this method works. More similar to ours are the work by Kranias and Samiotou (2004), based on the ESTeam CAT system, and the one by Espl`a et al. (2011). Kranias and Samiotou (2004) align the words in each TU at different sub-sentential levels by using a bilingual dictionary (Meyers et al., 1998). Then, when a TU is proposed to the CAT user, the alignments previously computed and the MT system are used, respectively, to detect the target words that need to be changed, and to propose a translation for them. Espl`a et al. (2011) use statistical word-alignment (SWA) models computed by means of GIZA++ (Och and Ney, 2003) to align the SL and TL segments of each TU in the TM. Then"
2011.mtsummit-papers.18,2008.amta-srw.4,0,0.0255193,"x, and is therefore able to use one or more MT systems at once. In addition, as our MT-based approach does not need to have access to the inner workings of the MT systems, it is capable of using on-line MT systems (thus avoiding any local installation) or even any other source of bilingual information such as dictionaries, glossaries, or terminology data bases. As regards commercial TM-based CAT tools, D´ej`aVu1 integrates example-based MT (EBMT) to suggest candidate translations in those cases in which 1 http://www.atril.com 173 an exact match is not found, but partial matches are available (Lagoudaki, 2008). The EBMT-inspired system is used to propose a translation by putting together sub-segments of the partial matchings available. Unfortunately, we have been unable to ﬁnd further details on how this method works. More similar to ours are the work by Kranias and Samiotou (2004), based on the ESTeam CAT system, and the one by Espl`a et al. (2011). Kranias and Samiotou (2004) align the words in each TU at different sub-sentential levels by using a bilingual dictionary (Meyers et al., 1998). Then, when a TU is proposed to the CAT user, the alignments previously computed and the MT system are used,"
2011.mtsummit-papers.18,meyers-etal-1998-multilingual,0,0.0390193,"those cases in which 1 http://www.atril.com 173 an exact match is not found, but partial matches are available (Lagoudaki, 2008). The EBMT-inspired system is used to propose a translation by putting together sub-segments of the partial matchings available. Unfortunately, we have been unable to ﬁnd further details on how this method works. More similar to ours are the work by Kranias and Samiotou (2004), based on the ESTeam CAT system, and the one by Espl`a et al. (2011). Kranias and Samiotou (2004) align the words in each TU at different sub-sentential levels by using a bilingual dictionary (Meyers et al., 1998). Then, when a TU is proposed to the CAT user, the alignments previously computed and the MT system are used, respectively, to detect the target words that need to be changed, and to propose a translation for them. Espl`a et al. (2011) use statistical word-alignment (SWA) models computed by means of GIZA++ (Och and Ney, 2003) to align the SL and TL segments of each TU in the TM. Then, when a TU (si , ti ) is proposed to the user, the pre-computed word alignments are used to determine the target words to change or keep unedited by computing the likelihood of each word wij in ti being kept unedi"
2011.mtsummit-papers.18,J03-1002,0,0.00344947,"ethod works. More similar to ours are the work by Kranias and Samiotou (2004), based on the ESTeam CAT system, and the one by Espl`a et al. (2011). Kranias and Samiotou (2004) align the words in each TU at different sub-sentential levels by using a bilingual dictionary (Meyers et al., 1998). Then, when a TU is proposed to the CAT user, the alignments previously computed and the MT system are used, respectively, to detect the target words that need to be changed, and to propose a translation for them. Espl`a et al. (2011) use statistical word-alignment (SWA) models computed by means of GIZA++ (Och and Ney, 2003) to align the SL and TL segments of each TU in the TM. Then, when a TU (si , ti ) is proposed to the user, the pre-computed word alignments are used to determine the target words to change or keep unedited by computing the likelihood of each word wij in ti being kept unedited:  psK (wij , s , si , ti ) = vil ∈aligned(wij ) matched(vil ) |aligned(wij )| where aligned(wij ) is the set of source words in si that are aligned with the target word wij , and matched(vil ) equals 1 if the source word vil is part of the match between si and s , the segment to be translated, and 0 otherwise. This lik"
2011.mtsummit-papers.18,P02-1040,0,0.0802484,"ain as TMin . Evaluation was carried out by simulating a CAT job in which the source segments in TMtrans are translated using the TUs in TMtest . For each source segment in TMtrans , and using the same FMS threshold Θ used during training, we computed the set of the matching TUs in TMtest , and classiﬁed the words in their target segments as “keep” or “change”. We used the general-purpose free/open-source MT system apertium-en-es, version 0.7, built upon version 3.2 of the Apertium free/open-source MT platform2 (Forcada et al., 2011). This is a rule-based MT system that achieves a BLEU score (Papineni et al., 2002) of 0.20 on the test set provided for the WMT10 translation task.3 This BLEU score may be considered low compared to those achieved by other MT systems, ranging around 0.27, for the same language pair and on the same test set (Callison-Burch et al., 2010). 4.1 Corpora The TMs we have used were extracted from two different parallel corpora already aligned at the sentence level: the JRC-Acquis corpus version 3 (Steinberger et al., 2006),4 which contains the total body of European Union law, and the EMEA corpus version 0.3 (Tiedemann, 2009),5 which is a compilation of documents from the European"
2011.mtsummit-papers.18,2009.mtsummit-papers.14,0,0.151512,"etric classiﬁer whose parameters, the set of feature weights, can be obtained in advance from a separate training TM and then used to translate texts from a different domain without a signiﬁcant loss of accuracy, as demonstrated by our experiments. CAT users could therefore use this MT-based approach in their desktop workstations provided that they have the classiﬁer, on-line access to the MT system(s), and a set of suitable feature weights. Related work. In the literature one can ﬁnd several approaches that combine the beneﬁts of MT and TMs beyond the obvious β-combination scenario deﬁned by Simard and Isabelle (2009), in which MT is used to translate a new segment when no matching TU above a FMS threshold β is found in the TM. Bic¸ici and Dymetman (2008) integrate a phrase-based statistical MT (PBSMT) (Koehn, 2010) system using discontinuous bilingual sub-segments into a TM-based CAT tool. The PBSMT system is trained on the same TM and, when a new source segment s is to be translated, the segments si and ti in the best matching TU are used to bias the statistical translation of s towards ti . This is done by augmenting the PBSMT translation table with bilingual sub-segments coming from the fuzzy match ("
2011.mtsummit-papers.18,steinberger-etal-2006-jrc,0,0.0286856,"Missing"
2011.mtsummit-papers.18,W10-3806,0,0.62089,"Missing"
2011.mtsummit-papers.18,W10-1703,0,\N,Missing
2012.eamt-1.54,2011.eamt-1.30,1,0.817279,"sible solutions would be to generate all possible combinations of translations, and score them on a language model of the target language. This approach is taken in the METIS - II system (Melero et al., 2007). This has the benefit of being easy to implement, and only requiring a bilingual dictionary and a monolingual target language corpus. It has the drawbacks of being both slow – many translations must be performed – and not very customisable – control over the final translation is left to the TL model. Another possible solution, and one that is already used in some Apertium language pairs (Brandt et al., 2011; Wiechetek et al., 2010) is to use constraint grammar (Karlsson et al., 1995) rules to choose between possible alternative translations. An advantage of this is that the constraint grammar formalism is well known, and powerful, allowing context searches of unlimited size. However, it is too slow to be able to be used for production systems, as the speed is in the order of a few hundred words per second as opposed to thousands of words per second for the slowest Apertium module. Another approach not requiring a parallel corpus is presented by Dagan and Itai (1994). They first parse the SL sent"
2012.eamt-1.54,2007.tmi-papers.6,0,0.0412968,"he most adequate sense. Thus, it is not necessary to choose between a series of finegrained senses if all these senses result in the same final translation. The dominant approach to MT for language pairs with sufficient training data is phrase-based statistical machine translation; in this approach, lexical selection is performed by a combination of coocurrence in the phrase table, and score from the targetlanguage model (Koehn, 2010). There have however been attempts to improve on this by looking at global lexical selection over the whole sentence, see e.g. (Venkatapathy and Bangalore, 2007; Carpuat and Wu, 2007). In order to test different approaches to lexical selection for RBMT, we use the Apertium (Forcada et al., 2011) platform. This free/open-source platform includes 30 language pairs (as of February 2012). Sánchez-Martínez et al. (2007) describe a method to perform lexical selection in Apertium based on training a source-language bag-of-words model using TL cooccurrence statistics. This approach was tested, but abandoned as it produced less adequate translations than using the translation marked as default by a linguist in the bilingual dictionary. Other possible solutions would be to generate"
2012.eamt-1.54,J94-4003,0,0.61255,"n some Apertium language pairs (Brandt et al., 2011; Wiechetek et al., 2010) is to use constraint grammar (Karlsson et al., 1995) rules to choose between possible alternative translations. An advantage of this is that the constraint grammar formalism is well known, and powerful, allowing context searches of unlimited size. However, it is too slow to be able to be used for production systems, as the speed is in the order of a few hundred words per second as opposed to thousands of words per second for the slowest Apertium module. Another approach not requiring a parallel corpus is presented by Dagan and Itai (1994). They first parse the SL sentence and extract syntactic relations, such as verb + object, they then translate these with a bilingual dictionary and use collocation statistics from a TL corpus to choose the most adequate translation. While this method does not rely on the existence of a parallel corpus, it does depend on some way of identifying SL syntactic relations – which may not be available in all RBMT systems. The rest of the paper is laid out as follows: Section 2 presents some design decisions that were made in the development of the module. Section 3 describes in detail the rule forma"
2012.eamt-1.54,J03-1002,0,0.00473992,"found in the Apertium monolingual dictionary. We use version 6.0 of the EuroParl corpus (Koehn, 2005), and take the first 1.4 million lines for training.1 We used the Apertium English to Spanish pair apertium-en-es2 as it is one of the few pairs that has dictionaries with more than one alternative translation per word.3 4.1 Learning lexical selection rules from a parallel corpus The procedure to learn rules from a parallel corpus is as follows: We first morphologically analyse and disambiguate for part-of-speech both the SL and TL sides of the corpus. These are then word-aligned with GIZA++ (Och and Ney, 2003). 1 The remaining lines were held out for future use. Available from http://wiki.apertium.org/wiki/ SVN; SVN revision: 35684 3 The lexical selection module is available as free/open-source software in the package apertium-lex-tools. This paper uses SVN revision: 35799 2 217 We then pass the SL side of the corpus through the lexical-transfer stage of the MT system we are learning the rules for; this gives three sets of sentences: the tagged SL sentences, the tagged TL sentences and the possible translations of the SL words into the TL yielded by the bilingual dictionary. We take these three set"
2012.eamt-1.54,P02-1040,0,0.0913672,"Missing"
2012.eamt-1.54,W04-3250,0,0.276079,"Missing"
2012.eamt-1.54,2005.mtsummit-papers.11,0,0.049944,"a parallel corpus, and test the module on a well-known task for the evaluation of MT. The experimental setup follows the training of the baseline system in the shared task on MT at WMT11 (Callison-Burch et al., 2011), with the following differences: In place of the default Moses perl-based tokeniser, tokenisation was done using the Apertium morphological analyser (CortésVaíllo and Ortiz-Rojas, 2011). The corpus was also not lowercased; instead the case of known words was changed to the dictionary case as found in the Apertium monolingual dictionary. We use version 6.0 of the EuroParl corpus (Koehn, 2005), and take the first 1.4 million lines for training.1 We used the Apertium English to Spanish pair apertium-en-es2 as it is one of the few pairs that has dictionaries with more than one alternative translation per word.3 4.1 Learning lexical selection rules from a parallel corpus The procedure to learn rules from a parallel corpus is as follows: We first morphologically analyse and disambiguate for part-of-speech both the SL and TL sides of the corpus. These are then word-aligned with GIZA++ (Och and Ney, 2003). 1 The remaining lines were held out for future use. Available from http://wiki.ape"
2012.eamt-1.54,J10-4005,0,0.0209504,"age (TL). The task is related to the task of word-sense disambiguation (Ide and Véronis, 1998). The difference is that its aim is to find the most adequate translation, not the most adequate sense. Thus, it is not necessary to choose between a series of finegrained senses if all these senses result in the same final translation. The dominant approach to MT for language pairs with sufficient training data is phrase-based statistical machine translation; in this approach, lexical selection is performed by a combination of coocurrence in the phrase table, and score from the targetlanguage model (Koehn, 2010). There have however been attempts to improve on this by looking at global lexical selection over the whole sentence, see e.g. (Venkatapathy and Bangalore, 2007; Carpuat and Wu, 2007). In order to test different approaches to lexical selection for RBMT, we use the Apertium (Forcada et al., 2011) platform. This free/open-source platform includes 30 language pairs (as of February 2012). Sánchez-Martínez et al. (2007) describe a method to perform lexical selection in Apertium based on training a source-language bag-of-words model using TL cooccurrence statistics. This approach was tested, but aba"
2012.eamt-1.54,W07-0413,0,0.0277975,"e most adequate translation, not the most adequate sense. Thus, it is not necessary to choose between a series of finegrained senses if all these senses result in the same final translation. The dominant approach to MT for language pairs with sufficient training data is phrase-based statistical machine translation; in this approach, lexical selection is performed by a combination of coocurrence in the phrase table, and score from the targetlanguage model (Koehn, 2010). There have however been attempts to improve on this by looking at global lexical selection over the whole sentence, see e.g. (Venkatapathy and Bangalore, 2007; Carpuat and Wu, 2007). In order to test different approaches to lexical selection for RBMT, we use the Apertium (Forcada et al., 2011) platform. This free/open-source platform includes 30 language pairs (as of February 2012). Sánchez-Martínez et al. (2007) describe a method to perform lexical selection in Apertium based on training a source-language bag-of-words model using TL cooccurrence statistics. This approach was tested, but abandoned as it produced less adequate translations than using the translation marked as default by a linguist in the bilingual dictionary. Other possible solution"
2012.eamt-1.54,H05-1097,0,0.430219,"Missing"
2012.eamt-1.54,2004.tmi-1.9,0,0.114922,"Missing"
2014.amta-researchers.4,2003.mtsummit-papers.4,0,0.0424401,"Our approach, while related to the research described above, exhibits three main novelties: (i) it removes the dependency on knowledge of the internal workings of the MT system used, (ii) it removes the need to modify an MT system’s behavior in some way and (iii) avoids having to pre-process a user’s TM. We repair the mismatched sub-segments in a translation unit using a simple, yet novel, method that, unlike those by Hewavitharana et al. (2005) and Dandapat et al. (2011), takes context around mismatched words into account. Patching uses overlapping sub-segments as powerful anchors much like Brown et al. (2003)’s maximal leftoverlap compositional (example-based) MT system where the use of overlapping sub-segments reduces ”boundary friction” problems and increases the likelihood of producing a correct translation. Since patching treats sources of external bilingual translation information as black boxes, we generate translations on the fly without training SMT models on the user’s TM. We are not aware of any research work that: (a) uses any source of bilingual information for translation or (b) uses the context around mismatched words for repair. In the following sections, we show how the fuzzy-match"
2014.amta-researchers.4,2011.eamt-1.28,1,0.77716,"threshold. Their work, unlike Koehn and Senellart (2010), takes matched parts in s and replaces them with their counterparts in t. The main drawback of the approaches from Ma et al. (2011), Koehn and Senellart (2010), Zhechev and Genabith (2010), and Bic¸ici and Dymetman (2008) is that they are all based on SMT and either have access to the internals of an SMT system trained on the Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 43 user’s or related data or modify its behavior in some way. Other research work (Hewavitharana et al., 2005; Dandapat et al., 2011) focuses on the identification of the sub-segments in the TL segment t of the translation unit (s, t) needed to produce t0 and then produces a translation by applying a set of edit operations over t. In particular, Hewavitharana et al. (2005) first align the mismatches in s to their TL translations in t by means of a modified IBM model 1 and then apply the same edit operations —substitutions, deletions and insertions— that are needed to convert s into s0 to the TL segment t. Their resulting translation may contain agreement and reordering errors because their method assumes that edit operation"
2014.amta-researchers.4,2011.mtsummit-papers.18,1,0.935321,"Missing"
2014.amta-researchers.4,2005.eamt-1.18,0,0.497123,"match score being used as a threshold. Their work, unlike Koehn and Senellart (2010), takes matched parts in s and replaces them with their counterparts in t. The main drawback of the approaches from Ma et al. (2011), Koehn and Senellart (2010), Zhechev and Genabith (2010), and Bic¸ici and Dymetman (2008) is that they are all based on SMT and either have access to the internals of an SMT system trained on the Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 43 user’s or related data or modify its behavior in some way. Other research work (Hewavitharana et al., 2005; Dandapat et al., 2011) focuses on the identification of the sub-segments in the TL segment t of the translation unit (s, t) needed to produce t0 and then produces a translation by applying a set of edit operations over t. In particular, Hewavitharana et al. (2005) first align the mismatches in s to their TL translations in t by means of a modified IBM model 1 and then apply the same edit operations —substitutions, deletions and insertions— that are needed to convert s into s0 to the TL segment t. Their resulting translation may contain agreement and reordering errors because their method ass"
2014.amta-researchers.4,2010.jec-1.4,0,0.0950407,"Missing"
2014.amta-researchers.4,P11-1124,0,0.11147,"Missing"
2014.amta-researchers.4,P00-1056,0,0.181992,"s.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 46 any translation τ of any segment σ in s contain words for which there is no evidence to modify them. In the absence of information, they will not be changed. 2.4 Step 4: pair translations of τ and τ 0 to form patching operators After the initial matching occurs from the translations of s to form (σ, τ ) pairs, (σ 0 , τ 0 ) pairs are created by translating mismatched sub-segments from s0 . The alignment found between words in s and words in s0 during fuzzy matching are used by an algorithm, analogous to that by Och and Ney (2000), to extract phrase pairs that project mismatched σ sub-segments in s into the corresponding sub-segments σ 0 in s0 . The σ 0 sub-segments are sent to the MT system or other source of bilingual information to obtain their translations τ 0 . The final result is a set of patching operators that contain translations that match the previously mismatched words in s0 and s. In steps 1 through 3, we have already created the (σ, τ ) pairs; now we translate s0 subsegments to form (σ 0 , τ 0 ) pairs. In our example, the (σ 0 , τ 0 ) pairs translated by Apertium (Forcada et al., 2011) are: σ0 σ positions"
2014.amta-researchers.4,2009.mtsummit-papers.14,0,0.772869,"Missing"
2014.amta-researchers.4,W10-3806,0,0.12996,"Missing"
2014.eamt-1.4,A92-1018,0,0.0166991,"le outputs, A is the |Γ|×|Γ |matrix of state-to-state transition probabilities, B is the |Γ|×|Σ |matrix with the probability of each observable output σ ∈ Σ being emitted from each state γ ∈ Γ, and the vector π, with dimensionality |Γ|, defines the initial probability of each state. The system produces an output each time a state is reached after a transition. In our method, Γ is made up of all the paradigms in the dictionary and Σ corresponds to the set of suffixes produced by all these paradigms. Our HMMs are trained in a way very similar to HMMs used in unsupervised part-of-speech tagging (Cutting et al., 1992), that is, by using the Baum-Welch algorithm (Baum, 1972) with an untagged corpus. The training corpus is built from a text corpus as follows: (i) the monolingual dictionary is used in order to obtain the set F of all possible word forms; (ii) the word forms in the text corpus that belong to F are assigned all their corresponding suffix and paradigm pairs; (iii) the word forms not in F are assigned the set of suffix and paradigm pairs obtained from the set L of their compatible candidates, as described in Section 4. Once the HMM is trained, the probability qt (cn ) of assigning the word form l"
2014.eamt-1.4,C12-3013,0,0.0171133,"ound. For instance, an expert could decide that in order to correctly choose the inflection paradigm of most verbs in French the infinitive and the first person plural present indicative forms are needed; dictionary developers must then provide these two forms when inserting a new verb. Bartuskov´a and Sedl´acek (2002) also present a tool for semi-automatic assignment of words to declension patterns; their system is based on a decision tree with a question in every node. Their proposal, unlike ours, works only for nouns and is aimed at experts because of the technical nature of the questions. Desai et al. (2012) focus on verbs and present a system for paradigm assignment based on the information collected from a corpus for each compatible paradigm; if the automatic method fails, users are then required to manually enter the correct paradigm. As regards the automatic acquisition of morˇ phological resources for MT, the work by Snajder (2013) is of particular interest: he turns the choice of the most appropriate paradigm for a given word into a machine learning problem. Given the values of a set of features extracted from a monolingual corpus and from the orthographic properties of the lemmas, each com"
2014.eamt-1.4,E12-1066,0,0.0644761,"Missing"
2014.eamt-1.4,R11-1047,1,0.894239,"Missing"
2014.eamt-1.4,sanchez-cartagena-etal-2012-source,1,0.803826,"Missing"
2014.eamt-1.4,2012.freeopmt-1.4,0,0.0315719,"Missing"
2014.eamt-1.4,W13-2201,0,\N,Missing
2015.eamt-1.19,2000.tc-1.5,0,\N,Missing
2015.eamt-1.20,J96-1002,0,0.220719,"actually lexical selection in MT; they used a parser to identify syntactic relations such as subject–object or subject–verb. After generating all the possible translations for a given input sentence using an ambiguous bilingual dictionary, they extract the syntactic tuples from the TL and count the frequency in a previously-trained TL model of tuples. They use maximum-likelihood estimation to calculate the probability that a given 1 145 Such as a morphological or syntactic analyser. TL tuple is the translation of a given SL tuple, with an automatically determined confidence threshold. Later, Berger et al. (1996) illustrated the use of maximum-entropy classifiers on the specific problem of lexical selection in IBM-style word-based statistical MT. Other authors (Melero et al., 2007) have used TL models to rank the translations resulting from all possible combinations of lexical selections. Nowadays, in state-of-the-art phrasebased statistical MT (Koehn, 2010), lexical selection is taken care of by a combination of the translation model and the language model. The translation model provides probabilities of translation between words or word sequences (often referred to as phrases) in the source and targ"
2015.eamt-1.20,J94-4003,0,0.491982,"mputational cost. 1 Mikel L. Forcada Dept. Lleng. i Sist. Inform., Universitat d’Alacant, E-03071 Alacant 1.1 Introduction Corpus-based machine translation (MT) has been the primary research direction in the field of MT in recent years. However, rule-based MT (RBMT) systems are still being developed, and there are many successful commercial and non-commercial systems. One reason for the continued development of RBMT systems is that in order to be successful, c 2015 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. Prior work Dagan and Itai (1994) used the term word sense disambiguation to refer to what is actually lexical selection in MT; they used a parser to identify syntactic relations such as subject–object or subject–verb. After generating all the possible translations for a given input sentence using an ambiguous bilingual dictionary, they extract the syntactic tuples from the TL and count the frequency in a previously-trained TL model of tuples. They use maximum-likelihood estimation to calculate the probability that a given 1 145 Such as a morphological or syntactic analyser. TL tuple is the translation of a given SL tuple, wi"
2015.eamt-1.20,W04-3250,0,0.0634887,"module returns more than one translation, Apertium will select the default one if marked or the first one of not.12 The table in Figure 2 gives an overview of the inputs.In the description it is assumed that the reference translation has been annotated by hand. However, hand annotation is a time-consuming process, and was not possible. A description of how the reference was built is given in Section 3.4. diff(Tr (si ), Tt (si )) = 3.3.2 3.3.3 Confidence intervals Confidence intervals for both metrics will be calculated through bootstrap resampling (Efron and Tibshirani, 1994) as described by Koehn (2004). In all cases, bootstrap resampling will be carried out for 1,000 iterations. Where the p = 0.05 confidence intervals overlap, we will also perform paired bootstrap resampling (Koehn, 2004). 3.4 For creating the test corpora, providing a SL corpus for training, and a TL corpus for scoring, we used four parallel corpora: • Ofis ar Brezhoneg (OAB): This parallel corpus of Breton and French has been collected specifically for lexical-selection experiments from translations produced by Ofis ar Brezhoneg ‘The Office of the Breton language’. The corpus has recently been made available online throug"
2015.eamt-1.20,2005.mtsummit-papers.11,0,0.020643,"is ar Brezhoneg ‘The Office of the Breton language’. The corpus has recently been made available online through OPUS.13 • South-East European Times (SETimes): Described in Tyers and Alperen (2010), this corpus is a multilingual corpus of the Balkan languages (and English) in the news domain. The Macedonian and English part will be used. • Open Data Euskadi (OpenData): This is a Basque–Spanish parallel corpus made from the translation memories of the Herri Arduralaritzaren Euskal Erakundea ‘Basque Institute of Public Administration’.14 • European Parliament Proceedings (EuroParl): Described by Koehn (2005), this is a multilingual corpus of the European Union official languages. We are using the English–Spanish data from version 7.15 Machine translation performance This is an extrinsic evaluation, which ideally would test how much the system improves as regards an approximate measurement of final translation quality in a real system. For this task, we use the widely-used BLEU metric (Papineni et al., 2002). This is not ideal for evaluating the task of a lexical selection module as the performance of the module will depend greatly on (a) the coverage of the bilingual dictionaries of the RBMT syst"
2015.eamt-1.20,J10-4005,0,0.0126561,"use maximum-likelihood estimation to calculate the probability that a given 1 145 Such as a morphological or syntactic analyser. TL tuple is the translation of a given SL tuple, with an automatically determined confidence threshold. Later, Berger et al. (1996) illustrated the use of maximum-entropy classifiers on the specific problem of lexical selection in IBM-style word-based statistical MT. Other authors (Melero et al., 2007) have used TL models to rank the translations resulting from all possible combinations of lexical selections. Nowadays, in state-of-the-art phrasebased statistical MT (Koehn, 2010), lexical selection is taken care of by a combination of the translation model and the language model. The translation model provides probabilities of translation between words or word sequences (often referred to as phrases) in the source and target language. The TL model provides probabilities of word sequences in the TL. Mareˇcek et al. (2010) trained a maximum-entropy lexical selector for their dependency-grammar-based transfer system TectoMT using a bilingual corpus. More recently, Tyers et al. (2012) presented a method of lexical selection for RBMT based on rules which select or remove t"
2015.eamt-1.20,P07-2045,0,0.00877625,"cluded to show the upper bound for the performance of the lexical-selection module. • Target language model (TLM). One method of lexical selection is to use the existing MT system to generate all the possible translations for an input sentence, and then score these translations on-line on a model of the TL. The highest scoring sentence is then output. This is the method used by Melero et al. (2007). 4 Results As we are working with binary features, we use the implementation of generalised iterative scaling 16 The exact configuration of GIZA ++ used is equivalent to running the M OSES toolkit (Koehn et al., 2007) in default configuration up to step three of training. 17 The development corpus was used for checking the value for frequency pruning of features. 150 Pair br-fr mk-en eu-es en-es Lines 57,305 190,493 765,115 1,467,708 Extract. 4,668 19,747 87,907 312,162 train 2,668 17,747 85,907 310,162 dev 1,000 1,000 1,000 1,000 test 1,000 1,000 1,000 1,000 No. amb 603 13,134 1,806 2,082 Av. amb 3.06 3.06 3.11 2.28 Table 1: Statistics about the source corpora. The column no. amb gives the number of unique tokens with more than one possible translation. The column av. amb gives the average number of trans"
2015.eamt-1.20,W10-1730,0,0.0445016,"Missing"
2015.eamt-1.20,P14-2123,0,0.0132542,"arrain    0 otherwise (1) This feature considers a context of zero words to the left of the problem word and one word (+ handi) to the right of it. As a result of training, each of the nF features hsk (t, c) in the classifier is assigned a weight λsk . Combining these weights of active features as in equation (2) yields the probability of a translation t for word s in context c. n ps (t|c) = (2) k=1 In this equation, Z s (c) is a normalising constant. Thus, the most probable translation t? can be found using t? = arg max ps (t|c) = arg max t∈Ts (s) 2 The work by Ravi and Knight (2011) and Nuhn and Ney (2014), who decipher word-ciphered text using monolingual corpora only may be seen as a generalised version of the problem of lexical selection without parallel corpora. F X 1 exp λsk hsk (t, c) Z s (c) nF X λsk hsk (t, c), t∈Ts (s) k=1 (3) 3 146 We follow the notation of Berger et al. (1996) S→ preposti=|G| → ({gi }i=1 , S) → lexsel → (g ? , S) → → τ (g ? , S) lexsel lexsel Figure 1: A schema of the lexical selection process: source sentence S has |G |lexical selection paths gi : lexsel selects one of them g ? , which is used to generate translation τ (g ? , S). where Ts (s) is the set of possible"
2015.eamt-1.20,J03-1002,0,0.00850824,"ough for all pairs there is a parallel corpus available for evaluation (see Section 3.3).5 Breton–French (Tyers, 2010): Bilingual dictionaries were not built with polysemy in mind from the outset, but some entries were added later to start work on lexical selection.6 Macedonian–English: The Macedonian–English pair in Apertium was created specifically for the purposes of running lexical-selection experiments. The lexical resources for the pair were tuned to the SETimes parallel corpus (Tyers and Alperen, 2010). The most probable entry from automatic word alignment of this corpus using GIZA ++ (Och and Ney, 2003) was checked to ensure that it was an adequate translation, and if so marked as the default.7 As a result of attempting to include all possible translations, the average number of translations per word is much higher than in other pairs.8 Basque–Spanish (Ginest´ı-Rosell et al., 2009): alternative translations were included in the bilingual dictionary.9 English–Spanish: The English–Spanish pair was developed from a combination of the English– Catalan and Spanish–Catalan pairs, and contains a number of entries in the bilingual dictionary with more than one translation.10 3.3 Performance measures"
2015.eamt-1.20,P02-1040,0,0.100052,"Missing"
2015.eamt-1.20,P11-1002,0,0.0207978,"andi (t, c) = handi follows arrain    0 otherwise (1) This feature considers a context of zero words to the left of the problem word and one word (+ handi) to the right of it. As a result of training, each of the nF features hsk (t, c) in the classifier is assigned a weight λsk . Combining these weights of active features as in equation (2) yields the probability of a translation t for word s in context c. n ps (t|c) = (2) k=1 In this equation, Z s (c) is a normalising constant. Thus, the most probable translation t? can be found using t? = arg max ps (t|c) = arg max t∈Ts (s) 2 The work by Ravi and Knight (2011) and Nuhn and Ney (2014), who decipher word-ciphered text using monolingual corpora only may be seen as a generalised version of the problem of lexical selection without parallel corpora. F X 1 exp λsk hsk (t, c) Z s (c) nF X λsk hsk (t, c), t∈Ts (s) k=1 (3) 3 146 We follow the notation of Berger et al. (1996) S→ preposti=|G| → ({gi }i=1 , S) → lexsel → (g ? , S) → → τ (g ? , S) lexsel lexsel Figure 1: A schema of the lexical selection process: source sentence S has |G |lexical selection paths gi : lexsel selects one of them g ? , which is used to generate translation τ (g ? , S). where Ts (s)"
2015.eamt-1.20,2010.eamt-1.13,1,0.827828,"uild RBMT systems. Translation is implemented as a pipeline consisting of the following modules: morphological analysis, morphological disambiguation, lexical transfer, lexical selection, structural transfer and morphological generation. 3.2 Language pairs Evaluation will be performed using four Apertium (Forcada et al., 2011) language pairs. These pairs have been selected as they include languages with different morphological complexity, and different amounts of resources available — although for all pairs there is a parallel corpus available for evaluation (see Section 3.3).5 Breton–French (Tyers, 2010): Bilingual dictionaries were not built with polysemy in mind from the outset, but some entries were added later to start work on lexical selection.6 Macedonian–English: The Macedonian–English pair in Apertium was created specifically for the purposes of running lexical-selection experiments. The lexical resources for the pair were tuned to the SETimes parallel corpus (Tyers and Alperen, 2010). The most probable entry from automatic word alignment of this corpus using GIZA ++ (Och and Ney, 2003) was checked to ensure that it was an adequate translation, and if so marked as the default.7 As a r"
2015.eamt-1.20,2012.eamt-1.54,1,0.760078,"Missing"
2015.eamt-1.20,H05-1097,0,0.0896984,"Missing"
2015.eamt-1.4,W14-3339,0,0.136457,"Missing"
2015.eamt-1.4,W13-2242,0,0.274712,"Missing"
2015.eamt-1.4,W11-2131,0,0.07342,"Missing"
2015.eamt-1.4,C04-1046,0,0.124736,"ield of machine translation (MT) have led to the adoption of this technology by many companies and institutions all around the world in order to bypass the linguistic barriers and reach out to broader audiences. Unfortunately, we are still far from the point of having MT systems able to produce translations with the level of quality required for dissemination in formal scenarios, where human supervision and MT post-editing are unavoidable. It therefore becomes critical to minimise the cost of this human post-editing. This has motivated a growing interest in the field of MT quality estimation (Blatz et al., 2004; Specia et al., 2010; Specia and Soricut, 2013), which is the field that focuses on developing techniques that allow to estimate the quality of the translation hypotheses produced by an MT system. Most efforts in MT quality estimation (MTQE) are aimed at evaluating the quality of whole translated segments, in terms of post-editing time, number of editions needed, and other related metrics (Blatz et al., 2004). Our work is focused on the sub-field of word-level MTQE. The main advantage of word-level MTQE is that it allows not only to estimate the effort needed to post-edit the output of an MT"
2015.eamt-1.4,J93-2003,0,0.0476226,"othesis T of the SL sentence S to help the interactive MT system to choose the translation suggestions to be made to the user. Ueffing and Ney (2005) extend this application to word-level MTQE also to automatically reject those target words t with low confidence scores from the translation proposals. This second approach incorporates the use of probabilistic lexicons as a source of translation information. Blatz et al. (2003) introduce a more complex collection of features for word-level MTQE, using semantic features based on WordNet (Miller, 1995), translation probabilities from IBM model 1 (Brown et al., 1993), word posterior probabilities (Blatz et al., 2003), and alignment templates from statistical MT (SMT) models. All the features they use are combined to train a binary classifier which is used to determine the confidence scores. Ueffing and Ney (2007) divide the features used 20 by their approach in two types: those which are independent of the MT system used for translation (system-independent), and those which are extracted from internal data of the SMT system they use for translation (system-dependent). These features are obtained by comparing the output of an SMT system T1 to a collection"
2015.eamt-1.4,W14-3340,0,0.0728255,"Missing"
2015.eamt-1.4,2011.mtsummit-papers.18,1,0.898234,"Missing"
2015.eamt-1.4,W03-0413,0,0.139643,"e fly for new translations. The rest of the paper is organised as follows. Section 2 briefly reviews the state of the art in word-level MTQE. Section 3 describes our binaryclassification approach, the sources of information, and the collection of features used. Section 4 describes the experimental setting used for our experiments, whereas Section 5 reports and discusses the results obtained. The paper ends with some concluding remarks and the description of ongoing and possible future work. 2 Related work Some of the early work on word-level MTQE can be found in the context of interactive MT (Gandrabur and Foster, 2003; Ueffing and Ney, 2005). Gandrabur and Foster (2003) obtain confidence scores for each word t in a given translation hypothesis T of the SL sentence S to help the interactive MT system to choose the translation suggestions to be made to the user. Ueffing and Ney (2005) extend this application to word-level MTQE also to automatically reject those target words t with low confidence scores from the translation proposals. This second approach incorporates the use of probabilistic lexicons as a source of translation information. Blatz et al. (2003) introduce a more complex collection of features f"
2015.eamt-1.4,2013.tc-1.10,0,0.0781078,"to the adoption of this technology by many companies and institutions all around the world in order to bypass the linguistic barriers and reach out to broader audiences. Unfortunately, we are still far from the point of having MT systems able to produce translations with the level of quality required for dissemination in formal scenarios, where human supervision and MT post-editing are unavoidable. It therefore becomes critical to minimise the cost of this human post-editing. This has motivated a growing interest in the field of MT quality estimation (Blatz et al., 2004; Specia et al., 2010; Specia and Soricut, 2013), which is the field that focuses on developing techniques that allow to estimate the quality of the translation hypotheses produced by an MT system. Most efforts in MT quality estimation (MTQE) are aimed at evaluating the quality of whole translated segments, in terms of post-editing time, number of editions needed, and other related metrics (Blatz et al., 2004). Our work is focused on the sub-field of word-level MTQE. The main advantage of word-level MTQE is that it allows not only to estimate the effort needed to post-edit the output of an MT system, but also to guide post-editors on which"
2015.eamt-1.4,P13-4014,0,0.0759888,"Missing"
2015.eamt-1.4,2005.eamt-1.35,0,0.0718838,"The rest of the paper is organised as follows. Section 2 briefly reviews the state of the art in word-level MTQE. Section 3 describes our binaryclassification approach, the sources of information, and the collection of features used. Section 4 describes the experimental setting used for our experiments, whereas Section 5 reports and discusses the results obtained. The paper ends with some concluding remarks and the description of ongoing and possible future work. 2 Related work Some of the early work on word-level MTQE can be found in the context of interactive MT (Gandrabur and Foster, 2003; Ueffing and Ney, 2005). Gandrabur and Foster (2003) obtain confidence scores for each word t in a given translation hypothesis T of the SL sentence S to help the interactive MT system to choose the translation suggestions to be made to the user. Ueffing and Ney (2005) extend this application to word-level MTQE also to automatically reject those target words t with low confidence scores from the translation proposals. This second approach incorporates the use of probabilistic lexicons as a source of translation information. Blatz et al. (2003) introduce a more complex collection of features for word-level MTQE, usin"
2015.eamt-1.4,J07-1003,0,0.0834112,"low confidence scores from the translation proposals. This second approach incorporates the use of probabilistic lexicons as a source of translation information. Blatz et al. (2003) introduce a more complex collection of features for word-level MTQE, using semantic features based on WordNet (Miller, 1995), translation probabilities from IBM model 1 (Brown et al., 1993), word posterior probabilities (Blatz et al., 2003), and alignment templates from statistical MT (SMT) models. All the features they use are combined to train a binary classifier which is used to determine the confidence scores. Ueffing and Ney (2007) divide the features used 20 by their approach in two types: those which are independent of the MT system used for translation (system-independent), and those which are extracted from internal data of the SMT system they use for translation (system-dependent). These features are obtained by comparing the output of an SMT system T1 to a collection of alternative T translations {Ti }N i=2 obtained by using the N -best list from the same SMT system. Several distance metrics are then used to check how often word tj , the word in position j of T , is found in each translation alternative Ti , and h"
2015.eamt-1.4,W14-3302,0,\N,Missing
2015.eamt-1.45,E06-1032,0,\N,Missing
2015.eamt-1.45,W10-1751,0,\N,Missing
2015.eamt-1.45,W14-3301,0,\N,Missing
2015.eamt-1.45,P02-1040,0,\N,Missing
2015.eamt-1.45,W14-3319,1,\N,Missing
2015.eamt-1.45,P11-1105,0,\N,Missing
2015.eamt-1.45,P10-2041,0,\N,Missing
2015.eamt-1.45,W05-0909,0,\N,Missing
2015.eamt-1.45,P07-2045,0,\N,Missing
2015.eamt-1.45,W07-0718,0,\N,Missing
2015.eamt-1.45,C14-1111,0,\N,Missing
2015.eamt-1.45,P12-3005,0,\N,Missing
2015.eamt-1.45,2012.eamt-1.67,1,\N,Missing
2015.eamt-1.45,2014.eamt-1.4,1,\N,Missing
2015.eamt-1.45,W14-3320,0,\N,Missing
2015.eamt-1.45,2005.mtsummit-papers.11,0,\N,Missing
2015.eamt-1.45,ljubesic-etal-2014-tweetcat,1,\N,Missing
2015.eamt-1.45,W15-3036,1,\N,Missing
2015.eamt-1.45,rubino-etal-2014-quality,1,\N,Missing
2015.eamt-1.45,W15-3022,1,\N,Missing
2015.eamt-1.45,W15-4903,1,\N,Missing
2015.eamt-1.45,2015.eamt-1.4,1,\N,Missing
2015.eamt-1.45,espla-gomis-etal-2014-comparing,1,\N,Missing
2015.eamt-1.45,W15-3001,0,\N,Missing
2015.eamt-1.45,ljubesic-toral-2014-cawac,1,\N,Missing
2015.eamt-1.45,W14-0405,1,\N,Missing
2015.eamt-1.45,2005.iwslt-1.8,0,\N,Missing
2015.eamt-1.45,W16-3421,1,\N,Missing
2015.eamt-1.45,D07-1078,0,\N,Missing
2015.eamt-1.45,W08-0509,0,\N,Missing
2015.eamt-1.45,W11-2123,0,\N,Missing
2015.eamt-1.45,P14-1129,0,\N,Missing
2015.eamt-1.45,W16-2347,0,\N,Missing
2015.eamt-1.45,W16-2375,1,\N,Missing
2015.eamt-1.45,W16-2367,1,\N,Missing
2015.eamt-1.45,W16-3423,1,\N,Missing
2015.eamt-1.5,C04-1046,0,0.314325,"ensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. 27 Machine translation: You could, for instance, use machine translation (MT) to get a draft of the translation of each segment, MT(si ); vendors and experts tell you that you will save effort by postediting MT(si ) into your desired translation ti .1 Machine translation output MT(si ) may just be text, but it could come with annotations to help you make the most of it; for instance, words could be color-coded according to how confident the system is about them (Ueffing and Ney, 2007; Ueffing and Ney, 2005; Blatz et al., 2004), or unknown words that come out untranslated may be marked so that you spot them clearly. Machine-translated segments could even be accompanied by indicators of their estimated quality (Specia and Soricut, 2013; Specia et al., 2010; Blatz et al., 2004) which may be used to ascertain whether the output of the MT system is worth being post-edited or not. If someone measured your post-editing effort (in time, in number of keystrokes, in number of words changed, in money you would have to pay another translator to do it, etc.), when turning MT(si ) into ti , they could call that effort eMT i . Tr"
2015.eamt-1.5,2011.eamt-1.28,1,0.850282,"ually as a percentage called fuzzy match score that accounts for the amount of text that is common to both segments2 — and even marks for you the words in s?i that do not match those in si . Let’s call all this information TM(si ): your job is to use it to turn t?i into the final translation ti . If the fuzzy match is good, you will spend less effort than if you started from scratch. Let us call eTM the effort to turn the t?i provided by i TM(si ) into the desired translation ti .3 Mixing them up: You could even have available another technology, fuzzy-match repair (FMR; (Ortega et al., 2014; Dandapat et al., 2011; Hewavitharana et al., 2005; Kranias and Samiotou, 2004)), that integrates the two technologies just mentioned: after a suitable fuzzy match is found, machine translation (or another source of bilingual information) is used to repair, i.e. edit some parts of t?i , to take into account what changes from s?i to si to try to save even more effort; it tells you all that TM(si ) tells you, but also marks the parts that have been repaired. Fuzzy-match repair is one of the technologies that TAUS, the Translation Automation User Society, calls advanced leveraging;4 commercial examples of these are De"
2015.eamt-1.5,W10-1751,0,0.0550422,"Missing"
2015.eamt-1.5,2012.amta-papers.6,0,0.0136534,"., 2002), using reference translations in a development set. Most of these automatic 9 The measurements of effort that one can find in literature vary from simple scores for “perceived” post-editing effort (usually scores taking 3 or 4 values) to actual post-editing time (see, for instance, the quality estimation task in WMT 2014 (Bojar et al., 2014)) measures are measures of similarity (or dissimilarity) between raw and reference translations. Researchers hope that their use during tuning will lead to a reduction in translation effort, although this is not currently guaranteed —for instance, Denkowski and Lavie (2012) found that BLEU could not distinguish between raw and post-edited machine translation. Generally, an automatic evaluation measure for technology X may have the form i i eˆX (X(si ; ~λX ), {tij }nj=1 ;µ ~ X ), where {tij }nj=1 is the set of reference translations for segment si in the development set and µ ~ X is a set of tunable X i parameters. Ideally, eˆ (X(si ; ~λX ), {tij }nj=1 ;µ ~X) should approximate eX (X(si ; ~λX )), but tuning of µ ~ X is surprisingly absent from current MT practice (with some exceptions, see Denkowski and Lavie i (2010)). In fact, eˆX (X(si ; ~λX ), {tij }nj=1 ;µ ~"
2015.eamt-1.5,P10-1064,0,0.0407252,"Missing"
2015.eamt-1.5,2010.amta-papers.27,0,0.0336096,"Missing"
2015.eamt-1.5,2005.eamt-1.18,0,0.0141542,"alled fuzzy match score that accounts for the amount of text that is common to both segments2 — and even marks for you the words in s?i that do not match those in si . Let’s call all this information TM(si ): your job is to use it to turn t?i into the final translation ti . If the fuzzy match is good, you will spend less effort than if you started from scratch. Let us call eTM the effort to turn the t?i provided by i TM(si ) into the desired translation ti .3 Mixing them up: You could even have available another technology, fuzzy-match repair (FMR; (Ortega et al., 2014; Dandapat et al., 2011; Hewavitharana et al., 2005; Kranias and Samiotou, 2004)), that integrates the two technologies just mentioned: after a suitable fuzzy match is found, machine translation (or another source of bilingual information) is used to repair, i.e. edit some parts of t?i , to take into account what changes from s?i to si to try to save even more effort; it tells you all that TM(si ) tells you, but also marks the parts that have been repaired. Fuzzy-match repair is one of the technologies that TAUS, the Translation Automation User Society, calls advanced leveraging;4 commercial examples of these are DeepMiner in Atril’s D´ej`a Vu"
2015.eamt-1.5,J10-4005,0,0.0235175,"on a development set made of bilingual segments and translation effort measurements eMT (MT(si ; ~λMT )).9 X? where ei i is the effort expended in translating segment si using the best technology Xi? for that segment, that is, the one that minimizes that effort. To minimize the translation effort on a specific task, designers have to work in two main areas: Improving each technology: One is to improve the output of each technology X, ideally focusing on those cases when X is going to be selected. Some such technologies have tunable parameters; for instance, feature weights in statistical MT (Koehn, 2010, p. 255); for other technologies, this is not usually reported, but it is not impossible to think, for instance, of fuzzy-match scores that give different weights to different kinds of edit operations. Let us call ~λX the vector of tunable parameters for technology X; as the output of technology X varies with these parameters, we can write its output like this: X(si ; ~λX ). Learning to select the best technology: The other one is that the CAT environment needs a way to select the best technology Xi? for each segment si , obviously without measuring the actual effort. To do this, CAT designer"
2015.eamt-1.5,kranias-samiotou-2004-automatic,0,0.0269917,"accounts for the amount of text that is common to both segments2 — and even marks for you the words in s?i that do not match those in si . Let’s call all this information TM(si ): your job is to use it to turn t?i into the final translation ti . If the fuzzy match is good, you will spend less effort than if you started from scratch. Let us call eTM the effort to turn the t?i provided by i TM(si ) into the desired translation ti .3 Mixing them up: You could even have available another technology, fuzzy-match repair (FMR; (Ortega et al., 2014; Dandapat et al., 2011; Hewavitharana et al., 2005; Kranias and Samiotou, 2004)), that integrates the two technologies just mentioned: after a suitable fuzzy match is found, machine translation (or another source of bilingual information) is used to repair, i.e. edit some parts of t?i , to take into account what changes from s?i to si to try to save even more effort; it tells you all that TM(si ) tells you, but also marks the parts that have been repaired. Fuzzy-match repair is one of the technologies that TAUS, the Translation Automation User Society, calls advanced leveraging;4 commercial examples of these are DeepMiner in Atril’s D´ej`a Vu,5 and ALTM in MultiCorpora’s"
2015.eamt-1.5,P03-1021,0,0.328022,"nologies and for all segments. Therefore it is in principle not easy to determine the parameters θ~X to get good estimates e˜X (X(si ; ~λX ); θ~X ). We will see a way to do this below. Tuning technologies is also hard: Technologies may have tunable parameters ~λX which determine the output they produce. Obviously, one cannot just repetitively measure the actual effort spent by translators in editing their output for a wide variety of values of ~λX , as this is clearly impracticable; therefore, an alternative is needed. When X = MT, this is usually done by means of an algorithm that optimizes (Och, 2003; Chiang, 2012) automatic evaluation measures, such as BLEU (Papineni et al., 2002), using reference translations in a development set. Most of these automatic 9 The measurements of effort that one can find in literature vary from simple scores for “perceived” post-editing effort (usually scores taking 3 or 4 values) to actual post-editing time (see, for instance, the quality estimation task in WMT 2014 (Bojar et al., 2014)) measures are measures of similarity (or dissimilarity) between raw and reference translations. Researchers hope that their use during tuning will lead to a reduction in tr"
2015.eamt-1.5,2014.amta-researchers.4,1,0.890202,"Missing"
2015.eamt-1.5,P02-1040,0,0.103881,"o determine the parameters θ~X to get good estimates e˜X (X(si ; ~λX ); θ~X ). We will see a way to do this below. Tuning technologies is also hard: Technologies may have tunable parameters ~λX which determine the output they produce. Obviously, one cannot just repetitively measure the actual effort spent by translators in editing their output for a wide variety of values of ~λX , as this is clearly impracticable; therefore, an alternative is needed. When X = MT, this is usually done by means of an algorithm that optimizes (Och, 2003; Chiang, 2012) automatic evaluation measures, such as BLEU (Papineni et al., 2002), using reference translations in a development set. Most of these automatic 9 The measurements of effort that one can find in literature vary from simple scores for “perceived” post-editing effort (usually scores taking 3 or 4 values) to actual post-editing time (see, for instance, the quality estimation task in WMT 2014 (Bojar et al., 2014)) measures are measures of similarity (or dissimilarity) between raw and reference translations. Researchers hope that their use during tuning will lead to a reduction in translation effort, although this is not currently guaranteed —for instance, Denkowsk"
2015.eamt-1.5,2013.mtsummit-papers.21,0,0.0138473,"egment si in the development set and µ ~ X is a set of tunable X i parameters. Ideally, eˆ (X(si ; ~λX ), {tij }nj=1 ;µ ~X) should approximate eX (X(si ; ~λX )), but tuning of µ ~ X is surprisingly absent from current MT practice (with some exceptions, see Denkowski and Lavie i (2010)). In fact, eˆX (X(si ; ~λX ), {tij }nj=1 ;µ ~ X ) can be seen as a special estimator of effort, much like e˜X (X(si ; ~λX ); θ~X ), but informed with reference i translations {tij }nj=1 when they are available. This is similar to the use of pseudo-reference translations in machine translation quality estimation (Shah et al., 2013; Soricut and Narsale, 2012; Soricut et al., 2012; Soricut and Echihabi, 2010), but with actual references. Table 1 summarizes the main concepts and the notation used along the paper. 3 A general framework for training the whole CAT environment We describe a possible workflow to tune simultaneously the different technologies that may be used in a CAT environment and the estimators used to select them on a segment basis: 1. Design automatic evaluation measures eˆX (X(si , ~λX ), {tij }; µ ~ X ) and estimators X X X e˜ (X(si ; ~λ ); θ~ ) for each technology X ∈ X , based on a series of relevant"
2015.eamt-1.5,2009.mtsummit-papers.14,0,0.0271238,"ven better, couldn’t the decision of selecting the best technology Xi? , that is, the one that minimizes your effort for each segment si , be made automatically? It is therefore clear that a framework that allows to seamlessly integrate all the translation technologies available in the CAT system is very much needed to make the most of all of them and minimize translation effort as much as possible. Previous work on technology selection: The specific case of automatically choosing between machine translation output and translation memory fuzzy matches has received attention in the last years. Simard and Isabelle (2009) proposed a simple approach called β-combination, which simply selects machine translation when there is no translation memory proposal with a fuzzy match score above a given threshold β, which can be tuned. He et al. (2010a) and He et al. (2010b) approach this problem, which they call translation recommendation, by training a classifier which selects which of the two, TM(si ) or MT(si ), gets the lowest value for an approximate indicator of effort, called translation error rate (TER, (Snover et al., 2006)). Their training compares outputs to preexisting reference translations; their ideas are"
2015.eamt-1.5,2006.amta-papers.25,0,0.034682,"put and translation memory fuzzy matches has received attention in the last years. Simard and Isabelle (2009) proposed a simple approach called β-combination, which simply selects machine translation when there is no translation memory proposal with a fuzzy match score above a given threshold β, which can be tuned. He et al. (2010a) and He et al. (2010b) approach this problem, which they call translation recommendation, by training a classifier which selects which of the two, TM(si ) or MT(si ), gets the lowest value for an approximate indicator of effort, called translation error rate (TER, (Snover et al., 2006)). Their training compares outputs to preexisting reference translations; their ideas are generalized in the approach proposed in this paper. The next section explains two ways to minimize the effort needed to perform a translation job in a CAT environment integrating different technologies. Section 3 then describes our proposal for a general framework for training the whole CAT environment. Finally, we discuss the implications of having such a framework. 2 to come up with a set of estimators e˜X , one for each technology. These estimators should be trained to give the best possible estimate o"
2015.eamt-1.5,P10-1063,0,0.0244939,"arameters. Ideally, eˆ (X(si ; ~λX ), {tij }nj=1 ;µ ~X) should approximate eX (X(si ; ~λX )), but tuning of µ ~ X is surprisingly absent from current MT practice (with some exceptions, see Denkowski and Lavie i (2010)). In fact, eˆX (X(si ; ~λX ), {tij }nj=1 ;µ ~ X ) can be seen as a special estimator of effort, much like e˜X (X(si ; ~λX ); θ~X ), but informed with reference i translations {tij }nj=1 when they are available. This is similar to the use of pseudo-reference translations in machine translation quality estimation (Shah et al., 2013; Soricut and Narsale, 2012; Soricut et al., 2012; Soricut and Echihabi, 2010), but with actual references. Table 1 summarizes the main concepts and the notation used along the paper. 3 A general framework for training the whole CAT environment We describe a possible workflow to tune simultaneously the different technologies that may be used in a CAT environment and the estimators used to select them on a segment basis: 1. Design automatic evaluation measures eˆX (X(si , ~λX ), {tij }; µ ~ X ) and estimators X X X e˜ (X(si ; ~λ ); θ~ ) for each technology X ∈ X , based on a series of relevant features that can easily be extracted from si and X(si ), and which will depen"
2015.eamt-1.5,W12-3121,0,0.0187232,"velopment set and µ ~ X is a set of tunable X i parameters. Ideally, eˆ (X(si ; ~λX ), {tij }nj=1 ;µ ~X) should approximate eX (X(si ; ~λX )), but tuning of µ ~ X is surprisingly absent from current MT practice (with some exceptions, see Denkowski and Lavie i (2010)). In fact, eˆX (X(si ; ~λX ), {tij }nj=1 ;µ ~ X ) can be seen as a special estimator of effort, much like e˜X (X(si ; ~λX ); θ~X ), but informed with reference i translations {tij }nj=1 when they are available. This is similar to the use of pseudo-reference translations in machine translation quality estimation (Shah et al., 2013; Soricut and Narsale, 2012; Soricut et al., 2012; Soricut and Echihabi, 2010), but with actual references. Table 1 summarizes the main concepts and the notation used along the paper. 3 A general framework for training the whole CAT environment We describe a possible workflow to tune simultaneously the different technologies that may be used in a CAT environment and the estimators used to select them on a segment basis: 1. Design automatic evaluation measures eˆX (X(si , ~λX ), {tij }; µ ~ X ) and estimators X X X e˜ (X(si ; ~λ ); θ~ ) for each technology X ∈ X , based on a series of relevant features that can easily be"
2015.eamt-1.5,W12-3118,0,0.0179831,"a set of tunable X i parameters. Ideally, eˆ (X(si ; ~λX ), {tij }nj=1 ;µ ~X) should approximate eX (X(si ; ~λX )), but tuning of µ ~ X is surprisingly absent from current MT practice (with some exceptions, see Denkowski and Lavie i (2010)). In fact, eˆX (X(si ; ~λX ), {tij }nj=1 ;µ ~ X ) can be seen as a special estimator of effort, much like e˜X (X(si ; ~λX ); θ~X ), but informed with reference i translations {tij }nj=1 when they are available. This is similar to the use of pseudo-reference translations in machine translation quality estimation (Shah et al., 2013; Soricut and Narsale, 2012; Soricut et al., 2012; Soricut and Echihabi, 2010), but with actual references. Table 1 summarizes the main concepts and the notation used along the paper. 3 A general framework for training the whole CAT environment We describe a possible workflow to tune simultaneously the different technologies that may be used in a CAT environment and the estimators used to select them on a segment basis: 1. Design automatic evaluation measures eˆX (X(si , ~λX ), {tij }; µ ~ X ) and estimators X X X e˜ (X(si ; ~λ ); θ~ ) for each technology X ∈ X , based on a series of relevant features that can easily be extracted from si and"
2015.eamt-1.5,2013.tc-1.10,0,0.103188,"segment, MT(si ); vendors and experts tell you that you will save effort by postediting MT(si ) into your desired translation ti .1 Machine translation output MT(si ) may just be text, but it could come with annotations to help you make the most of it; for instance, words could be color-coded according to how confident the system is about them (Ueffing and Ney, 2007; Ueffing and Ney, 2005; Blatz et al., 2004), or unknown words that come out untranslated may be marked so that you spot them clearly. Machine-translated segments could even be accompanied by indicators of their estimated quality (Specia and Soricut, 2013; Specia et al., 2010; Blatz et al., 2004) which may be used to ascertain whether the output of the MT system is worth being post-edited or not. If someone measured your post-editing effort (in time, in number of keystrokes, in number of words changed, in money you would have to pay another translator to do it, etc.), when turning MT(si ) into ti , they could call that effort eMT i . Translation memory: You could also use a translation memory (TM; (Somers, 2003)), where previously translated segments s are stored together with their translations t in pairs called translation units (s, t). The"
2015.eamt-1.5,H05-1096,0,0.0348615,"rs. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. 27 Machine translation: You could, for instance, use machine translation (MT) to get a draft of the translation of each segment, MT(si ); vendors and experts tell you that you will save effort by postediting MT(si ) into your desired translation ti .1 Machine translation output MT(si ) may just be text, but it could come with annotations to help you make the most of it; for instance, words could be color-coded according to how confident the system is about them (Ueffing and Ney, 2007; Ueffing and Ney, 2005; Blatz et al., 2004), or unknown words that come out untranslated may be marked so that you spot them clearly. Machine-translated segments could even be accompanied by indicators of their estimated quality (Specia and Soricut, 2013; Specia et al., 2010; Blatz et al., 2004) which may be used to ascertain whether the output of the MT system is worth being post-edited or not. If someone measured your post-editing effort (in time, in number of keystrokes, in number of words changed, in money you would have to pay another translator to do it, etc.), when turning MT(si ) into ti , they could call t"
2015.eamt-1.5,J07-1003,0,0.0180096,"; they c 2015 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. 27 Machine translation: You could, for instance, use machine translation (MT) to get a draft of the translation of each segment, MT(si ); vendors and experts tell you that you will save effort by postediting MT(si ) into your desired translation ti .1 Machine translation output MT(si ) may just be text, but it could come with annotations to help you make the most of it; for instance, words could be color-coded according to how confident the system is about them (Ueffing and Ney, 2007; Ueffing and Ney, 2005; Blatz et al., 2004), or unknown words that come out untranslated may be marked so that you spot them clearly. Machine-translated segments could even be accompanied by indicators of their estimated quality (Specia and Soricut, 2013; Specia et al., 2010; Blatz et al., 2004) which may be used to ascertain whether the output of the MT system is worth being post-edited or not. If someone measured your post-editing effort (in time, in number of keystrokes, in number of words changed, in money you would have to pay another translator to do it, etc.), when turning MT(si ) into"
2016.amta-researchers.3,2011.eamt-1.28,1,0.956104,"to determine the potential of the method. The paper ends with some remarks and a description of future research lines. 2 Related Work In the literature, one can ﬁnd many papers addressing the combination of machine translation and translation memories, most of which explore different ways of integrating sub-segments from the translation memory into the decoding process of a phrase-based statistical machine translation system (Bic¸ici and Dymetman, 2008; Simard and Isabelle, 2009; Zhechev and Genabith, 2010; Koehn and Senellart, 2010; Li et al., 2016). Alternative approaches, such as those by Dandapat et al. (2011), Hewavitharana et al. (2005) and Kranias and Samiotou (2004), use instead the target segment t in a translation unit (s, t) as the backbone or the basis of the translation to be produced and describe ways to repair it by modifying those sub-segments in t that are the translation of the mismatched sub-segments in s. The method proposed here, which extends that of Ortega et al. (2014), belongs to this second group. Dandapat et al. (2011)’s method ﬁrst aligns, in a way similar to ours, the words in s and s using the (word-based) edit distance (Levenshtein, 1966) and marks the mismatched subsegm"
2016.amta-researchers.3,2011.mtsummit-papers.18,1,0.846391,"Missing"
2016.amta-researchers.3,2005.eamt-1.18,0,0.757637,"ial of the method. The paper ends with some remarks and a description of future research lines. 2 Related Work In the literature, one can ﬁnd many papers addressing the combination of machine translation and translation memories, most of which explore different ways of integrating sub-segments from the translation memory into the decoding process of a phrase-based statistical machine translation system (Bic¸ici and Dymetman, 2008; Simard and Isabelle, 2009; Zhechev and Genabith, 2010; Koehn and Senellart, 2010; Li et al., 2016). Alternative approaches, such as those by Dandapat et al. (2011), Hewavitharana et al. (2005) and Kranias and Samiotou (2004), use instead the target segment t in a translation unit (s, t) as the backbone or the basis of the translation to be produced and describe ways to repair it by modifying those sub-segments in t that are the translation of the mismatched sub-segments in s. The method proposed here, which extends that of Ortega et al. (2014), belongs to this second group. Dandapat et al. (2011)’s method ﬁrst aligns, in a way similar to ours, the words in s and s using the (word-based) edit distance (Levenshtein, 1966) and marks the mismatched subsegments in s and s for translat"
2016.amta-researchers.3,J10-4005,0,0.249192,"ed) edit distance (Levenshtein, 1966) and marks the mismatched subsegments in s and s for translation. It then aligns the mismatch sub-segments in s with their 3 The translation memory of the Directorate General for Translation of the European Comission, https://ec. europa.eu/jrc/en/language-technologies/dgt-translation-memory 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S counterparts in t by using a sub-segmental translation memory built on the user’s translation memory following the standard method to obtain phrase tables in statistical MT (Koehn, 2010). Finally the sub-segments in t aligned to mismatched sub-segments in s are replaced by the translations of the corresponding sub-segments in s as they are found in the sub-segmental translation memory. The main differences with the approach described here are (a) that Dandapat et al. (2011) do not take into account the context words around the mismatches —which may lead to incorrect translations due to boundary friction problems such as incorrect agreement or incomplete word reorderings— and (b) that they rely on the user’s translation memory (which may be small) rather than on an external S"
2016.amta-researchers.3,2010.jec-1.4,0,0.66572,"ss the experimental settings and the results of an oracle evaluation we have conducted to determine the potential of the method. The paper ends with some remarks and a description of future research lines. 2 Related Work In the literature, one can ﬁnd many papers addressing the combination of machine translation and translation memories, most of which explore different ways of integrating sub-segments from the translation memory into the decoding process of a phrase-based statistical machine translation system (Bic¸ici and Dymetman, 2008; Simard and Isabelle, 2009; Zhechev and Genabith, 2010; Koehn and Senellart, 2010; Li et al., 2016). Alternative approaches, such as those by Dandapat et al. (2011), Hewavitharana et al. (2005) and Kranias and Samiotou (2004), use instead the target segment t in a translation unit (s, t) as the backbone or the basis of the translation to be produced and describe ways to repair it by modifying those sub-segments in t that are the translation of the mismatched sub-segments in s. The method proposed here, which extends that of Ortega et al. (2014), belongs to this second group. Dandapat et al. (2011)’s method ﬁrst aligns, in a way similar to ours, the words in s and s using"
2016.amta-researchers.3,kranias-samiotou-2004-automatic,0,0.791791,"s with some remarks and a description of future research lines. 2 Related Work In the literature, one can ﬁnd many papers addressing the combination of machine translation and translation memories, most of which explore different ways of integrating sub-segments from the translation memory into the decoding process of a phrase-based statistical machine translation system (Bic¸ici and Dymetman, 2008; Simard and Isabelle, 2009; Zhechev and Genabith, 2010; Koehn and Senellart, 2010; Li et al., 2016). Alternative approaches, such as those by Dandapat et al. (2011), Hewavitharana et al. (2005) and Kranias and Samiotou (2004), use instead the target segment t in a translation unit (s, t) as the backbone or the basis of the translation to be produced and describe ways to repair it by modifying those sub-segments in t that are the translation of the mismatched sub-segments in s. The method proposed here, which extends that of Ortega et al. (2014), belongs to this second group. Dandapat et al. (2011)’s method ﬁrst aligns, in a way similar to ours, the words in s and s using the (word-based) edit distance (Levenshtein, 1966) and marks the mismatched subsegments in s and s for translation. It then aligns the mismatch"
2016.amta-researchers.3,2009.mtsummit-papers.14,0,0.721024,"om working on the same mismatch. Sections 5 and 6 discuss the experimental settings and the results of an oracle evaluation we have conducted to determine the potential of the method. The paper ends with some remarks and a description of future research lines. 2 Related Work In the literature, one can ﬁnd many papers addressing the combination of machine translation and translation memories, most of which explore different ways of integrating sub-segments from the translation memory into the decoding process of a phrase-based statistical machine translation system (Bic¸ici and Dymetman, 2008; Simard and Isabelle, 2009; Zhechev and Genabith, 2010; Koehn and Senellart, 2010; Li et al., 2016). Alternative approaches, such as those by Dandapat et al. (2011), Hewavitharana et al. (2005) and Kranias and Samiotou (2004), use instead the target segment t in a translation unit (s, t) as the backbone or the basis of the translation to be produced and describe ways to repair it by modifying those sub-segments in t that are the translation of the mismatched sub-segments in s. The method proposed here, which extends that of Ortega et al. (2014), belongs to this second group. Dandapat et al. (2011)’s method ﬁrst aligns,"
2016.amta-researchers.3,2013.tc-1.10,0,0.0586535,"Missing"
2016.amta-researchers.3,W10-3806,0,0.0208675,"atch. Sections 5 and 6 discuss the experimental settings and the results of an oracle evaluation we have conducted to determine the potential of the method. The paper ends with some remarks and a description of future research lines. 2 Related Work In the literature, one can ﬁnd many papers addressing the combination of machine translation and translation memories, most of which explore different ways of integrating sub-segments from the translation memory into the decoding process of a phrase-based statistical machine translation system (Bic¸ici and Dymetman, 2008; Simard and Isabelle, 2009; Zhechev and Genabith, 2010; Koehn and Senellart, 2010; Li et al., 2016). Alternative approaches, such as those by Dandapat et al. (2011), Hewavitharana et al. (2005) and Kranias and Samiotou (2004), use instead the target segment t in a translation unit (s, t) as the backbone or the basis of the translation to be produced and describe ways to repair it by modifying those sub-segments in t that are the translation of the mismatched sub-segments in s. The method proposed here, which extends that of Ortega et al. (2014), belongs to this second group. Dandapat et al. (2011)’s method ﬁrst aligns, in a way similar to ours, t"
2016.amta-researchers.6,P06-1067,0,0.110386,"Missing"
2016.amta-researchers.6,2014.amta-workshop.1,0,0.0663824,"Missing"
2016.amta-researchers.6,J09-1002,0,0.074298,"Missing"
2016.amta-researchers.6,C04-1046,0,0.0963888,"Missing"
2016.amta-researchers.6,W16-3415,0,0.0207163,"Missing"
2016.amta-researchers.6,1997.mtsummit-papers.1,0,0.0229932,"Missing"
2016.amta-researchers.6,P05-3026,0,0.0446338,"Missing"
2016.amta-researchers.6,W04-3250,0,0.0249073,"troducing their own words when an appropriate equivalent is not found in the suggestion. Our approach, described in a previous work (P´erez-Ortiz et al., 2014), follows a different paradigm known as interactive translation prediction (ITP), which, instead of presenting a translation proposal that gets reshaped by the target-language sentence formed in the translator’s mind, focuses on offering translation suggestions as the translation is carried out. Most state-ofthe-art ITP approaches obtain the suggestions by means of a modiﬁed (or tailor-made) statistical machine translation system (SMT) (Koehn, 2004) that is able to provide additional information (such as word alignments, alternative translations, and scores or probabilities for the translation). These systems are able to leverage more information from the bilingual resource than if it were used unmodiﬁed as a black-box, but doing so they inherit the common requirements of SMT, 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S namely, the dependency on the availability of extensive parallel corpora. It is worth noting that integrating other resources of bilingual information would be almost im"
2016.amta-researchers.6,2005.mtsummit-papers.11,0,0.0152387,") = 0 otherwise p∈overlap(T ) We deﬁne a second feature that discredits suggestions originating from positions that have evidence (in the form of suggestions that overlap with T ) of having already been covered in the translation:  Wpast (j, T ) if p ∈ P S accepted f30 = (2) 0 otherwise j∈span(p) An example of how this both features work is discussed in Figure 2. 4 Experimental setup and results As explained in Section 3.2, four different conﬁgurations are trained. Training, development and test data are extracted from a corpus with 15,000 English–Spanish sentences extracted from Europarl (Koehn, 2005) version 7, a collection of proceedings from the European Parliament; 11,000 sentences were used as training set, 1,000 as development or validation set and the remaining 3,000 as test set. As a bilingual MT engine capable of providing the translation of the sub-segments we used the free/open-source statistical MT system Moses (Koehn et al., 2007) trained over 155,760 independent sentences from the same corpus, following the standard procedure for training a baseline system.11 The evaluation was conducted for the translation of texts from English to Spanish. 11 The corpora is available at http"
2016.amta-researchers.6,P09-4005,0,0.052029,"Missing"
2016.amta-researchers.6,P07-2045,0,0.00311766,"is discussed in Figure 2. 4 Experimental setup and results As explained in Section 3.2, four different conﬁgurations are trained. Training, development and test data are extracted from a corpus with 15,000 English–Spanish sentences extracted from Europarl (Koehn, 2005) version 7, a collection of proceedings from the European Parliament; 11,000 sentences were used as training set, 1,000 as development or validation set and the remaining 3,000 as test set. As a bilingual MT engine capable of providing the translation of the sub-segments we used the free/open-source statistical MT system Moses (Koehn et al., 2007) trained over 155,760 independent sentences from the same corpus, following the standard procedure for training a baseline system.11 The evaluation was conducted for the translation of texts from English to Spanish. 11 The corpora is available at http://www.statmt.org/europarl. The sentences match those we used in a previous paper (Torregrosa et al., 2014). 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S To generate the training set, using a source sentence and a reference, an automatic system iteratively considers the ﬁrst letter of each word an"
2016.amta-researchers.6,langlais-etal-2000-evaluation,0,0.112552,"Missing"
2016.amta-researchers.6,macklovitch-2006-transtype2,0,0.098006,"Missing"
2016.amta-researchers.6,E14-2012,0,0.0327751,"Missing"
2016.amta-researchers.6,W14-0309,1,0.882872,"Missing"
2016.amta-researchers.6,2011.eamt-1.15,0,0.0264675,"Missing"
2020.acl-main.417,bojar-etal-2012-joy,0,0.0615774,"Missing"
2020.acl-main.417,P91-1022,0,0.752852,"ctors may be recentered around the mean vector for a web domain (Germann, 2016) Document alignment quality can be improved with additional features such ratio of shared links, similarity of link URLs, ratio of shared images, binary feature indicating if the documents are linked, DOM structure similarity (Espl`a-Gomis et al., 2016), same numbers (Papavassiliou et al., 2016), or same named entities (Lohar et al., 2016). Guo et al. (2019) introduce the use of document embeddings, constructed from sentence embeddings, to the document alignment task. 2.3 Sentence Alignment Early sentence aligners (Brown et al., 1991; Gale and Church, 1993) use scoring functions based only on the number of words or characters in each sentence and alignment algorithms based on dynamic programming. Europarl, for example, used metadata to align paragraphs, typically consisting of 2-5 sentences, and using Gale and Church (1993)’s method to align sentences within corresponding paragraphs. Later work added lexical features and heuristics to speed up search, such as limiting the search space to be near the diagonal (Moore, 2002; Varga et al., 2005). More recent work introduced scoring methods that use MT to get both documents in"
2020.acl-main.417,buck-etal-2014-n,1,0.887657,"Missing"
2020.acl-main.417,W16-2347,1,0.931151,"set of patterns for language marking or simple Levenshtein distance (Le et al., 2016). Content matching requires crossing the language barrier at some point, typically by using bilingual dictionaries or translating one of the documents into the other document’s language (Uszkoreit et al., 2010). Documents may be represented by vectors over word frequencies, typically td-idf-weighted. Vectors may also be constructed over bigrams (Dara and Lin, 2016) or even higher order n-grams 4 http://opus.lingfil.uu.se/ (Uszkoreit et al., 2010). The vectors are then typically matched with cosine similarity (Buck and Koehn, 2016a). The raw vectors may be recentered around the mean vector for a web domain (Germann, 2016) Document alignment quality can be improved with additional features such ratio of shared links, similarity of link URLs, ratio of shared images, binary feature indicating if the documents are linked, DOM structure similarity (Espl`a-Gomis et al., 2016), same numbers (Papavassiliou et al., 2016), or same named entities (Lohar et al., 2016). Guo et al. (2019) introduce the use of document embeddings, constructed from sentence embeddings, to the document alignment task. 2.3 Sentence Alignment Early sente"
2020.acl-main.417,W11-1218,0,0.0324271,"Missing"
2020.acl-main.417,W16-2365,1,0.928849,"set of patterns for language marking or simple Levenshtein distance (Le et al., 2016). Content matching requires crossing the language barrier at some point, typically by using bilingual dictionaries or translating one of the documents into the other document’s language (Uszkoreit et al., 2010). Documents may be represented by vectors over word frequencies, typically td-idf-weighted. Vectors may also be constructed over bigrams (Dara and Lin, 2016) or even higher order n-grams 4 http://opus.lingfil.uu.se/ (Uszkoreit et al., 2010). The vectors are then typically matched with cosine similarity (Buck and Koehn, 2016a). The raw vectors may be recentered around the mean vector for a web domain (Germann, 2016) Document alignment quality can be improved with additional features such ratio of shared links, similarity of link URLs, ratio of shared images, binary feature indicating if the documents are linked, DOM structure similarity (Espl`a-Gomis et al., 2016), same numbers (Papavassiliou et al., 2016), or same named entities (Lohar et al., 2016). Guo et al. (2019) introduce the use of document embeddings, constructed from sentence embeddings, to the document alignment task. 2.3 Sentence Alignment Early sente"
2020.acl-main.417,2012.eamt-1.60,0,0.0178318,"2.1 Acquisition Efforts Most publicly available parallel corpora are the result of targeted efforts to extract the translations from a specific source. The French–English Canadian Hansards3 were used in the earliest work on statistical machine translation. A similar popular corpus is Europarl (Koehn, 2005), used throughout the WMT evaluation campaign. Multi-lingual web sites are attractive targets. Rafalovitch and Dale (2009); Ziemski et al. (2015) extract data from the United Nations, T¨ager (2011) from European Patents, Lison and Tiedemann (2016) from a collection of TV and movie subtitles. Cettolo et al. (2012) explain the creation of a multilingual parallel corpus of subtitles from the TED Talks website which is popular due to its use in the IWSLT evaluation campaign. 3 https://www.isi.edu/natural-language/ download/hansard/ 4555 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4555–4567 c July 5 - 10, 2020. 2020 Association for Computational Linguistics There are also various efforts targeted at a single language pair. Martin et al. (2003) build a parallel corpus for Inuktitut–English. Utiyama and Isahara (2003); Fukushima et al. (2006) worked on creat"
2020.acl-main.417,P05-1074,0,0.166175,"s Translation. We report on methods to create the largest publicly available parallel corpora by crawling the web, using open source software. We empirically compare alternative methods and publish benchmark data sets for sentence alignment and sentence pair filtering. We also describe the parallel corpora released and evaluate their quality and their usefulness to create machine translation systems. 1 2 Introduction Parallel corpora are essential for building highquality machine translation systems and have found uses in many other natural language applications, such as learning paraphrases (Bannard and Callison-Burch, 2005; Hu et al., 2019) or cross-lingual projection of language tools (Yarowsky et al., 2001). We report on work to create the largest publicly available parallel corpora by crawling hundreds of thousands of web sites, using open source tools. The processing pipeline consists of the steps: crawling, text extraction, document alignment, sentence alignment, and sentence pair filtering. We describe these steps in detail in Sections 4–8. For some of these steps we evaluate several methods empirically in terms of their impact on machine translation quality. We provide the data resources used in these ev"
2020.acl-main.417,W19-5435,1,0.894745,"Missing"
2020.acl-main.417,2005.mtsummit-papers.11,1,0.214853,"modular pipeline that allows harvesting parallel corpora from multilingual websites or from preexisting or historical web crawls such as the one available as part of the Internet Archive.2 1 2 https://github.com/bitextor/bitextor https://archive.org/ Related Work While the idea of mining the web for parallel data has been already pursued in the 20th century (Resnik, 1999), the most serious efforts have been limited to large companies such as Google (Uszkoreit et al., 2010) and Microsoft (Rarrick et al., 2011), or targeted efforts on specific domains such as the Canadian Hansards and Europarl (Koehn, 2005). The book Bitext Alignment (Tiedemann, 2011) describes some of the challenges in greater detail. 2.1 Acquisition Efforts Most publicly available parallel corpora are the result of targeted efforts to extract the translations from a specific source. The French–English Canadian Hansards3 were used in the earliest work on statistical machine translation. A similar popular corpus is Europarl (Koehn, 2005), used throughout the WMT evaluation campaign. Multi-lingual web sites are attractive targets. Rafalovitch and Dale (2009); Ziemski et al. (2015) extract data from the United Nations, T¨ager (201"
2020.acl-main.417,W19-5404,1,0.891381,"Missing"
2020.acl-main.417,W18-6453,1,0.89611,"Missing"
2020.acl-main.417,W19-5438,0,0.038302,"Missing"
2020.acl-main.417,W16-2371,0,0.0501587,"Missing"
2020.acl-main.417,I08-2120,0,0.0456513,"age/ download/hansard/ 4555 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4555–4567 c July 5 - 10, 2020. 2020 Association for Computational Linguistics There are also various efforts targeted at a single language pair. Martin et al. (2003) build a parallel corpus for Inuktitut–English. Utiyama and Isahara (2003); Fukushima et al. (2006) worked on creating Japanese–English corpora. Uchiyama and Isahara (2007) report on the efforts to build a Japanese–English patent corpus and Macken et al. (2007) on efforts on a broad-based Dutch–English corpus. Li and Liu (2008) mine the web for a Chinese–English corpus. A large Czech–English corpus from various sources was collected (Bojar et al., 2010), linguistically annotated (Bojar et al., 2012), and has been continuously extended to over 300 million words (Bojar et al., 2016). All these efforts rely on methods and implementations that are quite specific for each use case, not documented in great detail, and not publicly available. A discussion of the pitfalls during the construction of parallel corpora is given by Kaalep and Veskis (2007). A large collection of corpora is maintained at the OPUS web site4 (Tiede"
2020.acl-main.417,L16-1147,0,0.0317824,"t (Tiedemann, 2011) describes some of the challenges in greater detail. 2.1 Acquisition Efforts Most publicly available parallel corpora are the result of targeted efforts to extract the translations from a specific source. The French–English Canadian Hansards3 were used in the earliest work on statistical machine translation. A similar popular corpus is Europarl (Koehn, 2005), used throughout the WMT evaluation campaign. Multi-lingual web sites are attractive targets. Rafalovitch and Dale (2009); Ziemski et al. (2015) extract data from the United Nations, T¨ager (2011) from European Patents, Lison and Tiedemann (2016) from a collection of TV and movie subtitles. Cettolo et al. (2012) explain the creation of a multilingual parallel corpus of subtitles from the TED Talks website which is popular due to its use in the IWSLT evaluation campaign. 3 https://www.isi.edu/natural-language/ download/hansard/ 4555 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4555–4567 c July 5 - 10, 2020. 2020 Association for Computational Linguistics There are also various efforts targeted at a single language pair. Martin et al. (2003) build a parallel corpus for Inuktitut–English."
2020.acl-main.417,W16-2372,0,0.0140892,"ra and Lin, 2016) or even higher order n-grams 4 http://opus.lingfil.uu.se/ (Uszkoreit et al., 2010). The vectors are then typically matched with cosine similarity (Buck and Koehn, 2016a). The raw vectors may be recentered around the mean vector for a web domain (Germann, 2016) Document alignment quality can be improved with additional features such ratio of shared links, similarity of link URLs, ratio of shared images, binary feature indicating if the documents are linked, DOM structure similarity (Espl`a-Gomis et al., 2016), same numbers (Papavassiliou et al., 2016), or same named entities (Lohar et al., 2016). Guo et al. (2019) introduce the use of document embeddings, constructed from sentence embeddings, to the document alignment task. 2.3 Sentence Alignment Early sentence aligners (Brown et al., 1991; Gale and Church, 1993) use scoring functions based only on the number of words or characters in each sentence and alignment algorithms based on dynamic programming. Europarl, for example, used metadata to align paragraphs, typically consisting of 2-5 sentences, and using Gale and Church (1993)’s method to align sentences within corresponding paragraphs. Later work added lexical features and heuris"
2020.acl-main.417,2007.mtsummit-papers.42,0,0.0236436,"e in the IWSLT evaluation campaign. 3 https://www.isi.edu/natural-language/ download/hansard/ 4555 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4555–4567 c July 5 - 10, 2020. 2020 Association for Computational Linguistics There are also various efforts targeted at a single language pair. Martin et al. (2003) build a parallel corpus for Inuktitut–English. Utiyama and Isahara (2003); Fukushima et al. (2006) worked on creating Japanese–English corpora. Uchiyama and Isahara (2007) report on the efforts to build a Japanese–English patent corpus and Macken et al. (2007) on efforts on a broad-based Dutch–English corpus. Li and Liu (2008) mine the web for a Chinese–English corpus. A large Czech–English corpus from various sources was collected (Bojar et al., 2010), linguistically annotated (Bojar et al., 2012), and has been continuously extended to over 300 million words (Bojar et al., 2016). All these efforts rely on methods and implementations that are quite specific for each use case, not documented in great detail, and not publicly available. A discussion of the pitfalls during the construction of parallel corpora is given by Kaalep and Veskis (2007). A la"
2020.acl-main.417,W03-0320,0,0.154143,"ons, T¨ager (2011) from European Patents, Lison and Tiedemann (2016) from a collection of TV and movie subtitles. Cettolo et al. (2012) explain the creation of a multilingual parallel corpus of subtitles from the TED Talks website which is popular due to its use in the IWSLT evaluation campaign. 3 https://www.isi.edu/natural-language/ download/hansard/ 4555 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4555–4567 c July 5 - 10, 2020. 2020 Association for Computational Linguistics There are also various efforts targeted at a single language pair. Martin et al. (2003) build a parallel corpus for Inuktitut–English. Utiyama and Isahara (2003); Fukushima et al. (2006) worked on creating Japanese–English corpora. Uchiyama and Isahara (2007) report on the efforts to build a Japanese–English patent corpus and Macken et al. (2007) on efforts on a broad-based Dutch–English corpus. Li and Liu (2008) mine the web for a Chinese–English corpus. A large Czech–English corpus from various sources was collected (Bojar et al., 2010), linguistically annotated (Bojar et al., 2012), and has been continuously extended to over 300 million words (Bojar et al., 2016). All these e"
2020.acl-main.417,moore-2002-fast,0,0.25649,"nce embeddings, to the document alignment task. 2.3 Sentence Alignment Early sentence aligners (Brown et al., 1991; Gale and Church, 1993) use scoring functions based only on the number of words or characters in each sentence and alignment algorithms based on dynamic programming. Europarl, for example, used metadata to align paragraphs, typically consisting of 2-5 sentences, and using Gale and Church (1993)’s method to align sentences within corresponding paragraphs. Later work added lexical features and heuristics to speed up search, such as limiting the search space to be near the diagonal (Moore, 2002; Varga et al., 2005). More recent work introduced scoring methods that use MT to get both documents into the same language (Sennrich and Volk, 2010) or use pruned phrase tables from a statistical MT system (Gomes and Lopes, 2016). Both methods “anchor” highprobability 1–1 alignments in the search space and then fill in and refine alignments. They later propose an extension (Sennrich and Volk, 2011) in which an SMT system is bootstrapped from an initial alignment and then used in Bleualign. Vecalign (Thompson and Koehn, 2019) is a sentence alignment method that relies on bilingual sentence emb"
2020.acl-main.417,J05-4003,0,0.157471,"Our work exploits web sites that provide roughly the same content in multiple languages, leading us to the assumption to find pairs of web pages which are translations of each other, with translated sentences following the same order. This assumption does not hold in less consistently translated web content such as Wikipedia, or accidental parallel sentence found in news stories about the same subject matter written in multiple languages. There have been increasing efforts to mine sentence pairs from large pools of multi-lingual text, which are treated as unstructured bags of sen4557 tences. Munteanu and Marcu (2005) use document retrieval and a maximum entropy classifier to identify parallel sentence pairs in a multi-lingual collection of news stories. Bilingual sentence embeddings (Guo et al., 2018) and multilingual sentence embeddings (Artetxe and Schwenk, 2018) were tested on their ability to reconstruct parallel corpora. This lead to work to construct WikiMatrix, a large corpus of parallel sentences from Wikipedia (Schwenk et al., 2019) based on cosine distance of their crosslingual sentence embeddings. 3 Identifying Multi-Lingual Web Sites Since the start of the collection effort in 2015, we identif"
2020.eamt-1.32,D18-1399,0,0.0443274,"Missing"
2020.eamt-1.32,W19-6721,1,0.873671,"Missing"
2020.eamt-1.32,W18-6320,1,0.800242,"el iter. backt. 1 iter. backt. 2 iter. backt. 3 iter. backt. 4 + NewsCrawl 4 + NewsCrawl + tags 4 Google Translate sw→en only parallel iter. backt. 1 iter. backt. 2 iter. backt. 3 iter. backt. 4 + tags 4 Google Translate - BLEU chrF++ 22.23 25.59 26.22 26.36 26.58 26.77 27.42 23.24 46.34 50.08 50.91 51.09 51.39 51.46 52.11 48.80 22.66 29.29 29.70 29.99 30.19 30.55 30.36 44.62 51.19 51.82 51.98 52.10 52.72 53.32 en→sw. This corresponds to the content creation use case which will use translation predominantly in this direction, and where the correctness of the translation is key. • Gap filling (Forcada et al., 2018) (GF) is used to test sw→en. This corresponds to the media monitoring use case which will use translation almost exclusively in this direction and where getting the gist of the meaning of a sentence is enough to fulfil the use-case, perfect translation of sentence structure is less important. Custom interfaces were created to support both evaluations; see figures 2 and 3 for DA and GF, respectively. Table 5: Automatic evaluation results obtained for the difEvaluators were recruited from within the media ferent development steps of the MT systems: only parallel partner organisations to complete"
2020.eamt-1.32,W08-0509,0,0.0194309,"arallel data are identified. Bitextor supports two strategies for document alignment: one based on bilingual lexicons and another based on MT. The last option was not feasible in this work as no high-quality MT system between sw and en was available; therefore, the first one was used. This method combines information from bilingual lexicons, the HTML structure of the documents, and the URL to obtain a confidence score for every pair of documents to be aligned (Espl`a-Gomis and Forcada, 2010). The bilingual lexicon used was automatically obtained from the word alignments obtained with mgiza++ (Gao and Vogel, 2008) for the following corpora: EUBookshop v2, Ubuntu and Tanzil (see Table 1). A total of 180 520 pairs of documents were obtained by using this method. • A collection of pairs of segments that are wrongly aligned to train a language model: following Bicleaner’s documentation, this collection was obtained from the raw parallel corpus by applying the “hard rules” implemented in Bicleaner. Sentence alignment. In this step, aligned documents are segmented and aligned at the sentence level. Two sentence-alignment tools are supported by Bitextor: Hunalign (Varga et al., 2007) and BLEUalign (Sennrich a"
2020.eamt-1.32,C16-1294,0,0.0246662,"Missing"
2020.eamt-1.32,W18-2703,0,0.020172,"24 821 40 000 000 en tokens 7 536 537 796 199 072 sw tokens 6 191 959 - existing data for the en→sw direction and the MT system was re-trained. 414 598 - 8 377 157 5.4 5 687 000 - 174 867 482 2 000 1 863 1 969 41 726 41 097 43 149 42 037 41 188 43 174 Table 4: Size of the corpora used to build the NMT systems after preprocesing. For the en NewsCrawl corpus, only the size of the subset that has been used for training is displayed. Token counts were calculated before BPE splitting. correlated with the quality of the system that translates the TL monolingual corpus into the source language (SL) (Hoang et al., 2018, Sec. 3). We took advantage of the fact that we are building systems for both the en→sw and sw→en directions and applied an iterative back-translation (Hoang et al., 2018) algorithm that simultaneously leverages monolingual sw and monolingual en data. It can be outlined as follows: 1. With the best identified hyper-parameters for each direction we built a system using only parallel data. 2. en and sw monolingual data were backtranslated with the systems built in the previous step. 3. Systems in both directions were trained on the combination of the back-translated data and the parallel data."
2020.eamt-1.32,P18-4020,0,0.0147447,"balVoicesv2015 and GlobalVoices-v2017q3, together with the other parallel corpora listed in Table 1 were deduplicated to obtain the final parallel corpus used to train the NMT systems. All corpora were tokenised with the Moses tokeniser (Koehn et al., 2007) and truecased. Parallel sentences with more than 100 tokens in either side were removed. Words were split in sub-word units with byte pair encoding (BPE; Sennrich et al. (2016c)). Table 4 reports the size of the corpora after this pre-processing. 5.2 Neural machine translation architecture We trained the NMT models with the Marian toolkit (Junczys-Dowmunt et al., 2018). Since training hyper-parameters can have a large impact in the quality of the resulting system (Lim et al., 2018), we carried out a grid search in order to find the best hyper-parameters for each translation direction. We explored both the Transformer (Vaswani et al., 2017) and recurrent neural network (RNN) with attention (Bahdanau et al., 2014) architectures. Our starting points were the Transformer hyperparameters15 described by Sennrich et al. (2017) and the RNN hyper-parameters16 described by Sennrich et al. (2016a). For each translation direction and architecture, we explored the follo"
2020.eamt-1.32,P07-2045,0,0.00737981,"e concatenation of GlobalVoices-v2015 and GlobalVoices-v2017q3, and split them into two halves (with 2 000 sentences each), which were used respectively as development and test corpora. The half reserved to be used as test corpus was further filtered to remove the sentences that could be found in any of the monolingual corpora. The remaining sentences from GlobalVoicesv2015 and GlobalVoices-v2017q3, together with the other parallel corpora listed in Table 1 were deduplicated to obtain the final parallel corpus used to train the NMT systems. All corpora were tokenised with the Moses tokeniser (Koehn et al., 2007) and truecased. Parallel sentences with more than 100 tokens in either side were removed. Words were split in sub-word units with byte pair encoding (BPE; Sennrich et al. (2016c)). Table 4 reports the size of the corpora after this pre-processing. 5.2 Neural machine translation architecture We trained the NMT models with the Marian toolkit (Junczys-Dowmunt et al., 2018). Since training hyper-parameters can have a large impact in the quality of the resulting system (Lim et al., 2018), we carried out a grid search in order to find the best hyper-parameters for each translation direction. We expl"
2020.eamt-1.32,W17-4707,0,0.0216419,"5 million sentences. Since the sw NewsCrawl corpus was made available near the end of the development of our MT systems, it could not be used during the iterative back-translation process. Nevertheless, we added it afterwards: the sw NewsCrawl was back-translated with the last available sw→en system obtained after completing all the iterations, concatenated to the Integrating linguistic information In addition to the corpora described above, linguistic information encoded in a more explicit representation was also employed to build the MT systems. In particular, we explored the interleaving (Nadejde et al., 2017) of linguistic tags in the TL side of the training corpus with the aim of enhancing the grammatical correctness of the translations. Morphological taggers were used to obtain the interleaved tags added to the training corpus. The sw text was tagged with TreeTagger (Schmid, 2013). We used a model17 trained on the Helsinki Corpus of Swahili.18 The en text was tagged with the Stanford tagger (Qi et al., 2018), which was trained on the English Web Treebank (Silveira et al., 2014). Figure 1 shows examples of en→sw and sw→en training parallel sentences with interleaved tags. While the tags returned"
2020.eamt-1.32,P02-1040,0,0.107107,"ading what?’) Change in word order, use of auxiliaries Comparative form of adjective (‘-er’) or ‘more’ ’have’ No change in word order Amesoma (‘He has read’); Amesoma? (‘Has he read?’) Absolute form of adjective Virusi ni ndogo (‘A virus is small’) Virusi ni ndogo kuliko bakteria (‘A virus is smaller than a bacterium’, lit. ‘A virus is small where there is a bacterium’) Nina swali (‘I have a question’, lit. ‘I-am-with question’) Comparative Predicative Possession Mainly prefixing conjunctional (‘to be with’) Table 3: A summary of linguistic contrasts between English and Swahili. highest BLEU (Papineni et al., 2002) score on the development set. We obtained the highest test BLEU scores for en→sw with an RNN architecture, 30 000 BPE operations, tied embeddings and single GPU, while the highest ones for sw→en were obtained with a Transformer architecture, 30 000 BPE operations, tied embeddings and two GPUs. 5.3 Leveraging monolingual data Once the best hyper-parameters were identified, we tried to improve the systems by making use of the monolingual corpora via back-translation. Backtranslation (Sennrich et al., 2016b) is a widespread method for integrating target-language (TL) monolingual corpora into NMT"
2020.eamt-1.32,E17-2025,0,0.0141716,"n. We explored both the Transformer (Vaswani et al., 2017) and recurrent neural network (RNN) with attention (Bahdanau et al., 2014) architectures. Our starting points were the Transformer hyperparameters15 described by Sennrich et al. (2017) and the RNN hyper-parameters16 described by Sennrich et al. (2016a). For each translation direction and architecture, we explored the following hyper-parameters: • Number of BPE operations: 15 000, 30 000, or 85 000. • Batch size: 8 000 tokens (trained on one GPU) or 16 000 tokens (trained on two GPUs). • Whether to tie the embeddings for both languages (Press and Wolf, 2017) We trained a system for each combination of hyper-parameters, using only the parallel data described above. Early stopping was based on perplexity on the development set and patience was set to 5. We selected the checkpoint that obtained the 15 https://github.com/marian-nmt/ marian-examples/tree/master/ wmt2017-transformer 16 https://github.com/marian-nmt/ marian-examples/tree/master/ training-basics Feature Coding of plurality in nouns Number of categories encoded in a single-word verb Value in English Plural suffix Value in Swahili Plural prefix Few (number, person, tense) Definite articles"
2020.eamt-1.32,K18-2016,0,0.0620134,"Missing"
2020.eamt-1.32,W18-6488,1,0.855142,"Missing"
2020.eamt-1.32,2010.amta-papers.14,0,0.0157459,"gel, 2008) for the following corpora: EUBookshop v2, Ubuntu and Tanzil (see Table 1). A total of 180 520 pairs of documents were obtained by using this method. • A collection of pairs of segments that are wrongly aligned to train a language model: following Bicleaner’s documentation, this collection was obtained from the raw parallel corpus by applying the “hard rules” implemented in Bicleaner. Sentence alignment. In this step, aligned documents are segmented and aligned at the sentence level. Two sentence-alignment tools are supported by Bitextor: Hunalign (Varga et al., 2007) and BLEUalign (Sennrich and Volk, 2010). We used Hunalign because BLEUalign requires an MT system to be available. The same bilingual dictionary used for document alignment was provided to Hunalign in order to improve the accuracy of the alignment. After applying Hunalign, 2 051 678 unique segment pairs were obtained. Cleaning. Bicleaner11 (S´anchez-Cartagena et al., 2018) was used to clean the raw corpora obtained after sentence alignment. Cleaning implies removing the noisy sentence pairs that are either incorrectly aligned or not in the expected languages.12 Bicleaner cleaning models require some languagedependent resources: • T"
2020.eamt-1.32,P16-1009,0,0.496034,"168 8 269 266 sw tokens 2 981 699 1 206 757 1 734 247 546 107 476 478 2 655 228 170 6 948 341 Table 1: Parallel English–Swahili corpora used to train the NMT systems described in this work. GV stands for the GlobalVoices corpus. automatic evaluation measures, describes a manual evaluation we are conducting and provides preliminary results. The paper ends with some concluding remarks. 2 Monolingual and bilingual corpora Parallel data is the basic resource required to train NMT. Additionally, it is common practice to use synthetic parallel corpora obtained by back-translating monolingual data (Sennrich et al., 2016b). This section describes the corpora we used to train the NMT systems described in Section 5. Tables 1 and 2 describe the parallel and monolingual corpora we used, respectively. As regards parallel corpora, with the exception of GoURMET and SAWA, all of them were downloaded from the OPUS website,2 one of the largest repositories of parallel data on the Internet.3 We used two additional parallel corpora: the SAWA corpus (De Pauw et al., 2011), that was kindly provided by their editors, and the GoURMET corpus, that was crawled from the web following the method described in Section 3. As regard"
2020.eamt-1.32,P16-1162,0,0.700793,"168 8 269 266 sw tokens 2 981 699 1 206 757 1 734 247 546 107 476 478 2 655 228 170 6 948 341 Table 1: Parallel English–Swahili corpora used to train the NMT systems described in this work. GV stands for the GlobalVoices corpus. automatic evaluation measures, describes a manual evaluation we are conducting and provides preliminary results. The paper ends with some concluding remarks. 2 Monolingual and bilingual corpora Parallel data is the basic resource required to train NMT. Additionally, it is common practice to use synthetic parallel corpora obtained by back-translating monolingual data (Sennrich et al., 2016b). This section describes the corpora we used to train the NMT systems described in Section 5. Tables 1 and 2 describe the parallel and monolingual corpora we used, respectively. As regards parallel corpora, with the exception of GoURMET and SAWA, all of them were downloaded from the OPUS website,2 one of the largest repositories of parallel data on the Internet.3 We used two additional parallel corpora: the SAWA corpus (De Pauw et al., 2011), that was kindly provided by their editors, and the GoURMET corpus, that was crawled from the web following the method described in Section 3. As regard"
2020.eamt-1.32,silveira-etal-2014-gold,0,0.0658922,"Missing"
2020.eamt-1.8,D18-1549,0,0.0199032,"n though the problem can be partially mitigated with accurate hyper-parameter tuning (Sennrich and Zhang, 2019), taking advantage of additional resources can help to further improve the quality of the system. Monolingual texts in both languages can be leveraged with the help of back-translation (Sennrich et al., 2016a; Hoang et al., 2018) to generate synthetic c 2020 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. parallel corpora. It is also possible to use only monolingual corpora and follow an unsupervised NMT approach (Artetxe et al., 2018). Parallel corpora from related language pairs can also be leveraged thanks to multilingual NMT (Johnson et al., 2017) and other forms of transfer learning (Kocmi and Bojar, 2018). In addition to the use of corpora, linguistic resources can also be used to improve NMT. If morphological analysers or syntactic parsers are available, they can be used to build a richer representation of the words being translated (Sennrich and Haddow, 2016; Nadejde et al., 2017). Even full rule-based MT (RBMT) systems can be combined with NMT in order to build hybrid systems (Huang et al., 2020). In this work, we"
2020.eamt-1.8,D16-1162,0,0.0268629,"gical analyzers, monolingual and bilingual dictionaries. As for bilingual text corpora, today OPUS2 contains about 400,000 sentence pairs, most of them very specialized, in the field of computer science. The Apertium Breton–French system: The Apertium platform3 contains an MT system designed to allow French-speaking readers to access written Breton content (gisting).4 This MT system 1 Integration of bilingual segments into NMT. The integration of bilingual segments, which could be produced by an RBMT system, into an NMT system has received some attention recently. One of the first approaches (Arthur et al., 2016), which can Breton–French machine translation https://www.freelang.com/enligne/breton. php 2 http://opus.nlpl.eu 3 http://www.apertium.org 4 Developers deliberately chose not develop French–Breton MT, deeming it too risky in terms of the socio-linguistic situation, as users would assume the machine-translated Breton to be us, nuine en-kk data augmentation  Joint SL-TL BPE applied after segmentation without ata* ed data and U chrF++ 6 2 6 1 0 27.80 39.93 39.67 42.87 42.86 3 44.46 6 44.99 9 45.24 ): 2nd n (no cluster) (Tyers, 2010), the only one in the world for Breton, was released in May 2009"
2020.eamt-1.8,P17-1175,0,0.0210054,"Miceli Barone et al., 2017, Sec. 4.2) and a deep output that combines the context vector, the recurrent hidden state and the embedding of the previous symbol. The multi-source recurrent NMT system contains two encoders (one for each input) which do not share parameters. The modifications in the decoder that allow it to accommodate the two encoders are the following: • The initial state of the decoder is obtained after concatenating the averaged encoder states of the two input sequences. • The conditional GRU (cGRU) unit with attention in the decoder is replaced by a doublyattentive cGRU cell (Calixto et al., 2017) featuring two independent attention mechanisms. • The context vector used in the deep output is replaced by the concatenation of the context vectors of the two inputs. For further details, the reader is referred to JunczysDowmunt and Grundkiewicz (2017). Our transformer models follow the architecture proposed by Vaswani et al. (2017). A transformer model contains an encoder and a decoder. The encoder is made of stacked layers, each containing a self-attention unit and a feed-forward unit. The decoder is also made of stacked layers, each containing a self-attention unit, an encoder–decoder att"
2020.eamt-1.8,W17-4716,0,0.020906,"r NMT that could contain multiple-token bilingual segments. They modelled decoding as a mixture of two processes: generating a word with the standard NMT model, or introducing a phrase from the phrase memory. Zhang et al. (2017) formalised the strategy of Tang et al. (2016) as a posterior regularization approach (Ganchev et al., 2010). Feng et al. (2018) designed a phrase attention mechanism that could be used either without additional supervision or with an external bilingual lexicon. Another related line of research modifies the beam search algorithm to meet some terminological constraints (Chatterjee et al., 2017; Post and Vilar, 2018). 2 The Breton language (Brezhoneg in Breton) is a Celtic language of the Brittonic group that is spoken in the west of Brittany (Breizh Izel or “Lower Brittany”) in France, and the main language with which it has contact is French, the only official language; in fact, Breton, spoken by about 200,000 people, has virtually no legal recognition in France. Resources for Breton: Programs like Firefox, Google applications and some Microsoft programs have been localized and there is a 70,000-page Breton Wikipedia. There is little software dedicated to Breton; most of it free/o"
2020.eamt-1.8,A92-1018,0,0.181783,"Missing"
2020.eamt-1.8,W08-0328,0,0.0570623,"e concluding remarks. Hybrid systems combining rule-based and corpus-based approaches. The creation of hybrid systems combining RBMT and statistical MT (SMT) has been explored by many authors. The most relevant approach for this work (Tyers, 2009) enlarged the training corpus of an SMT system with 116,500 sentence pairs made up of all possible inflected Breton forms and their inflected French translations as present in an earlier version of the Apertium Breton–French system we are using. Schwenk et al. (2009) followed a similar approach for other language pairs. More sophisticated approaches (Eisele et al., 2008; Enache et al., 2012; S´anchez-Cartagena et al., 2016) involve modifying the SMT architecture. Concerning the combination of RBMT and NMT, a relevant line of research involves choosing the best output (either RBMT or NMT) for each source sentence. For instance, Huang et al. (2020) propose training an automatic classifier for this task and use some features to help predict how difficult is the source sentence for each system: for instance, the degree of morphological and syntactic ambiguity is useful to estimate how difficult is the sentence for the RBMT system, while the token frequency on th"
2020.eamt-1.8,P18-4020,0,0.015036,"fore, RBMT systems that perform full syntactic analysis are more effective than Apertium when dealing, for instance, with long-range reorderings. Corpus train dev test • A post-generator which performs inter-word orthographic operations: contractions, elisions marked by apostrophes, etc.8 3.2 Multi-source neural machine translation We experimented with the transformer (Vaswani et al., 2017) and the recurrent attentional encoder– decoder (Bahdanau et al., 2015, hereinafter recurrent) NMT architectures. In both cases, we followed the multi-source architectures implemented in the Marian toolkit (Junczys-Dowmunt et al., 2018), which are described next. Our recurrent NMT systems follow the same architecture as Nematus (Sennrich et al., 2017b), namely a bidirectional gated recurrent unit (GRU) encoder, a conditional GRU decoder with attention (Miceli Barone et al., 2017, Sec. 4.2) and a deep output that combines the context vector, the recurrent hidden state and the embedding of the previous symbol. The multi-source recurrent NMT system contains two encoders (one for each input) which do not share parameters. The modifications in the decoder that allow it to accommodate the two encoders are the following: • The init"
2020.eamt-1.8,2012.eamt-1.61,0,0.0289606,"Hybrid systems combining rule-based and corpus-based approaches. The creation of hybrid systems combining RBMT and statistical MT (SMT) has been explored by many authors. The most relevant approach for this work (Tyers, 2009) enlarged the training corpus of an SMT system with 116,500 sentence pairs made up of all possible inflected Breton forms and their inflected French translations as present in an earlier version of the Apertium Breton–French system we are using. Schwenk et al. (2009) followed a similar approach for other language pairs. More sophisticated approaches (Eisele et al., 2008; Enache et al., 2012; S´anchez-Cartagena et al., 2016) involve modifying the SMT architecture. Concerning the combination of RBMT and NMT, a relevant line of research involves choosing the best output (either RBMT or NMT) for each source sentence. For instance, Huang et al. (2020) propose training an automatic classifier for this task and use some features to help predict how difficult is the source sentence for each system: for instance, the degree of morphological and syntactic ambiguity is useful to estimate how difficult is the sentence for the RBMT system, while the token frequency on the training corpus can"
2020.eamt-1.8,W18-6325,0,0.0179531,"ove the quality of the system. Monolingual texts in both languages can be leveraged with the help of back-translation (Sennrich et al., 2016a; Hoang et al., 2018) to generate synthetic c 2020 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. parallel corpora. It is also possible to use only monolingual corpora and follow an unsupervised NMT approach (Artetxe et al., 2018). Parallel corpora from related language pairs can also be leveraged thanks to multilingual NMT (Johnson et al., 2017) and other forms of transfer learning (Kocmi and Bojar, 2018). In addition to the use of corpora, linguistic resources can also be used to improve NMT. If morphological analysers or syntactic parsers are available, they can be used to build a richer representation of the words being translated (Sennrich and Haddow, 2016; Nadejde et al., 2017). Even full rule-based MT (RBMT) systems can be combined with NMT in order to build hybrid systems (Huang et al., 2020). In this work, we focus on an under-resourced language pair: Breton–French, and study mechanisms to build a hybrid system by combining NMT with the Breton–French system built with the Apertium RBMT"
2020.eamt-1.8,W18-6459,0,0.0264215,"f a recurrent attentional encoder–decoder (Bahdanau et al., 2015) model to decide the target language (TL) word translation probabilities that needed to be boosted in the final softmax layer. Tang et al. (2016) and Wang et al. (2017) relied on a phrase memory for NMT that could contain multiple-token bilingual segments. They modelled decoding as a mixture of two processes: generating a word with the standard NMT model, or introducing a phrase from the phrase memory. Zhang et al. (2017) formalised the strategy of Tang et al. (2016) as a posterior regularization approach (Ganchev et al., 2010). Feng et al. (2018) designed a phrase attention mechanism that could be used either without additional supervision or with an external bilingual lexicon. Another related line of research modifies the beam search algorithm to meet some terminological constraints (Chatterjee et al., 2017; Post and Vilar, 2018). 2 The Breton language (Brezhoneg in Breton) is a Celtic language of the Brittonic group that is spoken in the west of Brittany (Breizh Izel or “Lower Brittany”) in France, and the main language with which it has contact is French, the only official language; in fact, Breton, spoken by about 200,000 people,"
2020.eamt-1.8,W17-3204,0,0.0221557,"ural MT (NMT) for the Breton–French under-resourced language pair in an attempt to study to what extent the RBMT resources help improve the translation quality of the NMT system. We combine both translation approaches in a multi-source NMT architecture and find out that, even though the RBMT system has a low performance according to automatic evaluation metrics, using it leads to improved translation quality. 1 Introduction Corpus-based approaches to machine translation (MT), such as neural MT (NMT), struggle when the size of the available parallel corpora for a given language pair is scarce (Koehn and Knowles, 2017). Even though the problem can be partially mitigated with accurate hyper-parameter tuning (Sennrich and Zhang, 2019), taking advantage of additional resources can help to further improve the quality of the system. Monolingual texts in both languages can be leveraged with the help of back-translation (Sennrich et al., 2016a; Hoang et al., 2018) to generate synthetic c 2020 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. parallel corpora. It is also possible to use only monolingual corpora and follow an unsupervised NMT appro"
2020.eamt-1.8,W17-4710,0,0.133921,"ons marked by apostrophes, etc.8 3.2 Multi-source neural machine translation We experimented with the transformer (Vaswani et al., 2017) and the recurrent attentional encoder– decoder (Bahdanau et al., 2015, hereinafter recurrent) NMT architectures. In both cases, we followed the multi-source architectures implemented in the Marian toolkit (Junczys-Dowmunt et al., 2018), which are described next. Our recurrent NMT systems follow the same architecture as Nematus (Sennrich et al., 2017b), namely a bidirectional gated recurrent unit (GRU) encoder, a conditional GRU decoder with attention (Miceli Barone et al., 2017, Sec. 4.2) and a deep output that combines the context vector, the recurrent hidden state and the embedding of the previous symbol. The multi-source recurrent NMT system contains two encoders (one for each input) which do not share parameters. The modifications in the decoder that allow it to accommodate the two encoders are the following: • The initial state of the decoder is obtained after concatenating the averaged encoder states of the two input sequences. • The conditional GRU (cGRU) unit with attention in the decoder is replaced by a doublyattentive cGRU cell (Calixto et al., 2017) feat"
2020.eamt-1.8,W18-2703,0,0.0136508,"ation metrics, using it leads to improved translation quality. 1 Introduction Corpus-based approaches to machine translation (MT), such as neural MT (NMT), struggle when the size of the available parallel corpora for a given language pair is scarce (Koehn and Knowles, 2017). Even though the problem can be partially mitigated with accurate hyper-parameter tuning (Sennrich and Zhang, 2019), taking advantage of additional resources can help to further improve the quality of the system. Monolingual texts in both languages can be leveraged with the help of back-translation (Sennrich et al., 2016a; Hoang et al., 2018) to generate synthetic c 2020 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. parallel corpora. It is also possible to use only monolingual corpora and follow an unsupervised NMT approach (Artetxe et al., 2018). Parallel corpora from related language pairs can also be leveraged thanks to multilingual NMT (Johnson et al., 2017) and other forms of transfer learning (Kocmi and Bojar, 2018). In addition to the use of corpora, linguistic resources can also be used to improve NMT. If morphological analysers or syntactic parsers a"
2020.eamt-1.8,Q17-1024,0,0.0454399,"Missing"
2020.eamt-1.8,I17-1013,0,0.0400098,"Missing"
2020.eamt-1.8,W18-6467,0,0.0202696,"ins an encoder and a decoder. The encoder is made of stacked layers, each containing a self-attention unit and a feed-forward unit. The decoder is also made of stacked layers, each containing a self-attention unit, an encoder–decoder attention unit and feed-forward unit. The multisource transformer systems contain two encoders and two encoder–decoder attention units in each decoder layer. This transformer multi-source architecture was also used in the winning submission to the 2018 WMT automatic post-editing shared task (Chatterjee et al., 2018). For further details, the reader is referred to Junczys-Dowmunt and Grundkiewicz (2018). 8 In French: a` + lequel → auquel; de + hˆotels → d’hˆotels, etc. # sent. 139,489 2,000 3,000 # br tokens 1,096,311 25,291 37,054 # fr tokens 1,116,100 24,835 36,346 Table 1: Number of parallel sentences and tokens in Breton and French for the corpora used for train/dev/test corpora. 4 Experiments and results For the experiments we used the following corpora available at OPUS:9 Tatoeba, GNOME, OfisPublik, KDE4, wikimedia, Ubuntu and OpenSubtitles. For development and testing we used the same portions of the OfisPublik corpus used by S´anchezCartagena et al. (2015), the rest of corpora, after"
2020.eamt-1.8,W17-4707,0,0.0213213,"rks, attribution, CCBY-ND. parallel corpora. It is also possible to use only monolingual corpora and follow an unsupervised NMT approach (Artetxe et al., 2018). Parallel corpora from related language pairs can also be leveraged thanks to multilingual NMT (Johnson et al., 2017) and other forms of transfer learning (Kocmi and Bojar, 2018). In addition to the use of corpora, linguistic resources can also be used to improve NMT. If morphological analysers or syntactic parsers are available, they can be used to build a richer representation of the words being translated (Sennrich and Haddow, 2016; Nadejde et al., 2017). Even full rule-based MT (RBMT) systems can be combined with NMT in order to build hybrid systems (Huang et al., 2020). In this work, we focus on an under-resourced language pair: Breton–French, and study mechanisms to build a hybrid system by combining NMT with the Breton–French system built with the Apertium RBMT platform (Forcada et al., 2011). We aim at producing sentences that combine knowledge extracted from the parallel corpus and from the RBMT system. Hence, we go beyond approaches that simply choose the best system (either RBMT or NMT) for each input sentence (see below). We use mult"
2020.eamt-1.8,J11-4002,0,0.0942918,"Missing"
2020.eamt-1.8,N18-1119,0,0.0132948,"multiple-token bilingual segments. They modelled decoding as a mixture of two processes: generating a word with the standard NMT model, or introducing a phrase from the phrase memory. Zhang et al. (2017) formalised the strategy of Tang et al. (2016) as a posterior regularization approach (Ganchev et al., 2010). Feng et al. (2018) designed a phrase attention mechanism that could be used either without additional supervision or with an external bilingual lexicon. Another related line of research modifies the beam search algorithm to meet some terminological constraints (Chatterjee et al., 2017; Post and Vilar, 2018). 2 The Breton language (Brezhoneg in Breton) is a Celtic language of the Brittonic group that is spoken in the west of Brittany (Breizh Izel or “Lower Brittany”) in France, and the main language with which it has contact is French, the only official language; in fact, Breton, spoken by about 200,000 people, has virtually no legal recognition in France. Resources for Breton: Programs like Firefox, Google applications and some Microsoft programs have been localized and there is a 70,000-page Breton Wikipedia. There is little software dedicated to Breton; most of it free/open-source, such as the"
2020.eamt-1.8,K18-2016,0,0.128736,"t attentional encoder–decoder (Bahdanau et al., 2015) model to decide the target language (TL) word translation probabilities that needed to be boosted in the final softmax layer. Tang et al. (2016) and Wang et al. (2017) relied on a phrase memory for NMT that could contain multiple-token bilingual segments. They modelled decoding as a mixture of two processes: generating a word with the standard NMT model, or introducing a phrase from the phrase memory. Zhang et al. (2017) formalised the strategy of Tang et al. (2016) as a posterior regularization approach (Ganchev et al., 2010). Feng et al. (2018) designed a phrase attention mechanism that could be used either without additional supervision or with an external bilingual lexicon. Another related line of research modifies the beam search algorithm to meet some terminological constraints (Chatterjee et al., 2017; Post and Vilar, 2018). 2 The Breton language (Brezhoneg in Breton) is a Celtic language of the Brittonic group that is spoken in the west of Brittany (Breizh Izel or “Lower Brittany”) in France, and the main language with which it has contact is French, the only official language; in fact, Breton, spoken by about 200,000 people,"
2020.eamt-1.8,W19-5339,1,0.75374,"rning the combination of RBMT and NMT, a relevant line of research involves choosing the best output (either RBMT or NMT) for each source sentence. For instance, Huang et al. (2020) propose training an automatic classifier for this task and use some features to help predict how difficult is the source sentence for each system: for instance, the degree of morphological and syntactic ambiguity is useful to estimate how difficult is the sentence for the RBMT system, while the token frequency on the training corpus can help to assess how difficult it is for the NMT system. Similarly, Singh et al. (2019) use confidence scores computed for each system to choose the best alternative for each source sentence. Torregrosa et al. (2019) experimented with the integration of RBMT bilingual dictionaries and syntactic parsers into NMT without success. Finally, the multi-source architecture studied in this paper has been preliminary explored by S´anchez-Cartagena et al. (2019). The main differences with this work are: i) they did not study the impact of the different components of the RBMT system; and ii) they did not perform a hyper-parameter search, which could explain the poor performance of their tr"
2020.eamt-1.8,W09-0423,0,0.0335454,"Section 4 presents the experiments carried out and discusses the results obtained. The paper ends with some concluding remarks. Hybrid systems combining rule-based and corpus-based approaches. The creation of hybrid systems combining RBMT and statistical MT (SMT) has been explored by many authors. The most relevant approach for this work (Tyers, 2009) enlarged the training corpus of an SMT system with 116,500 sentence pairs made up of all possible inflected Breton forms and their inflected French translations as present in an earlier version of the Apertium Breton–French system we are using. Schwenk et al. (2009) followed a similar approach for other language pairs. More sophisticated approaches (Eisele et al., 2008; Enache et al., 2012; S´anchez-Cartagena et al., 2016) involve modifying the SMT architecture. Concerning the combination of RBMT and NMT, a relevant line of research involves choosing the best output (either RBMT or NMT) for each source sentence. For instance, Huang et al. (2020) propose training an automatic classifier for this task and use some features to help predict how difficult is the source sentence for each system: for instance, the degree of morphological and syntactic ambiguity"
2020.eamt-1.8,W16-2209,0,0.0212494,"0 licence, no derivative works, attribution, CCBY-ND. parallel corpora. It is also possible to use only monolingual corpora and follow an unsupervised NMT approach (Artetxe et al., 2018). Parallel corpora from related language pairs can also be leveraged thanks to multilingual NMT (Johnson et al., 2017) and other forms of transfer learning (Kocmi and Bojar, 2018). In addition to the use of corpora, linguistic resources can also be used to improve NMT. If morphological analysers or syntactic parsers are available, they can be used to build a richer representation of the words being translated (Sennrich and Haddow, 2016; Nadejde et al., 2017). Even full rule-based MT (RBMT) systems can be combined with NMT in order to build hybrid systems (Huang et al., 2020). In this work, we focus on an under-resourced language pair: Breton–French, and study mechanisms to build a hybrid system by combining NMT with the Breton–French system built with the Apertium RBMT platform (Forcada et al., 2011). We aim at producing sentences that combine knowledge extracted from the parallel corpus and from the RBMT system. Hence, we go beyond approaches that simply choose the best system (either RBMT or NMT) for each input sentence ("
2020.eamt-1.8,P19-1021,0,0.0181827,"sources help improve the translation quality of the NMT system. We combine both translation approaches in a multi-source NMT architecture and find out that, even though the RBMT system has a low performance according to automatic evaluation metrics, using it leads to improved translation quality. 1 Introduction Corpus-based approaches to machine translation (MT), such as neural MT (NMT), struggle when the size of the available parallel corpora for a given language pair is scarce (Koehn and Knowles, 2017). Even though the problem can be partially mitigated with accurate hyper-parameter tuning (Sennrich and Zhang, 2019), taking advantage of additional resources can help to further improve the quality of the system. Monolingual texts in both languages can be leveraged with the help of back-translation (Sennrich et al., 2016a; Hoang et al., 2018) to generate synthetic c 2020 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. parallel corpora. It is also possible to use only monolingual corpora and follow an unsupervised NMT approach (Artetxe et al., 2018). Parallel corpora from related language pairs can also be leveraged thanks to multilingua"
2020.eamt-1.8,P16-1009,0,0.0390644,"rding to automatic evaluation metrics, using it leads to improved translation quality. 1 Introduction Corpus-based approaches to machine translation (MT), such as neural MT (NMT), struggle when the size of the available parallel corpora for a given language pair is scarce (Koehn and Knowles, 2017). Even though the problem can be partially mitigated with accurate hyper-parameter tuning (Sennrich and Zhang, 2019), taking advantage of additional resources can help to further improve the quality of the system. Monolingual texts in both languages can be leveraged with the help of back-translation (Sennrich et al., 2016a; Hoang et al., 2018) to generate synthetic c 2020 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. parallel corpora. It is also possible to use only monolingual corpora and follow an unsupervised NMT approach (Artetxe et al., 2018). Parallel corpora from related language pairs can also be leveraged thanks to multilingual NMT (Johnson et al., 2017) and other forms of transfer learning (Kocmi and Bojar, 2018). In addition to the use of corpora, linguistic resources can also be used to improve NMT. If morphological analysers"
2020.eamt-1.8,P16-1162,0,0.0324381,"rding to automatic evaluation metrics, using it leads to improved translation quality. 1 Introduction Corpus-based approaches to machine translation (MT), such as neural MT (NMT), struggle when the size of the available parallel corpora for a given language pair is scarce (Koehn and Knowles, 2017). Even though the problem can be partially mitigated with accurate hyper-parameter tuning (Sennrich and Zhang, 2019), taking advantage of additional resources can help to further improve the quality of the system. Monolingual texts in both languages can be leveraged with the help of back-translation (Sennrich et al., 2016a; Hoang et al., 2018) to generate synthetic c 2020 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. parallel corpora. It is also possible to use only monolingual corpora and follow an unsupervised NMT approach (Artetxe et al., 2018). Parallel corpora from related language pairs can also be leveraged thanks to multilingual NMT (Johnson et al., 2017) and other forms of transfer learning (Kocmi and Bojar, 2018). In addition to the use of corpora, linguistic resources can also be used to improve NMT. If morphological analysers"
2020.eamt-1.8,E17-3017,0,0.0648516,"Missing"
2020.eamt-1.8,E17-1100,1,0.902103,"Missing"
2020.eamt-1.8,W19-6725,0,0.0191191,"MT or NMT) for each source sentence. For instance, Huang et al. (2020) propose training an automatic classifier for this task and use some features to help predict how difficult is the source sentence for each system: for instance, the degree of morphological and syntactic ambiguity is useful to estimate how difficult is the sentence for the RBMT system, while the token frequency on the training corpus can help to assess how difficult it is for the NMT system. Similarly, Singh et al. (2019) use confidence scores computed for each system to choose the best alternative for each source sentence. Torregrosa et al. (2019) experimented with the integration of RBMT bilingual dictionaries and syntactic parsers into NMT without success. Finally, the multi-source architecture studied in this paper has been preliminary explored by S´anchez-Cartagena et al. (2019). The main differences with this work are: i) they did not study the impact of the different components of the RBMT system; and ii) they did not perform a hyper-parameter search, which could explain the poor performance of their transformer systems. In addition, we conduct an automatic analysis of the errors produced by our hybrid approach. only be applied t"
2020.eamt-1.8,2009.eamt-1.29,0,0.720817,"ms, including approaches for integrating external bilingual segments into NMT. Section 2 then explains the resources available for Breton– French and the challenges of translating between Breton and French. Section 3 describes the hybrid architecture chosen. Section 4 presents the experiments carried out and discusses the results obtained. The paper ends with some concluding remarks. Hybrid systems combining rule-based and corpus-based approaches. The creation of hybrid systems combining RBMT and statistical MT (SMT) has been explored by many authors. The most relevant approach for this work (Tyers, 2009) enlarged the training corpus of an SMT system with 116,500 sentence pairs made up of all possible inflected Breton forms and their inflected French translations as present in an earlier version of the Apertium Breton–French system we are using. Schwenk et al. (2009) followed a similar approach for other language pairs. More sophisticated approaches (Eisele et al., 2008; Enache et al., 2012; S´anchez-Cartagena et al., 2016) involve modifying the SMT architecture. Concerning the combination of RBMT and NMT, a relevant line of research involves choosing the best output (either RBMT or NMT) for e"
2020.eamt-1.8,2010.eamt-1.13,0,0.0336913,"d some attention recently. One of the first approaches (Arthur et al., 2016), which can Breton–French machine translation https://www.freelang.com/enligne/breton. php 2 http://opus.nlpl.eu 3 http://www.apertium.org 4 Developers deliberately chose not develop French–Breton MT, deeming it too risky in terms of the socio-linguistic situation, as users would assume the machine-translated Breton to be us, nuine en-kk data augmentation  Joint SL-TL BPE applied after segmentation without ata* ed data and U chrF++ 6 2 6 1 0 27.80 39.93 39.67 42.87 42.86 3 44.46 6 44.99 9 45.24 ): 2nd n (no cluster) (Tyers, 2010), the only one in the world for Breton, was released in May 2009 as the result of the joint efforts of the Ofis ar Brezhoneg,5 the Spanish company Prompsit Language Engineering, and the Universitat d’Alacant and is based on the Apertium platform (Forcada et al., 2011). Dictionary development started with the free dictionaries for Breton in Lexilogos.6 Development of the Apertium Breton– French MT system slowly continues. The quality of the French generated is not suitable for publishing, but may be used to get a rough idea of the meaning of a Breton text. transformer br text RNN + RBMT weighte"
2020.eamt-1.8,D17-1149,0,0.0194142,"are: i) they did not study the impact of the different components of the RBMT system; and ii) they did not perform a hyper-parameter search, which could explain the poor performance of their transformer systems. In addition, we conduct an automatic analysis of the errors produced by our hybrid approach. only be applied to single-token bilingual segments, used the attention weights of a recurrent attentional encoder–decoder (Bahdanau et al., 2015) model to decide the target language (TL) word translation probabilities that needed to be boosted in the final softmax layer. Tang et al. (2016) and Wang et al. (2017) relied on a phrase memory for NMT that could contain multiple-token bilingual segments. They modelled decoding as a mixture of two processes: generating a word with the standard NMT model, or introducing a phrase from the phrase memory. Zhang et al. (2017) formalised the strategy of Tang et al. (2016) as a posterior regularization approach (Ganchev et al., 2010). Feng et al. (2018) designed a phrase attention mechanism that could be used either without additional supervision or with an external bilingual lexicon. Another related line of research modifies the beam search algorithm to meet some"
2020.eamt-1.8,P17-1139,0,0.0444111,"Missing"
2020.eamt-1.8,N16-1004,0,0.0186629,"Missing"
2021.mtsummit-research.8,2020.wmt-1.5,1,0.795255,"en prepended to the source sentence to specify the output language. We use WMT data (see Section 4.1) for training and early stopping. Training of the from-scratch system. Training consists of fine-tuning a pretrained model with Pashto–English parallel data, using it to generate initial backtranslations which are combined with the parallel data and used to train another round of the model, starting again from a pretrained model. At this point, we include the first 220,000 sentence pairs of “Bytedance” filtered parallel data, sorted by filtering rank. Following similar work with English–Tamil (Bawden et al., 2020), we start with our mBART-like model and we fine-tune it in the Pashto→English direction with our parallel data. Then we use this model to backtranslate the Pashto monolingual data, generating a pseudo-parallel corpus which we combine with our true parallel corpus and use to train a English→Pashto model again starting from mBART. We use this model to backtranslate the first 5,000,000 monolingual English sentences (we also experimented with the full corpus, but found minimal difference), and we train another round of Pashto→English followed by another round of English→Pashto, both initialized f"
2021.mtsummit-research.8,W18-2716,0,0.065695,"Missing"
2021.mtsummit-research.8,D18-2012,0,0.0212368,"f these could be Pashto speakers. Pashto (also spelled Pukhto and Pakhto is an Iranian language of the Indo-European family and is grouped with other Iranian languages such as Persian, Dari, Tajiki, in spite of major linguistic diferences among them. Pashto is written with a unique enriched Perso-Arabic script with 45 letters and four diacritics. Translating between English and Pashto poses interesting challenges. Pashto has a richer morphology than that of English; the induced data sparseness may partly be remedied with segmentation in subword units tokenization models such as SentencePiece (Kudo and Richardson, 2018), as used in mBART50. There are Pashto categories in Pashto that do not overtly exist in English (such as verb aspect or the oblique case in general nouns) and categories in English that do not overtly exist in Pashto (such as definite and indefinite articles), which may pose a certain challenge when having to generate correct text in machine translation output. Due to the chronic political and social instability and conflict that Afghanistan has experienced in its recent history, the country features prominently in global news coverage. Closely following the developments there remains a key p"
2021.mtsummit-research.8,2020.tacl-1.47,0,0.318683,"he BBC and DW for a short period of time. Given the impact of the COVID-19 pandemic, a twomonth period was considered realistic. On 1 February 2021, BBC and DW revealed the chosen language to be Pashto. By completing and documenting how this challenge was addressed, we prove we are able to bootstrap a new high quality neural machine translation task within a very limited window of time. There has also been a considerable amount of recent interest in using pretrained language models for improving performance on downstream natural language processing tasks, especially in a low resource setting (Liu et al., 2020; Brown et al., 2020; Qiu et al., 2020), but how best to do this is still an open question. A key question in this work is how best to use training data which is not English (en) to Pashto (ps) translations. We experimented, on the one hand, with pretraining models on a high-resource language pair (German–English, one of the most studied high-resource language pairs) and, on the other hand, with fine-tuning an existing large pretrained translation model (mBART50) trained on parallel data involving English and 49 languages including Pashto (Tang et al., 2020). We show that both approaches perfo"
2021.mtsummit-research.8,W17-4770,0,0.0257416,"Missing"
2021.mtsummit-research.8,W18-6319,0,0.0127791,"tively small decrease in memory consumption: for example, the GPU memory requirements of mBART n–to–n at inference time (setting the maximum number of tokens per mini-batch to 100) moved from around 4 GB to around 3 GB. 5 Results and Discussion Tables 2 and 3 show BLEU and chrF2 scores, respectively, for the English to Pashto systems with different test sets. The evaluation metrics for the Google MT system are also included for reference purposes. Similarly, tables 4 and 5 show BLEU and chrF2 scores, respectively, for the Pashto to English systems. All the scores were computed with sacrebleu (Post, 2018). The test sets considered are the two in-house parallel sets created by BBC and DW (see Section 3) and the devtest set provided in the FLORES19 benchmark (2,698 sentences). 19 https://github.com/facebookresearch/flores 8 Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 99 Google from-scratch mBART50 + small + small, large + small, large, synthetic BBC test DW test FLORES devtest 35.03 20.00 19.42 22.55 25.27 25.38 24.65 15.06 15.30 17.50 19.13 17.88 21.54 14.90 14.59 14.77 17.71 17.08 Table 4: BLEU scores of the Pa"
forcada-2014-annotation,2011.mtsummit-papers.18,1,\N,Missing
forcada-2014-annotation,W07-1501,0,\N,Missing
forcada-2014-annotation,J03-1002,0,\N,Missing
forcada-2014-annotation,W14-0309,1,\N,Missing
J02-2004,J00-1002,0,0.192245,"r removed from the language it accepts; both operations are very important when dictionary maintenance is performed and solve the dictionary construction problem addressed by Daciuk et al. as a special case. The algorithms proposed here may be straightforwardly derived from the customary textbook constructions for the intersection and the complementation of finitestate automata; the algorithms exploit the special properties of the automata resulting from the intersection operation when one of the finite-state automata accepts a single string. 1. Introduction In a recent paper in this journal, Daciuk et al. (2000) describe two methods for constructing incrementally minimal, deterministic, acyclic finite-state automata (dictionaries) from sets of strings: The first method adds strings in dictionary order, and the second one is for unsorted data. Adding an entry is an important dictionary maintenance operation, but so is removing an entry from the dictionary, for example, if it is found to be incorrect. Since ordering cannot obviously be expected in the removal case, we will focus on the second, more general problem (a solution for which has already been sketched by Revuz [2000]). But dictionaries or acy"
S12-1065,P06-1114,0,0.0607265,"oduction Cross-lingual textual entailment (CLTE) detection (Mehdad et al., 2010) is an extension of the textual entailment (TE) detection (Dagan et al., 2006) problem. TE detection consists of finding out, for two text fragments T and H in the same language, whether T entails H from a semantic point of view or not. CLTE presents a similar problem, but with T and H written in different languages. During the last years, many authors have focused on resolving TE detection, as solutions to this problem have proved to be useful in many natural language processing tasks, such as question answering (Harabagiu and Hickl, 2006) or machine translation (MT) (Mirkin et al., 2009; Pad´o et al., 2009). Therefore, CLTE may also be useful for related tasks in which more than one language is involved, such as cross-lingual question answering or cross-lingual information retrieval. Although CLTE detection is a relatively new problem, it has already been tackled. Mehdad et al. (2010) propose to use machine translation (MT) to translate H from LH , the language of H, into LT , the language of T , and then use any of the state-of-the-art TE approaches. In a later work (Mehdad et al., 2011), the authors use MT, but in a more ela"
S12-1065,N03-1017,0,0.00633838,"Missing"
S12-1065,P07-2045,0,0.00420345,"ding feature function computes, for the total number of sub-segments of a given length l ∈ [1, L] obtained from a text fragment S, the fraction of them which appear in a sub-segment link. It is applied both to H and T and is defined as: Fl0 (S) = Linkedl (S)/(|S |− l + 1) where Linkedl is the number of sub-segments from S with length l which appear in a sub-segment link. 3 (both sentences entail each other); and no entailment (neither of the sentences entails each other). For the whole data set, both sentences in each instance were tokenized using the scripts1 included in the Moses MT system (Koehn et al., 2007). Each sentence was segmented to get all possible sub-segments which were then translated into the other language. Experimental settings The experiments designed for this task are aimed at evaluating the features proposed in Section 2. We evaluate our CLTE approach using the English– Spanish data sets provided in the task 8 of SemEval 2012 (Negri et al., 2012). • Google Translate:4 an online MT system by Google Inc. • Microsoft Translator:5 an online MT system by Microsoft. External resources were also used for the extended features described in Section 2.2. We used the stemmer6 and the stopwo"
S12-1065,N10-1045,0,0.550758,"n them, which can be used to determine whether T entails H or not. Note that MT is used as a black box, i.e. sub-segment translations may be collected from any MT system, and that our approach could even use any other sources of bilingual sub-sentential information. It is even possible to combine different MT systems as we do in our experiments. This is a key point of our work, since 472 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 472–476, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics it uses MT in a more elaborate way than Mehdad et al. (2010), and it does not depend on a specific MT approach. Another important difference between this work and that of Mehdad et al. (2011) is the set of features used for classification. The paper is organized as follows: Section 2 describes the method used to collect the MT information and obtain the features; Section 3 explains the experimental framework; Section 4 shows the results obtained for the different features combination proposed; the paper ends with concluding remarks. 2 Features from machine translation Our approach uses MT as a black box to detect parallelisms between the text fragments"
S12-1065,P11-1134,0,0.502404,"sks, such as question answering (Harabagiu and Hickl, 2006) or machine translation (MT) (Mirkin et al., 2009; Pad´o et al., 2009). Therefore, CLTE may also be useful for related tasks in which more than one language is involved, such as cross-lingual question answering or cross-lingual information retrieval. Although CLTE detection is a relatively new problem, it has already been tackled. Mehdad et al. (2010) propose to use machine translation (MT) to translate H from LH , the language of H, into LT , the language of T , and then use any of the state-of-the-art TE approaches. In a later work (Mehdad et al., 2011), the authors use MT, but in a more elaborate way. They train a phrase-based statistical MT (PBSMT) system (Koehn et al., 2003) translating from LH to LT , and use the translation table obtained as a by-product of the training process to extract a set of features which are processed by a support vector machine classifier (Theodoridis and Koutroumbas, 2009, Sect. 3.7) to decide whether T entails H or not. Castillo (2011) discusses another machine learning approach in which the features are obtained from semantic similarity measures based on WordNet (Miller, 1995). In this work we present a new"
S12-1065,P09-1089,0,0.0187436,"on (Mehdad et al., 2010) is an extension of the textual entailment (TE) detection (Dagan et al., 2006) problem. TE detection consists of finding out, for two text fragments T and H in the same language, whether T entails H from a semantic point of view or not. CLTE presents a similar problem, but with T and H written in different languages. During the last years, many authors have focused on resolving TE detection, as solutions to this problem have proved to be useful in many natural language processing tasks, such as question answering (Harabagiu and Hickl, 2006) or machine translation (MT) (Mirkin et al., 2009; Pad´o et al., 2009). Therefore, CLTE may also be useful for related tasks in which more than one language is involved, such as cross-lingual question answering or cross-lingual information retrieval. Although CLTE detection is a relatively new problem, it has already been tackled. Mehdad et al. (2010) propose to use machine translation (MT) to translate H from LH , the language of H, into LT , the language of T , and then use any of the state-of-the-art TE approaches. In a later work (Mehdad et al., 2011), the authors use MT, but in a more elaborate way. They train a phrase-based statistical"
S12-1065,D11-1062,0,0.173937,"Missing"
S12-1065,S12-1053,0,0.0318287,"gment link. 3 (both sentences entail each other); and no entailment (neither of the sentences entails each other). For the whole data set, both sentences in each instance were tokenized using the scripts1 included in the Moses MT system (Koehn et al., 2007). Each sentence was segmented to get all possible sub-segments which were then translated into the other language. Experimental settings The experiments designed for this task are aimed at evaluating the features proposed in Section 2. We evaluate our CLTE approach using the English– Spanish data sets provided in the task 8 of SemEval 2012 (Negri et al., 2012). • Google Translate:4 an online MT system by Google Inc. • Microsoft Translator:5 an online MT system by Microsoft. External resources were also used for the extended features described in Section 2.2. We used the stemmer6 and the stopwords list provided by the SnowBall project for Spanish7 and English.8 Classifier. We used the implementation of support vector machine included in the WEKA v.3.6.6 data mining software package (Hall et al., 2009) for multiclass classification, and a polynomial kernel. 4 Results and discussion We tried the different features proposed in Section 2 in isolation, a"
S12-1065,P09-1034,0,0.041275,"Missing"
S12-1065,W07-1401,0,\N,Missing
W05-0818,1992.tmi-1.9,0,0.263753,"Missing"
W05-0818,W01-1406,0,0.0804179,"Missing"
W05-0818,W03-0301,0,0.0502943,"Missing"
W05-0818,P00-1056,0,0.0610365,"quality of training and reference corpora used in these experiments should be investigated. Then, the only conclusion that can be taken at this moment is that LIHLA, with its heuristics and/or default parameters, can not be indistinctly applied to any pair of languages. Despite of its performance, LIHLA has some 3 For more details of these experiments see (Caseli et al., accepted paper). Table 2: LIHLA results for Romanian–English 113 advantages when compared to other lexical alignment methods found in the literature, such as: it does not need to be trained for a new pair of languages (as in Och and Ney (2000)) and neither does it require pre-processing steps to handle texts (as in G´omez Guinovart and Sacau Fontenla (2004)). Furthermore, the whole alignment process (bilingual lexical generation and alignment itself) has proved to be very fast as mentioned previously. 4 Concluding remarks This paper has presented a lexical alignment method, LIHLA, which aligns words and multiword units based on initial statistical word-to-word correspondences and language-independent heuristics. In the experiments carried out at the “Shared task on word alignment” which took place at the Workshop on Building and Us"
W05-0818,ayan-etal-2004-multi,0,0.0658665,"Missing"
W05-0818,W01-0718,0,0.0688568,"Missing"
W05-0818,wu-wang-2004-improving-domain,0,0.0139391,"MT) (Somers, 1999) and statistical machine translation (SMT) (Ayan et al., 2004; Och and Ney, 2000), transfer rule learning (Carl, 2001; Menezes and Richardson, 2001), bilingual lexicography (G´omez Guinovart and Sacau Fontenla, 2004), and word sense disambiguation (Gale et al., 1992), among others. Aligning two (or more) texts means finding correspondences (translation equivalences) between segments (paragraphs, sentences, words, etc.) of the source text and segments of its translation (the target text). Following the same idea of many recently proposed approaches on lexical alignment (e.g., Wu and Wang (2004) and Ayan et al. (2004)), the method described in this paper, LIHLA (LanguageIndependent Heuristics Lexical Aligner) starts from 2 How LIHLA works As the first step, LIHLA uses alignments between single words defined in two bilingual lexicons (source–target and target–source) generated from sentence-aligned parallel texts using NATools.1 Given two sentence-aligned corpus files, the NATools word aligner —based on the Twenty-One system (Hiemstra, 1998)— counts the co-occurrences of words in all aligned sentence pairs and builds a sparse matrix of word-to-word probabilities (Model A) using an ite"
W05-0818,2001.mtsummit-ebmt.4,0,\N,Missing
W10-1720,P96-1041,0,0.070396,"M Target tokens 45M 2.7M 190M 1.6M 69M Table 1: Statistics of en–cs and en–es parallel data. Monolingual data: For language modeling purposes, in addition to the target parts of the bilingual data, we used the monolingual News corpus for cs; and the Gigaword corpus for es. For both languages, we used the SRILM toolkit (Stolcke, 2002) to train a 5-gram language model using all monolingual data provided. However, for en–es we used the IRSTLM toolkit (Federico and Cettolo, 2007) to train a 5-gram language model using the es Gigaword corpus. Both language models use modified Kneser-Ney smoothing (Chen and Goodman, 1996). Statistics for the monolingual corpora are given in Table 2. Corpus E/N/NC/UN Gigaword News Language es es cs Sentences 9,6M 40M 13M Tokens 290M 1,2G 210M Table 2: Statistics of Monolingual Data. E/N/NC/UN refers to Europarl/News/News Commentary/United Nations corpora. For all the systems except Apertium, we first lowercase and tokenize all the monolingual and bilingual data using the tools provided by the WMT10 organizers. After translation, system 145combination output is detokenised and true-cased. 3.2 English–Czech (en–cs) Experiments ˇ The CzEng corpus (Bojar and Zabokrtsk´ y, 2009) is"
W10-1720,J07-2003,0,0.0791111,"ngually chunking both source and target the N -best list generated by the combination modsides of the dataset using a marker-based chunker ule. Figure 1 illustrates the architecture. (Gough and Way, 2004); 3) Factored translation model (Koehn and Hoang, 2007); 4) Source-side 2.2 Example-Based Machine Translation context-informed (SSCI) systems (Stroppa et al., The EBMT system uses a language-specific, re2007); 5) the moses-chart (a Moses impleduced set of closed-class marker morphemes or mentation of the hierarchical phrase-based (HPB) lexemes (Gough and Way, 2004) to define a way approach of Chiang (2007)) and 6) Apertium (Forto segment sentences into chunks, which are then cada et al., 2009) rule-based machine translation aligned using an edit-distance-style algorithm, in (RBMT). Finally, we use a word-level combination framework (Rosti et al., 2007) to combine the 143which edit costs depend on word-to-word translaProceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 143–148, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics the above three translation factors: an SF to SF decoding path and a path which maps lemma to l"
W10-1720,W07-0712,0,0.00757178,"Corpus Langs. Sent. Europarl News-comm UN News-Comm CzEng en–es en–es en–es en–cs en–cs 1.6M 97k 5.9M 85k 7.8M Source tokens 43M 2.4M 160M 1.8M 80M Target tokens 45M 2.7M 190M 1.6M 69M Table 1: Statistics of en–cs and en–es parallel data. Monolingual data: For language modeling purposes, in addition to the target parts of the bilingual data, we used the monolingual News corpus for cs; and the Gigaword corpus for es. For both languages, we used the SRILM toolkit (Stolcke, 2002) to train a 5-gram language model using all monolingual data provided. However, for en–es we used the IRSTLM toolkit (Federico and Cettolo, 2007) to train a 5-gram language model using the es Gigaword corpus. Both language models use modified Kneser-Ney smoothing (Chen and Goodman, 1996). Statistics for the monolingual corpora are given in Table 2. Corpus E/N/NC/UN Gigaword News Language es es cs Sentences 9,6M 40M 13M Tokens 290M 1,2G 210M Table 2: Statistics of Monolingual Data. E/N/NC/UN refers to Europarl/News/News Commentary/United Nations corpora. For all the systems except Apertium, we first lowercase and tokenize all the monolingual and bilingual data using the tools provided by the WMT10 organizers. After translation, system 1"
W10-1720,2009.freeopmt-1.3,1,0.877177,"Missing"
W10-1720,2004.tmi-1.11,1,0.0884387,"English–Spanish (en– phrase-based and tree-based MT. es) and English–Czech (en–cs) translation The combination structure uses the MBR and tasks. For these two tasks, we employ several CN decoders, and is based on a word-level comindividual MT systems: 1) Baseline: phrasebination strategy (Du et al., 2009). In the final based SMT (Koehn et al., 2007); 2) EBMT: stage, we use a new rescoring module to process Monolingually chunking both source and target the N -best list generated by the combination modsides of the dataset using a marker-based chunker ule. Figure 1 illustrates the architecture. (Gough and Way, 2004); 3) Factored translation model (Koehn and Hoang, 2007); 4) Source-side 2.2 Example-Based Machine Translation context-informed (SSCI) systems (Stroppa et al., The EBMT system uses a language-specific, re2007); 5) the moses-chart (a Moses impleduced set of closed-class marker morphemes or mentation of the hierarchical phrase-based (HPB) lexemes (Gough and Way, 2004) to define a way approach of Chiang (2007)) and 6) Apertium (Forto segment sentences into chunks, which are then cada et al., 2009) rule-based machine translation aligned using an edit-distance-style algorithm, in (RBMT). Finally, we"
W10-1720,Y09-1019,1,0.818274,"oding paths based on 1 http://www.apertium.org (1) We use a memory-based machine learning (MBL) classifier (TRIBL:2 Daelemans and van den Bosch (2005)) that is able to estimate P (ˆ ek |fˆk , CI(fˆk )) by similarity-based reasoning over memorized nearest-neighbour examples of source–target phrase translations. In equation (1), SSCI may include any feature (lexical, syntactic, etc.), which can provide useful information to disambiguate a given source phrase. In addition to using local words and PoS-tags as features, as in (Stroppa et al., 2007), we incorporate grammatical dependency relations (Haque et al., 2009a) and supertags (Haque et al., 2009b) as syntactic source context features in the log-linear PB-SMT model. In addition to the above feature, we derived a ˆ best , defined in (2): simple binary feature h ( 1 if eˆk maximizes P (ˆ ek |fˆk , CI(fˆk )) 0 otherwise (2) We performed experiments by integrating these ˆ MBL and h ˆ best , directly into the two features, h log-linear framework of Moses. ˆ best = h 2.6 Hierarchical PB-SMT model For the en–cs translation task, we built a weighted synchronous context-free grammar model (Chiang, 2007) of translation that uses the bilingual phrase pairs of"
W10-1720,2009.eamt-1.32,1,0.0893869,"oding paths based on 1 http://www.apertium.org (1) We use a memory-based machine learning (MBL) classifier (TRIBL:2 Daelemans and van den Bosch (2005)) that is able to estimate P (ˆ ek |fˆk , CI(fˆk )) by similarity-based reasoning over memorized nearest-neighbour examples of source–target phrase translations. In equation (1), SSCI may include any feature (lexical, syntactic, etc.), which can provide useful information to disambiguate a given source phrase. In addition to using local words and PoS-tags as features, as in (Stroppa et al., 2007), we incorporate grammatical dependency relations (Haque et al., 2009a) and supertags (Haque et al., 2009b) as syntactic source context features in the log-linear PB-SMT model. In addition to the above feature, we derived a ˆ best , defined in (2): simple binary feature h ( 1 if eˆk maximizes P (ˆ ek |fˆk , CI(fˆk )) 0 otherwise (2) We performed experiments by integrating these ˆ MBL and h ˆ best , directly into the two features, h log-linear framework of Moses. ˆ best = h 2.6 Hierarchical PB-SMT model For the en–cs translation task, we built a weighted synchronous context-free grammar model (Chiang, 2007) of translation that uses the bilingual phrase pairs of"
W10-1720,W04-3250,0,0.127192,"Missing"
W10-1720,2005.mtsummit-papers.11,0,0.0279389,"gth posterior probability (Zens and Ney, 2006); • N -gram posterior probabilities within the N Best list (Zens and Ney, 2006); • Minimum Bayes Risk probability; • Length ratio between source and target sentence; The weights are optimized via MERT. This section describes our experimental setup for the en–cs and en–es translation tasks. 3.1 Data Bilingual data: In the experiments we used data sets provided by the workshop organizers. For the en–cs translation table extraction we employed both parallel corpora (News-Commentary10 and CzEng 0.9), and for the en–es experiments, we used the Europarl(Koehn, 2005), News Commentary and United Nations parallel data. We used a maximum sentence length of 80 for en–es and 40 for en–cs. Detailed statistics are shown in Table 1. Corpus Langs. Sent. Europarl News-comm UN News-Comm CzEng en–es en–es en–es en–cs en–cs 1.6M 97k 5.9M 85k 7.8M Source tokens 43M 2.4M 160M 1.8M 80M Target tokens 45M 2.7M 190M 1.6M 69M Table 1: Statistics of en–cs and en–es parallel data. Monolingual data: For language modeling purposes, in addition to the target parts of the bilingual data, we used the monolingual News corpus for cs; and the Gigaword corpus for es. For both languages"
W10-1720,P07-2045,0,0.0171046,"Examples). This system exploits example-based pects of both the EBMT and SMT paradigms. MT, statistical MT (SMT), and system combinaThe architecture includes various individual systion techniques. tems: phrase-based, example-based, hierarchical We participated in the English–Spanish (en– phrase-based and tree-based MT. es) and English–Czech (en–cs) translation The combination structure uses the MBR and tasks. For these two tasks, we employ several CN decoders, and is based on a word-level comindividual MT systems: 1) Baseline: phrasebination strategy (Du et al., 2009). In the final based SMT (Koehn et al., 2007); 2) EBMT: stage, we use a new rescoring module to process Monolingually chunking both source and target the N -best list generated by the combination modsides of the dataset using a marker-based chunker ule. Figure 1 illustrates the architecture. (Gough and Way, 2004); 3) Factored translation model (Koehn and Hoang, 2007); 4) Source-side 2.2 Example-Based Machine Translation context-informed (SSCI) systems (Stroppa et al., The EBMT system uses a language-specific, re2007); 5) the moses-chart (a Moses impleduced set of closed-class marker morphemes or mentation of the hierarchical phrase-based"
W10-1720,N04-1022,0,0.0614202,"ribe the modular design of our multi-engine machine translation (MT) system with particular focus on the components used in this participation. We participated in the English– Spanish and English–Czech translation tasks, in which we employed our multiengine architecture to translate. We also participated in the system combination task which was carried out by the MBR decoder and confusion network decoder. 1 Introduction multiple translation hypotheses and employ a new rescoring model to generate the final translation. For the system combination task, we first use the minimum Bayes-risk (MBR) (Kumar and Byrne, 2004) decoder to select the best hypothesis as the alignment reference for the confusion network (CN) (Mangu et al., 2000). We then build the CN using the TER metric (Snover et al., 2006), and finally search for the best translation. The remainder of this paper is organised as follows: Section 2 details the various components of our system, in particular the multi-engine strategies used for the shared task. In Section 3, we outline the complete system setup for the shared task and provide evaluation results on the test set. Section 4 concludes the paper. 2 The M AT R E X System 2.1 System Architect"
W10-1720,P03-1021,0,0.0273052,"output of each individual system. The CN is built by aligning other hypotheses against the backbone, based on the TER metric. Null words are allowed in the alignment. Either votes or different confidence measures are assigned to each word in the network. Each arc in the CN represents an alternative word at that position in the sentence and the number of votes for each word is counted when constructing the network. The features we used are as follows: • • • • word posterior probability (Fiscus, 1997); 3, 4-gram target language model; word length penalty; Null word length penalty; We use MERT (Och, 2003) to tune the weights of the CN. 2.8 Rescoring Rescoring is a very important part in postprocessing which can select a better hypothesis from the N -best list. We augmented our previous rescoring model (Du et al., 2009) with more large-scale data. The features we used include: • Direct and inverse IBM model; • 3, 4-gram target language model; • 3, 4, 5-gram PoS language model (Schmid, 1994; Ratnaparkhi, 1996); • Sentence length posterior probability (Zens and Ney, 2006); • N -gram posterior probabilities within the N Best list (Zens and Ney, 2006); • Minimum Bayes Risk probability; • Length rat"
W10-1720,P02-1038,0,0.00732471,"xtracted by the baseline system adding word alignment information. 2.3 Apertium RBMT Apertium1 is a free/open-source platform for RBMT. The current version of the en–es system in Apertium was used for the system combination task (section 2.7), and its morphological analysers and part-of-speech taggers were used to build a factored Moses model. 2.4 Factored Translation Model We also used a factored model for the en–es translation task. Factored models (Koehn and Hoang, 2007) facilitate the translation by breaking it down into several factors which are further combined using a log-linear model (Och and Ney, 2002). We used three factors in our factored translation model, which are used in two different decoding paths: a surface form (SF) to SF translation factor, a lemma to lemma translation factor, and a part-ofspeech (PoS) to PoS translation factor. Finally, we used two decoding paths based on 1 http://www.apertium.org (1) We use a memory-based machine learning (MBL) classifier (TRIBL:2 Daelemans and van den Bosch (2005)) that is able to estimate P (ˆ ek |fˆk , CI(fˆk )) by similarity-based reasoning over memorized nearest-neighbour examples of source–target phrase translations. In equation (1), SSCI"
W10-1720,P02-1040,0,0.113346,"Missing"
W10-1720,2006.amta-papers.25,0,0.0454791,"and English–Czech translation tasks, in which we employed our multiengine architecture to translate. We also participated in the system combination task which was carried out by the MBR decoder and confusion network decoder. 1 Introduction multiple translation hypotheses and employ a new rescoring model to generate the final translation. For the system combination task, we first use the minimum Bayes-risk (MBR) (Kumar and Byrne, 2004) decoder to select the best hypothesis as the alignment reference for the confusion network (CN) (Mangu et al., 2000). We then build the CN using the TER metric (Snover et al., 2006), and finally search for the best translation. The remainder of this paper is organised as follows: Section 2 details the various components of our system, in particular the multi-engine strategies used for the shared task. In Section 3, we outline the complete system setup for the shared task and provide evaluation results on the test set. Section 4 concludes the paper. 2 The M AT R E X System 2.1 System Architecture In this paper, we present the DCU multi-engine The M AT R E X system is a combination-based MT system M AT R E X (Machine Translation using multi-engine architecture, which explo"
W10-1720,2007.tmi-papers.28,1,0.632786,"Missing"
W10-1720,2006.iwslt-evaluation.4,1,0.38824,"tion factors: an SF to SF decoding path and a path which maps lemma to lemma, PoS to PoS, and an SF generated using the TL lemma and PoS. The lemmas and PoS for en and es were obtained using Apertium (section 2.3). 2.5 Source-Side Context-informed PB-SMT One natural way to express a context-informed ˆ MBL ) is to view it as the conditional feature (h probability of the target phrases (ˆ ek ) given the ˆ source phrase (fk ) and its source-side context information (CI): ˆ MBL = log P (ˆ h ek |fˆk , CI(fˆk )) Figure 1: System Framework. tion probabilities and the amount of word-to-word cognates (Stroppa and Way, 2006). Once these phrase pairs were obtained they were merged with the phrase pairs extracted by the baseline system adding word alignment information. 2.3 Apertium RBMT Apertium1 is a free/open-source platform for RBMT. The current version of the en–es system in Apertium was used for the system combination task (section 2.7), and its morphological analysers and part-of-speech taggers were used to build a factored Moses model. 2.4 Factored Translation Model We also used a factored model for the en–es translation task. Factored models (Koehn and Hoang, 2007) facilitate the translation by breaking it"
W10-1720,W06-3110,0,0.03498,"word posterior probability (Fiscus, 1997); 3, 4-gram target language model; word length penalty; Null word length penalty; We use MERT (Och, 2003) to tune the weights of the CN. 2.8 Rescoring Rescoring is a very important part in postprocessing which can select a better hypothesis from the N -best list. We augmented our previous rescoring model (Du et al., 2009) with more large-scale data. The features we used include: • Direct and inverse IBM model; • 3, 4-gram target language model; • 3, 4, 5-gram PoS language model (Schmid, 1994; Ratnaparkhi, 1996); • Sentence length posterior probability (Zens and Ney, 2006); • N -gram posterior probabilities within the N Best list (Zens and Ney, 2006); • Minimum Bayes Risk probability; • Length ratio between source and target sentence; The weights are optimized via MERT. This section describes our experimental setup for the en–cs and en–es translation tasks. 3.1 Data Bilingual data: In the experiments we used data sets provided by the workshop organizers. For the en–cs translation table extraction we employed both parallel corpora (News-Commentary10 and CzEng 0.9), and for the en–es experiments, we used the Europarl(Koehn, 2005), News Commentary and United Natio"
W10-1720,W96-0213,0,0.0699016,"ng the network. The features we used are as follows: • • • • word posterior probability (Fiscus, 1997); 3, 4-gram target language model; word length penalty; Null word length penalty; We use MERT (Och, 2003) to tune the weights of the CN. 2.8 Rescoring Rescoring is a very important part in postprocessing which can select a better hypothesis from the N -best list. We augmented our previous rescoring model (Du et al., 2009) with more large-scale data. The features we used include: • Direct and inverse IBM model; • 3, 4-gram target language model; • 3, 4, 5-gram PoS language model (Schmid, 1994; Ratnaparkhi, 1996); • Sentence length posterior probability (Zens and Ney, 2006); • N -gram posterior probabilities within the N Best list (Zens and Ney, 2006); • Minimum Bayes Risk probability; • Length ratio between source and target sentence; The weights are optimized via MERT. This section describes our experimental setup for the en–cs and en–es translation tasks. 3.1 Data Bilingual data: In the experiments we used data sets provided by the workshop organizers. For the en–cs translation table extraction we employed both parallel corpora (News-Commentary10 and CzEng 0.9), and for the en–es experiments, we us"
W10-1720,N07-1029,0,0.00800063,"d Hoang, 2007); 4) Source-side 2.2 Example-Based Machine Translation context-informed (SSCI) systems (Stroppa et al., The EBMT system uses a language-specific, re2007); 5) the moses-chart (a Moses impleduced set of closed-class marker morphemes or mentation of the hierarchical phrase-based (HPB) lexemes (Gough and Way, 2004) to define a way approach of Chiang (2007)) and 6) Apertium (Forto segment sentences into chunks, which are then cada et al., 2009) rule-based machine translation aligned using an edit-distance-style algorithm, in (RBMT). Finally, we use a word-level combination framework (Rosti et al., 2007) to combine the 143which edit costs depend on word-to-word translaProceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 143–148, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics the above three translation factors: an SF to SF decoding path and a path which maps lemma to lemma, PoS to PoS, and an SF generated using the TL lemma and PoS. The lemmas and PoS for en and es were obtained using Apertium (section 2.3). 2.5 Source-Side Context-informed PB-SMT One natural way to express a context-informed ˆ MBL ) is to view it"
W10-1720,D07-1091,0,\N,Missing
W10-1720,2003.mtsummit-systems.3,0,\N,Missing
W10-1720,W10-1742,1,\N,Missing
W14-0309,langlais-etal-2000-evaluation,0,0.765651,"Missing"
W14-0309,macklovitch-2006-transtype2,0,0.707621,"Missing"
W14-0309,P11-4012,0,0.0263647,"Missing"
W14-0309,J09-1002,0,0.237084,"Missing"
W14-0309,2005.eamt-1.6,0,0.0377888,"is the fact that the underlying translation engine needs to be a glass-box resource, that is, a resource whose behaviour is modified to meet the ITP system needs. The approaches rely on a statistical MT (Koehn, 2010) system which is adapted to provide the list of n-best completions for the remainder of the sentence, given the current sentence prefix already introduced by the user; in order to meet the resulting time constraints, the decoder of the statistical MT system cannot be executed after each keystroke and techniques to compute the search graph once and then reuse it have been proposed (Bender et al., 2005). However, it 3 In the case of touch devices, other means of interaction (such as gestures) may exist. 4 58 http://www.translationzone.com/ gestions on the fly in a drop-down list with items based on the current prefix, although this prefix will correspond to the first characters of the word currently being typed and not to the part of the target sentence already entered; users may accept these suggestions (using cursor keys, the mouse or specific hot keys) or ignore them and continue typing. A screenshot of the interface is shown in Figure 1. Despite the cognitive load inherent to any predict"
W14-0309,steinberger-etal-2012-dgt,0,0.0714984,"Missing"
W14-0309,1997.mtsummit-papers.1,0,0.918142,", 2003) to be systems that produce a first (and usually incorrect) prototype of the translation which is then edited by the human translator in order to produce a target-language text that is adequate for publishing. In both situations, the suggestion may be considered as a source of inspiration by the human translators, who will assemble the final translation by on some occasions accepting and re1 The name interactive translation prediction has recently been proposed (Alabau et al., 2013) for this research field, which has historically been referred to as target-text mediated interactive MT (Foster et al., 1997) or simply interactive MT (Barrachina et al., 2009). Despite the traditional term, we consider the recent one to be more suitable for our approach since it is not exclusively based on MT. 2 The interaction can be compared to that of word completion mechanisms in input text boxes and word processors. 57 Workshop on Humans and Computer-assisted Translation, pages 57–65, c Gothenburg, Sweden, 26 April 2014. 2014 Association for Computational Linguistics this paper we shall explore the use of an MT system, but they may also consist of translation memories, dictionaries, catalogues of bilingual phr"
W14-0309,2009.mtsummit-papers.8,0,0.0327809,"Missing"
W14-0309,P09-4005,0,0.350506,"Missing"
W14-0309,J10-4005,0,0.0627225,"Missing"
W15-3036,C04-1046,0,0.174452,"te) and the bilingual concordancer Reverso Context. After obtaining the subsegment correspondences, a collection of features is extracted from them, which are then used by a binary classifer to obtain the final “GOOD” or “BAD” word-level quality labels. We prepared two submissions for this year’s edition of WMT 2015: one using the features produced by our system, and one combining them with the baseline features published by the organisers of the task, which were ranked third and first for the sub-task, respectively. 1 translation for dissemination. Consequently, MT quality estimation (MTQE) (Blatz et al., 2004; Specia et al., 2010; Specia and Soricut, 2013) has emerged as a mean to minimise the post-editing effort by developing techniques that allow to estimate the quality of the translation hypotheses produced by an MT system. In order to boost the scientific efforts on this problem, the WMT 2015 MTQE shared task proposes three tasks that allow to compare different approaches at three different levels: segment-level (sub-task 1), word-level (sub-task 2), and document-level (sub-task 3). Our submissions tackle the word-level MTQE sub-task, which proposes a framework for evaluating and comparing dif"
W15-3036,S12-1065,1,0.85218,"Missing"
W15-3036,W15-4903,1,0.387153,"Missing"
W15-3036,2013.tc-1.10,0,0.0629579,"o Context. After obtaining the subsegment correspondences, a collection of features is extracted from them, which are then used by a binary classifer to obtain the final “GOOD” or “BAD” word-level quality labels. We prepared two submissions for this year’s edition of WMT 2015: one using the features produced by our system, and one combining them with the baseline features published by the organisers of the task, which were ranked third and first for the sub-task, respectively. 1 translation for dissemination. Consequently, MT quality estimation (MTQE) (Blatz et al., 2004; Specia et al., 2010; Specia and Soricut, 2013) has emerged as a mean to minimise the post-editing effort by developing techniques that allow to estimate the quality of the translation hypotheses produced by an MT system. In order to boost the scientific efforts on this problem, the WMT 2015 MTQE shared task proposes three tasks that allow to compare different approaches at three different levels: segment-level (sub-task 1), word-level (sub-task 2), and document-level (sub-task 3). Our submissions tackle the word-level MTQE sub-task, which proposes a framework for evaluating and comparing different approaches. This year, the sub-task used"
W15-3036,2015.eamt-1.4,1,\N,Missing
W15-4903,W14-3339,0,0.11879,"Missing"
W15-4903,W13-2242,0,0.24993,"Missing"
W15-4903,W11-2131,0,0.0732552,"Missing"
W15-4903,C04-1046,0,0.168128,"ﬁeld of machine translation (MT) have led to the adoption of this technology by many companies and institutions all around the world in order to bypass the linguistic barriers and reach out to broader audiences. Unfortunately, we are still far from the point of having MT systems able to produce translations with the level of quality required for dissemination in formal scenarios, where human supervision and MT post-editing are unavoidable. It therefore becomes critical to minimise the cost of this human post-editing. This has motivated a growing interest in the ﬁeld of MT quality estimation (Blatz et al., 2004; Specia et al., 2010; Specia and Soricut, 2013), which is the ﬁeld that focuses on developing techniques that allow to estimate the quality of the translation hypotheses produced by an MT system. Most efforts in MT quality estimation (MTQE) are aimed at evaluating the quality of whole translated segments, in terms of post-editing time, number of editions needed, and other related metrics (Blatz et al., 2004). Our work is focused on the sub-ﬁeld of word-level MTQE. The main advantage of word-level MTQE is that it allows not only to estimate the effort needed to post-edit the output of an MT sy"
W15-4903,J93-2003,0,0.0449269,"ypothesis T of the SL sentence S to help the interactive MT system to choose the translation suggestions to be made to the user. Uefﬁng and Ney (2005) extend this application to word-level MTQE also to automatically reject those target words t with low conﬁdence scores from the translation proposals. This second approach incorporates the use of probabilistic lexicons as a source of translation information. Blatz et al. (2003) introduce a more complex collection of features for word-level MTQE, using semantic features based on WordNet (Miller, 1995), translation probabilities from IBM model 1 (Brown et al., 1993), word posterior probabilities (Blatz et al., 2003), and alignment templates from statistical MT (SMT) models. All the features they use are combined to train a binary classiﬁer which is used to determine the conﬁdence scores. Uefﬁng and Ney (2007) divide the features used 20 by their approach in two types: those which are independent of the MT system used for translation (system-independent), and those which are extracted from internal data of the SMT system they use for translation (system-dependent). These features are obtained by comparing the output of an SMT system T1 to a collection of"
W15-4903,W14-3340,0,0.0649819,"Missing"
W15-4903,2011.mtsummit-papers.18,1,0.897823,"Missing"
W15-4903,W03-0413,0,0.113451,"the ﬂy for new translations. The rest of the paper is organised as follows. Section 2 brieﬂy reviews the state of the art in word-level MTQE. Section 3 describes our binaryclassiﬁcation approach, the sources of information, and the collection of features used. Section 4 describes the experimental setting used for our experiments, whereas Section 5 reports and discusses the results obtained. The paper ends with some concluding remarks and the description of ongoing and possible future work. 2 Related work Some of the early work on word-level MTQE can be found in the context of interactive MT (Gandrabur and Foster, 2003; Uefﬁng and Ney, 2005). Gandrabur and Foster (2003) obtain conﬁdence scores for each word t in a given translation hypothesis T of the SL sentence S to help the interactive MT system to choose the translation suggestions to be made to the user. Uefﬁng and Ney (2005) extend this application to word-level MTQE also to automatically reject those target words t with low conﬁdence scores from the translation proposals. This second approach incorporates the use of probabilistic lexicons as a source of translation information. Blatz et al. (2003) introduce a more complex collection of features for w"
W15-4903,2013.tc-1.10,0,0.0690461,"d to the adoption of this technology by many companies and institutions all around the world in order to bypass the linguistic barriers and reach out to broader audiences. Unfortunately, we are still far from the point of having MT systems able to produce translations with the level of quality required for dissemination in formal scenarios, where human supervision and MT post-editing are unavoidable. It therefore becomes critical to minimise the cost of this human post-editing. This has motivated a growing interest in the ﬁeld of MT quality estimation (Blatz et al., 2004; Specia et al., 2010; Specia and Soricut, 2013), which is the ﬁeld that focuses on developing techniques that allow to estimate the quality of the translation hypotheses produced by an MT system. Most efforts in MT quality estimation (MTQE) are aimed at evaluating the quality of whole translated segments, in terms of post-editing time, number of editions needed, and other related metrics (Blatz et al., 2004). Our work is focused on the sub-ﬁeld of word-level MTQE. The main advantage of word-level MTQE is that it allows not only to estimate the effort needed to post-edit the output of an MT system, but also to guide post-editors on which wo"
W15-4903,P13-4014,0,0.0715472,"Missing"
W15-4903,2005.eamt-1.35,0,0.057781,". The rest of the paper is organised as follows. Section 2 brieﬂy reviews the state of the art in word-level MTQE. Section 3 describes our binaryclassiﬁcation approach, the sources of information, and the collection of features used. Section 4 describes the experimental setting used for our experiments, whereas Section 5 reports and discusses the results obtained. The paper ends with some concluding remarks and the description of ongoing and possible future work. 2 Related work Some of the early work on word-level MTQE can be found in the context of interactive MT (Gandrabur and Foster, 2003; Uefﬁng and Ney, 2005). Gandrabur and Foster (2003) obtain conﬁdence scores for each word t in a given translation hypothesis T of the SL sentence S to help the interactive MT system to choose the translation suggestions to be made to the user. Uefﬁng and Ney (2005) extend this application to word-level MTQE also to automatically reject those target words t with low conﬁdence scores from the translation proposals. This second approach incorporates the use of probabilistic lexicons as a source of translation information. Blatz et al. (2003) introduce a more complex collection of features for word-level MTQE, using s"
W15-4903,J07-1003,0,0.0732554,"th low conﬁdence scores from the translation proposals. This second approach incorporates the use of probabilistic lexicons as a source of translation information. Blatz et al. (2003) introduce a more complex collection of features for word-level MTQE, using semantic features based on WordNet (Miller, 1995), translation probabilities from IBM model 1 (Brown et al., 1993), word posterior probabilities (Blatz et al., 2003), and alignment templates from statistical MT (SMT) models. All the features they use are combined to train a binary classiﬁer which is used to determine the conﬁdence scores. Uefﬁng and Ney (2007) divide the features used 20 by their approach in two types: those which are independent of the MT system used for translation (system-independent), and those which are extracted from internal data of the SMT system they use for translation (system-dependent). These features are obtained by comparing the output of an SMT system T1 to a collection of alternative T translations {Ti }N i=2 obtained by using the N -best list from the same SMT system. Several distance metrics are then used to check how often word tj , the word in position j of T , is found in each translation alternative Ti , and h"
W15-4903,W14-3302,0,\N,Missing
W15-4904,C04-1046,0,0.309248,"censed under a Creative � Commons 3.0 licence, no derivative works, attribution, CCBY-ND. 27 Machine translation: You could, for instance, use machine translation (MT) to get a draft of the translation of each segment, MT(si ); vendors and experts tell you that you will save effort by postediting MT(si ) into your desired translation ti .1 Machine translation output MT(si ) may just be text, but it could come with annotations to help you make the most of it; for instance, words could be color-coded according to how conﬁdent the system is about them (Uefﬁng and Ney, 2007; Uefﬁng and Ney, 2005; Blatz et al., 2004), or unknown words that come out untranslated may be marked so that you spot them clearly. Machine-translated segments could even be accompanied by indicators of their estimated quality (Specia and Soricut, 2013; Specia et al., 2010; Blatz et al., 2004) which may be used to ascertain whether the output of the MT system is worth being post-edited or not. If someone measured your post-editing effort (in time, in number of keystrokes, in number of words changed, in money you would have to pay another translator to do it, etc.), when turning MT(si ) into ti , they could call that effort eMT i . Tr"
W15-4904,2011.eamt-1.28,1,0.853803,"sually as a percentage called fuzzy match score that accounts for the amount of text that is common to both segments2 — and even marks for you the words in s�i that do not match those in si . Let’s call all this information TM(si ): your job is to use it to turn t�i into the ﬁnal translation ti . If the fuzzy match is good, you will spend less effort than if you started from scratch. Let us call eTM the effort to turn the t�i provided by i TM(si ) into the desired translation ti .3 Mixing them up: You could even have available another technology, fuzzy-match repair (FMR; (Ortega et al., 2014; Dandapat et al., 2011; Hewavitharana et al., 2005; Kranias and Samiotou, 2004)), that integrates the two technologies just mentioned: after a suitable fuzzy match is found, machine translation (or another source of bilingual information) is used to repair, i.e. edit some parts of t�i , to take into account what changes from s�i to si to try to save even more effort; it tells you all that TM(si ) tells you, but also marks the parts that have been repaired. Fuzzy-match repair is one of the technologies that TAUS, the Translation Automation User Society, calls advanced leveraging;4 commercial examples of these are De"
W15-4904,W10-1751,0,0.122711,"Missing"
W15-4904,2012.amta-papers.6,0,0.0132858,"l., 2002), using reference translations in a development set. Most of these automatic 9 The measurements of effort that one can ﬁnd in literature vary from simple scores for “perceived” post-editing effort (usually scores taking 3 or 4 values) to actual post-editing time (see, for instance, the quality estimation task in WMT 2014 (Bojar et al., 2014)) measures are measures of similarity (or dissimilarity) between raw and reference translations. Researchers hope that their use during tuning will lead to a reduction in translation effort, although this is not currently guaranteed —for instance, Denkowski and Lavie (2012) found that BLEU could not distinguish between raw and post-edited machine translation. Generally, an automatic evaluation measure for technology X may have the form i i eˆX (X(si ; �λX ), {tij }nj=1 ;µ � X ), where {tij }nj=1 is the set of reference translations for segment si in the development set and µ � X is a set of tunable X i ;µ �X) parameters. Ideally, eˆ (X(si ; �λX ), {tij }nj=1 should approximate eX (X(si ; �λX )), but tuning of µ � X is surprisingly absent from current MT practice (with some exceptions, see Denkowski and Lavie i (2010)). In fact, eˆX (X(si ; �λX ), {tij }nj=1 ;µ �"
W15-4904,P10-1064,0,0.170309,"Missing"
W15-4904,2010.amta-papers.27,0,0.0353888,"Missing"
W15-4904,2005.eamt-1.18,0,0.0159116,"called fuzzy match score that accounts for the amount of text that is common to both segments2 — and even marks for you the words in s�i that do not match those in si . Let’s call all this information TM(si ): your job is to use it to turn t�i into the ﬁnal translation ti . If the fuzzy match is good, you will spend less effort than if you started from scratch. Let us call eTM the effort to turn the t�i provided by i TM(si ) into the desired translation ti .3 Mixing them up: You could even have available another technology, fuzzy-match repair (FMR; (Ortega et al., 2014; Dandapat et al., 2011; Hewavitharana et al., 2005; Kranias and Samiotou, 2004)), that integrates the two technologies just mentioned: after a suitable fuzzy match is found, machine translation (or another source of bilingual information) is used to repair, i.e. edit some parts of t�i , to take into account what changes from s�i to si to try to save even more effort; it tells you all that TM(si ) tells you, but also marks the parts that have been repaired. Fuzzy-match repair is one of the technologies that TAUS, the Translation Automation User Society, calls advanced leveraging;4 commercial examples of these are DeepMiner in Atril’s D´ej`a Vu"
W15-4904,J10-4005,0,0.0421896,"d on a development set made of bilingual segments and translation effort measurements eMT (MT(si ; �λMT )).9 X� where ei i is the effort expended in translating segment si using the best technology Xi� for that segment, that is, the one that minimizes that effort. To minimize the translation effort on a speciﬁc task, designers have to work in two main areas: Improving each technology: One is to improve the output of each technology X, ideally focusing on those cases when X is going to be selected. Some such technologies have tunable parameters; for instance, feature weights in statistical MT (Koehn, 2010, p. 255); for other technologies, this is not usually reported, but it is not impossible to think, for instance, of fuzzy-match scores that give different weights to different kinds of edit operations. Let us call �λX the vector of tunable parameters for technology X; as the output of technology X varies with these parameters, we can write its output like this: X(si ; �λX ). Learning to select the best technology: The other one is that the CAT environment needs a way to select the best technology Xi� for each segment si , obviously without measuring the actual effort. To do this, CAT designer"
W15-4904,kranias-samiotou-2004-automatic,0,0.46605,"t accounts for the amount of text that is common to both segments2 — and even marks for you the words in s�i that do not match those in si . Let’s call all this information TM(si ): your job is to use it to turn t�i into the ﬁnal translation ti . If the fuzzy match is good, you will spend less effort than if you started from scratch. Let us call eTM the effort to turn the t�i provided by i TM(si ) into the desired translation ti .3 Mixing them up: You could even have available another technology, fuzzy-match repair (FMR; (Ortega et al., 2014; Dandapat et al., 2011; Hewavitharana et al., 2005; Kranias and Samiotou, 2004)), that integrates the two technologies just mentioned: after a suitable fuzzy match is found, machine translation (or another source of bilingual information) is used to repair, i.e. edit some parts of t�i , to take into account what changes from s�i to si to try to save even more effort; it tells you all that TM(si ) tells you, but also marks the parts that have been repaired. Fuzzy-match repair is one of the technologies that TAUS, the Translation Automation User Society, calls advanced leveraging;4 commercial examples of these are DeepMiner in Atril’s D´ej`a Vu,5 and ALTM in MultiCorpora’s"
W15-4904,P03-1021,0,0.330667,"nologies and for all segments. Therefore it is in principle not easy to determine the parameters θ�X to get good estimates e˜X (X(si ; �λX ); θ�X ). We will see a way to do this below. Tuning technologies is also hard: Technologies may have tunable parameters �λX which determine the output they produce. Obviously, one cannot just repetitively measure the actual effort spent by translators in editing their output for a wide variety of values of �λX , as this is clearly impracticable; therefore, an alternative is needed. When X = MT, this is usually done by means of an algorithm that optimizes (Och, 2003; Chiang, 2012) automatic evaluation measures, such as BLEU (Papineni et al., 2002), using reference translations in a development set. Most of these automatic 9 The measurements of effort that one can ﬁnd in literature vary from simple scores for “perceived” post-editing effort (usually scores taking 3 or 4 values) to actual post-editing time (see, for instance, the quality estimation task in WMT 2014 (Bojar et al., 2014)) measures are measures of similarity (or dissimilarity) between raw and reference translations. Researchers hope that their use during tuning will lead to a reduction in tra"
W15-4904,2014.amta-researchers.4,1,0.842404,"Missing"
W15-4904,P02-1040,0,0.109824,"o determine the parameters θ�X to get good estimates e˜X (X(si ; �λX ); θ�X ). We will see a way to do this below. Tuning technologies is also hard: Technologies may have tunable parameters �λX which determine the output they produce. Obviously, one cannot just repetitively measure the actual effort spent by translators in editing their output for a wide variety of values of �λX , as this is clearly impracticable; therefore, an alternative is needed. When X = MT, this is usually done by means of an algorithm that optimizes (Och, 2003; Chiang, 2012) automatic evaluation measures, such as BLEU (Papineni et al., 2002), using reference translations in a development set. Most of these automatic 9 The measurements of effort that one can ﬁnd in literature vary from simple scores for “perceived” post-editing effort (usually scores taking 3 or 4 values) to actual post-editing time (see, for instance, the quality estimation task in WMT 2014 (Bojar et al., 2014)) measures are measures of similarity (or dissimilarity) between raw and reference translations. Researchers hope that their use during tuning will lead to a reduction in translation effort, although this is not currently guaranteed —for instance, Denkowski"
W15-4904,2013.mtsummit-papers.21,0,0.0142137,"egment si in the development set and µ � X is a set of tunable X i ;µ �X) parameters. Ideally, eˆ (X(si ; �λX ), {tij }nj=1 should approximate eX (X(si ; �λX )), but tuning of µ � X is surprisingly absent from current MT practice (with some exceptions, see Denkowski and Lavie i (2010)). In fact, eˆX (X(si ; �λX ), {tij }nj=1 ;µ � X ) can be seen as a special estimator of effort, much like e˜X (X(si ; �λX ); θ�X ), but informed with reference i translations {tij }nj=1 when they are available. This is similar to the use of pseudo-reference translations in machine translation quality estimation (Shah et al., 2013; Soricut and Narsale, 2012; Soricut et al., 2012; Soricut and Echihabi, 2010), but with actual references. Table 1 summarizes the main concepts and the notation used along the paper. 3 A general framework for training the whole CAT environment We describe a possible workﬂow to tune simultaneously the different technologies that may be used in a CAT environment and the estimators used to select them on a segment basis: 1. Design automatic evaluation measures eˆX (X(si , �λX ), {tij }; µ � X ) and estimators X X X e˜ (X(si ; �λ ); θ� ) for each technology X ∈ X , based on a series of relevant f"
W15-4904,2009.mtsummit-papers.14,0,0.192959,"even better, couldn’t the decision of selecting the best technology Xi� , that is, the one that minimizes your effort for each segment si , be made automatically? It is therefore clear that a framework that allows to seamlessly integrate all the translation technologies available in the CAT system is very much needed to make the most of all of them and minimize translation effort as much as possible. Previous work on technology selection: The speciﬁc case of automatically choosing between machine translation output and translation memory fuzzy matches has received attention in the last years. Simard and Isabelle (2009) proposed a simple approach called β-combination, which simply selects machine translation when there is no translation memory proposal with a fuzzy match score above a given threshold β, which can be tuned. He et al. (2010a) and He et al. (2010b) approach this problem, which they call translation recommendation, by training a classiﬁer which selects which of the two, TM(si ) or MT(si ), gets the lowest value for an approximate indicator of effort, called translation error rate (TER, (Snover et al., 2006)). Their training compares outputs to preexisting reference translations; their ideas are"
W15-4904,2006.amta-papers.25,0,0.102842,"tput and translation memory fuzzy matches has received attention in the last years. Simard and Isabelle (2009) proposed a simple approach called β-combination, which simply selects machine translation when there is no translation memory proposal with a fuzzy match score above a given threshold β, which can be tuned. He et al. (2010a) and He et al. (2010b) approach this problem, which they call translation recommendation, by training a classiﬁer which selects which of the two, TM(si ) or MT(si ), gets the lowest value for an approximate indicator of effort, called translation error rate (TER, (Snover et al., 2006)). Their training compares outputs to preexisting reference translations; their ideas are generalized in the approach proposed in this paper. The next section explains two ways to minimize the effort needed to perform a translation job in a CAT environment integrating different technologies. Section 3 then describes our proposal for a general framework for training the whole CAT environment. Finally, we discuss the implications of having such a framework. 2 Minimizing translation effort to come up with a set of estimators e˜X , one for each technology. These estimators should be trained to giv"
W15-4904,P10-1063,0,0.0247347,"µ �X) parameters. Ideally, eˆ (X(si ; �λX ), {tij }nj=1 should approximate eX (X(si ; �λX )), but tuning of µ � X is surprisingly absent from current MT practice (with some exceptions, see Denkowski and Lavie i (2010)). In fact, eˆX (X(si ; �λX ), {tij }nj=1 ;µ � X ) can be seen as a special estimator of effort, much like e˜X (X(si ; �λX ); θ�X ), but informed with reference i translations {tij }nj=1 when they are available. This is similar to the use of pseudo-reference translations in machine translation quality estimation (Shah et al., 2013; Soricut and Narsale, 2012; Soricut et al., 2012; Soricut and Echihabi, 2010), but with actual references. Table 1 summarizes the main concepts and the notation used along the paper. 3 A general framework for training the whole CAT environment We describe a possible workﬂow to tune simultaneously the different technologies that may be used in a CAT environment and the estimators used to select them on a segment basis: 1. Design automatic evaluation measures eˆX (X(si , �λX ), {tij }; µ � X ) and estimators X X X e˜ (X(si ; �λ ); θ� ) for each technology X ∈ X , based on a series of relevant features that can easily be extracted from si and X(si ), and which will depend"
W15-4904,W12-3121,0,0.0184337,"velopment set and µ � X is a set of tunable X i ;µ �X) parameters. Ideally, eˆ (X(si ; �λX ), {tij }nj=1 should approximate eX (X(si ; �λX )), but tuning of µ � X is surprisingly absent from current MT practice (with some exceptions, see Denkowski and Lavie i (2010)). In fact, eˆX (X(si ; �λX ), {tij }nj=1 ;µ � X ) can be seen as a special estimator of effort, much like e˜X (X(si ; �λX ); θ�X ), but informed with reference i translations {tij }nj=1 when they are available. This is similar to the use of pseudo-reference translations in machine translation quality estimation (Shah et al., 2013; Soricut and Narsale, 2012; Soricut et al., 2012; Soricut and Echihabi, 2010), but with actual references. Table 1 summarizes the main concepts and the notation used along the paper. 3 A general framework for training the whole CAT environment We describe a possible workﬂow to tune simultaneously the different technologies that may be used in a CAT environment and the estimators used to select them on a segment basis: 1. Design automatic evaluation measures eˆX (X(si , �λX ), {tij }; µ � X ) and estimators X X X e˜ (X(si ; �λ ); θ� ) for each technology X ∈ X , based on a series of relevant features that can easily be"
W15-4904,W12-3118,0,0.0163317,"a set of tunable X i ;µ �X) parameters. Ideally, eˆ (X(si ; �λX ), {tij }nj=1 should approximate eX (X(si ; �λX )), but tuning of µ � X is surprisingly absent from current MT practice (with some exceptions, see Denkowski and Lavie i (2010)). In fact, eˆX (X(si ; �λX ), {tij }nj=1 ;µ � X ) can be seen as a special estimator of effort, much like e˜X (X(si ; �λX ); θ�X ), but informed with reference i translations {tij }nj=1 when they are available. This is similar to the use of pseudo-reference translations in machine translation quality estimation (Shah et al., 2013; Soricut and Narsale, 2012; Soricut et al., 2012; Soricut and Echihabi, 2010), but with actual references. Table 1 summarizes the main concepts and the notation used along the paper. 3 A general framework for training the whole CAT environment We describe a possible workﬂow to tune simultaneously the different technologies that may be used in a CAT environment and the estimators used to select them on a segment basis: 1. Design automatic evaluation measures eˆX (X(si , �λX ), {tij }; µ � X ) and estimators X X X e˜ (X(si ; �λ ); θ� ) for each technology X ∈ X , based on a series of relevant features that can easily be extracted from si and"
W15-4904,2013.tc-1.10,0,0.102852,"ach segment, MT(si ); vendors and experts tell you that you will save effort by postediting MT(si ) into your desired translation ti .1 Machine translation output MT(si ) may just be text, but it could come with annotations to help you make the most of it; for instance, words could be color-coded according to how conﬁdent the system is about them (Uefﬁng and Ney, 2007; Uefﬁng and Ney, 2005; Blatz et al., 2004), or unknown words that come out untranslated may be marked so that you spot them clearly. Machine-translated segments could even be accompanied by indicators of their estimated quality (Specia and Soricut, 2013; Specia et al., 2010; Blatz et al., 2004) which may be used to ascertain whether the output of the MT system is worth being post-edited or not. If someone measured your post-editing effort (in time, in number of keystrokes, in number of words changed, in money you would have to pay another translator to do it, etc.), when turning MT(si ) into ti , they could call that effort eMT i . Translation memory: You could also use a translation memory (TM; (Somers, 2003)), where previously translated segments s are stored together with their translations t in pairs called translation units (s, t). The"
W15-4904,H05-1096,0,0.0369622,"rs. This article is licensed under a Creative � Commons 3.0 licence, no derivative works, attribution, CCBY-ND. 27 Machine translation: You could, for instance, use machine translation (MT) to get a draft of the translation of each segment, MT(si ); vendors and experts tell you that you will save effort by postediting MT(si ) into your desired translation ti .1 Machine translation output MT(si ) may just be text, but it could come with annotations to help you make the most of it; for instance, words could be color-coded according to how conﬁdent the system is about them (Uefﬁng and Ney, 2007; Uefﬁng and Ney, 2005; Blatz et al., 2004), or unknown words that come out untranslated may be marked so that you spot them clearly. Machine-translated segments could even be accompanied by indicators of their estimated quality (Specia and Soricut, 2013; Specia et al., 2010; Blatz et al., 2004) which may be used to ascertain whether the output of the MT system is worth being post-edited or not. If someone measured your post-editing effort (in time, in number of keystrokes, in number of words changed, in money you would have to pay another translator to do it, etc.), when turning MT(si ) into ti , they could call t"
W15-4904,J07-1003,0,0.0199256,"they c 2015 The authors. This article is licensed under a Creative � Commons 3.0 licence, no derivative works, attribution, CCBY-ND. 27 Machine translation: You could, for instance, use machine translation (MT) to get a draft of the translation of each segment, MT(si ); vendors and experts tell you that you will save effort by postediting MT(si ) into your desired translation ti .1 Machine translation output MT(si ) may just be text, but it could come with annotations to help you make the most of it; for instance, words could be color-coded according to how conﬁdent the system is about them (Uefﬁng and Ney, 2007; Uefﬁng and Ney, 2005; Blatz et al., 2004), or unknown words that come out untranslated may be marked so that you spot them clearly. Machine-translated segments could even be accompanied by indicators of their estimated quality (Specia and Soricut, 2013; Specia et al., 2010; Blatz et al., 2004) which may be used to ascertain whether the output of the MT system is worth being post-edited or not. If someone measured your post-editing effort (in time, in number of keystrokes, in number of words changed, in money you would have to pay another translator to do it, etc.), when turning MT(si ) into"
W15-4918,J10-4005,0,0.031301,"t results depending on text domain and the relative number of gaps in a sentence. The paper is organised as follows: in section 2 we describe the gap-ﬁlling method for assimilation evaluation: the task layout, the choice of words, and how the tasks are generated. Section 3 introduces the experimental material, the evaluators, the distribution of tasks and the evaluation procedure. In section 4 we describe and discuss the experiment results. Finally, section 5 draws some conclusions. This paper is concerned primarily with assimilation evaluation; for a deeper discussion on evaluation see e.g. (Koehn, 2010, ch. 8). 2 Methodology This section discusses the reasoning behind the gap-ﬁlling method and task structure. The gapﬁlling method of evaluating machine translation for assimilation purposes is based on the following hypothesis: a reader’s understanding of a given text correlates with the number of words they are able to correctly restore in the text. Therefore, the base of an assimilation task is a (reference) sen2 https://github.com/Sereni/Appraise tence, where some of the words are blacked out, or removed. The sentence is produced by a human (as opposed to machine-translated), and it is in"
W15-4918,2000.tc-1.5,0,0.793401,"post-editing and comparison by bilingual experts (Ginestí-Rosell et al., 2009), and multiple choice tests (Jones et al., 2007; Trosterud and Unhammer, 2012). These approaches are often costly and prone to subjectivity: see the discussion by O’Regan and Forcada (2013). As an alternative, the modiﬁcation of cloze testing (Taylor, 1953) was introduced for assimilation evaluation, ﬁrst by Trosterud and Unhammer (2012) as a supplementary technique, and then by O’Regan and Forcada (2013) as a stand-alone method. Prior to this, cloze tests have been used to evaluate raw MT quality (Van Slype, 1979; Somers and Wild, 2000). While these authors ask informants to ﬁll gaps in MT output, Trosterud and Unhammer (2012) and O’Regan and Forcada (2013) ask informants to ﬁll gaps in the reference (human) translation. A designated number of keywords is removed from the human-translated sentences. The evaluators are then asked to ﬁll the gaps with suitable words with and without the help of MT output. The gap-ﬁlling task models how well users comprehend the key points of the text, as it is roughly equivalent with answering questions. Thus, the method does not directly evaluate the quality of machine-produced text, but rath"
W15-4918,2012.freeopmt-1.3,0,0.132569,"primary purpose. Despite the fact that, as a result of widespread usage of online MT, assimilation (or gisting) c 2015 The authors. This article is licensed under a Creative � Commons 3.0 licence, no derivative works, attribution, CCBY-ND. is currently the most frequent application of MT (in 2012, daily output of Google Translate matched the yearly output of human translations1 ), few methodologies are established for assimilation evaluation of MT. The methods include post-editing and comparison by bilingual experts (Ginestí-Rosell et al., 2009), and multiple choice tests (Jones et al., 2007; Trosterud and Unhammer, 2012). These approaches are often costly and prone to subjectivity: see the discussion by O’Regan and Forcada (2013). As an alternative, the modiﬁcation of cloze testing (Taylor, 1953) was introduced for assimilation evaluation, ﬁrst by Trosterud and Unhammer (2012) as a supplementary technique, and then by O’Regan and Forcada (2013) as a stand-alone method. Prior to this, cloze tests have been used to evaluate raw MT quality (Van Slype, 1979; Somers and Wild, 2000). While these authors ask informants to ﬁll gaps in MT output, Trosterud and Unhammer (2012) and O’Regan and Forcada (2013) ask informa"
W15-4919,J96-1002,0,0.236061,"s actually lexical selection in MT; they used a parser to identify syntactic relations such as subject–object or subject–verb. After generating all the possible translations for a given input sentence using an ambiguous bilingual dictionary, they extract the syntactic tuples from the TL and count the frequency in a previously-trained TL model of tuples. They use maximum-likelihood estimation to calculate the probability that a given 1 145 Such as a morphological or syntactic analyser. TL tuple is the translation of a given SL tuple, with an automatically determined conﬁdence threshold. Later, Berger et al. (1996) illustrated the use of maximum-entropy classiﬁers on the speciﬁc problem of lexical selection in IBM-style word-based statistical MT. Other authors (Melero et al., 2007) have used TL models to rank the translations resulting from all possible combinations of lexical selections. Nowadays, in state-of-the-art phrasebased statistical MT (Koehn, 2010), lexical selection is taken care of by a combination of the translation model and the language model. The translation model provides probabilities of translation between words or word sequences (often referred to as phrases) in the source and target"
W15-4919,J94-4003,0,0.496098,"putational cost. 1 Mikel L. Forcada Dept. Lleng. i Sist. Inform., Universitat d’Alacant, E-03071 Alacant 1.1 Introduction Corpus-based machine translation (MT) has been the primary research direction in the ﬁeld of MT in recent years. However, rule-based MT (RBMT) systems are still being developed, and there are many successful commercial and non-commercial systems. One reason for the continued development of RBMT systems is that in order to be successful, c 2015 The authors. This article is licensed under a Creative � Commons 3.0 licence, no derivative works, attribution, CCBY-ND. Prior work Dagan and Itai (1994) used the term word sense disambiguation to refer to what is actually lexical selection in MT; they used a parser to identify syntactic relations such as subject–object or subject–verb. After generating all the possible translations for a given input sentence using an ambiguous bilingual dictionary, they extract the syntactic tuples from the TL and count the frequency in a previously-trained TL model of tuples. They use maximum-likelihood estimation to calculate the probability that a given 1 145 Such as a morphological or syntactic analyser. TL tuple is the translation of a given SL tuple, wi"
W15-4919,W04-3250,0,0.066516,"tion module returns more than one translation, Apertium will select the default one if marked or the ﬁrst one of not.12 The table in Figure 2 gives an overview of the inputs.In the description it is assumed that the reference translation has been annotated by hand. However, hand annotation is a time-consuming process, and was not possible. A description of how the reference was built is given in Section 3.4. diﬀ(Tr (si ), Tt (si )) = 3.3.2 3.3.3 Conﬁdence intervals Conﬁdence intervals for both metrics will be calculated through bootstrap resampling (Efron and Tibshirani, 1994) as described by Koehn (2004). In all cases, bootstrap resampling will be carried out for 1,000 iterations. Where the p = 0.05 conﬁdence intervals overlap, we will also perform paired bootstrap resampling (Koehn, 2004). 3.4 For creating the test corpora, providing a SL corpus for training, and a TL corpus for scoring, we used four parallel corpora: • Oﬁs ar Brezhoneg (OAB): This parallel corpus of Breton and French has been collected speciﬁcally for lexical-selection experiments from translations produced by Oﬁs ar Brezhoneg ‘The Ofﬁce of the Breton language’. The corpus has recently been made available online through OPU"
W15-4919,2005.mtsummit-papers.11,0,0.0323006,"Oﬁs ar Brezhoneg ‘The Ofﬁce of the Breton language’. The corpus has recently been made available online through OPUS.13 • South-East European Times (SETimes): Described in Tyers and Alperen (2010), this corpus is a multilingual corpus of the Balkan languages (and English) in the news domain. The Macedonian and English part will be used. • Open Data Euskadi (OpenData): This is a Basque–Spanish parallel corpus made from the translation memories of the Herri Arduralaritzaren Euskal Erakundea ‘Basque Institute of Public Administration’.14 • European Parliament Proceedings (EuroParl): Described by Koehn (2005), this is a multilingual corpus of the European Union ofﬁcial languages. We are using the English–Spanish data from version 7.15 Machine translation performance This is an extrinsic evaluation, which ideally would test how much the system improves as regards an approximate measurement of ﬁnal translation quality in a real system. For this task, we use the widely-used BLEU metric (Papineni et al., 2002). This is not ideal for evaluating the task of a lexical selection module as the performance of the module will depend greatly on (a) the coverage of the bilingual dictionaries of the RBMT system"
W15-4919,J10-4005,0,0.0125822,"ey use maximum-likelihood estimation to calculate the probability that a given 1 145 Such as a morphological or syntactic analyser. TL tuple is the translation of a given SL tuple, with an automatically determined conﬁdence threshold. Later, Berger et al. (1996) illustrated the use of maximum-entropy classiﬁers on the speciﬁc problem of lexical selection in IBM-style word-based statistical MT. Other authors (Melero et al., 2007) have used TL models to rank the translations resulting from all possible combinations of lexical selections. Nowadays, in state-of-the-art phrasebased statistical MT (Koehn, 2010), lexical selection is taken care of by a combination of the translation model and the language model. The translation model provides probabilities of translation between words or word sequences (often referred to as phrases) in the source and target language. The TL model provides probabilities of word sequences in the TL. Mareˇcek et al. (2010) trained a maximum-entropy lexical selector for their dependency-grammar-based transfer system TectoMT using a bilingual corpus. More recently, Tyers et al. (2012) presented a method of lexical selection for RBMT based on rules which select or remove t"
W15-4919,P07-2045,0,0.0115657,"Missing"
W15-4919,W10-1730,0,0.0432849,"Missing"
W15-4919,P14-2123,0,0.0130906,"di follows arrain    0 otherwise (1) This feature considers a context of zero words to the left of the problem word and one word (+ handi) to the right of it. As a result of training, each of the nF features hsk (t, c) in the classiﬁer is assigned a weight λsk . Combining these weights of active features as in equation (2) yields the probability of a translation t for word s in context c. n ps (t|c) = (2) k=1 In this equation, Z s (c) is a normalising constant. Thus, the most probable translation t� can be found using t� = arg max ps (t|c) = arg max 2 The work by Ravi and Knight (2011) and Nuhn and Ney (2014), who decipher word-ciphered text using monolingual corpora only may be seen as a generalised version of the problem of lexical selection without parallel corpora. F � 1 λsk hsk (t, c) exp Z s (c) t∈Ts (s) 3 146 nF � t∈Ts (s) k=1 We follow the notation of Berger et al. (1996) λsk hsk (t, c), (3) S→ preposti=|G| → ({gi }i=1 , S) → lexsel → (g � , S) → → τ (g � , S) lexsel lexsel Figure 1: A schema of the lexical selection process: source sentence S has |G |lexical selection paths gi : lexsel selects one of them g � , which is used to generate translation τ (g � , S). where Ts (s) is the set of"
W15-4919,J03-1002,0,0.007507,"hough for all pairs there is a parallel corpus available for evaluation (see Section 3.3).5 Breton–French (Tyers, 2010): Bilingual dictionaries were not built with polysemy in mind from the outset, but some entries were added later to start work on lexical selection.6 Macedonian–English: The Macedonian–English pair in Apertium was created speciﬁcally for the purposes of running lexical-selection experiments. The lexical resources for the pair were tuned to the SETimes parallel corpus (Tyers and Alperen, 2010). The most probable entry from automatic word alignment of this corpus using GIZA ++ (Och and Ney, 2003) was checked to ensure that it was an adequate translation, and if so marked as the default.7 As a result of attempting to include all possible translations, the average number of translations per word is much higher than in other pairs.8 Basque–Spanish (Ginest´ı-Rosell et al., 2009): alternative translations were included in the bilingual dictionary.9 English–Spanish: The English–Spanish pair was developed from a combination of the English– Catalan and Spanish–Catalan pairs, and contains a number of entries in the bilingual dictionary with more than one translation.10 3.3 Performance measures"
W15-4919,P02-1040,0,0.0984287,"Missing"
W15-4919,P11-1002,0,0.0208438,"rrain  +handi (t, c) = handi follows arrain    0 otherwise (1) This feature considers a context of zero words to the left of the problem word and one word (+ handi) to the right of it. As a result of training, each of the nF features hsk (t, c) in the classiﬁer is assigned a weight λsk . Combining these weights of active features as in equation (2) yields the probability of a translation t for word s in context c. n ps (t|c) = (2) k=1 In this equation, Z s (c) is a normalising constant. Thus, the most probable translation t� can be found using t� = arg max ps (t|c) = arg max 2 The work by Ravi and Knight (2011) and Nuhn and Ney (2014), who decipher word-ciphered text using monolingual corpora only may be seen as a generalised version of the problem of lexical selection without parallel corpora. F � 1 λsk hsk (t, c) exp Z s (c) t∈Ts (s) 3 146 nF � t∈Ts (s) k=1 We follow the notation of Berger et al. (1996) λsk hsk (t, c), (3) S→ preposti=|G| → ({gi }i=1 , S) → lexsel → (g � , S) → → τ (g � , S) lexsel lexsel Figure 1: A schema of the lexical selection process: source sentence S has |G |lexical selection paths gi : lexsel selects one of them g � , which is used to generate translation τ (g � , S). whe"
W15-4919,2010.eamt-1.13,1,0.828127,"uild RBMT systems. Translation is implemented as a pipeline consisting of the following modules: morphological analysis, morphological disambiguation, lexical transfer, lexical selection, structural transfer and morphological generation. 3.2 Language pairs Evaluation will be performed using four Apertium (Forcada et al., 2011) language pairs. These pairs have been selected as they include languages with different morphological complexity, and different amounts of resources available — although for all pairs there is a parallel corpus available for evaluation (see Section 3.3).5 Breton–French (Tyers, 2010): Bilingual dictionaries were not built with polysemy in mind from the outset, but some entries were added later to start work on lexical selection.6 Macedonian–English: The Macedonian–English pair in Apertium was created speciﬁcally for the purposes of running lexical-selection experiments. The lexical resources for the pair were tuned to the SETimes parallel corpus (Tyers and Alperen, 2010). The most probable entry from automatic word alignment of this corpus using GIZA ++ (Och and Ney, 2003) was checked to ensure that it was an adequate translation, and if so marked as the default.7 As a re"
W15-4919,2012.eamt-1.54,1,0.760313,"Missing"
W15-4919,H05-1097,0,0.0938983,"Missing"
W15-4944,E06-1032,0,\N,Missing
W15-4944,W10-1751,0,\N,Missing
W15-4944,W14-3301,0,\N,Missing
W15-4944,P02-1040,0,\N,Missing
W15-4944,W14-3319,1,\N,Missing
W15-4944,P11-1105,0,\N,Missing
W15-4944,P10-2041,0,\N,Missing
W15-4944,W05-0909,0,\N,Missing
W15-4944,P07-2045,0,\N,Missing
W15-4944,W07-0718,0,\N,Missing
W15-4944,C14-1111,0,\N,Missing
W15-4944,P12-3005,0,\N,Missing
W15-4944,2012.eamt-1.67,1,\N,Missing
W15-4944,2014.eamt-1.4,1,\N,Missing
W15-4944,W14-3320,0,\N,Missing
W15-4944,2005.mtsummit-papers.11,0,\N,Missing
W15-4944,ljubesic-etal-2014-tweetcat,1,\N,Missing
W15-4944,W15-3036,1,\N,Missing
W15-4944,rubino-etal-2014-quality,1,\N,Missing
W15-4944,W15-3022,1,\N,Missing
W15-4944,W15-4903,1,\N,Missing
W15-4944,2015.eamt-1.4,1,\N,Missing
W15-4944,espla-gomis-etal-2014-comparing,1,\N,Missing
W15-4944,W15-3001,0,\N,Missing
W15-4944,ljubesic-toral-2014-cawac,1,\N,Missing
W15-4944,W14-0405,1,\N,Missing
W15-4944,2005.iwslt-1.8,0,\N,Missing
W15-4944,W16-3421,1,\N,Missing
W15-4944,D07-1078,0,\N,Missing
W15-4944,W08-0509,0,\N,Missing
W15-4944,W11-2123,0,\N,Missing
W15-4944,P14-1129,0,\N,Missing
W15-4944,W16-2347,0,\N,Missing
W15-4944,W16-2375,1,\N,Missing
W15-4944,W16-2367,1,\N,Missing
W15-4944,W16-3423,1,\N,Missing
W16-2367,2012.eamt-1.60,0,0.0268115,"sk. 1 Introduction Parallel data harvesting has become a critical problem for many cross-lingual tasks in natural language processing. These data are the basis of many approaches, specially in the case of corpus-based machine translation (MT). One of the main sources of new parallel data is the Internet; in fact, many solutions have been proposed for exploiting specific websites by learning features of their structure. Some popular examples of corpora built by mining specific websites are the Europarl Corpus (Koehn, 2005), which exploits the European Parliament website, or the TED2013 corpus (Cettolo et al., 2012), that mines bitexts from the TED talks website,1 a site that provides videos of public speeches and their transcriptions translated into several languages. Nevertheless, defining methodologies to surf the Web and identify parallel documents in any website is still an open problem. Some of the earliest tools developed for this purpose are STRAND (Resnik and Smith, 2003) and BITS (Ma and Liberman, 1999). These tools use similarities in the URLs and the content of the webpages to detect parallel documents in a given web domain. 1 Based on these principles, many later approaches have been propose"
W16-2367,2008.tc-1.1,0,0.134589,"Missing"
W16-2367,2005.mtsummit-papers.11,0,0.116176,"tool and discusses the results obtained on the data sets published for the shared task. 1 Introduction Parallel data harvesting has become a critical problem for many cross-lingual tasks in natural language processing. These data are the basis of many approaches, specially in the case of corpus-based machine translation (MT). One of the main sources of new parallel data is the Internet; in fact, many solutions have been proposed for exploiting specific websites by learning features of their structure. Some popular examples of corpora built by mining specific websites are the Europarl Corpus (Koehn, 2005), which exploits the European Parliament website, or the TED2013 corpus (Cettolo et al., 2012), that mines bitexts from the TED talks website,1 a site that provides videos of public speeches and their transcriptions translated into several languages. Nevertheless, defining methodologies to surf the Web and identify parallel documents in any website is still an open problem. Some of the earliest tools developed for this purpose are STRAND (Resnik and Smith, 2003) and BITS (Ma and Liberman, 1999). These tools use similarities in the URLs and the content of the webpages to detect parallel documen"
W16-2367,P12-3005,0,0.0583915,"rmalise the HTML markup into XHTML and remove boilerplates. After normalisation, exact duplicates are discarded. This module outputs a list of tab-separated fields, in which every line corresponds to a file. Four fields are included in each line: the MIME type,10 the character encoding, the local path to the file processed, and the content of the document after normalisation encoded in base64;11 this format is henceforth called ett. 3. Language identification: this module receives as an input the list of processed documents in format ett; the language of each document is detected with LangID (Lui and Baldwin, 2012),12 keeping only those documents in one of the target languages (L1 or L2 ). Before language identification, Apache Tika is used to convert the XHTML content of the document into plain text. The module outputs a list of files in lett format, which consists of the same fields than ett plus the language identifier of the document and the plain text extracted, encoded in base64. The architecture of Bitextor in versions since 4.0 is based on a Unix-style pipeline, in which a collection of scripts are connected using text interfaces. This architecture favours the parallelisation of subtasks and eas"
W16-2367,1999.mtsummit-1.79,0,0.323869,"f their structure. Some popular examples of corpora built by mining specific websites are the Europarl Corpus (Koehn, 2005), which exploits the European Parliament website, or the TED2013 corpus (Cettolo et al., 2012), that mines bitexts from the TED talks website,1 a site that provides videos of public speeches and their transcriptions translated into several languages. Nevertheless, defining methodologies to surf the Web and identify parallel documents in any website is still an open problem. Some of the earliest tools developed for this purpose are STRAND (Resnik and Smith, 2003) and BITS (Ma and Liberman, 1999). These tools use similarities in the URLs and the content of the webpages to detect parallel documents in a given web domain. 1 Based on these principles, many later approaches have been proposed (Nie et al., 1999; Chen et al., 2004; Zhang et al., 2006; D´esilets et al., 2008; San Vicente and Manterola, 2012; Papavassiliou et al., 2013); this paper describes the participation of Prompsit Language Engineering and the Universitat d’Alacant in the shared task on document alignment of WMT 2016, based on one of these tools: Bitextor (Espl`a-Gomis and Forcada, 2010). The rest of the paper is organi"
W16-2367,W13-2506,0,0.0561442,"ions translated into several languages. Nevertheless, defining methodologies to surf the Web and identify parallel documents in any website is still an open problem. Some of the earliest tools developed for this purpose are STRAND (Resnik and Smith, 2003) and BITS (Ma and Liberman, 1999). These tools use similarities in the URLs and the content of the webpages to detect parallel documents in a given web domain. 1 Based on these principles, many later approaches have been proposed (Nie et al., 1999; Chen et al., 2004; Zhang et al., 2006; D´esilets et al., 2008; San Vicente and Manterola, 2012; Papavassiliou et al., 2013); this paper describes the participation of Prompsit Language Engineering and the Universitat d’Alacant in the shared task on document alignment of WMT 2016, based on one of these tools: Bitextor (Espl`a-Gomis and Forcada, 2010). The rest of the paper is organised as follows: Section 2 describes the main features of Bitextor, highlighting the main differences between versions 4.1 and 5.0. Section 3 describes the steps taken to produce the submissions for the shared task on document alignment in WMT 2016, and discusses the results obtained. Finally, some concluding remarks are provided in Secti"
W16-2367,J03-3002,0,0.663942,"ic websites by learning features of their structure. Some popular examples of corpora built by mining specific websites are the Europarl Corpus (Koehn, 2005), which exploits the European Parliament website, or the TED2013 corpus (Cettolo et al., 2012), that mines bitexts from the TED talks website,1 a site that provides videos of public speeches and their transcriptions translated into several languages. Nevertheless, defining methodologies to surf the Web and identify parallel documents in any website is still an open problem. Some of the earliest tools developed for this purpose are STRAND (Resnik and Smith, 2003) and BITS (Ma and Liberman, 1999). These tools use similarities in the URLs and the content of the webpages to detect parallel documents in a given web domain. 1 Based on these principles, many later approaches have been proposed (Nie et al., 1999; Chen et al., 2004; Zhang et al., 2006; D´esilets et al., 2008; San Vicente and Manterola, 2012; Papavassiliou et al., 2013); this paper describes the participation of Prompsit Language Engineering and the Universitat d’Alacant in the shared task on document alignment of WMT 2016, based on one of these tools: Bitextor (Espl`a-Gomis and Forcada, 2010)"
W16-2367,san-vicente-manterola-2012-paco2,0,0.0165921,"speeches and their transcriptions translated into several languages. Nevertheless, defining methodologies to surf the Web and identify parallel documents in any website is still an open problem. Some of the earliest tools developed for this purpose are STRAND (Resnik and Smith, 2003) and BITS (Ma and Liberman, 1999). These tools use similarities in the URLs and the content of the webpages to detect parallel documents in a given web domain. 1 Based on these principles, many later approaches have been proposed (Nie et al., 1999; Chen et al., 2004; Zhang et al., 2006; D´esilets et al., 2008; San Vicente and Manterola, 2012; Papavassiliou et al., 2013); this paper describes the participation of Prompsit Language Engineering and the Universitat d’Alacant in the shared task on document alignment of WMT 2016, based on one of these tools: Bitextor (Espl`a-Gomis and Forcada, 2010). The rest of the paper is organised as follows: Section 2 describes the main features of Bitextor, highlighting the main differences between versions 4.1 and 5.0. Section 3 describes the steps taken to produce the submissions for the shared task on document alignment in WMT 2016, and discusses the results obtained. Finally, some concluding"
W16-2383,C04-1046,0,0.163519,"on have been used: machine translation (Lucy LT KWIK Translator and Google Translate) and the bilingual concordancer Reverso Context. Building upon the word-level approach implemented for WMT 2015, a method for phrase-based MTQE is proposed which builds on the probabilities obtained for word-level MTQE. For each sub-task we have submitted two systems: one using the features produced exclusively based on online sources of bilingual information, and one combining them with the baseline features provided by the organisers of the task. 1 Introduction Machine translation quality estimation (MTQE) (Blatz et al., 2004; Specia et al., 2010; Specia and Soricut, 2013) has aroused the interest of both the scientific community and translation companies on account of its noticeable advantages: it can be used to help professional translators in post-editing, to estimate the translation productivity for different translation technologies, or even for budgeting translation projects. In this context, the WMT 2016 MTQE shared task becomes one of the best scenarios in which different approaches to MTQE can be evaluated and compared for different granularities: segment-level (sub-task 1), phrase-level and word-level (s"
W16-2383,W15-3036,1,0.568476,"Missing"
W16-2383,2013.tc-1.10,0,0.0306421,"ucy LT KWIK Translator and Google Translate) and the bilingual concordancer Reverso Context. Building upon the word-level approach implemented for WMT 2015, a method for phrase-based MTQE is proposed which builds on the probabilities obtained for word-level MTQE. For each sub-task we have submitted two systems: one using the features produced exclusively based on online sources of bilingual information, and one combining them with the baseline features provided by the organisers of the task. 1 Introduction Machine translation quality estimation (MTQE) (Blatz et al., 2004; Specia et al., 2010; Specia and Soricut, 2013) has aroused the interest of both the scientific community and translation companies on account of its noticeable advantages: it can be used to help professional translators in post-editing, to estimate the translation productivity for different translation technologies, or even for budgeting translation projects. In this context, the WMT 2016 MTQE shared task becomes one of the best scenarios in which different approaches to MTQE can be evaluated and compared for different granularities: segment-level (sub-task 1), phrase-level and word-level (sub-task 2), and document-level (sub-task 3). For"
W16-3405,2008.tc-1.1,0,0.116724,"Missing"
W16-3405,2005.mtsummit-papers.11,0,0.0799397,"off Annotation of Web Content 153 memory of the European Commission’s Directorate General for Translation (Steinberger et al., 2012); a comprehensive repository of such sentence-aligned bitexts is provided by OPUS1 (Tiedemann, 2012). But in view of the fact that the Internet is packed with webpages which are mutual translations, it is not uncommon for researchers and practitioners to build sentencealigned bitext by harvesting these webpages, pairing them, sentence-aligning them, and making the resulting corpora publicly available. The most famous example would probably be the Europarl corpus (Koehn, 2005). Contrary to what may be commonly believed, distribution of web-crawled bitext is far from being free from legal implications,2 and may sometimes actually violate the usage restrictions of web content, as will be discussed in Section 2. As the distribution and availability of sentence-aligned bitext is key to the development of statistical machine translation systems —in particular when it comes to adapt an existing system to a specific domain (Pecina et al., 2012)—but also to save professional translation effort, Section 3 proposes an alternative: instead of copying and distributing copies o"
W16-3405,P09-5002,0,0.0241853,"of sentence-aligned crawled bitext The importance of bitext or parallel text in current translation technologies is hard to emphasize. Isabelle et al. (1993) —but also Simard et al. (1993)— are famously quoted for saying that “Existing translations contain more solutions to more translation problems than any other currently available resource”, but the formulation of the concept of bitext as a translation object can be traced back to Harris (1988). For bitexts to be used in two key translation technologies, namely corpus-based machine translation —particularly statistical machine translation (Koehn, 2009), but also example-based machine translation (Carl and Way, 2003)— and computer-aided translation (Bowker and Fisher, 2010), they have to be segmented and aligned, usually sentence by sentence. Sentence-aligned bitexts, frequently in the form of translation memories, are usually obtained as a by-product of computer-aided translation processes, and many of them have been made publicly available, such as DGT-MT, the translation Stand-off Annotation of Web Content 153 memory of the European Commission’s Directorate General for Translation (Steinberger et al., 2012); a comprehensive repository of"
W16-3405,1999.mtsummit-1.79,0,0.173158,"ksum of the text. Instead of using the standard TMX segment element (seg), a web segment element (webseg) contains a pointer to a particular segment, made up of an URL, a fragment identifier using Xpointer notation, and a character range inside the selected element (0:11 in English and 0:10 in Spanish). 5 Implementation: stand-off crawlers Given the fact that there is a number of bilingual web crawlers able to harvest bitexts from the Internet, such as Bitextor (Espl`a-Gomis and Forcada, 2010), ILSP Focused Crawler (Mastropavlos and Papavassiliou, 2011), STRAND (Resnik and Smith, 2003), BITS (Ma and Liberman, 1999), or WeBiText (D´esilets et al., 2008), it seems more reasonable to consider adapting an existing parallel data crawler to produce deferred translation memories than implementing a new stand-off crawler from scratch. In general, most of these parallel data crawlers work following a similar process: 1. 2. 3. 4. 37 several documents from a given website are downloaded; documents are pre-processed and their language is identified; parallel documents are identified (document alignment) using heuristics; optionally, parallel documents are segment-aligned. https://www.gala-global.org/tmx-14b Stand-o"
W16-3405,2012.eamt-1.38,0,0.0439163,"Missing"
W16-3405,J03-3002,0,0.124428,"h variant contains the MD5 checksum of the text. Instead of using the standard TMX segment element (seg), a web segment element (webseg) contains a pointer to a particular segment, made up of an URL, a fragment identifier using Xpointer notation, and a character range inside the selected element (0:11 in English and 0:10 in Spanish). 5 Implementation: stand-off crawlers Given the fact that there is a number of bilingual web crawlers able to harvest bitexts from the Internet, such as Bitextor (Espl`a-Gomis and Forcada, 2010), ILSP Focused Crawler (Mastropavlos and Papavassiliou, 2011), STRAND (Resnik and Smith, 2003), BITS (Ma and Liberman, 1999), or WeBiText (D´esilets et al., 2008), it seems more reasonable to consider adapting an existing parallel data crawler to produce deferred translation memories than implementing a new stand-off crawler from scratch. In general, most of these parallel data crawlers work following a similar process: 1. 2. 3. 4. 37 several documents from a given website are downloaded; documents are pre-processed and their language is identified; parallel documents are identified (document alignment) using heuristics; optionally, parallel documents are segment-aligned. https://www.g"
W16-3405,steinberger-etal-2006-jrc,0,0.126462,"Missing"
W16-3405,steinberger-etal-2012-dgt,0,0.0383009,"Missing"
W16-3405,tiedemann-2012-parallel,0,0.0429287,"nd Way, 2003)— and computer-aided translation (Bowker and Fisher, 2010), they have to be segmented and aligned, usually sentence by sentence. Sentence-aligned bitexts, frequently in the form of translation memories, are usually obtained as a by-product of computer-aided translation processes, and many of them have been made publicly available, such as DGT-MT, the translation Stand-off Annotation of Web Content 153 memory of the European Commission’s Directorate General for Translation (Steinberger et al., 2012); a comprehensive repository of such sentence-aligned bitexts is provided by OPUS1 (Tiedemann, 2012). But in view of the fact that the Internet is packed with webpages which are mutual translations, it is not uncommon for researchers and practitioners to build sentencealigned bitext by harvesting these webpages, pairing them, sentence-aligning them, and making the resulting corpora publicly available. The most famous example would probably be the Europarl corpus (Koehn, 2005). Contrary to what may be commonly believed, distribution of web-crawled bitext is far from being free from legal implications,2 and may sometimes actually violate the usage restrictions of web content, as will be discus"
W18-6320,2009.mtsummit-posters.5,0,0.123004,"Missing"
W18-6320,W15-4918,1,0.909017,"Missing"
W18-6320,W16-2301,1,0.829153,"Missing"
W18-6320,2001.mtsummit-papers.20,0,0.488483,"Missing"
W18-6320,1999.mtsummit-1.42,0,0.32113,"Missing"
W18-6320,W11-2123,0,0.0202271,"Missing"
W18-6320,E06-1032,0,0.205885,"Missing"
W18-6320,N07-2020,0,0.0997368,"Missing"
W18-6320,L16-1048,0,0.0241495,"Missing"
W18-6320,2014.eamt-1.40,0,0.0438492,"Missing"
W18-6320,2000.tc-1.5,0,0.320841,"Missing"
W18-6320,W15-2402,0,0.0382638,"Missing"
W18-6320,stymne-etal-2012-eye,0,0.038532,"Missing"
W18-6320,P07-2045,1,0.0112762,"Missing"
W18-6320,1993.tmi-1.22,0,0.82749,"Missing"
W18-6320,W11-2401,0,0.033505,"Missing"
W18-6320,2012.freeopmt-1.3,0,0.127684,"Missing"
W18-6320,weiss-ahrenberg-2012-error,0,0.0760185,"Missing"
W18-6320,N16-1125,0,0.0253847,"Missing"
W18-6453,W18-6475,0,0.0511142,"Missing"
W18-6453,W11-1218,0,0.0394428,"ine translation, but more recent work targets neural models. Carpuat et al. (2017) focus on identifying semantic differences in translation pairs using cross-lingual textual entailment and additional length-based features, and demonstrates that removing such sentences improves neural machine translation performance. As Rarrick et al. (2011) point out, one type of noise in parallel corpora extracted from the web are translations that have been created by machine translation. Venugopal et al. (2011) propose a method to watermark the output of machine translation systems to aid this distinction. Antonova and Misyurev (2011) report that rule-based machine translation output can be detected due to certain word choices, and statistical machine translation output can be detected due to lack of reordering. Belinkov and Bisk (2017) investigate the impact of noise on neural machine translation. They focus on creating systems that can translate the kinds of orthographic errors (typos, misspellings, etc.) that humans can comprehend. In contrast, Khayrallah and Koehn (2018) address noisy training data and focus on types of noise occurring in web-crawled corpora. They carried out a study how noise that occurs in crawled pa"
W18-6453,W18-6476,0,0.042088,"Missing"
W18-6453,W18-6477,0,0.0952092,"Missing"
W18-6453,W18-6472,0,0.0451806,"Missing"
W18-6453,W18-6478,0,0.0570614,"Missing"
W18-6453,D11-1033,0,0.165705,"investigate the impact of noise on neural machine translation. They focus on creating systems that can translate the kinds of orthographic errors (typos, misspellings, etc.) that humans can comprehend. In contrast, Khayrallah and Koehn (2018) address noisy training data and focus on types of noise occurring in web-crawled corpora. They carried out a study how noise that occurs in crawled parallel text impacts statistical and neural machine translation. There is a rich literature on data selection which aims at sub-sampling parallel data relevant for a task-specific machine translation system (Axelrod et al., 2011). van der Wees et al. (2017) find that the existing data selection methods developed for statistical machine translation are less effective for neural machine translation. This is different from our goals of handling noise since those methods tend to discard perfectly fine sentence pairs (say, about cooking recipes) that are just not relevant for the targeted domain (say, software manuals). Our task is focused on data quality that is relevant for all domains. 4 https://drive.google.com/drive/folders/ 1zZNPlAThm-Rnvxsy8rXzChC49bc0 TGO 727 Type of Noise Okay Misaligned sentences Third language B"
W18-6453,P18-4020,1,0.833453,"Missing"
W18-6453,W18-6473,0,0.0626345,"Missing"
W18-6453,W18-2709,1,0.900845,"and adherence to sentence-bysentence correspondences. The other extreme are sentence pairs extracted with fully automatic processes from indiscriminate crawling of the World Wide Web. The Shared Task on Parallel Corpus Filtering targets the second extreme, although the methods developed for this data condition should also carry over to less noisy parallel corpora. In setting this task, we were motivated by our ongoing efforts to create large publicly available parallel corpora from web sources and the recognition that noisy parallel data is especially a concern for neural machine translation (Khayrallah and Koehn, 2018). This paper gives an overview of the task, presents its results and provides some analysis. 2 Related Work 1 http://opus.lingfil.uu.se/ http://www.paracrawl.eu/ 3 NLP4TM 2016: Shared task http://rgcl.wlv.ac.uk/nlp4tm2016/shared-task/ Although the idea of crawling the web indiscriminately for parallel data goes back to the 20th century (Resnik, 1999), work in the academic com2 726 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 726–739 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https:"
W18-6453,W18-6474,0,0.0714008,"Missing"
W18-6453,W18-6479,1,0.790029,"Missing"
W18-6453,2005.mtsummit-papers.11,1,0.26682,"Missing"
W18-6453,W16-2347,1,0.895759,"Missing"
W18-6453,P07-2045,1,0.0147689,"Missing"
W18-6453,W17-3209,0,0.0604803,"ed corpora, and evaluation translation quality on blind test sets using the BLEU score. For development purposes, we released configuration files and scripts that mirror the official testing procedure with a development test set. The development pack consists of • a script to subsample corpora based on quality scores • a Moses configuration file to train and test a statistical machine translation system • Marian scripts to train and test a neural machine translation system Most of this work was done in the context of statistical machine translation, but more recent work targets neural models. Carpuat et al. (2017) focus on identifying semantic differences in translation pairs using cross-lingual textual entailment and additional length-based features, and demonstrates that removing such sentences improves neural machine translation performance. As Rarrick et al. (2011) point out, one type of noise in parallel corpora extracted from the web are translations that have been created by machine translation. Venugopal et al. (2011) propose a method to watermark the output of machine translation systems to aid this distinction. Antonova and Misyurev (2011) report that rule-based machine translation output can"
W18-6453,P13-2061,0,0.0539603,"ea of crawling the web indiscriminately for parallel data goes back to the 20th century (Resnik, 1999), work in the academic com2 726 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 726–739 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64080 3 to filter a parallel corpus; Xu and Koehn (2017) generate synthetic noisy data (inadequate and nonfluent translations) and use this data to train a classifier to identify good sentence pairs from a noisy corpus; and Cui et al. (2013) use a graph-based random walk algorithm and extract phrase pair scores to weight the phrase translation probabilities to bias towards more trustworthy ones. Task The shared task tackled the problem of filtering parallel corpora. Given a noisy parallel corpus (crawled from the web), participants developed methods to filter it to a smaller size of high quality sentence pairs. Specifically, we provided a very noisy 1 billion word (English token count) German–English corpus crawled from the web by the Paracrawl project. We asked participants to subselect sentence pairs that amount to (a) 10 milli"
W18-6453,W18-6480,0,0.0668994,"Missing"
W18-6453,skadins-etal-2014-billions,0,0.0402284,"Missing"
W18-6453,W18-6481,0,0.0862833,"Missing"
W18-6453,W18-6488,0,0.0644507,"Missing"
W18-6453,W18-6482,0,0.0428675,"Missing"
W18-6453,2011.eamt-1.25,0,0.413479,"Missing"
W18-6453,W18-6483,0,0.069782,"Missing"
W18-6453,2011.mtsummit-papers.47,0,0.73575,"Missing"
W18-6453,W18-6484,0,0.0645499,"Missing"
W18-6453,tiedemann-2012-parallel,0,0.0910955,"It contains news stories that were either translated from German to English or from English to German. NEWSTEST 2018 IWSLT 2017 ACQUIS IWSLT 2017 The test set from the IWSLT 2017 evaluation campaign. It consists of transcripts of talks given at the TED conference. They cover generally accessible topics in the area of technology, entertainment, and design. EMEA GLOBALVOICES KDE ate the machine translation systems trained on the subsampled data sets. Word counts are obtained with wc on untokenized text. This test set was extracted from the Acquis Communtaire corpus, which is available on OPUS7 (Tiedemann, 2012) (which was the source to create the subsequent 3 test sets). The test set consists of laws of the European Union that have to be incorporated into the national laws of the EU member countries. We only used sentences with 15 to 80 words, and removed any duplicate sentence pairs. 5 Evaluation Protocol The testing setup mirrors the development environment that we provided to the participants. 5.1 Particpants We received submissions from 17 different organizations. See Table 3 for the complete list of participants. The participant’s organizations are quite diverse, with 3 participants from Spain,"
W18-6453,W18-6485,0,0.0339437,"Missing"
W18-6453,D17-1147,0,0.150155,"Missing"
W18-6453,W18-6486,0,0.0808535,"Missing"
W18-6453,2009.mtsummit-posters.15,0,0.47964,"Missing"
W18-6453,2011.mtsummit-papers.48,0,0.14183,"of • a script to subsample corpora based on quality scores • a Moses configuration file to train and test a statistical machine translation system • Marian scripts to train and test a neural machine translation system Most of this work was done in the context of statistical machine translation, but more recent work targets neural models. Carpuat et al. (2017) focus on identifying semantic differences in translation pairs using cross-lingual textual entailment and additional length-based features, and demonstrates that removing such sentences improves neural machine translation performance. As Rarrick et al. (2011) point out, one type of noise in parallel corpora extracted from the web are translations that have been created by machine translation. Venugopal et al. (2011) propose a method to watermark the output of machine translation systems to aid this distinction. Antonova and Misyurev (2011) report that rule-based machine translation output can be detected due to certain word choices, and statistical machine translation output can be detected due to lack of reordering. Belinkov and Bisk (2017) investigate the impact of noise on neural machine translation. They focus on creating systems that can tran"
W18-6453,P99-1068,0,0.254476,"In setting this task, we were motivated by our ongoing efforts to create large publicly available parallel corpora from web sources and the recognition that noisy parallel data is especially a concern for neural machine translation (Khayrallah and Koehn, 2018). This paper gives an overview of the task, presents its results and provides some analysis. 2 Related Work 1 http://opus.lingfil.uu.se/ http://www.paracrawl.eu/ 3 NLP4TM 2016: Shared task http://rgcl.wlv.ac.uk/nlp4tm2016/shared-task/ Although the idea of crawling the web indiscriminately for parallel data goes back to the 20th century (Resnik, 1999), work in the academic com2 726 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 726–739 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64080 3 to filter a parallel corpus; Xu and Koehn (2017) generate synthetic noisy data (inadequate and nonfluent translations) and use this data to train a classifier to identify good sentence pairs from a noisy corpus; and Cui et al. (2013) use a graph-based random walk algorithm and extract phrase pair scores to weight the"
W18-6453,D11-1126,0,0.109697,"ripts to train and test a neural machine translation system Most of this work was done in the context of statistical machine translation, but more recent work targets neural models. Carpuat et al. (2017) focus on identifying semantic differences in translation pairs using cross-lingual textual entailment and additional length-based features, and demonstrates that removing such sentences improves neural machine translation performance. As Rarrick et al. (2011) point out, one type of noise in parallel corpora extracted from the web are translations that have been created by machine translation. Venugopal et al. (2011) propose a method to watermark the output of machine translation systems to aid this distinction. Antonova and Misyurev (2011) report that rule-based machine translation output can be detected due to certain word choices, and statistical machine translation output can be detected due to lack of reordering. Belinkov and Bisk (2017) investigate the impact of noise on neural machine translation. They focus on creating systems that can translate the kinds of orthographic errors (typos, misspellings, etc.) that humans can comprehend. In contrast, Khayrallah and Koehn (2018) address noisy training d"
W18-6453,W18-6487,0,0.0438357,"Missing"
W18-6453,W18-6489,0,0.0366714,"Missing"
W18-6453,D17-1319,1,0.672175,"s and provides some analysis. 2 Related Work 1 http://opus.lingfil.uu.se/ http://www.paracrawl.eu/ 3 NLP4TM 2016: Shared task http://rgcl.wlv.ac.uk/nlp4tm2016/shared-task/ Although the idea of crawling the web indiscriminately for parallel data goes back to the 20th century (Resnik, 1999), work in the academic com2 726 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 726–739 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64080 3 to filter a parallel corpus; Xu and Koehn (2017) generate synthetic noisy data (inadequate and nonfluent translations) and use this data to train a classifier to identify good sentence pairs from a noisy corpus; and Cui et al. (2013) use a graph-based random walk algorithm and extract phrase pair scores to weight the phrase translation probabilities to bias towards more trustworthy ones. Task The shared task tackled the problem of filtering parallel corpora. Given a noisy parallel corpus (crawled from the web), participants developed methods to filter it to a smaller size of high quality sentence pairs. Specifically, we provided a very nois"
W18-6464,W13-2242,0,0.328437,"Missing"
W18-6464,W17-4760,0,0.030293,"ghlight four groups of contributions: 801 • To estimate the sentence-level quality of MT output for a source segment S, Bic¸ici (2013) chooses sentence pairs from a parallel corpus which are close to S, and builds an SMT system whose internals when translating S are examined to extract features. • MULTILIZER, one of the participants in the sentence-level MT QE task at WMT 2014 (Bojar et al., 2014) uses other MT systems to translate S into the target language (TL) and T into the source language (SL). The results are compared to the original SL and TL segments to obtain indicators of quality. • Blain et al. (2017) use bilexical embeddings (obtained from SL and TL word embeddings Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 801–808 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64091 and word-aligned parallel corpora) to model the strength of the relationship between SL and TL words, in order to estimate sentencelevel and word-level MT quality. • Finally, Espl`a-Gomis et al. (2015a,b), and Espl`a-Gomis et al. (2016) perform word-level MT QE by using other MT syste"
W18-6464,W15-4903,1,0.868199,"Missing"
W18-6464,C04-1046,0,0.0918984,"o estimate quality at the sentence level. The paper is organized as follows: section 2 briefly reviews previous work on word-level MT QE; section 3 describes the method used to label words and gaps, paying special attention to the features extracted (sections 3.1 and 3.2) and the neural network (NN) architecture and its training (section 3.3); section 4 describes the datasets used; section 5 shows the main results; and, finally, section 6 closes the paper with concluding remarks. Related work Pioneering work on word-level MT QE dealt with predictive/interactive MT (Gandrabur and Foster, 2003; Blatz et al., 2004; Ueffing and Ney, 2005, 2007), often under the name of confidence estimation. Estimations relied on the internals of the actual MT system —for instance, studying the nbest translations (Ueffing and Ney, 2007)— or used external sources of bilingual information; for instance, both Blatz et al. (2004) and Ueffing and Ney (2005) used probabilistic dictionaries; in the case of Blatz et al. (2004), as one of many features in a binary classifier for each word. The last decade has witnessed an explosion of work in word-level MT QE, with most of the recent advances made by participants in the shared t"
W18-6464,W16-2383,1,0.89035,"Missing"
W18-6464,W03-0413,0,0.0996183,"redicted at the word level to estimate quality at the sentence level. The paper is organized as follows: section 2 briefly reviews previous work on word-level MT QE; section 3 describes the method used to label words and gaps, paying special attention to the features extracted (sections 3.1 and 3.2) and the neural network (NN) architecture and its training (section 3.3); section 4 describes the datasets used; section 5 shows the main results; and, finally, section 6 closes the paper with concluding remarks. Related work Pioneering work on word-level MT QE dealt with predictive/interactive MT (Gandrabur and Foster, 2003; Blatz et al., 2004; Ueffing and Ney, 2005, 2007), often under the name of confidence estimation. Estimations relied on the internals of the actual MT system —for instance, studying the nbest translations (Ueffing and Ney, 2007)— or used external sources of bilingual information; for instance, both Blatz et al. (2004) and Ueffing and Ney (2005) used probabilistic dictionaries; in the case of Blatz et al. (2004), as one of many features in a binary classifier for each word. The last decade has witnessed an explosion of work in word-level MT QE, with most of the recent advances made by particip"
W18-6464,W15-3036,1,0.889532,"Missing"
W18-6464,W17-4763,0,0.0602735,"d (wordaligned) SL words into feature vectors — extended with the baseline features provided by WMT15 (Bojar et al., 2015) organizers— to perform word-level MT QE. • Martins et al. (2016) achieved the best results in the word-level MT QE shared task at WMT 2016 (Bojar et al., 2016) by combining a feedforward NN with two recurrent NNs whose predictions were fed into a linear sequential model together with the baseline features provided by the organizers of the task. An extension (Martins et al., 2017) uses the output of an automatic post-editing tool, with a clear improvement in performance. • Kim et al. (2017a,b) obtained in WMT 2017 (Bojar et al., 2017) results which were better or comparable to those by Martins et al. (2017), using a three-level stacked architecture trained in a multi-task fashion, combining a neural word prediction model trained on large-scale parallel corpora, and word- and sentence-level MT QE models. Our approach uses a much simpler architecture than the last two approaches, containing no recurrent NNs, but just feed-forward NNs applied to a fixedlength context window around the word or gap about which a decision is being made (similarly to a convolutional approach). This ma"
W18-6464,W15-3037,0,0.151616,"l MT quality. • Finally, Espl`a-Gomis et al. (2015a,b), and Espl`a-Gomis et al. (2016) perform word-level MT QE by using other MT systems to translate sub-segments of S and T and extracting features describing the way in which these translated sub-segments match sub-segments of T . This is the work most related to the one presented in this paper. Only the last two groups of work actually tackle the problem of word-level MT QE, and none of them are able to identify the gaps where insertions are needed. As regards the use of neural networks (NN) in MT QE, we can highlight a few contributions: • Kreutzer et al. (2015) use a deep feed-forward NN to process the concatenated vector embeddings of neighbouring TL words and (wordaligned) SL words into feature vectors — extended with the baseline features provided by WMT15 (Bojar et al., 2015) organizers— to perform word-level MT QE. • Martins et al. (2016) achieved the best results in the word-level MT QE shared task at WMT 2016 (Bojar et al., 2016) by combining a feedforward NN with two recurrent NNs whose predictions were fed into a linear sequential model together with the baseline features provided by the organizers of the task. An extension (Martins et al.,"
W18-6464,W16-2387,0,0.0826382,"s the work most related to the one presented in this paper. Only the last two groups of work actually tackle the problem of word-level MT QE, and none of them are able to identify the gaps where insertions are needed. As regards the use of neural networks (NN) in MT QE, we can highlight a few contributions: • Kreutzer et al. (2015) use a deep feed-forward NN to process the concatenated vector embeddings of neighbouring TL words and (wordaligned) SL words into feature vectors — extended with the baseline features provided by WMT15 (Bojar et al., 2015) organizers— to perform word-level MT QE. • Martins et al. (2016) achieved the best results in the word-level MT QE shared task at WMT 2016 (Bojar et al., 2016) by combining a feedforward NN with two recurrent NNs whose predictions were fed into a linear sequential model together with the baseline features provided by the organizers of the task. An extension (Martins et al., 2017) uses the output of an automatic post-editing tool, with a clear improvement in performance. • Kim et al. (2017a,b) obtained in WMT 2017 (Bojar et al., 2017) results which were better or comparable to those by Martins et al. (2017), using a three-level stacked architecture trained"
W18-6464,Q17-1015,0,0.0142485,"r et al. (2015) use a deep feed-forward NN to process the concatenated vector embeddings of neighbouring TL words and (wordaligned) SL words into feature vectors — extended with the baseline features provided by WMT15 (Bojar et al., 2015) organizers— to perform word-level MT QE. • Martins et al. (2016) achieved the best results in the word-level MT QE shared task at WMT 2016 (Bojar et al., 2016) by combining a feedforward NN with two recurrent NNs whose predictions were fed into a linear sequential model together with the baseline features provided by the organizers of the task. An extension (Martins et al., 2017) uses the output of an automatic post-editing tool, with a clear improvement in performance. • Kim et al. (2017a,b) obtained in WMT 2017 (Bojar et al., 2017) results which were better or comparable to those by Martins et al. (2017), using a three-level stacked architecture trained in a multi-task fashion, combining a neural word prediction model trained on large-scale parallel corpora, and word- and sentence-level MT QE models. Our approach uses a much simpler architecture than the last two approaches, containing no recurrent NNs, but just feed-forward NNs applied to a fixedlength context wind"
W18-6464,2005.eamt-1.35,0,0.0493499,"t the sentence level. The paper is organized as follows: section 2 briefly reviews previous work on word-level MT QE; section 3 describes the method used to label words and gaps, paying special attention to the features extracted (sections 3.1 and 3.2) and the neural network (NN) architecture and its training (section 3.3); section 4 describes the datasets used; section 5 shows the main results; and, finally, section 6 closes the paper with concluding remarks. Related work Pioneering work on word-level MT QE dealt with predictive/interactive MT (Gandrabur and Foster, 2003; Blatz et al., 2004; Ueffing and Ney, 2005, 2007), often under the name of confidence estimation. Estimations relied on the internals of the actual MT system —for instance, studying the nbest translations (Ueffing and Ney, 2007)— or used external sources of bilingual information; for instance, both Blatz et al. (2004) and Ueffing and Ney (2005) used probabilistic dictionaries; in the case of Blatz et al. (2004), as one of many features in a binary classifier for each word. The last decade has witnessed an explosion of work in word-level MT QE, with most of the recent advances made by participants in the shared tasks on MT QE at the di"
W18-6464,J07-1003,0,0.0388073,"special attention to the features extracted (sections 3.1 and 3.2) and the neural network (NN) architecture and its training (section 3.3); section 4 describes the datasets used; section 5 shows the main results; and, finally, section 6 closes the paper with concluding remarks. Related work Pioneering work on word-level MT QE dealt with predictive/interactive MT (Gandrabur and Foster, 2003; Blatz et al., 2004; Ueffing and Ney, 2005, 2007), often under the name of confidence estimation. Estimations relied on the internals of the actual MT system —for instance, studying the nbest translations (Ueffing and Ney, 2007)— or used external sources of bilingual information; for instance, both Blatz et al. (2004) and Ueffing and Ney (2005) used probabilistic dictionaries; in the case of Blatz et al. (2004), as one of many features in a binary classifier for each word. The last decade has witnessed an explosion of work in word-level MT QE, with most of the recent advances made by participants in the shared tasks on MT QE at the different editions of the Conference on Statistical Machine Translation (WMT). Therefore, we briefly review those papers related to our approach: those using an external bilingual source s"
W18-6464,2015.eamt-1.4,1,\N,Missing
W19-6721,W18-6488,0,0.104438,"Missing"
