2006.amta-papers.22,P05-1048,0,0.0243424,"the translation. State-of-the-art SMT systems use a phrase-based approach to translation (Och et al., 1999; Koehn et al., 2003; Tillmann, 2003; Zens and Ney, 2004), where translations are obtained by concatenating translations of chunks of words (as opposed to single words) in the input sentence. When training and test data are matched, a phrase-based SMT system can implicitly perform word-sense disambiguation (WSD) and choose the correct translation because the local context of the input word is taken into consideration. In fact, additional explicit WSD has not been shown to be helpful (e.g. Carpuat and Wu, 2005). Under mismatched conditions, however, lexical ambiguity may become a much more significant source of errors because word senses occurring in the test data may never have occurred in similar context in the training data. In this paper we present CL-MT, a hybrid controlled language-statistical MT system where cross-domain SMT is improved by human guided WSD. This study is part of a larger research effort on utilizing machine translation technology to enhance human-human communication. A typical application scenario is on-the-fly automatic email translation, where two users that do not share th"
2006.amta-papers.22,W03-1001,0,0.0133099,"best performance is typically achieved when the texts to be translated are drawn from the same population of texts as the training data. Unfortunately, many real world applications are for target domains or genres for which readily available parallel training corpora do not exist. Mismatches between training and test data result in deteriorated performance. One source of translation errors is lexical ambiguity in the input text, which may result in lexical errors in the translation. State-of-the-art SMT systems use a phrase-based approach to translation (Och et al., 1999; Koehn et al., 2003; Tillmann, 2003; Zens and Ney, 2004), where translations are obtained by concatenating translations of chunks of words (as opposed to single words) in the input sentence. When training and test data are matched, a phrase-based SMT system can implicitly perform word-sense disambiguation (WSD) and choose the correct translation because the local context of the input word is taken into consideration. In fact, additional explicit WSD has not been shown to be helpful (e.g. Carpuat and Wu, 2005). Under mismatched conditions, however, lexical ambiguity may become a much more significant source of errors because wor"
2006.amta-papers.22,H05-1097,0,0.0173506,"e cases where CLRelated Research There have been several research papers recently on incorporating WSD into SMT. Carpuat and Wu conducted experiments using a WSD classifier for Chinese based on an ensemble of naïve Bayes, maximum entropy, AdaBoost, and Kernel PCAbased classifiers (Carpuat and Wu, 2005). These classifiers had a much richer feature set of contextual information than was available to the phrasal SMT system that Carpuat and Wu used. They found that BLEU scores declined when the WSD system was used to override the translation chosen by the SMT system. A research group at Stanford (Vickrey et al., 2005) applied automatic WSD where the word senses of an English word were taken to be its possible French translations. Their system succeeded in finding the correct translation in a “fill in the blank” experiment, but did not find significant improvements in translation accuracy of full sentences. The use of human-verified WSD has been explored by Translution.com (Orasan et al., 2005). Their method applies only to language pairs where both languages have EuroWordNet thesauri (www.illc.uva.nl/EuroWordNet). They use WordNet’s interlingual index to link word senses in the source language with corresp"
2006.amta-papers.22,W99-0905,0,0.0159966,"e of human-verified WSD has been explored by Translution.com (Orasan et al., 2005). Their method applies only to language pairs where both languages have EuroWordNet thesauri (www.illc.uva.nl/EuroWordNet). They use WordNet’s interlingual index to link word senses in the source language with corresponding senses in the target language. They reported on techniques to prune out irrelevant word senses to avoid overburdening a user, but did not report on how the WSD affected translation accuracy. A promising approach to building a CL lexicon without an MRD available is corpus-based cluster200 ing (Kikui, 1999). Kikui uses distributional clustering to identify the word sense of a source language word, and then tests each translation from a bilingual dictionary to find a translation whose context in the target language corpus best matches the context for that sense in the source language corpus. The controlled language of CL-MT is qualitatively different than that of other research in controlled language. Our CL lexicon is designed to be domain independent and must deal directly with ambiguity of nearly all terms. Other CL systems have been developed for narrow domains, or at best, with a domain-inde"
2006.amta-papers.22,P95-1026,0,0.193642,"Missing"
2006.amta-papers.22,W05-0821,1,0.884247,"Missing"
2006.amta-papers.22,N04-1033,0,0.0174891,"e is typically achieved when the texts to be translated are drawn from the same population of texts as the training data. Unfortunately, many real world applications are for target domains or genres for which readily available parallel training corpora do not exist. Mismatches between training and test data result in deteriorated performance. One source of translation errors is lexical ambiguity in the input text, which may result in lexical errors in the translation. State-of-the-art SMT systems use a phrase-based approach to translation (Och et al., 1999; Koehn et al., 2003; Tillmann, 2003; Zens and Ney, 2004), where translations are obtained by concatenating translations of chunks of words (as opposed to single words) in the input sentence. When training and test data are matched, a phrase-based SMT system can implicitly perform word-sense disambiguation (WSD) and choose the correct translation because the local context of the input word is taken into consideration. In fact, additional explicit WSD has not been shown to be helpful (e.g. Carpuat and Wu, 2005). Under mismatched conditions, however, lexical ambiguity may become a much more significant source of errors because word senses occurring in"
2006.amta-papers.22,W05-0820,0,0.0205761,"Missing"
2006.amta-papers.22,N03-1017,0,0.00450801,"anslation model. The best performance is typically achieved when the texts to be translated are drawn from the same population of texts as the training data. Unfortunately, many real world applications are for target domains or genres for which readily available parallel training corpora do not exist. Mismatches between training and test data result in deteriorated performance. One source of translation errors is lexical ambiguity in the input text, which may result in lexical errors in the translation. State-of-the-art SMT systems use a phrase-based approach to translation (Och et al., 1999; Koehn et al., 2003; Tillmann, 2003; Zens and Ney, 2004), where translations are obtained by concatenating translations of chunks of words (as opposed to single words) in the input sentence. When training and test data are matched, a phrase-based SMT system can implicitly perform word-sense disambiguation (WSD) and choose the correct translation because the local context of the input word is taken into consideration. In fact, additional explicit WSD has not been shown to be helpful (e.g. Carpuat and Wu, 2005). Under mismatched conditions, however, lexical ambiguity may become a much more significant source of er"
2006.amta-papers.22,koen-2004-pharaoh,0,0.0465878,"Missing"
2006.amta-papers.22,2003.eamt-1.10,0,0.0442136,"each translation from a bilingual dictionary to find a translation whose context in the target language corpus best matches the context for that sense in the source language corpus. The controlled language of CL-MT is qualitatively different than that of other research in controlled language. Our CL lexicon is designed to be domain independent and must deal directly with ambiguity of nearly all terms. Other CL systems have been developed for narrow domains, or at best, with a domain-independent architecture that relies on domain-specific knowledge. The Kant system (Nyberg and Mitamura, 1996; Mitamura et al., 2003) was developed primarily for one-way translation of Caterpillar Tractor manuals from English. Nearly all of the content words are restricted to a single word sense, and multi-word noun phrases are only allowed if explicitly in the lexicon. Kant would reject “oil filter change” even though “oil filter” and “change” are both in the lexicon (“change of oil filter” is permitted). Attempto Controlled English (ACE) (Fuchs et al. 1998) and Processable English (PENG) (Schwitter 2002) are similarly designed for technical specifications in narrow domains. 6 Conclusions and Future Work We have tested the"
2006.amta-papers.22,W99-0604,0,0.0230932,"ed to train the translation model. The best performance is typically achieved when the texts to be translated are drawn from the same population of texts as the training data. Unfortunately, many real world applications are for target domains or genres for which readily available parallel training corpora do not exist. Mismatches between training and test data result in deteriorated performance. One source of translation errors is lexical ambiguity in the input text, which may result in lexical errors in the translation. State-of-the-art SMT systems use a phrase-based approach to translation (Och et al., 1999; Koehn et al., 2003; Tillmann, 2003; Zens and Ney, 2004), where translations are obtained by concatenating translations of chunks of words (as opposed to single words) in the input sentence. When training and test data are matched, a phrase-based SMT system can implicitly perform word-sense disambiguation (WSD) and choose the correct translation because the local context of the input word is taken into consideration. In fact, additional explicit WSD has not been shown to be helpful (e.g. Carpuat and Wu, 2005). Under mismatched conditions, however, lexical ambiguity may become a much more sign"
2006.iwslt-evaluation.21,1983.tc-1.13,0,0.576574,"specific processing are presented in Sections 6 and 7. We then present experiments and the official evaluation results. Section 10 describes additional analyses performed after the official evaluation and Section 11 concludes. 2. Data The UW system participated in the open data track. For training we used the BTEC Italian-English training data provided for this evaluation campaign, along with the devset1, devset2, and devset3, resulting in approximately 190K words of in-domain training data (including punctuation). In addition, we used the publicly available Europarl corpus of Italian/English [1] for training the translation model. This corpus is very different from BTEC in that it contains edited transcriptions of parliamentary proceedings; thus, the domain differs from that of a travel task, and the style is that of written text. The size of the Europarl corpus is approximately 17M words. We also used the Fisher corpus for training certain second-pass language models. The Fisher corpus is a collection of English conversational telephone speech covering a variety of speakers and topics. It consists of approximately 2.3M word tokens. All development/evaluation was done on devset4, sin"
2006.iwslt-evaluation.21,koen-2004-pharaoh,0,0.0495537,"immediata scarcerazione. However, these modification did not affect translation performance significantly. 145 4. First-Pass Translation System Two analogous lexical scores are computed, e.g.: We use a multi-pass statistical phrase-based translation system based on a log-linear probability model: e∗ = argmaxe p(e|f ) = argmaxe { K X λk φk (e, f )} (1) where e is and English and f a foreign sentence, φ(e, f ) is a feature function defined on both sentences, and λ is a feature weight. The first pass generates up to 2000 translation hypotheses per sentence using the public-domain Pharaoh decoder [2] and a combination of the following nine model scores: two phrase-based translation scores two lexical translation scores data source indicator feature word transition penalty phrase penalty distortion penalty language model score X 1 p(fj |ei ) |{i|a(i) = j}| (3) a(i)=j where j ranges over words in phrase f¯ and i ranges over words in phrase e¯. Here, we use two phrase tables concomitantly, one trained from each data source (BTEC and Europarl). We use the two phrase tables jointly, without renormalization of probabilities. An additional binary feature in the log-linear combination indicates w"
2006.iwslt-evaluation.21,P03-1021,0,0.0553976,"o of these are explained below (Section 4.1). The word transition and phrase penalty are constant weights added for each word/phrase used in the translation, thus controlling the length of the translation. The distortion penalty assigns a weight proportional to the distance by which phrases are reordered during decoding; here, the distortion penalty is constant since monotone decoding is used and no reordering is allowed. (Initial experiments show that monotone decoding outperforms non-monotone decoding.) Weights for these scores are optimized using the minimumerror rate training procedure in [3]. The optimization criterion is the BLEU score on the development set as defined above (Section 2). The second pass rescores the first-pass output with additional, more advanced model scores. A postprocessing step is then performed to restore true case and punctuation. 4.1. Translation Model The translation model is defined over a segmentation of source and target sentence into phrases: f = f¯1 , f¯2 , ..., f¯M and e = e¯1 , e¯2 , ..., e¯M . Phrase pairs of up to length 7 are extracted from the training corpus which was previously wordaligned using GIZA++. The extraction method is the techniqu"
2006.iwslt-evaluation.21,J03-1002,0,0.00589373,"ion criterion is the BLEU score on the development set as defined above (Section 2). The second pass rescores the first-pass output with additional, more advanced model scores. A postprocessing step is then performed to restore true case and punctuation. 4.1. Translation Model The translation model is defined over a segmentation of source and target sentence into phrases: f = f¯1 , f¯2 , ..., f¯M and e = e¯1 , e¯2 , ..., e¯M . Phrase pairs of up to length 7 are extracted from the training corpus which was previously wordaligned using GIZA++. The extraction method is the technique described in [4] and implemented in [2]: the corpus is first aligned in both translation directions, the intersection of the alignment points is taken, and additional alignment points are added heuristically. For each phrase pair, two phrasal ¯ e) and P (¯ ¯ are computed translation probabilities, P (f|¯ e|f), (one for each direction) from the relative frequency estimate on the training data, e.g.: count(¯ e, f¯) P (¯ e|f¯) = ¯ count(f) J Y j=1 k=1 • • • • • • • Scorelex (f¯|¯ e) = (2) 5. Rescoring The rescoring stage uses the first pass model scores along with five additional scores, as described below. Scor"
2006.iwslt-evaluation.21,N04-1021,0,0.0566079,"tional scores are: • • • • • a 4-gram language model score a POS n-gram score rank in N-best list Factored Language Model score ratio, and focused language model score The last three are novel features in our system. 4-gram language model score (lm) This is the score of a 4-gram language model trained on the English side of the BTEC training corpus using modified Kneser-Ney smoothing. POS n-gram model score (pos) The part-of-speech (POS) sequence of a given target sentence can be indicative of the sentence’s syntactical wellformedness and thus translation fluency. Although it was cautioned in [6] that applying POS taggers directly to MT hypotheses may generate unexpected results (e.g. inserting a verb tag when there is no verb in the sentence), in practice we have found it useful to apply a POS language model to our N-best lists. We obtain POS annotations by applying the Maximum Entropy tagger of [7]. This tagger has been trained on the Wall Street Journal corpus; we apply it directly to our training set and N-best lists. In order to increase the training data for the POS n-gram we also used the Fisher 146 250 histogram count (bin=50) 200 150 100 50 0 0 100 200 300 400 rank of oracle"
2006.iwslt-evaluation.21,N03-2002,1,0.881972,"nk:2) the store is open on sundays (rank:1) the store is it open on sundays (rank:3) For our experiments, we slightly modified the above rank feature by applying a log function to the raw values. This bounds the features to a smaller range, similar to that of other features in the log-linear combination. We found that this did slightly better in our experiments than raw integer ranks. As shown in experiments (Section 8), the rank feature, is consistently the most useful feature in rescoring despite its simplicity. Ratio of Factored Language Model scores (ratio) Factored Language Models (FLMs) [8] are a flexible language modeling framework that can incorporate diverse sources of information, such as morphology, POS tags, etc. Previous experiments on using FLMs to rescore machine translation N-best lists have seen mixed results: little gain was shown for translation into English [9] but larger gains were shown for translation into Spanish, a morphologically richer language, especially under mismatched conditions [10]. Here, we use FLMs with three sources of information: words, part-of-speech, and data-driven word clusters in a trigram context. Word clusters were obtained by Brown cluste"
2006.iwslt-evaluation.21,W05-0821,1,0.855585,"ar combination. We found that this did slightly better in our experiments than raw integer ranks. As shown in experiments (Section 8), the rank feature, is consistently the most useful feature in rescoring despite its simplicity. Ratio of Factored Language Model scores (ratio) Factored Language Models (FLMs) [8] are a flexible language modeling framework that can incorporate diverse sources of information, such as morphology, POS tags, etc. Previous experiments on using FLMs to rescore machine translation N-best lists have seen mixed results: little gain was shown for translation into English [9] but larger gains were shown for translation into Spanish, a morphologically richer language, especially under mismatched conditions [10]. Here, we use FLMs with three sources of information: words, part-of-speech, and data-driven word clusters in a trigram context. Word clusters were obtained by Brown clustering [11] using 500 word classes. In this work, we apply FLMs to rescore English, but improve upon previous attempts by using two FLMs together in a discriminative fashion. In order to train the backoff structure and smoothing options of an FLM we use a genetic algorithm [12]. This require"
2006.iwslt-evaluation.21,J92-4003,0,0.173358,"a flexible language modeling framework that can incorporate diverse sources of information, such as morphology, POS tags, etc. Previous experiments on using FLMs to rescore machine translation N-best lists have seen mixed results: little gain was shown for translation into English [9] but larger gains were shown for translation into Spanish, a morphologically richer language, especially under mismatched conditions [10]. Here, we use FLMs with three sources of information: words, part-of-speech, and data-driven word clusters in a trigram context. Word clusters were obtained by Brown clustering [11] using 500 word classes. In this work, we apply FLMs to rescore English, but improve upon previous attempts by using two FLMs together in a discriminative fashion. In order to train the backoff structure and smoothing options of an FLM we use a genetic algorithm [12]. This requires a held-out set for iteratively optimizing the model parameters. While normally the references for some development set would be used for this purpose, in the context of machine translation we use the oracle-best hypotheses from the first pass, to ensure that the model is optimized on hypotheses that are likely to re"
2006.iwslt-evaluation.21,C04-1022,1,0.893952,"n into English [9] but larger gains were shown for translation into Spanish, a morphologically richer language, especially under mismatched conditions [10]. Here, we use FLMs with three sources of information: words, part-of-speech, and data-driven word clusters in a trigram context. Word clusters were obtained by Brown clustering [11] using 500 word classes. In this work, we apply FLMs to rescore English, but improve upon previous attempts by using two FLMs together in a discriminative fashion. In order to train the backoff structure and smoothing options of an FLM we use a genetic algorithm [12]. This requires a held-out set for iteratively optimizing the model parameters. While normally the references for some development set would be used for this purpose, in the context of machine translation we use the oracle-best hypotheses from the first pass, to ensure that the model is optimized on hypotheses that are likely to result in a good BLEU score. Here, we form two held-out sets, one consisting of the set of oracle best hypotheses from the N-best lists, and the other consisting of the set of oracle worst hypotheses from the N-best lists. The FLM optimized on the oracle best hypothese"
2006.iwslt-evaluation.21,N04-1023,0,0.0463176,"est hypotheses from the N-best lists, and the other consisting of the set of oracle worst hypotheses from the N-best lists. The FLM optimized on the oracle best hypotheses should give high probability to sentences with high BLEU scores, while the FLM optimized on the oracle worst sentences will give high probability to sentences with low BLEU scores. The score used for rescoring then LM1 (e) is φratio (e) = F F LM2 (e) , where F LMi (e), i = 1, 2 is the probability of sentence e evaluated by the first and second FLMs, respectively. This method is analogous to the “splitting” technique used in [13], which divides the N-best list into good and bad sentences for training a perceptron-style learner. Our method differs in that instead of using a discriminative classifier, we use two generative models (FLMs) and take the log-probability ratio. This allows us to take advantage of the estimation techniques developed for language models. Focused LM (focus, focusF) The focused language model is a dynamically generated language model that focuses only on those words that occur in the N-best list. During the training phase of rescoring, we collect all the words in the N-best lists and use our trai"
2006.iwslt-evaluation.21,E06-1005,0,0.0472346,"Missing"
2007.iwslt-1.13,H05-1022,0,0.0124251,"= f¯1 , f¯2 , ..., f¯M and e = e¯1 , e¯2 , ..., e¯M , the phrasal translation score for e¯ given f¯ is computed as count(¯ e, f¯) P (¯ e|f¯) = (2) ¯ count(f) i.e. as the relative frequency estimate from the phrasesegmented training corpus. The lexical score is computed as Scorelex (¯ e|f¯) = J Y j=1 I X 1 p(fj |ei ) |{j|a(i) = j}| (3) a(i)=j where j ranges over words in phrase f¯ and i ranges over words in phrase e¯. Phrases are extracted from the word-aligned training corpus using the heuristic technique described in [6]. For word alignment we use an HMM-based word to phrase alignment model [1]. Under this model, target phrases are generated by individual source words, including the NULL word. The alignment of source words and target phrases is governed by a first-order Markov process. For a target sentence f of length m, segmented into K phrases, a source sentence e of length l, and a sequence of alignment variables aK 1 , the alignment model is specified as # sentence pairs average # words K K P (aK 1 , h1 , φ1 |K, m, e) = K Y P (ak , hk , φk |ak−1 φk−1 , e) p(ak |ak−1 , hk ; l)d(hk )n(φk ; eak ) k=1 where hK 1 is a series of binary variables indicating whether a phrase is inserte"
2007.iwslt-1.13,N06-4004,0,0.0115201,"ength l, and a sequence of alignment variables aK 1 , the alignment model is specified as # sentence pairs average # words K K P (aK 1 , h1 , φ1 |K, m, e) = K Y P (ak , hk , φk |ak−1 φk−1 , e) p(ak |ak−1 , hk ; l)d(hk )n(φk ; eak ) k=1 where hK 1 is a series of binary variables indicating whether a phrase is inserted or not, φK 1 are variables controlling the length of the target phrase, n(φ; e) is a simple length model for a word e producing a phrase of length φ, and d is an i.i.d. process with d(0) = p0 , d(1) = 1 − p0 . We use the word-to-phrase alignment as implemented by the MTTK package [2]. A previous comparison against GIZA++ based word alignment on the IWSLT 2006 training corpus for Italian-to-English found a marginal improvement of this model over GIZA++. Word count and phrase count penalties are constant weights added for each word/phrase used in the translation; the distortion penalty is a weight that increases in proportion to the number of positions by which phrases are reordered during translation. The language model score is obtained from a trigram trained using SRILM [11]. The weights for these scores are optimized using an in-house implementation of the minimum-error"
2007.iwslt-1.13,P07-2045,0,0.0173868,"n. An additional feature function (or functions, in the case of more than two data sources) in the log-linear model indicates which data source a phrase pair was extracted from; the weight for this feature is optimized along with all other feature weights to maximize the BLEU score on the development set. Identical phrase pairs extracted from different data sources are included in the phrase table multiple times, along with their different scores and respective data source features. We found this method to be helpful on previous IWSLT tasks. 2.2. Decoding For decoding we use the Moses package [4] in its basic form, i.e. without making use of any of its advanced features such as factored translation models. We utilize two decoding passes, a first pass that generates up to 2000 hypotheses per sentence, and a second pass that rescores the initial hypotheses by utilizing additional model scores. For both systems we use a part-of-speech based trigram language model in the second pass. The parts-of-speech were annotated using a maximum-entropy tagger for English [8]. Additional models are specific to each language pair and are discussed Europarl 625,320 17M Table 1: Sizes of the Italian dat"
2007.iwslt-1.13,P03-1051,0,0.0337865,"Missing"
2007.iwslt-1.13,J03-1002,0,0.00279589,"2 and 3. For a segmentation of source and target sentences into phrases, f = f¯1 , f¯2 , ..., f¯M and e = e¯1 , e¯2 , ..., e¯M , the phrasal translation score for e¯ given f¯ is computed as count(¯ e, f¯) P (¯ e|f¯) = (2) ¯ count(f) i.e. as the relative frequency estimate from the phrasesegmented training corpus. The lexical score is computed as Scorelex (¯ e|f¯) = J Y j=1 I X 1 p(fj |ei ) |{j|a(i) = j}| (3) a(i)=j where j ranges over words in phrase f¯ and i ranges over words in phrase e¯. Phrases are extracted from the word-aligned training corpus using the heuristic technique described in [6]. For word alignment we use an HMM-based word to phrase alignment model [1]. Under this model, target phrases are generated by individual source words, including the NULL word. The alignment of source words and target phrases is governed by a first-order Markov process. For a target sentence f of length m, segmented into K phrases, a source sentence e of length l, and a sequence of alignment variables aK 1 , the alignment model is specified as # sentence pairs average # words K K P (aK 1 , h1 , φ1 |K, m, e) = K Y P (ak , hk , φk |ak−1 φk−1 , e) p(ak |ak−1 , hk ; l)d(hk )n(φk ; eak ) k=1 where"
2007.iwslt-1.13,P03-1021,0,0.00447938,"based word alignment on the IWSLT 2006 training corpus for Italian-to-English found a marginal improvement of this model over GIZA++. Word count and phrase count penalties are constant weights added for each word/phrase used in the translation; the distortion penalty is a weight that increases in proportion to the number of positions by which phrases are reordered during translation. The language model score is obtained from a trigram trained using SRILM [11]. The weights for these scores are optimized using an in-house implementation of the minimum-error rate training procedure developed in [7]. Our optimization criterion is the BLEU score on the development set. For both systems we use additional out-of-corpus data sources. As described in [3], these are integrated into the system by training separate phrase tables on each data source and using both tables jointly during decoding without renormalization. An additional feature function (or functions, in the case of more than two data sources) in the log-linear model indicates which data source a phrase pair was extracted from; the weight for this feature is optimized along with all other feature weights to maximize the BLEU score on"
2007.iwslt-1.13,W96-0213,0,\N,Missing
2007.iwslt-1.13,D08-1076,0,\N,Missing
2007.iwslt-1.13,2006.iwslt-evaluation.21,1,\N,Missing
2007.mtsummit-papers.39,W05-0909,0,0.0435449,"to characteristics of the translation input is growing more and more difficult. Problems are aggravated further in the case of speech translation (of e.g. broadcast news, talkshows, etc.), where the input to the translation module is provided by an automatic speech recognition (ASR) system whose performance also influences the quality of the final translation output. During machine translation (MT) system development, automatic evaluation criteria are commonly used to judge performance, such as the BLEU score (Papineni et al. 2002), the NIST score (Doddington 2002), or, more recently, METEOR (Banerjee & Lavie 2005). Although the use of fully automated evaluation criteria is helpful in accelerating the system development cycle, all of the above criteria have shown to be inferior to human judgments of translation performance. Moreover, they do not yield any insight into precisely which input characteristics caused particular translation errors, or which system components need to be improved in order to reach the desired performance level. Human analysis of machine translation errors, on the other hand, is costly and time-intensive and can typically not be performed on a regular basis in the course of syst"
2007.mtsummit-papers.39,E06-1005,0,0.0419776,"Missing"
2007.mtsummit-papers.39,niessen-etal-2000-evaluation,0,0.0362707,"tained from human annotations and are statistically related to measurements of the overall system performance. The various input document features are then ranked with respect to their impact on translation performance. Previous Work Most work on error analysis in statistical machine translation has made use of extensive human analysis, such as classifying unsatisfactory output into categories such as wrong word choice, missing content words, missing function words, etc. (see e.g. Koehn 2003, Och et al. 2003). Previous work on automatic or semi-automatic error analysis in SMT systems includes Niessen et al. (2000), Popovic et al. (2006a) and Popovic et al. (2006b). In Niessen et al. (2002), a graphical user interface was presented that automatically extracts various error measures for translation candidates and thus facilitates manual error analysis. In Popovic et al. (2006a) and Popovic et al. (2006b), errors in an English-Spanish statistical MT system were analyzed with respect to their syntactic and morphological origin. This was done by modifying the references and the machine translation output by eliminating morphological inflections or suspected reordered constituents, and by analyzing the resul"
2007.mtsummit-papers.39,P02-1040,0,0.0911609,"As a consequence, diagnosing problems in translation performance and relating them to characteristics of the translation input is growing more and more difficult. Problems are aggravated further in the case of speech translation (of e.g. broadcast news, talkshows, etc.), where the input to the translation module is provided by an automatic speech recognition (ASR) system whose performance also influences the quality of the final translation output. During machine translation (MT) system development, automatic evaluation criteria are commonly used to judge performance, such as the BLEU score (Papineni et al. 2002), the NIST score (Doddington 2002), or, more recently, METEOR (Banerjee & Lavie 2005). Although the use of fully automated evaluation criteria is helpful in accelerating the system development cycle, all of the above criteria have shown to be inferior to human judgments of translation performance. Moreover, they do not yield any insight into precisely which input characteristics caused particular translation errors, or which system components need to be improved in order to reach the desired performance level. Human analysis of machine translation errors, on the other hand, is costly and time-"
2007.mtsummit-papers.39,W06-3101,0,0.229094,"Missing"
2009.iwslt-evaluation.19,P05-1071,0,0.0368422,"op in BLEU on the held-out set. However, we experimented with combining both tables. To this end the individual tables were combined into a single table containing the 11 standard features (phrasal, lexical, and reordering scores and phrase penalty) plus two additional binary features that indicate which alignment model produced each entry in the phrase table. The weights for these features were optimized along with all other features in the first-pass MERT tuning. 4.3. Language Models 5.1. Preprocessing We preprocessed the Arabic data by using the the Columbia University MADA and TOKAN tools [5]. We compared two tokenization schemes: the first splits off the conjunctions w+, f+, the particles l+, the b+ preposition and the definite article Al+. It also normalizes different variants of alif, final yaa and taa marbuta. The second scheme (equivalent to TOKAN’s D2 scheme) does not split off Al+ but instead separates the prefix s+. Differences between the two schemes were slight; the first scheme yielded a 0.2 increase in BLEU on the heldout set. 5.2. Word Alignment and Phrase Tables As in the Chinese-English system, we trained word alignments using both GIZA++ and MTTK. We found that MTT"
2009.iwslt-evaluation.19,N03-1017,0,0.00778709,"-best lists, which has previously shown improvements on 2007 IWSLT data. We additionally explored different preprocessing schemes for both language pairs, as well as methods for combining phrase tables based on different word alignments. In the following sections we first describe the data, general baseline system and post-processing steps, before describing language-pair specific methods and the semi-supervised reranking method. Our baseline system for this year’s task is a state-of-the-art, two-pass phrase-based statistical machine translation system, based on a log-linear translation model [7]. e∗ = argmaxe p(e|f ) = argmaxe { K X λk φk (e, f )} (1) k=1 where e is an English sentence, f is a foreign sentence, φk (e, f ) is a feature function defined on both sentences, and λk is a feature weight. We trained this model within the Moses development and decoding framework [8]. The feature functions used in this year’s system include: • two phrase-based translation scores, one for each translation direction 2. Corpora and Preprocessing As mandated by the evaluation guidelines, the only data that was used for system development was the official data provided by IWSLT. Training data for t"
2009.iwslt-evaluation.19,P07-2045,0,0.0428537,"e data, general baseline system and post-processing steps, before describing language-pair specific methods and the semi-supervised reranking method. Our baseline system for this year’s task is a state-of-the-art, two-pass phrase-based statistical machine translation system, based on a log-linear translation model [7]. e∗ = argmaxe p(e|f ) = argmaxe { K X λk φk (e, f )} (1) k=1 where e is an English sentence, f is a foreign sentence, φk (e, f ) is a feature function defined on both sentences, and λk is a feature weight. We trained this model within the Moses development and decoding framework [8]. The feature functions used in this year’s system include: • two phrase-based translation scores, one for each translation direction 2. Corpora and Preprocessing As mandated by the evaluation guidelines, the only data that was used for system development was the official data provided by IWSLT. Training data for the BTEC tasks consisted of approximately 20,000 sentence pairs in both the ChineseEnglish and Arabic-English tracks. We used the combined development datasets (about 500 sentences each) for initial system tuning, except for the IWSLT 2008 eval set, which we used as a held-out set for"
2009.iwslt-evaluation.19,J03-1002,0,0.0225109,"es the probablity of a sequence of orientations o = (o1 , o2 , . . . , oM ) P (o|f, e) = M Y P (oi |¯ ei , f¯ai , ai , ai−1 ) (4) i=1 where each oi takes one of the three values: monotone, swap, and discontinuous. This model adds six feature functions to the overall log-linear model: for each of the three orientations, the orders of the source phrase with respect to both the previous and the next source phrase are considered. The feature scores are again estimated by relative frequency. The training corpus was word-aligned by GIZA++; subsequently, phrases were extracted using the technique in [6] and as implemented in the Moses training scripts [8]. We also used an alternative word-alignment based on the MTTK [6] implementation of an HMM-based word-to-phrase alignment model with bigram probabilities. This yielded mixed results, as described in later sections. Word count and phrase count penalties are constant weights added for each word/phrase used in the translation; the distortion penalty is a weight that increases in proportion to the number of positions by which phrases are reordered during translation. The language models used are n-gram models as further described below. The wei"
2009.iwslt-evaluation.19,P03-1021,0,0.012006,"of an HMM-based word-to-phrase alignment model with bigram probabilities. This yielded mixed results, as described in later sections. Word count and phrase count penalties are constant weights added for each word/phrase used in the translation; the distortion penalty is a weight that increases in proportion to the number of positions by which phrases are reordered during translation. The language models used are n-gram models as further described below. The weights for these scores were optimized using an in-house implementation of the minimum-error rate training (MERT) procedure developed in [9]. Our optimization criterion was the BLEU score on the available development set. 3.2. Language Models For first-pass decoding we used trigram language models. We built all of our language models using the SRILM toolkit [4] with modified Kneser-Ney discounting and interpolating all n-gram estimates of order > 1. Due to the small size of the training corpus, we experimented with lowering the minimum count requirement to 1 for all n-grams. This yielded different results for the two different tasks, which are further described below. 3.3. Decoding Our system used the Moses decoder to generate 100"
2009.iwslt-evaluation.19,W08-0336,0,0.0155344,"ut by re-attaching the possessive particle and restoring true case. Truecasing is done by a noisy-channel model as implemented in the disambig tool in the SRILM package. It uses a 4-gram model trained over a mixed-case representation of the BTEC training corpus and a probabilistic mapping table for lowercaseuppercase word variants. The first letter at the beginning of each sentence was uppercased deterministically. 4. Chinese → English 4.1. Preprocessing Although the Chinese training data was pre-segmented we nonetheless explored other segmentation tools. First, we used the Stanford segmenter [2] to resegment the Chinese data, as it provides templates for annotating numbers and dates, potentially aiding in word alignment and phrase extraction. In another experiment, an in-house tool [11] was used to simply markup dates and numbers in the presegmented BTEC data. Third, we developed our own markup tool for numbers. In both Chinese and English, numbers are represented by a combination of a limited set of number words. A simple method for detecting numbers is to first obtain a set of number words and then search for subsentential chunks that are comprised of only number words. These chunk"
2012.amta-papers.29,E09-1005,0,0.0142723,"restrictions were observed in computing coverage rates. and j. The relatedness measure is then computed as point-wise mutual information: p(i, j) p(i) ∗ p(j) Cd (i, j) ∗ N = log Cd (i) ∗ Cd (j) w(i, j) = log (4) where N is the total number of documents in Wikipedia. The three procedures achieve different coverage rates on the German word translations, as shown in Table 3. The count-based method obtains the highest coverage, followed by GermaNet and Wiktionary respectively. Scoring To score all target-language translations for a given source word we use personalized PageRank (Haveliwala, 2002; Agirre and Soroa, 2009), which exploits prior weights on nodes of interest along with the properties of the graph structure. For each node vi ∈ V , let In(vi ) be the set of nodes that point to vi and Out(vi ) the set of nodes that vi points to. Although they are always the same in monolingual graphs, In(vi ) can be different from Out(vi ) in bilingual graphs as described in Section 4.2. R(vi ), the PageRank score of vi , is computed iteratively as R(vi ) = (1 − d)ui X +d R(vj ) P vj ∈In(vi ) k∈Out(vj ) wjk (5) motiv (6) The personalized PageRank algorithm ranks all nodes with u(w) and the translations of w are then"
2012.amta-papers.29,P05-1048,0,0.0914759,"k; they evaluated only small subsets of words and did not use standard machine translation evaluation measures to assess the impact on translation performance. Previous studies addressing the WTD problem in SMT have relied on supervised approaches that involve training statistical classifiers to distinguish between all of the translation options (single words or phrases in a phrase-based SMT system) for the most frequent source phrases. Classifiers were trained based on features of the source phrases and their contexts in the parallel training data. In early experimental investigations (e.g. (Carpuat and Wu, 2005; Cabezas and Resnik, 2005) results based on this approach were inconclusive and showed a lack of improvement or only marginal improvements in standard automatic evaluation scores. Statistically significant improvements to state-of-the-art SMT systems were later reported in (Gimenez and Marquez, 2007; Chang et al., 2007; Carpuat and Wu, 2007). When large amounts of in-domain parallel training data are not available, supervised WTD is suboptimal. However, unsupervised WTD has not been investigated as an alternative. The work most closely related to our scenario is the cross-lingual word sense d"
2012.amta-papers.29,D07-1007,0,0.158639,"ions (single words or phrases in a phrase-based SMT system) for the most frequent source phrases. Classifiers were trained based on features of the source phrases and their contexts in the parallel training data. In early experimental investigations (e.g. (Carpuat and Wu, 2005; Cabezas and Resnik, 2005) results based on this approach were inconclusive and showed a lack of improvement or only marginal improvements in standard automatic evaluation scores. Statistically significant improvements to state-of-the-art SMT systems were later reported in (Gimenez and Marquez, 2007; Chang et al., 2007; Carpuat and Wu, 2007). When large amounts of in-domain parallel training data are not available, supervised WTD is suboptimal. However, unsupervised WTD has not been investigated as an alternative. The work most closely related to our scenario is the cross-lingual word sense disambiguation benchmark task in the recent 2010 SemEval evaluations (Lefever and Hoste, 2010). Here, systems were required to generate sets of target-language translations for English source words, each of which was embedded in an example sentence. Proposed translations were compared against manually annotated translations and were evaluated"
2012.amta-papers.29,P07-1005,0,0.0865613,"the translation options (single words or phrases in a phrase-based SMT system) for the most frequent source phrases. Classifiers were trained based on features of the source phrases and their contexts in the parallel training data. In early experimental investigations (e.g. (Carpuat and Wu, 2005; Cabezas and Resnik, 2005) results based on this approach were inconclusive and showed a lack of improvement or only marginal improvements in standard automatic evaluation scores. Statistically significant improvements to state-of-the-art SMT systems were later reported in (Gimenez and Marquez, 2007; Chang et al., 2007; Carpuat and Wu, 2007). When large amounts of in-domain parallel training data are not available, supervised WTD is suboptimal. However, unsupervised WTD has not been investigated as an alternative. The work most closely related to our scenario is the cross-lingual word sense disambiguation benchmark task in the recent 2010 SemEval evaluations (Lefever and Hoste, 2010). Here, systems were required to generate sets of target-language translations for English source words, each of which was embedded in an example sentence. Proposed translations were compared against manually annotated translati"
2012.amta-papers.29,P05-1033,0,0.107739,"set. More details on baseline system training can be found in (Yang and Kirchhoff, 2010). Table 1 shows the BLEU and positionindependent error rate (PER) score of the baseline system on the evaluation data, in comparison to that of a freely available generic SMT system (Google Translate3 ). Both systems have similar performance; Google Translate achieves better BLEU scores but lower PER. This is explained by the fact that our system achieves more 1-gram matches but has worse word ordering than Google Translate. In addition to a standard phrase-based system, a hierarchical phrase-based system (Chiang, 2005; Li et al., 2009) was trained for comparison; however, its performance only differed insignificantly from that of the standard phrase-based system. 4 Word Translation Disambiguation Approaches Following monolingual unsupervised WSD approaches, and expanding on (Yang and Kirch1 2 3 www.iccs.inf.ed.ac.uk/˜pkoehn/publications/de-news www.dict.cc and www-user.tu-chemnitz.de/ding www.google.com/translate hoff, 2010), we utilize a graph-based ranking algorithm for disambiguation. We compare two different graph-based methods, a monolingual method that only uses information from the target language,"
2012.amta-papers.29,J94-4003,0,0.290952,"d the top node is chosen as the correct word sense. In machine translation the problem is to identify the most appropriate target-language translation for a source-language word in a given context. The ultimate goal is to discriminate between actual lexical items and not word sense labels per se – although different translation options often correspond to different senses of the source-language word they may also represent alternative translations of the same sourcelanguage word sense. We therefore refer to this problem as word translation disambiguation (WTD). WTD has been addressed in e.g. (Dagan and Itai, 1994; Kikui, 1999; Li and Li, 2002). However, these studies predate the current SMT framework; they evaluated only small subsets of words and did not use standard machine translation evaluation measures to assess the impact on translation performance. Previous studies addressing the WTD problem in SMT have relied on supervised approaches that involve training statistical classifiers to distinguish between all of the translation options (single words or phrases in a phrase-based SMT system) for the most frequent source phrases. Classifiers were trained based on features of the source phrases and th"
2012.amta-papers.29,W07-0719,0,0.025526,"distinguish between all of the translation options (single words or phrases in a phrase-based SMT system) for the most frequent source phrases. Classifiers were trained based on features of the source phrases and their contexts in the parallel training data. In early experimental investigations (e.g. (Carpuat and Wu, 2005; Cabezas and Resnik, 2005) results based on this approach were inconclusive and showed a lack of improvement or only marginal improvements in standard automatic evaluation scores. Statistically significant improvements to state-of-the-art SMT systems were later reported in (Gimenez and Marquez, 2007; Chang et al., 2007; Carpuat and Wu, 2007). When large amounts of in-domain parallel training data are not available, supervised WTD is suboptimal. However, unsupervised WTD has not been investigated as an alternative. The work most closely related to our scenario is the cross-lingual word sense disambiguation benchmark task in the recent 2010 SemEval evaluations (Lefever and Hoste, 2010). Here, systems were required to generate sets of target-language translations for English source words, each of which was embedded in an example sentence. Proposed translations were compared against manually"
2012.amta-papers.29,P10-4004,0,0.0126104,"es of Wikimedia are: (a) larger (and still growing) coverage than either WordNet or standard dictionaries; (b) public availability; and (c) inclusion of not only sense glosses but also explicit links to translations of word senses into other languages in Wiktionary. Finally, word co-occurrence based approaches count the relative frequency of two words occurring in the same document, or some measure derived from such counts (such as mutual information). We compare three monolingual disambiguation approaches exploiting each type of relatedness measure: Wordnet-based relatedness We use GermaNet (Henrich and Hinrichs, 2010) to compute a path-based word relatedness score, expressed as the reciprocal of the average distance between two words in the GermaNet database across all combinations of synsets to which either word belongs. w(i, j) = 1+ P si ,sj 1 P 1 d(si , sj )/ (1) si ,sj ∀(si , sj ) ∈ {si ∈ Si , sj ∈ Sj , d(si , sj ) &lt; inf} where Si is the set of synsets containing word i and d(si , sj ) is the shortest-path distance between si and sj as computed by the get shortest path routine in the GermaNet perl API. When no path is found, the shortestpath distance is infinite. For example, synsets of different POS c"
2012.amta-papers.29,W08-0510,0,0.0206767,"Missing"
2012.amta-papers.29,W99-0905,0,0.0304711,"en as the correct word sense. In machine translation the problem is to identify the most appropriate target-language translation for a source-language word in a given context. The ultimate goal is to discriminate between actual lexical items and not word sense labels per se – although different translation options often correspond to different senses of the source-language word they may also represent alternative translations of the same sourcelanguage word sense. We therefore refer to this problem as word translation disambiguation (WTD). WTD has been addressed in e.g. (Dagan and Itai, 1994; Kikui, 1999; Li and Li, 2002). However, these studies predate the current SMT framework; they evaluated only small subsets of words and did not use standard machine translation evaluation measures to assess the impact on translation performance. Previous studies addressing the WTD problem in SMT have relied on supervised approaches that involve training statistical classifiers to distinguish between all of the translation options (single words or phrases in a phrase-based SMT system) for the most frequent source phrases. Classifiers were trained based on features of the source phrases and their contexts"
2012.amta-papers.29,2005.mtsummit-papers.11,0,0.0761933,"Missing"
2012.amta-papers.29,W04-0834,0,0.035388,"g an SMT system across domains or styles: when the distributions underlying the training and test data differ strongly, little relevant training material will be available for the word sense classifier. In this study we therefore focus on the problem of unsupervised word transBackground and Related Work The process of automatically selecting the appropriate word sense in a given context is referred to as word sense disambiguation (WSD). In monolingual settings both supervised and unsupervised WSD approaches have been proposed (see (Navigli, 2009) for an overview). Supervised approaches (e.g. (Lee et al., 2004; Joshi et al., 2006)) typically represent a word and its surrounding context as a feature vector. Feature vectors and manually annotated gold labels are then used to train a statistical classifier which is subsequently applied to unlabeled test data to predict the correct sense for each word instance. State-of-the-art unsupervised WSD algorithms mostly rely on graph-based ranking algorithms (Mihalcea, 2005; Navigli and Lapata, 2007), where all possible senses of the words of interest in a sentence or document are represented as nodes in a graph. Nodes are connected by edges weighted with a si"
2012.amta-papers.29,S10-1003,0,0.0311537,"clusive and showed a lack of improvement or only marginal improvements in standard automatic evaluation scores. Statistically significant improvements to state-of-the-art SMT systems were later reported in (Gimenez and Marquez, 2007; Chang et al., 2007; Carpuat and Wu, 2007). When large amounts of in-domain parallel training data are not available, supervised WTD is suboptimal. However, unsupervised WTD has not been investigated as an alternative. The work most closely related to our scenario is the cross-lingual word sense disambiguation benchmark task in the recent 2010 SemEval evaluations (Lefever and Hoste, 2010). Here, systems were required to generate sets of target-language translations for English source words, each of which was embedded in an example sentence. Proposed translations were compared against manually annotated translations and were evaluated by precision and recall. This task is similar to ours in that no gold labels were provided and translations were judged with respect to how context-appropriate they were. The difference is that the SemEval task only included nouns whereas our goal is to disambiguate translation candidates for all open-class words in the source language. In additio"
2012.amta-papers.29,P02-1044,0,0.021738,"rect word sense. In machine translation the problem is to identify the most appropriate target-language translation for a source-language word in a given context. The ultimate goal is to discriminate between actual lexical items and not word sense labels per se – although different translation options often correspond to different senses of the source-language word they may also represent alternative translations of the same sourcelanguage word sense. We therefore refer to this problem as word translation disambiguation (WTD). WTD has been addressed in e.g. (Dagan and Itai, 1994; Kikui, 1999; Li and Li, 2002). However, these studies predate the current SMT framework; they evaluated only small subsets of words and did not use standard machine translation evaluation measures to assess the impact on translation performance. Previous studies addressing the WTD problem in SMT have relied on supervised approaches that involve training statistical classifiers to distinguish between all of the translation options (single words or phrases in a phrase-based SMT system) for the most frequent source phrases. Classifiers were trained based on features of the source phrases and their contexts in the parallel tr"
2012.amta-papers.29,W09-0424,0,0.0157228,"ils on baseline system training can be found in (Yang and Kirchhoff, 2010). Table 1 shows the BLEU and positionindependent error rate (PER) score of the baseline system on the evaluation data, in comparison to that of a freely available generic SMT system (Google Translate3 ). Both systems have similar performance; Google Translate achieves better BLEU scores but lower PER. This is explained by the fact that our system achieves more 1-gram matches but has worse word ordering than Google Translate. In addition to a standard phrase-based system, a hierarchical phrase-based system (Chiang, 2005; Li et al., 2009) was trained for comparison; however, its performance only differed insignificantly from that of the standard phrase-based system. 4 Word Translation Disambiguation Approaches Following monolingual unsupervised WSD approaches, and expanding on (Yang and Kirch1 2 3 www.iccs.inf.ed.ac.uk/˜pkoehn/publications/de-news www.dict.cc and www-user.tu-chemnitz.de/ding www.google.com/translate hoff, 2010), we utilize a graph-based ranking algorithm for disambiguation. We compare two different graph-based methods, a monolingual method that only uses information from the target language, and a bilingual me"
2012.amta-papers.29,N07-1025,0,0.0357613,"glosses (Lesk, 1986), and word cooccurrence counts (Gabrilovich and Markovitch, 2005; Mihalcea and Corley, 2006). Wordnetstyle resources provide an explicit account of semantic relationships between words, such as hyponymy and hyperonymy but they only exist for a small number of languages and typically have low coverage. The gloss-based approach computes the degree of word overlap in dictionary glosses for different words. While traditional dictionaries may also suffer from the low coverage problem, more recent approaches use Wikimedia (Wiktionary and/or Wikipedia) resources for this purpose (Mihaelcea, 2007; Ponzetto and Strube, 2007; Zesch et al., 2008a; Zesch et al., 2008b). The advantages of Wikimedia are: (a) larger (and still growing) coverage than either WordNet or standard dictionaries; (b) public availability; and (c) inclusion of not only sense glosses but also explicit links to translations of word senses into other languages in Wiktionary. Finally, word co-occurrence based approaches count the relative frequency of two words occurring in the same document, or some measure derived from such counts (such as mutual information). We compare three monolingual disambiguation approaches expl"
2012.amta-papers.29,H05-1052,0,0.103506,"sense disambiguation (WSD). In monolingual settings both supervised and unsupervised WSD approaches have been proposed (see (Navigli, 2009) for an overview). Supervised approaches (e.g. (Lee et al., 2004; Joshi et al., 2006)) typically represent a word and its surrounding context as a feature vector. Feature vectors and manually annotated gold labels are then used to train a statistical classifier which is subsequently applied to unlabeled test data to predict the correct sense for each word instance. State-of-the-art unsupervised WSD algorithms mostly rely on graph-based ranking algorithms (Mihalcea, 2005; Navigli and Lapata, 2007), where all possible senses of the words of interest in a sentence or document are represented as nodes in a graph. Nodes are connected by edges weighted with a similarity or relatedness measure. A link analysis algorithm such as PageRank (Brin and Page, 1998) is used to assign scores to each node; nodes representing mutually exclusive word senses are then ranked by this score and the top node is chosen as the correct word sense. In machine translation the problem is to identify the most appropriate target-language translation for a source-language word in a given co"
2012.amta-papers.29,N04-3012,0,0.0740702,"Missing"
2012.amta-papers.29,S10-1027,0,0.0126074,"used for creating pseudo-glosses for our monolingual graph, we treat every English word sense separately; thus, descriptions of different senses for the same English word are not merged. Table 4 shows the senses and corresponding glosses of the English word folder. The weights of monolingual edges connecting nodes in the same language are then determined as shown in Equation 3. The weights of bilingual edges connecting nodes of English sense to German words are uniformly set to 1. Finally, the PageRank algorithm is run as described above. Multilingual graphs were used for crosslingual WSD in (Silberer and Ponzetto, 2010), where graphs were constructed over nodes representing source words and their translations into different target languages, provided by wordaligned parallel corpora. Differing from our work, the use of multilingual graphs there was to disambiguate the sense of a given source word. Translation candidates corresponding to the selected sense were subsequently ranked by their translation frequencies in the parallel corpora but not by the scores induced from the graphs. In addition, bilingual edges in the graphs, which encode translation relationships, were only used to select disambiguating evide"
2012.amta-papers.29,C10-1138,1,0.904563,"all open-class words in the source language. In addition, there was no mismatch between the test data and the data that provided the translation inventory. By contrast, in this study we have only limited domainmatched parallel training data, which increases the difficulty of the problem. Finally, we also evaluate the effect of WTD on end-to-end translation performance. 3 Data and System Our study is part of a research effort on translating unconstrained conversational speech. For this purpose we use the translated portion of the the AMI multimodal meeting corpus (Carletta, 2007) described in (Yang and Kirchhoff, 2010). This subset consists of 10 meetings whose audio transcriptions were translated from English into German. The length of a single meeting conversation ranges between 2300 and 5700 words; in total the corpus contains roughly 36K words. Translating meeting conversations is a difficult task and one of the unsolved challenges for SMT – in addition to the wide variety of topics and domains encountered in meetings, conversational speech differs strongly from text in style. Large amounts of relevant training data are non-existent and generally hard to collect since it requires both the transcription"
2012.amta-papers.29,zesch-etal-2008-extracting,0,0.0273659,"Missing"
2012.eamt-1.35,W07-0718,0,0.233587,"y, i.e. on designing evaluation metrics that can be computed automatically for the purpose of system tuning and development. These include e.g. BLEU (Papineni et al., 2002), position-independent word error rate (PER), METEOR (Lavie and Agarwal, 2007), or translation error rate (TER) (Snover et al., 2006). Human c 2012 European Association for Machine Translation. 119 evaluation (see (Denkowskie and Lavie, 2010) for a recent overview) typically involves rating translation output with respect to fluency and adequacy (LDC, 2005), or directly comparing and ranking two or more translation outputs (Callison-Burch et al., 2007). All of these evaluation techniques provide a global assessment of overall translation performance without regard to different error types. More fine-grained analyses of individual MT errors often include manual or (semi-) automatic error annotation to gain insights into the strengths and weaknesses of MT engines (Vilar et al., 2006; Popovic and Ney, 2011; Condon et al., 2010; Farreus et al., 2012). There have also been studies of how MT errors influence the work of post-editors with respect to productivity, speed, etc. (Krings, 2001; O’Brien, 2011) or the performance of backend applications"
2012.eamt-1.35,condon-etal-2010-evaluation,0,0.0203592,"enkowskie and Lavie, 2010) for a recent overview) typically involves rating translation output with respect to fluency and adequacy (LDC, 2005), or directly comparing and ranking two or more translation outputs (Callison-Burch et al., 2007). All of these evaluation techniques provide a global assessment of overall translation performance without regard to different error types. More fine-grained analyses of individual MT errors often include manual or (semi-) automatic error annotation to gain insights into the strengths and weaknesses of MT engines (Vilar et al., 2006; Popovic and Ney, 2011; Condon et al., 2010; Farreus et al., 2012). There have also been studies of how MT errors influence the work of post-editors with respect to productivity, speed, etc. (Krings, 2001; O’Brien, 2011) or the performance of backend applications like information retrieval (Parton and McKeown, 2010). In contrast to this line of research, there is surprisingly little work that directly investigates which types of errors are intuitively the most disliked by users of machine translation. Although there is ample anecdotal evidence of users’ reactions to machine translation, it is difficult to find formal, quantitative stud"
2012.eamt-1.35,2010.amta-papers.20,0,0.0625251,"work in machine translation (MT) evaluation research falls into three different categories: automatic evaluation, human evaluation, and embedded application evaluation. Much effort has focused on the first category, i.e. on designing evaluation metrics that can be computed automatically for the purpose of system tuning and development. These include e.g. BLEU (Papineni et al., 2002), position-independent word error rate (PER), METEOR (Lavie and Agarwal, 2007), or translation error rate (TER) (Snover et al., 2006). Human c 2012 European Association for Machine Translation. 119 evaluation (see (Denkowskie and Lavie, 2010) for a recent overview) typically involves rating translation output with respect to fluency and adequacy (LDC, 2005), or directly comparing and ranking two or more translation outputs (Callison-Burch et al., 2007). All of these evaluation techniques provide a global assessment of overall translation performance without regard to different error types. More fine-grained analyses of individual MT errors often include manual or (semi-) automatic error annotation to gain insights into the strengths and weaknesses of MT engines (Vilar et al., 2006; Popovic and Ney, 2011; Condon et al., 2010; Farre"
2012.eamt-1.35,W07-0734,0,0.0186367,"ate that word order errors are clearly the most dispreferred error type, followed by word sense, morphological, and function word errors. 1 Introduction Current work in machine translation (MT) evaluation research falls into three different categories: automatic evaluation, human evaluation, and embedded application evaluation. Much effort has focused on the first category, i.e. on designing evaluation metrics that can be computed automatically for the purpose of system tuning and development. These include e.g. BLEU (Papineni et al., 2002), position-independent word error rate (PER), METEOR (Lavie and Agarwal, 2007), or translation error rate (TER) (Snover et al., 2006). Human c 2012 European Association for Machine Translation. 119 evaluation (see (Denkowskie and Lavie, 2010) for a recent overview) typically involves rating translation output with respect to fluency and adequacy (LDC, 2005), or directly comparing and ranking two or more translation outputs (Callison-Burch et al., 2007). All of these evaluation techniques provide a global assessment of overall translation performance without regard to different error types. More fine-grained analyses of individual MT errors often include manual or (semi-"
2012.eamt-1.35,P02-1040,0,0.0949774,"tudy and obtain utility values for individual error types. Our results indicate that word order errors are clearly the most dispreferred error type, followed by word sense, morphological, and function word errors. 1 Introduction Current work in machine translation (MT) evaluation research falls into three different categories: automatic evaluation, human evaluation, and embedded application evaluation. Much effort has focused on the first category, i.e. on designing evaluation metrics that can be computed automatically for the purpose of system tuning and development. These include e.g. BLEU (Papineni et al., 2002), position-independent word error rate (PER), METEOR (Lavie and Agarwal, 2007), or translation error rate (TER) (Snover et al., 2006). Human c 2012 European Association for Machine Translation. 119 evaluation (see (Denkowskie and Lavie, 2010) for a recent overview) typically involves rating translation output with respect to fluency and adequacy (LDC, 2005), or directly comparing and ranking two or more translation outputs (Callison-Burch et al., 2007). All of these evaluation techniques provide a global assessment of overall translation performance without regard to different error types. Mor"
2012.eamt-1.35,C10-2109,0,0.0172667,"chniques provide a global assessment of overall translation performance without regard to different error types. More fine-grained analyses of individual MT errors often include manual or (semi-) automatic error annotation to gain insights into the strengths and weaknesses of MT engines (Vilar et al., 2006; Popovic and Ney, 2011; Condon et al., 2010; Farreus et al., 2012). There have also been studies of how MT errors influence the work of post-editors with respect to productivity, speed, etc. (Krings, 2001; O’Brien, 2011) or the performance of backend applications like information retrieval (Parton and McKeown, 2010). In contrast to this line of research, there is surprisingly little work that directly investigates which types of errors are intuitively the most disliked by users of machine translation. Although there is ample anecdotal evidence of users’ reactions to machine translation, it is difficult to find formal, quantitative studies of how users perceive the severity of different translation errors and what trade-offs they would make between different errors if they were given a choice. User preferences might sometimes diverge strongly from the system development directions suggested by automatic e"
2012.eamt-1.35,J11-4002,0,0.0358698,"119 evaluation (see (Denkowskie and Lavie, 2010) for a recent overview) typically involves rating translation output with respect to fluency and adequacy (LDC, 2005), or directly comparing and ranking two or more translation outputs (Callison-Burch et al., 2007). All of these evaluation techniques provide a global assessment of overall translation performance without regard to different error types. More fine-grained analyses of individual MT errors often include manual or (semi-) automatic error annotation to gain insights into the strengths and weaknesses of MT engines (Vilar et al., 2006; Popovic and Ney, 2011; Condon et al., 2010; Farreus et al., 2012). There have also been studies of how MT errors influence the work of post-editors with respect to productivity, speed, etc. (Krings, 2001; O’Brien, 2011) or the performance of backend applications like information retrieval (Parton and McKeown, 2010). In contrast to this line of research, there is surprisingly little work that directly investigates which types of errors are intuitively the most disliked by users of machine translation. Although there is ample anecdotal evidence of users’ reactions to machine translation, it is difficult to find form"
2012.eamt-1.35,2006.amta-papers.25,0,0.0423343,"d error type, followed by word sense, morphological, and function word errors. 1 Introduction Current work in machine translation (MT) evaluation research falls into three different categories: automatic evaluation, human evaluation, and embedded application evaluation. Much effort has focused on the first category, i.e. on designing evaluation metrics that can be computed automatically for the purpose of system tuning and development. These include e.g. BLEU (Papineni et al., 2002), position-independent word error rate (PER), METEOR (Lavie and Agarwal, 2007), or translation error rate (TER) (Snover et al., 2006). Human c 2012 European Association for Machine Translation. 119 evaluation (see (Denkowskie and Lavie, 2010) for a recent overview) typically involves rating translation output with respect to fluency and adequacy (LDC, 2005), or directly comparing and ranking two or more translation outputs (Callison-Burch et al., 2007). All of these evaluation techniques provide a global assessment of overall translation performance without regard to different error types. More fine-grained analyses of individual MT errors often include manual or (semi-) automatic error annotation to gain insights into the"
2012.eamt-1.35,vilar-etal-2006-error,0,0.150669,"Missing"
2013.mtsummit-wptp.4,2009.mtsummit-btm.7,0,0.0202146,"system specifically targeted to their typical workflow. We report results from user testing and participatory design studies and conclude with a set of recommendations and best practices for introducing machine translation plus post-editing to lay user communities. 1 Introduction Over the past decade the development of machine translation (MT) technology has made rapid progress and has reached a high level of maturity. MT is now routinely being used for a variety of tasks. Machine translation plus post-editing (MT+PE) has been shown to significantly increase translator productivity (see e.g. (Guerberof, 2009; Plitt and Masselot, 2010; Green et al., 2013)) and has become a common procedure for many language service providers, corporations, and government organizations. However, there are still many communities that could potentially benefit from MT but that do not currently use it. These are often non-profit, educational, faith-based, or research organizations whose members may be experts in a particular domain but who are “lay” users from an MT perspective, i.e. they are not trained translators or posteditors. Lay communities may be prevented from using MT by a lack of awareness of the current ca"
2013.mtsummit-wptp.4,2012.amta-wptp.2,0,0.028349,"rning effect, but others did not. Timing seems to be largely dependent on the individual, with some post-editors taking more time to double-check and ensure corMT+PE preferred 18 HT preferred 16 Equivalent 16 Table 1: Number of votes assigned to categories in qualitative comparison of human-only translation (HT) and MT+PE output. Each of the 25 document was rated twice by two independent reviewers. rect translations after the initial post-editing pass, whereas others do a single integrated pass over the text. This is in line with similar observations reported in the literature (O’Brien, 2006; Koponen et al., 2012). The quality rating results (Table 1) showed that overall the quality of post-edited documents did not differ from their human-translated counterparts – preference ratings for documents that were not judged equivalent were distributed approximately evenly across the three different categories. With regard to translation errors post-editors found word order errors the most difficult to process and to correct, followed by word sense errors. They did notice a fair amount of morphological errors but these were considered less distracting. Again, this is similar to results reported in other studie"
2013.mtsummit-wptp.4,2012.amta-wptp.6,0,0.0428338,"ted post-editors. In sum, these are features that mirror not only the typical workflow but also the professional hierarchy or social network that exists within their community. 5 Related Work A variety of translation management and postediting systems have been developed in the past. Most of them (e.g. SDL Trados2 , Wordfast3 , etc.) are commercial products aimed at language professionals, such as translators and language service providers. Their price is often prohibitive and they frequently require software installation on the user side. Other systems, such as MemSource Cloud 4 , SmartMATE (Penkale and Way, 2012), or Wordbee5 , work in the cloud but may still be too expensive for non-professional users. Among free or open-source systems, Google’s Translator Toolkit6 comes close to our requirements in that it allows collaborative post-editing and document sharing. On the other hand it lacks 2 http://www.trados.com http://www.wordfast.net 4 http://www.memsource.com/translation-cloud 5 http://www.wordbee.com 6 http://www.google.translate/toolkit 3 essential features for our intended use, such as incorporating meta-information about documents and post-editors. A web-based translation management system int"
2020.acl-main.240,D17-1039,0,0.0333953,"LMs, our approach remains generative. Log probabilities model the joint distribution; PLL does so as well, albeit implicitly (Appendix B). PLL’s summands (conditional probabilities) remain accessible for Gibbs sampling and are not tailored to any metric. The two approaches are complementary; for example, one could use PLL as a “prior” or regularizer for scores given by discriminatively-finetuned BERT models in tasks like passage re-ranking (Nogueira and Cho, 2019). Language model integration. Beyond finetuning pretrained LMs and MLMs, monolingual pretraining has also improved NMT performance (Ramachandran et al., 2017; Conneau and Lample, 2019). However, modular integration of language representation models remains prevalent for various pragmatic reasons, similar to fusion in ASR. Contemporary examples are the use of finetuned BERT scores in a question-answering pipeline (Nogueira and Cho, 2019), or “as-is” cosine similarity scores from BERT to evaluate generated text (Zhang et al., 2020). For example, one might have no pretrained multilingual LMs for decoder initialization or fusion, as such models are difficult to train (Ragni et al., 2016). However, one may have an M-BERT or XLM for the target language/"
2020.acl-main.240,P04-1007,0,0.143463,"ure context have been used in MT (Finch and Sumita, 2009; Xiong et al., 2011) and perennially in ASR (Shi et al., 2013; Arisoy et al., 2015; Chen et al., 2017) to positive effect. However, these are not “deep bidirectional” as they model interactions between W<t and W>t via the forward and backward context vectors, while MLMs model all pairwise interactions ws and ws0 via dotproduct attention (compare ELMo versus BERT). Their PLLs would have different properties from ours (e.g., their cross-entropies in Figure 4 may be convex instead of flat). Discriminative language modeling. Previous works (Roark et al., 2004; Huang et al., 2018) have explored training language models that directly optimize for a downstream metric (WER, BLEU). While we also eschew using log probabilities from conventional LMs, our approach remains generative. Log probabilities model the joint distribution; PLL does so as well, albeit implicitly (Appendix B). PLL’s summands (conditional probabilities) remain accessible for Gibbs sampling and are not tailored to any metric. The two approaches are complementary; for example, one could use PLL as a “prior” or regularizer for scores given by discriminatively-finetuned BERT models in ta"
2020.acl-main.240,W18-6321,0,0.0553077,"English-target NMT in Table 3. As 100-best can be worse than 4-best due to the beam search curse (Yang et al., 2018; Murray 2702 and Chiang, 2018), we first decode both beam sizes to ensure no systematic degradation in our models. Hypothesis rescoring with BERT (base) gives up to +1.1 BLEU over our strong 100-best baselines, remaining competitive with GPT-2. Using RoBERTa (large) gives up to +1.7 BLEU over the baseline. Incidentally, we have demonstrated conclusive improvements on Transformers via LM rescoring for the first time, despite only using N -best lists; the most recent fusion work (Stahlberg et al., 2018) only used LSTM-based models. Model TED Talks gl→en sk→en ar→en Neubig and Hu (2018) Aharoni et al. (2019) our baseline (4-best) our baseline (100-best) 16.2 – 18.47 18.55 24.0 – 29.37 29.20 – 27.84 33.39 33.40 GPT-2 (117M, cased) BERT (base, cased) RoBERTa (base, cased) 19.24 19.09 19.22 30.38 30.27 30.80 34.41 34.32 34.45 GPT-2 (345M, cased) BERT (large, cased) RoBERTa (large, cased) 19.16 19.30 19.36 30.76 30.31 30.87 34.62 34.47 34.73 29.09 – 31.94 31.84 – 23.31 30.50 30.44 – 12.95 13.95 13.94 M-BERT (base, uncased) M-BERT (base, cased) XLM (base*, uncased) + TLM objective 32.12 32.07 32.2"
2020.acl-main.240,D18-1100,0,0.0252848,"lar approach, we ask whether a shared multilingual MLM can improve translation into different target languages. We use the 100+ language M-BERT models, and the 15-language XLM models (Conneau and Lample, 2019) optionally trained with a crosslingual translation LM objective (TLM). Monolingual training was done on Wikipedia, which gives e.g., 6GB of German text; see Table 4. The 100-language M-BERT models gave no consistent improvement. The 15-language XLMs fared better, giving +0.2-0.4 BLEU, perhaps from their use of language tokens and fewer languages. Our https://github.com/dbmdz/german-bert Wang et al. (2018) Aharoni et al. (2019) our baseline (4-best) our baseline (100-best) German BERT results suggest an out-of-the-box upper bound of +0.8 BLEU, as we found with English BERT on similar resources. We expect that increasing training data and model size will boost XLM performance, as in Section 3.3. We also consider a non-English, higher-resource target by rescoring a pre-existing WMT 2014 English-German system (trained on 4.5M sentence pairs) with German BERT (base) models1 trained on 16GB of text, similar to English BERT. From 27.3 BLEU we get +0.5, +0.3 from uncased, cased; a diminished but prese"
2020.acl-main.240,2020.tacl-1.25,0,0.242633,"Missing"
2020.acl-main.240,Q19-1040,0,0.116828,"Missing"
2020.acl-main.240,P11-1129,0,0.0314559,"ter α = 0.6, widely used in NMT baselines like ours, as there exists C such that LPS2S (W ) = 5 (5 + |W |)0.6 ≈ (5 + 1)0.6 Z 0 |W | C dx. (5 + x)0.4 Related work Our work extends the closest previous works (Wang and Cho, 2019; Shin et al., 2019) with regards to experiments and tasks, as outlined in Section 2.1. Furthermore, neither work considers the inference cost of masked rescoring, which we address with our maskless scoring approach, or analyze PLL’s numerical properties. Future context. Log probabilities conditioned on past and future context have been used in MT (Finch and Sumita, 2009; Xiong et al., 2011) and perennially in ASR (Shi et al., 2013; Arisoy et al., 2015; Chen et al., 2017) to positive effect. However, these are not “deep bidirectional” as they model interactions between W<t and W>t via the forward and backward context vectors, while MLMs model all pairwise interactions ws and ws0 via dotproduct attention (compare ELMo versus BERT). Their PLLs would have different properties from ours (e.g., their cross-entropies in Figure 4 may be convex instead of flat). Discriminative language modeling. Previous works (Roark et al., 2004; Huang et al., 2018) have explored training language model"
2020.acl-main.240,D18-1342,0,0.060753,"Missing"
2020.acl-main.240,P16-1008,0,0.0249136,"ds, at the cost of adequacy: clasping truth and 7→ class in truth and, Union by the Union Sivities 7→ 4.2 Interpolation with direct models Union by the Union by the Union Civities. We observed that log g = PLL(W ) is not unduly affected by unconditional token frequencies; this mitigates degradation in adequacy upon interpolation with PS2S . Consider a two-word proper noun, e.g., W = “San Francisco”: log PLM (W ) = log PLM (San) + log PLM (Francisco |San) One can view these as exacerbations of the rare word problem due to overconfident logits (Nguyen and Chiang, 2018), and of over-translation (Tu et al., 2016). Meanwhile, BERT rewards selfconsistency, which lets rarer but still-fluent words with better acoustic or translation scores to persist:  log PMLM (San |Francisco) clasping truth and 7→ clasping truth in, Union by the Union Sivities 7→ + log PMLM (Francisco |San) = PLL(W ). Union by the Union of LiberCivities, 2705 System Model Output sentence BLiMP (S-V agreement) BERT GPT-2 The pamphlets about Winston Churchill have resembled those photographs. The pamphlets about Winston Churchill has resembled those photographs. BLiMP (island) BERT GPT-2 Who does Amanda find while thinking about Lucille?"
2020.acl-main.240,W19-2304,0,0.432017,"uring an internship at Amazon AWS AI. In contrast, conventional language models (LMs) predict wt using only past tokens W<t := (w1 , . . . , wt−1 ). However, this allows LMs to estimate log probabilities for a sentence W via the P|W | chain rule (log PLM (W ) = t=1 log PLM (wt | W<t )), which can be used out of the box to rescore hypotheses in end-to-end speech recognition and machine translation (Chan et al., 2016; Gulcehre et al., 2015), and to evaluate sentences for linguistic acceptability (Lau et al., 2017). Our work studies the corresponding pseudo-loglikelihood scores (PLLs) from MLMs (Wang and Cho, 2019), given by summing the conditional log probabilities log PMLM (wt |W	 ) of each sentence token (Shin et al., 2019). These are induced in BERT by replacing wt with [MASK] (Figure 1). 2699 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2699–2712 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Let Θ denote our model’s parameters. Our score is PLL(W ) := |W | X 2 2.1 log PMLM (wt |W	 ; Θ). t=1 PLLs and their corresponding pseudo-perplexities (PPPLs) (Section 2.3) are intrinsic values one can assign to sentences and corpora, allo"
2020.acl-main.240,J93-2003,0,\N,Missing
2020.acl-main.240,D09-1117,0,\N,Missing
2020.acl-main.240,D18-1150,0,\N,Missing
2020.acl-main.240,N18-2084,0,\N,Missing
2020.acl-main.240,D18-1103,0,\N,Missing
2020.acl-main.240,2015.iwslt-evaluation.1,0,\N,Missing
2020.acl-main.240,N19-1388,0,\N,Missing
2020.acl-main.240,N19-1409,0,\N,Missing
2020.acl-main.240,N18-1031,1,\N,Missing
2020.acl-main.240,N19-1423,0,\N,Missing
2020.acl-main.240,P19-1285,0,\N,Missing
2020.nlpmc-1.8,W19-3515,1,0.824073,"ed notes (Edwards et al., 2017), telemedicine and even doctor-patient conversations (Chiu et al., 2017), without any human intervention. These systems ease the burden of long hours of administrative work and also promote better engagement with patients. However, the generated ASR outputs are typically devoid of punctuation and truecasing thereby making it difficult to comprehend. Furthermore, their recovery improves 53 Proceedings of the 1st Workshop on NLP for Medical Conversations, pages 53–62 c Online, 10, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 Bodapati et al., 2019). We compare word and subword models across different architectures and show that subword models consistently outperform the former. in-depth analysis of the effectiveness of using pretrained masked language models like BERT and its successors to address the data scarcity problem. • Data scarcity: Data scarcity is one of the major bottlenecks in supervised learning. When it comes to the medical domain, obtaining data is not as straight-forward as some of the other domains where abundance of text is available. On the other hand, obtaining large amounts of data is a tedious and costly process; p"
2020.nlpmc-1.8,D19-1433,0,0.0488769,"Missing"
2020.nlpmc-1.8,2012.iwslt-papers.15,0,0.0258026,"pproaches Neural approaches for punctuation and truecasing can be classified into two broad categories: sequence labeling based models and MT-based seq2seq models. These approaches have proven to be quite effective in capturing the contextual information and achieved huge success. While some approaches considered only punctuation prediction, some others jointly modeled punctuation and truecasing. One set of approaches treated punctuation as a machine translation problem and used phrase based statistical machine translation systems to output punctuated and true cased text (Peitz et al., 2011b; Cho et al., 2012; Driesen et al., 2014). Inspired by recent end-to-end approaches, (Yi and Tao, 2019) proposed the use of self-attention based transformer model to predict punctuation marks as output sequence for given word sequences. Most recently, (Nguyen et al., 2019b) proposed joint modeling of punctuation and truecasing by generating words with punctuation marks as part of the decoding. Although seq2seq based approaches have shown a strong performance, they are intensive, demanding and are not suitable for production deployment at large scale. For sequence labeling problem, each word in the input is tagg"
2020.nlpmc-1.8,2021.ccl-1.108,0,0.0712116,"Missing"
2020.nlpmc-1.8,D10-1018,0,0.0214429,"st the performance of the model quantitatively. To address this issue, we propose a data augmentation based approach using n-best lists from ASR. 2.1 Earlier methods In earlier efforts, punctuation prediction has been approached by using finite state or hidden Markov models (Gotoh and Renals, 2000; Christensen et al., 2001a). Several other approaches addressed it as a language modeling problem by predicting the most probable sequence of words with punctuation marks inserted (Stolcke et al., 1998; Beeferman et al., 1998; Gravano et al., 2009). Some others used conditional random fields (CRFs) (Lu and Ng, 2010; Ueffing et al., 2013) and maximum entropy using n-grams (Huang and Zweig, 2002). The rise of stronger machine learning techniques such as deep and/or recurrent neural networks replaced these conventional models. The contributions of this work are: • A general post-processing framework for conditional joint labeling of punctuation and truecasing for medical ASR (clinical dictation and conversations). 2.2 • An analysis comparing different embeddings that are suitable for the medical domain. An Using acoustic information Some methods used only acoustic information such as speech rate, intonatio"
2020.nlpmc-1.8,2011.iwslt-papers.7,0,0.0760033,"uage models. Neural approaches Neural approaches for punctuation and truecasing can be classified into two broad categories: sequence labeling based models and MT-based seq2seq models. These approaches have proven to be quite effective in capturing the contextual information and achieved huge success. While some approaches considered only punctuation prediction, some others jointly modeled punctuation and truecasing. One set of approaches treated punctuation as a machine translation problem and used phrase based statistical machine translation systems to output punctuated and true cased text (Peitz et al., 2011b; Cho et al., 2012; Driesen et al., 2014). Inspired by recent end-to-end approaches, (Yi and Tao, 2019) proposed the use of self-attention based transformer model to predict punctuation marks as output sequence for given word sequences. Most recently, (Nguyen et al., 2019b) proposed joint modeling of punctuation and truecasing by generating words with punctuation marks as part of the decoding. Although seq2seq based approaches have shown a strong performance, they are intensive, demanding and are not suitable for production deployment at large scale. For sequence labeling problem, each word i"
2020.nlpmc-1.8,W17-2319,0,0.157021,"postprocessing component takes care of punctuation restoration. This process is usually error-prone as the clinicians may struggle with appropriate punctuation insertion during dictation. Moreover, doctor-patient conversations lack explicit vocalization of punctuation marks motivating the need for automatic prediction of punctuation and truecasing. In this work, we aim to solve the problem of automatic punctuation and truecasing restoration to medical ASR system text outputs. Most recent approaches to punctuation and truecasing restoration problem rely on deep learning (Nguyen et al., 2019a; Salloum et al., 2017). Although it is a well explored problem in the literature, most of these improvements do not directly translate to great real world performance in all settings. For example, unlike general text, it is a harder problem to solve when applied to the medical domain for various reasons and we illustrate each of them: Automatic speech recognition (ASR) systems in the medical domain that focus on transcribing clinical dictations and doctor-patient conversations often pose many challenges due to the complexity of the domain. ASR output typically undergoes automatic punctuation to enable users to spea"
2021.ecnlp-1.3,2020.emnlp-main.733,0,0.0294234,"rity and are considered as powerful language learners (Radford et al., 2016; Brown et al., 2020). However, the sentence or document embeddings derived from such an MLM without finetuning on in-domain data is shown to be inferior in terms of the ability to capture semantic information that can be used in similarity related tasks (Reimers and Gurevych, 2019). Instead of using the [CLS] vector to obtain sentence embeddings, in this paper we take the average of context embeddings from last two layers as these are shown to be consistently better than using [CLS] vector (Reimers and Gurevych, 2019; Li et al., 2020). Transformer-XL LM conditioning on dialogue acts Dialogue acts (DA) in a conversation represent the intention of an utterance and is intended towards capturing the action that an agent is trying to accomplish (Austin, 1975). An example conversation snippet with DA is shown in Table 1. DA classification is typically performed in a separate component that is part of a downstream NLU system and consumes the outputs generated by ASR. The classified DA is an important contextual signal that provides hints about the type of speech pattern that can be expected in the next turn. We utilize these sign"
2021.ecnlp-1.3,W16-3603,0,0.154697,"CNLP 4), pages 18–25 August 5, 2021. ©2021 Association for Computational Linguistics span context helps with contextualiziation and improves WER and IC F1 in e-commerce chatbots. 2019; Li et al., 2018; Raju et al., 2018; Chen et al., 2015). (Irie et al., 2018) used a mixture of domain experts which are dynamically interpolated. It is also shown in (Liu et al., 2020), that using a hybrid pointer network over contextual metadata can also help in transcribing long form social media audio. Joint learning NLU tasks such as intent detection and slot filling have been explored with RNN based LMs in (Liu and Lane, 2016) and more recently in (Rao et al., 2020), where they show that a jointly trained model consisting of both ASR and NLU tasks interfaced with a neural network based interface helps incorporate semantic information from NLU and improves ASR that comprises a LSTM based NLM. In (Yang et al., 2020) tried to incorporate joint slot and intent detection into a LSTM based rescorer with a goal of improving accuracy on rare words in an end-to-end ASR system. However, none of the previous work utilize dialogue acts with a non-recurrent based LM such as Transformer-XL nor optimize towards improving robustne"
2021.ecnlp-1.3,P19-1285,0,0.0416021,"Missing"
2021.ecnlp-1.3,P18-1027,0,0.164814,"on and LM tasks which significantly improves content WERR and SL F1. • We show that adapting the NLM towards user provided speech patterns by using BERT on domain specific text is an efficient and effective method to perform on-the-fly adaptation of a domain-general NLM towards ecommerce utterances. 2 Related Work Incorporating cross utterance context has been well explored with both recurrent and non-recurrent NLMs. With LSTM NLMs, long span context is usually propogated without resetting hidden states across sentences or using longer sequence lengths (Xiong et al., 2018a; Irie et al., 2019; Khandelwal et al., 2018; Parthasarathy et al., 2019). In (Xiong et al., 2018b), along with longer history, information about turn taking and speaker overlap is used to improve contextualization in human to human conversations. With transformer architecture based on self attention (Vaswani et al., 2017) (Dai et al., 2019) showed that by utilizing segment wise recurrence Transformer-XL (TXL) (Dai et al., 2019) is able to effectively leverage long span context while decoding. More recently, improving contextualization of the TXL models included adding a LSTM fusion layer to complement the advantages of recurrent with n"
2021.ecnlp-1.3,D19-1460,0,0.0212744,"sed a fixed αSD of 0.8 for the slot detection loss. Table 3: Relative perplexity reduction (PPLR) from the various TXL models on a general domain eval set (PPLgen ) and on e-commerce domain eval set (PPLecom ). 4 4.1 ASR setup and NLM setup Experimental Setup Dataset We required task-oriented dialogue datasets with actor, dialogue acts and the slot entities annotated. Since no single dataset was large enough to train a reliable language model, we used a combination of Schema-Guided Dialogue Dataset (Rastogi et al., 2019), MultiWOZ 2.1 (Eric et al., 2019; Budzianowski et al., 2018), MultiDoGo (Peskov et al., 2019) along with anonymized in-house datasets that belong to two e-commerce usecases : retail and fastfood delivery. The final LM training data consisted of 260k training samples, 56k validation and evaluation samples and around 9.9 million running words. We used a vocabulary of size 25k. We evaluated our models on anonymized in-house 8kHz close-talk audio. These audio comprised of task-oriented conversations with multiple speakers and acoustic conditions representative of real world usage and belonged to the same two usecases mentioned above. The average number of turns in the audio dataset was 5."
2021.ecnlp-1.3,D18-1296,0,0.0280787,"that is jointly trained on slot detection and LM tasks which significantly improves content WERR and SL F1. • We show that adapting the NLM towards user provided speech patterns by using BERT on domain specific text is an efficient and effective method to perform on-the-fly adaptation of a domain-general NLM towards ecommerce utterances. 2 Related Work Incorporating cross utterance context has been well explored with both recurrent and non-recurrent NLMs. With LSTM NLMs, long span context is usually propogated without resetting hidden states across sentences or using longer sequence lengths (Xiong et al., 2018a; Irie et al., 2019; Khandelwal et al., 2018; Parthasarathy et al., 2019). In (Xiong et al., 2018b), along with longer history, information about turn taking and speaker overlap is used to improve contextualization in human to human conversations. With transformer architecture based on self attention (Vaswani et al., 2017) (Dai et al., 2019) showed that by utilizing segment wise recurrence Transformer-XL (TXL) (Dai et al., 2019) is able to effectively leverage long span context while decoding. More recently, improving contextualization of the TXL models included adding a LSTM fusion layer to"
2021.ecnlp-1.3,D19-1410,0,0.0130653,"ation of LM and slot detection losses: Ltotal = LLM + αSD LSD User Bot inform thank-you req-more (8) models (MLM) such as BERT, can be used to derive a fixed size semantic representation from this lexical information. Large pretrained MLMs are gaining widespread popularity and are considered as powerful language learners (Radford et al., 2016; Brown et al., 2020). However, the sentence or document embeddings derived from such an MLM without finetuning on in-domain data is shown to be inferior in terms of the ability to capture semantic information that can be used in similarity related tasks (Reimers and Gurevych, 2019). Instead of using the [CLS] vector to obtain sentence embeddings, in this paper we take the average of context embeddings from last two layers as these are shown to be consistently better than using [CLS] vector (Reimers and Gurevych, 2019; Li et al., 2020). Transformer-XL LM conditioning on dialogue acts Dialogue acts (DA) in a conversation represent the intention of an utterance and is intended towards capturing the action that an agent is trying to accomplish (Austin, 1975). An example conversation snippet with DA is shown in Table 1. DA classification is typically performed in a separate"
2021.ecnlp-1.3,2020.ecnlp-1.6,0,0.0327879,"ctively leverage long span context while decoding. More recently, improving contextualization of the TXL models included adding a LSTM fusion layer to complement the advantages of recurrent with non-recurrent models (Sun et al., 2021). (Shenoy et al., 2021) incorporated a non-finetuned masked LM fusion in order to make the domain adaptation of TXL models quick and on-the-fly using embeddings derived from customer provided data and incorporated dialogue acts but only with an LSTM based LM. While (Sunkara et al., 2020) tried to fuse multi-model features into a seq-to-seq LSTM based network. In (Sharma, 2020) cross utterance context was effectively used to perform better intent classification with e-commerce voice assistants. For domain adaptation, previous techniques explored include using an explicit topic vector as classified by a separate domain classifier and incorporating a neural cache (Mikolov and Zweig, 3 Approach A standard language model in an ASR system computes a probability distribution over a sequence of words W = w0 , ..., wN auto-regressively as: p(W ) = N Y p(wi |w1 , w2 , ..., wi−1 ) (1) i=1 In our experiments, along with historical context, we condition the LM on additional con"
2021.naacl-main.154,H91-1098,0,0.345977,"Missing"
2021.naacl-main.154,N19-1423,0,0.0276175,"Missing"
2021.naacl-main.154,D18-1045,0,0.070253,"Missing"
2021.naacl-main.154,D19-1633,0,0.0876317,"oding time linear in output sequence length, which is slow for long sequences. By contrast, nonautoregressive (NAR) models decode all output tokens independently and in parallel. When combined with self-attention, one gets fast, constanttime inference in NMT (Gu et al., 2018) and endto-end ASR (Salazar et al., 2019). However, these models underperform their autoregressive counter∗ Work done during an internship at Amazon AWS AI. parts, as the conditional independence between output tokens results in globally inconsistent outputs.1 To mitigate these issues, infilling methods like Mask-Predict (Ghazvininejad et al., 2019) refine an initial non-autoregressive proposal, repeatedly predicting a masked subset of low-confidence proposal tokens in a fixed number of decoding passes. In ASR, most iterative non-autoregressive methods use infilling, like A-FMLM (Chen et al., 2020), Imputer (Chan et al., 2020), and Mask-CTC (Higuchi et al., 2020). However, during training, infilling requires partial proposals to be simulated by synthetically masking ground truths or samples from an expert. The resulting train-test mismatch leads to poor-quality generation. An alternative proposed in NMT is iterative refinement (Lee et al"
2021.naacl-main.154,2020.acl-main.240,1,0.894123,"Missing"
C04-1022,N03-2002,1,0.811768,"urkish, Russian, or Arabic. Such languages have a large number of word types in relation to the number of word tokens in a given text, as has been demonstrated in a number of previous studies (Geutner, 1995; Kiecza et al., 1999; Hakkani-T¨ ur et al., 2002; Kirchhoff et al., 2003). This in turn results in a high perplexity and in a large number of out-of-vocabulary (OOV) words when applying a trained language model to a new unseen text. 2.1 Factored Word Representations A recently developed approach that addresses this problem is that of Factored Language Models (FLMs) (Kirchhoff et al., 2002; Bilmes and Kirchhoff, 2003), whose basic idea is to decompose words into sets of features (or factors) instead of viewing them as unanalyzable wholes. Probabilistic language models can then be constructed over (sub)sets of word features instead of, or in addition to, the word variables themselves. For instance, words can be decomposed into stems/lexemes and POS tags indicating their morphological features, as shown below: Word: Stem: Tag: Stock Stock Nsg prices price N3pl are be V3pl rising rise Vpart Such a representation serves to express lexical and syntactic generalizations, which would otherwise remain obscured. It"
C04-1022,J92-4003,0,0.339397,"Missing"
C04-1022,C00-1042,0,0.0237462,"models that perform well in sparse data conditions. The factored representation was constructed using linguistic information from the corpus lexicon, in combination with automatic morphological analysis tools. It includes, in addition to the word, the stem, a morphological tag, the root, and the pattern. The latter two are components which when combined form the stem. An example of this factored word representation is shown below: Word:il+dOr/Morph:noun+masc-sg+article/ Stem:dOr/Root:dwr/Pattern:CCC For our Turkish experiments we used a morphologically annotated corpus of Turkish (Hakkani-T¨ ur et al., 2000). The annotation was performed by applying a morphological analyzer, followed by automatic morphological disambiguation as described in (Hakkani-T¨ ur et al., 2002). The morphological tags consist of the initial root, followed by a sequence of inflectional groups delimited by derivation boundaries (ˆDB). A sample annotation (for the word yararlanmak, consisting of the root yarar plus three inflectional groups) is shown below: yararmanlak: yarar+Noun+A3sg+Pnon+Nom ˆDB+Verb+Acquire+Pos ˆDB+Noun+Inf+A3sg+Pnon+Nom We removed segmentation marks (for titles and paragraph boundaries) from the corpus"
C04-1022,N04-4034,0,0.0101525,"ighted mean, product, and maximum of the smoothed probability distributions over all subsets of the conditioning factors. In addition to different choices for g, different discounting parameters can be chosen at different levels in the backoff graph. For instance, at the topmost node, Kneser-Ney discounting might be chosen whereas at a lower node Good-Turing might be applied. FLMs have been implemented as an add-on to the widely-used SRILM toolkit1 and have been used successfully for the purpose of morpheme-based language modeling (Bilmes and Kirchhoff, 2003), multi-speaker language modeling (Ji and Bilmes, 2004), and speech recognition (Kirchhoff et al., 2003). 3 Learning FLM Structure In order to use an FLM, three types of parameters need to be specified: the initial conditioning factors, the backoff graph, and the smoothing options. The goal of structure learning is to find the parameter combinations that create FLMs that achieve a low perplexity on unseen test data. The resulting model space is extremely large: given a factored word representation  with a total of k factors, there are Pk k n=1 n possible subsets of initial conditioning factors. For a set of m conditioning factors, there are up to"
C04-1022,P99-1033,0,0.0132766,"nnotation (for the word yararlanmak, consisting of the root yarar plus three inflectional groups) is shown below: yararmanlak: yarar+Noun+A3sg+Pnon+Nom ˆDB+Verb+Acquire+Pos ˆDB+Noun+Inf+A3sg+Pnon+Nom We removed segmentation marks (for titles and paragraph boundaries) from the corpus but included punctuation. Words may have different numbers of inflectional groups, but the FLM representation requires the same number of factors for each word; we therefore had to map the original morphological tags to a fixed-length factored representation. This was done using linguistic knowledge: according to (Oflazer, 1999), the final inflectional group in each dependent word has a special status since it determines inflectional markings on head words following the dependent word. The final inflectional group was therefore analyzed into separate factors indicating the number (N), case (C), part-of-speech (P) and all other information (O). Additional factors for the word are the root (R) and all remaining information in the original tag not subsumed by the other factors (G). The word itself is used as another factor (W). Thus, the above example would be factorized as follows: W:yararlanmak/R:yarar/P:NounInf-N:A3s"
C04-1022,J03-4001,0,\N,Missing
C04-1022,P98-2240,0,\N,Missing
C04-1022,C98-2235,0,\N,Missing
C10-1138,P05-1048,0,0.0807808,"large. However, for practical applications (e.g. information extraction or human browsing of meeting translations) the correct translation of content words and referring expressions would be very important. In the remainder of the paper we therefore describe initial experiments designed to address the most important source of contextual errors, viz. word sense confusions. 5 Resolving Word Sense Disambiguation Errors The problem of word sense disambiguation (WSD) in MT has received a fair amount of attention before. Initial experiments designed at integrating a WSD component into an MT system (Carpuat and Wu, 2005) did not meet with success; however, WSD was subsequently demonstrated to be successful in data-matched conditions (Carpuat and Wu, 2007; Chan et al., 2007). The approach pursued by these latter approaches is to train a supervised word sense classifier on different phrase translation options provided by the phrase table of an initial baseline system (i.e. the task is to separate different phrase senses rather than word senses). The input features to the classifier consist of word features obtained from the immediate context of the phrase in questions, i.e. from the same sentence or from the tw"
C10-1138,D07-1007,0,0.0571862,"n of content words and referring expressions would be very important. In the remainder of the paper we therefore describe initial experiments designed to address the most important source of contextual errors, viz. word sense confusions. 5 Resolving Word Sense Disambiguation Errors The problem of word sense disambiguation (WSD) in MT has received a fair amount of attention before. Initial experiments designed at integrating a WSD component into an MT system (Carpuat and Wu, 2005) did not meet with success; however, WSD was subsequently demonstrated to be successful in data-matched conditions (Carpuat and Wu, 2007; Chan et al., 2007). The approach pursued by these latter approaches is to train a supervised word sense classifier on different phrase translation options provided by the phrase table of an initial baseline system (i.e. the task is to separate different phrase senses rather than word senses). The input features to the classifier consist of word features obtained from the immediate context of the phrase in questions, i.e. from the same sentence or from the two or three preceding sentences. The classifier is usually trained only for those phrases that are sufficiently frequent in the training"
C10-1138,P07-1005,0,0.0561962,"referring expressions would be very important. In the remainder of the paper we therefore describe initial experiments designed to address the most important source of contextual errors, viz. word sense confusions. 5 Resolving Word Sense Disambiguation Errors The problem of word sense disambiguation (WSD) in MT has received a fair amount of attention before. Initial experiments designed at integrating a WSD component into an MT system (Carpuat and Wu, 2005) did not meet with success; however, WSD was subsequently demonstrated to be successful in data-matched conditions (Carpuat and Wu, 2007; Chan et al., 2007). The approach pursued by these latter approaches is to train a supervised word sense classifier on different phrase translation options provided by the phrase table of an initial baseline system (i.e. the task is to separate different phrase senses rather than word senses). The input features to the classifier consist of word features obtained from the immediate context of the phrase in questions, i.e. from the same sentence or from the two or three preceding sentences. The classifier is usually trained only for those phrases that are sufficiently frequent in the training data. By contrast, o"
C10-1138,N09-1004,0,0.0381271,"Missing"
C10-1138,2009.eamt-1.32,0,0.0163954,"nder of speakers and listeners was utilized in a transfer-based spoken-language translation system for travel dialogs. In (Kumar et al., 2008) 1227 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1227–1235, Beijing, August 2010 statistically predicted dialog acts were used in a phrase-based SMT system for three different dialog tasks and were shown to improve performance. Recently, contextual source-language features have been incorporated into translation models to predict translation phrases for traveling domain tasks (Stroppa et al., 2007; Haque et al., 2009). However, we are not aware of any work addressing contextual modeling for statistical translation of spoken meeting-style interactions, not least due to the lack of a relevant corpus. The first goal of this study is to provide a quantitative analysis of the impact of the lack of contextual modeling on translation performance. To this end we have collected a small corpus of parallel multi-party meeting data. A baseline SMT system was trained for this corpus from freely available data resources, and contextual translation errors were manually analyzed with respect to the type of knowledge sourc"
C10-1138,2005.eamt-1.19,0,0.0358289,"entary proceedings. Spoken language translation has mostly concentrated on twoperson dialogues, such as travel expressions or patient-provider interactions in the medical domain. Recently, more advanced spoken-language data has been addressed, such as speeches (St¨uker et al., 2007), lectures (Waibel and F¨ugen, 2008), The problem of speech disfluencies has been addressed by disfluency removal techniques that are applied prior to translation (Rao et al., 2007; Wang et al., 2010). Training data sparsity has been addressed by adding data from out-of-domain resources (e.g. (Matusov et al., 2004; Hildebrandt et al., 2005; Wu et al., 2008)), exploiting comparable rather than parallel corpora (Munteanu and Marcu, 2005), or paraphrasing techniques (Callison-Burch et al., 2006). The lack of contextual modeling, by contrast, has so far not been investigated in depth, although it is a generally recognized problem in machine translation. Early attempts at modeling contextual information in machine translation include (Mima et al., 1998), where information about the role, rank and gender of speakers and listeners was utilized in a transfer-based spoken-language translation system for travel dialogs. In (Kumar et al.,"
C10-1138,W08-0510,0,0.0719025,"Training Data Since transcription and translation of multiparty spoken conversations is extremely timeconsuming and costly, it is unlikely that parallel conversational data will ever be produced on a sufficiently large scale for a variety of different meeting types, topics, and target languages. In order to mimic this situation we trained an initial EnglishGerman SMT system on freely available out-ofdomain data resources. We considered the followTranslation Systems We trained a standard statistical phrase-based English-German translation system from the resources described above using Moses (Hoang and Koehn, 2008). Individual language models were trained for each data source and were then linearly interpolated with weights optimized on the development set. Similarly, individual phrase tables were trained and were then combined into a single table. Binary indicator features were added for each phrase pair, indicating which data source it was extracted from. Duplicated phrase pairs were merged into a single entry by averaging their scores (geometric mean) over all duplicated entries. The weights for binary indicator features were optimized along with all other standard features on the development set. Ou"
C10-1138,2005.mtsummit-papers.11,0,0.0100275,"herefore bias the development and tuning of the MT system towards these short utterances at the expense of longer, more informative utterances. Table 1 shows the word counts of the translated meetings after the preprocessing steps described above. As an indicator of inter-translator 1228 ID ES2008a IB4001 IB4002 IB4003 IB4004 IB4005 IS1008a IS1008b IS1008c TS3005a type S NS NS NS NS NS S S S S # utter. 224 419 447 476 593 381 191 353 308 245 # word 2327 3879 3246 5118 5696 4719 2058 3661 3351 2339 ing parallel corpora: news text (de-news1 , 1.5M words), EU parliamentary proceedings (Europarl (Koehn, 2005), 24M words) and EU legal documents (JRC Acquis2 , 35M words), as well as two generic English-German machine-readable dictionaries3 ,4 (672k and 140k entries, respectively). S-BLEU 21.5 24.5 30.5 24.1 26.9 30.4 25.8 24.1 19.6 28.1 3 Table 1: Sizes and symmetric BLEU scores for translated meetings from the AMI corpus (S = scenario meeting, NS = non-scenario meeting). agreement we computed the symmetric BLEU (S-BLEU) scores on the reference translations (i.e. using one translation as the reference and the other as the hypothesis, then switching them and averaging the results). As we can see, sco"
C10-1138,P08-2057,0,0.0260056,"et al., 2005; Wu et al., 2008)), exploiting comparable rather than parallel corpora (Munteanu and Marcu, 2005), or paraphrasing techniques (Callison-Burch et al., 2006). The lack of contextual modeling, by contrast, has so far not been investigated in depth, although it is a generally recognized problem in machine translation. Early attempts at modeling contextual information in machine translation include (Mima et al., 1998), where information about the role, rank and gender of speakers and listeners was utilized in a transfer-based spoken-language translation system for travel dialogs. In (Kumar et al., 2008) 1227 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1227–1235, Beijing, August 2010 statistically predicted dialog acts were used in a phrase-based SMT system for three different dialog tasks and were shown to improve performance. Recently, contextual source-language features have been incorporated into translation models to predict translation phrases for traveling domain tasks (Stroppa et al., 2007; Haque et al., 2009). However, we are not aware of any work addressing contextual modeling for statistical translation of spoken meeting-style"
C10-1138,W09-0424,0,0.0412307,"Missing"
C10-1138,2004.iwslt-papers.7,0,0.0635111,"Missing"
C10-1138,J05-4003,0,0.0335087,"uch as travel expressions or patient-provider interactions in the medical domain. Recently, more advanced spoken-language data has been addressed, such as speeches (St¨uker et al., 2007), lectures (Waibel and F¨ugen, 2008), The problem of speech disfluencies has been addressed by disfluency removal techniques that are applied prior to translation (Rao et al., 2007; Wang et al., 2010). Training data sparsity has been addressed by adding data from out-of-domain resources (e.g. (Matusov et al., 2004; Hildebrandt et al., 2005; Wu et al., 2008)), exploiting comparable rather than parallel corpora (Munteanu and Marcu, 2005), or paraphrasing techniques (Callison-Burch et al., 2006). The lack of contextual modeling, by contrast, has so far not been investigated in depth, although it is a generally recognized problem in machine translation. Early attempts at modeling contextual information in machine translation include (Mima et al., 1998), where information about the role, rank and gender of speakers and listeners was utilized in a transfer-based spoken-language translation system for travel dialogs. In (Kumar et al., 2008) 1227 Proceedings of the 23rd International Conference on Computational Linguistics (Coling"
C10-1138,C08-1125,0,0.026682,"language translation has mostly concentrated on twoperson dialogues, such as travel expressions or patient-provider interactions in the medical domain. Recently, more advanced spoken-language data has been addressed, such as speeches (St¨uker et al., 2007), lectures (Waibel and F¨ugen, 2008), The problem of speech disfluencies has been addressed by disfluency removal techniques that are applied prior to translation (Rao et al., 2007; Wang et al., 2010). Training data sparsity has been addressed by adding data from out-of-domain resources (e.g. (Matusov et al., 2004; Hildebrandt et al., 2005; Wu et al., 2008)), exploiting comparable rather than parallel corpora (Munteanu and Marcu, 2005), or paraphrasing techniques (Callison-Burch et al., 2006). The lack of contextual modeling, by contrast, has so far not been investigated in depth, although it is a generally recognized problem in machine translation. Early attempts at modeling contextual information in machine translation include (Mima et al., 1998), where information about the role, rank and gender of speakers and listeners was utilized in a transfer-based spoken-language translation system for travel dialogs. In (Kumar et al., 2008) 1227 Procee"
C10-1138,2007.tmi-papers.28,0,\N,Missing
C10-1138,N06-1003,0,\N,Missing
C10-1138,zesch-etal-2008-extracting,0,\N,Missing
D14-1014,ambati-etal-2010-active,0,0.0196872,"Missing"
D14-1014,D11-1033,0,0.181205,"Missing"
D14-1014,W11-2131,0,0.0650143,"Missing"
D14-1014,W13-2206,0,0.0188484,"Missing"
D14-1014,P10-1088,0,0.0201928,"Missing"
D14-1014,P05-1032,0,0.0196014,"Missing"
D14-1014,2012.amta-papers.3,0,0.0781453,"Missing"
D14-1014,D08-1089,0,0.0673807,"Missing"
D14-1014,P09-1021,0,0.0595634,"Missing"
D14-1014,2005.eamt-1.19,0,0.168537,"Missing"
D14-1014,N07-1008,0,0.0699532,"Missing"
D14-1014,P07-2045,0,0.00351735,"Missing"
D14-1014,2005.iwslt-1.7,0,0.0682569,"Missing"
D14-1014,W04-3250,0,0.261481,"Missing"
D14-1014,P79-1022,0,0.318909,"Missing"
D14-1014,N10-1134,1,0.0716549,"Missing"
D14-1014,P11-1052,1,0.506983,"Missing"
D14-1014,D07-1104,0,0.0476268,"Missing"
D14-1014,D07-1036,0,0.134878,"Missing"
D14-1014,2012.eamt-1.65,0,0.0475943,"Missing"
D14-1014,N13-1086,1,0.530359,"Missing"
D14-1014,P10-2041,0,0.182505,"Missing"
E06-1006,N03-2002,1,0.594796,"distribution. For the case of trigrams, this can be expressed as: pBO (wt |wt−1 , wt−2 ) = ( (2) dc pM L (wt |wt−1 , wt−2 ) if c > τ α(wt−1 , wt−2 )pBO (wt |wt−1 ) otherwise where pM L denotes the maximum-likelihood estimate, c denotes the count of the triple (wi , wi−1 , wi−2 ) in the training data, τ is the count threshold above which the maximum-likelihood estimate is retained, and dN (wi ,wi−1 ,wi−2 ) is a discounting factor (generally between 0 and 1) that is applied to the higher-order distribution. The normalization factor α(wi−1 , wi−2 ) ensures that the distribution sums to one. In (Bilmes and Kirchhoff, 2003) this method was generalized to a backoff model with multiple paths, allowing the combination of different backed-off probability estimates. Hierarchical backoff schemes have also been used by (Zitouni et al., 2003) for language modeling and by (Gildea, 2001) for semantic role labeling. (Resnik et al., 2001) used backoff translation lexicons for cross-language information retrieval. More recently, (Xi and Hwa, 2005) have used backoff models for combining in-domain and 42 out-of-domain data for the purpose of bootstrapping a part-of-speech tagger for Chinese, outperforming standard methods such"
E06-1006,corston-oliver-gamon-2004-normalizing,0,0.0889109,"n. Section 8 concludes. 41 2 Morphology in SMT Systems Previous approaches have used morpho-syntactic knowledge mainly at the low-level stages of a machine translation system, i.e. for preprocessing. (Niessen and Ney, 2001a) use morpho-syntactic knowledge for reordering certain syntactic constructions that differ in word order in the source vs. target language (German and English). Reordering is applied before training and after generating the output in the target language. Normalization of English/German inflectional morphology to base forms for the purpose of word alignment is performed in (Corston-Oliver and Gamon, 2004) and (Koehn, 2005), demonstrating that the vocabulary size can be reduced significantly without affecting performance. Similar morphological simplifications have been applied to other languages such as Romanian (Fraser and Marcu, 2005) in order to decrease word alignment error rate. In (Niessen and Ney, 2001b), a hierarchical lexicon model is used that represents words as combinations of full forms, base forms, and part-of-speech tags, and that allows the word alignment training procedure to interpolate counts based on the different levels of representation. (Goldwater and McCloskey, 2005) inv"
E06-1006,W05-0814,0,0.0149949,"ic knowledge for reordering certain syntactic constructions that differ in word order in the source vs. target language (German and English). Reordering is applied before training and after generating the output in the target language. Normalization of English/German inflectional morphology to base forms for the purpose of word alignment is performed in (Corston-Oliver and Gamon, 2004) and (Koehn, 2005), demonstrating that the vocabulary size can be reduced significantly without affecting performance. Similar morphological simplifications have been applied to other languages such as Romanian (Fraser and Marcu, 2005) in order to decrease word alignment error rate. In (Niessen and Ney, 2001b), a hierarchical lexicon model is used that represents words as combinations of full forms, base forms, and part-of-speech tags, and that allows the word alignment training procedure to interpolate counts based on the different levels of representation. (Goldwater and McCloskey, 2005) investigate various morphological modifications for Czech-English translations: a subset of the vocabulary was converted to stems, pseudowords consisting of morphological tags were introduced, and combinations of stems and morphological t"
E06-1006,H05-1085,0,0.23842,"n (Corston-Oliver and Gamon, 2004) and (Koehn, 2005), demonstrating that the vocabulary size can be reduced significantly without affecting performance. Similar morphological simplifications have been applied to other languages such as Romanian (Fraser and Marcu, 2005) in order to decrease word alignment error rate. In (Niessen and Ney, 2001b), a hierarchical lexicon model is used that represents words as combinations of full forms, base forms, and part-of-speech tags, and that allows the word alignment training procedure to interpolate counts based on the different levels of representation. (Goldwater and McCloskey, 2005) investigate various morphological modifications for Czech-English translations: a subset of the vocabulary was converted to stems, pseudowords consisting of morphological tags were introduced, and combinations of stems and morphological tags were used as new word forms. Small improvements were found in combination with a word-to-word translation model. Most of these techniques have focused on improving word alignment or reducing vocabulary size; however, it is often the case that better word alignment does not improve the overall translation performance of a standard phrase-based SMT system."
E06-1006,W05-0820,0,0.0290392,"Missing"
E06-1006,W05-0830,0,0.0197674,"ry was converted to stems, pseudowords consisting of morphological tags were introduced, and combinations of stems and morphological tags were used as new word forms. Small improvements were found in combination with a word-to-word translation model. Most of these techniques have focused on improving word alignment or reducing vocabulary size; however, it is often the case that better word alignment does not improve the overall translation performance of a standard phrase-based SMT system. Phrase-based models themselves have not benefited much from additional morpho-syntactic knowledge; e.g. (Lioma and Ounis, 2005) do not report any improvement from integrating part-ofspeech information at the phrase level. One successful application of morphological knowledge is (de Gispert et al., 2005), where knowledge-based morphological techniques are used to identify unseen verb forms in the test text and to generate inflected forms in the target language based on annotated POS tags and lemmas. Phrase prediction in the target language is conditioned on the phrase in the source language as well the corresponding tuple of lemmatized phrases. This technique worked well for translating from a morphologically poor lang"
E06-1006,2001.mtsummit-papers.45,0,0.0490766,"y-inflected languages, German and Finnish, into English and present improvements over a state-of-the-art system. The rest of the paper is structured as follows: The following section discusses related background work. Section 4 describes the proposed model; Sections 5 and 6 provide details about the data and baseline system used in this study. Section 7 provides experimental results and discussion. Section 8 concludes. 41 2 Morphology in SMT Systems Previous approaches have used morpho-syntactic knowledge mainly at the low-level stages of a machine translation system, i.e. for preprocessing. (Niessen and Ney, 2001a) use morpho-syntactic knowledge for reordering certain syntactic constructions that differ in word order in the source vs. target language (German and English). Reordering is applied before training and after generating the output in the target language. Normalization of English/German inflectional morphology to base forms for the purpose of word alignment is performed in (Corston-Oliver and Gamon, 2004) and (Koehn, 2005), demonstrating that the vocabulary size can be reduced significantly without affecting performance. Similar morphological simplifications have been applied to other languag"
E06-1006,W01-1407,0,0.0235932,"y-inflected languages, German and Finnish, into English and present improvements over a state-of-the-art system. The rest of the paper is structured as follows: The following section discusses related background work. Section 4 describes the proposed model; Sections 5 and 6 provide details about the data and baseline system used in this study. Section 7 provides experimental results and discussion. Section 8 concludes. 41 2 Morphology in SMT Systems Previous approaches have used morpho-syntactic knowledge mainly at the low-level stages of a machine translation system, i.e. for preprocessing. (Niessen and Ney, 2001a) use morpho-syntactic knowledge for reordering certain syntactic constructions that differ in word order in the source vs. target language (German and English). Reordering is applied before training and after generating the output in the target language. Normalization of English/German inflectional morphology to base forms for the purpose of word alignment is performed in (Corston-Oliver and Gamon, 2004) and (Koehn, 2005), demonstrating that the vocabulary size can be reduced significantly without affecting performance. Similar morphological simplifications have been applied to other languag"
E06-1006,P03-1021,0,0.0367343,"he full training set. The sizes of the partitions are shown in Table 1, together with the resulting percentage of out-of-vocabulary (OOV) words in the development and test sets (“type” refers to a unique word in the vocabulary, “token” to an instance in the actual text). 6 System We use a two-pass phrase-based statistical MT system using GIZA++ (Och and Ney, 2000) for word alignment and Pharaoh (Koehn, 2004) for phrase extraction and decoding. Word alignment is performed in both directions using the IBM4 model. Phrases are then extracted from the word alignments using the method described in (Och and Ney, 2003). For first-pass decoding we use Pharaoh in n-best mode. The decoder uses a weighted combination of seven scores: 4 translation model scores (phrase-based and lexical scores for both directions), a trigram language model score, a distortion score, and a word penalty. Nonmonotonic decoding is used, with no limit on the 44 Set train1 train2 train3 train4 train5 # sent 5K 25K 50K 250K 751K Set train1 train2 train3 train4 train5 # sent 5K 25K 50K 250K 717K German-English # words oov dev 101K 7.9/42.6 505K 3.8/22.1 1013K 2.7/16.1 5082K 1.3/8.1 15258K 0.8/4.9 Finnish-English # words oov dev 78K 16.6"
E06-1006,H01-1033,0,0.0265817,"t threshold above which the maximum-likelihood estimate is retained, and dN (wi ,wi−1 ,wi−2 ) is a discounting factor (generally between 0 and 1) that is applied to the higher-order distribution. The normalization factor α(wi−1 , wi−2 ) ensures that the distribution sums to one. In (Bilmes and Kirchhoff, 2003) this method was generalized to a backoff model with multiple paths, allowing the combination of different backed-off probability estimates. Hierarchical backoff schemes have also been used by (Zitouni et al., 2003) for language modeling and by (Gildea, 2001) for semantic role labeling. (Resnik et al., 2001) used backoff translation lexicons for cross-language information retrieval. More recently, (Xi and Hwa, 2005) have used backoff models for combining in-domain and 42 out-of-domain data for the purpose of bootstrapping a part-of-speech tagger for Chinese, outperforming standard methods such as EM. i i i 4 Backoff Models in MT In order to handle unseen words in the test data we propose a hierarchical backoff model that uses morphological information. Several morphological operations, in particular stemming and compound splitting, are interleaved such that a more specific form (i.e. a form close"
E06-1006,H05-1107,0,0.0246731,"ctor (generally between 0 and 1) that is applied to the higher-order distribution. The normalization factor α(wi−1 , wi−2 ) ensures that the distribution sums to one. In (Bilmes and Kirchhoff, 2003) this method was generalized to a backoff model with multiple paths, allowing the combination of different backed-off probability estimates. Hierarchical backoff schemes have also been used by (Zitouni et al., 2003) for language modeling and by (Gildea, 2001) for semantic role labeling. (Resnik et al., 2001) used backoff translation lexicons for cross-language information retrieval. More recently, (Xi and Hwa, 2005) have used backoff models for combining in-domain and 42 out-of-domain data for the purpose of bootstrapping a part-of-speech tagger for Chinese, outperforming standard methods such as EM. i i i 4 Backoff Models in MT In order to handle unseen words in the test data we propose a hierarchical backoff model that uses morphological information. Several morphological operations, in particular stemming and compound splitting, are interleaved such that a more specific form (i.e. a form closer to the full word form) is chosen before a more general form (i.e. a form that has undergone morphological pr"
E06-1006,koen-2004-pharaoh,0,0.249781,"urther described be00 , f 00 ) = split(f ). The match with low), i.e.(fi1 i i2 the original word-based phrase table is then performed again. If this step fails for either of the 000 = two parts of f 00 , stemming is applied again: fi1 00 ) and f 000 = stem(f 00 ), and a match with stem(fi1 i2 i2 the stemmed phrase table entries is carried out. Only if the attempted match fails at this level is the input passed on verbatim in the translation output. The backoff procedure could in principle be performed on demand by a specialized decoder; however, since we use an off-the-shelf decoder (Pharaoh (Koehn, 2004)), backoff is implicitly enforced by providing a phrase-table that includes all required backoff levels and by preprocessing the test data accordingly. The phrase table will thus include entries for phrases based on full word forms as well as for their stemmed and/or split counterparts. For each entry with decomposed morphological i1 i2 i i1 i1 i2 i2 i1 i2 Figure 1: Backoff procedure. forms, four probabilities need to be provided: two phrasal translation scores for both translation directions, p(¯ e|f¯) and p(f¯|¯ e), and two corresponding lexical scores, which are computed as a product of the"
E06-1006,2005.mtsummit-papers.11,0,0.0791424,"gy in SMT Systems Previous approaches have used morpho-syntactic knowledge mainly at the low-level stages of a machine translation system, i.e. for preprocessing. (Niessen and Ney, 2001a) use morpho-syntactic knowledge for reordering certain syntactic constructions that differ in word order in the source vs. target language (German and English). Reordering is applied before training and after generating the output in the target language. Normalization of English/German inflectional morphology to base forms for the purpose of word alignment is performed in (Corston-Oliver and Gamon, 2004) and (Koehn, 2005), demonstrating that the vocabulary size can be reduced significantly without affecting performance. Similar morphological simplifications have been applied to other languages such as Romanian (Fraser and Marcu, 2005) in order to decrease word alignment error rate. In (Niessen and Ney, 2001b), a hierarchical lexicon model is used that represents words as combinations of full forms, base forms, and part-of-speech tags, and that allows the word alignment training procedure to interpolate counts based on the different levels of representation. (Goldwater and McCloskey, 2005) investigate various m"
E06-1006,W04-3250,0,\N,Missing
N06-2001,W03-1021,0,\N,Missing
N06-2001,N03-2002,1,\N,Missing
N06-2001,C04-1022,1,\N,Missing
N06-2001,J92-4003,0,\N,Missing
N06-2001,H05-1026,0,\N,Missing
N07-1026,P06-1017,0,0.0200953,"ce label information will be propagated from labeled points through the entire space. This is why LP works well with a local distance measure that might be unsuitable as a global distance measure. Applications of LP include handwriting recognition (Zhu and Ghahramani, 2002), image classification (Balcan et al., 2005) and retrieval (Qin et al., 2005), and protein classification (Weston et al., 2003). In NLP, label propagation has been used for word sense disambiguation (Niu et al., 2005), document classification (Zhu, 2005), sentiment analysis (Goldberg and Zhu, 2006), and relation extraction (Chen et al., 2006). 2.2 Graph construction One of the main problems in LP, as well as other graph-based learning techniques, is how to best construct the graph. Currently, graph construction “is more of an art than science” (Zhu, 2005). Typically, edge weights are derived from a simple Euclidean or cosine distance measure, regardless of the nature of the underlying features. Edges are then established either by connecting all nodes, by applying a single global threshold to the edge weights, or by connecting each node to its k nearest neighbors according to the edge weights. This procedure is often suboptimal: E"
N07-1026,W06-3808,0,0.498094,"me label. The global distance is less relevant since label information will be propagated from labeled points through the entire space. This is why LP works well with a local distance measure that might be unsuitable as a global distance measure. Applications of LP include handwriting recognition (Zhu and Ghahramani, 2002), image classification (Balcan et al., 2005) and retrieval (Qin et al., 2005), and protein classification (Weston et al., 2003). In NLP, label propagation has been used for word sense disambiguation (Niu et al., 2005), document classification (Zhu, 2005), sentiment analysis (Goldberg and Zhu, 2006), and relation extraction (Chen et al., 2006). 2.2 Graph construction One of the main problems in LP, as well as other graph-based learning techniques, is how to best construct the graph. Currently, graph construction “is more of an art than science” (Zhu, 2005). Typically, edge weights are derived from a simple Euclidean or cosine distance measure, regardless of the nature of the underlying features. Edges are then established either by connecting all nodes, by applying a single global threshold to the edge weights, or by connecting each node to its k nearest neighbors according to the edge w"
N07-1026,W04-0831,0,0.0606024,"Missing"
N07-1026,H05-1049,0,0.025514,"ifier predictions with known features. The proposed approach is evaluated on a lexicon learning task using the Wall Street Journal (WSJ) corpus, and on the SENSEVAL-3 word sense disambiguation task. In both cases our technique significantly outperforms our baseline systems (label propagation using standard graph construction and discriminatively trained supervised classifiers). 2 Background Several graph-based learning techniques have recently been developed and applied to NLP problems: minimum cuts (Pang and Lee, 2004), random walks (Mihalcea, 2005; Otterbacher et al., 2005), graph matching (Haghighi et al., 2005), and label propagation (Niu et al., 2005). Here we focus on label propagation as a learning technique. 2.1 Label propagation The basic label propagation (LP) algorithm (Zhu and Ghahramani, 2002; Zhu, 2005) has as inputs: • a labeled set {(x1 , y1 ), (x2 , y2 ), . . . , (xn , yn )}, where xi are samples (feature vectors) and y i ∈ {1, 2, . . . , C} are their corresponding labels; • an unlabeled set {xn+1 , . . . , xN }; • a distance measure d(i, j) i, j ∈ {1, . . . N } defined on the feature space. The goal is to infer the labels {yn+1 , . . . , yN } for the unlabeled set. The algorithm repres"
N07-1026,W02-1006,0,0.162105,"; • Boolean: word contains other special characters (e.g. “&”). We have also experimented with shorter suffixes and with prefixes but those features tended to degrade 207 performance. 4.2 SENSEVAL-3 word sense disambiguation task The second task is word sense disambiguation using the SENSEVAL-3 corpus (Mihalcea et al., 2004), to enable a comparison of our method with previously published results. The goal is to disambiguate the different senses of each of 57 words given the sentences within which they occur. There are 7860 samples for training and 3944 for testing. In line with existing work (Lee and Ng, 2002; Niu et al., 2005), we use the following features: • Integer × 7: seven features consisting of the POS of the previous three words, the POS of the next three words, and the POS of the word itself. We used the MXPOST tagger (Ratnaparkhi, 1996) for POS annotation. • Integer ×hvariable lengthi: a bag of all words in the surrounding context. • Integer × 15: Local collocations C ij (i, j are the bounds of the collocation window)—word combinations from the context of the word to disambiguate. In addition to the 11 collocations used in similar work (Lee and Ng, 2002), we also used C−3,1 , C−3,2 , C−"
N07-1026,W04-0834,0,0.0367934,"Missing"
N07-1026,W04-0807,0,0.033064,"-letter suffix of the word; • Integer × 4: The indices of the four most frequent words that immediately precede the word in the WSJ text; • Boolean: word contains capital letters; • Boolean: word consists only of capital letters; • Boolean: word contains digits; • Boolean: word contains a hyphen; • Boolean: word contains other special characters (e.g. “&”). We have also experimented with shorter suffixes and with prefixes but those features tended to degrade 207 performance. 4.2 SENSEVAL-3 word sense disambiguation task The second task is word sense disambiguation using the SENSEVAL-3 corpus (Mihalcea et al., 2004), to enable a comparison of our method with previously published results. The goal is to disambiguate the different senses of each of 57 words given the sentences within which they occur. There are 7860 samples for training and 3944 for testing. In line with existing work (Lee and Ng, 2002; Niu et al., 2005), we use the following features: • Integer × 7: seven features consisting of the POS of the previous three words, the POS of the next three words, and the POS of the word itself. We used the MXPOST tagger (Ratnaparkhi, 1996) for POS annotation. • Integer ×hvariable lengthi: a bag of all wor"
N07-1026,H05-1052,0,0.0302114,"7 Association for Computational Linguistics nation of classifier predictions with known features. The proposed approach is evaluated on a lexicon learning task using the Wall Street Journal (WSJ) corpus, and on the SENSEVAL-3 word sense disambiguation task. In both cases our technique significantly outperforms our baseline systems (label propagation using standard graph construction and discriminatively trained supervised classifiers). 2 Background Several graph-based learning techniques have recently been developed and applied to NLP problems: minimum cuts (Pang and Lee, 2004), random walks (Mihalcea, 2005; Otterbacher et al., 2005), graph matching (Haghighi et al., 2005), and label propagation (Niu et al., 2005). Here we focus on label propagation as a learning technique. 2.1 Label propagation The basic label propagation (LP) algorithm (Zhu and Ghahramani, 2002; Zhu, 2005) has as inputs: • a labeled set {(x1 , y1 ), (x2 , y2 ), . . . , (xn , yn )}, where xi are samples (feature vectors) and y i ∈ {1, 2, . . . , C} are their corresponding labels; • an unlabeled set {xn+1 , . . . , xN }; • a distance measure d(i, j) i, j ∈ {1, . . . N } defined on the feature space. The goal is to infer the labe"
N07-1026,W04-0839,0,0.0163663,"the previous three words, the POS of the next three words, and the POS of the word itself. We used the MXPOST tagger (Ratnaparkhi, 1996) for POS annotation. • Integer ×hvariable lengthi: a bag of all words in the surrounding context. • Integer × 15: Local collocations C ij (i, j are the bounds of the collocation window)—word combinations from the context of the word to disambiguate. In addition to the 11 collocations used in similar work (Lee and Ng, 2002), we also used C−3,1 , C−3,2 , C−2,3 , C−1,3 . Note that syntactic features, which have been used in some previous studies on this dataset (Mohammad and Pedersen, 2004), were not included. We apply a simple feature selection method: a feature X is selected if the conditional entropy H(Y |X) is above a fixed threshold (1 bit) in the training set, and if X also occurs in the test set (note that no label information from the test data is used for this purpose). 5 Experiments For both tasks we compare the performance of a supervised classifier, label propagation using the standard input features and either Euclidean or cosine distance, and LP using the output from a first-pass supervised classifier. 5.1 5.1.1 Lexicon acquisition task First-pass classifier For th"
N07-1026,P05-1049,0,0.473853,"osed approach is evaluated on a lexicon learning task using the Wall Street Journal (WSJ) corpus, and on the SENSEVAL-3 word sense disambiguation task. In both cases our technique significantly outperforms our baseline systems (label propagation using standard graph construction and discriminatively trained supervised classifiers). 2 Background Several graph-based learning techniques have recently been developed and applied to NLP problems: minimum cuts (Pang and Lee, 2004), random walks (Mihalcea, 2005; Otterbacher et al., 2005), graph matching (Haghighi et al., 2005), and label propagation (Niu et al., 2005). Here we focus on label propagation as a learning technique. 2.1 Label propagation The basic label propagation (LP) algorithm (Zhu and Ghahramani, 2002; Zhu, 2005) has as inputs: • a labeled set {(x1 , y1 ), (x2 , y2 ), . . . , (xn , yn )}, where xi are samples (feature vectors) and y i ∈ {1, 2, . . . , C} are their corresponding labels; • an unlabeled set {xn+1 , . . . , xN }; • a distance measure d(i, j) i, j ∈ {1, . . . N } defined on the feature space. The goal is to infer the labels {yn+1 , . . . , yN } for the unlabeled set. The algorithm represents all N data points as vertices in an u"
N07-1026,H05-1115,0,0.0117137,"r Computational Linguistics nation of classifier predictions with known features. The proposed approach is evaluated on a lexicon learning task using the Wall Street Journal (WSJ) corpus, and on the SENSEVAL-3 word sense disambiguation task. In both cases our technique significantly outperforms our baseline systems (label propagation using standard graph construction and discriminatively trained supervised classifiers). 2 Background Several graph-based learning techniques have recently been developed and applied to NLP problems: minimum cuts (Pang and Lee, 2004), random walks (Mihalcea, 2005; Otterbacher et al., 2005), graph matching (Haghighi et al., 2005), and label propagation (Niu et al., 2005). Here we focus on label propagation as a learning technique. 2.1 Label propagation The basic label propagation (LP) algorithm (Zhu and Ghahramani, 2002; Zhu, 2005) has as inputs: • a labeled set {(x1 , y1 ), (x2 , y2 ), . . . , (xn , yn )}, where xi are samples (feature vectors) and y i ∈ {1, 2, . . . , C} are their corresponding labels; • an unlabeled set {xn+1 , . . . , xN }; • a distance measure d(i, j) i, j ∈ {1, . . . N } defined on the feature space. The goal is to infer the labels {yn+1 , . . . , yN } for"
N07-1026,P04-1035,0,0.00507866,"1, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics nation of classifier predictions with known features. The proposed approach is evaluated on a lexicon learning task using the Wall Street Journal (WSJ) corpus, and on the SENSEVAL-3 word sense disambiguation task. In both cases our technique significantly outperforms our baseline systems (label propagation using standard graph construction and discriminatively trained supervised classifiers). 2 Background Several graph-based learning techniques have recently been developed and applied to NLP problems: minimum cuts (Pang and Lee, 2004), random walks (Mihalcea, 2005; Otterbacher et al., 2005), graph matching (Haghighi et al., 2005), and label propagation (Niu et al., 2005). Here we focus on label propagation as a learning technique. 2.1 Label propagation The basic label propagation (LP) algorithm (Zhu and Ghahramani, 2002; Zhu, 2005) has as inputs: • a labeled set {(x1 , y1 ), (x2 , y2 ), . . . , (xn , yn )}, where xi are samples (feature vectors) and y i ∈ {1, 2, . . . , C} are their corresponding labels; • an unlabeled set {xn+1 , . . . , xN }; • a distance measure d(i, j) i, j ∈ {1, . . . N } defined on the feature space."
N07-1026,W96-0213,0,0.0172139,"task is word sense disambiguation using the SENSEVAL-3 corpus (Mihalcea et al., 2004), to enable a comparison of our method with previously published results. The goal is to disambiguate the different senses of each of 57 words given the sentences within which they occur. There are 7860 samples for training and 3944 for testing. In line with existing work (Lee and Ng, 2002; Niu et al., 2005), we use the following features: • Integer × 7: seven features consisting of the POS of the previous three words, the POS of the next three words, and the POS of the word itself. We used the MXPOST tagger (Ratnaparkhi, 1996) for POS annotation. • Integer ×hvariable lengthi: a bag of all words in the surrounding context. • Integer × 15: Local collocations C ij (i, j are the bounds of the collocation window)—word combinations from the context of the word to disambiguate. In addition to the 11 collocations used in similar work (Lee and Ng, 2002), we also used C−3,1 , C−3,2 , C−2,3 , C−1,3 . Note that syntactic features, which have been used in some previous studies on this dataset (Mohammad and Pedersen, 2004), were not included. We apply a simple feature selection method: a feature X is selected if the conditional"
N07-1026,W04-0856,0,0.0556995,"Missing"
N09-1014,N07-1026,1,0.875641,"gns similar soft labels (identical hard labels) to nodes linked by edges with large weights (i.e., highly similar samples). The labeling decision takes into account not only similarities between labeled and unlabeled nodes (as in nearest-neighbor approaches) but also similarities among unlabeled nodes. Label propagation has been used successfully for various classification tasks, e.g. image classification and handwriting recognition (Zhu, 2005). In natural language processing, label propagation has been used for document classification (Zhu, 2005), word sense disambiguation (Niu et al., 2005; Alexandrescu and Kirchhoff, 2007), and sentiment categorization (Goldberg and Zhu, 2006). 3 Graph-Based Learning for Machine Translation Our goal is to exploit graph-based learning for improving consistency in statistical phrase-based machine translation. Intuitively, a set of similar source sentences should receive similar target-language translations. This means that similarities between training and test sentences should be taken into account, but also similarities between different test sentences, which is a source of information currently not exploited by standard SMT systems. To this end we define a graph over the train"
N09-1014,E06-1032,0,0.0143076,"002) and a score based on string kernels. In using BLEU we treat each sentence as a complete document. BLEU is not symmetric—when comparing two sentences, different results are obtained depending on which one is considered the reference and which one is the hypothesis. For computing similarities between train and test translations, we use the train translation as 122 the reference. For computing similarity between two test hypotheses, we compute BLEU in both directions and take the average. We note that more appropriate distance measures are certainly possible. Many previous studies, such as (Callison-Burch et al., 2006), have pointed out drawbacks of BLEU, and any other similarity measure could be utilized instead. In particular, similarity measures that model aspects of sentences that are ill handled by standard phrase-based decoders (such as syntactic structure or semantic information) could be useful here. A more general way of computing similarity between strings is provided by string kernels (Lodhi et al., 2002; Rousu and Shawe-Taylor, 2005), which have been extensively used in bioinformatics and email spam detection. String kernels map strings into a feature space defined by all possible substrings of"
N09-1014,W06-3808,0,0.347027,"y edges with large weights (i.e., highly similar samples). The labeling decision takes into account not only similarities between labeled and unlabeled nodes (as in nearest-neighbor approaches) but also similarities among unlabeled nodes. Label propagation has been used successfully for various classification tasks, e.g. image classification and handwriting recognition (Zhu, 2005). In natural language processing, label propagation has been used for document classification (Zhu, 2005), word sense disambiguation (Niu et al., 2005; Alexandrescu and Kirchhoff, 2007), and sentiment categorization (Goldberg and Zhu, 2006). 3 Graph-Based Learning for Machine Translation Our goal is to exploit graph-based learning for improving consistency in statistical phrase-based machine translation. Intuitively, a set of similar source sentences should receive similar target-language translations. This means that similarities between training and test sentences should be taken into account, but also similarities between different test sentences, which is a source of information currently not exploited by standard SMT systems. To this end we define a graph over the training and test sets with edges between test and training"
N09-1014,P07-2045,0,0.00359131,"lly extracted parallel data) in the case of the AE system. Set # sent pairs # words # refs IE train IE dev-1 IE dev-2 IE eval 26.5K 500 496 724 160K 4308 4204 6481 1 1 1 4 AE train AE dev4 AE dev5 AE eval 23K 489 500 489 160K 5392 5981 2893 1 7 7 6 Table 1: Data set sizes and reference translations count. Our baseline is a standard phrase-based SMT system based on a log-linear model with the following feature functions: two phrase-based translation scores, two lexical translation scores, word count and phrase count penalty, distortion score, and language model score. We use the Moses decoder (Koehn et al., 2007) with a reordering limit of 4 for both languages, which generates N -best lists of up to 2000 hypotheses per sentence in a first pass. The second pass uses a part-of-speech (POS) based trigram model, trained on POS sequences generated by a MaxEnt tagger (Ratnaparkhi, 1996). The language models are trained on the English side using SRILM (Stolcke, 2002) and modified Kneser-Ney discounting for the first-pass models and WittenBell discounting for the POS models. The baseline system yields state-of-the-art performance. Weighting dev-2 eval System dev-2 eval none (baseline) (a) (b) (c) 22.3/53.3 23"
N09-1014,2005.mtsummit-papers.11,0,0.00470617,"es. The second is a standard travel expression translation task consisting entirely of read input. For our experiments we chose the text input (correct transcription) condition only. The data set sizes are shown in Table 1. We split the IE development set into two subsets of 500 and 496 sentences each. The first set (dev-1) is used to train the system parameters of the baseline system and as a training set for GBL. The second is used to tune the GBL parameters. For each language pair, the baseline system was trained with additional out-of-domain text data: the Italian-English Europarl corpus (Koehn, 2005) in the case of the IE system, and 5.5M words of newswire data (LDC Arabic Newswire, MultipleTranslation Corpus and ISI automatically extracted parallel data) in the case of the AE system. Set # sent pairs # words # refs IE train IE dev-1 IE dev-2 IE eval 26.5K 500 496 724 160K 4308 4204 6481 1 1 1 4 AE train AE dev4 AE dev5 AE eval 23K 489 500 489 160K 5392 5981 2893 1 7 7 6 Table 1: Data set sizes and reference translations count. Our baseline is a standard phrase-based SMT system based on a log-linear model with the following feature functions: two phrase-based translation scores, two lexic"
N09-1014,P01-1050,0,0.0112782,"s are (usually commercial) databases of segment translations extracted from a large database of translation examples. They are typically used by human translators to retrieve translation candidates for subsequences of a new input text. Matches can be exact or fuzzy; the latter is similar to the identification of graph neighborhoods in our approach. However, our GBL scheme propagates similarity scores not just from known to unknown sentences but also indirectly, via connections through other unknown sentences. The combination of a translation memory and statistical translation was reported in (Marcu, 2001); however, this is a combination of word-based and phrase-based translation predating the current phrase-based approach to SMT. 7 Conclusion 6 Related Work GBL is an instance of semi-supervised learning, specifically transductive learning. A different form of semi-supervised learning (self-training) has been applied to MT by (Ueffing et al., 2007). Ours is the first study to explore a graph-based learning approach. In the machine learning community, work on applying GBL to structured outputs is beginning to emerge. Transductive graph-based regularization has been applied to large-margin learni"
N09-1014,1992.tmi-1.15,0,0.0372904,"e me could you take a photo of us pardon would you mind taking a photo of us pardon me could you take our picture pardon me would you take a picture of us excuse me could you take a picture of u Similar sentences: could you get two tickets for us please take a picture for me could you please take a picture of us as opposed to entire graphs. String kernel representations have been used in MT (Szedmak, 2007) in a kernel regression based framework, which, however, was an entirely supervised framework. Finally, our approach can be likened to a probabilistic implementation of translation memories (Maruyana and Watanabe, 1992; Veale and Way, 1997). Translation memories are (usually commercial) databases of segment translations extracted from a large database of translation examples. They are typically used by human translators to retrieve translation candidates for subsequences of a new input text. Matches can be exact or fuzzy; the latter is similar to the identification of graph neighborhoods in our approach. However, our GBL scheme propagates similarity scores not just from known to unknown sentences but also indirectly, via connections through other unknown sentences. The combination of a translation memory an"
N09-1014,P05-1049,0,0.0413516,"ent possible, assigns similar soft labels (identical hard labels) to nodes linked by edges with large weights (i.e., highly similar samples). The labeling decision takes into account not only similarities between labeled and unlabeled nodes (as in nearest-neighbor approaches) but also similarities among unlabeled nodes. Label propagation has been used successfully for various classification tasks, e.g. image classification and handwriting recognition (Zhu, 2005). In natural language processing, label propagation has been used for document classification (Zhu, 2005), word sense disambiguation (Niu et al., 2005; Alexandrescu and Kirchhoff, 2007), and sentiment categorization (Goldberg and Zhu, 2006). 3 Graph-Based Learning for Machine Translation Our goal is to exploit graph-based learning for improving consistency in statistical phrase-based machine translation. Intuitively, a set of similar source sentences should receive similar target-language translations. This means that similarities between training and test sentences should be taken into account, but also similarities between different test sentences, which is a source of information currently not exploited by standard SMT systems. To this e"
N09-1014,P02-1040,0,0.0947574,"milarity threshold results in a rich graph with a large number of edges but possibly introduces noise. A higher threshold leads to a small graph emphasizing highly similar samples but with too many disconnected components. The similarity measure is also the means by which domain knowledge can be incorporated into the graph construction process. Similarity may be defined at the level of surface word strings, but may also include linguistic information such as morphological features, part-of-speech tags, or syntactic structures. Here, we compare two similarity measures: the familiar BLEU score (Papineni et al., 2002) and a score based on string kernels. In using BLEU we treat each sentence as a complete document. BLEU is not symmetric—when comparing two sentences, different results are obtained depending on which one is considered the reference and which one is the hypothesis. For computing similarities between train and test translations, we use the train translation as 122 the reference. For computing similarity between two test hypotheses, we compute BLEU in both directions and take the average. We note that more appropriate distance measures are certainly possible. Many previous studies, such as (Call"
N09-1014,N07-2047,0,0.0355353,"Missing"
N09-1014,P07-1004,0,0.0114159,"approach. However, our GBL scheme propagates similarity scores not just from known to unknown sentences but also indirectly, via connections through other unknown sentences. The combination of a translation memory and statistical translation was reported in (Marcu, 2001); however, this is a combination of word-based and phrase-based translation predating the current phrase-based approach to SMT. 7 Conclusion 6 Related Work GBL is an instance of semi-supervised learning, specifically transductive learning. A different form of semi-supervised learning (self-training) has been applied to MT by (Ueffing et al., 2007). Ours is the first study to explore a graph-based learning approach. In the machine learning community, work on applying GBL to structured outputs is beginning to emerge. Transductive graph-based regularization has been applied to large-margin learning on structured data (Altun et al., 2005). However, scalability quickly becomes a problem with these approaches; we solve that issue by working on transitive closures 126 We have presented a graph-based learning scheme to implement a consistency model for SMT that encourages similar inputs to receive similar outputs. Evaluation on two small-scale"
N13-1086,P79-1022,0,0.419949,"and c(·) ≥ 0 is sentence cost. f (S) was instantiated by a form of saturated coverage: X fSC (S) = min{Ci (S), αCi (V )} (2) i∈V P where Ci (S) = j∈S wij , and where wij ≥ 0 indicates the similarity between sentences i and j — Ci : 2V → R is itself monotone submodular (modular in fact) and 0 ≤ α ≤ 1 is a saturation coefficient. fSC (S) is monotone submodular and therefore has the previously mentioned performance guarantees. The weighting function w was implemented as the cosine similarity between TF-IDF weighted n-gram count vectors for the sentences in the dataset. 3.2 Submodular functions (Edmonds, 1970) have been widely studied in mathematics, economics, and operations research and have recently attracted interest in machine learning (Krause and Guestrin, 2011). A submodular function is defined as follows: Given a finite ground set of objects (samples) V = {v1 , ..., vn } and a function f : 2V → R+ that returns a real value for any subset S ⊆ V , f is submodular if ∀A ⊆ B, and v ∈ / B, f (A + v) − f (A) ≥ f (B + v) − f (B). That is, the incremental “value” of v decreases when the set in which v is considered grows from A to B. Powerful optimization guarantees exist for certain subtypes of su"
N13-1086,P11-1052,1,0.599785,"hone-based TF-IDF measure. The latter is comparable to methods used in this paper, though the first term in their objective function still requires a word recognizer. In (Wu et al., 2007) acoustic training data associated with transcriptions is subselected to maximize the entropy of the distribution over linguistic units (phones or words). Most importantly, all these methods select samples in a greedy fashion without optimality guarantees. As we will explain in the next section, greedy selection is near-optimal only when applied to monotone submodular functions. 3 Submodular Functions 3.1 In (Lin and Bilmes, 2011) submodular functions were recently applied to extractive document summarization. The problem was formulated as a monotone submodular function that had to be maximized subject to cardinality or knapsack constraints: argmaxS⊆V {f (S) : c(S) ≤ K} 722 (1) where V is the set of sentences to be summarized, K is the maximum number of sentences to be selected, and c(·) ≥ 0 is sentence cost. f (S) was instantiated by a form of saturated coverage: X fSC (S) = min{Ci (S), αCi (V )} (2) i∈V P where Ci (S) = j∈S wij , and where wij ≥ 0 indicates the similarity between sentences i and j — Ci : 2V → R is it"
N15-1102,P14-2131,0,0.0208441,"Missing"
N15-1102,D13-1174,0,0.0170118,"2007; Toutanova et al., 2008) morphological generation was applied as a postprocessing step for translation into morphologically-rich languages. A maximumentropy Markov model was trained to predict the correct inflection for every stemmed word in the 995 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 995–1000, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics machine translation output from a first-pass system, conditioned on a set of lexical, morphological and syntactic features. More recently, (Chahuneau et al., 2013) applied a similar translate-and-inflect approach, utilizing unsupervised in addition to supervised morphological analyses. Inflection generation models were also used by (Fraser et al., 2012; Weller et al., 2013) for translation into German, and by (El Kholy and Habash, 2012) for Modern Standard Arabic. (Sultan, 2011) added both syntactic information on the source side that was used in filtering the phrase table, plus postprocessing on the target side for English-Arabic translation. Still other approaches enrich the translation system with morphology-aware feature functions or specific agreem"
N15-1102,condon-etal-2010-evaluation,0,0.0130305,"ils about the system can be found in (et al., 2013). The system was evaluated in live mode with native IA speakers as part of the DARPA BOLT Phase-II benchmark evaluations. The predefined scenarios included military and humanitarian assistance/disaster relief scenarios as well as general topics. All system interactions were logged and evaluated by bilingual human assessors. During debriefing sessions with the users, some users voiced dissatisfaction with the translation quality, and a subsequent detailed error analysis was conducted on the logs of 30 interactions. Similar to previous studies (Condon et al., 2010) we found that a frequently recurring problem was wrong morphological verb forms in the IA output. Some examples are shown in Table 1. In Example 1, to make sure should be translated by a first-person plural verb but it is translated by a second-person plural form, changing the meaning to (you (pl.) make sure). The desired verb form would be ntAkd. Similarly, in Example 2 the translation of transport should agree with the translations of someone and the preceding 1 2 you need to tell the locals to evacuate the area so we can secure the area to make sure no one gets hurt lAzm tqwl Alhm AhAly Al"
N15-1102,de-marneffe-etal-2006-generating,0,0.00759804,"Missing"
N15-1102,P14-1129,0,0.103931,"Missing"
N15-1102,2012.eamt-1.6,0,0.0466328,"Missing"
N15-1102,E12-1068,0,0.0168012,"ict the correct inflection for every stemmed word in the 995 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 995–1000, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics machine translation output from a first-pass system, conditioned on a set of lexical, morphological and syntactic features. More recently, (Chahuneau et al., 2013) applied a similar translate-and-inflect approach, utilizing unsupervised in addition to supervised morphological analyses. Inflection generation models were also used by (Fraser et al., 2012; Weller et al., 2013) for translation into German, and by (El Kholy and Habash, 2012) for Modern Standard Arabic. (Sultan, 2011) added both syntactic information on the source side that was used in filtering the phrase table, plus postprocessing on the target side for English-Arabic translation. Still other approaches enrich the translation system with morphology-aware feature functions or specific agreement models (Koehn and Hoang, 2007; Green and DeNero, 2012; Williams and Koehn, 2011). In contrast to the above studies, which have concentrated on text translation, this paper focuses on spok"
N15-1102,H05-1085,0,0.0241787,"a translate-and-inflect method, where a first-pass SMT system is trained on lemmatized forms, and the correct inflection for every word is predicted in a second pass by statistical classifiers trained on a combination of source and target language features. This paper looks at morphological modeling from a different perspective, namely to improve SMT in a real-time speech2 Prior Work Much work in SMT has addressed the issue of translating from morphologically-rich languages by preprocessing the source and/or target data by e.g., stemming and morphological decomposition (Popovic and Ney, 2004; Goldwater and McClosky, 2005), compound splitting (Koehn and Knight, 2003), or various forms of tokenization (Lee, 2004; Habash and Sadat, 2006). In (Minkov et al., 2007; Toutanova et al., 2008) morphological generation was applied as a postprocessing step for translation into morphologically-rich languages. A maximumentropy Markov model was trained to predict the correct inflection for every stemmed word in the 995 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 995–1000, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics machi"
N15-1102,N06-2013,0,0.0319757,"for every word is predicted in a second pass by statistical classifiers trained on a combination of source and target language features. This paper looks at morphological modeling from a different perspective, namely to improve SMT in a real-time speech2 Prior Work Much work in SMT has addressed the issue of translating from morphologically-rich languages by preprocessing the source and/or target data by e.g., stemming and morphological decomposition (Popovic and Ney, 2004; Goldwater and McClosky, 2005), compound splitting (Koehn and Knight, 2003), or various forms of tokenization (Lee, 2004; Habash and Sadat, 2006). In (Minkov et al., 2007; Toutanova et al., 2008) morphological generation was applied as a postprocessing step for translation into morphologically-rich languages. A maximumentropy Markov model was trained to predict the correct inflection for every stemmed word in the 995 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 995–1000, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics machine translation output from a first-pass system, conditioned on a set of lexical, morphological and syntactic featur"
N15-1102,D07-1091,0,0.0560918,"imilar translate-and-inflect approach, utilizing unsupervised in addition to supervised morphological analyses. Inflection generation models were also used by (Fraser et al., 2012; Weller et al., 2013) for translation into German, and by (El Kholy and Habash, 2012) for Modern Standard Arabic. (Sultan, 2011) added both syntactic information on the source side that was used in filtering the phrase table, plus postprocessing on the target side for English-Arabic translation. Still other approaches enrich the translation system with morphology-aware feature functions or specific agreement models (Koehn and Hoang, 2007; Green and DeNero, 2012; Williams and Koehn, 2011). In contrast to the above studies, which have concentrated on text translation, this paper focuses on spoken language translation within a bilingual human-human dialog system. Thus, our main goal is not to predict the correct morphological form of every word, but to prevent communication errors resulting from the mishandling of morphology. The intended use in a real-time dialog system imposes additional constraints on morphological modeling: any proposed approach should not add a significant computational burden to the overall system that mig"
N15-1102,E03-1076,0,0.0290989,"SMT system is trained on lemmatized forms, and the correct inflection for every word is predicted in a second pass by statistical classifiers trained on a combination of source and target language features. This paper looks at morphological modeling from a different perspective, namely to improve SMT in a real-time speech2 Prior Work Much work in SMT has addressed the issue of translating from morphologically-rich languages by preprocessing the source and/or target data by e.g., stemming and morphological decomposition (Popovic and Ney, 2004; Goldwater and McClosky, 2005), compound splitting (Koehn and Knight, 2003), or various forms of tokenization (Lee, 2004; Habash and Sadat, 2006). In (Minkov et al., 2007; Toutanova et al., 2008) morphological generation was applied as a postprocessing step for translation into morphologically-rich languages. A maximumentropy Markov model was trained to predict the correct inflection for every stemmed word in the 995 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 995–1000, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics machine translation output from a first-pass syste"
N15-1102,N04-4015,0,0.0499508,"inflection for every word is predicted in a second pass by statistical classifiers trained on a combination of source and target language features. This paper looks at morphological modeling from a different perspective, namely to improve SMT in a real-time speech2 Prior Work Much work in SMT has addressed the issue of translating from morphologically-rich languages by preprocessing the source and/or target data by e.g., stemming and morphological decomposition (Popovic and Ney, 2004; Goldwater and McClosky, 2005), compound splitting (Koehn and Knight, 2003), or various forms of tokenization (Lee, 2004; Habash and Sadat, 2006). In (Minkov et al., 2007; Toutanova et al., 2008) morphological generation was applied as a postprocessing step for translation into morphologically-rich languages. A maximumentropy Markov model was trained to predict the correct inflection for every stemmed word in the 995 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 995–1000, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics machine translation output from a first-pass system, conditioned on a set of lexical, morpholog"
N15-1102,H05-1066,0,0.025524,"Missing"
N15-1102,P07-1017,0,0.0222083,"n a second pass by statistical classifiers trained on a combination of source and target language features. This paper looks at morphological modeling from a different perspective, namely to improve SMT in a real-time speech2 Prior Work Much work in SMT has addressed the issue of translating from morphologically-rich languages by preprocessing the source and/or target data by e.g., stemming and morphological decomposition (Popovic and Ney, 2004; Goldwater and McClosky, 2005), compound splitting (Koehn and Knight, 2003), or various forms of tokenization (Lee, 2004; Habash and Sadat, 2006). In (Minkov et al., 2007; Toutanova et al., 2008) morphological generation was applied as a postprocessing step for translation into morphologically-rich languages. A maximumentropy Markov model was trained to predict the correct inflection for every stemmed word in the 995 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 995–1000, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics machine translation output from a first-pass system, conditioned on a set of lexical, morphological and syntactic features. More recently, (Chahu"
N15-1102,nasr-etal-2014-automatically,0,0.0454064,"Missing"
N15-1102,popovic-ney-2004-towards,0,0.0256329,"problem have utilized a translate-and-inflect method, where a first-pass SMT system is trained on lemmatized forms, and the correct inflection for every word is predicted in a second pass by statistical classifiers trained on a combination of source and target language features. This paper looks at morphological modeling from a different perspective, namely to improve SMT in a real-time speech2 Prior Work Much work in SMT has addressed the issue of translating from morphologically-rich languages by preprocessing the source and/or target data by e.g., stemming and morphological decomposition (Popovic and Ney, 2004; Goldwater and McClosky, 2005), compound splitting (Koehn and Knight, 2003), or various forms of tokenization (Lee, 2004; Habash and Sadat, 2006). In (Minkov et al., 2007; Toutanova et al., 2008) morphological generation was applied as a postprocessing step for translation into morphologically-rich languages. A maximumentropy Markov model was trained to predict the correct inflection for every stemmed word in the 995 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 995–1000, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for"
N15-1102,P08-1059,0,0.0235313,"atistical classifiers trained on a combination of source and target language features. This paper looks at morphological modeling from a different perspective, namely to improve SMT in a real-time speech2 Prior Work Much work in SMT has addressed the issue of translating from morphologically-rich languages by preprocessing the source and/or target data by e.g., stemming and morphological decomposition (Popovic and Ney, 2004; Goldwater and McClosky, 2005), compound splitting (Koehn and Knight, 2003), or various forms of tokenization (Lee, 2004; Habash and Sadat, 2006). In (Minkov et al., 2007; Toutanova et al., 2008) morphological generation was applied as a postprocessing step for translation into morphologically-rich languages. A maximumentropy Markov model was trained to predict the correct inflection for every stemmed word in the 995 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 995–1000, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics machine translation output from a first-pass system, conditioned on a set of lexical, morphological and syntactic features. More recently, (Chahuneau et al., 2013) applie"
N15-1102,P13-1058,0,0.0158673,"ction for every stemmed word in the 995 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 995–1000, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics machine translation output from a first-pass system, conditioned on a set of lexical, morphological and syntactic features. More recently, (Chahuneau et al., 2013) applied a similar translate-and-inflect approach, utilizing unsupervised in addition to supervised morphological analyses. Inflection generation models were also used by (Fraser et al., 2012; Weller et al., 2013) for translation into German, and by (El Kholy and Habash, 2012) for Modern Standard Arabic. (Sultan, 2011) added both syntactic information on the source side that was used in filtering the phrase table, plus postprocessing on the target side for English-Arabic translation. Still other approaches enrich the translation system with morphology-aware feature functions or specific agreement models (Koehn and Hoang, 2007; Green and DeNero, 2012; Williams and Koehn, 2011). In contrast to the above studies, which have concentrated on text translation, this paper focuses on spoken language translatio"
N15-1102,W11-2126,0,0.0133641,"g unsupervised in addition to supervised morphological analyses. Inflection generation models were also used by (Fraser et al., 2012; Weller et al., 2013) for translation into German, and by (El Kholy and Habash, 2012) for Modern Standard Arabic. (Sultan, 2011) added both syntactic information on the source side that was used in filtering the phrase table, plus postprocessing on the target side for English-Arabic translation. Still other approaches enrich the translation system with morphology-aware feature functions or specific agreement models (Koehn and Hoang, 2007; Green and DeNero, 2012; Williams and Koehn, 2011). In contrast to the above studies, which have concentrated on text translation, this paper focuses on spoken language translation within a bilingual human-human dialog system. Thus, our main goal is not to predict the correct morphological form of every word, but to prevent communication errors resulting from the mishandling of morphology. The intended use in a real-time dialog system imposes additional constraints on morphological modeling: any proposed approach should not add a significant computational burden to the overall system that might result in delays in translation or response gene"
N15-1102,P12-1016,0,\N,Missing
P08-2010,J05-1003,0,0.027915,"when selecting the number of boosting iterations based on Dev. Last column indicates absolute improvements. BoostedMERT outperforms MERT by 0.8 points on Eval. 4 Related Work Various methods are used to optimize log-linear models in re-ranking (Shen et al., 2004; Venugopal et al., 2005; Smith and Eisner, 2006). Although this line of work is worthwhile, we believe more gain is possible if we go beyond log-linear models. For example, Shen’s method (2004) produces largemargins but observed little gains in performance. Our BoostedMERT should not be confused with other boosting algorithms such as (Collins and Koo, 2005; Kudo et al., 2005). These algorithms are called boosting because they iteratively choose features (weak learners) and optimize the weights for the boost/exponential loss. They do not, however, maintain a distribution over N-best lists. The idea of maintaining a distribution over Nbest lists is novel. To the best of our knowledge, the most similar algorithm is AdaRank (Xu and Li, 2007), developed for document ranking in information retrieval. Our main difference lies in Lines 4-7 in Algorithm 1: AdaRank proposes a simple closed form solution for α and combines only weak features, not full lea"
P08-2010,P05-1024,0,0.0287612,"er of boosting iterations based on Dev. Last column indicates absolute improvements. BoostedMERT outperforms MERT by 0.8 points on Eval. 4 Related Work Various methods are used to optimize log-linear models in re-ranking (Shen et al., 2004; Venugopal et al., 2005; Smith and Eisner, 2006). Although this line of work is worthwhile, we believe more gain is possible if we go beyond log-linear models. For example, Shen’s method (2004) produces largemargins but observed little gains in performance. Our BoostedMERT should not be confused with other boosting algorithms such as (Collins and Koo, 2005; Kudo et al., 2005). These algorithms are called boosting because they iteratively choose features (weak learners) and optimize the weights for the boost/exponential loss. They do not, however, maintain a distribution over N-best lists. The idea of maintaining a distribution over Nbest lists is novel. To the best of our knowledge, the most similar algorithm is AdaRank (Xu and Li, 2007), developed for document ranking in information retrieval. Our main difference lies in Lines 4-7 in Algorithm 1: AdaRank proposes a simple closed form solution for α and combines only weak features, not full learners (as in MERT)."
P08-2010,N04-1021,0,0.110436,"Missing"
P08-2010,P03-1021,0,0.213836,"Missing"
P08-2010,N04-1023,0,0.175171,"estigate this in future work. Train, Best BLEU Dev, Best BLEU Eval, Best BLEU Eval, Selected BLEU MERT 40.3 24.0 41.2 41.2 BOOST 41.0 25.0 43.7 42.0 ∆ 0.7 1.0 2.5 0.8 5 Table 1: The first three rows show the BLEU score for Train, Dev, and Eval from 30 iterations of BoostedMERT or 30 random re-restarts of MERT. The last row shows the actual BLEU on Eval when selecting the number of boosting iterations based on Dev. Last column indicates absolute improvements. BoostedMERT outperforms MERT by 0.8 points on Eval. 4 Related Work Various methods are used to optimize log-linear models in re-ranking (Shen et al., 2004; Venugopal et al., 2005; Smith and Eisner, 2006). Although this line of work is worthwhile, we believe more gain is possible if we go beyond log-linear models. For example, Shen’s method (2004) produces largemargins but observed little gains in performance. Our BoostedMERT should not be confused with other boosting algorithms such as (Collins and Koo, 2005; Kudo et al., 2005). These algorithms are called boosting because they iteratively choose features (weak learners) and optimize the weights for the boost/exponential loss. They do not, however, maintain a distribution over N-best lists. The"
P08-2010,P06-2101,0,0.0539644,"LEU Dev, Best BLEU Eval, Best BLEU Eval, Selected BLEU MERT 40.3 24.0 41.2 41.2 BOOST 41.0 25.0 43.7 42.0 ∆ 0.7 1.0 2.5 0.8 5 Table 1: The first three rows show the BLEU score for Train, Dev, and Eval from 30 iterations of BoostedMERT or 30 random re-restarts of MERT. The last row shows the actual BLEU on Eval when selecting the number of boosting iterations based on Dev. Last column indicates absolute improvements. BoostedMERT outperforms MERT by 0.8 points on Eval. 4 Related Work Various methods are used to optimize log-linear models in re-ranking (Shen et al., 2004; Venugopal et al., 2005; Smith and Eisner, 2006). Although this line of work is worthwhile, we believe more gain is possible if we go beyond log-linear models. For example, Shen’s method (2004) produces largemargins but observed little gains in performance. Our BoostedMERT should not be confused with other boosting algorithms such as (Collins and Koo, 2005; Kudo et al., 2005). These algorithms are called boosting because they iteratively choose features (weak learners) and optimize the weights for the boost/exponential loss. They do not, however, maintain a distribution over N-best lists. The idea of maintaining a distribution over Nbest li"
P08-2010,W05-0836,0,\N,Missing
W03-0703,H01-1051,0,0.0719664,"Missing"
W03-0703,P00-1013,0,0.0155944,"aving multiple concurrent subdialogs directly affects the dialog planning component. Different user queries and dialog states might need to be tracked simultaneously, and formal models of this type of interaction need to be established. Recently, probabilistic models of human-computer dialogs have become increasingly popular. The most commonly used paradigm is that of Markov decision processes and partially observable Markov decision processes, where the entire dialog is modelled as a sequence of states and associated actions, each of which has a certain value (or reward) (Singh et al., 1999; Roy et al., 2000). The goal is to to choose that sequence of actions which maximizes the overall reward in response to the user’s query. States can be thought of as representing the underlying intentions of the user. These are typically not entirely transparent but only indirectly (or partially) observable through the speech input. Multi-party dialogs might require extensions to this and other modeling frameworks. For instance, it is unclear whether multiple parallel subdialogs can be modelled by a single state sequence (i.e. a single decision process), or whether multiple, partially independent decision proce"
W03-0703,damianos-etal-2000-evaluating,0,\N,Missing
W04-1612,W02-0504,0,0.188104,"Missing"
W05-0708,C04-1080,0,0.0403864,"Missing"
W05-0708,A00-1031,0,0.0638306,"by a state-ofthe-art Modern Standard Arabic tagger applied to Egyptian Arabic. 1 Introduction Part-of-speech (POS) tagging is a core natural language processing task that can benefit a wide range of downstream processing applications. Tagging is often the first step towards parsing or chunking (Osborne, 2000; Koeling, 2000), and knowledge of POS tags can benefit statistical language models for speech recognition or machine translation (Heeman, 1998; Vergyri et al., 2004). Many approaches for POS tagging have been developed in the past, including rule-based tagging (Brill, 1995), HMM taggers (Brants, 2000; Cutting and others, 1992), maximum-entropy models (Rathnaparki, 1996), cyclic dependency networks (Toutanova et al., 2003), memory-based learning (Daelemans et al., 1996), etc. All of these approaches require either a large amount of annotated training data (for supervised tagging) or a lexicon listing all possible tags for each word (for unsupervised tagging). Taggers have been developed for a variety of languages, including Modern Standard Arabic (MSA) (Khoja, 2001; Diab et al., 2004). Since large amount of text material as well as standard lexicons can be obtained in these cases, POS tagg"
W05-0708,W95-0101,0,0.165377,"on hand-annotated data and, by a state-ofthe-art Modern Standard Arabic tagger applied to Egyptian Arabic. 1 Introduction Part-of-speech (POS) tagging is a core natural language processing task that can benefit a wide range of downstream processing applications. Tagging is often the first step towards parsing or chunking (Osborne, 2000; Koeling, 2000), and knowledge of POS tags can benefit statistical language models for speech recognition or machine translation (Heeman, 1998; Vergyri et al., 2004). Many approaches for POS tagging have been developed in the past, including rule-based tagging (Brill, 1995), HMM taggers (Brants, 2000; Cutting and others, 1992), maximum-entropy models (Rathnaparki, 1996), cyclic dependency networks (Toutanova et al., 2003), memory-based learning (Daelemans et al., 1996), etc. All of these approaches require either a large amount of annotated training data (for supervised tagging) or a lexicon listing all possible tags for each word (for unsupervised tagging). Taggers have been developed for a variety of languages, including Modern Standard Arabic (MSA) (Khoja, 2001; Diab et al., 2004). Since large amount of text material as well as standard lexicons can be obtain"
W05-0708,J92-4003,0,0.132836,"Missing"
W05-0708,N03-2003,0,0.0143299,"ty p(w0:M , t0:M ) becomes: M Y pE (wi |ti )(λpE (ti |hi ) + (1 − λ)pL (ti |hi )) (2) i=0 Here λ defines the interpolation weights for the ECA contextual model pE (ti |hi ) and the LCA contextual model pL (ti |hi ). pE (wn |tn ) is the ECA lexi60 cal model. The interpolation weight λ is estimated by maximizing the likelihood of a held-out data set given the combined model. As an extension, we allow the interpolation weights to be a function of the current tag: λ(ti ), since class-dependent interpolation has shown improvements over basic interpolation in applications such as language modeling (Bulyko et al., 2003). 5.2 Joint Training of Contextual Model As an alternative to model interpolation, we consider training a single model jointly from the two different data sets. The underlying assumption of this technique is that tag sequences in LCA and ECA are generated by the same process, whereas the observations (the words) are generated from the tag by two different processes in the two different dialects. The HMM model for joint training is expressed as: M Y (αi pE (wi |ti ) + (1 − αi )pL (wi |ti ))pE+L (ti |hi ) i=0 where αi =  1 if word wi is ECA 0 otherwise (3) A single conditional probability table"
W05-0708,A92-1018,0,0.170834,"Missing"
W05-0708,W96-0102,0,0.0143507,"task that can benefit a wide range of downstream processing applications. Tagging is often the first step towards parsing or chunking (Osborne, 2000; Koeling, 2000), and knowledge of POS tags can benefit statistical language models for speech recognition or machine translation (Heeman, 1998; Vergyri et al., 2004). Many approaches for POS tagging have been developed in the past, including rule-based tagging (Brill, 1995), HMM taggers (Brants, 2000; Cutting and others, 1992), maximum-entropy models (Rathnaparki, 1996), cyclic dependency networks (Toutanova et al., 2003), memory-based learning (Daelemans et al., 1996), etc. All of these approaches require either a large amount of annotated training data (for supervised tagging) or a lexicon listing all possible tags for each word (for unsupervised tagging). Taggers have been developed for a variety of languages, including Modern Standard Arabic (MSA) (Khoja, 2001; Diab et al., 2004). Since large amount of text material as well as standard lexicons can be obtained in these cases, POS tagging is a straightforward task. The dialects of Arabic, by contrast, are spoken rather than written languages. Apart from small amounts of written dialectal material in e.g."
W05-0708,P00-1026,0,0.014269,"Missing"
W05-0708,N04-4038,0,0.120933,"oaches for POS tagging have been developed in the past, including rule-based tagging (Brill, 1995), HMM taggers (Brants, 2000; Cutting and others, 1992), maximum-entropy models (Rathnaparki, 1996), cyclic dependency networks (Toutanova et al., 2003), memory-based learning (Daelemans et al., 1996), etc. All of these approaches require either a large amount of annotated training data (for supervised tagging) or a lexicon listing all possible tags for each word (for unsupervised tagging). Taggers have been developed for a variety of languages, including Modern Standard Arabic (MSA) (Khoja, 2001; Diab et al., 2004). Since large amount of text material as well as standard lexicons can be obtained in these cases, POS tagging is a straightforward task. The dialects of Arabic, by contrast, are spoken rather than written languages. Apart from small amounts of written dialectal material in e.g. plays, novels, chat rooms, etc., data can only be obtained by recording and manually transcribing actual conversations. Moreover, there is no universally agreed upon writing standard for dialects (though several standardization efforts are underway); any largescale data collection and transcription effort therefore req"
W05-0708,W04-3229,0,0.0124355,"alectal Data Sharing Next we examine whether unannotated corpora in other dialects (LCA) can be used to further improve the ECA tagger. The problem of data sparseness for Arabic dialects would be less severe if we were able to exploit the commonalities between similar dialects. In natural language processing, Kim & Khudanpur (2004) have explored techniques for using parallel Chinese/English corpora for language modeling. Parallel corpora have also been used to infer morphological analyzers, POS taggers, and noun phrase bracketers by projections via word alignments (Yarowsky et al., 2001). In (Hana et al., 2004), Czech data is used to develop a morphological analyzer for Russian. In contrast to these works, we do not require parallel/comparable corpora or a bilingual dictionary, which may be difficult to obtain. Our goal is to develop general algorithms for utilizing the commonalities across dialects for developing a tool for a specific dialect. Although dialects can differ very strongly, they are similar in that they exhibit morphological simplifications and a different word order compared to MSA (e.g. SVO rather than VSO order), and close dialects share some vocabulary. Each of the tagger component"
W05-0708,W00-0729,0,0.01355,"Standard Arabic. No dialect-specific tools are used. We present several statistical modeling and cross-dialectal data sharing techniques to enhance the performance of the baseline tagger and compare the results to those obtained by a supervised tagger trained on hand-annotated data and, by a state-ofthe-art Modern Standard Arabic tagger applied to Egyptian Arabic. 1 Introduction Part-of-speech (POS) tagging is a core natural language processing task that can benefit a wide range of downstream processing applications. Tagging is often the first step towards parsing or chunking (Osborne, 2000; Koeling, 2000), and knowledge of POS tags can benefit statistical language models for speech recognition or machine translation (Heeman, 1998; Vergyri et al., 2004). Many approaches for POS tagging have been developed in the past, including rule-based tagging (Brill, 1995), HMM taggers (Brants, 2000; Cutting and others, 1992), maximum-entropy models (Rathnaparki, 1996), cyclic dependency networks (Toutanova et al., 2003), memory-based learning (Daelemans et al., 1996), etc. All of these approaches require either a large amount of annotated training data (for supervised tagging) or a lexicon listing all poss"
W05-0708,W00-0731,0,0.0291547,"yzer for Modern Standard Arabic. No dialect-specific tools are used. We present several statistical modeling and cross-dialectal data sharing techniques to enhance the performance of the baseline tagger and compare the results to those obtained by a supervised tagger trained on hand-annotated data and, by a state-ofthe-art Modern Standard Arabic tagger applied to Egyptian Arabic. 1 Introduction Part-of-speech (POS) tagging is a core natural language processing task that can benefit a wide range of downstream processing applications. Tagging is often the first step towards parsing or chunking (Osborne, 2000; Koeling, 2000), and knowledge of POS tags can benefit statistical language models for speech recognition or machine translation (Heeman, 1998; Vergyri et al., 2004). Many approaches for POS tagging have been developed in the past, including rule-based tagging (Brill, 1995), HMM taggers (Brants, 2000; Cutting and others, 1992), maximum-entropy models (Rathnaparki, 1996), cyclic dependency networks (Toutanova et al., 2003), memory-based learning (Daelemans et al., 1996), etc. All of these approaches require either a large amount of annotated training data (for supervised tagging) or a lexicon"
W05-0708,N03-1033,0,0.0051223,"S) tagging is a core natural language processing task that can benefit a wide range of downstream processing applications. Tagging is often the first step towards parsing or chunking (Osborne, 2000; Koeling, 2000), and knowledge of POS tags can benefit statistical language models for speech recognition or machine translation (Heeman, 1998; Vergyri et al., 2004). Many approaches for POS tagging have been developed in the past, including rule-based tagging (Brill, 1995), HMM taggers (Brants, 2000; Cutting and others, 1992), maximum-entropy models (Rathnaparki, 1996), cyclic dependency networks (Toutanova et al., 2003), memory-based learning (Daelemans et al., 1996), etc. All of these approaches require either a large amount of annotated training data (for supervised tagging) or a lexicon listing all possible tags for each word (for unsupervised tagging). Taggers have been developed for a variety of languages, including Modern Standard Arabic (MSA) (Khoja, 2001; Diab et al., 2004). Since large amount of text material as well as standard lexicons can be obtained in these cases, POS tagging is a straightforward task. The dialects of Arabic, by contrast, are spoken rather than written languages. Apart from sma"
W05-0708,N01-1026,0,0.0147868,"training lexicon. 5 Cross-Dialectal Data Sharing Next we examine whether unannotated corpora in other dialects (LCA) can be used to further improve the ECA tagger. The problem of data sparseness for Arabic dialects would be less severe if we were able to exploit the commonalities between similar dialects. In natural language processing, Kim & Khudanpur (2004) have explored techniques for using parallel Chinese/English corpora for language modeling. Parallel corpora have also been used to infer morphological analyzers, POS taggers, and noun phrase bracketers by projections via word alignments (Yarowsky et al., 2001). In (Hana et al., 2004), Czech data is used to develop a morphological analyzer for Russian. In contrast to these works, we do not require parallel/comparable corpora or a bilingual dictionary, which may be difficult to obtain. Our goal is to develop general algorithms for utilizing the commonalities across dialects for developing a tool for a specific dialect. Although dialects can differ very strongly, they are similar in that they exhibit morphological simplifications and a different word order compared to MSA (e.g. SVO rather than VSO order), and close dialects share some vocabulary. Each"
W05-0821,N03-2002,1,0.7066,"on (ASR), there exists a large body of work on statistical language modeling, addressing e.g. the use of word classes, language model adaptation, or alternative probability estimation techniques. The goal of this study was to use some of the language modeling techniques that have proved beneficial for ASR in the past and to investigate whether they transfer to statistical machine translation. In particular, this includes language models that make use of morphological and part-of-speech information, so-called factored language models. 2 Factored Language Models A factored language model (FLM) (Bilmes and Kirchhoff, 2003) is based on a representation of words as feature vectors and can utilize a variety of additional information sources in addition to words, such as part-of-speech (POS) information, morphological information, or semantic features, in a unified and principled framework. Assuming that each 125 Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 125–128, c Ann Arbor, June 2005. Association for Computational Linguistics, 2005 word w can be decomposed into k features, i.e. w ≡ f 1:K , a trigram model can be defined as p(f11:K , f21:K , ..., fT1:K ) ≈ T Y 1:K 1:K p(ft1:K |ft−"
W05-0821,C04-1022,1,0.856033,". Several different g functions can be chosen, e.g. the mean, weighted mean, product, minimum or maximum of the smoothed probability distributions over all subsets of conditioning factors. In addition to different choices for g, different discounting parameters can be selected at different levels in the backoff graph. One difficulty in training FLMs is the choice of the best combination of conditioning factors, backoff path(s) and smoothing options. Since the space of different combinations is too large to be searched exhaustively, we use a guided search procedure based on Genetic Algorithms (Duh and Kirchhoff, 2004), which optimizes the FLM structure with respect to the desired criterion. In ASR, this is usually the perplexity of the language model on a held-out dataset; here, we use the BLEU scores of the oracle 1-best hypotheses on the development set, as described below. FLMs have previously shown significant improvements in perplexity and word error rate on several ASR tasks (e.g. (Vergyri et al., 2004)). 3 Baseline System We used a fairly simple baseline system trained using standard tools, i.e. GIZA++ (Och and Ney, 2000) for training word alignments and Pharaoh (Koehn, 2004) for phrase-based decodi"
W05-0821,koen-2004-pharaoh,0,0.237068,"Algorithms (Duh and Kirchhoff, 2004), which optimizes the FLM structure with respect to the desired criterion. In ASR, this is usually the perplexity of the language model on a held-out dataset; here, we use the BLEU scores of the oracle 1-best hypotheses on the development set, as described below. FLMs have previously shown significant improvements in perplexity and word error rate on several ASR tasks (e.g. (Vergyri et al., 2004)). 3 Baseline System We used a fairly simple baseline system trained using standard tools, i.e. GIZA++ (Och and Ney, 2000) for training word alignments and Pharaoh (Koehn, 2004) for phrase-based decoding. The training data was that provided on the ACL05 Shared MT task website for 4 different language pairs (translation from Finnish, Spanish, French into English); no additional data was used. Preprocessing consisted of lowercasing the data and filtering out sentences with a length ratio greater than 9. The total number of training sentences and words per language pair ranged between 11.3M words (Finnish-English) and 15.7M words (Spanish-English). The development data consisted of the development sets provided on the website (2000 sentences each). We trained our own wo"
W05-0821,W96-0213,0,\N,Missing
W06-1647,C04-1080,0,0.0521538,"Missing"
W06-1647,W95-0101,0,0.137353,"Missing"
W06-1647,N04-4038,0,0.0548523,"Missing"
W06-1647,W05-0708,1,0.940065,"und will achieve a lower expected test risk. Therefore, one can use the bound as a principled way of choosing the parameters in the Transductive Clustering algorithm: First, a large number of different clusterings is created; then the one that achieves the lowest PACBayesian bound is chosen. The pseudo-code is given in Figure 2. (El-Yaniv and Gerzon, 2005) has applied the Transductive Clustering algorithm successfully to binary classification problems and demonstrated improvements over the current state-of-the-art Spectral Graph Transducers (Section 3.4). We use the algorithm as described in (Duh and Kirchhoff, 2005b), which adapts the algorithm to structured output problems. In particular, the modification involves a different estimate of the priors p(h), which was assumed to be uniform in (El-Yaniv and Gerzon, 2005). Since there are many possible h, adopting a uniform prior will lead to small values of p(h) and thus a loose bound for all h. Probability mass should only be spent on POS-sets that are possible, and as such, we calculate p(h) based on frequencies of compound-labels in the training data (i.e. an empirical prior). Transductive Clustering How does a transductive algorithm effectively utilize"
W06-1647,P05-1071,0,0.0547921,"Missing"
W06-1647,E06-1047,0,\N,Missing
W08-0314,atserias-etal-2006-freeling,0,0.030555,"Missing"
W08-0314,P08-2010,1,0.78181,"used to train the language model. The second language model used for rescoring was a 5-gram model over part-of-speech (POS) tags. This model was built using the Spanish side of the English-Spanish parallel training corpus. The POS tags were obtained from the corpus using Freeling v2.0 (Atserias et al, 2006). We selected the language models for our translation system were selected based on performance on the English-to-Spanish task, and reused them for the German-to-Spanish task. 4 Boosted Reranking We submitted an alternative system, based on a different re-ranking method, called BoostedMERT (Duh and Kirchhoff, 2008), for each task. BoostedMERT is a novel boosting algorithm that uses Minimum Error Rate Training (MERT) as a weak learner to build a re-ranker that is richer than the standard log-linear models. This is motivated by the observation that log-linear models, as trained by MERT, often do not attain the oracle BLEU scores of the Nbest lists in the development set. While this may be due to a local optimum in MERT, we hypothesize that log-linear models based on our K re-ranking features are also not sufficiently expressive. BoostedMERT is inspired by the idea of Boosting (for classification), which h"
W08-0314,N03-1017,0,0.00598649,"task. For German-to-Spanish translation we additionally investigated simplifications of German morphology, which is known to be fairly complex due to a large number of compounds and inflections. In the following sections we first describe the data, baseline system and postprocessing steps before describing boosted N-best list reranking and morphology-based preprocessing for German. 3 3.1 System Overview Translation model The system developed for this year’s shared task is a state-of-the-art, two-pass phrase-based statistical machine translation system based on a log-linear translation model (Koehn et al, 2003). The translation models and training method follow the standard Moses (Koehn et al, 2007) setup distributed as part of the shared task. We used the training method suggested in the Moses documentation, with lexicalized reordering (the msd-bidirectional-fe option) enabled. The system was tuned via Minimum Error Rate Training (MERT) on the first 500 sentences of the devtest2006 dataset. 123 Proceedings of the Third Workshop on Statistical Machine Translation, pages 123–126, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics 3.2 Decoding Our system used the Moses de"
W08-0314,2005.mtsummit-papers.11,0,0.034177,"Missing"
W08-0314,E06-1006,1,0.854058,"ed by a high number of noun compounds and rich inflectional paradigms. Simplification of morphology can produce better word alignment, and thus better phrasal translations, and can also significantly reduce the out-of-vocabulary rate. We therefore applied two operations: (a) splitting of compound words and (b) stemming. After basic preprocessing, the German half of the training corpus was first tagged by the German version of TreeTagger (Schmid, 1994), to identify partof-speech tags. All nouns were then collected into a noun list, which was used by a simple compound splitter, as described in (Yang and Kirchhoff, 2006). This splitter scans the compound word, hypothesizing segmentations, and selects the first segmentation that produces two nouns that occur individually in the corpus. After splitting the compound nouns in the filtered corpus, we used the TreeTagger again, only this time to lemmatize the (filtered) training corpus. The stemmed version of the German text was used to train the translation system’s word alignments (through the end of step 3 in the Moses training script). After training the alignments, they were projected back onto the unstemmed corpus. The parallel 125 phrases were then extracted"
W08-0314,W96-0213,0,\N,Missing
W08-0314,N04-1023,0,\N,Missing
W08-0314,E06-1005,0,\N,Missing
W08-0314,N03-2002,1,\N,Missing
W08-0314,W05-0821,1,\N,Missing
W08-0314,P05-1071,0,\N,Missing
W08-0314,D08-1076,0,\N,Missing
W08-0314,C04-1022,1,\N,Missing
W08-0314,N06-4004,0,\N,Missing
W08-0314,J92-4003,0,\N,Missing
W08-0314,P07-2045,0,\N,Missing
W08-0314,W08-0336,0,\N,Missing
W08-0314,J03-1002,0,\N,Missing
W08-0314,N04-1021,0,\N,Missing
W10-4357,W97-1401,0,0.107306,"Missing"
W10-4357,P07-2024,0,0.0283948,"Missing"
W10-4357,W09-3944,0,0.029513,"Missing"
W10-4357,N06-2010,0,0.0292664,"ential type distinguishes singular and plural usages; (2) a three way classification between generic, singular, or plural types. Our empirical results have shown that incorporation of gesture improves performance on differentiating between the generic and the referential type. Incorporation of gesture can also compensate for limitations in automated language processing (e.g., reliable recognition of dialogue acts) and achieve comparable results. These findings have important implications for machine translation of you expressions from multiparty meetings. 2 Gestures in Coreference Resolution. Eisenstein and Davis (2006; 2007) examined coreference resolution on a corpus of speaker-listener pairs in which the speaker had to describe the workings of a mechanical device to the listener, with the help of visual aids. In this gesture heavy dataset, they found gesture data to be helpful in resolving references. In our previous work (2009), we examined gestures for the identification of coreference on multparty meeting data. We found that gestures only provided limited help in the coreference identification task. Given the nature of the meetings under investigation, although gestures have not been shown to be effec"
W10-4357,P07-1045,0,0.0635699,"Missing"
W10-4357,E09-1032,0,0.0324627,"Missing"
W10-4357,2007.sigdial-1.40,0,0.261074,"ng schon gefallen ist. • Singular you: EN: Do you want an extra piece of paper? DE: M¨ochtest du noch ein Blatt Papier? • Plural you: EN: Hope you are all happy! DE: Ich hoffe, ihr seid alle zufrieden! These examples show that correctly identifying different types of You plays an important role in the correct translation of you in different context. To address this issue, this paper investigates the role of hand gestures in disambiguating different usages of you in multiparty meetings. Although identification of you type has been investigated before in the context of addressee identification (Gupta et al., 2007b; Gupta et al., 2007a; Frampton et al., 2009; Purver et al., 2009), our work here focuses on two new angles. First, because of our different application on machine translation, rather than processing you at an utterance level to identify addressee, our work here concerns each occurrence of you within each utterance. Second and more importantly, our work investigates the role of corresponding hand gestures in the disambiguation process. This aspect has not been examined in previous work. When several speakers are conversing in a situated environment, they often overtly gesture at one another t"
W10-4357,P07-2027,0,0.399617,"ng schon gefallen ist. • Singular you: EN: Do you want an extra piece of paper? DE: M¨ochtest du noch ein Blatt Papier? • Plural you: EN: Hope you are all happy! DE: Ich hoffe, ihr seid alle zufrieden! These examples show that correctly identifying different types of You plays an important role in the correct translation of you in different context. To address this issue, this paper investigates the role of hand gestures in disambiguating different usages of you in multiparty meetings. Although identification of you type has been investigated before in the context of addressee identification (Gupta et al., 2007b; Gupta et al., 2007a; Frampton et al., 2009; Purver et al., 2009), our work here focuses on two new angles. First, because of our different application on machine translation, rather than processing you at an utterance level to identify addressee, our work here concerns each occurrence of you within each utterance. Second and more importantly, our work investigates the role of corresponding hand gestures in the disambiguation process. This aspect has not been examined in previous work. When several speakers are conversing in a situated environment, they often overtly gesture at one another t"
W16-6107,E06-2021,0,0.0929723,"Missing"
W16-6107,N06-1020,0,0.0125279,"model trained using modified KneserNey smoothing. The n-gram order was varied between 3 and 5 and optimized on the development set. 55 Self-training Self-training is a general way of utilizing unsupervised data in a classification system. Starting with a system trained on limited data, the system is applied to unlabeled data. The system’s predictions are then filtered according to the probability or confidence of the prediction, and the most likely or confident hypotheses are added back to the training data. This procedure can be iterated. Self-training has been used in NLP for e.g., parsing (McClosky et al., 2006) and machine translation (Ueffing et al., 2007). In the context of AA resolution, (Pakhomov, 2002) has used a similar approach to enrich the training data for a maximum entropy classifier. Here, we use the top-1 hypotheses of our firstpass SMT system to generate additional training data for both the first and second pass language models. To this end we apply the SMT system to the i2b2 and Cases data. Additionally we utilize up to 2000 nursing notes from the MIMIC-II corpus that do not overlap with our development or evaluation sets. One-best hypotheses are generated from our initial SMT system"
W16-6107,W12-2406,0,0.0595138,"Missing"
W16-6107,okazaki-ananiadou-2006-clustering,0,0.0324852,"een compiled by rule-based (Ao and Takagi, 2005) or machine learning techniques (Movshovitz-Attias and Cohen, 2012; Henriksson et al., 2014; Okazaki et al., 2010), often aided by the fact that biomedical texts tend to define AAs at their first mention. Disambiguation of biomedical AAs has been achieved using traditional machine learning approaches, such as vector space methods (Stevenson et al., 2009), naive Bayes classifiers (Bracewell et al., 2005; Stevenson et al., 2009), and SVMs (Joshi et al., 2006; Stevenson et al., 2009). Clustering has also been used for the purpose of disambiguation (Okazaki and Ananiadou, 2006). Studies on AAs in clinical text are rarer than those for biomedical texts. In (Pakhomov et al., 2005), disambiguation of clinical AAs was achieved using decision trees and maximum entropy models trained on bag-of-word features from hand-annotated and web-collected text. Moon et al. (2012; 2015) similarly investigated several supervised machine learning techniques and text features for disambiguation of AAs in clinical text, including naive Bayes classifiers, SVMs and decision trees trained on bag-ofword features or Unified Medical Language System (UMLS) concepts. They also noted general prob"
W16-6107,P02-1021,0,0.0415502,"ed on the development set. 55 Self-training Self-training is a general way of utilizing unsupervised data in a classification system. Starting with a system trained on limited data, the system is applied to unlabeled data. The system’s predictions are then filtered according to the probability or confidence of the prediction, and the most likely or confident hypotheses are added back to the training data. This procedure can be iterated. Self-training has been used in NLP for e.g., parsing (McClosky et al., 2006) and machine translation (Ueffing et al., 2007). In the context of AA resolution, (Pakhomov, 2002) has used a similar approach to enrich the training data for a maximum entropy classifier. Here, we use the top-1 hypotheses of our firstpass SMT system to generate additional training data for both the first and second pass language models. To this end we apply the SMT system to the i2b2 and Cases data. Additionally we utilize up to 2000 nursing notes from the MIMIC-II corpus that do not overlap with our development or evaluation sets. One-best hypotheses are generated from our initial SMT system, and are combined with the target side of the term mapping list. This set is then used to retrain"
W16-6107,W09-1309,0,0.536838,"Missing"
W16-6107,P07-1004,0,0.0307823,". The n-gram order was varied between 3 and 5 and optimized on the development set. 55 Self-training Self-training is a general way of utilizing unsupervised data in a classification system. Starting with a system trained on limited data, the system is applied to unlabeled data. The system’s predictions are then filtered according to the probability or confidence of the prediction, and the most likely or confident hypotheses are added back to the training data. This procedure can be iterated. Self-training has been used in NLP for e.g., parsing (McClosky et al., 2006) and machine translation (Ueffing et al., 2007). In the context of AA resolution, (Pakhomov, 2002) has used a similar approach to enrich the training data for a maximum entropy classifier. Here, we use the top-1 hypotheses of our firstpass SMT system to generate additional training data for both the first and second pass language models. To this end we apply the SMT system to the i2b2 and Cases data. Additionally we utilize up to 2000 nursing notes from the MIMIC-II corpus that do not overlap with our development or evaluation sets. One-best hypotheses are generated from our initial SMT system, and are combined with the target side of the"
W16-6107,W15-3822,0,0.209587,"m hand-annotated and web-collected text. Moon et al. (2012; 2015) similarly investigated several supervised machine learning techniques and text features for disambiguation of AAs in clinical text, including naive Bayes classifiers, SVMs and decision trees trained on bag-ofword features or Unified Medical Language System (UMLS) concepts. They also noted general problems with AA disambiguation in clinical text, such as shortage of training data due to patient privacy constraints, lack of resources developed for clinical text, and non-standard and highly variable language use in clinical notes. Wu et al. (2015) extended SVM 53 resp care note : pt on nrb mask + 6l nc required nt sx due inability to clear secretions. sx copious th yellow sput. pt sats didn’t recover after sx + a&a tx. Figure 1: Sample nursing note. classification with vectors based on neural word embeddings. Several systems that participated in the ShaRe/CLEF eHealth Challenge Task on AA normalization (Mowery et al., 2016) utilized conditional random fields (e.g.,(Wu et al., 2013)). Customized expansion dictionaries for clinical text were added in (Xia et al., 2013). Finally, AA identification and expansion for general English has bee"
W18-1806,P16-1160,0,0.0442824,"ed from the parallel training data will not have sufﬁcient information to translate OOVs in the test data, as most of these will never have occurred in the training data. External dictionaries could be used in this case, which however requires a principled method of choosing between different translations. Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 55 An alternative strategy to address the rare and unknown word problem is to use subword units, i.e., the original text is segmented into chunks of characters, individual characters, or byte sequences. In Chung et al. (2016), a pure character-level decoder is used while Luong and Manning (2016) use a mixed model where the word-level decoder can fall back on the characterlevel decoder. The byte-pair encoding (BPE) approach of Sennrich et al. (2015) segments the input text into subword units based on an iterative merging of frequent character n-grams and a ﬁxed upper size of the subword inventory. The main motivation given for the subword unit approach is that often a transparent relationship exists between OOVs and other known words: compound words and morphological variants can beneﬁt from substantial overlap wit"
W18-1806,D13-1143,0,0.0316109,"DWWLFH LW LV RQH RI WKH ([SDQGHGODWWLFH LW LV RQH RI WKH XQLW DFFHVV FRXQWULHV FRRUGLQDWH SDUWQHUV RSHUDWLQJ  XQLW FRXQWULHV SDU WQH UV DFFHVV FRRUGLQDWH LQ WKH DUHD WKH DUHD LQ LQ LQ RSHUDWLQJ Figure 1: Compressed and expanded lattice representations of an MT hypothesis enriched with candidate translations of OOV words. 3.1 Count-Based Models We compare several different models for rescoring, the simplest of which is a sentence completion model. OOV word translation can be formulated as sentence completion problem, where contextual information informs the ﬁlling of blanks in a sentence. Gubbins and Vlachos (2013) proposed to use a language model over a dependency tree for this task, whereas Woods (2016) and R¨oder et al. (2015) measure the degree of association between candidate words and other parts of the sentence using mutual information. In the same spirit our model chooses one out of several possible translation options for each OOV slot in the lattice based its average pointwise mutual information (PMI) with surrounding content words in the sentence (stopwords are ignored). PMI between words x and y is computed as: P M I(x, y) = log P (x, y) P (x)P (y) (1) The algorithm chooses one word at a tim"
W18-1806,2016.amta-researchers.13,0,0.0174681,"t al. (2014) and Zhao et al. (2015) explored the possibility of extracting features from extra monolingual corpora to help cover untranslated phrases. Speciﬁcally, Saluja et al. (2014) induced new translation rules from monolingual data with a semi-supervised algorithm. Zhao et al. (2015) obtained translation rules for OOV words based on phrases with similar continuous representations for which a translation is known. Most of the studies described above have focused on neural MT for language pairs with sufﬁcient training data. Recent work on OOV translation for low-resource languages includes Gujral et al. (2016), where a combination of approaches (surface and word-embedding based word similarity, transliteration) is used to generate multiple translation candidates for each OOV to improve phrase-based MT. The choice of a particular translation is then made either by a target-side language model or by the translation model itself through a secondary phrase table enriched with OOV-speciﬁc features. 3 OOV Disambiguation With Context Models Our goal is to facilitate the integration of externally generated translation candidates, such as translation dictionaries, by utilizing a larger amount of target-side"
W18-1806,P16-1014,0,0.0393131,"map all OOVs (as well as rare words) to a single unknown word token. Luong et al. (2014) trained an NMT system using external word alignment information, which allowed the system to output positional information about OOVs, which were then translated using a dictionary trained from parallel data. Working within the context of neural sequence-to-sequence models with attention, Bahdanau et al. (2014) and Jean et al. (2014) pursued the same strategy, with the exception that alignment information was derived from the attention layer in the neural model rather than an external knowledge source. In Gulcehre et al. (2016) a pointer model was used that can perform both copying and dictionary lookup. None of these studies address the problem of translation ambiguities resulting from added external knowledge sources. In truly low-resource languages, a dictionary obtained from the parallel training data will not have sufﬁcient information to translate OOVs in the test data, as most of these will never have occurred in the training data. External dictionaries could be used in this case, which however requires a principled method of choosing between different translations. Proceedings of AMTA 2018, vol. 1: MT Resear"
W18-1806,N13-1056,0,0.0321567,"novel translations must be produced for words that were never seen and that are unrelated to other words. A third class of approaches tries to leverage cognates and lexical borrowing. Tsvetkov and Dyer (2015) show that OOV words in low-resource languages that are loan words from a highresource language can be translated via the high-resource language. However, the translation of OOV words in that work uses a ﬁxed lexicon, not taking possibly multiple candidates into consideration. Finally, other studies have tried to exploit additional monolingual data in the source and/or target language. In Irvine and Callison-Burch (2013) new translation pairs were induced from monolingual corpora based on predictive features such as document timestamps, topic features, word frequency, and orthographic features. Saluja et al. (2014) and Zhao et al. (2015) explored the possibility of extracting features from extra monolingual corpora to help cover untranslated phrases. Speciﬁcally, Saluja et al. (2014) induced new translation rules from monolingual data with a semi-supervised algorithm. Zhao et al. (2015) obtained translation rules for OOV words based on phrases with similar continuous representations for which a translation is"
W18-1806,W16-6107,1,0.812554,"ctor c is a position-dependent weighted linear combination of all hidden states 1, ..., M in the previous sentence. cs−1,n = M  αm,n hs−1,m (7) m=1 The attention weights are computed as an,m = waT tanh(Wa1 hs,n + Wa2 hs−1,m ) αn = sof tmax(an ) (8) (9) The attention weights an,m encode the importance of each word in the previous sentence for the current word. DCLMs were shown to obtain reductions in perplexity compared to standard and hierarchical recurrent language models (Ji et al. (2015)); however, they were also observed to have a tendency towards overﬁtting when training data is sparse (Kirchhoff and Turner (2016)). A different issue in applying neural language models to lattice rescoring is that each path in the lattice deﬁnes a different context; however, it is computationally infeasible to exhaustively rescore all paths. The number of OOV words per sentence is typically 3-5 in our tasks, and the number of translation candidates per word may go up to 30. In standard back-off n-gram Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 58 models, lattice paths are merged based on identical truncated word histories, but this options is not available to us in neural langu"
W18-1806,P07-2045,0,0.0146733,"/ 4,263 28,686 / 439 / 423,069 1,802 / 24 / 25,730 196 / 7/ 4,227 Table 1: Number of sentence pairs/documents/target language words in the training, development and test sets for each language. training data. The same data was used for training the sentence-level LSTMs, to be able to directly compare the effect of document-level vs. sentence-level context on OOV disambiguation. 4.3 Baseline MT Systems Baseline MT systems were developed for these tasks using phrase-based MT and attentionbased neural MT (the Transformer model of Vaswani et al. (2017))3 . The PBMT system was trained using Moses (Koehn et al. (2007)) and uses a ﬂat phrase-based model with a maximum phrase length of 7, a backoff 4-gram language model trained on the target side of the parallel training data, and a bidirectional reordering model. The log-linear weights were trained using minimum error rate training on the dev set. The Transformer model was trained using a shared byte-pair encoding, resulting in a subword vocabulary of 8,000 word pieces. The hyperparameters of the Transformer models were tuned on the development sets with respect to the number of layers, layer dimensionality, learning rate, and regularization (dropout). The"
W18-1806,W17-3204,0,0.0308772,"nsformer models were tuned on the development sets with respect to the number of layers, layer dimensionality, learning rate, and regularization (dropout). The best parameters turned out to be: dropout rate of 0.1 at all layers, a learning rate of 0.2, 2 layers in the encoder and 2 layers in the decoder, and a hidden layer dimensionality of 512. The beam size during decoding is 4. Baseline results are shown in Table 2. Scoring was done in a case-insensitive fashion against a single reference translation. Previous studies of neural sequence-to-sequence models for resource-poor scenarios (e.g., Koehn and Knowles (2017)) have found that PBMT models performed signiﬁcantly better on low-resource languages unless the NMT models were enriched with additional components, such as a lexical memory (Nguyen and Chiang (2017)). By contrast, we ﬁnd that self-attention based neural MT model performs comparably to PBMT, without any modiﬁcations to the basic model. A major contributor to the performance of the NMT models is the segmentation induced by byte-pair encoding, which results in system that outperform PBMT systems in 4 out of 6 language pairs. With a word-based vocabulary, NMT underperforms PBMT in most cases. No"
W18-1806,P16-1100,0,0.0610602,"tion to translate OOVs in the test data, as most of these will never have occurred in the training data. External dictionaries could be used in this case, which however requires a principled method of choosing between different translations. Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 55 An alternative strategy to address the rare and unknown word problem is to use subword units, i.e., the original text is segmented into chunks of characters, individual characters, or byte sequences. In Chung et al. (2016), a pure character-level decoder is used while Luong and Manning (2016) use a mixed model where the word-level decoder can fall back on the characterlevel decoder. The byte-pair encoding (BPE) approach of Sennrich et al. (2015) segments the input text into subword units based on an iterative merging of frequent character n-grams and a ﬁxed upper size of the subword inventory. The main motivation given for the subword unit approach is that often a transparent relationship exists between OOVs and other known words: compound words and morphological variants can beneﬁt from substantial overlap with other words in the same language, and cognates and named entities ben"
W18-1806,H05-1052,0,0.0284037,"ds in the sentence (stopwords are ignored). PMI between words x and y is computed as: P M I(x, y) = log P (x, y) P (x)P (y) (1) The algorithm chooses one word at a time, proceeding from left to right. The chosen translation becomes part of the context used for computing PMI for the next set of OOV words. Therefore, the entire space of possible combinations of OOV translations is never fully explored but only searched greedily from left to right. Moreover, this method only focuses on content words and ignores word order in the PMI computation. The second model is a graph-based reranking model (Mihalcea (2005); Yang and Kirchhoff (2012)), where an undirected graph is built over all OOV translation options and content words in a sentence. Graph edges are weighted with the same PMI values as in the sentence completion approach. PageRank (Page et al. (1999)) is then used to score each option based on ’votes’ from connected words, and for each OOV slot, the options with the highest score is chosen. The PageRank score is computed as R(vi ) = (1 − d) + d ·  j∈IN (vi ) R(vj ) |OU T (vj )| (2) where v is a vertex in the graph, d is a damping factor, and IN () and OU T () are in-degree and Proceedings of A"
W18-1806,P14-1064,0,0.0208354,"how that OOV words in low-resource languages that are loan words from a highresource language can be translated via the high-resource language. However, the translation of OOV words in that work uses a ﬁxed lexicon, not taking possibly multiple candidates into consideration. Finally, other studies have tried to exploit additional monolingual data in the source and/or target language. In Irvine and Callison-Burch (2013) new translation pairs were induced from monolingual corpora based on predictive features such as document timestamps, topic features, word frequency, and orthographic features. Saluja et al. (2014) and Zhao et al. (2015) explored the possibility of extracting features from extra monolingual corpora to help cover untranslated phrases. Speciﬁcally, Saluja et al. (2014) induced new translation rules from monolingual data with a semi-supervised algorithm. Zhao et al. (2015) obtained translation rules for OOV words based on phrases with similar continuous representations for which a translation is known. Most of the studies described above have focused on neural MT for language pairs with sufﬁcient training data. Recent work on OOV translation for low-resource languages includes Gujral et al"
W18-1806,P15-2021,0,0.0222373,"rd unit approach is that often a transparent relationship exists between OOVs and other known words: compound words and morphological variants can beneﬁt from substantial overlap with other words in the same language, and cognates and named entities beneﬁt from cross-lingual overlap. However, in resource-poor settings a substantial percentage of OOVs has no overt relationship with other words; instead, genuinely novel translations must be produced for words that were never seen and that are unrelated to other words. A third class of approaches tries to leverage cognates and lexical borrowing. Tsvetkov and Dyer (2015) show that OOV words in low-resource languages that are loan words from a highresource language can be translated via the high-resource language. However, the translation of OOV words in that work uses a ﬁxed lexicon, not taking possibly multiple candidates into consideration. Finally, other studies have tried to exploit additional monolingual data in the source and/or target language. In Irvine and Callison-Burch (2013) new translation pairs were induced from monolingual corpora based on predictive features such as document timestamps, topic features, word frequency, and orthographic features"
W18-1806,P16-2071,0,0.0140548,"UDWLQJ  XQLW FRXQWULHV SDU WQH UV DFFHVV FRRUGLQDWH LQ WKH DUHD WKH DUHD LQ LQ LQ RSHUDWLQJ Figure 1: Compressed and expanded lattice representations of an MT hypothesis enriched with candidate translations of OOV words. 3.1 Count-Based Models We compare several different models for rescoring, the simplest of which is a sentence completion model. OOV word translation can be formulated as sentence completion problem, where contextual information informs the ﬁlling of blanks in a sentence. Gubbins and Vlachos (2013) proposed to use a language model over a dependency tree for this task, whereas Woods (2016) and R¨oder et al. (2015) measure the degree of association between candidate words and other parts of the sentence using mutual information. In the same spirit our model chooses one out of several possible translation options for each OOV slot in the lattice based its average pointwise mutual information (PMI) with surrounding content words in the sentence (stopwords are ignored). PMI between words x and y is computed as: P M I(x, y) = log P (x, y) P (x)P (y) (1) The algorithm chooses one word at a time, proceeding from left to right. The chosen translation becomes part of the context used fo"
W18-1806,2012.amta-papers.29,1,0.742374,"e (stopwords are ignored). PMI between words x and y is computed as: P M I(x, y) = log P (x, y) P (x)P (y) (1) The algorithm chooses one word at a time, proceeding from left to right. The chosen translation becomes part of the context used for computing PMI for the next set of OOV words. Therefore, the entire space of possible combinations of OOV translations is never fully explored but only searched greedily from left to right. Moreover, this method only focuses on content words and ignores word order in the PMI computation. The second model is a graph-based reranking model (Mihalcea (2005); Yang and Kirchhoff (2012)), where an undirected graph is built over all OOV translation options and content words in a sentence. Graph edges are weighted with the same PMI values as in the sentence completion approach. PageRank (Page et al. (1999)) is then used to score each option based on ’votes’ from connected words, and for each OOV slot, the options with the highest score is chosen. The PageRank score is computed as R(vi ) = (1 − d) + d ·  j∈IN (vi ) R(vj ) |OU T (vj )| (2) where v is a vertex in the graph, d is a damping factor, and IN () and OU T () are in-degree and Proceedings of AMTA 2018, vol. 1: MT Resear"
W18-1806,N15-1176,0,0.0224486,"-resource languages that are loan words from a highresource language can be translated via the high-resource language. However, the translation of OOV words in that work uses a ﬁxed lexicon, not taking possibly multiple candidates into consideration. Finally, other studies have tried to exploit additional monolingual data in the source and/or target language. In Irvine and Callison-Burch (2013) new translation pairs were induced from monolingual corpora based on predictive features such as document timestamps, topic features, word frequency, and orthographic features. Saluja et al. (2014) and Zhao et al. (2015) explored the possibility of extracting features from extra monolingual corpora to help cover untranslated phrases. Speciﬁcally, Saluja et al. (2014) induced new translation rules from monolingual data with a semi-supervised algorithm. Zhao et al. (2015) obtained translation rules for OOV words based on phrases with similar continuous representations for which a translation is known. Most of the studies described above have focused on neural MT for language pairs with sufﬁcient training data. Recent work on OOV translation for low-resource languages includes Gujral et al. (2016), where a combi"
W18-1806,P15-1001,0,\N,Missing
W18-1806,P16-1162,0,\N,Missing
W19-5906,N19-1423,0,0.108664,"ence. In this work, we propose fully non-recurrent and label-recurrent model paradigms including self-attention and convolution for comparison to state-of-the-art recurrent models in terms of accuracy and speed. To achieve this, we design a framework for joint IC-SL models that is modularized into different components and makes the task agnostic to type of neural network used. This, in turn, makes the model architecture simpler, easy to understand and renders the task network agnostic, allowing for easier plug and play using existing components, such as pre-trained contextual word embeddings (Devlin et al., 2019), etc. This is essential for easier model debugging and quicker experimentation, especially in industrial settings. Using this framework, we identify three distinct model families of interest: fully recurrent, Introduction At the core of task-oriented dialogue systems are spoken language understanding (SLU) models, tasked with determining the intent of users’ utterances and labeling semantically relevant words at each turn of the conversation. Performance on these tasks, known as intent classification (IC) and slot labeling (SL), upper-bounds the utility of such dialogue systems. A large body"
W19-5906,N18-2118,0,0.116769,"Missing"
W19-5906,H90-1021,0,0.78356,"Missing"
W19-5906,D14-1179,0,0.0169062,"Missing"
W19-5906,P18-1027,0,0.031535,"new; though the latter is recurrent both in word contextualization and slot label prediction, it is distinct from past models in that the two recurrent components are completely decoupled until the prediction step. weight given to position j when contextualizing position i. The function f determines which relative positions to group together with a single relative position vector. Given the generally small datasets in IC+SL, we use the following relative position function, which buckets relative positions together in exponentially larger groups as distance increases, following the results of Khandelwal et al. (2018), that LSTMs represent position fuzzily at long relative distances. f (i, j) =  ±1 , |j − i |= 1   ±2 , |j − i |∈ {2, 3}  ±3 , |j − i |∈ {4..7} ... 5 We evaluate our framework and models on the ATIS data set (Hemphill et al., 1990) of spoken airline reservation requests and the Snips NLU Benchmark set (Coucke et al., 2018). The ATIS training set contains 4978 utterances from the ATIS-2 and ATIS-3 corpora; the test set consists of 893 utterances from the ATIS-3 NOV93 and DEC94 data sets. The number of slot labels is 127, and the number of intent classes is 18. Only the words themselves are"
W19-5906,P18-1249,0,0.0307386,"ttention and LSTM layers along with the gating mechanism for this task. Non-recurrent modeling for language has been re-visited recently, even as recurrent techniques continue to be dominant. Dilated CNNs (Yu and Koltun, 2015) with CRF label modeling were applied to named entity recognition by Strubell et al. (2017), and earlier were applied to SL by Xu and Sarikaya (2013). Convolutional and attentionbased sentence encoders have been applied in complex tasks, including machine translation, natural language inference, and parsing. (Gehring et al., 2017; Vaswani et al., 2017; Shen et al., 2017; Kitaev and Klein, 2018) We draw from both of these bodies of work to propose a simple yet highly effective family of IC+SL models. label-recurrent, and non-recurrent. Recent stateof-the-art models fall into the first category, as encoder-decoder architectures have recurrent encoders to perform word context encoding, and predict slot label sequences using recurrent decoders that use both word and label information as they decode (Hakkani-T¨ur et al., 2016; Liu and Lane, 2016; Li et al., 2018). In second category, we have ‘non-recurrent’ networks: fully feed-forward, attention-based, or convolutional models, for examp"
W19-5906,D18-1417,0,0.219806,"Amazon AI 46 Proceedings of the SIGDial 2019 Conference, pages 46–55 c Stockholm, Sweden, 11-13 September 2019. 2019 Association for Computational Linguistics Schmidhuber, 1997) and Gated Recurrent Unit models (Cho et al.) were proposed for slot labeling by Yao et al. (2014) and Zhang and Wang (2016) respectively, while Guo et al. (2014) used recursive neural networks. Subsequent improvements to recurrent neural modeling techniques, like bidirectionality and attention (Bahdanau et al., 2014) were incorporated into IC+SL in recent years as well (Hakkani-T¨ur et al., 2016; Liu and Lane, 2016). Li et al. (2018) introduced a self-attention based joint model where they used self-attention and LSTM layers along with the gating mechanism for this task. Non-recurrent modeling for language has been re-visited recently, even as recurrent techniques continue to be dominant. Dilated CNNs (Yu and Koltun, 2015) with CRF label modeling were applied to named entity recognition by Strubell et al. (2017), and earlier were applied to SL by Xu and Sarikaya (2013). Convolutional and attentionbased sentence encoders have been applied in complex tasks, including machine translation, natural language inference, and pars"
W19-5906,P19-1519,0,0.0716867,"Missing"
W19-5906,N18-2074,0,0.137821,"ag pi,SL = tanh(W (5) [hi ; hi ] + b(5) ) αj = PT T (5) x 0 ) j j 0 =1 exp(xi W (10) Word contextualization models In this section, we describe word contextualization models with the goal of identifying non-recurrent architectures that achieve high accuracy and faster speed than recurrent models. 4.1 4.2.1 Relative position representations We found in early experiments that the absolute position embeddings in self-attention models are insufficient for representing order. Hence, in all attention models except when explicitly noted, we use relative position representations as follows. We follow Shaw et al. (2018), who improved the absolute position representations of the Transformer model (Vaswani et al., 2017) by learning vector representations of relative positions and incorporating them into the self-attention mechanism as follows: Feed-forward model In this model, we set h1:T = x1:T + a1:T , where a1:T is a learned absolute position representation, with one vector learned per absolute position, as used in (Gehring et al., 2017). While extremely simple, this model provides a useful baseline as a totally context-free model. It also permits us to analyze the contribution of a label-recurrent componen"
W19-5906,D17-1283,0,0.0305038,"ent improvements to recurrent neural modeling techniques, like bidirectionality and attention (Bahdanau et al., 2014) were incorporated into IC+SL in recent years as well (Hakkani-T¨ur et al., 2016; Liu and Lane, 2016). Li et al. (2018) introduced a self-attention based joint model where they used self-attention and LSTM layers along with the gating mechanism for this task. Non-recurrent modeling for language has been re-visited recently, even as recurrent techniques continue to be dominant. Dilated CNNs (Yu and Koltun, 2015) with CRF label modeling were applied to named entity recognition by Strubell et al. (2017), and earlier were applied to SL by Xu and Sarikaya (2013). Convolutional and attentionbased sentence encoders have been applied in complex tasks, including machine translation, natural language inference, and parsing. (Gehring et al., 2017; Vaswani et al., 2017; Shen et al., 2017; Kitaev and Klein, 2018) We draw from both of these bodies of work to propose a simple yet highly effective family of IC+SL models. label-recurrent, and non-recurrent. Recent stateof-the-art models fall into the first category, as encoder-decoder architectures have recurrent encoders to perform word context encoding,"
W19-5906,N18-2050,0,0.0994491,"Missing"
W19-5906,W18-2501,0,\N,Missing
