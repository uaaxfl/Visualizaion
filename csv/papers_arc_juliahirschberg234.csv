2021.emnlp-main.364,{CH}o{R}a{L}: Collecting Humor Reaction Labels from Millions of Social Media Users,2021,-1,-1,3,0,9454,zixiaofan yang,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Humor detection has gained attention in recent years due to the desire to understand user-generated content with figurative language. However, substantial individual and cultural differences in humor perception make it very difficult to collect a large-scale humor dataset with reliable humor labels. We propose CHoRaL, a framework to generate perceived humor labels on Facebook posts, using the naturally available user reactions to these posts with no manual annotation needed. CHoRaL provides both binary labels and continuous scores of humor and non-humor. We present the largest dataset to date with labeled humor on 785K posts related to COVID-19. Additionally, we analyze the expression of COVID-related humor in social media by extracting lexico-semantic and affective features from the posts, and build humor detection models with performance similar to humans. CHoRaL enables the development of large-scale humor detection models on any topic and opens a new path to the study of humor on social media."
2021.eacl-main.23,"{``}Talk to me with left, right, and angles{''}: Lexical entrainment in spoken {H}ebrew dialogue",2021,-1,-1,4,0,10541,andreas weise,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"It has been well-documented for several languages that human interlocutors tend to adapt their linguistic productions to become more similar to each other. This behavior, known as entrainment, affects lexical choice as well, both with regard to specific words, such as referring expressions, and overall style. We offer what we believe to be the first investigation of such lexical entrainment in Hebrew. Using two existing measures, we analyze Hebrew speakers interacting in a Map Task, a popular experimental setup, and find rich evidence of lexical entrainment. Analyzing speaker pairs by the combination of their genders as well as speakers by their individual gender, we find no clear pattern of differences. We do, however, find that speakers in a position of less power entrain more than those with greater power, which matches theoretical accounts. Overall, our results mostly accord with those for American English, with a lack of entrainment on hedge words being the main difference."
2021.clpsych-1.14,Automatic Detection and Prediction of Psychiatric Hospitalizations From Social Media Posts,2021,-1,-1,5,1,10878,zhengping jiang,Proceedings of the Seventh Workshop on Computational Linguistics and Clinical Psychology: Improving Access,0,"We address the problem of predicting psychiatric hospitalizations using linguistic features drawn from social media posts. We formulate this novel task and develop an approach to automatically extract time spans of self-reported psychiatric hospitalizations. Using this dataset, we build predictive models of psychiatric hospitalization, comparing feature sets, user vs. post classification, and comparing model performance using a varying time window of posts. Our best model achieves an F1 of .718 using 7 days of posts. Our results suggest that this is a useful framework for collecting hospitalization data, and that social media data can be leveraged to predict acute psychiatric crises before they occur, potentially saving lives and improving outcomes for individuals with mental illness."
2020.tacl-1.14,Acoustic-Prosodic and Lexical Cues to Deception and Trust: Deciphering How People Detect Lies,2020,35,0,5,0,762,xi chen,Transactions of the Association for Computational Linguistics,0,"Humans rarely perform better than chance at lie detection. To better understand human perception of deception, we created a game framework, LieCatcher, to collect ratings of perceived deception using a large corpus of deceptive and truthful interviews. We analyzed the acoustic-prosodic and linguistic characteristics of language trusted and mistrusted by raters and compared these to characteristics of actual truthful and deceptive language to understand how perception aligns with reality. With this data we built classifiers to automatically distinguish trusted from mistrusted speech, achieving an F1 of 66.1{\%}. We next evaluated whether the strategies raters said they used to discriminate between truthful and deceptive responses were in fact useful. Our results show that, although several prosodic and lexical features were consistently perceived as trustworthy, they were not reliable cues. Also, the strategies that judges reported using in deception detection were not helpful for the task. Our work sheds light on the nature of trusted language and provides insight into the challenging problem of human deception detection."
2020.louhi-1.16,Detection of Mental Health from {R}eddit via Deep Contextualized Representations,2020,-1,-1,4,1,10878,zhengping jiang,Proceedings of the 11th International Workshop on Health Text Mining and Information Analysis,0,"We address the problem of automatic detection of psychiatric disorders from the linguistic content of social media posts. We build a large scale dataset of Reddit posts from users with eight disorders and a control user group. We extract and analyze linguistic characteristics of posts and identify differences between diagnostic groups. We build strong classification models based on deep contextualized word representations and show that they outperform previously applied statistical models with simple linguistic features by large margins. We compare user-level and post-level classification performance, as well as an ensembled multiclass model."
2020.alw-1.2,A Novel Methodology for Developing Automatic Harassment Classifiers for {T}witter,2020,-1,-1,5,0,22380,ishaan arora,Proceedings of the Fourth Workshop on Online Abuse and Harms,0,"Most efforts at identifying abusive speech online rely on public corpora that have been scraped from websites using keyword-based queries or released by site or platform owners for research purposes. These are typically labeled by crowd-sourced annotators {--} not the targets of the abuse themselves. While this method of data collection supports fast development of machine learning classifiers, the models built on them often fail in the context of real-world harassment and abuse, which contain nuances less easily identified by non-targets. Here, we present a mixed-methods approach to create classifiers for abuse and harassment which leverages direct engagement with the target group in order to achieve high quality and ecological validity of data sets and labels, and to generate deeper insights into the key tactics of bad actors. We use women journalists{'} experience on Twitter as an initial community of focus. We identify several structural mechanisms of abuse that we believe will generalize to other target communities."
W19-4001,Crowdsourced Hedge Term Disambiguation,2019,0,0,2,1,17200,morgan ulinski,Proceedings of the 13th Linguistic Annotation Workshop,0,"We address the issue of acquiring quality annotations of hedging words and phrases, linguistic phenomenona in which words, sounds, or other constructions are used to express ambiguity or uncertainty. Due to the limited availability of existing corpora annotated for hedging, linguists and other language scientists have been constrained as to the extent they can study this phenomenon. In this paper, we introduce a new method of acquiring hedging annotations via crowdsourcing, based on reformulating the task of labeling hedges as a simple word sense disambiguation task. We also introduce a new hedging corpus we have constructed by applying this method, a collection of forum posts annotated using Amazon Mechanical Turk. We found that the crowdsourced judgments we obtained had an inter-annotator agreement of 92.89{\%} (Fleiss{'} Kappa=0.751) and, when comparing a subset of these annotations to an expert-annotated gold standard, an accuracy of 96.65{\%}."
W19-1607,{S}patial{N}et: A Declarative Resource for Spatial Relations,2019,-1,-1,3,1,17200,morgan ulinski,Proceedings of the Combined Workshop on Spatial Language Understanding ({S}p{LU}) and Grounded Communication for Robotics ({R}obo{NLP}),0,"This paper introduces SpatialNet, a novel resource which links linguistic expressions to actual spatial configurations. SpatialNet is based on FrameNet (Ruppenhofer et al., 2016) and VigNet (Coyne et al., 2011), two resources which use frame semantics to encode lexical meaning. SpatialNet uses a deep semantic representation of spatial relations to provide a formal description of how a language expresses spatial information. This formal representation of the lexical semantics of spatial language also provides a consistent way to represent spatial meaning across multiple languages. In this paper, we describe the structure of SpatialNet, with examples from English and German. We also show how SpatialNet can be combined with other existing NLP tools to create a text-to-scene system for a language."
W18-3201,Joint Part-of-Speech and Language {ID} Tagging for Code-Switched Data,2018,0,3,2,0,4810,victor soto,Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching,0,"Code-switching is the fluent alternation between two or more languages in conversation between bilinguals. Large populations of speakers code-switch during communication, but little effort has been made to develop tools for code-switching, including part-of-speech taggers. In this paper, we propose an approach to POS tagging of code-switched English-Spanish data based on recurrent neural networks. We test our model on known monolingual benchmarks to demonstrate that our neural POS tagging model is on par with state-of-the-art methods. We next test our code-switched methods on the Miami Bangor corpus of English Spanish conversation, focusing on two types of experiments: POS tagging alone, for which we achieve 96.34{\%} accuracy, and joint part-of-speech and language ID tagging, which achieves similar POS tagging accuracy (96.39{\%}) and very high language ID accuracy (98.78{\%}). Finally, we show that our proposed models outperform other state-of-the-art code-switched taggers."
W18-3219,Named Entity Recognition on Code-Switched Data: Overview of the {CALCS} 2018 Shared Task,2018,0,10,5,0,134,gustavo aguilar,Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching,0,"In the third shared task of the Computational Approaches to Linguistic Code-Switching (CALCS) workshop, we focus on Named Entity Recognition (NER) on code-switched social-media data. We divide the shared task into two competitions based on the English-Spanish (ENG-SPA) and Modern Standard Arabic-Egyptian (MSA-EGY) language pairs. We use Twitter data and 9 entity types to establish a new dataset for code-switched NER benchmarks. In addition to the CS phenomenon, the diversity of the entities and the social media challenges make the task considerably hard to process. As a result, the best scores of the competitions are 63.76{\%} and 71.61{\%} for ENG-SPA and MSA-EGY, respectively. We present the scores of 9 participants and discuss the most common challenges among submissions."
W18-1301,Using Hedge Detection to Improve Committed Belief Tagging,2018,0,0,3,1,17200,morgan ulinski,Proceedings of the Workshop on Computational Semantics beyond Events and Roles,0,"We describe a novel method for identifying hedge terms using a set of manually constructed rules. We present experiments adding hedge features to a committed belief system to improve classification. We compare performance of this system (a) without hedging features, (b) with dictionary-based features, and (c) with rule-based features. We find that using hedge features improves performance of the committed belief system, particularly in identifying instances of non-committed belief and reported belief."
N18-1176,Linguistic Cues to Deception and Perceived Deception in Interview Dialogues,2018,0,2,3,1,2900,sarah levitan,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",0,"We explore deception detection in interview dialogues. We analyze a set of linguistic features in both truthful and deceptive responses to interview questions. We also study the perception of deception, identifying characteristics of statements that are perceived as truthful or deceptive by interviewers. Our analysis show significant differences between truthful and deceptive question responses, as well as variations in deception patterns across gender and native language. This analysis motivated our selection of features for machine learning experiments aimed at classifying globally deceptive speech. Our best classification performance is 72.74{\%} F1-Score (about 17{\%} better than human performance), which is achieved using a combination of linguistic features and individual traits."
L18-1107,Collecting Code-Switched Data from Social Media,2018,0,0,4,0,29623,gideon mendels,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1237,Evaluating the {W}ords{E}ye Text-to-Scene System: Imaginative and Realistic Sentences,2018,0,0,3,1,17200,morgan ulinski,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
S17-1013,Comparing Approaches for Automatic Question Identification,2017,0,1,4,0,29483,angel maredia,Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*{SEM} 2017),0,"Collecting spontaneous speech corpora that are open-ended, yet topically constrained, is increasingly popular for research in spoken dialogue systems and speaker state, inter alia. Typically, these corpora are labeled by human annotators, either in the lab or through crowd-sourcing; however, this is cumbersome and time-consuming for large corpora. We present four different approaches to automatically tagging a corpus when general topics of the conversations are known. We develop these approaches on the Columbia X-Cultural Deception corpus and find accuracy that significantly exceeds the baseline. Finally, we conduct a cross-corpus evaluation by testing the best performing approach on the Columbia/SRI/Colorado corpus."
W16-5812,Part of Speech Tagging for Code Switched Data,2016,11,7,7,0.833333,24834,fahad alghamdi,Proceedings of the Second Workshop on Computational Approaches to Code Switching,0,None
W16-2609,Babler - Data Collection from the Web to Support Speech Recognition and Keyword Search,2016,21,1,3,0,29623,gideon mendels,Proceedings of the 10th Web as Corpus Workshop,0,None
W16-0806,"Identifying Individual Differences in Gender, Ethnicity, and Personality from Dialogue for Deception Detection",2016,-1,-1,7,1,2900,sarah levitan,Proceedings of the Second Workshop on Computational Approaches to Deception Detection,0,None
C16-1043,Incrementally Learning a Dependency Parser to Support Language Documentation in Field Linguistics,2016,12,1,2,1,17200,morgan ulinski,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"We present experiments in incrementally learning a dependency parser. The parser will be used in the WordsEye Linguistics Tools (WELT) (Ulinski et al., 2014) which supports field linguists documenting a language{'}s syntax and semantics. Our goal is to make syntactic annotation faster for field linguists. We have created a new parallel corpus of descriptions of spatial relations and motion events, based on pictures and video clips used by field linguists for elicitation of language from native speaker informants. We collected descriptions for each picture and video from native speakers in English, Spanish, German, and Egyptian Arabic. We compare the performance of MSTParser (McDonald et al., 2006) and MaltParser (Nivre et al., 2006) when trained on small amounts of this data. We find that MaltParser achieves the best performance. We also present the results of experiments using the parser to assist with annotation. We find that even when the parser is trained on a single sentence from the corpus, annotation time significantly decreases."
W15-4644,"Acoustic-prosodic entrainment in {S}lovak, {S}panish, {E}nglish and {C}hinese: A cross-linguistic comparison",2015,33,8,4,1,10544,rivka levitan,Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,"It is well established that speakers of Standard American English entrain, or become more similar to each other as they speak, in acoustic-prosodic features of their speech as well as other behaviors. Entrainment in other languages is less well understood. This work uses a variety of metrics to measure acoustic-prosodic entrainment in four comparable corpora of task-oriented conversational speech in Slovak, Spanish, English and Chinese. We report the results of these experiments and describe trends and patterns that can be observed from comparing acoustic-prosodic entrainment in these four languages. We find evidence of a variety of forms of entrainment across all the languages studied, with some evidence of individual differences as well within the languages."
S15-1009,A New Dataset and Evaluation for Belief/Factuality,2015,15,5,3,0,90,vinodkumar prabhakaran,Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics,0,"The terms xe2x80x9cbeliefxe2x80x9d and xe2x80x9cfactualityxe2x80x9d both refer to the intention of the writer to present the propositional content of an utterance as firmly believed by the writer, not firmly believed, or having some other status. This paper presents an ongoing annotation effort and an associated evaluation."
W14-4331,Detecting Inappropriate Clarification Requests in Spoken Dialogue Systems,2014,10,3,5,0,38453,alex liu,Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue ({SIGDIAL}),0,"Spoken Dialogue Systems ask for clarification when they think they have misunderstood users. Such requests may differ depending on the information the system believes it needs to clarify. However, when the error type or location is misidentified, clarification requests appear confusing or inappropriate. We describe a classifier that identifies inappropriate requests, trained on features extracted from user responses in laboratory studies. This classifier achieves 88.5% accuracy and .885 Fmeasure in detecting such requests."
W14-3907,Overview for the First Shared Task on Language Identification in Code-Switched Data,2014,16,46,9,0,136,thamar solorio,Proceedings of the First Workshop on Computational Approaches to Code Switching,0,"We present an overview of the first shared task on language identification on codeswitched data. The shared task included code-switched data from four language pairs: Modern Standard ArabicDialectal Arabic (MSA-DA), MandarinEnglish (MAN-EN), Nepali-English (NEPEN), and Spanish-English (SPA-EN). A total of seven teams participated in the task and submitted 42 system runs. The evaluation showed that language identification at the token level is more difficult when the languages present are closely related, as in the case of MSA-DA, where the prediction performance was the lowest among all language pairs. In contrast, the language pairs with the higest F-measure where SPA-EN and NEP-EN. The task made evident that language identification in code-switched data is still far from solved and warrants further research."
W14-2202,Documenting Endangered Languages with the {W}ords{E}ye Linguistics Tool,2014,13,3,5,1,17200,morgan ulinski,Proceedings of the 2014 Workshop on the Use of Computational Methods in the Study of Endangered Languages,0,"In this paper, we describe how field linguists can use the WordsEye Linguistics Tool (WELT) to study endangered languages. WELT is a tool under development for eliciting endangered language data and formally documenting a language, based on WordsEye (Coyne and Sproat, 2001), a text-to-scene generation tool that produces 3D scenes from text input. First, a linguist uses WELT to create elicitation materials and collect language data. Next, he or she uses WELT to formally document the language. Finally, the formal models are used to create a textto-scene system that takes input in the endangered language and generates a picture representing its meaning."
P14-5009,{WELT}: Using Graphics Generation in Linguistic Fieldwork,2014,12,0,4,1,17200,morgan ulinski,Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"We describe the WordsEye Linguistics tool (WELT), a novel tool for the documentation and preservation of endangered languages. WELT is based on WordsEye (Coyne and Sproat, 2001), a text-toscene tool that automatically generates 3D scenes from written input. WELT has two modes of operation. In the first mode, English input automatically generates a picture which can be used to elicit a description in the target language. In the second mode, the linguist formally documents the grammar of an endangered language, thereby creating a system that takes input in the endangered language and generates a picture according to the grammar; the picture can then be used to verify the grammar with native speakers. We will demonstrate WELTxe2x80x99s use on scenarios involving Arrernte and Nahuatl."
mata-etal-2014-teenage,Teenage and adult speech in school context: building and processing a corpus of {E}uropean {P}ortuguese,2014,14,4,4,0,39270,ana mata,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"We present a corpus of European Portuguese spoken by teenagers and adults in school context, CPE-FACES, with an overview of the differential characteristics of high school oral presentations and the challenges this data poses to automatic speech processing. The CPE-FACES corpus has been created with two main goals: to provide a resource for the study of prosodic patterns in both spontaneous and prepared unscripted speech, and to capture inter-speaker and speaking style variations common at school, for research on oral presentations. Research on speaking styles is still largely based on adult speech. References to teenagers are sparse and cross-analyses of speech types comparing teenagers and adults are rare. We expect CPE-FACES, currently a unique resource in this domain, will contribute to filling this gap in European Portuguese. Focusing on disfluencies and phrase-final phonetic-phonological processes we show the impact of teenage speech on the automatic segmentation of oral presentations. Analyzing fluent final intonation contours in declarative utterances, we also show that communicative situation specificities, speaker status and cross-gender differences are key factors in speaking style variation at school."
2014.iwslt-papers.2,Towards simultaneous interpreting: the timing of incremental machine translation and speech synthesis,2014,18,1,3,0,21478,timo baumann,Proceedings of the 11th International Workshop on Spoken Language Translation: Papers,0,"In simultaneous interpreting, human experts incrementally construct and extend partial hypotheses about the source speaker{'}s message, and start to verbalize a corresponding message in the target language, based on a partial translation {--} which may have to be corrected occasionally. They commence the target utterance in the hope that they will be able to finish understanding the source speaker{'}s message and determine its translation in time for the unfolding delivery. Of course, both incremental understanding and translation by humans can be garden-pathed, although experts are able to optimize their delivery so as to balance the goals of minimal latency, translation quality and high speech fluency with few corrections. We investigate the temporal properties of both translation input and output to evaluate the tradeoff between low latency and translation quality. In addition, we estimate the improvements that can be gained with a tempo-elastic speech synthesizer."
W13-4020,Exploring Features For Localized Detection of Speech Recognition Errors,2013,15,5,3,0,30105,eli pincus,Proceedings of the {SIGDIAL} 2013 Conference,0,"We address the problem of localized error detection in Automatic Speech Recognition (ASR) output to support the generation of targeted clarifications in spoken dialogue systems. Localized error detection finds specific mis-recognized words in a user utterance. Targeted clarifications, in contrast with generic xe2x80x98please repeat/rephrasexe2x80x99 clarifications, target a specific mis-recognized word in an utterance (Stoyanchev et al., 2012a) and require accurate detection of such words. We extend and modify work presented in (Stoyanchev et al., 2012b) by experimenting with a new set of features for predicting the likelihood of a local error in an ASR hypothesis on an unsifted version of the original dataset. We improve over baseline results, where only ASRgenerated features are used, by constructing optimal feature sets for utterance and word mis-recognition prediction. The f-measure for identifying incorrect utterances improves by 2.2% and by 3.9% for identifiying incorrect words."
W13-4021,Modelling Human Clarification Strategies,2013,12,9,3,0,33730,svetlana stoyanchev,Proceedings of the {SIGDIAL} 2013 Conference,0,"We model human responses to speech recognition errors from a corpus of human clarification strategies. We employ learning techniques to study 1) the decision to either stop and ask a clarification question or to continue the dialogue without clarification, and 2) the decision to ask a targeted clarification question or a more generic question. Targeted clarification questions focus specifically on the part of an utterance that is misrecognized, in contrast with generic requests to xe2x80x98please repeatxe2x80x99 or xe2x80x98please rephrasexe2x80x99. Our goal is to generate targeted clarification strategies for handling errors in spoken dialogue systems, when appropriate. Our experiments show that linguistic features, in particular the inferred part-ofspeech of a misrecognized word are predictive of human clarification decisions. A combination of linguistic features predicts a userxe2x80x99s decision to continue or stop a dialogue with accuracy of 72.8% over a majority baseline accuracy of 59.1%. The same set of features predict the decision to ask a targeted question with accuracy of 74.6% compared with the majority baseline of 71.8%. 1"
W12-2103,Detecting Hate Speech on the World Wide Web,2012,-1,-1,2,0,42366,william warner,Proceedings of the Second Workshop on Language in Social Media,0,None
N12-1002,Acoustic-Prosodic Entrainment and Social Behavior,2012,21,41,5,1,10544,rivka levitan,Proceedings of the 2012 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"In conversation, speakers have been shown to entrain, or become more similar to each other, in various ways. We measure entrainment on eight acoustic features extracted from the speech of subjects playing a cooperative computer game and associate the degree of entrainment with a number of manually-labeled social variables acquired using Amazon Mechanical Turk, as well as objective measures of dialogue success. We find that male-female pairs entrain on all features, while male-male pairs entrain only on particular acoustic features (intensity mean, intensity maximum and syllables per second). We further determine that entrainment is more important to the perception of female-male social behavior than it is for same-gender pairs, and it is more important to the smoothness and flow of male-male dialogue than it is for female-female or mixed-gender pairs. Finally, we find that entrainment is more pronounced when intensity or speaking rate is especially high or low."
J12-1001,Affirmative Cue Words in Task-Oriented Dialogue,2012,80,33,2,1,14960,agustin gravano,Computational Linguistics,0,"We present a series of studies of affirmative cue words-a family of cue words such as okay or alright that speakers use frequently in conversation. These words pose a challenge for spoken dialogue systems because of their ambiguity: They may be used for agreeing with what the interlocutor has said, indicating continued attention, or for cueing the start of a new topic, among other meanings. We describe differences in the acoustic/prosodic realization of such functions in a corpus of spontaneous, task-oriented dialogues in Standard American English. These results are important both for interpretation and for production in spoken language applications. We also assess the predictive power of computational methods for the automatic disambiguation of these words. We find that contextual information and final intonation figure as the most salient cues to automatic disambiguation."
W11-2018,Detecting Levels of Interest from Spoken Dialog with Multistream Prediction Feedback and Similarity Based Hierarchical Fusion Learning,2011,31,18,2,0,3991,william wang,Proceedings of the {SIGDIAL} 2011 Conference,0,"Detecting levels of interest from speakers is a new problem in Spoken Dialog Understanding with significant impact on real world business applications. Previous work has focused on the analysis of traditional acoustic signals and shallow lexical features. In this paper, we present a novel hierarchical fusion learning model that takes feedback from previous multistream predictions of prominent seed samples into account and uses a mean cosine similarity measure to learn rules that improve reclassification. Our method is domain-independent and can be adapted to other speech and language processing areas where domain adaptation is expensive to perform. Incorporating Discriminative Term Frequency and Inverse Document Frequency (D-TFIDF), lexical affect scoring, and low and high level prosodic and acoustic features, our experiments outperform the published results of all systems participating in the 2010 Inter-speech Paralinguistic Affect Subchallenge."
P11-2020,Entrainment in Speech Preceding Backchannels.,2011,19,41,3,1,10544,rivka levitan,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,0,"In conversation, when speech is followed by a backchannel, evidence of continued engagement by one's dialogue partner, that speech displays a combination of cues that appear to signal to one's interlocutor that a backchannel is appropriate. We term these cues back-channel-preceding cues (BPC)s, and examine the Columbia Games Corpus for evidence of entrainment on such cues. Entrainment, the phenomenon of dialogue partners becoming more similar to each other, is widely believed to be crucial to conversation quality and success. Our results show that speaking partners entrain on BPCs; that is, they tend to use similar sets of BPCs; this similarity increases over the course of a dialogue; and this similarity is associated with measures of dialogue coordination and task success."
W09-3936,Turn-Yielding Cues in Task-Oriented Dialogue,2009,24,36,2,1,14960,agustin gravano,Proceedings of the {SIGDIAL} 2009 Conference,0,"We examine a number of objective, automatically computable TURN-YIELDING CUES --- distinct prosodic, acoustic and syntactic events in a speaker's speech that tend to precede a smooth turn exchange --- in the Columbia Games Corpus, a large corpus of task-oriented dialogues. We show that the likelihood of occurrence of a turn-taking attempt from the interlocutor increases linearly with the number of cues conjointly displayed by the speaker. Our results are important for improving the coordination of speaking turns in interactive voice-response systems, so that systems can correctly estimate when the user is willing to yield the conversational floor, and so that they can produce their own turn-yielding cues appropriately."
W09-0807,Spoken {A}rabic Dialect Identification Using Phonotactic Modeling,2009,15,80,2,1,9761,fadi biadsy,Proceedings of the {EACL} 2009 Workshop on Computational Approaches to {S}emitic Languages,0,"The Arabic language is a collection of multiple variants, among which Modern Standard Arabic (MSA) has a special status as the formal written standard language of the media, culture and education across the Arab world. The other variants are informal spoken dialects that are the media of communication for daily life. Arabic dialects differ substantially from MSA and each other in terms of phonology, morphology, lexical choice and syntax. In this paper, we describe a system that automatically identifies the Arabic dialect (Gulf, Iraqi, Levantine, Egyptian and MSA) of a speaker given a sample of his/her speech. The phonotactic approach we use proves to be effective in identifying these dialects with considerable overall accuracy --- 81.60% using 30s test utterances."
N09-2021,"Detecting Pitch Accents at the Word, Syllable and Vowel Level",2009,9,35,2,1,30147,andrew rosenberg,"Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers",0,"The automatic identification of prosodic events such as pitch accent in English has long been a topic of interest to speech researchers, with applications to a variety of spoken language processing tasks. However, much remains to be understood about the best methods for obtaining high accuracy detection. We describe experiments examining the optimal domain for accent analysis. Specifically, we compare pitch accent identification at the syllable, vowel or word level as domains for analysis of acoustic indicators of accent. Our results indicate that a word-based approach is superior to syllable- or vowel-based detection, achieving an accuracy of 84.2%."
N09-1045,Improving the {A}rabic Pronunciation Dictionary for Phone and Word Recognition with Linguistically-Based Pronunciation Rules,2009,12,43,3,1,9761,fadi biadsy,Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"In this paper, we show that linguistically motivated pronunciation rules can improve phone and word recognition results for Modern Standard Arabic (MSA). Using these rules and the MADA morphological analysis and disambiguation tool, multiple pronunciations per word are automatically generated to build two pronunciation dictionaries; one for training and another for decoding. We demonstrate that the use of these rules can significantly improve both MSA phone recognition and MSA word recognition accuracies over a baseline system using pronunciation rules typically employed in previous work on MSA Automatic Speech Recognition (ASR). We obtain a significant improvement in absolute accuracy in phone recognition of 3.77%--7.29% and a significant improvement of 4.1% in absolute accuracy in ASR."
W08-0121,"Speaking More Like You: Lexical, Acoustic/Prosodic, and Discourse Entrainment in Spoken Dialogue Systems",2008,0,3,1,1,9456,julia hirschberg,Proceedings of the 9th {SIG}dial Workshop on Discourse and Dialogue,0,"When people engage in conversation, they adapt the way they speak to the speaking style of their conversational partner in a variety of ways. For example, they may adopt a certain way of describing something based upon the way their conversational partner describes it, or adapt their pitch range or speaking rate to a conversational partner's. They may even align their turn-taking style or use of cue phrases to match their partner's. These types of entrainment have been shown to correlate with various measures of task success and dialogue naturalness. While there is considerable evidence for lexical entrainment from laboratory experiments, much less is known about other types of acoustic-prosodic and discourse-level entrainment and little work has been done to examine entrainments in multiple modalities for the same dialogue. We will discuss work on entrainment in multiple dimensions in the Columbia Games Corpus. Our goal is to understand how the different varieties of entrainment correlate with one another and to determine which types of entrainment will be both useful and feasible for Spoken Dialogue Systems."
P08-2043,High Frequency Word Entrainment in Spoken Dialogue,2008,10,93,3,0,8333,ani nenkova,"Proceedings of ACL-08: HLT, Short Papers",0,"Cognitive theories of dialogue hold that entrainment, the automatic alignment between dialogue partners at many levels of linguistic representation, is key to facilitating both production and comprehension in dialogue. In this paper we examine novel types of entrainment in two corpora---Switchboard and the Columbia Games corpus. We examine entrainment in use of high-frequency words (the most common words in the corpus), and its association with dialogue naturalness and flow, as well as with task success. Our results show that such entrainment is predictive of the perceived naturalness of dialogues and is significantly correlated with task success; in overall interaction flow, higher degrees of entrainment are associated with more overlaps and fewer interruptions."
P08-1092,An Unsupervised Approach to Biography Production Using {W}ikipedia,2008,17,47,2,1,9761,fadi biadsy,Proceedings of ACL-08: HLT,1,"We describe an unsupervised approach to multi-document sentence-extraction based summarization for the task of producing biographies. We utilize Wikipedia to automatically construct a corpus of biographical sentences and TDT4 to construct a corpus of non-biographical sentences. We build a biographical-sentence classifier from these corpora and an SVM regression model for sentence ordering from the Wikipedia corpus. We evaluate our work on the DUC2004 evaluation data and with human judges. Overall, our system significantly outperforms all systems that participated in DUC2004, according to the ROUGE-L metric, and is preferred by human subjects."
P07-1101,On the role of context and prosody in the interpretation of {`}okay{'},2007,11,29,4,1,14960,agustin gravano,Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,1,"We examine the effect of contextual and acoustic cues in the disambiguation of three discourse-pragmatic functions of the word okay. Results of a perception study show that contextual cues are stronger predictors of discourse function than acoustic cues. However, acoustic features capturing the pitch excursion at the right edge of okay feature prominently in disambiguation, whether other contextual cues are present or not."
D07-1043,{V}-Measure: A Conditional Entropy-Based External Cluster Evaluation Measure,2007,29,600,2,1,30147,andrew rosenberg,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,"We present V-measure, an external entropybased cluster evaluation measure. Vmeasure provides an elegant solution to many problems that affect previously defined cluster evaluation measures including 1) dependence on clustering algorithm or data set, 2) the xe2x80x9cproblem of matchingxe2x80x9d, where the clustering of only a portion of data points are evaluated and 3) accurate evaluation and combination of two desirable aspects of clustering, homogeneity and completeness. We compare V-measure to a number of popular cluster evaluation measures and demonstrate that it satisfies several desirable properties of clustering solutions, using simulated clustering results. Finally, we use V-measure to evaluate two clustering tasks: document clustering and pitch accent type clustering."
N06-2023,Summarizing Speech Without Text Using Hidden {M}arkov Models,2006,12,62,2,0,41621,sameer maskey,"Proceedings of the Human Language Technology Conference of the {NAACL}, Companion Volume: Short Papers",0,"We present a method for summarizing speech documents without using any type of transcript/text in a Hidden Markov Model framework. The hidden variables or states in the model represent whether a sentence is to be included in a summary or not, and the acoustic/prosodic features are the observation vectors. The model predicts the optimal sequence of segments that best summarize the document. We evaluate our method by comparing the predicted summary with one generated by a human summarizer. Our results indicate that we can generate 'good' summaries even when using only acoustic/prosodic information, which points toward the possibility of text-independent summarization for spoken documents."
N06-2032,"Story Segmentation of Broadcast News in {E}nglish, {M}andarin and {A}rabic",2006,21,39,2,1,30147,andrew rosenberg,"Proceedings of the Human Language Technology Conference of the {NAACL}, Companion Volume: Short Papers",0,"In this paper, we present results from a Broadcast News story segmentation system developed for the SRI NIGHTINGALE system operating on English, Arabic and Mandarin news shows to provide input to subsequent question-answering processes. Using a rule-induction algorithm with automatically extracted acoustic and lexical features, we report success rates that are competitive with state-of-the-art systems on each input language. We further demonstrate that features useful for English and Mandarin are not discriminative for Arabic."
J06-3004,Characterizing and Predicting Corrections in Spoken Dialogue Systems,2006,37,49,3,0,6782,diane litman,Computational Linguistics,0,"This article focuses on the analysis and prediction of corrections, defined as turns where a user tries to correct a prior error made by a spoken dialogue system. We describe our labeling procedure of various corrections types and statistical analyses of their features in a corpus collected from a train information spoken dialogue system. We then present results of machine-learning experiments designed to identify user corrections of speech recognition errors. We investigate the predictive power of features automatically computable from the prosody of the turn, the speech recognition process, experimental conditions, and the dialogue history. Our best-performing features reduce classification error from baselines of 25.70xe2x80x9328.99% to 15.72%."
P04-1085,Identifying Agreement and Disagreement in Conversational Speech: Use of {B}ayesian Networks to Model Pragmatic Dependencies,2004,18,170,3,0,4268,michel galley,Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ({ACL}-04),1,"We describe a statistical approach for modeling agreements and disagreements in conversational interaction. Our approach first identifies adjacency pairs using maximum entropy ranking based on a set of lexical, durational, and structural features that look both forward and backward in the discourse. We then classify utterances as agreement or disagreement using these adjacency pairs and features that represent various pragmatic influences of previous agreement or disagreement on the current utterance. Our approach achieves 86.9% accuracy, a 4.9% increase over previous work."
W01-1610,Labeling Corrections and Aware Sites in Spoken Dialogue Systems,2001,15,5,1,1,9456,julia hirschberg,Proceedings of the Second {SIG}dial Workshop on Discourse and Dialogue,0,"This paper deals with user corrections and aware sites of system errors in the TOOT spoken dialogue system. We first describe our corpus, and give details on our procedure to label corrections and aware sites. Then, we show that corrections and aware sites exhibit some prosodic and other properties which set them apart from 'normal' utterances. It appears that some correction types, such as simple repeats, are more likely to be correctly recognized than other types, such as paraphrases. We also present evidence that system dialogue strategy affects users' choice of correction type, suggesting that strategy-specific methods of detecting or coaching users on corrections may be useful. Aware sites tend to be shorter than other utterances, and are also more difficult to recognize correctly for the ASR system."
P01-1048,Predicting User Reactions to System Error,2001,12,22,2,0,6782,diane litman,Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,1,"This paper focuses on the analysis and prediction of so-called aware sites, defined as turns where a user of a spoken dialogue system first becomes aware that the system has made a speech recognition error. We describe statistical comparisons of features of these aware sites in a train timetable spoken dialogue corpus, which reveal significant prosodic differences between such turns, compared with turns that 'correct' speech recognition errors as well as with 'normal' turns that are neither aware sites nor corrections. We then present machine learning results in which we show how prosodic features in combination with other automatically available features can predict whether or not a user turn was a normal turn, a correction, and/or an aware site."
N01-1027,Identifying User Corrections Automatically in Spoken Dialogue Systems,2001,16,35,1,1,9456,julia hirschberg,Second Meeting of the North {A}merican Chapter of the Association for Computational Linguistics,0,"We present results of machine learning experiments designed to identify user corrections of speech recognition errors in a corpus collected from a train information spoken dialogue system. We investigate the predictive power of features automatically computable from the prosody of the turn, the speech recognition process, experimental conditions, and the dialogue history. Our best performing features reduce classification error from baselines of 25.70-28.99% to 15.72%."
H01-1064,{SCANM}ail: Audio Navigation in the Voicemail Domain,2001,8,20,2,0,51817,michiel bacchiani,Proceedings of the First International Conference on Human Language Technology Research,0,"This paper describes SCANMail, a system that allows users to browse and search their voicemail messages by content through a GUI. Content based navigation is realized by use of automatic speech recognition, information retrieval, information extraction and human computer interaction technology. In addition to the browsing and querying functionalities, acoustics-based caller ID technology is used to proposes caller names from existing caller acoustic models trained from user feedback. The GUI browser also provides a note-taking capability. Comparing SCANMail to a regular voicemail interface in a user study, SCANMail performed better both in terms of objective (time to and quality of solutions) as well as subjective objectives."
P00-1030,Modeling Local Context for Pitch Accent Prediction,2000,17,39,2,0,9597,shimei pan,Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,1,"Pitch accent placement is a major topic in intonational phonology research and its application to speech synthesis. What factors influence whether or not a word is made intonationally prominent or not is an open question. In this paper, we investigate how one aspect of a word's local context --- its collocation with neighboring words --- influences whether it is accented or not. Results of experiments on two transcribed speech corpora in a medical domain show that such collocation information is a useful predictor of pitch accent placement."
A00-2029,Predicting Automatic Speech Recognition Performance Using Prosodic Cues,2000,23,67,2,0.182636,6782,diane litman,1st Meeting of the North {A}merican Chapter of the Association for Computational Linguistics,0,"In spoken dialogue systems, it is important for a system to know how likely a speech recognition hypothesis is to be correct, so it can reprompt for fresh input, or, in cases where many errors have occurred, change its interaction strategy or switch the caller to a human attendant. We have discovered prosodic features which more accurately predict when a recognition hypothesis contains a word error than the acoustic confidence score thresholds traditionally used in automatic speech recognition. We present analytic results indicating that there are significant prosodic differences between correctly and incorrectly recognized turns in the TOOT train information corpus. We then present machine learning results showing how the use of prosodic features to automatically predict correct versus incorrectly recognized turns improves over the use of acoustic confidence scores alone."
W98-0206,{``}{I} just played that a minute ago!:{''} Designing User Interfaces for Audio Navigation,1998,8,0,1,1,9456,julia hirschberg,Content Visualization and Intermedia Representations ({CVIR}{'}98),0,None
P96-1038,A Prosodic Analysis of Discourse Segments in Direction-Giving Monologues,1996,35,146,1,1,9456,julia hirschberg,34th Annual Meeting of the Association for Computational Linguistics,1,"This paper reports on corpus-based research into the relationship between intonational variation and discourse structure. We examine the effects of speaking style (read versus spontaneous) and of discourse segmentation method (text-alone versus text-and-speech) on the nature of this relationship. We also compare the acoustic-prosodic features of initial, medial, and final utterances in a discourse segment."
P93-1007,A Speech-First Model for Repair Detection and Correction,1993,22,1,2,0,54323,christine nakatani,31st Annual Meeting of the Association for Computational Linguistics,1,"Interpreting fully natural speech is an important goal for spoken language understanding systems. However, while corpus studies have shown that about 10% of spontaneous utterances contain self-corrections, or REPAIRS, little is known about the extent to which cues in the speech signal may facilitate repair processing. We identify several cues based on acoustic and prosodic analysis of repairs in the DARPA Air Travel Information System database, and propose methods for exploiting these cues to detect and correct repairs."
J93-3003,Empirical Studies on the Disambiguation of Cue Phrases,1993,33,244,1,1,9456,julia hirschberg,Computational Linguistics,0,"Cue phrases are linguistic expressions such as now and well that function as explicit indicators of the structure of a discourse. For example, now may signal the beginning of a subtopic or a return to a previous topic, while well may mark subsequent material as a response to prior material, or as an explanatory comment. However, while cue phrases may convey discourse structure, each also has one or more alternate uses. While incidentally may be used sententially as an adverbial, for example, the discourse use initiates a digression. Although distinguishing discourse and sentential uses of cue phrases is critical to the interpretation and generation of discourse, the question of how speakers and hearers accomplish this disambiguation is rarely addressed.This paper reports results of empirical studies on discourse and sentential uses of cue phrases, in which both text-based and prosodic features were examined for disambiguating power. Based on these studies, it is proposed that discourse versus sentential usage may be distinguished by intonational features, specifically, pitch accent and prosodic phrasing. A prosodic model that characterizes these distinctions is identified. This model is associated with features identifiable from text analysis, including orthography and part of speech, to permit the application of the results of the prosodic analysis to the generation of appropriate intonational features for discourse and sentential uses of cue phrases in synthetic speech."
H93-1066,A Speech-First Model for Repair Detection and Correction,1993,22,1,2,0,54323,christine nakatani,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop Held at Plainsboro, New Jersey, March 21-24, 1993",0,"Interpreting fully natural speech is an important goal for spoken language understanding systems. However, while corpus studies have shown that about 10% of spontaneous utterances contain self-corrections, or REPAIRS, little is known about the extent to which cues in the speech signal may facilitate repair processing. We identify several cues based on acoustic and prosodic analysis of repairs in the DARPA Air Travel Information System database, and propose methods for exploiting these cues to detect and correct repairs."
H92-1084,Session 13: Prosody,1992,-1,-1,2,0,54006,patti price,"Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, {F}ebruary 23-26, 1992",0,None
H92-1089,Intonational Features of Local and Global Discourse Structure,1992,12,71,1,1,9456,julia hirschberg,"Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, {F}ebruary 23-26, 1992",0,"We present results of a study of the relationship between intonational features including pitch range, timing, and amplitude and aspects of discourse structure defined in terms of Grosz and Sidner's (1986) model of discourse. We compare structural labelings of AP news text with prosodic/acoustic features examined from recordings of the same text read by a professional newscaster. We find significant correlations between prosodic/acoustic characteristics and both local and global aspects of discourse structure identified by our labelers. Our results have applications for speech synthesis and, potentially, for speech recognition."
H91-1074,Predicting Intonational Boundaries Automatically from Text: The {ATIS} Domain,1991,13,20,2,0,56827,michelle wang,"Speech and Natural Language: Proceedings of a Workshop Held at Pacific Grove, California, {F}ebruary 19-22, 1991",0,"Relating the intonational characteristics of an utterance to other features inferable from its text is important both for speech recognition and for speech synthesis. This work investigates techniques for predicting the location of intonational phrase boundaries in natural speech, through analyzing a utterances from the DARPA Air Travel Information Service database. For statistical modeling, we employ Classification and Regression Tree (CART) techniques. We achieve success rates of just over 90%."
C90-2044,Disambiguating Cue Phrases in Text and Speech,1990,18,34,2,0.182636,6782,diane litman,{COLING} 1990 Volume 2: Papers presented to the 13th International Conference on Computational Linguistics,0,"Cue phrases are linguistic expressions such as 'now' and 'well' that may explicitly mark the structure of a discourse. For example, while the cue phrase 'incidentally' may be used SENTENTIALLY as an adverbial, the DISCOURSE use initiates a digression. In [8], we noted the ambiguity of cue phrases with respect to discourse and sentential usage and proposed an intonational model for their disambiguation. In this paper, we extend our previous characterization of cue phrases and generalize its domain of coverage, based on a larger and more comprehensive empirical study: an examination of all cue phrases produced by a single speaker in recorded natural speech. We also associate this prosodic model with orthographic and part-of-speech analyses of cue phrases in text. Such a dual model provides both theoretical justification for current computational models of discourse and practical application to the generation of synthetic speech."
H89-2004,Distinguishing Questions by Contour Speech Recognition Tasks,1989,5,5,1,1,9456,julia hirschberg,"Speech and Natural Language: Proceedings of a Workshop Held at Cape Cod, Massachusetts, October 15-18, 1989",0,"It is generally acknowledged today that, while the intonational features speakers select when they utter a sentence are not determined by the syntax, semantics or discourse context of that sentence, knowledge of these factors can help to constrain the possible intonational features speakers are likely to choose. So, while intonational variation poses a challenge to speech recognition in one sense -- in presenting yet another indicator of over-all utterance meaning to be recognized -- regularities noted between intonational features and the syntax, semantics and discourse features of an utterance also present rich possibilities for help in the recognition task."
P88-1023,Assigning Intonational Features in Synthesized Spoken Directions,1988,17,48,2,0,27722,james davis,26th Annual Meeting of the Association for Computational Linguistics,1,"Speakers convey much of the information hearers use to interpret discourse by varying prosodic features such as PHRASING, PITCH ACCENT placement, TUNE, and PITCH RANGE. The ability to emulate such variation is crucial to effective (synthetic) speech generation. While text-to-speech synthesis must rely primarily upon structural information to determine appropriate intonational features, speech synthesized from an abstract representation of the message to be conveyed may employ much richer sources. The implementation of an intonation assignment component for Direction Assistance, a program which generates spoken directions, provides a first approximation of how recent models of discourse structure can be used to control intonational variation in ways that build upon recent research in intonational meaning. The implementation further suggests ways in which these discourse models might be augmented to permit the assignment of appropriate intonational features."
T87-1019,NO TITLE,1987,-1,-1,1,1,9456,julia hirschberg,Theoretical Issues in Natural Language Processing 3,0,None
P87-1023,Now Let{'}s Talk About Now; Identifying Cue Phrases Intonationally,1987,18,75,1,1,9456,julia hirschberg,25th Annual Meeting of the Association for Computational Linguistics,1,"Cue phrases are words and phrases such as now and by the way which may be used to convey explicit information about the structure of a discourse. However, while cue phrases may convey discourse structure, each may also be used to different effect. The question of how speakers and hearers distinguish between such uses of cue phrases has not been addressed in discourse studies to date. Based on a study of now in natural recorded discourse, we propose that cue and non-cue usage can be distinguished intonationally, on the basis of phrasing and accent."
P86-1021,The Intonational Structuring of Discourse,1986,29,159,1,1,9456,julia hirschberg,24th Annual Meeting of the Association for Computational Linguistics,1,"We propose a mapping between prosodic phenomena and semantico-pragmatic effects based upon the hypothesis that intonation conveys information about the intentional as well as the attentional structure of discourse. In particular, we discuss how variations in pitch range and choice of accent and tune can help to convey such information as: discourse segmentation and topic structure, appropriate choice of referent, the distinction between 'given' and 'new' information, conceptual contrast or parallelism between mentioned items, and subordination relationships between propositions salient in the discourse. Our goals for this research are practical as well as theoretical. In particular, we are investigating the problem of intonational assignment in synthetic speech."
P84-1012,Toward a Redefinition of Yes/No Questions,1984,6,9,1,1,9456,julia hirschberg,10th International Conference on Computational Linguistics and 22nd Annual Meeting of the Association for Computational Linguistics,1,"While both theoretical and empirical studies of question-answering have revealed the inadequacy of traditional definitions of yes-no questions (YNQs), little progress has been made toward a more satisfactory redefinition. This paper reviews the limitations of several proposed revisions. It proposes a new definition of YNQs based upon research on a type of conversational implicature, termed here scalar implicature, that helps define appropriate responses to YNQs. By representing YNQs as scalar queries it is possible to support a wider variety of system and user responses in a principled way."
