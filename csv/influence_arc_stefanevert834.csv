2020.lrec-1.410,L16-1446,1,0.841482,"hey target and from the point of view of information needs that users may have. The CQLF standard family aims at capturing the commonalities while at the same time defining and circumscribing the matrix of potential variation among CQLs. The existence of a large number of different corpus query languages poses an epistemic challenge for the research community, since (1) different CQLs have to be learned in order to be able to address specific information needs, and (2) some language resources are in practice only accessible through corpus management systems that feature only a single CQL (cf. Bański et al., 2016). We are thus in need of better interoperability across corpus query systems, realized – in one approach – by abstracting away from individual CQLs to see how far their queries can be compared or even transferred between the various corpus analysis platforms. The present paper describes the second part of the Corpus Query Lingua Franca (CQLF) family of standards. CQLF aims, inter alia, at facilitating the comparison of the properties of different corpus query languages. It is meant to provide a common reference taxonomy based on “a consistent, stable and highly expressive set of category labe"
2020.lrec-1.410,frick-etal-2012-evaluating,1,0.78717,"w in a moderated community process. 2. CQLF part I: Metamodel The CQLF Metamodel provides a coarse-grained classification of CQLs, describing their scope at a general level with conformance conditions meant to be satisfied by a wide range of CQLs. It provides the “skeleton” of a CQL taxonomy by setting up basic categories of corpus queries (CQLF levels and modules) and the dependencies among them, cf. Figure 1.2 1 The ISO standardization process is described at https://www.iso.org/stages-and-resources-for-standards-develop ment.html 2 For the context of the CQLF endeavour, see Mueller (2010), Frick et al. (2012), and Bański et al. (2016). An implementation of early CQLF ideas in the KorAP project is discussed by Bingel and Diewald (2015). 3346 search needs within a taxonomy of CQL capabilities. This is achieved mainly through the Expressive Power taxonomy (described in Section 4), complemented by the CQLF Metamodel taxonomy (formalizing the first part of the standard) and a taxonomy of CQLs. Links between the CQL taxonomy and the Expressive Power taxonomy describe the search needs satisfied by a given CQL (and thus its degree of conformance to the CQLF Ontology), while at the same time providing end"
2020.lrec-1.754,W16-2606,1,0.923267,"Missing"
2020.lrec-1.754,C10-3009,0,0.0764979,"Missing"
2020.lrec-1.754,W16-2615,0,0.0141424,"een implemented, inter alia, by SoMaJo (Proisl and Uhrig, 2016)3 , the winning tokenizer of the shared task. For POS tagging, the STTS IBK tag set (Beißwenger et al., 2015b)2 has been used, which builds on the Stuttgart-T¨ubingen-Tagset (STTS; Schiller et al. (1999)) and extends it with tags for phenomena found in CMC genres (emoticons, hashtags, etc.) or in spontaneous spoken or conceptually oral language (e. g. various types of contractions). Pretrained tagger models for STTS IBK are available, inter alia, for SoMeWeTa (Proisl, 2018)4 , GermaPOS (Remus et al., 2016)5 and the LTL-UDE system (Horsmann and Zesch, 2016)6 . Subsequently, Rehbein et al. (2018) manually added sentence boundaries to the EmpiriST corpus, automatically mapped the part-of-speech tags to UD POS tags (Nivre et al., 2016)7 and incorporated the dataset into their harmonised testsuite for POS tagging of German social media data.8 For the identification of sentence boundaries, they used the following rules to guide the segmentation: • Hashtags and URLs at the beginning or the end of the tweet that are not integrated in the sentence are separated and form their own unit [. . . ]. • Emoticons are treated as non-verbal comments to the text"
2020.lrec-1.754,D14-1108,0,0.0188332,"ers. The creation of these “gold standards” for corpus annotation layers is thus an inevitable (though labour-intensive and tedious) endeavour to make language data accessible. While there is a comparatively large amount of manually annotated data available for English, especially for standard (newspaper) texts, other languages and registers (such as computer-mediated communication, CMC) do not enjoy such great popularity. A notable exception are English Twitter data, for which both manually annotated corpora and designated tools have been developed (Ritter et al., 2011; Owoputi et al., 2013; Kong et al., 2014). Our focus is on German web and CMC data. Off-the-shelf natural language processing (NLP) tools trained on newspaper corpora typically show a relatively poor performance on this kind of out-of-domain data (Giesbrecht and Evert, 2009; Neunerdt et al., 2013). As has been noted before, there are major linguistic differences between CMC and standard German (Haase et al., 1997; Runkehl et al., 1998; Dietterle et al., 2017; Beißwenger and Pappert, 2018): Computer-mediated communication, and chat communication in particular, has often been described as being “conceptually oral”, i. e. exhibiting phe"
2020.lrec-1.754,L16-1262,0,0.0417289,"Missing"
2020.lrec-1.754,N13-1039,0,0.0304626,"Missing"
2020.lrec-1.754,W16-2607,1,0.878447,"d task1 1 https://sites.google.com/site/ empirist2015/ 6142 and featured manual tokenization and part-of-speech tagging according to custom annotation guidelines. The tokenization guidelines (Beißwenger et al., 2015a)2 cover a wide range of CMC-specific phenomena, including, for example, frequently used acronyms (aka, cu), typos and speed-writing phenomena (schona ber ‘yesb ut’, maldr¨uber), contracted forms (machstes from machst es or even machst du es ‘make you it’, nochn from noch ein ‘another’), emoticons, hashtags, addressing terms, etc., and have been implemented, inter alia, by SoMaJo (Proisl and Uhrig, 2016)3 , the winning tokenizer of the shared task. For POS tagging, the STTS IBK tag set (Beißwenger et al., 2015b)2 has been used, which builds on the Stuttgart-T¨ubingen-Tagset (STTS; Schiller et al. (1999)) and extends it with tags for phenomena found in CMC genres (emoticons, hashtags, etc.) or in spontaneous spoken or conceptually oral language (e. g. various types of contractions). Pretrained tagger models for STTS IBK are available, inter alia, for SoMeWeTa (Proisl, 2018)4 , GermaPOS (Remus et al., 2016)5 and the LTL-UDE system (Horsmann and Zesch, 2016)6 . Subsequently, Rehbein et al. (2018"
2020.lrec-1.754,L18-1106,1,0.850836,"ein ‘another’), emoticons, hashtags, addressing terms, etc., and have been implemented, inter alia, by SoMaJo (Proisl and Uhrig, 2016)3 , the winning tokenizer of the shared task. For POS tagging, the STTS IBK tag set (Beißwenger et al., 2015b)2 has been used, which builds on the Stuttgart-T¨ubingen-Tagset (STTS; Schiller et al. (1999)) and extends it with tags for phenomena found in CMC genres (emoticons, hashtags, etc.) or in spontaneous spoken or conceptually oral language (e. g. various types of contractions). Pretrained tagger models for STTS IBK are available, inter alia, for SoMeWeTa (Proisl, 2018)4 , GermaPOS (Remus et al., 2016)5 and the LTL-UDE system (Horsmann and Zesch, 2016)6 . Subsequently, Rehbein et al. (2018) manually added sentence boundaries to the EmpiriST corpus, automatically mapped the part-of-speech tags to UD POS tags (Nivre et al., 2016)7 and incorporated the dataset into their harmonised testsuite for POS tagging of German social media data.8 For the identification of sentence boundaries, they used the following rules to guide the segmentation: • Hashtags and URLs at the beginning or the end of the tweet that are not integrated in the sentence are separated and form"
2020.lrec-1.754,W16-2613,0,0.020167,"hashtags, addressing terms, etc., and have been implemented, inter alia, by SoMaJo (Proisl and Uhrig, 2016)3 , the winning tokenizer of the shared task. For POS tagging, the STTS IBK tag set (Beißwenger et al., 2015b)2 has been used, which builds on the Stuttgart-T¨ubingen-Tagset (STTS; Schiller et al. (1999)) and extends it with tags for phenomena found in CMC genres (emoticons, hashtags, etc.) or in spontaneous spoken or conceptually oral language (e. g. various types of contractions). Pretrained tagger models for STTS IBK are available, inter alia, for SoMeWeTa (Proisl, 2018)4 , GermaPOS (Remus et al., 2016)5 and the LTL-UDE system (Horsmann and Zesch, 2016)6 . Subsequently, Rehbein et al. (2018) manually added sentence boundaries to the EmpiriST corpus, automatically mapped the part-of-speech tags to UD POS tags (Nivre et al., 2016)7 and incorporated the dataset into their harmonised testsuite for POS tagging of German social media data.8 For the identification of sentence boundaries, they used the following rules to guide the segmentation: • Hashtags and URLs at the beginning or the end of the tweet that are not integrated in the sentence are separated and form their own unit [. . . ]. • Emotic"
2020.lrec-1.754,D11-1141,0,0.0984761,"Missing"
2020.lrec-1.754,C04-1024,0,0.157918,"Missing"
2021.conll-1.46,J10-4006,0,0.0723477,"iation Norms (USF, 5019 stimuli, 6000 subjects; Nelson et al., 2004). Pre-processing All items in EAT and USF were POS-tagged using frequency information from the large, publicly available Web corpus ENCOW 20142 to guess the most probable POS tag for each word form out of context. POS tags are not employed in the evaluation experiments conducted in this paper, but we believe that guessing POS based on frequency is cognitively plausible for out-ofcontext tasks and distribute POS annotation with the official release, to support further evaluation with POS-disambiguated DSM representations (e.g. Baroni and Lenci, 2010). We chose ENCOW because it is publicly available, large (10 bln words) and thus suitable for the extraction of reliable cooccurrence estimates and vector representations. FAST contains lemmatized items. This choice has practical reasons – many DSMs and other evaluation datasets are also lemmatized – and is supported by the literature. Bullinaria and Levy (2012) compare DSMs built from raw, stemmed and lemmatized data. In a selection of semantic similarity tasks, a slight advantage is found for the lemmatized and stemmed models over the raw ones (with variations due to the different tasks, and"
2021.conll-1.46,2020.acl-main.431,0,0.0177413,". 10 words, we also experiment with a mixed span of size 2 for the DSM and of size 10 for first-order collocations (span mix in the result tables). Embeddings: word2vec (Mikolov et al., 2013) trained on 100G tokens of Google News data,10 GloVe (Pennington et al., 2014) trained on 6G tokens of wikipedia + newspapers or 42G tokens of Common Crawl (CC) Web data,11 and FastText (Joulin et al., 2017) trained on 600G tokens of CC. We did not include contextualized embeddings because free associations are inherently an out-ofcontext task. We are aware that type-level representations can be obtained (Bommasani et al., 2020), but this is not the primary purpose of contextualised models and it would imply a number of design choices without clear guidelines. Given that the purpose of this paper is to release the dataset and provide baselines, we decided to stick to wellestablished type-level models. 5 Results Tables 1 and 2 report evaluation results on the multiple-choice and lexical access task, respecto discarding the first SVD dimensions, which Lapesa et al. (2014) found beneficial for modeling paradigmatic relations. 10 https://code.google.com/archive/p/ word2vec/ 11 https://nlp.stanford.edu/projects/ glove/ ti"
2021.conll-1.46,W16-5312,1,0.902334,"Missing"
2021.conll-1.46,K19-1050,0,0.0205375,"ationally expensive (because of the larger number of candidates to be considered), poses a harder machine-learning problem (learning to rank), and suitable evaluation metrics are less obvious (see Sec. 4.1). This paper makes the following contributions: we release FAST and propose two new evaluation tasks based on it; we report preliminary modelling results as a baseline for further work. 2 Related work Evaluation in terms of cognitive modeling is challenging from many points of view. First, the modeled data are typically continuous (reaction times or EEG signal, see Mandera et al. (2017) and Hollenstein et al. (2019) for an overview) or, when categorical (free association norms), much less constrained compared to the data used in standard DSM evaluation tasks because the models have to handle a large and diverse vocabulary. Second, a speaker’s behaviour results from the interplay of many factors, some of which we expect to be conflated in distributional meaning representations and intertwined with ‘true’ lexical semantics (e.g, ambiguity, culture-specific information), others to be completely absent (e.g., individual differences between speakers). Third, cognitive modeling datasets typically have not been"
2021.conll-1.46,Q15-1016,0,0.281305,"USF=442.0 and EAT=602.4. 4.2 Models We experimented with the following models, created from 8.5G tokens of the ENCOW 2014 Web corpus unless otherwise specified: First order co-occurrence (collocations), based on a symmetric span of 2 vs. 10 words and quantified by conditional probability (P (w2 |w1 )), logtransformed G2 , PPMI, and MI2 (Evert, 2008). Count DSMs (second order), based on a symmetric span of 2 vs. 10 words and SVD dimensionality reduction, with other parameter settings as in Lapesa and Evert (2014).8 We experimented with one further parameter, Caron P (Bullinaria and Levy, 2012; Levy et al., 2015a). Standard SVDreduced models were compared with their P = 0 counterpart; this operation has the effect of scaling down the prominence of the first SVD dimensions in the semantic representation and tailoring them towards paradigmatic relations.9 8 Target terms: All POS-disambiguated lemmas from the FAST data set combined with a standard vocabulary of frequent words, 58k terms in total. Context dimensions: same 58k POS-disambiguated lemmas as target terms. Feature scoring: sparse simple-log likelihood with an additional log transformation. Similarity computation: cosine similarity (using angle"
2021.conll-1.46,N15-1098,0,0.184948,"USF=442.0 and EAT=602.4. 4.2 Models We experimented with the following models, created from 8.5G tokens of the ENCOW 2014 Web corpus unless otherwise specified: First order co-occurrence (collocations), based on a symmetric span of 2 vs. 10 words and quantified by conditional probability (P (w2 |w1 )), logtransformed G2 , PPMI, and MI2 (Evert, 2008). Count DSMs (second order), based on a symmetric span of 2 vs. 10 words and SVD dimensionality reduction, with other parameter settings as in Lapesa and Evert (2014).8 We experimented with one further parameter, Caron P (Bullinaria and Levy, 2012; Levy et al., 2015a). Standard SVDreduced models were compared with their P = 0 counterpart; this operation has the effect of scaling down the prominence of the first SVD dimensions in the semantic representation and tailoring them towards paradigmatic relations.9 8 Target terms: All POS-disambiguated lemmas from the FAST data set combined with a standard vocabulary of frequent words, 58k terms in total. Context dimensions: same 58k POS-disambiguated lemmas as target terms. Feature scoring: sparse simple-log likelihood with an additional log transformation. Similarity computation: cosine similarity (using angle"
2021.conll-1.46,2020.acl-main.465,0,0.0610551,"Missing"
2021.conll-1.46,E17-2068,0,0.0216163,"rm well in paradigmatic tasks (with P = 0) with a first-order association known to perform well in collocation extraction (MI2 ), even if they are not the best individual models, and b) in addition to the span size of 2 vs. 10 words, we also experiment with a mixed span of size 2 for the DSM and of size 10 for first-order collocations (span mix in the result tables). Embeddings: word2vec (Mikolov et al., 2013) trained on 100G tokens of Google News data,10 GloVe (Pennington et al., 2014) trained on 6G tokens of wikipedia + newspapers or 42G tokens of Common Crawl (CC) Web data,11 and FastText (Joulin et al., 2017) trained on 600G tokens of CC. We did not include contextualized embeddings because free associations are inherently an out-ofcontext task. We are aware that type-level representations can be obtained (Bommasani et al., 2020), but this is not the primary purpose of contextualised models and it would imply a number of design choices without clear guidelines. Given that the purpose of this paper is to release the dataset and provide baselines, we decided to stick to wellestablished type-level models. 5 Results Tables 1 and 2 report evaluation results on the multiple-choice and lexical access tas"
2021.conll-1.46,W14-1503,0,0.0277923,"eliable cooccurrence estimates and vector representations. FAST contains lemmatized items. This choice has practical reasons – many DSMs and other evaluation datasets are also lemmatized – and is supported by the literature. Bullinaria and Levy (2012) compare DSMs built from raw, stemmed and lemmatized data. In a selection of semantic similarity tasks, a slight advantage is found for the lemmatized and stemmed models over the raw ones (with variations due to the different tasks, and to the in2 corporafromtheweb.org/encow14/ teraction with other parameters). For a different selection of tasks, Kiela and Clark (2014) report best performance for stemmed data vs. inflected, lemmatized, POS- and CCG-tagged.3 Lapesa and Evert (2013) obtain similar results for modeling semantic priming. We carried out lemmatization with morpha, a robust morphological analyzer.4 Unknown words were lemmatized based on their POS tag. All items in USF and EAT were annotated with their ENCOW lemma frequency (using the same lemmatization), as a basis for our sampling procedure. Dataset compilation The compilation procedure of the FAST dataset is based on the following assumptions: (a) words that were produced only by a single subjec"
2021.conll-1.46,D14-1162,0,0.0900386,"Missing"
2021.conll-1.46,C02-1007,0,0.333199,"Missing"
2021.conll-1.46,W13-2608,1,0.768793,"cal reasons – many DSMs and other evaluation datasets are also lemmatized – and is supported by the literature. Bullinaria and Levy (2012) compare DSMs built from raw, stemmed and lemmatized data. In a selection of semantic similarity tasks, a slight advantage is found for the lemmatized and stemmed models over the raw ones (with variations due to the different tasks, and to the in2 corporafromtheweb.org/encow14/ teraction with other parameters). For a different selection of tasks, Kiela and Clark (2014) report best performance for stemmed data vs. inflected, lemmatized, POS- and CCG-tagged.3 Lapesa and Evert (2013) obtain similar results for modeling semantic priming. We carried out lemmatization with morpha, a robust morphological analyzer.4 Unknown words were lemmatized based on their POS tag. All items in USF and EAT were annotated with their ENCOW lemma frequency (using the same lemmatization), as a basis for our sampling procedure. Dataset compilation The compilation procedure of the FAST dataset is based on the following assumptions: (a) words that were produced only by a single subject in response to a stimulus can be used as plausible distractors in a multiple-choice task; (b) first responses to"
2021.conll-1.46,rapp-2014-corpus,0,0.0389962,"Missing"
2021.conll-1.46,Q14-1041,1,0.916187,"e often obtained for P = 0 (i.e. using Ur as the dimensionality-reduced representation), which equalizes the contributions of all dimensions. The effect is comparable 591 Combined first- and second-order, based on the harmonic mean of neighbour ranks. Since cosine similarity (second-order) and association measures (first-order) are on entirely different scales, it would be difficult to combine them into a single composite score for a given word pair (A, B). We therefore use neighour rank, i.e. the rank of B among the nearest neighbours of A, as an index of distributional relatednes, following Lapesa and Evert (2014). For the first-order data, we order all words in the vocabulary by decreasing association score and take the rank of B in this ordering as its neighbour rank (Michelbacher et al., 2011). We consider the harmonic mean a better strategy for combining ranks than average, minimum or maximum because it is not dominated by extreme values. To exploit complementarity between different DSMs we a) combine a DSM known to perform well in paradigmatic tasks (with P = 0) with a first-order association known to perform well in collocation extraction (MI2 ), even if they are not the best individual models, a"
2021.conll-1.46,S14-1020,1,0.827477,"xtualized embeddings because free associations are inherently an out-ofcontext task. We are aware that type-level representations can be obtained (Bommasani et al., 2020), but this is not the primary purpose of contextualised models and it would imply a number of design choices without clear guidelines. Given that the purpose of this paper is to release the dataset and provide baselines, we decided to stick to wellestablished type-level models. 5 Results Tables 1 and 2 report evaluation results on the multiple-choice and lexical access task, respecto discarding the first SVD dimensions, which Lapesa et al. (2014) found beneficial for modeling paradigmatic relations. 10 https://code.google.com/archive/p/ word2vec/ 11 https://nlp.stanford.edu/projects/ glove/ tively. Items not covered by a model (miss) are ignored for the evaluation metrics, so as not to give an unfair advantage to models built specifically for the task vocabulary.12 Overall, first-order models outperform DSMs in the multiple-choice task, while DSMs are better in the lexical access task, achieving optimal performance with larger spans (known to introduce some first-order information into the DSMs). Setting P = 0 improves lexical access"
2021.conll-1.46,W14-4701,0,0.0458633,"Missing"
C04-1136,J93-1003,0,0.0860083,"Missing"
C04-1136,P01-1025,1,0.842878,"d TPs divided by the total number of TPs in the candidate set. While the evaluation of extraction tools (e.g. in information retrieval) usually requires that both precision and recall are high, ranking methods often put greater weight on high precision, possibly at the price of missing a considerable number of TPs. Moreover, when n-best lists of the same size are compared, precision and recall are fully equivalent.2 For these reasons, I will concentrate on the precision Π here. As an example, consider the identification of collocations from text corpora. Following the methodology described by Evert and Krenn (2001), German PP-verb combinations were extracted from a chunk-parsed version of the Frankfurter Rundschau Corpus.3 A cooccurrence frequency threshold of 2 Namely, Π = nTP · R/n, where nTP stands for the total number of TPs in the candidate set. 3 The Frankfurter Rundschau Corpus is a German newspaper corpus, comprising ca. 40 million words of text. It is part of the ECI Multilingual Corpus 1 distributed by ELSNET. For this 50 f ≥ 30 was applied, resulting in a candidate set of 5 102 PP-verb pairs. The candidates were then ranked according to the scores assigned by four association measures: the lo"
C04-1136,C00-2137,0,0.0233185,"ers understand the necessity of testing whether their results are statistically significant, but it is fairly unclear which tests are appropriate. For instance, Krenn (2000) applies the standard χ2 test to her comparative evaluation of collocation extraction methods. She is aware, though, that this test assumes independent samples and is hardly suitable for different ranking methods applied to the same candidate set: Krenn and Evert (2001) suggest several alternative tests for related samples. A wide range of exact and asymptotic tests as well as computationally intensive randomisation tests (Yeh, 2000) are available and add to the confusion about an appropriate choice. The aim of this paper is to formulate a statistical model that interprets the evaluation of ranking methods as a random experiment. This model defines the degree to which evaluation results are affected by random variation, allowing us to derive appropriate significance tests. After formalising the evaluation procedure in Section 2, I recast the procedure as a random experiment and make the underlying assumptions explicit (Section 3.1). On the basis of this model, I develop significance tests for the precision of a single ran"
C04-1136,C04-1123,0,\N,Missing
C14-2024,J10-4006,0,0.352582,"Missing"
C14-2024,P13-4006,0,0.0175345,"y, 2010), a modern reimplementation of the HAL model, and Semantic Vectors (Widdows and Cohen, 2010), which enforces a random indexing representation in order to improve scalability. A typical example of the second category is the S-Space package (Jurgens and Stevens, 2010), which defines a complete pipeline for building and evaluating a DSM; researchers wishing e.g. to evaluate the model on a different task need to implement the evaluation procedure in the form of a suitable Java ˇ uˇrek and Sojka, 2010) and class. Two Python-based software packages in this category are Gensim (Reh˚ DISSECT (Dinu et al., 2013), which has a particular focus on learning compositional models. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 110 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations, pages 110–114, Dublin, Ireland, August 23-29 2014. In order to avoid locking its users into a particular framework or class of distributional model, the wordspace package takes a different approach. It bu"
C14-2024,P10-4006,0,\N,Missing
E03-1080,P01-1025,1,0.786565,"idates are ranked by statistical measures based on their frequency ""profiles"". Usually, word pairs are considered likely to be collocations if their cooccurrence frequency is much higher than expected by chance. Authors typically evaluate the performance of a single collocation extraction system as a whole (e.g. Smadj a (1993)). A small number of in-depth comparative evaluations (mostly Daille (1994) and Krenn (2000)) concentrate on the quality of the statistical measures and the corresponding ranking of the candidates, and to a lesser extent on the performance of linguistic filters. Although Evert and Krenn (2001) are aware of the influence that the first extraction step has on their results, they fail to give a quantitative evaluation of different pre-processing and extraction methods. Our research aims to fill this gap. Currently, we are evaluating methods for the extraction of adjective+noun pairs from German newspaper text. It is planned to extend our work to other types of collocations, including PP+verb and noun+verb pairs. 83 2 Evaluation procedure With a collocation definition that is not based purely on observed frequencies, the statistical ranking of candidates has to be evaluated against a m"
E03-1080,kermes-evert-2002-yac,1,0.79721,"Missing"
E03-1080,mengel-lezius-2000-xml,0,0.0125345,"ations and noncollocational candidates. However, the annotation of tokens rather than types is a prohibitively laborious task. Fortunately, a German treebank corpus is available, from which the gold standard data can be extracted by automatic means. The Negra corpus l (Skut et al., 1998) consists of 355 096 tokens of German newspaper text with manually corrected part-of-speech tagging, morpho-syntactic annotations, and parse trees. We used XSLT stylesheets to extract a reference list of 19 771 instances of adjective+noun pairs from a version of the Negra corpus encoded in the TigerXML format (Mengel and Lezius, 2000). Unfortunately, the syntactic annotation scheme of the Negra treebank (Skut et al., 1997), which omits all projections that are not strictly necessary to determine the constituent structure of a sentence, is not very well suited for automatic extraction tasks. So far, we have only been able to extract adjective+noun pairs. We plan to use the TIGERSearch tool 2 in combination with stylesheets to obtain reference data for PP+verb and noun+verb pairs. 4 Pre - processing and extraction methods In addition to the hand-corrected part-of-speech tags in the Negra corpus, we used the IMS TreeTagger (S"
E03-1080,A97-1014,0,0.0114954,"prohibitively laborious task. Fortunately, a German treebank corpus is available, from which the gold standard data can be extracted by automatic means. The Negra corpus l (Skut et al., 1998) consists of 355 096 tokens of German newspaper text with manually corrected part-of-speech tagging, morpho-syntactic annotations, and parse trees. We used XSLT stylesheets to extract a reference list of 19 771 instances of adjective+noun pairs from a version of the Negra corpus encoded in the TigerXML format (Mengel and Lezius, 2000). Unfortunately, the syntactic annotation scheme of the Negra treebank (Skut et al., 1997), which omits all projections that are not strictly necessary to determine the constituent structure of a sentence, is not very well suited for automatic extraction tasks. So far, we have only been able to extract adjective+noun pairs. We plan to use the TIGERSearch tool 2 in combination with stylesheets to obtain reference data for PP+verb and noun+verb pairs. 4 Pre - processing and extraction methods In addition to the hand-corrected part-of-speech tags in the Negra corpus, we used the IMS TreeTagger (Schmid, 1994) for automatic tagging. http://www.coli.uni-sb.de/sfb378/negra-corpus/ 2 http:"
E03-1080,J93-1007,0,0.527345,"Missing"
E03-1080,J90-1003,0,\N,Missing
E17-2063,J10-4006,0,0.829458,"antic models. We evaluate dependencyfiltered and dependency-structured DSMs in a number of standard semantic similarity tasks, systematically exploring their parameter space in order to give them a “fair shot” against window-based models. Our results show that properly tuned window-based DSMs still outperform the dependencybased models in most tasks. There appears to be little need for the language-dependent resources and computational cost associated with syntactic analysis.1 1 Introduction Distributional semantic models (DSMs) based on syntactic dependency relations (Pad´o and Lapata, 2007; Baroni and Lenci, 2010) represent a more linguistically informed version of the widely-used window-based DSMs (Sahlgren, 2006; Bullinaria and Levy, 2007; Bullinaria and Levy, 2012). Both types of DSMs operationalize the meaning of a target word t as a set of co-occurrence patterns extracted from language corpora. While windowbased DSMs adopt a surface-oriented perspective (two words co-occur if they appear within a certain span, e.g. of 4 tokens), dependency-based DSMs adopt a syntactic perspective on co-occurrence: “nearness” is defined by the presence of a syntactic relation between target and features (e.g. direc"
E17-2063,P14-1023,0,0.335153,"r. These evaluation studies, however, were restricted to a specific corpus (BNC in Pad´o and Lapata (2007)) or task (noun clustering in Rothenh¨ausler and Sch¨utze (2009)), or based on a very specific notion of co-occurrence (Baroni and Lenci, 2010)2 . Meanwhile, extensive evaluation studies and parameter tuning led to significant improvements in the performance of window-based models (Bullinaria and Levy, 2007; Bullinaria and Levy, 2012; Lapesa and Evert, 2014) to the point that dependency-based DSMs currently hold the state-of-the-art only in very few standard semantic similarity tasks; see Baroni et al. (2014) and Lapesa and Evert (2014) for an overview of the state of the art. Among recent comparative evaluation studies, only Kiela and Clark (2014) attempt a direct comparison between the parameter spaces of window-based and syntax-based DSMs: once again, window-based models are found to perform better (with the exception of models built from the large Google Books N-gram corpus), but the scope of this comparison is rather limited. The aim of this paper is to establish a fair ground for the comparison between window-based and dependency-based DSMs. To that end, we take as a reference point the larg"
E17-2063,W15-0104,0,0.102245,"Missing"
E17-2063,D14-1082,0,0.0305399,"tasets. After extracting dependency paths from the source corpora, the DSMs were compiled using the UCS toolkit6 and the wordspace package for R (Evert, 2014). We evaluate the following parameters: Source corpus (abbreviated in the plots as corpus): BNC7 , WaCkypedia EN, and ukWaC8 ; Format of dependency relations (dep.style): Basic vs. collapsed with propagation of conjuncts (De Marneffe et al., 2006; De Marneffe and Manning, 2008); Annotation pipeline (parser): TreeTagger (Schmid, 1995) and MALT parser (Nivre, 2003) vs. bidirectional POS tagger and Neural Network parser of Stanford CoreNLP (Chen and Manning, 2014); Path length (path.length): we include paths with a maximum length of 1, 2, 3, 4 or 5 edges; Type of dependency relations (dep.type): paths composed only of core dependencies (main actants of the sentence) vs. paths that also allow external dependencies (inter-clausal relations and conjuncts); Threshold for context selection (orig.dim): we select the 5k, 10k, 20k, 50k, or 100k most frequent context dimensions; Score for feature weighting (score): frequency, tf.idf, Dice coefficient, simple log-likelihood, Mutual Information (MI), t-score, or z-score;9 Feature transformation (transformation):"
E17-2063,de-marneffe-etal-2006-generating,0,0.0184209,"Missing"
E17-2063,J10-4007,0,0.0770757,"Missing"
E17-2063,C14-2024,1,0.557996,"ncy-typed (Rothenh¨ausler and Sch¨utze, 2009; Baroni and Lenci, 2010) DSMs indicated that syntax-based semantic representations are indeed superior. These evaluation studies, however, were restricted to a specific corpus (BNC in Pad´o and Lapata (2007)) or task (noun clustering in Rothenh¨ausler and Sch¨utze (2009)), or based on a very specific notion of co-occurrence (Baroni and Lenci, 2010)2 . Meanwhile, extensive evaluation studies and parameter tuning led to significant improvements in the performance of window-based models (Bullinaria and Levy, 2007; Bullinaria and Levy, 2012; Lapesa and Evert, 2014) to the point that dependency-based DSMs currently hold the state-of-the-art only in very few standard semantic similarity tasks; see Baroni et al. (2014) and Lapesa and Evert (2014) for an overview of the state of the art. Among recent comparative evaluation studies, only Kiela and Clark (2014) attempt a direct comparison between the parameter spaces of window-based and syntax-based DSMs: once again, window-based models are found to perform better (with the exception of models built from the large Google Books N-gram corpus), but the scope of this comparison is rather limited. The aim of this"
E17-2063,W14-1503,0,0.127149,"enh¨ausler and Sch¨utze (2009)), or based on a very specific notion of co-occurrence (Baroni and Lenci, 2010)2 . Meanwhile, extensive evaluation studies and parameter tuning led to significant improvements in the performance of window-based models (Bullinaria and Levy, 2007; Bullinaria and Levy, 2012; Lapesa and Evert, 2014) to the point that dependency-based DSMs currently hold the state-of-the-art only in very few standard semantic similarity tasks; see Baroni et al. (2014) and Lapesa and Evert (2014) for an overview of the state of the art. Among recent comparative evaluation studies, only Kiela and Clark (2014) attempt a direct comparison between the parameter spaces of window-based and syntax-based DSMs: once again, window-based models are found to perform better (with the exception of models built from the large Google Books N-gram corpus), but the scope of this comparison is rather limited. The aim of this paper is to establish a fair ground for the comparison between window-based and dependency-based DSMs. To that end, we take as a reference point the large parameter set evaluated by 1 The analysis presented in this paper is complemented by supplementary materials, which are available for downlo"
E17-2063,W13-2608,1,0.815386,"be considered roughly equivalent to the comparison between undirected and directed windows in a bag-of-words DSM. All the other parameters are shared with window-based DSMs. Evaluation methodology We tested all possible combinations of the parameters described above, resulting in a total of 806400 runs per model class (filtered vs. typed), which were generated and evaluated on a large HPC cluster within approximately 6 weeks. To meaningfully interpret the evaluation results, we apply a model selection methodology that is sensitive to parameter interactions and robust to overfitting. Following Lapesa and Evert (2013), we analyze the influence of individual parameters and their interactions using general linear models with performance (accuracy, correlation, purity) as a dependent variable and the model parameters as independent variables, including all two-way interactions. Analysis of variance – which is straightforward for our full factorial design – is used to quantify the impact of each parameter or interaction. Robust optimal parameter settings are identified with the help of effect displays (Fox, 2003), which show the partial effect of one or two parameters by marginalizing over all other parameters"
E17-2063,Q14-1041,1,0.885216,"or dependency-typed (Rothenh¨ausler and Sch¨utze, 2009; Baroni and Lenci, 2010) DSMs indicated that syntax-based semantic representations are indeed superior. These evaluation studies, however, were restricted to a specific corpus (BNC in Pad´o and Lapata (2007)) or task (noun clustering in Rothenh¨ausler and Sch¨utze (2009)), or based on a very specific notion of co-occurrence (Baroni and Lenci, 2010)2 . Meanwhile, extensive evaluation studies and parameter tuning led to significant improvements in the performance of window-based models (Bullinaria and Levy, 2007; Bullinaria and Levy, 2012; Lapesa and Evert, 2014) to the point that dependency-based DSMs currently hold the state-of-the-art only in very few standard semantic similarity tasks; see Baroni et al. (2014) and Lapesa and Evert (2014) for an overview of the state of the art. Among recent comparative evaluation studies, only Kiela and Clark (2014) attempt a direct comparison between the parameter spaces of window-based and syntax-based DSMs: once again, window-based models are found to perform better (with the exception of models built from the large Google Books N-gram corpus), but the scope of this comparison is rather limited. The aim of this"
E17-2063,S14-1020,1,0.903763,"which are available for download at http://www.linguistik.fau.de/dsmeval/. 2 Among the dependency-based DSMs evaluated by Baroni and Lenci (2010), the best performing one relies on type-based co-occurrence: the co-occurrence strength between a target and a context is quantified as the number of different patterns in which they occur. 394 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 394–400, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics Lapesa and Evert (2014) and Lapesa et al. (2014) for window-based models. We carry out a parallel evaluation for dependency-based DSMs using the same tasks, datasets, parameters – adding some parameters specific to syntax-based models (such as the parser used and the type of allowed dependency relations) – and model selection methodology, allowing for a direct comparison of the results. We address the question of whether dependencybased models can significantly improve DSM performance if the parameters are properly set, and whether the degree of the improvement justifies the increased complexity of the extraction process. In either case, a"
E17-2063,P14-2050,0,0.0684835,"significantly improve DSM performance if the parameters are properly set, and whether the degree of the improvement justifies the increased complexity of the extraction process. In either case, a more thorough understanding of the parameter space will be beneficial for applications that prefer dependency-based DSMs on general grounds, e.g. because of an integration with syntactic structure (Erk et al., 2010). While the evaluation reported here does not encompass predict-type models, we believe that our findings also apply to the usefulness of dependency information in neural word embeddings (Levy and Goldberg, 2014). 2 Evaluation setting Tasks & Datasets Our evaluation covers all tasks and datasets used by Lapesa and Evert (2014) and Lapesa et al. (2014). For space reasons, we present detailed results for one representative dataset from each task3 : the TOEFL synonym test dataset (Landauer and Dumais, 1997) for the multiple-choice synonymy task (performance: accuracy); the Generalized Event Knowledge (McRae and Matzuki, 2009) dataset (GEK), a collection of 402 triples (target, consistent prime, inconsistent prime), for the multiple-choice semantic priming task (performance: accuracy)4 ; the WordSim-353 ("
E17-2063,W03-3017,0,0.120811,"Distributional Memory (Baroni and Lenci, 2010) and extended to cover all items in our datasets. After extracting dependency paths from the source corpora, the DSMs were compiled using the UCS toolkit6 and the wordspace package for R (Evert, 2014). We evaluate the following parameters: Source corpus (abbreviated in the plots as corpus): BNC7 , WaCkypedia EN, and ukWaC8 ; Format of dependency relations (dep.style): Basic vs. collapsed with propagation of conjuncts (De Marneffe et al., 2006; De Marneffe and Manning, 2008); Annotation pipeline (parser): TreeTagger (Schmid, 1995) and MALT parser (Nivre, 2003) vs. bidirectional POS tagger and Neural Network parser of Stanford CoreNLP (Chen and Manning, 2014); Path length (path.length): we include paths with a maximum length of 1, 2, 3, 4 or 5 edges; Type of dependency relations (dep.type): paths composed only of core dependencies (main actants of the sentence) vs. paths that also allow external dependencies (inter-clausal relations and conjuncts); Threshold for context selection (orig.dim): we select the 5k, 10k, 20k, 50k, or 100k most frequent context dimensions; Score for feature weighting (score): frequency, tf.idf, Dice coefficient, simple log-"
E17-2063,J07-2002,0,0.612936,"Missing"
E17-2063,W09-0203,0,0.069587,"Missing"
E17-2063,Q14-1020,0,0.0535187,"Missing"
evert-2004-statistical,evert-etal-2004-identifying,1,\N,Missing
evert-2008-lightweight,J03-3001,0,\N,Missing
evert-2008-lightweight,P01-1005,0,\N,Missing
evert-etal-2004-identifying,heid-etal-2004-tools,1,\N,Missing
evert-etal-2004-identifying,evert-2004-statistical,1,\N,Missing
kermes-evert-2002-yac,mengel-lezius-2000-xml,0,\N,Missing
kermes-evert-2002-yac,A97-1014,0,\N,Missing
N10-4006,P98-2127,0,0.0838595,"contexts in which words occur. DSMs are a promising technique for solving the lexical acquisition bottleneck by unsupervised learning, and their distributed representation provides a cognitively plausible, robust and flexible architecture for the organisation and processing of semantic information. Since the seminal papers of Landauer & Dumais (1997) and Schütze (1998), DSMs have been an active area of research in computational linguistics. Amongst many other tasks, they have been applied to solving the TOEFL synonym test (Landauer & Dumais 1997, Rapp 2004), automatic thesaurus construction (Lin 1998), identification of translation equivalents (Rapp 1999), word sense induction and discrimination (Schütze 1998), POS induction (Schütze 1995), identification of analogical relations (Turney 2006), PP attachment disambiguation (Pantel & Lin 2000), semantic classification (Versley 2008), as well as the prediction of fMRI (Mitchell et al. 2008) and EEG (Murphy et al. 2009) data. Recent years have seen renewed and rapidly growing interest in distributional approaches, as shown by the series of workshops on DSM held at Context 2007 [1], ESSLLI 2008 [2], EACL 2009 [3], CogSci 2009 [4], NAACL-HLT 201"
N10-4006,D09-1065,0,0.011431,"tze (1998), DSMs have been an active area of research in computational linguistics. Amongst many other tasks, they have been applied to solving the TOEFL synonym test (Landauer & Dumais 1997, Rapp 2004), automatic thesaurus construction (Lin 1998), identification of translation equivalents (Rapp 1999), word sense induction and discrimination (Schütze 1998), POS induction (Schütze 1995), identification of analogical relations (Turney 2006), PP attachment disambiguation (Pantel & Lin 2000), semantic classification (Versley 2008), as well as the prediction of fMRI (Mitchell et al. 2008) and EEG (Murphy et al. 2009) data. Recent years have seen renewed and rapidly growing interest in distributional approaches, as shown by the series of workshops on DSM held at Context 2007 [1], ESSLLI 2008 [2], EACL 2009 [3], CogSci 2009 [4], NAACL-HLT 2010 [5], ACL 2010 [6] and ESSLLI 2010 [7]. The proposed tutorial aims to - introduce the most common DSM architectures and their parameters, as well as prototypical applications; - equip participants with the mathematical techniques needed for the implementation of DSMs, in particular those of matrix algebra; - illustrate visualisation techniques and mathematical argument"
N10-4006,P00-1014,0,0.0323717,"ure for the organisation and processing of semantic information. Since the seminal papers of Landauer & Dumais (1997) and Schütze (1998), DSMs have been an active area of research in computational linguistics. Amongst many other tasks, they have been applied to solving the TOEFL synonym test (Landauer & Dumais 1997, Rapp 2004), automatic thesaurus construction (Lin 1998), identification of translation equivalents (Rapp 1999), word sense induction and discrimination (Schütze 1998), POS induction (Schütze 1995), identification of analogical relations (Turney 2006), PP attachment disambiguation (Pantel & Lin 2000), semantic classification (Versley 2008), as well as the prediction of fMRI (Mitchell et al. 2008) and EEG (Murphy et al. 2009) data. Recent years have seen renewed and rapidly growing interest in distributional approaches, as shown by the series of workshops on DSM held at Context 2007 [1], ESSLLI 2008 [2], EACL 2009 [3], CogSci 2009 [4], NAACL-HLT 2010 [5], ACL 2010 [6] and ESSLLI 2010 [7]. The proposed tutorial aims to - introduce the most common DSM architectures and their parameters, as well as prototypical applications; - equip participants with the mathematical techniques needed for the"
N10-4006,P99-1067,0,0.0756085,"echnique for solving the lexical acquisition bottleneck by unsupervised learning, and their distributed representation provides a cognitively plausible, robust and flexible architecture for the organisation and processing of semantic information. Since the seminal papers of Landauer & Dumais (1997) and Schütze (1998), DSMs have been an active area of research in computational linguistics. Amongst many other tasks, they have been applied to solving the TOEFL synonym test (Landauer & Dumais 1997, Rapp 2004), automatic thesaurus construction (Lin 1998), identification of translation equivalents (Rapp 1999), word sense induction and discrimination (Schütze 1998), POS induction (Schütze 1995), identification of analogical relations (Turney 2006), PP attachment disambiguation (Pantel & Lin 2000), semantic classification (Versley 2008), as well as the prediction of fMRI (Mitchell et al. 2008) and EEG (Murphy et al. 2009) data. Recent years have seen renewed and rapidly growing interest in distributional approaches, as shown by the series of workshops on DSM held at Context 2007 [1], ESSLLI 2008 [2], EACL 2009 [3], CogSci 2009 [4], NAACL-HLT 2010 [5], ACL 2010 [6] and ESSLLI 2010 [7]. The proposed t"
N10-4006,rapp-2004-freely,0,0.0162737,"paces -- through a statistical analysis of the contexts in which words occur. DSMs are a promising technique for solving the lexical acquisition bottleneck by unsupervised learning, and their distributed representation provides a cognitively plausible, robust and flexible architecture for the organisation and processing of semantic information. Since the seminal papers of Landauer & Dumais (1997) and Schütze (1998), DSMs have been an active area of research in computational linguistics. Amongst many other tasks, they have been applied to solving the TOEFL synonym test (Landauer & Dumais 1997, Rapp 2004), automatic thesaurus construction (Lin 1998), identification of translation equivalents (Rapp 1999), word sense induction and discrimination (Schütze 1998), POS induction (Schütze 1995), identification of analogical relations (Turney 2006), PP attachment disambiguation (Pantel & Lin 2000), semantic classification (Versley 2008), as well as the prediction of fMRI (Mitchell et al. 2008) and EEG (Murphy et al. 2009) data. Recent years have seen renewed and rapidly growing interest in distributional approaches, as shown by the series of workshops on DSM held at Context 2007 [1], ESSLLI 2008 [2],"
N10-4006,E95-1020,0,0.128858,"nd their distributed representation provides a cognitively plausible, robust and flexible architecture for the organisation and processing of semantic information. Since the seminal papers of Landauer & Dumais (1997) and Schütze (1998), DSMs have been an active area of research in computational linguistics. Amongst many other tasks, they have been applied to solving the TOEFL synonym test (Landauer & Dumais 1997, Rapp 2004), automatic thesaurus construction (Lin 1998), identification of translation equivalents (Rapp 1999), word sense induction and discrimination (Schütze 1998), POS induction (Schütze 1995), identification of analogical relations (Turney 2006), PP attachment disambiguation (Pantel & Lin 2000), semantic classification (Versley 2008), as well as the prediction of fMRI (Mitchell et al. 2008) and EEG (Murphy et al. 2009) data. Recent years have seen renewed and rapidly growing interest in distributional approaches, as shown by the series of workshops on DSM held at Context 2007 [1], ESSLLI 2008 [2], EACL 2009 [3], CogSci 2009 [4], NAACL-HLT 2010 [5], ACL 2010 [6] and ESSLLI 2010 [7]. The proposed tutorial aims to - introduce the most common DSM architectures and their parameters, as"
N10-4006,J98-1004,0,0.146227,"least to a certain extent) be inferred from its usage, i.e. its distribution in text. Therefore, these models dynamically build semantic representations -- in the form of highdimensional vector spaces -- through a statistical analysis of the contexts in which words occur. DSMs are a promising technique for solving the lexical acquisition bottleneck by unsupervised learning, and their distributed representation provides a cognitively plausible, robust and flexible architecture for the organisation and processing of semantic information. Since the seminal papers of Landauer & Dumais (1997) and Schütze (1998), DSMs have been an active area of research in computational linguistics. Amongst many other tasks, they have been applied to solving the TOEFL synonym test (Landauer & Dumais 1997, Rapp 2004), automatic thesaurus construction (Lin 1998), identification of translation equivalents (Rapp 1999), word sense induction and discrimination (Schütze 1998), POS induction (Schütze 1995), identification of analogical relations (Turney 2006), PP attachment disambiguation (Pantel & Lin 2000), semantic classification (Versley 2008), as well as the prediction of fMRI (Mitchell et al. 2008) and EEG (Murphy et"
N10-4006,J06-3003,0,0.0151056,"ely plausible, robust and flexible architecture for the organisation and processing of semantic information. Since the seminal papers of Landauer & Dumais (1997) and Schütze (1998), DSMs have been an active area of research in computational linguistics. Amongst many other tasks, they have been applied to solving the TOEFL synonym test (Landauer & Dumais 1997, Rapp 2004), automatic thesaurus construction (Lin 1998), identification of translation equivalents (Rapp 1999), word sense induction and discrimination (Schütze 1998), POS induction (Schütze 1995), identification of analogical relations (Turney 2006), PP attachment disambiguation (Pantel & Lin 2000), semantic classification (Versley 2008), as well as the prediction of fMRI (Mitchell et al. 2008) and EEG (Murphy et al. 2009) data. Recent years have seen renewed and rapidly growing interest in distributional approaches, as shown by the series of workshops on DSM held at Context 2007 [1], ESSLLI 2008 [2], EACL 2009 [3], CogSci 2009 [4], NAACL-HLT 2010 [5], ACL 2010 [6] and ESSLLI 2010 [7]. The proposed tutorial aims to - introduce the most common DSM architectures and their parameters, as well as prototypical applications; - equip participan"
N10-4006,C98-2122,0,\N,Missing
P01-1025,J93-1003,0,0.0703324,"on of lexical association mea1 See for instance (Manning and Schütze, 1999, chapter 5), (Kilgarriff, 1996), and (Pedersen, 1996). Brigitte Krenn Austrian Research Institute for Artificial Intelligence (ÖFAI) Schottengasse 3 A-1010 Vienna, Austria brigitte@ai.univie.ac.at sures (AMs). Based on these requirements, we introduce an experimentation procedure, and discuss the evaluation results for a number of widely used AMs. Finally, methods and strategies for handling low-frequency data are suggested. ) The measures2 – Mutual Information ( (Church and Hanks, 1989), the log-likelihood ratio test (Dunning, 1993), two statistical tests: t-test and -test, and co-occurrence frequency – are applied to two sets of data: adjective-noun (AdjN) pairs and preposition-noun-verb (PNV) triples, where the AMs are applied to (PN,V) pairs. See section 3 for a description of the base data. For evaluation of the association measures, -best strategies (section 4.1) are supplemented with precision and recall graphs (section 4.2) over the complete data sets. Samples comprising particular frequency strata (high versus low frequencies) are examined (section 4.3). In section 5, methods for the treatment of low-frequency da"
P01-1025,J00-3001,0,0.0171173,"Missing"
P01-1025,P89-1010,0,\N,Missing
P07-1114,C00-1027,0,\N,Missing
Q14-1041,J10-4006,0,0.362956,"ed with an additional transformation; whether to apply dimensionality reduction, and the number of reduced dimensions; metric for measuring distances between vectors. Different design choices – technically, the DSM parameters – can result in quite different similarities for the same words (Sahlgren, 2006). DSMs have already proven successful in modeling lexical meaning: they have been applied in Natural Language Processing (Sch¨utze, 1998; Lin, 1998), Information Retrieval (Salton et al., 1975), and Cognitive Modeling (Landauer and Dumais, 1997; Lund and Burgess, 1996; Pad´o and Lapata, 2007; Baroni and Lenci, 2010). Recently, the field of Distributional Semantics has moved towards new challenges, such as predicting brain activation (Mitchell et al., 2008; Murphy et al., 2012; Bullinaria and Levy, 2013) and modeling meaning composition (Baroni et al., 2014, and references therein). Despite such progress, a full understanding of the different parameters governing a DSM and their influence on model performance has not been achieved yet. The present paper is a contribution towards this 531 Transactions of the Association for Computational Linguistics, vol. 2, pp. 531–545, 2014. Action Editor: Janyce Wiebe."
Q14-1041,D10-1115,0,0.0475095,"beyond 20000 or 50000 dimensions is rarely sufficient to justify the increased processing cost. • A novel contribution of our work is the systematic evaluation of a parameter that has been given little attention in DSM research so far: the index of distributional relatedness. Our results show that, even if the parameter is not among the most influential ones, neighbor rank consistently outperforms distance. Without SVD dimensionality reduction, the difference is more pronounced: this result is particularly interesting for compositionality tasks, where SVD has been reported to be detrimental (Baroni and Zamparelli, 2010). In such cases, the benefits of using neighbor rank clearly outweigh the increased (but manageable) computational complexity. Ongoing work focuses on the extension of the evaluation setting to further parameters (e.g., new distance metrics and association scores, Caron’s (2001) exponent p) and tasks (e.g., compositionality tasks, meaning in context), as well as the evaluation of dependency-based models. We are also working on a refined model selection methodology involving a systematic analysis of three-way interactions and the exclusion of inferior parameter values (such as Manhattan distanc"
Q14-1041,2014.lilt-9.5,0,0.00561879,"ifferent similarities for the same words (Sahlgren, 2006). DSMs have already proven successful in modeling lexical meaning: they have been applied in Natural Language Processing (Sch¨utze, 1998; Lin, 1998), Information Retrieval (Salton et al., 1975), and Cognitive Modeling (Landauer and Dumais, 1997; Lund and Burgess, 1996; Pad´o and Lapata, 2007; Baroni and Lenci, 2010). Recently, the field of Distributional Semantics has moved towards new challenges, such as predicting brain activation (Mitchell et al., 2008; Murphy et al., 2012; Bullinaria and Levy, 2013) and modeling meaning composition (Baroni et al., 2014, and references therein). Despite such progress, a full understanding of the different parameters governing a DSM and their influence on model performance has not been achieved yet. The present paper is a contribution towards this 531 Transactions of the Association for Computational Linguistics, vol. 2, pp. 531–545, 2014. Action Editor: Janyce Wiebe. c Submission batch: 3/2014; Revision batch 9/2014; Published 12/2014. 2014 Association for Computational Linguistics. goal: it presents the results of a large-scale evaluation of window-based DSMs on a wide variety of semantic tasks. More comple"
Q14-1041,C14-2024,1,0.764073,"LLI 2008 set, containing 44 concrete nouns grouped into 6 classes;5 and the Mitchell set, containing 60 nouns grouped into 12 classes (Mitchell et al., 2008), also employed by Bullinaria and Levy (2012). 3.2 Parameters DSMs evaluated in this paper belong to the class of window-based models. All models use the same large vocabulary of target words (27522 lemma types), which is based on the vocabulary of Distributional Memory (Baroni and Lenci, 2010) and has been extended to cover all items in our datasets. Distributional models were built using the UCS toolkit6 and the wordspace package for R (Evert, 2014). The following parameters have been evaluated:7 • Source Corpus (abbreviated in the plots as corpus): the corpora from which we compiled our DSMs differ in both size and quality, and they represent standard choices in DSM evaluation. Evaluated corpora in this study are: British National Corpus8 ; ukWaC; WaCkypedia EN9 ; • Context window: – Direction* (win.direction): we collected cooccurrence counts both using a directed window (i.e., separate co-occurrence counts for 4 Other clustering studies have often been carried out using the CLUTO toolkit (Karypis, 2003) with standard settings, which c"
Q14-1041,W14-1503,0,0.394298,"arameter tuning, and to compare it to competing models; examples are Pado and Lapata’s (2007) Dependency Vectors as well as Baroni and Lenci’s (2010) Distributional Memory. Since both studies focus on testing a single new model with fixed parameters (or a small number of new models), we will not go into further detail concerning them. Alternatively, the evaluation may be conducted via incremental tuning of parameters, which are tested sequentially to identify their best performing values on a number of tasks, as has been done by Bullinaria and Levy (2007; 2012), Polajnar and Clark (2014), and Kiela and Clark (2014). Bullinaria and Levy (2007) report on a systematic study of the impact of a number of parameters (shape and size of the co-occurrence window, distance metric, association score for co-occurrence counts) on a number of tasks (including the TOEFL synonym task, which is also evaluated in our study). Evaluated models were based on the British National Corpus. Bullinaria and Levy (2007) found that vectors scored with Pointwise Mutual Information, built from very small context windows with as many context dimensions as possible, and using cosine distance ensured the best performance across all task"
Q14-1041,W13-2608,1,0.745442,"ters for the underlying word-level DSMs. At the level of parameter coverage, this work evaluates most of the relevant parameters considered in comparable state-of-the-art studies (Bullinaria and Levy, 2007; Bullinaria and Levy, 2012); it also introduces an additional one, which has received little attention in the literature: the index of distributional relatedness, which connects distances in the DSM space to semantic similarity. We compare direct use of distance measures to neighbor rank. Neighbor rank has already been successfully used to model priming effects with DSMs (Hare et al., 2009; Lapesa and Evert, 2013); the present study extends its evaluation to standard tasks. We show that neighbor rank consistently improves the performance of DSMs compared to distance, but the degree of this improvement varies from task to task. At the level of task coverage, the present study includes most of the standard datasets used in comparative studies (Bullinaria and Levy, 2007; Baroni and Lenci, 2010; Bullinaria and Levy, 2012). We consider three types of evaluation tasks: multiple choice (TOEFL test), correlation to human similarity ratings, and semantic clustering. At the level of methodology, our work adopts"
Q14-1041,S14-1020,1,0.933176,"number of context dimensions, use of stemming, lemmatization and stopwords, similarity metric, score for feature weighting. Best results were obtained with large corpora and small window sizes, around 50000 context dimensions, stemming, Positive Mutual Information, and a mean-adjusted version of cosine distance. Even though we adopt a different approach than these incremental tuning studies, there is considerable overlap in the evaluated parameters and tasks, which will be pointed out in section 3. An alternative to incremental tuning is the methodology proposed by Lapesa and Evert (2013) and Lapesa et al. (2014). They systematically test a large number of parameter combinations and use linear regression to determine the importance of individual parameters and their interactions. As their evaluation methodology is adopted in the present work and described in more detail in section 4, we will not discuss it here and instead focus on the main results. DSMs are evaluated in the task of modeling semantic priming. This task, albeit not standard in DSM evaluation, is of great interest as priming experiments provide a window into the structure of the mental lexicon. Both studies showed that neighbor rank out"
Q14-1041,P98-2127,0,0.0445548,"nvolves many design choices, such as: selection of a source corpus, size of the co-occurrence window; choice of a suitable scoring function, possibly combined with an additional transformation; whether to apply dimensionality reduction, and the number of reduced dimensions; metric for measuring distances between vectors. Different design choices – technically, the DSM parameters – can result in quite different similarities for the same words (Sahlgren, 2006). DSMs have already proven successful in modeling lexical meaning: they have been applied in Natural Language Processing (Sch¨utze, 1998; Lin, 1998), Information Retrieval (Salton et al., 1975), and Cognitive Modeling (Landauer and Dumais, 1997; Lund and Burgess, 1996; Pad´o and Lapata, 2007; Baroni and Lenci, 2010). Recently, the field of Distributional Semantics has moved towards new challenges, such as predicting brain activation (Mitchell et al., 2008; Murphy et al., 2012; Bullinaria and Levy, 2013) and modeling meaning composition (Baroni et al., 2014, and references therein). Despite such progress, a full understanding of the different parameters governing a DSM and their influence on model performance has not been achieved yet. The"
Q14-1041,P08-1028,0,0.226848,"Missing"
Q14-1041,S12-1019,0,0.01057,"Different design choices – technically, the DSM parameters – can result in quite different similarities for the same words (Sahlgren, 2006). DSMs have already proven successful in modeling lexical meaning: they have been applied in Natural Language Processing (Sch¨utze, 1998; Lin, 1998), Information Retrieval (Salton et al., 1975), and Cognitive Modeling (Landauer and Dumais, 1997; Lund and Burgess, 1996; Pad´o and Lapata, 2007; Baroni and Lenci, 2010). Recently, the field of Distributional Semantics has moved towards new challenges, such as predicting brain activation (Mitchell et al., 2008; Murphy et al., 2012; Bullinaria and Levy, 2013) and modeling meaning composition (Baroni et al., 2014, and references therein). Despite such progress, a full understanding of the different parameters governing a DSM and their influence on model performance has not been achieved yet. The present paper is a contribution towards this 531 Transactions of the Association for Computational Linguistics, vol. 2, pp. 531–545, 2014. Action Editor: Janyce Wiebe. c Submission batch: 3/2014; Revision batch 9/2014; Published 12/2014. 2014 Association for Computational Linguistics. goal: it presents the results of a large-scal"
Q14-1041,J07-2002,0,0.0510769,"Missing"
Q14-1041,E14-1025,0,0.0871719,"tasks, applying little or no parameter tuning, and to compare it to competing models; examples are Pado and Lapata’s (2007) Dependency Vectors as well as Baroni and Lenci’s (2010) Distributional Memory. Since both studies focus on testing a single new model with fixed parameters (or a small number of new models), we will not go into further detail concerning them. Alternatively, the evaluation may be conducted via incremental tuning of parameters, which are tested sequentially to identify their best performing values on a number of tasks, as has been done by Bullinaria and Levy (2007; 2012), Polajnar and Clark (2014), and Kiela and Clark (2014). Bullinaria and Levy (2007) report on a systematic study of the impact of a number of parameters (shape and size of the co-occurrence window, distance metric, association score for co-occurrence counts) on a number of tasks (including the TOEFL synonym task, which is also evaluated in our study). Evaluated models were based on the British National Corpus. Bullinaria and Levy (2007) found that vectors scored with Pointwise Mutual Information, built from very small context windows with as many context dimensions as possible, and using cosine distance ensured the best"
Q14-1041,W09-0203,0,0.108783,"Missing"
Q14-1041,J98-1004,0,0.0952959,"Missing"
Q14-1041,W12-5105,0,0.014752,"ding that sparse association measures (with negative values clamped to zero) achieve the best performance can be connected to the positive impact of context selection highlighted by Polajnar and Clark (2014): ongoing work targets a more specific analysis of their “thinning” effect on distributional vectors. • Another group of parameters (corpus, window size, dimensionality reduction parameters) is also influential in all tasks, but shows more variation wrt. the best parameter values. Except for the TOEFL task, best results are obtained with the WaCkypedia corpus, confirming the observation of Sridharan and Murphy (2012) that corpus quality compensates for size to some extent. Window size and dimensionality reduction show a more task-specific behavior, even though it is possible to find a good compromise in a 4 word window, a reduced space of 500 dimensions and skipping of the first 50 dimensions. The latter result confirms the findings of Bullinaria and Levy (2007; 2012) in their clustering experiments. • The number of context dimensions turned out to be less crucial. While very high-dimensional spaces usually result in better performance, the increase beyond 20000 or 50000 dimensions is rarely sufficient to"
Q14-1041,N12-1077,0,0.0200618,"Best Settings Dataset TOEFL RATINGS CLUSTERING GENERAL TOEFL 92.5 85.0 75.0 90.0 RG65 0.84 0.86 0.84 0.87 WS353 0.62 0.67 0.64 0.68 AP 0.62 0.66 0.67 0.67 BATTIG 0.87 0.91 0.98 0.90 ESSLLI 0.66 0.77 0.80 0.77 MITCHELL 0.75 0.83 0.88 0.83 Table 7: General best Settings – Performance 6 Conclusion In this paper, we reported the results of a large-scale evaluation of window-based Distributional Semantic Models, involving a wide range of parameters and tasks. Our model selection methodology is robust to overfitting and sensitive to parameter interactions. 18 The ACL wiki lists the hybrid model of Yih and Qazvinian (2012) as the best model on RG65 with ρ = 0.89, but does not specify its Pearson correlation r. In our comparison table, we show the best Pearson correlation, achieved by Hassan and Mihalcea (2011), which is also the best corpus-based model. 19 Halawi et al. (2012) report Spearman’s ρ. The ρ values for our best setting are: RG65: 0.85, WS353: 0.70; best setting for the ratings task: RG65: 0.82, WS353: 0.67; best general setting: RG65: 0.87, WS353: 0.70. 20 http://wordspace.collocations.de/ 542 It allowed us to identify parameter configurations that perform well across different datasets within the s"
Q14-1041,C14-1163,0,0.0245291,"Missing"
Q14-1041,J10-1001,0,\N,Missing
Q14-1041,C98-2122,0,\N,Missing
S13-1026,S12-1051,0,0.057863,"sure semantic similarity can be beneficial for many applications, e.g. in the domains of MT evaluation, information extraction, question answering, and summarization. For the shared task, STS was measured on a scale ranging from 0 (indicating no similarity at all) to 5 (semantic equivalence). The system predictions were evaluated against manually annotated data. 2 Description of our approach Our system KLUE-CORE uses two approaches to estimate STS between pairs of sentences: a distriThe training data We trained our system on manually annotated sentence pairs from the STS task at SemEval 2012 (Agirre et al., 2012). Pooling the STS 2012 training and test data, we obtained 5 data sets from different domains, comprising a total of 5 343 sentence pairs annotated with a semantic similarity score in the range [0, 5]. The data sets are paraphrase sentence pairs (MSRpar), sentence pairs from video descriptions (MSRvid), MT evaluation sentence pairs (MTnews and MTeuroparl), and glosses from two different lexical semantic resources (OnWN). All sentence pairs were pre-processed with TreeTagger (Schmid, 1995)1 for part-of-speech annotation and lemmatization. 1 http://www.ims.uni-stuttgart.de/forschung/ ressourcen/"
S13-1026,S13-1004,0,0.0352157,"90 submissions from 35 different teams. Given the simple nature of our approach, which uses only WordNet and unannotated corpus data as external resources, we consider this a remarkably good result, making the system an interesting tool for a wide range of practical applications. All similarity scores obtained in this way were passed to a ridge regression learner in order to obtain a final STS score. The predictions for new sentence pairs were then transformed to the range [0, 5], as required by the task definition. 2.1 1 Introduction The *SEM 2013 shared task on Semantic Textual Similarity (Agirre et al., 2013) required participants to implement a software system that is able to predict the semantic textual similarity (STS) of sentence pairs. Being able to reliably measure semantic similarity can be beneficial for many applications, e.g. in the domains of MT evaluation, information extraction, question answering, and summarization. For the shared task, STS was measured on a scale ranging from 0 (indicating no similarity at all) to 5 (semantic equivalence). The system predictions were evaluated against manually annotated data. 2 Description of our approach Our system KLUE-CORE uses two approaches to"
S13-1026,J10-4006,0,0.0445064,"a completely unsupervised manner from distributional semantic models. 2.2.1 WordNet We computed three state-of-the-art WordNet similarity measures, namely path similarity, Wu-Palmer similarity and Leacock-Chodorow similarity (Budanitsky and Hirst, 2006). As usual, for each pair of words the synsets with the highest similarity score were selected. For all three measures, we made use of the implementations provided as part of the Natural Language ToolKit for Python (Bird et al., 2009). 2.2.2 Distributional semantics Word similarity scores were also obtained from two DSM: Distributional Memory (Baroni and Lenci, 2010) and a model compiled from a version of the English Wikipedia.2 For Distributional Memory, we chose the collapsed W ×W matricization, resulting in a 30 686 × 30 686 matrix that was further reduced to 300 latent dimensions using randomized SVD (Halko et al., 2009). For the Wikipedia DSM, we used a L2/R2 context window and mid-frequency feature terms, resulting in a 77 598 × 30 484 matrix. Co-occurrence frequency counts were weighted using sparse log-likelihood association scores with a square root transformation, and reduced to 300 latent dimensions with randomized SVD. In both cases, target te"
S13-1026,J06-1003,0,0.217355,"1: Proceedings of the Main Conference c and the Shared Task, pages 181–186, Atlanta, Georgia, June 13-14, 2013. 2013 Association for Computational Linguistics 2.2 Similarity on word level Our alignment model (Sec. 2.3.1) is based on similarity scores for pairs of words. We obtained a total of 11 different word similarity measures from WordNet (Miller et al., 1990) and in a completely unsupervised manner from distributional semantic models. 2.2.1 WordNet We computed three state-of-the-art WordNet similarity measures, namely path similarity, Wu-Palmer similarity and Leacock-Chodorow similarity (Budanitsky and Hirst, 2006). As usual, for each pair of words the synsets with the highest similarity score were selected. For all three measures, we made use of the implementations provided as part of the Natural Language ToolKit for Python (Bird et al., 2009). 2.2.2 Distributional semantics Word similarity scores were also obtained from two DSM: Distributional Memory (Baroni and Lenci, 2010) and a model compiled from a version of the English Wikipedia.2 For Distributional Memory, we chose the collapsed W ×W matricization, resulting in a 30 686 × 30 686 matrix that was further reduced to 300 latent dimensions using ran"
S13-1026,J98-1004,0,0.242204,"e word alignment model where each word in the shorter sentence is aligned to the semantically most similar word in the longer sentence (short), and vice versa (long). Note that multiple source words may be aligned to the same target word, and target words can remain unaligned without penalty. Semantic similarities are then averaged across all alignment pairs. In total, we obtained 22 sentence similarity scores from this approach. 2.3.2 Distributional similarity We computed distributional similarity between the sentences in each pair directly using bag-of-words centroid vectors as suggested by Schütze (1998), based on the two word-level DSM introduced in Sec. 2.2.2. For each sentence pair and DSM, we computed (i) the angle between the centroid vectors of the two sentences and (ii) a z-score relative to all other sentences in the same data set of the training or test collection. Both values are measures of semantic distance, but are automatically transformed into similarity measures by the regression learner (Sec. 2.4). For the z-scores, we computed the semantic distance (i.e. angle) between the first sentence of a given pair and the second sentences of all word pairs in the same data set. The res"
S13-2065,P11-2008,0,0.0206663,"Missing"
S13-2065,S13-2052,0,\N,Missing
S14-1020,W09-3207,0,0.0700477,"Missing"
S14-1020,J10-4006,0,0.615986,"s. syntagmatic relations. 1 Introduction Distributional takes on the representation and acquisition of word meaning rely on the assumption that words with similar meaning tend to occur in similar contexts: this assumption, known as distributional hypothesis, has been first proposed by Harris (1954). Distributional Semantic Models (henceforth, DSMs) are computational models that operationalize the distributional hypothesis; they produce semantic representations for words in the form of distributional vectors recording patterns of co-occurrence in large samples of language data (Sahlgren, 2006; Baroni and Lenci, 2010; Turney and Pantel, 2010). Comparison between distributional vectors allows the identification of shared contexts as an empirical correlate of 160 Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 160–170, Dublin, Ireland, August 23-24 2014. labels syntagmatic and paradigmatic to characterize different types of semantic relations, and we will use the labels first-order and second-order to characterize corpus-based models with respect to the kind of co-occurrence information they encode. We will refer to collocation lists and termdocument DSMs"
S14-1020,W11-2501,0,0.0505922,"different semantic relations, namely: synonyms (SYN), 436 triples (example of a consistent prime and target: frigid–cold); antonyms (ANT): 135 triples (e.g., hot–cold); cohyponyms (COH): 159 triples (e.g., table–chair); forward phrasal associates (FPA): 144 triples (e.g., help–wanted); back2 3.2 Evaluated Parameters DSMs evaluated in this paper belong to the class of bag-of-words models. We defined a large vocabulary of target words (27522 lemma types) containing all the items from the evaluated datasets as well as items from other state-of-the-art evaluation studies (Baroni and Lenci, 2010; Baroni and Lenci, 2011). Context words were filtered by partof-speech (nouns, verbs, adjectives, and adverbs). Distributional models were built using the UCS toolkit3 and the wordspace package for R4 . The following parameters have been evaluated: • Source corpus (abbreviated as corpus in plots 1-4): We compiled DSMs from three corpora often used in DSM evaluation studies and that 3 4 The dataset is available at http://spp.montana.edu/ 162 http://www.collocations.de/software.html http://r-forge.r-project.org/projects/wordspace/ differ in both size and quality: British National Corpus5 , ukWaC, and WaCkypedia EN6 . •"
S14-1020,D10-1115,0,0.0421147,"matic Paradigmatic Paradigmatic Paradigmatic GEK FPA BPA SYN COH ANT Unreduced Min Max Mean 54.8 98.4 86.6 41.0 98.0 82.3 49.4 97.7 83.8 54.8 98.4 86.6 49.0 100.0 92.6 69.6 100.0 94.2 Reduced Min Max Mean 48.0 97.0 80.8 43.0 98.6 82.1 41.6 98.9 83.9 57.3 99.0 88.2 54.3 100.0 94.0 57.8 100.0 94.3 Table 2: Distribution of Accuracy alization encoded in the reduced dimensions) is irrelevant to other tasks, but crucial for modeling the relations in the GEK dataset. This interpretation is consistent with the detrimental effect of SVD in tasks involving vector composition reported in the literature (Baroni and Zamparelli, 2010). 5.1 Importance of Parameters To obtain further insights into DSM performance we explore the effect of specific model parameters, comparing syntagmatic vs. paradigmatic relations and reduced vs. unreduced runs. In order to establish a ranking of the parameters according to their importance wrt. model performance, we use a feature ablation approach. The ablation value for a given parameter is the proportion of variance (R2 ) explained by this parameter together with all its interactions, corresponding to the reduction in adjusted R2 of the linear model fit if the parameter were left out. In ot"
S14-1020,W13-2608,1,0.935349,"Summing up, in both Rapp (2002) and Sahlgren (2006) it is claimed that second-order models perform poorly in predicting syntagmatic relations. However, neither of those studies involves datasets containing exclusively syntagmatic relations, as the evaluation focuses either on paradigmatic relations (TOEFL multiple choice test, antonymy test) or on resources containing both types of relations (thesauri, association norms). 3 ward phrasal associates (BPA): 89 triples (e.g., wanted–help). The second priming dataset is the Generalized Event Knowledge dataset (henceforth GEK), already evaluated in Lapesa and Evert (2013): a collection of 402 triples (target, consistent prime, inconsistent prime) from three priming studies conducted to demonstrate that event knowledge is responsible for facilitation of the processing of words that denote events and their participants. The first study was conducted by Ferretti et al. (2001), who found that verbs facilitate the processing of nouns denoting prototypical participants in the depicted event and of adjectives denoting features of prototypical participants. The study covered five thematic relations: agent (e.g., pay–customer), patient, feature of the patient, instrume"
S14-1020,S12-1012,0,0.0190868,"lection methodology. Section 5 presents the results of our evaluation study. Section 6 summarizes main findings and sketches ongoing and future work. 2 Previous Work In this section we discuss previous work relevant to the distributional modeling of paradigmatic and syntagmatic relations. For space constraints, we focus only on two studies (Rapp, 2002; Sahlgren, 2006) in which the two classes of relations are compared at a global level, and not on studies that are concerned with specific semantic relations, e.g., synonymy (Edmonds and Hirst, 2002; Curran, 2003), hypernymy (Weeds et al., 2004; Lenci and Benotto, 2012) or syntagmatic predicate preferences (McCarthy and Carroll, 2003; Erk et al., 2010), etc. In previous studies, the comparison of syntagmatic and paradigmatic relations has been implemented in terms of an opposition between different classes of corpus-based models: term-context models (words as targets, documents or context regions as features) vs. bag-of-words models (words as targets and features) in Sahlgren (2006); collocation lists vs. bag-of-words models in Rapp (2002). Given the high terminological variation in the literature, in this paper we will adopt the 1 Term-document models encod"
S14-1020,J03-4004,0,0.0554907,"uation study. Section 6 summarizes main findings and sketches ongoing and future work. 2 Previous Work In this section we discuss previous work relevant to the distributional modeling of paradigmatic and syntagmatic relations. For space constraints, we focus only on two studies (Rapp, 2002; Sahlgren, 2006) in which the two classes of relations are compared at a global level, and not on studies that are concerned with specific semantic relations, e.g., synonymy (Edmonds and Hirst, 2002; Curran, 2003), hypernymy (Weeds et al., 2004; Lenci and Benotto, 2012) or syntagmatic predicate preferences (McCarthy and Carroll, 2003; Erk et al., 2010), etc. In previous studies, the comparison of syntagmatic and paradigmatic relations has been implemented in terms of an opposition between different classes of corpus-based models: term-context models (words as targets, documents or context regions as features) vs. bag-of-words models (words as targets and features) in Sahlgren (2006); collocation lists vs. bag-of-words models in Rapp (2002). Given the high terminological variation in the literature, in this paper we will adopt the 1 Term-document models encode first-order information because dot products between row vector"
S14-1020,J02-2001,0,0.0634824,"k, datasets and evaluated parameters. Section 4 introduces our model selection methodology. Section 5 presents the results of our evaluation study. Section 6 summarizes main findings and sketches ongoing and future work. 2 Previous Work In this section we discuss previous work relevant to the distributional modeling of paradigmatic and syntagmatic relations. For space constraints, we focus only on two studies (Rapp, 2002; Sahlgren, 2006) in which the two classes of relations are compared at a global level, and not on studies that are concerned with specific semantic relations, e.g., synonymy (Edmonds and Hirst, 2002; Curran, 2003), hypernymy (Weeds et al., 2004; Lenci and Benotto, 2012) or syntagmatic predicate preferences (McCarthy and Carroll, 2003; Erk et al., 2010), etc. In previous studies, the comparison of syntagmatic and paradigmatic relations has been implemented in terms of an opposition between different classes of corpus-based models: term-context models (words as targets, documents or context regions as features) vs. bag-of-words models (words as targets and features) in Sahlgren (2006); collocation lists vs. bag-of-words models in Rapp (2002). Given the high terminological variation in the"
S14-1020,J10-4007,0,0.0759452,"Missing"
S14-1020,P04-1003,0,0.0323045,"group synonyms with antonyms and cohyponyms from SPP as paradigmatic relations, and the entire GEK dataset with backward and forward phrasal associates from SPP as syntagmatic relations. Experimental Setting 3.1 Evaluation Task and Data In this study, bag-of-words DSMs are evaluated on two datasets containing experimental items from two priming studies. Each item is a word triple (target, consistent prime, inconsistent prime) with a particular semantic relation between target and consistent prime. Following previous work on modeling priming effects as a comparison between prime-target pairs (McDonald and Brew, 2004; Pad´o and Lapata, 2007; Herda˘gdelen et al., 2009), we evaluate our models in a classification task. The goal is to identify the consistent prime on the basis of its distributional relatedness to the target: if a particular DSM (i.e., a certain parameter combination) is sensitive to a specific relation (or group of relations), we expect the consistent primes to be closer to the target in semantic space than the inconsistent ones. The first dataset is derived from the Semantic Priming Project (SPP) (Hutchison et al., 2013). To the best of our knowledge, our study represents the first evaluati"
S14-1020,J07-2002,0,0.252789,"Missing"
S14-1020,C02-1007,0,0.720429,"they are also called relations in absentia (Sahlgren, 2006) because paradigmatically related words do not co-occur. Examples of paradigmatic relations are synonyms (e.g., frigid–cold) and antonyms (e.g., cold–hot). Syntagmatic relations hold between words that cooccur (relations in praesentia) and therefore exhibit a similar distribution across contexts. Typical examples of syntagmatic relations are phrasal associates (e.g., help–wanted) and syntactic collocations (e.g., dog–bark). Distributional modeling has already tackled the issue of paradigmatic and syntagmatic relations (Sahlgren, 2006; Rapp, 2002). Key contributions of the present work are the scope of its evaluation (in terms of semantic relations and model parameters) and the new perspective on paradigmatic vs. syntagmatic models provided by our results. Concerning the scope of the evaluation, this is the first study in which the comparison involves such a wide range of semantic relations (paradigmatic: synonyms, antonyms and co-hyponyms; syntagmatic: syntactic collocations, backward and forward phrasal associates). Moreover, our evaluation covers a large number of DSM parameters: source corpus, size and direction of the context wind"
S14-1020,I13-1056,1,0.831603,"g experiments. The leading theme of our study is a comparison between syntagmatic and paradigmatic relations in terms of the aspects of distributional similarity that characterize them. Our results show that second-order DSMs are capable of capturing both syntagmatic and paradigmatic relations, if parameters are properly tuned. Size of the co-occurrence window as well as parameters connected to dimensionality reduction play a key role in adapting DSMs to particular relations. Even if we do not address the more specific task of distinguishing between relations (e.g., synonyms vs. antonyms; see Scheible et al. (2013) and references therein), we believe that such applications may benefit from our detailed analyses on the effects of DSM parameters. Ongoing and future work is concerned with the expansion of the evaluation setting to other classes of models (first-order models, dependency-based second-order models) and parameters (e.g., dimensionality reduction with Random Indexing). corpus win score transf r.dim d.sk acc best ukwac 16 s-ll log 900 50 96.0 97.0 ukwac 8 z-sc root 900 0 93.0 98.6 ukwac 8 z-sc root 900 0 95.5 98.9 ukwac 4 s-ll log 900 50 96.3 99.0 ukwac 4 s-ll log 900 50 98.7 100 wacky 8 s-ll lo"
S14-1020,J98-1004,0,0.596471,"Missing"
S14-1020,C04-1146,0,0.103187,"Missing"
S14-2093,S12-1051,0,0.0415429,"466 0.672 0.675 0.653 0.674 0.657 0.636 0.617 0.673 0.771 0.764 0.719 0.772 0.836 0.834 0.780 0.849 0.690 0.690 0.636 0.687 0.700 0.694 0.654 0.706 best individual training data best individ., no deps best individ., no deps, no WN best individ. + experimental 0.475 0.465 0.448 0.475 0.706 0.700 0.722 0.711 0.711 0.699 0.677 0.715 0.788 0.781 0.752 0.795 0.852 0.848 0.791 0.864 0.715 0.722 0.706 0.721 0.727 0.722 0.697 0.733 n ea m w. 0.733 s ew t-n ee tw N nW O 0.643 es es ag im lin ad he 0.349 s ew best run m ru fo n ftde ftde Run Table 3: Results for task 10. on semantic textual similarity (Agirre et al., 2012; Agirre et al., 2013). It comprises two subtasks: English semantic textual similarity and Spanish semantic textual similarity. For each subtask, there are sentence pairs from various genres. We only participate in the English subtask and take the 13th place out of 38 with a weighted mean of Pearson correlation coefficients of 0.694 (best system: 0.761). 5.2 In another experiment we try to optimize our strategy of finding the best subset of the training data for each test data set. Doing that gives us a considerably higher weighted mean than using the same training data for every test data set"
S14-2093,S13-1004,0,0.139421,"em for measuring semantic similarity. At the core of the system is a word-to-word alignment of two texts using a maximum weight matching algorithm. The system participated in three SemEval-2014 shared tasks and the competitive results are evidence for its usability in that broad field of application. 1 Introduction Semantic similarity measures the semantic equivalence between two texts ranging from total difference to complete semantic equivalence and is usually encoded as a number in a closed interval, e. g. [0, 5]. Here is an example for interpreting the numeric similarity scores taken from Agirre et al. (2013, 33): broad range of NLP applications, e. g. paraphrasing, MT evaluation, information extraction, question answering and summarization. A general system for semantic similarity aiming at being applicable in such a broad scope has to be able to adapt to the use case at hand, because different use cases might, for example, require different similarity scales: For one application, two texts dealing roughly with the same topic should get a high similarity score, whereas for another application being able to distinguish between subtle differences in meaning might be important. The three SemEval-20"
S14-2093,J06-1003,0,0.0810087,"distribution of cosines between lemma pairs. For both alignments, we categorize the cosines between aligned lemma pairs into five heuristically determined intervals ([0.2, 0.35), [0.35, 0.5), [0.5, 0.7), [0.7, 0.999), [0.999, 1.0])5 and use the proportions as features. Intuitively, the top bins correspond to links between identical words, paradigmatically related words and topically related words. All in all, we use a total of 18 features computed from the DSM-based alignments. Measures Based on Distributional Word Similarities 2.4.3 Measures Based on WordNet We utilize two state-of-the-art (Budanitsky and Hirst, 2006) WordNet similarity measures for creating alignments: Leacock and Chodorow’s (1998) normalized path length and Lin’s (1998) universal similarity measure. For both of those similarity measures we compute the best one-to-one and the best one-to-many alignment. For each alignment we compute the following two similarity measures: I ) the arithmetic mean of the similarities between the aligned words from text A and text B and II ) the arithmetic mean ignoring identical word pairs. For each of the two DSMs described in Section 2.3 we compute the best one-to-one and the best one5 Values in the interv"
S14-2093,J93-1004,0,0.701478,"y large number of sentences in the SICK dataset (Marelli et al., 2014b) that mainly differ in their use of negation, e. g. sentence pair 42 in the training data that has a gold similarity score of 3.4: Simple Measures We use four simple heuristic similarity measures that need very little preprocessing. The first two are word form overlap and lemma overlap between the two texts. We take the sets of word form tokens/lemmatized tokens in text A and text B and calculate the Jaccard coefficient: overlap = |A ∩ B| . |A ∪ B| The third is a heuristic for the difference in text length that was used by Gale and Church (1993) as a similarity measure for aligning sentences: zi = ∑Nj=1 b j di , where di = bi − N ai . σd ∑ j=1 a j For each of the N text pairs we calculate the difference di between the observed length of text B and 1 http://nlp.stanford.edu/software/corenlp. shtml 2 http://networkx.github.com/ 3 That is because in the CCprocessed variant of the Stanford Dependencies most prepositions are “collapsed” into dependency relations and are therefore represented as edges and not as vertices in the graph. • Two people are kickboxing and spectators are watching • Two people are kickboxing and spectators are not"
S14-2093,S13-1026,1,0.86574,"another application being able to distinguish between subtle differences in meaning might be important. The three SemEval-2014 shared tasks focussing on semantic similarity (cf. Sections 3, 4 and 5 for more detailed task descriptions) provide a rich testbed for such a general system, as the individual tasks and subtasks have slightly different objectives. In the remainder of this paper, we describe SemantiKLUE, a general system for measuring semantic similarity between texts that we built based on our experience from participating in the *SEM 2013 shared task on “Semantic Textual Similarity” (Greiner et al., 2013). 0. The two sentences are on different topics. 1. The two sentences are not equivalent, but are on the same topic. 2. The two sentences are not equivalent, but share some details. 3. The two sentences are roughly equivalent, but some important information differs/missing. 4. The two sentences are mostly equivalent, but some unimportant details differ. 5. The two sentences are completely equivalent, as they mean the same thing. Systems capable of reliably predicting the semantic similarity between two texts can be beneficial for a This work is licensed under a Creative Commons Attribution 4.0"
S14-2093,S13-1005,0,0.0313102,"ecause in the CCprocessed variant of the Stanford Dependencies most prepositions are “collapsed” into dependency relations and are therefore represented as edges and not as vertices in the graph. • Two people are kickboxing and spectators are watching • Two people are kickboxing and spectators are not watching 2.3 Measures Based on Distributional Document Similarity We obtain document similarity scores from two large-vocabulary distributional semantic models (DSMs). The first model is based on a 10-billion word Web corpus consisting of Wackypedia and ukWaC (Baroni et al., 2009), UMBC WebBase (Han et al., 2013), and UKCOW 2012 (Schäfer and Bildhauer, 2012). Target terms and feature terms are POS-disambiguated lemmata.4 We use parameters suggested by recent evaluation experiments: co-occurrence counts in a symmetric 4-word window, the most frequent 30,000 lexical words as features, log-likelihood scores with an additional log-transformation, and SVD dimensionality reduction of L2-normalized vectors to 1000 latent dimensions. This model provides distributional representations for 150,000 POS-disambiguated lemmata as target terms. The second model was derived from the second release of the Google Books"
S14-2093,S14-2003,0,0.0575208,"an overview of the results. Run r ρ MSE Acc. primary run best run 0.780 0.782 0.736 0.738 0.403 0.398 0.823 0.823 complete system no deps no deps, no WN complete + experimental 0.798 0.793 0.763 0.801 0.754 0.748 0.713 0.757 0.373 0.383 0.432 0.367 0.820 0.817 0.793 0.823 only DSM alignment only WordNet only simple only DSM document only deps 0.729 0.708 0.676 0.660 0.576 0.670 0.636 0.667 0.568 0.565 0.484 0.515 0.561 0.585 0.688 0.746 0.715 0.754 0.567 0.614 SemEval-2014 Task 3 Unlike the other tasks, which focus on similar-sized texts, the shared task on “Cross-Level Semantic Similarity” (Jurgens et al., 2014) is about measuring semantic similarity between textual units of different lengths. It comprises four subtasks comparing I ) paragraphs to sentences, II ) sentences to phrases, III ) phrases to words and IV ) words to word senses (taken from WordNet). Due to the nature of this task, performance in it might be especially useful as an indicator for the usefulness of a system in the area of summarization. SemantiKLUE takes the fourth place out of 38 in both the official ranking by Pearson correlation and the alternative ranking by Spearman correlation. 4.2 Additional Preprocessing For the officia"
S14-2093,P12-3029,0,0.036025,"Missing"
S14-2093,S14-2001,0,0.186364,"on tokens.3 For some tasks, we perform some additional task-specific preprocessing steps prior to parsing, cf. task descriptions below. 2.2 the expected length of text B based on the length of text A. By dividing that difference di by the standard deviation of all those differences, we obtain our heuristic zi . The fourth is a binary feature expressing whether the two texts differ in their use of negation. We check if one of the texts contains any of the lemmata no, not or none and the other doesn’t. That feature is motivated by the comparatively large number of sentences in the SICK dataset (Marelli et al., 2014b) that mainly differ in their use of negation, e. g. sentence pair 42 in the training data that has a gold similarity score of 3.4: Simple Measures We use four simple heuristic similarity measures that need very little preprocessing. The first two are word form overlap and lemma overlap between the two texts. We take the sets of word form tokens/lemmatized tokens in text A and text B and calculate the Jaccard coefficient: overlap = |A ∩ B| . |A ∪ B| The third is a heuristic for the difference in text length that was used by Gale and Church (1993) as a similarity measure for aligning sentences"
S14-2093,marelli-etal-2014-sick,0,0.152916,"on tokens.3 For some tasks, we perform some additional task-specific preprocessing steps prior to parsing, cf. task descriptions below. 2.2 the expected length of text B based on the length of text A. By dividing that difference di by the standard deviation of all those differences, we obtain our heuristic zi . The fourth is a binary feature expressing whether the two texts differ in their use of negation. We check if one of the texts contains any of the lemmata no, not or none and the other doesn’t. That feature is motivated by the comparatively large number of sentences in the SICK dataset (Marelli et al., 2014b) that mainly differ in their use of negation, e. g. sentence pair 42 in the training data that has a gold similarity score of 3.4: Simple Measures We use four simple heuristic similarity measures that need very little preprocessing. The first two are word form overlap and lemma overlap between the two texts. We take the sets of word form tokens/lemmatized tokens in text A and text B and calculate the Jaccard coefficient: overlap = |A ∩ B| . |A ∪ B| The third is a heuristic for the difference in text length that was used by Gale and Church (1993) as a similarity measure for aligning sentences"
S14-2093,schafer-bildhauer-2012-building,0,0.0319637,"f the Stanford Dependencies most prepositions are “collapsed” into dependency relations and are therefore represented as edges and not as vertices in the graph. • Two people are kickboxing and spectators are watching • Two people are kickboxing and spectators are not watching 2.3 Measures Based on Distributional Document Similarity We obtain document similarity scores from two large-vocabulary distributional semantic models (DSMs). The first model is based on a 10-billion word Web corpus consisting of Wackypedia and ukWaC (Baroni et al., 2009), UMBC WebBase (Han et al., 2013), and UKCOW 2012 (Schäfer and Bildhauer, 2012). Target terms and feature terms are POS-disambiguated lemmata.4 We use parameters suggested by recent evaluation experiments: co-occurrence counts in a symmetric 4-word window, the most frequent 30,000 lexical words as features, log-likelihood scores with an additional log-transformation, and SVD dimensionality reduction of L2-normalized vectors to 1000 latent dimensions. This model provides distributional representations for 150,000 POS-disambiguated lemmata as target terms. The second model was derived from the second release of the Google Books N-Grams database (Lin et al., 2012), using th"
S14-2093,J98-1004,0,0.138539,"ns for 150,000 POS-disambiguated lemmata as target terms. The second model was derived from the second release of the Google Books N-Grams database (Lin et al., 2012), using the dependency pairs provided in this version. Target and feature terms are case-folded word forms; co-occurrence ounts are based on direct syntactic relations. Here, the most frequent 50,000 word forms were used as features. All other parameters are identical to the first DSM. This model provides distributional representations for 250,000 word forms. We compute bag-of-words centroid vectors for each text as suggested by (Schütze, 1998). For each 533 4 e.g. can_N for the noun can text pair and DSM, we calculate the cosine similarity between the two centroid vectors as a measure of their semantic similarity. We also determine the number of unknown words in both texts according to both DSMs as additional features. 2.4 Alignment-based Measures We also use features based on word-level similarity. We separately compute similarities between words using state-of-the-art WordNet similarity measures and the two distributional semantic models described above. The words from both texts are then aligned using those similarity scores to"
S14-2093,S14-2010,0,\N,Missing
S14-2096,evert-2008-lightweight,1,0.773217,"r normal words and hashtags with a score threshold of 0.8 a manual extension including synonyms, antonyms and several word lists from online sources, compiled by the SNAP team (Schulze Wettendorf et al., 2014) an automatic extension with distributionally similar words (DSM extension), using a strategy similar to Proisl et al. (2013) • Word form unigrams and bigrams. After some experimentation, the document frequency threshold was set to f ≥ 5 for subtask B and f ≥ 2 for subtask A. • In order to include information from character n-grams, we used a Perl implementation of ngram language models (Evert, 2008) that has already been applied successfully to text categorization tasks (boilerplate detection in the CLEANEVAL 2007 competition). We trained three separate models on positive, negative and neutral messages. We selected a 5-gram model (n = 5) with strong smoothing (q = 0.7), which minimized cross-entropy on the training data (measured by cross-validation). For each message in the training and test data, three features were generated, specifying per-character crossentropy for each of the three n-gram models.8 • Counts of positive and negative emoticons using the same lists as in the KLUE syste"
S14-2096,D13-1125,0,0.0258686,"Missing"
S14-2096,S13-2054,0,0.0213971,"o a clear improvement. • A negation heuristic, which inverts the polarity score of the first sentiment word within 4 tokens after a negation marker. In the bag-ofwords representation, the next 3 tokens after a negation marker are prefixed with not_. • For subtask A, these features were computed both for the marked word or phrase and for the rest of the message. In order to improve the KLUE classifier, we drew inspiration from two other systems participating in the SemEval-2013 task: NRC-Canada (Mohammad et al., 2013), which won the task by a large margin over competing systems, and GU-MLT-LT (Günther and Furrer, 2013), which used similar features to our classifier, but obtained better results due to careful selection and tuning of the machine learning algorithm. Mohammad et al. (2013) used a huge set of features, including several sentiment lexica (both manually and automatically created), word n-grams (up to 4-grams with low frequency threshold), character n-grams (3-grams to 5-grams), Twitter-derived word clusters and a negation heuristic similar to our approach. Features with the largest impact in subtask B were sentiment lexica (esp. large automatically generated word lists), word n-grams, character n-"
S14-2096,S13-2053,0,0.157413,"single words (unigrams) were used, since experiments with additional bigram features did not lead to a clear improvement. • A negation heuristic, which inverts the polarity score of the first sentiment word within 4 tokens after a negation marker. In the bag-ofwords representation, the next 3 tokens after a negation marker are prefixed with not_. • For subtask A, these features were computed both for the marked word or phrase and for the rest of the message. In order to improve the KLUE classifier, we drew inspiration from two other systems participating in the SemEval-2013 task: NRC-Canada (Mohammad et al., 2013), which won the task by a large margin over competing systems, and GU-MLT-LT (Günther and Furrer, 2013), which used similar features to our classifier, but obtained better results due to careful selection and tuning of the machine learning algorithm. Mohammad et al. (2013) used a huge set of features, including several sentiment lexica (both manually and automatically created), word n-grams (up to 4-grams with low frequency threshold), character n-grams (3-grams to 5-grams), Twitter-derived word clusters and a negation heuristic similar to our approach. Features with the largest impact in subt"
S14-2096,S13-2052,0,0.0756951,"and Paul Greiner and Besim Kabashi Friedrich-Alexander-Universität Erlangen-Nürnberg Department Germanistik und Komparatistik Professur für Korpuslinguistik Bismarckstr. 6, 91054 Erlangen, Germany {stefan.evert,thomas.proisl,paul.greiner,besim.kabashi}@fau.de Abstract SentiKLUE is an update of the KLUE polarity classifier – which achieved good and robust results in SemEval-2013 with a simple feature set – implemented in 48 hours. 1 Introduction The SemEval-2014 shared task on “Sentiment Analysis in Twitter” (Rosenthal et al., 2014) is a rerun of the corresponding shared task from SemEval2013 (Nakov et al., 2013) with new test data. It focuses on polarity classification in computermediated communication such as Twitter, other micro-blogging services, and SMS. There are two subtasks: the goal of Message Polarity Classification (B) is to classify an entire SMS, tweet or other message as positive (pos), negative (neg) or neutral (ntr); in the subtask on Contextual Polarity Disambiguation (A), a single word or short phrase has to be classified in the context of the whole message. The training data are the same as in SemEval2013. The test data from 2013 are used as a development set in order to select feat"
S14-2096,S13-2065,1,0.714381,"w Twitter messages, LiveJournal entries as out-of-domain data, and a small number of tweets containing sarcasm (see Rosenthal et al. (2014) for further details). For subtask B, there are 10,239 training items, 5,907 items in the development set, and 3,861 additional unseen items in the new test set. For subtask A, there are 9,505 training items, 6,769 items in the development set, and 3,912 additional items in the test set. Our team participated in the SemEval-2013 shared task with a relatively simple, but robust system (KLUE) based on a maximum entropy classifier and a small set of features (Proisl et al., 2013). Despite its simplicity, KLUE performed very well in subtask B, ranking 5th out of 36 constrained systems on the Twitter data and 3rd out of 28 on the SMS data. Results for contextual polarity disambiguation (subtask A) were less encouraging, with rank 14 out of 21 constrained systems on the Twitter data and rank 12 out of 19 on the SMS data. This paper describes our efforts to bring the KLUE system up to date within a period of 48 hours. The results obtained by the new SentiKLUE system are summarised in Table 1, showing that the update was successful. The ranking of the system has improved s"
S14-2096,S14-2009,0,0.229896,"or other message as positive (pos), negative (neg) or neutral (ntr); in the subtask on Contextual Polarity Disambiguation (A), a single word or short phrase has to be classified in the context of the whole message. The training data are the same as in SemEval2013. The test data from 2013 are used as a development set in order to select features and tune machine learning algorithms, but may not be included in the training data. The 2014 test set comprises the development data, new Twitter messages, LiveJournal entries as out-of-domain data, and a small number of tweets containing sarcasm (see Rosenthal et al. (2014) for further details). For subtask B, there are 10,239 training items, 5,907 items in the development set, and 3,861 additional unseen items in the new test set. For subtask A, there are 9,505 training items, 6,769 items in the development set, and 3,912 additional items in the test set. Our team participated in the SemEval-2013 shared task with a relatively simple, but robust system (KLUE) based on a maximum entropy classifier and a small set of features (Proisl et al., 2013). Despite its simplicity, KLUE performed very well in subtask B, ranking 5th out of 36 constrained systems on the Twitt"
S14-2096,S14-2101,1,0.874352,"ammad et al., 2013)6 , which was compiled from a corpus of 1.6 million tweets for NRC-Canada; we created separate lists for normal words and hashtags with a score threshold of 1.0 NRC Hashtag Sentiment Lexicon (Mohammad et al., 2013)7 , which contains words that exhibit a strong statistical association (PMI score) to positive or negative hashtags, also compiled for NRC-Canada; again, we created separate lists for normal words and hashtags with a score threshold of 0.8 a manual extension including synonyms, antonyms and several word lists from online sources, compiled by the SNAP team (Schulze Wettendorf et al., 2014) an automatic extension with distributionally similar words (DSM extension), using a strategy similar to Proisl et al. (2013) • Word form unigrams and bigrams. After some experimentation, the document frequency threshold was set to f ≥ 5 for subtask B and f ≥ 2 for subtask A. • In order to include information from character n-grams, we used a Perl implementation of ngram language models (Evert, 2008) that has already been applied successfully to text categorization tasks (boilerplate detection in the CLEANEVAL 2007 competition). We trained three separate models on positive, negative and neutra"
S14-2096,H05-1044,0,0.0748487,"Missing"
S14-2101,H92-1022,0,0.0428385,"raction module (subtask 1) was to treat aspect term extraction as a tagging task. We used a standard IOB tagset indicating whether each token is at the beginning of an aspect term (ATB), inside an aspect term (ATI), or not part of an aspect term at all (ATX). First experiments were carried out with unigram, bigram and trigram taggers implemented in NLTK (Bird et al., 2009), which were trained on IOB tags derived from the annotations in the Task 4 gold standard (comprising both trial and training data). We also tested higher-order n-gram taggers and the NLTK implementation of the Brill tagger (Brill, 1992). For a more sophisticated approach we used RFTagger (Schmid and Laws, 2008), which extends the standard HMM tagging model with complex hidden states that consist of features corresponding to different pieces of information. RFTagger was developed for morphological tagging, where complex tags such as N.Reg.Nom.Sg.Neut are decomposed into the main syntactic category (N) and additional morpho-syntactic features representing case (Nom), number (Sg), etc. In our case, the tagger was used for joint annotation of part-of-speech tags and IOB tags for the aspect term boundaries, based on the rationale"
S14-2101,S14-2004,0,0.0429233,"le 1 shows our ranking among all constrained systems (counting only the best run from each team), the score achieved by SNAP (accuracy or F-score, depending on subtask), and the score achieved by the best system in the respective subtask. Because of a preprocessing mistake that was only discovered after phase A of the evaluation had ended, results for subtasks 1 and 3 are significantly lower than the results achieved during development of the system. Introduction This paper describes the approach of the SemaNtic Analyis Project (SNAP) to Task 4 of SemEval2014: Aspect Based Sentiment Analysis (Pontiki et al., 2014). SNAP is a team of undergraduate students at the Corpus Linguistics Group, FAU Erlangen-N¨urnberg, who carried out this work as part of a seminar in computational linguistics. Task 4 was divided into the four subtasks Aspect term extraction (1), Aspect term polarity (2), Aspect category detection (3) and Aspect category polarity (4), which were evaluated in two phases (A: subtasks 1/3; B: subtasks 2/4). Subtasks 1 and 3 were carried out on two different datasets, one of laptop reviews and one of restaurant reviews. Subtasks 2 and 4 only made use of the latter. 2 Sentiment lexicon Early on in"
S14-2101,S13-2065,1,0.719348,"tes a serious problem with the restaurant data, which has surprisingly low recall, 3 nlp.stanford.edu/software/dependencies 580 manual.pdf 5.1 Features based on a sentiment lexica with different classifiers (Maximum Entropy, Linear SVM and SVMs with RBF kernel) and various subsets of features. By default, we worked on the level of single opinion words that express a positive or negative polarity (sg). We added the following features in different combinations: an extended list of opinion words (ex) obtained from a distribution semantic model, based on nearest neighbours of known opinion words (Proisl et al., 2013); potential misspellings of know opinion words, within a maximal Levenshtein distance of 1 (lv); word combinations and fixed phrases (ml) containing up to 3 words (e.g., good mannered, put forth, tried and true, up and down); and the sums of positive and negative opinion words in the whole sentence (st). The best results for the laptops data were achieved with a Maximum Entropy classifier, excluding misspellings (lv) and word combinations (ml); the corresponding line in Table 4 is highlighted in bold font. Even though MaxEnt achieved the best results during development, we decided to use SVM w"
S14-2101,C08-1098,0,0.0272074,"a tagging task. We used a standard IOB tagset indicating whether each token is at the beginning of an aspect term (ATB), inside an aspect term (ATI), or not part of an aspect term at all (ATX). First experiments were carried out with unigram, bigram and trigram taggers implemented in NLTK (Bird et al., 2009), which were trained on IOB tags derived from the annotations in the Task 4 gold standard (comprising both trial and training data). We also tested higher-order n-gram taggers and the NLTK implementation of the Brill tagger (Brill, 1992). For a more sophisticated approach we used RFTagger (Schmid and Laws, 2008), which extends the standard HMM tagging model with complex hidden states that consist of features corresponding to different pieces of information. RFTagger was developed for morphological tagging, where complex tags such as N.Reg.Nom.Sg.Neut are decomposed into the main syntactic category (N) and additional morpho-syntactic features representing case (Nom), number (Sg), etc. In our case, the tagger was used for joint annotation of part-of-speech tags and IOB tags for the aspect term boundaries, based on the rationale that the additional information encoded in the hidden states (compared to a"
S14-2101,H05-1044,0,0.0414464,"en as a foundation and expanded with extensive manual additions. The first step was an exhaustive manual websearch to find additional candidates for the lexicon. The candidates were converted to a common format, and redundant entries were discareded. The next step consisted of further expansion with the help of online thesauri, from which large number of synonyms and antonyms for existing entries were obtained. Since the coverage of the lexicon was still found to be insufficient, it was further complemented with entries from two other existing sentiment lexica, AFINN (Nielsen, 2011) and MPQA (Wilson et al., 2005). Finally the augmented lexicon was compared with the original word lists from AFINN, MPQA and Bing Liu in order to measure the reliabilty of the entries. The reliability score of each entry is the number of sources in which it is found. 3 Aspect term extraction The approach chosen by the aspect term extraction module (subtask 1) was to treat aspect term extraction as a tagging task. We used a standard IOB tagset indicating whether each token is at the beginning of an aspect term (ATB), inside an aspect term (ATI), or not part of an aspect term at all (ATX). First experiments were carried out"
S15-2020,J93-1004,0,0.737615,"computation of all 39 similarity measures for words and tokens in each sentence. Prepositions, articles, conjunctions as well as auxiliary verbs like be and have were ignored in the computation of token-based measures. 2.2 Similarity Measures: Overview A detailed description of all 39 similarity measures used as features in SemantiKLUE is provided in Proisl et al., 2014 (Sections 2.2 - 2.7). Similarity measures used by our system include: • Heuristic similarity measures: word form overlap and lemma overlap between two texts computed with Jaccard coefficient; difference in text length used by Gale and Church (1993); a binary feature to treat negation in each sentence pair. • Document similarity measures based on two distributional models: a model based on nonlemmatized information, built from the second release of the Google Books N-Grams database (Lin et al., 2012); a lemmatized model, built from a 10billion word Web corpus3 . • Alignment-based measures: one-to-one alignment and one-to-many alignment for both words and lemmata, computed via maximum weight matching, based on cosine similarity between two words in paired sentences as edge weight. Figure 1 visualizes a one-to-many alignment based on lemma"
S15-2020,S13-1005,0,0.033111,"ne alignment and one-to-many alignment for both words and lemmata, computed via maximum weight matching, based on cosine similarity between two words in paired sentences as edge weight. Figure 1 visualizes a one-to-many alignment based on lemmatized data. The colors of the connections correspond to different cosine ranges, reported in the legend to the right of the plot. • WordNet-based similarity measures: Leacock and Chodorow’s (1998) normalized path length 1 http://nlp.stanford.edu/software/corenlp.shtml http://networkx.github.com 3 Wackypedia and UkWaC (Baroni et al., 2009), UMBC WebBase (Han et al., 2013), and UKCOW 2012 (Sch¨afer and Bildhauer, 2012). 2 112 Figure 1: One-to-many alignment plot. Sentences: “A black and white dog is jumping into the water” , “A white dog runs across the water”; Subset: Images; Gold Score: 2.8; SemantiKLUE score: 2.93. and Lin’s (1998) universal similarity measure. Using these similarity measures, the best one-toone and the best one-to-many alignment are computed. After that, the arithmetic mean of the similarities between the aligned words from text A and text B with and without identical word pairs is calculated. An additional WordNet-based feature is the numb"
S15-2020,P12-3029,0,0.0244444,"Missing"
S15-2020,S14-2093,1,0.826127,"sentence was represented as a graph using the CCprocessed variant of the Stanford Dependencies (collapsed dependencies with propagation of conjunct dependencies) implemented with the NetworkX2 module. This graph representation was involved in the computation of all 39 similarity measures for words and tokens in each sentence. Prepositions, articles, conjunctions as well as auxiliary verbs like be and have were ignored in the computation of token-based measures. 2.2 Similarity Measures: Overview A detailed description of all 39 similarity measures used as features in SemantiKLUE is provided in Proisl et al., 2014 (Sections 2.2 - 2.7). Similarity measures used by our system include: • Heuristic similarity measures: word form overlap and lemma overlap between two texts computed with Jaccard coefficient; difference in text length used by Gale and Church (1993); a binary feature to treat negation in each sentence pair. • Document similarity measures based on two distributional models: a model based on nonlemmatized information, built from the second release of the Google Books N-Grams database (Lin et al., 2012); a lemmatized model, built from a 10billion word Web corpus3 . • Alignment-based measures: one"
S15-2020,schafer-bildhauer-2012-building,0,0.0336738,"Missing"
S15-2020,J98-1004,0,0.210754,"Missing"
S15-2020,S15-2045,0,\N,Missing
S15-2103,S14-2096,1,0.815471,"sification and Association Nataliia Plotnikova and Micha Kohl and Kevin Volkert and Andreas Lerner and Natalie Dykes and Heiko Ermer and Stefan Evert Professur f¨ur Korpuslinguistik Friedrich-Alexander-Universit¨at Erlangen-N¨urnberg Bismarckstr. 6, 91054 Erlangen, Germany {nataliia.plotnikova, micha.kohl, kevin.volkert, andreas.lerner, natalie.dykes, heiko.ermer, stefan.evert}@fau.de Abstract This paper describes the KLUEless system which participated in the SemEval-2015 task on “Sentiment Analysis in Twitter”. This year the updated system based on the developments for the same task in 2014 (Evert et al., 2014) and 2013 (Proisl et al., 2013) participated in all five subtasks. The paper gives an overview of the core features extended by different additional features and parameters required for individual subtasks. Experiments carried out after the evaluation period on the test dataset 2015 with the gold standard available are integrated into each subtask to explain the submitted feature selection. 1 Introduction The SemEval-2015 shared task on ”Sentiment Analysis in Twitter” (Rosenthal et al., 2015) is a rerun of the shared task from SemEval-2014 (Rosenthal et al., 2014) with three new subtasks. Whil"
S15-2103,S13-2053,0,0.0612016,"Python script based on code from Russell (2014). The downloaded tweet texts were stripped of retweet boilerplate and usernames and URLs were replaced with anonymous placeholders. Redundant tweets and tweets containing no useful information (e.g. no English words) were discarded, resulting in a total of about 6.5 million. We used three sources to annotate our tweet data. One was our main KLUEless system, assigning either positive, negative or neutral sentiment to a tweet. The other two were manually annotated lists of 328 hashtags (manually selected and reannotated from a lexicon generated by Mohammad et al. (2013)) and 67 emoticons (manually selected from a list generated from wikipedia articles6,7 ). Tweets were tagged positive when they contained at least one positive and no negative hashtag or emoticon respectively and vice versa. Because annotation based on hashtags and emoticons showed promising results on the test data and because we wanted to rely as little as possible on existing sentiment lexica that greatly influence the annotations provided by our KLUEless system, we gave priority to hashtag and emoticon based sentiments in this order and fell back to KLUEless annotations if either no other"
S15-2103,S13-2052,0,0.0614605,"en message, subtask C, D and E were new. In subtask C a topic was given, towards which the sentiment in a message had to be identified. Subtask D was similar to subtask C, as the sentiment towards a given topic had to be identified, but in this subtask several messages were given from which the sentiment had to be drawn. Ultimately in subtask E, the sentiment of a given word or phrase had to be measured on a score ranging [0, 1], indicating its association with positive sentiment. The training data for subtasks A and B are the same as in SemEval-2014 (Rosenthal et al., 2014) and SemEval-2013 (Nakov et al., 2013). For subtask A, there are 9,505 training items with 6,769 items in development set and 3,912 items in the test set. For subtask B, there are 10,239 training items, 5,907 items in the development set and 3,861 in the test set. For subtasks C and D the same training sets as for subtasks A and B were used by our team. A pilot task E aimed at evaluation of automatic methods of generating sentiment lexicons had no training set, a detailed approach used for this subtask will be given in Section 3. This paper describes the updated system with our efforts to improve it after the evaluation period. Th"
S15-2103,S13-2065,1,0.90752,"aliia Plotnikova and Micha Kohl and Kevin Volkert and Andreas Lerner and Natalie Dykes and Heiko Ermer and Stefan Evert Professur f¨ur Korpuslinguistik Friedrich-Alexander-Universit¨at Erlangen-N¨urnberg Bismarckstr. 6, 91054 Erlangen, Germany {nataliia.plotnikova, micha.kohl, kevin.volkert, andreas.lerner, natalie.dykes, heiko.ermer, stefan.evert}@fau.de Abstract This paper describes the KLUEless system which participated in the SemEval-2015 task on “Sentiment Analysis in Twitter”. This year the updated system based on the developments for the same task in 2014 (Evert et al., 2014) and 2013 (Proisl et al., 2013) participated in all five subtasks. The paper gives an overview of the core features extended by different additional features and parameters required for individual subtasks. Experiments carried out after the evaluation period on the test dataset 2015 with the gold standard available are integrated into each subtask to explain the submitted feature selection. 1 Introduction The SemEval-2015 shared task on ”Sentiment Analysis in Twitter” (Rosenthal et al., 2015) is a rerun of the shared task from SemEval-2014 (Rosenthal et al., 2014) with three new subtasks. While subtasks A and B were identic"
S15-2103,S14-2009,0,0.0298537,"pments for the same task in 2014 (Evert et al., 2014) and 2013 (Proisl et al., 2013) participated in all five subtasks. The paper gives an overview of the core features extended by different additional features and parameters required for individual subtasks. Experiments carried out after the evaluation period on the test dataset 2015 with the gold standard available are integrated into each subtask to explain the submitted feature selection. 1 Introduction The SemEval-2015 shared task on ”Sentiment Analysis in Twitter” (Rosenthal et al., 2015) is a rerun of the shared task from SemEval-2014 (Rosenthal et al., 2014) with three new subtasks. While subtasks A and B were identical to the tasks of SemEval-2014 and dealt with the identification of polarity in a given message, subtask C, D and E were new. In subtask C a topic was given, towards which the sentiment in a message had to be identified. Subtask D was similar to subtask C, as the sentiment towards a given topic had to be identified, but in this subtask several messages were given from which the sentiment had to be drawn. Ultimately in subtask E, the sentiment of a given word or phrase had to be measured on a score ranging [0, 1], indicating its asso"
S15-2103,S15-2078,0,0.0785149,"nt Analysis in Twitter”. This year the updated system based on the developments for the same task in 2014 (Evert et al., 2014) and 2013 (Proisl et al., 2013) participated in all five subtasks. The paper gives an overview of the core features extended by different additional features and parameters required for individual subtasks. Experiments carried out after the evaluation period on the test dataset 2015 with the gold standard available are integrated into each subtask to explain the submitted feature selection. 1 Introduction The SemEval-2015 shared task on ”Sentiment Analysis in Twitter” (Rosenthal et al., 2015) is a rerun of the shared task from SemEval-2014 (Rosenthal et al., 2014) with three new subtasks. While subtasks A and B were identical to the tasks of SemEval-2014 and dealt with the identification of polarity in a given message, subtask C, D and E were new. In subtask C a topic was given, towards which the sentiment in a message had to be identified. Subtask D was similar to subtask C, as the sentiment towards a given topic had to be identified, but in this subtask several messages were given from which the sentiment had to be drawn. Ultimately in subtask E, the sentiment of a given word or"
soria-etal-2002-advanced,bernsen-etal-2002-nite,1,\N,Missing
W10-1505,U07-1008,0,0.0929891,"ch can be restricted to a single file containing 50–90 MiB of compressed text – every query requires a linear scan of the full database. This paper presents a simple open-source software solution to the third problem, called Web1T5Easy. The n-gram data are encoded and indexed in a relational database. Building on convenient open-source tools such as SQLite and Perl, the software aims to strike a good balance between search efficiency and ease of use and implementation. With its focus on interactive, but accurate search it complements the approximate indexing and batch processing approaches of Hawker et al. (2007). Web1T5-Easy can be downloaded from http://webascorpus.sf.net/Web1T5-Easy/.1 1 An online demo of the complete Web1T5 database is available at http://cogsci.uos.de/~korpora/ws/Web1T5/. 32 Proceedings of the NAACL HLT 2010 Sixth Web as Corpus Workshop, pages 32–40, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics word 1 word 2 word 3 f supplement supplement supplement supplement supplement supplement supplement depend depending depends depends derived des described on on entirely on from coups in 193 174 94 338 2668 77 200 Table 1: Example of Web1T5 3-gram fr"
W13-2608,J10-4006,0,0.477251,"ss (words connected to a particular situation) rather than attributional similarity (synonyms and near-synonyms). DSMs evaluated in this study belong to the class of bag-of-words models: the distributional vector of a target word consists of co-occurrence counts with other words, resulting in a word-word cooccurrence matrix. The models cover a large vocabulary of target words (27668 words in the untagged version; 31713 words in the part-of-speech tagged version). It contains the stimuli from the datasets described in section 2 and further target words from state-of-the-art evaluation studies (Baroni and Lenci, 2010; Baroni and Lenci, 2011; Mitchell and Lapata, 2008). Contexts are filtered by part-of-speech (nouns, verbs, adjectives, and adverbs) and by frequency thresholds. Neither syntax nor word order were taken into account when gathering co-occurrence information. Distributional models were built using the UCS 67 toolkit1 and the wordspace package for R2 . The evaluated parameters are: – Average rank: average of backward and forward association. Indexes of distributional relatedness were considered as an additional parameter in the evaluation, labeled relatedness index below. Every combination of th"
W13-2608,W11-2501,0,0.0496379,"particular situation) rather than attributional similarity (synonyms and near-synonyms). DSMs evaluated in this study belong to the class of bag-of-words models: the distributional vector of a target word consists of co-occurrence counts with other words, resulting in a word-word cooccurrence matrix. The models cover a large vocabulary of target words (27668 words in the untagged version; 31713 words in the part-of-speech tagged version). It contains the stimuli from the datasets described in section 2 and further target words from state-of-the-art evaluation studies (Baroni and Lenci, 2010; Baroni and Lenci, 2011; Mitchell and Lapata, 2008). Contexts are filtered by part-of-speech (nouns, verbs, adjectives, and adverbs) and by frequency thresholds. Neither syntax nor word order were taken into account when gathering co-occurrence information. Distributional models were built using the UCS 67 toolkit1 and the wordspace package for R2 . The evaluated parameters are: – Average rank: average of backward and forward association. Indexes of distributional relatedness were considered as an additional parameter in the evaluation, labeled relatedness index below. Every combination of the parameters described i"
W13-2608,P04-1003,0,0.463976,"Missing"
W13-2608,P08-1028,0,0.0203585,"ather than attributional similarity (synonyms and near-synonyms). DSMs evaluated in this study belong to the class of bag-of-words models: the distributional vector of a target word consists of co-occurrence counts with other words, resulting in a word-word cooccurrence matrix. The models cover a large vocabulary of target words (27668 words in the untagged version; 31713 words in the part-of-speech tagged version). It contains the stimuli from the datasets described in section 2 and further target words from state-of-the-art evaluation studies (Baroni and Lenci, 2010; Baroni and Lenci, 2011; Mitchell and Lapata, 2008). Contexts are filtered by part-of-speech (nouns, verbs, adjectives, and adverbs) and by frequency thresholds. Neither syntax nor word order were taken into account when gathering co-occurrence information. Distributional models were built using the UCS 67 toolkit1 and the wordspace package for R2 . The evaluated parameters are: – Average rank: average of backward and forward association. Indexes of distributional relatedness were considered as an additional parameter in the evaluation, labeled relatedness index below. Every combination of the parameters described in section 3.1 with each valu"
W13-2608,J07-2002,0,0.432691,"Missing"
W13-2608,W09-3207,0,0.772216,"Missing"
W14-4707,J10-4006,0,0.0456444,"th nodes (rows of the co-occurrence matrix) and collocates (columns of the co-occurrence matrix) are chosen from this vocabulary. Collection of first-order models involved the manipulation of a number of parameters, briefly summarized below. We adopted three different window sizes: • symmetric window, 2 words to the left and to the right of the node; • asymmetric window, 3 words to the left of the node; • asymmetric window, 3 words to the right of the node. We tested the following association scores (Evert, 2008): • co-occurrence frequency; • simple log-likelihood (similar to local MI used by Baroni and Lenci (2010)); • conditional probability. Our experiments involved a third parameter, the index of association strength, which determines alternative ways of quantifying the degree of association between targets and contexts in the first-order model. Given two words a and b represented in a first-order model, we propose two alternative ways of quantifying the degree of association between a and b. The first option (and standard in corpus-based modeling) is to compute the association score between a and b. The alternative choice is based on rank among collocates. Given two words a and b, in our task stimul"
W14-4707,W11-2501,0,0.0198769,"imulus words: this leads us to believe that an integration of the two sources may produce improvements in NaDiR’s performance. At a general level, we plan to make more elaborate use of the training data. In the experiments presented in this paper, training data were used to set a frequency threshold for potential responses, train the part-of-speech classifiers, and find the best configuration for first- and second-order models. A possible new application of NaDiR is the modeling of datasets containing semantic norms or concept properties, such as the McRae norms (McRae et al., 2005) or BLESS (Baroni and Lenci, 2011). Those datasets are standard in DSM evaluation, and their modeling can be implemented in terms of a reverse 57 association task, with the additional advantage that the relations between concepts and properties in those datasets are labelled with property types for the McRae norms (e.g., encyclopedic, taxonomic, situated) or semantic relations (e.g., hypernymy, meronymy, event-related) for BLESS. This allows a specific evaluation for each property type or semantic relation, which will in turn give new insights into the semantic knowledge encoded in the different corpus-based representations (f"
W14-4707,C14-2024,1,0.888429,"Missing"
W14-4707,W13-2608,1,0.909748,"NaDiR uses either first-order or second-order co-occurrence statistics to predict the association strength between stimuli and responses. In the first case (“collocations”), we apply one of several standard statistical association measures to co-occurrence counts obtained from a large corpus. In the second case, association is quantified by cosine similarity in a distributional semantic model built from the same corpus. Both first-order and second-order statistics were collected from UKWaC in order to compete in the constrained track of the shared task. Recent experiments (Hare et al., 2009; Lapesa and Evert, 2013; Lapesa et al., to appear) suggest that semantic relations are often better captured by neighbour ranks rather than direct use of statistical association measures or cosine similarity values. Therefore, NaDiR can alternatively quantify association strength by collocate rank and similarity by neighbour rank. In our experiments (section 5), we compare the different approaches. NaDiR is designed for the multiword association task, and it contains additional features related to the particular design of the CogALex shared task: • We reduce the number of candidates by selecting the most likely resp"
W14-4707,S14-1020,1,0.840738,"Missing"
W14-4707,C02-1007,0,0.29957,"aspects of the NaDiR algorithm, described in section 4. 3 Related Work Previous studies based on free association norms differ considerably in terms of the type of task (regular free association task – one stimulus, one response vs. multiword association task – many stimuli, one response), gold standards, and key features of the evaluated models (e.g., source corpora used and choice of a candidate vocabulary from which responses are selected). In regular free association tasks (one stimulus, one response), responses are known to contain both paradigmatically and syntagmatically related words. Rapp (2002) proposes to integrate first-order (cooccurrence lists) and second-order (bag-of-words DSMs) information to distinguish syntagmatic from paradigmatic relations by exploiting the comparison of most salient collocates and nearest neighbors. A task derived from the EAT norms was used in the ESSLLI 2008 shared task2 . Results from firstorder co-occurrence data turned out to be much better than those from second-order DSMs, in line with the findings made by Rapp (2002) and Wettler et al. (2005). A similar picture emerges from studies on the multiword association task. Models based on first-order co"
W14-4707,rapp-2014-corpus,0,0.340042,"rk on NaDiR. 2 The Task and its Problems The shared task datasets are derived from the Edinburgh Associative Thesaurus (Kiss et al., 1973)1 . The Edinburgh Associative Thesaurus (henceforth, EAT) contains free associations to approximately 8000 English cue words. For each cue (e.g., visual) EAT lists all associations collected in the survey (e.g., aid, eyes, aids, see, eye, seen, sight, etc.) sorted according to the number of subjects who responded with the respective word. The CogALex shared task on multiword association is based on the EAT dataset, and is in fact a reverse association task (Rapp, 2014). The top five responses for a target word are provided as stimuli (e.g., aid, eyes, aids, see, eye), and the participating systems are required to generate the original cue as a response (e.g., visual). The training and the test sets are random extracts of 2000 EAT This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1 http://www.eat.rl.ac.uk/ 50 Zock/Rapp/Huang (eds.): Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon"
W15-0709,Q14-1041,1,0.828846,"10. 8. 7. 6. 5. 4. 3. 2. 1. ● ● ● ● ● ● ● ● ● 0.02 relative frequency 0.06 ● ● ● ● ● ● ● ● he that was in I a of to the and 0.00 tains texts published between 1827 and 1934 originating mainly from Ebooks libres et gratuits.3 The collection of German novels consists of texts from the 19th and the first half of the 20th Century which come from the TextGrid collection.4 Our experiments extend the previous study in three respects: 1. We use a different clustering algorithm, partitioning around medoids (Kaufman and Rousseeuw, 1990), which has proven to be very robust especially on linguistic data (Lapesa and Evert, 2014). The number of clusters is set to 25, corresponding to the number of different authors in each of the collections. 2. We evaluate clustering quality using a wellestablished criterion, the chance-adjusted Rand index (Hubert and Arabie, 1985), rather than cluster purity. This improves comparability with other evaluation studies. 3. Jannidis et al. (2015) consider only three arbitrarily chosen values nw = 100, 1000, 5 000. Since clustering quality does not always improve if a larger number of mfw is used, this approach draws an incomplete picture and does not show whether there is a clearly defi"
W16-2606,W14-0612,0,0.0259379,"cases were collected in a supplementary document available to the annotators. The manual tokenization was carried out in a plain text editor, starting from whitespacetokenized files in one-token-per-line format. Annotators were instructed to make no other changes to the files than inserting additional line breaks as token boundaries (except for a few special cases), but were allowed to mark unclear cases with comments. The tokenizations were compared and adjudicated using the kdiff3 utility.5 In the next step, manual tagging was partly carried out with the Web-based annotation platform CorA6 (Bollmann et al., 2014), partly with Annotation procedure All data sets were manually tokenized and PoS tagged by multiple annotators, based on the official tokenization (Beißwenger et al., 2015a) and tagging guidelines (Schiller et al., 1999; Beißwenger et al., 2015b), see Sec. 2.2. Cases of disagreement were then adjudicated by the task or3 5 http://agd.ids-mannheim.de/folk.shtml https://sites.google.com/site/ empirist2015/home/annotation-guidelines http://kdiff3.sourceforge.net/ https://www.linguistics.rub.de/ comphist/resources/cora/ 4 6 47 gold BT BT 96.04 FW 94.05 91.05 It is interesting to note that for both"
W16-2606,P82-1020,0,0.861878,"Missing"
W16-2606,W16-2615,0,0.122656,"Missing"
W16-2606,R15-1049,0,0.156952,"Missing"
W16-2606,W16-2613,0,0.0619604,"Missing"
W16-2606,D11-1141,0,0.0825345,"Missing"
W16-2606,W16-2608,0,0.132775,"Missing"
W16-2606,schafer-bildhauer-2012-building,0,0.0881763,"Missing"
W16-2606,W16-2607,0,0.11803,"Missing"
W16-2606,N03-1033,0,0.074658,"Missing"
W16-2606,zinsmeister-etal-2014-adapting,0,0.0166304,"he same approaches and models, or do we need different tools for the two types of corpora? Tackling the linguistic peculiarities of CMC data with NLP tools is an open issue in corpus and computational linguistics, which has been addressed by an increasing number of papers and approaches over the past years (as a desideratum e.g. Beißwenger and Storrer, 2008; King, 2009; for the development of NLP tools e.g. Ritter et al., 2011; Gimpel et al., 2011; Owoputi et al., 2015; Avontuur et al., 2012; Bartz et al., 2013; Neunerdt et al., 2013; Rehbein, 2013; Rehbein et al., 2013; Horbach et al., 2015; Zinsmeister et al., 2014; Ljubeˇsi´c et al., 2015). Issues of processing and annotating CMC data have also been a central topic 2 The EmpiriST gold standard The gold standard developed for the shared task comprises roughly 10,000 tokens of training data provided to participants as well as roughly 10,000 tokens of unseen test data used in the evaluation phase. It was compiled from data samples considered representative for the two types of corpora: (i) a CMC subset covering discourse from a range of CMC/social media genres, and (ii) a Web corpora subset containing CC-licensed Web pages from different genres. 1 45 http"
W16-2606,P11-2008,0,\N,Missing
W16-2606,W14-4901,0,\N,Missing
W16-2606,W16-2614,0,\N,Missing
W16-5309,P14-1023,0,0.234255,". Licence details: http:// semantic relation. Participants were provided with training and test datasets extracted from EVALution 1.0 (Santus et al., 2015b), as well as a scoring script for evaluating the output of their systems. The shared task has been intended and designed as a “friendly competition”: the goal was to identify strengths and weaknesses of various methods, rather than just “crowning” the best-performing model. In total, seven systems participated in the shared task. Most of them exploited Distributional Semantic Models (DSMs), either of the count-based or word-embedding type (Baroni et al., 2014). Most of them relied on distance or nearest neighbors in subtask 1, and on machine learning classifiers (e.g., Support Vector Machine (SVM), Convolutional Neural Network (CNN) and Random Forest (RF)) in subtask 2. Some systems enriched the DSM representation by adopting patterns (e.g., LexNet, the best system in subtask 2) or extracting distributional properties with unsupervised measures (e.g., ROOT18). This paper reports the results achieved by the participating systems, providing insights about their respective strengths and weaknesses. It is organized as follows. Section 2 surveys similar"
W16-5309,S15-2151,0,0.0565631,"Missing"
W16-5309,S16-1168,0,0.0528522,"Missing"
W16-5309,S07-1003,0,0.0961225,"Missing"
W16-5309,N16-2002,1,0.850221,"as a negative impact on their performance on previously unseen words, lexically split datasets and unseen switched pairs (Santus et al., 2016). One of the ongoing disputes in the NLP community concerns the relative merits and demerits of countbased distributional models and word embeddings (which are obtained by training neural networks rather than counting co-occurrence frequencies). While the latter seem to outperform the former in several tasks such as similarity estimation (Baroni et al., 2014), both types of models are subject to variation at the level of individual linguistic relations (Gladkova et al., 2016). Levy et al. (2015a) have also shown that optimization of hyperparameters can make a bigger difference than the choice between different models. Finally, very recently, several scholars have investigated the possibility of integrating different kinds of information. Kiela et al. (2015) have used image generality for hypernymy detection, while Shwartz et al. (2016) have tried to identify the same relation by combining pattern-based and distributional information. 3 3.1 Shared task Task description The CogALex-V shared task was conducted as a “friendly competition” where participants had access"
W16-5309,C92-2082,0,0.387764,"for the Identification of Semantic Relations Up to this date, several corpus-based approaches to the identification of semantic relations have been proposed. Most of them, however, focus on a single semantic relation with the ambitious objective of isolating it from all the others. Dealing with multiple relations has been found particularly challenging, and few systems have attempted multi-class classifications. The exceptions include Turney (2008) and Pantel and Pennacchiotti (2006). Early approaches rely on lexical-syntactic patterns (e.g. “tools such as hammers”). After the seminal work of Hearst (1992) who sketched methods for pattern discovery, Snow et al. (2004) adopted machine learning over dependency-paths-based features. While these approaches focused on hypernyms, Pantel and Pennacchiotti (2006) introduced Espresso, able to identify several semantic relations (i.e. hypernymy, part-of, succession, reaction and production) as well as to maximize recall by using the Web and precision by assessing the reliability of the patterns. Other pattern-based approaches to synonymy and antonymy are reported by Lin et al. (2003), Turney (2008), Wang et al. (2010) and Lobanova et al. (2010). The majo"
W16-5309,S10-1006,0,0.0774016,"Missing"
W16-5309,P15-2020,0,0.0161834,"(which are obtained by training neural networks rather than counting co-occurrence frequencies). While the latter seem to outperform the former in several tasks such as similarity estimation (Baroni et al., 2014), both types of models are subject to variation at the level of individual linguistic relations (Gladkova et al., 2016). Levy et al. (2015a) have also shown that optimization of hyperparameters can make a bigger difference than the choice between different models. Finally, very recently, several scholars have investigated the possibility of integrating different kinds of information. Kiela et al. (2015) have used image generality for hypernymy detection, while Shwartz et al. (2016) have tried to identify the same relation by combining pattern-based and distributional information. 3 3.1 Shared task Task description The CogALex-V shared task was conducted as a “friendly competition” where participants had access to both training and testing datasets, released on the 8th and the 27th of September 2016, respectively. The participants were asked to evaluate the output of their system with the official evaluation script, released with the test set together with random and majority baselines. Each"
W16-5309,Q14-1041,1,0.84926,"the corpus. In subtask 1, LexNet is combined with vector cosine (calculated on word2vec embeddings trained on Google News) through weights that were learned on a validation set. In subtask 2, in order to avoid a bias towards the majority class RANDOM, a Multi-Layer Perceptron (MLP) is trained and applied only on pairs that were classified as related in subtask 1. The third system, Mach5, investigates the structure and hyperparameters of two traditional dependency-filtered and dependency-structured DSMs trained on a Web corpus of 9.5 billion words. The author sets most parameters according to Lapesa and Evert (2014), focusing on feature selection and optimization of SVD dimensions. Distance information is used directly in subtask 1, while for subtask 2 a linear SVM classifier is applied to 1200-dimensional vectors representing partial Euclidean distance in the two SVD-reduced spaces. Given the competitive results in subtask 1 and the much lower performance achieved in subtask 2, it is evident that Mach5 was optimized for identifying non-random pairs rather than for recognizing and discriminating specific semantic relations. The other systems include ROOT18, which relies on several unsupervised features e"
W16-5309,S12-1012,1,0.87775,"The major limitation of pattern-based approaches is that they require words to co-occur in the same sentence, strongly impacting the recall. Distributional approaches have therefore been adopted to reduce such limitations. They are based on the Distributional Hypothesis (Harris, 1954; Firth, 1957) that words occurring in similar contexts also bear similar meaning. Distributional approaches can be (i) unsupervised, generally consisting of mathematical functions that implement linguistic hypotheses about how and which contexts are relevant to identify specific relations (Kotlerman et al., 2010; Lenci and Benotto, 2012; Santus et al., 2014); or (ii) supervised, generally consisting of algorithms that automatically learn some distributional information about the words holding a specific relation (Weeds et al., 2014; Roller et al., 2014; Roller and Erk, 2016; Santus et al., 2016; Nguyen et al., 2016; Shwartz et al., 2016). While unsupervised approaches are commonly outperformed by supervised ones, the latter – which rely on distributional word vectors, either concatenated or combined through algebraic functions – seem to learn specific lexical properties of the words in the pairs rather than the general seman"
W16-5309,Q15-1016,0,0.0399784,"nsisting of algorithms that automatically learn some distributional information about the words holding a specific relation (Weeds et al., 2014; Roller et al., 2014; Roller and Erk, 2016; Santus et al., 2016; Nguyen et al., 2016; Shwartz et al., 2016). While unsupervised approaches are commonly outperformed by supervised ones, the latter – which rely on distributional word vectors, either concatenated or combined through algebraic functions – seem to learn specific lexical properties of the words in the pairs rather than the general semantic relation existing between them (Weeds et al., 2014; Levy et al., 2015b). This has a negative impact on their performance on previously unseen words, lexically split datasets and unseen switched pairs (Santus et al., 2016). One of the ongoing disputes in the NLP community concerns the relative merits and demerits of countbased distributional models and word embeddings (which are obtained by training neural networks rather than counting co-occurrence frequencies). While the latter seem to outperform the former in several tasks such as similarity estimation (Baroni et al., 2014), both types of models are subject to variation at the level of individual linguistic r"
W16-5309,N15-1098,0,0.0544365,"nsisting of algorithms that automatically learn some distributional information about the words holding a specific relation (Weeds et al., 2014; Roller et al., 2014; Roller and Erk, 2016; Santus et al., 2016; Nguyen et al., 2016; Shwartz et al., 2016). While unsupervised approaches are commonly outperformed by supervised ones, the latter – which rely on distributional word vectors, either concatenated or combined through algebraic functions – seem to learn specific lexical properties of the words in the pairs rather than the general semantic relation existing between them (Weeds et al., 2014; Levy et al., 2015b). This has a negative impact on their performance on previously unseen words, lexically split datasets and unseen switched pairs (Santus et al., 2016). One of the ongoing disputes in the NLP community concerns the relative merits and demerits of countbased distributional models and word embeddings (which are obtained by training neural networks rather than counting co-occurrence frequencies). While the latter seem to outperform the former in several tasks such as similarity estimation (Baroni et al., 2014), both types of models are subject to variation at the level of individual linguistic r"
W16-5309,J10-3003,0,0.034749,"valuation metrics, the seven participating systems and their results. The best performing system in subtask 1 is GHHH (F1 = 0.790), while the best system in subtask 2 is LexNet (F1 = 0.445). The dataset and the task description are available at https://sites.google.com/site/cogalex2016/home/shared-task. 1 Introduction Determining automatically if words are semantically related, and in what way, is important for Natural Language Processing (NLP) applications such as thesaurus generation (Grefenstette, 1994), ontology learning (Zouaq and Nkambou, 2008), paraphrase generation and identification (Madnani and Dorr, 2010), as well as for drawing inferences (Martinez-G´omez et al., 2016). Many NLP applications make use of handcrafted resources such as WordNet (Fellbaum, 1998). However, creating these resources is expensive and time-consuming; they are available for only a few languages, and their coverage inevitably lags behind the lexical and conceptual proliferation. In the last decades, a number of corpus-based approaches have investigated the possibility of identifying lexical semantic relations by observing word usage. Even though these methods are still far from being able to provide a comprehensive model"
W16-5309,P16-4015,0,0.0228902,"Missing"
W16-5309,P16-2074,0,0.0481365,"rth, 1957) that words occurring in similar contexts also bear similar meaning. Distributional approaches can be (i) unsupervised, generally consisting of mathematical functions that implement linguistic hypotheses about how and which contexts are relevant to identify specific relations (Kotlerman et al., 2010; Lenci and Benotto, 2012; Santus et al., 2014); or (ii) supervised, generally consisting of algorithms that automatically learn some distributional information about the words holding a specific relation (Weeds et al., 2014; Roller et al., 2014; Roller and Erk, 2016; Santus et al., 2016; Nguyen et al., 2016; Shwartz et al., 2016). While unsupervised approaches are commonly outperformed by supervised ones, the latter – which rely on distributional word vectors, either concatenated or combined through algebraic functions – seem to learn specific lexical properties of the words in the pairs rather than the general semantic relation existing between them (Weeds et al., 2014; Levy et al., 2015b). This has a negative impact on their performance on previously unseen words, lexically split datasets and unseen switched pairs (Santus et al., 2016). One of the ongoing disputes in the NLP community concerns"
W16-5309,P06-1015,0,0.206698,"well as further information about the shared task are available at https://sites.google. com/site/cogalex2016/home/shared-task. 70 2.2 Methods for the Identification of Semantic Relations Up to this date, several corpus-based approaches to the identification of semantic relations have been proposed. Most of them, however, focus on a single semantic relation with the ambitious objective of isolating it from all the others. Dealing with multiple relations has been found particularly challenging, and few systems have attempted multi-class classifications. The exceptions include Turney (2008) and Pantel and Pennacchiotti (2006). Early approaches rely on lexical-syntactic patterns (e.g. “tools such as hammers”). After the seminal work of Hearst (1992) who sketched methods for pattern discovery, Snow et al. (2004) adopted machine learning over dependency-paths-based features. While these approaches focused on hypernyms, Pantel and Pennacchiotti (2006) introduced Espresso, able to identify several semantic relations (i.e. hypernymy, part-of, succession, reaction and production) as well as to maximize recall by using the Web and precision by assessing the reliability of the patterns. Other pattern-based approaches to sy"
W16-5309,D14-1162,0,0.0882794,"nal publicly available word embeddings trained on huge corpora (Google News, Common Crawl and Wikipedia + Gigaword 5). The authors found that linear regression works better in subtask 1 (i.e. binary 74 classification), while multi-task CNN performs better in subtask 2, which involves multi-class classification. Analogy was instead found less appropriate for semantic relation identification. LexNet relies on Wikipedia + Gigaword 5 and Google News corpora, leveraging the combination of distributional and path-based information. The authors merged the 50-dimensional GloVe pre-trained embeddings (Pennington et al., 2014) for the words in the pairs with the average embedding vector – created using a LSTM (Hochreiter and Schmidhuber, 1997) – of all the dependency paths that connect them in the corpus. In subtask 1, LexNet is combined with vector cosine (calculated on word2vec embeddings trained on Google News) through weights that were learned on a validation set. In subtask 2, in order to avoid a bias towards the majority class RANDOM, a Multi-Layer Perceptron (MLP) is trained and applied only on pairs that were classified as related in subtask 1. The third system, Mach5, investigates the structure and hyperpa"
W16-5309,D16-1234,0,0.0642002,"Distributional Hypothesis (Harris, 1954; Firth, 1957) that words occurring in similar contexts also bear similar meaning. Distributional approaches can be (i) unsupervised, generally consisting of mathematical functions that implement linguistic hypotheses about how and which contexts are relevant to identify specific relations (Kotlerman et al., 2010; Lenci and Benotto, 2012; Santus et al., 2014); or (ii) supervised, generally consisting of algorithms that automatically learn some distributional information about the words holding a specific relation (Weeds et al., 2014; Roller et al., 2014; Roller and Erk, 2016; Santus et al., 2016; Nguyen et al., 2016; Shwartz et al., 2016). While unsupervised approaches are commonly outperformed by supervised ones, the latter – which rely on distributional word vectors, either concatenated or combined through algebraic functions – seem to learn specific lexical properties of the words in the pairs rather than the general semantic relation existing between them (Weeds et al., 2014; Levy et al., 2015b). This has a negative impact on their performance on previously unseen words, lexically split datasets and unseen switched pairs (Santus et al., 2016). One of the ongo"
W16-5309,C14-1097,0,0.0666826,"hey are based on the Distributional Hypothesis (Harris, 1954; Firth, 1957) that words occurring in similar contexts also bear similar meaning. Distributional approaches can be (i) unsupervised, generally consisting of mathematical functions that implement linguistic hypotheses about how and which contexts are relevant to identify specific relations (Kotlerman et al., 2010; Lenci and Benotto, 2012; Santus et al., 2014); or (ii) supervised, generally consisting of algorithms that automatically learn some distributional information about the words holding a specific relation (Weeds et al., 2014; Roller et al., 2014; Roller and Erk, 2016; Santus et al., 2016; Nguyen et al., 2016; Shwartz et al., 2016). While unsupervised approaches are commonly outperformed by supervised ones, the latter – which rely on distributional word vectors, either concatenated or combined through algebraic functions – seem to learn specific lexical properties of the words in the pairs rather than the general semantic relation existing between them (Weeds et al., 2014; Levy et al., 2015b). This has a negative impact on their performance on previously unseen words, lexically split datasets and unseen switched pairs (Santus et al.,"
W16-5309,W15-4208,1,0.913468,"m gladkova@phiz.c.u-tokyo.ac.jp Stefan Evert FAU Erlangen-N¨urnberg, Germany stefan.evert@fau.de Alessandro Lenci University of Pisa, Italy alessandro.lenci@unipi.it Abstract The shared task of the 5th Workshop on Cognitive Aspects of the Lexicon (CogALex-V) aims at providing a common benchmark for testing current corpus-based methods for the identification of lexical semantic relations (synonymy, antonymy, hypernymy, part-whole meronymy) and at gaining a better understanding of their respective strengths and weaknesses. The shared task uses a challenging dataset extracted from EVALution 1.0 (Santus et al., 2015b), which contains word pairs holding the above-mentioned relations as well as semantically unrelated control items (random). The task is split into two subtasks: (i) identification of related word pairs vs. unrelated ones; (ii) classification of the word pairs according to their semantic relation. This paper describes the subtasks, the dataset, the evaluation metrics, the seven participating systems and their results. The best performing system in subtask 1 is GHHH (F1 = 0.790), while the best system in subtask 2 is LexNet (F1 = 0.445). The dataset and the task description are available at ht"
W16-5309,L16-1722,1,0.853809,"sis (Harris, 1954; Firth, 1957) that words occurring in similar contexts also bear similar meaning. Distributional approaches can be (i) unsupervised, generally consisting of mathematical functions that implement linguistic hypotheses about how and which contexts are relevant to identify specific relations (Kotlerman et al., 2010; Lenci and Benotto, 2012; Santus et al., 2014); or (ii) supervised, generally consisting of algorithms that automatically learn some distributional information about the words holding a specific relation (Weeds et al., 2014; Roller et al., 2014; Roller and Erk, 2016; Santus et al., 2016; Nguyen et al., 2016; Shwartz et al., 2016). While unsupervised approaches are commonly outperformed by supervised ones, the latter – which rely on distributional word vectors, either concatenated or combined through algebraic functions – seem to learn specific lexical properties of the words in the pairs rather than the general semantic relation existing between them (Weeds et al., 2014; Levy et al., 2015b). This has a negative impact on their performance on previously unseen words, lexically split datasets and unseen switched pairs (Santus et al., 2016). One of the ongoing disputes in the N"
W16-5309,P16-1226,0,0.369399,"occurring in similar contexts also bear similar meaning. Distributional approaches can be (i) unsupervised, generally consisting of mathematical functions that implement linguistic hypotheses about how and which contexts are relevant to identify specific relations (Kotlerman et al., 2010; Lenci and Benotto, 2012; Santus et al., 2014); or (ii) supervised, generally consisting of algorithms that automatically learn some distributional information about the words holding a specific relation (Weeds et al., 2014; Roller et al., 2014; Roller and Erk, 2016; Santus et al., 2016; Nguyen et al., 2016; Shwartz et al., 2016). While unsupervised approaches are commonly outperformed by supervised ones, the latter – which rely on distributional word vectors, either concatenated or combined through algebraic functions – seem to learn specific lexical properties of the words in the pairs rather than the general semantic relation existing between them (Weeds et al., 2014; Levy et al., 2015b). This has a negative impact on their performance on previously unseen words, lexically split datasets and unseen switched pairs (Santus et al., 2016). One of the ongoing disputes in the NLP community concerns the relative merits an"
W16-5309,C08-1114,0,0.0585021,"and test data as well as further information about the shared task are available at https://sites.google. com/site/cogalex2016/home/shared-task. 70 2.2 Methods for the Identification of Semantic Relations Up to this date, several corpus-based approaches to the identification of semantic relations have been proposed. Most of them, however, focus on a single semantic relation with the ambitious objective of isolating it from all the others. Dealing with multiple relations has been found particularly challenging, and few systems have attempted multi-class classifications. The exceptions include Turney (2008) and Pantel and Pennacchiotti (2006). Early approaches rely on lexical-syntactic patterns (e.g. “tools such as hammers”). After the seminal work of Hearst (1992) who sketched methods for pattern discovery, Snow et al. (2004) adopted machine learning over dependency-paths-based features. While these approaches focused on hypernyms, Pantel and Pennacchiotti (2006) introduced Espresso, able to identify several semantic relations (i.e. hypernymy, part-of, succession, reaction and production) as well as to maximize recall by using the Web and precision by assessing the reliability of the patterns."
W16-5309,C14-1212,0,0.0769981,"such limitations. They are based on the Distributional Hypothesis (Harris, 1954; Firth, 1957) that words occurring in similar contexts also bear similar meaning. Distributional approaches can be (i) unsupervised, generally consisting of mathematical functions that implement linguistic hypotheses about how and which contexts are relevant to identify specific relations (Kotlerman et al., 2010; Lenci and Benotto, 2012; Santus et al., 2014); or (ii) supervised, generally consisting of algorithms that automatically learn some distributional information about the words holding a specific relation (Weeds et al., 2014; Roller et al., 2014; Roller and Erk, 2016; Santus et al., 2016; Nguyen et al., 2016; Shwartz et al., 2016). While unsupervised approaches are commonly outperformed by supervised ones, the latter – which rely on distributional word vectors, either concatenated or combined through algebraic functions – seem to learn specific lexical properties of the words in the pairs rather than the general semantic relation existing between them (Weeds et al., 2014; Levy et al., 2015b). This has a negative impact on their performance on previously unseen words, lexically split datasets and unseen switched p"
W16-5312,P07-2009,0,0.0194465,"Recall 100 Training data |unreduced DSM 40 50 60 70 80 90 1 100 2 5 10 20 50 100 200 500 K features threshold Figure 1: Left panel: Application of DSM distances (angles) to the identification of semantically related words. Right panel: Performance in subtask 1 depending on the number of feature dimensions used. advantage of this huge corpus is its full coverage of the CogALex-V training and test sets, so that no special handling of unseen words is required. Both a dependency-filtered and a dependency-structured DSM were compiled from syntactic dependencies obtained with the robust C&C parser (Curran et al., 2007). The target vocabulary of 26,450 lemmas extends the vocabulary of Distributional Memory (Baroni and Lenci, 2010) with all words in the training and test sets of the shared task. Similar to the gold standard, the DSM uses lemmatized words (from TreeTagger) but does not distinguish between homonyms with different parts of speech (such as clearADJ and clearVERB ). The 120,000 most frequent lemmas were extracted as features for the dependency-filtered model (henceforth DepFilt); the 300,000 most frequent relation-lemma combinations (e.g. OBJ=cat) were extracted as features for the dependency-stru"
W16-5312,C14-2024,1,0.822111,"ith all words in the training and test sets of the shared task. Similar to the gold standard, the DSM uses lemmatized words (from TreeTagger) but does not distinguish between homonyms with different parts of speech (such as clearADJ and clearVERB ). The 120,000 most frequent lemmas were extracted as features for the dependency-filtered model (henceforth DepFilt); the 300,000 most frequent relation-lemma combinations (e.g. OBJ=cat) were extracted as features for the dependency-structured model (henceforth DepStruct). Some basic parameters were set according to the recommendations of Lapesa and Evert (2014): sparse (i.e. non-negative) simple log-likelihood (simple-ll) is used as an association measure for feature weighting and an additional log transformation is applied to the simple-ll scores. The models use angular distance (equivalent to cosine similarity) and explore logarithmic neighbour rank as an index of semantic (dis)similarity. Other parameters are tuned incrementally on the training data, as described in the following subsections. The main tuning criterion is the F1 score achieved by an optimal cutoff threshold on the training data of subtask 1. 2.1 Feature selection A first step is t"
W16-5312,W14-1503,0,0.236948,"Missing"
W16-5312,Q14-1041,1,0.885287,"ci, 2010) with all words in the training and test sets of the shared task. Similar to the gold standard, the DSM uses lemmatized words (from TreeTagger) but does not distinguish between homonyms with different parts of speech (such as clearADJ and clearVERB ). The 120,000 most frequent lemmas were extracted as features for the dependency-filtered model (henceforth DepFilt); the 300,000 most frequent relation-lemma combinations (e.g. OBJ=cat) were extracted as features for the dependency-structured model (henceforth DepStruct). Some basic parameters were set according to the recommendations of Lapesa and Evert (2014): sparse (i.e. non-negative) simple log-likelihood (simple-ll) is used as an association measure for feature weighting and an additional log transformation is applied to the simple-ll scores. The models use angular distance (equivalent to cosine similarity) and explore logarithmic neighbour rank as an index of semantic (dis)similarity. Other parameters are tuned incrementally on the training data, as described in the following subsections. The main tuning criterion is the F1 score achieved by an optimal cutoff threshold on the training data of subtask 1. 2.1 Feature selection A first step is t"
W16-5312,J07-2002,0,0.155125,"Missing"
W16-5312,W15-4208,0,0.0513338,"Missing"
W16-5312,schafer-bildhauer-2012-building,0,0.0253598,"Missing"
W16-5312,J98-1004,0,0.673541,"Missing"
W18-6234,S14-2096,1,0.879651,"Missing"
W18-6234,S13-2065,1,0.896663,"Missing"
W18-6234,W16-2607,1,0.803314,"ation period). Note that we do not restrict the contexts in which the emotion words occur, i. e. the emotion words do not have to be followed by that, because or when. After balancing the data, we have approximately 159.000 items per class. System Description 3.1 Representations derived through unsupervised methods Data Preprocessing and Additional Data The data sets released by the organizers of the shared task contain the full text of the tweets, with the emotion word, usernames and URLs being substituted by placeholders. We tokenize the text with the web and social media tokenizer SoMaJo2 (Proisl and Uhrig, 2016) and convert it to lowercase. In addition to the official data sets, we use two resources: ENCOW143 (Sch¨afer and Bildhauer, 2012; Sch¨afer, 2015) and an in-house collection of 114 million deduplicated English tweets (see Sch¨afer et al. (2017) for the deduplication algorithm), collected between February 2017 and June 2018.4 We tokenize the tweets with SoMaJo (but not ENCOW14, which is already tokenized), mask 3.4 Network Architecture We experiment with three variants of a neural network architecture implemented using Keras6 (Chollet et al., 2015) and visualized in Figure 1. 2 https://github.c"
W18-6234,P82-1020,0,0.789499,"Missing"
W18-6234,W18-6207,0,0.0418002,"Missing"
W18-6234,W18-6206,0,0.0660866,"Missing"
W18-6234,C16-1146,0,0.0611665,"Missing"
W18-6234,schafer-bildhauer-2012-building,0,0.0187696,"Missing"
